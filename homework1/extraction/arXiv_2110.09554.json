{
    "id_table_1": {
        "caption": "Table 1: 2D and 3D pose estimation accuracy comparison on Human3.6M. The metric of 2D pose is JDR (%), and the metric of 3D pose is MPJPE (mm). All networks are pretrained on COCO [Lin et al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,\r\nDollár, and Zitnick] and then finetuned on Human 3.6M [Ionescu et al.(2014)Ionescu, Papava, Olaru, and\r\nSminchisescu]. All images are resized to 256×256256256256\\times 256.",
        "table": [
            "<table id=\"S4.T1.2.2\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T1.2.2.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.2.2.2.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\">Method</th>&#13;\n<th id=\"S4.T1.2.2.2.4\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">Params</th>&#13;\n<td id=\"S4.T1.2.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_tt\">MACs</td>&#13;\n<td id=\"S4.T1.2.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">Inference Time (s)</td>&#13;\n<td id=\"S4.T1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">JDR (%) <math id=\"S4.T1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S4.T1.1.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S4.T1.1.1.1.1.m1.1.1\" xref=\"S4.T1.1.1.1.1.m1.1.1.cmml\">&#8593;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.1.1.1.1.m1.1b\"><ci id=\"S4.T1.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T1.1.1.1.1.m1.1.1\">&#8593;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.1.1.1.1.m1.1c\">\\uparrow</annotation></semantics></math>&#13;\n</td>&#13;\n<td id=\"S4.T1.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">MPJPE (mm) <math id=\"S4.T1.2.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T1.2.2.2.2.m1.1a\"><mo stretchy=\"false\" id=\"S4.T1.2.2.2.2.m1.1.1\" xref=\"S4.T1.2.2.2.2.m1.1.1.cmml\">&#8595;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.2.2.2.2.m1.1b\"><ci id=\"S4.T1.2.2.2.2.m1.1.1.cmml\" xref=\"S4.T1.2.2.2.2.m1.1.1\">&#8595;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.2.2.2.2.m1.1c\">\\downarrow</annotation></semantics></math>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.2.2.3.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.2.2.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Single view - Simple Baseline<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bibx53\" title=\"\" class=\"ltx_ref\">Xiao et&#160;al.(2018)Xiao, Wu, and Wei</a>]</cite>&#13;\n</th>&#13;\n<th id=\"S4.T1.2.2.3.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">34M</th>&#13;\n<td id=\"S4.T1.2.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">51.7G</td>&#13;\n<td id=\"S4.T1.2.2.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">-</td>&#13;\n<td id=\"S4.T1.2.2.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">98.5</td>&#13;\n<td id=\"S4.T1.2.2.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">30.2</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.2.2.4.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.2.2.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Single view - TransPose <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bibx56\" title=\"\" class=\"ltx_ref\">Yang et&#160;al.(2020)Yang, Quan, Nie, and Yang</a>]</cite>&#13;\n</th>&#13;\n<th id=\"S4.T1.2.2.4.2.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">5M</th>&#13;\n<td id=\"S4.T1.2.2.4.2.3\" class=\"ltx_td ltx_align_center\">43.6G</td>&#13;\n<td id=\"S4.T1.2.2.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\">-</td>&#13;\n<td id=\"S4.T1.2.2.4.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\">98.6</td>&#13;\n<td id=\"S4.T1.2.2.4.2.6\" class=\"ltx_td ltx_align_center\">30.5</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.2.2.5.3\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.2.2.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Crossview Fusion <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bibx34\" title=\"\" class=\"ltx_ref\">Qiu et&#160;al.(2019)Qiu, Wang, Wang, Wang, and Zeng</a>]</cite>&#13;\n</th>&#13;\n<th id=\"S4.T1.2.2.5.3.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">235M</th>&#13;\n<td id=\"S4.T1.2.2.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">55.1G</td>&#13;\n<td id=\"S4.T1.2.2.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.048</td>&#13;\n<td id=\"S4.T1.2.2.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.2.2.5.3.5.1\" class=\"ltx_text ltx_font_bold\">99.4</span></td>&#13;\n<td id=\"S4.T1.2.2.5.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\">27.8</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.2.2.6.4\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.2.2.6.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Epipolar Transformer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bibx19\" title=\"\" class=\"ltx_ref\">He et&#160;al.(2020)He, Yan, Fragkiadaki, and Yu</a>]</cite>&#13;\n</th>&#13;\n<th id=\"S4.T1.2.2.6.4.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">34M</th>&#13;\n<td id=\"S4.T1.2.2.6.4.3\" class=\"ltx_td ltx_align_center\">51.7G</td>&#13;\n<td id=\"S4.T1.2.2.6.4.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0.086</td>&#13;\n<td id=\"S4.T1.2.2.6.4.5\" class=\"ltx_td ltx_align_center ltx_border_r\">98.6</td>&#13;\n<td id=\"S4.T1.2.2.6.4.6\" class=\"ltx_td ltx_align_center\">27.1</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.2.2.7.5\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.2.2.7.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">TransFusion</th>&#13;\n<th id=\"S4.T1.2.2.7.5.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span id=\"S4.T1.2.2.7.5.2.1\" class=\"ltx_text ltx_font_bold\">5M</span></th>&#13;\n<td id=\"S4.T1.2.2.7.5.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">50.2G</td>&#13;\n<td id=\"S4.T1.2.2.7.5.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T1.2.2.7.5.4.1\" class=\"ltx_text ltx_font_bold\">0.032</span></td>&#13;\n<td id=\"S4.T1.2.2.7.5.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T1.2.2.7.5.5.1\" class=\"ltx_text ltx_font_bold\">99.4</span></td>&#13;\n<td id=\"S4.T1.2.2.7.5.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.2.2.7.5.6.1\" class=\"ltx_text ltx_font_bold\">25.8</span></td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "[Xiao et al.(2018)Xiao, Wu, and Wei] [Xiao et al.(2018)Xiao, Wu, and Wei]\r\n\r\nBin Xiao, Haiping Wu, and Yichen Wei.\r\n\r\n\r\nSimple baselines for human pose estimation and tracking.\r\n\r\n\r\nIn Proceedings of the European conference on computer vision\r\n(ECCV), pages 466–481, 2018.",
            "[Yang et al.(2020)Yang, Quan, Nie, and Yang] [Yang et al.(2020)Yang, Quan, Nie, and Yang]\r\n\r\nSen Yang, Zhibin Quan, Mu Nie, and Wankou Yang.\r\n\r\n\r\nTranspose: Towards explainable human pose estimation by transformer.\r\n\r\n\r\narXiv preprint arXiv:2012.14214, 2020.",
            "[Qiu et al.(2019)Qiu, Wang, Wang, Wang, and Zeng] [Qiu et al.(2019)Qiu, Wang, Wang, Wang, and Zeng]\r\n\r\nHaibo Qiu, Chunyu Wang, Jingdong Wang, Naiyan Wang, and Wenjun Zeng.\r\n\r\n\r\nCross view fusion for 3d human pose estimation.\r\n\r\n\r\nIn Proceedings of the IEEE/CVF International Conference on\r\nComputer Vision, pages 4342–4351, 2019.",
            "[He et al.(2020)He, Yan, Fragkiadaki, and Yu] [He et al.(2020)He, Yan, Fragkiadaki, and Yu]\r\n\r\nYihui He, Rui Yan, Katerina Fragkiadaki, and Shoou-I Yu.\r\n\r\n\r\nEpipolar transformers.\r\n\r\n\r\nIn Proceedings of the IEEE/CVF Conference on Computer Vision\r\nand Pattern Recognition, pages 7779–7788, 2020.",
            "[Lin et al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,\r\nDollár, and Zitnick] [Lin et al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,\r\nDollár, and Zitnick]\r\n\r\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva\r\nRamanan, Piotr Dollár, and C Lawrence Zitnick.\r\n\r\n\r\nMicrosoft coco: Common objects in context.\r\n\r\n\r\nIn European conference on computer vision, pages 740–755.\r\nSpringer, 2014.",
            "[Ionescu et al.(2014)Ionescu, Papava, Olaru, and\r\nSminchisescu] [Ionescu et al.(2014)Ionescu, Papava, Olaru, and\r\nSminchisescu]\r\n\r\nCatalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu.\r\n\r\n\r\nHuman3.6m: Large scale datasets and predictive methods for 3d human\r\nsensing in natural environments.\r\n\r\n\r\nIEEE Transactions on Pattern Analysis and Machine\r\nIntelligence, 36(7):1325–1339, jul 2014."
        ],
        "references": [
            "The results of both 2D and 3D pose estimation are shown in Table 1.\r\nWe also shown the number of parameters of each model, the MACs (multiply-add operations).\r\nBesides, we also report the inference time to obtain the 3D pose from 4 views on a single 2080Ti GPU of all multiview methods.\r\nFor both 2D and 3D pose estimation, TransFusion consistently outperforms or achieves comparable performance with epipolar transformers [He et al.(2020)He, Yan, Fragkiadaki, and Yu] and cross-view fusion [Qiu et al.(2019)Qiu, Wang, Wang, Wang, and Zeng].\r\nNote that JDR is a relative loose metric, with a wider threshold which tolerates small errors, so the improvement on 2D is not very obvious. However, on the 3D metric, which directly computes the distance, our improvement is much more significant. Moreover, as in Table 2, our method can achieve significant improvement on sophisticated poses sequences such as \"Phone\" and \"Smoke\", which usually encounters heave occlusions for certain views.\r\nThis result suggests that fusing features from the entire images of other views, instead of just features along the epipolar line [He et al.(2020)He, Yan, Fragkiadaki, and Yu], can bring more benefits.\r\nBesides, comparing to the single view TransPose [Yang et al.(2020)Yang, Quan, Nie, and Yang], our Transfusion can achieve 4.74.74.7 mm gain on 3D. Thus, the improvement is not only from the TransPose architecture, but from the fusion with other views.\r\nMoreover, our method is lightweight and efficient. It only requires 2.1%percent2.12.1\\% (5M / 235M) of the parameters of cross-view fusion [Qiu et al.(2019)Qiu, Wang, Wang, Wang, and Zeng]. Benefit from the parallel computing of transformers architectures, it further reduces the inference time, while the operation of sampling along epipolar lines [He et al.(2020)He, Yan, Fragkiadaki, and Yu] is time-consuming."
        ]
    },
    "id_table_2": {
        "caption": "Table 2: The MPJPE of each pose sequence on Human 3.6M.",
        "table": [
            "<table id=\"S4.T2.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T2.1.1.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\">Method</th>&#13;\n<th id=\"S4.T2.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Dir</th>&#13;\n<th id=\"S4.T2.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Disc</th>&#13;\n<th id=\"S4.T2.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Eat</th>&#13;\n<th id=\"S4.T2.1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Greet</th>&#13;\n<th id=\"S4.T2.1.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Phone</th>&#13;\n<th id=\"S4.T2.1.1.1.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Pose</th>&#13;\n<th id=\"S4.T2.1.1.1.1.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Purch</th>&#13;\n<th id=\"S4.T2.1.1.1.1.9\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Sit</th>&#13;\n<th id=\"S4.T2.1.1.1.1.10\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">SitD</th>&#13;\n<th id=\"S4.T2.1.1.1.1.11\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Smoke</th>&#13;\n<th id=\"S4.T2.1.1.1.1.12\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Photo</th>&#13;\n<th id=\"S4.T2.1.1.1.1.13\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Wait</th>&#13;\n<th id=\"S4.T2.1.1.1.1.14\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WalkD</th>&#13;\n<th id=\"S4.T2.1.1.1.1.15\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Walk</th>&#13;\n<th id=\"S4.T2.1.1.1.1.16\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WalkT</th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T2.1.1.2.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Crossview Fusion<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bibx34\" title=\"\" class=\"ltx_ref\">Qiu et&#160;al.(2019)Qiu, Wang, Wang, Wang, and Zeng</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T2.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">24.0</td>&#13;\n<td id=\"S4.T2.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">28.8</td>&#13;\n<td id=\"S4.T2.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">25.6</td>&#13;\n<td id=\"S4.T2.1.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">24.5</td>&#13;\n<td id=\"S4.T2.1.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">28.3</td>&#13;\n<td id=\"S4.T2.1.1.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">24.4</td>&#13;\n<td id=\"S4.T2.1.1.2.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\">26.9</td>&#13;\n<td id=\"S4.T2.1.1.2.1.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.1.1.2.1.9.1\" class=\"ltx_text ltx_font_bold\">30.7</span></td>&#13;\n<td id=\"S4.T2.1.1.2.1.10\" class=\"ltx_td ltx_align_center ltx_border_t\">34.4</td>&#13;\n<td id=\"S4.T2.1.1.2.1.11\" class=\"ltx_td ltx_align_center ltx_border_t\">29.0</td>&#13;\n<td id=\"S4.T2.1.1.2.1.12\" class=\"ltx_td ltx_align_center ltx_border_t\">32.6</td>&#13;\n<td id=\"S4.T2.1.1.2.1.13\" class=\"ltx_td ltx_align_center ltx_border_t\">25.1</td>&#13;\n<td id=\"S4.T2.1.1.2.1.14\" class=\"ltx_td ltx_align_center ltx_border_t\">24.3</td>&#13;\n<td id=\"S4.T2.1.1.2.1.15\" class=\"ltx_td ltx_align_center ltx_border_t\">30.8</td>&#13;\n<td id=\"S4.T2.1.1.2.1.16\" class=\"ltx_td ltx_align_center ltx_border_t\">24.9</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.1.1.3.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Epipolar transformers <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bibx19\" title=\"\" class=\"ltx_ref\">He et&#160;al.(2020)He, Yan, Fragkiadaki, and Yu</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T2.1.1.3.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.1.1.3.2.2.1\" class=\"ltx_text ltx_font_bold\">23.2</span></td>&#13;\n<td id=\"S4.T2.1.1.3.2.3\" class=\"ltx_td ltx_align_center\">27.1</td>&#13;\n<td id=\"S4.T2.1.1.3.2.4\" class=\"ltx_td ltx_align_center\">23.4</td>&#13;\n<td id=\"S4.T2.1.1.3.2.5\" class=\"ltx_td ltx_align_center\">22.4</td>&#13;\n<td id=\"S4.T2.1.1.3.2.6\" class=\"ltx_td ltx_align_center\">32.4</td>&#13;\n<td id=\"S4.T2.1.1.3.2.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.1.1.3.2.7.1\" class=\"ltx_text ltx_font_bold\">21.4</span></td>&#13;\n<td id=\"S4.T2.1.1.3.2.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.1.1.3.2.8.1\" class=\"ltx_text ltx_font_bold\">22.6</span></td>&#13;\n<td id=\"S4.T2.1.1.3.2.9\" class=\"ltx_td ltx_align_center\">37.3</td>&#13;\n<td id=\"S4.T2.1.1.3.2.10\" class=\"ltx_td ltx_align_center\">35.4</td>&#13;\n<td id=\"S4.T2.1.1.3.2.11\" class=\"ltx_td ltx_align_center\">29.0</td>&#13;\n<td id=\"S4.T2.1.1.3.2.12\" class=\"ltx_td ltx_align_center\">27.7</td>&#13;\n<td id=\"S4.T2.1.1.3.2.13\" class=\"ltx_td ltx_align_center\">24.2</td>&#13;\n<td id=\"S4.T2.1.1.3.2.14\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.1.1.3.2.14.1\" class=\"ltx_text ltx_font_bold\">21.2</span></td>&#13;\n<td id=\"S4.T2.1.1.3.2.15\" class=\"ltx_td ltx_align_center\">26.6</td>&#13;\n<td id=\"S4.T2.1.1.3.2.16\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.1.1.3.2.16.1\" class=\"ltx_text ltx_font_bold\">22.3</span></td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.1.1.4.3\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.1.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">TransFusion</th>&#13;\n<td id=\"S4.T2.1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">24.4</td>&#13;\n<td id=\"S4.T2.1.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.1.1.4.3.3.1\" class=\"ltx_text ltx_font_bold\">26.4</span></td>&#13;\n<td id=\"S4.T2.1.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.1.1.4.3.4.1\" class=\"ltx_text ltx_font_bold\">23.4</span></td>&#13;\n<td id=\"S4.T2.1.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.1.1.4.3.5.1\" class=\"ltx_text ltx_font_bold\">21.1</span></td>&#13;\n<td id=\"S4.T2.1.1.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.1.1.4.3.6.1\" class=\"ltx_text ltx_font_bold\">25.2</span></td>&#13;\n<td id=\"S4.T2.1.1.4.3.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">23.2</td>&#13;\n<td id=\"S4.T2.1.1.4.3.8\" class=\"ltx_td ltx_align_center ltx_border_bb\">24.7</td>&#13;\n<td id=\"S4.T2.1.1.4.3.9\" class=\"ltx_td ltx_align_center ltx_border_bb\">33.8</td>&#13;\n<td id=\"S4.T2.1.1.4.3.10\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.1.1.4.3.10.1\" class=\"ltx_text ltx_font_bold\">29.8</span></td>&#13;\n<td id=\"S4.T2.1.1.4.3.11\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.1.1.4.3.11.1\" class=\"ltx_text ltx_font_bold\">26.4</span></td>&#13;\n<td id=\"S4.T2.1.1.4.3.12\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.1.1.4.3.12.1\" class=\"ltx_text ltx_font_bold\">26.8</span></td>&#13;\n<td id=\"S4.T2.1.1.4.3.13\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.1.1.4.3.13.1\" class=\"ltx_text ltx_font_bold\">24.2</span></td>&#13;\n<td id=\"S4.T2.1.1.4.3.14\" class=\"ltx_td ltx_align_center ltx_border_bb\">23.2</td>&#13;\n<td id=\"S4.T2.1.1.4.3.15\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.1.1.4.3.15.1\" class=\"ltx_text ltx_font_bold\">26.1</span></td>&#13;\n<td id=\"S4.T2.1.1.4.3.16\" class=\"ltx_td ltx_align_center ltx_border_bb\">23.3</td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "[Qiu et al.(2019)Qiu, Wang, Wang, Wang, and Zeng] [Qiu et al.(2019)Qiu, Wang, Wang, Wang, and Zeng]\r\n\r\nHaibo Qiu, Chunyu Wang, Jingdong Wang, Naiyan Wang, and Wenjun Zeng.\r\n\r\n\r\nCross view fusion for 3d human pose estimation.\r\n\r\n\r\nIn Proceedings of the IEEE/CVF International Conference on\r\nComputer Vision, pages 4342–4351, 2019.",
            "[He et al.(2020)He, Yan, Fragkiadaki, and Yu] [He et al.(2020)He, Yan, Fragkiadaki, and Yu]\r\n\r\nYihui He, Rui Yan, Katerina Fragkiadaki, and Shoou-I Yu.\r\n\r\n\r\nEpipolar transformers.\r\n\r\n\r\nIn Proceedings of the IEEE/CVF Conference on Computer Vision\r\nand Pattern Recognition, pages 7779–7788, 2020."
        ],
        "references": [
            "The results of both 2D and 3D pose estimation are shown in Table 1.\r\nWe also shown the number of parameters of each model, the MACs (multiply-add operations).\r\nBesides, we also report the inference time to obtain the 3D pose from 4 views on a single 2080Ti GPU of all multiview methods.\r\nFor both 2D and 3D pose estimation, TransFusion consistently outperforms or achieves comparable performance with epipolar transformers [He et al.(2020)He, Yan, Fragkiadaki, and Yu] and cross-view fusion [Qiu et al.(2019)Qiu, Wang, Wang, Wang, and Zeng].\r\nNote that JDR is a relative loose metric, with a wider threshold which tolerates small errors, so the improvement on 2D is not very obvious. However, on the 3D metric, which directly computes the distance, our improvement is much more significant. Moreover, as in Table 2, our method can achieve significant improvement on sophisticated poses sequences such as \"Phone\" and \"Smoke\", which usually encounters heave occlusions for certain views.\r\nThis result suggests that fusing features from the entire images of other views, instead of just features along the epipolar line [He et al.(2020)He, Yan, Fragkiadaki, and Yu], can bring more benefits.\r\nBesides, comparing to the single view TransPose [Yang et al.(2020)Yang, Quan, Nie, and Yang], our Transfusion can achieve 4.74.74.7 mm gain on 3D. Thus, the improvement is not only from the TransPose architecture, but from the fusion with other views.\r\nMoreover, our method is lightweight and efficient. It only requires 2.1%percent2.12.1\\% (5M / 235M) of the parameters of cross-view fusion [Qiu et al.(2019)Qiu, Wang, Wang, Wang, and Zeng]. Benefit from the parallel computing of transformers architectures, it further reduces the inference time, while the operation of sampling along epipolar lines [He et al.(2020)He, Yan, Fragkiadaki, and Yu] is time-consuming."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Ablation studies on different types of 3D positional encoding  ",
        "table": [
            "<table id=\"S4.T3.3.3\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T3.2.2.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.2.2.2.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\">Method</th>&#13;\n<th id=\"S4.T3.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\">2D Pose / JDR (%) <math id=\"S4.T3.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S4.T3.1.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S4.T3.1.1.1.1.m1.1.1\" xref=\"S4.T3.1.1.1.1.m1.1.1.cmml\">&#8593;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.1.1.1.1.m1.1b\"><ci id=\"S4.T3.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T3.1.1.1.1.m1.1.1\">&#8593;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.1.1.1.1.m1.1c\">\\uparrow</annotation></semantics></math>&#13;\n</th>&#13;\n<th id=\"S4.T3.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">3D Pose / MPJPE (mm) <math id=\"S4.T3.2.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T3.2.2.2.2.m1.1a\"><mo stretchy=\"false\" id=\"S4.T3.2.2.2.2.m1.1.1\" xref=\"S4.T3.2.2.2.2.m1.1.1.cmml\">&#8595;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.2.2.2.2.m1.1b\"><ci id=\"S4.T3.2.2.2.2.m1.1.1.cmml\" xref=\"S4.T3.2.2.2.2.m1.1.1\">&#8595;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.2.2.2.2.m1.1c\">\\downarrow</annotation></semantics></math>&#13;\n</th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T3.3.3.4.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.3.3.4.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">TransFusion - without 3D positional encoding</th>&#13;\n<td id=\"S4.T3.3.3.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">98.5</td>&#13;\n<td id=\"S4.T3.3.3.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">35.9</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T3.3.3.5.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.3.3.5.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">TransFusion - learnable 3D positional encoding</th>&#13;\n<td id=\"S4.T3.3.3.5.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">96.0</td>&#13;\n<td id=\"S4.T3.3.3.5.2.3\" class=\"ltx_td ltx_align_center\">57.3</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T3.3.3.3\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.3.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">TransFusion - GPE without <math id=\"S4.T3.3.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"L_{\\text{pos}}\" display=\"inline\"><semantics id=\"S4.T3.3.3.3.1.m1.1a\"><msub id=\"S4.T3.3.3.3.1.m1.1.1\" xref=\"S4.T3.3.3.3.1.m1.1.1.cmml\"><mi id=\"S4.T3.3.3.3.1.m1.1.1.2\" xref=\"S4.T3.3.3.3.1.m1.1.1.2.cmml\">L</mi><mtext id=\"S4.T3.3.3.3.1.m1.1.1.3\" xref=\"S4.T3.3.3.3.1.m1.1.1.3a.cmml\">pos</mtext></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.3.3.3.1.m1.1b\"><apply id=\"S4.T3.3.3.3.1.m1.1.1.cmml\" xref=\"S4.T3.3.3.3.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.T3.3.3.3.1.m1.1.1.1.cmml\" xref=\"S4.T3.3.3.3.1.m1.1.1\">subscript</csymbol><ci id=\"S4.T3.3.3.3.1.m1.1.1.2.cmml\" xref=\"S4.T3.3.3.3.1.m1.1.1.2\">&#119871;</ci><ci id=\"S4.T3.3.3.3.1.m1.1.1.3a.cmml\" xref=\"S4.T3.3.3.3.1.m1.1.1.3\"><mtext mathsize=\"70%\" id=\"S4.T3.3.3.3.1.m1.1.1.3.cmml\" xref=\"S4.T3.3.3.3.1.m1.1.1.3\">pos</mtext></ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.3.3.3.1.m1.1c\">L_{\\text{pos}}</annotation></semantics></math>&#13;\n</th>&#13;\n<td id=\"S4.T3.3.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">99.3</td>&#13;\n<td id=\"S4.T3.3.3.3.3\" class=\"ltx_td ltx_align_center\">26.8</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T3.3.3.6.3\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.3.3.6.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">TransFusion</th>&#13;\n<td id=\"S4.T3.3.3.6.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T3.3.3.6.3.2.1\" class=\"ltx_text ltx_font_bold\">99.4</span></td>&#13;\n<td id=\"S4.T3.3.3.6.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T3.3.3.6.3.3.1\" class=\"ltx_text ltx_font_bold\">25.8</span></td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [],
        "references": [
            "We conduct ablation studies on the GPE to verify its significance. In detail, we consider 3 settings: 1) training without 3D geometry positional encoding, 2) applying a learnable 3D positional encoding, i.e., directly learn 𝐄𝐆isubscriptsubscript𝐄𝐆𝑖\\mathbf{E_{G}}_{i} from scratch 3) training the 3D GPE without epipolar field constraints Lpossubscript𝐿posL_{\\text{pos}}.\r\nTable 3 presents the results. Without the 3D location information, the performance of 1) and 2) are even worse than the single view TransPose, we hypothesize that the 2D sine PE makes the transformer easy to attend the same pixel location of all views, and the learned 3D PE is easy to overfit the training examples.\r\nWithout Lpossubscript𝐿posL_{\\text{pos}}, the error will also increase. Thus the guide from the epipolar field is favorable, as it imposes correspondence for cross-view attention."
        ]
    },
    "id_table_4": {
        "caption": "Table 4: 2D and 3D pose estimation accuracy comparison on Ski-Pose. ",
        "table": [
            "<table id=\"S4.T4.2.2\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T4.2.2.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T4.2.2.2.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\">Method</th>&#13;\n<th id=\"S4.T4.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\">2D Pose / JDR (%) <math id=\"S4.T4.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S4.T4.1.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S4.T4.1.1.1.1.m1.1.1\" xref=\"S4.T4.1.1.1.1.m1.1.1.cmml\">&#8593;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.1.1.1.1.m1.1b\"><ci id=\"S4.T4.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T4.1.1.1.1.m1.1.1\">&#8593;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.1.1.1.1.m1.1c\">\\uparrow</annotation></semantics></math>&#13;\n</th>&#13;\n<th id=\"S4.T4.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">3D Pose / MPJPE (mm) <math id=\"S4.T4.2.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T4.2.2.2.2.m1.1a\"><mo stretchy=\"false\" id=\"S4.T4.2.2.2.2.m1.1.1\" xref=\"S4.T4.2.2.2.2.m1.1.1.cmml\">&#8595;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.2.2.2.2.m1.1b\"><ci id=\"S4.T4.2.2.2.2.m1.1.1.cmml\" xref=\"S4.T4.2.2.2.2.m1.1.1\">&#8595;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.2.2.2.2.m1.1c\">\\downarrow</annotation></semantics></math>&#13;\n</th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T4.2.2.3.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T4.2.2.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Single view - Simple Baseline <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bibx53\" title=\"\" class=\"ltx_ref\">Xiao et&#160;al.(2018)Xiao, Wu, and Wei</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T4.2.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">94.5</td>&#13;\n<td id=\"S4.T4.2.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">39.6</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T4.2.2.4.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T4.2.2.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Epipolar Transformer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bibx19\" title=\"\" class=\"ltx_ref\">He et&#160;al.(2020)He, Yan, Fragkiadaki, and Yu</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T4.2.2.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">94.9</td>&#13;\n<td id=\"S4.T4.2.2.4.2.3\" class=\"ltx_td ltx_align_center\">34.2</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T4.2.2.5.3\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T4.2.2.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">TransFusion</th>&#13;\n<td id=\"S4.T4.2.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T4.2.2.5.3.2.1\" class=\"ltx_text ltx_font_bold\">96.0</span></td>&#13;\n<td id=\"S4.T4.2.2.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T4.2.2.5.3.3.1\" class=\"ltx_text ltx_font_bold\">31.6</span></td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "[Xiao et al.(2018)Xiao, Wu, and Wei] [Xiao et al.(2018)Xiao, Wu, and Wei]\r\n\r\nBin Xiao, Haiping Wu, and Yichen Wei.\r\n\r\n\r\nSimple baselines for human pose estimation and tracking.\r\n\r\n\r\nIn Proceedings of the European conference on computer vision\r\n(ECCV), pages 466–481, 2018.",
            "[He et al.(2020)He, Yan, Fragkiadaki, and Yu] [He et al.(2020)He, Yan, Fragkiadaki, and Yu]\r\n\r\nYihui He, Rui Yan, Katerina Fragkiadaki, and Shoou-I Yu.\r\n\r\n\r\nEpipolar transformers.\r\n\r\n\r\nIn Proceedings of the IEEE/CVF Conference on Computer Vision\r\nand Pattern Recognition, pages 7779–7788, 2020."
        ],
        "references": [
            "We further apply TransFusion on the Ski-Pose dataset to verify its generalization ability. Results are presented in Table 4.\r\nIn the settings with six cameras, the Crossview Fusion is too huge (537M) to train on the 2080Ti GPU. Similar to Human 3.6M, TransFusion still outperform or achieve comparable performance with other fusion methods, while it is much lightweight.\r\nThus, our method is also effective in outdoor multi-view settings."
        ]
    }
}