{
    "id_table_1": {
        "caption": "Table 1: Semantic feature extractor ablation study.",
        "table": [
            "<table id=\"S4.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T1.1.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.1.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">&#13;\n<span id=\"S4.T1.1.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T1.1.1.1.1.1.1\" class=\"ltx_p\" style=\"width:346.9pt;\"><span id=\"S4.T1.1.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Design choice</span></span>&#13;\n</span>&#13;\n</th>&#13;\n<th id=\"S4.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">EPE</span></th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T1.1.2.1\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T1.1.2.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">&#13;\n<span id=\"S4.T1.1.2.1.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T1.1.2.1.1.1.1\" class=\"ltx_p\" style=\"width:346.9pt;\">No semantic feature extractor (SFE)</span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">13.06</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.1.3.2\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T1.1.3.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top\">&#13;\n<span id=\"S4.T1.1.3.2.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T1.1.3.2.1.1.1\" class=\"ltx_p\" style=\"width:346.9pt;\">SFE (heatmaps only)</span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T1.1.3.2.2\" class=\"ltx_td ltx_align_center\">11.69</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.1.4.3\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T1.1.4.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top\">&#13;\n<span id=\"S4.T1.1.4.3.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T1.1.4.3.1.1.1\" class=\"ltx_p\" style=\"width:346.9pt;\">SFE (heatmaps + silhouette)</span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T1.1.4.3.2\" class=\"ltx_td ltx_align_center\">11.52</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.1.5.4\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T1.1.5.4.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b\">&#13;\n<span id=\"S4.T1.1.5.4.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T1.1.5.4.1.1.1\" class=\"ltx_p\" style=\"width:346.9pt;\">ObMan pre-trained SFE (heatmaps + silhoutte)</span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T1.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_b\">11.12</td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [],
        "references": [
            "The proposed framework contains several design choices that were made in order to obtain stable 3D hand pose and shape estimations.\r\nWe therefore performed ablation studies to assess the effectiveness of each of these decisions. The obtained results are summarized in Tables 1, 2, 3, and 4, where each table shows the results for a given component, i.e., the SFE, VE, HE, and advanced framework components.\r\nAll of the reported EPE scores were computed for the STB dataset, while pre-training of the SFE unit was carried out exclusively on the ObMan dataset, since it contains a high number of synthetic images under various conditions.\r\nFor both collections, mini-batches of size six and an Adam optimizer with a learning rate of 10âˆ’4superscript10410^{-4} and a weight decay of 10âˆ’5superscript10510^{-5} were used to train the system. The framework was trained for 60 and 80 epochs on the ObMan and STB datasets, respectively, as the former contained substantially more samples.\r\nIn relation to the STB training time, which involves the entire framework, each mini-batch required âˆ¼similar-to\\sim0.5 seconds to be analyzed by the specified hardware configuration, giving a total of âˆ¼similar-to\\sim1278 s per training epoch.",
            "The first experiment quantitatively evaluated the usefulness of the SFE component in terms of producing more accurate 3D hand pose and shape estimations. As shown in TableÂ 1, while it is still possible to achieve estimations by feeding the input image directly to the VE (i.e., with no SFE), an improvement of âˆ¼similar-to\\sim1.5 mm in the estimation can be obtained by extracting the semantic features. This indicates that the 2D heatmaps allow the network to focus on the positions of the hand joints, whereas generating the heatmaps and silhouette simultaneously enables a comprehensive view of the hand that forces the joints to be placed in the right position when moving to a 3D plane. This behavior is further supported by the results of pre-training the SFE component on the ObMan dataset, where the occlusions force the network to create meaningful abstractions for both the 2D heatmaps and silhouette."
        ]
    },
    "id_table_2": {
        "caption": "Table 2: Viewpoint encoder ablation study.",
        "table": [
            "<table id=\"S4.T2.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T2.2.3.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.2.3.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">&#13;\n<span id=\"S4.T2.2.3.1.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T2.2.3.1.1.1.1\" class=\"ltx_p\" style=\"width:325.2pt;\"><span id=\"S4.T2.2.3.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Design choice</span></span>&#13;\n</span>&#13;\n</th>&#13;\n<th id=\"S4.T2.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T2.2.3.1.2.1\" class=\"ltx_text ltx_font_bold\">EPE</span></th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T2.2.4.1\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T2.2.4.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">&#13;\n<span id=\"S4.T2.2.4.1.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T2.2.4.1.1.1.1\" class=\"ltx_p\" style=\"width:325.2pt;\">2*ResNet modules VE</span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T2.2.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">28.77</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.2.5.2\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T2.2.5.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top\">&#13;\n<span id=\"S4.T2.2.5.2.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T2.2.5.2.1.1.1\" class=\"ltx_p\" style=\"width:325.2pt;\">4*ResNet modules VE</span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T2.2.5.2.2\" class=\"ltx_td ltx_align_center\">11.12</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.2.6.3\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T2.2.6.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top\">&#13;\n<span id=\"S4.T2.2.6.3.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T2.2.6.3.1.1.1\" class=\"ltx_p\" style=\"width:325.2pt;\">5*ResNet modules VE</span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T2.2.6.3.2\" class=\"ltx_td ltx_align_center\">12.25</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T2.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">&#13;\n<span id=\"S4.T2.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T2.1.1.1.1.1\" class=\"ltx_p\" style=\"width:325.2pt;\">3*1024 dense layers VE (VE<sub id=\"S4.T2.1.1.1.1.1.1\" class=\"ltx_sub\">1</sub>)</span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T2.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">10.79</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.2.2\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T2.2.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top\">&#13;\n<span id=\"S4.T2.2.2.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T2.2.2.1.1.1\" class=\"ltx_p\" style=\"width:325.2pt;\">2*2048/1*1024 dense layers VE (VE<sub id=\"S4.T2.2.2.1.1.1.1\" class=\"ltx_sub\">2</sub>)</span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T2.2.2.2\" class=\"ltx_td ltx_align_center\">11.12</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.2.7.4\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T2.2.7.4.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b\">&#13;\n<span id=\"S4.T2.2.7.4.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T2.2.7.4.1.1.1\" class=\"ltx_p\" style=\"width:325.2pt;\">3*2048 dense layers VE</span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T2.2.7.4.2\" class=\"ltx_td ltx_align_center ltx_border_b\">11.86</td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [],
        "references": [
            "The proposed framework contains several design choices that were made in order to obtain stable 3D hand pose and shape estimations.\r\nWe therefore performed ablation studies to assess the effectiveness of each of these decisions. The obtained results are summarized in Tables 1, 2, 3, and 4, where each table shows the results for a given component, i.e., the SFE, VE, HE, and advanced framework components.\r\nAll of the reported EPE scores were computed for the STB dataset, while pre-training of the SFE unit was carried out exclusively on the ObMan dataset, since it contains a high number of synthetic images under various conditions.\r\nFor both collections, mini-batches of size six and an Adam optimizer with a learning rate of 10âˆ’4superscript10410^{-4} and a weight decay of 10âˆ’5superscript10510^{-5} were used to train the system. The framework was trained for 60 and 80 epochs on the ObMan and STB datasets, respectively, as the former contained substantially more samples.\r\nIn relation to the STB training time, which involves the entire framework, each mini-batch required âˆ¼similar-to\\sim0.5 seconds to be analyzed by the specified hardware configuration, giving a total of âˆ¼similar-to\\sim1278 s per training epoch.",
            "The second test gauged the quality of the VE in terms of estimating view parameters vğ‘£v. As shown in TableÂ 2, using either a low or high number of ResNet modules (i.e., two in the former case or five or more in the latter) to produce the latent space representation lvsubscriptğ‘™ğ‘£l_{v} results in an increased EPE score, which is often associated with underfitting and overfitting. Slightly better performances can be achieved by reducing the sizes of the dense layers used to build up the vector vğ‘£v. However, although the smaller VE (i.e., Vâ€‹E1ğ‘‰subscriptğ¸1VE_{1}) can perform better than a larger one (i.e., Vâ€‹E2ğ‘‰subscriptğ¸2VE_{2}), this result does not apply when extra steps are included, such as skeleton adaptation and hourglass output concatenation (shown in TableÂ 4), suggesting that some information can still be lost."
        ]
    },
    "id_table_3": {
        "caption": "Table 3: Hand pose/shape estimator ablation study.",
        "table": [
            "<table id=\"S4.T3.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T3.2.3.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.2.3.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">&#13;\n<span id=\"S4.T3.2.3.1.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T3.2.3.1.1.1.1\" class=\"ltx_p\" style=\"width:325.2pt;\"><span id=\"S4.T3.2.3.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Design choice</span></span>&#13;\n</span>&#13;\n</th>&#13;\n<th id=\"S4.T3.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T3.2.3.1.2.1\" class=\"ltx_text ltx_font_bold\">EPE</span></th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T3.2.4.1\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T3.2.4.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">&#13;\n<span id=\"S4.T3.2.4.1.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T3.2.4.1.1.1.1\" class=\"ltx_p\" style=\"width:325.2pt;\">15 PCA parameters MANO layer</span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T3.2.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">12.31</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T3.2.5.2\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T3.2.5.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top\">&#13;\n<span id=\"S4.T3.2.5.2.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T3.2.5.2.1.1.1\" class=\"ltx_p\" style=\"width:325.2pt;\">30 PCA parameters MANO layer</span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T3.2.5.2.2\" class=\"ltx_td ltx_align_center\">11.47</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T3.2.6.3\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T3.2.6.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top\">&#13;\n<span id=\"S4.T3.2.6.3.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T3.2.6.3.1.1.1\" class=\"ltx_p\" style=\"width:325.2pt;\">45 PCA parameters MANO layer</span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T3.2.6.3.2\" class=\"ltx_td ltx_align_center\">11.12</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T3.2.7.4\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T3.2.7.4.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">&#13;\n<span id=\"S4.T3.2.7.4.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T3.2.7.4.1.1.1\" class=\"ltx_p\" style=\"width:325.2pt;\">No 2D re-projection</span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T3.2.7.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\">11.56</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T3.2.8.5\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T3.2.8.5.1\" class=\"ltx_td ltx_align_justify ltx_align_top\">&#13;\n<span id=\"S4.T3.2.8.5.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T3.2.8.5.1.1.1\" class=\"ltx_p\" style=\"width:325.2pt;\">With 2D re-projection</span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T3.2.8.5.2\" class=\"ltx_td ltx_align_center\">11.12</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T3.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T3.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">&#13;\n<span id=\"S4.T3.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T3.1.1.1.1.1\" class=\"ltx_p\" style=\"width:325.2pt;\">No <math id=\"S4.T3.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"L_{Reg}\" display=\"inline\"><semantics id=\"S4.T3.1.1.1.1.1.m1.1a\"><msub id=\"S4.T3.1.1.1.1.1.m1.1.1\" xref=\"S4.T3.1.1.1.1.1.m1.1.1.cmml\"><mi id=\"S4.T3.1.1.1.1.1.m1.1.1.2\" xref=\"S4.T3.1.1.1.1.1.m1.1.1.2.cmml\">L</mi><mrow id=\"S4.T3.1.1.1.1.1.m1.1.1.3\" xref=\"S4.T3.1.1.1.1.1.m1.1.1.3.cmml\"><mi id=\"S4.T3.1.1.1.1.1.m1.1.1.3.2\" xref=\"S4.T3.1.1.1.1.1.m1.1.1.3.2.cmml\">R</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.T3.1.1.1.1.1.m1.1.1.3.1\" xref=\"S4.T3.1.1.1.1.1.m1.1.1.3.1.cmml\">&#8203;</mo><mi id=\"S4.T3.1.1.1.1.1.m1.1.1.3.3\" xref=\"S4.T3.1.1.1.1.1.m1.1.1.3.3.cmml\">e</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.T3.1.1.1.1.1.m1.1.1.3.1a\" xref=\"S4.T3.1.1.1.1.1.m1.1.1.3.1.cmml\">&#8203;</mo><mi id=\"S4.T3.1.1.1.1.1.m1.1.1.3.4\" xref=\"S4.T3.1.1.1.1.1.m1.1.1.3.4.cmml\">g</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.1.1.1.1.1.m1.1b\"><apply id=\"S4.T3.1.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T3.1.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.T3.1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S4.T3.1.1.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S4.T3.1.1.1.1.1.m1.1.1.2.cmml\" xref=\"S4.T3.1.1.1.1.1.m1.1.1.2\">&#119871;</ci><apply id=\"S4.T3.1.1.1.1.1.m1.1.1.3.cmml\" xref=\"S4.T3.1.1.1.1.1.m1.1.1.3\"><times id=\"S4.T3.1.1.1.1.1.m1.1.1.3.1.cmml\" xref=\"S4.T3.1.1.1.1.1.m1.1.1.3.1\"/><ci id=\"S4.T3.1.1.1.1.1.m1.1.1.3.2.cmml\" xref=\"S4.T3.1.1.1.1.1.m1.1.1.3.2\">&#119877;</ci><ci id=\"S4.T3.1.1.1.1.1.m1.1.1.3.3.cmml\" xref=\"S4.T3.1.1.1.1.1.m1.1.1.3.3\">&#119890;</ci><ci id=\"S4.T3.1.1.1.1.1.m1.1.1.3.4.cmml\" xref=\"S4.T3.1.1.1.1.1.m1.1.1.3.4\">&#119892;</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.1.1.1.1.1.m1.1c\">L_{Reg}</annotation></semantics></math> loss</span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T3.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">10.10</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T3.2.2\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T3.2.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b\">&#13;\n<span id=\"S4.T3.2.2.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T3.2.2.1.1.1\" class=\"ltx_p\" style=\"width:325.2pt;\">With <math id=\"S4.T3.2.2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"L_{Reg}\" display=\"inline\"><semantics id=\"S4.T3.2.2.1.1.1.m1.1a\"><msub id=\"S4.T3.2.2.1.1.1.m1.1.1\" xref=\"S4.T3.2.2.1.1.1.m1.1.1.cmml\"><mi id=\"S4.T3.2.2.1.1.1.m1.1.1.2\" xref=\"S4.T3.2.2.1.1.1.m1.1.1.2.cmml\">L</mi><mrow id=\"S4.T3.2.2.1.1.1.m1.1.1.3\" xref=\"S4.T3.2.2.1.1.1.m1.1.1.3.cmml\"><mi id=\"S4.T3.2.2.1.1.1.m1.1.1.3.2\" xref=\"S4.T3.2.2.1.1.1.m1.1.1.3.2.cmml\">R</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.T3.2.2.1.1.1.m1.1.1.3.1\" xref=\"S4.T3.2.2.1.1.1.m1.1.1.3.1.cmml\">&#8203;</mo><mi id=\"S4.T3.2.2.1.1.1.m1.1.1.3.3\" xref=\"S4.T3.2.2.1.1.1.m1.1.1.3.3.cmml\">e</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.T3.2.2.1.1.1.m1.1.1.3.1a\" xref=\"S4.T3.2.2.1.1.1.m1.1.1.3.1.cmml\">&#8203;</mo><mi id=\"S4.T3.2.2.1.1.1.m1.1.1.3.4\" xref=\"S4.T3.2.2.1.1.1.m1.1.1.3.4.cmml\">g</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.2.2.1.1.1.m1.1b\"><apply id=\"S4.T3.2.2.1.1.1.m1.1.1.cmml\" xref=\"S4.T3.2.2.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.T3.2.2.1.1.1.m1.1.1.1.cmml\" xref=\"S4.T3.2.2.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S4.T3.2.2.1.1.1.m1.1.1.2.cmml\" xref=\"S4.T3.2.2.1.1.1.m1.1.1.2\">&#119871;</ci><apply id=\"S4.T3.2.2.1.1.1.m1.1.1.3.cmml\" xref=\"S4.T3.2.2.1.1.1.m1.1.1.3\"><times id=\"S4.T3.2.2.1.1.1.m1.1.1.3.1.cmml\" xref=\"S4.T3.2.2.1.1.1.m1.1.1.3.1\"/><ci id=\"S4.T3.2.2.1.1.1.m1.1.1.3.2.cmml\" xref=\"S4.T3.2.2.1.1.1.m1.1.1.3.2\">&#119877;</ci><ci id=\"S4.T3.2.2.1.1.1.m1.1.1.3.3.cmml\" xref=\"S4.T3.2.2.1.1.1.m1.1.1.3.3\">&#119890;</ci><ci id=\"S4.T3.2.2.1.1.1.m1.1.1.3.4.cmml\" xref=\"S4.T3.2.2.1.1.1.m1.1.1.3.4\">&#119892;</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.2.2.1.1.1.m1.1c\">L_{Reg}</annotation></semantics></math> loss</span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T3.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_b\">11.12</td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [],
        "references": [
            "The proposed framework contains several design choices that were made in order to obtain stable 3D hand pose and shape estimations.\r\nWe therefore performed ablation studies to assess the effectiveness of each of these decisions. The obtained results are summarized in Tables 1, 2, 3, and 4, where each table shows the results for a given component, i.e., the SFE, VE, HE, and advanced framework components.\r\nAll of the reported EPE scores were computed for the STB dataset, while pre-training of the SFE unit was carried out exclusively on the ObMan dataset, since it contains a high number of synthetic images under various conditions.\r\nFor both collections, mini-batches of size six and an Adam optimizer with a learning rate of 10âˆ’4superscript10410^{-4} and a weight decay of 10âˆ’5superscript10510^{-5} were used to train the system. The framework was trained for 60 and 80 epochs on the ObMan and STB datasets, respectively, as the former contained substantially more samples.\r\nIn relation to the STB training time, which involves the entire framework, each mini-batch required âˆ¼similar-to\\sim0.5 seconds to be analyzed by the specified hardware configuration, giving a total of âˆ¼similar-to\\sim1278 s per training epoch.",
            "The third experiment, which is summarized in TableÂ 3, focused on the hand pose and shape estimator. Tests were performed on the number of parameters of the MANO layer, the proposed 2D re-projection, and the regularization loss. Increasing the number of hand articulations allowed us, as expected, to obtain more realistic hands and consequently more precise estimations when all 45 values were used. Applying the proposed 2D re-projection further reduced the EPE score by providing the MANO layer with direct feedback on its output. Employing the regularization loss resulted in a higher keypoint distance, with the difference derived from the hand shape collapsing onto itself as shown in the bottom row of Fig.Â 6."
        ]
    },
    "id_table_4": {
        "caption": "Table 4: Advanced components ablation study.",
        "table": [
            "<table id=\"S4.T4.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T4.2.3.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T4.2.3.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">&#13;\n<span id=\"S4.T4.2.3.1.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T4.2.3.1.1.1.1\" class=\"ltx_p\" style=\"width:325.2pt;\"><span id=\"S4.T4.2.3.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Design choice</span></span>&#13;\n</span>&#13;\n</th>&#13;\n<th id=\"S4.T4.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T4.2.3.1.2.1\" class=\"ltx_text ltx_font_bold\">EPE</span></th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T4.2.4.1\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T4.2.4.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">&#13;\n<span id=\"S4.T4.2.4.1.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T4.2.4.1.1.1.1\" class=\"ltx_p\" style=\"width:325.2pt;\">No adapt skeleton &amp; hourglass summation</span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T4.2.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">11.12</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T4.2.5.2\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T4.2.5.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top\">&#13;\n<span id=\"S4.T4.2.5.2.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T4.2.5.2.1.1.1\" class=\"ltx_p\" style=\"width:325.2pt;\">With adapt skeleton &amp; hourglass summation</span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T4.2.5.2.2\" class=\"ltx_td ltx_align_center\">9.10</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T4.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T4.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top\">&#13;\n<span id=\"S4.T4.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T4.1.1.1.1.1\" class=\"ltx_p\" style=\"width:325.2pt;\">With adapt skeleton &amp; hourglass concatenation &amp; (VE<sub id=\"S4.T4.1.1.1.1.1.1\" class=\"ltx_sub\">1</sub>)</span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T4.1.1.2\" class=\"ltx_td ltx_align_center\">9.03</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T4.2.2\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T4.2.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b\">&#13;\n<span id=\"S4.T4.2.2.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T4.2.2.1.1.1\" class=\"ltx_p\" style=\"width:325.2pt;\">With adapt skeleton &amp; hourglass concatenation &amp; (VE<sub id=\"S4.T4.2.2.1.1.1.1\" class=\"ltx_sub\">2</sub>)</span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T4.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_b\">8.79</td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [],
        "references": [
            "The proposed framework contains several design choices that were made in order to obtain stable 3D hand pose and shape estimations.\r\nWe therefore performed ablation studies to assess the effectiveness of each of these decisions. The obtained results are summarized in Tables 1, 2, 3, and 4, where each table shows the results for a given component, i.e., the SFE, VE, HE, and advanced framework components.\r\nAll of the reported EPE scores were computed for the STB dataset, while pre-training of the SFE unit was carried out exclusively on the ObMan dataset, since it contains a high number of synthetic images under various conditions.\r\nFor both collections, mini-batches of size six and an Adam optimizer with a learning rate of 10âˆ’4superscript10410^{-4} and a weight decay of 10âˆ’5superscript10510^{-5} were used to train the system. The framework was trained for 60 and 80 epochs on the ObMan and STB datasets, respectively, as the former contained substantially more samples.\r\nIn relation to the STB training time, which involves the entire framework, each mini-batch required âˆ¼similar-to\\sim0.5 seconds to be analyzed by the specified hardware configuration, giving a total of âˆ¼similar-to\\sim1278 s per training epoch.",
            "The second test gauged the quality of the VE in terms of estimating view parameters vğ‘£v. As shown in TableÂ 2, using either a low or high number of ResNet modules (i.e., two in the former case or five or more in the latter) to produce the latent space representation lvsubscriptğ‘™ğ‘£l_{v} results in an increased EPE score, which is often associated with underfitting and overfitting. Slightly better performances can be achieved by reducing the sizes of the dense layers used to build up the vector vğ‘£v. However, although the smaller VE (i.e., Vâ€‹E1ğ‘‰subscriptğ¸1VE_{1}) can perform better than a larger one (i.e., Vâ€‹E2ğ‘‰subscriptğ¸2VE_{2}), this result does not apply when extra steps are included, such as skeleton adaptation and hourglass output concatenation (shown in TableÂ 4), suggesting that some information can still be lost.",
            "The fourth and last test, reported in TableÂ 4, dealt with advanced strategies, i.e., skeleton adaptation (adapt skeleton) and hourglass output concatenation, rather than summation. The former strategy allowed for a significant performance boost (a 2 mm lower EPE) since it directly refines the 3D joints produced by the MANO layer. Stacked hourglass output concatenation improved the precision of the system by a further 0.31 mm, since it provided the VE with a finegrained input representation. However, this detailed input description requires bigger dense layers (i.e., Vâ€‹E2ğ‘‰subscriptğ¸2VE_{2}) to avoid losing any information. Consequently, using smaller dense layers (i.e., Vâ€‹E1ğ‘‰subscriptğ¸1VE_{1}) results in an increase in the EPE score."
        ]
    },
    "id_table_5": {
        "caption": "Table 5: AUC state-of-the-art comparison on STB dataset. Works are subdivided according to their input and output types.",
        "table": [
            "<table id=\"S4.T5.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T5.1.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T5.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span id=\"S4.T5.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Model</span></th>&#13;\n<td id=\"S4.T5.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T5.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Input</span></td>&#13;\n<td id=\"S4.T5.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T5.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Output</span></td>&#13;\n<td id=\"S4.T5.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T5.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">AUC</span></td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.1.2.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T5.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">CHPR <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">28</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T5.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">Depth</td>&#13;\n<td id=\"S4.T5.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">3D skeleton</td>&#13;\n<td id=\"S4.T5.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.839</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.1.3.3\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T5.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ICPPSO <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib31\" title=\"\" class=\"ltx_ref\">31</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T5.1.3.3.2\" class=\"ltx_td ltx_align_center\">RGB-D</td>&#13;\n<td id=\"S4.T5.1.3.3.3\" class=\"ltx_td ltx_align_center\">3D skeleton</td>&#13;\n<td id=\"S4.T5.1.3.3.4\" class=\"ltx_td ltx_align_center\">0.748</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.1.4.4\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T5.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">PSO <cite class=\"ltx_cite ltx_citemacro_citep\">[<a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">30</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T5.1.4.4.2\" class=\"ltx_td ltx_align_center\">RGB-D</td>&#13;\n<td id=\"S4.T5.1.4.4.3\" class=\"ltx_td ltx_align_center\">3D skeleton</td>&#13;\n<td id=\"S4.T5.1.4.4.4\" class=\"ltx_td ltx_align_center\">0.709</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.1.5.5\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T5.1.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Dibra et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">9</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T5.1.5.5.2\" class=\"ltx_td ltx_align_center\">RGB-D</td>&#13;\n<td id=\"S4.T5.1.5.5.3\" class=\"ltx_td ltx_align_center\">3D skeleton</td>&#13;\n<td id=\"S4.T5.1.5.5.4\" class=\"ltx_td ltx_align_center\">0.923</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.1.6.6\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T5.1.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Cai et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib32\" title=\"\" class=\"ltx_ref\">32</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T5.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\">RGB</td>&#13;\n<td id=\"S4.T5.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_t\">3D skeleton</td>&#13;\n<td id=\"S4.T5.1.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.996</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.1.7.7\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T5.1.7.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Iqbal et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">21</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T5.1.7.7.2\" class=\"ltx_td ltx_align_center\">RGB</td>&#13;\n<td id=\"S4.T5.1.7.7.3\" class=\"ltx_td ltx_align_center\">3D skeleton</td>&#13;\n<td id=\"S4.T5.1.7.7.4\" class=\"ltx_td ltx_align_center\">0.994</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.1.8.8\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T5.1.8.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Hasson et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib35\" title=\"\" class=\"ltx_ref\">35</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T5.1.8.8.2\" class=\"ltx_td ltx_align_center\">RGB</td>&#13;\n<td id=\"S4.T5.1.8.8.3\" class=\"ltx_td ltx_align_center\">3D skeleton</td>&#13;\n<td id=\"S4.T5.1.8.8.4\" class=\"ltx_td ltx_align_center\">0.992</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.1.9.9\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T5.1.9.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Yang and Yao <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib36\" title=\"\" class=\"ltx_ref\">36</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T5.1.9.9.2\" class=\"ltx_td ltx_align_center\">RGB</td>&#13;\n<td id=\"S4.T5.1.9.9.3\" class=\"ltx_td ltx_align_center\">3D skeleton</td>&#13;\n<td id=\"S4.T5.1.9.9.4\" class=\"ltx_td ltx_align_center\">0.991</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.1.10.10\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T5.1.10.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Spurr et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib45\" title=\"\" class=\"ltx_ref\">45</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T5.1.10.10.2\" class=\"ltx_td ltx_align_center\">RGB</td>&#13;\n<td id=\"S4.T5.1.10.10.3\" class=\"ltx_td ltx_align_center\">3D skeleton</td>&#13;\n<td id=\"S4.T5.1.10.10.4\" class=\"ltx_td ltx_align_center\">0.983</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.1.11.11\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T5.1.11.11.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Zummermann and Brox <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib33\" title=\"\" class=\"ltx_ref\">33</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T5.1.11.11.2\" class=\"ltx_td ltx_align_center\">RGB</td>&#13;\n<td id=\"S4.T5.1.11.11.3\" class=\"ltx_td ltx_align_center\">3D skeleton</td>&#13;\n<td id=\"S4.T5.1.11.11.4\" class=\"ltx_td ltx_align_center\">0.986</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.1.12.12\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T5.1.12.12.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Mueller et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">10</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T5.1.12.12.2\" class=\"ltx_td ltx_align_center\">RGB</td>&#13;\n<td id=\"S4.T5.1.12.12.3\" class=\"ltx_td ltx_align_center\">3D skeleton</td>&#13;\n<td id=\"S4.T5.1.12.12.4\" class=\"ltx_td ltx_align_center\">0.965</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.1.13.13\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T5.1.13.13.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Panteleris et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib46\" title=\"\" class=\"ltx_ref\">46</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T5.1.13.13.2\" class=\"ltx_td ltx_align_center\">RGB</td>&#13;\n<td id=\"S4.T5.1.13.13.3\" class=\"ltx_td ltx_align_center\">3D skeleton</td>&#13;\n<td id=\"S4.T5.1.13.13.4\" class=\"ltx_td ltx_align_center\">0.941</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.1.14.14\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T5.1.14.14.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Ge et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">25</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T5.1.14.14.2\" class=\"ltx_td ltx_align_center ltx_border_t\">RGB</td>&#13;\n<td id=\"S4.T5.1.14.14.3\" class=\"ltx_td ltx_align_center ltx_border_t\">3D skeleton+mesh</td>&#13;\n<td id=\"S4.T5.1.14.14.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.998</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.1.15.15\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T5.1.15.15.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Baek et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib34\" title=\"\" class=\"ltx_ref\">34</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T5.1.15.15.2\" class=\"ltx_td ltx_align_center\">RGB</td>&#13;\n<td id=\"S4.T5.1.15.15.3\" class=\"ltx_td ltx_align_center\">3D skeleton+mesh</td>&#13;\n<td id=\"S4.T5.1.15.15.4\" class=\"ltx_td ltx_align_center\">0.995</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.1.16.16\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T5.1.16.16.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Zhang et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">26</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T5.1.16.16.2\" class=\"ltx_td ltx_align_center\">RGB</td>&#13;\n<td id=\"S4.T5.1.16.16.3\" class=\"ltx_td ltx_align_center\">3D skeleton+mesh</td>&#13;\n<td id=\"S4.T5.1.16.16.4\" class=\"ltx_td ltx_align_center\">0.995</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.1.17.17\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T5.1.17.17.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Boukhayma et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib37\" title=\"\" class=\"ltx_ref\">37</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T5.1.17.17.2\" class=\"ltx_td ltx_align_center\">RGB</td>&#13;\n<td id=\"S4.T5.1.17.17.3\" class=\"ltx_td ltx_align_center\">3D skeleton+mesh</td>&#13;\n<td id=\"S4.T5.1.17.17.4\" class=\"ltx_td ltx_align_center\">0.993</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.1.18.18\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T5.1.18.18.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">ours</th>&#13;\n<td id=\"S4.T5.1.18.18.2\" class=\"ltx_td ltx_align_center ltx_border_b\">RGB</td>&#13;\n<td id=\"S4.T5.1.18.18.3\" class=\"ltx_td ltx_align_center ltx_border_b\">3D skeleton+mesh</td>&#13;\n<td id=\"S4.T5.1.18.18.4\" class=\"ltx_td ltx_align_center ltx_border_b\">0.995</td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "[28] [28]\r\n\r\nX.Â Sun, Y.Â Wei, S.Â Liang, X.Â Tang, J.Â Sun, Cascaded hand pose regression, in:\r\nIEEE Conf. Comput. Vis. Patt. Recognit. (CVPR), 2015, pp. 824â€“832.",
            "[31] [31]\r\n\r\nC.Â Qian, X.Â Sun, Y.Â Wei, X.Â Tang, J.Â Sun, Realtime and robust hand tracking\r\nfrom depth, in: IEEE Conf. Comput. Vis. Patt. Recognit. (CVPR), 2014, pp.\r\n1â€“8.",
            "[30] [30]\r\n\r\nI.Â Oikonomidis, N.Â Kyriazis, A.Â A. Argyros, Efficient model-based 3d tracking\r\nof hand articulations using kinect, in: Br. Mach. Vis. Conf. (BMVC), 2011,\r\npp. 1â€“11.",
            "[9] [9]\r\n\r\nE.Â Dibra, S.Â Melchior, A.Â Balkis, T.Â Wolf, C.Â Oztireli, M.Â Gross, Monocular rgb\r\nhand pose inference from unsupervised refinable nets, in: IEEE Conf. Comput.\r\nVis. Patt. Recognit. Workshops (CVPRW), 2018, pp. 1075â€“1085.",
            "[32] [32]\r\n\r\nY.Â Cai, L.Â Ge, J.Â Cai, N.Â Magnenat-Thalmann, J.Â Yuan, 3d hand pose estimation\r\nusing synthetic data and weakly labeled rgb images, IEEE Trans. Pattern Anal.\r\nMach. Intell. 43Â (11) (2020) 3739â€“3753.",
            "[21] [21]\r\n\r\nU.Â Iqbal, P.Â Molchanov, T.Â Breuel JuergenÂ Gall, J.Â Kautz, Hand pose estimation\r\nvia latent 2.5d heatmap regression, in: Eur. Conf. Comput. Vis. (ECCV), 2018,\r\npp. 118â€“134.",
            "[35] [35]\r\n\r\nY.Â Hasson, G.Â Varol, D.Â Tzionas, I.Â Kalevatykh, M.Â J. Black, I.Â Laptev,\r\nC.Â Schmid, Learning joint reconstruction of hands and manipulated objects,\r\nin: IEEE/CVF Conf. Comput. Vis. Patt. Recognit. (CVPR), 2019, pp.\r\n11807â€“11816.",
            "[36] [36]\r\n\r\nL.Â Yang, A.Â Yao, Disentangling latent hands for image synthesis and pose\r\nestimation, in: IEEE/CVF Conf. Comput. Vis. Patt. Recognit. (CVPR), 2019, pp.\r\n9877â€“9886.",
            "[45] [45]\r\n\r\nA.Â Spurr, J.Â Song, S.Â Park, O.Â Hilliges, Cross-modal deep variational hand pose\r\nestimation, in: IEEE Conf. Comput. Vis. Patt. Recognit. (CVPR), 2018, pp.\r\n89â€“98.",
            "[33] [33]\r\n\r\nC.Â Zimmermann, T.Â Brox, Learning to estimate 3d hand pose from single rgb\r\nimages, in: IEEE Int. Conf. Comput. Vis. (ICCV), 2017, pp. 4903â€“4911.",
            "[10] [10]\r\n\r\nF.Â Mueller, F.Â Bernard, O.Â Sotnychenko, D.Â Mehta, S.Â Sridhar, D.Â Casas,\r\nC.Â Theobalt, Ganerated hands for real-time 3d hand tracking from monocular\r\nrgb, in: IEEE Conf. Comput. Vis. Patt. Recognit. (CVPR), 2018, pp. 49â€“59.",
            "[46] [46]\r\n\r\nP.Â Panteleris, I.Â Oikonomidis, A.Â Argyros, Using a single rgb frame for real\r\ntime 3d hand pose estimation in the wild, in: IEEE Winter Conf. Appl. Comput.\r\nVis. (WCCV), 2018, pp. 436â€“445.",
            "[25] [25]\r\n\r\nL.Â Ge, Z.Â Ren, Y.Â Li, Z.Â Xue, Y.Â Wang, J.Â Cai, J.Â Yuan, 3d hand shape and pose\r\nestimation from a single rgb image, in: IEEE/CVF Conf. Comput. Vis. Patt.\r\nRecognit. (CVPR), 2019, pp. 10833â€“10842.",
            "[34] [34]\r\n\r\nS.Â Baek, K.Â I. Kim, T.-K. Kim, Weakly-supervised domain adaptation via gan and\r\nmesh model for estimating 3d hand poses interacting objects, in: IEEE/CVF\r\nConf. Comput. Vis. Patt. Recognit. (CVPR), 2020, pp. 6121â€“6131.",
            "[26] [26]\r\n\r\nX.Â Zhang, Q.Â Li, H.Â Mo, W.Â Zhang, W.Â Zheng, End-to-end hand mesh recovery from\r\na monocular rgb image, in: IEEE Int. Conf. Comput. Vis. (ICCV), 2019, pp.\r\n2354â€“2364.",
            "[37] [37]\r\n\r\nA.Â Boukhayma, R.Â d. Bem, P.Â H. Torr, 3d hand shape and pose from images in the\r\nwild, in: IEEE/CVF Conf. Comput. Vis. Patt. Recognit. (CVPR), 2019, pp.\r\n10843â€“10852."
        ],
        "references": [
            "To demonstrate the effectiveness of the proposed framework, a state-of-the-art 3D PCK AUC comparison was carried out, as shown in TableÂ 5. As can be seen, the presented system is competitive with other successful schemes while using only RGB images and outputting both 3D skeleton locations and mesh, indicating that all of our design choices allow the framework to generate good estimates using only RGB information. It is particularly interesting that the proposed method was able to easily outperform systems that exploited depth data, suggesting that the simultaneous use of the multi-task SFE, VE, and 2D re-projection can help to produce correct estimations by compensating for the missing depth information.\r\nSpecifically, the multi-task SFE enables the implementation of a customized VE that, unlike the scheme in [37], does not require to be pre-trained on a synthetic dataset in order to perform well; there is also no need to normalize the latent feature space representation by using a VAE to disentangle different factors influencing the hand representation in order to obtain accurate 3D hand poses, as described in [36].\r\nFurthermore, thanks to the re-projection module, these results are obtained without applying an iterative regression module to the MANO layer, unlike in [26], and [34], where progressive changes are carried out recurrently to refine the estimation parameters, thus simplifying the training procedure. In addition, the solutions implemented in the proposed framework allow us to avoid input assumptions and post-processing operations, unlike the majority of schemes in the literature where some parameter (e.g., global hand scale or root joint depth) is assumed to be known at test time, and secondly to achieve similar performance to the best model devised in [25], even though the latter scheme employs a more powerful solution (i.e., a graph CNN instead of a fixed MANO layer) for the 3D hand pose and shape estimation.",
            "To assess the generalizability of the proposed framework, experiments were performed on the RWTH and Senz3D datasets. Since our architecture does not include a classification component, it was extended by attaching the same classifier described in [33] to handle the new task. This classifier takes as input the 3D joint coordinates generated by the MANO layer, and consists of three fully connected layers with a ReLU activation function.\r\nNote that all of the weights (except those used for the classifier) are frozen when training the system on the hand gesture recognition task, so that it is possible to correctly evaluate the generalizability of the framework.\r\nMoreover, although weights are frozen, the entire framework still needs to be executed. Hence, the majority of the training time is spent on the generation of the 3D joint coordinates, since this is where most of the computation is performed; as a result, each mini-batch is analyzed by the specified hardware configuration in âˆ¼similar-to\\sim222 ms (i.e., âˆ¼similar-to\\sim37 ms per image), giving total times of âˆ¼similar-to\\sim206 s and âˆ¼similar-to\\sim235 s per epoch for the RWTH and Senz3D datasets, respectively.\r\nOur experiments followed the testing protocol devised in [9] in order to present a fair comparison; this consisted of 10-fold cross-validation with non-overlapping 80/20 splits for the training and test sets, respectively. In a similar way to other state-of-the-art works, all images were cropped close to the hand to remove as much background as possible and meet the requirements for the input size, i.e., 256Ã—256256256256\\times 256.\r\nThe results are shown in TableÂ 6, and a comparison with other schemes in the literature is presented.\r\nIt can be seen that our framework consistently outperformed another work focusing on 3D pose and shape estimation (i.e., [33]) on both datasets, meaning that it generates more accurate joint coordinates from the RGB image; the same result was also obtained for the estimation task, as shown in TableÂ 5.\r\nHowever, methods that exploit depth information (i.e., [9] or concentrate on hand gesture classification (i.e., [47]) can still achieve slightly higher performance. There are two reasons for this.\r\nFirstly, by concentrating on the hand gesture classification task, lower performance is achieved on the estimation task, although similar information, such as the 3D joint locations, is used. As a matter of fact, even though they exploit depth information in their work, the authors of [9] obtained an AUC score of 0.923, while the scheme in [33] and the proposed framework achieved AUC scores of 0.986 and 0.994 on the STB dataset for 3D hand pose estimation, respectively. Secondly, as discussed in SectionÂ 4.3.1 and shown by the qualitative results in Fig.Â 7, the proposed architecture could be improved further by increasing the estimation accuracy of the 2D joints and silhouette, indicating that if a good hand abstraction is used to derive the 3D hand pose and shape, this can be effective for the hand gesture recognition task."
        ]
    },
    "id_table_6": {
        "caption": "Table 6: Hand-gesture recognition accuracy comparison.",
        "table": [
            "<table id=\"S4.T6.2\" class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T6.2.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T6.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span id=\"S4.T6.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Model</span></th>&#13;\n<th id=\"S4.T6.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T6.2.1.1.2.1\" class=\"ltx_text ltx_font_bold\">RWTH</span></th>&#13;\n<th id=\"S4.T6.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T6.2.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Senz3D</span></th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T6.2.2.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T6.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Papadimitriou and Potamianos <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib47\" title=\"\" class=\"ltx_ref\">47</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T6.2.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">73.92%</td>&#13;\n<td id=\"S4.T6.2.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">-</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T6.2.3.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T6.2.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Memo and Zanuttigh <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib48\" title=\"\" class=\"ltx_ref\">48</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T6.2.3.2.2\" class=\"ltx_td ltx_align_center\">-</td>&#13;\n<td id=\"S4.T6.2.3.2.3\" class=\"ltx_td ltx_align_center\">90.00%</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T6.2.4.3\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T6.2.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Dibra et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">9</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T6.2.4.3.2\" class=\"ltx_td ltx_align_center\">73.60%</td>&#13;\n<td id=\"S4.T6.2.4.3.3\" class=\"ltx_td ltx_align_center\">94.00%</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T6.2.5.4\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T6.2.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Dreuw et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">43</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T6.2.5.4.2\" class=\"ltx_td ltx_align_center\">63.44%</td>&#13;\n<td id=\"S4.T6.2.5.4.3\" class=\"ltx_td ltx_align_center\">-</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T6.2.6.5\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T6.2.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Zimmerman and Brox <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib33\" title=\"\" class=\"ltx_ref\">33</a>]</cite>*</th>&#13;\n<td id=\"S4.T6.2.6.5.2\" class=\"ltx_td ltx_align_center\">66.80%</td>&#13;\n<td id=\"S4.T6.2.6.5.3\" class=\"ltx_td ltx_align_center\">77.00%</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T6.2.7.6\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T6.2.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">ours*</th>&#13;\n<td id=\"S4.T6.2.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_b\">72.03%</td>&#13;\n<td id=\"S4.T6.2.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_b\">92.83%</td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "[47] [47]\r\n\r\nK.Â Papadimitriou, G.Â Potamianos, Fingerspelled alphabet sign recognition in\r\nupper-body videos, in: Eur. Signal Process. Conf. (EUSIPCO), 2019, pp. 1â€“5.",
            "[48] [48]\r\n\r\nA.Â Memo, P.Â Zanuttigh, Head-mounted gesture controlled interface for\r\nhuman-computer interaction, Multimedia Tools Appl. 77Â (1) (2018) 27â€“53.",
            "[9] [9]\r\n\r\nE.Â Dibra, S.Â Melchior, A.Â Balkis, T.Â Wolf, C.Â Oztireli, M.Â Gross, Monocular rgb\r\nhand pose inference from unsupervised refinable nets, in: IEEE Conf. Comput.\r\nVis. Patt. Recognit. Workshops (CVPRW), 2018, pp. 1075â€“1085.",
            "[43] [43]\r\n\r\nP.Â Dreuw, T.Â Deselaers, D.Â Keysers, H.Â Ney, Modeling image variability in\r\nappearance-based gesture recognition, in: Eur. Conf. Comput. Vis. Workshops\r\n(ECCVW), 2006, pp. 7â€“18.",
            "[33] [33]\r\n\r\nC.Â Zimmermann, T.Â Brox, Learning to estimate 3d hand pose from single rgb\r\nimages, in: IEEE Int. Conf. Comput. Vis. (ICCV), 2017, pp. 4903â€“4911."
        ],
        "references": [
            "To assess the generalizability of the proposed framework, experiments were performed on the RWTH and Senz3D datasets. Since our architecture does not include a classification component, it was extended by attaching the same classifier described in [33] to handle the new task. This classifier takes as input the 3D joint coordinates generated by the MANO layer, and consists of three fully connected layers with a ReLU activation function.\r\nNote that all of the weights (except those used for the classifier) are frozen when training the system on the hand gesture recognition task, so that it is possible to correctly evaluate the generalizability of the framework.\r\nMoreover, although weights are frozen, the entire framework still needs to be executed. Hence, the majority of the training time is spent on the generation of the 3D joint coordinates, since this is where most of the computation is performed; as a result, each mini-batch is analyzed by the specified hardware configuration in âˆ¼similar-to\\sim222 ms (i.e., âˆ¼similar-to\\sim37 ms per image), giving total times of âˆ¼similar-to\\sim206 s and âˆ¼similar-to\\sim235 s per epoch for the RWTH and Senz3D datasets, respectively.\r\nOur experiments followed the testing protocol devised in [9] in order to present a fair comparison; this consisted of 10-fold cross-validation with non-overlapping 80/20 splits for the training and test sets, respectively. In a similar way to other state-of-the-art works, all images were cropped close to the hand to remove as much background as possible and meet the requirements for the input size, i.e., 256Ã—256256256256\\times 256.\r\nThe results are shown in TableÂ 6, and a comparison with other schemes in the literature is presented.\r\nIt can be seen that our framework consistently outperformed another work focusing on 3D pose and shape estimation (i.e., [33]) on both datasets, meaning that it generates more accurate joint coordinates from the RGB image; the same result was also obtained for the estimation task, as shown in TableÂ 5.\r\nHowever, methods that exploit depth information (i.e., [9] or concentrate on hand gesture classification (i.e., [47]) can still achieve slightly higher performance. There are two reasons for this.\r\nFirstly, by concentrating on the hand gesture classification task, lower performance is achieved on the estimation task, although similar information, such as the 3D joint locations, is used. As a matter of fact, even though they exploit depth information in their work, the authors of [9] obtained an AUC score of 0.923, while the scheme in [33] and the proposed framework achieved AUC scores of 0.986 and 0.994 on the STB dataset for 3D hand pose estimation, respectively. Secondly, as discussed in SectionÂ 4.3.1 and shown by the qualitative results in Fig.Â 7, the proposed architecture could be improved further by increasing the estimation accuracy of the 2D joints and silhouette, indicating that if a good hand abstraction is used to derive the 3D hand pose and shape, this can be effective for the hand gesture recognition task."
        ]
    }
}