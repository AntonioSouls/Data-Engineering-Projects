{
    "id_table_1": {
        "caption": "Table 1: Comparisons with the state-of-the-art methods on the\r\nAFLW2000 and BIWI dataset. All models are trained on the 300W-LP dataset. 1 These methods allow full range predictions.",
        "table": [
            "<table id=\"S2.T1.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S2.T1.3.4.1\" class=\"ltx_tr\">&#13;\n<th id=\"S2.T1.3.4.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"/>&#13;\n<th id=\"S2.T1.3.4.1.2\" class=\"ltx_td ltx_th ltx_th_column ltx_border_t\"/>&#13;\n<th id=\"S2.T1.3.4.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"4\">&#13;\n<span id=\"S2.T1.3.4.1.3.1\" class=\"ltx_text ltx_font_bold\">AFLW2000</span></th>&#13;\n<th id=\"S2.T1.3.4.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"4\"><span id=\"S2.T1.3.4.1.4.1\" class=\"ltx_text ltx_font_bold\">BIWI</span></th>&#13;\n</tr>&#13;\n<tr id=\"S2.T1.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S2.T1.1.1.2\" class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"/>&#13;\n<th id=\"S2.T1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Full Range<sup id=\"S2.T1.1.1.1.1\" class=\"ltx_sup\">1</sup></th>&#13;\n<th id=\"S2.T1.1.1.3\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t\">Yaw</th>&#13;\n<th id=\"S2.T1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Pitch</th>&#13;\n<th id=\"S2.T1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Roll</th>&#13;\n<th id=\"S2.T1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">MAE</th>&#13;\n<th id=\"S2.T1.1.1.7\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t\">Yaw</th>&#13;\n<th id=\"S2.T1.1.1.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Pitch</th>&#13;\n<th id=\"S2.T1.1.1.9\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Roll</th>&#13;\n<th id=\"S2.T1.1.1.10\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">MAE</th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S2.T1.2.2\" class=\"ltx_tr\">&#13;\n<th id=\"S2.T1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">HopeNet (<math id=\"S2.T1.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha=2\" display=\"inline\"><semantics id=\"S2.T1.2.2.1.m1.1a\"><mrow id=\"S2.T1.2.2.1.m1.1.1\" xref=\"S2.T1.2.2.1.m1.1.1.cmml\"><mi id=\"S2.T1.2.2.1.m1.1.1.2\" xref=\"S2.T1.2.2.1.m1.1.1.2.cmml\">&#945;</mi><mo id=\"S2.T1.2.2.1.m1.1.1.1\" xref=\"S2.T1.2.2.1.m1.1.1.1.cmml\">=</mo><mn id=\"S2.T1.2.2.1.m1.1.1.3\" xref=\"S2.T1.2.2.1.m1.1.1.3.cmml\">2</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.2.2.1.m1.1b\"><apply id=\"S2.T1.2.2.1.m1.1.1.cmml\" xref=\"S2.T1.2.2.1.m1.1.1\"><eq id=\"S2.T1.2.2.1.m1.1.1.1.cmml\" xref=\"S2.T1.2.2.1.m1.1.1.1\"/><ci id=\"S2.T1.2.2.1.m1.1.1.2.cmml\" xref=\"S2.T1.2.2.1.m1.1.1.2\">&#120572;</ci><cn type=\"integer\" id=\"S2.T1.2.2.1.m1.1.1.3.cmml\" xref=\"S2.T1.2.2.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.2.2.1.m1.1c\">\\alpha=2</annotation></semantics></math>) &#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib5\" title=\"\" class=\"ltx_ref\">5</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S2.T1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">&#10007;</td>&#13;\n<td id=\"S2.T1.2.2.3\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">6.47</td>&#13;\n<td id=\"S2.T1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">6.56</td>&#13;\n<td id=\"S2.T1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">5.44</td>&#13;\n<td id=\"S2.T1.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\">6.16</td>&#13;\n<td id=\"S2.T1.2.2.7\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">5.17</td>&#13;\n<td id=\"S2.T1.2.2.8\" class=\"ltx_td ltx_align_center ltx_border_t\">6.98</td>&#13;\n<td id=\"S2.T1.2.2.9\" class=\"ltx_td ltx_align_center ltx_border_t\">3.39</td>&#13;\n<td id=\"S2.T1.2.2.10\" class=\"ltx_td ltx_align_center ltx_border_t\">5.18</td>&#13;\n</tr>&#13;\n<tr id=\"S2.T1.3.3\" class=\"ltx_tr\">&#13;\n<th id=\"S2.T1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">HopeNet (<math id=\"S2.T1.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha=1\" display=\"inline\"><semantics id=\"S2.T1.3.3.1.m1.1a\"><mrow id=\"S2.T1.3.3.1.m1.1.1\" xref=\"S2.T1.3.3.1.m1.1.1.cmml\"><mi id=\"S2.T1.3.3.1.m1.1.1.2\" xref=\"S2.T1.3.3.1.m1.1.1.2.cmml\">&#945;</mi><mo id=\"S2.T1.3.3.1.m1.1.1.1\" xref=\"S2.T1.3.3.1.m1.1.1.1.cmml\">=</mo><mn id=\"S2.T1.3.3.1.m1.1.1.3\" xref=\"S2.T1.3.3.1.m1.1.1.3.cmml\">1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.3.3.1.m1.1b\"><apply id=\"S2.T1.3.3.1.m1.1.1.cmml\" xref=\"S2.T1.3.3.1.m1.1.1\"><eq id=\"S2.T1.3.3.1.m1.1.1.1.cmml\" xref=\"S2.T1.3.3.1.m1.1.1.1\"/><ci id=\"S2.T1.3.3.1.m1.1.1.2.cmml\" xref=\"S2.T1.3.3.1.m1.1.1.2\">&#120572;</ci><cn type=\"integer\" id=\"S2.T1.3.3.1.m1.1.1.3.cmml\" xref=\"S2.T1.3.3.1.m1.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.3.3.1.m1.1c\">\\alpha=1</annotation></semantics></math>) &#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib5\" title=\"\" class=\"ltx_ref\">5</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S2.T1.3.3.2\" class=\"ltx_td ltx_align_center\">&#10007;</td>&#13;\n<td id=\"S2.T1.3.3.3\" class=\"ltx_td ltx_nopad_l ltx_align_center\">6.92</td>&#13;\n<td id=\"S2.T1.3.3.4\" class=\"ltx_td ltx_align_center\">6.64</td>&#13;\n<td id=\"S2.T1.3.3.5\" class=\"ltx_td ltx_align_center\">5.67</td>&#13;\n<td id=\"S2.T1.3.3.6\" class=\"ltx_td ltx_align_center\">6.41</td>&#13;\n<td id=\"S2.T1.3.3.7\" class=\"ltx_td ltx_nopad_l ltx_align_center\">4.81</td>&#13;\n<td id=\"S2.T1.3.3.8\" class=\"ltx_td ltx_align_center\">6.61</td>&#13;\n<td id=\"S2.T1.3.3.9\" class=\"ltx_td ltx_align_center\">3.27</td>&#13;\n<td id=\"S2.T1.3.3.10\" class=\"ltx_td ltx_align_center\">4.90</td>&#13;\n</tr>&#13;\n<tr id=\"S2.T1.3.5.1\" class=\"ltx_tr\">&#13;\n<th id=\"S2.T1.3.5.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">FSA-Net&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">9</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S2.T1.3.5.1.2\" class=\"ltx_td ltx_align_center\">&#10007;</td>&#13;\n<td id=\"S2.T1.3.5.1.3\" class=\"ltx_td ltx_nopad_l ltx_align_center\">4.50</td>&#13;\n<td id=\"S2.T1.3.5.1.4\" class=\"ltx_td ltx_align_center\">6.08</td>&#13;\n<td id=\"S2.T1.3.5.1.5\" class=\"ltx_td ltx_align_center\">4.64</td>&#13;\n<td id=\"S2.T1.3.5.1.6\" class=\"ltx_td ltx_align_center\">5.07</td>&#13;\n<td id=\"S2.T1.3.5.1.7\" class=\"ltx_td ltx_nopad_l ltx_align_center\">4.27</td>&#13;\n<td id=\"S2.T1.3.5.1.8\" class=\"ltx_td ltx_align_center\">4.96</td>&#13;\n<td id=\"S2.T1.3.5.1.9\" class=\"ltx_td ltx_align_center\">2.76</td>&#13;\n<td id=\"S2.T1.3.5.1.10\" class=\"ltx_td ltx_align_center\">4.00</td>&#13;\n</tr>&#13;\n<tr id=\"S2.T1.3.6.2\" class=\"ltx_tr\">&#13;\n<th id=\"S2.T1.3.6.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">HPE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">7</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S2.T1.3.6.2.2\" class=\"ltx_td ltx_align_center\">&#10007;</td>&#13;\n<td id=\"S2.T1.3.6.2.3\" class=\"ltx_td ltx_nopad_l ltx_align_center\">4.80</td>&#13;\n<td id=\"S2.T1.3.6.2.4\" class=\"ltx_td ltx_align_center\">6.18</td>&#13;\n<td id=\"S2.T1.3.6.2.5\" class=\"ltx_td ltx_align_center\">4.87</td>&#13;\n<td id=\"S2.T1.3.6.2.6\" class=\"ltx_td ltx_align_center\">5.28</td>&#13;\n<td id=\"S2.T1.3.6.2.7\" class=\"ltx_td ltx_nopad_l ltx_align_center\">3.12</td>&#13;\n<td id=\"S2.T1.3.6.2.8\" class=\"ltx_td ltx_align_center\">5.18</td>&#13;\n<td id=\"S2.T1.3.6.2.9\" class=\"ltx_td ltx_align_center\">4.57</td>&#13;\n<td id=\"S2.T1.3.6.2.10\" class=\"ltx_td ltx_align_center\">4.29</td>&#13;\n</tr>&#13;\n<tr id=\"S2.T1.3.7.3\" class=\"ltx_tr\">&#13;\n<th id=\"S2.T1.3.7.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">QuatNet&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">6</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S2.T1.3.7.3.2\" class=\"ltx_td ltx_align_center\">&#10007;</td>&#13;\n<td id=\"S2.T1.3.7.3.3\" class=\"ltx_td ltx_nopad_l ltx_align_center\">3.97</td>&#13;\n<td id=\"S2.T1.3.7.3.4\" class=\"ltx_td ltx_align_center\">5.62</td>&#13;\n<td id=\"S2.T1.3.7.3.5\" class=\"ltx_td ltx_align_center\">3.92</td>&#13;\n<td id=\"S2.T1.3.7.3.6\" class=\"ltx_td ltx_align_center\">4.50</td>&#13;\n<td id=\"S2.T1.3.7.3.7\" class=\"ltx_td ltx_nopad_l ltx_align_center\"><span id=\"S2.T1.3.7.3.7.1\" class=\"ltx_text ltx_font_bold\">2.94</span></td>&#13;\n<td id=\"S2.T1.3.7.3.8\" class=\"ltx_td ltx_align_center\">5.49</td>&#13;\n<td id=\"S2.T1.3.7.3.9\" class=\"ltx_td ltx_align_center\">4.01</td>&#13;\n<td id=\"S2.T1.3.7.3.10\" class=\"ltx_td ltx_align_center\">4.15</td>&#13;\n</tr>&#13;\n<tr id=\"S2.T1.3.8.4\" class=\"ltx_tr\">&#13;\n<th id=\"S2.T1.3.8.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">WHENet-V&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">8</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S2.T1.3.8.4.2\" class=\"ltx_td ltx_align_center\">&#10007;</td>&#13;\n<td id=\"S2.T1.3.8.4.3\" class=\"ltx_td ltx_nopad_l ltx_align_center\">4.44</td>&#13;\n<td id=\"S2.T1.3.8.4.4\" class=\"ltx_td ltx_align_center\">5.75</td>&#13;\n<td id=\"S2.T1.3.8.4.5\" class=\"ltx_td ltx_align_center\">4.31</td>&#13;\n<td id=\"S2.T1.3.8.4.6\" class=\"ltx_td ltx_align_center\">4.83</td>&#13;\n<td id=\"S2.T1.3.8.4.7\" class=\"ltx_td ltx_nopad_l ltx_align_center\">3.60</td>&#13;\n<td id=\"S2.T1.3.8.4.8\" class=\"ltx_td ltx_align_center\"><span id=\"S2.T1.3.8.4.8.1\" class=\"ltx_text ltx_font_bold\">4.10</span></td>&#13;\n<td id=\"S2.T1.3.8.4.9\" class=\"ltx_td ltx_align_center\">2.73</td>&#13;\n<td id=\"S2.T1.3.8.4.10\" class=\"ltx_td ltx_align_center\">3.48</td>&#13;\n</tr>&#13;\n<tr id=\"S2.T1.3.9.5\" class=\"ltx_tr\">&#13;\n<th id=\"S2.T1.3.9.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">WHENet&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">8</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S2.T1.3.9.5.2\" class=\"ltx_td ltx_align_center\">&#10003;&#10007;</td>&#13;\n<td id=\"S2.T1.3.9.5.3\" class=\"ltx_td ltx_nopad_l ltx_align_center\">5.11</td>&#13;\n<td id=\"S2.T1.3.9.5.4\" class=\"ltx_td ltx_align_center\">6.24</td>&#13;\n<td id=\"S2.T1.3.9.5.5\" class=\"ltx_td ltx_align_center\">4.92</td>&#13;\n<td id=\"S2.T1.3.9.5.6\" class=\"ltx_td ltx_align_center\">5.42</td>&#13;\n<td id=\"S2.T1.3.9.5.7\" class=\"ltx_td ltx_nopad_l ltx_align_center\">3.99</td>&#13;\n<td id=\"S2.T1.3.9.5.8\" class=\"ltx_td ltx_align_center\">4.39</td>&#13;\n<td id=\"S2.T1.3.9.5.9\" class=\"ltx_td ltx_align_center\">3.06</td>&#13;\n<td id=\"S2.T1.3.9.5.10\" class=\"ltx_td ltx_align_center\">3.81</td>&#13;\n</tr>&#13;\n<tr id=\"S2.T1.3.10.6\" class=\"ltx_tr\">&#13;\n<th id=\"S2.T1.3.10.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">TriNet&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">10</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S2.T1.3.10.6.2\" class=\"ltx_td ltx_align_center\">&#10003;</td>&#13;\n<td id=\"S2.T1.3.10.6.3\" class=\"ltx_td ltx_nopad_l ltx_align_center\">4.04</td>&#13;\n<td id=\"S2.T1.3.10.6.4\" class=\"ltx_td ltx_align_center\">5.77</td>&#13;\n<td id=\"S2.T1.3.10.6.5\" class=\"ltx_td ltx_align_center\">4.20</td>&#13;\n<td id=\"S2.T1.3.10.6.6\" class=\"ltx_td ltx_align_center\">4.67</td>&#13;\n<td id=\"S2.T1.3.10.6.7\" class=\"ltx_td ltx_nopad_l ltx_align_center\">4.11</td>&#13;\n<td id=\"S2.T1.3.10.6.8\" class=\"ltx_td ltx_align_center\">4.76</td>&#13;\n<td id=\"S2.T1.3.10.6.9\" class=\"ltx_td ltx_align_center\">3.05</td>&#13;\n<td id=\"S2.T1.3.10.6.10\" class=\"ltx_td ltx_align_center\">3.97</td>&#13;\n</tr>&#13;\n<tr id=\"S2.T1.3.11.7\" class=\"ltx_tr\">&#13;\n<th id=\"S2.T1.3.11.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">FDN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S2.T1.3.11.7.2\" class=\"ltx_td ltx_align_center\">&#10007;</td>&#13;\n<td id=\"S2.T1.3.11.7.3\" class=\"ltx_td ltx_nopad_l ltx_align_center\">3.78</td>&#13;\n<td id=\"S2.T1.3.11.7.4\" class=\"ltx_td ltx_align_center\">5.61</td>&#13;\n<td id=\"S2.T1.3.11.7.5\" class=\"ltx_td ltx_align_center\">3.88</td>&#13;\n<td id=\"S2.T1.3.11.7.6\" class=\"ltx_td ltx_align_center\">4.42</td>&#13;\n<td id=\"S2.T1.3.11.7.7\" class=\"ltx_td ltx_nopad_l ltx_align_center\">4.52</td>&#13;\n<td id=\"S2.T1.3.11.7.8\" class=\"ltx_td ltx_align_center\">4.70</td>&#13;\n<td id=\"S2.T1.3.11.7.9\" class=\"ltx_td ltx_align_center\"><span id=\"S2.T1.3.11.7.9.1\" class=\"ltx_text ltx_font_bold\">2.56</span></td>&#13;\n<td id=\"S2.T1.3.11.7.10\" class=\"ltx_td ltx_align_center\">3.93</td>&#13;\n</tr>&#13;\n<tr id=\"S2.T1.3.12.8\" class=\"ltx_tr\">&#13;\n<th id=\"S2.T1.3.12.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t\">6DRepNet</th>&#13;\n<td id=\"S2.T1.3.12.8.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">&#10003;</td>&#13;\n<td id=\"S2.T1.3.12.8.3\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S2.T1.3.12.8.3.1\" class=\"ltx_text ltx_font_bold\">3.63</span></td>&#13;\n<td id=\"S2.T1.3.12.8.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S2.T1.3.12.8.4.1\" class=\"ltx_text ltx_font_bold\">4.91</span></td>&#13;\n<td id=\"S2.T1.3.12.8.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S2.T1.3.12.8.5.1\" class=\"ltx_text ltx_font_bold\">3.37</span></td>&#13;\n<td id=\"S2.T1.3.12.8.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">&#13;\n<span id=\"S2.T1.3.12.8.6.1\" class=\"ltx_text ltx_font_bold\">3.97</span></td>&#13;\n<td id=\"S2.T1.3.12.8.7\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_b ltx_border_t\">3.24</td>&#13;\n<td id=\"S2.T1.3.12.8.8\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">4.48</td>&#13;\n<td id=\"S2.T1.3.12.8.9\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">2.68</td>&#13;\n<td id=\"S2.T1.3.12.8.10\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S2.T1.3.12.8.10.1\" class=\"ltx_text ltx_font_bold\">3.47</span></td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "[5] [5]\r\n\r\nNataniel Ruiz, Eunji Chong, and James M. Rehg,\r\n\r\n\r\n“Fine-grained head pose estimation without keypoints,”\r\n\r\n\r\n2018 IEEE/CVF Conference on Computer Vision and Pattern\r\nRecognition Workshops (CVPRW), pp. 2155–215509, 2018.",
            "[5] [5]\r\n\r\nNataniel Ruiz, Eunji Chong, and James M. Rehg,\r\n\r\n\r\n“Fine-grained head pose estimation without keypoints,”\r\n\r\n\r\n2018 IEEE/CVF Conference on Computer Vision and Pattern\r\nRecognition Workshops (CVPRW), pp. 2155–215509, 2018.",
            "[9] [9]\r\n\r\nTsun-Yi Yang, Yi-Ting Chen, Yen-Yu Lin, and Yung-Yu Chuang,\r\n\r\n\r\n“Fsa-net: Learning fine-grained structure aggregation for head pose\r\nestimation from a single image,”\r\n\r\n\r\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\r\nPattern Recognition (CVPR), June 2019.",
            "[7] [7]\r\n\r\nBin Huang, Renwen Chen, Wang Xu, and Qinbang Zhou,\r\n\r\n\r\n“Improving head pose estimation using two-stage ensembles with top-k\r\nregression,”\r\n\r\n\r\nImage Vis. Comput., vol. 93, pp. 103827, 2020.",
            "[6] [6]\r\n\r\nHeng-Wei Hsu, Tung-Yu Wu, Sheng Wan, Wing Hung Wong, and Chen-Yi Lee,\r\n\r\n\r\n“Quatnet: Quaternion-based head pose estimation with multiregression\r\nloss,”\r\n\r\n\r\nIEEE Transactions on Multimedia, vol. 21, no. 4, pp.\r\n1035–1046, 2019.",
            "[8] [8]\r\n\r\nYijun Zhou and James Gregson,\r\n\r\n\r\n“Whenet: Real-time fine-grained estimation for wide range head\r\npose,”\r\n\r\n\r\nin 31st British Machine Vision Conference 2020, BMVC 2020,\r\nVirtual Event, UK, September 7-10, 2020. 2020, BMVA Press.",
            "[8] [8]\r\n\r\nYijun Zhou and James Gregson,\r\n\r\n\r\n“Whenet: Real-time fine-grained estimation for wide range head\r\npose,”\r\n\r\n\r\nin 31st British Machine Vision Conference 2020, BMVC 2020,\r\nVirtual Event, UK, September 7-10, 2020. 2020, BMVA Press.",
            "[10] [10]\r\n\r\nZhiwen Cao, Zongcheng Chu, Dongfang Liu, and Yingjie Chen,\r\n\r\n\r\n“A vector-based representation to enhance head pose estimation,”\r\n\r\n\r\nin Proceedings of the IEEE/CVF Winter Conference on Applications\r\nof Computer Vision (WACV), January 2021, pp. 1188–1197.",
            "[11] [11]\r\n\r\nHao Zhang, Mengmeng Wang, Yong Liu, and Yi Yuan,\r\n\r\n\r\n“Fdn: Feature decoupling network for head pose estimation,”\r\n\r\n\r\nin AAAI, 2020."
        ],
        "references": [
            "Experiment 1: In our first experiment, we follow the convention by using the synthetic 300W-LP dataset for training and the two real-world datasets ALFW2000 and BIWI for testing. The standard evaluation metric is the Mean Absolute Error (MAE) of the Euler angles, so we convert our rotation matrix predictions into Euler angles for better comparison. Table 1 shows the results from our first experiment setup. We compare our methods against the results reported by other state-of-the-art landmark-free approaches for head pose estimation. It demonstrates that our method outperforms the current state of the art by almost 20% on the AFLW2000 test dataset and achieves the lowest error rate on all three kinds of rotation angles yaw, pitch, and roll. On the BIWI dataset, our method achieves state-of-the-art results for the MAE. In contrast to other methods that report very diverging results of the single angle errors, or approach reports overall very balanced errors. This indicates that our network is able to learn in a consistent and robust manner."
        ]
    },
    "id_table_2": {
        "caption": "Table 2: Comparisons with the state-of-the-art methods on the\r\nBIWI dataset. 70% of the BIWI dataset is used for training and the remaining 30% for testing.",
        "table": [
            "<table id=\"S3.T2.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S3.T2.1.2.1\" class=\"ltx_tr\">&#13;\n<th id=\"S3.T2.1.2.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"/>&#13;\n<th id=\"S3.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"4\"><span id=\"S3.T2.1.2.1.2.1\" class=\"ltx_text ltx_font_bold\">BIWI</span></th>&#13;\n</tr>&#13;\n<tr id=\"S3.T2.1.3.2\" class=\"ltx_tr\">&#13;\n<th id=\"S3.T2.1.3.2.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"/>&#13;\n<th id=\"S3.T2.1.3.2.2\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t\">Yaw</th>&#13;\n<th id=\"S3.T2.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Pitch</th>&#13;\n<th id=\"S3.T2.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Roll</th>&#13;\n<th id=\"S3.T2.1.3.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">MAE</th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S3.T2.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S3.T2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">HopeNet (<math id=\"S3.T2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha=1\" display=\"inline\"><semantics id=\"S3.T2.1.1.1.m1.1a\"><mrow id=\"S3.T2.1.1.1.m1.1.1\" xref=\"S3.T2.1.1.1.m1.1.1.cmml\"><mi id=\"S3.T2.1.1.1.m1.1.1.2\" xref=\"S3.T2.1.1.1.m1.1.1.2.cmml\">&#945;</mi><mo id=\"S3.T2.1.1.1.m1.1.1.1\" xref=\"S3.T2.1.1.1.m1.1.1.1.cmml\">=</mo><mn id=\"S3.T2.1.1.1.m1.1.1.3\" xref=\"S3.T2.1.1.1.m1.1.1.3.cmml\">1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.T2.1.1.1.m1.1b\"><apply id=\"S3.T2.1.1.1.m1.1.1.cmml\" xref=\"S3.T2.1.1.1.m1.1.1\"><eq id=\"S3.T2.1.1.1.m1.1.1.1.cmml\" xref=\"S3.T2.1.1.1.m1.1.1.1\"/><ci id=\"S3.T2.1.1.1.m1.1.1.2.cmml\" xref=\"S3.T2.1.1.1.m1.1.1.2\">&#120572;</ci><cn type=\"integer\" id=\"S3.T2.1.1.1.m1.1.1.3.cmml\" xref=\"S3.T2.1.1.1.m1.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T2.1.1.1.m1.1c\">\\alpha=1</annotation></semantics></math>)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib5\" title=\"\" class=\"ltx_ref\">5</a>]</cite></th>&#13;\n<td id=\"S3.T2.1.1.2\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">3.29</td>&#13;\n<td id=\"S3.T2.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">3.39</td>&#13;\n<td id=\"S3.T2.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">3.00</td>&#13;\n<td id=\"S3.T2.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">3.23</td>&#13;\n</tr>&#13;\n<tr id=\"S3.T2.1.4.1\" class=\"ltx_tr\">&#13;\n<th id=\"S3.T2.1.4.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">FSA-Net&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">9</a>]</cite></th>&#13;\n<td id=\"S3.T2.1.4.1.2\" class=\"ltx_td ltx_nopad_l ltx_align_center\">2.89</td>&#13;\n<td id=\"S3.T2.1.4.1.3\" class=\"ltx_td ltx_align_center\">4.29</td>&#13;\n<td id=\"S3.T2.1.4.1.4\" class=\"ltx_td ltx_align_center\">3.60</td>&#13;\n<td id=\"S3.T2.1.4.1.5\" class=\"ltx_td ltx_align_center\">3.60</td>&#13;\n</tr>&#13;\n<tr id=\"S3.T2.1.5.2\" class=\"ltx_tr\">&#13;\n<th id=\"S3.T2.1.5.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">TriNet&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">10</a>]</cite></th>&#13;\n<td id=\"S3.T2.1.5.2.2\" class=\"ltx_td ltx_nopad_l ltx_align_center\">2.93</td>&#13;\n<td id=\"S3.T2.1.5.2.3\" class=\"ltx_td ltx_align_center\">3.04</td>&#13;\n<td id=\"S3.T2.1.5.2.4\" class=\"ltx_td ltx_align_center\">2.44</td>&#13;\n<td id=\"S3.T2.1.5.2.5\" class=\"ltx_td ltx_align_center\">2.80</td>&#13;\n</tr>&#13;\n<tr id=\"S3.T2.1.6.3\" class=\"ltx_tr\">&#13;\n<th id=\"S3.T2.1.6.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">FDN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>]</cite></th>&#13;\n<td id=\"S3.T2.1.6.3.2\" class=\"ltx_td ltx_nopad_l ltx_align_center\">3.00</td>&#13;\n<td id=\"S3.T2.1.6.3.3\" class=\"ltx_td ltx_align_center\">3.98</td>&#13;\n<td id=\"S3.T2.1.6.3.4\" class=\"ltx_td ltx_align_center\">2.88</td>&#13;\n<td id=\"S3.T2.1.6.3.5\" class=\"ltx_td ltx_align_center\">3.29</td>&#13;\n</tr>&#13;\n<tr id=\"S3.T2.1.7.4\" class=\"ltx_tr\">&#13;\n<th id=\"S3.T2.1.7.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t\">6DRepNet</th>&#13;\n<td id=\"S3.T2.1.7.4.2\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S3.T2.1.7.4.2.1\" class=\"ltx_text ltx_font_bold\">2.69</span></td>&#13;\n<td id=\"S3.T2.1.7.4.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S3.T2.1.7.4.3.1\" class=\"ltx_text ltx_font_bold\">2.92</span></td>&#13;\n<td id=\"S3.T2.1.7.4.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S3.T2.1.7.4.4.1\" class=\"ltx_text ltx_font_bold\">2.36</span></td>&#13;\n<td id=\"S3.T2.1.7.4.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S3.T2.1.7.4.5.1\" class=\"ltx_text ltx_font_bold\">2.66</span></td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "[5] [5]\r\n\r\nNataniel Ruiz, Eunji Chong, and James M. Rehg,\r\n\r\n\r\n“Fine-grained head pose estimation without keypoints,”\r\n\r\n\r\n2018 IEEE/CVF Conference on Computer Vision and Pattern\r\nRecognition Workshops (CVPRW), pp. 2155–215509, 2018.",
            "[9] [9]\r\n\r\nTsun-Yi Yang, Yi-Ting Chen, Yen-Yu Lin, and Yung-Yu Chuang,\r\n\r\n\r\n“Fsa-net: Learning fine-grained structure aggregation for head pose\r\nestimation from a single image,”\r\n\r\n\r\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\r\nPattern Recognition (CVPR), June 2019.",
            "[10] [10]\r\n\r\nZhiwen Cao, Zongcheng Chu, Dongfang Liu, and Yingjie Chen,\r\n\r\n\r\n“A vector-based representation to enhance head pose estimation,”\r\n\r\n\r\nin Proceedings of the IEEE/CVF Winter Conference on Applications\r\nof Computer Vision (WACV), January 2021, pp. 1188–1197.",
            "[11] [11]\r\n\r\nHao Zhang, Mengmeng Wang, Yong Liu, and Yi Yuan,\r\n\r\n\r\n“Fdn: Feature decoupling network for head pose estimation,”\r\n\r\n\r\nin AAAI, 2020."
        ],
        "references": [
            "For better interpretation, we added an extra column to show which methods are in general able to predict full-range rotations and which restrict their predictions within a certain range of angles. It shows that besides our approach, only two other methods target full range regression. The remaining methods have a special network architecture dedicated to narrow range head pose predictions. Most similar to our method, TriNet [10] uses the matrix representation and directly predicts the complete matrix. As this prediction cannot be assumed to satisfy the orthogonality constraint, it needs an excessive post-process by finding an appropriate rotation matrix that is close to the prediction and at the same time has orthogonal column norm vectors. The second method marked with wide range prediction probabilities, WHENet [8], only allows the full rotation for the yaw (but not for pitch and roll) by adding more classes to their classification problem. It is noticeable that this network performs worse compared to their similar network adaptation WHENet-V that has been restricted to be only capable of predicting angles between -90° and 90°. We argue that this decline in accuracy could be caused by the introduced label ambiguities as they use the Euler representation for training.\r\n\r\nFig. 2 shows qualitative results from the AFLW2000 dataset. We visualized the predicted Euler angles to demonstrate how the head poses are estimated in the image samples.\r\n\r\n\r\nExperiment 2: For our second experiment, we follow the convention by FSA-Net [9] and randomly split the BIWI dataset in a ratio of 7:3 for training and testing, respectively. Table 2 shows our results compared with other state-of-the-art methods that followed the same testing strategy. It demonstrates that our method outperforms all other methods not only in the overall MAE but at the same time also in predicting the yaw, pitch, and roll equally to our experiment on the AFLW2000 dataset. This supports the observed robustness in experiment 1, that achieving stable accurate results for all three angles does not only depend on the trained dataset, but rather on our proposed method itself."
        ]
    },
    "id_table_3": {
        "caption": "Table 3: Comparison of the MAE between ℓ2subscriptℓ2\\ell_{2} and geodesic loss.",
        "table": [
            "<table id=\"S3.T3.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S3.T3.1.2.1\" class=\"ltx_tr\">&#13;\n<th id=\"S3.T3.1.2.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"/>&#13;\n<th id=\"S3.T3.1.2.1.2\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T3.1.2.1.2.1\" class=\"ltx_text ltx_font_bold\">AFLW2000</span></th>&#13;\n<th id=\"S3.T3.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T3.1.2.1.3.1\" class=\"ltx_text ltx_font_bold\">BIWI</span></th>&#13;\n<th id=\"S3.T3.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T3.1.2.1.4.1\" class=\"ltx_text ltx_font_bold\">70/30 BIWI</span></th>&#13;\n</tr>&#13;\n<tr id=\"S3.T3.1.3.2\" class=\"ltx_tr\">&#13;\n<th id=\"S3.T3.1.3.2.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"/>&#13;\n<th id=\"S3.T3.1.3.2.2\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t\">MAE</th>&#13;\n<th id=\"S3.T3.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">MAE</th>&#13;\n<th id=\"S3.T3.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">MAE</th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S3.T3.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S3.T3.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">&#13;\n<math id=\"S3.T3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\ell_{2}\" display=\"inline\"><semantics id=\"S3.T3.1.1.1.m1.1a\"><msub id=\"S3.T3.1.1.1.m1.1.1\" xref=\"S3.T3.1.1.1.m1.1.1.cmml\"><mi mathvariant=\"normal\" id=\"S3.T3.1.1.1.m1.1.1.2\" xref=\"S3.T3.1.1.1.m1.1.1.2.cmml\">&#8467;</mi><mn id=\"S3.T3.1.1.1.m1.1.1.3\" xref=\"S3.T3.1.1.1.m1.1.1.3.cmml\">2</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T3.1.1.1.m1.1b\"><apply id=\"S3.T3.1.1.1.m1.1.1.cmml\" xref=\"S3.T3.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T3.1.1.1.m1.1.1.1.cmml\" xref=\"S3.T3.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T3.1.1.1.m1.1.1.2.cmml\" xref=\"S3.T3.1.1.1.m1.1.1.2\">&#8467;</ci><cn type=\"integer\" id=\"S3.T3.1.1.1.m1.1.1.3.cmml\" xref=\"S3.T3.1.1.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T3.1.1.1.m1.1c\">\\ell_{2}</annotation></semantics></math></th>&#13;\n<td id=\"S3.T3.1.1.2\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">4.13</td>&#13;\n<td id=\"S3.T3.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">3.70</td>&#13;\n<td id=\"S3.T3.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">2.84</td>&#13;\n</tr>&#13;\n<tr id=\"S3.T3.1.4.1\" class=\"ltx_tr\">&#13;\n<th id=\"S3.T3.1.4.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">Geodesic</th>&#13;\n<td id=\"S3.T3.1.4.1.2\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_b\">3.97</td>&#13;\n<td id=\"S3.T3.1.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_b\">3.47</td>&#13;\n<td id=\"S3.T3.1.4.1.4\" class=\"ltx_td ltx_align_center ltx_border_b\">2.66</td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [],
        "references": [
            "Experiment 3: Most current methods use the ℓ2subscriptℓ2\\ell_{2}-norm for calculating the loss in the training procedure. We argue that the Geodesic distance is a better distance metric to measure the prediction accuracy for head pose orientation. To prove this, we conduct another experiment where we repeat our previous tests, but this time train our network with the ℓ2subscriptℓ2\\ell_{2} distance loss. Table 3 shows these results compared to our models trained with geodesic distance loss. It states that the networks with geodesic loss penalty performed slightly better than those that were trained with ℓ2subscriptℓ2\\ell_{2}-norm.\r\n\r\n\r\nExperiment 4: In a final experiment, we analyze the impact of the chosen backbone on the results. ResNet50 is a popular standard network that is also used by HopeNet and TriNet, so we use it as the comparison backbone. Table 4 shows that our method with RepVGG backbone is capable to perform about 7% better on all test scenarios against the ℓ2subscriptℓ2\\ell_{2} loss. However, with the aid of the ResNet50 backbone, our method would still achieve state-of-the-art results on the AFLW2000 dataset."
        ]
    },
    "id_table_4": {
        "caption": "Table 4: Comparison of the MAE between the different backbones.",
        "table": [
            "<table id=\"S3.T4.2\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S3.T4.2.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S3.T4.2.1.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"/>&#13;\n<th id=\"S3.T4.2.1.1.2\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T4.2.1.1.2.1\" class=\"ltx_text ltx_font_bold\">AFLW2000</span></th>&#13;\n<th id=\"S3.T4.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T4.2.1.1.3.1\" class=\"ltx_text ltx_font_bold\">BIWI</span></th>&#13;\n<th id=\"S3.T4.2.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T4.2.1.1.4.1\" class=\"ltx_text ltx_font_bold\">70/30 BIWI</span></th>&#13;\n</tr>&#13;\n<tr id=\"S3.T4.2.2.2\" class=\"ltx_tr\">&#13;\n<th id=\"S3.T4.2.2.2.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"/>&#13;\n<th id=\"S3.T4.2.2.2.2\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t\">MAE</th>&#13;\n<th id=\"S3.T4.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">MAE</th>&#13;\n<th id=\"S3.T4.2.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">MAE</th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S3.T4.2.3.1\" class=\"ltx_tr\">&#13;\n<th id=\"S3.T4.2.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">ResNet50</th>&#13;\n<td id=\"S3.T4.2.3.1.2\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">4.26</td>&#13;\n<td id=\"S3.T4.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">3.76</td>&#13;\n<td id=\"S3.T4.2.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">3.09</td>&#13;\n</tr>&#13;\n<tr id=\"S3.T4.2.4.2\" class=\"ltx_tr\">&#13;\n<th id=\"S3.T4.2.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">RepVGG-B1g2</th>&#13;\n<td id=\"S3.T4.2.4.2.2\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_b\">3.97</td>&#13;\n<td id=\"S3.T4.2.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_b\">3.47</td>&#13;\n<td id=\"S3.T4.2.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_b\">2.66</td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [],
        "references": [
            "Experiment 3: Most current methods use the ℓ2subscriptℓ2\\ell_{2}-norm for calculating the loss in the training procedure. We argue that the Geodesic distance is a better distance metric to measure the prediction accuracy for head pose orientation. To prove this, we conduct another experiment where we repeat our previous tests, but this time train our network with the ℓ2subscriptℓ2\\ell_{2} distance loss. Table 3 shows these results compared to our models trained with geodesic distance loss. It states that the networks with geodesic loss penalty performed slightly better than those that were trained with ℓ2subscriptℓ2\\ell_{2}-norm.\r\n\r\n\r\nExperiment 4: In a final experiment, we analyze the impact of the chosen backbone on the results. ResNet50 is a popular standard network that is also used by HopeNet and TriNet, so we use it as the comparison backbone. Table 4 shows that our method with RepVGG backbone is capable to perform about 7% better on all test scenarios against the ℓ2subscriptℓ2\\ell_{2} loss. However, with the aid of the ResNet50 backbone, our method would still achieve state-of-the-art results on the AFLW2000 dataset."
        ]
    }
}