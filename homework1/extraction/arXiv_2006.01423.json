{
    "id_table_1": {
        "caption": "Table 1: Summary of the related surveys of human motion analysis and HPE.",
        "table": [
            "<table id=\"S1.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S1.T1.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S1.T1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">No.</span></th>&#13;\n<th id=\"S1.T1.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.1.1.1.1\" class=\"ltx_p\" style=\"width:170.7pt;\"><span id=\"S1.T1.1.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Survey </span><math id=\"S1.T1.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\&amp;\" display=\"inline\"><semantics id=\"S1.T1.1.1.1.1.1.m1.1a\"><mo mathsize=\"80%\" id=\"S1.T1.1.1.1.1.1.m1.1.1\" xref=\"S1.T1.1.1.1.1.1.m1.1.1.cmml\">&amp;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S1.T1.1.1.1.1.1.m1.1b\"><and id=\"S1.T1.1.1.1.1.1.m1.1.1.cmml\" xref=\"S1.T1.1.1.1.1.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.T1.1.1.1.1.1.m1.1c\">\\&amp;</annotation></semantics></math><span id=\"S1.T1.1.1.1.1.1.2\" class=\"ltx_text\" style=\"font-size:80%;\"> Reference</span></span>&#13;\n</span>&#13;\n</th>&#13;\n<th id=\"S1.T1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Venue</span></th>&#13;\n<th id=\"S1.T1.1.1.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.1.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.1.4.1.1\" class=\"ltx_p\" style=\"width:241.8pt;\"><span id=\"S1.T1.1.1.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Content</span></span>&#13;\n</span>&#13;\n</th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S1.T1.1.2.1\" class=\"ltx_tr\">&#13;\n<td id=\"S1.T1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">&#13;\n<table id=\"S1.T1.1.2.1.1.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S1.T1.1.2.1.1.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S1.T1.1.2.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S1.T1.1.2.1.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">1</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n</td>&#13;\n<td id=\"S1.T1.1.2.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.2.1.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.2.1.2.1.1\" class=\"ltx_p\" style=\"width:170.7pt;\"><span id=\"S1.T1.1.2.1.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Human motion analysis: A review </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.2.1.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Aggarwal and Cai<span id=\"S1.T1.1.2.1.2.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">1999</a><span id=\"S1.T1.1.2.1.2.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S1.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.2.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">CVIU</span></td>&#13;\n<td id=\"S1.T1.1.2.1.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.2.1.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.2.1.4.1.1\" class=\"ltx_p\" style=\"width:241.8pt;\"><span id=\"S1.T1.1.2.1.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A review of human motion analysis including body structure analysis, motion tracking and action recognition.</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S1.T1.1.3.2\" class=\"ltx_tr\">&#13;\n<td id=\"S1.T1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">2</span></td>&#13;\n<td id=\"S1.T1.1.3.2.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.3.2.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.3.2.2.1.1\" class=\"ltx_p\" style=\"width:170.7pt;\"><span id=\"S1.T1.1.3.2.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">The visual analysis of human movement: A survey </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.3.2.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Gavrila<span id=\"S1.T1.1.3.2.2.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib42\" title=\"\" class=\"ltx_ref\">1999</a><span id=\"S1.T1.1.3.2.2.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S1.T1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.3.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">CVIU</span></td>&#13;\n<td id=\"S1.T1.1.3.2.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.3.2.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.3.2.4.1.1\" class=\"ltx_p\" style=\"width:241.8pt;\"><span id=\"S1.T1.1.3.2.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A survey of whole-body and hand motion analysis.</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S1.T1.1.4.3\" class=\"ltx_tr\">&#13;\n<td id=\"S1.T1.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.4.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">3</span></td>&#13;\n<td id=\"S1.T1.1.4.3.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.4.3.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.4.3.2.1.1\" class=\"ltx_p\" style=\"width:170.7pt;\"><span id=\"S1.T1.1.4.3.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A survey of computer vision-based human motion capture </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.4.3.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Moeslund and Granum<span id=\"S1.T1.1.4.3.2.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib109\" title=\"\" class=\"ltx_ref\">2001</a><span id=\"S1.T1.1.4.3.2.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S1.T1.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.4.3.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">CVIU</span></td>&#13;\n<td id=\"S1.T1.1.4.3.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.4.3.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.4.3.4.1.1\" class=\"ltx_p\" style=\"width:241.8pt;\"><span id=\"S1.T1.1.4.3.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">An overview based on motion capture system, including initialization, tracking, pose estimation, and recognition.</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S1.T1.1.5.4\" class=\"ltx_tr\">&#13;\n<td id=\"S1.T1.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.5.4.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">4</span></td>&#13;\n<td id=\"S1.T1.1.5.4.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.5.4.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.5.4.2.1.1\" class=\"ltx_p\" style=\"width:170.7pt;\"><span id=\"S1.T1.1.5.4.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A survey on visual surveillance of object motion and behaviors </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.5.4.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Hu et&#160;al.<span id=\"S1.T1.1.5.4.2.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib54\" title=\"\" class=\"ltx_ref\">2004</a><span id=\"S1.T1.1.5.4.2.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S1.T1.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.5.4.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">TSMCS</span></td>&#13;\n<td id=\"S1.T1.1.5.4.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.5.4.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.5.4.4.1.1\" class=\"ltx_p\" style=\"width:241.8pt;\"><span id=\"S1.T1.1.5.4.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A summary of human motion analysis based one the framework of visual surveillance in dynamic scenes.</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S1.T1.1.6.5\" class=\"ltx_tr\">&#13;\n<td id=\"S1.T1.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.6.5.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">5</span></td>&#13;\n<td id=\"S1.T1.1.6.5.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.6.5.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.6.5.2.1.1\" class=\"ltx_p\" style=\"width:170.7pt;\"><span id=\"S1.T1.1.6.5.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A survey of advances in vision-based human motion capture and analysis </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.6.5.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Moeslund et&#160;al.<span id=\"S1.T1.1.6.5.2.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib110\" title=\"\" class=\"ltx_ref\">2006</a><span id=\"S1.T1.1.6.5.2.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S1.T1.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.6.5.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">CVIU</span></td>&#13;\n<td id=\"S1.T1.1.6.5.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.6.5.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.6.5.4.1.1\" class=\"ltx_p\" style=\"width:241.8pt;\"><span id=\"S1.T1.1.6.5.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Further summary of human motion capture and analysis from 2000 to 2006, following </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.6.5.4.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Moeslund and Granum<span id=\"S1.T1.1.6.5.4.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib109\" title=\"\" class=\"ltx_ref\">2001</a><span id=\"S1.T1.1.6.5.4.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S1.T1.1.6.5.4.1.1.5\" class=\"ltx_text\" style=\"font-size:80%;\">.</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S1.T1.1.7.6\" class=\"ltx_tr\">&#13;\n<td id=\"S1.T1.1.7.6.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.7.6.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">6</span></td>&#13;\n<td id=\"S1.T1.1.7.6.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.7.6.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.7.6.2.1.1\" class=\"ltx_p\" style=\"width:170.7pt;\"><span id=\"S1.T1.1.7.6.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Vision-based human motion analysis: An overview </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.7.6.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Poppe<span id=\"S1.T1.1.7.6.2.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib134\" title=\"\" class=\"ltx_ref\">2007</a><span id=\"S1.T1.1.7.6.2.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S1.T1.1.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.7.6.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">CVIU</span></td>&#13;\n<td id=\"S1.T1.1.7.6.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.7.6.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.7.6.4.1.1\" class=\"ltx_p\" style=\"width:241.8pt;\"><span id=\"S1.T1.1.7.6.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A summary of vision-based human motion analysis with markerless data.</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S1.T1.1.8.7\" class=\"ltx_tr\">&#13;\n<td id=\"S1.T1.1.8.7.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.8.7.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">7</span></td>&#13;\n<td id=\"S1.T1.1.8.7.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.8.7.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.8.7.2.1.1\" class=\"ltx_p\" style=\"width:170.7pt;\"><span id=\"S1.T1.1.8.7.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">3D human motion analysis in monocular video: techniques and challenges </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.8.7.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Sminchisescu<span id=\"S1.T1.1.8.7.2.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib150\" title=\"\" class=\"ltx_ref\">2008</a><span id=\"S1.T1.1.8.7.2.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S1.T1.1.8.7.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<table id=\"S1.T1.1.8.7.3.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S1.T1.1.8.7.3.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S1.T1.1.8.7.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S1.T1.1.8.7.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Book</span></td>&#13;\n</tr>&#13;\n<tr id=\"S1.T1.1.8.7.3.1.2\" class=\"ltx_tr\">&#13;\n<td id=\"S1.T1.1.8.7.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S1.T1.1.8.7.3.1.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Chapter</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n</td>&#13;\n<td id=\"S1.T1.1.8.7.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.8.7.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.8.7.4.1.1\" class=\"ltx_p\" style=\"width:241.8pt;\"><span id=\"S1.T1.1.8.7.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">An overview of reconstructing 3D human motion with video sequences from single-view camera.</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S1.T1.1.9.8\" class=\"ltx_tr\">&#13;\n<td id=\"S1.T1.1.9.8.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.9.8.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">8</span></td>&#13;\n<td id=\"S1.T1.1.9.8.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.9.8.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.9.8.2.1.1\" class=\"ltx_p\" style=\"width:170.7pt;\"><span id=\"S1.T1.1.9.8.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Advances in view-invariant human motion analysis: A review </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.9.8.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Ji and Liu<span id=\"S1.T1.1.9.8.2.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib67\" title=\"\" class=\"ltx_ref\">2010</a><span id=\"S1.T1.1.9.8.2.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S1.T1.1.9.8.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.9.8.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">TSMCS</span></td>&#13;\n<td id=\"S1.T1.1.9.8.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.9.8.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.9.8.4.1.1\" class=\"ltx_p\" style=\"width:241.8pt;\"><span id=\"S1.T1.1.9.8.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A summary of human motion analysis, including human detection, view-invariant pose representation and estimation, and behavior understanding.</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S1.T1.1.10.9\" class=\"ltx_tr\">&#13;\n<td id=\"S1.T1.1.10.9.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.10.9.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">9</span></td>&#13;\n<td id=\"S1.T1.1.10.9.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.10.9.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.10.9.2.1.1\" class=\"ltx_p\" style=\"width:170.7pt;\"><span id=\"S1.T1.1.10.9.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Visual analysis of humans </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.10.9.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Moeslund et&#160;al.<span id=\"S1.T1.1.10.9.2.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib111\" title=\"\" class=\"ltx_ref\">2011</a><span id=\"S1.T1.1.10.9.2.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S1.T1.1.10.9.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.10.9.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Book</span></td>&#13;\n<td id=\"S1.T1.1.10.9.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.10.9.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.10.9.4.1.1\" class=\"ltx_p\" style=\"width:241.8pt;\"><span id=\"S1.T1.1.10.9.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A comprehensive overview of human analysis, including detection and tracking, pose estimation, recognition, and applications with human body and face.</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S1.T1.1.11.10\" class=\"ltx_tr\">&#13;\n<td id=\"S1.T1.1.11.10.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.11.10.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">10</span></td>&#13;\n<td id=\"S1.T1.1.11.10.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.11.10.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.11.10.2.1.1\" class=\"ltx_p\" style=\"width:170.7pt;\"><span id=\"S1.T1.1.11.10.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Human pose estimation and activity recognition from multi-view videos: Comparative explorations of recent developments </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.11.10.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Holte et&#160;al.<span id=\"S1.T1.1.11.10.2.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib52\" title=\"\" class=\"ltx_ref\">2012</a><span id=\"S1.T1.1.11.10.2.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S1.T1.1.11.10.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.11.10.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">JSTSP</span></td>&#13;\n<td id=\"S1.T1.1.11.10.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.11.10.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.11.10.4.1.1\" class=\"ltx_p\" style=\"width:241.8pt;\"><span id=\"S1.T1.1.11.10.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A review of model-based 3D HPE and action recognition methods under multi-view.</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S1.T1.1.12.11\" class=\"ltx_tr\">&#13;\n<td id=\"S1.T1.1.12.11.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.12.11.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">11</span></td>&#13;\n<td id=\"S1.T1.1.12.11.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.12.11.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.12.11.2.1.1\" class=\"ltx_p\" style=\"width:170.7pt;\"><span id=\"S1.T1.1.12.11.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A survey of human motion analysis using depth imagery </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.12.11.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Chen et&#160;al.<span id=\"S1.T1.1.12.11.2.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">2013</a><span id=\"S1.T1.1.12.11.2.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S1.T1.1.12.11.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.12.11.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">PRL</span></td>&#13;\n<td id=\"S1.T1.1.12.11.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.12.11.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.12.11.4.1.1\" class=\"ltx_p\" style=\"width:241.8pt;\"><span id=\"S1.T1.1.12.11.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A survey of traditional RGB-D-based human action recognition methods, including description of sensors, corresponding datasets, and approaches.</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S1.T1.1.13.12\" class=\"ltx_tr\">&#13;\n<td id=\"S1.T1.1.13.12.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.13.12.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">12</span></td>&#13;\n<td id=\"S1.T1.1.13.12.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.13.12.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.13.12.2.1.1\" class=\"ltx_p\" style=\"width:170.7pt;\"><span id=\"S1.T1.1.13.12.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A survey on model based approaches for 2D and 3D visual human pose recovery </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.13.12.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Perez-Sala et&#160;al.<span id=\"S1.T1.1.13.12.2.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib128\" title=\"\" class=\"ltx_ref\">2014</a><span id=\"S1.T1.1.13.12.2.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S1.T1.1.13.12.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.13.12.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Sensors</span></td>&#13;\n<td id=\"S1.T1.1.13.12.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.13.12.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.13.12.4.1.1\" class=\"ltx_p\" style=\"width:241.8pt;\"><span id=\"S1.T1.1.13.12.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A survey of model-based approaches for HPE, grouped in five main modules: appearance, viewpoint, spatial relations, temporal consistence, and behavior.</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S1.T1.1.14.13\" class=\"ltx_tr\">&#13;\n<td id=\"S1.T1.1.14.13.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.14.13.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">13</span></td>&#13;\n<td id=\"S1.T1.1.14.13.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.14.13.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.14.13.2.1.1\" class=\"ltx_p\" style=\"width:170.7pt;\"><span id=\"S1.T1.1.14.13.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A survey of human pose estimation: the body parts parsing based methods </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.14.13.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Liu et&#160;al.<span id=\"S1.T1.1.14.13.2.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib94\" title=\"\" class=\"ltx_ref\">2015</a><span id=\"S1.T1.1.14.13.2.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S1.T1.1.14.13.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.14.13.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">JVCIR</span></td>&#13;\n<td id=\"S1.T1.1.14.13.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.14.13.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.14.13.4.1.1\" class=\"ltx_p\" style=\"width:241.8pt;\"><span id=\"S1.T1.1.14.13.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A survey of body parts parsing-based HPE methods under both single-view and multiple-view from different input sources(images, videos, depth).</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S1.T1.1.15.14\" class=\"ltx_tr\">&#13;\n<td id=\"S1.T1.1.15.14.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.15.14.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">14</span></td>&#13;\n<td id=\"S1.T1.1.15.14.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.15.14.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.15.14.2.1.1\" class=\"ltx_p\" style=\"width:170.7pt;\"><span id=\"S1.T1.1.15.14.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Human pose estimation from monocular images: A comprehensive survey </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.15.14.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Gong et&#160;al.<span id=\"S1.T1.1.15.14.2.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib47\" title=\"\" class=\"ltx_ref\">2016</a><span id=\"S1.T1.1.15.14.2.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S1.T1.1.15.14.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.15.14.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Sensors</span></td>&#13;\n<td id=\"S1.T1.1.15.14.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.15.14.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.15.14.4.1.1\" class=\"ltx_p\" style=\"width:241.8pt;\"><span id=\"S1.T1.1.15.14.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A survey of monocular-based traditional HPE methods with a few deep learning-based methods.</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S1.T1.1.16.15\" class=\"ltx_tr\">&#13;\n<td id=\"S1.T1.1.16.15.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.16.15.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">15</span></td>&#13;\n<td id=\"S1.T1.1.16.15.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.16.15.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.16.15.2.1.1\" class=\"ltx_p\" style=\"width:170.7pt;\"><span id=\"S1.T1.1.16.15.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">3d human pose estimation: A review of the literature and analysis of covariates </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.16.15.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Sarafianos et&#160;al.<span id=\"S1.T1.1.16.15.2.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib145\" title=\"\" class=\"ltx_ref\">2016</a><span id=\"S1.T1.1.16.15.2.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S1.T1.1.16.15.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.16.15.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">CVIU</span></td>&#13;\n<td id=\"S1.T1.1.16.15.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.16.15.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.16.15.4.1.1\" class=\"ltx_p\" style=\"width:241.8pt;\"><span id=\"S1.T1.1.16.15.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A review of 3D HPE methods with different type of inputs(e.g., single image or video, monocular or multi-view).</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S1.T1.1.17.16\" class=\"ltx_tr\">&#13;\n<td id=\"S1.T1.1.17.16.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.17.16.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">16</span></td>&#13;\n<td id=\"S1.T1.1.17.16.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.17.16.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.17.16.2.1.1\" class=\"ltx_p\" style=\"width:170.7pt;\"><span id=\"S1.T1.1.17.16.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">RGB-D-based human motion recognition with deep learning: A survey </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.17.16.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Wang et&#160;al.<span id=\"S1.T1.1.17.16.2.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib172\" title=\"\" class=\"ltx_ref\">2018b</a><span id=\"S1.T1.1.17.16.2.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S1.T1.1.17.16.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.17.16.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">CVIU</span></td>&#13;\n<td id=\"S1.T1.1.17.16.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.17.16.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.17.16.4.1.1\" class=\"ltx_p\" style=\"width:241.8pt;\"><span id=\"S1.T1.1.17.16.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A survey of RGB-D-based motion recognition in four categories: RGB-based, depth-based, skeleton-based, and RGB-D-based.</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S1.T1.1.18.17\" class=\"ltx_tr\">&#13;\n<td id=\"S1.T1.1.18.17.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.18.17.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">17</span></td>&#13;\n<td id=\"S1.T1.1.18.17.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.18.17.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.18.17.2.1.1\" class=\"ltx_p\" style=\"width:170.7pt;\"><span id=\"S1.T1.1.18.17.2.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S1.T1.1.18.17.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.18.17.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Ours</span></td>&#13;\n<td id=\"S1.T1.1.18.17.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">&#13;\n<span id=\"S1.T1.1.18.17.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S1.T1.1.18.17.4.1.1\" class=\"ltx_p\" style=\"width:241.8pt;\"><span id=\"S1.T1.1.18.17.4.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">A comprehensive survey of deep learning-based monocular HPE research and human pose datasets, organized into four groups: 2D single HPE, 2D multi-HPE, 3D single HPE and 3D multi-HPE</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n",
            "<table id=\"S1.T1.1.2.1.1.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S1.T1.1.2.1.1.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S1.T1.1.2.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S1.T1.1.2.1.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">1</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n\n",
            "<table id=\"S1.T1.1.8.7.3.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S1.T1.1.8.7.3.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S1.T1.1.8.7.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S1.T1.1.8.7.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Book</span></td>&#13;\n</tr>&#13;\n<tr id=\"S1.T1.1.8.7.3.1.2\" class=\"ltx_tr\">&#13;\n<td id=\"S1.T1.1.8.7.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S1.T1.1.8.7.3.1.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Chapter</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "(Aggarwal and Cai, 1999) Aggarwal and Cai (1999)\r\n\r\nAggarwal, J.K., Cai, Q.,\r\n1999.\r\n\r\n\r\nHuman motion analysis: A review.\r\n\r\n\r\nComputer Vision and Image Understanding\r\n73, 428–440.",
            "(Gavrila, 1999) Gavrila (1999)\r\n\r\nGavrila, D.M., 1999.\r\n\r\n\r\nThe visual analysis of human movement: A survey.\r\n\r\n\r\nComputer Vision and Image Understanding\r\n73, 82–98.",
            "(Moeslund and Granum, 2001) Moeslund and Granum (2001)\r\n\r\nMoeslund, T.B., Granum, E.,\r\n2001.\r\n\r\n\r\nA survey of computer vision-based human motion\r\ncapture.\r\n\r\n\r\nComputer Vision and Image Understanding\r\n81, 231–268.",
            "(Hu et al., 2004) Hu et al. (2004)\r\n\r\nHu, W., Tan, T., Wang,\r\nL., Maybank, S., 2004.\r\n\r\n\r\nA survey on visual surveillance of object motion and\r\nbehaviors.\r\n\r\n\r\nIEEE Transactions on Systems, Man, and Cybernetics,\r\nPart C (Applications and Reviews) 34,\r\n334–352.",
            "(Moeslund et al., 2006) Moeslund et al. (2006)\r\n\r\nMoeslund, T.B., Hilton, A.,\r\nKrüger, V., 2006.\r\n\r\n\r\nA survey of advances in vision-based human motion\r\ncapture and analysis.\r\n\r\n\r\nComputer Vision and Image Understanding\r\n104, 90–126.",
            "(Moeslund and Granum, 2001) Moeslund and Granum (2001)\r\n\r\nMoeslund, T.B., Granum, E.,\r\n2001.\r\n\r\n\r\nA survey of computer vision-based human motion\r\ncapture.\r\n\r\n\r\nComputer Vision and Image Understanding\r\n81, 231–268.",
            "(Poppe, 2007) Poppe (2007)\r\n\r\nPoppe, R., 2007.\r\n\r\n\r\nVision-based human motion analysis: An overview.\r\n\r\n\r\nComputer Vision and Image Understanding\r\n108, 4–18.",
            "(Sminchisescu, 2008) Sminchisescu (2008)\r\n\r\nSminchisescu, C., 2008.\r\n\r\n\r\n3d human motion analysis in monocular video:\r\ntechniques and challenges, in: Human Motion.\r\nSpringer, pp. 185–211.",
            "(Ji and Liu, 2010) Ji and Liu (2010)\r\n\r\nJi, X., Liu, H., 2010.\r\n\r\n\r\nAdvances in view-invariant human motion analysis: A\r\nreview.\r\n\r\n\r\nIEEE Transactions on Systems, Man, and Cybernetics,\r\nPart C (Applications and Reviews) 40,\r\n13–24.",
            "(Moeslund et al., 2011) Moeslund et al. (2011)\r\n\r\nMoeslund, T.B., Hilton, A.,\r\nKrüger, V., Sigal, L.,\r\n2011.\r\n\r\n\r\nVisual analysis of humans.\r\n\r\n\r\nSpringer.",
            "(Holte et al., 2012) Holte et al. (2012)\r\n\r\nHolte, M.B., Tran, C.,\r\nTrivedi, M.M., Moeslund, T.B.,\r\n2012.\r\n\r\n\r\nHuman pose estimation and activity recognition from\r\nmulti-view videos: Comparative explorations of recent developments.\r\n\r\n\r\nIEEE Journal of selected topics in signal\r\nprocessing 6, 538–552.",
            "(Chen et al., 2013) Chen et al. (2013)\r\n\r\nChen, L., Wei, H.,\r\nFerryman, J., 2013.\r\n\r\n\r\nA survey of human motion analysis using depth\r\nimagery.\r\n\r\n\r\nPattern Recognition Letters 34,\r\n1995–2006.",
            "(Perez-Sala et al., 2014) Perez-Sala et al. (2014)\r\n\r\nPerez-Sala, X., Escalera, S.,\r\nAngulo, C., Gonzalez, J.,\r\n2014.\r\n\r\n\r\nA survey on model based approaches for 2d and 3d\r\nvisual human pose recovery.\r\n\r\n\r\nSensors 14,\r\n4189–4210.",
            "(Liu et al., 2015) Liu et al. (2015)\r\n\r\nLiu, Z., Zhu, J., Bu, J.,\r\nChen, C., 2015.\r\n\r\n\r\nA survey of human pose estimation: the body parts\r\nparsing based methods.\r\n\r\n\r\nJournal of Visual Communication and Image\r\nRepresentation 32, 10–19.",
            "(Gong et al., 2016) Gong et al. (2016)\r\n\r\nGong, W., Zhang, X.,\r\nGonzàlez, J., Sobral, A.,\r\nBouwmans, T., Tu, C.,\r\nZahzah, E.h., 2016.\r\n\r\n\r\nHuman pose estimation from monocular images: A\r\ncomprehensive survey.\r\n\r\n\r\nSensors 16,\r\n1966.",
            "(Sarafianos et al., 2016) Sarafianos et al. (2016)\r\n\r\nSarafianos, N., Boteanu, B.,\r\nIonescu, B., Kakadiaris, I.A.,\r\n2016.\r\n\r\n\r\n3d human pose estimation: A review of the literature\r\nand analysis of covariates.\r\n\r\n\r\nComputer Vision and Image Understanding\r\n152, 1–20.",
            "(Wang et al., 2018b) Wang et al. (2018b)\r\n\r\nWang, P., Li, W.,\r\nOgunbona, P., Wan, J.,\r\nEscalera, S., 2018b.\r\n\r\n\r\nRgb-d-based human motion recognition with deep\r\nlearning: A survey.\r\n\r\n\r\nComputer Vision and Image Understanding\r\n171, 118–139."
        ],
        "references": [
            "As listed in Table 1, with the development of human pose estimation in the past decades, several notable surveys summarized the research work in this area. The surveys (Aggarwal and Cai, 1999; Gavrila, 1999; Poppe, 2007; Ji and Liu, 2010; Moeslund et al., 2011) reviewed the early work of human motion analysis in many aspects (e.g., detection and tracking, pose estimation, recognition) and described the relation between human pose estimation and other related tasks.\r\nWhile Hu et al. (2004) summarized the research of human motion analysis for video surveillance application, the reviews (Moeslund and Granum, 2001; Moeslund et al., 2006) focused on the human motion capture systems. More recent surveys were mainly focusing on relatively narrow directions, such as RGB-D-based action recognition(Chen et al., 2013; Wang et al., 2018b), 3D HPE (Sminchisescu, 2008; Holte et al., 2012; Sarafianos et al., 2016), model-based HPE (Holte et al., 2012; Perez-Sala et al., 2014), body parts-based HPE (Liu et al., 2015), and monocular-based HPE (Sminchisescu, 2008; Gong et al., 2016)."
        ]
    },
    "id_table_2": {
        "caption": "Table 2: The Categories of deep learning-based monocular human pose estimation.",
        "table": [
            "<table id=\"S2.T2.3\" class=\"ltx_tabular ltx_centering ltx_align_middle\">&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S2.T2.3.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S2.T2.3.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S2.T2.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Direction</span></td>&#13;\n<td id=\"S2.T2.3.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S2.T2.3.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Sub-direction</span></td>&#13;\n<td id=\"S2.T2.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S2.T2.3.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Categories</span></td>&#13;\n<td id=\"S2.T2.3.1.1.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S2.T2.3.1.1.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S2.T2.3.1.1.4.1.1\" class=\"ltx_p\" style=\"width:321.5pt;\"><span id=\"S2.T2.3.1.1.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Sub-categories</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S2.T2.3.2.2\" class=\"ltx_tr\">&#13;\n<td id=\"S2.T2.3.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"12\"><span id=\"S2.T2.3.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-113.8pt;\">2D HPE</span></td>&#13;\n<td id=\"S2.T2.3.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"8\"><span id=\"S2.T2.3.2.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-71.1pt;\">2D Single</span></td>&#13;\n<td id=\"S2.T2.3.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S2.T2.3.2.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-22.8pt;\">Regression-based</span></td>&#13;\n<td id=\"S2.T2.3.2.2.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S2.T2.3.2.2.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S2.T2.3.2.2.4.1.1\" class=\"ltx_p\" style=\"width:321.5pt;\"><span id=\"S2.T2.3.2.2.4.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">(1) Direct prediction</span><span id=\"S2.T2.3.2.2.4.1.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">: </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.2.2.4.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Krizhevsky et&#160;al.<span id=\"S2.T2.3.2.2.4.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib78\" title=\"\" class=\"ltx_ref\">2012</a><span id=\"S2.T2.3.2.2.4.1.1.5.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S2.T2.3.2.2.4.1.1.6\" class=\"ltx_text\" style=\"font-size:80%;\">, on video&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.2.2.4.1.1.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Pfister et&#160;al.<span id=\"S2.T2.3.2.2.4.1.1.8.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib130\" title=\"\" class=\"ltx_ref\">2014</a><span id=\"S2.T2.3.2.2.4.1.1.9.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S2.T2.3.3.3\" class=\"ltx_tr\">&#13;\n<td id=\"S2.T2.3.3.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">&#13;\n<span id=\"S2.T2.3.3.3.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S2.T2.3.3.3.1.1.1\" class=\"ltx_p\" style=\"width:321.5pt;\"><span id=\"S2.T2.3.3.3.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">(2) Supervision improvement</span><span id=\"S2.T2.3.3.3.1.1.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">: transform heatmaps to joint coordinates&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.3.3.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Luvizon et&#160;al.<span id=\"S2.T2.3.3.3.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib99\" title=\"\" class=\"ltx_ref\">2017</a>; Nibali et&#160;al.<span id=\"S2.T2.3.3.3.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib116\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S2.T2.3.3.3.1.1.1.5.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S2.T2.3.3.3.1.1.1.6\" class=\"ltx_text\" style=\"font-size:80%;\">, recursive refinement&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.3.3.1.1.1.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Carreira et&#160;al.<span id=\"S2.T2.3.3.3.1.1.1.8.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib14\" title=\"\" class=\"ltx_ref\">2016</a><span id=\"S2.T2.3.3.3.1.1.1.9.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S2.T2.3.3.3.1.1.1.10\" class=\"ltx_text\" style=\"font-size:80%;\">, bone-based constraint&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.3.3.1.1.1.11.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Sun et&#160;al.<span id=\"S2.T2.3.3.3.1.1.1.12.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib152\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S2.T2.3.3.3.1.1.1.13.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S2.T2.3.4.4\" class=\"ltx_tr\">&#13;\n<td id=\"S2.T2.3.4.4.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">&#13;\n<span id=\"S2.T2.3.4.4.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S2.T2.3.4.4.1.1.1\" class=\"ltx_p\" style=\"width:321.5pt;\"><span id=\"S2.T2.3.4.4.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">(3) Multi-task</span><span id=\"S2.T2.3.4.4.1.1.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">: with body part detection&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.4.4.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Li et&#160;al.<span id=\"S2.T2.3.4.4.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib88\" title=\"\" class=\"ltx_ref\">2014</a><span id=\"S2.T2.3.4.4.1.1.1.5.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S2.T2.3.4.4.1.1.1.6\" class=\"ltx_text\" style=\"font-size:80%;\">, with person detection and action classification&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.4.4.1.1.1.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Gkioxari et&#160;al.<span id=\"S2.T2.3.4.4.1.1.1.8.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib44\" title=\"\" class=\"ltx_ref\">2014a</a><span id=\"S2.T2.3.4.4.1.1.1.9.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S2.T2.3.4.4.1.1.1.10\" class=\"ltx_text\" style=\"font-size:80%;\">, with heatmap-based joint detection&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.4.4.1.1.1.11.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Fan et&#160;al.<span id=\"S2.T2.3.4.4.1.1.1.12.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib37\" title=\"\" class=\"ltx_ref\">2015</a><span id=\"S2.T2.3.4.4.1.1.1.13.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S2.T2.3.4.4.1.1.1.14\" class=\"ltx_text\" style=\"font-size:80%;\">, with action recognition on video sequences&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.4.4.1.1.1.15.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Luvizon et&#160;al.<span id=\"S2.T2.3.4.4.1.1.1.16.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib98\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S2.T2.3.4.4.1.1.1.17.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S2.T2.3.5.5\" class=\"ltx_tr\">&#13;\n<td id=\"S2.T2.3.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"5\"><span id=\"S2.T2.3.5.5.1.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-45.5pt;\">Detection-based</span></td>&#13;\n<td id=\"S2.T2.3.5.5.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S2.T2.3.5.5.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S2.T2.3.5.5.2.1.1\" class=\"ltx_p\" style=\"width:321.5pt;\"><span id=\"S2.T2.3.5.5.2.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">(1) Patch-based</span><span id=\"S2.T2.3.5.5.2.1.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">: </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.5.5.2.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Jain et&#160;al.<span id=\"S2.T2.3.5.5.2.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib63\" title=\"\" class=\"ltx_ref\">2013</a>; Chen and Yuille<span id=\"S2.T2.3.5.5.2.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib20\" title=\"\" class=\"ltx_ref\">2014</a>; Ramakrishna et&#160;al.<span id=\"S2.T2.3.5.5.2.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib137\" title=\"\" class=\"ltx_ref\">2014</a><span id=\"S2.T2.3.5.5.2.1.1.5.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S2.T2.3.6.6\" class=\"ltx_tr\">&#13;\n<td id=\"S2.T2.3.6.6.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">&#13;\n<span id=\"S2.T2.3.6.6.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S2.T2.3.6.6.1.1.1\" class=\"ltx_p\" style=\"width:321.5pt;\"><span id=\"S2.T2.3.6.6.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">(2) Network design</span><span id=\"S2.T2.3.6.6.1.1.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">: </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.6.6.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Tompson et&#160;al.<span id=\"S2.T2.3.6.6.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib163\" title=\"\" class=\"ltx_ref\">2015</a>; Bulat and Tzimiropoulos<span id=\"S2.T2.3.6.6.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">2016</a>; Xiao et&#160;al.<span id=\"S2.T2.3.6.6.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib176\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S2.T2.3.6.6.1.1.1.5.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S2.T2.3.6.6.1.1.1.6\" class=\"ltx_text\" style=\"font-size:80%;\">, multi-scale inputs&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.6.6.1.1.1.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Rafi et&#160;al.<span id=\"S2.T2.3.6.6.1.1.1.8.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib136\" title=\"\" class=\"ltx_ref\">2016</a><span id=\"S2.T2.3.6.6.1.1.1.9.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S2.T2.3.6.6.1.1.1.10\" class=\"ltx_text\" style=\"font-size:80%;\">, heatmap-based improvement&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.6.6.1.1.1.11.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Papandreou et&#160;al.<span id=\"S2.T2.3.6.6.1.1.1.12.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib123\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S2.T2.3.6.6.1.1.1.13.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S2.T2.3.6.6.1.1.1.14\" class=\"ltx_text\" style=\"font-size:80%;\">, Hourglass&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.6.6.1.1.1.15.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Newell et&#160;al.<span id=\"S2.T2.3.6.6.1.1.1.16.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib115\" title=\"\" class=\"ltx_ref\">2016</a><span id=\"S2.T2.3.6.6.1.1.1.17.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S2.T2.3.6.6.1.1.1.18\" class=\"ltx_text\" style=\"font-size:80%;\">, CPM&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.6.6.1.1.1.19.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Wei et&#160;al.<span id=\"S2.T2.3.6.6.1.1.1.20.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib174\" title=\"\" class=\"ltx_ref\">2016</a><span id=\"S2.T2.3.6.6.1.1.1.21.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S2.T2.3.6.6.1.1.1.22\" class=\"ltx_text\" style=\"font-size:80%;\">, PRM&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.6.6.1.1.1.23.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Yang et&#160;al.<span id=\"S2.T2.3.6.6.1.1.1.24.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib177\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S2.T2.3.6.6.1.1.1.25.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S2.T2.3.6.6.1.1.1.26\" class=\"ltx_text\" style=\"font-size:80%;\">, feed forward module&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.6.6.1.1.1.27.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Belagiannis and Zisserman<span id=\"S2.T2.3.6.6.1.1.1.28.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S2.T2.3.6.6.1.1.1.29.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S2.T2.3.6.6.1.1.1.30\" class=\"ltx_text\" style=\"font-size:80%;\">, HRNet&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.6.6.1.1.1.31.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Sun et&#160;al.<span id=\"S2.T2.3.6.6.1.1.1.32.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib151\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S2.T2.3.6.6.1.1.1.33.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S2.T2.3.6.6.1.1.1.34\" class=\"ltx_text\" style=\"font-size:80%;\">, GAN&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.6.6.1.1.1.35.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Chou et&#160;al.<span id=\"S2.T2.3.6.6.1.1.1.36.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">2017</a>; Chen et&#160;al.<span id=\"S2.T2.3.6.6.1.1.1.36.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">2017</a>; Peng et&#160;al.<span id=\"S2.T2.3.6.6.1.1.1.36.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib127\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S2.T2.3.6.6.1.1.1.37.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S2.T2.3.7.7\" class=\"ltx_tr\">&#13;\n<td id=\"S2.T2.3.7.7.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">&#13;\n<span id=\"S2.T2.3.7.7.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S2.T2.3.7.7.1.1.1\" class=\"ltx_p\" style=\"width:321.5pt;\"><span id=\"S2.T2.3.7.7.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">(3) Body structure constraint</span><span id=\"S2.T2.3.7.7.1.1.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">: </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.7.7.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Tompson et&#160;al.<span id=\"S2.T2.3.7.7.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib164\" title=\"\" class=\"ltx_ref\">2014</a>; Lifshitz et&#160;al.<span id=\"S2.T2.3.7.7.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib91\" title=\"\" class=\"ltx_ref\">2016</a>; Yang et&#160;al.<span id=\"S2.T2.3.7.7.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib178\" title=\"\" class=\"ltx_ref\">2016</a>; Gkioxari et&#160;al.<span id=\"S2.T2.3.7.7.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib46\" title=\"\" class=\"ltx_ref\">2016</a>; Chu et&#160;al.<span id=\"S2.T2.3.7.7.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\">2016</a>, <a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">2017</a>; Ning et&#160;al.<span id=\"S2.T2.3.7.7.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib119\" title=\"\" class=\"ltx_ref\">2018</a>; Ke et&#160;al.<span id=\"S2.T2.3.7.7.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib74\" title=\"\" class=\"ltx_ref\">2018</a>; Tang et&#160;al.<span id=\"S2.T2.3.7.7.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib157\" title=\"\" class=\"ltx_ref\">2018a</a>; Tang and Wu<span id=\"S2.T2.3.7.7.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib156\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S2.T2.3.7.7.1.1.1.5.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S2.T2.3.8.8\" class=\"ltx_tr\">&#13;\n<td id=\"S2.T2.3.8.8.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">&#13;\n<span id=\"S2.T2.3.8.8.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S2.T2.3.8.8.1.1.1\" class=\"ltx_p\" style=\"width:321.5pt;\"><span id=\"S2.T2.3.8.8.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">(4) Temporal constraint</span><span id=\"S2.T2.3.8.8.1.1.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">: </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.8.8.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Jain et&#160;al.<span id=\"S2.T2.3.8.8.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib64\" title=\"\" class=\"ltx_ref\">2014</a>; Pfister et&#160;al.<span id=\"S2.T2.3.8.8.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib129\" title=\"\" class=\"ltx_ref\">2015</a>; Luo et&#160;al.<span id=\"S2.T2.3.8.8.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib97\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S2.T2.3.8.8.1.1.1.5.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S2.T2.3.9.9\" class=\"ltx_tr\">&#13;\n<td id=\"S2.T2.3.9.9.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">&#13;\n<span id=\"S2.T2.3.9.9.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S2.T2.3.9.9.1.1.1\" class=\"ltx_p\" style=\"width:321.5pt;\"><span id=\"S2.T2.3.9.9.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">(5) Network compression</span><span id=\"S2.T2.3.9.9.1.1.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">: </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.9.9.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Tang et&#160;al.<span id=\"S2.T2.3.9.9.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib158\" title=\"\" class=\"ltx_ref\">2018b</a>; Debnath et&#160;al.<span id=\"S2.T2.3.9.9.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">2018</a>; Feng et&#160;al.<span id=\"S2.T2.3.9.9.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S2.T2.3.9.9.1.1.1.5.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S2.T2.3.10.10\" class=\"ltx_tr\">&#13;\n<td id=\"S2.T2.3.10.10.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"4\"><span id=\"S2.T2.3.10.10.1.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-22.8pt;\">2D Multiple</span></td>&#13;\n<td id=\"S2.T2.3.10.10.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S2.T2.3.10.10.2.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-5.7pt;\">Top-down</span></td>&#13;\n<td id=\"S2.T2.3.10.10.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S2.T2.3.10.10.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S2.T2.3.10.10.3.1.1\" class=\"ltx_p\" style=\"width:321.5pt;\"><span id=\"S2.T2.3.10.10.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">coarse-to-fine&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.10.10.3.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Iqbal and Gall<span id=\"S2.T2.3.10.10.3.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib60\" title=\"\" class=\"ltx_ref\">2016</a>; Huang et&#160;al.<span id=\"S2.T2.3.10.10.3.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib55\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S2.T2.3.10.10.3.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S2.T2.3.10.10.3.1.1.5\" class=\"ltx_text\" style=\"font-size:80%;\">, bounding box refinement&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.10.10.3.1.1.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Fang et&#160;al.<span id=\"S2.T2.3.10.10.3.1.1.7.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib38\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S2.T2.3.10.10.3.1.1.8.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S2.T2.3.10.10.3.1.1.9\" class=\"ltx_text\" style=\"font-size:80%;\">, multi-level feature fusion&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.10.10.3.1.1.10.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Xiao et&#160;al.<span id=\"S2.T2.3.10.10.3.1.1.11.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib176\" title=\"\" class=\"ltx_ref\">2018</a>; Chen et&#160;al.<span id=\"S2.T2.3.10.10.3.1.1.11.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S2.T2.3.10.10.3.1.1.12.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S2.T2.3.10.10.3.1.1.13\" class=\"ltx_text\" style=\"font-size:80%;\">, results refinement&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.10.10.3.1.1.14.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Moon et&#160;al.<span id=\"S2.T2.3.10.10.3.1.1.15.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib112\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S2.T2.3.10.10.3.1.1.16.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S2.T2.3.11.11\" class=\"ltx_tr\">&#13;\n<td id=\"S2.T2.3.11.11.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S2.T2.3.11.11.1.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-11.4pt;\">Bottom-up</span></td>&#13;\n<td id=\"S2.T2.3.11.11.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S2.T2.3.11.11.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S2.T2.3.11.11.2.1.1\" class=\"ltx_p\" style=\"width:321.5pt;\"><span id=\"S2.T2.3.11.11.2.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">(1) Two-stage</span><span id=\"S2.T2.3.11.11.2.1.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">: DeepCut&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.11.11.2.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Pishchulin et&#160;al.<span id=\"S2.T2.3.11.11.2.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib131\" title=\"\" class=\"ltx_ref\">2016</a><span id=\"S2.T2.3.11.11.2.1.1.5.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S2.T2.3.11.11.2.1.1.6\" class=\"ltx_text\" style=\"font-size:80%;\">, DeeperCut&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.11.11.2.1.1.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Insafutdinov et&#160;al.<span id=\"S2.T2.3.11.11.2.1.1.8.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib58\" title=\"\" class=\"ltx_ref\">2016</a><span id=\"S2.T2.3.11.11.2.1.1.9.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S2.T2.3.11.11.2.1.1.10\" class=\"ltx_text\" style=\"font-size:80%;\">, OpenPose&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.11.11.2.1.1.11.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Cao et&#160;al.<span id=\"S2.T2.3.11.11.2.1.1.12.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">2016</a><span id=\"S2.T2.3.11.11.2.1.1.13.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S2.T2.3.11.11.2.1.1.14\" class=\"ltx_text\" style=\"font-size:80%;\">, PPN&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.11.11.2.1.1.15.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Nie et&#160;al.<span id=\"S2.T2.3.11.11.2.1.1.16.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib118\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S2.T2.3.11.11.2.1.1.17.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S2.T2.3.11.11.2.1.1.18\" class=\"ltx_text\" style=\"font-size:80%;\">, PifPafNet&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.11.11.2.1.1.19.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Kreiss et&#160;al.<span id=\"S2.T2.3.11.11.2.1.1.20.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib77\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S2.T2.3.11.11.2.1.1.21.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S2.T2.3.12.12\" class=\"ltx_tr\">&#13;\n<td id=\"S2.T2.3.12.12.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">&#13;\n<span id=\"S2.T2.3.12.12.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S2.T2.3.12.12.1.1.1\" class=\"ltx_p\" style=\"width:321.5pt;\"><span id=\"S2.T2.3.12.12.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">(2) Single-stage</span><span id=\"S2.T2.3.12.12.1.1.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">: heatmaps and associative embedding maps&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.12.12.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Newell et&#160;al.<span id=\"S2.T2.3.12.12.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib114\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S2.T2.3.12.12.1.1.1.5.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S2.T2.3.13.13\" class=\"ltx_tr\">&#13;\n<td id=\"S2.T2.3.13.13.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">&#13;\n<span id=\"S2.T2.3.13.13.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S2.T2.3.13.13.1.1.1\" class=\"ltx_p\" style=\"width:321.5pt;\"><span id=\"S2.T2.3.13.13.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">(3) Multi-task</span><span id=\"S2.T2.3.13.13.1.1.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">: instance segmentation&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.13.13.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Papandreou et&#160;al.<span id=\"S2.T2.3.13.13.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib122\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S2.T2.3.13.13.1.1.1.5.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S2.T2.3.13.13.1.1.1.6\" class=\"ltx_text\" style=\"font-size:80%;\">, keypoint detection and semantic segmentation&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.13.13.1.1.1.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Kocabas et&#160;al.<span id=\"S2.T2.3.13.13.1.1.1.8.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib76\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S2.T2.3.13.13.1.1.1.9.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S2.T2.3.14.14\" class=\"ltx_tr\">&#13;\n<td id=\"S2.T2.3.14.14.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"5\"><span id=\"S2.T2.3.14.14.1.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-51.2pt;\">3D HPE</span></td>&#13;\n<td id=\"S2.T2.3.14.14.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"4\"><span id=\"S2.T2.3.14.14.2.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-37.0pt;\">3D Single</span></td>&#13;\n<td id=\"S2.T2.3.14.14.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S2.T2.3.14.14.3.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\">Model-free</span></td>&#13;\n<td id=\"S2.T2.3.14.14.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S2.T2.3.14.14.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S2.T2.3.14.14.4.1.1\" class=\"ltx_p\" style=\"width:321.5pt;\"><span id=\"S2.T2.3.14.14.4.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">(1) Single-stage</span><span id=\"S2.T2.3.14.14.4.1.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">: direct prediction&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.14.14.4.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Li and Chan<span id=\"S2.T2.3.14.14.4.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib87\" title=\"\" class=\"ltx_ref\">2014</a>; Pavlakos et&#160;al.<span id=\"S2.T2.3.14.14.4.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib125\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S2.T2.3.14.14.4.1.1.5.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S2.T2.3.14.14.4.1.1.6\" class=\"ltx_text\" style=\"font-size:80%;\">, body structure constraint&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.14.14.4.1.1.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Li et&#160;al.<span id=\"S2.T2.3.14.14.4.1.1.8.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib89\" title=\"\" class=\"ltx_ref\">2015b</a>; Tekin et&#160;al.<span id=\"S2.T2.3.14.14.4.1.1.8.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib159\" title=\"\" class=\"ltx_ref\">2016</a>; Sun et&#160;al.<span id=\"S2.T2.3.14.14.4.1.1.8.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib152\" title=\"\" class=\"ltx_ref\">2017</a>; Pavlakos et&#160;al.<span id=\"S2.T2.3.14.14.4.1.1.8.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib124\" title=\"\" class=\"ltx_ref\">2018a</a><span id=\"S2.T2.3.14.14.4.1.1.9.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S2.T2.3.15.15\" class=\"ltx_tr\">&#13;\n<td id=\"S2.T2.3.15.15.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">&#13;\n<span id=\"S2.T2.3.15.15.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S2.T2.3.15.15.1.1.1\" class=\"ltx_p\" style=\"width:321.5pt;\"><span id=\"S2.T2.3.15.15.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">(2) 2D-to-3D</span><span id=\"S2.T2.3.15.15.1.1.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">: </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.15.15.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Martinez et&#160;al.<span id=\"S2.T2.3.15.15.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib103\" title=\"\" class=\"ltx_ref\">2017</a>; Zhou et&#160;al.<span id=\"S2.T2.3.15.15.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib184\" title=\"\" class=\"ltx_ref\">2017</a>; Tekin et&#160;al.<span id=\"S2.T2.3.15.15.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib160\" title=\"\" class=\"ltx_ref\">2017</a>; Li and Lee<span id=\"S2.T2.3.15.15.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib85\" title=\"\" class=\"ltx_ref\">2019</a>; Qammaz and Argyros<span id=\"S2.T2.3.15.15.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib135\" title=\"\" class=\"ltx_ref\">2019</a>; Chen and Ramanan<span id=\"S2.T2.3.15.15.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">2017</a>; Moreno-Noguer<span id=\"S2.T2.3.15.15.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib113\" title=\"\" class=\"ltx_ref\">2017</a>; Wang et&#160;al.<span id=\"S2.T2.3.15.15.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib171\" title=\"\" class=\"ltx_ref\">2018a</a>; Yang et&#160;al.<span id=\"S2.T2.3.15.15.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib179\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S2.T2.3.15.15.1.1.1.5.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S2.T2.3.16.16\" class=\"ltx_tr\">&#13;\n<td id=\"S2.T2.3.16.16.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S2.T2.3.16.16.1.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-14.2pt;\">Model-based</span></td>&#13;\n<td id=\"S2.T2.3.16.16.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S2.T2.3.16.16.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S2.T2.3.16.16.2.1.1\" class=\"ltx_p\" style=\"width:321.5pt;\"><span id=\"S2.T2.3.16.16.2.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">(1) SMPL-based</span><span id=\"S2.T2.3.16.16.2.1.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">: </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.16.16.2.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Bogo et&#160;al.<span id=\"S2.T2.3.16.16.2.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">2016</a>; Tan et&#160;al.<span id=\"S2.T2.3.16.16.2.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib155\" title=\"\" class=\"ltx_ref\">2017</a>; Pavlakos et&#160;al.<span id=\"S2.T2.3.16.16.2.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib126\" title=\"\" class=\"ltx_ref\">2018b</a>; Omran et&#160;al.<span id=\"S2.T2.3.16.16.2.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib120\" title=\"\" class=\"ltx_ref\">2018</a>; Varol et&#160;al.<span id=\"S2.T2.3.16.16.2.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib167\" title=\"\" class=\"ltx_ref\">2018</a>; Kanazawa et&#160;al.<span id=\"S2.T2.3.16.16.2.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib73\" title=\"\" class=\"ltx_ref\">2018</a>; Arnab et&#160;al.<span id=\"S2.T2.3.16.16.2.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S2.T2.3.16.16.2.1.1.5.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S2.T2.3.17.17\" class=\"ltx_tr\">&#13;\n<td id=\"S2.T2.3.17.17.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">&#13;\n<span id=\"S2.T2.3.17.17.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S2.T2.3.17.17.1.1.1\" class=\"ltx_p\" style=\"width:321.5pt;\"><span id=\"S2.T2.3.17.17.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">(2) Kinematic model-based</span><span id=\"S2.T2.3.17.17.1.1.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">: </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.17.17.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Mehta et&#160;al.<span id=\"S2.T2.3.17.17.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib104\" title=\"\" class=\"ltx_ref\">2017a</a>; Nie et&#160;al.<span id=\"S2.T2.3.17.17.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib117\" title=\"\" class=\"ltx_ref\">2017</a>; Zhou et&#160;al.<span id=\"S2.T2.3.17.17.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib185\" title=\"\" class=\"ltx_ref\">2016</a>; Mehta et&#160;al.<span id=\"S2.T2.3.17.17.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib107\" title=\"\" class=\"ltx_ref\">2017c</a>; Rhodin et&#160;al.<span id=\"S2.T2.3.17.17.1.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib139\" title=\"\" class=\"ltx_ref\">2018a</a><span id=\"S2.T2.3.17.17.1.1.1.5.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S2.T2.3.18.18\" class=\"ltx_tr\">&#13;\n<td id=\"S2.T2.3.18.18.1\" class=\"ltx_td ltx_border_r\"/>&#13;\n<td id=\"S2.T2.3.18.18.2\" class=\"ltx_td ltx_border_r\"/>&#13;\n<td id=\"S2.T2.3.18.18.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">&#13;\n<span id=\"S2.T2.3.18.18.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S2.T2.3.18.18.3.1.1\" class=\"ltx_p\" style=\"width:321.5pt;\"><span id=\"S2.T2.3.18.18.3.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">(3) Other model-based</span><span id=\"S2.T2.3.18.18.3.1.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">: probabilistic model&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.18.18.3.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Tome et&#160;al.<span id=\"S2.T2.3.18.18.3.1.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib162\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S2.T2.3.18.18.3.1.1.5.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S2.T2.3.19.19\" class=\"ltx_tr\">&#13;\n<td id=\"S2.T2.3.19.19.1\" class=\"ltx_td ltx_border_b ltx_border_l ltx_border_r\"/>&#13;\n<td id=\"S2.T2.3.19.19.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S2.T2.3.19.19.2.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-2.8pt;\">3D Multiple</span></td>&#13;\n<td id=\"S2.T2.3.19.19.3\" class=\"ltx_td ltx_border_b ltx_border_r ltx_border_t\"/>&#13;\n<td id=\"S2.T2.3.19.19.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">&#13;\n<span id=\"S2.T2.3.19.19.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S2.T2.3.19.19.4.1.1\" class=\"ltx_p\" style=\"width:321.5pt;\"><span id=\"S2.T2.3.19.19.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">bottom-up&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.19.19.4.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Mehta et&#160;al.<span id=\"S2.T2.3.19.19.4.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib106\" title=\"\" class=\"ltx_ref\">2017b</a><span id=\"S2.T2.3.19.19.4.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S2.T2.3.19.19.4.1.1.5\" class=\"ltx_text\" style=\"font-size:80%;\">, top-down&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.19.19.4.1.1.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Rogez et&#160;al.<span id=\"S2.T2.3.19.19.4.1.1.7.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib141\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S2.T2.3.19.19.4.1.1.8.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S2.T2.3.19.19.4.1.1.9\" class=\"ltx_text\" style=\"font-size:80%;\">, SMPL-based&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.19.19.4.1.1.10.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Zanfir et&#160;al.<span id=\"S2.T2.3.19.19.4.1.1.11.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib181\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S2.T2.3.19.19.4.1.1.12.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S2.T2.3.19.19.4.1.1.13\" class=\"ltx_text\" style=\"font-size:80%;\">, real-time&#160;</span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S2.T2.3.19.19.4.1.1.14.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Mehta et&#160;al.<span id=\"S2.T2.3.19.19.4.1.1.15.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib105\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S2.T2.3.19.19.4.1.1.16.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "(Krizhevsky et al., 2012) Krizhevsky et al. (2012)\r\n\r\nKrizhevsky, A., Sutskever, I.,\r\nHinton, G.E., 2012.\r\n\r\n\r\nImagenet classification with deep convolutional\r\nneural networks, in: Advances in neural information\r\nprocessing systems, pp. 1097–1105.",
            "(Pfister et al., 2014) Pfister et al. (2014)\r\n\r\nPfister, T., Simonyan, K.,\r\nCharles, J., Zisserman, A.,\r\n2014.\r\n\r\n\r\nDeep convolutional neural networks for efficient pose\r\nestimation in gesture videos, in: Proc. Asian Conference\r\non Computer Vision, Springer. pp.\r\n538–552.",
            "(Luvizon et al., 2017; Nibali et al., 2018) Luvizon et al. (2017)\r\n\r\nLuvizon, D.C., Tabia, H.,\r\nPicard, D., 2017.\r\n\r\n\r\nHuman pose regression by combining indirect part\r\ndetection and contextual information.\r\n\r\n\r\narXiv preprint arXiv:1710.02322 .",
            "(Carreira et al., 2016) Carreira et al. (2016)\r\n\r\nCarreira, J., Agrawal, P.,\r\nFragkiadaki, K., Malik, J.,\r\n2016.\r\n\r\n\r\nHuman pose estimation with iterative error feedback,\r\nin: Proc. IEEE Conference on Computer Vision and Pattern\r\nRecognition, pp. 4733–4742.",
            "(Sun et al., 2017) Sun et al. (2017)\r\n\r\nSun, X., Shang, J., Liang,\r\nS., Wei, Y., 2017.\r\n\r\n\r\nCompositional human pose regression, in:\r\nProc. IEEE International Conference on Computer Vision,\r\np. 7.",
            "(Li et al., 2014) Li et al. (2014)\r\n\r\nLi, S., Liu, Z.Q., Chan,\r\nA.B., 2014.\r\n\r\n\r\nHeterogeneous multi-task learning for human pose\r\nestimation with deep convolutional neural network, in:\r\nProc. IEEE Conference on Computer Vision and Pattern\r\nRecognition Workshops, pp. 482–489.",
            "(Gkioxari et al., 2014a) Gkioxari et al. (2014a)\r\n\r\nGkioxari, G., Hariharan, B.,\r\nGirshick, R., Malik, J.,\r\n2014a.\r\n\r\n\r\nR-cnns for pose estimation and action detection.\r\n\r\n\r\narXiv preprint arXiv:1406.5212 .",
            "(Fan et al., 2015) Fan et al. (2015)\r\n\r\nFan, X., Zheng, K., Lin,\r\nY., Wang, S., 2015.\r\n\r\n\r\nCombining local appearance and holistic view:\r\nDual-source deep neural networks for human pose estimation.\r\n\r\n\r\narXiv preprint arXiv:1504.07159 .",
            "(Luvizon et al., 2018) Luvizon et al. (2018)\r\n\r\nLuvizon, D.C., Picard, D.,\r\nTabia, H., 2018.\r\n\r\n\r\n2d/3d pose estimation and action recognition using\r\nmultitask deep learning, in: Proc. IEEE Conference on\r\nComputer Vision and Pattern Recognition, pp. 5137–5146.",
            "(Jain et al., 2013; Chen and Yuille, 2014; Ramakrishna et al., 2014) Jain et al. (2013)\r\n\r\nJain, A., Tompson, J.,\r\nAndriluka, M., Taylor, G.W.,\r\nBregler, C., 2013.\r\n\r\n\r\nLearning human pose estimation features with\r\nconvolutional networks.\r\n\r\n\r\narXiv preprint arXiv:1312.7302 .",
            "(Tompson et al., 2015; Bulat and Tzimiropoulos, 2016; Xiao et al., 2018) Tompson et al. (2015)\r\n\r\nTompson, J., Goroshin, R.,\r\nJain, A., LeCun, Y.,\r\nBregler, C., 2015.\r\n\r\n\r\nEfficient object localization using convolutional\r\nnetworks, in: Proc. IEEE Conference on Computer Vision\r\nand Pattern Recognition, pp. 648–656.",
            "(Rafi et al., 2016) Rafi et al. (2016)\r\n\r\nRafi, U., Leibe, B., Gall,\r\nJ., Kostrikov, I., 2016.\r\n\r\n\r\nAn efficient convolutional network for human pose\r\nestimation, in: Proc. British Machine Vision\r\nConference, p. 2.",
            "(Papandreou et al., 2017) Papandreou et al. (2017)\r\n\r\nPapandreou, G., Zhu, T.,\r\nKanazawa, N., Toshev, A.,\r\nTompson, J., Bregler, C.,\r\nMurphy, K., 2017.\r\n\r\n\r\nTowards accurate multi-person pose estimation in the\r\nwild, in: Proc. IEEE Conference on Computer Vision and\r\nPattern Recognition, pp. 4903–4911.",
            "(Newell et al., 2016) Newell et al. (2016)\r\n\r\nNewell, A., Yang, K.,\r\nDeng, J., 2016.\r\n\r\n\r\nStacked hourglass networks for human pose\r\nestimation, in: Proc. European Conference on Computer\r\nVision, Springer. pp. 483–499.",
            "(Wei et al., 2016) Wei et al. (2016)\r\n\r\nWei, S.E., Ramakrishna, V.,\r\nKanade, T., Sheikh, Y.,\r\n2016.\r\n\r\n\r\nConvolutional pose machines, in:\r\nProc. IEEE Conference on Computer Vision and Pattern\r\nRecognition, pp. 4724–4732.",
            "(Yang et al., 2017) Yang et al. (2017)\r\n\r\nYang, W., Li, S., Ouyang,\r\nW., Li, H., Wang, X.,\r\n2017.\r\n\r\n\r\nLearning feature pyramids for human pose estimation,\r\nin: Proc. IEEE International Conference on Computer\r\nVision, pp. 1281–1290.",
            "(Belagiannis and Zisserman, 2017) Belagiannis and Zisserman (2017)\r\n\r\nBelagiannis, V., Zisserman, A.,\r\n2017.\r\n\r\n\r\nRecurrent human pose estimation, in:\r\nProc. IEEE Conference on Automatic Face and Gesture\r\nRecognition, IEEE. pp. 468–475.",
            "(Sun et al., 2019) Sun et al. (2019)\r\n\r\nSun, K., Xiao, B., Liu,\r\nD., Wang, J., 2019.\r\n\r\n\r\nDeep high-resolution representation learning for\r\nhuman pose estimation, in: Proc. IEEE Conference on\r\nComputer Vision and Pattern Recognition.",
            "(Chou et al., 2017; Chen et al., 2017; Peng et al., 2018) Chou et al. (2017)\r\n\r\nChou, C.J., Chien, J.T.,\r\nChen, H.T., 2017.\r\n\r\n\r\nSelf adversarial training for human pose estimation.\r\n\r\n\r\narXiv preprint arXiv:1707.02439 .",
            "(Tompson et al., 2014; Lifshitz et al., 2016; Yang et al., 2016; Gkioxari et al., 2016; Chu et al., 2016, 2017; Ning et al., 2018; Ke et al., 2018; Tang et al., 2018a; Tang and Wu, 2019) Tompson et al. (2014)\r\n\r\nTompson, J.J., Jain, A.,\r\nLeCun, Y., Bregler, C.,\r\n2014.\r\n\r\n\r\nJoint training of a convolutional network and a\r\ngraphical model for human pose estimation, in: Advances\r\nin neural information processing systems, pp. 1799–1807.",
            "(Jain et al., 2014; Pfister et al., 2015; Luo et al., 2018) Jain et al. (2014)\r\n\r\nJain, A., Tompson, J.,\r\nLeCun, Y., Bregler, C.,\r\n2014.\r\n\r\n\r\nModeep: A deep learning framework using motion\r\nfeatures for human pose estimation, in: Proc. Asian\r\nconference on computer vision, Springer. pp.\r\n302–315.",
            "(Tang et al., 2018b; Debnath et al., 2018; Feng et al., 2019) Tang et al. (2018b)\r\n\r\nTang, Z., Peng, X., Geng,\r\nS., Wu, L., Zhang, S.,\r\nMetaxas, D., 2018b.\r\n\r\n\r\nQuantized densely connected u-nets for efficient\r\nlandmark localization, in: Proc. European Conference on\r\nComputer Vision, pp. 339–354.",
            "(Iqbal and Gall, 2016; Huang et al., 2017) Iqbal and Gall (2016)\r\n\r\nIqbal, U., Gall, J., 2016.\r\n\r\n\r\nMulti-person pose estimation with local\r\njoint-to-person associations, in: Proc. European\r\nConference on Computer Vision, Springer. pp.\r\n627–642.",
            "(Fang et al., 2017) Fang et al. (2017)\r\n\r\nFang, H., Xie, S., Tai,\r\nY.W., Lu, C., 2017.\r\n\r\n\r\nRmpe: Regional multi-person pose estimation, in:\r\nProc. IEEE International Conference on Computer Vision,\r\npp. 2334–2343.",
            "(Xiao et al., 2018; Chen et al., 2018) Xiao et al. (2018)\r\n\r\nXiao, B., Wu, H., Wei,\r\nY., 2018.\r\n\r\n\r\nSimple baselines for human pose estimation and\r\ntracking, in: Proc. European Conference on Computer\r\nVision, pp. 466–481.",
            "(Moon et al., 2019) Moon et al. (2019)\r\n\r\nMoon, G., Chang, J.Y.,\r\nLee, K.M., 2019.\r\n\r\n\r\nPosefix: Model-agnostic general human pose refinement\r\nnetwork, in: Proc. IEEE Conference on Computer Vision\r\nand Pattern Recognition, pp. 7773–7781.",
            "(Pishchulin et al., 2016) Pishchulin et al. (2016)\r\n\r\nPishchulin, L., Insafutdinov, E.,\r\nTang, S., Andres, B.,\r\nAndriluka, M., Gehler, P.V.,\r\nSchiele, B., 2016.\r\n\r\n\r\nDeepcut: Joint subset partition and labeling for\r\nmulti person pose estimation, in: Proc. IEEE Conference\r\non Computer Vision and Pattern Recognition, pp. 4929–4937.",
            "(Insafutdinov et al., 2016) Insafutdinov et al. (2016)\r\n\r\nInsafutdinov, E., Pishchulin, L.,\r\nAndres, B., Andriluka, M.,\r\nSchiele, B., 2016.\r\n\r\n\r\nDeepercut: A deeper, stronger, and faster\r\nmulti-person pose estimation model, in: Proc. European\r\nConference on Computer Vision, Springer. pp.\r\n34–50.",
            "(Cao et al., 2016) Cao et al. (2016)\r\n\r\nCao, Z., Simon, T., Wei,\r\nS.E., Sheikh, Y., 2016.\r\n\r\n\r\nRealtime multi-person 2d pose estimation using part\r\naffinity fields.\r\n\r\n\r\narXiv preprint arXiv:1611.08050 .",
            "(Nie et al., 2018) Nie et al. (2018)\r\n\r\nNie, X., Feng, J., Xing,\r\nJ., Yan, S., 2018.\r\n\r\n\r\nPose partition networks for multi-person pose\r\nestimation, in: Proc. European Conference on Computer\r\nVision, pp. 684–699.",
            "(Kreiss et al., 2019) Kreiss et al. (2019)\r\n\r\nKreiss, S., Bertoni, L.,\r\nAlahi, A., 2019.\r\n\r\n\r\nPifpaf: Composite fields for human pose estimation,\r\nin: Proc. IEEE Conference on Computer Vision and Pattern\r\nRecognition, pp. 11977–11986.",
            "(Newell et al., 2017) Newell et al. (2017)\r\n\r\nNewell, A., Huang, Z.,\r\nDeng, J., 2017.\r\n\r\n\r\nAssociative embedding: End-to-end learning for joint\r\ndetection and grouping, in: Advances in Neural\r\nInformation Processing Systems, pp. 2277–2287.",
            "(Papandreou et al., 2018) Papandreou et al. (2018)\r\n\r\nPapandreou, G., Zhu, T.,\r\nChen, L.C., Gidaris, S.,\r\nTompson, J., Murphy, K.,\r\n2018.\r\n\r\n\r\nPersonlab: Person pose estimation and instance\r\nsegmentation with a bottom-up, part-based, geometric embedding model.\r\n\r\n\r\narXiv preprint arXiv:1803.08225 .",
            "(Kocabas et al., 2018) Kocabas et al. (2018)\r\n\r\nKocabas, M., Karagoz, S.,\r\nAkbas, E., 2018.\r\n\r\n\r\nMultiposenet: Fast multi-person pose estimation using\r\npose residual network, in: Proc. European Conference on\r\nComputer Vision, Springer. pp.\r\n437–453.",
            "(Li and Chan, 2014; Pavlakos et al., 2017) Li and Chan (2014)\r\n\r\nLi, S., Chan, A.B., 2014.\r\n\r\n\r\n3d human pose estimation from monocular images with\r\ndeep convolutional neural network, in: Proc. Asian\r\nConference on Computer Vision, Springer. pp.\r\n332–347.",
            "(Li et al., 2015b; Tekin et al., 2016; Sun et al., 2017; Pavlakos et al., 2018a) Li et al. (2015b)\r\n\r\nLi, S., Zhang, W., Chan,\r\nA.B., 2015b.\r\n\r\n\r\nMaximum-margin structured learning with deep networks\r\nfor 3d human pose estimation, in: Proc. IEEE\r\nInternational Conference on Computer Vision, pp.\r\n2848–2856.",
            "(Martinez et al., 2017; Zhou et al., 2017; Tekin et al., 2017; Li and Lee, 2019; Qammaz and Argyros, 2019; Chen and Ramanan, 2017; Moreno-Noguer, 2017; Wang et al., 2018a; Yang et al., 2018) Martinez et al. (2017)\r\n\r\nMartinez, J., Hossain, R.,\r\nRomero, J., Little, J.J.,\r\n2017.\r\n\r\n\r\nA simple yet effective baseline for 3d human pose\r\nestimation, in: Proc. IEEE International Conference on\r\nComputer Vision, pp. 2640–2649.",
            "(Bogo et al., 2016; Tan et al., 2017; Pavlakos et al., 2018b; Omran et al., 2018; Varol et al., 2018; Kanazawa et al., 2018; Arnab et al., 2019) Bogo et al. (2016)\r\n\r\nBogo, F., Kanazawa, A.,\r\nLassner, C., Gehler, P.,\r\nRomero, J., Black, M.J.,\r\n2016.\r\n\r\n\r\nKeep it smpl: Automatic estimation of 3d human pose\r\nand shape from a single image, in: Proc. European\r\nConference on Computer Vision, Springer. pp.\r\n561–578.",
            "(Mehta et al., 2017a; Nie et al., 2017; Zhou et al., 2016; Mehta et al., 2017c; Rhodin et al., 2018a) Mehta et al. (2017a)\r\n\r\nMehta, D., Rhodin, H.,\r\nCasas, D., Fua, P.,\r\nSotnychenko, O., Xu, W.,\r\nTheobalt, C., 2017a.\r\n\r\n\r\nMonocular 3d human pose estimation in the wild using\r\nimproved cnn supervision, in: Proc. IEEE International\r\nConference on 3D Vision, IEEE. pp.\r\n506–516.",
            "(Tome et al., 2017) Tome et al. (2017)\r\n\r\nTome, D., Russell, C.,\r\nAgapito, L., 2017.\r\n\r\n\r\nLifting from the deep: Convolutional 3d pose\r\nestimation from a single image, pp. 2500–2509.",
            "(Mehta et al., 2017b) Mehta et al. (2017b)\r\n\r\nMehta, D., Sotnychenko, O.,\r\nMueller, F., Xu, W.,\r\nSridhar, S., Pons-Moll, G.,\r\nTheobalt, C., 2017b.\r\n\r\n\r\nSingle-shot multi-person 3d body pose estimation from\r\nmonocular rgb input.\r\n\r\n\r\narXiv preprint arXiv:1712.03453 .",
            "(Rogez et al., 2017) Rogez et al. (2017)\r\n\r\nRogez, G., Weinzaepfel, P.,\r\nSchmid, C., 2017.\r\n\r\n\r\nLcr-net: Localization-classification-regression for\r\nhuman pose, in: Proc. IEEE Conference on Computer Vision\r\nand Pattern Recognition, pp. 3433–3441.",
            "(Zanfir et al., 2018) Zanfir et al. (2018)\r\n\r\nZanfir, A., Marinoiu, E.,\r\nSminchisescu, C., 2018.\r\n\r\n\r\nMonocular 3d pose and shape estimation of multiple\r\npeople in natural scenes-the importance of multiple scene constraints, in:\r\nProc. IEEE Conference on Computer Vision and Pattern\r\nRecognition, pp. 2148–2157.",
            "(Mehta et al., 2019) Mehta et al. (2019)\r\n\r\nMehta, D., Sotnychenko, O.,\r\nMueller, F., Xu, W.,\r\nElgharib, M., Fua, P.,\r\nSeidel, H.P., Rhodin, H.,\r\nPons-Moll, G., Theobalt, C.,\r\n2019.\r\n\r\n\r\nXnect: Real-time multi-person 3d human pose\r\nestimation with a single rgb camera.\r\n\r\n\r\narXiv:1907.00837 ."
        ],
        "references": [
            "The papers of human pose estimation can be categorized in different ways. Based on whether to use designed human body models or not, the methods can be categorized into generative methods (model-based) and discriminative methods (model-free). According to from which level (high-level abstraction or low-level pixel evidence) to start the processing, they can be classified into top-down methods and bottom-up methods. More details of different category strategies for HPE approaches are summarized in Table 2 and described in Section 2.1.",
            "This survey reviews the recent work in two main sections: 2D human pose estimation (Section 3) and 3D human pose estimation (Section 4). For each section, we further divide them into subsections based on their respective characteristics (see a summary of all the categories and the corresponding papers in Table 2.)"
        ]
    },
    "id_table_3": {
        "caption": "Table 3: Summary of 2D single person pose estimation methods. Note that the last column shows the PCKh@0.5 scores on the Max Planck Institute for Informatics (MPII) Human Pose testing set.",
        "table": [
            "<table id=\"S3.T3.16\" class=\"ltx_tabular ltx_centering ltx_align_middle\">&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S3.T3.16.17.1\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T3.16.17.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S3.T3.16.17.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Methods</span></td>&#13;\n<td id=\"S3.T3.16.17.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.16.17.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Backbone</span></td>&#13;\n<td id=\"S3.T3.16.17.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.16.17.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Input size</span></td>&#13;\n<td id=\"S3.T3.16.17.1.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.16.17.1.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T3.16.17.1.4.1.1\" class=\"ltx_p\" style=\"width:256.1pt;\"><span id=\"S3.T3.16.17.1.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Highlights</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.16.17.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.16.17.1.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">PCKh (%)</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T3.16.18.2\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T3.16.18.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" colspan=\"5\"><span id=\"S3.T3.16.18.2.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Regression-based</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T3.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T3.1.1.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T3.1.1.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Toshev and Szegedy<span id=\"S3.T3.1.1.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib165\" title=\"\" class=\"ltx_ref\">2014</a><span id=\"S3.T3.1.1.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T3.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<table id=\"S3.T3.1.1.3.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S3.T3.1.1.3.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T3.1.1.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S3.T3.1.1.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">AlexNet</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n</td>&#13;\n<td id=\"S3.T3.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">220</span><math id=\"S3.T3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S3.T3.1.1.1.m1.1a\"><mo mathsize=\"80%\" id=\"S3.T3.1.1.1.m1.1.1\" xref=\"S3.T3.1.1.1.m1.1.1.cmml\">&#215;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T3.1.1.1.m1.1b\"><times id=\"S3.T3.1.1.1.m1.1.1.cmml\" xref=\"S3.T3.1.1.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T3.1.1.1.m1.1c\">\\times</annotation></semantics></math><span id=\"S3.T3.1.1.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">220</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.1.1.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.1.1.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T3.1.1.4.1.1\" class=\"ltx_p\" style=\"width:256.1pt;\"><span id=\"S3.T3.1.1.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Direct regression, multi-stage refinement</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.1.1.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T3.2.2\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T3.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T3.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Carreira et&#160;al.<span id=\"S3.T3.2.2.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib14\" title=\"\" class=\"ltx_ref\">2016</a><span id=\"S3.T3.2.2.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T3.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.2.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">GoogleNet</span></td>&#13;\n<td id=\"S3.T3.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">224</span><math id=\"S3.T3.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S3.T3.2.2.1.m1.1a\"><mo mathsize=\"80%\" id=\"S3.T3.2.2.1.m1.1.1\" xref=\"S3.T3.2.2.1.m1.1.1.cmml\">&#215;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T3.2.2.1.m1.1b\"><times id=\"S3.T3.2.2.1.m1.1.1.cmml\" xref=\"S3.T3.2.2.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T3.2.2.1.m1.1c\">\\times</annotation></semantics></math><span id=\"S3.T3.2.2.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">224</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.2.2.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.2.2.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T3.2.2.4.1.1\" class=\"ltx_p\" style=\"width:256.1pt;\"><span id=\"S3.T3.2.2.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Iterative error feedback refinement from initial pose.</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.2.2.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">81.3</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T3.3.3\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T3.3.3.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T3.3.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Sun et&#160;al.<span id=\"S3.T3.3.3.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib152\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S3.T3.3.3.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T3.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.3.3.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">ResNet-50</span></td>&#13;\n<td id=\"S3.T3.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.3.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">224</span><math id=\"S3.T3.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S3.T3.3.3.1.m1.1a\"><mo mathsize=\"80%\" id=\"S3.T3.3.3.1.m1.1.1\" xref=\"S3.T3.3.3.1.m1.1.1.cmml\">&#215;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T3.3.3.1.m1.1b\"><times id=\"S3.T3.3.3.1.m1.1.1.cmml\" xref=\"S3.T3.3.3.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T3.3.3.1.m1.1c\">\\times</annotation></semantics></math><span id=\"S3.T3.3.3.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">224</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.3.3.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.3.3.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T3.3.3.4.1.1\" class=\"ltx_p\" style=\"width:256.1pt;\"><span id=\"S3.T3.3.3.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Bone based representation as additional constraint, general for both 2D/3D HPE</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.3.3.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">86.4</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T3.4.4\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T3.4.4.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T3.4.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Luvizon et&#160;al.<span id=\"S3.T3.4.4.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib99\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S3.T3.4.4.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T3.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<table id=\"S3.T3.4.4.3.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S3.T3.4.4.3.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T3.4.4.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S3.T3.4.4.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Inception-v4+</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T3.4.4.3.1.2\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T3.4.4.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S3.T3.4.4.3.1.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Hourglass</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n</td>&#13;\n<td id=\"S3.T3.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.4.4.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">256</span><math id=\"S3.T3.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S3.T3.4.4.1.m1.1a\"><mo mathsize=\"80%\" id=\"S3.T3.4.4.1.m1.1.1\" xref=\"S3.T3.4.4.1.m1.1.1.cmml\">&#215;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T3.4.4.1.m1.1b\"><times id=\"S3.T3.4.4.1.m1.1.1.cmml\" xref=\"S3.T3.4.4.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T3.4.4.1.m1.1c\">\\times</annotation></semantics></math><span id=\"S3.T3.4.4.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">256</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.4.4.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.4.4.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T3.4.4.4.1.1\" class=\"ltx_p\" style=\"width:256.1pt;\"><span id=\"S3.T3.4.4.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Multi-stage architecture, proposed soft-argmax function to convert heatmaps into joint locations</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.4.4.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">91.2</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T3.16.19.3\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T3.16.19.3.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" colspan=\"5\"><span id=\"S3.T3.16.19.3.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Detection-based</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T3.5.5\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T3.5.5.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T3.5.5.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Tompson et&#160;al.<span id=\"S3.T3.5.5.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib164\" title=\"\" class=\"ltx_ref\">2014</a><span id=\"S3.T3.5.5.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T3.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.5.5.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">AlexNet</span></td>&#13;\n<td id=\"S3.T3.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.5.5.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">320</span><math id=\"S3.T3.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S3.T3.5.5.1.m1.1a\"><mo mathsize=\"80%\" id=\"S3.T3.5.5.1.m1.1.1\" xref=\"S3.T3.5.5.1.m1.1.1.cmml\">&#215;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T3.5.5.1.m1.1b\"><times id=\"S3.T3.5.5.1.m1.1.1.cmml\" xref=\"S3.T3.5.5.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T3.5.5.1.m1.1c\">\\times</annotation></semantics></math><span id=\"S3.T3.5.5.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">240</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.5.5.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.5.5.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T3.5.5.4.1.1\" class=\"ltx_p\" style=\"width:256.1pt;\"><span id=\"S3.T3.5.5.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Heatmap representation, multi-scale input, MRF-like Spatial-Model</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.5.5.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">79.6</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T3.6.6\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T3.6.6.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T3.6.6.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Yang et&#160;al.<span id=\"S3.T3.6.6.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib178\" title=\"\" class=\"ltx_ref\">2016</a><span id=\"S3.T3.6.6.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T3.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.6.6.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">VGG</span></td>&#13;\n<td id=\"S3.T3.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.6.6.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">112</span><math id=\"S3.T3.6.6.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S3.T3.6.6.1.m1.1a\"><mo mathsize=\"80%\" id=\"S3.T3.6.6.1.m1.1.1\" xref=\"S3.T3.6.6.1.m1.1.1.cmml\">&#215;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T3.6.6.1.m1.1b\"><times id=\"S3.T3.6.6.1.m1.1.1.cmml\" xref=\"S3.T3.6.6.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T3.6.6.1.m1.1c\">\\times</annotation></semantics></math><span id=\"S3.T3.6.6.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">112</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.6.6.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.6.6.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T3.6.6.4.1.1\" class=\"ltx_p\" style=\"width:256.1pt;\"><span id=\"S3.T3.6.6.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Jointly learning DCNNs with deformable mixture of parts models</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.6.6.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.6.6.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T3.7.7\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T3.7.7.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T3.7.7.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Newell et&#160;al.<span id=\"S3.T3.7.7.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib115\" title=\"\" class=\"ltx_ref\">2016</a><span id=\"S3.T3.7.7.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T3.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.7.7.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Hourglass</span></td>&#13;\n<td id=\"S3.T3.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.7.7.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">256</span><math id=\"S3.T3.7.7.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S3.T3.7.7.1.m1.1a\"><mo mathsize=\"80%\" id=\"S3.T3.7.7.1.m1.1.1\" xref=\"S3.T3.7.7.1.m1.1.1.cmml\">&#215;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T3.7.7.1.m1.1b\"><times id=\"S3.T3.7.7.1.m1.1.1.cmml\" xref=\"S3.T3.7.7.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T3.7.7.1.m1.1c\">\\times</annotation></semantics></math><span id=\"S3.T3.7.7.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">256</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.7.7.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.7.7.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T3.7.7.4.1.1\" class=\"ltx_p\" style=\"width:256.1pt;\"><span id=\"S3.T3.7.7.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Proposed stacked Hourglass architecture with intermediate supervision.</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.7.7.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">90.9</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T3.8.8\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T3.8.8.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T3.8.8.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Wei et&#160;al.<span id=\"S3.T3.8.8.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib174\" title=\"\" class=\"ltx_ref\">2016</a><span id=\"S3.T3.8.8.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T3.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.8.8.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">CPM</span></td>&#13;\n<td id=\"S3.T3.8.8.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.8.8.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">368</span><math id=\"S3.T3.8.8.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S3.T3.8.8.1.m1.1a\"><mo mathsize=\"80%\" id=\"S3.T3.8.8.1.m1.1.1\" xref=\"S3.T3.8.8.1.m1.1.1.cmml\">&#215;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T3.8.8.1.m1.1b\"><times id=\"S3.T3.8.8.1.m1.1.1.cmml\" xref=\"S3.T3.8.8.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T3.8.8.1.m1.1c\">\\times</annotation></semantics></math><span id=\"S3.T3.8.8.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">368</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.8.8.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.8.8.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T3.8.8.4.1.1\" class=\"ltx_p\" style=\"width:256.1pt;\"><span id=\"S3.T3.8.8.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Proposed Convolutional Pose Machines (CPM) with intermediate input and supervision, learn spatial correlations among body parts</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.8.8.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">88.5</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T3.9.9\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T3.9.9.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T3.9.9.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Chu et&#160;al.<span id=\"S3.T3.9.9.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S3.T3.9.9.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T3.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.9.9.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Hourglass</span></td>&#13;\n<td id=\"S3.T3.9.9.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.9.9.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">256</span><math id=\"S3.T3.9.9.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S3.T3.9.9.1.m1.1a\"><mo mathsize=\"80%\" id=\"S3.T3.9.9.1.m1.1.1\" xref=\"S3.T3.9.9.1.m1.1.1.cmml\">&#215;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T3.9.9.1.m1.1b\"><times id=\"S3.T3.9.9.1.m1.1.1.cmml\" xref=\"S3.T3.9.9.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T3.9.9.1.m1.1c\">\\times</annotation></semantics></math><span id=\"S3.T3.9.9.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">256</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.9.9.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.9.9.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T3.9.9.4.1.1\" class=\"ltx_p\" style=\"width:256.1pt;\"><span id=\"S3.T3.9.9.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Multi-resolution attention maps from multi-scale features, proposed micro hourglass residual units to increase the receptive field</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.9.9.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.9.9.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">91.5</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T3.10.10\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T3.10.10.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T3.10.10.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Yang et&#160;al.<span id=\"S3.T3.10.10.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib177\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S3.T3.10.10.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T3.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.10.10.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Hourglass</span></td>&#13;\n<td id=\"S3.T3.10.10.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.10.10.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">256</span><math id=\"S3.T3.10.10.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S3.T3.10.10.1.m1.1a\"><mo mathsize=\"80%\" id=\"S3.T3.10.10.1.m1.1.1\" xref=\"S3.T3.10.10.1.m1.1.1.cmml\">&#215;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T3.10.10.1.m1.1b\"><times id=\"S3.T3.10.10.1.m1.1.1.cmml\" xref=\"S3.T3.10.10.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T3.10.10.1.m1.1c\">\\times</annotation></semantics></math><span id=\"S3.T3.10.10.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">256</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.10.10.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.10.10.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T3.10.10.4.1.1\" class=\"ltx_p\" style=\"width:256.1pt;\"><span id=\"S3.T3.10.10.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Proposed Pyramid Residual Module (PRM) learns filters for input features with different resolutions</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.10.10.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.10.10.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">92.0</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T3.11.11\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T3.11.11.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T3.11.11.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Chen et&#160;al.<span id=\"S3.T3.11.11.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S3.T3.11.11.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T3.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.11.11.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">conv-deconv</span></td>&#13;\n<td id=\"S3.T3.11.11.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.11.11.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">256</span><math id=\"S3.T3.11.11.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S3.T3.11.11.1.m1.1a\"><mo mathsize=\"80%\" id=\"S3.T3.11.11.1.m1.1.1\" xref=\"S3.T3.11.11.1.m1.1.1.cmml\">&#215;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T3.11.11.1.m1.1b\"><times id=\"S3.T3.11.11.1.m1.1.1.cmml\" xref=\"S3.T3.11.11.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T3.11.11.1.m1.1c\">\\times</annotation></semantics></math><span id=\"S3.T3.11.11.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">256</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.11.11.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.11.11.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T3.11.11.4.1.1\" class=\"ltx_p\" style=\"width:256.1pt;\"><span id=\"S3.T3.11.11.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">GAN, stacked conv-deconv architecture, multi-task for pose and occlusion, two discriminators for distinguishing whether the pose is &#8217;real&#8217; and the confidence is strong</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.11.11.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.11.11.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">91.9</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T3.12.12\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T3.12.12.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T3.12.12.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Peng et&#160;al.<span id=\"S3.T3.12.12.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib127\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S3.T3.12.12.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T3.12.12.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.12.12.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Hourglass</span></td>&#13;\n<td id=\"S3.T3.12.12.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.12.12.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">256</span><math id=\"S3.T3.12.12.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S3.T3.12.12.1.m1.1a\"><mo mathsize=\"80%\" id=\"S3.T3.12.12.1.m1.1.1\" xref=\"S3.T3.12.12.1.m1.1.1.cmml\">&#215;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T3.12.12.1.m1.1b\"><times id=\"S3.T3.12.12.1.m1.1.1.cmml\" xref=\"S3.T3.12.12.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T3.12.12.1.m1.1c\">\\times</annotation></semantics></math><span id=\"S3.T3.12.12.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">256</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.12.12.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.12.12.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T3.12.12.4.1.1\" class=\"ltx_p\" style=\"width:256.1pt;\"><span id=\"S3.T3.12.12.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">GAN, proposed augmentation network to generate data augmentations without looking for more data</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.12.12.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.12.12.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">91.5</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T3.13.13\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T3.13.13.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T3.13.13.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Ke et&#160;al.<span id=\"S3.T3.13.13.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib74\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S3.T3.13.13.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T3.13.13.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.13.13.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Hourglass</span></td>&#13;\n<td id=\"S3.T3.13.13.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.13.13.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">256</span><math id=\"S3.T3.13.13.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S3.T3.13.13.1.m1.1a\"><mo mathsize=\"80%\" id=\"S3.T3.13.13.1.m1.1.1\" xref=\"S3.T3.13.13.1.m1.1.1.cmml\">&#215;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T3.13.13.1.m1.1b\"><times id=\"S3.T3.13.13.1.m1.1.1.cmml\" xref=\"S3.T3.13.13.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T3.13.13.1.m1.1c\">\\times</annotation></semantics></math><span id=\"S3.T3.13.13.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">256</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.13.13.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.13.13.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T3.13.13.4.1.1\" class=\"ltx_p\" style=\"width:256.1pt;\"><span id=\"S3.T3.13.13.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Improved Hourglass network with multi-scale intermediate supervision, multi-scale feature combination, structure-aware loss and data augmentation of joints masking</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.13.13.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.13.13.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">92.1</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T3.14.14\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T3.14.14.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T3.14.14.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Tang et&#160;al.<span id=\"S3.T3.14.14.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib157\" title=\"\" class=\"ltx_ref\">2018a</a><span id=\"S3.T3.14.14.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T3.14.14.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.14.14.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Hourglass</span></td>&#13;\n<td id=\"S3.T3.14.14.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.14.14.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">256</span><math id=\"S3.T3.14.14.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S3.T3.14.14.1.m1.1a\"><mo mathsize=\"80%\" id=\"S3.T3.14.14.1.m1.1.1\" xref=\"S3.T3.14.14.1.m1.1.1.cmml\">&#215;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T3.14.14.1.m1.1b\"><times id=\"S3.T3.14.14.1.m1.1.1.cmml\" xref=\"S3.T3.14.14.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T3.14.14.1.m1.1c\">\\times</annotation></semantics></math><span id=\"S3.T3.14.14.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">256</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.14.14.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.14.14.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T3.14.14.4.1.1\" class=\"ltx_p\" style=\"width:256.1pt;\"><span id=\"S3.T3.14.14.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Compositional model, hierarchical representation of body parts for intermediate supervision</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.14.14.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.14.14.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">92.3</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T3.15.15\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T3.15.15.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T3.15.15.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Sun et&#160;al.<span id=\"S3.T3.15.15.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib151\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S3.T3.15.15.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T3.15.15.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.15.15.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">HRNet</span></td>&#13;\n<td id=\"S3.T3.15.15.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.15.15.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">256</span><math id=\"S3.T3.15.15.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S3.T3.15.15.1.m1.1a\"><mo mathsize=\"80%\" id=\"S3.T3.15.15.1.m1.1.1\" xref=\"S3.T3.15.15.1.m1.1.1.cmml\">&#215;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T3.15.15.1.m1.1b\"><times id=\"S3.T3.15.15.1.m1.1.1.cmml\" xref=\"S3.T3.15.15.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T3.15.15.1.m1.1c\">\\times</annotation></semantics></math><span id=\"S3.T3.15.15.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">256</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.15.15.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.15.15.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T3.15.15.4.1.1\" class=\"ltx_p\" style=\"width:256.1pt;\"><span id=\"S3.T3.15.15.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">high-resolution representations of features across the whole network, multi-scale fusion.</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.15.15.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T3.15.15.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">92.3</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T3.16.16\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T3.16.16.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T3.16.16.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Tang and Wu<span id=\"S3.T3.16.16.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib156\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S3.T3.16.16.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T3.16.16.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S3.T3.16.16.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Hourglass</span></td>&#13;\n<td id=\"S3.T3.16.16.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.16.16.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">256</span><math id=\"S3.T3.16.16.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S3.T3.16.16.1.m1.1a\"><mo mathsize=\"80%\" id=\"S3.T3.16.16.1.m1.1.1\" xref=\"S3.T3.16.16.1.m1.1.1.cmml\">&#215;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T3.16.16.1.m1.1b\"><times id=\"S3.T3.16.16.1.m1.1.1.cmml\" xref=\"S3.T3.16.16.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T3.16.16.1.m1.1c\">\\times</annotation></semantics></math><span id=\"S3.T3.16.16.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">256</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.16.16.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T3.16.16.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T3.16.16.4.1.1\" class=\"ltx_p\" style=\"width:256.1pt;\"><span id=\"S3.T3.16.16.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">data-driven joint grouping, proposed part-based branching network (PBN) to learn representations specific to each part group.</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T3.16.16.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S3.T3.16.16.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">92.7</span></td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n",
            "<table id=\"S3.T3.1.1.3.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S3.T3.1.1.3.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T3.1.1.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S3.T3.1.1.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">AlexNet</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n\n",
            "<table id=\"S3.T3.4.4.3.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S3.T3.4.4.3.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T3.4.4.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S3.T3.4.4.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Inception-v4+</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T3.4.4.3.1.2\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T3.4.4.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S3.T3.4.4.3.1.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Hourglass</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "(Toshev and Szegedy, 2014) Toshev and Szegedy (2014)\r\n\r\nToshev, A., Szegedy, C.,\r\n2014.\r\n\r\n\r\nDeeppose: Human pose estimation via deep neural\r\nnetworks, in: Proc. IEEE Conference on Computer Vision\r\nand Pattern Recognition, pp. 1653–1660.",
            "(Carreira et al., 2016) Carreira et al. (2016)\r\n\r\nCarreira, J., Agrawal, P.,\r\nFragkiadaki, K., Malik, J.,\r\n2016.\r\n\r\n\r\nHuman pose estimation with iterative error feedback,\r\nin: Proc. IEEE Conference on Computer Vision and Pattern\r\nRecognition, pp. 4733–4742.",
            "(Sun et al., 2017) Sun et al. (2017)\r\n\r\nSun, X., Shang, J., Liang,\r\nS., Wei, Y., 2017.\r\n\r\n\r\nCompositional human pose regression, in:\r\nProc. IEEE International Conference on Computer Vision,\r\np. 7.",
            "(Luvizon et al., 2017) Luvizon et al. (2017)\r\n\r\nLuvizon, D.C., Tabia, H.,\r\nPicard, D., 2017.\r\n\r\n\r\nHuman pose regression by combining indirect part\r\ndetection and contextual information.\r\n\r\n\r\narXiv preprint arXiv:1710.02322 .",
            "(Tompson et al., 2014) Tompson et al. (2014)\r\n\r\nTompson, J.J., Jain, A.,\r\nLeCun, Y., Bregler, C.,\r\n2014.\r\n\r\n\r\nJoint training of a convolutional network and a\r\ngraphical model for human pose estimation, in: Advances\r\nin neural information processing systems, pp. 1799–1807.",
            "(Yang et al., 2016) Yang et al. (2016)\r\n\r\nYang, W., Ouyang, W., Li,\r\nH., Wang, X., 2016.\r\n\r\n\r\nEnd-to-end learning of deformable mixture of parts\r\nand deep convolutional neural networks for human pose estimation, in:\r\nProc. IEEE Conference on Computer Vision and Pattern\r\nRecognition, pp. 3073–3082.",
            "(Newell et al., 2016) Newell et al. (2016)\r\n\r\nNewell, A., Yang, K.,\r\nDeng, J., 2016.\r\n\r\n\r\nStacked hourglass networks for human pose\r\nestimation, in: Proc. European Conference on Computer\r\nVision, Springer. pp. 483–499.",
            "(Wei et al., 2016) Wei et al. (2016)\r\n\r\nWei, S.E., Ramakrishna, V.,\r\nKanade, T., Sheikh, Y.,\r\n2016.\r\n\r\n\r\nConvolutional pose machines, in:\r\nProc. IEEE Conference on Computer Vision and Pattern\r\nRecognition, pp. 4724–4732.",
            "(Chu et al., 2017) Chu et al. (2017)\r\n\r\nChu, X., Yang, W., Ouyang,\r\nW., Ma, C., Yuille, A.L.,\r\nWang, X., 2017.\r\n\r\n\r\nMulti-context attention for human pose estimation.\r\n\r\n\r\narXiv preprint arXiv:1702.07432\r\n1.",
            "(Yang et al., 2017) Yang et al. (2017)\r\n\r\nYang, W., Li, S., Ouyang,\r\nW., Li, H., Wang, X.,\r\n2017.\r\n\r\n\r\nLearning feature pyramids for human pose estimation,\r\nin: Proc. IEEE International Conference on Computer\r\nVision, pp. 1281–1290.",
            "(Chen et al., 2017) Chen et al. (2017)\r\n\r\nChen, Y., Shen, C., Wei,\r\nX.S., Liu, L., Yang, J.,\r\n2017.\r\n\r\n\r\nAdversarial posenet: A structure-aware convolutional\r\nnetwork for human pose estimation.\r\n\r\n\r\nCoRR, abs/1705.00389 2.",
            "(Peng et al., 2018) Peng et al. (2018)\r\n\r\nPeng, X., Tang, Z., Yang,\r\nF., Feris, R.S., Metaxas, D.,\r\n2018.\r\n\r\n\r\nJointly optimize data augmentation and network\r\ntraining: Adversarial data augmentation in human pose estimation, in:\r\nProc. IEEE Conference on Computer Vision and Pattern\r\nRecognition, pp. 2226–2234.",
            "(Ke et al., 2018) Ke et al. (2018)\r\n\r\nKe, L., Chang, M.C., Qi,\r\nH., Lyu, S., 2018.\r\n\r\n\r\nMulti-scale structure-aware network for human pose\r\nestimation.\r\n\r\n\r\narXiv preprint arXiv:1803.09894 .",
            "(Tang et al., 2018a) Tang et al. (2018a)\r\n\r\nTang, W., Yu, P., Wu, Y.,\r\n2018a.\r\n\r\n\r\nDeeply learned compositional models for human pose\r\nestimation, in: Proc. European Conference on Computer\r\nVision, pp. 190–206.",
            "(Sun et al., 2019) Sun et al. (2019)\r\n\r\nSun, K., Xiao, B., Liu,\r\nD., Wang, J., 2019.\r\n\r\n\r\nDeep high-resolution representation learning for\r\nhuman pose estimation, in: Proc. IEEE Conference on\r\nComputer Vision and Pattern Recognition.",
            "(Tang and Wu, 2019) Tang and Wu (2019)\r\n\r\nTang, W., Wu, Y., 2019.\r\n\r\n\r\nDoes learning specific features for related parts\r\nhelp human pose estimation?, in: Proc. IEEE Conference\r\non Computer Vision and Pattern Recognition, pp. 1107–1116."
        ],
        "references": [
            "Based on the different formulations of human pose estimation task, the proposed methods using CNNs can be classified into two categories: regression-based methods and detection-based methods. Regression-based methods attempt to learn a mapping from image to kinematic body joint coordinates by an end-to-end framework and generally directly produce joint coordinates (Toshev and Szegedy, 2014). Detection-based methods are intended to predict approximate locations of body parts (Chen and Yuille, 2014) or joints (Newell et al., 2016), usually are supervised by a sequence of rectangular windows (each including a specific body part) (Jain et al., 2013; Chen and Yuille, 2014) or heatmaps (each indicating one joint position by a 2D Gaussian distribution centered at the joint location) (Newell et al., 2016; Wei et al., 2016). Each of these two kinds of methods has its advantages and disadvantages. Direct regression learning of only one single point is a difficulty since it is a highly nonlinear problem and lacks robustness, while heatmap learning is supervised by dense pixel information which results in better robustness. Compared to the original image size, heatmap representation has much lower resolution due to the pooling operation in CNNs, which limits the accuracy of joint coordinate estimation. And obtaining joint coordinates from heatmap is normally a non-differentiable process that blocks the network to be trained end-to-end. The recent representative work for 2D single person pose estimation are summarized in Table 3, the last column is the comparisons of PCKh@0.5 scores on the MPII testing set. More details of datasets and evaluation metrics are described in Section 5."
        ]
    },
    "id_table_4": {
        "caption": "Table 4: Comparison of 2D multi-person pose estimation methods. Note that the last column shows the Average Precision (AP) scores on the COCO test-dev set. The results with * were obtained with COCO16 training set, while others with COCO17 training set.",
        "table": [
            "<table id=\"S3.T4.5\" class=\"ltx_tabular ltx_centering ltx_align_middle\">&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S3.T4.5.6.1\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T4.5.6.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S3.T4.5.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Methods</span></td>&#13;\n<td id=\"S3.T4.5.6.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.5.6.1.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.5.6.1.2.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S3.T4.5.6.1.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Network type</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.5.6.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.5.6.1.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.5.6.1.3.1.1\" class=\"ltx_p\" style=\"width:298.8pt;\"><span id=\"S3.T4.5.6.1.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Highlights</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.5.6.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T4.5.6.1.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">AP Score (%)</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T4.5.7.2\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T4.5.7.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" colspan=\"4\"><span id=\"S3.T4.5.7.2.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Top-down</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T4.5.8.3\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T4.5.8.3.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T4.5.8.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Iqbal and Gall<span id=\"S3.T4.5.8.3.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib60\" title=\"\" class=\"ltx_ref\">2016</a><span id=\"S3.T4.5.8.3.1.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T4.5.8.3.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.5.8.3.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.5.8.3.2.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S3.T4.5.8.3.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Faster R-CNN + CPM</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.5.8.3.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.5.8.3.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.5.8.3.3.1.1\" class=\"ltx_p\" style=\"width:298.8pt;\"><span id=\"S3.T4.5.8.3.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">After person detection and single HPE, refines detected local joint candidates with Integer Linear Programming (ILP).</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.5.8.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T4.5.8.3.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T4.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T4.1.1.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T4.1.1.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Fang et&#160;al.<span id=\"S3.T4.1.1.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib38\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S3.T4.1.1.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T4.1.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.1.1.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.1.1.3.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S3.T4.1.1.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Faster R-CNN + Hourglass</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.1.1.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.1.1.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.1.1.4.1.1\" class=\"ltx_p\" style=\"width:298.8pt;\"><span id=\"S3.T4.1.1.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Combines symmetric spatial transformer network (SSTN) and Hourglass model to do SPPE on detected results; proposes a parametric pose NMS for refining pose proposals; designs a pose-guided proposals generator to augment the existing training samples</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><math id=\"S3.T4.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"63.3^{*}\" display=\"inline\"><semantics id=\"S3.T4.1.1.1.m1.1a\"><msup id=\"S3.T4.1.1.1.m1.1.1\" xref=\"S3.T4.1.1.1.m1.1.1.cmml\"><mn mathsize=\"80%\" id=\"S3.T4.1.1.1.m1.1.1.2\" xref=\"S3.T4.1.1.1.m1.1.1.2.cmml\">63.3</mn><mo mathsize=\"80%\" id=\"S3.T4.1.1.1.m1.1.1.3\" xref=\"S3.T4.1.1.1.m1.1.1.3.cmml\">&#8727;</mo></msup><annotation-xml encoding=\"MathML-Content\" id=\"S3.T4.1.1.1.m1.1b\"><apply id=\"S3.T4.1.1.1.m1.1.1.cmml\" xref=\"S3.T4.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T4.1.1.1.m1.1.1.1.cmml\" xref=\"S3.T4.1.1.1.m1.1.1\">superscript</csymbol><cn type=\"float\" id=\"S3.T4.1.1.1.m1.1.1.2.cmml\" xref=\"S3.T4.1.1.1.m1.1.1.2\">63.3</cn><times id=\"S3.T4.1.1.1.m1.1.1.3.cmml\" xref=\"S3.T4.1.1.1.m1.1.1.3\"/></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T4.1.1.1.m1.1c\">63.3^{*}</annotation></semantics></math></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T4.2.2\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T4.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T4.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Papandreou et&#160;al.<span id=\"S3.T4.2.2.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib123\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S3.T4.2.2.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T4.2.2.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.2.2.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.2.2.3.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S3.T4.2.2.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Faster R-CNN + ResNet-101</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.2.2.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.2.2.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.2.2.4.1.1\" class=\"ltx_p\" style=\"width:298.8pt;\"><span id=\"S3.T4.2.2.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Produces heatmap and offset map of each joint for SPPE and combines them with an aggregation procedure; uses keypoint-based NMS to avoid duplicate poses</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><math id=\"S3.T4.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"64.9^{*}\" display=\"inline\"><semantics id=\"S3.T4.2.2.1.m1.1a\"><msup id=\"S3.T4.2.2.1.m1.1.1\" xref=\"S3.T4.2.2.1.m1.1.1.cmml\"><mn mathsize=\"80%\" id=\"S3.T4.2.2.1.m1.1.1.2\" xref=\"S3.T4.2.2.1.m1.1.1.2.cmml\">64.9</mn><mo mathsize=\"80%\" id=\"S3.T4.2.2.1.m1.1.1.3\" xref=\"S3.T4.2.2.1.m1.1.1.3.cmml\">&#8727;</mo></msup><annotation-xml encoding=\"MathML-Content\" id=\"S3.T4.2.2.1.m1.1b\"><apply id=\"S3.T4.2.2.1.m1.1.1.cmml\" xref=\"S3.T4.2.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T4.2.2.1.m1.1.1.1.cmml\" xref=\"S3.T4.2.2.1.m1.1.1\">superscript</csymbol><cn type=\"float\" id=\"S3.T4.2.2.1.m1.1.1.2.cmml\" xref=\"S3.T4.2.2.1.m1.1.1.2\">64.9</cn><times id=\"S3.T4.2.2.1.m1.1.1.3.cmml\" xref=\"S3.T4.2.2.1.m1.1.1.3\"/></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T4.2.2.1.m1.1c\">64.9^{*}</annotation></semantics></math></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T4.3.3\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T4.3.3.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T4.3.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Huang et&#160;al.<span id=\"S3.T4.3.3.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib55\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S3.T4.3.3.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T4.3.3.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.3.3.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.3.3.3.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S3.T4.3.3.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Faster R-CNN + Inception-v2</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.3.3.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.3.3.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.3.3.4.1.1\" class=\"ltx_p\" style=\"width:298.8pt;\"><span id=\"S3.T4.3.3.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Produces coarse and fine poses for SPPE with multi-level supervisions; multi-scale features fusion</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><math id=\"S3.T4.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"72.2^{*}\" display=\"inline\"><semantics id=\"S3.T4.3.3.1.m1.1a\"><msup id=\"S3.T4.3.3.1.m1.1.1\" xref=\"S3.T4.3.3.1.m1.1.1.cmml\"><mn mathsize=\"80%\" id=\"S3.T4.3.3.1.m1.1.1.2\" xref=\"S3.T4.3.3.1.m1.1.1.2.cmml\">72.2</mn><mo mathsize=\"80%\" id=\"S3.T4.3.3.1.m1.1.1.3\" xref=\"S3.T4.3.3.1.m1.1.1.3.cmml\">&#8727;</mo></msup><annotation-xml encoding=\"MathML-Content\" id=\"S3.T4.3.3.1.m1.1b\"><apply id=\"S3.T4.3.3.1.m1.1.1.cmml\" xref=\"S3.T4.3.3.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T4.3.3.1.m1.1.1.1.cmml\" xref=\"S3.T4.3.3.1.m1.1.1\">superscript</csymbol><cn type=\"float\" id=\"S3.T4.3.3.1.m1.1.1.2.cmml\" xref=\"S3.T4.3.3.1.m1.1.1.2\">72.2</cn><times id=\"S3.T4.3.3.1.m1.1.1.3.cmml\" xref=\"S3.T4.3.3.1.m1.1.1.3\"/></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T4.3.3.1.m1.1c\">72.2^{*}</annotation></semantics></math></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T4.4.4\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T4.4.4.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T4.4.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>He et&#160;al.<span id=\"S3.T4.4.4.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib51\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S3.T4.4.4.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T4.4.4.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.4.4.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.4.4.3.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S3.T4.4.4.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Mask R-CNN + ResNet-FPN</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.4.4.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.4.4.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.4.4.4.1.1\" class=\"ltx_p\" style=\"width:298.8pt;\"><span id=\"S3.T4.4.4.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">An extension of Mask R-CNN framework; predicts keypoints and human mask synchronously</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><math id=\"S3.T4.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"63.1^{*}\" display=\"inline\"><semantics id=\"S3.T4.4.4.1.m1.1a\"><msup id=\"S3.T4.4.4.1.m1.1.1\" xref=\"S3.T4.4.4.1.m1.1.1.cmml\"><mn mathsize=\"80%\" id=\"S3.T4.4.4.1.m1.1.1.2\" xref=\"S3.T4.4.4.1.m1.1.1.2.cmml\">63.1</mn><mo mathsize=\"80%\" id=\"S3.T4.4.4.1.m1.1.1.3\" xref=\"S3.T4.4.4.1.m1.1.1.3.cmml\">&#8727;</mo></msup><annotation-xml encoding=\"MathML-Content\" id=\"S3.T4.4.4.1.m1.1b\"><apply id=\"S3.T4.4.4.1.m1.1.1.cmml\" xref=\"S3.T4.4.4.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T4.4.4.1.m1.1.1.1.cmml\" xref=\"S3.T4.4.4.1.m1.1.1\">superscript</csymbol><cn type=\"float\" id=\"S3.T4.4.4.1.m1.1.1.2.cmml\" xref=\"S3.T4.4.4.1.m1.1.1.2\">63.1</cn><times id=\"S3.T4.4.4.1.m1.1.1.3.cmml\" xref=\"S3.T4.4.4.1.m1.1.1.3\"/></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T4.4.4.1.m1.1c\">63.1^{*}</annotation></semantics></math></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T4.5.9.4\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T4.5.9.4.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T4.5.9.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Xiao et&#160;al.<span id=\"S3.T4.5.9.4.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib176\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S3.T4.5.9.4.1.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T4.5.9.4.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.5.9.4.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.5.9.4.2.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S3.T4.5.9.4.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Faster R-CNN + ResNet</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.5.9.4.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.5.9.4.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.5.9.4.3.1.1\" class=\"ltx_p\" style=\"width:298.8pt;\"><span id=\"S3.T4.5.9.4.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Simply adds a few deconvolutional layers after ResNet to generate heatmaps from deep and low resolution features</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.5.9.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T4.5.9.4.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">73.7</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T4.5.10.5\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T4.5.10.5.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T4.5.10.5.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Chen et&#160;al.<span id=\"S3.T4.5.10.5.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S3.T4.5.10.5.1.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T4.5.10.5.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.5.10.5.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.5.10.5.2.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S3.T4.5.10.5.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FPN + CPN</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.5.10.5.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.5.10.5.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.5.10.5.3.1.1\" class=\"ltx_p\" style=\"width:298.8pt;\"><span id=\"S3.T4.5.10.5.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Proposes CPN with feature pyramid; two-stage network; online hard keypoints mining</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.5.10.5.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T4.5.10.5.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">73.0</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T4.5.11.6\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T4.5.11.6.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T4.5.11.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Moon et&#160;al.<span id=\"S3.T4.5.11.6.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib112\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S3.T4.5.11.6.1.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T4.5.11.6.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.5.11.6.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.5.11.6.2.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S3.T4.5.11.6.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">ResNet + upsampling</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.5.11.6.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.5.11.6.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.5.11.6.3.1.1\" class=\"ltx_p\" style=\"width:298.8pt;\"><span id=\"S3.T4.5.11.6.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">proposes PoseFix net to refine estimated pose from any HPE methods based on pose error distributions</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.5.11.6.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T4.5.11.6.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T4.5.12.7\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T4.5.12.7.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T4.5.12.7.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Sun et&#160;al.<span id=\"S3.T4.5.12.7.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib151\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S3.T4.5.12.7.1.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T4.5.12.7.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.5.12.7.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.5.12.7.2.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S3.T4.5.12.7.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Faster R-CNN + HRNet</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.5.12.7.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.5.12.7.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.5.12.7.3.1.1\" class=\"ltx_p\" style=\"width:298.8pt;\"><span id=\"S3.T4.5.12.7.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">high-resolution representations of features across the whole network, multi-scale fusion</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.5.12.7.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T4.5.12.7.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">75.5</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T4.5.13.8\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T4.5.13.8.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" colspan=\"4\"><span id=\"S3.T4.5.13.8.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Bottom-up</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T4.5.14.9\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T4.5.14.9.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T4.5.14.9.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Pishchulin et&#160;al.<span id=\"S3.T4.5.14.9.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib131\" title=\"\" class=\"ltx_ref\">2016</a><span id=\"S3.T4.5.14.9.1.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T4.5.14.9.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.5.14.9.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.5.14.9.2.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S3.T4.5.14.9.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Fast R-CNN</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.5.14.9.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.5.14.9.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.5.14.9.3.1.1\" class=\"ltx_p\" style=\"width:298.8pt;\"><span id=\"S3.T4.5.14.9.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Formulate the distinguishing different persons as an ILP problem; cluster detected part candidates; combine person clusters and labeled parts to obtain final poses</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.5.14.9.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T4.5.14.9.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T4.5.15.10\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T4.5.15.10.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T4.5.15.10.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Insafutdinov et&#160;al.<span id=\"S3.T4.5.15.10.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib58\" title=\"\" class=\"ltx_ref\">2016</a><span id=\"S3.T4.5.15.10.1.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T4.5.15.10.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.5.15.10.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.5.15.10.2.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S3.T4.5.15.10.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">ResNet</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.5.15.10.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.5.15.10.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.5.15.10.3.1.1\" class=\"ltx_p\" style=\"width:298.8pt;\"><span id=\"S3.T4.5.15.10.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Employs image-conditioned pairwise terms to assemble the part proposals</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.5.15.10.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T4.5.15.10.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T4.5.5\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T4.5.5.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T4.5.5.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Cao et&#160;al.<span id=\"S3.T4.5.5.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">2016</a><span id=\"S3.T4.5.5.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T4.5.5.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.5.5.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.5.5.3.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S3.T4.5.5.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">VGG-19 + CPM</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.5.5.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.5.5.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.5.5.4.1.1\" class=\"ltx_p\" style=\"width:298.8pt;\"><span id=\"S3.T4.5.5.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">OpenPose; real-time; Simultaneous joints detection and association in a two-branch architecture; propose Part Affinity Fields (PAFs) to encode the location and orientation of limbs</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><math id=\"S3.T4.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"61.8^{*}\" display=\"inline\"><semantics id=\"S3.T4.5.5.1.m1.1a\"><msup id=\"S3.T4.5.5.1.m1.1.1\" xref=\"S3.T4.5.5.1.m1.1.1.cmml\"><mn mathsize=\"80%\" id=\"S3.T4.5.5.1.m1.1.1.2\" xref=\"S3.T4.5.5.1.m1.1.1.2.cmml\">61.8</mn><mo mathsize=\"80%\" id=\"S3.T4.5.5.1.m1.1.1.3\" xref=\"S3.T4.5.5.1.m1.1.1.3.cmml\">&#8727;</mo></msup><annotation-xml encoding=\"MathML-Content\" id=\"S3.T4.5.5.1.m1.1b\"><apply id=\"S3.T4.5.5.1.m1.1.1.cmml\" xref=\"S3.T4.5.5.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T4.5.5.1.m1.1.1.1.cmml\" xref=\"S3.T4.5.5.1.m1.1.1\">superscript</csymbol><cn type=\"float\" id=\"S3.T4.5.5.1.m1.1.1.2.cmml\" xref=\"S3.T4.5.5.1.m1.1.1.2\">61.8</cn><times id=\"S3.T4.5.5.1.m1.1.1.3.cmml\" xref=\"S3.T4.5.5.1.m1.1.1.3\"/></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T4.5.5.1.m1.1c\">61.8^{*}</annotation></semantics></math></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T4.5.16.11\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T4.5.16.11.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T4.5.16.11.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Newell et&#160;al.<span id=\"S3.T4.5.16.11.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib114\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S3.T4.5.16.11.1.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T4.5.16.11.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.5.16.11.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.5.16.11.2.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S3.T4.5.16.11.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Hourglass</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.5.16.11.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.5.16.11.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.5.16.11.3.1.1\" class=\"ltx_p\" style=\"width:298.8pt;\"><span id=\"S3.T4.5.16.11.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Simultaneous joints detection and association in one branch; propose dense associative embedding tags for detected joints grouping</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.5.16.11.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T4.5.16.11.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">65.5</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T4.5.17.12\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T4.5.17.12.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T4.5.17.12.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Nie et&#160;al.<span id=\"S3.T4.5.17.12.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib118\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S3.T4.5.17.12.1.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T4.5.17.12.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.5.17.12.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.5.17.12.2.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S3.T4.5.17.12.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Hourglass</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.5.17.12.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.5.17.12.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.5.17.12.3.1.1\" class=\"ltx_p\" style=\"width:298.8pt;\"><span id=\"S3.T4.5.17.12.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Simultaneous joints detection and association in a two-branch architecture; generate partitions in the embedding space parameterized by person centroids over joint candidates; estimate pose instances by a local greedy inference approach</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.5.17.12.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T4.5.17.12.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T4.5.18.13\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T4.5.18.13.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T4.5.18.13.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Papandreou et&#160;al.<span id=\"S3.T4.5.18.13.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib122\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S3.T4.5.18.13.1.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T4.5.18.13.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.5.18.13.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.5.18.13.2.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S3.T4.5.18.13.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">ResNet</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.5.18.13.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.5.18.13.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.5.18.13.3.1.1\" class=\"ltx_p\" style=\"width:298.8pt;\"><span id=\"S3.T4.5.18.13.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Multi-task (pose estimation and instance segmentation) network; simultaneous joints detection and association in a multi-branch architecture; multi-range joint offsets following tree-structured kinematic graph to guide joints grouping</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.5.18.13.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T4.5.18.13.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">68.7</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T4.5.19.14\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T4.5.19.14.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T4.5.19.14.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Kocabas et&#160;al.<span id=\"S3.T4.5.19.14.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib76\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S3.T4.5.19.14.1.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T4.5.19.14.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.5.19.14.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.5.19.14.2.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S3.T4.5.19.14.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">ResNet-FPN + RetinaNet</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.5.19.14.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.5.19.14.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.5.19.14.3.1.1\" class=\"ltx_p\" style=\"width:298.8pt;\"><span id=\"S3.T4.5.19.14.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Multi-task (pose estimation, person detection and person segmentation) network; simultaneous keypoint detection and person detection in a two-branch architecture; proposes a Pose Residual Network (PRN) to assign keypoint detection to person instances</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.5.19.14.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T4.5.19.14.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">69.6</span></td>&#13;\n</tr>&#13;\n<tr id=\"S3.T4.5.20.15\" class=\"ltx_tr\">&#13;\n<td id=\"S3.T4.5.20.15.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S3.T4.5.20.15.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Kreiss et&#160;al.<span id=\"S3.T4.5.20.15.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib77\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S3.T4.5.20.15.1.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S3.T4.5.20.15.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.5.20.15.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.5.20.15.2.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S3.T4.5.20.15.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">ResNet-50</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.5.20.15.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">&#13;\n<span id=\"S3.T4.5.20.15.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S3.T4.5.20.15.3.1.1\" class=\"ltx_p\" style=\"width:298.8pt;\"><span id=\"S3.T4.5.20.15.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">predicts Part Intensity Fields (PIF) and Part Association Fields (PAF) to represent body joints location and body joints association; works well under low-resolution</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S3.T4.5.20.15.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S3.T4.5.20.15.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">66.7</span></td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "(Iqbal and Gall, 2016) Iqbal and Gall (2016)\r\n\r\nIqbal, U., Gall, J., 2016.\r\n\r\n\r\nMulti-person pose estimation with local\r\njoint-to-person associations, in: Proc. European\r\nConference on Computer Vision, Springer. pp.\r\n627–642.",
            "(Fang et al., 2017) Fang et al. (2017)\r\n\r\nFang, H., Xie, S., Tai,\r\nY.W., Lu, C., 2017.\r\n\r\n\r\nRmpe: Regional multi-person pose estimation, in:\r\nProc. IEEE International Conference on Computer Vision,\r\npp. 2334–2343.",
            "(Papandreou et al., 2017) Papandreou et al. (2017)\r\n\r\nPapandreou, G., Zhu, T.,\r\nKanazawa, N., Toshev, A.,\r\nTompson, J., Bregler, C.,\r\nMurphy, K., 2017.\r\n\r\n\r\nTowards accurate multi-person pose estimation in the\r\nwild, in: Proc. IEEE Conference on Computer Vision and\r\nPattern Recognition, pp. 4903–4911.",
            "(Huang et al., 2017) Huang et al. (2017)\r\n\r\nHuang, S., Gong, M., Tao,\r\nD., 2017.\r\n\r\n\r\nA coarse-fine network for keypoint localization, in:\r\nProc. IEEE International Conference on Computer Vision,\r\npp. 3028–3037.",
            "(He et al., 2017) He et al. (2017)\r\n\r\nHe, K., Gkioxari, G.,\r\nDollár, P., Girshick, R.,\r\n2017.\r\n\r\n\r\nMask r-cnn, in: Proc. IEEE\r\nInternational Conference on Computer Vision, IEEE.\r\npp. 2980–2988.",
            "(Xiao et al., 2018) Xiao et al. (2018)\r\n\r\nXiao, B., Wu, H., Wei,\r\nY., 2018.\r\n\r\n\r\nSimple baselines for human pose estimation and\r\ntracking, in: Proc. European Conference on Computer\r\nVision, pp. 466–481.",
            "(Chen et al., 2018) Chen et al. (2018)\r\n\r\nChen, Y., Wang, Z., Peng,\r\nY., Zhang, Z., Yu, G.,\r\nSun, J., 2018.\r\n\r\n\r\nCascaded pyramid network for multi-person pose\r\nestimation, in: Proc. IEEE Conference on Computer Vision\r\nand Pattern Recognition, pp. 7103–7112.",
            "(Moon et al., 2019) Moon et al. (2019)\r\n\r\nMoon, G., Chang, J.Y.,\r\nLee, K.M., 2019.\r\n\r\n\r\nPosefix: Model-agnostic general human pose refinement\r\nnetwork, in: Proc. IEEE Conference on Computer Vision\r\nand Pattern Recognition, pp. 7773–7781.",
            "(Sun et al., 2019) Sun et al. (2019)\r\n\r\nSun, K., Xiao, B., Liu,\r\nD., Wang, J., 2019.\r\n\r\n\r\nDeep high-resolution representation learning for\r\nhuman pose estimation, in: Proc. IEEE Conference on\r\nComputer Vision and Pattern Recognition.",
            "(Pishchulin et al., 2016) Pishchulin et al. (2016)\r\n\r\nPishchulin, L., Insafutdinov, E.,\r\nTang, S., Andres, B.,\r\nAndriluka, M., Gehler, P.V.,\r\nSchiele, B., 2016.\r\n\r\n\r\nDeepcut: Joint subset partition and labeling for\r\nmulti person pose estimation, in: Proc. IEEE Conference\r\non Computer Vision and Pattern Recognition, pp. 4929–4937.",
            "(Insafutdinov et al., 2016) Insafutdinov et al. (2016)\r\n\r\nInsafutdinov, E., Pishchulin, L.,\r\nAndres, B., Andriluka, M.,\r\nSchiele, B., 2016.\r\n\r\n\r\nDeepercut: A deeper, stronger, and faster\r\nmulti-person pose estimation model, in: Proc. European\r\nConference on Computer Vision, Springer. pp.\r\n34–50.",
            "(Cao et al., 2016) Cao et al. (2016)\r\n\r\nCao, Z., Simon, T., Wei,\r\nS.E., Sheikh, Y., 2016.\r\n\r\n\r\nRealtime multi-person 2d pose estimation using part\r\naffinity fields.\r\n\r\n\r\narXiv preprint arXiv:1611.08050 .",
            "(Newell et al., 2017) Newell et al. (2017)\r\n\r\nNewell, A., Huang, Z.,\r\nDeng, J., 2017.\r\n\r\n\r\nAssociative embedding: End-to-end learning for joint\r\ndetection and grouping, in: Advances in Neural\r\nInformation Processing Systems, pp. 2277–2287.",
            "(Nie et al., 2018) Nie et al. (2018)\r\n\r\nNie, X., Feng, J., Xing,\r\nJ., Yan, S., 2018.\r\n\r\n\r\nPose partition networks for multi-person pose\r\nestimation, in: Proc. European Conference on Computer\r\nVision, pp. 684–699.",
            "(Papandreou et al., 2018) Papandreou et al. (2018)\r\n\r\nPapandreou, G., Zhu, T.,\r\nChen, L.C., Gidaris, S.,\r\nTompson, J., Murphy, K.,\r\n2018.\r\n\r\n\r\nPersonlab: Person pose estimation and instance\r\nsegmentation with a bottom-up, part-based, geometric embedding model.\r\n\r\n\r\narXiv preprint arXiv:1803.08225 .",
            "(Kocabas et al., 2018) Kocabas et al. (2018)\r\n\r\nKocabas, M., Karagoz, S.,\r\nAkbas, E., 2018.\r\n\r\n\r\nMultiposenet: Fast multi-person pose estimation using\r\npose residual network, in: Proc. European Conference on\r\nComputer Vision, Springer. pp.\r\n437–453.",
            "(Kreiss et al., 2019) Kreiss et al. (2019)\r\n\r\nKreiss, S., Bertoni, L.,\r\nAlahi, A., 2019.\r\n\r\n\r\nPifpaf: Composite fields for human pose estimation,\r\nin: Proc. IEEE Conference on Computer Vision and Pattern\r\nRecognition, pp. 11977–11986."
        ],
        "references": [
            "Top-down methods generally employ person detectors to obtain a set of the bounding box of people in the input image and then directly leverage existing single-person pose estimators to predict human poses. The predicted poses heavily depend on the precision of the person detection. The runtime for the whole system is proportional based on the number of persons. While bottom-up methods directly predict all the 2D joints of all persons and then assemble them into independent skeletons. Correct grouping of joint points in a complex environment is a challenging research task. Table 4 summarizes recent deep learning-based work about 2D multi-person pose estimation methods in both top-down and bottom-up categories. The last column of Table 4 is the Average Precision (AP) scores on the COCO test-dev dataset. More details of datasets and evaluation metrics are described in Section 5."
        ]
    },
    "id_table_5": {
        "caption": "Table 5: Comparison of 3D single person pose estimation methods. Here “E.” stands for “Extra data” and “T.” indicates “Temporal info”. The last column is the Mean Per Joint Position Error (MPJPE) in millimeter on Human3.6M dataset under protocol #1. The results with ∗* were reported from 6 actions in testing set, while others from all 17 actions. The results with ††\\dagger were reported with 2D joint ground truth. The methods with ##\\# report joint rotation as well.",
        "table": [
            "<table id=\"S4.T5.42\" class=\"ltx_tabular ltx_centering ltx_align_middle\">&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T5.42.37.1\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.42.37.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S4.T5.42.37.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Methods</span></td>&#13;\n<td id=\"S4.T5.42.37.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.42.37.1.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.42.37.1.2.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S4.T5.42.37.1.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Backbone</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.42.37.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.42.37.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">E.</span></td>&#13;\n<td id=\"S4.T5.42.37.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.42.37.1.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">T.</span></td>&#13;\n<td id=\"S4.T5.42.37.1.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.42.37.1.5.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.42.37.1.5.1.1\" class=\"ltx_p\" style=\"width:270.3pt;\"><span id=\"S4.T5.42.37.1.5.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Highlights</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.42.37.1.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.42.37.1.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.42.37.1.6.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><span id=\"S4.T5.42.37.1.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">MPJPE (mm)</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.42.38.2\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.42.38.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" colspan=\"6\"><span id=\"S4.T5.42.38.2.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Model-free</span></td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.7.1\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.7.1.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T5.7.1.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Li and Chan<span id=\"S4.T5.7.1.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib87\" title=\"\" class=\"ltx_ref\">2014</a><span id=\"S4.T5.7.1.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S4.T5.7.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.7.1.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.7.1.3.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S4.T5.7.1.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">shallow CNNs</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.7.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.7.1.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.7.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.7.1.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.7.1.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.7.1.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.7.1.6.1.1\" class=\"ltx_p\" style=\"width:270.3pt;\"><span id=\"S4.T5.7.1.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A multi-task network to predict of body part detection with sliding windows and 3D pose estimation jointly</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.7.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.7.1.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.7.1.1.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><math id=\"S4.T5.7.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"132.2^{*}\" display=\"inline\"><semantics id=\"S4.T5.7.1.1.1.1.m1.1a\"><msup id=\"S4.T5.7.1.1.1.1.m1.1.1\" xref=\"S4.T5.7.1.1.1.1.m1.1.1.cmml\"><mn mathsize=\"80%\" id=\"S4.T5.7.1.1.1.1.m1.1.1.2\" xref=\"S4.T5.7.1.1.1.1.m1.1.1.2.cmml\">132.2</mn><mo mathsize=\"80%\" id=\"S4.T5.7.1.1.1.1.m1.1.1.3\" xref=\"S4.T5.7.1.1.1.1.m1.1.1.3.cmml\">&#8727;</mo></msup><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.7.1.1.1.1.m1.1b\"><apply id=\"S4.T5.7.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T5.7.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.T5.7.1.1.1.1.m1.1.1.1.cmml\" xref=\"S4.T5.7.1.1.1.1.m1.1.1\">superscript</csymbol><cn type=\"float\" id=\"S4.T5.7.1.1.1.1.m1.1.1.2.cmml\" xref=\"S4.T5.7.1.1.1.1.m1.1.1.2\">132.2</cn><times id=\"S4.T5.7.1.1.1.1.m1.1.1.3.cmml\" xref=\"S4.T5.7.1.1.1.1.m1.1.1.3\"/></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.7.1.1.1.1.m1.1c\">132.2^{*}</annotation></semantics></math></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.8.2\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.8.2.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T5.8.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Li et&#160;al.<span id=\"S4.T5.8.2.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib89\" title=\"\" class=\"ltx_ref\">2015b</a><span id=\"S4.T5.8.2.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S4.T5.8.2.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.8.2.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.8.2.3.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S4.T5.8.2.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">shallow CNNs</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.8.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.8.2.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.8.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.8.2.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.8.2.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.8.2.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.8.2.6.1.1\" class=\"ltx_p\" style=\"width:270.3pt;\"><span id=\"S4.T5.8.2.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Compute matching score of image-pose pairs</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.8.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.8.2.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.8.2.1.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><math id=\"S4.T5.8.2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"120.2^{*}\" display=\"inline\"><semantics id=\"S4.T5.8.2.1.1.1.m1.1a\"><msup id=\"S4.T5.8.2.1.1.1.m1.1.1\" xref=\"S4.T5.8.2.1.1.1.m1.1.1.cmml\"><mn mathsize=\"80%\" id=\"S4.T5.8.2.1.1.1.m1.1.1.2\" xref=\"S4.T5.8.2.1.1.1.m1.1.1.2.cmml\">120.2</mn><mo mathsize=\"80%\" id=\"S4.T5.8.2.1.1.1.m1.1.1.3\" xref=\"S4.T5.8.2.1.1.1.m1.1.1.3.cmml\">&#8727;</mo></msup><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.8.2.1.1.1.m1.1b\"><apply id=\"S4.T5.8.2.1.1.1.m1.1.1.cmml\" xref=\"S4.T5.8.2.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.T5.8.2.1.1.1.m1.1.1.1.cmml\" xref=\"S4.T5.8.2.1.1.1.m1.1.1\">superscript</csymbol><cn type=\"float\" id=\"S4.T5.8.2.1.1.1.m1.1.1.2.cmml\" xref=\"S4.T5.8.2.1.1.1.m1.1.1.2\">120.2</cn><times id=\"S4.T5.8.2.1.1.1.m1.1.1.3.cmml\" xref=\"S4.T5.8.2.1.1.1.m1.1.1.3\"/></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.8.2.1.1.1.m1.1c\">120.2^{*}</annotation></semantics></math></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.9.3\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.9.3.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T5.9.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Tekin et&#160;al.<span id=\"S4.T5.9.3.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib159\" title=\"\" class=\"ltx_ref\">2016</a><span id=\"S4.T5.9.3.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S4.T5.9.3.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.9.3.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.9.3.3.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S4.T5.9.3.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">auto-encoder+ shallow CNNs</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.9.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.9.3.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.9.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.9.3.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.9.3.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.9.3.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.9.3.6.1.1\" class=\"ltx_p\" style=\"width:270.3pt;\"><span id=\"S4.T5.9.3.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Employ an auto-encoder to learn a high-dimensional representation of 3D pose; use a shallow CNNs network to learn the high-dimensional pose representation</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.9.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.9.3.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.9.3.1.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><math id=\"S4.T5.9.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"116.8^{*}\" display=\"inline\"><semantics id=\"S4.T5.9.3.1.1.1.m1.1a\"><msup id=\"S4.T5.9.3.1.1.1.m1.1.1\" xref=\"S4.T5.9.3.1.1.1.m1.1.1.cmml\"><mn mathsize=\"80%\" id=\"S4.T5.9.3.1.1.1.m1.1.1.2\" xref=\"S4.T5.9.3.1.1.1.m1.1.1.2.cmml\">116.8</mn><mo mathsize=\"80%\" id=\"S4.T5.9.3.1.1.1.m1.1.1.3\" xref=\"S4.T5.9.3.1.1.1.m1.1.1.3.cmml\">&#8727;</mo></msup><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.9.3.1.1.1.m1.1b\"><apply id=\"S4.T5.9.3.1.1.1.m1.1.1.cmml\" xref=\"S4.T5.9.3.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.T5.9.3.1.1.1.m1.1.1.1.cmml\" xref=\"S4.T5.9.3.1.1.1.m1.1.1\">superscript</csymbol><cn type=\"float\" id=\"S4.T5.9.3.1.1.1.m1.1.1.2.cmml\" xref=\"S4.T5.9.3.1.1.1.m1.1.1.2\">116.8</cn><times id=\"S4.T5.9.3.1.1.1.m1.1.1.3.cmml\" xref=\"S4.T5.9.3.1.1.1.m1.1.1.3\"/></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.9.3.1.1.1.m1.1c\">116.8^{*}</annotation></semantics></math></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.10.4\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.10.4.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T5.10.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Tekin et&#160;al.<span id=\"S4.T5.10.4.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib160\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S4.T5.10.4.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S4.T5.10.4.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.10.4.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.10.4.3.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S4.T5.10.4.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Hourglass</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.10.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.10.4.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>&#13;\n<td id=\"S4.T5.10.4.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.10.4.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.10.4.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.10.4.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.10.4.6.1.1\" class=\"ltx_p\" style=\"width:270.3pt;\"><span id=\"S4.T5.10.4.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Predict 2D heatmaps for joints first; then use a trainable fusion architecture to combine 2D heatmaps and extracted features; 2D module is pre-trained with MPII</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.10.4.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.10.4.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.10.4.1.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><math id=\"S4.T5.10.4.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"69.7\" display=\"inline\"><semantics id=\"S4.T5.10.4.1.1.1.m1.1a\"><mn mathsize=\"80%\" id=\"S4.T5.10.4.1.1.1.m1.1.1\" xref=\"S4.T5.10.4.1.1.1.m1.1.1.cmml\">69.7</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.10.4.1.1.1.m1.1b\"><cn type=\"float\" id=\"S4.T5.10.4.1.1.1.m1.1.1.cmml\" xref=\"S4.T5.10.4.1.1.1.m1.1.1\">69.7</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.10.4.1.1.1.m1.1c\">69.7</annotation></semantics></math></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.12.6\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.12.6.3\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T5.12.6.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Chen and Ramanan<span id=\"S4.T5.12.6.3.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S4.T5.12.6.3.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S4.T5.12.6.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.12.6.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.12.6.4.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S4.T5.12.6.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">CPM</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.12.6.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.12.6.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>&#13;\n<td id=\"S4.T5.12.6.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.12.6.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.12.6.7\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.12.6.7.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.12.6.7.1.1\" class=\"ltx_p\" style=\"width:270.3pt;\"><span id=\"S4.T5.12.6.7.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Estimate 2D poses from images first; then estimate depth of them by matching to a library of 3D poses; 2D module is pre-trained with MPII</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.12.6.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.12.6.2.2\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.12.6.2.2.2\" class=\"ltx_p\" style=\"width:22.8pt;\"><math id=\"S4.T5.11.5.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"82.7\" display=\"inline\"><semantics id=\"S4.T5.11.5.1.1.1.m1.1a\"><mn mathsize=\"80%\" id=\"S4.T5.11.5.1.1.1.m1.1.1\" xref=\"S4.T5.11.5.1.1.1.m1.1.1.cmml\">82.7</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.11.5.1.1.1.m1.1b\"><cn type=\"float\" id=\"S4.T5.11.5.1.1.1.m1.1.1.cmml\" xref=\"S4.T5.11.5.1.1.1.m1.1.1\">82.7</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.11.5.1.1.1.m1.1c\">82.7</annotation></semantics></math><span id=\"S4.T5.12.6.2.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S4.T5.12.6.2.2.2.m2.1\" class=\"ltx_Math\" alttext=\"/57.5^{\\dagger}\" display=\"inline\"><semantics id=\"S4.T5.12.6.2.2.2.m2.1a\"><mrow id=\"S4.T5.12.6.2.2.2.m2.1.1\" xref=\"S4.T5.12.6.2.2.2.m2.1.1.cmml\"><mi id=\"S4.T5.12.6.2.2.2.m2.1.1.2\" xref=\"S4.T5.12.6.2.2.2.m2.1.1.2.cmml\"/><mo maxsize=\"80%\" minsize=\"80%\" stretchy=\"true\" symmetric=\"true\" id=\"S4.T5.12.6.2.2.2.m2.1.1.1\" xref=\"S4.T5.12.6.2.2.2.m2.1.1.1.cmml\">/</mo><msup id=\"S4.T5.12.6.2.2.2.m2.1.1.3\" xref=\"S4.T5.12.6.2.2.2.m2.1.1.3.cmml\"><mn mathsize=\"80%\" id=\"S4.T5.12.6.2.2.2.m2.1.1.3.2\" xref=\"S4.T5.12.6.2.2.2.m2.1.1.3.2.cmml\">57.5</mn><mo mathsize=\"80%\" id=\"S4.T5.12.6.2.2.2.m2.1.1.3.3\" xref=\"S4.T5.12.6.2.2.2.m2.1.1.3.3.cmml\">&#8224;</mo></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.12.6.2.2.2.m2.1b\"><apply id=\"S4.T5.12.6.2.2.2.m2.1.1.cmml\" xref=\"S4.T5.12.6.2.2.2.m2.1.1\"><divide id=\"S4.T5.12.6.2.2.2.m2.1.1.1.cmml\" xref=\"S4.T5.12.6.2.2.2.m2.1.1.1\"/><csymbol cd=\"latexml\" id=\"S4.T5.12.6.2.2.2.m2.1.1.2.cmml\" xref=\"S4.T5.12.6.2.2.2.m2.1.1.2\">absent</csymbol><apply id=\"S4.T5.12.6.2.2.2.m2.1.1.3.cmml\" xref=\"S4.T5.12.6.2.2.2.m2.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.T5.12.6.2.2.2.m2.1.1.3.1.cmml\" xref=\"S4.T5.12.6.2.2.2.m2.1.1.3\">superscript</csymbol><cn type=\"float\" id=\"S4.T5.12.6.2.2.2.m2.1.1.3.2.cmml\" xref=\"S4.T5.12.6.2.2.2.m2.1.1.3.2\">57.5</cn><ci id=\"S4.T5.12.6.2.2.2.m2.1.1.3.3.cmml\" xref=\"S4.T5.12.6.2.2.2.m2.1.1.3.3\">&#8224;</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.12.6.2.2.2.m2.1c\">/57.5^{\\dagger}</annotation></semantics></math></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.13.7\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.13.7.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T5.13.7.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Moreno-Noguer<span id=\"S4.T5.13.7.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib113\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S4.T5.13.7.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S4.T5.13.7.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.13.7.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.13.7.3.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S4.T5.13.7.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">CPM</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.13.7.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.13.7.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>&#13;\n<td id=\"S4.T5.13.7.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.13.7.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.13.7.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.13.7.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.13.7.6.1.1\" class=\"ltx_p\" style=\"width:270.3pt;\"><span id=\"S4.T5.13.7.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Use Euclidean Distance Matrices (EDMs) to encoding pairwise distances of 2D and 3D body joints; train a network to learn 2D-to-3D EDM regression; jointly trained with other 3D (Humaneva-I) dataset</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.13.7.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.13.7.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.13.7.1.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><math id=\"S4.T5.13.7.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"87.3\" display=\"inline\"><semantics id=\"S4.T5.13.7.1.1.1.m1.1a\"><mn mathsize=\"80%\" id=\"S4.T5.13.7.1.1.1.m1.1.1\" xref=\"S4.T5.13.7.1.1.1.m1.1.1.cmml\">87.3</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.13.7.1.1.1.m1.1b\"><cn type=\"float\" id=\"S4.T5.13.7.1.1.1.m1.1.1.cmml\" xref=\"S4.T5.13.7.1.1.1.m1.1.1\">87.3</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.13.7.1.1.1.m1.1c\">87.3</annotation></semantics></math></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.14.8\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.14.8.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T5.14.8.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Pavlakos et&#160;al.<span id=\"S4.T5.14.8.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib125\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S4.T5.14.8.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S4.T5.14.8.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.14.8.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.14.8.3.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S4.T5.14.8.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Hourglass</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.14.8.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.14.8.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>&#13;\n<td id=\"S4.T5.14.8.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.14.8.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.14.8.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.14.8.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.14.8.6.1.1\" class=\"ltx_p\" style=\"width:270.3pt;\"><span id=\"S4.T5.14.8.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Volumetric representation for 3D human pose; a coarse-to-fine prediction scheme; 2D module is pre-trained with MPII</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.14.8.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.14.8.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.14.8.1.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><math id=\"S4.T5.14.8.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"71.9\" display=\"inline\"><semantics id=\"S4.T5.14.8.1.1.1.m1.1a\"><mn mathsize=\"80%\" id=\"S4.T5.14.8.1.1.1.m1.1.1\" xref=\"S4.T5.14.8.1.1.1.m1.1.1.cmml\">71.9</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.14.8.1.1.1.m1.1b\"><cn type=\"float\" id=\"S4.T5.14.8.1.1.1.m1.1.1.cmml\" xref=\"S4.T5.14.8.1.1.1.m1.1.1\">71.9</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.14.8.1.1.1.m1.1c\">71.9</annotation></semantics></math></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.15.9\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.15.9.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T5.15.9.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Zhou et&#160;al.<span id=\"S4.T5.15.9.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib184\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S4.T5.15.9.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S4.T5.15.9.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.15.9.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.15.9.3.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S4.T5.15.9.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Hourglass</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.15.9.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.15.9.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>&#13;\n<td id=\"S4.T5.15.9.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.15.9.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.15.9.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.15.9.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.15.9.6.1.1\" class=\"ltx_p\" style=\"width:270.3pt;\"><span id=\"S4.T5.15.9.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A proposed loss induced from a geometric constraint for 2D data; bone-length constraints; jointly trained with 2D (MPII) dataset</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.15.9.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.15.9.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.15.9.1.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><math id=\"S4.T5.15.9.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"64.9\" display=\"inline\"><semantics id=\"S4.T5.15.9.1.1.1.m1.1a\"><mn mathsize=\"80%\" id=\"S4.T5.15.9.1.1.1.m1.1.1\" xref=\"S4.T5.15.9.1.1.1.m1.1.1.cmml\">64.9</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.15.9.1.1.1.m1.1b\"><cn type=\"float\" id=\"S4.T5.15.9.1.1.1.m1.1.1.cmml\" xref=\"S4.T5.15.9.1.1.1.m1.1.1\">64.9</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.15.9.1.1.1.m1.1c\">64.9</annotation></semantics></math></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.17.11\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.17.11.3\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T5.17.11.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Martinez et&#160;al.<span id=\"S4.T5.17.11.3.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib103\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S4.T5.17.11.3.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S4.T5.17.11.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.17.11.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.17.11.4.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S4.T5.17.11.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Hourglass</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.17.11.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.17.11.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>&#13;\n<td id=\"S4.T5.17.11.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.17.11.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.17.11.7\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.17.11.7.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.17.11.7.1.1\" class=\"ltx_p\" style=\"width:270.3pt;\"><span id=\"S4.T5.17.11.7.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Directly map predicted 2D poses to 3D poses with two linear layers; 2D module is pre-trained with MPII; process in real-time</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.17.11.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.17.11.2.2\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.17.11.2.2.2\" class=\"ltx_p\" style=\"width:22.8pt;\"><math id=\"S4.T5.16.10.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"62.9\" display=\"inline\"><semantics id=\"S4.T5.16.10.1.1.1.m1.1a\"><mn mathsize=\"80%\" id=\"S4.T5.16.10.1.1.1.m1.1.1\" xref=\"S4.T5.16.10.1.1.1.m1.1.1.cmml\">62.9</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.16.10.1.1.1.m1.1b\"><cn type=\"float\" id=\"S4.T5.16.10.1.1.1.m1.1.1.cmml\" xref=\"S4.T5.16.10.1.1.1.m1.1.1\">62.9</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.16.10.1.1.1.m1.1c\">62.9</annotation></semantics></math><span id=\"S4.T5.17.11.2.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S4.T5.17.11.2.2.2.m2.1\" class=\"ltx_Math\" alttext=\"/45.5^{\\dagger}\" display=\"inline\"><semantics id=\"S4.T5.17.11.2.2.2.m2.1a\"><mrow id=\"S4.T5.17.11.2.2.2.m2.1.1\" xref=\"S4.T5.17.11.2.2.2.m2.1.1.cmml\"><mi id=\"S4.T5.17.11.2.2.2.m2.1.1.2\" xref=\"S4.T5.17.11.2.2.2.m2.1.1.2.cmml\"/><mo maxsize=\"80%\" minsize=\"80%\" stretchy=\"true\" symmetric=\"true\" id=\"S4.T5.17.11.2.2.2.m2.1.1.1\" xref=\"S4.T5.17.11.2.2.2.m2.1.1.1.cmml\">/</mo><msup id=\"S4.T5.17.11.2.2.2.m2.1.1.3\" xref=\"S4.T5.17.11.2.2.2.m2.1.1.3.cmml\"><mn mathsize=\"80%\" id=\"S4.T5.17.11.2.2.2.m2.1.1.3.2\" xref=\"S4.T5.17.11.2.2.2.m2.1.1.3.2.cmml\">45.5</mn><mo mathsize=\"80%\" id=\"S4.T5.17.11.2.2.2.m2.1.1.3.3\" xref=\"S4.T5.17.11.2.2.2.m2.1.1.3.3.cmml\">&#8224;</mo></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.17.11.2.2.2.m2.1b\"><apply id=\"S4.T5.17.11.2.2.2.m2.1.1.cmml\" xref=\"S4.T5.17.11.2.2.2.m2.1.1\"><divide id=\"S4.T5.17.11.2.2.2.m2.1.1.1.cmml\" xref=\"S4.T5.17.11.2.2.2.m2.1.1.1\"/><csymbol cd=\"latexml\" id=\"S4.T5.17.11.2.2.2.m2.1.1.2.cmml\" xref=\"S4.T5.17.11.2.2.2.m2.1.1.2\">absent</csymbol><apply id=\"S4.T5.17.11.2.2.2.m2.1.1.3.cmml\" xref=\"S4.T5.17.11.2.2.2.m2.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.T5.17.11.2.2.2.m2.1.1.3.1.cmml\" xref=\"S4.T5.17.11.2.2.2.m2.1.1.3\">superscript</csymbol><cn type=\"float\" id=\"S4.T5.17.11.2.2.2.m2.1.1.3.2.cmml\" xref=\"S4.T5.17.11.2.2.2.m2.1.1.3.2\">45.5</cn><ci id=\"S4.T5.17.11.2.2.2.m2.1.1.3.3.cmml\" xref=\"S4.T5.17.11.2.2.2.m2.1.1.3.3\">&#8224;</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.17.11.2.2.2.m2.1c\">/45.5^{\\dagger}</annotation></semantics></math></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.19.13\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.18.12.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">&#13;\n<cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T5.18.12.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Sun et&#160;al.<span id=\"S4.T5.18.12.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib152\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S4.T5.18.12.1.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><sup id=\"S4.T5.18.12.1.4\" class=\"ltx_sup\"><span id=\"S4.T5.18.12.1.4.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">#</span></sup>&#13;\n</td>&#13;\n<td id=\"S4.T5.19.13.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.19.13.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.19.13.3.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S4.T5.19.13.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">ResNet</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.19.13.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.19.13.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>&#13;\n<td id=\"S4.T5.19.13.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.19.13.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.19.13.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.19.13.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.19.13.6.1.1\" class=\"ltx_p\" style=\"width:270.3pt;\"><span id=\"S4.T5.19.13.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A bone-based representation involving body structure information to enhance robustness; bone-length constraints; jointly trained with 2D (MPII) dataset</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.19.13.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.19.13.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.19.13.2.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><math id=\"S4.T5.19.13.2.1.1.m1.1\" class=\"ltx_Math\" alttext=\"48.3\" display=\"inline\"><semantics id=\"S4.T5.19.13.2.1.1.m1.1a\"><mn mathsize=\"80%\" id=\"S4.T5.19.13.2.1.1.m1.1.1\" xref=\"S4.T5.19.13.2.1.1.m1.1.1.cmml\">48.3</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.19.13.2.1.1.m1.1b\"><cn type=\"float\" id=\"S4.T5.19.13.2.1.1.m1.1.1.cmml\" xref=\"S4.T5.19.13.2.1.1.m1.1.1\">48.3</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.19.13.2.1.1.m1.1c\">48.3</annotation></semantics></math></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.20.14\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.20.14.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T5.20.14.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Yang et&#160;al.<span id=\"S4.T5.20.14.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib179\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S4.T5.20.14.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S4.T5.20.14.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.20.14.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.20.14.3.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S4.T5.20.14.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Hourglass</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.20.14.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.20.14.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>&#13;\n<td id=\"S4.T5.20.14.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.20.14.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.20.14.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.20.14.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.20.14.6.1.1\" class=\"ltx_p\" style=\"width:270.3pt;\"><span id=\"S4.T5.20.14.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Adversarial learning for domain adaptation of 2D/3D datasets; adopted generator from </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T5.20.14.6.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Zhou et&#160;al.<span id=\"S4.T5.20.14.6.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib184\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S4.T5.20.14.6.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S4.T5.20.14.6.1.1.5\" class=\"ltx_text\" style=\"font-size:80%;\">; multi-source discriminator with image, pairwise geometric structure and joint location; jointly trained with 2D (MPII) dataset</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.20.14.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.20.14.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.20.14.1.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><math id=\"S4.T5.20.14.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"58.6\" display=\"inline\"><semantics id=\"S4.T5.20.14.1.1.1.m1.1a\"><mn mathsize=\"80%\" id=\"S4.T5.20.14.1.1.1.m1.1.1\" xref=\"S4.T5.20.14.1.1.1.m1.1.1.cmml\">58.6</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.20.14.1.1.1.m1.1b\"><cn type=\"float\" id=\"S4.T5.20.14.1.1.1.m1.1.1.cmml\" xref=\"S4.T5.20.14.1.1.1.m1.1.1\">58.6</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.20.14.1.1.1.m1.1c\">58.6</annotation></semantics></math></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.21.15\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.21.15.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citet\">Pavlakos et&#160;al. <span id=\"S4.T5.21.15.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span><a href=\"#bib.bib124\" title=\"\" class=\"ltx_ref\">2018a</a><span id=\"S4.T5.21.15.2.2.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S4.T5.21.15.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.21.15.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.21.15.3.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S4.T5.21.15.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Hourglass</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.21.15.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.21.15.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>&#13;\n<td id=\"S4.T5.21.15.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.21.15.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.21.15.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.21.15.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.21.15.6.1.1\" class=\"ltx_p\" style=\"width:270.3pt;\"><span id=\"S4.T5.21.15.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Volumetric representation for 3D human pose; additional ordinal depths annotations for human joints; jointly trained with 2D (MPII) and 3D (Humaneva-I) datasets</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.21.15.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.21.15.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.21.15.1.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><math id=\"S4.T5.21.15.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"56.2\" display=\"inline\"><semantics id=\"S4.T5.21.15.1.1.1.m1.1a\"><mn mathsize=\"80%\" id=\"S4.T5.21.15.1.1.1.m1.1.1\" xref=\"S4.T5.21.15.1.1.1.m1.1.1.cmml\">56.2</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.21.15.1.1.1.m1.1b\"><cn type=\"float\" id=\"S4.T5.21.15.1.1.1.m1.1.1.cmml\" xref=\"S4.T5.21.15.1.1.1.m1.1.1\">56.2</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.21.15.1.1.1.m1.1c\">56.2</annotation></semantics></math></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.22.16\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.22.16.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T5.22.16.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Sun et&#160;al.<span id=\"S4.T5.22.16.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib153\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S4.T5.22.16.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S4.T5.22.16.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.22.16.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.22.16.3.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S4.T5.22.16.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Mask R-CNN</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.22.16.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.22.16.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>&#13;\n<td id=\"S4.T5.22.16.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.22.16.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.22.16.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.22.16.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.22.16.6.1.1\" class=\"ltx_p\" style=\"width:270.3pt;\"><span id=\"S4.T5.22.16.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Volumetric representation for 3D human pose; integral operation unifies the heat map representation and joint regression; jointly trained with 2D (MPII) dataset</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.22.16.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.22.16.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.22.16.1.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><math id=\"S4.T5.22.16.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"40.6\" display=\"inline\"><semantics id=\"S4.T5.22.16.1.1.1.m1.1a\"><mn mathsize=\"80%\" id=\"S4.T5.22.16.1.1.1.m1.1.1\" xref=\"S4.T5.22.16.1.1.1.m1.1.1.cmml\">40.6</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.22.16.1.1.1.m1.1b\"><cn type=\"float\" id=\"S4.T5.22.16.1.1.1.m1.1.1.cmml\" xref=\"S4.T5.22.16.1.1.1.m1.1.1\">40.6</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.22.16.1.1.1.m1.1c\">40.6</annotation></semantics></math></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.23.17\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.23.17.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T5.23.17.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Li and Lee<span id=\"S4.T5.23.17.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib85\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S4.T5.23.17.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S4.T5.23.17.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.23.17.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.23.17.3.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S4.T5.23.17.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Hourglass</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.23.17.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.23.17.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>&#13;\n<td id=\"S4.T5.23.17.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.23.17.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.23.17.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.23.17.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.23.17.6.1.1\" class=\"ltx_p\" style=\"width:270.3pt;\"><span id=\"S4.T5.23.17.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Multiple hypotheses of 3D poses are generated from 2D poses; the best one is chosen by 2D reprojections; 2D module is pre-trained with MPII</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.23.17.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.23.17.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.23.17.1.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><math id=\"S4.T5.23.17.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"52.7\" display=\"inline\"><semantics id=\"S4.T5.23.17.1.1.1.m1.1a\"><mn mathsize=\"80%\" id=\"S4.T5.23.17.1.1.1.m1.1.1\" xref=\"S4.T5.23.17.1.1.1.m1.1.1.cmml\">52.7</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.23.17.1.1.1.m1.1b\"><cn type=\"float\" id=\"S4.T5.23.17.1.1.1.m1.1.1.cmml\" xref=\"S4.T5.23.17.1.1.1.m1.1.1\">52.7</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.23.17.1.1.1.m1.1c\">52.7</annotation></semantics></math></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.42.39.3\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.42.39.3.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" colspan=\"6\"><span id=\"S4.T5.42.39.3.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Model-based</span></td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.25.19\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.24.18.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">&#13;\n<cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T5.24.18.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Bogo et&#160;al.<span id=\"S4.T5.24.18.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">2016</a><span id=\"S4.T5.24.18.1.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><sup id=\"S4.T5.24.18.1.4\" class=\"ltx_sup\"><span id=\"S4.T5.24.18.1.4.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">#</span></sup>&#13;\n</td>&#13;\n<td id=\"S4.T5.25.19.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.25.19.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.25.19.3.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S4.T5.25.19.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">DeepCut</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.25.19.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.25.19.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.25.19.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.25.19.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.25.19.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.25.19.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.25.19.6.1.1\" class=\"ltx_p\" style=\"width:270.3pt;\"><span id=\"S4.T5.25.19.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">SMPL model; fit SMPL model to 2D joints by minimizing the distance between 2D joints and projected 3D model joints</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.25.19.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.25.19.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.25.19.2.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><math id=\"S4.T5.25.19.2.1.1.m1.1\" class=\"ltx_Math\" alttext=\"82.3\" display=\"inline\"><semantics id=\"S4.T5.25.19.2.1.1.m1.1a\"><mn mathsize=\"80%\" id=\"S4.T5.25.19.2.1.1.m1.1.1\" xref=\"S4.T5.25.19.2.1.1.m1.1.1.cmml\">82.3</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.25.19.2.1.1.m1.1b\"><cn type=\"float\" id=\"S4.T5.25.19.2.1.1.m1.1.1.cmml\" xref=\"S4.T5.25.19.2.1.1.m1.1.1\">82.3</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.25.19.2.1.1.m1.1c\">82.3</annotation></semantics></math></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.27.21\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.26.20.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">&#13;\n<cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T5.26.20.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Zhou et&#160;al.<span id=\"S4.T5.26.20.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib185\" title=\"\" class=\"ltx_ref\">2016</a><span id=\"S4.T5.26.20.1.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><sup id=\"S4.T5.26.20.1.4\" class=\"ltx_sup\"><span id=\"S4.T5.26.20.1.4.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">#</span></sup>&#13;\n</td>&#13;\n<td id=\"S4.T5.27.21.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.27.21.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.27.21.3.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S4.T5.27.21.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">ResNet</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.27.21.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.27.21.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.27.21.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.27.21.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.27.21.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.27.21.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.27.21.6.1.1\" class=\"ltx_p\" style=\"width:270.3pt;\"><span id=\"S4.T5.27.21.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">kinematic model; embedded a kinematic object model into network for general articulated object pose estimation; orientation and rotational constrains</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.27.21.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.27.21.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.27.21.2.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><math id=\"S4.T5.27.21.2.1.1.m1.1\" class=\"ltx_Math\" alttext=\"107.3\" display=\"inline\"><semantics id=\"S4.T5.27.21.2.1.1.m1.1a\"><mn mathsize=\"80%\" id=\"S4.T5.27.21.2.1.1.m1.1.1\" xref=\"S4.T5.27.21.2.1.1.m1.1.1.cmml\">107.3</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.27.21.2.1.1.m1.1b\"><cn type=\"float\" id=\"S4.T5.27.21.2.1.1.m1.1.1.cmml\" xref=\"S4.T5.27.21.2.1.1.m1.1.1\">107.3</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.27.21.2.1.1.m1.1c\">107.3</annotation></semantics></math></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.29.23\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.28.22.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">&#13;\n<cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T5.28.22.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Mehta et&#160;al.<span id=\"S4.T5.28.22.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib107\" title=\"\" class=\"ltx_ref\">2017c</a><span id=\"S4.T5.28.22.1.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><sup id=\"S4.T5.28.22.1.4\" class=\"ltx_sup\"><span id=\"S4.T5.28.22.1.4.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">#</span></sup>&#13;\n</td>&#13;\n<td id=\"S4.T5.29.23.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.29.23.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.29.23.3.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S4.T5.29.23.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">ResNet</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.29.23.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.29.23.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>&#13;\n<td id=\"S4.T5.29.23.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.29.23.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>&#13;\n<td id=\"S4.T5.29.23.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.29.23.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.29.23.6.1.1\" class=\"ltx_p\" style=\"width:270.3pt;\"><span id=\"S4.T5.29.23.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A real-time pipeline with temporal smooth filter and model-based kinematic skeleton fitting; 2D module is pre-trained with MPII and LSP; process in real-time; provide body height</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.29.23.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.29.23.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.29.23.2.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><math id=\"S4.T5.29.23.2.1.1.m1.1\" class=\"ltx_Math\" alttext=\"80.5\" display=\"inline\"><semantics id=\"S4.T5.29.23.2.1.1.m1.1a\"><mn mathsize=\"80%\" id=\"S4.T5.29.23.2.1.1.m1.1.1\" xref=\"S4.T5.29.23.2.1.1.m1.1.1.cmml\">80.5</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.29.23.2.1.1.m1.1b\"><cn type=\"float\" id=\"S4.T5.29.23.2.1.1.m1.1.1.cmml\" xref=\"S4.T5.29.23.2.1.1.m1.1.1\">80.5</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.29.23.2.1.1.m1.1c\">80.5</annotation></semantics></math></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.42.40.4\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.42.40.4.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T5.42.40.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Tan et&#160;al.<span id=\"S4.T5.42.40.4.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib155\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S4.T5.42.40.4.1.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S4.T5.42.40.4.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.42.40.4.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.42.40.4.2.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S4.T5.42.40.4.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">shallow CNNs</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.42.40.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.42.40.4.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.42.40.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.42.40.4.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.42.40.4.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.42.40.4.5.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.42.40.4.5.1.1\" class=\"ltx_p\" style=\"width:270.3pt;\"><span id=\"S4.T5.42.40.4.5.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">SMPL model; first train a decoder to predict a 2D body silhouette from parameters of SMPL; then train a encoder-decoder network with images and corresponding silhouettes; the trained encoder can predict parameters of SMPL from images</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.42.40.4.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.42.40.4.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.42.40.4.6.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><span id=\"S4.T5.42.40.4.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.30.24\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.30.24.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T5.30.24.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Mehta et&#160;al.<span id=\"S4.T5.30.24.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib104\" title=\"\" class=\"ltx_ref\">2017a</a><span id=\"S4.T5.30.24.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S4.T5.30.24.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.30.24.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.30.24.3.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S4.T5.30.24.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Resnet</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.30.24.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.30.24.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>&#13;\n<td id=\"S4.T5.30.24.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.30.24.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.30.24.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.30.24.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.30.24.6.1.1\" class=\"ltx_p\" style=\"width:270.3pt;\"><span id=\"S4.T5.30.24.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Kinematic model; transfer learning from features learned for 2D pose estimation; 2D pose prediction as auxiliary task; predict relative joint locations following the kinematic tree body model; jointly trained with 2D (MPII and LSP) datasets</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.30.24.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.30.24.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.30.24.1.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><math id=\"S4.T5.30.24.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"74.1\" display=\"inline\"><semantics id=\"S4.T5.30.24.1.1.1.m1.1a\"><mn mathsize=\"80%\" id=\"S4.T5.30.24.1.1.1.m1.1.1\" xref=\"S4.T5.30.24.1.1.1.m1.1.1.cmml\">74.1</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.30.24.1.1.1.m1.1b\"><cn type=\"float\" id=\"S4.T5.30.24.1.1.1.m1.1.1.cmml\" xref=\"S4.T5.30.24.1.1.1.m1.1.1\">74.1</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.30.24.1.1.1.m1.1c\">74.1</annotation></semantics></math></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.31.25\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.31.25.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T5.31.25.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Nie et&#160;al.<span id=\"S4.T5.31.25.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib117\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S4.T5.31.25.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S4.T5.31.25.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.31.25.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.31.25.3.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S4.T5.31.25.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">RMPE + LSTM</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.31.25.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.31.25.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>&#13;\n<td id=\"S4.T5.31.25.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.31.25.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.31.25.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.31.25.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.31.25.6.1.1\" class=\"ltx_p\" style=\"width:270.3pt;\"><span id=\"S4.T5.31.25.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Kinematic model; joint depth estimation from global 2D pose with skeleton-LSTM and local body parts with patch-LSTM; 2D module is pre-trained with MPII</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.31.25.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.31.25.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.31.25.1.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><math id=\"S4.T5.31.25.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"79.5\" display=\"inline\"><semantics id=\"S4.T5.31.25.1.1.1.m1.1a\"><mn mathsize=\"80%\" id=\"S4.T5.31.25.1.1.1.m1.1.1\" xref=\"S4.T5.31.25.1.1.1.m1.1.1.cmml\">79.5</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.31.25.1.1.1.m1.1b\"><cn type=\"float\" id=\"S4.T5.31.25.1.1.1.m1.1.1.cmml\" xref=\"S4.T5.31.25.1.1.1.m1.1.1\">79.5</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.31.25.1.1.1.m1.1c\">79.5</annotation></semantics></math></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.33.27\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.32.26.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">&#13;\n<cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T5.32.26.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Kanazawa et&#160;al.<span id=\"S4.T5.32.26.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib73\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S4.T5.32.26.1.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><sup id=\"S4.T5.32.26.1.4\" class=\"ltx_sup\"><span id=\"S4.T5.32.26.1.4.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">#</span></sup>&#13;\n</td>&#13;\n<td id=\"S4.T5.33.27.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.33.27.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.33.27.3.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S4.T5.33.27.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">ResNet</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.33.27.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.33.27.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>&#13;\n<td id=\"S4.T5.33.27.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.33.27.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.33.27.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.33.27.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.33.27.6.1.1\" class=\"ltx_p\" style=\"width:270.3pt;\"><span id=\"S4.T5.33.27.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">SMPL model; adversarial learning for domain adaptation of 2D images and 3D human body model; propose a framework to learn parameters of SMPL; jointly trained with 2D (LSP, MPII and COCO) datasets; process in real-time</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.33.27.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.33.27.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.33.27.2.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><math id=\"S4.T5.33.27.2.1.1.m1.1\" class=\"ltx_Math\" alttext=\"88.0\" display=\"inline\"><semantics id=\"S4.T5.33.27.2.1.1.m1.1a\"><mn mathsize=\"80%\" id=\"S4.T5.33.27.2.1.1.m1.1.1\" xref=\"S4.T5.33.27.2.1.1.m1.1.1.cmml\">88.0</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.33.27.2.1.1.m1.1b\"><cn type=\"float\" id=\"S4.T5.33.27.2.1.1.m1.1.1.cmml\" xref=\"S4.T5.33.27.2.1.1.m1.1.1\">88.0</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.33.27.2.1.1.m1.1c\">88.0</annotation></semantics></math></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.35.29\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.34.28.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">&#13;\n<cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T5.34.28.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Pavlakos et&#160;al.<span id=\"S4.T5.34.28.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib126\" title=\"\" class=\"ltx_ref\">2018b</a><span id=\"S4.T5.34.28.1.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><sup id=\"S4.T5.34.28.1.4\" class=\"ltx_sup\"><span id=\"S4.T5.34.28.1.4.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">#</span></sup>&#13;\n</td>&#13;\n<td id=\"S4.T5.35.29.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.35.29.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.35.29.3.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S4.T5.35.29.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Hourglass</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.35.29.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.35.29.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>&#13;\n<td id=\"S4.T5.35.29.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.35.29.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.35.29.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.35.29.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.35.29.6.1.1\" class=\"ltx_p\" style=\"width:270.3pt;\"><span id=\"S4.T5.35.29.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">SMPL model; first predict 2D heatmaps of joint and human silhouette; second generate parameters of SMPL; 2D module is trained with MPII and LSP</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.35.29.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.35.29.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.35.29.2.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><math id=\"S4.T5.35.29.2.1.1.m1.1\" class=\"ltx_Math\" alttext=\"75.9\" display=\"inline\"><semantics id=\"S4.T5.35.29.2.1.1.m1.1a\"><mn mathsize=\"80%\" id=\"S4.T5.35.29.2.1.1.m1.1.1\" xref=\"S4.T5.35.29.2.1.1.m1.1.1.cmml\">75.9</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.35.29.2.1.1.m1.1b\"><cn type=\"float\" id=\"S4.T5.35.29.2.1.1.m1.1.1.cmml\" xref=\"S4.T5.35.29.2.1.1.m1.1.1\">75.9</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.35.29.2.1.1.m1.1c\">75.9</annotation></semantics></math></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.37.31\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.36.30.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">&#13;\n<cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T5.36.30.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Omran et&#160;al.<span id=\"S4.T5.36.30.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib120\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S4.T5.36.30.1.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><sup id=\"S4.T5.36.30.1.4\" class=\"ltx_sup\"><span id=\"S4.T5.36.30.1.4.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">#</span></sup>&#13;\n</td>&#13;\n<td id=\"S4.T5.37.31.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.37.31.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.37.31.3.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S4.T5.37.31.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">RefineNet</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.37.31.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.37.31.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.37.31.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.37.31.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.37.31.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.37.31.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.37.31.6.1.1\" class=\"ltx_p\" style=\"width:270.3pt;\"><span id=\"S4.T5.37.31.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">SMPL model; first predict 2D body parts segmentation from the RGB image; second take this segmentation to predict the parameters of SMPL</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.37.31.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.37.31.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.37.31.2.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><math id=\"S4.T5.37.31.2.1.1.m1.1\" class=\"ltx_Math\" alttext=\"59.9\" display=\"inline\"><semantics id=\"S4.T5.37.31.2.1.1.m1.1a\"><mn mathsize=\"80%\" id=\"S4.T5.37.31.2.1.1.m1.1.1\" xref=\"S4.T5.37.31.2.1.1.m1.1.1.cmml\">59.9</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.37.31.2.1.1.m1.1b\"><cn type=\"float\" id=\"S4.T5.37.31.2.1.1.m1.1.1.cmml\" xref=\"S4.T5.37.31.2.1.1.m1.1.1\">59.9</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.37.31.2.1.1.m1.1c\">59.9</annotation></semantics></math></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.38.32\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.38.32.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T5.38.32.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Varol et&#160;al.<span id=\"S4.T5.38.32.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib167\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S4.T5.38.32.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S4.T5.38.32.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.38.32.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.38.32.3.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S4.T5.38.32.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Hourglass</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.38.32.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.38.32.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>&#13;\n<td id=\"S4.T5.38.32.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.38.32.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.38.32.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.38.32.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.38.32.6.1.1\" class=\"ltx_p\" style=\"width:270.3pt;\"><span id=\"S4.T5.38.32.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">SMPL model; first predict 2D pose and 2D body parts segmentation; second predict 3D pose; finally predict volumetric shape to fit SMPL model; 2D modules are trained with MPII and SURREAL</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.38.32.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.38.32.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.38.32.1.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><math id=\"S4.T5.38.32.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"49.0\" display=\"inline\"><semantics id=\"S4.T5.38.32.1.1.1.m1.1a\"><mn mathsize=\"80%\" id=\"S4.T5.38.32.1.1.1.m1.1.1\" xref=\"S4.T5.38.32.1.1.1.m1.1.1.cmml\">49.0</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.38.32.1.1.1.m1.1b\"><cn type=\"float\" id=\"S4.T5.38.32.1.1.1.m1.1.1.cmml\" xref=\"S4.T5.38.32.1.1.1.m1.1.1\">49.0</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.38.32.1.1.1.m1.1c\">49.0</annotation></semantics></math></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.41.35\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.39.33.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">&#13;\n<cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T5.39.33.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Arnab et&#160;al.<span id=\"S4.T5.39.33.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S4.T5.39.33.1.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><sup id=\"S4.T5.39.33.1.4\" class=\"ltx_sup\"><span id=\"S4.T5.39.33.1.4.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">#</span></sup>&#13;\n</td>&#13;\n<td id=\"S4.T5.41.35.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.41.35.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.41.35.4.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S4.T5.41.35.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">ResNet</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.41.35.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.41.35.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>&#13;\n<td id=\"S4.T5.41.35.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.41.35.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>&#13;\n<td id=\"S4.T5.41.35.7\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.41.35.7.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.41.35.7.1.1\" class=\"ltx_p\" style=\"width:270.3pt;\"><span id=\"S4.T5.41.35.7.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">SMPL model; 2D keypoints, SMPL and camera parameters estimation; off-line bundle adjustment with temporal constraints; 2D module is trained with COCO</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.41.35.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.41.35.3.2\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.41.35.3.2.2\" class=\"ltx_p\" style=\"width:22.8pt;\"><math id=\"S4.T5.40.34.2.1.1.m1.1\" class=\"ltx_Math\" alttext=\"77.8\" display=\"inline\"><semantics id=\"S4.T5.40.34.2.1.1.m1.1a\"><mn mathsize=\"80%\" id=\"S4.T5.40.34.2.1.1.m1.1.1\" xref=\"S4.T5.40.34.2.1.1.m1.1.1.cmml\">77.8</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.40.34.2.1.1.m1.1b\"><cn type=\"float\" id=\"S4.T5.40.34.2.1.1.m1.1.1.cmml\" xref=\"S4.T5.40.34.2.1.1.m1.1.1\">77.8</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.40.34.2.1.1.m1.1c\">77.8</annotation></semantics></math><span id=\"S4.T5.41.35.3.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S4.T5.41.35.3.2.2.m2.1\" class=\"ltx_Math\" alttext=\"/63.3^{\\dagger}\" display=\"inline\"><semantics id=\"S4.T5.41.35.3.2.2.m2.1a\"><mrow id=\"S4.T5.41.35.3.2.2.m2.1.1\" xref=\"S4.T5.41.35.3.2.2.m2.1.1.cmml\"><mi id=\"S4.T5.41.35.3.2.2.m2.1.1.2\" xref=\"S4.T5.41.35.3.2.2.m2.1.1.2.cmml\"/><mo maxsize=\"80%\" minsize=\"80%\" stretchy=\"true\" symmetric=\"true\" id=\"S4.T5.41.35.3.2.2.m2.1.1.1\" xref=\"S4.T5.41.35.3.2.2.m2.1.1.1.cmml\">/</mo><msup id=\"S4.T5.41.35.3.2.2.m2.1.1.3\" xref=\"S4.T5.41.35.3.2.2.m2.1.1.3.cmml\"><mn mathsize=\"80%\" id=\"S4.T5.41.35.3.2.2.m2.1.1.3.2\" xref=\"S4.T5.41.35.3.2.2.m2.1.1.3.2.cmml\">63.3</mn><mo mathsize=\"80%\" id=\"S4.T5.41.35.3.2.2.m2.1.1.3.3\" xref=\"S4.T5.41.35.3.2.2.m2.1.1.3.3.cmml\">&#8224;</mo></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.41.35.3.2.2.m2.1b\"><apply id=\"S4.T5.41.35.3.2.2.m2.1.1.cmml\" xref=\"S4.T5.41.35.3.2.2.m2.1.1\"><divide id=\"S4.T5.41.35.3.2.2.m2.1.1.1.cmml\" xref=\"S4.T5.41.35.3.2.2.m2.1.1.1\"/><csymbol cd=\"latexml\" id=\"S4.T5.41.35.3.2.2.m2.1.1.2.cmml\" xref=\"S4.T5.41.35.3.2.2.m2.1.1.2\">absent</csymbol><apply id=\"S4.T5.41.35.3.2.2.m2.1.1.3.cmml\" xref=\"S4.T5.41.35.3.2.2.m2.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.T5.41.35.3.2.2.m2.1.1.3.1.cmml\" xref=\"S4.T5.41.35.3.2.2.m2.1.1.3\">superscript</csymbol><cn type=\"float\" id=\"S4.T5.41.35.3.2.2.m2.1.1.3.2.cmml\" xref=\"S4.T5.41.35.3.2.2.m2.1.1.3.2\">63.3</cn><ci id=\"S4.T5.41.35.3.2.2.m2.1.1.3.3.cmml\" xref=\"S4.T5.41.35.3.2.2.m2.1.1.3.3\">&#8224;</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.41.35.3.2.2.m2.1c\">/63.3^{\\dagger}</annotation></semantics></math></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.42.36\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.42.36.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T5.42.36.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Tome et&#160;al.<span id=\"S4.T5.42.36.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib162\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S4.T5.42.36.2.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S4.T5.42.36.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.42.36.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.42.36.3.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S4.T5.42.36.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">CPM</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.42.36.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.42.36.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>&#13;\n<td id=\"S4.T5.42.36.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.42.36.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.42.36.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.42.36.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.42.36.6.1.1\" class=\"ltx_p\" style=\"width:270.3pt;\"><span id=\"S4.T5.42.36.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Pre-trained probabilistic 3D pose model; 3D lifting and projection by probabilistic model within the CPM-like network; 2D module is pre-trained with MPII; process in real-time</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.42.36.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.42.36.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.42.36.1.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><math id=\"S4.T5.42.36.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"88.4\" display=\"inline\"><semantics id=\"S4.T5.42.36.1.1.1.m1.1a\"><mn mathsize=\"80%\" id=\"S4.T5.42.36.1.1.1.m1.1.1\" xref=\"S4.T5.42.36.1.1.1.m1.1.1.cmml\">88.4</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.42.36.1.1.1.m1.1b\"><cn type=\"float\" id=\"S4.T5.42.36.1.1.1.m1.1.1.cmml\" xref=\"S4.T5.42.36.1.1.1.m1.1.1\">88.4</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.42.36.1.1.1.m1.1c\">88.4</annotation></semantics></math></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.42.41.5\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.42.41.5.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T5.42.41.5.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Rhodin et&#160;al.<span id=\"S4.T5.42.41.5.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib139\" title=\"\" class=\"ltx_ref\">2018a</a><span id=\"S4.T5.42.41.5.1.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></td>&#13;\n<td id=\"S4.T5.42.41.5.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.42.41.5.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.42.41.5.2.1.1\" class=\"ltx_p\" style=\"width:48.4pt;\"><span id=\"S4.T5.42.41.5.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Hourglass</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.42.41.5.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T5.42.41.5.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.42.41.5.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T5.42.41.5.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>&#13;\n<td id=\"S4.T5.42.41.5.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.42.41.5.5.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.42.41.5.5.1.1\" class=\"ltx_p\" style=\"width:270.3pt;\"><span id=\"S4.T5.42.41.5.5.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A latent variable body model learned from multi-view images; an encoder-decoder to predict a novel view image from a given one; the pre-trained encoder with additional shallow layers to predict 3D poses from images</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T5.42.41.5.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T5.42.41.5.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T5.42.41.5.6.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><span id=\"S4.T5.42.41.5.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "(Li and Chan, 2014) Li and Chan (2014)\r\n\r\nLi, S., Chan, A.B., 2014.\r\n\r\n\r\n3d human pose estimation from monocular images with\r\ndeep convolutional neural network, in: Proc. Asian\r\nConference on Computer Vision, Springer. pp.\r\n332–347.",
            "(Li et al., 2015b) Li et al. (2015b)\r\n\r\nLi, S., Zhang, W., Chan,\r\nA.B., 2015b.\r\n\r\n\r\nMaximum-margin structured learning with deep networks\r\nfor 3d human pose estimation, in: Proc. IEEE\r\nInternational Conference on Computer Vision, pp.\r\n2848–2856.",
            "(Tekin et al., 2016) Tekin et al. (2016)\r\n\r\nTekin, B., Katircioglu, I.,\r\nSalzmann, M., Lepetit, V.,\r\nFua, P., 2016.\r\n\r\n\r\nStructured prediction of 3d human pose with deep\r\nneural networks.\r\n\r\n\r\narXiv preprint arXiv:1605.05180 .",
            "(Tekin et al., 2017) Tekin et al. (2017)\r\n\r\nTekin, B., Marquez Neila, P.,\r\nSalzmann, M., Fua, P.,\r\n2017.\r\n\r\n\r\nLearning to fuse 2d and 3d image cues for monocular\r\nbody pose estimation, in: Proc. IEEE International\r\nConference on Computer Vision, pp. 3941–3950.",
            "(Chen and Ramanan, 2017) Chen and Ramanan (2017)\r\n\r\nChen, C.H., Ramanan, D.,\r\n2017.\r\n\r\n\r\n3d human pose estimation= 2d pose estimation+\r\nmatching, in: Proc. IEEE Conference on Computer Vision\r\nand Pattern Recognition, pp. 7035–7043.",
            "(Moreno-Noguer, 2017) Moreno-Noguer (2017)\r\n\r\nMoreno-Noguer, F., 2017.\r\n\r\n\r\n3d human pose estimation from a single image via\r\ndistance matrix regression, in: Proc. IEEE Conference on\r\nComputer Vision and Pattern Recognition, pp. 1561–1570.",
            "(Pavlakos et al., 2017) Pavlakos et al. (2017)\r\n\r\nPavlakos, G., Zhou, X.,\r\nDerpanis, K.G., Daniilidis, K.,\r\n2017.\r\n\r\n\r\nCoarse-to-fine volumetric prediction for single-image\r\n3d human pose, in: Proc. IEEE Conference on Computer\r\nVision and Pattern Recognition, pp. 1263–1272.",
            "(Zhou et al., 2017) Zhou et al. (2017)\r\n\r\nZhou, X., Huang, Q., Sun,\r\nX., Xue, X., Wei, Y.,\r\n2017.\r\n\r\n\r\nTowards 3d human pose estimation in the wild: a\r\nweakly-supervised approach, in: Proc. IEEE International\r\nConference on Computer Vision, pp. 398–407.",
            "(Martinez et al., 2017) Martinez et al. (2017)\r\n\r\nMartinez, J., Hossain, R.,\r\nRomero, J., Little, J.J.,\r\n2017.\r\n\r\n\r\nA simple yet effective baseline for 3d human pose\r\nestimation, in: Proc. IEEE International Conference on\r\nComputer Vision, pp. 2640–2649.",
            "(Sun et al., 2017) Sun et al. (2017)\r\n\r\nSun, X., Shang, J., Liang,\r\nS., Wei, Y., 2017.\r\n\r\n\r\nCompositional human pose regression, in:\r\nProc. IEEE International Conference on Computer Vision,\r\np. 7.",
            "(Yang et al., 2018) Yang et al. (2018)\r\n\r\nYang, W., Ouyang, W.,\r\nWang, X., Ren, J., Li,\r\nH., Wang, X., 2018.\r\n\r\n\r\n3d human pose estimation in the wild by adversarial\r\nlearning, in: Proc. IEEE Conference on Computer Vision\r\nand Pattern Recognition, pp. 5255–5264.",
            "(Zhou et al., 2017) Zhou et al. (2017)\r\n\r\nZhou, X., Huang, Q., Sun,\r\nX., Xue, X., Wei, Y.,\r\n2017.\r\n\r\n\r\nTowards 3d human pose estimation in the wild: a\r\nweakly-supervised approach, in: Proc. IEEE International\r\nConference on Computer Vision, pp. 398–407.",
            "Pavlakos et al. (2018a) Pavlakos et al. (2018a)\r\n\r\nPavlakos, G., Zhou, X.,\r\nDaniilidis, K., 2018a.\r\n\r\n\r\nOrdinal depth supervision for 3d human pose\r\nestimation.\r\n\r\n\r\narXiv preprint arXiv:1805.04095 .",
            "(Sun et al., 2018) Sun et al. (2018)\r\n\r\nSun, X., Xiao, B., Wei,\r\nF., Liang, S., Wei, Y.,\r\n2018.\r\n\r\n\r\nIntegral human pose regression, in:\r\nProc. European Conference on Computer Vision, pp.\r\n529–545.",
            "(Li and Lee, 2019) Li and Lee (2019)\r\n\r\nLi, C., Lee, G.H., 2019.\r\n\r\n\r\nGenerating multiple hypotheses for 3d human pose\r\nestimation with mixture density network, in: Proc. IEEE\r\nConference on Computer Vision and Pattern Recognition, pp.\r\n9887–9895.",
            "(Bogo et al., 2016) Bogo et al. (2016)\r\n\r\nBogo, F., Kanazawa, A.,\r\nLassner, C., Gehler, P.,\r\nRomero, J., Black, M.J.,\r\n2016.\r\n\r\n\r\nKeep it smpl: Automatic estimation of 3d human pose\r\nand shape from a single image, in: Proc. European\r\nConference on Computer Vision, Springer. pp.\r\n561–578.",
            "(Zhou et al., 2016) Zhou et al. (2016)\r\n\r\nZhou, X., Sun, X., Zhang,\r\nW., Liang, S., Wei, Y.,\r\n2016.\r\n\r\n\r\nDeep kinematic pose regression, in:\r\nProc. European Conference on Computer Vision,\r\nSpringer. pp. 186–201.",
            "(Mehta et al., 2017c) Mehta et al. (2017c)\r\n\r\nMehta, D., Sridhar, S.,\r\nSotnychenko, O., Rhodin, H.,\r\nShafiei, M., Seidel, H.P.,\r\nXu, W., Casas, D.,\r\nTheobalt, C., 2017c.\r\n\r\n\r\nVnect: Real-time 3d human pose estimation with a\r\nsingle rgb camera.\r\n\r\n\r\nACM Transactions on Graphics 36,\r\n44.",
            "(Tan et al., 2017) Tan et al. (2017)\r\n\r\nTan, J., Budvytis, I.,\r\nCipolla, R., 2017.\r\n\r\n\r\nIndirect deep structured learning for 3d human body\r\nshape and pose prediction, in: Proc. British Machine\r\nVision Conference.",
            "(Mehta et al., 2017a) Mehta et al. (2017a)\r\n\r\nMehta, D., Rhodin, H.,\r\nCasas, D., Fua, P.,\r\nSotnychenko, O., Xu, W.,\r\nTheobalt, C., 2017a.\r\n\r\n\r\nMonocular 3d human pose estimation in the wild using\r\nimproved cnn supervision, in: Proc. IEEE International\r\nConference on 3D Vision, IEEE. pp.\r\n506–516.",
            "(Nie et al., 2017) Nie et al. (2017)\r\n\r\nNie, B.X., Wei, P., Zhu,\r\nS.C., 2017.\r\n\r\n\r\nMonocular 3d human pose estimation by predicting\r\ndepth on joints, in: Proc. IEEE International Conference\r\non Computer Vision, pp. 3447–3455.",
            "(Kanazawa et al., 2018) Kanazawa et al. (2018)\r\n\r\nKanazawa, A., Black, M.J.,\r\nJacobs, D.W., Malik, J.,\r\n2018.\r\n\r\n\r\nEnd-to-end recovery of human shape and pose, in:\r\nProc. IEEE Conference on Computer Vision and Pattern\r\nRecognition, pp. 7122–7131.",
            "(Pavlakos et al., 2018b) Pavlakos et al. (2018b)\r\n\r\nPavlakos, G., Zhu, L.,\r\nZhou, X., Daniilidis, K.,\r\n2018b.\r\n\r\n\r\nLearning to estimate 3d human pose and shape from a\r\nsingle color image.\r\n\r\n\r\narXiv preprint arXiv:1805.04092 .",
            "(Omran et al., 2018) Omran et al. (2018)\r\n\r\nOmran, M., Lassner, C.,\r\nPons-Moll, G., Gehler, P.,\r\nSchiele, B., 2018.\r\n\r\n\r\nNeural body fitting: Unifying deep learning and model\r\nbased human pose and shape estimation, in: Proc. IEEE\r\nInternational Conference on 3D Vision, IEEE. pp.\r\n484–494.",
            "(Varol et al., 2018) Varol et al. (2018)\r\n\r\nVarol, G., Ceylan, D.,\r\nRussell, B., Yang, J.,\r\nYumer, E., Laptev, I.,\r\nSchmid, C., 2018.\r\n\r\n\r\nBodynet: Volumetric inference of 3d human body\r\nshapes.\r\n\r\n\r\narXiv preprint arXiv:1804.04875 .",
            "(Arnab et al., 2019) Arnab et al. (2019)\r\n\r\nArnab, A., Doersch, C.,\r\nZisserman, A., 2019.\r\n\r\n\r\nExploiting temporal context for 3d human pose\r\nestimation in the wild, in: Proc. IEEE Conference on\r\nComputer Vision and Pattern Recognition, pp. 3395–3404.",
            "(Tome et al., 2017) Tome et al. (2017)\r\n\r\nTome, D., Russell, C.,\r\nAgapito, L., 2017.\r\n\r\n\r\nLifting from the deep: Convolutional 3d pose\r\nestimation from a single image, pp. 2500–2509.",
            "(Rhodin et al., 2018a) Rhodin et al. (2018a)\r\n\r\nRhodin, H., Salzmann, M.,\r\nFua, P., 2018a.\r\n\r\n\r\nUnsupervised geometry-aware representation for 3d\r\nhuman pose estimation.\r\n\r\n\r\narXiv:1804.01110 ."
        ],
        "references": [
            "Compared to 2D HPE, 3D HPE is more challenging since it needs to predict the depth information of body joints. In addition, the training data for 3D HPE are not easy to obtain as 2D HPE. Most existing datasets are obtained under constrained environments with limited generalizability. For single person pose estimation, the bounding box of the person in the image is normally provided, and hence it is not necessary to combine the process of person detection. In this section, we divide the methods of 3D single person pose estimation into model-free and model-based categories and summarize the recent work in Table 5. The last column of Table 5 is the comparisons of Mean Per Joint Position Error (MPJPE) in millimeter on Human3.6M dataset under protocol #1. More details of datasets and evaluation metrics are described in Section 5."
        ]
    },
    "id_table_6": {
        "caption": "Table 6: Summary of 3D multi-person pose estimation methods.",
        "table": [
            "<table id=\"S4.T6.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T6.3.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T6.3.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S4.T6.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Methods</span></th>&#13;\n<th id=\"S4.T6.3.1.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T6.3.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T6.3.1.1.2.1.1\" class=\"ltx_p\" style=\"width:54.1pt;\"><span id=\"S4.T6.3.1.1.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Network type</span></span>&#13;\n</span>&#13;\n</th>&#13;\n<th id=\"S4.T6.3.1.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T6.3.1.1.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T6.3.1.1.3.1.1\" class=\"ltx_p\" style=\"width:327.2pt;\"><span id=\"S4.T6.3.1.1.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Highlights</span></span>&#13;\n</span>&#13;\n</th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T6.3.2.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T6.3.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T6.3.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Mehta et&#160;al.<span id=\"S4.T6.3.2.1.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib106\" title=\"\" class=\"ltx_ref\">2017b</a><span id=\"S4.T6.3.2.1.1.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></th>&#13;\n<td id=\"S4.T6.3.2.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T6.3.2.1.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T6.3.2.1.2.1.1\" class=\"ltx_p\" style=\"width:54.1pt;\"><span id=\"S4.T6.3.2.1.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">ResNet</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T6.3.2.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T6.3.2.1.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T6.3.2.1.3.1.1\" class=\"ltx_p\" style=\"width:327.2pt;\"><span id=\"S4.T6.3.2.1.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Propose an occlusion-robust pose-maps (ORPM) for full-body pose inference even under (self-)occlusions; combine 2D pose and part affinity fields to infer person instances</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T6.3.3.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T6.3.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T6.3.3.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Rogez et&#160;al.<span id=\"S4.T6.3.3.2.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib141\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S4.T6.3.3.2.1.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></th>&#13;\n<td id=\"S4.T6.3.3.2.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T6.3.3.2.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T6.3.3.2.2.1.1\" class=\"ltx_p\" style=\"width:54.1pt;\">&#13;\n<span id=\"S4.T6.3.3.2.2.1.1.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<span id=\"S4.T6.3.3.2.2.1.1.1.1\" class=\"ltx_tr\">&#13;\n<span id=\"S4.T6.3.3.2.2.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T6.3.3.2.2.1.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Faster R-CNN</span></span></span>&#13;\n<span id=\"S4.T6.3.3.2.2.1.1.1.2\" class=\"ltx_tr\">&#13;\n<span id=\"S4.T6.3.3.2.2.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T6.3.3.2.2.1.1.1.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">+ VGG-16</span></span></span>&#13;\n</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T6.3.3.2.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T6.3.3.2.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T6.3.3.2.3.1.1\" class=\"ltx_p\" style=\"width:327.2pt;\"><span id=\"S4.T6.3.3.2.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Localize human bounding boxes with Faster R-CNN; classify the closest anchor-pose for each proposal; regress anchor-pose to get final pose</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T6.3.4.3\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T6.3.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T6.3.4.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Zanfir et&#160;al.<span id=\"S4.T6.3.4.3.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib181\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S4.T6.3.4.3.1.3.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></th>&#13;\n<td id=\"S4.T6.3.4.3.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T6.3.4.3.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T6.3.4.3.2.1.1\" class=\"ltx_p\" style=\"width:54.1pt;\"><span id=\"S4.T6.3.4.3.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">DMHS</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T6.3.4.3.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T6.3.4.3.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T6.3.4.3.3.1.1\" class=\"ltx_p\" style=\"width:327.2pt;\"><span id=\"S4.T6.3.4.3.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Feed forward process of body parts semantic segmentation and 3d pose estimates; feed backward process of refining pose and shape parameters of a body model SMPL</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T6.3.5.4\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T6.3.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citet\">Mehta et&#160;al. <span id=\"S4.T6.3.5.4.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span><a href=\"#bib.bib105\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S4.T6.3.5.4.1.2.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></th>&#13;\n<td id=\"S4.T6.3.5.4.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T6.3.5.4.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T6.3.5.4.2.1.1\" class=\"ltx_p\" style=\"width:54.1pt;\"><span id=\"S4.T6.3.5.4.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">SelecSLS Net</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S4.T6.3.5.4.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">&#13;\n<span id=\"S4.T6.3.5.4.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S4.T6.3.5.4.3.1.1\" class=\"ltx_p\" style=\"width:327.2pt;\"><span id=\"S4.T6.3.5.4.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Real-time; a new CNN architecture that uses selective long and short range skip connections; 2D and 3D pose features prediction along with identity assignments for all visible joints of all individuals; complete 3D pose reconstruction including occluded joints; temporal stability refinement and kinematic skeleton fitting.</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "(Mehta et al., 2017b) Mehta et al. (2017b)\r\n\r\nMehta, D., Sotnychenko, O.,\r\nMueller, F., Xu, W.,\r\nSridhar, S., Pons-Moll, G.,\r\nTheobalt, C., 2017b.\r\n\r\n\r\nSingle-shot multi-person 3d body pose estimation from\r\nmonocular rgb input.\r\n\r\n\r\narXiv preprint arXiv:1712.03453 .",
            "(Rogez et al., 2017) Rogez et al. (2017)\r\n\r\nRogez, G., Weinzaepfel, P.,\r\nSchmid, C., 2017.\r\n\r\n\r\nLcr-net: Localization-classification-regression for\r\nhuman pose, in: Proc. IEEE Conference on Computer Vision\r\nand Pattern Recognition, pp. 3433–3441.",
            "(Zanfir et al., 2018) Zanfir et al. (2018)\r\n\r\nZanfir, A., Marinoiu, E.,\r\nSminchisescu, C., 2018.\r\n\r\n\r\nMonocular 3d pose and shape estimation of multiple\r\npeople in natural scenes-the importance of multiple scene constraints, in:\r\nProc. IEEE Conference on Computer Vision and Pattern\r\nRecognition, pp. 2148–2157.",
            "Mehta et al. (2019) Mehta et al. (2019)\r\n\r\nMehta, D., Sotnychenko, O.,\r\nMueller, F., Xu, W.,\r\nElgharib, M., Fua, P.,\r\nSeidel, H.P., Rhodin, H.,\r\nPons-Moll, G., Theobalt, C.,\r\n2019.\r\n\r\n\r\nXnect: Real-time multi-person 3d human pose\r\nestimation with a single rgb camera.\r\n\r\n\r\narXiv:1907.00837 ."
        ],
        "references": [
            "The achievements of monocular 3D multi-person pose estimation are based on 3D single person pose estimation and other deep learning methods. This research field is pretty new and only a few methods are proposed. Table 6 summarizes these methods."
        ]
    },
    "id_table_7": {
        "caption": "Table 7: Popular 2D databases for human pose estimation. Selected example images with annotations are shown in Fig. 5. Here Jnt. indicates the number of joints",
        "table": [
            "<table id=\"S5.T7.24\" class=\"ltx_tabular ltx_centering ltx_align_middle\">&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S5.T7.24.25.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T7.24.25.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T7.24.25.1.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.24.25.1.1.1.1\" class=\"ltx_p\" style=\"width:31.3pt;\"><span id=\"S5.T7.24.25.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Dataset</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.24.25.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T7.24.25.1.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.24.25.1.2.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\"><span id=\"S5.T7.24.25.1.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Single/</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.24.25.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S5.T7.24.25.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Jnt.</span></td>&#13;\n<td id=\"S5.T7.24.25.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\"><span id=\"S5.T7.24.25.1.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">Number of images/videos</span></td>&#13;\n<td id=\"S5.T7.24.25.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T7.24.25.1.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">Evaluation</span></td>&#13;\n<td id=\"S5.T7.24.25.1.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" rowspan=\"2\">&#13;\n<span id=\"S5.T7.24.25.1.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.24.25.1.6.1.1\" class=\"ltx_p\" style=\"width:216.2pt;\"><span id=\"S5.T7.24.25.1.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Highlights</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T7.24.26.2\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T7.24.26.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r\">&#13;\n<span id=\"S5.T7.24.26.2.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.24.26.2.1.1.1\" class=\"ltx_p\" style=\"width:31.3pt;\"><span id=\"S5.T7.24.26.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">name</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.24.26.2.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">&#13;\n<span id=\"S5.T7.24.26.2.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.24.26.2.2.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\"><span id=\"S5.T7.24.26.2.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Multiple</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.24.26.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T7.24.26.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Train</span></td>&#13;\n<td id=\"S5.T7.24.26.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T7.24.26.2.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">Val</span></td>&#13;\n<td id=\"S5.T7.24.26.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T7.24.26.2.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">Test</span></td>&#13;\n<td id=\"S5.T7.24.26.2.6\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T7.24.26.2.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">protocol</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T7.24.27.3\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T7.24.27.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" colspan=\"8\"><span id=\"S5.T7.24.27.3.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Image-based</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T7.2.2\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T7.2.2.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\">&#13;\n<span id=\"S5.T7.2.2.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.2.2.3.1.1\" class=\"ltx_p\" style=\"width:31.3pt;\"><span id=\"S5.T7.2.2.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\">FLIC</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.2.2.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\" rowspan=\"3\">&#13;\n<span id=\"S5.T7.2.2.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.2.2.4.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\"><span id=\"S5.T7.2.2.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-19.9pt;\">single</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\" rowspan=\"3\"><span id=\"S5.T7.2.2.5.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-19.9pt;\">10</span></td>&#13;\n<td id=\"S5.T7.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\"><span id=\"S5.T7.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\"><math id=\"S5.T7.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T7.1.1.1.1.m1.1a\"><mo id=\"S5.T7.1.1.1.1.m1.1.1\" xref=\"S5.T7.1.1.1.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.1.1.1.1.m1.1b\"><approx id=\"S5.T7.1.1.1.1.m1.1.1.cmml\" xref=\"S5.T7.1.1.1.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.1.1.1.1.m1.1c\">\\approx</annotation></semantics></math>5k</span></td>&#13;\n<td id=\"S5.T7.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\"><span id=\"S5.T7.2.2.6.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\">0</span></td>&#13;\n<td id=\"S5.T7.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\"><span id=\"S5.T7.2.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\"><math id=\"S5.T7.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T7.2.2.2.1.m1.1a\"><mo id=\"S5.T7.2.2.2.1.m1.1.1\" xref=\"S5.T7.2.2.2.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.2.2.2.1.m1.1b\"><approx id=\"S5.T7.2.2.2.1.m1.1.1.cmml\" xref=\"S5.T7.2.2.2.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.2.2.2.1.m1.1c\">\\approx</annotation></semantics></math>1k</span></td>&#13;\n<td id=\"S5.T7.2.2.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\" rowspan=\"3\"><span id=\"S5.T7.2.2.7.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-22.8pt;\">PCP&amp;PCK</span></td>&#13;\n<td id=\"S5.T7.2.2.8\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\" rowspan=\"3\">&#13;\n<span id=\"S5.T7.2.2.8.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.2.2.8.1.1\" class=\"ltx_p\" style=\"width:216.2pt;\"><span id=\"S5.T7.2.2.8.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Upper body poses; Sampled from movies; FLIC-full is complete version <cite class=\"ltx_cite ltx_citemacro_citep\">(Sapp and Taskar, <a href=\"#bib.bib143\" title=\"\" class=\"ltx_ref\">2013</a>)</cite>; FLIC-plus is cleaned version <cite class=\"ltx_cite ltx_citemacro_citep\">(Tompson et&#160;al., <a href=\"#bib.bib164\" title=\"\" class=\"ltx_ref\">2014</a>)</cite>; FLIC is a simple version with no difficult poses.</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T7.3.3\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T7.3.3.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\">&#13;\n<span id=\"S5.T7.3.3.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.3.3.2.1.1\" class=\"ltx_p\" style=\"width:31.3pt;\"><span id=\"S5.T7.3.3.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\">&#13;\n<span id=\"S5.T7.3.3.2.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<span id=\"S5.T7.3.3.2.1.1.1.1.1\" class=\"ltx_tr\">&#13;\n<span id=\"S5.T7.3.3.2.1.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">FLIC-full</span></span>&#13;\n</span></span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\"><span id=\"S5.T7.3.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\"><math id=\"S5.T7.3.3.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T7.3.3.1.1.m1.1a\"><mo id=\"S5.T7.3.3.1.1.m1.1.1\" xref=\"S5.T7.3.3.1.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.3.3.1.1.m1.1b\"><approx id=\"S5.T7.3.3.1.1.m1.1.1.cmml\" xref=\"S5.T7.3.3.1.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.3.3.1.1.m1.1c\">\\approx</annotation></semantics></math>20k</span></td>&#13;\n<td id=\"S5.T7.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\"><span id=\"S5.T7.3.3.3.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\">0</span></td>&#13;\n<td id=\"S5.T7.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\"><span id=\"S5.T7.3.3.4.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\">0</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T7.4.4\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T7.4.4.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\">&#13;\n<span id=\"S5.T7.4.4.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.4.4.2.1.1\" class=\"ltx_p\" style=\"width:31.3pt;\"><span id=\"S5.T7.4.4.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\">&#13;\n<span id=\"S5.T7.4.4.2.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<span id=\"S5.T7.4.4.2.1.1.1.1.1\" class=\"ltx_tr\">&#13;\n<span id=\"S5.T7.4.4.2.1.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">FLIC-plus</span></span>&#13;\n</span></span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\"><span id=\"S5.T7.4.4.1.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\"><math id=\"S5.T7.4.4.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T7.4.4.1.1.m1.1a\"><mo id=\"S5.T7.4.4.1.1.m1.1.1\" xref=\"S5.T7.4.4.1.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.4.4.1.1.m1.1b\"><approx id=\"S5.T7.4.4.1.1.m1.1.1.cmml\" xref=\"S5.T7.4.4.1.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.4.4.1.1.m1.1c\">\\approx</annotation></semantics></math>17k</span></td>&#13;\n<td id=\"S5.T7.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\"><span id=\"S5.T7.4.4.3.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\">0</span></td>&#13;\n<td id=\"S5.T7.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\"><span id=\"S5.T7.4.4.4.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\">0</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T7.6.6\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T7.6.6.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\">&#13;\n<span id=\"S5.T7.6.6.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.6.6.3.1.1\" class=\"ltx_p\" style=\"width:31.3pt;\"><span id=\"S5.T7.6.6.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\">LSP</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.6.6.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\" rowspan=\"2\">&#13;\n<span id=\"S5.T7.6.6.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.6.6.4.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\"><span id=\"S5.T7.6.6.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-19.9pt;\">single</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.6.6.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\" rowspan=\"2\"><span id=\"S5.T7.6.6.5.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-19.9pt;\">14</span></td>&#13;\n<td id=\"S5.T7.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\"><span id=\"S5.T7.5.5.1.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\"><math id=\"S5.T7.5.5.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T7.5.5.1.1.m1.1a\"><mo id=\"S5.T7.5.5.1.1.m1.1.1\" xref=\"S5.T7.5.5.1.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.5.5.1.1.m1.1b\"><approx id=\"S5.T7.5.5.1.1.m1.1.1.cmml\" xref=\"S5.T7.5.5.1.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.5.5.1.1.m1.1c\">\\approx</annotation></semantics></math>1k</span></td>&#13;\n<td id=\"S5.T7.6.6.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\"><span id=\"S5.T7.6.6.6.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\">0</span></td>&#13;\n<td id=\"S5.T7.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\"><span id=\"S5.T7.6.6.2.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\"><math id=\"S5.T7.6.6.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T7.6.6.2.1.m1.1a\"><mo id=\"S5.T7.6.6.2.1.m1.1.1\" xref=\"S5.T7.6.6.2.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.6.6.2.1.m1.1b\"><approx id=\"S5.T7.6.6.2.1.m1.1.1.cmml\" xref=\"S5.T7.6.6.2.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.6.6.2.1.m1.1c\">\\approx</annotation></semantics></math>1k</span></td>&#13;\n<td id=\"S5.T7.6.6.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\" rowspan=\"2\"><span id=\"S5.T7.6.6.7.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-19.9pt;\">PCP</span></td>&#13;\n<td id=\"S5.T7.6.6.8\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\" rowspan=\"2\">&#13;\n<span id=\"S5.T7.6.6.8.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.6.6.8.1.1\" class=\"ltx_p\" style=\"width:216.2pt;\"><span id=\"S5.T7.6.6.8.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">full-body poses; From Flickr with 8 sports tags <cite class=\"ltx_cite ltx_citemacro_citep\">(Johnson and Everingham, <a href=\"#bib.bib68\" title=\"\" class=\"ltx_ref\">2010</a>)</cite>; Extended by adding most challenging poses lie in 3 tags <cite class=\"ltx_cite ltx_citemacro_citep\">(Johnson and Everingham, <a href=\"#bib.bib69\" title=\"\" class=\"ltx_ref\">2011</a>)</cite>.</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T7.7.7\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T7.7.7.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\">&#13;\n<span id=\"S5.T7.7.7.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.7.7.2.1.1\" class=\"ltx_p\" style=\"width:31.3pt;\"><span id=\"S5.T7.7.7.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\">&#13;\n<span id=\"S5.T7.7.7.2.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<span id=\"S5.T7.7.7.2.1.1.1.1.1\" class=\"ltx_tr\">&#13;\n<span id=\"S5.T7.7.7.2.1.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">LSP-</span></span>&#13;\n<span id=\"S5.T7.7.7.2.1.1.1.1.2\" class=\"ltx_tr\">&#13;\n<span id=\"S5.T7.7.7.2.1.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">extended</span></span>&#13;\n</span></span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\"><span id=\"S5.T7.7.7.1.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\"><math id=\"S5.T7.7.7.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T7.7.7.1.1.m1.1a\"><mo id=\"S5.T7.7.7.1.1.m1.1.1\" xref=\"S5.T7.7.7.1.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.7.7.1.1.m1.1b\"><approx id=\"S5.T7.7.7.1.1.m1.1.1.cmml\" xref=\"S5.T7.7.7.1.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.7.7.1.1.m1.1c\">\\approx</annotation></semantics></math>10k</span></td>&#13;\n<td id=\"S5.T7.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\"><span id=\"S5.T7.7.7.3.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\">0</span></td>&#13;\n<td id=\"S5.T7.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\"><span id=\"S5.T7.7.7.4.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\">0</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T7.9.9\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T7.9.9.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\" rowspan=\"2\">&#13;\n<span id=\"S5.T7.9.9.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.9.9.3.1.1\" class=\"ltx_p\" style=\"width:31.3pt;\"><span id=\"S5.T7.9.9.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-19.9pt;\">MPII</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.9.9.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\">&#13;\n<span id=\"S5.T7.9.9.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.9.9.4.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\"><span id=\"S5.T7.9.9.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\">single</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.9.9.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\" rowspan=\"2\"><span id=\"S5.T7.9.9.5.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-19.9pt;\">16</span></td>&#13;\n<td id=\"S5.T7.8.8.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\"><span id=\"S5.T7.8.8.1.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\"><math id=\"S5.T7.8.8.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T7.8.8.1.1.m1.1a\"><mo id=\"S5.T7.8.8.1.1.m1.1.1\" xref=\"S5.T7.8.8.1.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.8.8.1.1.m1.1b\"><approx id=\"S5.T7.8.8.1.1.m1.1.1.cmml\" xref=\"S5.T7.8.8.1.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.8.8.1.1.m1.1c\">\\approx</annotation></semantics></math>29k</span></td>&#13;\n<td id=\"S5.T7.9.9.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\"><span id=\"S5.T7.9.9.6.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\">0</span></td>&#13;\n<td id=\"S5.T7.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\"><span id=\"S5.T7.9.9.2.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\"><math id=\"S5.T7.9.9.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T7.9.9.2.1.m1.1a\"><mo id=\"S5.T7.9.9.2.1.m1.1.1\" xref=\"S5.T7.9.9.2.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.9.9.2.1.m1.1b\"><approx id=\"S5.T7.9.9.2.1.m1.1.1.cmml\" xref=\"S5.T7.9.9.2.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.9.9.2.1.m1.1c\">\\approx</annotation></semantics></math>12k</span></td>&#13;\n<td id=\"S5.T7.9.9.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\"><span id=\"S5.T7.9.9.7.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\">PCPm/PCKh</span></td>&#13;\n<td id=\"S5.T7.9.9.8\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\" rowspan=\"2\">&#13;\n<span id=\"S5.T7.9.9.8.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.9.9.8.1.1\" class=\"ltx_p\" style=\"width:216.2pt;\"><span id=\"S5.T7.9.9.8.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Various body poses; Downloaded videos from YouTube; Multiple annotations (bounding boxes, 3D viewpoint of the head and torso, position of the eyes and nose, joint locations); <cite class=\"ltx_cite ltx_citemacro_citep\">(Andriluka et&#160;al., <a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\">2014</a>)</cite>.</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T7.11.11\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T7.11.11.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\">&#13;\n<span id=\"S5.T7.11.11.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.11.11.3.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\"><span id=\"S5.T7.11.11.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\">multiple</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.10.10.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\"><span id=\"S5.T7.10.10.1.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\"><math id=\"S5.T7.10.10.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T7.10.10.1.1.m1.1a\"><mo id=\"S5.T7.10.10.1.1.m1.1.1\" xref=\"S5.T7.10.10.1.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.10.10.1.1.m1.1b\"><approx id=\"S5.T7.10.10.1.1.m1.1.1.cmml\" xref=\"S5.T7.10.10.1.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.10.10.1.1.m1.1c\">\\approx</annotation></semantics></math>3.8k</span></td>&#13;\n<td id=\"S5.T7.11.11.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\"><span id=\"S5.T7.11.11.4.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\">0</span></td>&#13;\n<td id=\"S5.T7.11.11.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\"><span id=\"S5.T7.11.11.2.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\"><math id=\"S5.T7.11.11.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T7.11.11.2.1.m1.1a\"><mo id=\"S5.T7.11.11.2.1.m1.1.1\" xref=\"S5.T7.11.11.2.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.11.11.2.1.m1.1b\"><approx id=\"S5.T7.11.11.2.1.m1.1.1.cmml\" xref=\"S5.T7.11.11.2.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.11.11.2.1.m1.1c\">\\approx</annotation></semantics></math>1.7k</span></td>&#13;\n<td id=\"S5.T7.11.11.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\"><span id=\"S5.T7.11.11.5.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\">mAP</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T7.14.14\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T7.14.14.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\">&#13;\n<span id=\"S5.T7.14.14.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.14.14.4.1.1\" class=\"ltx_p\" style=\"width:31.3pt;\"><span id=\"S5.T7.14.14.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\">COCO16</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.14.14.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\" rowspan=\"2\">&#13;\n<span id=\"S5.T7.14.14.5.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.14.14.5.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\"><span id=\"S5.T7.14.14.5.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-19.9pt;\">multiple</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.14.14.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\" rowspan=\"2\"><span id=\"S5.T7.14.14.6.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-19.9pt;\">17</span></td>&#13;\n<td id=\"S5.T7.12.12.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\"><span id=\"S5.T7.12.12.1.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\"><math id=\"S5.T7.12.12.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T7.12.12.1.1.m1.1a\"><mo id=\"S5.T7.12.12.1.1.m1.1.1\" xref=\"S5.T7.12.12.1.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.12.12.1.1.m1.1b\"><approx id=\"S5.T7.12.12.1.1.m1.1.1.cmml\" xref=\"S5.T7.12.12.1.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.12.12.1.1.m1.1c\">\\approx</annotation></semantics></math>45k</span></td>&#13;\n<td id=\"S5.T7.13.13.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\"><span id=\"S5.T7.13.13.2.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\"><math id=\"S5.T7.13.13.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T7.13.13.2.1.m1.1a\"><mo id=\"S5.T7.13.13.2.1.m1.1.1\" xref=\"S5.T7.13.13.2.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.13.13.2.1.m1.1b\"><approx id=\"S5.T7.13.13.2.1.m1.1.1.cmml\" xref=\"S5.T7.13.13.2.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.13.13.2.1.m1.1c\">\\approx</annotation></semantics></math>22k</span></td>&#13;\n<td id=\"S5.T7.14.14.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\"><span id=\"S5.T7.14.14.3.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\"><math id=\"S5.T7.14.14.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T7.14.14.3.1.m1.1a\"><mo id=\"S5.T7.14.14.3.1.m1.1.1\" xref=\"S5.T7.14.14.3.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.14.14.3.1.m1.1b\"><approx id=\"S5.T7.14.14.3.1.m1.1.1.cmml\" xref=\"S5.T7.14.14.3.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.14.14.3.1.m1.1c\">\\approx</annotation></semantics></math>80k</span></td>&#13;\n<td id=\"S5.T7.14.14.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\" rowspan=\"2\"><span id=\"S5.T7.14.14.7.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-19.9pt;\">AP</span></td>&#13;\n<td id=\"S5.T7.14.14.8\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\" rowspan=\"2\">&#13;\n<span id=\"S5.T7.14.14.8.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.14.14.8.1.1\" class=\"ltx_p\" style=\"width:216.2pt;\"><span id=\"S5.T7.14.14.8.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Various body poses; From Google, Bing and Flickr; Multiple annotations (bounding boxes, human body masks, joint locations); With about 120K unlabeled images for semi-supervised learning; <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al., <a href=\"#bib.bib93\" title=\"\" class=\"ltx_ref\">2014</a>)</cite></span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T7.17.17\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T7.17.17.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\">&#13;\n<span id=\"S5.T7.17.17.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.17.17.4.1.1\" class=\"ltx_p\" style=\"width:31.3pt;\"><span id=\"S5.T7.17.17.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\">COCO17</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.15.15.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\"><span id=\"S5.T7.15.15.1.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\"><math id=\"S5.T7.15.15.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T7.15.15.1.1.m1.1a\"><mo id=\"S5.T7.15.15.1.1.m1.1.1\" xref=\"S5.T7.15.15.1.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.15.15.1.1.m1.1b\"><approx id=\"S5.T7.15.15.1.1.m1.1.1.cmml\" xref=\"S5.T7.15.15.1.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.15.15.1.1.m1.1c\">\\approx</annotation></semantics></math>64k</span></td>&#13;\n<td id=\"S5.T7.16.16.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\"><span id=\"S5.T7.16.16.2.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\"><math id=\"S5.T7.16.16.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T7.16.16.2.1.m1.1a\"><mo id=\"S5.T7.16.16.2.1.m1.1.1\" xref=\"S5.T7.16.16.2.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.16.16.2.1.m1.1b\"><approx id=\"S5.T7.16.16.2.1.m1.1.1.cmml\" xref=\"S5.T7.16.16.2.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.16.16.2.1.m1.1c\">\\approx</annotation></semantics></math>2.7k</span></td>&#13;\n<td id=\"S5.T7.17.17.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:13.0pt;\"><span id=\"S5.T7.17.17.3.1\" class=\"ltx_text\" style=\"font-size:80%;position:relative; bottom:-8.5pt;\"><math id=\"S5.T7.17.17.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T7.17.17.3.1.m1.1a\"><mo id=\"S5.T7.17.17.3.1.m1.1.1\" xref=\"S5.T7.17.17.3.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.17.17.3.1.m1.1b\"><approx id=\"S5.T7.17.17.3.1.m1.1.1.cmml\" xref=\"S5.T7.17.17.3.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.17.17.3.1.m1.1c\">\\approx</annotation></semantics></math>40k</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T7.20.20\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T7.20.20.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T7.20.20.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.20.20.4.1.1\" class=\"ltx_p\" style=\"width:31.3pt;\">&#13;\n<span id=\"S5.T7.20.20.4.1.1.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<span id=\"S5.T7.20.20.4.1.1.1.1\" class=\"ltx_tr\">&#13;\n<span id=\"S5.T7.20.20.4.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T7.20.20.4.1.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">AIC-</span></span></span>&#13;\n<span id=\"S5.T7.20.20.4.1.1.1.2\" class=\"ltx_tr\">&#13;\n<span id=\"S5.T7.20.20.4.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T7.20.20.4.1.1.1.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">HKD</span></span></span>&#13;\n</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.20.20.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T7.20.20.5.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.20.20.5.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\"><span id=\"S5.T7.20.20.5.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">multiple</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.20.20.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T7.20.20.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">14</span></td>&#13;\n<td id=\"S5.T7.18.18.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<math id=\"S5.T7.18.18.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T7.18.18.1.m1.1a\"><mo mathsize=\"80%\" id=\"S5.T7.18.18.1.m1.1.1\" xref=\"S5.T7.18.18.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.18.18.1.m1.1b\"><approx id=\"S5.T7.18.18.1.m1.1.1.cmml\" xref=\"S5.T7.18.18.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.18.18.1.m1.1c\">\\approx</annotation></semantics></math><span id=\"S5.T7.18.18.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">210k</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.19.19.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<math id=\"S5.T7.19.19.2.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T7.19.19.2.m1.1a\"><mo mathsize=\"80%\" id=\"S5.T7.19.19.2.m1.1.1\" xref=\"S5.T7.19.19.2.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.19.19.2.m1.1b\"><approx id=\"S5.T7.19.19.2.m1.1.1.cmml\" xref=\"S5.T7.19.19.2.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.19.19.2.m1.1c\">\\approx</annotation></semantics></math><span id=\"S5.T7.19.19.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">30k</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.20.20.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<math id=\"S5.T7.20.20.3.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T7.20.20.3.m1.1a\"><mo mathsize=\"80%\" id=\"S5.T7.20.20.3.m1.1.1\" xref=\"S5.T7.20.20.3.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.20.20.3.m1.1b\"><approx id=\"S5.T7.20.20.3.m1.1.1.cmml\" xref=\"S5.T7.20.20.3.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.20.20.3.m1.1c\">\\approx</annotation></semantics></math><span id=\"S5.T7.20.20.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">60k</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.20.20.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T7.20.20.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">AP</span></td>&#13;\n<td id=\"S5.T7.20.20.8\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T7.20.20.8.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.20.20.8.1.1\" class=\"ltx_p\" style=\"width:216.2pt;\"><span id=\"S5.T7.20.20.8.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Various body poses; From Internet search engines; Multiple annotations (bounding boxes, joint locations); </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S5.T7.20.20.8.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Wu et&#160;al.<span id=\"S5.T7.20.20.8.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib175\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S5.T7.20.20.8.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T7.24.28.4\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T7.24.28.4.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" colspan=\"8\"><span id=\"S5.T7.24.28.4.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Video-based</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T7.22.22\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T7.22.22.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T7.22.22.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.22.22.3.1.1\" class=\"ltx_p\" style=\"width:31.3pt;\">&#13;\n<span id=\"S5.T7.22.22.3.1.1.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<span id=\"S5.T7.22.22.3.1.1.1.1\" class=\"ltx_tr\">&#13;\n<span id=\"S5.T7.22.22.3.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T7.22.22.3.1.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Penn</span></span></span>&#13;\n<span id=\"S5.T7.22.22.3.1.1.1.2\" class=\"ltx_tr\">&#13;\n<span id=\"S5.T7.22.22.3.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T7.22.22.3.1.1.1.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Action</span></span></span>&#13;\n</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.22.22.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T7.22.22.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.22.22.4.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\"><span id=\"S5.T7.22.22.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">single</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.22.22.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T7.22.22.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">13</span></td>&#13;\n<td id=\"S5.T7.21.21.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<math id=\"S5.T7.21.21.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T7.21.21.1.m1.1a\"><mo mathsize=\"80%\" id=\"S5.T7.21.21.1.m1.1.1\" xref=\"S5.T7.21.21.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.21.21.1.m1.1b\"><approx id=\"S5.T7.21.21.1.m1.1.1.cmml\" xref=\"S5.T7.21.21.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.21.21.1.m1.1c\">\\approx</annotation></semantics></math><span id=\"S5.T7.21.21.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">1k</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.22.22.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T7.22.22.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">0</span></td>&#13;\n<td id=\"S5.T7.22.22.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<math id=\"S5.T7.22.22.2.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T7.22.22.2.m1.1a\"><mo mathsize=\"80%\" id=\"S5.T7.22.22.2.m1.1.1\" xref=\"S5.T7.22.22.2.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.22.22.2.m1.1b\"><approx id=\"S5.T7.22.22.2.m1.1.1.cmml\" xref=\"S5.T7.22.22.2.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.22.22.2.m1.1c\">\\approx</annotation></semantics></math><span id=\"S5.T7.22.22.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">1k</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.22.22.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T7.22.22.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>&#13;\n<td id=\"S5.T7.22.22.8\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T7.22.22.8.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.22.22.8.1.1\" class=\"ltx_p\" style=\"width:216.2pt;\">&#13;\n<span id=\"S5.T7.22.22.8.1.1.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<span id=\"S5.T7.22.22.8.1.1.1.1\" class=\"ltx_tr\">&#13;\n<span id=\"S5.T7.22.22.8.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"S5.T7.22.22.8.1.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">full-body poses; From YouTube; 15 actions; Multiple annotations</span></span></span>&#13;\n<span id=\"S5.T7.22.22.8.1.1.1.2\" class=\"ltx_tr\">&#13;\n<span id=\"S5.T7.22.22.8.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"S5.T7.22.22.8.1.1.1.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(joint locations, bounding boxes, action classes) </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S5.T7.22.22.8.1.1.1.2.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Zhang et&#160;al.<span id=\"S5.T7.22.22.8.1.1.1.2.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib182\" title=\"\" class=\"ltx_ref\">2013</a><span id=\"S5.T7.22.22.8.1.1.1.2.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S5.T7.22.22.8.1.1.1.2.1.5\" class=\"ltx_text\" style=\"font-size:80%;\">.</span></span></span>&#13;\n</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T7.24.24\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T7.24.24.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T7.24.24.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.24.24.3.1.1\" class=\"ltx_p\" style=\"width:31.3pt;\"><span id=\"S5.T7.24.24.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">J-HMDB</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.24.24.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T7.24.24.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.24.24.4.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\"><span id=\"S5.T7.24.24.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">single</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.24.24.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T7.24.24.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">15</span></td>&#13;\n<td id=\"S5.T7.23.23.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<math id=\"S5.T7.23.23.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T7.23.23.1.m1.1a\"><mo mathsize=\"80%\" id=\"S5.T7.23.23.1.m1.1.1\" xref=\"S5.T7.23.23.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.23.23.1.m1.1b\"><approx id=\"S5.T7.23.23.1.m1.1.1.cmml\" xref=\"S5.T7.23.23.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.23.23.1.m1.1c\">\\approx</annotation></semantics></math><span id=\"S5.T7.23.23.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.6k</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.24.24.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T7.24.24.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">0</span></td>&#13;\n<td id=\"S5.T7.24.24.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<math id=\"S5.T7.24.24.2.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T7.24.24.2.m1.1a\"><mo mathsize=\"80%\" id=\"S5.T7.24.24.2.m1.1.1\" xref=\"S5.T7.24.24.2.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.24.24.2.m1.1b\"><approx id=\"S5.T7.24.24.2.m1.1.1.cmml\" xref=\"S5.T7.24.24.2.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.24.24.2.m1.1c\">\\approx</annotation></semantics></math><span id=\"S5.T7.24.24.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.3k</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.24.24.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T7.24.24.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>&#13;\n<td id=\"S5.T7.24.24.8\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T7.24.24.8.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.24.24.8.1.1\" class=\"ltx_p\" style=\"width:216.2pt;\">&#13;\n<span id=\"S5.T7.24.24.8.1.1.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<span id=\"S5.T7.24.24.8.1.1.1.1\" class=\"ltx_tr\">&#13;\n<span id=\"S5.T7.24.24.8.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"S5.T7.24.24.8.1.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">full-body poses; Generated from action recognition dataset; 21</span></span></span>&#13;\n<span id=\"S5.T7.24.24.8.1.1.1.2\" class=\"ltx_tr\">&#13;\n<span id=\"S5.T7.24.24.8.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"S5.T7.24.24.8.1.1.1.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">actions; Multiple annotations (joint positions and relations, optical</span></span></span>&#13;\n<span id=\"S5.T7.24.24.8.1.1.1.3\" class=\"ltx_tr\">&#13;\n<span id=\"S5.T7.24.24.8.1.1.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"S5.T7.24.24.8.1.1.1.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">flows, segmentation masks) </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S5.T7.24.24.8.1.1.1.3.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Jhuang et&#160;al.<span id=\"S5.T7.24.24.8.1.1.1.3.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib65\" title=\"\" class=\"ltx_ref\">2013</a><span id=\"S5.T7.24.24.8.1.1.1.3.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S5.T7.24.24.8.1.1.1.3.1.5\" class=\"ltx_text\" style=\"font-size:80%;\">.</span></span></span>&#13;\n</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T7.24.29.5\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T7.24.29.5.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T7.24.29.5.1.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.24.29.5.1.1.1\" class=\"ltx_p\" style=\"width:31.3pt;\">&#13;\n<span id=\"S5.T7.24.29.5.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<span id=\"S5.T7.24.29.5.1.1.1.1.1\" class=\"ltx_tr\">&#13;\n<span id=\"S5.T7.24.29.5.1.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T7.24.29.5.1.1.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">PoseTrack</span></span></span>&#13;\n</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.24.29.5.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T7.24.29.5.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.24.29.5.2.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\"><span id=\"S5.T7.24.29.5.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">multiple</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T7.24.29.5.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S5.T7.24.29.5.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">15</span></td>&#13;\n<td id=\"S5.T7.24.29.5.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S5.T7.24.29.5.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">292</span></td>&#13;\n<td id=\"S5.T7.24.29.5.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S5.T7.24.29.5.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">50</span></td>&#13;\n<td id=\"S5.T7.24.29.5.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S5.T7.24.29.5.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">208</span></td>&#13;\n<td id=\"S5.T7.24.29.5.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S5.T7.24.29.5.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">mAP</span></td>&#13;\n<td id=\"S5.T7.24.29.5.8\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T7.24.29.5.8.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T7.24.29.5.8.1.1\" class=\"ltx_p\" style=\"width:216.2pt;\">&#13;\n<span id=\"S5.T7.24.29.5.8.1.1.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<span id=\"S5.T7.24.29.5.8.1.1.1.1\" class=\"ltx_tr\">&#13;\n<span id=\"S5.T7.24.29.5.8.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"S5.T7.24.29.5.8.1.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Various body poses; Extended from MPII; Dense annotations</span></span></span>&#13;\n<span id=\"S5.T7.24.29.5.8.1.1.1.2\" class=\"ltx_tr\">&#13;\n<span id=\"S5.T7.24.29.5.8.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"S5.T7.24.29.5.8.1.1.1.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(joint locations, head bounding boxes) </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S5.T7.24.29.5.8.1.1.1.2.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Andriluka et&#160;al.<span id=\"S5.T7.24.29.5.8.1.1.1.2.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib3\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S5.T7.24.29.5.8.1.1.1.2.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S5.T7.24.29.5.8.1.1.1.2.1.5\" class=\"ltx_text\" style=\"font-size:80%;\">.</span></span></span>&#13;\n</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "(Sapp and Taskar, 2013) Sapp and Taskar (2013)\r\n\r\nSapp, B., Taskar, B., 2013.\r\n\r\n\r\nModec: Multimodal decomposable models for human pose\r\nestimation, in: Proc. IEEE Conference on Computer Vision\r\nand Pattern Recognition, pp. 3674–3681.",
            "(Tompson et al., 2014) Tompson et al. (2014)\r\n\r\nTompson, J.J., Jain, A.,\r\nLeCun, Y., Bregler, C.,\r\n2014.\r\n\r\n\r\nJoint training of a convolutional network and a\r\ngraphical model for human pose estimation, in: Advances\r\nin neural information processing systems, pp. 1799–1807.",
            "(Johnson and Everingham, 2010) Johnson and Everingham (2010)\r\n\r\nJohnson, S., Everingham, M.,\r\n2010.\r\n\r\n\r\nClustered pose and nonlinear appearance models for\r\nhuman pose estimation, in: Proc. British Machine Vision\r\nConference, p. 5.",
            "(Johnson and Everingham, 2011) Johnson and Everingham (2011)\r\n\r\nJohnson, S., Everingham, M.,\r\n2011.\r\n\r\n\r\nLearning effective human pose estimation from\r\ninaccurate annotation, in: Proc. IEEE Conference on\r\nComputer Vision and Pattern Recognition, pp. 1465–1472.",
            "(Andriluka et al., 2014) Andriluka et al. (2014)\r\n\r\nAndriluka, M., Pishchulin, L.,\r\nGehler, P., Schiele, B.,\r\n2014.\r\n\r\n\r\n2d human pose estimation: New benchmark and state of\r\nthe art analysis, in: Proc. IEEE Conference on Computer\r\nVision and Pattern Recognition, pp. 3686–3693.",
            "(Lin et al., 2014) Lin et al. (2014)\r\n\r\nLin, T.Y., Maire, M.,\r\nBelongie, S., Hays, J.,\r\nPerona, P., Ramanan, D.,\r\nDollár, P., Zitnick, C.L.,\r\n2014.\r\n\r\n\r\nMicrosoft coco: Common objects in context, in:\r\nProc. European Conference on Computer Vision,\r\nSpringer. pp. 740–755.",
            "(Wu et al., 2017) Wu et al. (2017)\r\n\r\nWu, J., Zheng, H., Zhao,\r\nB., Li, Y., Yan, B.,\r\nLiang, R., Wang, W.,\r\nZhou, S., Lin, G., Fu,\r\nY., et al., 2017.\r\n\r\n\r\nAi challenger: A large-scale dataset for going deeper\r\nin image understanding.\r\n\r\n\r\narXiv preprint arXiv:1711.06475 .",
            "(Zhang et al., 2013) Zhang et al. (2013)\r\n\r\nZhang, W., Zhu, M.,\r\nDerpanis, K.G., 2013.\r\n\r\n\r\nFrom actemes to action: A strongly-supervised\r\nrepresentation for detailed action understanding, in:\r\nProc. IEEE International Conference on Computer Vision,\r\npp. 2248–2255.",
            "(Jhuang et al., 2013) Jhuang et al. (2013)\r\n\r\nJhuang, H., Gall, J.,\r\nZuffi, S., Schmid, C.,\r\nBlack, M.J., 2013.\r\n\r\n\r\nTowards understanding action recognition, in:\r\nProc. IEEE International Conference on Computer Vision,\r\npp. 3192–3199.",
            "(Andriluka et al., 2018) Andriluka et al. (2018)\r\n\r\nAndriluka, M., Iqbal, U.,\r\nMilan, A., Insafutdinov, E.,\r\nPishchulin, L., Gall, J.,\r\nSchiele, B., 2018.\r\n\r\n\r\nPosetrack: A benchmark for human pose estimation and\r\ntracking, in: Proc. IEEE Conference on Computer Vision\r\nand Pattern Recognition, pp. 5167–5176."
        ],
        "references": [
            "Above earlier datasets for 2D human pose estimation have many shortcomings such as few scenes, monotonous view angle, lack of diverse activities, and limited number of images. The scale is the most important aspect of a dataset for deep learning-based methods. Small training sets are insufficient for learning robust features, unsuitable for networks with deep layers and complex design, and may easily cause overfitting. Thus in this section, we only introduce 2D human pose datasets with the number of images for training over 1,000. The features of these selected 2D HPE datasets are summarized in Table 7 and some sample images with annotations are illustrated in Fig. 5.",
            "Microsoft Common Objects in Context (COCO) Dataset (Lin et al., 2014) is a large-scale dataset that was originally proposed for daily object detection and segmentation in natural environments. With improvements and extensions, the usage of COCO covers image captioning and keypoint detection. Images are collected from Google, Bing, and Flickr image search with isolated or pairwise object categories. Annotations were conducted on Amazon Mechanical Turk. The whole set contains more than 200,000200000200,000 images and 250,000250000250,000 labeled person instances. Suitable examples are selected for human pose estimation, thus forming two datasets: COCO keypoints 2016 and COCO keypoints 2017, corresponding to two public keypoint detection challenges respectively. The only difference between these two versions is the train/val/test splitting strategy based on community feedback (shown in Table 7), and cross-year results can be compared directly since the images in the test set are same. The COCO Keypoint Detection Challenge aims to localize keypoints of people in uncontrolled images. The annotations for each person include 17 body joints with visibility and left/right labels, and instance human body segmentation. Note that COCO dataset contains about 120K unlabeled images following the same class distribution as the labeled images which can be used for unsupervised or semi-supervised learning."
        ]
    },
    "id_table_8": {
        "caption": "Table 8: Summary of commonly used evaluation metrics for 2D HPE.",
        "table": [
            "<table id=\"S5.T8.32\" class=\"ltx_tabular ltx_centering ltx_align_middle\">&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S5.T8.32.33.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.33.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S5.T8.32.33.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Metric</span></td>&#13;\n<td id=\"S5.T8.32.33.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T8.32.33.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Meaning</span></td>&#13;\n<td id=\"S5.T8.32.33.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\"><span id=\"S5.T8.32.33.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Typical datasets and Description</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.32.34.2\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.34.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" colspan=\"4\"><span id=\"S5.T8.32.34.2.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Single person</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.32.35.3\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.35.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S5.T8.32.35.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">PCP</span></td>&#13;\n<td id=\"S5.T8.32.35.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<table id=\"S5.T8.32.35.3.2.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T8.32.35.3.2.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.35.3.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.35.3.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Percentage</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.32.35.3.2.1.2\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.35.3.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.35.3.2.1.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">of Correct</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.32.35.3.2.1.3\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.35.3.2.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.35.3.2.1.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Parts</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n</td>&#13;\n<td id=\"S5.T8.32.35.3.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S5.T8.32.35.3.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">LSP</span></td>&#13;\n<td id=\"S5.T8.32.35.3.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">&#13;\n<table id=\"S5.T8.32.35.3.4.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T8.32.35.3.4.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.35.3.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.35.3.4.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Percentage of correct predicted Parts which their end points fall within a threshold</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.32.36.4\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.36.4.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S5.T8.32.36.4.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">PCK</span></td>&#13;\n<td id=\"S5.T8.32.36.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<table id=\"S5.T8.32.36.4.2.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T8.32.36.4.2.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.36.4.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.36.4.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Percentage</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.32.36.4.2.1.2\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.36.4.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.36.4.2.1.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">of Correct</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.32.36.4.2.1.3\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.36.4.2.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.36.4.2.1.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Keypoints</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n</td>&#13;\n<td id=\"S5.T8.32.36.4.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">&#13;\n<table id=\"S5.T8.32.36.4.3.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T8.32.36.4.3.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.36.4.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.36.4.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">LSP</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.32.36.4.3.1.2\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.36.4.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.36.4.3.1.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">MPII</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n</td>&#13;\n<td id=\"S5.T8.32.36.4.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">&#13;\n<table id=\"S5.T8.32.36.4.4.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T8.32.36.4.4.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.36.4.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.36.4.4.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Percentage of correct predicted joints which fall within a threshold</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.32.37.5\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.37.5.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" colspan=\"4\"><span id=\"S5.T8.32.37.5.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Multiple person</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.32.38.6\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.38.6.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"6\"><span id=\"S5.T8.32.38.6.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">AP</span></td>&#13;\n<td id=\"S5.T8.32.38.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"6\"><span id=\"S5.T8.32.38.6.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#13;\n<span id=\"S5.T8.32.38.6.2.1.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<span id=\"S5.T8.32.38.6.2.1.1.1\" class=\"ltx_tr\">&#13;\n<span id=\"S5.T8.32.38.6.2.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Average</span></span>&#13;\n<span id=\"S5.T8.32.38.6.2.1.1.2\" class=\"ltx_tr\">&#13;\n<span id=\"S5.T8.32.38.6.2.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Precision</span></span>&#13;\n</span></span></td>&#13;\n<td id=\"S5.T8.32.38.6.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">&#13;\n<table id=\"S5.T8.32.38.6.3.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T8.32.38.6.3.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.38.6.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.38.6.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">MPII</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.32.38.6.3.1.2\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.38.6.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.38.6.3.1.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">PoseTrack</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n</td>&#13;\n<td id=\"S5.T8.32.38.6.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">&#13;\n<table id=\"S5.T8.32.38.6.4.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T8.32.38.6.4.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.38.6.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"S5.T8.32.38.6.4.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">mean AP (mAP) is reported by AP for each body part after assigning predicted</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.32.38.6.4.1.2\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.38.6.4.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"S5.T8.32.38.6.4.1.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">pose to the ground truth pose by PCKh score.</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.2.2\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"5\"><span id=\"S5.T8.2.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#13;\n<span id=\"S5.T8.2.2.3.1.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<span id=\"S5.T8.2.2.3.1.1.1\" class=\"ltx_tr\">&#13;\n<span id=\"S5.T8.2.2.3.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">COCO</span></span>&#13;\n</span></span></td>&#13;\n<td id=\"S5.T8.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">&#13;\n<math id=\"S5.T8.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\bullet\" display=\"inline\"><semantics id=\"S5.T8.1.1.1.m1.1a\"><mo mathsize=\"80%\" id=\"S5.T8.1.1.1.m1.1.1\" xref=\"S5.T8.1.1.1.m1.1.1.cmml\">&#8729;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.1.1.1.m1.1b\"><ci id=\"S5.T8.1.1.1.m1.1.1.cmml\" xref=\"S5.T8.1.1.1.m1.1.1\">&#8729;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.1.1.1.m1.1c\">\\bullet</annotation></semantics></math><span id=\"S5.T8.2.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S5.T8.2.2.2.m2.1\" class=\"ltx_Math\" alttext=\"AP_{coco}\" display=\"inline\"><semantics id=\"S5.T8.2.2.2.m2.1a\"><mrow id=\"S5.T8.2.2.2.m2.1.1\" xref=\"S5.T8.2.2.2.m2.1.1.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.2.2.2.m2.1.1.2\" xref=\"S5.T8.2.2.2.m2.1.1.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.2.2.2.m2.1.1.1\" xref=\"S5.T8.2.2.2.m2.1.1.1.cmml\">&#8203;</mo><msub id=\"S5.T8.2.2.2.m2.1.1.3\" xref=\"S5.T8.2.2.2.m2.1.1.3.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.2.2.2.m2.1.1.3.2\" xref=\"S5.T8.2.2.2.m2.1.1.3.2.cmml\">P</mi><mrow id=\"S5.T8.2.2.2.m2.1.1.3.3\" xref=\"S5.T8.2.2.2.m2.1.1.3.3.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.2.2.2.m2.1.1.3.3.2\" xref=\"S5.T8.2.2.2.m2.1.1.3.3.2.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.2.2.2.m2.1.1.3.3.1\" xref=\"S5.T8.2.2.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.2.2.2.m2.1.1.3.3.3\" xref=\"S5.T8.2.2.2.m2.1.1.3.3.3.cmml\">o</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.2.2.2.m2.1.1.3.3.1a\" xref=\"S5.T8.2.2.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.2.2.2.m2.1.1.3.3.4\" xref=\"S5.T8.2.2.2.m2.1.1.3.3.4.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.2.2.2.m2.1.1.3.3.1b\" xref=\"S5.T8.2.2.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.2.2.2.m2.1.1.3.3.5\" xref=\"S5.T8.2.2.2.m2.1.1.3.3.5.cmml\">o</mi></mrow></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.2.2.2.m2.1b\"><apply id=\"S5.T8.2.2.2.m2.1.1.cmml\" xref=\"S5.T8.2.2.2.m2.1.1\"><times id=\"S5.T8.2.2.2.m2.1.1.1.cmml\" xref=\"S5.T8.2.2.2.m2.1.1.1\"/><ci id=\"S5.T8.2.2.2.m2.1.1.2.cmml\" xref=\"S5.T8.2.2.2.m2.1.1.2\">&#119860;</ci><apply id=\"S5.T8.2.2.2.m2.1.1.3.cmml\" xref=\"S5.T8.2.2.2.m2.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S5.T8.2.2.2.m2.1.1.3.1.cmml\" xref=\"S5.T8.2.2.2.m2.1.1.3\">subscript</csymbol><ci id=\"S5.T8.2.2.2.m2.1.1.3.2.cmml\" xref=\"S5.T8.2.2.2.m2.1.1.3.2\">&#119875;</ci><apply id=\"S5.T8.2.2.2.m2.1.1.3.3.cmml\" xref=\"S5.T8.2.2.2.m2.1.1.3.3\"><times id=\"S5.T8.2.2.2.m2.1.1.3.3.1.cmml\" xref=\"S5.T8.2.2.2.m2.1.1.3.3.1\"/><ci id=\"S5.T8.2.2.2.m2.1.1.3.3.2.cmml\" xref=\"S5.T8.2.2.2.m2.1.1.3.3.2\">&#119888;</ci><ci id=\"S5.T8.2.2.2.m2.1.1.3.3.3.cmml\" xref=\"S5.T8.2.2.2.m2.1.1.3.3.3\">&#119900;</ci><ci id=\"S5.T8.2.2.2.m2.1.1.3.3.4.cmml\" xref=\"S5.T8.2.2.2.m2.1.1.3.3.4\">&#119888;</ci><ci id=\"S5.T8.2.2.2.m2.1.1.3.3.5.cmml\" xref=\"S5.T8.2.2.2.m2.1.1.3.3.5\">&#119900;</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.2.2.2.m2.1c\">AP_{coco}</annotation></semantics></math><span id=\"S5.T8.2.2.2.2\" class=\"ltx_text\" style=\"font-size:80%;\">: at OKS=.50:.05:.95 (primary metric)</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.4.4\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.4.4.2\" class=\"ltx_td ltx_align_left ltx_border_r\">&#13;\n<math id=\"S5.T8.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\bullet\" display=\"inline\"><semantics id=\"S5.T8.3.3.1.m1.1a\"><mo mathsize=\"80%\" id=\"S5.T8.3.3.1.m1.1.1\" xref=\"S5.T8.3.3.1.m1.1.1.cmml\">&#8729;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.3.3.1.m1.1b\"><ci id=\"S5.T8.3.3.1.m1.1.1.cmml\" xref=\"S5.T8.3.3.1.m1.1.1\">&#8729;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.3.3.1.m1.1c\">\\bullet</annotation></semantics></math><span id=\"S5.T8.4.4.2.1\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S5.T8.4.4.2.m2.1\" class=\"ltx_Math\" alttext=\"AP^{OKS=.50}_{coco}\" display=\"inline\"><semantics id=\"S5.T8.4.4.2.m2.1a\"><mrow id=\"S5.T8.4.4.2.m2.1.1\" xref=\"S5.T8.4.4.2.m2.1.1.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.4.4.2.m2.1.1.2\" xref=\"S5.T8.4.4.2.m2.1.1.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.4.4.2.m2.1.1.1\" xref=\"S5.T8.4.4.2.m2.1.1.1.cmml\">&#8203;</mo><msubsup id=\"S5.T8.4.4.2.m2.1.1.3\" xref=\"S5.T8.4.4.2.m2.1.1.3.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.4.4.2.m2.1.1.3.2.2\" xref=\"S5.T8.4.4.2.m2.1.1.3.2.2.cmml\">P</mi><mrow id=\"S5.T8.4.4.2.m2.1.1.3.3\" xref=\"S5.T8.4.4.2.m2.1.1.3.3.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.4.4.2.m2.1.1.3.3.2\" xref=\"S5.T8.4.4.2.m2.1.1.3.3.2.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.4.4.2.m2.1.1.3.3.1\" xref=\"S5.T8.4.4.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.4.4.2.m2.1.1.3.3.3\" xref=\"S5.T8.4.4.2.m2.1.1.3.3.3.cmml\">o</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.4.4.2.m2.1.1.3.3.1a\" xref=\"S5.T8.4.4.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.4.4.2.m2.1.1.3.3.4\" xref=\"S5.T8.4.4.2.m2.1.1.3.3.4.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.4.4.2.m2.1.1.3.3.1b\" xref=\"S5.T8.4.4.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.4.4.2.m2.1.1.3.3.5\" xref=\"S5.T8.4.4.2.m2.1.1.3.3.5.cmml\">o</mi></mrow><mrow id=\"S5.T8.4.4.2.m2.1.1.3.2.3\" xref=\"S5.T8.4.4.2.m2.1.1.3.2.3.cmml\"><mrow id=\"S5.T8.4.4.2.m2.1.1.3.2.3.2\" xref=\"S5.T8.4.4.2.m2.1.1.3.2.3.2.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.4.4.2.m2.1.1.3.2.3.2.2\" xref=\"S5.T8.4.4.2.m2.1.1.3.2.3.2.2.cmml\">O</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.4.4.2.m2.1.1.3.2.3.2.1\" xref=\"S5.T8.4.4.2.m2.1.1.3.2.3.2.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.4.4.2.m2.1.1.3.2.3.2.3\" xref=\"S5.T8.4.4.2.m2.1.1.3.2.3.2.3.cmml\">K</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.4.4.2.m2.1.1.3.2.3.2.1a\" xref=\"S5.T8.4.4.2.m2.1.1.3.2.3.2.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.4.4.2.m2.1.1.3.2.3.2.4\" xref=\"S5.T8.4.4.2.m2.1.1.3.2.3.2.4.cmml\">S</mi></mrow><mo mathsize=\"80%\" id=\"S5.T8.4.4.2.m2.1.1.3.2.3.1\" xref=\"S5.T8.4.4.2.m2.1.1.3.2.3.1.cmml\">=</mo><mn mathsize=\"80%\" id=\"S5.T8.4.4.2.m2.1.1.3.2.3.3\" xref=\"S5.T8.4.4.2.m2.1.1.3.2.3.3.cmml\">.50</mn></mrow></msubsup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.4.4.2.m2.1b\"><apply id=\"S5.T8.4.4.2.m2.1.1.cmml\" xref=\"S5.T8.4.4.2.m2.1.1\"><times id=\"S5.T8.4.4.2.m2.1.1.1.cmml\" xref=\"S5.T8.4.4.2.m2.1.1.1\"/><ci id=\"S5.T8.4.4.2.m2.1.1.2.cmml\" xref=\"S5.T8.4.4.2.m2.1.1.2\">&#119860;</ci><apply id=\"S5.T8.4.4.2.m2.1.1.3.cmml\" xref=\"S5.T8.4.4.2.m2.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S5.T8.4.4.2.m2.1.1.3.1.cmml\" xref=\"S5.T8.4.4.2.m2.1.1.3\">subscript</csymbol><apply id=\"S5.T8.4.4.2.m2.1.1.3.2.cmml\" xref=\"S5.T8.4.4.2.m2.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S5.T8.4.4.2.m2.1.1.3.2.1.cmml\" xref=\"S5.T8.4.4.2.m2.1.1.3\">superscript</csymbol><ci id=\"S5.T8.4.4.2.m2.1.1.3.2.2.cmml\" xref=\"S5.T8.4.4.2.m2.1.1.3.2.2\">&#119875;</ci><apply id=\"S5.T8.4.4.2.m2.1.1.3.2.3.cmml\" xref=\"S5.T8.4.4.2.m2.1.1.3.2.3\"><eq id=\"S5.T8.4.4.2.m2.1.1.3.2.3.1.cmml\" xref=\"S5.T8.4.4.2.m2.1.1.3.2.3.1\"/><apply id=\"S5.T8.4.4.2.m2.1.1.3.2.3.2.cmml\" xref=\"S5.T8.4.4.2.m2.1.1.3.2.3.2\"><times id=\"S5.T8.4.4.2.m2.1.1.3.2.3.2.1.cmml\" xref=\"S5.T8.4.4.2.m2.1.1.3.2.3.2.1\"/><ci id=\"S5.T8.4.4.2.m2.1.1.3.2.3.2.2.cmml\" xref=\"S5.T8.4.4.2.m2.1.1.3.2.3.2.2\">&#119874;</ci><ci id=\"S5.T8.4.4.2.m2.1.1.3.2.3.2.3.cmml\" xref=\"S5.T8.4.4.2.m2.1.1.3.2.3.2.3\">&#119870;</ci><ci id=\"S5.T8.4.4.2.m2.1.1.3.2.3.2.4.cmml\" xref=\"S5.T8.4.4.2.m2.1.1.3.2.3.2.4\">&#119878;</ci></apply><cn type=\"float\" id=\"S5.T8.4.4.2.m2.1.1.3.2.3.3.cmml\" xref=\"S5.T8.4.4.2.m2.1.1.3.2.3.3\">.50</cn></apply></apply><apply id=\"S5.T8.4.4.2.m2.1.1.3.3.cmml\" xref=\"S5.T8.4.4.2.m2.1.1.3.3\"><times id=\"S5.T8.4.4.2.m2.1.1.3.3.1.cmml\" xref=\"S5.T8.4.4.2.m2.1.1.3.3.1\"/><ci id=\"S5.T8.4.4.2.m2.1.1.3.3.2.cmml\" xref=\"S5.T8.4.4.2.m2.1.1.3.3.2\">&#119888;</ci><ci id=\"S5.T8.4.4.2.m2.1.1.3.3.3.cmml\" xref=\"S5.T8.4.4.2.m2.1.1.3.3.3\">&#119900;</ci><ci id=\"S5.T8.4.4.2.m2.1.1.3.3.4.cmml\" xref=\"S5.T8.4.4.2.m2.1.1.3.3.4\">&#119888;</ci><ci id=\"S5.T8.4.4.2.m2.1.1.3.3.5.cmml\" xref=\"S5.T8.4.4.2.m2.1.1.3.3.5\">&#119900;</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.4.4.2.m2.1c\">AP^{OKS=.50}_{coco}</annotation></semantics></math><span id=\"S5.T8.4.4.2.2\" class=\"ltx_text\" style=\"font-size:80%;\">: at OKS=.50 (loose metric)</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.6.6\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.6.6.2\" class=\"ltx_td ltx_align_left ltx_border_r\">&#13;\n<math id=\"S5.T8.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"\\bullet\" display=\"inline\"><semantics id=\"S5.T8.5.5.1.m1.1a\"><mo mathsize=\"80%\" id=\"S5.T8.5.5.1.m1.1.1\" xref=\"S5.T8.5.5.1.m1.1.1.cmml\">&#8729;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.5.5.1.m1.1b\"><ci id=\"S5.T8.5.5.1.m1.1.1.cmml\" xref=\"S5.T8.5.5.1.m1.1.1\">&#8729;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.5.5.1.m1.1c\">\\bullet</annotation></semantics></math><span id=\"S5.T8.6.6.2.1\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S5.T8.6.6.2.m2.1\" class=\"ltx_Math\" alttext=\"AP^{OKS=.75}_{coco}\" display=\"inline\"><semantics id=\"S5.T8.6.6.2.m2.1a\"><mrow id=\"S5.T8.6.6.2.m2.1.1\" xref=\"S5.T8.6.6.2.m2.1.1.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.6.6.2.m2.1.1.2\" xref=\"S5.T8.6.6.2.m2.1.1.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.6.6.2.m2.1.1.1\" xref=\"S5.T8.6.6.2.m2.1.1.1.cmml\">&#8203;</mo><msubsup id=\"S5.T8.6.6.2.m2.1.1.3\" xref=\"S5.T8.6.6.2.m2.1.1.3.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.6.6.2.m2.1.1.3.2.2\" xref=\"S5.T8.6.6.2.m2.1.1.3.2.2.cmml\">P</mi><mrow id=\"S5.T8.6.6.2.m2.1.1.3.3\" xref=\"S5.T8.6.6.2.m2.1.1.3.3.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.6.6.2.m2.1.1.3.3.2\" xref=\"S5.T8.6.6.2.m2.1.1.3.3.2.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.6.6.2.m2.1.1.3.3.1\" xref=\"S5.T8.6.6.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.6.6.2.m2.1.1.3.3.3\" xref=\"S5.T8.6.6.2.m2.1.1.3.3.3.cmml\">o</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.6.6.2.m2.1.1.3.3.1a\" xref=\"S5.T8.6.6.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.6.6.2.m2.1.1.3.3.4\" xref=\"S5.T8.6.6.2.m2.1.1.3.3.4.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.6.6.2.m2.1.1.3.3.1b\" xref=\"S5.T8.6.6.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.6.6.2.m2.1.1.3.3.5\" xref=\"S5.T8.6.6.2.m2.1.1.3.3.5.cmml\">o</mi></mrow><mrow id=\"S5.T8.6.6.2.m2.1.1.3.2.3\" xref=\"S5.T8.6.6.2.m2.1.1.3.2.3.cmml\"><mrow id=\"S5.T8.6.6.2.m2.1.1.3.2.3.2\" xref=\"S5.T8.6.6.2.m2.1.1.3.2.3.2.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.6.6.2.m2.1.1.3.2.3.2.2\" xref=\"S5.T8.6.6.2.m2.1.1.3.2.3.2.2.cmml\">O</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.6.6.2.m2.1.1.3.2.3.2.1\" xref=\"S5.T8.6.6.2.m2.1.1.3.2.3.2.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.6.6.2.m2.1.1.3.2.3.2.3\" xref=\"S5.T8.6.6.2.m2.1.1.3.2.3.2.3.cmml\">K</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.6.6.2.m2.1.1.3.2.3.2.1a\" xref=\"S5.T8.6.6.2.m2.1.1.3.2.3.2.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.6.6.2.m2.1.1.3.2.3.2.4\" xref=\"S5.T8.6.6.2.m2.1.1.3.2.3.2.4.cmml\">S</mi></mrow><mo mathsize=\"80%\" id=\"S5.T8.6.6.2.m2.1.1.3.2.3.1\" xref=\"S5.T8.6.6.2.m2.1.1.3.2.3.1.cmml\">=</mo><mn mathsize=\"80%\" id=\"S5.T8.6.6.2.m2.1.1.3.2.3.3\" xref=\"S5.T8.6.6.2.m2.1.1.3.2.3.3.cmml\">.75</mn></mrow></msubsup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.6.6.2.m2.1b\"><apply id=\"S5.T8.6.6.2.m2.1.1.cmml\" xref=\"S5.T8.6.6.2.m2.1.1\"><times id=\"S5.T8.6.6.2.m2.1.1.1.cmml\" xref=\"S5.T8.6.6.2.m2.1.1.1\"/><ci id=\"S5.T8.6.6.2.m2.1.1.2.cmml\" xref=\"S5.T8.6.6.2.m2.1.1.2\">&#119860;</ci><apply id=\"S5.T8.6.6.2.m2.1.1.3.cmml\" xref=\"S5.T8.6.6.2.m2.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S5.T8.6.6.2.m2.1.1.3.1.cmml\" xref=\"S5.T8.6.6.2.m2.1.1.3\">subscript</csymbol><apply id=\"S5.T8.6.6.2.m2.1.1.3.2.cmml\" xref=\"S5.T8.6.6.2.m2.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S5.T8.6.6.2.m2.1.1.3.2.1.cmml\" xref=\"S5.T8.6.6.2.m2.1.1.3\">superscript</csymbol><ci id=\"S5.T8.6.6.2.m2.1.1.3.2.2.cmml\" xref=\"S5.T8.6.6.2.m2.1.1.3.2.2\">&#119875;</ci><apply id=\"S5.T8.6.6.2.m2.1.1.3.2.3.cmml\" xref=\"S5.T8.6.6.2.m2.1.1.3.2.3\"><eq id=\"S5.T8.6.6.2.m2.1.1.3.2.3.1.cmml\" xref=\"S5.T8.6.6.2.m2.1.1.3.2.3.1\"/><apply id=\"S5.T8.6.6.2.m2.1.1.3.2.3.2.cmml\" xref=\"S5.T8.6.6.2.m2.1.1.3.2.3.2\"><times id=\"S5.T8.6.6.2.m2.1.1.3.2.3.2.1.cmml\" xref=\"S5.T8.6.6.2.m2.1.1.3.2.3.2.1\"/><ci id=\"S5.T8.6.6.2.m2.1.1.3.2.3.2.2.cmml\" xref=\"S5.T8.6.6.2.m2.1.1.3.2.3.2.2\">&#119874;</ci><ci id=\"S5.T8.6.6.2.m2.1.1.3.2.3.2.3.cmml\" xref=\"S5.T8.6.6.2.m2.1.1.3.2.3.2.3\">&#119870;</ci><ci id=\"S5.T8.6.6.2.m2.1.1.3.2.3.2.4.cmml\" xref=\"S5.T8.6.6.2.m2.1.1.3.2.3.2.4\">&#119878;</ci></apply><cn type=\"float\" id=\"S5.T8.6.6.2.m2.1.1.3.2.3.3.cmml\" xref=\"S5.T8.6.6.2.m2.1.1.3.2.3.3\">.75</cn></apply></apply><apply id=\"S5.T8.6.6.2.m2.1.1.3.3.cmml\" xref=\"S5.T8.6.6.2.m2.1.1.3.3\"><times id=\"S5.T8.6.6.2.m2.1.1.3.3.1.cmml\" xref=\"S5.T8.6.6.2.m2.1.1.3.3.1\"/><ci id=\"S5.T8.6.6.2.m2.1.1.3.3.2.cmml\" xref=\"S5.T8.6.6.2.m2.1.1.3.3.2\">&#119888;</ci><ci id=\"S5.T8.6.6.2.m2.1.1.3.3.3.cmml\" xref=\"S5.T8.6.6.2.m2.1.1.3.3.3\">&#119900;</ci><ci id=\"S5.T8.6.6.2.m2.1.1.3.3.4.cmml\" xref=\"S5.T8.6.6.2.m2.1.1.3.3.4\">&#119888;</ci><ci id=\"S5.T8.6.6.2.m2.1.1.3.3.5.cmml\" xref=\"S5.T8.6.6.2.m2.1.1.3.3.5\">&#119900;</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.6.6.2.m2.1c\">AP^{OKS=.75}_{coco}</annotation></semantics></math><span id=\"S5.T8.6.6.2.2\" class=\"ltx_text\" style=\"font-size:80%;\">: at OKS=.75 (strict metric)</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.12.12\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.12.12.6\" class=\"ltx_td ltx_align_left ltx_border_r\">&#13;\n<math id=\"S5.T8.7.7.1.m1.1\" class=\"ltx_Math\" alttext=\"\\bullet\" display=\"inline\"><semantics id=\"S5.T8.7.7.1.m1.1a\"><mo mathsize=\"80%\" id=\"S5.T8.7.7.1.m1.1.1\" xref=\"S5.T8.7.7.1.m1.1.1.cmml\">&#8729;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.7.7.1.m1.1b\"><ci id=\"S5.T8.7.7.1.m1.1.1.cmml\" xref=\"S5.T8.7.7.1.m1.1.1\">&#8729;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.7.7.1.m1.1c\">\\bullet</annotation></semantics></math><span id=\"S5.T8.12.12.6.1\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S5.T8.8.8.2.m2.1\" class=\"ltx_Math\" alttext=\"AP^{medium}_{coco}\" display=\"inline\"><semantics id=\"S5.T8.8.8.2.m2.1a\"><mrow id=\"S5.T8.8.8.2.m2.1.1\" xref=\"S5.T8.8.8.2.m2.1.1.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.8.8.2.m2.1.1.2\" xref=\"S5.T8.8.8.2.m2.1.1.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.8.8.2.m2.1.1.1\" xref=\"S5.T8.8.8.2.m2.1.1.1.cmml\">&#8203;</mo><msubsup id=\"S5.T8.8.8.2.m2.1.1.3\" xref=\"S5.T8.8.8.2.m2.1.1.3.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.8.8.2.m2.1.1.3.2.2\" xref=\"S5.T8.8.8.2.m2.1.1.3.2.2.cmml\">P</mi><mrow id=\"S5.T8.8.8.2.m2.1.1.3.3\" xref=\"S5.T8.8.8.2.m2.1.1.3.3.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.8.8.2.m2.1.1.3.3.2\" xref=\"S5.T8.8.8.2.m2.1.1.3.3.2.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.8.8.2.m2.1.1.3.3.1\" xref=\"S5.T8.8.8.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.8.8.2.m2.1.1.3.3.3\" xref=\"S5.T8.8.8.2.m2.1.1.3.3.3.cmml\">o</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.8.8.2.m2.1.1.3.3.1a\" xref=\"S5.T8.8.8.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.8.8.2.m2.1.1.3.3.4\" xref=\"S5.T8.8.8.2.m2.1.1.3.3.4.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.8.8.2.m2.1.1.3.3.1b\" xref=\"S5.T8.8.8.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.8.8.2.m2.1.1.3.3.5\" xref=\"S5.T8.8.8.2.m2.1.1.3.3.5.cmml\">o</mi></mrow><mrow id=\"S5.T8.8.8.2.m2.1.1.3.2.3\" xref=\"S5.T8.8.8.2.m2.1.1.3.2.3.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.8.8.2.m2.1.1.3.2.3.2\" xref=\"S5.T8.8.8.2.m2.1.1.3.2.3.2.cmml\">m</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.8.8.2.m2.1.1.3.2.3.1\" xref=\"S5.T8.8.8.2.m2.1.1.3.2.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.8.8.2.m2.1.1.3.2.3.3\" xref=\"S5.T8.8.8.2.m2.1.1.3.2.3.3.cmml\">e</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.8.8.2.m2.1.1.3.2.3.1a\" xref=\"S5.T8.8.8.2.m2.1.1.3.2.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.8.8.2.m2.1.1.3.2.3.4\" xref=\"S5.T8.8.8.2.m2.1.1.3.2.3.4.cmml\">d</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.8.8.2.m2.1.1.3.2.3.1b\" xref=\"S5.T8.8.8.2.m2.1.1.3.2.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.8.8.2.m2.1.1.3.2.3.5\" xref=\"S5.T8.8.8.2.m2.1.1.3.2.3.5.cmml\">i</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.8.8.2.m2.1.1.3.2.3.1c\" xref=\"S5.T8.8.8.2.m2.1.1.3.2.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.8.8.2.m2.1.1.3.2.3.6\" xref=\"S5.T8.8.8.2.m2.1.1.3.2.3.6.cmml\">u</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.8.8.2.m2.1.1.3.2.3.1d\" xref=\"S5.T8.8.8.2.m2.1.1.3.2.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.8.8.2.m2.1.1.3.2.3.7\" xref=\"S5.T8.8.8.2.m2.1.1.3.2.3.7.cmml\">m</mi></mrow></msubsup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.8.8.2.m2.1b\"><apply id=\"S5.T8.8.8.2.m2.1.1.cmml\" xref=\"S5.T8.8.8.2.m2.1.1\"><times id=\"S5.T8.8.8.2.m2.1.1.1.cmml\" xref=\"S5.T8.8.8.2.m2.1.1.1\"/><ci id=\"S5.T8.8.8.2.m2.1.1.2.cmml\" xref=\"S5.T8.8.8.2.m2.1.1.2\">&#119860;</ci><apply id=\"S5.T8.8.8.2.m2.1.1.3.cmml\" xref=\"S5.T8.8.8.2.m2.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S5.T8.8.8.2.m2.1.1.3.1.cmml\" xref=\"S5.T8.8.8.2.m2.1.1.3\">subscript</csymbol><apply id=\"S5.T8.8.8.2.m2.1.1.3.2.cmml\" xref=\"S5.T8.8.8.2.m2.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S5.T8.8.8.2.m2.1.1.3.2.1.cmml\" xref=\"S5.T8.8.8.2.m2.1.1.3\">superscript</csymbol><ci id=\"S5.T8.8.8.2.m2.1.1.3.2.2.cmml\" xref=\"S5.T8.8.8.2.m2.1.1.3.2.2\">&#119875;</ci><apply id=\"S5.T8.8.8.2.m2.1.1.3.2.3.cmml\" xref=\"S5.T8.8.8.2.m2.1.1.3.2.3\"><times id=\"S5.T8.8.8.2.m2.1.1.3.2.3.1.cmml\" xref=\"S5.T8.8.8.2.m2.1.1.3.2.3.1\"/><ci id=\"S5.T8.8.8.2.m2.1.1.3.2.3.2.cmml\" xref=\"S5.T8.8.8.2.m2.1.1.3.2.3.2\">&#119898;</ci><ci id=\"S5.T8.8.8.2.m2.1.1.3.2.3.3.cmml\" xref=\"S5.T8.8.8.2.m2.1.1.3.2.3.3\">&#119890;</ci><ci id=\"S5.T8.8.8.2.m2.1.1.3.2.3.4.cmml\" xref=\"S5.T8.8.8.2.m2.1.1.3.2.3.4\">&#119889;</ci><ci id=\"S5.T8.8.8.2.m2.1.1.3.2.3.5.cmml\" xref=\"S5.T8.8.8.2.m2.1.1.3.2.3.5\">&#119894;</ci><ci id=\"S5.T8.8.8.2.m2.1.1.3.2.3.6.cmml\" xref=\"S5.T8.8.8.2.m2.1.1.3.2.3.6\">&#119906;</ci><ci id=\"S5.T8.8.8.2.m2.1.1.3.2.3.7.cmml\" xref=\"S5.T8.8.8.2.m2.1.1.3.2.3.7\">&#119898;</ci></apply></apply><apply id=\"S5.T8.8.8.2.m2.1.1.3.3.cmml\" xref=\"S5.T8.8.8.2.m2.1.1.3.3\"><times id=\"S5.T8.8.8.2.m2.1.1.3.3.1.cmml\" xref=\"S5.T8.8.8.2.m2.1.1.3.3.1\"/><ci id=\"S5.T8.8.8.2.m2.1.1.3.3.2.cmml\" xref=\"S5.T8.8.8.2.m2.1.1.3.3.2\">&#119888;</ci><ci id=\"S5.T8.8.8.2.m2.1.1.3.3.3.cmml\" xref=\"S5.T8.8.8.2.m2.1.1.3.3.3\">&#119900;</ci><ci id=\"S5.T8.8.8.2.m2.1.1.3.3.4.cmml\" xref=\"S5.T8.8.8.2.m2.1.1.3.3.4\">&#119888;</ci><ci id=\"S5.T8.8.8.2.m2.1.1.3.3.5.cmml\" xref=\"S5.T8.8.8.2.m2.1.1.3.3.5\">&#119900;</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.8.8.2.m2.1c\">AP^{medium}_{coco}</annotation></semantics></math><span id=\"S5.T8.12.12.6.2\" class=\"ltx_text\" style=\"font-size:80%;\">: for medium objects: </span><math id=\"S5.T8.9.9.3.m3.1\" class=\"ltx_Math\" alttext=\"32^{2}\" display=\"inline\"><semantics id=\"S5.T8.9.9.3.m3.1a\"><msup id=\"S5.T8.9.9.3.m3.1.1\" xref=\"S5.T8.9.9.3.m3.1.1.cmml\"><mn mathsize=\"80%\" id=\"S5.T8.9.9.3.m3.1.1.2\" xref=\"S5.T8.9.9.3.m3.1.1.2.cmml\">32</mn><mn mathsize=\"80%\" id=\"S5.T8.9.9.3.m3.1.1.3\" xref=\"S5.T8.9.9.3.m3.1.1.3.cmml\">2</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.9.9.3.m3.1b\"><apply id=\"S5.T8.9.9.3.m3.1.1.cmml\" xref=\"S5.T8.9.9.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T8.9.9.3.m3.1.1.1.cmml\" xref=\"S5.T8.9.9.3.m3.1.1\">superscript</csymbol><cn type=\"integer\" id=\"S5.T8.9.9.3.m3.1.1.2.cmml\" xref=\"S5.T8.9.9.3.m3.1.1.2\">32</cn><cn type=\"integer\" id=\"S5.T8.9.9.3.m3.1.1.3.cmml\" xref=\"S5.T8.9.9.3.m3.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.9.9.3.m3.1c\">32^{2}</annotation></semantics></math><span id=\"S5.T8.12.12.6.3\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S5.T8.10.10.4.m4.1\" class=\"ltx_Math\" alttext=\"&lt;\" display=\"inline\"><semantics id=\"S5.T8.10.10.4.m4.1a\"><mo mathsize=\"80%\" id=\"S5.T8.10.10.4.m4.1.1\" xref=\"S5.T8.10.10.4.m4.1.1.cmml\">&lt;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.10.10.4.m4.1b\"><lt id=\"S5.T8.10.10.4.m4.1.1.cmml\" xref=\"S5.T8.10.10.4.m4.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.10.10.4.m4.1c\">&lt;</annotation></semantics></math><span id=\"S5.T8.12.12.6.4\" class=\"ltx_text\" style=\"font-size:80%;\"> area </span><math id=\"S5.T8.11.11.5.m5.1\" class=\"ltx_Math\" alttext=\"&lt;\" display=\"inline\"><semantics id=\"S5.T8.11.11.5.m5.1a\"><mo mathsize=\"80%\" id=\"S5.T8.11.11.5.m5.1.1\" xref=\"S5.T8.11.11.5.m5.1.1.cmml\">&lt;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.11.11.5.m5.1b\"><lt id=\"S5.T8.11.11.5.m5.1.1.cmml\" xref=\"S5.T8.11.11.5.m5.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.11.11.5.m5.1c\">&lt;</annotation></semantics></math><span id=\"S5.T8.12.12.6.5\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S5.T8.12.12.6.m6.1\" class=\"ltx_Math\" alttext=\"96^{2}\" display=\"inline\"><semantics id=\"S5.T8.12.12.6.m6.1a\"><msup id=\"S5.T8.12.12.6.m6.1.1\" xref=\"S5.T8.12.12.6.m6.1.1.cmml\"><mn mathsize=\"80%\" id=\"S5.T8.12.12.6.m6.1.1.2\" xref=\"S5.T8.12.12.6.m6.1.1.2.cmml\">96</mn><mn mathsize=\"80%\" id=\"S5.T8.12.12.6.m6.1.1.3\" xref=\"S5.T8.12.12.6.m6.1.1.3.cmml\">2</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.12.12.6.m6.1b\"><apply id=\"S5.T8.12.12.6.m6.1.1.cmml\" xref=\"S5.T8.12.12.6.m6.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T8.12.12.6.m6.1.1.1.cmml\" xref=\"S5.T8.12.12.6.m6.1.1\">superscript</csymbol><cn type=\"integer\" id=\"S5.T8.12.12.6.m6.1.1.2.cmml\" xref=\"S5.T8.12.12.6.m6.1.1.2\">96</cn><cn type=\"integer\" id=\"S5.T8.12.12.6.m6.1.1.3.cmml\" xref=\"S5.T8.12.12.6.m6.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.12.12.6.m6.1c\">96^{2}</annotation></semantics></math>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.16.16\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.16.16.4\" class=\"ltx_td ltx_align_left ltx_border_r\">&#13;\n<math id=\"S5.T8.13.13.1.m1.1\" class=\"ltx_Math\" alttext=\"\\bullet\" display=\"inline\"><semantics id=\"S5.T8.13.13.1.m1.1a\"><mo mathsize=\"80%\" id=\"S5.T8.13.13.1.m1.1.1\" xref=\"S5.T8.13.13.1.m1.1.1.cmml\">&#8729;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.13.13.1.m1.1b\"><ci id=\"S5.T8.13.13.1.m1.1.1.cmml\" xref=\"S5.T8.13.13.1.m1.1.1\">&#8729;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.13.13.1.m1.1c\">\\bullet</annotation></semantics></math><span id=\"S5.T8.16.16.4.1\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S5.T8.14.14.2.m2.1\" class=\"ltx_Math\" alttext=\"AP^{large}_{coco}\" display=\"inline\"><semantics id=\"S5.T8.14.14.2.m2.1a\"><mrow id=\"S5.T8.14.14.2.m2.1.1\" xref=\"S5.T8.14.14.2.m2.1.1.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.14.14.2.m2.1.1.2\" xref=\"S5.T8.14.14.2.m2.1.1.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.14.14.2.m2.1.1.1\" xref=\"S5.T8.14.14.2.m2.1.1.1.cmml\">&#8203;</mo><msubsup id=\"S5.T8.14.14.2.m2.1.1.3\" xref=\"S5.T8.14.14.2.m2.1.1.3.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.14.14.2.m2.1.1.3.2.2\" xref=\"S5.T8.14.14.2.m2.1.1.3.2.2.cmml\">P</mi><mrow id=\"S5.T8.14.14.2.m2.1.1.3.3\" xref=\"S5.T8.14.14.2.m2.1.1.3.3.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.14.14.2.m2.1.1.3.3.2\" xref=\"S5.T8.14.14.2.m2.1.1.3.3.2.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.14.14.2.m2.1.1.3.3.1\" xref=\"S5.T8.14.14.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.14.14.2.m2.1.1.3.3.3\" xref=\"S5.T8.14.14.2.m2.1.1.3.3.3.cmml\">o</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.14.14.2.m2.1.1.3.3.1a\" xref=\"S5.T8.14.14.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.14.14.2.m2.1.1.3.3.4\" xref=\"S5.T8.14.14.2.m2.1.1.3.3.4.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.14.14.2.m2.1.1.3.3.1b\" xref=\"S5.T8.14.14.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.14.14.2.m2.1.1.3.3.5\" xref=\"S5.T8.14.14.2.m2.1.1.3.3.5.cmml\">o</mi></mrow><mrow id=\"S5.T8.14.14.2.m2.1.1.3.2.3\" xref=\"S5.T8.14.14.2.m2.1.1.3.2.3.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.14.14.2.m2.1.1.3.2.3.2\" xref=\"S5.T8.14.14.2.m2.1.1.3.2.3.2.cmml\">l</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.14.14.2.m2.1.1.3.2.3.1\" xref=\"S5.T8.14.14.2.m2.1.1.3.2.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.14.14.2.m2.1.1.3.2.3.3\" xref=\"S5.T8.14.14.2.m2.1.1.3.2.3.3.cmml\">a</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.14.14.2.m2.1.1.3.2.3.1a\" xref=\"S5.T8.14.14.2.m2.1.1.3.2.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.14.14.2.m2.1.1.3.2.3.4\" xref=\"S5.T8.14.14.2.m2.1.1.3.2.3.4.cmml\">r</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.14.14.2.m2.1.1.3.2.3.1b\" xref=\"S5.T8.14.14.2.m2.1.1.3.2.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.14.14.2.m2.1.1.3.2.3.5\" xref=\"S5.T8.14.14.2.m2.1.1.3.2.3.5.cmml\">g</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.14.14.2.m2.1.1.3.2.3.1c\" xref=\"S5.T8.14.14.2.m2.1.1.3.2.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.14.14.2.m2.1.1.3.2.3.6\" xref=\"S5.T8.14.14.2.m2.1.1.3.2.3.6.cmml\">e</mi></mrow></msubsup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.14.14.2.m2.1b\"><apply id=\"S5.T8.14.14.2.m2.1.1.cmml\" xref=\"S5.T8.14.14.2.m2.1.1\"><times id=\"S5.T8.14.14.2.m2.1.1.1.cmml\" xref=\"S5.T8.14.14.2.m2.1.1.1\"/><ci id=\"S5.T8.14.14.2.m2.1.1.2.cmml\" xref=\"S5.T8.14.14.2.m2.1.1.2\">&#119860;</ci><apply id=\"S5.T8.14.14.2.m2.1.1.3.cmml\" xref=\"S5.T8.14.14.2.m2.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S5.T8.14.14.2.m2.1.1.3.1.cmml\" xref=\"S5.T8.14.14.2.m2.1.1.3\">subscript</csymbol><apply id=\"S5.T8.14.14.2.m2.1.1.3.2.cmml\" xref=\"S5.T8.14.14.2.m2.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S5.T8.14.14.2.m2.1.1.3.2.1.cmml\" xref=\"S5.T8.14.14.2.m2.1.1.3\">superscript</csymbol><ci id=\"S5.T8.14.14.2.m2.1.1.3.2.2.cmml\" xref=\"S5.T8.14.14.2.m2.1.1.3.2.2\">&#119875;</ci><apply id=\"S5.T8.14.14.2.m2.1.1.3.2.3.cmml\" xref=\"S5.T8.14.14.2.m2.1.1.3.2.3\"><times id=\"S5.T8.14.14.2.m2.1.1.3.2.3.1.cmml\" xref=\"S5.T8.14.14.2.m2.1.1.3.2.3.1\"/><ci id=\"S5.T8.14.14.2.m2.1.1.3.2.3.2.cmml\" xref=\"S5.T8.14.14.2.m2.1.1.3.2.3.2\">&#119897;</ci><ci id=\"S5.T8.14.14.2.m2.1.1.3.2.3.3.cmml\" xref=\"S5.T8.14.14.2.m2.1.1.3.2.3.3\">&#119886;</ci><ci id=\"S5.T8.14.14.2.m2.1.1.3.2.3.4.cmml\" xref=\"S5.T8.14.14.2.m2.1.1.3.2.3.4\">&#119903;</ci><ci id=\"S5.T8.14.14.2.m2.1.1.3.2.3.5.cmml\" xref=\"S5.T8.14.14.2.m2.1.1.3.2.3.5\">&#119892;</ci><ci id=\"S5.T8.14.14.2.m2.1.1.3.2.3.6.cmml\" xref=\"S5.T8.14.14.2.m2.1.1.3.2.3.6\">&#119890;</ci></apply></apply><apply id=\"S5.T8.14.14.2.m2.1.1.3.3.cmml\" xref=\"S5.T8.14.14.2.m2.1.1.3.3\"><times id=\"S5.T8.14.14.2.m2.1.1.3.3.1.cmml\" xref=\"S5.T8.14.14.2.m2.1.1.3.3.1\"/><ci id=\"S5.T8.14.14.2.m2.1.1.3.3.2.cmml\" xref=\"S5.T8.14.14.2.m2.1.1.3.3.2\">&#119888;</ci><ci id=\"S5.T8.14.14.2.m2.1.1.3.3.3.cmml\" xref=\"S5.T8.14.14.2.m2.1.1.3.3.3\">&#119900;</ci><ci id=\"S5.T8.14.14.2.m2.1.1.3.3.4.cmml\" xref=\"S5.T8.14.14.2.m2.1.1.3.3.4\">&#119888;</ci><ci id=\"S5.T8.14.14.2.m2.1.1.3.3.5.cmml\" xref=\"S5.T8.14.14.2.m2.1.1.3.3.5\">&#119900;</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.14.14.2.m2.1c\">AP^{large}_{coco}</annotation></semantics></math><span id=\"S5.T8.16.16.4.2\" class=\"ltx_text\" style=\"font-size:80%;\">: for large objects: area </span><math id=\"S5.T8.15.15.3.m3.1\" class=\"ltx_Math\" alttext=\"&gt;\" display=\"inline\"><semantics id=\"S5.T8.15.15.3.m3.1a\"><mo mathsize=\"80%\" id=\"S5.T8.15.15.3.m3.1.1\" xref=\"S5.T8.15.15.3.m3.1.1.cmml\">&gt;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.15.15.3.m3.1b\"><gt id=\"S5.T8.15.15.3.m3.1.1.cmml\" xref=\"S5.T8.15.15.3.m3.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.15.15.3.m3.1c\">&gt;</annotation></semantics></math><span id=\"S5.T8.16.16.4.3\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S5.T8.16.16.4.m4.1\" class=\"ltx_Math\" alttext=\"96^{2}\" display=\"inline\"><semantics id=\"S5.T8.16.16.4.m4.1a\"><msup id=\"S5.T8.16.16.4.m4.1.1\" xref=\"S5.T8.16.16.4.m4.1.1.cmml\"><mn mathsize=\"80%\" id=\"S5.T8.16.16.4.m4.1.1.2\" xref=\"S5.T8.16.16.4.m4.1.1.2.cmml\">96</mn><mn mathsize=\"80%\" id=\"S5.T8.16.16.4.m4.1.1.3\" xref=\"S5.T8.16.16.4.m4.1.1.3.cmml\">2</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.16.16.4.m4.1b\"><apply id=\"S5.T8.16.16.4.m4.1.1.cmml\" xref=\"S5.T8.16.16.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T8.16.16.4.m4.1.1.1.cmml\" xref=\"S5.T8.16.16.4.m4.1.1\">superscript</csymbol><cn type=\"integer\" id=\"S5.T8.16.16.4.m4.1.1.2.cmml\" xref=\"S5.T8.16.16.4.m4.1.1.2\">96</cn><cn type=\"integer\" id=\"S5.T8.16.16.4.m4.1.1.3.cmml\" xref=\"S5.T8.16.16.4.m4.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.16.16.4.m4.1c\">96^{2}</annotation></semantics></math>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.18.18\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.18.18.3\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"5\"><span id=\"S5.T8.18.18.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">AR</span></td>&#13;\n<td id=\"S5.T8.18.18.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"5\"><span id=\"S5.T8.18.18.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#13;\n<span id=\"S5.T8.18.18.4.1.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<span id=\"S5.T8.18.18.4.1.1.1\" class=\"ltx_tr\">&#13;\n<span id=\"S5.T8.18.18.4.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Average</span></span>&#13;\n<span id=\"S5.T8.18.18.4.1.1.2\" class=\"ltx_tr\">&#13;\n<span id=\"S5.T8.18.18.4.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Recall</span></span>&#13;\n</span></span></td>&#13;\n<td id=\"S5.T8.18.18.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"5\"><span id=\"S5.T8.18.18.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">&#13;\n<span id=\"S5.T8.18.18.5.1.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<span id=\"S5.T8.18.18.5.1.1.1\" class=\"ltx_tr\">&#13;\n<span id=\"S5.T8.18.18.5.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">COCO</span></span>&#13;\n</span></span></td>&#13;\n<td id=\"S5.T8.18.18.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">&#13;\n<math id=\"S5.T8.17.17.1.m1.1\" class=\"ltx_Math\" alttext=\"\\bullet\" display=\"inline\"><semantics id=\"S5.T8.17.17.1.m1.1a\"><mo mathsize=\"80%\" id=\"S5.T8.17.17.1.m1.1.1\" xref=\"S5.T8.17.17.1.m1.1.1.cmml\">&#8729;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.17.17.1.m1.1b\"><ci id=\"S5.T8.17.17.1.m1.1.1.cmml\" xref=\"S5.T8.17.17.1.m1.1.1\">&#8729;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.17.17.1.m1.1c\">\\bullet</annotation></semantics></math><span id=\"S5.T8.18.18.2.1\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S5.T8.18.18.2.m2.1\" class=\"ltx_Math\" alttext=\"AR_{coco}\" display=\"inline\"><semantics id=\"S5.T8.18.18.2.m2.1a\"><mrow id=\"S5.T8.18.18.2.m2.1.1\" xref=\"S5.T8.18.18.2.m2.1.1.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.18.18.2.m2.1.1.2\" xref=\"S5.T8.18.18.2.m2.1.1.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.18.18.2.m2.1.1.1\" xref=\"S5.T8.18.18.2.m2.1.1.1.cmml\">&#8203;</mo><msub id=\"S5.T8.18.18.2.m2.1.1.3\" xref=\"S5.T8.18.18.2.m2.1.1.3.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.18.18.2.m2.1.1.3.2\" xref=\"S5.T8.18.18.2.m2.1.1.3.2.cmml\">R</mi><mrow id=\"S5.T8.18.18.2.m2.1.1.3.3\" xref=\"S5.T8.18.18.2.m2.1.1.3.3.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.18.18.2.m2.1.1.3.3.2\" xref=\"S5.T8.18.18.2.m2.1.1.3.3.2.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.18.18.2.m2.1.1.3.3.1\" xref=\"S5.T8.18.18.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.18.18.2.m2.1.1.3.3.3\" xref=\"S5.T8.18.18.2.m2.1.1.3.3.3.cmml\">o</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.18.18.2.m2.1.1.3.3.1a\" xref=\"S5.T8.18.18.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.18.18.2.m2.1.1.3.3.4\" xref=\"S5.T8.18.18.2.m2.1.1.3.3.4.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.18.18.2.m2.1.1.3.3.1b\" xref=\"S5.T8.18.18.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.18.18.2.m2.1.1.3.3.5\" xref=\"S5.T8.18.18.2.m2.1.1.3.3.5.cmml\">o</mi></mrow></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.18.18.2.m2.1b\"><apply id=\"S5.T8.18.18.2.m2.1.1.cmml\" xref=\"S5.T8.18.18.2.m2.1.1\"><times id=\"S5.T8.18.18.2.m2.1.1.1.cmml\" xref=\"S5.T8.18.18.2.m2.1.1.1\"/><ci id=\"S5.T8.18.18.2.m2.1.1.2.cmml\" xref=\"S5.T8.18.18.2.m2.1.1.2\">&#119860;</ci><apply id=\"S5.T8.18.18.2.m2.1.1.3.cmml\" xref=\"S5.T8.18.18.2.m2.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S5.T8.18.18.2.m2.1.1.3.1.cmml\" xref=\"S5.T8.18.18.2.m2.1.1.3\">subscript</csymbol><ci id=\"S5.T8.18.18.2.m2.1.1.3.2.cmml\" xref=\"S5.T8.18.18.2.m2.1.1.3.2\">&#119877;</ci><apply id=\"S5.T8.18.18.2.m2.1.1.3.3.cmml\" xref=\"S5.T8.18.18.2.m2.1.1.3.3\"><times id=\"S5.T8.18.18.2.m2.1.1.3.3.1.cmml\" xref=\"S5.T8.18.18.2.m2.1.1.3.3.1\"/><ci id=\"S5.T8.18.18.2.m2.1.1.3.3.2.cmml\" xref=\"S5.T8.18.18.2.m2.1.1.3.3.2\">&#119888;</ci><ci id=\"S5.T8.18.18.2.m2.1.1.3.3.3.cmml\" xref=\"S5.T8.18.18.2.m2.1.1.3.3.3\">&#119900;</ci><ci id=\"S5.T8.18.18.2.m2.1.1.3.3.4.cmml\" xref=\"S5.T8.18.18.2.m2.1.1.3.3.4\">&#119888;</ci><ci id=\"S5.T8.18.18.2.m2.1.1.3.3.5.cmml\" xref=\"S5.T8.18.18.2.m2.1.1.3.3.5\">&#119900;</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.18.18.2.m2.1c\">AR_{coco}</annotation></semantics></math><span id=\"S5.T8.18.18.2.2\" class=\"ltx_text\" style=\"font-size:80%;\">: at OKS=.50:.05:.95</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.20.20\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.20.20.2\" class=\"ltx_td ltx_align_left ltx_border_r\">&#13;\n<math id=\"S5.T8.19.19.1.m1.1\" class=\"ltx_Math\" alttext=\"\\bullet\" display=\"inline\"><semantics id=\"S5.T8.19.19.1.m1.1a\"><mo mathsize=\"80%\" id=\"S5.T8.19.19.1.m1.1.1\" xref=\"S5.T8.19.19.1.m1.1.1.cmml\">&#8729;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.19.19.1.m1.1b\"><ci id=\"S5.T8.19.19.1.m1.1.1.cmml\" xref=\"S5.T8.19.19.1.m1.1.1\">&#8729;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.19.19.1.m1.1c\">\\bullet</annotation></semantics></math><span id=\"S5.T8.20.20.2.1\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S5.T8.20.20.2.m2.1\" class=\"ltx_Math\" alttext=\"AR^{OKS=.50}_{coco}\" display=\"inline\"><semantics id=\"S5.T8.20.20.2.m2.1a\"><mrow id=\"S5.T8.20.20.2.m2.1.1\" xref=\"S5.T8.20.20.2.m2.1.1.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.20.20.2.m2.1.1.2\" xref=\"S5.T8.20.20.2.m2.1.1.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.20.20.2.m2.1.1.1\" xref=\"S5.T8.20.20.2.m2.1.1.1.cmml\">&#8203;</mo><msubsup id=\"S5.T8.20.20.2.m2.1.1.3\" xref=\"S5.T8.20.20.2.m2.1.1.3.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.20.20.2.m2.1.1.3.2.2\" xref=\"S5.T8.20.20.2.m2.1.1.3.2.2.cmml\">R</mi><mrow id=\"S5.T8.20.20.2.m2.1.1.3.3\" xref=\"S5.T8.20.20.2.m2.1.1.3.3.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.20.20.2.m2.1.1.3.3.2\" xref=\"S5.T8.20.20.2.m2.1.1.3.3.2.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.20.20.2.m2.1.1.3.3.1\" xref=\"S5.T8.20.20.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.20.20.2.m2.1.1.3.3.3\" xref=\"S5.T8.20.20.2.m2.1.1.3.3.3.cmml\">o</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.20.20.2.m2.1.1.3.3.1a\" xref=\"S5.T8.20.20.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.20.20.2.m2.1.1.3.3.4\" xref=\"S5.T8.20.20.2.m2.1.1.3.3.4.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.20.20.2.m2.1.1.3.3.1b\" xref=\"S5.T8.20.20.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.20.20.2.m2.1.1.3.3.5\" xref=\"S5.T8.20.20.2.m2.1.1.3.3.5.cmml\">o</mi></mrow><mrow id=\"S5.T8.20.20.2.m2.1.1.3.2.3\" xref=\"S5.T8.20.20.2.m2.1.1.3.2.3.cmml\"><mrow id=\"S5.T8.20.20.2.m2.1.1.3.2.3.2\" xref=\"S5.T8.20.20.2.m2.1.1.3.2.3.2.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.20.20.2.m2.1.1.3.2.3.2.2\" xref=\"S5.T8.20.20.2.m2.1.1.3.2.3.2.2.cmml\">O</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.20.20.2.m2.1.1.3.2.3.2.1\" xref=\"S5.T8.20.20.2.m2.1.1.3.2.3.2.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.20.20.2.m2.1.1.3.2.3.2.3\" xref=\"S5.T8.20.20.2.m2.1.1.3.2.3.2.3.cmml\">K</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.20.20.2.m2.1.1.3.2.3.2.1a\" xref=\"S5.T8.20.20.2.m2.1.1.3.2.3.2.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.20.20.2.m2.1.1.3.2.3.2.4\" xref=\"S5.T8.20.20.2.m2.1.1.3.2.3.2.4.cmml\">S</mi></mrow><mo mathsize=\"80%\" id=\"S5.T8.20.20.2.m2.1.1.3.2.3.1\" xref=\"S5.T8.20.20.2.m2.1.1.3.2.3.1.cmml\">=</mo><mn mathsize=\"80%\" id=\"S5.T8.20.20.2.m2.1.1.3.2.3.3\" xref=\"S5.T8.20.20.2.m2.1.1.3.2.3.3.cmml\">.50</mn></mrow></msubsup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.20.20.2.m2.1b\"><apply id=\"S5.T8.20.20.2.m2.1.1.cmml\" xref=\"S5.T8.20.20.2.m2.1.1\"><times id=\"S5.T8.20.20.2.m2.1.1.1.cmml\" xref=\"S5.T8.20.20.2.m2.1.1.1\"/><ci id=\"S5.T8.20.20.2.m2.1.1.2.cmml\" xref=\"S5.T8.20.20.2.m2.1.1.2\">&#119860;</ci><apply id=\"S5.T8.20.20.2.m2.1.1.3.cmml\" xref=\"S5.T8.20.20.2.m2.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S5.T8.20.20.2.m2.1.1.3.1.cmml\" xref=\"S5.T8.20.20.2.m2.1.1.3\">subscript</csymbol><apply id=\"S5.T8.20.20.2.m2.1.1.3.2.cmml\" xref=\"S5.T8.20.20.2.m2.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S5.T8.20.20.2.m2.1.1.3.2.1.cmml\" xref=\"S5.T8.20.20.2.m2.1.1.3\">superscript</csymbol><ci id=\"S5.T8.20.20.2.m2.1.1.3.2.2.cmml\" xref=\"S5.T8.20.20.2.m2.1.1.3.2.2\">&#119877;</ci><apply id=\"S5.T8.20.20.2.m2.1.1.3.2.3.cmml\" xref=\"S5.T8.20.20.2.m2.1.1.3.2.3\"><eq id=\"S5.T8.20.20.2.m2.1.1.3.2.3.1.cmml\" xref=\"S5.T8.20.20.2.m2.1.1.3.2.3.1\"/><apply id=\"S5.T8.20.20.2.m2.1.1.3.2.3.2.cmml\" xref=\"S5.T8.20.20.2.m2.1.1.3.2.3.2\"><times id=\"S5.T8.20.20.2.m2.1.1.3.2.3.2.1.cmml\" xref=\"S5.T8.20.20.2.m2.1.1.3.2.3.2.1\"/><ci id=\"S5.T8.20.20.2.m2.1.1.3.2.3.2.2.cmml\" xref=\"S5.T8.20.20.2.m2.1.1.3.2.3.2.2\">&#119874;</ci><ci id=\"S5.T8.20.20.2.m2.1.1.3.2.3.2.3.cmml\" xref=\"S5.T8.20.20.2.m2.1.1.3.2.3.2.3\">&#119870;</ci><ci id=\"S5.T8.20.20.2.m2.1.1.3.2.3.2.4.cmml\" xref=\"S5.T8.20.20.2.m2.1.1.3.2.3.2.4\">&#119878;</ci></apply><cn type=\"float\" id=\"S5.T8.20.20.2.m2.1.1.3.2.3.3.cmml\" xref=\"S5.T8.20.20.2.m2.1.1.3.2.3.3\">.50</cn></apply></apply><apply id=\"S5.T8.20.20.2.m2.1.1.3.3.cmml\" xref=\"S5.T8.20.20.2.m2.1.1.3.3\"><times id=\"S5.T8.20.20.2.m2.1.1.3.3.1.cmml\" xref=\"S5.T8.20.20.2.m2.1.1.3.3.1\"/><ci id=\"S5.T8.20.20.2.m2.1.1.3.3.2.cmml\" xref=\"S5.T8.20.20.2.m2.1.1.3.3.2\">&#119888;</ci><ci id=\"S5.T8.20.20.2.m2.1.1.3.3.3.cmml\" xref=\"S5.T8.20.20.2.m2.1.1.3.3.3\">&#119900;</ci><ci id=\"S5.T8.20.20.2.m2.1.1.3.3.4.cmml\" xref=\"S5.T8.20.20.2.m2.1.1.3.3.4\">&#119888;</ci><ci id=\"S5.T8.20.20.2.m2.1.1.3.3.5.cmml\" xref=\"S5.T8.20.20.2.m2.1.1.3.3.5\">&#119900;</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.20.20.2.m2.1c\">AR^{OKS=.50}_{coco}</annotation></semantics></math><span id=\"S5.T8.20.20.2.2\" class=\"ltx_text\" style=\"font-size:80%;\">: at OKS=.50</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.22.22\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.22.22.2\" class=\"ltx_td ltx_align_left ltx_border_r\">&#13;\n<math id=\"S5.T8.21.21.1.m1.1\" class=\"ltx_Math\" alttext=\"\\bullet\" display=\"inline\"><semantics id=\"S5.T8.21.21.1.m1.1a\"><mo mathsize=\"80%\" id=\"S5.T8.21.21.1.m1.1.1\" xref=\"S5.T8.21.21.1.m1.1.1.cmml\">&#8729;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.21.21.1.m1.1b\"><ci id=\"S5.T8.21.21.1.m1.1.1.cmml\" xref=\"S5.T8.21.21.1.m1.1.1\">&#8729;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.21.21.1.m1.1c\">\\bullet</annotation></semantics></math><span id=\"S5.T8.22.22.2.1\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S5.T8.22.22.2.m2.1\" class=\"ltx_Math\" alttext=\"AR^{OKS=.75}_{coco}\" display=\"inline\"><semantics id=\"S5.T8.22.22.2.m2.1a\"><mrow id=\"S5.T8.22.22.2.m2.1.1\" xref=\"S5.T8.22.22.2.m2.1.1.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.22.22.2.m2.1.1.2\" xref=\"S5.T8.22.22.2.m2.1.1.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.22.22.2.m2.1.1.1\" xref=\"S5.T8.22.22.2.m2.1.1.1.cmml\">&#8203;</mo><msubsup id=\"S5.T8.22.22.2.m2.1.1.3\" xref=\"S5.T8.22.22.2.m2.1.1.3.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.22.22.2.m2.1.1.3.2.2\" xref=\"S5.T8.22.22.2.m2.1.1.3.2.2.cmml\">R</mi><mrow id=\"S5.T8.22.22.2.m2.1.1.3.3\" xref=\"S5.T8.22.22.2.m2.1.1.3.3.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.22.22.2.m2.1.1.3.3.2\" xref=\"S5.T8.22.22.2.m2.1.1.3.3.2.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.22.22.2.m2.1.1.3.3.1\" xref=\"S5.T8.22.22.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.22.22.2.m2.1.1.3.3.3\" xref=\"S5.T8.22.22.2.m2.1.1.3.3.3.cmml\">o</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.22.22.2.m2.1.1.3.3.1a\" xref=\"S5.T8.22.22.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.22.22.2.m2.1.1.3.3.4\" xref=\"S5.T8.22.22.2.m2.1.1.3.3.4.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.22.22.2.m2.1.1.3.3.1b\" xref=\"S5.T8.22.22.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.22.22.2.m2.1.1.3.3.5\" xref=\"S5.T8.22.22.2.m2.1.1.3.3.5.cmml\">o</mi></mrow><mrow id=\"S5.T8.22.22.2.m2.1.1.3.2.3\" xref=\"S5.T8.22.22.2.m2.1.1.3.2.3.cmml\"><mrow id=\"S5.T8.22.22.2.m2.1.1.3.2.3.2\" xref=\"S5.T8.22.22.2.m2.1.1.3.2.3.2.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.22.22.2.m2.1.1.3.2.3.2.2\" xref=\"S5.T8.22.22.2.m2.1.1.3.2.3.2.2.cmml\">O</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.22.22.2.m2.1.1.3.2.3.2.1\" xref=\"S5.T8.22.22.2.m2.1.1.3.2.3.2.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.22.22.2.m2.1.1.3.2.3.2.3\" xref=\"S5.T8.22.22.2.m2.1.1.3.2.3.2.3.cmml\">K</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.22.22.2.m2.1.1.3.2.3.2.1a\" xref=\"S5.T8.22.22.2.m2.1.1.3.2.3.2.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.22.22.2.m2.1.1.3.2.3.2.4\" xref=\"S5.T8.22.22.2.m2.1.1.3.2.3.2.4.cmml\">S</mi></mrow><mo mathsize=\"80%\" id=\"S5.T8.22.22.2.m2.1.1.3.2.3.1\" xref=\"S5.T8.22.22.2.m2.1.1.3.2.3.1.cmml\">=</mo><mn mathsize=\"80%\" id=\"S5.T8.22.22.2.m2.1.1.3.2.3.3\" xref=\"S5.T8.22.22.2.m2.1.1.3.2.3.3.cmml\">.75</mn></mrow></msubsup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.22.22.2.m2.1b\"><apply id=\"S5.T8.22.22.2.m2.1.1.cmml\" xref=\"S5.T8.22.22.2.m2.1.1\"><times id=\"S5.T8.22.22.2.m2.1.1.1.cmml\" xref=\"S5.T8.22.22.2.m2.1.1.1\"/><ci id=\"S5.T8.22.22.2.m2.1.1.2.cmml\" xref=\"S5.T8.22.22.2.m2.1.1.2\">&#119860;</ci><apply id=\"S5.T8.22.22.2.m2.1.1.3.cmml\" xref=\"S5.T8.22.22.2.m2.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S5.T8.22.22.2.m2.1.1.3.1.cmml\" xref=\"S5.T8.22.22.2.m2.1.1.3\">subscript</csymbol><apply id=\"S5.T8.22.22.2.m2.1.1.3.2.cmml\" xref=\"S5.T8.22.22.2.m2.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S5.T8.22.22.2.m2.1.1.3.2.1.cmml\" xref=\"S5.T8.22.22.2.m2.1.1.3\">superscript</csymbol><ci id=\"S5.T8.22.22.2.m2.1.1.3.2.2.cmml\" xref=\"S5.T8.22.22.2.m2.1.1.3.2.2\">&#119877;</ci><apply id=\"S5.T8.22.22.2.m2.1.1.3.2.3.cmml\" xref=\"S5.T8.22.22.2.m2.1.1.3.2.3\"><eq id=\"S5.T8.22.22.2.m2.1.1.3.2.3.1.cmml\" xref=\"S5.T8.22.22.2.m2.1.1.3.2.3.1\"/><apply id=\"S5.T8.22.22.2.m2.1.1.3.2.3.2.cmml\" xref=\"S5.T8.22.22.2.m2.1.1.3.2.3.2\"><times id=\"S5.T8.22.22.2.m2.1.1.3.2.3.2.1.cmml\" xref=\"S5.T8.22.22.2.m2.1.1.3.2.3.2.1\"/><ci id=\"S5.T8.22.22.2.m2.1.1.3.2.3.2.2.cmml\" xref=\"S5.T8.22.22.2.m2.1.1.3.2.3.2.2\">&#119874;</ci><ci id=\"S5.T8.22.22.2.m2.1.1.3.2.3.2.3.cmml\" xref=\"S5.T8.22.22.2.m2.1.1.3.2.3.2.3\">&#119870;</ci><ci id=\"S5.T8.22.22.2.m2.1.1.3.2.3.2.4.cmml\" xref=\"S5.T8.22.22.2.m2.1.1.3.2.3.2.4\">&#119878;</ci></apply><cn type=\"float\" id=\"S5.T8.22.22.2.m2.1.1.3.2.3.3.cmml\" xref=\"S5.T8.22.22.2.m2.1.1.3.2.3.3\">.75</cn></apply></apply><apply id=\"S5.T8.22.22.2.m2.1.1.3.3.cmml\" xref=\"S5.T8.22.22.2.m2.1.1.3.3\"><times id=\"S5.T8.22.22.2.m2.1.1.3.3.1.cmml\" xref=\"S5.T8.22.22.2.m2.1.1.3.3.1\"/><ci id=\"S5.T8.22.22.2.m2.1.1.3.3.2.cmml\" xref=\"S5.T8.22.22.2.m2.1.1.3.3.2\">&#119888;</ci><ci id=\"S5.T8.22.22.2.m2.1.1.3.3.3.cmml\" xref=\"S5.T8.22.22.2.m2.1.1.3.3.3\">&#119900;</ci><ci id=\"S5.T8.22.22.2.m2.1.1.3.3.4.cmml\" xref=\"S5.T8.22.22.2.m2.1.1.3.3.4\">&#119888;</ci><ci id=\"S5.T8.22.22.2.m2.1.1.3.3.5.cmml\" xref=\"S5.T8.22.22.2.m2.1.1.3.3.5\">&#119900;</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.22.22.2.m2.1c\">AR^{OKS=.75}_{coco}</annotation></semantics></math><span id=\"S5.T8.22.22.2.2\" class=\"ltx_text\" style=\"font-size:80%;\">: at OKS=.75</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.28.28\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.28.28.6\" class=\"ltx_td ltx_align_left ltx_border_r\">&#13;\n<math id=\"S5.T8.23.23.1.m1.1\" class=\"ltx_Math\" alttext=\"\\bullet\" display=\"inline\"><semantics id=\"S5.T8.23.23.1.m1.1a\"><mo mathsize=\"80%\" id=\"S5.T8.23.23.1.m1.1.1\" xref=\"S5.T8.23.23.1.m1.1.1.cmml\">&#8729;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.23.23.1.m1.1b\"><ci id=\"S5.T8.23.23.1.m1.1.1.cmml\" xref=\"S5.T8.23.23.1.m1.1.1\">&#8729;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.23.23.1.m1.1c\">\\bullet</annotation></semantics></math><span id=\"S5.T8.28.28.6.1\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S5.T8.24.24.2.m2.1\" class=\"ltx_Math\" alttext=\"AR^{medium}_{coco}\" display=\"inline\"><semantics id=\"S5.T8.24.24.2.m2.1a\"><mrow id=\"S5.T8.24.24.2.m2.1.1\" xref=\"S5.T8.24.24.2.m2.1.1.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.24.24.2.m2.1.1.2\" xref=\"S5.T8.24.24.2.m2.1.1.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.24.24.2.m2.1.1.1\" xref=\"S5.T8.24.24.2.m2.1.1.1.cmml\">&#8203;</mo><msubsup id=\"S5.T8.24.24.2.m2.1.1.3\" xref=\"S5.T8.24.24.2.m2.1.1.3.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.24.24.2.m2.1.1.3.2.2\" xref=\"S5.T8.24.24.2.m2.1.1.3.2.2.cmml\">R</mi><mrow id=\"S5.T8.24.24.2.m2.1.1.3.3\" xref=\"S5.T8.24.24.2.m2.1.1.3.3.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.24.24.2.m2.1.1.3.3.2\" xref=\"S5.T8.24.24.2.m2.1.1.3.3.2.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.24.24.2.m2.1.1.3.3.1\" xref=\"S5.T8.24.24.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.24.24.2.m2.1.1.3.3.3\" xref=\"S5.T8.24.24.2.m2.1.1.3.3.3.cmml\">o</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.24.24.2.m2.1.1.3.3.1a\" xref=\"S5.T8.24.24.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.24.24.2.m2.1.1.3.3.4\" xref=\"S5.T8.24.24.2.m2.1.1.3.3.4.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.24.24.2.m2.1.1.3.3.1b\" xref=\"S5.T8.24.24.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.24.24.2.m2.1.1.3.3.5\" xref=\"S5.T8.24.24.2.m2.1.1.3.3.5.cmml\">o</mi></mrow><mrow id=\"S5.T8.24.24.2.m2.1.1.3.2.3\" xref=\"S5.T8.24.24.2.m2.1.1.3.2.3.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.24.24.2.m2.1.1.3.2.3.2\" xref=\"S5.T8.24.24.2.m2.1.1.3.2.3.2.cmml\">m</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.24.24.2.m2.1.1.3.2.3.1\" xref=\"S5.T8.24.24.2.m2.1.1.3.2.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.24.24.2.m2.1.1.3.2.3.3\" xref=\"S5.T8.24.24.2.m2.1.1.3.2.3.3.cmml\">e</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.24.24.2.m2.1.1.3.2.3.1a\" xref=\"S5.T8.24.24.2.m2.1.1.3.2.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.24.24.2.m2.1.1.3.2.3.4\" xref=\"S5.T8.24.24.2.m2.1.1.3.2.3.4.cmml\">d</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.24.24.2.m2.1.1.3.2.3.1b\" xref=\"S5.T8.24.24.2.m2.1.1.3.2.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.24.24.2.m2.1.1.3.2.3.5\" xref=\"S5.T8.24.24.2.m2.1.1.3.2.3.5.cmml\">i</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.24.24.2.m2.1.1.3.2.3.1c\" xref=\"S5.T8.24.24.2.m2.1.1.3.2.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.24.24.2.m2.1.1.3.2.3.6\" xref=\"S5.T8.24.24.2.m2.1.1.3.2.3.6.cmml\">u</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.24.24.2.m2.1.1.3.2.3.1d\" xref=\"S5.T8.24.24.2.m2.1.1.3.2.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.24.24.2.m2.1.1.3.2.3.7\" xref=\"S5.T8.24.24.2.m2.1.1.3.2.3.7.cmml\">m</mi></mrow></msubsup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.24.24.2.m2.1b\"><apply id=\"S5.T8.24.24.2.m2.1.1.cmml\" xref=\"S5.T8.24.24.2.m2.1.1\"><times id=\"S5.T8.24.24.2.m2.1.1.1.cmml\" xref=\"S5.T8.24.24.2.m2.1.1.1\"/><ci id=\"S5.T8.24.24.2.m2.1.1.2.cmml\" xref=\"S5.T8.24.24.2.m2.1.1.2\">&#119860;</ci><apply id=\"S5.T8.24.24.2.m2.1.1.3.cmml\" xref=\"S5.T8.24.24.2.m2.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S5.T8.24.24.2.m2.1.1.3.1.cmml\" xref=\"S5.T8.24.24.2.m2.1.1.3\">subscript</csymbol><apply id=\"S5.T8.24.24.2.m2.1.1.3.2.cmml\" xref=\"S5.T8.24.24.2.m2.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S5.T8.24.24.2.m2.1.1.3.2.1.cmml\" xref=\"S5.T8.24.24.2.m2.1.1.3\">superscript</csymbol><ci id=\"S5.T8.24.24.2.m2.1.1.3.2.2.cmml\" xref=\"S5.T8.24.24.2.m2.1.1.3.2.2\">&#119877;</ci><apply id=\"S5.T8.24.24.2.m2.1.1.3.2.3.cmml\" xref=\"S5.T8.24.24.2.m2.1.1.3.2.3\"><times id=\"S5.T8.24.24.2.m2.1.1.3.2.3.1.cmml\" xref=\"S5.T8.24.24.2.m2.1.1.3.2.3.1\"/><ci id=\"S5.T8.24.24.2.m2.1.1.3.2.3.2.cmml\" xref=\"S5.T8.24.24.2.m2.1.1.3.2.3.2\">&#119898;</ci><ci id=\"S5.T8.24.24.2.m2.1.1.3.2.3.3.cmml\" xref=\"S5.T8.24.24.2.m2.1.1.3.2.3.3\">&#119890;</ci><ci id=\"S5.T8.24.24.2.m2.1.1.3.2.3.4.cmml\" xref=\"S5.T8.24.24.2.m2.1.1.3.2.3.4\">&#119889;</ci><ci id=\"S5.T8.24.24.2.m2.1.1.3.2.3.5.cmml\" xref=\"S5.T8.24.24.2.m2.1.1.3.2.3.5\">&#119894;</ci><ci id=\"S5.T8.24.24.2.m2.1.1.3.2.3.6.cmml\" xref=\"S5.T8.24.24.2.m2.1.1.3.2.3.6\">&#119906;</ci><ci id=\"S5.T8.24.24.2.m2.1.1.3.2.3.7.cmml\" xref=\"S5.T8.24.24.2.m2.1.1.3.2.3.7\">&#119898;</ci></apply></apply><apply id=\"S5.T8.24.24.2.m2.1.1.3.3.cmml\" xref=\"S5.T8.24.24.2.m2.1.1.3.3\"><times id=\"S5.T8.24.24.2.m2.1.1.3.3.1.cmml\" xref=\"S5.T8.24.24.2.m2.1.1.3.3.1\"/><ci id=\"S5.T8.24.24.2.m2.1.1.3.3.2.cmml\" xref=\"S5.T8.24.24.2.m2.1.1.3.3.2\">&#119888;</ci><ci id=\"S5.T8.24.24.2.m2.1.1.3.3.3.cmml\" xref=\"S5.T8.24.24.2.m2.1.1.3.3.3\">&#119900;</ci><ci id=\"S5.T8.24.24.2.m2.1.1.3.3.4.cmml\" xref=\"S5.T8.24.24.2.m2.1.1.3.3.4\">&#119888;</ci><ci id=\"S5.T8.24.24.2.m2.1.1.3.3.5.cmml\" xref=\"S5.T8.24.24.2.m2.1.1.3.3.5\">&#119900;</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.24.24.2.m2.1c\">AR^{medium}_{coco}</annotation></semantics></math><span id=\"S5.T8.28.28.6.2\" class=\"ltx_text\" style=\"font-size:80%;\">: for medium objects: </span><math id=\"S5.T8.25.25.3.m3.1\" class=\"ltx_Math\" alttext=\"32^{2}\" display=\"inline\"><semantics id=\"S5.T8.25.25.3.m3.1a\"><msup id=\"S5.T8.25.25.3.m3.1.1\" xref=\"S5.T8.25.25.3.m3.1.1.cmml\"><mn mathsize=\"80%\" id=\"S5.T8.25.25.3.m3.1.1.2\" xref=\"S5.T8.25.25.3.m3.1.1.2.cmml\">32</mn><mn mathsize=\"80%\" id=\"S5.T8.25.25.3.m3.1.1.3\" xref=\"S5.T8.25.25.3.m3.1.1.3.cmml\">2</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.25.25.3.m3.1b\"><apply id=\"S5.T8.25.25.3.m3.1.1.cmml\" xref=\"S5.T8.25.25.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T8.25.25.3.m3.1.1.1.cmml\" xref=\"S5.T8.25.25.3.m3.1.1\">superscript</csymbol><cn type=\"integer\" id=\"S5.T8.25.25.3.m3.1.1.2.cmml\" xref=\"S5.T8.25.25.3.m3.1.1.2\">32</cn><cn type=\"integer\" id=\"S5.T8.25.25.3.m3.1.1.3.cmml\" xref=\"S5.T8.25.25.3.m3.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.25.25.3.m3.1c\">32^{2}</annotation></semantics></math><span id=\"S5.T8.28.28.6.3\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S5.T8.26.26.4.m4.1\" class=\"ltx_Math\" alttext=\"&lt;\" display=\"inline\"><semantics id=\"S5.T8.26.26.4.m4.1a\"><mo mathsize=\"80%\" id=\"S5.T8.26.26.4.m4.1.1\" xref=\"S5.T8.26.26.4.m4.1.1.cmml\">&lt;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.26.26.4.m4.1b\"><lt id=\"S5.T8.26.26.4.m4.1.1.cmml\" xref=\"S5.T8.26.26.4.m4.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.26.26.4.m4.1c\">&lt;</annotation></semantics></math><span id=\"S5.T8.28.28.6.4\" class=\"ltx_text\" style=\"font-size:80%;\"> area </span><math id=\"S5.T8.27.27.5.m5.1\" class=\"ltx_Math\" alttext=\"&lt;\" display=\"inline\"><semantics id=\"S5.T8.27.27.5.m5.1a\"><mo mathsize=\"80%\" id=\"S5.T8.27.27.5.m5.1.1\" xref=\"S5.T8.27.27.5.m5.1.1.cmml\">&lt;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.27.27.5.m5.1b\"><lt id=\"S5.T8.27.27.5.m5.1.1.cmml\" xref=\"S5.T8.27.27.5.m5.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.27.27.5.m5.1c\">&lt;</annotation></semantics></math><span id=\"S5.T8.28.28.6.5\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S5.T8.28.28.6.m6.1\" class=\"ltx_Math\" alttext=\"96^{2}\" display=\"inline\"><semantics id=\"S5.T8.28.28.6.m6.1a\"><msup id=\"S5.T8.28.28.6.m6.1.1\" xref=\"S5.T8.28.28.6.m6.1.1.cmml\"><mn mathsize=\"80%\" id=\"S5.T8.28.28.6.m6.1.1.2\" xref=\"S5.T8.28.28.6.m6.1.1.2.cmml\">96</mn><mn mathsize=\"80%\" id=\"S5.T8.28.28.6.m6.1.1.3\" xref=\"S5.T8.28.28.6.m6.1.1.3.cmml\">2</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.28.28.6.m6.1b\"><apply id=\"S5.T8.28.28.6.m6.1.1.cmml\" xref=\"S5.T8.28.28.6.m6.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T8.28.28.6.m6.1.1.1.cmml\" xref=\"S5.T8.28.28.6.m6.1.1\">superscript</csymbol><cn type=\"integer\" id=\"S5.T8.28.28.6.m6.1.1.2.cmml\" xref=\"S5.T8.28.28.6.m6.1.1.2\">96</cn><cn type=\"integer\" id=\"S5.T8.28.28.6.m6.1.1.3.cmml\" xref=\"S5.T8.28.28.6.m6.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.28.28.6.m6.1c\">96^{2}</annotation></semantics></math>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.32.32\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.32.4\" class=\"ltx_td ltx_align_left ltx_border_r\">&#13;\n<math id=\"S5.T8.29.29.1.m1.1\" class=\"ltx_Math\" alttext=\"\\bullet\" display=\"inline\"><semantics id=\"S5.T8.29.29.1.m1.1a\"><mo mathsize=\"80%\" id=\"S5.T8.29.29.1.m1.1.1\" xref=\"S5.T8.29.29.1.m1.1.1.cmml\">&#8729;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.29.29.1.m1.1b\"><ci id=\"S5.T8.29.29.1.m1.1.1.cmml\" xref=\"S5.T8.29.29.1.m1.1.1\">&#8729;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.29.29.1.m1.1c\">\\bullet</annotation></semantics></math><span id=\"S5.T8.32.32.4.1\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S5.T8.30.30.2.m2.1\" class=\"ltx_Math\" alttext=\"AR^{large}_{coco}\" display=\"inline\"><semantics id=\"S5.T8.30.30.2.m2.1a\"><mrow id=\"S5.T8.30.30.2.m2.1.1\" xref=\"S5.T8.30.30.2.m2.1.1.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.30.30.2.m2.1.1.2\" xref=\"S5.T8.30.30.2.m2.1.1.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.30.30.2.m2.1.1.1\" xref=\"S5.T8.30.30.2.m2.1.1.1.cmml\">&#8203;</mo><msubsup id=\"S5.T8.30.30.2.m2.1.1.3\" xref=\"S5.T8.30.30.2.m2.1.1.3.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.30.30.2.m2.1.1.3.2.2\" xref=\"S5.T8.30.30.2.m2.1.1.3.2.2.cmml\">R</mi><mrow id=\"S5.T8.30.30.2.m2.1.1.3.3\" xref=\"S5.T8.30.30.2.m2.1.1.3.3.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.30.30.2.m2.1.1.3.3.2\" xref=\"S5.T8.30.30.2.m2.1.1.3.3.2.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.30.30.2.m2.1.1.3.3.1\" xref=\"S5.T8.30.30.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.30.30.2.m2.1.1.3.3.3\" xref=\"S5.T8.30.30.2.m2.1.1.3.3.3.cmml\">o</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.30.30.2.m2.1.1.3.3.1a\" xref=\"S5.T8.30.30.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.30.30.2.m2.1.1.3.3.4\" xref=\"S5.T8.30.30.2.m2.1.1.3.3.4.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.30.30.2.m2.1.1.3.3.1b\" xref=\"S5.T8.30.30.2.m2.1.1.3.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.30.30.2.m2.1.1.3.3.5\" xref=\"S5.T8.30.30.2.m2.1.1.3.3.5.cmml\">o</mi></mrow><mrow id=\"S5.T8.30.30.2.m2.1.1.3.2.3\" xref=\"S5.T8.30.30.2.m2.1.1.3.2.3.cmml\"><mi mathsize=\"80%\" id=\"S5.T8.30.30.2.m2.1.1.3.2.3.2\" xref=\"S5.T8.30.30.2.m2.1.1.3.2.3.2.cmml\">l</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.30.30.2.m2.1.1.3.2.3.1\" xref=\"S5.T8.30.30.2.m2.1.1.3.2.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.30.30.2.m2.1.1.3.2.3.3\" xref=\"S5.T8.30.30.2.m2.1.1.3.2.3.3.cmml\">a</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.30.30.2.m2.1.1.3.2.3.1a\" xref=\"S5.T8.30.30.2.m2.1.1.3.2.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.30.30.2.m2.1.1.3.2.3.4\" xref=\"S5.T8.30.30.2.m2.1.1.3.2.3.4.cmml\">r</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.30.30.2.m2.1.1.3.2.3.1b\" xref=\"S5.T8.30.30.2.m2.1.1.3.2.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.30.30.2.m2.1.1.3.2.3.5\" xref=\"S5.T8.30.30.2.m2.1.1.3.2.3.5.cmml\">g</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T8.30.30.2.m2.1.1.3.2.3.1c\" xref=\"S5.T8.30.30.2.m2.1.1.3.2.3.1.cmml\">&#8203;</mo><mi mathsize=\"80%\" id=\"S5.T8.30.30.2.m2.1.1.3.2.3.6\" xref=\"S5.T8.30.30.2.m2.1.1.3.2.3.6.cmml\">e</mi></mrow></msubsup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.30.30.2.m2.1b\"><apply id=\"S5.T8.30.30.2.m2.1.1.cmml\" xref=\"S5.T8.30.30.2.m2.1.1\"><times id=\"S5.T8.30.30.2.m2.1.1.1.cmml\" xref=\"S5.T8.30.30.2.m2.1.1.1\"/><ci id=\"S5.T8.30.30.2.m2.1.1.2.cmml\" xref=\"S5.T8.30.30.2.m2.1.1.2\">&#119860;</ci><apply id=\"S5.T8.30.30.2.m2.1.1.3.cmml\" xref=\"S5.T8.30.30.2.m2.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S5.T8.30.30.2.m2.1.1.3.1.cmml\" xref=\"S5.T8.30.30.2.m2.1.1.3\">subscript</csymbol><apply id=\"S5.T8.30.30.2.m2.1.1.3.2.cmml\" xref=\"S5.T8.30.30.2.m2.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S5.T8.30.30.2.m2.1.1.3.2.1.cmml\" xref=\"S5.T8.30.30.2.m2.1.1.3\">superscript</csymbol><ci id=\"S5.T8.30.30.2.m2.1.1.3.2.2.cmml\" xref=\"S5.T8.30.30.2.m2.1.1.3.2.2\">&#119877;</ci><apply id=\"S5.T8.30.30.2.m2.1.1.3.2.3.cmml\" xref=\"S5.T8.30.30.2.m2.1.1.3.2.3\"><times id=\"S5.T8.30.30.2.m2.1.1.3.2.3.1.cmml\" xref=\"S5.T8.30.30.2.m2.1.1.3.2.3.1\"/><ci id=\"S5.T8.30.30.2.m2.1.1.3.2.3.2.cmml\" xref=\"S5.T8.30.30.2.m2.1.1.3.2.3.2\">&#119897;</ci><ci id=\"S5.T8.30.30.2.m2.1.1.3.2.3.3.cmml\" xref=\"S5.T8.30.30.2.m2.1.1.3.2.3.3\">&#119886;</ci><ci id=\"S5.T8.30.30.2.m2.1.1.3.2.3.4.cmml\" xref=\"S5.T8.30.30.2.m2.1.1.3.2.3.4\">&#119903;</ci><ci id=\"S5.T8.30.30.2.m2.1.1.3.2.3.5.cmml\" xref=\"S5.T8.30.30.2.m2.1.1.3.2.3.5\">&#119892;</ci><ci id=\"S5.T8.30.30.2.m2.1.1.3.2.3.6.cmml\" xref=\"S5.T8.30.30.2.m2.1.1.3.2.3.6\">&#119890;</ci></apply></apply><apply id=\"S5.T8.30.30.2.m2.1.1.3.3.cmml\" xref=\"S5.T8.30.30.2.m2.1.1.3.3\"><times id=\"S5.T8.30.30.2.m2.1.1.3.3.1.cmml\" xref=\"S5.T8.30.30.2.m2.1.1.3.3.1\"/><ci id=\"S5.T8.30.30.2.m2.1.1.3.3.2.cmml\" xref=\"S5.T8.30.30.2.m2.1.1.3.3.2\">&#119888;</ci><ci id=\"S5.T8.30.30.2.m2.1.1.3.3.3.cmml\" xref=\"S5.T8.30.30.2.m2.1.1.3.3.3\">&#119900;</ci><ci id=\"S5.T8.30.30.2.m2.1.1.3.3.4.cmml\" xref=\"S5.T8.30.30.2.m2.1.1.3.3.4\">&#119888;</ci><ci id=\"S5.T8.30.30.2.m2.1.1.3.3.5.cmml\" xref=\"S5.T8.30.30.2.m2.1.1.3.3.5\">&#119900;</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.30.30.2.m2.1c\">AR^{large}_{coco}</annotation></semantics></math><span id=\"S5.T8.32.32.4.2\" class=\"ltx_text\" style=\"font-size:80%;\">: for large objects: area </span><math id=\"S5.T8.31.31.3.m3.1\" class=\"ltx_Math\" alttext=\"&gt;\" display=\"inline\"><semantics id=\"S5.T8.31.31.3.m3.1a\"><mo mathsize=\"80%\" id=\"S5.T8.31.31.3.m3.1.1\" xref=\"S5.T8.31.31.3.m3.1.1.cmml\">&gt;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.31.31.3.m3.1b\"><gt id=\"S5.T8.31.31.3.m3.1.1.cmml\" xref=\"S5.T8.31.31.3.m3.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.31.31.3.m3.1c\">&gt;</annotation></semantics></math><span id=\"S5.T8.32.32.4.3\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S5.T8.32.32.4.m4.1\" class=\"ltx_Math\" alttext=\"96^{2}\" display=\"inline\"><semantics id=\"S5.T8.32.32.4.m4.1a\"><msup id=\"S5.T8.32.32.4.m4.1.1\" xref=\"S5.T8.32.32.4.m4.1.1.cmml\"><mn mathsize=\"80%\" id=\"S5.T8.32.32.4.m4.1.1.2\" xref=\"S5.T8.32.32.4.m4.1.1.2.cmml\">96</mn><mn mathsize=\"80%\" id=\"S5.T8.32.32.4.m4.1.1.3\" xref=\"S5.T8.32.32.4.m4.1.1.3.cmml\">2</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.32.32.4.m4.1b\"><apply id=\"S5.T8.32.32.4.m4.1.1.cmml\" xref=\"S5.T8.32.32.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T8.32.32.4.m4.1.1.1.cmml\" xref=\"S5.T8.32.32.4.m4.1.1\">superscript</csymbol><cn type=\"integer\" id=\"S5.T8.32.32.4.m4.1.1.2.cmml\" xref=\"S5.T8.32.32.4.m4.1.1.2\">96</cn><cn type=\"integer\" id=\"S5.T8.32.32.4.m4.1.1.3.cmml\" xref=\"S5.T8.32.32.4.m4.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.32.32.4.m4.1c\">96^{2}</annotation></semantics></math>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.32.39.7\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.39.7.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S5.T8.32.39.7.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">OKS</span></td>&#13;\n<td id=\"S5.T8.32.39.7.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">&#13;\n<table id=\"S5.T8.32.39.7.2.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T8.32.39.7.2.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.39.7.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.39.7.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Object</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.32.39.7.2.1.2\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.39.7.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.39.7.2.1.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Keypoint</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.32.39.7.2.1.3\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.39.7.2.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.39.7.2.1.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Similarity</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n</td>&#13;\n<td id=\"S5.T8.32.39.7.3\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S5.T8.32.39.7.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">COCO</span></td>&#13;\n<td id=\"S5.T8.32.39.7.4\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">&#13;\n<table id=\"S5.T8.32.39.7.4.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T8.32.39.7.4.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.39.7.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.39.7.4.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A similar role as the Intersection over Union (IoU) for AP/AR.</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n</td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n",
            "<table id=\"S5.T8.32.35.3.2.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T8.32.35.3.2.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.35.3.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.35.3.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Percentage</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.32.35.3.2.1.2\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.35.3.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.35.3.2.1.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">of Correct</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.32.35.3.2.1.3\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.35.3.2.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.35.3.2.1.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Parts</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n\n",
            "<table id=\"S5.T8.32.35.3.4.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T8.32.35.3.4.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.35.3.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.35.3.4.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Percentage of correct predicted Parts which their end points fall within a threshold</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n\n",
            "<table id=\"S5.T8.32.36.4.2.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T8.32.36.4.2.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.36.4.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.36.4.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Percentage</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.32.36.4.2.1.2\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.36.4.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.36.4.2.1.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">of Correct</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.32.36.4.2.1.3\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.36.4.2.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.36.4.2.1.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Keypoints</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n\n",
            "<table id=\"S5.T8.32.36.4.3.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T8.32.36.4.3.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.36.4.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.36.4.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">LSP</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.32.36.4.3.1.2\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.36.4.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.36.4.3.1.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">MPII</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n\n",
            "<table id=\"S5.T8.32.36.4.4.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T8.32.36.4.4.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.36.4.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.36.4.4.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Percentage of correct predicted joints which fall within a threshold</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n\n",
            "<table id=\"S5.T8.32.38.6.3.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T8.32.38.6.3.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.38.6.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.38.6.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">MPII</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.32.38.6.3.1.2\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.38.6.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.38.6.3.1.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">PoseTrack</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n\n",
            "<table id=\"S5.T8.32.38.6.4.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T8.32.38.6.4.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.38.6.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"S5.T8.32.38.6.4.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">mean AP (mAP) is reported by AP for each body part after assigning predicted</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.32.38.6.4.1.2\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.38.6.4.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"S5.T8.32.38.6.4.1.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">pose to the ground truth pose by PCKh score.</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n\n",
            "<table id=\"S5.T8.32.39.7.2.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T8.32.39.7.2.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.39.7.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.39.7.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Object</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.32.39.7.2.1.2\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.39.7.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.39.7.2.1.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Keypoint</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T8.32.39.7.2.1.3\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.39.7.2.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.39.7.2.1.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Similarity</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n\n",
            "<table id=\"S5.T8.32.39.7.4.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T8.32.39.7.4.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T8.32.39.7.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T8.32.39.7.4.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A similar role as the Intersection over Union (IoU) for AP/AR.</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [],
        "references": [
            "Different datasets have different features (e.g. various range of human body sizes, upper/full human body) and different task requirements (single/multiple pose estimation), so there are several evaluation metrics for 2D human pose estimation. The summary of different evaluation metrics which are commonly used are listed in Table 8.",
            "Average Precision (AP), Average Recall (AR) and their variants. In (Lin et al., 2014), evaluating multi-person pose estimation results as an object detection problem is further designed. AP, AR, and their variants are reported based on an analogous similarity measure: object keypoint similarity (OKS) which plays the same role as the Intersection over Union (IoU). Additional, AP/AR with different human body scales are also reported in COCO dataset. Table 8 summarizes all above evaluation metrics."
        ]
    },
    "id_table_9": {
        "caption": "Table 9: Popular databases for 3D human pose estimation. Selected example images with annotations are shown in Fig. 6 (Cams. indicates the number of cameras; Jnt. indicates the number of joints)",
        "table": [
            "<table id=\"S5.T9.11\" class=\"ltx_tabular ltx_centering ltx_align_middle\">&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S5.T9.11.12.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T9.11.12.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S5.T9.11.12.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Dataset</span></td>&#13;\n<td id=\"S5.T9.11.12.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" rowspan=\"2\">&#13;\n<span id=\"S5.T9.11.12.1.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.11.12.1.2.1.1\" class=\"ltx_p\" style=\"width:19.9pt;\"><span id=\"S5.T9.11.12.1.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Cams.</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.11.12.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" rowspan=\"2\">&#13;\n<span id=\"S5.T9.11.12.1.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.11.12.1.3.1.1\" class=\"ltx_p\" style=\"width:14.2pt;\"><span id=\"S5.T9.11.12.1.3.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Jnt.</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.11.12.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\"><span id=\"S5.T9.11.12.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">Number of frames/videos</span></td>&#13;\n<td id=\"S5.T9.11.12.1.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T9.11.12.1.5.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.11.12.1.5.1.1\" class=\"ltx_p\" style=\"width:42.7pt;\"><span id=\"S5.T9.11.12.1.5.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Evaluation</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.11.12.1.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" rowspan=\"2\">&#13;\n<span id=\"S5.T9.11.12.1.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.11.12.1.6.1.1\" class=\"ltx_p\" style=\"width:213.4pt;\"><span id=\"S5.T9.11.12.1.6.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Highlights</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T9.11.13.2\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T9.11.13.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\"><span id=\"S5.T9.11.13.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">name</span></td>&#13;\n<td id=\"S5.T9.11.13.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T9.11.13.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Train</span></td>&#13;\n<td id=\"S5.T9.11.13.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T9.11.13.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">Val</span></td>&#13;\n<td id=\"S5.T9.11.13.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T9.11.13.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">Test</span></td>&#13;\n<td id=\"S5.T9.11.13.2.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">&#13;\n<span id=\"S5.T9.11.13.2.5.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.11.13.2.5.1.1\" class=\"ltx_p\" style=\"width:42.7pt;\"><span id=\"S5.T9.11.13.2.5.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">protocol</span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T9.11.14.3\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T9.11.14.3.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" colspan=\"8\"><span id=\"S5.T9.11.14.3.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Single person</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T9.3.3\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T9.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\"><span id=\"S5.T9.3.3.4.1\" class=\"ltx_text\" style=\"font-size:90%;position:relative; bottom:-8.5pt;\">HumanEva-I</span></td>&#13;\n<td id=\"S5.T9.3.3.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\">&#13;\n<span id=\"S5.T9.3.3.5.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.3.3.5.1.1\" class=\"ltx_p\" style=\"width:19.9pt;\"><span id=\"S5.T9.3.3.5.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;position:relative; bottom:-8.5pt;\">7</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.3.3.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\" rowspan=\"2\">&#13;\n<span id=\"S5.T9.3.3.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.3.3.6.1.1\" class=\"ltx_p\" style=\"width:14.2pt;\"><span id=\"S5.T9.3.3.6.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;position:relative; bottom:-14.2pt;\">15</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\"><span id=\"S5.T9.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;position:relative; bottom:-8.5pt;\"><math id=\"S5.T9.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T9.1.1.1.1.m1.1a\"><mo id=\"S5.T9.1.1.1.1.m1.1.1\" xref=\"S5.T9.1.1.1.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T9.1.1.1.1.m1.1b\"><approx id=\"S5.T9.1.1.1.1.m1.1.1.cmml\" xref=\"S5.T9.1.1.1.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T9.1.1.1.1.m1.1c\">\\approx</annotation></semantics></math>6.8k</span></td>&#13;\n<td id=\"S5.T9.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\"><span id=\"S5.T9.2.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;position:relative; bottom:-8.5pt;\"><math id=\"S5.T9.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T9.2.2.2.1.m1.1a\"><mo id=\"S5.T9.2.2.2.1.m1.1.1\" xref=\"S5.T9.2.2.2.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T9.2.2.2.1.m1.1b\"><approx id=\"S5.T9.2.2.2.1.m1.1.1.cmml\" xref=\"S5.T9.2.2.2.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T9.2.2.2.1.m1.1c\">\\approx</annotation></semantics></math>6.8k</span></td>&#13;\n<td id=\"S5.T9.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\"><span id=\"S5.T9.3.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;position:relative; bottom:-8.5pt;\"><math id=\"S5.T9.3.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T9.3.3.3.1.m1.1a\"><mo id=\"S5.T9.3.3.3.1.m1.1.1\" xref=\"S5.T9.3.3.3.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T9.3.3.3.1.m1.1b\"><approx id=\"S5.T9.3.3.3.1.m1.1.1.cmml\" xref=\"S5.T9.3.3.3.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T9.3.3.3.1.m1.1c\">\\approx</annotation></semantics></math>24k</span></td>&#13;\n<td id=\"S5.T9.3.3.7\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\" rowspan=\"2\">&#13;\n<span id=\"S5.T9.3.3.7.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.3.3.7.1.1\" class=\"ltx_p\" style=\"width:42.7pt;\"><span id=\"S5.T9.3.3.7.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;position:relative; bottom:-14.2pt;\">&#160;&#160;&#160;&#160;MPJPE</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.3.3.8\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\" rowspan=\"2\">&#13;\n<span id=\"S5.T9.3.3.8.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.3.3.8.1.1\" class=\"ltx_p\" style=\"width:213.4pt;\"><span id=\"S5.T9.3.3.8.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">4/2 (I/II) subjects, 6/1 (I/II) actions, Vicon data, indoor environment. <cite class=\"ltx_cite ltx_citemacro_citep\">(Sigal et&#160;al., <a href=\"#bib.bib149\" title=\"\" class=\"ltx_ref\">2010</a>)</cite></span></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T9.4.4\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T9.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\"><span id=\"S5.T9.4.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;position:relative; bottom:-8.5pt;\">&#13;\n<span id=\"S5.T9.4.4.2.1.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<span id=\"S5.T9.4.4.2.1.1.1\" class=\"ltx_tr\">&#13;\n<span id=\"S5.T9.4.4.2.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">HumanEva-II</span></span>&#13;\n</span></span></td>&#13;\n<td id=\"S5.T9.4.4.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\">&#13;\n<span id=\"S5.T9.4.4.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.4.4.3.1.1\" class=\"ltx_p\" style=\"width:19.9pt;\"><span id=\"S5.T9.4.4.3.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;position:relative; bottom:-8.5pt;\">4</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\"><span id=\"S5.T9.4.4.4.1\" class=\"ltx_text\" style=\"font-size:90%;position:relative; bottom:-8.5pt;\">0</span></td>&#13;\n<td id=\"S5.T9.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\"><span id=\"S5.T9.4.4.5.1\" class=\"ltx_text\" style=\"font-size:90%;position:relative; bottom:-8.5pt;\">0</span></td>&#13;\n<td id=\"S5.T9.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:9.0pt;\"><span id=\"S5.T9.4.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;position:relative; bottom:-8.5pt;\"><math id=\"S5.T9.4.4.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T9.4.4.1.1.m1.1a\"><mo id=\"S5.T9.4.4.1.1.m1.1.1\" xref=\"S5.T9.4.4.1.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T9.4.4.1.1.m1.1b\"><approx id=\"S5.T9.4.4.1.1.m1.1.1.cmml\" xref=\"S5.T9.4.4.1.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T9.4.4.1.1.m1.1c\">\\approx</annotation></semantics></math>2.5k</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T9.7.7\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T9.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">&#13;\n<table id=\"S5.T9.7.7.4.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T9.7.7.4.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T9.7.7.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T9.7.7.4.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Human3.6M</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n</td>&#13;\n<td id=\"S5.T9.7.7.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T9.7.7.5.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.7.7.5.1.1\" class=\"ltx_p\" style=\"width:19.9pt;\"><span id=\"S5.T9.7.7.5.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">4</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.7.7.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T9.7.7.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.7.7.6.1.1\" class=\"ltx_p\" style=\"width:14.2pt;\"><span id=\"S5.T9.7.7.6.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">17</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<math id=\"S5.T9.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T9.5.5.1.m1.1a\"><mo mathsize=\"90%\" id=\"S5.T9.5.5.1.m1.1.1\" xref=\"S5.T9.5.5.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T9.5.5.1.m1.1b\"><approx id=\"S5.T9.5.5.1.m1.1.1.cmml\" xref=\"S5.T9.5.5.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T9.5.5.1.m1.1c\">\\approx</annotation></semantics></math><span id=\"S5.T9.5.5.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.5M</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<math id=\"S5.T9.6.6.2.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T9.6.6.2.m1.1a\"><mo mathsize=\"90%\" id=\"S5.T9.6.6.2.m1.1.1\" xref=\"S5.T9.6.6.2.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T9.6.6.2.m1.1b\"><approx id=\"S5.T9.6.6.2.m1.1.1.cmml\" xref=\"S5.T9.6.6.2.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T9.6.6.2.m1.1c\">\\approx</annotation></semantics></math><span id=\"S5.T9.6.6.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.6M</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">&#13;\n<math id=\"S5.T9.7.7.3.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T9.7.7.3.m1.1a\"><mo mathsize=\"90%\" id=\"S5.T9.7.7.3.m1.1.1\" xref=\"S5.T9.7.7.3.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T9.7.7.3.m1.1b\"><approx id=\"S5.T9.7.7.3.m1.1.1.cmml\" xref=\"S5.T9.7.7.3.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T9.7.7.3.m1.1c\">\\approx</annotation></semantics></math><span id=\"S5.T9.7.7.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.5M</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.7.7.7\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T9.7.7.7.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.7.7.7.1.1\" class=\"ltx_p\" style=\"width:42.7pt;\"><span id=\"S5.T9.7.7.7.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">MPJPE</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.7.7.8\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T9.7.7.8.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.7.7.8.1.1\" class=\"ltx_p\" style=\"width:213.4pt;\"><span id=\"S5.T9.7.7.8.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">11 subjects, 17 actions, Vicon data, multi-annotation (3D joints, person bounding boxes, depth data, 3D body scans), indoor environment. </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S5.T9.7.7.8.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>Ionescu et&#160;al.<span id=\"S5.T9.7.7.8.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib59\" title=\"\" class=\"ltx_ref\">2014</a><span id=\"S5.T9.7.7.8.1.1.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T9.8.8\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T9.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">&#13;\n<table id=\"S5.T9.8.8.2.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T9.8.8.2.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T9.8.8.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T9.8.8.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">TNT15</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n</td>&#13;\n<td id=\"S5.T9.8.8.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T9.8.8.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.8.8.3.1.1\" class=\"ltx_p\" style=\"width:19.9pt;\"><span id=\"S5.T9.8.8.3.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">8</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.8.8.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T9.8.8.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.8.8.4.1.1\" class=\"ltx_p\" style=\"width:14.2pt;\"><span id=\"S5.T9.8.8.4.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">15</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.8.8.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\">&#13;\n<math id=\"S5.T9.8.8.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T9.8.8.1.m1.1a\"><mo mathsize=\"90%\" id=\"S5.T9.8.8.1.m1.1.1\" xref=\"S5.T9.8.8.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T9.8.8.1.m1.1b\"><approx id=\"S5.T9.8.8.1.m1.1.1.cmml\" xref=\"S5.T9.8.8.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T9.8.8.1.m1.1c\">\\approx</annotation></semantics></math><span id=\"S5.T9.8.8.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">13k</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.8.8.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T9.8.8.5.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.8.8.5.1.1\" class=\"ltx_p\" style=\"width:42.7pt;\"><span id=\"S5.T9.8.8.5.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">HumanEva</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.8.8.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T9.8.8.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.8.8.6.1.1\" class=\"ltx_p\" style=\"width:213.4pt;\"><span id=\"S5.T9.8.8.6.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">4 subjects, 5 actions, IMU data, 3D body scans, indoor environment. </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S5.T9.8.8.6.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>von Marcard et&#160;al.<span id=\"S5.T9.8.8.6.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib102\" title=\"\" class=\"ltx_ref\">2016</a><span id=\"S5.T9.8.8.6.1.1.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T9.9.9\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T9.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">&#13;\n<table id=\"S5.T9.9.9.2.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T9.9.9.2.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T9.9.9.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T9.9.9.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">MPI-INF-3DHP</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n</td>&#13;\n<td id=\"S5.T9.9.9.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T9.9.9.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.9.9.3.1.1\" class=\"ltx_p\" style=\"width:19.9pt;\"><span id=\"S5.T9.9.9.3.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">14</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.9.9.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T9.9.9.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.9.9.4.1.1\" class=\"ltx_p\" style=\"width:14.2pt;\"><span id=\"S5.T9.9.9.4.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">15</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.9.9.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\">&#13;\n<math id=\"S5.T9.9.9.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T9.9.9.1.m1.1a\"><mo mathsize=\"90%\" id=\"S5.T9.9.9.1.m1.1.1\" xref=\"S5.T9.9.9.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T9.9.9.1.m1.1b\"><approx id=\"S5.T9.9.9.1.m1.1.1.cmml\" xref=\"S5.T9.9.9.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T9.9.9.1.m1.1c\">\\approx</annotation></semantics></math><span id=\"S5.T9.9.9.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.3M</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.9.9.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T9.9.9.5.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.9.9.5.1.1\" class=\"ltx_p\" style=\"width:42.7pt;\"><span id=\"S5.T9.9.9.5.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">3DPCK</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.9.9.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T9.9.9.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.9.9.6.1.1\" class=\"ltx_p\" style=\"width:213.4pt;\"><span id=\"S5.T9.9.9.6.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">8 subjects, 8 actions, commercial markerless system, indoor and outdoor scenes. </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S5.T9.9.9.6.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>Mehta et&#160;al.<span id=\"S5.T9.9.9.6.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib104\" title=\"\" class=\"ltx_ref\">2017a</a><span id=\"S5.T9.9.9.6.1.1.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T9.10.10\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T9.10.10.2\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">&#13;\n<table id=\"S5.T9.10.10.2.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T9.10.10.2.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T9.10.10.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T9.10.10.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">TotalCapture</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n</td>&#13;\n<td id=\"S5.T9.10.10.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T9.10.10.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.10.10.3.1.1\" class=\"ltx_p\" style=\"width:19.9pt;\"><span id=\"S5.T9.10.10.3.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">8</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.10.10.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T9.10.10.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.10.10.4.1.1\" class=\"ltx_p\" style=\"width:14.2pt;\"><span id=\"S5.T9.10.10.4.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">26</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.10.10.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\">&#13;\n<math id=\"S5.T9.10.10.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T9.10.10.1.m1.1a\"><mo mathsize=\"90%\" id=\"S5.T9.10.10.1.m1.1.1\" xref=\"S5.T9.10.10.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T9.10.10.1.m1.1b\"><approx id=\"S5.T9.10.10.1.m1.1.1.cmml\" xref=\"S5.T9.10.10.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T9.10.10.1.m1.1c\">\\approx</annotation></semantics></math><span id=\"S5.T9.10.10.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.9M</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.10.10.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T9.10.10.5.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.10.10.5.1.1\" class=\"ltx_p\" style=\"width:42.7pt;\"><span id=\"S5.T9.10.10.5.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">MPJPE</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.10.10.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T9.10.10.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.10.10.6.1.1\" class=\"ltx_p\" style=\"width:213.4pt;\"><span id=\"S5.T9.10.10.6.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">5 subjects, 5 actions, IMU and Vicon data, indoors environment. </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S5.T9.10.10.6.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>Trumble et&#160;al.<span id=\"S5.T9.10.10.6.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib166\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S5.T9.10.10.6.1.1.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T9.11.15.4\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T9.11.15.4.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" colspan=\"8\"><span id=\"S5.T9.11.15.4.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Multiple person</span></td>&#13;\n</tr>&#13;\n<tr id=\"S5.T9.11.16.5\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T9.11.16.5.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">&#13;\n<table id=\"S5.T9.11.16.5.1.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T9.11.16.5.1.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T9.11.16.5.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T9.11.16.5.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Panoptic</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n</td>&#13;\n<td id=\"S5.T9.11.16.5.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T9.11.16.5.2.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.11.16.5.2.1.1\" class=\"ltx_p\" style=\"width:19.9pt;\"><span id=\"S5.T9.11.16.5.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">521</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.11.16.5.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T9.11.16.5.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.11.16.5.3.1.1\" class=\"ltx_p\" style=\"width:14.2pt;\"><span id=\"S5.T9.11.16.5.3.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">15</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.11.16.5.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\"><span id=\"S5.T9.11.16.5.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">65 videos (5.5 hours)</span></td>&#13;\n<td id=\"S5.T9.11.16.5.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T9.11.16.5.5.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.11.16.5.5.1.1\" class=\"ltx_p\" style=\"width:42.7pt;\"><span id=\"S5.T9.11.16.5.5.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">3DPCK</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.11.16.5.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T9.11.16.5.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.11.16.5.6.1.1\" class=\"ltx_p\" style=\"width:213.4pt;\"><span id=\"S5.T9.11.16.5.6.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">up to 8 subjects in each video, social interactions, markerless studio, multi-annotation (3D joints, cloud points, optical flow), indoors environment. </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S5.T9.11.16.5.6.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>Joo et&#160;al.<span id=\"S5.T9.11.16.5.6.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib70\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S5.T9.11.16.5.6.1.1.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S5.T9.11.11\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T9.11.11.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">&#13;\n<table id=\"S5.T9.11.11.2.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T9.11.11.2.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T9.11.11.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T9.11.11.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">3DPW</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n</td>&#13;\n<td id=\"S5.T9.11.11.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T9.11.11.3.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.11.11.3.1.1\" class=\"ltx_p\" style=\"width:19.9pt;\"><span id=\"S5.T9.11.11.3.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">1</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.11.11.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T9.11.11.4.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.11.11.4.1.1\" class=\"ltx_p\" style=\"width:14.2pt;\"><span id=\"S5.T9.11.11.4.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">18</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.11.11.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" colspan=\"3\">&#13;\n<span id=\"S5.T9.11.11.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">60 videos (</span><math id=\"S5.T9.11.11.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T9.11.11.1.m1.1a\"><mo mathsize=\"90%\" id=\"S5.T9.11.11.1.m1.1.1\" xref=\"S5.T9.11.11.1.m1.1.1.cmml\">&#8776;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T9.11.11.1.m1.1b\"><approx id=\"S5.T9.11.11.1.m1.1.1.cmml\" xref=\"S5.T9.11.11.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T9.11.11.1.m1.1c\">\\approx</annotation></semantics></math><span id=\"S5.T9.11.11.1.2\" class=\"ltx_text\" style=\"font-size:90%;\">51k frames)</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.11.11.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T9.11.11.5.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.11.11.5.1.1\" class=\"ltx_p\" style=\"width:42.7pt;\"><span id=\"S5.T9.11.11.5.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">MPJPE MPJAE</span></span>&#13;\n</span>&#13;\n</td>&#13;\n<td id=\"S5.T9.11.11.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">&#13;\n<span id=\"S5.T9.11.11.6.1\" class=\"ltx_inline-block ltx_align_top\">&#13;\n<span id=\"S5.T9.11.11.6.1.1\" class=\"ltx_p\" style=\"width:213.4pt;\"><span id=\"S5.T9.11.11.6.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">7 subjects(up to 2), daily actions, estimated 3D poses from videos and attached IMUs, 3D body scans, SMPL model fitting, in the wild. </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S5.T9.11.11.6.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>von Marcard et&#160;al.<span id=\"S5.T9.11.11.6.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib101\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S5.T9.11.11.6.1.1.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite></span>&#13;\n</span>&#13;\n</td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n",
            "<table id=\"S5.T9.7.7.4.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T9.7.7.4.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T9.7.7.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T9.7.7.4.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Human3.6M</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n\n",
            "<table id=\"S5.T9.8.8.2.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T9.8.8.2.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T9.8.8.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T9.8.8.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">TNT15</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n\n",
            "<table id=\"S5.T9.9.9.2.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T9.9.9.2.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T9.9.9.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T9.9.9.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">MPI-INF-3DHP</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n\n",
            "<table id=\"S5.T9.10.10.2.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T9.10.10.2.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T9.10.10.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T9.10.10.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">TotalCapture</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n\n",
            "<table id=\"S5.T9.11.16.5.1.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T9.11.16.5.1.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T9.11.16.5.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T9.11.16.5.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Panoptic</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n\n",
            "<table id=\"S5.T9.11.11.2.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S5.T9.11.11.2.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S5.T9.11.11.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T9.11.11.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">3DPW</span></td>&#13;\n</tr>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "(Sigal et al., 2010) Sigal et al. (2010)\r\n\r\nSigal, L., Balan, A.O.,\r\nBlack, M.J., 2010.\r\n\r\n\r\nHumaneva: Synchronized video and motion capture\r\ndataset and baseline algorithm for evaluation of articulated human motion.\r\n\r\n\r\nInternational journal of computer vision\r\n87, 4.",
            "(Ionescu et al., 2014) Ionescu et al. (2014)\r\n\r\nIonescu, C., Papava, D.,\r\nOlaru, V., Sminchisescu, C.,\r\n2014.\r\n\r\n\r\nHuman3.6m: Large scale datasets and predictive\r\nmethods for 3d human sensing in natural environments.\r\n\r\n\r\nIEEE Transactions on Pattern Analysis and Machine\r\nIntelligence 36, 1325–1339.",
            "(von Marcard et al., 2016) von Marcard et al. (2016)\r\n\r\nvon Marcard, T., Pons-Moll, G.,\r\nRosenhahn, B., 2016.\r\n\r\n\r\nHuman pose estimation from video and imus.\r\n\r\n\r\nIEEE transactions on pattern analysis and machine\r\nintelligence 38, 1533–1547.",
            "(Mehta et al., 2017a) Mehta et al. (2017a)\r\n\r\nMehta, D., Rhodin, H.,\r\nCasas, D., Fua, P.,\r\nSotnychenko, O., Xu, W.,\r\nTheobalt, C., 2017a.\r\n\r\n\r\nMonocular 3d human pose estimation in the wild using\r\nimproved cnn supervision, in: Proc. IEEE International\r\nConference on 3D Vision, IEEE. pp.\r\n506–516.",
            "(Trumble et al., 2017) Trumble et al. (2017)\r\n\r\nTrumble, M., Gilbert, A.,\r\nMalleson, C., Hilton, A.,\r\nCollomosse, J., 2017.\r\n\r\n\r\nTotal capture: 3d human pose estimation fusing video\r\nand inertial sensors, in: Proc. British Machine Vision\r\nConference, pp. 1–13.",
            "(Joo et al., 2017) Joo et al. (2017)\r\n\r\nJoo, H., Simon, T., Li,\r\nX., Liu, H., Tan, L.,\r\nGui, L., Banerjee, S.,\r\nGodisart, T.S., Nabbe, B.,\r\nMatthews, I., Kanade, T.,\r\nNobuhara, S., Sheikh, Y.,\r\n2017.\r\n\r\n\r\nPanoptic studio: A massively multiview system for\r\nsocial interaction capture.\r\n\r\n\r\nIEEE Transactions on Pattern Analysis and Machine\r\nIntelligence 41, 190–204.",
            "(von Marcard et al., 2018) von Marcard et al. (2018)\r\n\r\nvon Marcard, T., Henschel, R.,\r\nBlack, M.J., Rosenhahn, B.,\r\nPons-Moll, G., 2018.\r\n\r\n\r\nRecovering accurate 3d human pose in the wild using\r\nimus and a moving camera, in: Proc. European Conference\r\non Computer Vision, pp. 601–617."
        ],
        "references": [
            "For a better understanding of the human body in 3D space, there are many kinds of body representations with different modern equipment. 3D human body shape scans, such as SCAPE (Anguelov et al., 2005), INRIA4D (INRIA4D, accessed on 2019) and FAUST (Bogo et al., 2014, 2017), 3D human body surface cloud points with time of flight (TOF) depth sensors (Shahroudy et al., 2016), 3D human body reflective markers capture with motion capture systems (MoCap) (Sigal et al., 2010; Ionescu et al., 2014), orientation and acceleration of 3D human body data with Inertial Measurement Unit (IMU) (von Marcard et al., 2016, 2018). It is difficult to summarize them all, this paper summarizes the datasets that involve RGB images and 3D joint coordinates. The details of the selected 3D datasets are summarized in Table 9 and some example images with annotations are shown in Fig. 6."
        ]
    }
}