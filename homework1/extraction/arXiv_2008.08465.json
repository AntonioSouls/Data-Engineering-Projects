{
    "id_table_1": {
        "caption": "Table 1: Single-view 6D pose estimation.  Comparisons with state-of-the-art methods on the YCB-Video (a) and T-LESS datasets (b).",
        "table": [
            "<table id=\"S4.T1.1.fig1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T1.1.fig1.1.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.1.fig1.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>&#13;\n<th id=\"S4.T1.1.fig1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">AUC of</th>&#13;\n<th id=\"S4.T1.1.fig1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">AUC of</th>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.1.fig1.1.2.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.1.fig1.1.2.2.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>&#13;\n<th id=\"S4.T1.1.fig1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">ADD-S</th>&#13;\n<th id=\"S4.T1.1.fig1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">ADD(-S)</th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T1.1.fig1.1.3.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.1.fig1.1.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PoseCNN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">18</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T1.1.fig1.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>&#13;\n<td id=\"S4.T1.1.fig1.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">61.3</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.1.fig1.1.4.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.1.fig1.1.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MCN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib20\" title=\"\" class=\"ltx_ref\">20</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T1.1.fig1.1.4.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">75.1</td>&#13;\n<td id=\"S4.T1.1.fig1.1.4.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.1.fig1.1.5.3\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.1.fig1.1.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PVNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib5\" title=\"\" class=\"ltx_ref\">5</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T1.1.fig1.1.5.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>&#13;\n<td id=\"S4.T1.1.fig1.1.5.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">73.4</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.1.fig1.1.6.4\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.1.fig1.1.6.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">DeepIM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">10</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T1.1.fig1.1.6.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">88.1</td>&#13;\n<td id=\"S4.T1.1.fig1.1.6.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">81.9</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.1.fig1.1.7.5\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.1.fig1.1.7.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Ours</th>&#13;\n<td id=\"S4.T1.1.fig1.1.7.5.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S4.T1.1.fig1.1.7.5.2.1\" class=\"ltx_text ltx_font_bold\">89.8</span></td>&#13;\n<td id=\"S4.T1.1.fig1.1.7.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S4.T1.1.fig1.1.7.5.3.1\" class=\"ltx_text ltx_font_bold\">84.5</span></td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n",
            "<table id=\"S4.T1.2.1.1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T1.2.1.1.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.2.1.1.1.1.2\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>&#13;\n<th id=\"S4.T1.2.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math id=\"S4.T1.2.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"e_{\\mathrm{vsd}}&lt;0.3\" display=\"inline\"><semantics id=\"S4.T1.2.1.1.1.1.1.m1.1a\"><mrow id=\"S4.T1.2.1.1.1.1.1.m1.1.1\" xref=\"S4.T1.2.1.1.1.1.1.m1.1.1.cmml\"><msub id=\"S4.T1.2.1.1.1.1.1.m1.1.1.2\" xref=\"S4.T1.2.1.1.1.1.1.m1.1.1.2.cmml\"><mi id=\"S4.T1.2.1.1.1.1.1.m1.1.1.2.2\" xref=\"S4.T1.2.1.1.1.1.1.m1.1.1.2.2.cmml\">e</mi><mi id=\"S4.T1.2.1.1.1.1.1.m1.1.1.2.3\" xref=\"S4.T1.2.1.1.1.1.1.m1.1.1.2.3.cmml\">vsd</mi></msub><mo id=\"S4.T1.2.1.1.1.1.1.m1.1.1.1\" xref=\"S4.T1.2.1.1.1.1.1.m1.1.1.1.cmml\">&lt;</mo><mn id=\"S4.T1.2.1.1.1.1.1.m1.1.1.3\" xref=\"S4.T1.2.1.1.1.1.1.m1.1.1.3.cmml\">0.3</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.2.1.1.1.1.1.m1.1b\"><apply id=\"S4.T1.2.1.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T1.2.1.1.1.1.1.m1.1.1\"><lt id=\"S4.T1.2.1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S4.T1.2.1.1.1.1.1.m1.1.1.1\"/><apply id=\"S4.T1.2.1.1.1.1.1.m1.1.1.2.cmml\" xref=\"S4.T1.2.1.1.1.1.1.m1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S4.T1.2.1.1.1.1.1.m1.1.1.2.1.cmml\" xref=\"S4.T1.2.1.1.1.1.1.m1.1.1.2\">subscript</csymbol><ci id=\"S4.T1.2.1.1.1.1.1.m1.1.1.2.2.cmml\" xref=\"S4.T1.2.1.1.1.1.1.m1.1.1.2.2\">&#119890;</ci><ci id=\"S4.T1.2.1.1.1.1.1.m1.1.1.2.3.cmml\" xref=\"S4.T1.2.1.1.1.1.1.m1.1.1.2.3\">vsd</ci></apply><cn type=\"float\" id=\"S4.T1.2.1.1.1.1.1.m1.1.1.3.cmml\" xref=\"S4.T1.2.1.1.1.1.1.m1.1.1.3\">0.3</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.2.1.1.1.1.1.m1.1c\">e_{\\mathrm{vsd}}&lt;0.3</annotation></semantics></math></th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T1.2.1.1.1.2.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.2.1.1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Implicit&#8201;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">12</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T1.2.1.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">26.8</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.2.1.1.1.3.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.2.1.1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Pix2pose&#8201;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">7</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T1.2.1.1.1.3.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">29.5</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.2.1.1.1.4.3\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.2.1.1.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Ours</th>&#13;\n<td id=\"S4.T1.2.1.1.1.4.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S4.T1.2.1.1.1.4.3.2.1\" class=\"ltx_text ltx_font_bold\">63.8</span></td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.2.1.1.1.5.4\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.2.1.1.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">w/o loss</th>&#13;\n<td id=\"S4.T1.2.1.1.1.5.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">60.1</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.2.1.1.1.6.5\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.2.1.1.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">w/o network</th>&#13;\n<td id=\"S4.T1.2.1.1.1.6.5.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">59.5</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.2.1.1.1.7.6\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.2.1.1.1.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">w/o rot.</th>&#13;\n<td id=\"S4.T1.2.1.1.1.7.6.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">61.0</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.2.1.1.1.8.7\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.2.1.1.1.8.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">w/o data augm.</th>&#13;\n<td id=\"S4.T1.2.1.1.1.8.7.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">37.0</td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "[18] [18]\r\n\r\nXiang, Y., Schmidt, T., Narayanan, V., Fox, D.:\r\n\r\n\r\nPoseCNN: A convolutional neural network for 6D object pose\r\nestimation in cluttered scenes.\r\n\r\n\r\nIn: Robotics: Science and Systems XIV. (2018)",
            "[20] [20]\r\n\r\nLi, C., Bai, J., Hager, G.D.:\r\n\r\n\r\nA unified framework for multi-view multi-class object pose\r\nestimation.\r\n\r\n\r\nIn: Proceedings of the European Conference on Computer Vision (ECCV).\r\n(2018) 254–269",
            "[5] [5]\r\n\r\nPeng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.:\r\n\r\n\r\nPvnet: Pixel-wise voting network for 6dof pose estimation.\r\n\r\n\r\nIn: Proceedings of the IEEE Conference on Computer Vision and Pattern\r\nRecognition. (2019) 4561–4570",
            "[10] [10]\r\n\r\nLi, Y., Wang, G., Ji, X., Xiang, Y., Fox, D.:\r\n\r\n\r\nDeepim: Deep iterative matching for 6d pose estimation.\r\n\r\n\r\nIn: Proceedings of the European Conference on Computer Vision (ECCV).\r\n(2018) 683–698",
            "[12] [12]\r\n\r\nSundermeyer, M., Marton, Z.C., Durner, M., Brucker, M., Triebel, R.:\r\n\r\n\r\nImplicit 3d orientation learning for 6d object detection from rgb\r\nimages.\r\n\r\n\r\nIn: Proceedings of the European Conference on Computer Vision\r\n(ECCV). (2018) 699–715",
            "[7] [7]\r\n\r\nPark, K., Patten, T., Vincze, M.:\r\n\r\n\r\nPix2pose: Pixel-wise coordinate regression of objects for 6d pose\r\nestimation.\r\n\r\n\r\nIn: Proceedings of the IEEE International Conference on Computer\r\nVision. (2019) 7668–7677"
        ],
        "references": [
            "Following [18, 10, 5], we evaluate on a subset of 2949\r\nkeyframes from videos of the 12 testing scenes.\r\nWe use the standard\r\nADD-S and ADD(-S) metrics and their area-under-the-curves [18]\r\n(please see appendix for details on the metrics). We evaluate\r\nour refinement method using the same detections and\r\ncoarse estimates as DeepIM [10], provided by\r\nPoseCNN [18]. We ran two iterations of pose refinement network.\r\nResults are shown in Table 1a.\r\nOur method improves over the current-state-of-the-art DeepIM [10], by approximately 2 points on the AUC of ADD-S and ADD(-S) metrics.",
            "As explained in Section 3.2, we use our single-view approach both for coarse pose estimation and refinement. We compare our method against the two recent RGB-only methods\r\nPix2Pose [7] and Implicit [12]. For a fair\r\ncomparison, we use the detections from the same RetinaNet model as in [7].\r\nWe report results on the SiSo task [44]\r\nand use the standard visual surface discrepancy (vsd) recall metric with the same parameters as in [7, 12].\r\nResults are presented in Table 1b.\r\nOn the evsd<0.3subscript𝑒vsd0.3e_{\\mathrm{vsd}}<0.3 metric, our {coarse + refinement} solution achieves a significant 34.2%percent34.234.2\\% absolute\r\nimprovement compared to existing state-of-the-art methods.\r\nNote that [10] did not report results on\r\nT-LESS.\r\nWe also evaluate on this dataset the benefits of the key components of our single view approach compared to the components used in DeepIM[10].\r\nMore precisely, we evaluate the\r\nimportance of the base network (our EfficientNet vs FlowNet pre-trained), loss (our\r\nsymmetric and disentangled vs. point-matching loss with L1subscript𝐿1L_{1} norm), rotation parametrization (our using\r\n[41] vs. quaternions) and data augmentation (our color\r\naugmentation, similar to [12] vs. none). Loss, network and rotation parametrization bring a small but clear improvement.\r\nUsing data augmentation is crucial on the T-LESS dataset where training is performed only on synthetic data and real images of the objects on dark background."
        ]
    },
    "id_table_2": {
        "caption": "Table 2: Multi-view multi-object results. (a) Our approach significantly\r\noutperforms [20] on the YCB-Video dataset in both the single view and multi-view scenarios while not requiring\r\nknown camera poses. (b) Results on the T-LESS dataset. Using multiple views clearly improves our results.",
        "table": [
            "<table id=\"S4.T2.fig1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T2.fig1.1.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.fig1.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>&#13;\n<th id=\"S4.T2.fig1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1 view</th>&#13;\n<th id=\"S4.T2.fig1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">5 views</th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T2.fig1.1.2.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.fig1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib20\" title=\"\" class=\"ltx_ref\">20</a>]</cite></th>&#13;\n<td id=\"S4.T2.fig1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">75.1</td>&#13;\n<td id=\"S4.T2.fig1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">80.2</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.fig1.1.3.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.fig1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Ours</th>&#13;\n<td id=\"S4.T2.fig1.1.3.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">89.8</td>&#13;\n<td id=\"S4.T2.fig1.1.3.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S4.T2.fig1.1.3.2.3.1\" class=\"ltx_text ltx_font_bold\">93.4</span></td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n",
            "<table id=\"S4.T2.3.3.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T2.3.3.3.4.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.3.3.3.4.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>&#13;\n<th id=\"S4.T2.3.3.3.4.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1 view</th>&#13;\n<th id=\"S4.T2.3.3.3.4.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4 views</th>&#13;\n<th id=\"S4.T2.3.3.3.4.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">8 views</th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T2.3.3.3.5.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.3.3.3.5.1.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">AUC of ADD-S</th>&#13;\n<td id=\"S4.T2.3.3.3.5.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">72.1</td>&#13;\n<td id=\"S4.T2.3.3.3.5.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">76.0</td>&#13;\n<td id=\"S4.T2.3.3.3.5.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S4.T2.3.3.3.5.1.4.1\" class=\"ltx_text ltx_font_bold\">78.9</span></td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.1.1.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.1.1.1.1.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">ADD-S <math id=\"S4.T2.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"&lt;\" display=\"inline\"><semantics id=\"S4.T2.1.1.1.1.1.m1.1a\"><mo id=\"S4.T2.1.1.1.1.1.m1.1.1\" xref=\"S4.T2.1.1.1.1.1.m1.1.1.cmml\">&lt;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.1.1.1.1.1.m1.1b\"><lt id=\"S4.T2.1.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T2.1.1.1.1.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.1.1.1.1.1.m1.1c\">&lt;</annotation></semantics></math> 0.1d</th>&#13;\n<td id=\"S4.T2.1.1.1.1.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">68.0</td>&#13;\n<td id=\"S4.T2.1.1.1.1.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">72.6</td>&#13;\n<td id=\"S4.T2.1.1.1.1.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S4.T2.1.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">76.6</span></td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.2.2.2.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.2.2.2.2.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math id=\"S4.T2.2.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"e_{\\mathrm{vsd}}&lt;0.3\" display=\"inline\"><semantics id=\"S4.T2.2.2.2.2.1.m1.1a\"><mrow id=\"S4.T2.2.2.2.2.1.m1.1.1\" xref=\"S4.T2.2.2.2.2.1.m1.1.1.cmml\"><msub id=\"S4.T2.2.2.2.2.1.m1.1.1.2\" xref=\"S4.T2.2.2.2.2.1.m1.1.1.2.cmml\"><mi id=\"S4.T2.2.2.2.2.1.m1.1.1.2.2\" xref=\"S4.T2.2.2.2.2.1.m1.1.1.2.2.cmml\">e</mi><mi id=\"S4.T2.2.2.2.2.1.m1.1.1.2.3\" xref=\"S4.T2.2.2.2.2.1.m1.1.1.2.3.cmml\">vsd</mi></msub><mo id=\"S4.T2.2.2.2.2.1.m1.1.1.1\" xref=\"S4.T2.2.2.2.2.1.m1.1.1.1.cmml\">&lt;</mo><mn id=\"S4.T2.2.2.2.2.1.m1.1.1.3\" xref=\"S4.T2.2.2.2.2.1.m1.1.1.3.cmml\">0.3</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.2.2.2.2.1.m1.1b\"><apply id=\"S4.T2.2.2.2.2.1.m1.1.1.cmml\" xref=\"S4.T2.2.2.2.2.1.m1.1.1\"><lt id=\"S4.T2.2.2.2.2.1.m1.1.1.1.cmml\" xref=\"S4.T2.2.2.2.2.1.m1.1.1.1\"/><apply id=\"S4.T2.2.2.2.2.1.m1.1.1.2.cmml\" xref=\"S4.T2.2.2.2.2.1.m1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S4.T2.2.2.2.2.1.m1.1.1.2.1.cmml\" xref=\"S4.T2.2.2.2.2.1.m1.1.1.2\">subscript</csymbol><ci id=\"S4.T2.2.2.2.2.1.m1.1.1.2.2.cmml\" xref=\"S4.T2.2.2.2.2.1.m1.1.1.2.2\">&#119890;</ci><ci id=\"S4.T2.2.2.2.2.1.m1.1.1.2.3.cmml\" xref=\"S4.T2.2.2.2.2.1.m1.1.1.2.3\">vsd</ci></apply><cn type=\"float\" id=\"S4.T2.2.2.2.2.1.m1.1.1.3.cmml\" xref=\"S4.T2.2.2.2.2.1.m1.1.1.3\">0.3</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.2.2.2.2.1.m1.1c\">e_{\\mathrm{vsd}}&lt;0.3</annotation></semantics></math></th>&#13;\n<td id=\"S4.T2.2.2.2.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">62.6</td>&#13;\n<td id=\"S4.T2.2.2.2.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">67.6</td>&#13;\n<td id=\"S4.T2.2.2.2.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S4.T2.2.2.2.2.4.1\" class=\"ltx_text ltx_font_bold\">71.6</span></td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.3.3.3.3\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.3.3.3.3.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">mAP@ADD-S<math id=\"S4.T2.3.3.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"&lt;\" display=\"inline\"><semantics id=\"S4.T2.3.3.3.3.1.m1.1a\"><mo id=\"S4.T2.3.3.3.3.1.m1.1.1\" xref=\"S4.T2.3.3.3.3.1.m1.1.1.cmml\">&lt;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.3.3.3.3.1.m1.1b\"><lt id=\"S4.T2.3.3.3.3.1.m1.1.1.cmml\" xref=\"S4.T2.3.3.3.3.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.3.3.3.3.1.m1.1c\">&lt;</annotation></semantics></math>0.1d</th>&#13;\n<td id=\"S4.T2.3.3.3.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">55.0</td>&#13;\n<td id=\"S4.T2.3.3.3.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">61.6</td>&#13;\n<td id=\"S4.T2.3.3.3.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S4.T2.3.3.3.3.4.1\" class=\"ltx_text ltx_font_bold\">69.0</span></td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "[20] [20]\r\n\r\nLi, C., Bai, J., Hager, G.D.:\r\n\r\n\r\nA unified framework for multi-view multi-class object pose\r\nestimation.\r\n\r\n\r\nIn: Proceedings of the European Conference on Computer Vision (ECCV).\r\n(2018) 254–269",
            "[20] [20]\r\n\r\nLi, C., Bai, J., Hager, G.D.:\r\n\r\n\r\nA unified framework for multi-view multi-class object pose\r\nestimation.\r\n\r\n\r\nIn: Proceedings of the European Conference on Computer Vision (ECCV).\r\n(2018) 254–269"
        ],
        "references": [
            "The problem that we consider,\r\nrecovering the 6D object poses of multiple known objects in a scene captured by several RGB images taken from unknown viewpoints\r\nhas not, to the best of our knowledge, been addressed by prior work reporting results on the YCB-Video and T-LESS datasets.\r\nThe closest work is\r\n[20], which considers multi-view scenarios on YCB-Video and uses\r\nground truth camera poses to align the viewpoints. In [20], results are provided for\r\nprediction using 5 views. We use our approach with the same number of input images but without using ground truth calibration and report results in Table 2a. Our method significantly outperforms\r\n[20] in both single-view and multi-view scenarios.",
            "We also perform multi-view experiments on T-LESS with a variable number of\r\nviews. We follow the multi-instance\r\nBOP[44] protocol for ADD-S<<0.1d and\r\nevsd<0.3subscript𝑒vsd0.3e_{\\mathrm{vsd}}<0.3. We also analyze precision-recall\r\ntradeoff similar to the standard practice in object detection.\r\nWe consider positive predictions that satisfy ADD-S<<0.1d and report\r\nmAP@ADD-S<<0.1d. Results are shown in Table 2b for the ViVo\r\ntask on 1000 images. To the best of our knowledge, no other method has reported results on this task. As expected, our multi-view approach brings significant improvements compared to only single-view baseline."
        ]
    },
    "id_table_3": {
        "caption": "Table 3: Benefits of the scene refinement stage. We report pose\r\nADD-S errors (in mm) for the inlier object\r\ncandidates before and after global scene refinement. Scene-refinement\r\nimproves 6D pose estimation accuracy.",
        "table": [
            "<table id=\"S4.T3.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T3.4.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.4.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>&#13;\n<th id=\"S4.T3.4.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">YCB dataset</th>&#13;\n<th id=\"S4.T3.4.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">T-LESS dataset</th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T3.4.2.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.4.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Before refinement</th>&#13;\n<td id=\"S4.T3.4.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">6.40</td>&#13;\n<td id=\"S4.T3.4.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4.43</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T3.4.3.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.4.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">After refinement</th>&#13;\n<td id=\"S4.T3.4.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S4.T3.4.3.2.2.1\" class=\"ltx_text ltx_font_bold\">5.05</span></td>&#13;\n<td id=\"S4.T3.4.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S4.T3.4.3.2.3.1\" class=\"ltx_text ltx_font_bold\">3.19</span></td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [],
        "references": [
            "To demonstrate the benefits\r\nof global scene refinement (stage 3), we report in Table 3\r\nthe average ADD-S errors of the inlier candidates before and after solving the\r\noptimization problem of Eq.(6).\r\nWe note a clear relative improvement, around 20% on both datasets.."
        ]
    }
}