{
    "id_table_1": {
        "caption": "Table 1: Quantitative benchmark, MPJPE (right) and reconstruction error (left). Lower scores are better.",
        "table": [
            "<table id=\"S4.T1.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T1.3.4.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.3.4.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt\"/>&#13;\n<td id=\"S4.T1.3.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span id=\"S4.T1.3.4.1.2.1\" class=\"ltx_text ltx_font_bold\">MPJPE</span></td>&#13;\n<td id=\"S4.T1.3.4.1.3\" class=\"ltx_td ltx_border_tt\"/>&#13;\n<td id=\"S4.T1.3.4.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span id=\"S4.T1.3.4.1.4.1\" class=\"ltx_text ltx_font_bold\">Rec. Error</span></td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.3.5.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.3.5.2.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"/>&#13;\n<td id=\"S4.T1.3.5.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">Cover 2</td>&#13;\n<td id=\"S4.T1.3.5.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">Cover 1</td>&#13;\n<td id=\"S4.T1.3.5.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">Uncover</td>&#13;\n<td id=\"S4.T1.3.5.2.5\" class=\"ltx_td\"/>&#13;\n<td id=\"S4.T1.3.5.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\">Cover 2</td>&#13;\n<td id=\"S4.T1.3.5.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\">Cover 1</td>&#13;\n<td id=\"S4.T1.3.5.2.8\" class=\"ltx_td ltx_align_center ltx_border_t\">Uncover</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.3.6.3\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.3.6.3.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t\">HMR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">13</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T1.3.6.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">109.83</td>&#13;\n<td id=\"S4.T1.3.6.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">106.64</td>&#13;\n<td id=\"S4.T1.3.6.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">100.86</td>&#13;\n<td id=\"S4.T1.3.6.3.5\" class=\"ltx_td ltx_border_t\"/>&#13;\n<td id=\"S4.T1.3.6.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\">100.03</td>&#13;\n<td id=\"S4.T1.3.6.3.7\" class=\"ltx_td ltx_align_center ltx_border_t\">98.42</td>&#13;\n<td id=\"S4.T1.3.6.3.8\" class=\"ltx_td ltx_align_center ltx_border_t\">93.04</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.3.7.4\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.3.7.4.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r\">HMR (4 modalities)</th>&#13;\n<td id=\"S4.T1.3.7.4.2\" class=\"ltx_td ltx_align_center\">94.16</td>&#13;\n<td id=\"S4.T1.3.7.4.3\" class=\"ltx_td ltx_align_center\">93.32</td>&#13;\n<td id=\"S4.T1.3.7.4.4\" class=\"ltx_td ltx_align_center\">93.05</td>&#13;\n<td id=\"S4.T1.3.7.4.5\" class=\"ltx_td\"/>&#13;\n<td id=\"S4.T1.3.7.4.6\" class=\"ltx_td ltx_align_center\">87.12</td>&#13;\n<td id=\"S4.T1.3.7.4.7\" class=\"ltx_td ltx_align_center\">86.54</td>&#13;\n<td id=\"S4.T1.3.7.4.8\" class=\"ltx_td ltx_align_center\">86.76</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.3.8.5\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.3.8.5.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r\"><span title=\"SMPL Optimization IN the loop\" class=\"ltx_glossaryref\"><span class=\"ltx_text ltx_glossary_long\">SMPL Optimization IN the loop</span></span> (<abbr title=\"SMPL Optimization IN the loop\" class=\"ltx_glossaryref\"><span class=\"ltx_text ltx_glossary_short\">SPIN</span></abbr>)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib14\" title=\"\" class=\"ltx_ref\">14</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T1.3.8.5.2\" class=\"ltx_td ltx_align_center\">99.36</td>&#13;\n<td id=\"S4.T1.3.8.5.3\" class=\"ltx_td ltx_align_center\">98.37</td>&#13;\n<td id=\"S4.T1.3.8.5.4\" class=\"ltx_td ltx_align_center\">87.77</td>&#13;\n<td id=\"S4.T1.3.8.5.5\" class=\"ltx_td\"/>&#13;\n<td id=\"S4.T1.3.8.5.6\" class=\"ltx_td ltx_align_center\">90.00</td>&#13;\n<td id=\"S4.T1.3.8.5.7\" class=\"ltx_td ltx_align_center\">88.98</td>&#13;\n<td id=\"S4.T1.3.8.5.8\" class=\"ltx_td ltx_align_center\">78.70</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.3.9.6\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.3.9.6.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r\">&#13;\n<abbr title=\"SMPL Optimization IN the loop\" class=\"ltx_glossaryref\"><span class=\"ltx_text ltx_glossary_short\">SPIN</span></abbr> (4 modalities)</th>&#13;\n<td id=\"S4.T1.3.9.6.2\" class=\"ltx_td ltx_align_center\">86.40</td>&#13;\n<td id=\"S4.T1.3.9.6.3\" class=\"ltx_td ltx_align_center\">85.72</td>&#13;\n<td id=\"S4.T1.3.9.6.4\" class=\"ltx_td ltx_align_center\">83.59</td>&#13;\n<td id=\"S4.T1.3.9.6.5\" class=\"ltx_td\"/>&#13;\n<td id=\"S4.T1.3.9.6.6\" class=\"ltx_td ltx_align_center\">78.20</td>&#13;\n<td id=\"S4.T1.3.9.6.7\" class=\"ltx_td ltx_align_center\">77.57</td>&#13;\n<td id=\"S4.T1.3.9.6.8\" class=\"ltx_td ltx_align_center\">74.66</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.3.10.7\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.3.10.7.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r\">Bodies At Rest&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">6</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T1.3.10.7.2\" class=\"ltx_td ltx_align_center\">96.10</td>&#13;\n<td id=\"S4.T1.3.10.7.3\" class=\"ltx_td ltx_align_center\">96.84</td>&#13;\n<td id=\"S4.T1.3.10.7.4\" class=\"ltx_td ltx_align_center\">96.10</td>&#13;\n<td id=\"S4.T1.3.10.7.5\" class=\"ltx_td\"/>&#13;\n<td id=\"S4.T1.3.10.7.6\" class=\"ltx_td ltx_align_center\">84.37</td>&#13;\n<td id=\"S4.T1.3.10.7.7\" class=\"ltx_td ltx_align_center\">84.12</td>&#13;\n<td id=\"S4.T1.3.10.7.8\" class=\"ltx_td ltx_align_center\">84.19</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.3.11.8\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.3.11.8.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r\">Bodies At Rest (4 modalities)</th>&#13;\n<td id=\"S4.T1.3.11.8.2\" class=\"ltx_td ltx_align_center\">86.98</td>&#13;\n<td id=\"S4.T1.3.11.8.3\" class=\"ltx_td ltx_align_center\">86.75</td>&#13;\n<td id=\"S4.T1.3.11.8.4\" class=\"ltx_td ltx_align_center\">86.83</td>&#13;\n<td id=\"S4.T1.3.11.8.5\" class=\"ltx_td\"/>&#13;\n<td id=\"S4.T1.3.11.8.6\" class=\"ltx_td ltx_align_center\">78.78</td>&#13;\n<td id=\"S4.T1.3.11.8.7\" class=\"ltx_td ltx_align_center\">78.51</td>&#13;\n<td id=\"S4.T1.3.11.8.8\" class=\"ltx_td ltx_align_center\">78.40</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.1.1.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t\">Ours (<math id=\"S4.T1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"k\" display=\"inline\"><semantics id=\"S4.T1.1.1.1.m1.1a\"><mi id=\"S4.T1.1.1.1.m1.1.1\" xref=\"S4.T1.1.1.1.m1.1.1.cmml\">k</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.1.1.1.m1.1b\"><ci id=\"S4.T1.1.1.1.m1.1.1.cmml\" xref=\"S4.T1.1.1.1.m1.1.1\">&#119896;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.1.1.1.m1.1c\">k</annotation></semantics></math> = 0)</th>&#13;\n<td id=\"S4.T1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">82.69</td>&#13;\n<td id=\"S4.T1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">82.08</td>&#13;\n<td id=\"S4.T1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">80.39</td>&#13;\n<td id=\"S4.T1.1.1.5\" class=\"ltx_td ltx_border_t\"/>&#13;\n<td id=\"S4.T1.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">74.41</td>&#13;\n<td id=\"S4.T1.1.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">73.56</td>&#13;\n<td id=\"S4.T1.1.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\">71.83</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.2.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.2.2.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r\">Ours (<math id=\"S4.T1.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"k\" display=\"inline\"><semantics id=\"S4.T1.2.2.1.m1.1a\"><mi id=\"S4.T1.2.2.1.m1.1.1\" xref=\"S4.T1.2.2.1.m1.1.1.cmml\">k</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.2.2.1.m1.1b\"><ci id=\"S4.T1.2.2.1.m1.1.1.cmml\" xref=\"S4.T1.2.2.1.m1.1.1\">&#119896;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.2.2.1.m1.1c\">k</annotation></semantics></math> = 1)</th>&#13;\n<td id=\"S4.T1.2.2.2\" class=\"ltx_td ltx_align_center\">81.65</td>&#13;\n<td id=\"S4.T1.2.2.3\" class=\"ltx_td ltx_align_center\">81.24</td>&#13;\n<td id=\"S4.T1.2.2.4\" class=\"ltx_td ltx_align_center\">80.44</td>&#13;\n<td id=\"S4.T1.2.2.5\" class=\"ltx_td\"/>&#13;\n<td id=\"S4.T1.2.2.6\" class=\"ltx_td ltx_align_center\">72.49</td>&#13;\n<td id=\"S4.T1.2.2.7\" class=\"ltx_td ltx_align_center\">71.95</td>&#13;\n<td id=\"S4.T1.2.2.8\" class=\"ltx_td ltx_align_center\">71.39</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.3.3\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.3.3.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_r\">Ours (<math id=\"S4.T1.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"k\" display=\"inline\"><semantics id=\"S4.T1.3.3.1.m1.1a\"><mi id=\"S4.T1.3.3.1.m1.1.1\" xref=\"S4.T1.3.3.1.m1.1.1.cmml\">k</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.3.3.1.m1.1b\"><ci id=\"S4.T1.3.3.1.m1.1.1.cmml\" xref=\"S4.T1.3.3.1.m1.1.1\">&#119896;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.3.3.1.m1.1c\">k</annotation></semantics></math> = 2)</th>&#13;\n<td id=\"S4.T1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.3.3.2.1\" class=\"ltx_text ltx_font_bold\">80.21</span></td>&#13;\n<td id=\"S4.T1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.3.3.3.1\" class=\"ltx_text ltx_font_bold\">79.92</span></td>&#13;\n<td id=\"S4.T1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.3.3.4.1\" class=\"ltx_text ltx_font_bold\">78.80</span></td>&#13;\n<td id=\"S4.T1.3.3.5\" class=\"ltx_td ltx_border_bb\"/>&#13;\n<td id=\"S4.T1.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.3.3.6.1\" class=\"ltx_text ltx_font_bold\">71.80</span></td>&#13;\n<td id=\"S4.T1.3.3.7\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.3.3.7.1\" class=\"ltx_text ltx_font_bold\">71.44</span></td>&#13;\n<td id=\"S4.T1.3.3.8\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.3.3.8.1\" class=\"ltx_text ltx_font_bold\">70.65</span></td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "[13] [13]\r\n\r\nAngjoo Kanazawa, MichaelÂ J Black, DavidÂ W Jacobs, and Jitendra Malik.\r\n\r\n\r\nEnd-to-end recovery of human shape and pose.\r\n\r\n\r\nIn IEEE Conf. Comput. Vis. Pattern Recog., 2018.",
            "[14] [14]\r\n\r\nNikos Kolotouros, Georgios Pavlakos, MichaelÂ J Black, and Kostas Daniilidis.\r\n\r\n\r\nLearning to reconstruct 3d human pose and shape via model-fitting in\r\nthe loop.\r\n\r\n\r\nIn Int. Conf. Comput. Vis., 2019.",
            "[6] [6]\r\n\r\nHenryÂ M Clever, Zackory Erickson, Ariel Kapusta, Greg Turk, Karen Liu, and\r\nCharlesÂ C Kemp.\r\n\r\n\r\nBodies at rest: 3d human pose and shape estimation from a pressure\r\nimage using synthetic data.\r\n\r\n\r\nIn IEEE Conf. Comput. Vis. Pattern Recog., pages 6215â€“6224,\r\n2020."
        ],
        "references": [
            "We report results of the proposed method in different fusion levels (\\iekğ‘˜k = 0, 1, 2). Considering other methods only use either RGB or PM to estimate human pose and shape, we first report results of their method using single modality according to their paper. Besides, we also show fusion results of their method by concatenating all four modalities (see TableÂ 1). Specifically, two general human pose estimation methodsÂ [13, 14] are evaluated using single RGB modality and the fusion of all four modalities separately. The in-bed pose estimation methodsÂ [6] are evaluated in both PM and fusion settings. As shown in TableÂ 1, the fusion of four modalities largely boost the performance of each, and balanced the estimation error among three cover types (\\ieuncover, cover 1, and cover2).\r\nStill, the proposed method out-performs state-of-the-art works by a large margin. We achieve better performance even in the first fusion level, where only IR and depth images are fused. Then in the next level (kğ‘˜k = 1), the occlusion-invariant information from PM further pushes the performance by correcting the estimation of the covered part. We can see from TableÂ 1 that the error of level kğ‘˜k = 1 dropped for cover 1 and cover 2 settings, but slightly increased in the uncovered setting. Finally, at the final level (kğ‘˜k = 2), the RGB is fused to provide additional details and an accurate shape for the uncovered part. Hence, we can see more performance improvement in the uncover setting instead of the other two."
        ]
    },
    "id_table_2": {
        "caption": "Table 2: Foreground-background segmentation. Higher scores are better. # of papamrters of models are also listed.",
        "table": [],
        "footnotes": [
            "[13] [13]\r\n\r\nAngjoo Kanazawa, MichaelÂ J Black, DavidÂ W Jacobs, and Jitendra Malik.\r\n\r\n\r\nEnd-to-end recovery of human shape and pose.\r\n\r\n\r\nIn IEEE Conf. Comput. Vis. Pattern Recog., 2018.",
            "[14] [14]\r\n\r\nNikos Kolotouros, Georgios Pavlakos, MichaelÂ J Black, and Kostas Daniilidis.\r\n\r\n\r\nLearning to reconstruct 3d human pose and shape via model-fitting in\r\nthe loop.\r\n\r\n\r\nIn Int. Conf. Comput. Vis., 2019.",
            "[6] [6]\r\n\r\nHenryÂ M Clever, Zackory Erickson, Ariel Kapusta, Greg Turk, Karen Liu, and\r\nCharlesÂ C Kemp.\r\n\r\n\r\nBodies at rest: 3d human pose and shape estimation from a pressure\r\nimage using synthetic data.\r\n\r\n\r\nIn IEEE Conf. Comput. Vis. Pattern Recog., pages 6215â€“6224,\r\n2020."
        ],
        "references": [
            "In this section, we use only IR and depth modalities to demonstrate the superior of our model even without pyramid fusion scheme and further prove the effectiveness of proposed attention-based reconstruction module. Qualitative results for foreground-background segmentation are shown in TableÂ 2. We also compare the number of parameters used in different models in TableÂ 2. We have larger model than HMR and SPIN because of the extra attention-based reconstruction model. However, comparing to Bodies At Rest, who also includes a similar recovering module, our model has much smaller size."
        ]
    },
    "id_table_3": {
        "caption": "Table 3: Ablation study. Comparison of pyramid fusion (P) and the contribution of each modality (evaluated by Reconstruction errors in mm).",
        "table": [],
        "footnotes": [],
        "references": [
            "To effectively fuse the different modalities in a way that best leverages the knowledge captured by the multimodal sensors, we propose a pyramid scheme to fuse the different modalities at different levels (Fig.Â 1). When training each fusion level, the learned model weights of previous level are freezed to enable the fine-tuning of the estimation. The first level (\\iek=0ğ‘˜0k=0) assumes the mean parameters for the 3D mesh (\\ie(Î²â†’0,Î¸â†’0)superscriptâ†’ğ›½0superscriptâ†’ğœƒ0(\\vec{\\beta}^{0},\\vec{\\theta}^{0})), with each proceeding level passed the predicted parameters from the level prior. Hence, the process starts at the bottom of the pyramid, passing in IDsubscriptğ¼D{I}_{\\text{D}} and IIRsubscriptğ¼IR{I}_{\\text{IR}}: the top performing modalities (TableÂ 3), and also the modalities that provide shape and other local information in the dark and while under the covers (\\ieoccluded).\r\nAt the next level (\\iek=1ğ‘˜1k=1), the parameters of the 3D mesh (Î²â†’1,Î¸â†’1)superscriptâ†’ğ›½1superscriptâ†’ğœƒ1(\\vec{\\beta}^{1},\\vec{\\theta}^{1}) are inferred by the learned mappings of the previous layer gkâˆ’1â€‹(â‹…)superscriptğ‘”ğ‘˜1â‹…g^{k-1}(\\cdot) (Fig.Â 3), from the input IDâŠ•IIRdirect-sumsubscriptğ¼Dsubscriptğ¼IR{I}_{\\text{D}}\\oplus{I}_{\\text{IR}} (\\ieconcatenate tensors across the channel dimension). Hence, the process at the first level can be expressed as g0â†’(Î²â†’1,Î¸â†’1)absentâ†’superscriptğ‘”0superscriptâ†’ğ›½1superscriptâ†’ğœƒ1g^{0}\\xrightarrow{}(\\vec{\\beta}^{1},\\vec{\\theta}^{1}).\r\nThe input of k=1ğ‘˜1k=1 is the three element tuple (IDâŠ•IIRâŠ•IPM,Î²â†’1,Î¸â†’1)direct-sumsubscriptğ¼Dsubscriptğ¼IRsubscriptğ¼PMsuperscriptâ†’ğ›½1superscriptâ†’ğœƒ1({I}_{\\text{D}}\\oplus{I}_{\\text{IR}}\\oplus{I}_{\\text{PM}},\\vec{\\beta}^{1},\\vec{\\theta}^{1}); the input to learn the mapping g1â†’(Î²â†’2,Î¸â†’2)absentâ†’superscriptğ‘”1superscriptâ†’ğ›½2superscriptâ†’ğœƒ2g^{1}\\xrightarrow{}(\\vec{\\beta}^{2},\\vec{\\theta}^{2}) which, again, go to the next level. Specifically, l=2ğ‘™2l=2 is the next, final level that is fed all four modalities to output the final 3D mesh prediction. The pseudocode of the training process are listed in AlgorithmÂ 1.",
            "Effect of pyramid fusion scheme.\r\nTo gain insight of pyramid fusion scheme and the contribution of each modality, we conduct experiments using 1) single modality only, 2) fusion with pyramid, and 3) fusion without pyramid scheme, separately (see TableÂ 3).\r\nNotice that RGB data is difficult to recover when all the body parts are covered. For fair comparison, we remove the reconstruction modules for all the experiments in this section.\r\nResults shown that RGB data performs the poorest on covered poses as expected. The performance of PM data are invariant to cover types. However, we notice that PM data does not outperform RGB data for uncovered poses, which demonstrates that PM data are inaccurate and vulnerable to missing information. In general, depth and IR data are more informative and can recover better knowledge for the in-bed pose estimation task than the other two modalities, though depth data may sometimes be affected by occlusions."
        ]
    }
}