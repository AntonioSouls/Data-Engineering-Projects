{
    "id_table_1": {
        "caption": "Table 1: Influence of the Anchor Selection Strategy. All the models are trained with the Automatic Weighting of Losses.",
        "table": [
            "<table id=\"S4.T1.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T1.2.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r\">Anchor Selection</th>&#13;\n<th id=\"S4.T1.2.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column\">AP</th>&#13;\n<th id=\"S4.T1.2.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column\">3DPCK</th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T1.2.2.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">No</th>&#13;\n<td id=\"S4.T1.2.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\">84.1</td>&#13;\n<td id=\"S4.T1.2.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_t\">80.7</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.2.3.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.2.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">BB-Aware <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib5\" title=\"\" class=\"ltx_ref\">5</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T1.2.3.2.2\" class=\"ltx_td ltx_align_left\">85.1</td>&#13;\n<td id=\"S4.T1.2.3.2.3\" class=\"ltx_td ltx_align_left\">81.9</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.2.4.3\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.2.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Pose-Aware (Ours)</th>&#13;\n<td id=\"S4.T1.2.4.3.2\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.2.4.3.2.1\" class=\"ltx_text ltx_font_bold\">85.3</span></td>&#13;\n<td id=\"S4.T1.2.4.3.3\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.2.4.3.3.1\" class=\"ltx_text ltx_font_bold\">83.2</span></td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "[5] [5]\r\n\r\nFlorian Chabot, Mohamed Chaouch, and QuocCuong Pham.\r\n\r\n\r\nLapnet : Automatic balanced loss and optimal assignment for real-time\r\ndense object detection.\r\n\r\n\r\narXiv preprint arXiv:1911.01149, 2019."
        ],
        "references": [
            "Pose-Aware Anchor Selection strategy:  Table 1 results show the effectiveness of the Pose-Aware Anchor Selection. We compare three variants of PandaNet. The first variant (first line) is a model where no anchor selection strategy is used. It corresponds to a model where only the PONO overlap Oi,j,asubscriptO_{i,j,a} is considered in equation 9. Using the Bounding-box Aware Anchor Selection [5] (second row), improves the model performance over this baseline. Box detection and 3D pose estimation take all benefit of this anchor selection strategy. Using the proposed Pose-Aware Anchor Selection (third row) maintain the AP value while improving the 3DPCK, showing its effectiveness for choosing better anchors for 3D pose estimation."
        ]
    },
    "id_table_2": {
        "caption": "Table 2: Influence of the Automatic Weighting of Losses. task, anchor and joint represent the type of trainable 位\\lambda weights. All the models are trained with the Pose-Aware Anchor Selection Strategy.",
        "table": [
            "<table id=\"S4.T2.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T2.4.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.4.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r\">task</th>&#13;\n<th id=\"S4.T2.4.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r\">anchor</th>&#13;\n<th id=\"S4.T2.4.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r\">joint</th>&#13;\n<th id=\"S4.T2.4.1.1.4\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column\">AP</th>&#13;\n<th id=\"S4.T2.4.1.1.5\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column\">3DPCK</th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T2.4.2.1\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T2.4.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">1</td>&#13;\n<td id=\"S4.T2.4.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">1</td>&#13;\n<td id=\"S4.T2.4.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">1</td>&#13;\n<td id=\"S4.T2.4.2.1.4\" class=\"ltx_td ltx_align_left ltx_border_t\">21.7</td>&#13;\n<td id=\"S4.T2.4.2.1.5\" class=\"ltx_td ltx_align_left ltx_border_t\">15.8</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.4.3.2\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T2.4.3.2.1\" class=\"ltx_td ltx_align_left ltx_border_r\">learned</td>&#13;\n<td id=\"S4.T2.4.3.2.2\" class=\"ltx_td ltx_align_left ltx_border_r\">1</td>&#13;\n<td id=\"S4.T2.4.3.2.3\" class=\"ltx_td ltx_align_left ltx_border_r\">1</td>&#13;\n<td id=\"S4.T2.4.3.2.4\" class=\"ltx_td ltx_align_left\">84.1</td>&#13;\n<td id=\"S4.T2.4.3.2.5\" class=\"ltx_td ltx_align_left\">80.8</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.4.4.3\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T2.4.4.3.1\" class=\"ltx_td ltx_align_left ltx_border_r\">learned</td>&#13;\n<td id=\"S4.T2.4.4.3.2\" class=\"ltx_td ltx_align_left ltx_border_r\">learned</td>&#13;\n<td id=\"S4.T2.4.4.3.3\" class=\"ltx_td ltx_align_left ltx_border_r\">1</td>&#13;\n<td id=\"S4.T2.4.4.3.4\" class=\"ltx_td ltx_align_left\">85.2</td>&#13;\n<td id=\"S4.T2.4.4.3.5\" class=\"ltx_td ltx_align_left\">81.7</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.4.5.4\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T2.4.5.4.1\" class=\"ltx_td ltx_align_left ltx_border_r\">learned</td>&#13;\n<td id=\"S4.T2.4.5.4.2\" class=\"ltx_td ltx_align_left ltx_border_r\">learned</td>&#13;\n<td id=\"S4.T2.4.5.4.3\" class=\"ltx_td ltx_align_left ltx_border_r\">learned</td>&#13;\n<td id=\"S4.T2.4.5.4.4\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T2.4.5.4.4.1\" class=\"ltx_text ltx_font_bold\">85.3</span></td>&#13;\n<td id=\"S4.T2.4.5.4.5\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T2.4.5.4.5.1\" class=\"ltx_text ltx_font_bold\">83.2</span></td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [],
        "references": [
            "Automatic Weighting of Losses: The influence of Automatic Weighting of Losses is detailed in Table 2. When the 位\\lambdas are all set to 1 (first line) and not trained, the model has poor performance on all tasks. Learning task-specific 位locsubscript\\lambda_{loc}, 位clssubscript\\lambda_{cls}, 位2Dsubscript2\\lambda_{2D} and 位3Dsubscript3\\lambda_{3D} (second row) allow the network to converge and to achieve good performances on all tasks.\r\nLearning anchor weights 位locasuperscriptsubscript\\lambda_{loc}^{a} and 位clsasuperscriptsubscript\\lambda_{cls}^{a} (third row) improves detection and 3D pose estimation performances. The best results are obtained when all 位\\lambdas are learned, showing the importance of the proposed automatic weighting of losses."
        ]
    },
    "id_table_3": {
        "caption": "Table 3: Distance wise 3DPCK on the JTA dataset. Distance are in meters.",
        "table": [
            "<table id=\"S4.T3.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T3.2.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Dist.</th>&#13;\n<th id=\"S4.T3.2.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">&lt;10</th>&#13;\n<th id=\"S4.T3.2.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">10-20</th>&#13;\n<th id=\"S4.T3.2.1.1.4\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">20-30</th>&#13;\n<th id=\"S4.T3.2.1.1.5\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">30-40</th>&#13;\n<th id=\"S4.T3.2.1.1.6\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">&gt;40</th>&#13;\n<th id=\"S4.T3.2.1.1.7\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">All</th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T3.2.2.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib3\" title=\"\" class=\"ltx_ref\">3</a>]</cite></th>&#13;\n<td id=\"S4.T3.2.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">55.8</td>&#13;\n<td id=\"S4.T3.2.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">68.4</td>&#13;\n<td id=\"S4.T3.2.2.1.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">57.8</td>&#13;\n<td id=\"S4.T3.2.2.1.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">49.3</td>&#13;\n<td id=\"S4.T3.2.2.1.6\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">41.7</td>&#13;\n<td id=\"S4.T3.2.2.1.7\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">43.9</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T3.2.3.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.2.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\">Ours</th>&#13;\n<td id=\"S4.T3.2.3.2.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\"><span id=\"S4.T3.2.3.2.2.1\" class=\"ltx_text ltx_font_bold\">95.6</span></td>&#13;\n<td id=\"S4.T3.2.3.2.3\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\"><span id=\"S4.T3.2.3.2.3.1\" class=\"ltx_text ltx_font_bold\">93.7</span></td>&#13;\n<td id=\"S4.T3.2.3.2.4\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\"><span id=\"S4.T3.2.3.2.4.1\" class=\"ltx_text ltx_font_bold\">87.3</span></td>&#13;\n<td id=\"S4.T3.2.3.2.5\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\"><span id=\"S4.T3.2.3.2.5.1\" class=\"ltx_text ltx_font_bold\">80.5</span></td>&#13;\n<td id=\"S4.T3.2.3.2.6\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\"><span id=\"S4.T3.2.3.2.6.1\" class=\"ltx_text ltx_font_bold\">71.2</span></td>&#13;\n<td id=\"S4.T3.2.3.2.7\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\"><span id=\"S4.T3.2.3.2.7.1\" class=\"ltx_text ltx_font_bold\">83.2</span></td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "[3] [3]\r\n\r\nAbdallah Benzine, Bertrand Luvison, QuocCuong Pham, and Catherine Achard.\r\n\r\n\r\nSingle-shot 3d multi-person pose estimation in complex images.\r\n\r\n\r\nPattern Recognition, page 107534, 2020."
        ],
        "references": [
            "Table 3 provides 3DPCK results according to the distance of people to the camera. PandaNet outperforms the model of Benzine et al. [3] on all camera distances demonstrating the ability of PandaNet to properly process people at all scales. While our model achieve very good results for people close to the camera (less than 20m), it also correctly handles people who are further from the camera."
        ]
    },
    "id_table_4": {
        "caption": "Table 4: Joint wise 3DPCK.",
        "table": [
            "<table id=\"S4.T4.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T4.2.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T4.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Method</th>&#13;\n<th id=\"S4.T4.2.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">head</th>&#13;\n<th id=\"S4.T4.2.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">neck</th>&#13;\n<th id=\"S4.T4.2.1.1.4\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">clavicles</th>&#13;\n<th id=\"S4.T4.2.1.1.5\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">shoulders</th>&#13;\n<th id=\"S4.T4.2.1.1.6\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">elbows</th>&#13;\n<th id=\"S4.T4.2.1.1.7\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">wrists</th>&#13;\n<th id=\"S4.T4.2.1.1.8\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">spines</th>&#13;\n<th id=\"S4.T4.2.1.1.9\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">hips.</th>&#13;\n<th id=\"S4.T4.2.1.1.10\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">knees</th>&#13;\n<th id=\"S4.T4.2.1.1.11\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">ankles</th>&#13;\n<th id=\"S4.T4.2.1.1.12\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">all</th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T4.2.2.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T4.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib3\" title=\"\" class=\"ltx_ref\">3</a>]</cite></th>&#13;\n<td id=\"S4.T4.2.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">41.1</td>&#13;\n<td id=\"S4.T4.2.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">44.6</td>&#13;\n<td id=\"S4.T4.2.2.1.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">44.9</td>&#13;\n<td id=\"S4.T4.2.2.1.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">33.8</td>&#13;\n<td id=\"S4.T4.2.2.1.6\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">27.2</td>&#13;\n<td id=\"S4.T4.2.2.1.7\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">19.0</td>&#13;\n<td id=\"S4.T4.2.2.1.8\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">74.4</td>&#13;\n<td id=\"S4.T4.2.2.1.9\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">73.9</td>&#13;\n<td id=\"S4.T4.2.2.1.10\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">25.7</td>&#13;\n<td id=\"S4.T4.2.2.1.11\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">8.9</td>&#13;\n<td id=\"S4.T4.2.2.1.12\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">43.9</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T4.2.3.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T4.2.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\">Ours</th>&#13;\n<td id=\"S4.T4.2.3.2.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\"><span id=\"S4.T4.2.3.2.2.1\" class=\"ltx_text ltx_font_bold\">92.7</span></td>&#13;\n<td id=\"S4.T4.2.3.2.3\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\"><span id=\"S4.T4.2.3.2.3.1\" class=\"ltx_text ltx_font_bold\">99.1</span></td>&#13;\n<td id=\"S4.T4.2.3.2.4\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\"><span id=\"S4.T4.2.3.2.4.1\" class=\"ltx_text ltx_font_bold\">97.0</span></td>&#13;\n<td id=\"S4.T4.2.3.2.5\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\"><span id=\"S4.T4.2.3.2.5.1\" class=\"ltx_text ltx_font_bold\">78.4</span></td>&#13;\n<td id=\"S4.T4.2.3.2.6\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\"><span id=\"S4.T4.2.3.2.6.1\" class=\"ltx_text ltx_font_bold\">72.1</span></td>&#13;\n<td id=\"S4.T4.2.3.2.7\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\"><span id=\"S4.T4.2.3.2.7.1\" class=\"ltx_text ltx_font_bold\">60.1</span></td>&#13;\n<td id=\"S4.T4.2.3.2.8\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\"><span id=\"S4.T4.2.3.2.8.1\" class=\"ltx_text ltx_font_bold\">99.9</span></td>&#13;\n<td id=\"S4.T4.2.3.2.9\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\"><span id=\"S4.T4.2.3.2.9.1\" class=\"ltx_text ltx_font_bold\">87.8</span></td>&#13;\n<td id=\"S4.T4.2.3.2.10\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\"><span id=\"S4.T4.2.3.2.10.1\" class=\"ltx_text ltx_font_bold\">71.8</span></td>&#13;\n<td id=\"S4.T4.2.3.2.11\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\"><span id=\"S4.T4.2.3.2.11.1\" class=\"ltx_text ltx_font_bold\">58.0</span></td>&#13;\n<td id=\"S4.T4.2.3.2.12\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\"><span id=\"S4.T4.2.3.2.12.1\" class=\"ltx_text ltx_font_bold\">83.2</span></td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "[3] [3]\r\n\r\nAbdallah Benzine, Bertrand Luvison, QuocCuong Pham, and Catherine Achard.\r\n\r\n\r\nSingle-shot 3d multi-person pose estimation in complex images.\r\n\r\n\r\nPattern Recognition, page 107534, 2020."
        ],
        "references": [
            "Table 4 provides joint-wise results on the JTA dataset. PandaNet outperforms the model of Benzine et al. [3] for all joints. In particular, it has no difficulties to estimate 3D coordinates for the joints that have the fewest degrees of freedom (head, neck, clavicles , hips and spines) with a 3DPCK for these joints greater than 92%. PandaNet increases the 3DPCK for the shoulders by 44.6% and for the elbows by 34.9%. Terminal joints (wrists and ankles) are the most difficult joints with a 3DPCK of 60.1% and 58.0% for these joints against 19.0% and 8.9% for [3]."
        ]
    },
    "id_table_5": {
        "caption": "Table 5: MPJPE in mm on the Panoptic dataset.",
        "table": [
            "<table id=\"S4.T5.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T5.2.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T5.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Method</th>&#13;\n<th id=\"S4.T5.2.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Haggling</th>&#13;\n<th id=\"S4.T5.2.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Mafia</th>&#13;\n<th id=\"S4.T5.2.1.1.4\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Ultimatum</th>&#13;\n<th id=\"S4.T5.2.1.1.5\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Pizza</th>&#13;\n<th id=\"S4.T5.2.1.1.6\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Mean</th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T5.2.2.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T5.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib48\" title=\"\" class=\"ltx_ref\">48</a>]</cite></th>&#13;\n<td id=\"S4.T5.2.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\">140.0</td>&#13;\n<td id=\"S4.T5.2.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_t\">165.9</td>&#13;\n<td id=\"S4.T5.2.2.1.4\" class=\"ltx_td ltx_align_left ltx_border_t\">150.7</td>&#13;\n<td id=\"S4.T5.2.2.1.5\" class=\"ltx_td ltx_align_left ltx_border_t\">156.0</td>&#13;\n<td id=\"S4.T5.2.2.1.6\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\">153.4</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.2.3.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T5.2.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">49</a>]</cite></th>&#13;\n<td id=\"S4.T5.2.3.2.2\" class=\"ltx_td ltx_align_left\">72.4</td>&#13;\n<td id=\"S4.T5.2.3.2.3\" class=\"ltx_td ltx_align_left\">78.8</td>&#13;\n<td id=\"S4.T5.2.3.2.4\" class=\"ltx_td ltx_align_left\">66.8</td>&#13;\n<td id=\"S4.T5.2.3.2.5\" class=\"ltx_td ltx_align_left\">94.3</td>&#13;\n<td id=\"S4.T5.2.3.2.6\" class=\"ltx_td ltx_nopad_r ltx_align_left\">72.1</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.2.4.3\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T5.2.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2</a>]</cite></th>&#13;\n<td id=\"S4.T5.2.4.3.2\" class=\"ltx_td ltx_align_left\">70.1</td>&#13;\n<td id=\"S4.T5.2.4.3.3\" class=\"ltx_td ltx_align_left\">66.6</td>&#13;\n<td id=\"S4.T5.2.4.3.4\" class=\"ltx_td ltx_align_left\">55.6</td>&#13;\n<td id=\"S4.T5.2.4.3.5\" class=\"ltx_td ltx_align_left\">78.4</td>&#13;\n<td id=\"S4.T5.2.4.3.6\" class=\"ltx_td ltx_nopad_r ltx_align_left\">68.5</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.2.5.4\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T5.2.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">Ours</th>&#13;\n<td id=\"S4.T5.2.5.4.2\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span id=\"S4.T5.2.5.4.2.1\" class=\"ltx_text ltx_font_bold\">40.6</span></td>&#13;\n<td id=\"S4.T5.2.5.4.3\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span id=\"S4.T5.2.5.4.3.1\" class=\"ltx_text ltx_font_bold\">37.6</span></td>&#13;\n<td id=\"S4.T5.2.5.4.4\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span id=\"S4.T5.2.5.4.4.1\" class=\"ltx_text ltx_font_bold\">31.3</span></td>&#13;\n<td id=\"S4.T5.2.5.4.5\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span id=\"S4.T5.2.5.4.5.1\" class=\"ltx_text ltx_font_bold\">55.8</span></td>&#13;\n<td id=\"S4.T5.2.5.4.6\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t\"><span id=\"S4.T5.2.5.4.6.1\" class=\"ltx_text ltx_font_bold\">42.7</span></td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "[48] [48]\r\n\r\nAndrei Zanfir, Elisabeta Marinoiu, and Cristian Sminchisescu.\r\n\r\n\r\nMonocular 3d pose and shape estimation of multiple people in natural\r\nscenesthe importance of multiple scene constraints.\r\n\r\n\r\nIn CVPR, 2018.",
            "[49] [49]\r\n\r\nAndrei Zanfir, Elisabeta Marinoiu, Mihai Zanfir, Alin-Ionut Popa, and Cristian\r\nSminchisescu.\r\n\r\n\r\nDeep network for the integrated 3d sensing of multiple people in\r\nnatural images.\r\n\r\n\r\nIn NIPS, 2018.",
            "[2] [2]\r\n\r\nAbdallah Benzine, Bertrand Luvison, QuocCuong Pham, and Catherine Achard.\r\n\r\n\r\nDeep, robust and single shot 3d multi-person human pose estimation\r\nfrom monocular images.\r\n\r\n\r\nIn ICIP, 2019."
        ],
        "references": [
            "On this dataset, PandaNet improves the results over the recent state of the art methods on all scenarios (Table 5). The average MPJPE is improved by 25.8mm compared to the best previous approach. While the results on JTA prove the ability of the model to deal with realistic urban scenes with many people at low resolution, results on the Panoptic dataset show that the approach is effective to manage people overlaps and crops that frequently occur in this dataset."
        ]
    },
    "id_table_6": {
        "caption": "Table 6: 3DPCK on the MuPoTS-3D dataset.",
        "table": [
            "<table id=\"S4.T6.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T6.2.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T6.2.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\">Method</th>&#13;\n<th id=\"S4.T6.2.1.1.2\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\">3DPCK</th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T6.2.2.1\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T6.2.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S4.T6.2.2.1.1.1\" class=\"ltx_text\">Two-Stage</span></td>&#13;\n<td id=\"S4.T6.2.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">LCR-Net<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">39</a>]</cite>&#13;\n</td>&#13;\n<td id=\"S4.T6.2.2.1.3\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">53.8</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T6.2.3.2\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T6.2.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_r\">LCR-Net++ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\">40</a>]</cite>&#13;\n</td>&#13;\n<td id=\"S4.T6.2.3.2.2\" class=\"ltx_td ltx_nopad_r ltx_align_center\">70.6</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T6.2.4.3\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T6.2.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Moon <em id=\"S4.T6.2.4.3.1.1\" class=\"ltx_emph ltx_font_italic\">et al</em>.<span id=\"S4.T6.2.4.3.1.2\" class=\"ltx_text\"/> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib29\" title=\"\" class=\"ltx_ref\">29</a>]</cite>&#13;\n</td>&#13;\n<td id=\"S4.T6.2.4.3.2\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T6.2.4.3.2.1\" class=\"ltx_text ltx_font_bold\">81.8</span></td>&#13;\n</tr>&#13;\n<tr id=\"S4.T6.2.5.4\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T6.2.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S4.T6.2.5.4.1.1\" class=\"ltx_text\">Single-Shot</span></td>&#13;\n<td id=\"S4.T6.2.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Mehta <em id=\"S4.T6.2.5.4.2.1\" class=\"ltx_emph ltx_font_italic\">et al</em>.<span id=\"S4.T6.2.5.4.2.2\" class=\"ltx_text\"/> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">28</a>]</cite>&#13;\n</td>&#13;\n<td id=\"S4.T6.2.5.4.3\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">66.0</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T6.2.6.5\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T6.2.6.5.1\" class=\"ltx_td ltx_align_center ltx_border_r\">XNect <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">27</a>]</cite>&#13;\n</td>&#13;\n<td id=\"S4.T6.2.6.5.2\" class=\"ltx_td ltx_nopad_r ltx_align_center\">70.4</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T6.2.7.6\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T6.2.7.6.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">PandaNet</td>&#13;\n<td id=\"S4.T6.2.7.6.2\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\"><span id=\"S4.T6.2.7.6.2.1\" class=\"ltx_text ltx_font_bold\">72.0</span></td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "[39] [39]\r\n\r\nGregory Rogez, Philippe Weinzaepfel, and Cordelia Schmid.\r\n\r\n\r\nLcr-net: Localization-classification-regression for human pose.\r\n\r\n\r\nIn CVPR, 2017.",
            "[40] [40]\r\n\r\nGregory Rogez, Philippe Weinzaepfel, and Cordelia Schmid.\r\n\r\n\r\nLcr-net++: Multi-person 2d and 3d pose detection in natural images.\r\n\r\n\r\nTPAMI, 2019.",
            "[29] [29]\r\n\r\nGyeongsik Moon, JuYong Chang, and KyoungMu Lee.\r\n\r\n\r\nCamera distance-aware top-down approach for 3d multi-person pose\r\nestimation from a single rgb image.\r\n\r\n\r\nICCV, 2019.",
            "[28] [28]\r\n\r\nDushyant Mehta, Oleksandr Sotnychenko, Franziska Mueller, Weipeng Xu, Srinath\r\nSridhar, Gerard Pons-Moll, and Christian Theobalt.\r\n\r\n\r\nSingle-shot multi-person 3d body pose estimation from monocular rgb\r\ninput.\r\n\r\n\r\n3DV, 2017.",
            "[27] [27]\r\n\r\nDushyant Mehta, Oleksandr Sotnychenko, Franziska Mueller, Weipeng Xu, Mohamed\r\nElgharib, Pascal Fua, Hans-Peter Seidel, Helge Rhodin, Gerard Pons-Moll, and\r\nChristian Theobalt.\r\n\r\n\r\nXnect: Real-time multi-person 3d human pose estimation with a single\r\nrgb camera.\r\n\r\n\r\narXiv preprint arXiv:1907.00837, 2019."
        ],
        "references": [
            "Table 6 provides 3DPCK results on this dataset. PandaNet achieve higher 3DPCK than previous single-shot approaches. It improves over an ORPM method [28] by 6% and over XNect [27] by 1.6%. XNect is composed of two different models. The first one predicts partial 2D and 3D pose encoding and the second one refines these encodings to get final full 3D poses. Consequently, the weaknesses of the first model (like joints occlusions and people crops) are compensated by the second one. We achieve better results with a single model without any refinement process. Compared to two-stage models, PandaNet achieves better results than LCR-Net [39] and LCR-Net++ [40]. Compared to the approach of Moon et al. [30], PandaNet has a lower 3DPCK. This top-down approach uses an external two-stage object detector (Faster-R CNN [37]) to compute human bounding boxes and forward each cropped subject to a single-person 3D person approach [43]. Therefore, the computation complexity of their model depends on the number of people in the image like illustrated in Figure 5. If the number of people is large, this approach scales badly. On the contrary, the proposed single-shot model allows a nearly constant inference time regarding the number of people. The inference time of PandaNet is about 140ms for images with 60 people on a NVIDIA Titan X GPU."
        ]
    }
}