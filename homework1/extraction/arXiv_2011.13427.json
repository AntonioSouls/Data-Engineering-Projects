{
    "id_table_1": {
        "caption": "Table 1: Comparison of different models on the Human3.6M dataset (in mm), with each model trained on different datasets. The second column MPJPE is calculated after Procrustes analysis (PA). Our model shows better result before and after the PA alignment.",
        "table": [
            "<table id=\"S4.T1.2.2.2\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T1.2.2.2.3.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.2.2.2.3.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"/>&#13;\n<th id=\"S4.T1.2.2.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S4.T1.2.2.2.3.1.2.1\" class=\"ltx_text\">MPJPE</span></th>&#13;\n<th id=\"S4.T1.2.2.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S4.T1.2.2.2.3.1.3.1\" class=\"ltx_text\">PA MPJPE</span></th>&#13;\n<th id=\"S4.T1.2.2.2.3.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">camera</th>&#13;\n<th id=\"S4.T1.2.2.2.3.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">feature</th>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.2.2.2.4.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.2.2.2.4.2.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r\"/>&#13;\n<th id=\"S4.T1.2.2.2.4.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">calib.</th>&#13;\n<th id=\"S4.T1.2.2.2.4.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">aggregation</th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T1.2.2.2.5.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.2.2.2.5.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt\">SPIN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib20\" title=\"\" class=\"ltx_ref\">20</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T1.2.2.2.5.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">-</td>&#13;\n<td id=\"S4.T1.2.2.2.5.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">41.1</td>&#13;\n<td id=\"S4.T1.2.2.2.5.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">No</td>&#13;\n<td id=\"S4.T1.2.2.2.5.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">No</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.2.2.2.6.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.2.2.2.6.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">MuVS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">7</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T1.2.2.2.6.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">58.22</td>&#13;\n<td id=\"S4.T1.2.2.2.6.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">47.09</td>&#13;\n<td id=\"S4.T1.2.2.2.6.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\">Yes</td>&#13;\n<td id=\"S4.T1.2.2.2.6.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\">No</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.2.2.2.7.3\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.2.2.2.7.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">Liang <em id=\"S4.T1.2.2.2.7.3.1.1\" class=\"ltx_emph ltx_font_italic\">et al</em>.<span id=\"S4.T1.2.2.2.7.3.1.2\" class=\"ltx_text\"/> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">21</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T1.2.2.2.7.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">79.85</td>&#13;\n<td id=\"S4.T1.2.2.2.7.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">45.13</td>&#13;\n<td id=\"S4.T1.2.2.2.7.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\">No</td>&#13;\n<td id=\"S4.T1.2.2.2.7.3.5\" class=\"ltx_td ltx_align_center ltx_border_r\">No</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.1.1.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">SPIN<sup id=\"S4.T1.1.1.1.1.1.1\" class=\"ltx_sup\">4</sup>&#13;\n</th>&#13;\n<td id=\"S4.T1.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r\">57.5</td>&#13;\n<td id=\"S4.T1.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r\">35.4</td>&#13;\n<td id=\"S4.T1.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_r\">No</td>&#13;\n<td id=\"S4.T1.1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_r\">No</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.2.2.2.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.2.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">SPIN<sup id=\"S4.T1.2.2.2.2.1.1\" class=\"ltx_sup\"><span id=\"S4.T1.2.2.2.2.1.1.1\" class=\"ltx_text ltx_font_italic\">4,cal</span></sup>&#13;\n</th>&#13;\n<td id=\"S4.T1.2.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">49.8</td>&#13;\n<td id=\"S4.T1.2.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">35.4</td>&#13;\n<td id=\"S4.T1.2.2.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\">Yes</td>&#13;\n<td id=\"S4.T1.2.2.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\">No</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.2.2.2.8.4\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.2.2.2.8.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\">Ours</th>&#13;\n<td id=\"S4.T1.2.2.2.8.4.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T1.2.2.2.8.4.2.1\" class=\"ltx_text ltx_font_bold\">46.9</span></td>&#13;\n<td id=\"S4.T1.2.2.2.8.4.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T1.2.2.2.8.4.3.1\" class=\"ltx_text ltx_font_bold\">32.5</span></td>&#13;\n<td id=\"S4.T1.2.2.2.8.4.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">Yes</td>&#13;\n<td id=\"S4.T1.2.2.2.8.4.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">Yes</td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "[20] [20]\r\n\r\nNikos Kolotouros, Georgios Pavlakos, Michael J Black, and Kostas Daniilidis.\r\n\r\n\r\nLearning to reconstruct 3d human pose and shape via model-fitting in\r\nthe loop.\r\n\r\n\r\nIn ICCV, 2019.",
            "[7] [7]\r\n\r\nYinghao Huang, Federica Bogo, Christoph Lassner, Angjoo Kanazawa, Peter V.\r\nGehler, Javier Romero, Ijaz Akhter, and Michael J. Black.\r\n\r\n\r\nTowards accurate marker-less human shape and pose estimation over\r\ntime.\r\n\r\n\r\n2017 International Conference on 3D Vision (3DV), Oct 2017.",
            "[21] [21]\r\n\r\nJunbang Liang and Ming C. Lin.\r\n\r\n\r\nShape-aware human pose and shape reconstruction using multi-view\r\nimages.\r\n\r\n\r\nIn International Conference on Computer Vision (ICCV), 2019."
        ],
        "references": [
            "Although each model was trained on the different datasets, for performance comparison, we tested on the same datasets. In Table 1, we present the performance of our model on the Human3.6M dataset and compare it against state-of-the-art regression methods [20, 21] and an optimization algorithm [7]. Our approach outperform the previous models both before and after Procrustes analysis."
        ]
    },
    "id_table_2": {
        "caption": "Table 2: Comparison of models on the MPI-INF-3DHP dataset, where each model is trained on different datasets. Evaluation was calculated after aligning the prediction using Procrustes analysis. Higher PCK / AUC and lower MPJPE stands for better results. Our approach outperforms the previous multi-view method.",
        "table": [
            "<table id=\"S4.T2.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T2.1.1.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.1.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"/>&#13;\n<th id=\"S4.T2.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">PCK</th>&#13;\n<th id=\"S4.T2.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">AUC</th>&#13;\n<th id=\"S4.T2.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">MPJPE</th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T2.1.1.2.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt\">Liang <em id=\"S4.T2.1.1.2.1.1.1\" class=\"ltx_emph ltx_font_italic\">et al</em>.<span id=\"S4.T2.1.1.2.1.1.2\" class=\"ltx_text\"/> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">21</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T2.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">95</td>&#13;\n<td id=\"S4.T2.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">63</td>&#13;\n<td id=\"S4.T2.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">62</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.1.1.3.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\">Ours</th>&#13;\n<td id=\"S4.T2.1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T2.1.1.3.2.2.1\" class=\"ltx_text ltx_font_bold\">97.4</span></td>&#13;\n<td id=\"S4.T2.1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T2.1.1.3.2.3.1\" class=\"ltx_text ltx_font_bold\">65.5</span></td>&#13;\n<td id=\"S4.T2.1.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T2.1.1.3.2.4.1\" class=\"ltx_text ltx_font_bold\">50.2</span></td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "[21] [21]\r\n\r\nJunbang Liang and Ming C. Lin.\r\n\r\n\r\nShape-aware human pose and shape reconstruction using multi-view\r\nimages.\r\n\r\n\r\nIn International Conference on Computer Vision (ICCV), 2019."
        ],
        "references": [
            "Similarly, we present our result on the MPI-INF-3DHP dataset in Table 2. Here we do not compare against SPIN [20], since the publicly available version of SPIN has been trained on all multi-view data from MPI-INF-3DHP, including the test subject that we used to evaluate our mode. These comparisons indicate that our model outperforms prior model-based approaches with multiple images."
        ]
    },
    "id_table_3": {
        "caption": "Table 3: Comparison of computational costs. We compare our model with the other aggregation methods in terms of the number of model parameters, model size, and inference time.",
        "table": [
            "<table id=\"S4.T3.1.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T3.1.1.1.2.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.1.1.1.2.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"/>&#13;\n<th id=\"S4.T3.1.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">number of</th>&#13;\n<th id=\"S4.T3.1.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">GPU memory</th>&#13;\n<th id=\"S4.T3.1.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">inference</th>&#13;\n</tr>&#13;\n<tr id=\"S4.T3.1.1.1.3.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.1.1.1.3.2.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r\"/>&#13;\n<th id=\"S4.T3.1.1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">parameters</th>&#13;\n<th id=\"S4.T3.1.1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">usage</th>&#13;\n<th id=\"S4.T3.1.1.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">time</th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T3.1.1.1.4.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.1.1.1.4.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt\">Liang <em id=\"S4.T3.1.1.1.4.1.1.1\" class=\"ltx_emph ltx_font_italic\">et al</em>.<span id=\"S4.T3.1.1.1.4.1.1.2\" class=\"ltx_text\"/> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">21</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T3.1.1.1.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">27M</td>&#13;\n<td id=\"S4.T3.1.1.1.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">1.30MB</td>&#13;\n<td id=\"S4.T3.1.1.1.4.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">29ms</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T3.1.1.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">SPIN<sup id=\"S4.T3.1.1.1.1.1.1\" class=\"ltx_sup\"><span id=\"S4.T3.1.1.1.1.1.1.1\" class=\"ltx_text ltx_font_italic\">4</span></sup>&#13;\n</th>&#13;\n<td id=\"S4.T3.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r\">27M</td>&#13;\n<td id=\"S4.T3.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r\">1.32GB</td>&#13;\n<td id=\"S4.T3.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_r\">6ms</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T3.1.1.1.5.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.1.1.1.5.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">Iskekov <em id=\"S4.T3.1.1.1.5.2.1.1\" class=\"ltx_emph ltx_font_italic\">et al</em>.<span id=\"S4.T3.1.1.1.5.2.1.2\" class=\"ltx_text\"/> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">9</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T3.1.1.1.5.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">86M</td>&#13;\n<td id=\"S4.T3.1.1.1.5.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">6.08GB</td>&#13;\n<td id=\"S4.T3.1.1.1.5.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\">91ms</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T3.1.1.1.6.3\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.1.1.1.6.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\">Ours</th>&#13;\n<td id=\"S4.T3.1.1.1.6.3.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">72M</td>&#13;\n<td id=\"S4.T3.1.1.1.6.3.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">1.98GB</td>&#13;\n<td id=\"S4.T3.1.1.1.6.3.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">14ms</td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "[21] [21]\r\n\r\nJunbang Liang and Ming C. Lin.\r\n\r\n\r\nShape-aware human pose and shape reconstruction using multi-view\r\nimages.\r\n\r\n\r\nIn International Conference on Computer Vision (ICCV), 2019.",
            "[9] [9]\r\n\r\nKarim Iskakov, Egor Burkov, Victor Lempitsky, and Yury Malkov.\r\n\r\n\r\nLearnable triangulation of human pose.\r\n\r\n\r\nIn International Conference on Computer Vision (ICCV), 2019."
        ],
        "references": []
    }
}