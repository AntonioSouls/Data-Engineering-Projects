{
    "id_table_1": {
        "caption": "Table 1: Results evaluated on the validation set with various frame-level features.",
        "table": [
            "<table id=\"S4.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T1.1.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"/>&#13;\n<td id=\"S4.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">Team 1</td>&#13;\n<td id=\"S4.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">Team 2</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.1.2.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">EfficientNet-B7</th>&#13;\n<td id=\"S4.T1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">38.80%</td>&#13;\n<td id=\"S4.T1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">&#8212;</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.1.3.3\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">I3D (Kinetics)</th>&#13;\n<td id=\"S4.T1.1.3.3.2\" class=\"ltx_td ltx_align_center\">47.46%</td>&#13;\n<td id=\"S4.T1.1.3.3.3\" class=\"ltx_td ltx_align_center\">&#8212;</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.1.4.4\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">I3D (BSL1K)</th>&#13;\n<td id=\"S4.T1.1.4.4.2\" class=\"ltx_td ltx_align_center\">68.65%</td>&#13;\n<td id=\"S4.T1.1.4.4.3\" class=\"ltx_td ltx_align_center\">&#8212;</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.1.5.5\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.1.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">OpenPose</th>&#13;\n<td id=\"S4.T1.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\">83.25%</td>&#13;\n<td id=\"S4.T1.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\">79.99%</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.1.6.6\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.1.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Holistic</th>&#13;\n<td id=\"S4.T1.1.6.6.2\" class=\"ltx_td ltx_align_center\">85.63%</td>&#13;\n<td id=\"S4.T1.1.6.6.3\" class=\"ltx_td ltx_align_center\">82.14%</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.1.7.7\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.1.7.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">OpenPose+Holistic</th>&#13;\n<td id=\"S4.T1.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">84.16%</td>&#13;\n<td id=\"S4.T1.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">82.89%</td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [],
        "references": [
            "Table 1 shows our teams’ results on the validation set. We note that both teams’ approaches using pose estimation performed similarly, with validation accuracy ranging between 80% and 85%. It rules out trivial errors and implementation issues that, despite working independently, and with two separate pose estimation tools, both teams achieve similar evaluation scores.\r\nFurthermore, from a comparison between the pose estimation based systems (80-85%) and the pretrained image feature extractors (38-68%), we can see that pose estimation features do indeed generalize better to the nature of the challenge, including unseen signers and backgrounds."
        ]
    }
}