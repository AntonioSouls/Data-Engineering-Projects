{
    "id_table_1": {
        "caption": "Table 1: Details regarding the RGB-D Scenes datasets. ",
        "table": [
            "<table id=\"S4.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T1.1.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding:1.5pt 2.0pt;\">Scene</th>&#13;\n<th id=\"S4.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:1.5pt 2.0pt;\">Number of frames</th>&#13;\n<th id=\"S4.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:1.5pt 2.0pt;\">Models per frame</th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T1.1.2.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 2.0pt;\">desk_1</th>&#13;\n<td id=\"S4.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 2.0pt;\">98</td>&#13;\n<td id=\"S4.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 2.0pt;\">1.89</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.1.3.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 2.0pt;\">desk_2</th>&#13;\n<td id=\"S4.T1.1.3.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">190</td>&#13;\n<td id=\"S4.T1.1.3.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">1.85</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.1.4.3\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 2.0pt;\">desk_3</th>&#13;\n<td id=\"S4.T1.1.4.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">228</td>&#13;\n<td id=\"S4.T1.1.4.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">2.56</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.1.5.4\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 2.0pt;\">kitchen_small_1</th>&#13;\n<td id=\"S4.T1.1.5.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">180</td>&#13;\n<td id=\"S4.T1.1.5.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">3.55</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.1.6.5\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 2.0pt;\">meeting_small_1</th>&#13;\n<td id=\"S4.T1.1.6.5.2\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">180</td>&#13;\n<td id=\"S4.T1.1.6.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">8.79</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.1.7.6\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.1.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 2.0pt;\">table_1</th>&#13;\n<td id=\"S4.T1.1.7.6.2\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">125</td>&#13;\n<td id=\"S4.T1.1.7.6.3\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">5.92</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.1.8.7\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.1.8.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 2.0pt;\">table_small_1</th>&#13;\n<td id=\"S4.T1.1.8.7.2\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">199</td>&#13;\n<td id=\"S4.T1.1.8.7.3\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">3.68</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.1.9.8\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.1.9.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 2.0pt;\">table_small_2</th>&#13;\n<td id=\"S4.T1.1.9.8.2\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">234</td>&#13;\n<td id=\"S4.T1.1.9.8.3\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">2.89</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.1.10.9\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.1.10.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t\" style=\"padding:1.5pt 2.0pt;\">Average</th>&#13;\n<td id=\"S4.T1.1.10.9.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding:1.5pt 2.0pt;\">179.25</td>&#13;\n<td id=\"S4.T1.1.10.9.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding:1.5pt 2.0pt;\">3.89</td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [],
        "references": [
            "We validate our proposal on the Washington RGB-D Object and Scenes datasets. Proposed by [Lai et al., 2011a] the RGB-D Object contains a collection of 300 instances of household objects, grouped in 51 distinct categories. Each object includes a set of views, captured from different viewpoints with a Kinect sensor. A collection of 3 images, including RGB, depth, and mask is presented for each view. In total, this dataset has about 250 thousand distinct images. The authors also provide a dataset of scenes, named RGB-D Scenes. This evaluation dataset has eight video sequences of every-day environments. A Kinect sensor positioned at a human eye-level height acquires all the images at a 640×480640480640\\times 480 resolution. This dataset is related to the first one, composed of 13 of the 51 object categories on the Object dataset. These objects are positioned over tables, desks, and kitchen surfaces, cluttered with viewpoints and occlusion variation, and have annotation at category and instance levels. A bidimensional bounding box represents the ground-truth of each object’s position. Figure 2 presents examples of both datasets. Table 1 gives some details regarding the name and size of the sequences, and their average number of objects."
        ]
    },
    "id_table_2": {
        "caption": "Table 2: Comparison of CNN color features on the Washington RGB-D Object dataset. The best result reported in blue, the second best in green, and the third in red. ",
        "table": [
            "<table id=\"S4.T2.11.11\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T2.11.11.12.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.11.11.12.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding:1.5pt 2.0pt;\">Method</th>&#13;\n<th id=\"S4.T2.11.11.12.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:1.5pt 2.0pt;\">LSO</th>&#13;\n<th id=\"S4.T2.11.11.12.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:1.5pt 2.0pt;\">ACF</th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T2.1.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 2.0pt;\">Lai <span id=\"S4.T2.1.1.1.2.1\" class=\"ltx_text ltx_font_italic\">et al.</span> (RF) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bibx17\" title=\"\" class=\"ltx_ref\">Lai et&#160;al., 2011a</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 2.0pt;\">59.9</td>&#13;\n<td id=\"S4.T2.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 2.0pt;\">90.1 <math id=\"S4.T2.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T2.1.1.1.1.m1.1a\"><mo id=\"S4.T2.1.1.1.1.m1.1.1\" xref=\"S4.T2.1.1.1.1.m1.1.1.cmml\">&#177;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.1.1.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T2.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T2.1.1.1.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.1.1.1.1.m1.1c\">\\pm</annotation></semantics></math> 0.8</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.2.2.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.2.2.2.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 2.0pt;\">Lai <span id=\"S4.T2.2.2.2.2.1\" class=\"ltx_text ltx_font_italic\">et al.</span> (kSVC) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bibx17\" title=\"\" class=\"ltx_ref\">Lai et&#160;al., 2011a</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T2.2.2.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">60.7</td>&#13;\n<td id=\"S4.T2.2.2.2.1\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">91.0 <math id=\"S4.T2.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T2.2.2.2.1.m1.1a\"><mo id=\"S4.T2.2.2.2.1.m1.1.1\" xref=\"S4.T2.2.2.2.1.m1.1.1.cmml\">&#177;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.2.2.2.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T2.2.2.2.1.m1.1.1.cmml\" xref=\"S4.T2.2.2.2.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.2.2.2.1.m1.1c\">\\pm</annotation></semantics></math> 0.5</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.3.3.3\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.3.3.3.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 2.0pt;\">IDL <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bibx18\" title=\"\" class=\"ltx_ref\">Lai et&#160;al., 2011b</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T2.3.3.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">-</td>&#13;\n<td id=\"S4.T2.3.3.3.1\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">54.8 <math id=\"S4.T2.3.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T2.3.3.3.1.m1.1a\"><mo id=\"S4.T2.3.3.3.1.m1.1.1\" xref=\"S4.T2.3.3.3.1.m1.1.1.cmml\">&#177;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.3.3.3.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T2.3.3.3.1.m1.1.1.cmml\" xref=\"S4.T2.3.3.3.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.3.3.3.1.m1.1c\">\\pm</annotation></semantics></math> 0.6</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.11.11.13.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.11.11.13.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 2.0pt;\">SP+HMP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bibx6\" title=\"\" class=\"ltx_ref\">Bo et&#160;al., 2013</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T2.11.11.13.1.2\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">92.1</td>&#13;\n<td id=\"S4.T2.11.11.13.1.3\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">-</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.11.11.14.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.11.11.14.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 2.0pt;\">Multi-Modal <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bibx29\" title=\"\" class=\"ltx_ref\">Schwarz et&#160;al., 2015</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T2.11.11.14.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">92.0</td>&#13;\n<td id=\"S4.T2.11.11.14.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">-</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.11.11.15.3\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.11.11.15.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 2.0pt;\">MDSI-CNN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bibx4\" title=\"\" class=\"ltx_ref\">Asif et&#160;al., 2017</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T2.11.11.15.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\"><span id=\"S4.T2.11.11.15.3.2.1\" class=\"ltx_text\" style=\"color:#0000FF;\">97.7</span></td>&#13;\n<td id=\"S4.T2.11.11.15.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">-</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.11.11.16.4\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.11.11.16.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 2.0pt;\">MM-LRF-ELM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bibx19\" title=\"\" class=\"ltx_ref\">Liu et&#160;al., 2018</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T2.11.11.16.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">91.0</td>&#13;\n<td id=\"S4.T2.11.11.16.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">-</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.11.11.17.5\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.11.11.17.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 2.0pt;\">HP-CNN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bibx37\" title=\"\" class=\"ltx_ref\">Zaki et&#160;al., 2019</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T2.11.11.17.5.2\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\"><span id=\"S4.T2.11.11.17.5.2.1\" class=\"ltx_text\" style=\"color:#218C21;\">95.5</span></td>&#13;\n<td id=\"S4.T2.11.11.17.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">-</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.4.4.4\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.4.4.4.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 2.0pt;\">AlexNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bibx16\" title=\"\" class=\"ltx_ref\">Krizhevsky et&#160;al., 2012</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T2.4.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 2.0pt;\">89.8</td>&#13;\n<td id=\"S4.T2.4.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 2.0pt;\">93.9 <math id=\"S4.T2.4.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T2.4.4.4.1.m1.1a\"><mo id=\"S4.T2.4.4.4.1.m1.1.1\" xref=\"S4.T2.4.4.4.1.m1.1.1.cmml\">&#177;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.4.4.4.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T2.4.4.4.1.m1.1.1.cmml\" xref=\"S4.T2.4.4.4.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.4.4.4.1.m1.1c\">\\pm</annotation></semantics></math> 0.4</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.5.5.5\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.5.5.5.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 2.0pt;\">ResNet101<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bibx13\" title=\"\" class=\"ltx_ref\">He et&#160;al., 2016</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T2.5.5.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\"><span id=\"S4.T2.5.5.5.3.1\" class=\"ltx_text\" style=\"color:#FF0000;\">94.1</span></td>&#13;\n<td id=\"S4.T2.5.5.5.1\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">95.3 <math id=\"S4.T2.5.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T2.5.5.5.1.m1.1a\"><mo id=\"S4.T2.5.5.5.1.m1.1.1\" xref=\"S4.T2.5.5.5.1.m1.1.1.cmml\">&#177;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.5.5.5.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T2.5.5.5.1.m1.1.1.cmml\" xref=\"S4.T2.5.5.5.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.5.5.5.1.m1.1c\">\\pm</annotation></semantics></math> 0.3</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.6.6.6\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.6.6.6.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 2.0pt;\">VGG16 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bibx30\" title=\"\" class=\"ltx_ref\">Simonyan and Zisserman, 2015</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T2.6.6.6.3\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">88.8</td>&#13;\n<td id=\"S4.T2.6.6.6.1\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">91.0 <math id=\"S4.T2.6.6.6.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T2.6.6.6.1.m1.1a\"><mo id=\"S4.T2.6.6.6.1.m1.1.1\" xref=\"S4.T2.6.6.6.1.m1.1.1.cmml\">&#177;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.6.6.6.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T2.6.6.6.1.m1.1.1.cmml\" xref=\"S4.T2.6.6.6.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.6.6.6.1.m1.1c\">\\pm</annotation></semantics></math> 0.6</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.7.7.7\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.7.7.7.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 2.0pt;\">Inception v3 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bibx32\" title=\"\" class=\"ltx_ref\">Szegedy et&#160;al., 2016</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T2.7.7.7.3\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">88.1</td>&#13;\n<td id=\"S4.T2.7.7.7.1\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">90.3 <math id=\"S4.T2.7.7.7.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T2.7.7.7.1.m1.1a\"><mo id=\"S4.T2.7.7.7.1.m1.1.1\" xref=\"S4.T2.7.7.7.1.m1.1.1.cmml\">&#177;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.7.7.7.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T2.7.7.7.1.m1.1.1.cmml\" xref=\"S4.T2.7.7.7.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.7.7.7.1.m1.1c\">\\pm</annotation></semantics></math> 0.4</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.8.8.8\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.8.8.8.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 2.0pt;\">MobileNet v2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bibx28\" title=\"\" class=\"ltx_ref\">Sandler et&#160;al., 2018</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T2.8.8.8.3\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">93.8</td>&#13;\n<td id=\"S4.T2.8.8.8.1\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\"><span id=\"S4.T2.8.8.8.1.1\" class=\"ltx_text\" style=\"color:#0000FF;\">95.8 <math id=\"S4.T2.8.8.8.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T2.8.8.8.1.1.m1.1a\"><mo mathcolor=\"#0000FF\" id=\"S4.T2.8.8.8.1.1.m1.1.1\" xref=\"S4.T2.8.8.8.1.1.m1.1.1.cmml\">&#177;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.8.8.8.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T2.8.8.8.1.1.m1.1.1.cmml\" xref=\"S4.T2.8.8.8.1.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.8.8.8.1.1.m1.1c\">\\pm</annotation></semantics></math> 0.3</span></td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.10.10.10\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.9.9.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 2.0pt;\">ResNeXt101 <math id=\"S4.T2.9.9.9.1.m1.1\" class=\"ltx_Math\" alttext=\"32\\times 8\" display=\"inline\"><semantics id=\"S4.T2.9.9.9.1.m1.1a\"><mrow id=\"S4.T2.9.9.9.1.m1.1.1\" xref=\"S4.T2.9.9.9.1.m1.1.1.cmml\"><mn id=\"S4.T2.9.9.9.1.m1.1.1.2\" xref=\"S4.T2.9.9.9.1.m1.1.1.2.cmml\">32</mn><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S4.T2.9.9.9.1.m1.1.1.1\" xref=\"S4.T2.9.9.9.1.m1.1.1.1.cmml\">&#215;</mo><mn id=\"S4.T2.9.9.9.1.m1.1.1.3\" xref=\"S4.T2.9.9.9.1.m1.1.1.3.cmml\">8</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.9.9.9.1.m1.1b\"><apply id=\"S4.T2.9.9.9.1.m1.1.1.cmml\" xref=\"S4.T2.9.9.9.1.m1.1.1\"><times id=\"S4.T2.9.9.9.1.m1.1.1.1.cmml\" xref=\"S4.T2.9.9.9.1.m1.1.1.1\"/><cn type=\"integer\" id=\"S4.T2.9.9.9.1.m1.1.1.2.cmml\" xref=\"S4.T2.9.9.9.1.m1.1.1.2\">32</cn><cn type=\"integer\" id=\"S4.T2.9.9.9.1.m1.1.1.3.cmml\" xref=\"S4.T2.9.9.9.1.m1.1.1.3\">8</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.9.9.9.1.m1.1c\">32\\times 8</annotation></semantics></math>d <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bibx36\" title=\"\" class=\"ltx_ref\">Xie et&#160;al., 2017</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T2.10.10.10.3\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\">93.9</td>&#13;\n<td id=\"S4.T2.10.10.10.2\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 2.0pt;\"><span id=\"S4.T2.10.10.10.2.1\" class=\"ltx_text\" style=\"color:#218C21;\">95.7 <math id=\"S4.T2.10.10.10.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T2.10.10.10.2.1.m1.1a\"><mo mathcolor=\"#218C21\" id=\"S4.T2.10.10.10.2.1.m1.1.1\" xref=\"S4.T2.10.10.10.2.1.m1.1.1.cmml\">&#177;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.10.10.10.2.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T2.10.10.10.2.1.m1.1.1.cmml\" xref=\"S4.T2.10.10.10.2.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.10.10.10.2.1.m1.1c\">\\pm</annotation></semantics></math> 0.4</span></td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.11.11.11\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.11.11.11.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" style=\"padding:1.5pt 2.0pt;\">EfficientNet B7 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bibx33\" title=\"\" class=\"ltx_ref\">Tan and Le, 2019</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T2.11.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding:1.5pt 2.0pt;\">93.8</td>&#13;\n<td id=\"S4.T2.11.11.11.1\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding:1.5pt 2.0pt;\"><span id=\"S4.T2.11.11.11.1.1\" class=\"ltx_text\" style=\"color:#FF0000;\">95.6 <math id=\"S4.T2.11.11.11.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T2.11.11.11.1.1.m1.1a\"><mo mathcolor=\"#FF0000\" id=\"S4.T2.11.11.11.1.1.m1.1.1\" xref=\"S4.T2.11.11.11.1.1.m1.1.1.cmml\">&#177;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.11.11.11.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T2.11.11.11.1.1.m1.1.1.cmml\" xref=\"S4.T2.11.11.11.1.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.11.11.11.1.1.m1.1c\">\\pm</annotation></semantics></math> 0.5</span></td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "[Lai et al., 2011a] Lai et al., 2011a\r\n\r\nLai, K., Bo, L., Ren, X., and Fox, D. (2011a).\r\n\r\n\r\nA large-scale hierarchical multi-view RGB-D object dataset.\r\n\r\n\r\nIn 2011 IEEE international conference on robotics and\r\nautomation, pages 1817–1824. IEEE.",
            "[Lai et al., 2011a] Lai et al., 2011a\r\n\r\nLai, K., Bo, L., Ren, X., and Fox, D. (2011a).\r\n\r\n\r\nA large-scale hierarchical multi-view RGB-D object dataset.\r\n\r\n\r\nIn 2011 IEEE international conference on robotics and\r\nautomation, pages 1817–1824. IEEE.",
            "[Lai et al., 2011b] Lai et al., 2011b\r\n\r\nLai, K., Bo, L., Ren, X., and Fox, D. (2011b).\r\n\r\n\r\nSparse distance learning for object recognition combining rgb and\r\ndepth information.\r\n\r\n\r\nIn 2011 IEEE International Conference on Robotics and\r\nAutomation, pages 4007–4013. IEEE.",
            "[Bo et al., 2013] Bo et al., 2013\r\n\r\nBo, L., Ren, X., and Fox, D. (2013).\r\n\r\n\r\nUnsupervised feature learning for RGB-D based object recognition.\r\n\r\n\r\nIn Experimental robotics, pages 387–402. Springer.",
            "[Schwarz et al., 2015] Schwarz et al., 2015\r\n\r\nSchwarz, M., Schulz, H., and Behnke, S. (2015).\r\n\r\n\r\nRGB-D object recognition and pose estimation based on pre-trained\r\nconvolutional neural network features.\r\n\r\n\r\nIn 2015 IEEE international conference on robotics and automation\r\n(ICRA), pages 1329–1335. IEEE.",
            "[Asif et al., 2017] Asif et al., 2017\r\n\r\nAsif, U., Bennamoun, M., and Sohel, F. A. (2017).\r\n\r\n\r\nA multi-modal, discriminative and spatially invariant CNN for\r\nRGB-D object labeling.\r\n\r\n\r\nIEEE transactions on pattern analysis and machine intelligence,\r\n40(9):2051–2065.",
            "[Liu et al., 2018] Liu et al., 2018\r\n\r\nLiu, H., Li, F., Xu, X., and Sun, F. (2018).\r\n\r\n\r\nMulti-modal local receptive field extreme learning machine for object\r\nrecognition.\r\n\r\n\r\nNeurocomputing, 277:4–11.",
            "[Zaki et al., 2019] Zaki et al., 2019\r\n\r\nZaki, H. F., Shafait, F., and Mian, A. (2019).\r\n\r\n\r\nViewpoint invariant semantic object and scene categorization with\r\nRGB-D sensors.\r\n\r\n\r\nAutonomous Robots, 43(4):1005–1022.",
            "[Krizhevsky et al., 2012] Krizhevsky et al., 2012\r\n\r\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2012).\r\n\r\n\r\nImagenet classification with deep convolutional neural networks.\r\n\r\n\r\nIn Advances in neural information processing systems, pages\r\n1097–1105.",
            "[He et al., 2016] He et al., 2016\r\n\r\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016).\r\n\r\n\r\nDeep residual learning for image recognition.\r\n\r\n\r\nIn Proceedings of the IEEE conference on computer vision and\r\npattern recognition, pages 770–778.",
            "[Simonyan and Zisserman, 2015] Simonyan and Zisserman, 2015\r\n\r\nSimonyan, K. and Zisserman, A. (2015).\r\n\r\n\r\nVery deep convolutional networks for large-scale image recognition.\r\n\r\n\r\nIn International Conference on Learning Representations.",
            "[Szegedy et al., 2016] Szegedy et al., 2016\r\n\r\nSzegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. (2016).\r\n\r\n\r\nRethinking the inception architecture for computer vision.\r\n\r\n\r\nIn Proceedings of the IEEE conference on computer vision and\r\npattern recognition, pages 2818–2826.",
            "[Sandler et al., 2018] Sandler et al., 2018\r\n\r\nSandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L.-C. (2018).\r\n\r\n\r\nMobilenetv2: Inverted residuals and linear bottlenecks.\r\n\r\n\r\nIn Proceedings of the IEEE conference on computer vision and\r\npattern recognition, pages 4510–4520.",
            "[Xie et al., 2017] Xie et al., 2017\r\n\r\nXie, S., Girshick, R., Dollár, P., Tu, Z., and He, K. (2017).\r\n\r\n\r\nAggregated residual transformations for deep neural networks.\r\n\r\n\r\nIn Proceedings of the IEEE conference on computer vision and\r\npattern recognition, pages 1492–1500.",
            "[Tan and Le, 2019] Tan and Le, 2019\r\n\r\nTan, M. and Le, Q. (2019).\r\n\r\n\r\nEfficientnet: Rethinking model scaling for convolutional neural\r\nnetworks.\r\n\r\n\r\nIn International Conference on Machine Learning, pages\r\n6105–6114."
        ],
        "references": [
            "In our trials, we explored the achievements of Table 2, and selected the most accurate networks: ResNet101 [He et al., 2016], MobileNet v2 [Sandler et al., 2018], ResNeXt101 32×\\times8d [Xie et al., 2017], and EfficientNet-B7 [Tan and Le, 2019]. These networks input a 224×244224244224\\times 244 pixel image and output a 1000 bins feature vector. We employed the Logistic regression classifier, chosen after a performance evaluation of standard classifiers, to name: Support Vector Classifier (SVC) with linear and radial-based kernels, Random forest, Multilayer perceptron, and Gaussian naïve Bayes. We explore two variants of our ML model: a pre-trained on the Washington RGB-D Object dataset, and a distinct model, also in such dataset, but with a reduced number of objects, i.e., those annotated on the Washington RGB-D Scenes dataset. The latter provides an application-oriented approach, reducing the number of achievable classes, the inference time, and model size (Table 4). To verify the best accurate classifier, we do not perform object detection. Instead, we get the ground-truth bounding boxes provided by the dataset, hence verifying for each ML system which is the best feasible performance.",
            "We evaluate our proposal, quantitatively, and qualitatively. First, we consider CNN feature extraction and classification accuracy based on the models trained in the Object dataset (Table 2). We also verify the entire dataset’s processing time, looking at the frame processing rate in both classification and pose estimation scenarios.",
            "To assess the generalization capacity of CNN pre-trained models, we perform an object detection evaluation on the Object dataset [Lai et al., 2011a]. Table 2 present results regarding classification of partial views of objects. We evaluate the instance recognition scenario, following [Lai et al., 2011a], i.e., considering Alternating contiguous frame (ACF) and Leave-sequence-out (LSO) scenarios. We compared our results with state-of-the-art object detection methods on this dataset. We perceived that pre-trained networks provide reliable results as off-the-shelf color feature extractors. In both evaluation approaches, tested networks present competitive results concerning the other competitors. In LSO, ResNet101 [He et al., 2016] features figures in the third position, and in ACF, 5 of 7 architectures outperform previous proposals."
        ]
    },
    "id_table_3": {
        "caption": "Table 3: Instance classification performance on the RGB-D Scenes datasets. ",
        "table": [
            "<table id=\"S4.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T3.1.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"/>&#13;\n<th id=\"S4.T3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" colspan=\"2\">MobileNet v2</th>&#13;\n<th id=\"S4.T3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" colspan=\"2\">Resnet101</th>&#13;\n<th id=\"S4.T3.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" colspan=\"2\">ResNeXt101 32x8d</th>&#13;\n<th id=\"S4.T3.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" colspan=\"2\">EfficientNet-B7</th>&#13;\n</tr>&#13;\n<tr id=\"S4.T3.1.2.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Scene</th>&#13;\n<th id=\"S4.T3.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Acc</th>&#13;\n<th id=\"S4.T3.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">FPS</th>&#13;\n<th id=\"S4.T3.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Acc</th>&#13;\n<th id=\"S4.T3.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">FPS</th>&#13;\n<th id=\"S4.T3.1.2.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Acc</th>&#13;\n<th id=\"S4.T3.1.2.2.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">FPS</th>&#13;\n<th id=\"S4.T3.1.2.2.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Acc</th>&#13;\n<th id=\"S4.T3.1.2.2.9\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">FPS</th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T3.1.3.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.1.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">desk_1</th>&#13;\n<td id=\"S4.T3.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">42.70%</td>&#13;\n<td id=\"S4.T3.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">13.03</td>&#13;\n<td id=\"S4.T3.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">51.89%</td>&#13;\n<td id=\"S4.T3.1.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">9.66</td>&#13;\n<td id=\"S4.T3.1.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">48.11%</td>&#13;\n<td id=\"S4.T3.1.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">7.63</td>&#13;\n<td id=\"S4.T3.1.3.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">49.73%</td>&#13;\n<td id=\"S4.T3.1.3.1.9\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">6.55</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T3.1.4.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.1.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">desk_2</th>&#13;\n<td id=\"S4.T3.1.4.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">41.76%</td>&#13;\n<td id=\"S4.T3.1.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">12.95</td>&#13;\n<td id=\"S4.T3.1.4.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">38.92%</td>&#13;\n<td id=\"S4.T3.1.4.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">9.31</td>&#13;\n<td id=\"S4.T3.1.4.2.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">55.40%</td>&#13;\n<td id=\"S4.T3.1.4.2.7\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">7.93</td>&#13;\n<td id=\"S4.T3.1.4.2.8\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">76.42%</td>&#13;\n<td id=\"S4.T3.1.4.2.9\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">6.35</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T3.1.5.3\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.1.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">desk_3</th>&#13;\n<td id=\"S4.T3.1.5.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">72.77%</td>&#13;\n<td id=\"S4.T3.1.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">9.84</td>&#13;\n<td id=\"S4.T3.1.5.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">52.57%</td>&#13;\n<td id=\"S4.T3.1.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">7.09</td>&#13;\n<td id=\"S4.T3.1.5.3.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">52.91%</td>&#13;\n<td id=\"S4.T3.1.5.3.7\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">5.78</td>&#13;\n<td id=\"S4.T3.1.5.3.8\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">90.58%</td>&#13;\n<td id=\"S4.T3.1.5.3.9\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4.60</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T3.1.6.4\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.1.6.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">kitchen_small_1</th>&#13;\n<td id=\"S4.T3.1.6.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">36.31%</td>&#13;\n<td id=\"S4.T3.1.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">7.97</td>&#13;\n<td id=\"S4.T3.1.6.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">34.74%</td>&#13;\n<td id=\"S4.T3.1.6.4.5\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">5.29</td>&#13;\n<td id=\"S4.T3.1.6.4.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">48.20%</td>&#13;\n<td id=\"S4.T3.1.6.4.7\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4.12</td>&#13;\n<td id=\"S4.T3.1.6.4.8\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">56.81%</td>&#13;\n<td id=\"S4.T3.1.6.4.9\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.25</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T3.1.7.5\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.1.7.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">meeting_small_1</th>&#13;\n<td id=\"S4.T3.1.7.5.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">41.40%</td>&#13;\n<td id=\"S4.T3.1.7.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.29</td>&#13;\n<td id=\"S4.T3.1.7.5.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">38.05%</td>&#13;\n<td id=\"S4.T3.1.7.5.5\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2.35</td>&#13;\n<td id=\"S4.T3.1.7.5.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">42.92%</td>&#13;\n<td id=\"S4.T3.1.7.5.7\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.74</td>&#13;\n<td id=\"S4.T3.1.7.5.8\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">50.63%</td>&#13;\n<td id=\"S4.T3.1.7.5.9\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.33</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T3.1.8.6\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.1.8.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">table_1</th>&#13;\n<td id=\"S4.T3.1.8.6.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">56.76%</td>&#13;\n<td id=\"S4.T3.1.8.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4.62</td>&#13;\n<td id=\"S4.T3.1.8.6.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">38.11%</td>&#13;\n<td id=\"S4.T3.1.8.6.5\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.43</td>&#13;\n<td id=\"S4.T3.1.8.6.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">31.08%</td>&#13;\n<td id=\"S4.T3.1.8.6.7\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2.49</td>&#13;\n<td id=\"S4.T3.1.8.6.8\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">61.49%</td>&#13;\n<td id=\"S4.T3.1.8.6.9\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2.00</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T3.1.9.7\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.1.9.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">table_small_1</th>&#13;\n<td id=\"S4.T3.1.9.7.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">75.03%</td>&#13;\n<td id=\"S4.T3.1.9.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">7.50</td>&#13;\n<td id=\"S4.T3.1.9.7.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">63.30%</td>&#13;\n<td id=\"S4.T3.1.9.7.5\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">5.33</td>&#13;\n<td id=\"S4.T3.1.9.7.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">65.35%</td>&#13;\n<td id=\"S4.T3.1.9.7.7\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.89</td>&#13;\n<td id=\"S4.T3.1.9.7.8\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">83.36%</td>&#13;\n<td id=\"S4.T3.1.9.7.9\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.16</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T3.1.10.8\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.1.10.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">table_small_2</th>&#13;\n<td id=\"S4.T3.1.10.8.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">55.39%</td>&#13;\n<td id=\"S4.T3.1.10.8.3\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">9.13</td>&#13;\n<td id=\"S4.T3.1.10.8.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">45.35%</td>&#13;\n<td id=\"S4.T3.1.10.8.5\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">6.88</td>&#13;\n<td id=\"S4.T3.1.10.8.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">49.34%</td>&#13;\n<td id=\"S4.T3.1.10.8.7\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">5.04</td>&#13;\n<td id=\"S4.T3.1.10.8.8\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">65.88%</td>&#13;\n<td id=\"S4.T3.1.10.8.9\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4.10</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T3.1.11.9\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T3.1.11.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Average</th>&#13;\n<td id=\"S4.T3.1.11.9.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">52.77%</td>&#13;\n<td id=\"S4.T3.1.11.9.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">6.99</td>&#13;\n<td id=\"S4.T3.1.11.9.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">45.37%</td>&#13;\n<td id=\"S4.T3.1.11.9.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">5.03</td>&#13;\n<td id=\"S4.T3.1.11.9.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">49.16%</td>&#13;\n<td id=\"S4.T3.1.11.9.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.80</td>&#13;\n<td id=\"S4.T3.1.11.9.8\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">66.86%</td>&#13;\n<td id=\"S4.T3.1.11.9.9\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.02</td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [],
        "references": [
            "We opposed the selected CNN architectures examining only a classification based on the RGB information, taking the annotated bounding box, and submitting to the Color Feature Classification stage of our pipeline (as in Section 3.1). Table 3 relates to instance-level recognition.",
            "Regarding the frame processing rate, it is essential to notice that the average number of models varies from 1.85 to 8.79 over the scenes, with almost four objects per frame in mean (Table 3). Thus, we can infer that our proposal can deliver a near-real-time FPS, inclusive in a multi-classification problem. When we consider only a single target, the performance is almost four times faster, as presented in Table 7, on the Color only column."
        ]
    },
    "id_table_4": {
        "caption": "Table 4: Performance comparison between full and a specific training set with objects from the Scenes dataset. ",
        "table": [
            "<table id=\"S4.T4.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T4.1.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T4.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" style=\"padding:1.5pt 5.0pt;\" rowspan=\"2\"><span id=\"S4.T4.1.1.1.1.1\" class=\"ltx_text\">DeepNet</span></th>&#13;\n<th id=\"S4.T4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding:1.5pt 5.0pt;\" colspan=\"2\">Full</th>&#13;\n<th id=\"S4.T4.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:1.5pt 5.0pt;\" colspan=\"2\">Scenes</th>&#13;\n</tr>&#13;\n<tr id=\"S4.T4.1.2.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T4.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">Acc</th>&#13;\n<th id=\"S4.T4.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">FPS</th>&#13;\n<th id=\"S4.T4.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">Acc</th>&#13;\n<th id=\"S4.T4.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">FPS</th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T4.1.3.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T4.1.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">MobileNet v2</th>&#13;\n<td id=\"S4.T4.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">52.77%</td>&#13;\n<td id=\"S4.T4.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">6.99</td>&#13;\n<td id=\"S4.T4.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">67.35%</td>&#13;\n<td id=\"S4.T4.1.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">24.62</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T4.1.4.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T4.1.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding:1.5pt 5.0pt;\">Resnet101</th>&#13;\n<td id=\"S4.T4.1.4.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">45.37%</td>&#13;\n<td id=\"S4.T4.1.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:1.5pt 5.0pt;\">5.03</td>&#13;\n<td id=\"S4.T4.1.4.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">61.41%</td>&#13;\n<td id=\"S4.T4.1.4.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">13.94</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T4.1.5.3\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T4.1.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding:1.5pt 5.0pt;\">ResNeXt101 32x8d</th>&#13;\n<td id=\"S4.T4.1.5.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">49.16%</td>&#13;\n<td id=\"S4.T4.1.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:1.5pt 5.0pt;\">3.80</td>&#13;\n<td id=\"S4.T4.1.5.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">59.04%</td>&#13;\n<td id=\"S4.T4.1.5.3.5\" class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">8.86</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T4.1.6.4\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T4.1.6.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\" style=\"padding:1.5pt 5.0pt;\">EfficientNet-B7</th>&#13;\n<td id=\"S4.T4.1.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding:1.5pt 5.0pt;\">66.86%</td>&#13;\n<td id=\"S4.T4.1.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"padding:1.5pt 5.0pt;\">3.02</td>&#13;\n<td id=\"S4.T4.1.6.4.4\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding:1.5pt 5.0pt;\">82.94%</td>&#13;\n<td id=\"S4.T4.1.6.4.5\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding:1.5pt 5.0pt;\">5.88</td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [],
        "references": [
            "In our trials, we explored the achievements of Table 2, and selected the most accurate networks: ResNet101 [He et al., 2016], MobileNet v2 [Sandler et al., 2018], ResNeXt101 32×\\times8d [Xie et al., 2017], and EfficientNet-B7 [Tan and Le, 2019]. These networks input a 224×244224244224\\times 244 pixel image and output a 1000 bins feature vector. We employed the Logistic regression classifier, chosen after a performance evaluation of standard classifiers, to name: Support Vector Classifier (SVC) with linear and radial-based kernels, Random forest, Multilayer perceptron, and Gaussian naïve Bayes. We explore two variants of our ML model: a pre-trained on the Washington RGB-D Object dataset, and a distinct model, also in such dataset, but with a reduced number of objects, i.e., those annotated on the Washington RGB-D Scenes dataset. The latter provides an application-oriented approach, reducing the number of achievable classes, the inference time, and model size (Table 4). To verify the best accurate classifier, we do not perform object detection. Instead, we get the ground-truth bounding boxes provided by the dataset, hence verifying for each ML system which is the best feasible performance.",
            "The full-set of the Object dataset contains 51 categories and 300 distinct instances. Concerning the Scenes dataset, the number of annotated samples drops to 6 categories and 22 instances, i.e., only a small set of objects of Object dataset is achievable on the Scenes dataset. When we use a model trained on the full-set, most categories or instances will never be detected. Thou, we learned a lighter classifier that considers only such specific instances (Table 4)."
        ]
    },
    "id_table_5": {
        "caption": "Table 5: Comparison between feature-based registration methods. Values reported consider the processing time (in seconds) for ten views of the same object and the ICP for the best one selected. ",
        "table": [
            "<table id=\"S4.T5.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T5.4.4\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T5.4.4.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Methods</th>&#13;\n<th id=\"S4.T5.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">&#13;\n<table id=\"S4.T5.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S4.T5.1.1.1.1.2\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.1.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Feature-based</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.1.1.1.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.1.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">time (<math id=\"S4.T5.1.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T5.1.1.1.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S4.T5.1.1.1.1.1.1.m1.1.1\" xref=\"S4.T5.1.1.1.1.1.1.m1.1.1.cmml\">&#8595;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.1.1.1.1.1.1.m1.1b\"><ci id=\"S4.T5.1.1.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T5.1.1.1.1.1.1.m1.1.1\">&#8595;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.1.1.1.1.1.1.m1.1c\">\\downarrow</annotation></semantics></math>)</td>&#13;\n</tr>&#13;\n</table>&#13;\n</th>&#13;\n<th id=\"S4.T5.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">ICP time (<math id=\"S4.T5.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T5.2.2.2.m1.1a\"><mo stretchy=\"false\" id=\"S4.T5.2.2.2.m1.1.1\" xref=\"S4.T5.2.2.2.m1.1.1.cmml\">&#8595;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.2.2.2.m1.1b\"><ci id=\"S4.T5.2.2.2.m1.1.1.cmml\" xref=\"S4.T5.2.2.2.m1.1.1\">&#8595;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.2.2.2.m1.1c\">\\downarrow</annotation></semantics></math>)</th>&#13;\n<th id=\"S4.T5.3.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Inlier ratio (<math id=\"S4.T5.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S4.T5.3.3.3.m1.1a\"><mo stretchy=\"false\" id=\"S4.T5.3.3.3.m1.1.1\" xref=\"S4.T5.3.3.3.m1.1.1.cmml\">&#8593;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.3.3.3.m1.1b\"><ci id=\"S4.T5.3.3.3.m1.1.1.cmml\" xref=\"S4.T5.3.3.3.m1.1.1\">&#8593;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.3.3.3.m1.1c\">\\uparrow</annotation></semantics></math>)</th>&#13;\n<th id=\"S4.T5.4.4.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">RMSE (<math id=\"S4.T5.4.4.4.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T5.4.4.4.m1.1a\"><mo stretchy=\"false\" id=\"S4.T5.4.4.4.m1.1.1\" xref=\"S4.T5.4.4.4.m1.1.1.cmml\">&#8595;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.4.4.4.m1.1b\"><ci id=\"S4.T5.4.4.4.m1.1.1.cmml\" xref=\"S4.T5.4.4.4.m1.1.1\">&#8595;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.4.4.4.m1.1c\">\\downarrow</annotation></semantics></math>)</th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T5.4.5.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T5.4.5.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">RANSAC</th>&#13;\n<td id=\"S4.T5.4.5.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0.7688</td>&#13;\n<td id=\"S4.T5.4.5.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0.0061</td>&#13;\n<td id=\"S4.T5.4.5.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0.2689</td>&#13;\n<td id=\"S4.T5.4.5.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0.0055</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.4.6.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T5.4.6.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">FGR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bibx38\" title=\"\" class=\"ltx_ref\">Zhou et&#160;al., 2016</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T5.4.6.2.2\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0.0580</td>&#13;\n<td id=\"S4.T5.4.6.2.3\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0.0075</td>&#13;\n<td id=\"S4.T5.4.6.2.4\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0.1895</td>&#13;\n<td id=\"S4.T5.4.6.2.5\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0.0059</td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n",
            "<table id=\"S4.T5.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">&#13;\n<tr id=\"S4.T5.1.1.1.1.2\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.1.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Feature-based</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T5.1.1.1.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T5.1.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">time (<math id=\"S4.T5.1.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T5.1.1.1.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S4.T5.1.1.1.1.1.1.m1.1.1\" xref=\"S4.T5.1.1.1.1.1.1.m1.1.1.cmml\">&#8595;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.1.1.1.1.1.1.m1.1b\"><ci id=\"S4.T5.1.1.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T5.1.1.1.1.1.1.m1.1.1\">&#8595;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.1.1.1.1.1.1.m1.1c\">\\downarrow</annotation></semantics></math>)</td>&#13;\n</tr>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "[Zhou et al., 2016] Zhou et al., 2016\r\n\r\nZhou, Q.-Y., Park, J., and Koltun, V. (2016).\r\n\r\n\r\nFast global registration.\r\n\r\n\r\nIn European Conference on Computer Vision, pages 766–782.\r\nSpringer."
        ],
        "references": [
            "In Table 5 we report an evaluation concerning the Feature-based registration and Fine-adjustment stages of our pipeline. Getting a set of ten randomly selected views of the same object, we perform a coarse estimation by using RANSAC or FGR. We evaluate quantitatively such methods concerning the inlier ratio, RMSE, and execution time. We apply the resulting affine transformation as the input of an ICP dense registration and evaluate if this input can imply differences in the processing time."
        ]
    },
    "id_table_6": {
        "caption": "Table 6: Comparison of the proposed pipeline with standard object recognition and pose estimation approaches. Baseline refer to [Aldoma et al., 2012a] and Boost to [Marcon et al., 2019]. Every trial employed FPFH [Rusu et al., 2009] as local descriptor with a uniform sampling as keypoint detector. Excepting the fisrt two rows, leaf size was set to 1 cm.. ",
        "table": [
            "<table id=\"S4.T6.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T6.3.4.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T6.3.4.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Method</th>&#13;\n<td id=\"S4.T6.3.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">AUC</td>&#13;\n<td id=\"S4.T6.3.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">FPS</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T6.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T6.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Baseline <math id=\"S4.T6.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"US_{0.02}\" display=\"inline\"><semantics id=\"S4.T6.1.1.1.m1.1a\"><mrow id=\"S4.T6.1.1.1.m1.1.1\" xref=\"S4.T6.1.1.1.m1.1.1.cmml\"><mi id=\"S4.T6.1.1.1.m1.1.1.2\" xref=\"S4.T6.1.1.1.m1.1.1.2.cmml\">U</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.T6.1.1.1.m1.1.1.1\" xref=\"S4.T6.1.1.1.m1.1.1.1.cmml\">&#8203;</mo><msub id=\"S4.T6.1.1.1.m1.1.1.3\" xref=\"S4.T6.1.1.1.m1.1.1.3.cmml\"><mi id=\"S4.T6.1.1.1.m1.1.1.3.2\" xref=\"S4.T6.1.1.1.m1.1.1.3.2.cmml\">S</mi><mn id=\"S4.T6.1.1.1.m1.1.1.3.3\" xref=\"S4.T6.1.1.1.m1.1.1.3.3.cmml\">0.02</mn></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T6.1.1.1.m1.1b\"><apply id=\"S4.T6.1.1.1.m1.1.1.cmml\" xref=\"S4.T6.1.1.1.m1.1.1\"><times id=\"S4.T6.1.1.1.m1.1.1.1.cmml\" xref=\"S4.T6.1.1.1.m1.1.1.1\"/><ci id=\"S4.T6.1.1.1.m1.1.1.2.cmml\" xref=\"S4.T6.1.1.1.m1.1.1.2\">&#119880;</ci><apply id=\"S4.T6.1.1.1.m1.1.1.3.cmml\" xref=\"S4.T6.1.1.1.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.T6.1.1.1.m1.1.1.3.1.cmml\" xref=\"S4.T6.1.1.1.m1.1.1.3\">subscript</csymbol><ci id=\"S4.T6.1.1.1.m1.1.1.3.2.cmml\" xref=\"S4.T6.1.1.1.m1.1.1.3.2\">&#119878;</ci><cn type=\"float\" id=\"S4.T6.1.1.1.m1.1.1.3.3.cmml\" xref=\"S4.T6.1.1.1.m1.1.1.3.3\">0.02</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T6.1.1.1.m1.1c\">US_{0.02}</annotation></semantics></math>&#13;\n</th>&#13;\n<td id=\"S4.T6.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0.0401</td>&#13;\n<td id=\"S4.T6.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0.0023</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T6.2.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T6.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Boost <math id=\"S4.T6.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"US_{0.02}\" display=\"inline\"><semantics id=\"S4.T6.2.2.1.m1.1a\"><mrow id=\"S4.T6.2.2.1.m1.1.1\" xref=\"S4.T6.2.2.1.m1.1.1.cmml\"><mi id=\"S4.T6.2.2.1.m1.1.1.2\" xref=\"S4.T6.2.2.1.m1.1.1.2.cmml\">U</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.T6.2.2.1.m1.1.1.1\" xref=\"S4.T6.2.2.1.m1.1.1.1.cmml\">&#8203;</mo><msub id=\"S4.T6.2.2.1.m1.1.1.3\" xref=\"S4.T6.2.2.1.m1.1.1.3.cmml\"><mi id=\"S4.T6.2.2.1.m1.1.1.3.2\" xref=\"S4.T6.2.2.1.m1.1.1.3.2.cmml\">S</mi><mn id=\"S4.T6.2.2.1.m1.1.1.3.3\" xref=\"S4.T6.2.2.1.m1.1.1.3.3.cmml\">0.02</mn></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T6.2.2.1.m1.1b\"><apply id=\"S4.T6.2.2.1.m1.1.1.cmml\" xref=\"S4.T6.2.2.1.m1.1.1\"><times id=\"S4.T6.2.2.1.m1.1.1.1.cmml\" xref=\"S4.T6.2.2.1.m1.1.1.1\"/><ci id=\"S4.T6.2.2.1.m1.1.1.2.cmml\" xref=\"S4.T6.2.2.1.m1.1.1.2\">&#119880;</ci><apply id=\"S4.T6.2.2.1.m1.1.1.3.cmml\" xref=\"S4.T6.2.2.1.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.T6.2.2.1.m1.1.1.3.1.cmml\" xref=\"S4.T6.2.2.1.m1.1.1.3\">subscript</csymbol><ci id=\"S4.T6.2.2.1.m1.1.1.3.2.cmml\" xref=\"S4.T6.2.2.1.m1.1.1.3.2\">&#119878;</ci><cn type=\"float\" id=\"S4.T6.2.2.1.m1.1.1.3.3.cmml\" xref=\"S4.T6.2.2.1.m1.1.1.3.3\">0.02</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T6.2.2.1.m1.1c\">US_{0.02}</annotation></semantics></math>&#13;\n</th>&#13;\n<td id=\"S4.T6.2.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0.0868</td>&#13;\n<td id=\"S4.T6.2.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0.0918</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T6.3.3\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T6.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Boost <math id=\"S4.T6.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"US_{0.01}\" display=\"inline\"><semantics id=\"S4.T6.3.3.1.m1.1a\"><mrow id=\"S4.T6.3.3.1.m1.1.1\" xref=\"S4.T6.3.3.1.m1.1.1.cmml\"><mi id=\"S4.T6.3.3.1.m1.1.1.2\" xref=\"S4.T6.3.3.1.m1.1.1.2.cmml\">U</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.T6.3.3.1.m1.1.1.1\" xref=\"S4.T6.3.3.1.m1.1.1.1.cmml\">&#8203;</mo><msub id=\"S4.T6.3.3.1.m1.1.1.3\" xref=\"S4.T6.3.3.1.m1.1.1.3.cmml\"><mi id=\"S4.T6.3.3.1.m1.1.1.3.2\" xref=\"S4.T6.3.3.1.m1.1.1.3.2.cmml\">S</mi><mn id=\"S4.T6.3.3.1.m1.1.1.3.3\" xref=\"S4.T6.3.3.1.m1.1.1.3.3.cmml\">0.01</mn></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T6.3.3.1.m1.1b\"><apply id=\"S4.T6.3.3.1.m1.1.1.cmml\" xref=\"S4.T6.3.3.1.m1.1.1\"><times id=\"S4.T6.3.3.1.m1.1.1.1.cmml\" xref=\"S4.T6.3.3.1.m1.1.1.1\"/><ci id=\"S4.T6.3.3.1.m1.1.1.2.cmml\" xref=\"S4.T6.3.3.1.m1.1.1.2\">&#119880;</ci><apply id=\"S4.T6.3.3.1.m1.1.1.3.cmml\" xref=\"S4.T6.3.3.1.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.T6.3.3.1.m1.1.1.3.1.cmml\" xref=\"S4.T6.3.3.1.m1.1.1.3\">subscript</csymbol><ci id=\"S4.T6.3.3.1.m1.1.1.3.2.cmml\" xref=\"S4.T6.3.3.1.m1.1.1.3.2\">&#119878;</ci><cn type=\"float\" id=\"S4.T6.3.3.1.m1.1.1.3.3.cmml\" xref=\"S4.T6.3.3.1.m1.1.1.3.3\">0.01</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T6.3.3.1.m1.1c\">US_{0.01}</annotation></semantics></math>&#13;\n</th>&#13;\n<td id=\"S4.T6.3.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0.1372</td>&#13;\n<td id=\"S4.T6.3.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0.0339</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T6.3.5.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T6.3.5.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Resnet101 + FGR</th>&#13;\n<td id=\"S4.T6.3.5.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0.2228</td>&#13;\n<td id=\"S4.T6.3.5.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">13.8321</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T6.3.6.3\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T6.3.6.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">ResNet101 + RANSAC</th>&#13;\n<td id=\"S4.T6.3.6.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0.2092</td>&#13;\n<td id=\"S4.T6.3.6.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.9649</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T6.3.7.4\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T6.3.7.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">MobileNet v2 + FGR</th>&#13;\n<td id=\"S4.T6.3.7.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0.2922</td>&#13;\n<td id=\"S4.T6.3.7.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">13.8939</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T6.3.8.5\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T6.3.8.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">MobileNet v2 + RANSAC</th>&#13;\n<td id=\"S4.T6.3.8.5.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0.2781</td>&#13;\n<td id=\"S4.T6.3.8.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.8905</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T6.3.9.6\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T6.3.9.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">ResNeXt101 32x8d + FGR</th>&#13;\n<td id=\"S4.T6.3.9.6.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0.2090</td>&#13;\n<td id=\"S4.T6.3.9.6.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">14.1813</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T6.3.10.7\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T6.3.10.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">ResNeXt101 32x8d + RANSAC</th>&#13;\n<td id=\"S4.T6.3.10.7.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0.1947</td>&#13;\n<td id=\"S4.T6.3.10.7.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2.0268</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T6.3.11.8\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T6.3.11.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">EfficientNet-B7 + FGR</th>&#13;\n<td id=\"S4.T6.3.11.8.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0.4123</td>&#13;\n<td id=\"S4.T6.3.11.8.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">8.9429</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T6.3.12.9\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T6.3.12.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">EfficientNet-B7 + RANSAC</th>&#13;\n<td id=\"S4.T6.3.12.9.2\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0.2994</td>&#13;\n<td id=\"S4.T6.3.12.9.3\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.4344</td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "[Aldoma et al., 2012a] Aldoma et al., 2012a\r\n\r\nAldoma, A., Marton, Z., Tombari, F., Wohlkinger, W., Potthast, C., Zeisl, B.,\r\nand Vincze, M. (2012a).\r\n\r\n\r\nThree-dimensional object recognition and 6 DoF pose estimation.\r\n\r\n\r\nIEEE Robotics & Automation Magazine, pages 80–91.",
            "[Marcon et al., 2019] Marcon et al., 2019\r\n\r\nMarcon, M., Spezialetti, R., Salti, S., Silva, L., and Di Stefano, L. (2019).\r\n\r\n\r\nBoosting object recognition in point clouds by saliency detection.\r\n\r\n\r\nIn International Conference on Image Analysis and Processing,\r\npages 321–331. Springer.",
            "[Rusu et al., 2009] Rusu et al., 2009\r\n\r\nRusu, R. B., Blodow, N., and Beetz, M. (2009).\r\n\r\n\r\nFast point feature histograms (FPFH) for 3D registration.\r\n\r\n\r\nIn 2009 IEEE international conference on robotics and\r\nautomation, pages 3212–3217. IEEE."
        ],
        "references": [
            "To evaluate more deeply if the ICP, after the feature-matching application, can surpass problems like a more rough estimation, we must assess an annotated pose. Unfortunately, the adopted dataset does not offer such data, and further studies may verify that affirmation on a pose-annotated dataset. However, we can evaluate the estimation correctness by employing the IoU and MIR metrics and verify if the feature-based registration step’s estimation is reliable compared to standard approaches. In Table 6 we perform such comparison regarding the AUC and FPS values of different setup of our proposed pipeline, the standard [Aldoma et al., 2012b], and the boosted [Marcon et al., 2019] pipelines.",
            "Results of Table 6 confirm our claim that performing the object detection on the RGB images improves results compared to traditional approaches. Both standard and boosted pipelines present accuracy results worst than all trials we run in our pipeline, even considering the same conditions of descriptors and leaf size, e.g., 1 cm of leaf size in Boost U​S0.01𝑈subscript𝑆0.01US_{0.01} trial. When we consider time processing, the difference is even more discrepant when our approach presents in the best case, a frame-rate of 14.18 against 0.09 FPS on the best standard approaches, which represents a remarkable improvement of more than 150×150\\times in speed. When using the EfficientNet/FGR pair, our proposal presents AUC (0.4123) three times higher than the Boosted pipeline (0.1372). We did not run the Baseline U​S0.01𝑈subscript𝑆0.01US_{0.01} because this method is very time-consuming and does not represent a reasonable choice regarding the boosted version (Boost U​S0.01𝑈subscript𝑆0.01US_{0.01}). We found a frame rate of 0.00050.00050.0005 for a small set of frames experimentally. Besides, the boosted pipeline [Marcon et al., 2019] gains on accuracy and time performance regarding the traditional version, as seen on the trials with a leaf size of 0.020.020.02 (Baseline U​S0.02𝑈subscript𝑆0.02US_{0.02} and Boost U​S0.02𝑈subscript𝑆0.02US_{0.02}), and such behavior is also expected on a smaller leaf size."
        ]
    },
    "id_table_7": {
        "caption": "Table 7: Single target pose estimation FPS. Color only refers to object classification, other columns refer to the pose aligment step, coarse (RANSAC and FGR) or fine (plus ICP). ",
        "table": [
            "<table id=\"S4.T7.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T7.3.1.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T7.3.1.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"/>&#13;\n<th id=\"S4.T7.3.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Color only</th>&#13;\n<th id=\"S4.T7.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">RANSAC</th>&#13;\n<th id=\"S4.T7.3.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">FGR</th>&#13;\n<th id=\"S4.T7.3.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">RANSAC + ICP</th>&#13;\n<th id=\"S4.T7.3.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">FGR + ICP</th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T7.3.2.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T7.3.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">MobileNet v2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bibx28\" title=\"\" class=\"ltx_ref\">Sandler et&#160;al., 2018</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T7.3.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">89.49</td>&#13;\n<td id=\"S4.T7.3.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.89</td>&#13;\n<td id=\"S4.T7.3.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">13.89</td>&#13;\n<td id=\"S4.T7.3.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.82</td>&#13;\n<td id=\"S4.T7.3.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">13.57</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T7.3.3.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T7.3.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">ResNet101 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bibx13\" title=\"\" class=\"ltx_ref\">He et&#160;al., 2016</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T7.3.3.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">52.45</td>&#13;\n<td id=\"S4.T7.3.3.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.96</td>&#13;\n<td id=\"S4.T7.3.3.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">13.83</td>&#13;\n<td id=\"S4.T7.3.3.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.81</td>&#13;\n<td id=\"S4.T7.3.3.2.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">13.39</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T7.3.4.3\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T7.3.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">ResNeXt101 32x8d <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bibx36\" title=\"\" class=\"ltx_ref\">Xie et&#160;al., 2017</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T7.3.4.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">33.73</td>&#13;\n<td id=\"S4.T7.3.4.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2.03</td>&#13;\n<td id=\"S4.T7.3.4.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">14.18</td>&#13;\n<td id=\"S4.T7.3.4.3.5\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2.09</td>&#13;\n<td id=\"S4.T7.3.4.3.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">13.32</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T7.3.5.4\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T7.3.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">EfficientNet-B7 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bibx33\" title=\"\" class=\"ltx_ref\">Tan and Le, 2019</a>]</cite>&#13;\n</th>&#13;\n<td id=\"S4.T7.3.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">22.51</td>&#13;\n<td id=\"S4.T7.3.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.43</td>&#13;\n<td id=\"S4.T7.3.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">8.94</td>&#13;\n<td id=\"S4.T7.3.5.4.5\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.40</td>&#13;\n<td id=\"S4.T7.3.5.4.6\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">8.55</td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [
            "[Sandler et al., 2018] Sandler et al., 2018\r\n\r\nSandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L.-C. (2018).\r\n\r\n\r\nMobilenetv2: Inverted residuals and linear bottlenecks.\r\n\r\n\r\nIn Proceedings of the IEEE conference on computer vision and\r\npattern recognition, pages 4510–4520.",
            "[He et al., 2016] He et al., 2016\r\n\r\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016).\r\n\r\n\r\nDeep residual learning for image recognition.\r\n\r\n\r\nIn Proceedings of the IEEE conference on computer vision and\r\npattern recognition, pages 770–778.",
            "[Xie et al., 2017] Xie et al., 2017\r\n\r\nXie, S., Girshick, R., Dollár, P., Tu, Z., and He, K. (2017).\r\n\r\n\r\nAggregated residual transformations for deep neural networks.\r\n\r\n\r\nIn Proceedings of the IEEE conference on computer vision and\r\npattern recognition, pages 1492–1500.",
            "[Tan and Le, 2019] Tan and Le, 2019\r\n\r\nTan, M. and Le, Q. (2019).\r\n\r\n\r\nEfficientnet: Rethinking model scaling for convolutional neural\r\nnetworks.\r\n\r\n\r\nIn International Conference on Machine Learning, pages\r\n6105–6114."
        ],
        "references": [
            "Regarding the frame processing rate, it is essential to notice that the average number of models varies from 1.85 to 8.79 over the scenes, with almost four objects per frame in mean (Table 3). Thus, we can infer that our proposal can deliver a near-real-time FPS, inclusive in a multi-classification problem. When we consider only a single target, the performance is almost four times faster, as presented in Table 7, on the Color only column.",
            "Now we report the processing rate regarding executing the three stages of our proposed pipeline. Table 7 presents the frame processing rate based on a single target object scenario. We evaluate referring to the first stage execution (Color only), the early two stages (Columns RANSAC, and FGR), and a pipeline’s full execution (+ICP)."
        ]
    }
}