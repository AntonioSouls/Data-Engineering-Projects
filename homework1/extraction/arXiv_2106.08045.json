{
    "id_table_1": {
        "caption": "Table 1: Object detection results after training Mask R-CNN on different synthetic datasets",
        "table": [
            "<table id=\"S4.T1.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<thead class=\"ltx_thead\">&#13;\n<tr id=\"S4.T1.3.3\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T1.3.3.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr\">Object</th>&#13;\n<th id=\"S4.T1.3.3.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">dataset</th>&#13;\n<th id=\"S4.T1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\"><math id=\"S4.T1.1.1.1.m1.1\" class=\"ltx_math_unparsed\" alttext=\"\\textrm{AP}_{50:95(\\%)}\" display=\"inline\"><semantics id=\"S4.T1.1.1.1.m1.1a\"><msub id=\"S4.T1.1.1.1.m1.1.1\"><mtext mathsize=\"80%\" id=\"S4.T1.1.1.1.m1.1.1.2\">AP</mtext><mrow id=\"S4.T1.1.1.1.m1.1.1.3\"><mn mathsize=\"80%\" id=\"S4.T1.1.1.1.m1.1.1.3.1\">50</mn><mo lspace=\"0.278em\" mathsize=\"80%\" rspace=\"0.278em\" id=\"S4.T1.1.1.1.m1.1.1.3.2\">:</mo><mn mathsize=\"80%\" id=\"S4.T1.1.1.1.m1.1.1.3.3\">95</mn><mrow id=\"S4.T1.1.1.1.m1.1.1.3.4\"><mo maxsize=\"80%\" minsize=\"80%\" id=\"S4.T1.1.1.1.m1.1.1.3.4.1\">(</mo><mo mathsize=\"80%\" id=\"S4.T1.1.1.1.m1.1.1.3.4.2\">%</mo><mo maxsize=\"80%\" minsize=\"80%\" id=\"S4.T1.1.1.1.m1.1.1.3.4.3\">)</mo></mrow></mrow></msub><annotation encoding=\"application/x-tex\" id=\"S4.T1.1.1.1.m1.1b\">\\textrm{AP}_{50:95(\\%)}</annotation></semantics></math></th>&#13;\n<th id=\"S4.T1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\"><math id=\"S4.T1.2.2.2.m1.1\" class=\"ltx_math_unparsed\" alttext=\"\\footnotesize\\textrm{AP}_{50(\\%)}\" display=\"inline\"><semantics id=\"S4.T1.2.2.2.m1.1a\"><msub id=\"S4.T1.2.2.2.m1.1.1\"><mtext mathsize=\"80%\" id=\"S4.T1.2.2.2.m1.1.1.2\">AP</mtext><mrow id=\"S4.T1.2.2.2.m1.1.1.3\"><mn mathsize=\"80%\" id=\"S4.T1.2.2.2.m1.1.1.3.1\">50</mn><mrow id=\"S4.T1.2.2.2.m1.1.1.3.2\"><mo maxsize=\"80%\" minsize=\"80%\" id=\"S4.T1.2.2.2.m1.1.1.3.2.1\">(</mo><mo mathsize=\"80%\" id=\"S4.T1.2.2.2.m1.1.1.3.2.2\">%</mo><mo maxsize=\"80%\" minsize=\"80%\" id=\"S4.T1.2.2.2.m1.1.1.3.2.3\">)</mo></mrow></mrow></msub><annotation encoding=\"application/x-tex\" id=\"S4.T1.2.2.2.m1.1b\">\\footnotesize\\textrm{AP}_{50(\\%)}</annotation></semantics></math></th>&#13;\n<th id=\"S4.T1.3.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><math id=\"S4.T1.3.3.3.m1.1\" class=\"ltx_math_unparsed\" alttext=\"\\textrm{AR}^{\\text{\\tiny max}=100(\\%)}\" display=\"inline\"><semantics id=\"S4.T1.3.3.3.m1.1a\"><msup id=\"S4.T1.3.3.3.m1.1.1\"><mtext mathsize=\"80%\" id=\"S4.T1.3.3.3.m1.1.1.2\">AR</mtext><mrow id=\"S4.T1.3.3.3.m1.1.1.3\"><mtext mathsize=\"71%\" id=\"S4.T1.3.3.3.m1.1.1.3.1\">max</mtext><mo mathsize=\"80%\" id=\"S4.T1.3.3.3.m1.1.1.3.2\">=</mo><mn mathsize=\"80%\" id=\"S4.T1.3.3.3.m1.1.1.3.3\">100</mn><mrow id=\"S4.T1.3.3.3.m1.1.1.3.4\"><mo maxsize=\"80%\" minsize=\"80%\" id=\"S4.T1.3.3.3.m1.1.1.3.4.1\">(</mo><mo mathsize=\"80%\" id=\"S4.T1.3.3.3.m1.1.1.3.4.2\">%</mo><mo maxsize=\"80%\" minsize=\"80%\" id=\"S4.T1.3.3.3.m1.1.1.3.4.3\">)</mo></mrow></mrow></msup><annotation encoding=\"application/x-tex\" id=\"S4.T1.3.3.3.m1.1b\">\\textrm{AR}^{\\text{\\tiny max}=100(\\%)}</annotation></semantics></math></th>&#13;\n</tr>&#13;\n</thead>&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T1.3.4.1\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T1.3.4.1.1\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\" rowspan=\"2\"><span id=\"S4.T1.3.4.1.1.1\" class=\"ltx_text\">Object 1</span></td>&#13;\n<td id=\"S4.T1.3.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">naive</td>&#13;\n<td id=\"S4.T1.3.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">9.3</td>&#13;\n<td id=\"S4.T1.3.4.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">12.5</td>&#13;\n<td id=\"S4.T1.3.4.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">9.6</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.3.5.2\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T1.3.5.2.1\" class=\"ltx_td ltx_align_center ltx_border_r\">realistic</td>&#13;\n<td id=\"S4.T1.3.5.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.3.5.2.2.1\" class=\"ltx_text ltx_font_bold\">66.8</span></td>&#13;\n<td id=\"S4.T1.3.5.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.3.5.2.3.1\" class=\"ltx_text ltx_font_bold\">82.8</span></td>&#13;\n<td id=\"S4.T1.3.5.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.3.5.2.4.1\" class=\"ltx_text ltx_font_bold\">80.3</span></td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.3.6.3\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T1.3.6.3.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_rr\" rowspan=\"2\">&#13;\n<span id=\"S4.T1.3.6.3.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span><span id=\"S4.T1.3.6.3.1.2\" class=\"ltx_text\">Object 2</span>&#13;\n</td>&#13;\n<td id=\"S4.T1.3.6.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">naive</td>&#13;\n<td id=\"S4.T1.3.6.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">0.9</td>&#13;\n<td id=\"S4.T1.3.6.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\">1.0</td>&#13;\n<td id=\"S4.T1.3.6.3.5\" class=\"ltx_td ltx_align_center\">0.1</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T1.3.7.4\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T1.3.7.4.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">realistic</td>&#13;\n<td id=\"S4.T1.3.7.4.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T1.3.7.4.2.1\" class=\"ltx_text ltx_font_bold\">50.4</span></td>&#13;\n<td id=\"S4.T1.3.7.4.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T1.3.7.4.3.1\" class=\"ltx_text ltx_font_bold\">68.7</span></td>&#13;\n<td id=\"S4.T1.3.7.4.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S4.T1.3.7.4.4.1\" class=\"ltx_text ltx_font_bold\">59.2</span></td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [],
        "references": [
            "We fine-tuned a pretrained Mask R-CNN with a ResNet-50 backbone for 15 epochs with an initial learning rate of 0.001 and a mini-batch size of 4 images. The learning rate was reduced by a factor of 10 at epochs 3, 6, 9 and 12. Stochastic gradient descent (SGD) with momentum (0.9) and weight decay (0.0005) was used for optimization. Our work is based on the torchvision implementation of Mask R-CNN [23]. In Table. 1, the accuracies of object detection in terms of Aâ€‹P50ğ´subscriptğ‘ƒ50AP_{50} (the average precision with IoU thresholded at 0.50), Aâ€‹P50:95ğ´subscriptğ‘ƒ:5095AP_{50:95} and Aâ€‹Rmâ€‹aâ€‹x=100ğ´superscriptğ‘…ğ‘šğ‘ğ‘¥100AR^{max=100} (the average recall with 100 detections per image) are tabulated. While training on the naive dataset does not generalize well to our heavily cluttered real scenarios, Mask R-CNN trained on the realistic dataset considerably boosts the performance."
        ]
    },
    "id_table_2": {
        "caption": "Table 2: Average recall of top 5 pose estimates sorted by three different approaches for selecting the best estimates",
        "table": [
            "<table id=\"S4.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T2.1.2.1\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr\" rowspan=\"2\"><span id=\"S4.T2.1.2.1.1.1\" class=\"ltx_text\">Object</span></th>&#13;\n<td id=\"S4.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r\">sorted by Mask</td>&#13;\n<td id=\"S4.T2.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r\">sorted by max</td>&#13;\n<td id=\"S4.T2.1.2.1.4\" class=\"ltx_td ltx_align_center\">sorted by depth</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T2.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r\">R-CNN scores</td>&#13;\n<td id=\"S4.T2.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r\">cosine sim.</td>&#13;\n<td id=\"S4.T2.1.1.1\" class=\"ltx_td ltx_align_center\">differences <math id=\"S4.T2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"(e_{i})\" display=\"inline\"><semantics id=\"S4.T2.1.1.1.m1.1a\"><mrow id=\"S4.T2.1.1.1.m1.1.1.1\" xref=\"S4.T2.1.1.1.m1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S4.T2.1.1.1.m1.1.1.1.2\" xref=\"S4.T2.1.1.1.m1.1.1.1.1.cmml\">(</mo><msub id=\"S4.T2.1.1.1.m1.1.1.1.1\" xref=\"S4.T2.1.1.1.m1.1.1.1.1.cmml\"><mi id=\"S4.T2.1.1.1.m1.1.1.1.1.2\" xref=\"S4.T2.1.1.1.m1.1.1.1.1.2.cmml\">e</mi><mi id=\"S4.T2.1.1.1.m1.1.1.1.1.3\" xref=\"S4.T2.1.1.1.m1.1.1.1.1.3.cmml\">i</mi></msub><mo stretchy=\"false\" id=\"S4.T2.1.1.1.m1.1.1.1.3\" xref=\"S4.T2.1.1.1.m1.1.1.1.1.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.1.1.1.m1.1b\"><apply id=\"S4.T2.1.1.1.m1.1.1.1.1.cmml\" xref=\"S4.T2.1.1.1.m1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.T2.1.1.1.m1.1.1.1.1.1.cmml\" xref=\"S4.T2.1.1.1.m1.1.1.1\">subscript</csymbol><ci id=\"S4.T2.1.1.1.m1.1.1.1.1.2.cmml\" xref=\"S4.T2.1.1.1.m1.1.1.1.1.2\">&#119890;</ci><ci id=\"S4.T2.1.1.1.m1.1.1.1.1.3.cmml\" xref=\"S4.T2.1.1.1.m1.1.1.1.1.3\">&#119894;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.1.1.1.m1.1c\">(e_{i})</annotation></semantics></math>&#13;\n</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.1.3.2\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t\">Object 1</th>&#13;\n<td id=\"S4.T2.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.509</td>&#13;\n<td id=\"S4.T2.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.394</td>&#13;\n<td id=\"S4.T2.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.1.3.2.4.1\" class=\"ltx_text ltx_font_bold\">0.812</span></td>&#13;\n</tr>&#13;\n<tr id=\"S4.T2.1.4.3\" class=\"ltx_tr\">&#13;\n<th id=\"S4.T2.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_rr\">&#13;\n<span id=\"S4.T2.1.4.3.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span>Object 2</th>&#13;\n<td id=\"S4.T2.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">0.533</td>&#13;\n<td id=\"S4.T2.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">0.449</td>&#13;\n<td id=\"S4.T2.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S4.T2.1.4.3.4.1\" class=\"ltx_text ltx_font_bold\">0.633</span></td>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [],
        "references": [
            "To this goal, we trained the autoencoder with a latent space size of 128. We chose the L2 loss function, a learning rate of 0.0001 and used the Adam optimizer with a batch size of 32 and trained it for 40K iterations. The pose error metrics used for evaluation are the Visible Surface Discrepancy (VSD), the Maximum Symmetry-aware Surface Distance (MSSD) and the Maximum Symmetry-aware Projection Distance (MSPD), that are being used in the BOP2020 challenge [11]. An estimated pose is considered as correct w.r.t. the pose-error function eğ‘’e if e<Î¸eğ‘’subscriptğœƒğ‘’e<\\theta_{e}, where eâˆˆ{eVSD,eMSSD,eMSPD}ğ‘’subscriptğ‘’VSDsubscriptğ‘’MSSDsubscriptğ‘’MSPDe\\in\\{e_{\\text{VSD}},e_{\\text{MSSD}},e_{\\text{MSPD}}\\} and Î¸esubscriptğœƒğ‘’\\theta_{e} is the threshold of correctness. We used the same values for Î¸esubscriptğœƒğ‘’\\theta_{e} as in [11] to calculate the average recall rates Aâ€‹RVSDğ´subscriptğ‘…VSDAR_{\\text{VSD}}\r\n, Aâ€‹RMSSDğ´subscriptğ‘…MSSDAR_{\\text{MSSD}} and Aâ€‹RMSPDğ´subscriptğ‘…MSPDAR_{\\text{MSPD}}. The performance of the method on a dataset is measured by the Average Recall Aâ€‹R=(Aâ€‹RVSD+Aâ€‹RMSSD+Aâ€‹RMSPD)/3ğ´ğ‘…ğ´subscriptğ‘…VSDğ´subscriptğ‘…MSSDğ´subscriptğ‘…MSPD3AR=\\big{(}AR_{\\text{VSD}}+AR_{\\text{MSSD}}+AR_{\\text{MSPD}}\\big{)}/3. \r\nIn Table 2, we compare the results of the different methods on selecting the best five pose estimates. In these experiments, we did not make use of ICP, but we took the depth measurement at the center of the object. It shows that sorting according to the vector (ei)subscriptğ‘’ğ‘–(e_{i}) in (2), results in superior performance compared to other approaches.\r\nThe experiments in Table 3 are conducted using the selection method defined by the vector (ei)subscriptğ‘’ğ‘–(e_{i}). We compare the results, when testing with only RGB information, using the depth measurement at the object center and the improvement through ICP refinement. While ICP refinement increases the performance slightly, the refinement of several hundred poses per image takes time, making it impractical for the usage in real-time settings. The experiments to reduce the noise in cluttered scenes, like feeding only the pixels visible in the mask to the autoencoder, or training the autoencoder with multiple objects, have shown, against our intuition, a worse performance than the normal pipeline. Note how incorporating the depth has improved the accuracy. We have shown qualitative results in the supplementary materials."
        ]
    },
    "id_table_3": {
        "caption": "Table 3: Average recall of top 5 pose estimates using ICP and depth measurements and combinations.",
        "table": [
            "<table id=\"S4.T3.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">&#13;\n<tbody class=\"ltx_tbody\">&#13;\n<tr id=\"S4.T3.2.1.1\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T3.2.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_rr\" rowspan=\"2\"><span id=\"S4.T3.2.1.1.1.1\" class=\"ltx_text\">Object</span></td>&#13;\n<td id=\"S4.T3.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r\">RGB</td>&#13;\n<td id=\"S4.T3.2.1.1.3\" class=\"ltx_td ltx_align_center\" colspan=\"4\">RGB + depth</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T3.2.2.2\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T3.2.2.2.1\" class=\"ltx_td ltx_border_r\"/>&#13;\n<td id=\"S4.T3.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">+ ICP</td>&#13;\n<td id=\"S4.T3.2.2.2.3\" class=\"ltx_td ltx_align_center\">normal</td>&#13;\n<td id=\"S4.T3.2.2.2.4\" class=\"ltx_td ltx_align_center\">mult.-obj.</td>&#13;\n<td id=\"S4.T3.2.2.2.5\" class=\"ltx_td ltx_align_center\">mask</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T3.2.3.3\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T3.2.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">Object 1</td>&#13;\n<td id=\"S4.T3.2.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.691</td>&#13;\n<td id=\"S4.T3.2.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.2.3.3.3.1\" class=\"ltx_text ltx_font_bold\">0.829</span></td>&#13;\n<td id=\"S4.T3.2.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.812</td>&#13;\n<td id=\"S4.T3.2.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.790</td>&#13;\n<td id=\"S4.T3.2.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\">0.798</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T3.2.4.4\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T3.2.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">&#13;\n<span id=\"S4.T3.2.4.4.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span>Object 2</td>&#13;\n<td id=\"S4.T3.2.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.348</td>&#13;\n<td id=\"S4.T3.2.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.2.4.4.3.1\" class=\"ltx_text ltx_font_bold\">0.703</span></td>&#13;\n<td id=\"S4.T3.2.4.4.4\" class=\"ltx_td ltx_align_center\">0.633</td>&#13;\n<td id=\"S4.T3.2.4.4.5\" class=\"ltx_td ltx_align_center\">0.627</td>&#13;\n<td id=\"S4.T3.2.4.4.6\" class=\"ltx_td ltx_align_center\">0.614</td>&#13;\n</tr>&#13;\n<tr id=\"S4.T3.2.5.5\" class=\"ltx_tr\">&#13;\n<td id=\"S4.T3.2.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t\">time (s)</td>&#13;\n<td id=\"S4.T3.2.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T3.2.5.5.2.1\" class=\"ltx_text ltx_font_bold\">0.699</span></td>&#13;\n<td id=\"S4.T3.2.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">18.39</td>&#13;\n<td id=\"S4.T3.2.5.5.4\" class=\"ltx_td ltx_border_b ltx_border_t\"/>&#13;\n<td id=\"S4.T3.2.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S4.T3.2.5.5.5.1\" class=\"ltx_text ltx_font_bold\">0.697</span></td>&#13;\n<td id=\"S4.T3.2.5.5.6\" class=\"ltx_td ltx_border_b ltx_border_t\"/>&#13;\n</tr>&#13;\n</tbody>&#13;\n</table>&#13;\n\n"
        ],
        "footnotes": [],
        "references": [
            "To this goal, we trained the autoencoder with a latent space size of 128. We chose the L2 loss function, a learning rate of 0.0001 and used the Adam optimizer with a batch size of 32 and trained it for 40K iterations. The pose error metrics used for evaluation are the Visible Surface Discrepancy (VSD), the Maximum Symmetry-aware Surface Distance (MSSD) and the Maximum Symmetry-aware Projection Distance (MSPD), that are being used in the BOP2020 challenge [11]. An estimated pose is considered as correct w.r.t. the pose-error function eğ‘’e if e<Î¸eğ‘’subscriptğœƒğ‘’e<\\theta_{e}, where eâˆˆ{eVSD,eMSSD,eMSPD}ğ‘’subscriptğ‘’VSDsubscriptğ‘’MSSDsubscriptğ‘’MSPDe\\in\\{e_{\\text{VSD}},e_{\\text{MSSD}},e_{\\text{MSPD}}\\} and Î¸esubscriptğœƒğ‘’\\theta_{e} is the threshold of correctness. We used the same values for Î¸esubscriptğœƒğ‘’\\theta_{e} as in [11] to calculate the average recall rates Aâ€‹RVSDğ´subscriptğ‘…VSDAR_{\\text{VSD}}\r\n, Aâ€‹RMSSDğ´subscriptğ‘…MSSDAR_{\\text{MSSD}} and Aâ€‹RMSPDğ´subscriptğ‘…MSPDAR_{\\text{MSPD}}. The performance of the method on a dataset is measured by the Average Recall Aâ€‹R=(Aâ€‹RVSD+Aâ€‹RMSSD+Aâ€‹RMSPD)/3ğ´ğ‘…ğ´subscriptğ‘…VSDğ´subscriptğ‘…MSSDğ´subscriptğ‘…MSPD3AR=\\big{(}AR_{\\text{VSD}}+AR_{\\text{MSSD}}+AR_{\\text{MSPD}}\\big{)}/3. \r\nIn Table 2, we compare the results of the different methods on selecting the best five pose estimates. In these experiments, we did not make use of ICP, but we took the depth measurement at the center of the object. It shows that sorting according to the vector (ei)subscriptğ‘’ğ‘–(e_{i}) in (2), results in superior performance compared to other approaches.\r\nThe experiments in Table 3 are conducted using the selection method defined by the vector (ei)subscriptğ‘’ğ‘–(e_{i}). We compare the results, when testing with only RGB information, using the depth measurement at the object center and the improvement through ICP refinement. While ICP refinement increases the performance slightly, the refinement of several hundred poses per image takes time, making it impractical for the usage in real-time settings. The experiments to reduce the noise in cluttered scenes, like feeding only the pixels visible in the mask to the autoencoder, or training the autoencoder with multiple objects, have shown, against our intuition, a worse performance than the normal pipeline. Note how incorporating the depth has improved the accuracy. We have shown qualitative results in the supplementary materials."
        ]
    }
}