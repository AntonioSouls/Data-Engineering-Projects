<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2205.02065] Mobile-URSONet: an Embeddable Neural Network for Onboard Spacecraft Pose Estimation</title><meta property="og:description" content="Spacecraft pose estimation is an essential computer vision application that can improve the autonomy of in-orbit operations. An ESA/Stanford competition brought out solutions that seem hardly compatible with the constr…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Mobile-URSONet: an Embeddable Neural Network for Onboard Spacecraft Pose Estimation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Mobile-URSONet: an Embeddable Neural Network for Onboard Spacecraft Pose Estimation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2205.02065">

<!--Generated on Mon Mar 11 12:58:43 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">Mobile-URSONet: an Embeddable Neural Network for Onboard Spacecraft Pose Estimation
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Julien Posso*, Guy Bois*, Yvon Savaria<math id="id1.1.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><ci id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\dagger</annotation></semantics></math>
<br class="ltx_break">Department of *Computer Engineering and <math id="id2.2.m2.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="id2.2.m2.1a"><mo id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><ci id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">\dagger</annotation></semantics></math>Electrical Engineering, École Polytechnique de Montréal

<br class="ltx_break">Montréal (QC), Canada

<br class="ltx_break">Email: {firstname.lastname}@polymtl.ca
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">Spacecraft pose estimation is an essential computer vision application that can improve the autonomy of in-orbit operations. An ESA/Stanford competition brought out solutions that seem hardly compatible with the constraints imposed on spacecraft onboard computers. URSONet is among the best in the competition for its generalization capabilities but at the cost of a tremendous number of parameters and high computational complexity. In this paper, we propose Mobile-URSONet: a spacecraft pose estimation convolutional neural network with 178 times fewer parameters while degrading accuracy by no more than four times compared to URSONet.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">INTRODUCTION</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Estimating the relative position and orientation (commonly called Pose estimation) of a known but uncooperative spacecraft from a monocular image is an essential computer vision application that allows improving the autonomy of in-orbit spacecraft operations: formation flying, autonomous docking, satellite maintenance, debris removal, etc… <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Debris removal is crucial for the future of low earth orbit operations as the exploitation of this orbit grows, especially with the Starlink and OneWeb constellations. A significant increase of debris will accompany these new constellations. Several research projects aim to solve this problem: RemoveDEBRIS from the Surrey Space Center, Restore-L from NASA, Phoenix program from DARPA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, or more recently the ClearSpace-1 mission from the European Space Agency (ESA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Sharma <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> were the first to propose using Convolutional Neural Networks (CNNs) for Spacecraft Pose Estimation (SPE) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. However, the popularity of CNNs applied to SPE increased in 2019 when ESA and Stanford SLAB (Space Rendezvous Laboratory) organized a competition that brought together 48 participants. Each team proposed a solution that is based at least in part on deep neural networks, now a dominant technique <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. The competition was based on the SPEED (Spacecraft PosE Estimation Dataset) dataset introduced earlier by Sharma <span id="S1.p2.1.2" class="ltx_text ltx_font_italic">et. al</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. The dataset contains 12000 synthetic images of the Tango satellite to train the models; 2998 synthetic images on which the participants were ranked (synthetic test set); and 300 real images, which allow characterizing the generalization capacity of the proposed models (real test set). A post-mortem webpage dedicated to model predictions evaluation is still available as the labels of the test sets are not provided <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Sharma <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, Chan <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and the EPFL CVLab team used a 3-step process to estimate spacecraft pose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. First, they use an object detection CNN to determine the region of interest and crop the input image. Then, another CNN regress keypoints. Finally, they solve Pose estimation using an off-the-shelf Perspective-n-Point (PnP) solver. Black <span id="S1.p3.1.3" class="ltx_text ltx_font_italic">et al.</span> follow the same 3-step process, but instead of using large CNNs, they use a MobileNet-v2 as keypoint regression network. Nevertheless, their method uses a complex pipeline in which the MobileNet-v2 CNN only represents 20.4% of the inference execution time<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Moreover, using keypoints limits the method to known spacecraft.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Proença <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">et al.</span> proposed URSONet: a straightforward way to solve SPE by regressing position and orientation using a single CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. It allows to directly optimize ESA competition metrics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. They also proposed to deal with the orientation estimation as a soft classification task which significantly improves the results. The orientation is encoded as a Gaussian random variable in a discrete output space so that the CNN learns to predict a mass density function <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Then, they used a softmax function and the quaternion averaging technique to predict the orientation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. URSONet stands out for its generalization capabilities as Proença <span id="S1.p4.1.2" class="ltx_text ltx_font_italic">et al.</span> obtained a good score on both synthetic and real test sets. In addition, as they do not rely on keypoints, their method would be able to generalize to objects with unknown geometry using SLAM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. However, it comes at the cost of a tremendous amount of parameters (500 million) and high computational complexity as they use an ensemble method based on three ResNet-101 CNNs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In the spirit of MobileNet proposed by Google <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, we propose Mobile-URSONet: a spacecraft pose estimation convolutional neural network adapted to spacecraft onboard computers. Our lightest model has 178 times fewer parameters while degrading accuracy by no more than four times compared to URSONet.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The outline of this paper is as follows: Section <a href="#S2" title="II PROPOSED METHOD ‣ Mobile-URSONet: an Embeddable Neural Network for Onboard Spacecraft Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> explains the methodology we adopted to optimize URSONet for embedded systems. Section <a href="#S3" title="III EXPERIMENTS ‣ Mobile-URSONet: an Embeddable Neural Network for Onboard Spacecraft Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> presents the experimental results obtained, and section <a href="#S4" title="IV CONCLUSIONS ‣ Mobile-URSONet: an Embeddable Neural Network for Onboard Spacecraft Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> concludes the paper.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">PROPOSED METHOD</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.5" class="ltx_p">Our analysis is based on the ESA competition evaluation metrics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> which includes: the mean absolute position error <math id="S2.p1.1.m1.1" class="ltx_Math" alttext="e_{t}" display="inline"><semantics id="S2.p1.1.m1.1a"><msub id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml"><mi id="S2.p1.1.m1.1.1.2" xref="S2.p1.1.m1.1.1.2.cmml">e</mi><mi id="S2.p1.1.m1.1.1.3" xref="S2.p1.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><apply id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p1.1.m1.1.1.1.cmml" xref="S2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.p1.1.m1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.2">𝑒</ci><ci id="S2.p1.1.m1.1.1.3.cmml" xref="S2.p1.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">e_{t}</annotation></semantics></math> (in meters), the mean absolute orientation error <math id="S2.p1.2.m2.1" class="ltx_Math" alttext="e_{q}" display="inline"><semantics id="S2.p1.2.m2.1a"><msub id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml"><mi id="S2.p1.2.m2.1.1.2" xref="S2.p1.2.m2.1.1.2.cmml">e</mi><mi id="S2.p1.2.m2.1.1.3" xref="S2.p1.2.m2.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><apply id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p1.2.m2.1.1.1.cmml" xref="S2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.p1.2.m2.1.1.2.cmml" xref="S2.p1.2.m2.1.1.2">𝑒</ci><ci id="S2.p1.2.m2.1.1.3.cmml" xref="S2.p1.2.m2.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">e_{q}</annotation></semantics></math> (in degrees), and the mean ESA score <math id="S2.p1.3.m3.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S2.p1.3.m3.1a"><mi id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><ci id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">E</annotation></semantics></math> (lower is better) which is evaluated on both the synthetic (<math id="S2.p1.4.m4.1" class="ltx_Math" alttext="E_{syn}" display="inline"><semantics id="S2.p1.4.m4.1a"><msub id="S2.p1.4.m4.1.1" xref="S2.p1.4.m4.1.1.cmml"><mi id="S2.p1.4.m4.1.1.2" xref="S2.p1.4.m4.1.1.2.cmml">E</mi><mrow id="S2.p1.4.m4.1.1.3" xref="S2.p1.4.m4.1.1.3.cmml"><mi id="S2.p1.4.m4.1.1.3.2" xref="S2.p1.4.m4.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.p1.4.m4.1.1.3.1" xref="S2.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S2.p1.4.m4.1.1.3.3" xref="S2.p1.4.m4.1.1.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S2.p1.4.m4.1.1.3.1a" xref="S2.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S2.p1.4.m4.1.1.3.4" xref="S2.p1.4.m4.1.1.3.4.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.1b"><apply id="S2.p1.4.m4.1.1.cmml" xref="S2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.p1.4.m4.1.1.1.cmml" xref="S2.p1.4.m4.1.1">subscript</csymbol><ci id="S2.p1.4.m4.1.1.2.cmml" xref="S2.p1.4.m4.1.1.2">𝐸</ci><apply id="S2.p1.4.m4.1.1.3.cmml" xref="S2.p1.4.m4.1.1.3"><times id="S2.p1.4.m4.1.1.3.1.cmml" xref="S2.p1.4.m4.1.1.3.1"></times><ci id="S2.p1.4.m4.1.1.3.2.cmml" xref="S2.p1.4.m4.1.1.3.2">𝑠</ci><ci id="S2.p1.4.m4.1.1.3.3.cmml" xref="S2.p1.4.m4.1.1.3.3">𝑦</ci><ci id="S2.p1.4.m4.1.1.3.4.cmml" xref="S2.p1.4.m4.1.1.3.4">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.1c">E_{syn}</annotation></semantics></math>) and real test sets (<math id="S2.p1.5.m5.1" class="ltx_Math" alttext="E_{real}" display="inline"><semantics id="S2.p1.5.m5.1a"><msub id="S2.p1.5.m5.1.1" xref="S2.p1.5.m5.1.1.cmml"><mi id="S2.p1.5.m5.1.1.2" xref="S2.p1.5.m5.1.1.2.cmml">E</mi><mrow id="S2.p1.5.m5.1.1.3" xref="S2.p1.5.m5.1.1.3.cmml"><mi id="S2.p1.5.m5.1.1.3.2" xref="S2.p1.5.m5.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.p1.5.m5.1.1.3.1" xref="S2.p1.5.m5.1.1.3.1.cmml">​</mo><mi id="S2.p1.5.m5.1.1.3.3" xref="S2.p1.5.m5.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.p1.5.m5.1.1.3.1a" xref="S2.p1.5.m5.1.1.3.1.cmml">​</mo><mi id="S2.p1.5.m5.1.1.3.4" xref="S2.p1.5.m5.1.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.p1.5.m5.1.1.3.1b" xref="S2.p1.5.m5.1.1.3.1.cmml">​</mo><mi id="S2.p1.5.m5.1.1.3.5" xref="S2.p1.5.m5.1.1.3.5.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.p1.5.m5.1b"><apply id="S2.p1.5.m5.1.1.cmml" xref="S2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.p1.5.m5.1.1.1.cmml" xref="S2.p1.5.m5.1.1">subscript</csymbol><ci id="S2.p1.5.m5.1.1.2.cmml" xref="S2.p1.5.m5.1.1.2">𝐸</ci><apply id="S2.p1.5.m5.1.1.3.cmml" xref="S2.p1.5.m5.1.1.3"><times id="S2.p1.5.m5.1.1.3.1.cmml" xref="S2.p1.5.m5.1.1.3.1"></times><ci id="S2.p1.5.m5.1.1.3.2.cmml" xref="S2.p1.5.m5.1.1.3.2">𝑟</ci><ci id="S2.p1.5.m5.1.1.3.3.cmml" xref="S2.p1.5.m5.1.1.3.3">𝑒</ci><ci id="S2.p1.5.m5.1.1.3.4.cmml" xref="S2.p1.5.m5.1.1.3.4">𝑎</ci><ci id="S2.p1.5.m5.1.1.3.5.cmml" xref="S2.p1.5.m5.1.1.3.5">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.5.m5.1c">E_{real}</annotation></semantics></math>).</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Generalization metric</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Many solutions proposed during the ESA competition do not obtain as good a score on the real images as on synthetic images. Kisantal <span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> explains that it comes from the change in statistical distribution between the synthetic and the real images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. However, they do not explain the huge differences in generalization between the different models. We propose a generalization metric to characterize these differences: <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="G_{factor}" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><msub id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml"><mi id="S2.SS1.p1.1.m1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.2.cmml">G</mi><mrow id="S2.SS1.p1.1.m1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.3.cmml"><mi id="S2.SS1.p1.1.m1.1.1.3.2" xref="S2.SS1.p1.1.m1.1.1.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.1.1.3.1" xref="S2.SS1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p1.1.m1.1.1.3.3" xref="S2.SS1.p1.1.m1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.1.1.3.1a" xref="S2.SS1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p1.1.m1.1.1.3.4" xref="S2.SS1.p1.1.m1.1.1.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.1.1.3.1b" xref="S2.SS1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p1.1.m1.1.1.3.5" xref="S2.SS1.p1.1.m1.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.1.1.3.1c" xref="S2.SS1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p1.1.m1.1.1.3.6" xref="S2.SS1.p1.1.m1.1.1.3.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.1.1.3.1d" xref="S2.SS1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p1.1.m1.1.1.3.7" xref="S2.SS1.p1.1.m1.1.1.3.7.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.2">𝐺</ci><apply id="S2.SS1.p1.1.m1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3"><times id="S2.SS1.p1.1.m1.1.1.3.1.cmml" xref="S2.SS1.p1.1.m1.1.1.3.1"></times><ci id="S2.SS1.p1.1.m1.1.1.3.2.cmml" xref="S2.SS1.p1.1.m1.1.1.3.2">𝑓</ci><ci id="S2.SS1.p1.1.m1.1.1.3.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3.3">𝑎</ci><ci id="S2.SS1.p1.1.m1.1.1.3.4.cmml" xref="S2.SS1.p1.1.m1.1.1.3.4">𝑐</ci><ci id="S2.SS1.p1.1.m1.1.1.3.5.cmml" xref="S2.SS1.p1.1.m1.1.1.3.5">𝑡</ci><ci id="S2.SS1.p1.1.m1.1.1.3.6.cmml" xref="S2.SS1.p1.1.m1.1.1.3.6">𝑜</ci><ci id="S2.SS1.p1.1.m1.1.1.3.7.cmml" xref="S2.SS1.p1.1.m1.1.1.3.7">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">G_{factor}</annotation></semantics></math>. It represents the ratio between the mean ESA score obtained on the real and synthetic test sets:</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.1" class="ltx_Math" alttext="G_{factor}=\frac{E_{real}}{E_{syn}}" display="block"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml"><msub id="S2.E1.m1.1.1.2" xref="S2.E1.m1.1.1.2.cmml"><mi id="S2.E1.m1.1.1.2.2" xref="S2.E1.m1.1.1.2.2.cmml">G</mi><mrow id="S2.E1.m1.1.1.2.3" xref="S2.E1.m1.1.1.2.3.cmml"><mi id="S2.E1.m1.1.1.2.3.2" xref="S2.E1.m1.1.1.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.2.3.1" xref="S2.E1.m1.1.1.2.3.1.cmml">​</mo><mi id="S2.E1.m1.1.1.2.3.3" xref="S2.E1.m1.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.2.3.1a" xref="S2.E1.m1.1.1.2.3.1.cmml">​</mo><mi id="S2.E1.m1.1.1.2.3.4" xref="S2.E1.m1.1.1.2.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.2.3.1b" xref="S2.E1.m1.1.1.2.3.1.cmml">​</mo><mi id="S2.E1.m1.1.1.2.3.5" xref="S2.E1.m1.1.1.2.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.2.3.1c" xref="S2.E1.m1.1.1.2.3.1.cmml">​</mo><mi id="S2.E1.m1.1.1.2.3.6" xref="S2.E1.m1.1.1.2.3.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.2.3.1d" xref="S2.E1.m1.1.1.2.3.1.cmml">​</mo><mi id="S2.E1.m1.1.1.2.3.7" xref="S2.E1.m1.1.1.2.3.7.cmml">r</mi></mrow></msub><mo id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml">=</mo><mfrac id="S2.E1.m1.1.1.3" xref="S2.E1.m1.1.1.3.cmml"><msub id="S2.E1.m1.1.1.3.2" xref="S2.E1.m1.1.1.3.2.cmml"><mi id="S2.E1.m1.1.1.3.2.2" xref="S2.E1.m1.1.1.3.2.2.cmml">E</mi><mrow id="S2.E1.m1.1.1.3.2.3" xref="S2.E1.m1.1.1.3.2.3.cmml"><mi id="S2.E1.m1.1.1.3.2.3.2" xref="S2.E1.m1.1.1.3.2.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.3.2.3.1" xref="S2.E1.m1.1.1.3.2.3.1.cmml">​</mo><mi id="S2.E1.m1.1.1.3.2.3.3" xref="S2.E1.m1.1.1.3.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.3.2.3.1a" xref="S2.E1.m1.1.1.3.2.3.1.cmml">​</mo><mi id="S2.E1.m1.1.1.3.2.3.4" xref="S2.E1.m1.1.1.3.2.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.3.2.3.1b" xref="S2.E1.m1.1.1.3.2.3.1.cmml">​</mo><mi id="S2.E1.m1.1.1.3.2.3.5" xref="S2.E1.m1.1.1.3.2.3.5.cmml">l</mi></mrow></msub><msub id="S2.E1.m1.1.1.3.3" xref="S2.E1.m1.1.1.3.3.cmml"><mi id="S2.E1.m1.1.1.3.3.2" xref="S2.E1.m1.1.1.3.3.2.cmml">E</mi><mrow id="S2.E1.m1.1.1.3.3.3" xref="S2.E1.m1.1.1.3.3.3.cmml"><mi id="S2.E1.m1.1.1.3.3.3.2" xref="S2.E1.m1.1.1.3.3.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.3.3.3.1" xref="S2.E1.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S2.E1.m1.1.1.3.3.3.3" xref="S2.E1.m1.1.1.3.3.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.3.3.3.1a" xref="S2.E1.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S2.E1.m1.1.1.3.3.3.4" xref="S2.E1.m1.1.1.3.3.3.4.cmml">n</mi></mrow></msub></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1"><eq id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"></eq><apply id="S2.E1.m1.1.1.2.cmml" xref="S2.E1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.2">subscript</csymbol><ci id="S2.E1.m1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.2.2">𝐺</ci><apply id="S2.E1.m1.1.1.2.3.cmml" xref="S2.E1.m1.1.1.2.3"><times id="S2.E1.m1.1.1.2.3.1.cmml" xref="S2.E1.m1.1.1.2.3.1"></times><ci id="S2.E1.m1.1.1.2.3.2.cmml" xref="S2.E1.m1.1.1.2.3.2">𝑓</ci><ci id="S2.E1.m1.1.1.2.3.3.cmml" xref="S2.E1.m1.1.1.2.3.3">𝑎</ci><ci id="S2.E1.m1.1.1.2.3.4.cmml" xref="S2.E1.m1.1.1.2.3.4">𝑐</ci><ci id="S2.E1.m1.1.1.2.3.5.cmml" xref="S2.E1.m1.1.1.2.3.5">𝑡</ci><ci id="S2.E1.m1.1.1.2.3.6.cmml" xref="S2.E1.m1.1.1.2.3.6">𝑜</ci><ci id="S2.E1.m1.1.1.2.3.7.cmml" xref="S2.E1.m1.1.1.2.3.7">𝑟</ci></apply></apply><apply id="S2.E1.m1.1.1.3.cmml" xref="S2.E1.m1.1.1.3"><divide id="S2.E1.m1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.3"></divide><apply id="S2.E1.m1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.2.1.cmml" xref="S2.E1.m1.1.1.3.2">subscript</csymbol><ci id="S2.E1.m1.1.1.3.2.2.cmml" xref="S2.E1.m1.1.1.3.2.2">𝐸</ci><apply id="S2.E1.m1.1.1.3.2.3.cmml" xref="S2.E1.m1.1.1.3.2.3"><times id="S2.E1.m1.1.1.3.2.3.1.cmml" xref="S2.E1.m1.1.1.3.2.3.1"></times><ci id="S2.E1.m1.1.1.3.2.3.2.cmml" xref="S2.E1.m1.1.1.3.2.3.2">𝑟</ci><ci id="S2.E1.m1.1.1.3.2.3.3.cmml" xref="S2.E1.m1.1.1.3.2.3.3">𝑒</ci><ci id="S2.E1.m1.1.1.3.2.3.4.cmml" xref="S2.E1.m1.1.1.3.2.3.4">𝑎</ci><ci id="S2.E1.m1.1.1.3.2.3.5.cmml" xref="S2.E1.m1.1.1.3.2.3.5">𝑙</ci></apply></apply><apply id="S2.E1.m1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.3.1.cmml" xref="S2.E1.m1.1.1.3.3">subscript</csymbol><ci id="S2.E1.m1.1.1.3.3.2.cmml" xref="S2.E1.m1.1.1.3.3.2">𝐸</ci><apply id="S2.E1.m1.1.1.3.3.3.cmml" xref="S2.E1.m1.1.1.3.3.3"><times id="S2.E1.m1.1.1.3.3.3.1.cmml" xref="S2.E1.m1.1.1.3.3.3.1"></times><ci id="S2.E1.m1.1.1.3.3.3.2.cmml" xref="S2.E1.m1.1.1.3.3.3.2">𝑠</ci><ci id="S2.E1.m1.1.1.3.3.3.3.cmml" xref="S2.E1.m1.1.1.3.3.3.3">𝑦</ci><ci id="S2.E1.m1.1.1.3.3.3.4.cmml" xref="S2.E1.m1.1.1.3.3.3.4">𝑛</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">G_{factor}=\frac{E_{real}}{E_{syn}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Table <a href="#S2.T1" title="TABLE I ‣ II-A Generalization metric ‣ II PROPOSED METHOD ‣ Mobile-URSONet: an Embeddable Neural Network for Onboard Spacecraft Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> presents the results and generalization factors of the first four participants of the ESA competition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> (including the baseline solution of Stanford SLAB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>), and the latest work published on the domain by Black <span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. We observe a wide variability in generalization factors: from 2.7 for the third in the competition (Proença <span id="S2.SS1.p3.1.2" class="ltx_text ltx_font_italic">et al.</span>) to 39.9 for the winners of the competition (Chen <span id="S2.SS1.p3.1.3" class="ltx_text ltx_font_italic">et al.</span>). Beyond solving the SPE task using a single CNN (without PnP methods), which is a path to pose estimation of unknown objects, Proença <span id="S2.SS1.p3.1.4" class="ltx_text ltx_font_italic">et al.</span> solution offers the best generalization factor. As robustness to changes in distribution is a key criterion when integrating such networks into embedded systems, we base our work on that of Proença <span id="S2.SS1.p3.1.5" class="ltx_text ltx_font_italic">et al.</span></p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>ESA score on the test sets and corresponding generalization factor</figcaption>
<table id="S2.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.3.3" class="ltx_tr">
<th id="S2.T1.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Participants</th>
<th id="S2.T1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><math id="S2.T1.1.1.1.m1.1" class="ltx_Math" alttext="E_{synth}" display="inline"><semantics id="S2.T1.1.1.1.m1.1a"><msub id="S2.T1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.m1.1.1.cmml"><mi id="S2.T1.1.1.1.m1.1.1.2" xref="S2.T1.1.1.1.m1.1.1.2.cmml">E</mi><mrow id="S2.T1.1.1.1.m1.1.1.3" xref="S2.T1.1.1.1.m1.1.1.3.cmml"><mi id="S2.T1.1.1.1.m1.1.1.3.2" xref="S2.T1.1.1.1.m1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.T1.1.1.1.m1.1.1.3.1" xref="S2.T1.1.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.T1.1.1.1.m1.1.1.3.3" xref="S2.T1.1.1.1.m1.1.1.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S2.T1.1.1.1.m1.1.1.3.1a" xref="S2.T1.1.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.T1.1.1.1.m1.1.1.3.4" xref="S2.T1.1.1.1.m1.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.T1.1.1.1.m1.1.1.3.1b" xref="S2.T1.1.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.T1.1.1.1.m1.1.1.3.5" xref="S2.T1.1.1.1.m1.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.T1.1.1.1.m1.1.1.3.1c" xref="S2.T1.1.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.T1.1.1.1.m1.1.1.3.6" xref="S2.T1.1.1.1.m1.1.1.3.6.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.m1.1b"><apply id="S2.T1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.1.1.1.m1.1.1.1.cmml" xref="S2.T1.1.1.1.m1.1.1">subscript</csymbol><ci id="S2.T1.1.1.1.m1.1.1.2.cmml" xref="S2.T1.1.1.1.m1.1.1.2">𝐸</ci><apply id="S2.T1.1.1.1.m1.1.1.3.cmml" xref="S2.T1.1.1.1.m1.1.1.3"><times id="S2.T1.1.1.1.m1.1.1.3.1.cmml" xref="S2.T1.1.1.1.m1.1.1.3.1"></times><ci id="S2.T1.1.1.1.m1.1.1.3.2.cmml" xref="S2.T1.1.1.1.m1.1.1.3.2">𝑠</ci><ci id="S2.T1.1.1.1.m1.1.1.3.3.cmml" xref="S2.T1.1.1.1.m1.1.1.3.3">𝑦</ci><ci id="S2.T1.1.1.1.m1.1.1.3.4.cmml" xref="S2.T1.1.1.1.m1.1.1.3.4">𝑛</ci><ci id="S2.T1.1.1.1.m1.1.1.3.5.cmml" xref="S2.T1.1.1.1.m1.1.1.3.5">𝑡</ci><ci id="S2.T1.1.1.1.m1.1.1.3.6.cmml" xref="S2.T1.1.1.1.m1.1.1.3.6">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.m1.1c">E_{synth}</annotation></semantics></math></th>
<th id="S2.T1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><math id="S2.T1.2.2.2.m1.1" class="ltx_Math" alttext="E_{real}" display="inline"><semantics id="S2.T1.2.2.2.m1.1a"><msub id="S2.T1.2.2.2.m1.1.1" xref="S2.T1.2.2.2.m1.1.1.cmml"><mi id="S2.T1.2.2.2.m1.1.1.2" xref="S2.T1.2.2.2.m1.1.1.2.cmml">E</mi><mrow id="S2.T1.2.2.2.m1.1.1.3" xref="S2.T1.2.2.2.m1.1.1.3.cmml"><mi id="S2.T1.2.2.2.m1.1.1.3.2" xref="S2.T1.2.2.2.m1.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.T1.2.2.2.m1.1.1.3.1" xref="S2.T1.2.2.2.m1.1.1.3.1.cmml">​</mo><mi id="S2.T1.2.2.2.m1.1.1.3.3" xref="S2.T1.2.2.2.m1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.T1.2.2.2.m1.1.1.3.1a" xref="S2.T1.2.2.2.m1.1.1.3.1.cmml">​</mo><mi id="S2.T1.2.2.2.m1.1.1.3.4" xref="S2.T1.2.2.2.m1.1.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.T1.2.2.2.m1.1.1.3.1b" xref="S2.T1.2.2.2.m1.1.1.3.1.cmml">​</mo><mi id="S2.T1.2.2.2.m1.1.1.3.5" xref="S2.T1.2.2.2.m1.1.1.3.5.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.m1.1b"><apply id="S2.T1.2.2.2.m1.1.1.cmml" xref="S2.T1.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.2.2.2.m1.1.1.1.cmml" xref="S2.T1.2.2.2.m1.1.1">subscript</csymbol><ci id="S2.T1.2.2.2.m1.1.1.2.cmml" xref="S2.T1.2.2.2.m1.1.1.2">𝐸</ci><apply id="S2.T1.2.2.2.m1.1.1.3.cmml" xref="S2.T1.2.2.2.m1.1.1.3"><times id="S2.T1.2.2.2.m1.1.1.3.1.cmml" xref="S2.T1.2.2.2.m1.1.1.3.1"></times><ci id="S2.T1.2.2.2.m1.1.1.3.2.cmml" xref="S2.T1.2.2.2.m1.1.1.3.2">𝑟</ci><ci id="S2.T1.2.2.2.m1.1.1.3.3.cmml" xref="S2.T1.2.2.2.m1.1.1.3.3">𝑒</ci><ci id="S2.T1.2.2.2.m1.1.1.3.4.cmml" xref="S2.T1.2.2.2.m1.1.1.3.4">𝑎</ci><ci id="S2.T1.2.2.2.m1.1.1.3.5.cmml" xref="S2.T1.2.2.2.m1.1.1.3.5">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.m1.1c">E_{real}</annotation></semantics></math></th>
<th id="S2.T1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><math id="S2.T1.3.3.3.m1.1" class="ltx_Math" alttext="G_{factor}" display="inline"><semantics id="S2.T1.3.3.3.m1.1a"><msub id="S2.T1.3.3.3.m1.1.1" xref="S2.T1.3.3.3.m1.1.1.cmml"><mi id="S2.T1.3.3.3.m1.1.1.2" xref="S2.T1.3.3.3.m1.1.1.2.cmml">G</mi><mrow id="S2.T1.3.3.3.m1.1.1.3" xref="S2.T1.3.3.3.m1.1.1.3.cmml"><mi id="S2.T1.3.3.3.m1.1.1.3.2" xref="S2.T1.3.3.3.m1.1.1.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.T1.3.3.3.m1.1.1.3.1" xref="S2.T1.3.3.3.m1.1.1.3.1.cmml">​</mo><mi id="S2.T1.3.3.3.m1.1.1.3.3" xref="S2.T1.3.3.3.m1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.T1.3.3.3.m1.1.1.3.1a" xref="S2.T1.3.3.3.m1.1.1.3.1.cmml">​</mo><mi id="S2.T1.3.3.3.m1.1.1.3.4" xref="S2.T1.3.3.3.m1.1.1.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.T1.3.3.3.m1.1.1.3.1b" xref="S2.T1.3.3.3.m1.1.1.3.1.cmml">​</mo><mi id="S2.T1.3.3.3.m1.1.1.3.5" xref="S2.T1.3.3.3.m1.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.T1.3.3.3.m1.1.1.3.1c" xref="S2.T1.3.3.3.m1.1.1.3.1.cmml">​</mo><mi id="S2.T1.3.3.3.m1.1.1.3.6" xref="S2.T1.3.3.3.m1.1.1.3.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.T1.3.3.3.m1.1.1.3.1d" xref="S2.T1.3.3.3.m1.1.1.3.1.cmml">​</mo><mi id="S2.T1.3.3.3.m1.1.1.3.7" xref="S2.T1.3.3.3.m1.1.1.3.7.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.3.m1.1b"><apply id="S2.T1.3.3.3.m1.1.1.cmml" xref="S2.T1.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.3.3.3.m1.1.1.1.cmml" xref="S2.T1.3.3.3.m1.1.1">subscript</csymbol><ci id="S2.T1.3.3.3.m1.1.1.2.cmml" xref="S2.T1.3.3.3.m1.1.1.2">𝐺</ci><apply id="S2.T1.3.3.3.m1.1.1.3.cmml" xref="S2.T1.3.3.3.m1.1.1.3"><times id="S2.T1.3.3.3.m1.1.1.3.1.cmml" xref="S2.T1.3.3.3.m1.1.1.3.1"></times><ci id="S2.T1.3.3.3.m1.1.1.3.2.cmml" xref="S2.T1.3.3.3.m1.1.1.3.2">𝑓</ci><ci id="S2.T1.3.3.3.m1.1.1.3.3.cmml" xref="S2.T1.3.3.3.m1.1.1.3.3">𝑎</ci><ci id="S2.T1.3.3.3.m1.1.1.3.4.cmml" xref="S2.T1.3.3.3.m1.1.1.3.4">𝑐</ci><ci id="S2.T1.3.3.3.m1.1.1.3.5.cmml" xref="S2.T1.3.3.3.m1.1.1.3.5">𝑡</ci><ci id="S2.T1.3.3.3.m1.1.1.3.6.cmml" xref="S2.T1.3.3.3.m1.1.1.3.6">𝑜</ci><ci id="S2.T1.3.3.3.m1.1.1.3.7.cmml" xref="S2.T1.3.3.3.m1.1.1.3.7">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.3.m1.1c">G_{factor}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.3.4.1" class="ltx_tr">
<th id="S2.T1.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Chen <span id="S2.T1.3.4.1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</th>
<td id="S2.T1.3.4.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0094</td>
<td id="S2.T1.3.4.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.3752</td>
<td id="S2.T1.3.4.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">39.9</td>
</tr>
<tr id="S2.T1.3.5.2" class="ltx_tr">
<th id="S2.T1.3.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">EPFL_cvlab</th>
<td id="S2.T1.3.5.2.2" class="ltx_td ltx_align_center ltx_border_r">0.0215</td>
<td id="S2.T1.3.5.2.3" class="ltx_td ltx_align_center ltx_border_r">0.1140</td>
<td id="S2.T1.3.5.2.4" class="ltx_td ltx_align_center ltx_border_r">5.3</td>
</tr>
<tr id="S2.T1.3.6.3" class="ltx_tr">
<th id="S2.T1.3.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Black <span id="S2.T1.3.6.3.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</th>
<td id="S2.T1.3.6.3.2" class="ltx_td ltx_align_center ltx_border_r">0.0409</td>
<td id="S2.T1.3.6.3.3" class="ltx_td ltx_align_center ltx_border_r">0.2918</td>
<td id="S2.T1.3.6.3.4" class="ltx_td ltx_align_center ltx_border_r">7.13</td>
</tr>
<tr id="S2.T1.3.7.4" class="ltx_tr">
<th id="S2.T1.3.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Proença <span id="S2.T1.3.7.4.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</th>
<td id="S2.T1.3.7.4.2" class="ltx_td ltx_align_center ltx_border_r">0.0571</td>
<td id="S2.T1.3.7.4.3" class="ltx_td ltx_align_center ltx_border_r">0.1555</td>
<td id="S2.T1.3.7.4.4" class="ltx_td ltx_align_center ltx_border_r">2.7</td>
</tr>
<tr id="S2.T1.3.8.5" class="ltx_tr">
<th id="S2.T1.3.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">Sharma <span id="S2.T1.3.8.5.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</th>
<td id="S2.T1.3.8.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.0626</td>
<td id="S2.T1.3.8.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.3951</td>
<td id="S2.T1.3.8.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">6.3</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Neural network architecture</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">To ensure a good ranking in the ESA competition, Proença <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">et. al.</span> focused on minimizing the ESA score regardless of the complexity of their model. We focused on the trade-off between the ESA score, the number of parameters, and the computational complexity of our model. Figure <a href="#S2.F1" title="Figure 1 ‣ II-B Neural network architecture ‣ II PROPOSED METHOD ‣ Mobile-URSONet: an Embeddable Neural Network for Onboard Spacecraft Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows an outline of Mobile-URSONet, the neural network we propose in this work. In the following paragraphs, we will explain in detail our architectural choices.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2205.02065/assets/fig/Mobile-URSONet.png" id="S2.F1.g1" class="ltx_graphics ltx_img_landscape" width="253" height="121" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Mobile-URSONet architecture</figcaption>
</figure>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">The first component of a CNN is the backbone that extracts features from the input image (Fig. <a href="#S2.F1" title="Figure 1 ‣ II-B Neural network architecture ‣ II PROPOSED METHOD ‣ Mobile-URSONet: an Embeddable Neural Network for Onboard Spacecraft Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Proença <span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> used a ResNet-101 backbone, a CNN that achieves 77% top-1 accuracy on the standard ImageNet benchmark. We use a MobileNet-v2 backbone that achieves only 72% top-1 accuracy on ImageNet but has 13 times fewer parameters. Bianco <span id="S2.SS2.p2.1.2" class="ltx_text ltx_font_italic">et. al.</span> show that MobileNet-V2 has a top-1 accuracy density (<span id="S2.SS2.p2.1.3" class="ltx_text ltx_font_italic">i.e.</span> accuracy per million parameters) more than 10 times better than the ResNet-101 used by Proença <span id="S2.SS2.p2.1.4" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. They also show that MobileNet-v2 inference requires only 0.3 GFLOPs (floating-point operations), while a ResNet-101 inference requires almost 8 GFLOPs. The computational complexity of our backbone is 26 times less than the original URSONet.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Moreover, MobileNet-v2 is designed to run in real-time on smartphone ARM processors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Table <a href="#S2.T2" title="TABLE II ‣ II-B Neural network architecture ‣ II PROPOSED METHOD ‣ Mobile-URSONet: an Embeddable Neural Network for Onboard Spacecraft Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> shows that it also run on weaker ARM processors now used in the space domain. For instance, the Eye-Sat nanosatellite embeds a Zynq 7030 MPSoC (Multiprocessor System on a Chip) developed by Xilinx, which has two ARM A-9 cores <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. More recent projects consider using MPSoC featuring four ARM A-53 cores <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
<figure id="S2.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Inference latency of MobileNet-v2 on various ARM processors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite></figcaption>
<table id="S2.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T2.1.1.1" class="ltx_tr">
<th id="S2.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Processor (compiler)</th>
<th id="S2.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Latency</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T2.1.2.1" class="ltx_tr">
<th id="S2.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Qualcomm ARM A-72 (TF-Lite)</th>
<td id="S2.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">75 ms</td>
</tr>
<tr id="S2.T2.1.3.2" class="ltx_tr">
<th id="S2.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Xilinx ARM A-53 (TVM)</th>
<td id="S2.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">132 ms</td>
</tr>
<tr id="S2.T2.1.4.3" class="ltx_tr">
<th id="S2.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">Xilinx ARM A-9 (TVM)</th>
<td id="S2.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">265 ms</td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">The second part of the neural network is the bottleneck (Fig. <a href="#S2.F1" title="Figure 1 ‣ II-B Neural network architecture ‣ II PROPOSED METHOD ‣ Mobile-URSONet: an Embeddable Neural Network for Onboard Spacecraft Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Many standard CNN architectures uses average pooling such as MobileNets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and ResNets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. It aims to merge semantically similar features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> by reducing the spatial dimension of the feature maps. Thus, it reduces the number of parameters and computational complexity of the subsequent layers. Proença <span id="S2.SS2.p4.1.1" class="ltx_text ltx_font_italic">et al.</span> replaced the standard average pooling layer by a 3x3 convolution layer with a stride of 2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. They show that increasing the number of feature maps in the bottleneck layer decrease the position and orientation error. We summarize their results in table <a href="#S2.T3" title="TABLE III ‣ II-B Neural network architecture ‣ II PROPOSED METHOD ‣ Mobile-URSONet: an Embeddable Neural Network for Onboard Spacecraft Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. The first line of the table (<span id="S2.SS2.p4.1.2" class="ltx_text ltx_font_italic">i.e.</span> the configuration with eight feature maps) has the same amount of parameters as an average pooling configuration. The table shows that multiplying the number of parameters by six only leads to a 3 degrees improvement in orientation error and 0.24 meters improvement in position error. In our opinion, it is not a suitable trade-off for an embedded system. That is why we kept the original average pooling layer.</p>
</div>
<figure id="S2.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Bottleneck size <span id="S2.T3.2.1" class="ltx_text ltx_font_italic">vs.</span> number of parameters, orientation and position error on URSONet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite></figcaption>
<table id="S2.T3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T3.3.1.1" class="ltx_tr">
<th id="S2.T3.3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"># feature maps</th>
<th id="S2.T3.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"># params (M)</th>
<th id="S2.T3.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Ori err. (°)</th>
<th id="S2.T3.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Pos err. (m)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T3.3.2.1" class="ltx_tr">
<th id="S2.T3.3.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">8</th>
<th id="S2.T3.3.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">40</th>
<td id="S2.T3.3.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.2</td>
<td id="S2.T3.3.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.72</td>
</tr>
<tr id="S2.T3.3.3.2" class="ltx_tr">
<th id="S2.T3.3.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">128</th>
<th id="S2.T3.3.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">80</th>
<td id="S2.T3.3.3.2.3" class="ltx_td ltx_align_center ltx_border_r">7.8</td>
<td id="S2.T3.3.3.2.4" class="ltx_td ltx_align_center ltx_border_r">0.54</td>
</tr>
<tr id="S2.T3.3.4.3" class="ltx_tr">
<th id="S2.T3.3.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">512</th>
<th id="S2.T3.3.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">240</th>
<td id="S2.T3.3.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">7.2</td>
<td id="S2.T3.3.4.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.48</td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">Moreover, the number of parameters of the neural network is agnostic to input image resolution thanks to the average pooling layer. Proença <span id="S2.SS2.p5.1.1" class="ltx_text ltx_font_italic">et al.</span> show that orientation estimation is sensitive to image resolution (<span id="S2.SS2.p5.1.2" class="ltx_text ltx_font_italic">i.e.</span> increasing image resolution improves accuracy). Thus, increasing image resolution is a more efficient solution to improve accuracy than removing the bottleneck as the power consumption highly depends on memory accesses <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.</p>
</div>
<div id="S2.SS2.p6" class="ltx_para">
<p id="S2.SS2.p6.1" class="ltx_p">The last part of the neural network is the two branches (Fig. <a href="#S2.F1" title="Figure 1 ‣ II-B Neural network architecture ‣ II PROPOSED METHOD ‣ Mobile-URSONet: an Embeddable Neural Network for Onboard Spacecraft Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>): the first part estimates orientation, while the other estimates position. Proença <span id="S2.SS2.p6.1.1" class="ltx_text ltx_font_italic">et al.</span> use two fully connected layers. We use a single fully connected layer to minimize the number of parameters. Our position branch has only 3843 parameters. The number of parameters of our orientation branch depends on the configuration: in regression, it has 5124 parameters; in soft classification, it has between 0.6 and 5 million parameters. It is 39 to 325 times fewer parameters than Proença <span id="S2.SS2.p6.1.2" class="ltx_text ltx_font_italic">et al.</span>, which has approximately 195 million parameters in their branches.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.5.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.6.2" class="ltx_text ltx_font_italic">Loss functions</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Target’s position estimation is an easy task solved using direct regression. It allows using ESA metrics as loss functions, as proposed by Proença <span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_italic">et al.</span>. The same cannot apply to orientation estimation, which explains why we try two methods (<span id="S2.SS3.p1.1.2" class="ltx_text ltx_font_italic">i.e.</span> regression and soft classification). In ESA metrics, the position error depends on the distance from the target spacecraft; the neural network is increasingly penalized as the target spacecraft is closer. However, this is not the case with the orientation in ESA metrics. We propose a variant of the loss function when orientation estimation is considered a regression task:</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.2" class="ltx_Math" alttext="L_{ori}=\frac{\arccos{|\textbf{q}\cdot\hat{\textbf{q}}|}}{||\textbf{t}||_{2}}" display="block"><semantics id="S2.E2.m1.2a"><mrow id="S2.E2.m1.2.3" xref="S2.E2.m1.2.3.cmml"><msub id="S2.E2.m1.2.3.2" xref="S2.E2.m1.2.3.2.cmml"><mi id="S2.E2.m1.2.3.2.2" xref="S2.E2.m1.2.3.2.2.cmml">L</mi><mrow id="S2.E2.m1.2.3.2.3" xref="S2.E2.m1.2.3.2.3.cmml"><mi id="S2.E2.m1.2.3.2.3.2" xref="S2.E2.m1.2.3.2.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.3.2.3.1" xref="S2.E2.m1.2.3.2.3.1.cmml">​</mo><mi id="S2.E2.m1.2.3.2.3.3" xref="S2.E2.m1.2.3.2.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.3.2.3.1a" xref="S2.E2.m1.2.3.2.3.1.cmml">​</mo><mi id="S2.E2.m1.2.3.2.3.4" xref="S2.E2.m1.2.3.2.3.4.cmml">i</mi></mrow></msub><mo id="S2.E2.m1.2.3.1" xref="S2.E2.m1.2.3.1.cmml">=</mo><mfrac id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml"><mrow id="S2.E2.m1.1.1.1" xref="S2.E2.m1.1.1.1.cmml"><mi id="S2.E2.m1.1.1.1.2" xref="S2.E2.m1.1.1.1.2.cmml">arccos</mi><mo id="S2.E2.m1.1.1.1a" xref="S2.E2.m1.1.1.1.cmml">⁡</mo><mrow id="S2.E2.m1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E2.m1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.2.1.cmml">|</mo><mrow id="S2.E2.m1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S2.E2.m1.1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.1.1.2a.cmml">q</mtext><mo lspace="0.222em" rspace="0.222em" id="S2.E2.m1.1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.1.cmml">⋅</mo><mover accent="true" id="S2.E2.m1.1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.1.1.3.cmml"><mtext class="ltx_mathvariant_bold" id="S2.E2.m1.1.1.1.1.1.1.3.2" xref="S2.E2.m1.1.1.1.1.1.1.3.2a.cmml">q</mtext><mo id="S2.E2.m1.1.1.1.1.1.1.3.1" xref="S2.E2.m1.1.1.1.1.1.1.3.1.cmml">^</mo></mover></mrow><mo stretchy="false" id="S2.E2.m1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.2.1.cmml">|</mo></mrow></mrow><msub id="S2.E2.m1.2.2.2" xref="S2.E2.m1.2.2.2.cmml"><mrow id="S2.E2.m1.2.2.2.3.2" xref="S2.E2.m1.2.2.2.3.1.cmml"><mo stretchy="false" id="S2.E2.m1.2.2.2.3.2.1" xref="S2.E2.m1.2.2.2.3.1.1.cmml">‖</mo><mtext class="ltx_mathvariant_bold" id="S2.E2.m1.2.2.2.1" xref="S2.E2.m1.2.2.2.1a.cmml">t</mtext><mo stretchy="false" id="S2.E2.m1.2.2.2.3.2.2" xref="S2.E2.m1.2.2.2.3.1.1.cmml">‖</mo></mrow><mn id="S2.E2.m1.2.2.2.4" xref="S2.E2.m1.2.2.2.4.cmml">2</mn></msub></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.2b"><apply id="S2.E2.m1.2.3.cmml" xref="S2.E2.m1.2.3"><eq id="S2.E2.m1.2.3.1.cmml" xref="S2.E2.m1.2.3.1"></eq><apply id="S2.E2.m1.2.3.2.cmml" xref="S2.E2.m1.2.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.2.3.2.1.cmml" xref="S2.E2.m1.2.3.2">subscript</csymbol><ci id="S2.E2.m1.2.3.2.2.cmml" xref="S2.E2.m1.2.3.2.2">𝐿</ci><apply id="S2.E2.m1.2.3.2.3.cmml" xref="S2.E2.m1.2.3.2.3"><times id="S2.E2.m1.2.3.2.3.1.cmml" xref="S2.E2.m1.2.3.2.3.1"></times><ci id="S2.E2.m1.2.3.2.3.2.cmml" xref="S2.E2.m1.2.3.2.3.2">𝑜</ci><ci id="S2.E2.m1.2.3.2.3.3.cmml" xref="S2.E2.m1.2.3.2.3.3">𝑟</ci><ci id="S2.E2.m1.2.3.2.3.4.cmml" xref="S2.E2.m1.2.3.2.3.4">𝑖</ci></apply></apply><apply id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2"><divide id="S2.E2.m1.2.2.3.cmml" xref="S2.E2.m1.2.2"></divide><apply id="S2.E2.m1.1.1.1.cmml" xref="S2.E2.m1.1.1.1"><arccos id="S2.E2.m1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.2"></arccos><apply id="S2.E2.m1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.1"><abs id="S2.E2.m1.1.1.1.1.2.1.cmml" xref="S2.E2.m1.1.1.1.1.1.2"></abs><apply id="S2.E2.m1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1"><ci id="S2.E2.m1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1">⋅</ci><ci id="S2.E2.m1.1.1.1.1.1.1.2a.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S2.E2.m1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2">q</mtext></ci><apply id="S2.E2.m1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3"><ci id="S2.E2.m1.1.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3.1">^</ci><ci id="S2.E2.m1.1.1.1.1.1.1.3.2a.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3.2"><mtext class="ltx_mathvariant_bold" id="S2.E2.m1.1.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3.2">q</mtext></ci></apply></apply></apply></apply><apply id="S2.E2.m1.2.2.2.cmml" xref="S2.E2.m1.2.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.2.2.cmml" xref="S2.E2.m1.2.2.2">subscript</csymbol><apply id="S2.E2.m1.2.2.2.3.1.cmml" xref="S2.E2.m1.2.2.2.3.2"><csymbol cd="latexml" id="S2.E2.m1.2.2.2.3.1.1.cmml" xref="S2.E2.m1.2.2.2.3.2.1">norm</csymbol><ci id="S2.E2.m1.2.2.2.1a.cmml" xref="S2.E2.m1.2.2.2.1"><mtext class="ltx_mathvariant_bold" id="S2.E2.m1.2.2.2.1.cmml" xref="S2.E2.m1.2.2.2.1">t</mtext></ci></apply><cn type="integer" id="S2.E2.m1.2.2.2.4.cmml" xref="S2.E2.m1.2.2.2.4">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.2c">L_{ori}=\frac{\arccos{|\textbf{q}\cdot\hat{\textbf{q}}|}}{||\textbf{t}||_{2}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">In our experiments, it does not lead to significant improvements compared to the regression loss function proposed by Proença <span id="S2.SS3.p3.1.1" class="ltx_text ltx_font_italic">et. al</span>. Thus, we also estimate orientation using soft classification as Proença <span id="S2.SS3.p3.1.2" class="ltx_text ltx_font_italic">et. al.</span> does. The associated loss function is a standard negative log-likelihood <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">EXPERIMENTS</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Implementation and training details</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Networks are trained on one Nvidia Tesla P100, using Pytorch on the SPEED dataset. We use the MobileNet-v2 backbone pre-trained on ImageNet to speed up training. We reserve 15% of the training SPEED dataset as a validation set. Parameters are updated using the SGD algorithm with a <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">momentum</span> of 0.9. We use a batch size of 32 images resized to 384 * 240 pixels. The learning rate starts at 0.01 for the first 30 epochs. Then it is decayed to 0.001 for the following 15 epochs. It finishes at 0.0001 for the last five epochs. We employ data augmentation on the training set using OpenCV to rotate the camera across the roll axis for half images with a maximum magnitude of 25°. We also use Pytorch transformations to add a Gaussian blur and randomly change the brightness, contrast, saturation, and hue of training images. When using soft classification, we have set <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi mathvariant="normal" id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\Delta</annotation></semantics></math> which controls the Gaussian width at 3 to act as a regularizer. The number of bins per dimension varies between 8 and 32. All hyper-parameters are tuned on the validation set. Our code is available at <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Results</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Orientation estimation through regression is done with very few parameters but leads to an average error of 32° on the validation set. Orientation estimation through soft classification implies many more parameters in the orientation branch: it depends on the cube of the number of bins per dimension (we encode rotations as three Euler angles and then convert it to quaternions). However, using soft classification improves the orientation error by a factor of three to five. Table <a href="#S3.T4" title="TABLE IV ‣ III-B Results ‣ III EXPERIMENTS ‣ Mobile-URSONet: an Embeddable Neural Network for Onboard Spacecraft Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> shows the results we obtain, with 8 to 32 bins per dimension. The 16-bins model has 2.6 times more parameters than the 8-bins model and an orientation error divided by 2. However, the 32-bins model has six times more parameters than the 16-bins model, with no significant improvement on the orientation error. The 24-bins model also does not bring improvements. It demonstrates a saturation effect on orientation error when increasing the number of bins per dimension. In addition, we notice that the 32-bins model is much more prone to overfitting. Based on these results, we believe that going beyond 16 bins per dimension is not worth it as we focus on the trade-off between the orientation error and the number of parameters of the CNN.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Effect of the number of bins per dimension on orientation error and the number of parameters</figcaption>
<table id="S3.T4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T4.2.2" class="ltx_tr">
<th id="S3.T4.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"># bins per dim.</th>
<th id="S3.T4.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"># params (M)</th>
<th id="S3.T4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<math id="S3.T4.1.1.1.m1.1" class="ltx_Math" alttext="e_{q}" display="inline"><semantics id="S3.T4.1.1.1.m1.1a"><msub id="S3.T4.1.1.1.m1.1.1" xref="S3.T4.1.1.1.m1.1.1.cmml"><mi id="S3.T4.1.1.1.m1.1.1.2" xref="S3.T4.1.1.1.m1.1.1.2.cmml">e</mi><mi id="S3.T4.1.1.1.m1.1.1.3" xref="S3.T4.1.1.1.m1.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.T4.1.1.1.m1.1b"><apply id="S3.T4.1.1.1.m1.1.1.cmml" xref="S3.T4.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T4.1.1.1.m1.1.1.1.cmml" xref="S3.T4.1.1.1.m1.1.1">subscript</csymbol><ci id="S3.T4.1.1.1.m1.1.1.2.cmml" xref="S3.T4.1.1.1.m1.1.1.2">𝑒</ci><ci id="S3.T4.1.1.1.m1.1.1.3.cmml" xref="S3.T4.1.1.1.m1.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.1.1.1.m1.1c">e_{q}</annotation></semantics></math> train (°)</th>
<th id="S3.T4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<math id="S3.T4.2.2.2.m1.1" class="ltx_Math" alttext="e_{q}" display="inline"><semantics id="S3.T4.2.2.2.m1.1a"><msub id="S3.T4.2.2.2.m1.1.1" xref="S3.T4.2.2.2.m1.1.1.cmml"><mi id="S3.T4.2.2.2.m1.1.1.2" xref="S3.T4.2.2.2.m1.1.1.2.cmml">e</mi><mi id="S3.T4.2.2.2.m1.1.1.3" xref="S3.T4.2.2.2.m1.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.T4.2.2.2.m1.1b"><apply id="S3.T4.2.2.2.m1.1.1.cmml" xref="S3.T4.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S3.T4.2.2.2.m1.1.1.1.cmml" xref="S3.T4.2.2.2.m1.1.1">subscript</csymbol><ci id="S3.T4.2.2.2.m1.1.1.2.cmml" xref="S3.T4.2.2.2.m1.1.1.2">𝑒</ci><ci id="S3.T4.2.2.2.m1.1.1.3.cmml" xref="S3.T4.2.2.2.m1.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.2.2.2.m1.1c">e_{q}</annotation></semantics></math> valid (°)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T4.2.3.1" class="ltx_tr">
<th id="S3.T4.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">8</th>
<td id="S3.T4.2.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.8</td>
<td id="S3.T4.2.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9.74</td>
<td id="S3.T4.2.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.3</td>
</tr>
<tr id="S3.T4.2.4.2" class="ltx_tr">
<th id="S3.T4.2.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">12</th>
<td id="S3.T4.2.4.2.2" class="ltx_td ltx_align_center ltx_border_r">4.4</td>
<td id="S3.T4.2.4.2.3" class="ltx_td ltx_align_center ltx_border_r">5.69</td>
<td id="S3.T4.2.4.2.4" class="ltx_td ltx_align_center ltx_border_r">7.43</td>
</tr>
<tr id="S3.T4.2.5.3" class="ltx_tr">
<th id="S3.T4.2.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">16</th>
<td id="S3.T4.2.5.3.2" class="ltx_td ltx_align_center ltx_border_r">7.4</td>
<td id="S3.T4.2.5.3.3" class="ltx_td ltx_align_center ltx_border_r">4.5</td>
<td id="S3.T4.2.5.3.4" class="ltx_td ltx_align_center ltx_border_r">6.29</td>
</tr>
<tr id="S3.T4.2.6.4" class="ltx_tr">
<th id="S3.T4.2.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">24</th>
<td id="S3.T4.2.6.4.2" class="ltx_td ltx_align_center ltx_border_r">19.9</td>
<td id="S3.T4.2.6.4.3" class="ltx_td ltx_align_center ltx_border_r">4.18</td>
<td id="S3.T4.2.6.4.4" class="ltx_td ltx_align_center ltx_border_r">6.12</td>
</tr>
<tr id="S3.T4.2.7.5" class="ltx_tr">
<th id="S3.T4.2.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">32</th>
<td id="S3.T4.2.7.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">44.2</td>
<td id="S3.T4.2.7.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">4.92</td>
<td id="S3.T4.2.7.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">7.29</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Table <a href="#S3.T5" title="TABLE V ‣ III-B Results ‣ III EXPERIMENTS ‣ Mobile-URSONet: an Embeddable Neural Network for Onboard Spacecraft Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> shows the orientation and position error of our selected models (from 8 to 16 bins) compared to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. As we said before, target position estimation is an easy task solved using direct regression. Position error is the same in all our experiments. It seems to depend only on the number of parameters of the backbone, as increasing the number of parameters in the position branch only increases overfitting. Our backbone uses 13 times fewer parameters than Proença’s backbone while degrading position error by no more than three times. Using soft classification, orientation error highly depends on the number of parameters of the orientation branch. Proença <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> used between 24 and 64 bins per dimension but only published the orientation error for their 24-bins model. The high number of parameters of URSONet causes overfitting that Proença <span id="S3.SS2.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> mitigates using a data augmentation strategy more refined than ours. It explains why they achieve a better orientation error of 4.0°, while our best model achieves only 6.3° orientation error.</p>
</div>
<figure id="S3.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Orientation and position error on validation set compared to Proença <span id="S3.T5.4.1" class="ltx_text ltx_font_italic">et al.</span></figcaption>
<table id="S3.T5.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T5.2.2" class="ltx_tr">
<th id="S3.T5.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"># bins per dim.</th>
<th id="S3.T5.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<math id="S3.T5.1.1.1.m1.1" class="ltx_Math" alttext="e_{q}" display="inline"><semantics id="S3.T5.1.1.1.m1.1a"><msub id="S3.T5.1.1.1.m1.1.1" xref="S3.T5.1.1.1.m1.1.1.cmml"><mi id="S3.T5.1.1.1.m1.1.1.2" xref="S3.T5.1.1.1.m1.1.1.2.cmml">e</mi><mi id="S3.T5.1.1.1.m1.1.1.3" xref="S3.T5.1.1.1.m1.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.T5.1.1.1.m1.1b"><apply id="S3.T5.1.1.1.m1.1.1.cmml" xref="S3.T5.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T5.1.1.1.m1.1.1.1.cmml" xref="S3.T5.1.1.1.m1.1.1">subscript</csymbol><ci id="S3.T5.1.1.1.m1.1.1.2.cmml" xref="S3.T5.1.1.1.m1.1.1.2">𝑒</ci><ci id="S3.T5.1.1.1.m1.1.1.3.cmml" xref="S3.T5.1.1.1.m1.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.1.1.1.m1.1c">e_{q}</annotation></semantics></math> (°)</th>
<th id="S3.T5.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<math id="S3.T5.2.2.2.m1.1" class="ltx_Math" alttext="e_{t}" display="inline"><semantics id="S3.T5.2.2.2.m1.1a"><msub id="S3.T5.2.2.2.m1.1.1" xref="S3.T5.2.2.2.m1.1.1.cmml"><mi id="S3.T5.2.2.2.m1.1.1.2" xref="S3.T5.2.2.2.m1.1.1.2.cmml">e</mi><mi id="S3.T5.2.2.2.m1.1.1.3" xref="S3.T5.2.2.2.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.T5.2.2.2.m1.1b"><apply id="S3.T5.2.2.2.m1.1.1.cmml" xref="S3.T5.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S3.T5.2.2.2.m1.1.1.1.cmml" xref="S3.T5.2.2.2.m1.1.1">subscript</csymbol><ci id="S3.T5.2.2.2.m1.1.1.2.cmml" xref="S3.T5.2.2.2.m1.1.1.2">𝑒</ci><ci id="S3.T5.2.2.2.m1.1.1.3.cmml" xref="S3.T5.2.2.2.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.2.2.2.m1.1c">e_{t}</annotation></semantics></math> (m)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T5.2.3.1" class="ltx_tr">
<th id="S3.T5.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Ours (8 bins)</th>
<td id="S3.T5.2.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.3</td>
<td id="S3.T5.2.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.54</td>
</tr>
<tr id="S3.T5.2.4.2" class="ltx_tr">
<th id="S3.T5.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Ours (12 bins)</th>
<td id="S3.T5.2.4.2.2" class="ltx_td ltx_align_center ltx_border_r">7.43</td>
<td id="S3.T5.2.4.2.3" class="ltx_td ltx_align_center ltx_border_r">0.51</td>
</tr>
<tr id="S3.T5.2.5.3" class="ltx_tr">
<th id="S3.T5.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Ours (16 bins)</th>
<td id="S3.T5.2.5.3.2" class="ltx_td ltx_align_center ltx_border_r">6.29</td>
<td id="S3.T5.2.5.3.3" class="ltx_td ltx_align_center ltx_border_r">0.56</td>
</tr>
<tr id="S3.T5.2.6.4" class="ltx_tr">
<th id="S3.T5.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">Proença <span id="S3.T5.2.6.4.1.1" class="ltx_text ltx_font_italic">et al.</span> (24 bins) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</th>
<td id="S3.T5.2.6.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">4.0</td>
<td id="S3.T5.2.6.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.17</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Figure <a href="#S3.F2" title="Figure 2 ‣ III-B Results ‣ III EXPERIMENTS ‣ Mobile-URSONet: an Embeddable Neural Network for Onboard Spacecraft Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the position and the orientation error of our 12-bins model on the validation set as a function of distance to the target. We see that the position error highly depends on the distance to the target satellite. We observe the same property for the orientation error. It is surprising as the loss function we use does not involve the distance with the target spacecraft (in soft classification configuration). High position and orientation errors appear when the target spacecraft is more than 10 meters apart from the camera. The number of outliers is small enough and occurs only when the target spacecraft is more than 15 meters apart from the camera. The closer the target spacecraft is, the more confidence we can have in the predictions of our model. It is a crucial property for autonomous docking or debris removal applications. It also offers an avenue to reduce both the position and the orientation error: zooming and cropping the image around the target instead of resizing the whole image as we do now.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2205.02065/assets/fig/error_by_distance.png" id="S3.F2.g1" class="ltx_graphics ltx_img_landscape" width="242" height="177" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Position and orientation error by distance for our 12-bins model</figcaption>
</figure>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">Table <a href="#S3.T6" title="TABLE VI ‣ III-B Results ‣ III EXPERIMENTS ‣ Mobile-URSONet: an Embeddable Neural Network for Onboard Spacecraft Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> summarizes the results of our models compared to our competitors. We propose the most lightweight spacecraft pose estimation models: ranging from 2.2 to 7.4 million parameters while keeping a good score on the synthetic test set and a good generalization factor. Our 8-bins model has 178 times fewer parameters while degrading the ESA score by no more than four times compared to URSONet. It has an accuracy density (<span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_italic">i.e.</span> ESA score per parameters) 139 times higher than the original URSONet. However, we notice that increasing the number of bins per dimension leads the model to overfit the synthetic images. Proença <span id="S3.SS2.p4.1.2" class="ltx_text ltx_font_italic">et al.</span> demonstrated that a 2.7 generalization factor is achievable while using 24 to 64 bins per orientation dimension. We believe we still have some margin to improve the generalization capabilities of our models by using a more advanced data augmentation technique and by investing more effort in hyper-parameter tuning.</p>
</div>
<figure id="S3.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VI: </span>ESA score on test set and generalization factor</figcaption>
<table id="S3.T6.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T6.3.3" class="ltx_tr">
<th id="S3.T6.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Participants</th>
<th id="S3.T6.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"># params (M)</th>
<th id="S3.T6.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><math id="S3.T6.1.1.1.m1.1" class="ltx_Math" alttext="E_{synth}" display="inline"><semantics id="S3.T6.1.1.1.m1.1a"><msub id="S3.T6.1.1.1.m1.1.1" xref="S3.T6.1.1.1.m1.1.1.cmml"><mi id="S3.T6.1.1.1.m1.1.1.2" xref="S3.T6.1.1.1.m1.1.1.2.cmml">E</mi><mrow id="S3.T6.1.1.1.m1.1.1.3" xref="S3.T6.1.1.1.m1.1.1.3.cmml"><mi id="S3.T6.1.1.1.m1.1.1.3.2" xref="S3.T6.1.1.1.m1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.T6.1.1.1.m1.1.1.3.1" xref="S3.T6.1.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.T6.1.1.1.m1.1.1.3.3" xref="S3.T6.1.1.1.m1.1.1.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.T6.1.1.1.m1.1.1.3.1a" xref="S3.T6.1.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.T6.1.1.1.m1.1.1.3.4" xref="S3.T6.1.1.1.m1.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.T6.1.1.1.m1.1.1.3.1b" xref="S3.T6.1.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.T6.1.1.1.m1.1.1.3.5" xref="S3.T6.1.1.1.m1.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.T6.1.1.1.m1.1.1.3.1c" xref="S3.T6.1.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.T6.1.1.1.m1.1.1.3.6" xref="S3.T6.1.1.1.m1.1.1.3.6.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.T6.1.1.1.m1.1b"><apply id="S3.T6.1.1.1.m1.1.1.cmml" xref="S3.T6.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T6.1.1.1.m1.1.1.1.cmml" xref="S3.T6.1.1.1.m1.1.1">subscript</csymbol><ci id="S3.T6.1.1.1.m1.1.1.2.cmml" xref="S3.T6.1.1.1.m1.1.1.2">𝐸</ci><apply id="S3.T6.1.1.1.m1.1.1.3.cmml" xref="S3.T6.1.1.1.m1.1.1.3"><times id="S3.T6.1.1.1.m1.1.1.3.1.cmml" xref="S3.T6.1.1.1.m1.1.1.3.1"></times><ci id="S3.T6.1.1.1.m1.1.1.3.2.cmml" xref="S3.T6.1.1.1.m1.1.1.3.2">𝑠</ci><ci id="S3.T6.1.1.1.m1.1.1.3.3.cmml" xref="S3.T6.1.1.1.m1.1.1.3.3">𝑦</ci><ci id="S3.T6.1.1.1.m1.1.1.3.4.cmml" xref="S3.T6.1.1.1.m1.1.1.3.4">𝑛</ci><ci id="S3.T6.1.1.1.m1.1.1.3.5.cmml" xref="S3.T6.1.1.1.m1.1.1.3.5">𝑡</ci><ci id="S3.T6.1.1.1.m1.1.1.3.6.cmml" xref="S3.T6.1.1.1.m1.1.1.3.6">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T6.1.1.1.m1.1c">E_{synth}</annotation></semantics></math></th>
<th id="S3.T6.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><math id="S3.T6.2.2.2.m1.1" class="ltx_Math" alttext="E_{real}" display="inline"><semantics id="S3.T6.2.2.2.m1.1a"><msub id="S3.T6.2.2.2.m1.1.1" xref="S3.T6.2.2.2.m1.1.1.cmml"><mi id="S3.T6.2.2.2.m1.1.1.2" xref="S3.T6.2.2.2.m1.1.1.2.cmml">E</mi><mrow id="S3.T6.2.2.2.m1.1.1.3" xref="S3.T6.2.2.2.m1.1.1.3.cmml"><mi id="S3.T6.2.2.2.m1.1.1.3.2" xref="S3.T6.2.2.2.m1.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.T6.2.2.2.m1.1.1.3.1" xref="S3.T6.2.2.2.m1.1.1.3.1.cmml">​</mo><mi id="S3.T6.2.2.2.m1.1.1.3.3" xref="S3.T6.2.2.2.m1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.T6.2.2.2.m1.1.1.3.1a" xref="S3.T6.2.2.2.m1.1.1.3.1.cmml">​</mo><mi id="S3.T6.2.2.2.m1.1.1.3.4" xref="S3.T6.2.2.2.m1.1.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.T6.2.2.2.m1.1.1.3.1b" xref="S3.T6.2.2.2.m1.1.1.3.1.cmml">​</mo><mi id="S3.T6.2.2.2.m1.1.1.3.5" xref="S3.T6.2.2.2.m1.1.1.3.5.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.T6.2.2.2.m1.1b"><apply id="S3.T6.2.2.2.m1.1.1.cmml" xref="S3.T6.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S3.T6.2.2.2.m1.1.1.1.cmml" xref="S3.T6.2.2.2.m1.1.1">subscript</csymbol><ci id="S3.T6.2.2.2.m1.1.1.2.cmml" xref="S3.T6.2.2.2.m1.1.1.2">𝐸</ci><apply id="S3.T6.2.2.2.m1.1.1.3.cmml" xref="S3.T6.2.2.2.m1.1.1.3"><times id="S3.T6.2.2.2.m1.1.1.3.1.cmml" xref="S3.T6.2.2.2.m1.1.1.3.1"></times><ci id="S3.T6.2.2.2.m1.1.1.3.2.cmml" xref="S3.T6.2.2.2.m1.1.1.3.2">𝑟</ci><ci id="S3.T6.2.2.2.m1.1.1.3.3.cmml" xref="S3.T6.2.2.2.m1.1.1.3.3">𝑒</ci><ci id="S3.T6.2.2.2.m1.1.1.3.4.cmml" xref="S3.T6.2.2.2.m1.1.1.3.4">𝑎</ci><ci id="S3.T6.2.2.2.m1.1.1.3.5.cmml" xref="S3.T6.2.2.2.m1.1.1.3.5">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T6.2.2.2.m1.1c">E_{real}</annotation></semantics></math></th>
<th id="S3.T6.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><math id="S3.T6.3.3.3.m1.1" class="ltx_Math" alttext="G_{factor}" display="inline"><semantics id="S3.T6.3.3.3.m1.1a"><msub id="S3.T6.3.3.3.m1.1.1" xref="S3.T6.3.3.3.m1.1.1.cmml"><mi id="S3.T6.3.3.3.m1.1.1.2" xref="S3.T6.3.3.3.m1.1.1.2.cmml">G</mi><mrow id="S3.T6.3.3.3.m1.1.1.3" xref="S3.T6.3.3.3.m1.1.1.3.cmml"><mi id="S3.T6.3.3.3.m1.1.1.3.2" xref="S3.T6.3.3.3.m1.1.1.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.T6.3.3.3.m1.1.1.3.1" xref="S3.T6.3.3.3.m1.1.1.3.1.cmml">​</mo><mi id="S3.T6.3.3.3.m1.1.1.3.3" xref="S3.T6.3.3.3.m1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.T6.3.3.3.m1.1.1.3.1a" xref="S3.T6.3.3.3.m1.1.1.3.1.cmml">​</mo><mi id="S3.T6.3.3.3.m1.1.1.3.4" xref="S3.T6.3.3.3.m1.1.1.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.T6.3.3.3.m1.1.1.3.1b" xref="S3.T6.3.3.3.m1.1.1.3.1.cmml">​</mo><mi id="S3.T6.3.3.3.m1.1.1.3.5" xref="S3.T6.3.3.3.m1.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.T6.3.3.3.m1.1.1.3.1c" xref="S3.T6.3.3.3.m1.1.1.3.1.cmml">​</mo><mi id="S3.T6.3.3.3.m1.1.1.3.6" xref="S3.T6.3.3.3.m1.1.1.3.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.T6.3.3.3.m1.1.1.3.1d" xref="S3.T6.3.3.3.m1.1.1.3.1.cmml">​</mo><mi id="S3.T6.3.3.3.m1.1.1.3.7" xref="S3.T6.3.3.3.m1.1.1.3.7.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.T6.3.3.3.m1.1b"><apply id="S3.T6.3.3.3.m1.1.1.cmml" xref="S3.T6.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S3.T6.3.3.3.m1.1.1.1.cmml" xref="S3.T6.3.3.3.m1.1.1">subscript</csymbol><ci id="S3.T6.3.3.3.m1.1.1.2.cmml" xref="S3.T6.3.3.3.m1.1.1.2">𝐺</ci><apply id="S3.T6.3.3.3.m1.1.1.3.cmml" xref="S3.T6.3.3.3.m1.1.1.3"><times id="S3.T6.3.3.3.m1.1.1.3.1.cmml" xref="S3.T6.3.3.3.m1.1.1.3.1"></times><ci id="S3.T6.3.3.3.m1.1.1.3.2.cmml" xref="S3.T6.3.3.3.m1.1.1.3.2">𝑓</ci><ci id="S3.T6.3.3.3.m1.1.1.3.3.cmml" xref="S3.T6.3.3.3.m1.1.1.3.3">𝑎</ci><ci id="S3.T6.3.3.3.m1.1.1.3.4.cmml" xref="S3.T6.3.3.3.m1.1.1.3.4">𝑐</ci><ci id="S3.T6.3.3.3.m1.1.1.3.5.cmml" xref="S3.T6.3.3.3.m1.1.1.3.5">𝑡</ci><ci id="S3.T6.3.3.3.m1.1.1.3.6.cmml" xref="S3.T6.3.3.3.m1.1.1.3.6">𝑜</ci><ci id="S3.T6.3.3.3.m1.1.1.3.7.cmml" xref="S3.T6.3.3.3.m1.1.1.3.7">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T6.3.3.3.m1.1c">G_{factor}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T6.3.4.1" class="ltx_tr">
<th id="S3.T6.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Black <span id="S3.T6.3.4.1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</th>
<td id="S3.T6.3.4.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.9</td>
<td id="S3.T6.3.4.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0409</td>
<td id="S3.T6.3.4.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.2918</td>
<td id="S3.T6.3.4.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7.13</td>
</tr>
<tr id="S3.T6.3.5.2" class="ltx_tr">
<th id="S3.T6.3.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Sharma <span id="S3.T6.3.5.2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</th>
<td id="S3.T6.3.5.2.2" class="ltx_td ltx_align_center ltx_border_r">11.2</td>
<td id="S3.T6.3.5.2.3" class="ltx_td ltx_align_center ltx_border_r">0.0626</td>
<td id="S3.T6.3.5.2.4" class="ltx_td ltx_align_center ltx_border_r">0.3951</td>
<td id="S3.T6.3.5.2.5" class="ltx_td ltx_align_center ltx_border_r">6.31</td>
</tr>
<tr id="S3.T6.3.6.3" class="ltx_tr">
<th id="S3.T6.3.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Proença <span id="S3.T6.3.6.3.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</th>
<td id="S3.T6.3.6.3.2" class="ltx_td ltx_align_center ltx_border_r">500</td>
<td id="S3.T6.3.6.3.3" class="ltx_td ltx_align_center ltx_border_r">0.0571</td>
<td id="S3.T6.3.6.3.4" class="ltx_td ltx_align_center ltx_border_r">0.1555</td>
<td id="S3.T6.3.6.3.5" class="ltx_td ltx_align_center ltx_border_r">2.72</td>
</tr>
<tr id="S3.T6.3.7.4" class="ltx_tr">
<th id="S3.T6.3.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">ours (regression)</th>
<td id="S3.T6.3.7.4.2" class="ltx_td ltx_align_center ltx_border_r">2.2</td>
<td id="S3.T6.3.7.4.3" class="ltx_td ltx_align_center ltx_border_r">0.6160</td>
<td id="S3.T6.3.7.4.4" class="ltx_td ltx_align_center ltx_border_r">0.7997</td>
<td id="S3.T6.3.7.4.5" class="ltx_td ltx_align_center ltx_border_r">1.30</td>
</tr>
<tr id="S3.T6.3.8.5" class="ltx_tr">
<th id="S3.T6.3.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">ours (8 bins)</th>
<td id="S3.T6.3.8.5.2" class="ltx_td ltx_align_center ltx_border_r">2.8</td>
<td id="S3.T6.3.8.5.3" class="ltx_td ltx_align_center ltx_border_r">0.2520</td>
<td id="S3.T6.3.8.5.4" class="ltx_td ltx_align_center ltx_border_r">0.7868</td>
<td id="S3.T6.3.8.5.5" class="ltx_td ltx_align_center ltx_border_r">3.12</td>
</tr>
<tr id="S3.T6.3.9.6" class="ltx_tr">
<th id="S3.T6.3.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">ours (12 bins)</th>
<td id="S3.T6.3.9.6.2" class="ltx_td ltx_align_center ltx_border_r">4.4</td>
<td id="S3.T6.3.9.6.3" class="ltx_td ltx_align_center ltx_border_r">0.2104</td>
<td id="S3.T6.3.9.6.4" class="ltx_td ltx_align_center ltx_border_r">1.2231</td>
<td id="S3.T6.3.9.6.5" class="ltx_td ltx_align_center ltx_border_r">5.81</td>
</tr>
<tr id="S3.T6.3.10.7" class="ltx_tr">
<th id="S3.T6.3.10.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">ours (16 bins)</th>
<td id="S3.T6.3.10.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">7.4</td>
<td id="S3.T6.3.10.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.1947</td>
<td id="S3.T6.3.10.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">1.2074</td>
<td id="S3.T6.3.10.7.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">6.20</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Future work</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Future works will explore in-depth embeddability by using quantization and pruning. It will further optimize the memory footprint of the parameters and the inference computational complexity of Mobile-URSONet. We plan to deploy these models on promising commercial chips for future satellite onboard computers, such as the Xilinx MPSoCs featuring ARM-A53 cores and programmable logic.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">CONCLUSIONS</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this paper, we analyzed URSONet, a popular neural network used for spacecraft pose estimation that seems hardly compatible with the constraints of onboard spacecraft computers. We found that three architectural choices have a dominant effect on both the number of parameters and the computational complexity of the CNN: the backbone, the bottleneck size, and the number of bins per dimension while predicting orientation using soft classification. By analyzing trade-offs for each of these three architectural choices, we were able to propose Mobile-URSONet, a mobile version of URSONet in the spirit of Google MobileNets. We showed that Mobile-URSONet achieves accuracy close to URSONet, while keeping the number of parameters and computational complexity compatible with the constraints of spacecraft onboard computers.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">ACKNOWLEDGMENTS</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The authors thank the Canadian Space Agency, MITACS and Space Codesign Systems for their financial contributions.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
P. F. Proença and Y. Gao, “Deep learning for spacecraft pose estimation from
photorealistic rendering,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">2020 IEEE International Conference
on Robotics and Automation (ICRA)</em>.   IEEE, 2020, pp. 6007–6013.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
“Kelvins - Pose Estimation Challenge.” [Online]. Available:
<a target="_blank" href="https://kelvins.esa.int/satellite-pose-estimation-challenge/home/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://kelvins.esa.int/satellite-pose-estimation-challenge/home/</a>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
“ESA commissions world’s first space debris
removal.” [Online]. Available:
<a target="_blank" href="https://www.esa.int/Safety_Security/Clean_Space/ESA_commissions_world_s_first_space_debris_removal" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.esa.int/Safety_Security/Clean_Space/ESA_commissions_world_s_first_space_debris_removal</a>

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
S. Sharma, C. Beierle, and S. D’Amico, “Pose estimation for non-cooperative
spacecraft rendezvous using convolutional neural networks,” in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">2018
IEEE Aerospace Conference</em>, Mar. 2018, pp. 1–12.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
M. Kisantal, S. Sharma, T. H. Park, D. Izzo, M. Märtens, and S. D’Amico,
“Satellite Pose Estimation Challenge: Dataset, Competition
Design and Results,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv:1911.02050 [cs]</em>, Apr. 2020, arXiv:
1911.02050. [Online]. Available: <a target="_blank" href="http://arxiv.org/abs/1911.02050" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1911.02050</a>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
S. Sharma and S. D’Amico, “Pose Estimation for Non-Cooperative
Rendezvous Using Neural Networks,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv:1906.09868 [cs]</em>,
Jun. 2019, arXiv: 1906.09868. [Online]. Available:
<a target="_blank" href="http://arxiv.org/abs/1906.09868" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1906.09868</a>

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
“Kelvins - Pose Estimation Challenge post mortem - Home.” [Online].
Available:
<a target="_blank" href="https://kelvins.esa.int/satellite-pose-estimation-challenge/leaderboard/post-mortem-leaderboard" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://kelvins.esa.int/satellite-pose-estimation-challenge/leaderboard/post-mortem-leaderboard</a>

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
B. Chen, J. Cao, A. Parra, and T.-J. Chin, “Satellite Pose Estimation with
Deep Landmark Regression and Nonlinear Pose Refinement,”
<em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv:1908.11542 [cs]</em>, Aug. 2019, arXiv: 1908.11542. [Online].
Available: <a target="_blank" href="http://arxiv.org/abs/1908.11542" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1908.11542</a>

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
K. Black, S. Shankar, D. Fonseka, J. Deutsch, A. Dhir, and M. R. Akella,
“Real-Time, Flight-Ready, Non-Cooperative Spacecraft Pose
Estimation Using Monocular Imagery,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv:2101.09553 [cs]</em>,
Jan. 2021, arXiv: 2101.09553. [Online]. Available:
<a target="_blank" href="http://arxiv.org/abs/2101.09553" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2101.09553</a>

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
F. L. Markley, Y. Cheng, J. L. Crassidis, and Y. Oshman,
“Averaging Quaternions,”
<em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Journal of Guidance, Control, and Dynamics</em>,
vol. 30, no. 4, pp. 1193–1197, Jul. 2007. [Online]. Available:
<a target="_blank" href="https://arc.aiaa.org/doi/10.2514/1.28949" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arc.aiaa.org/doi/10.2514/1.28949</a>

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand,
M. Andreetto, and H. Adam, “MobileNets: Efficient Convolutional
Neural Networks for Mobile Vision Applications,”
<em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv:1704.04861 [cs]</em>, Apr. 2017, arXiv: 1704.04861. [Online].
Available: <a target="_blank" href="http://arxiv.org/abs/1704.04861" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1704.04861</a>

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
S. Bianco, R. Cadene, L. Celona, and P. Napoletano, “Benchmark Analysis of
Representative Deep Neural Network Architectures,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE
Access</em>, vol. 6, pp. 64 270–64 277, 2018, arXiv: 1810.00736. [Online].
Available: <a target="_blank" href="http://arxiv.org/abs/1810.00736" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1810.00736</a>

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,
“MobileNetV2: Inverted Residuals and Linear
Bottlenecks,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>.   Salt Lake City, UT: IEEE, Jun. 2018, pp.
4510–4520. [Online]. Available:
<a target="_blank" href="https://ieeexplore.ieee.org/document/8578572/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ieeexplore.ieee.org/document/8578572/</a>

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
F. Apper, A. Ressouche, N. Humeau, M. Vuillemin, A. Gaboriaud, F. Viaud, and
M. Couture, “Eye-Sat: A 3U student CubeSat
from CNES packed with technology,” p. 10.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A. Pérez, A. Rodríguez, A. Otero, D. G. Arjona, Á. Jiménez-Peralo,
M. Á. Verdugo, and E. De La Torre, “Run-Time Reconfigurable
MPSoC-Based On-Board Processor for Vision-Based Space
Navigation,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 8, pp. 59 891–59 905, 2020,
conference Name: IEEE Access.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
C. M. Fuchs, P. Chou, X. Wen, N. M. Murillo, G. Furano, S. Holst,
A. Tavoularis, S.-K. Lu, A. Plaat, and K. Marinis, “A Fault-Tolerant
MPSoC For CubeSats,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">2019 IEEE International Symposium
on Defect and Fault Tolerance in VLSI and Nanotechnology Systems
(DFT)</em>, Oct. 2019, pp. 1–6, iSSN: 2377-7966.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
T. Moreau, T. Chen, L. Vega, J. Roesch, E. Yan, L. Zheng, J. Fromm, Z. Jiang,
L. Ceze, C. Guestrin, and A. Krishnamurthy, “A Hardware-Software
Blueprint for Flexible Deep Learning Specialization,”
<em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv:1807.04188 [cs, stat]</em>, Apr. 2019, arXiv: 1807.04188. [Online].
Available: <a target="_blank" href="http://arxiv.org/abs/1807.04188" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1807.04188</a>

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image
Recognition,” <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv:1512.03385 [cs]</em>, Dec. 2015, arXiv: 1512.03385.
[Online]. Available: <a target="_blank" href="http://arxiv.org/abs/1512.03385" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1512.03385</a>

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,”
<em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Nature</em>, vol. 521, no. 7553, pp. 436–444, May
2015, number: 7553 Publisher: Nature Publishing Group. [Online]. Available:
<a target="_blank" href="https://www.nature.com/articles/nature14539" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.nature.com/articles/nature14539</a>

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
M. Horowitz, “1.1 Computing’s energy problem (and what we can do about
it),” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">2014 IEEE International Solid-State Circuits
Conference Digest of Technical Papers (ISSCC)</em>, Feb. 2014, pp.
10–14, iSSN: 2376-8606.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
“Mobile-URSONet code (GitHub).” [Online]. Available:
<a target="_blank" href="https://github.com/possoj/Mobile-URSONet" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/possoj/Mobile-URSONet</a>

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2205.02064" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2205.02065" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2205.02065">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2205.02065" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2205.02066" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 12:58:43 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
