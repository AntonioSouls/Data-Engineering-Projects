<!DOCTYPE html><html lang="en" data-theme="light"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective</title>
<!--Generated on Sun Oct  6 12:34:25 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">




<base href="https://arxiv.org/html/2410.04466v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2410.04466v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" title="Toggle dark/light mode" aria-label="System preference">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        
        
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="https://arxiv.org/html/2410.04466v1/#myForm">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2410.04466v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2410.04466v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" title="Toggle dark/light mode">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        
        
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist"><li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#abstract" title="Abstract">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        Abstract
      </span>
    </a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S1" title="In Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S2" title="In Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Generative LLM Architecture</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S2.SS1" title="In 2 Generative LLM Architecture ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Vanilla Generative LLM</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S2.SS2" title="In 2 Generative LLM Architecture ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Efficient Generative LLM</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S2.SS2.SSS1" title="In 2.2 Efficient Generative LLM ‣ 2 Generative LLM Architecture ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Transformer-based LLM</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S2.SS2.SSS2" title="In 2.2 Efficient Generative LLM ‣ 2 Generative LLM Architecture ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>SSM-based LLM</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S2.SS2.SSS3" title="In 2.2 Efficient Generative LLM ‣ 2 Generative LLM Architecture ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.3 </span>Hybrid LLM</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3" title="In Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Optimizations on Hardware Platforms</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS1" title="In 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Quantization</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS1.SSS1" title="In 3.1 Quantization ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS1.SSS2" title="In 3.1 Quantization ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>CPU</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS1.SSS3" title="In 3.1 Quantization ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.3 </span>GPU</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS1.SSS4" title="In 3.1 Quantization ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.4 </span>FPGA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS1.SSS5" title="In 3.1 Quantization ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.5 </span>ASIC</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS1.SSS6" title="In 3.1 Quantization ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.6 </span>PIM/NDP</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS1.SSS7" title="In 3.1 Quantization ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.7 </span>Quantitative Comparison</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS2" title="In 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Sparsity</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS2.SSS1" title="In 3.2 Sparsity ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS2.SSS2" title="In 3.2 Sparsity ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>CPU</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS2.SSS3" title="In 3.2 Sparsity ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span>GPU</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS2.SSS4" title="In 3.2 Sparsity ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.4 </span>FPGA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS2.SSS5" title="In 3.2 Sparsity ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.5 </span>ASIC</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS2.SSS6" title="In 3.2 Sparsity ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.6 </span>PIM/NDP</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS2.SSS7" title="In 3.2 Sparsity ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.7 </span>Comparison</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS3" title="In 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Fast Decoding</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS3.SSS1" title="In 3.3 Fast Decoding ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS3.SSS2" title="In 3.3 Fast Decoding ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>GPU</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS3.SSS3" title="In 3.3 Fast Decoding ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.3 </span>ASIC</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS3.SSS4" title="In 3.3 Fast Decoding ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.4 </span>PIM/NDP</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS3.SSS5" title="In 3.3 Fast Decoding ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.5 </span>Comparison</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS4" title="In 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Operator Optimization</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS4.SSS1" title="In 3.4 Operator Optimization ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span>Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS4.SSS2" title="In 3.4 Operator Optimization ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span>GPU</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS4.SSS3" title="In 3.4 Operator Optimization ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.3 </span>ASIC</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS4.SSS4" title="In 3.4 Operator Optimization ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.4 </span>PIM/NDP</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS4.SSS5" title="In 3.4 Operator Optimization ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.5 </span>Comparison</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS5" title="In 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Heterogeneous Cooperation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS5.SSS1" title="In 3.5 Heterogeneous Cooperation ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.1 </span>Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS5.SSS2" title="In 3.5 Heterogeneous Cooperation ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.2 </span>CPU</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS5.SSS3" title="In 3.5 Heterogeneous Cooperation ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.3 </span>PIM/NDP</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS5.SSS4" title="In 3.5 Heterogeneous Cooperation ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.4 </span>Comparison</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS6" title="In 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>Homogeneous Cooperation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS6.SSS1" title="In 3.6 Homogeneous Cooperation ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.1 </span>Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS6.SSS2" title="In 3.6 Homogeneous Cooperation ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.2 </span>CPU</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.SS6.SSS3" title="In 3.6 Homogeneous Cooperation ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.3 </span>FPGA</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S4" title="In Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S4.SS1" title="In 4 Discussion ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S4.SS2" title="In 4 Discussion ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Hardware Comparison</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S4.SS2.SSS1" title="In 4.2 Hardware Comparison ‣ 4 Discussion ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Small batch size (bs=1)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S4.SS2.SSS2" title="In 4.2 Hardware Comparison ‣ 4 Discussion ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Large batch size (bs=8)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S4.SS3" title="In 4 Discussion ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Optimization Method Comparison</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S4.SS3.SSS1" title="In 4.3 Optimization Method Comparison ‣ 4 Discussion ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Small batch size (bs=1)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S4.SS3.SSS2" title="In 4.3 Optimization Method Comparison ‣ 4 Discussion ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span>Large batch size (bs=8)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S4.SS4" title="In 4 Discussion ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Discussion on Edge-side Scenarios</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S5" title="In Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib" title="References">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        References
      </span>
    </a></li></ol></nav>

<div class="ltx_page_content"><div id="target-section" class="section"><a id="license-tr" href="https://info.arxiv.org/help/license/index.html#licenses-available">License: CC BY 4.0</a><div id="watermark-tr">arXiv:2410.04466v1 [cs.AR] 06 Oct 2024</div></div>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Large Language Model Inference Acceleration: 
<br class="ltx_break">A Comprehensive Hardware Perspective</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Jinhao Li 
<br class="ltx_break">Shanghai Jiao Tong University 
<br class="ltx_break">&amp;Jiaming Xu 
<br class="ltx_break">Shanghai Jiao Tong University 
<br class="ltx_break">&amp; Infinigence-AI 
<br class="ltx_break">&amp;Shan Huang 
<br class="ltx_break">Shanghai Jiao Tong University 
<br class="ltx_break">&amp;&nbsp;&nbsp;&nbsp;&nbsp;Yonghua Chen 
<br class="ltx_break">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Infinigence-AI&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
<br class="ltx_break">&amp;&nbsp;&nbsp;&nbsp;Wen Li 
<br class="ltx_break">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Infinigence-AI&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
<br class="ltx_break">&amp;Jun Liu 
<br class="ltx_break">Shanghai Jiao Tong University 
<br class="ltx_break">&amp;Yaoxiu Lian 
<br class="ltx_break">Shanghai Jiao Tong University 
<br class="ltx_break">&amp;Jiayi Pan 
<br class="ltx_break">Shanghai Jiao Tong University 
<br class="ltx_break">&amp;Li Ding 
<br class="ltx_break">Shanghai Jiao Tong University 
<br class="ltx_break">&amp;Hao Zhou 
<br class="ltx_break">Shanghai Jiao Tong University 
<br class="ltx_break">&amp;Guohao Dai 
<br class="ltx_break">Shanghai Jiao Tong University 
<br class="ltx_break">&amp; Infinigence-AI 
<br class="ltx_break">
</span><span class="ltx_author_notes">First author: kimholee@sjtu.edu.cnCorresponding author: daiguohao@sjtu.edu.cn</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract" id="abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id1.id1">Large Language Models (LLMs) have demonstrated remarkable capabilities across various fields, from natural language understanding to text generation.
Compared to non-generative LLMs like BERT and DeBERTa, generative LLMs like GPT series and Llama series are currently the main focus due to their superior algorithmic performance.
The advancements in generative LLMs are closely intertwined with the development of hardware capabilities.
Various hardware platforms exhibit distinct hardware characteristics, which can help improve LLM inference performance.
Therefore, this paper comprehensively surveys efficient generative LLM inference on different hardware platforms.
First, we provide an overview of the algorithm architecture of mainstream generative LLMs and delve into the inference process.
Then, we summarize different optimization methods for different platforms such as CPU, GPU, FPGA, ASIC, and PIM/NDP, and provide inference results for generative LLMs.
Furthermore, we perform a qualitative and quantitative comparison of inference performance with batch sizes 1 and 8 on different hardware platforms by considering hardware power consumption, absolute inference speed (tokens/s), and energy efficiency (tokens/J).
We compare the performance of the same optimization methods across different hardware platforms, the performance across different hardware platforms, and the performance of different methods on the same hardware platform.
This provides a systematic and comprehensive summary of existing inference acceleration work by integrating software optimization methods and hardware platforms, which can point to the future trends and potential developments of generative LLMs and hardware technology for edge-side scenarios.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<p class="ltx_p" id="p1.7"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="p1.7.1">K</em><span class="ltx_text ltx_font_bold" id="p1.7.2">eywords</span> Generative Large Language Model &nbsp;<math alttext="\cdot" class="ltx_Math" display="inline" id="p1.1.m1.1"><semantics id="p1.1.m1.1a"><mo id="p1.1.m1.1.1" xref="p1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.1.m1.1b"><ci id="p1.1.m1.1.1.cmml" xref="p1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.1.m1.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="p1.1.m1.1d">⋅</annotation></semantics></math>
Hardware &nbsp;<math alttext="\cdot" class="ltx_Math" display="inline" id="p1.2.m2.1"><semantics id="p1.2.m2.1a"><mo id="p1.2.m2.1.1" xref="p1.2.m2.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.2.m2.1b"><ci id="p1.2.m2.1.1.cmml" xref="p1.2.m2.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.2.m2.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="p1.2.m2.1d">⋅</annotation></semantics></math>
CPU &nbsp;<math alttext="\cdot" class="ltx_Math" display="inline" id="p1.3.m3.1"><semantics id="p1.3.m3.1a"><mo id="p1.3.m3.1.1" xref="p1.3.m3.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.3.m3.1b"><ci id="p1.3.m3.1.1.cmml" xref="p1.3.m3.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.3.m3.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="p1.3.m3.1d">⋅</annotation></semantics></math>
GPU &nbsp;<math alttext="\cdot" class="ltx_Math" display="inline" id="p1.4.m4.1"><semantics id="p1.4.m4.1a"><mo id="p1.4.m4.1.1" xref="p1.4.m4.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.4.m4.1b"><ci id="p1.4.m4.1.1.cmml" xref="p1.4.m4.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.4.m4.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="p1.4.m4.1d">⋅</annotation></semantics></math>
FPGA &nbsp;<math alttext="\cdot" class="ltx_Math" display="inline" id="p1.5.m5.1"><semantics id="p1.5.m5.1a"><mo id="p1.5.m5.1.1" xref="p1.5.m5.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.5.m5.1b"><ci id="p1.5.m5.1.1.cmml" xref="p1.5.m5.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.5.m5.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="p1.5.m5.1d">⋅</annotation></semantics></math>
ASIC &nbsp;<math alttext="\cdot" class="ltx_Math" display="inline" id="p1.6.m6.1"><semantics id="p1.6.m6.1a"><mo id="p1.6.m6.1.1" xref="p1.6.m6.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.6.m6.1b"><ci id="p1.6.m6.1.1.cmml" xref="p1.6.m6.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.6.m6.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="p1.6.m6.1d">⋅</annotation></semantics></math>
PIM &nbsp;<math alttext="\cdot" class="ltx_Math" display="inline" id="p1.7.m7.1"><semantics id="p1.7.m7.1a"><mo id="p1.7.m7.1.1" xref="p1.7.m7.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.7.m7.1b"><ci id="p1.7.m7.1.1.cmml" xref="p1.7.m7.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.7.m7.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="p1.7.m7.1d">⋅</annotation></semantics></math>
NDP</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large Language Models (LLMs) have become cornerstones of modern artificial intelligence, demonstrating remarkable capabilities across a spectrum of fields, from natural language understanding to text generation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib5" title="">5</a>]</cite>.
LLMs can be categorized into two primary types: generative LLMs and non-generative LLMs.
Non-generative LLMs, such as BERT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib6" title="">6</a>]</cite>, RoBERTa&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib7" title="">7</a>]</cite>, ELECTRA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib8" title="">8</a>]</cite>, and DeBERTa&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib9" title="">9</a>]</cite>, are designed to classify and make predictions based on input text.
These models typically range in size from millions of parameters, allowing them to excel in tasks that require discernment and nuanced understanding.
BERT, introduced in 2018, only has 340 million parameters.
RoBERTa, introduced in 2019, slightly increases to 355 million parameters.
And DeBERTa, released in 2021, increases to 1.5 billion.
Generative LLMs, like GPT series&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib13" title="">13</a>]</cite>, T5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib14" title="">14</a>]</cite>, OPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib15" title="">15</a>]</cite>, BLOOM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib16" title="">16</a>]</cite>, and Llama series&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib19" title="">19</a>]</cite>, have taken language generation to new heights.
The model size increase of generative LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib52" title="">52</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib57" title="">57</a>]</cite> are particularly notable in the past 6 years, as shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_tag">1</span></a>.
In 2018, GPT1 has only 110 million parameters, which grows to 1.5 billion in GPT2 in 2019.
GPT3, launched in 2020, grows to 175B parameters dramatically, and GPT3.5 maintains the same size.
After 2022, the model size maintains to several hundreds and thousands of billions like GPT4, Llama3, and Grok1&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib58" title="">58</a>]</cite>.
The evolution of LLMs has been characterized by an exponential growth in model parameters, which has been instrumental in enhancing their performance and versatility.
Compared to non-generative LLMs, generative LLMs are currently the primary focus of research and development in the field of LLMs for their superior algorithmic performance.
In recent years, after reaching the trillion-parameter scale in 2022, the parameter size of generative LLMs has stopped growing at an exponential rate. Two main reasons can explain this phenomenon: (1) As the amount of computation increases, the demand for computing power also rises significantly. The slow growth of hardware capabilities, particularly the slowing down of Moore’s Law&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib59" title="">59</a>]</cite>, limits the improvement of single-chip computing power.
(2) Researchers have found that model performance is not solely dependent on the number of parameters, but also on the quantity and quality of training data&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib60" title="">60</a>]</cite>. By providing more qualified training tokens, the algorithmic performance can be further improved&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib19" title="">19</a>]</cite>.
At the same time, the parameter sizes of generative LLMs have shifted from "small to large" to "remaining stable" or even "shrinking".
More models with fewer parameters are being released, particularly those that are better suited for deployment on edge devices.
Notably, OpenAI’s recent o1&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib61" title="">61</a>]</cite> improves algorithm performance by introducing Chain-of-Thought (CoT) reasoning and multi-step inference. This new computational paradigm increases the importance of inference within the model, further highlighting the need to accelerate inference efficiency.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="469" id="S1.F1.g1" src="https://arxiv.org/html/2410.04466v1/x1.png" width="822">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Typical LLM model size in the past six years.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.2">The advancements in generative LLMs are closely intertwined with the development of hardware capabilities.
Due to the continuation of Moore’s Law, from 2018 to 2022, GPU manufacturing processes have progressed from 12nm to 3nm, and the floating-point performance of single GPU die has increased from 130 TFLOPS to 989 TFLOPS.
During model training, GPUs are used predominantly due to the user-friendliness of the CUDA programming stack&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib62" title="">62</a>]</cite> and the high scalability of GPU chips (<span class="ltx_text ltx_font_italic" id="S1.p2.2.1">e.g.</span> NVLink&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib63" title="">63</a>]</cite>).
During inference, various hardware options like CPU, GPU, FPGA, and ASIC exhibit distinct hardware characteristics, which can help improving LLM inference performance.
CPUs offer high programmability with a computing power of approximately 4 to 70 TOPS and with power consumption around from 4W to &gt;200W.
Modern CPUs (including some System-on-Chips, SoCs) enhance AI performance by integrating domains-specific architecture (DSA) units. These include Apple’s Neural Engine in the M2 Ultra&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib64" title="">64</a>]</cite>, Qualcomm’s NPU in the Snapdragon 8 Gen3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib65" title="">65</a>]</cite>, and Intel’s AVX/AMX ISA extensions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib66" title="">66</a>]</cite>.
GPUs excel in parallelism and computing power, delivering between <math alttext="\sim" class="ltx_Math" display="inline" id="S1.p2.1.m1.1"><semantics id="S1.p2.1.m1.1a"><mo id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><csymbol cd="latexml" id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S1.p2.1.m1.1d">∼</annotation></semantics></math>70 to &gt;1000 TOPS and featuring an impressive memory bandwidth of up to 1555 GB/s.
On one hand, GPUs integrate a large number of SIMD cores and Tensor Cores in NVIDIA V100/A100/H100&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib67" title="">67</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib68" title="">68</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib69" title="">69</a>]</cite> or Matrix Cores in AMD Instinct MI100/MI200/MI300 series&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib71" title="">71</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib72" title="">72</a>]</cite> to enhance computing powers.
On the other hand, GPUs support lower precision computations, such as INT8, FP8 and INT4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib68" title="">68</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib69" title="">69</a>]</cite>, which allows for more multiplication units to be packed into a given chip area.
Nevertheless, their power consumption is significantly higher, ranging from <math alttext="\sim" class="ltx_Math" display="inline" id="S1.p2.2.m2.1"><semantics id="S1.p2.2.m2.1a"><mo id="S1.p2.2.m2.1.1" xref="S1.p2.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S1.p2.2.m2.1b"><csymbol cd="latexml" id="S1.p2.2.m2.1.1.cmml" xref="S1.p2.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.2.m2.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S1.p2.2.m2.1d">∼</annotation></semantics></math>20W to &gt;700W.
FPGAs offer substantial parallelism and optimization capabilities, with computing performance between 50 to 100 TOPS. They are also more power-efficient, consuming about 75 to 100W.
ASICs are often designed for specific applications and custom silicon designs, resulting in a wide range of computing power from GOPS to TOPS.
Their power consumption can vary from the 0.1W range to several hundred watts.
Due to their specialized design, ASICs generally offer higher computational efficiency and better energy efficiency compared to GPUs and CPUs.
The unique attributes of each hardware type influence their optimal application in various inference scenarios.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Here, we list and compare the existing surveys of LLM inference in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_tag">1</span></a>.
Previous surveys&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib73" title="">73</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib74" title="">74</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib75" title="">75</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib76" title="">76</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib77" title="">77</a>]</cite> primarily summarize various software optimization methods like quantization, sparsity, fast decoding for generative LLMs from an algorithm perspective.
However, they do not take into account whether different optimization methods exhibit varying inference performance across different hardware platforms, and similarly, they also lack a fair and quantitative comparison.
Surveys&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib78" title="">78</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib79" title="">79</a>]</cite> focus on accelerating transformer-based LLMs, including non-generative LLMs like BERT and a very small number of generative LLMs like GPT, but merely list the work done on different hardware platforms. They lack a summary and abstraction of the optimization methods used by different accelerators.
Additionally, it only provides a relative comparison of speedup and energy efficiency with different baselines while lacking a fair comparison of inference performance, as what matters most for generative LLMs is the absolute inference speed (tokens per second, tokens/s) and inference energy efficiency (tokens per joule, tokens/J).
Like <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib78" title="">78</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib79" title="">79</a>]</cite>, surveys&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib80" title="">80</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib81" title="">81</a>]</cite> mainly focus on non-generative LLMs and one or two specific hardware platforms.
Our survey focuses solely on generative LLMs, summarizing various software optimization methods in conjunction with multiple hardware platforms, including CPUs, GPUs, FPGAs, ASICs, and PIM/NDPs. For the first time, we innovatively use performance metrics that matter to generative LLMs: the number of tokens generated per second (tokens/s) and the number of tokens generated per joule (tokens/J). We compare <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">(1) the performance of the same optimization methods across different hardware platforms</span>, <span class="ltx_text ltx_font_bold" id="S1.p3.1.2">(2) the performance across different hardware platforms</span>, and <span class="ltx_text ltx_font_bold" id="S1.p3.1.3">(3) the performance of different methods on the same hardware platform</span>. This provides a systematic and comprehensive summary of existing inference acceleration work by integrating software optimization methods and hardware platforms.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S1.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of existing LLM surveys</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S1.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.T1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S1.T1.1.1.1.1" rowspan="2"><span class="ltx_text" id="S1.T1.1.1.1.1.1">Survey</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.1.1.2">Generative</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.1.1.3">Software</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="5" id="S1.T1.1.1.1.4">Hardware Platforms</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.1.1.5">Quantitative</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.2.2.1">LLM</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.2.2.2">Optimization</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S1.T1.1.2.2.3">CPU</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S1.T1.1.2.2.4">GPU</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S1.T1.1.2.2.5">FPGA</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S1.T1.1.2.2.6">ASIC</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.2.2.7">PIM/NDP</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.2.2.8">Comparison</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S1.T1.1.3.3.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib73" title="">73</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib74" title="">74</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib75" title="">75</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib76" title="">76</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib77" title="">77</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.3.3.2"><span class="ltx_text" id="S1.T1.1.3.3.2.1" style="color:#00FF00;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.3.3.3"><span class="ltx_text" id="S1.T1.1.3.3.3.1" style="color:#00FF00;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.3.3.4"><span class="ltx_text" id="S1.T1.1.3.3.4.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.3.3.5"><span class="ltx_text" id="S1.T1.1.3.3.5.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.3.3.6"><span class="ltx_text" id="S1.T1.1.3.3.6.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.3.3.7"><span class="ltx_text" id="S1.T1.1.3.3.7.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.3.3.8"><span class="ltx_text" id="S1.T1.1.3.3.8.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.3.3.9"><span class="ltx_text" id="S1.T1.1.3.3.9.1" style="color:#FF0000;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S1.T1.1.4.4.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib78" title="">78</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib79" title="">79</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.4.4.2"><span class="ltx_text" id="S1.T1.1.4.4.2.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.4.4.3"><span class="ltx_text" id="S1.T1.1.4.4.3.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.4.4.4"><span class="ltx_text" id="S1.T1.1.4.4.4.1" style="color:#00FF00;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.4.4.5"><span class="ltx_text" id="S1.T1.1.4.4.5.1" style="color:#00FF00;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.4.4.6"><span class="ltx_text" id="S1.T1.1.4.4.6.1" style="color:#00FF00;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.4.4.7"><span class="ltx_text" id="S1.T1.1.4.4.7.1" style="color:#00FF00;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.4.4.8"><span class="ltx_text" id="S1.T1.1.4.4.8.1" style="color:#00FF00;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.4.4.9"><span class="ltx_text" id="S1.T1.1.4.4.9.1" style="color:#00FF00;">✓</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S1.T1.1.5.5.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib80" title="">80</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.5.5.2"><span class="ltx_text" id="S1.T1.1.5.5.2.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.5.5.3"><span class="ltx_text" id="S1.T1.1.5.5.3.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.5.5.4"><span class="ltx_text" id="S1.T1.1.5.5.4.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.5.5.5"><span class="ltx_text" id="S1.T1.1.5.5.5.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.5.5.6"><span class="ltx_text" id="S1.T1.1.5.5.6.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.5.5.7"><span class="ltx_text" id="S1.T1.1.5.5.7.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.5.5.8"><span class="ltx_text" id="S1.T1.1.5.5.8.1" style="color:#00FF00;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.5.5.9"><span class="ltx_text" id="S1.T1.1.5.5.9.1" style="color:#00FF00;">✓</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S1.T1.1.6.6.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib81" title="">81</a>]</cite></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.6.6.2"><span class="ltx_text" id="S1.T1.1.6.6.2.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.6.6.3"><span class="ltx_text" id="S1.T1.1.6.6.3.1" style="color:#00FF00;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.6.6.4"><span class="ltx_text" id="S1.T1.1.6.6.4.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.6.6.5"><span class="ltx_text" id="S1.T1.1.6.6.5.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.6.6.6"><span class="ltx_text" id="S1.T1.1.6.6.6.1" style="color:#00FF00;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.6.6.7"><span class="ltx_text" id="S1.T1.1.6.6.7.1" style="color:#00FF00;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.6.6.8"><span class="ltx_text" id="S1.T1.1.6.6.8.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.6.6.9"><span class="ltx_text" id="S1.T1.1.6.6.9.1" style="color:#FF0000;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.7.7">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S1.T1.1.7.7.1">Ours</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S1.T1.1.7.7.2"><span class="ltx_text" id="S1.T1.1.7.7.2.1" style="color:#00FF00;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S1.T1.1.7.7.3"><span class="ltx_text" id="S1.T1.1.7.7.3.1" style="color:#00FF00;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S1.T1.1.7.7.4"><span class="ltx_text" id="S1.T1.1.7.7.4.1" style="color:#00FF00;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S1.T1.1.7.7.5"><span class="ltx_text" id="S1.T1.1.7.7.5.1" style="color:#00FF00;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S1.T1.1.7.7.6"><span class="ltx_text" id="S1.T1.1.7.7.6.1" style="color:#00FF00;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S1.T1.1.7.7.7"><span class="ltx_text" id="S1.T1.1.7.7.7.1" style="color:#00FF00;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S1.T1.1.7.7.8"><span class="ltx_text" id="S1.T1.1.7.7.8.1" style="color:#00FF00;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S1.T1.1.7.7.9"><span class="ltx_text" id="S1.T1.1.7.7.9.1" style="color:#00FF00;">✓</span></td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">The article is meticulously structured to comprehensively summarize different optimizations on different hardware platforms for generative LLMs.
Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S2" title="2 Generative LLM Architecture ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_tag">2</span></a> delves into the inference process of LLMs, providing an overview of the architecture and functioning of mainstream generative LLMs.
Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3" title="3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_tag">3</span></a> first summarizes the different optimization methods on various platforms such as CPU, GPU, FPGA, ASIC, and PIM/NDP in tabular form, and then provides a detailed description of each method and related works. Additionally, for each method, we also perform a qualified and quantitative comparison to show the difference among the hardware platforms.
Furthermore, section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S4" title="4 Discussion ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_tag">4</span></a> performs a qualitative and quantitative comparison of inference performance with batch sizes 1 and 8 on different hardware platforms.
And we also point to the future trends and potential developments of generative LLMs and hardware technology for edge-side scenarios.
Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S5" title="5 Conclusion ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_tag">5</span></a> summarizes the work of this survey.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Generative LLM Architecture</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Vanilla Generative LLM</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The most common generative LLM is based on the transformer structure due to its abilities for capturing long-term dependencies&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib82" title="">82</a>]</cite>.
The inference of generative LLM consists of two stages, the prefill stage and the decode stage, as shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S2.F2" title="Figure 2 ‣ 2.1 Vanilla Generative LLM ‣ 2 Generative LLM Architecture ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_tag">2</span></a>.
In the prefill stage, the input text is converted into embeddings and input into the LLM all at once. Each transformer layer performs operations, and the intermediate calculation results (K and V) are saved in the KV cache of the self-attention block for use in the decode stage. After completing calculating the last layer and LM head, the first token is generated.
Then, in the decode stage, the LLM generates each token output autoregressively and updates the KV cache each time until generating the stop string.
In the prefill stage, the attention computation has quadratical computational and storage complexity related with input text length. To address this limitation, many hardware-efficient LLMs focus on more efficient processing of longer length.
For daily application scenarios where the number of input tokens is from 128 to 256 and output tokens larger than 32, we obtain that the time proportion of the decode stage exceeds 80% by profiling Llama2-7B on
single NVIDIA A100 GPU, as shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S2.F2" title="Figure 2 ‣ 2.1 Vanilla Generative LLM ‣ 2 Generative LLM Architecture ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_tag">2</span></a> (c).
Therefore, we summarize these LLMs in sub-section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S2.SS2" title="2.2 Efficient Generative LLM ‣ 2 Generative LLM Architecture ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
And for the decode stage, we summarize hardware optimization methods for different platforms such as CPU, GPU, FPGA, ASIC, and PIM/NDP, and provide inference results for generative LLMs in latter sub-sections.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="308" id="S2.F2.g1" src="https://arxiv.org/html/2410.04466v1/x2.png" width="788">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>LLM inference includes prefill and decode stages. During inference in daily scenarios (input tokens: 128<math alttext="\leq" class="ltx_Math" display="inline" id="S2.F2.4.m1.1"><semantics id="S2.F2.4.m1.1b"><mo id="S2.F2.4.m1.1.1" xref="S2.F2.4.m1.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="S2.F2.4.m1.1c"><leq id="S2.F2.4.m1.1.1.cmml" xref="S2.F2.4.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.4.m1.1d">\leq</annotation><annotation encoding="application/x-llamapun" id="S2.F2.4.m1.1e">≤</annotation></semantics></math>I<math alttext="\leq" class="ltx_Math" display="inline" id="S2.F2.5.m2.1"><semantics id="S2.F2.5.m2.1b"><mo id="S2.F2.5.m2.1.1" xref="S2.F2.5.m2.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="S2.F2.5.m2.1c"><leq id="S2.F2.5.m2.1.1.cmml" xref="S2.F2.5.m2.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.5.m2.1d">\leq</annotation><annotation encoding="application/x-llamapun" id="S2.F2.5.m2.1e">≤</annotation></semantics></math>256, output tokens: O<math alttext="\geq" class="ltx_Math" display="inline" id="S2.F2.6.m3.1"><semantics id="S2.F2.6.m3.1b"><mo id="S2.F2.6.m3.1.1" xref="S2.F2.6.m3.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S2.F2.6.m3.1c"><geq id="S2.F2.6.m3.1.1.cmml" xref="S2.F2.6.m3.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.6.m3.1d">\geq</annotation><annotation encoding="application/x-llamapun" id="S2.F2.6.m3.1e">≥</annotation></semantics></math>32), the time of decode stage is dominant.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Efficient Generative LLM</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Transformer-based LLM</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.3">Transformer-XL&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib83" title="">83</a>]</cite> adopts a segment-level recurrence mechanism and a novel positional encoding scheme to learn dependencies beyond a fixed length without disrupting temporal coherence.
Linear Transformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib84" title="">84</a>]</cite> represents self-attention as a linear dot product of kernel feature maps and alters the computation order by leveraging the associativity of matrix multiplication. This modification reduces the complexity from <math alttext="O(L^{2})" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p1.1.m1.1"><semantics id="S2.SS2.SSS1.p1.1.m1.1a"><mrow id="S2.SS2.SSS1.p1.1.m1.1.1" xref="S2.SS2.SSS1.p1.1.m1.1.1.cmml"><mi id="S2.SS2.SSS1.p1.1.m1.1.1.3" xref="S2.SS2.SSS1.p1.1.m1.1.1.3.cmml">O</mi><mo id="S2.SS2.SSS1.p1.1.m1.1.1.2" xref="S2.SS2.SSS1.p1.1.m1.1.1.2.cmml">⁢</mo><mrow id="S2.SS2.SSS1.p1.1.m1.1.1.1.1" xref="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1.cmml"><mo id="S2.SS2.SSS1.p1.1.m1.1.1.1.1.2" stretchy="false" xref="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1.cmml">(</mo><msup id="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1" xref="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1.cmml"><mi id="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1.2" xref="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1.2.cmml">L</mi><mn id="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1.3" xref="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1.3.cmml">2</mn></msup><mo id="S2.SS2.SSS1.p1.1.m1.1.1.1.1.3" stretchy="false" xref="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.1.m1.1b"><apply id="S2.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1"><times id="S2.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1.2"></times><ci id="S2.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1.3">𝑂</ci><apply id="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1.1.1">superscript</csymbol><ci id="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1.2.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1.2">𝐿</ci><cn id="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1.3.cmml" type="integer" xref="S2.SS2.SSS1.p1.1.m1.1.1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.1.m1.1c">O(L^{2})</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p1.1.m1.1d">italic_O ( italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )</annotation></semantics></math> to <math alttext="O(L)" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p1.2.m2.1"><semantics id="S2.SS2.SSS1.p1.2.m2.1a"><mrow id="S2.SS2.SSS1.p1.2.m2.1.2" xref="S2.SS2.SSS1.p1.2.m2.1.2.cmml"><mi id="S2.SS2.SSS1.p1.2.m2.1.2.2" xref="S2.SS2.SSS1.p1.2.m2.1.2.2.cmml">O</mi><mo id="S2.SS2.SSS1.p1.2.m2.1.2.1" xref="S2.SS2.SSS1.p1.2.m2.1.2.1.cmml">⁢</mo><mrow id="S2.SS2.SSS1.p1.2.m2.1.2.3.2" xref="S2.SS2.SSS1.p1.2.m2.1.2.cmml"><mo id="S2.SS2.SSS1.p1.2.m2.1.2.3.2.1" stretchy="false" xref="S2.SS2.SSS1.p1.2.m2.1.2.cmml">(</mo><mi id="S2.SS2.SSS1.p1.2.m2.1.1" xref="S2.SS2.SSS1.p1.2.m2.1.1.cmml">L</mi><mo id="S2.SS2.SSS1.p1.2.m2.1.2.3.2.2" stretchy="false" xref="S2.SS2.SSS1.p1.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.2.m2.1b"><apply id="S2.SS2.SSS1.p1.2.m2.1.2.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.2"><times id="S2.SS2.SSS1.p1.2.m2.1.2.1.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.2.1"></times><ci id="S2.SS2.SSS1.p1.2.m2.1.2.2.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.2.2">𝑂</ci><ci id="S2.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.1">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.2.m2.1c">O(L)</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p1.2.m2.1d">italic_O ( italic_L )</annotation></semantics></math>, where <math alttext="L" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p1.3.m3.1"><semantics id="S2.SS2.SSS1.p1.3.m3.1a"><mi id="S2.SS2.SSS1.p1.3.m3.1.1" xref="S2.SS2.SSS1.p1.3.m3.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.3.m3.1b"><ci id="S2.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S2.SS2.SSS1.p1.3.m3.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.3.m3.1c">L</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p1.3.m3.1d">italic_L</annotation></semantics></math> is the context length, significantly accelerating the computation of autoregressive Transformers.
Another efficient structure is the Attention-Free Transformer (AFT)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib85" title="">85</a>]</cite>. Unlike vanilla transformers, which first compute the query-key product, AFT combines the key and value with a set of learned positional biases before performing element-wise multiplication with the query. As a result, the memory complexity of AFT is linear with respect to both the context size and feature dimensions, enabling support for larger input lengths and model sizes.
Based on AFT, the Receptance Weighted Key Value (RWKV)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib86" title="">86</a>]</cite> combines the efficient parallel training capabilities of Transformers with the efficient inference of RNNs. It leverages linear attention mechanisms and allows the model to be expressed as either a transformer or an RNN.
It also enables parallel computation during training while maintaining constant computational and memory complexity during inference.
DiJiang&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib87" title="">87</a>]</cite> introduces a novel frequency-domain kernelization method based on the Discrete Cosine Transform (DCT). It points out that improving attention mechanisms often requires extensive retraining, which is impractical for large language models with vast numbers of parameters. This approach enables the conversion of a pre-trained standard Transformer into a model with linear complexity and low training costs, utilizing a weighted quasi-Monte Carlo method for sampling.
Extensive experiments demonstrate that this method achieves performance comparable to the vanilla transformer while significantly reducing training costs and substantially increasing inference speed.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>SSM-based LLM</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.3">State Space Model (SSM) defines a linear mapping from an input <math alttext="x" class="ltx_Math" display="inline" id="S2.SS2.SSS2.p1.1.m1.1"><semantics id="S2.SS2.SSS2.p1.1.m1.1a"><mi id="S2.SS2.SSS2.p1.1.m1.1.1" xref="S2.SS2.SSS2.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.1.m1.1b"><ci id="S2.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS2.p1.1.m1.1d">italic_x</annotation></semantics></math> to output <math alttext="y" class="ltx_Math" display="inline" id="S2.SS2.SSS2.p1.2.m2.1"><semantics id="S2.SS2.SSS2.p1.2.m2.1a"><mi id="S2.SS2.SSS2.p1.2.m2.1.1" xref="S2.SS2.SSS2.p1.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.2.m2.1b"><ci id="S2.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS2.p1.2.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.2.m2.1c">y</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS2.p1.2.m2.1d">italic_y</annotation></semantics></math> through a hidden state <math alttext="h" class="ltx_Math" display="inline" id="S2.SS2.SSS2.p1.3.m3.1"><semantics id="S2.SS2.SSS2.p1.3.m3.1a"><mi id="S2.SS2.SSS2.p1.3.m3.1.1" xref="S2.SS2.SSS2.p1.3.m3.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.3.m3.1b"><ci id="S2.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S2.SS2.SSS2.p1.3.m3.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.3.m3.1c">h</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS2.p1.3.m3.1d">italic_h</annotation></semantics></math>:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{split}h&amp;=A\times h+B\times x\\
y&amp;=C\times h\end{split}" class="ltx_Math" display="block" id="S2.E1.m1.14"><semantics id="S2.E1.m1.14a"><mtable columnspacing="0pt" displaystyle="true" id="S2.E1.m1.14.14" rowspacing="0pt" xref="S2.E1.m1.14.15.1.cmml"><mtr id="S2.E1.m1.14.14a" xref="S2.E1.m1.14.15.1.cmml"><mtd class="ltx_align_right" columnalign="right" id="S2.E1.m1.14.14b" xref="S2.E1.m1.14.15.1.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml">h</mi></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E1.m1.14.14c" xref="S2.E1.m1.14.15.1.cmml"><mrow id="S2.E1.m1.9.9.9.9.8" xref="S2.E1.m1.14.15.1.cmml"><mi id="S2.E1.m1.9.9.9.9.8.9" xref="S2.E1.m1.14.15.1a.cmml"></mi><mo id="S2.E1.m1.2.2.2.2.1.1" xref="S2.E1.m1.2.2.2.2.1.1.cmml">=</mo><mrow id="S2.E1.m1.9.9.9.9.8.10" xref="S2.E1.m1.14.15.1.cmml"><mrow id="S2.E1.m1.9.9.9.9.8.10.1" xref="S2.E1.m1.14.15.1.cmml"><mi id="S2.E1.m1.3.3.3.3.2.2" xref="S2.E1.m1.3.3.3.3.2.2.cmml">A</mi><mo id="S2.E1.m1.4.4.4.4.3.3" lspace="0.222em" rspace="0.222em" xref="S2.E1.m1.4.4.4.4.3.3.cmml">×</mo><mi id="S2.E1.m1.5.5.5.5.4.4" xref="S2.E1.m1.5.5.5.5.4.4.cmml">h</mi></mrow><mo id="S2.E1.m1.6.6.6.6.5.5" xref="S2.E1.m1.6.6.6.6.5.5.cmml">+</mo><mrow id="S2.E1.m1.9.9.9.9.8.10.2" xref="S2.E1.m1.14.15.1.cmml"><mi id="S2.E1.m1.7.7.7.7.6.6" xref="S2.E1.m1.7.7.7.7.6.6.cmml">B</mi><mo id="S2.E1.m1.8.8.8.8.7.7" lspace="0.222em" rspace="0.222em" xref="S2.E1.m1.8.8.8.8.7.7.cmml">×</mo><mi id="S2.E1.m1.9.9.9.9.8.8" xref="S2.E1.m1.9.9.9.9.8.8.cmml">x</mi></mrow></mrow></mrow></mtd></mtr><mtr id="S2.E1.m1.14.14d" xref="S2.E1.m1.14.15.1.cmml"><mtd class="ltx_align_right" columnalign="right" id="S2.E1.m1.14.14e" xref="S2.E1.m1.14.15.1.cmml"><mi id="S2.E1.m1.10.10.10.1.1.1" xref="S2.E1.m1.10.10.10.1.1.1.cmml">y</mi></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E1.m1.14.14f" xref="S2.E1.m1.14.15.1.cmml"><mrow id="S2.E1.m1.14.14.14.5.4" xref="S2.E1.m1.14.15.1.cmml"><mi id="S2.E1.m1.14.14.14.5.4.5" xref="S2.E1.m1.14.15.1a.cmml"></mi><mo id="S2.E1.m1.11.11.11.2.1.1" xref="S2.E1.m1.11.11.11.2.1.1.cmml">=</mo><mrow id="S2.E1.m1.14.14.14.5.4.6" xref="S2.E1.m1.14.15.1.cmml"><mi id="S2.E1.m1.12.12.12.3.2.2" xref="S2.E1.m1.12.12.12.3.2.2.cmml">C</mi><mo id="S2.E1.m1.13.13.13.4.3.3" lspace="0.222em" rspace="0.222em" xref="S2.E1.m1.13.13.13.4.3.3.cmml">×</mo><mi id="S2.E1.m1.14.14.14.5.4.4" xref="S2.E1.m1.14.14.14.5.4.4.cmml">h</mi></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S2.E1.m1.14b"><apply id="S2.E1.m1.14.15.1.cmml" xref="S2.E1.m1.14.14"><and id="S2.E1.m1.14.15.1a.cmml" xref="S2.E1.m1.9.9.9.9.8.9"></and><apply id="S2.E1.m1.14.15.1b.cmml" xref="S2.E1.m1.14.14"><eq id="S2.E1.m1.2.2.2.2.1.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1"></eq><ci id="S2.E1.m1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1">ℎ</ci><apply id="S2.E1.m1.14.15.1.4.cmml" xref="S2.E1.m1.14.14"><plus id="S2.E1.m1.6.6.6.6.5.5.cmml" xref="S2.E1.m1.6.6.6.6.5.5"></plus><apply id="S2.E1.m1.14.15.1.4.2.cmml" xref="S2.E1.m1.14.14"><times id="S2.E1.m1.4.4.4.4.3.3.cmml" xref="S2.E1.m1.4.4.4.4.3.3"></times><ci id="S2.E1.m1.3.3.3.3.2.2.cmml" xref="S2.E1.m1.3.3.3.3.2.2">𝐴</ci><ci id="S2.E1.m1.5.5.5.5.4.4.cmml" xref="S2.E1.m1.5.5.5.5.4.4">ℎ</ci></apply><apply id="S2.E1.m1.14.15.1.4.3.cmml" xref="S2.E1.m1.14.14"><times id="S2.E1.m1.14.15.1.4.3.1.cmml" xref="S2.E1.m1.9.9.9.9.8.9"></times><apply id="S2.E1.m1.14.15.1.4.3.2.cmml" xref="S2.E1.m1.14.14"><times id="S2.E1.m1.8.8.8.8.7.7.cmml" xref="S2.E1.m1.8.8.8.8.7.7"></times><ci id="S2.E1.m1.7.7.7.7.6.6.cmml" xref="S2.E1.m1.7.7.7.7.6.6">𝐵</ci><ci id="S2.E1.m1.9.9.9.9.8.8.cmml" xref="S2.E1.m1.9.9.9.9.8.8">𝑥</ci></apply><ci id="S2.E1.m1.10.10.10.1.1.1.cmml" xref="S2.E1.m1.10.10.10.1.1.1">𝑦</ci></apply></apply></apply><apply id="S2.E1.m1.14.15.1c.cmml" xref="S2.E1.m1.14.14"><eq id="S2.E1.m1.11.11.11.2.1.1.cmml" xref="S2.E1.m1.11.11.11.2.1.1"></eq><share href="https://arxiv.org/html/2410.04466v1#S2.E1.m1.14.15.1.4.cmml" id="S2.E1.m1.14.15.1d.cmml" xref="S2.E1.m1.9.9.9.9.8.9"></share><apply id="S2.E1.m1.14.15.1.6.cmml" xref="S2.E1.m1.14.14"><times id="S2.E1.m1.13.13.13.4.3.3.cmml" xref="S2.E1.m1.13.13.13.4.3.3"></times><ci id="S2.E1.m1.12.12.12.3.2.2.cmml" xref="S2.E1.m1.12.12.12.3.2.2">𝐶</ci><ci id="S2.E1.m1.14.14.14.5.4.4.cmml" xref="S2.E1.m1.14.14.14.5.4.4">ℎ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.14c">\begin{split}h&amp;=A\times h+B\times x\\
y&amp;=C\times h\end{split}</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.14d">start_ROW start_CELL italic_h end_CELL start_CELL = italic_A × italic_h + italic_B × italic_x end_CELL end_ROW start_ROW start_CELL italic_y end_CELL start_CELL = italic_C × italic_h end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS2.SSS2.p1.8">where <math alttext="A" class="ltx_Math" display="inline" id="S2.SS2.SSS2.p1.4.m1.1"><semantics id="S2.SS2.SSS2.p1.4.m1.1a"><mi id="S2.SS2.SSS2.p1.4.m1.1.1" xref="S2.SS2.SSS2.p1.4.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.4.m1.1b"><ci id="S2.SS2.SSS2.p1.4.m1.1.1.cmml" xref="S2.SS2.SSS2.p1.4.m1.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.4.m1.1c">A</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS2.p1.4.m1.1d">italic_A</annotation></semantics></math> is the state matrix, <math alttext="B" class="ltx_Math" display="inline" id="S2.SS2.SSS2.p1.5.m2.1"><semantics id="S2.SS2.SSS2.p1.5.m2.1a"><mi id="S2.SS2.SSS2.p1.5.m2.1.1" xref="S2.SS2.SSS2.p1.5.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.5.m2.1b"><ci id="S2.SS2.SSS2.p1.5.m2.1.1.cmml" xref="S2.SS2.SSS2.p1.5.m2.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.5.m2.1c">B</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS2.p1.5.m2.1d">italic_B</annotation></semantics></math> is the input matrix, and <math alttext="C" class="ltx_Math" display="inline" id="S2.SS2.SSS2.p1.6.m3.1"><semantics id="S2.SS2.SSS2.p1.6.m3.1a"><mi id="S2.SS2.SSS2.p1.6.m3.1.1" xref="S2.SS2.SSS2.p1.6.m3.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.6.m3.1b"><ci id="S2.SS2.SSS2.p1.6.m3.1.1.cmml" xref="S2.SS2.SSS2.p1.6.m3.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.6.m3.1c">C</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS2.p1.6.m3.1d">italic_C</annotation></semantics></math> is the output matrix.
In generative LLMs, SSM can understand and compress the input text into hidden states, and then generate output text based on these states.
The Structured State Space Sequence Model (S4)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib88" title="">88</a>]</cite> involves conditioning the matrix <math alttext="A" class="ltx_Math" display="inline" id="S2.SS2.SSS2.p1.7.m4.1"><semantics id="S2.SS2.SSS2.p1.7.m4.1a"><mi id="S2.SS2.SSS2.p1.7.m4.1.1" xref="S2.SS2.SSS2.p1.7.m4.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.7.m4.1b"><ci id="S2.SS2.SSS2.p1.7.m4.1.1.cmml" xref="S2.SS2.SSS2.p1.7.m4.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.7.m4.1c">A</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS2.p1.7.m4.1d">italic_A</annotation></semantics></math> with low-rank corrections, enabling it to be stably diagonalized, and simplifying the SSM to computations that involve an in-depth exploration of the Cauchy kernel.
It offers significantly higher computational efficiency compared to previous methods while retaining its theoretical advantages.
The Gated State Space Model (GSS)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib89" title="">89</a>]</cite> is
built on the effectiveness of gated activation functions. GSS demonstrates significantly faster training speeds on TPUs compared to S4, and it competes effectively with several Transformer-based LLMs.
Hyena&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib90" title="">90</a>]</cite> addresses that existing sub-quadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match the performance of transformers.
Therefore, it introduces a sub-quadratic direct replacement for attention, constructed using interleaved implicit parameterized long convolutions and data-controlled gating.
Hyena can improve accuracy by over 50 points compared to operators relying on state space models and other implicit and explicit methods.
Due to the inability to perform content-based reasoning for linear attention, gated convolution, recurrent models, and S4, Mamba&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib43" title="">43</a>]</cite> makes the SSM parameters a function of the input and enables the model to selectively propagate or forget information along the sequence length dimension based on the current token.
Additionally, a hardware-aware parallel algorithm was designed for the recurrent mode, enhancing computational efficiency.
DenseSSM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib91" title="">91</a>]</cite> enhance Mamba by selectively integrating shallow layer hidden states into deeper layers. Despite the dense connections, DenseSSM maintains both training parallelism and inference efficiency.
Mamba2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib44" title="">44</a>]</cite> demonstrates that transformer and SSM model families are closely related through various decompositions of a well-studied class of structured quasi-separable matrices. Mamba2 also introduces the State Space Duality (SSD) framework, with its core layer being an improved version of the selective SSM used in Mamba and offering a 2-8<math alttext="\times" class="ltx_Math" display="inline" id="S2.SS2.SSS2.p1.8.m5.1"><semantics id="S2.SS2.SSS2.p1.8.m5.1a"><mo id="S2.SS2.SSS2.p1.8.m5.1.1" xref="S2.SS2.SSS2.p1.8.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.8.m5.1b"><times id="S2.SS2.SSS2.p1.8.m5.1.1.cmml" xref="S2.SS2.SSS2.p1.8.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.8.m5.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS2.p1.8.m5.1d">×</annotation></semantics></math> speedup.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>Hybrid LLM</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS3.p1">
<p class="ltx_p" id="S2.SS2.SSS3.p1.1">Some other LLMs integrate transformer-based and SSM-based LLMs, leveraging the complete information extraction ability of attention and the information compression capability of SSM to enhance the performance for long inputs.
The Block-State-Transformer (BST)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib92" title="">92</a>]</cite> integrates an SSM sublayer for long-range contextualization with a block-transformer sublayer for short-term sequence representation. This architecture combines the strengths of SSMs and block attention, and explores three distinct, fully parallelizable variants.
Griffin&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib93" title="">93</a>]</cite> combines gated linear recurrence with local attention, featuring the Hawk layer (a type of RNN with gated linear recurrence).
Jamba&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib94" title="">94</a>]</cite> interleaves blocks of transformer and Mamba layers, harnessing the strengths of both model families. In some of these layers, mixture of expert (MoE) is added to increase model capacity while keeping the number of active parameters manageable.
Unlike BST, Infini-Transformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib95" title="">95</a>]</cite> combines masked local attention and long-term linear attention within a single Transformer block.
This Infini-Attention mechanism incorporates compressed memory into the original attention mechanism within the constraints of limited memory and computational resources.
MEGALODON&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib96" title="">96</a>]</cite> is a neural architecture designed for efficient sequence modeling with infinite context length. It builds on the MEGA architecture (exponential moving average with gated attention) and introduces complex exponential moving average (CEMA), timestep normalization layer, normalized attention mechanism, and a pre-norm configuration with two-hop residuals to enhance its capability and stability.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Optimizations on Hardware Platforms</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we provide an overview of the hardware platforms and various optimization techniques used in LLM inference. As shown in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.T2" title="Table 2 ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_tag">2</span></a>, the hardware platforms include CPU, GPU, FPGA, ASIC, and PIM/NDP, while the optimization methods include quantization, sparsity, fast decoding, operator optimization, heterogeneous cooperation, and homogeneous cooperation.
In the following sections, we will provide a detailed explanation of the principles of each optimization method and related works, followed by a qualified and quantitative comparison.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Existing generative LLM inference optimizations on different hardware platforms</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.2.1.1.1">
<span class="ltx_p" id="S3.T2.1.2.1.1.1.1" style="width:51.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.2.1.1.1.1.1">Methods</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.2.1.2.1">
<span class="ltx_p" id="S3.T2.1.2.1.2.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.2.1.2.1.1.1">CPU</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.2.1.3.1">
<span class="ltx_p" id="S3.T2.1.2.1.3.1.1" style="width:113.8pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.2.1.3.1.1.1">GPU</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.2.1.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.2.1.4.1">
<span class="ltx_p" id="S3.T2.1.2.1.4.1.1" style="width:65.4pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.2.1.4.1.1.1">FPGA</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.2.1.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.2.1.5.1">
<span class="ltx_p" id="S3.T2.1.2.1.5.1.1" style="width:65.4pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.2.1.5.1.1.1">ASIC</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.2.1.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.2.1.6.1">
<span class="ltx_p" id="S3.T2.1.2.1.6.1.1" style="width:65.4pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.2.1.6.1.1.1">PIM/NDP</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.3.2.1.1">
<span class="ltx_p" id="S3.T2.1.3.2.1.1.1" style="width:51.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.3.2.1.1.1.1">Quantization</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.3.2.2.1">
<span class="ltx_p" id="S3.T2.1.3.2.2.1.1" style="width:56.9pt;">Shen et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib97" title="">97</a>]</cite>, T-MAC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib98" title="">98</a>]</cite>, Snapdragon 8 Gen3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib65" title="">65</a>]</cite>, llama.cpp&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib99" title="">99</a>]</cite>, NoMAD-Attention&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib100" title="">100</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.3.2.3.1">
<span class="ltx_p" id="S3.T2.1.3.2.3.1.1" style="width:113.8pt;">GPTQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib101" title="">101</a>]</cite>, AWQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib102" title="">102</a>]</cite>, SpQR&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib103" title="">103</a>]</cite>, SqueezeLLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib104" title="">104</a>]</cite>, LLM-MQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib105" title="">105</a>]</cite>, APTQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib106" title="">106</a>]</cite>, Li et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib107" title="">107</a>]</cite>, LUT-GEMM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib108" title="">108</a>]</cite>, FLUTE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib109" title="">109</a>]</cite>, FP6-LLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib110" title="">110</a>]</cite>, LLM.int8&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib111" title="">111</a>]</cite>, SmoothQuant&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib112" title="">112</a>]</cite>, QUIK&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib113" title="">113</a>]</cite>, Atom&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib114" title="">114</a>]</cite>, LLM-FP4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib115" title="">115</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.3.2.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.3.2.4.1">
<span class="ltx_p" id="S3.T2.1.3.2.4.1.1" style="width:65.4pt;">FlexRun&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib116" title="">116</a>]</cite>, HLSTransform&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib117" title="">117</a>]</cite>, SECDA-LLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib118" title="">118</a>]</cite>, Chen et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib119" title="">119</a>]</cite>, FlightLLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib120" title="">120</a>]</cite>, EdgeLLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib121" title="">121</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.3.2.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.3.2.5.1">
<span class="ltx_p" id="S3.T2.1.3.2.5.1.1" style="width:65.4pt;">FIGNA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib122" title="">122</a>]</cite>, MECLA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib123" title="">123</a>]</cite>, OliVe&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib124" title="">124</a>]</cite>, Li et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib125" title="">125</a>]</cite>, Tender&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib126" title="">126</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.3.2.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.3.2.6.1">
<span class="ltx_p" id="S3.T2.1.3.2.6.1.1" style="width:65.4pt;">Guo et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib127" title="">127</a>]</cite>, TransPIM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib128" title="">128</a>]</cite>, Sharda et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib129" title="">129</a>]</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.2.1">
<span class="ltx_p" id="S3.T2.1.1.2.1.1" style="width:51.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.2.1.1.1">Sparsity</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.3.1">
<span class="ltx_p" id="S3.T2.1.1.3.1.1" style="width:56.9pt;">Turbo Sparse&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib130" title="">130</a>]</cite>, ProSparse&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib131" title="">131</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.1.1">
<span class="ltx_p" id="S3.T2.1.1.1.1.1" style="width:113.8pt;">LLM-pruner&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib132" title="">132</a>]</cite>, SparseGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib133" title="">133</a>]</cite>, Wanda&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib134" title="">134</a>]</cite>, E-Sparse&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib135" title="">135</a>]</cite>, Flash-LLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib136" title="">136</a>]</cite>, Agarwalla et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib137" title="">137</a>]</cite>, DejaVu&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib138" title="">138</a>]</cite>, Sparse Transformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib139" title="">139</a>]</cite>, Bigbird&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib140" title="">140</a>]</cite>, StreamingLLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib141" title="">141</a>]</cite>, Longformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib142" title="">142</a>]</cite>, Adaptively Sparse Attention&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib143" title="">143</a>]</cite>, Reformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib144" title="">144</a>]</cite>, Sparse Flash Attention&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib145" title="">145</a>]</cite>, Sparse Sinkhorn Attention&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib146" title="">146</a>]</cite>, H<sub class="ltx_sub" id="S3.T2.1.1.1.1.1.1">2</sub>O&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib147" title="">147</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.4.1">
<span class="ltx_p" id="S3.T2.1.1.4.1.1" style="width:65.4pt;">FlightLLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib120" title="">120</a>]</cite>, EdgeLLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib121" title="">121</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.1.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.5.1">
<span class="ltx_p" id="S3.T2.1.1.5.1.1" style="width:65.4pt;">Spatten&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib148" title="">148</a>]</cite>, TF-MVP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib149" title="">149</a>]</cite>, SOFA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib150" title="">150</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.1.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.6.1">
<span class="ltx_p" id="S3.T2.1.1.6.1.1" style="width:65.4pt;">LauWS&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib151" title="">151</a>]</cite>, HARDSEA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib152" title="">152</a>]</cite>, Sharda et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib129" title="">129</a>]</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.4.3.1.1">
<span class="ltx_p" id="S3.T2.1.4.3.1.1.1" style="width:51.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.4.3.1.1.1.1">Fast Decoding</span></span>
</span>
</td>
<td class="ltx_td ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.4.3.2"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.4.3.3.1">
<span class="ltx_p" id="S3.T2.1.4.3.3.1.1" style="width:113.8pt;">LLMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib153" title="">153</a>]</cite>, Speculative decoding&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib154" title="">154</a>]</cite>, Lookahead&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib155" title="">155</a>]</cite>, Medusa&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib156" title="">156</a>]</cite>, EAGLE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib157" title="">157</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib158" title="">158</a>]</cite>, Ouroboros&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib159" title="">159</a>]</cite>, Sequoia&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib160" title="">160</a>]</cite>, Draft&amp;Verify&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib161" title="">161</a>]</cite>, Kangaroo&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib162" title="">162</a>]</cite>, LayerSkip&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib163" title="">163</a>]</cite>, Adainfer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib164" title="">164</a>]</cite>, RAEE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib165" title="">165</a>]</cite>, MOD&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib166" title="">166</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.4.3.4"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.4.3.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.4.3.5.1">
<span class="ltx_p" id="S3.T2.1.4.3.5.1.1" style="width:65.4pt;">C-Transformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib167" title="">167</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.4.3.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.4.3.6.1">
<span class="ltx_p" id="S3.T2.1.4.3.6.1.1" style="width:65.4pt;">SpecPIM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib168" title="">168</a>]</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.5.4.1.1">
<span class="ltx_p" id="S3.T2.1.5.4.1.1.1" style="width:51.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.5.4.1.1.1.1">Operator Optimization</span></span>
</span>
</td>
<td class="ltx_td ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.5.4.2"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.5.4.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.5.4.3.1">
<span class="ltx_p" id="S3.T2.1.5.4.3.1.1" style="width:113.8pt;">FlashAttention&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib169" title="">169</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib170" title="">170</a>]</cite>, FlashDecoding&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib171" title="">171</a>]</cite>, FlashDecoding++&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib172" title="">172</a>]</cite>, DeepSpeed&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib173" title="">173</a>]</cite>, vLLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib174" title="">174</a>]</cite>, OpenPPL&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib175" title="">175</a>]</cite>, cuBLAS&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib176" title="">176</a>]</cite>, TensorRT-LLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib177" title="">177</a>]</cite>, CUTLASS&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib178" title="">178</a>]</cite>, ByteTransformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib179" title="">179</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.5.4.4"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.5.4.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.5.4.5.1">
<span class="ltx_p" id="S3.T2.1.5.4.5.1.1" style="width:65.4pt;">LPU&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib180" title="">180</a>]</cite>, Groq LPU&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib181" title="">181</a>]</cite>, ConSmax&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib182" title="">182</a>]</cite>, MARCA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib183" title="">183</a>]</cite>, TCP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib184" title="">184</a>]</cite>, Habana Gaudi&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib185" title="">185</a>]</cite>, Gaudi2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib186" title="">186</a>]</cite>, Gaudi3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib187" title="">187</a>]</cite>, Cerebras WSE-3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib188" title="">188</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.5.4.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.5.4.6.1">
<span class="ltx_p" id="S3.T2.1.5.4.6.1.1" style="width:65.4pt;">PIMnast&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib189" title="">189</a>]</cite>, AttentionLego&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib190" title="">190</a>]</cite>, PIM-GPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib191" title="">191</a>]</cite>, SAL-PIM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib192" title="">192</a>]</cite>, PipePIM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib193" title="">193</a>]</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.6.5.1.1">
<span class="ltx_p" id="S3.T2.1.6.5.1.1.1" style="width:51.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.6.5.1.1.1.1">Heterogeneous Cooperation</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.6.5.2.1">
<span class="ltx_p" id="S3.T2.1.6.5.2.1.1" style="width:56.9pt;">Kim et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib194" title="">194</a>]</cite>, PowerInfer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib195" title="">195</a>]</cite>, PowerInfer-2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib196" title="">196</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.6.5.3"></td>
<td class="ltx_td ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.6.5.4"></td>
<td class="ltx_td ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.6.5.5"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.6.5.6">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.6.5.6.1">
<span class="ltx_p" id="S3.T2.1.6.5.6.1.1" style="width:65.4pt;">NeuPIMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib197" title="">197</a>]</cite>, IANUS&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib198" title="">198</a>]</cite>, MoNDE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib199" title="">199</a>]</cite>, Sharda et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib129" title="">129</a>]</cite>, AttAcc&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib200" title="">200</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib201" title="">201</a>]</cite>, Kang et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib202" title="">202</a>]</cite>, Kim et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib203" title="">203</a>]</cite>, H3D-Transformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib204" title="">204</a>]</cite>, CXL-PNM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib205" title="">205</a>]</cite>, 3D-HI&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib206" title="">206</a>]</cite>, SK Hynix AiMX/AiMX-xPU&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib207" title="">207</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib208" title="">208</a>]</cite>, Cambricon-LLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib209" title="">209</a>]</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.7.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.7.6.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.7.6.1.1">
<span class="ltx_p" id="S3.T2.1.7.6.1.1.1" style="width:51.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.7.6.1.1.1.1">Homogeneous Cooperation</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.7.6.2.1">
<span class="ltx_p" id="S3.T2.1.7.6.2.1.1" style="width:56.9pt;">He et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib210" title="">210</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib211" title="">211</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.7.6.3"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.7.6.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.7.6.4.1">
<span class="ltx_p" id="S3.T2.1.7.6.4.1.1" style="width:65.4pt;">DFX&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib212" title="">212</a>]</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.7.6.5"></td>
<td class="ltx_td ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.7.6.6"></td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Quantization</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Overview</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">Quantization converts the model’s weights and activations from high-precision formats (32-bit floating-point numbers) to low-precision formats (such as 4-bit integers). This process aims to reduce the model’s storage requirements and computational costs while maintaining its accuracy.
From the perspective of data format, quantization includes uniform and non-uniform quantization.
Uniform quantization is a method where the value range is divided into several equal intervals. In uniform quantization, the entire range of values is partitioned into equally sized intervals, with each interval mapped to a discrete representation value. These discrete values are typically represented using fewer bits (e.g., 8 bits). The advantages of uniform quantization include its simplicity and high computational efficiency. However, it may not effectively capture the data distribution characteristics, especially when the data distribution is uneven, potentially leading to significant information loss.
Non-uniform quantization, on the other hand, uses intervals of varying sizes based on the actual data distribution. It divides the data range into different-sized intervals, for example, using smaller intervals in regions where the data distribution is dense and larger intervals where the distribution is sparse. This approach can better preserve the details and features of the data, thus improving the model’s accuracy. Non-uniform quantization typically requires additional computation and storage to manage the quantization intervals, but it provides higher precision and effectiveness in quantization.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS1.p2">
<p class="ltx_p" id="S3.SS1.SSS1.p2.1">Granularity in quantization is crucial for determining model performance and efficiency. The granularity are group-wise, channel-wise, and tensor-wise.
Group-wise granularity is a coarser approach where multiple channels or layers are quantized with the same parameters. This means that within a group, all channels or layers use identical quantization settings. The advantage of group-level granularity is its simplicity and relatively low computational and storage overhead. However, it may not capture the individual characteristics of each channel or layer as effectively, potentially resulting in some compromise in model performance.
Channel-wise granularity involves quantizing each channel individually within the model. Each channel can have its own quantization parameters, allowing for more precise adjustments according to the weight distribution and activation characteristics of each channel. This granularity offers a balance between precision and flexibility, though it increases the complexity of implementation and computation.
Tensor-wise granularity is the most detailed approach, where each tensor (such as weight tensors or activation tensors) is quantized separately. This means that each tensor has its own quantization parameters, enabling the highest degree of adaptation to the specific characteristics of each tensor and providing the best precision. However, this level of granularity comes with the highest computational and storage costs and is the most complex to implement.
There are two main quantization methods: weight-only quantization and weight-activation quantization.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="233" id="S3.F3.g1" src="https://arxiv.org/html/2410.04466v1/x3.png" width="814">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Two main quantization methods: weight-only quantization and weight-activation quantization.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS1.p3">
<p class="ltx_p" id="S3.SS1.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS1.p3.1.1">Weight-Only Quantization.</span>
Weight-only quantization involves converting the model’s weight parameters from high-precision formats (like 32-bit floating-point numbers) to low-precision formats (such as 8-bit integers). This process typically includes discretizing the weights by mapping them to a finite set of discrete values and then representing these values with fewer bits (e.g., 8 bits). This approach significantly reduces storage requirements and accelerates computation. Weight-only quantization can be implemented using methods such as uniform quantization, which divides the weight range into equal intervals, or non-uniform quantization, which adjusts the intervals based on the distribution of weights to better preserve model accuracy. Matrix decomposition quantization is a specialized method where a large matrix is approximated by the product of several smaller matrices. This technique reduces the computational and storage requirements by representing a large matrix with multiple smaller matrices, which can be stored and processed in lower precision formats. This method is particularly beneficial for managing extremely large models, as it helps lower computational complexity and storage overhead.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS1.p4">
<p class="ltx_p" id="S3.SS1.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS1.p4.1.1">Weight-Activation Quantization.</span>
Weight-activation quantization extends the concept of weight-only quantization to include the activations generated during model inference. In this method, both the weights and the activations at each layer are quantized to lower precision formats. This reduces memory bandwidth requirements and enhances inference speed. The challenge with weight-activation quantization is to manage the trade-off between quantization errors and model accuracy. Techniques such as dynamic range quantization or specific quantization schemes are used to balance precision and computational efficiency.
Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.T3" title="Table 3 ‣ 3.1.1 Overview ‣ 3.1 Quantization ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_tag">3</span></a> shows the usage of two quantization methods across different hardware platforms.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Quantization on CPU, GPU, FPGA, ASIC, and PIM/NDP</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.1.1.1">Hardware</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.1.2">Weight-Only Quantization</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.1.3">Weight-Activation Quantization</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.2.2.1">CPU</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.2.2.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.2.2.3">✗</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.3.3.1">GPU</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.3.3.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.3.3.3">✓</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.4.4.1">FPGA</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.4.4.2">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.4.4.3">✓</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.5.5.1">ASIC</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.5.5.2">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.5.5.3">✓</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.6.6.1">PIM/NDP</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T3.1.6.6.2">✗</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T3.1.6.6.3">✓</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>CPU</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.p1.1.1">Weight-Only Quantization.</span>
The optimization methods of quantization on CPUs mainly focus on weight-only quantization.
Shen et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib97" title="">97</a>]</cite> leverage Intel Neural Compressor to automate the INT4 quantization process with negligible accuracy loss, supporting various quantization recipes such as GPTQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib101" title="">101</a>]</cite>, AWQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib102" title="">102</a>]</cite> and TEQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib213" title="">213</a>]</cite>.
They further develop a tensor library tailored for CPUs, which supports mainstream instruction sets like AVX2, AVX512, AVX512_VNNI, and AMX.
By providing INT4 dequantization kernels on x86 CPUs, the experimental results on mainstream LLMs including Llama2, Llama and GPT-NeoX shows the latency of token generation is ranging from 12.5 tokens/s to 50 tokens/s for models with parameters ranging from 6B to 20B, by using a single socket of 4th Generation Intel Xeon Scalable Processors&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib66" title="">66</a>]</cite>.
Qualcomm Snapdragon 8 Gen3 SoC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib65" title="">65</a>]</cite> utilizes its proprietary Hexagon 700 AI processor and quantization techniques to support the efficient LLM execution.
For Llama2-7B with 4-bit quantization, it achieves about 15 tokens/s.
Some open-source repositories like llama.cpp&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib99" title="">99</a>]</cite> are designed for efficient LLM inference across diverse hardware platforms including CPUs, GPUs and ASICs.
For Llama2-7B with 4-bit quantization, llama.cpp achieves 6 tokens/s with a single core and 32 tokens/s with eight cores on Apple M2-Ultra processors.
For Llama2-7B with 2-bit quantization, llama.cpp achieves 4 tokens/s with a single core and 21 tokens/s with eight cores on M2-Ultra.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS2.p2">
<p class="ltx_p" id="S3.SS1.SSS2.p2.1">Due to the overheads of weight dequantization from integer to floating,
T-MAC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib98" title="">98</a>]</cite> leverages lookup tables (LUTs) for efficient low-bit LLM inference on edge CPUs, circumventing the need for dequantization and mixed precision matrix multiplication.
For Llama2-7B with 4-bit quantization, T-MAC achieves 10 tokens/s with a single core and 38 tokens/s with eight cores on Apple M2-Ultra processors&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib64" title="">64</a>]</cite>, and 3 tokens/s on Raspberry Pi 5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib214" title="">214</a>]</cite> integrated with ARM Cortex-A76.
For Llama2-7B with 2-bit quantization, T-MAC achieves 17 tokens/s with a single core and 50 tokens/s with eight cores on M2-Ultra, and 6 tokens/s on Raspberry Pi 5.
Furthermore, due to the vast quantities of expensive Multiply-Add (MAD) matrix operations in the attention computations, NoMAD-Attention&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib100" title="">100</a>]</cite> design an efficient attention algorithm that replaces MAD operations with in-register lookups.
Through hardware-aware algorithmic designs, NoMAD-Attention achieves the computation of attention scores using repeated fast accesses to SIMD registers despite their highly limited sizes.
Empirical evaluations demonstrate that for CodeLlama-7B with 4-bit quantization, NoMAD-Attention achieves 9 tokens/s with short context (e.g. 128) and 4 tokens/s with long context (e.g. 16k) on 2 Intel Xeon E5-2695 V3 14-core CPUs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>GPU</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS3.p1">
<p class="ltx_p" id="S3.SS1.SSS3.p1.3"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS3.p1.3.1">Weight-Only Quantization.</span>
GPTQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib101" title="">101</a>]</cite> is an one-shot weight quantization method based on approximate second-order information and error compensation, that is both highly-accurate and highly-efficient.
It can quantize GPT models with 175 billion parameters in approximately four GPU hours, reducing the bitwidth down to 3-bit or 4-bit per weight, with negligible accuracy degradation relative to the uncompressed baseline.
Experimental results show that the average time of per token of 3-bit OPT-175B model obtained via GPTQ running on a single A100 (80GB) is 14.1 tokens/s, which is about 3.25<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p1.1.m1.1"><semantics id="S3.SS1.SSS3.p1.1.m1.1a"><mo id="S3.SS1.SSS3.p1.1.m1.1.1" xref="S3.SS1.SSS3.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.1.m1.1b"><times id="S3.SS1.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p1.1.m1.1d">×</annotation></semantics></math> faster than the FP16 version (running on 5 GPUs).
On more accessible GPUs, such as the NVIDIA A6000 (48GB), the average time of per token is 7.7 tokens/s (running on 2 GPUs), which is about 4.53<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p1.2.m2.1"><semantics id="S3.SS1.SSS3.p1.2.m2.1a"><mo id="S3.SS1.SSS3.p1.2.m2.1.1" xref="S3.SS1.SSS3.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.2.m2.1b"><times id="S3.SS1.SSS3.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS3.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p1.2.m2.1d">×</annotation></semantics></math> faster than the FP16 version (running on 8 GPUs).
AWQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib102" title="">102</a>]</cite> is based on the observation that protecting 1% of salient weights whose activations are extremely large can greatly reduce quantization error.
It first searches for the optimal per-channel scaling and then multiplies the salient weights with the per-channel scalings.
It also reduces the bitwidth down to 3 or 4 bits per weight.
Experimental results with INT4 implementation show that for Llama-2-7B, it improves the inference speed from 52 tokens/s to 194 tokens/s on RTX 4090 desktop GPU (3.73<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p1.3.m3.1"><semantics id="S3.SS1.SSS3.p1.3.m3.1a"><mo id="S3.SS1.SSS3.p1.3.m3.1.1" xref="S3.SS1.SSS3.p1.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.3.m3.1b"><times id="S3.SS1.SSS3.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS3.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p1.3.m3.1d">×</annotation></semantics></math> speedup).
For Llama-2-13B, the inference speed is 110 tokens/s on RTX 4090 desktop GPU.
On the laptop RTX 4070 GPU (8GB), it is able to run Llama-2-13B models at 33 tokens/s, while the FP16 implementation cannot fit 7B models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS3.p2">
<p class="ltx_p" id="S3.SS1.SSS3.p2.1">To further reduce the accuracy loss for smaller models in the 1-10B parameter range, SpQR&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib103" title="">103</a>]</cite> works by identifying and isolating outlier weights, which cause particularly-large quantization errors, and storing them in higher precision like half data type (16-bit), while compressing all other weights to 3-4 bits, and achieves relative accuracy losses of less than 1% in perplexity for highly-accurate LLaMA and Falcon LLMs.
Experimental results show that SpQR with 3-bit and 16-bit quantization achieves 57 tokens/s, 44 tokens/s, 22 tokens/s and 12 tokens/s on A100 GPU, respectively.
Unlike SpQR, SqueezeLLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib104" title="">104</a>]</cite> proposes a sensitivity-based non-uniform quantization method, which searches for the optimal bit precision assignment based on second-order information.
It also applies dense and sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format.
Experimental results show that SqueezeLLM with 3bit and 16-bit quantization achieves 63.5 tokens/s, 49.2 tokens/s, 29.1 tokens/s and 14.5 tokens/s on A6000 GPU, respectively.
LLM-MQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib105" title="">105</a>]</cite> proposes sensitivity-based precision allocation to assign the proper bitwidth for each layer within the given budget for weight memory based on their first-order information and quantization error.
It also develops an efficient CUDA core kernels to accelerate LLMs by fusing the dequantization and general matrix-vector multiplication (GEMV).
LLM-MQ deploys INT4 quantized Llama2-7B model on NVIDIA T4 GPU achieves up to 1.6<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p2.1.m1.1"><semantics id="S3.SS1.SSS3.p2.1.m1.1a"><mo id="S3.SS1.SSS3.p2.1.m1.1.1" xref="S3.SS1.SSS3.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p2.1.m1.1b"><times id="S3.SS1.SSS3.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS3.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p2.1.m1.1d">×</annotation></semantics></math> end-to-end speedup compared to the pytorch FP16 baseline.
APTQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib106" title="">106</a>]</cite> proposes an attention-aware 2/4-bit mixed-precision quantization for LLMs, which considers not only the second-order information of each layer’s weights, but also, for the first time, the nonlinear effect of attention outputs on the entire model.
Li et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib107" title="">107</a>]</cite> are the first to propose an intra-weight mixed-precision quantization for LLMs to further reduce accuracy loss under 3-bit.
By applying 2/4-bit mixed-precision quantization with memory alignment and exclusive 2-bit sparse outlier reservation with minimum speed degradation, it achieves 2.91-bit for each weight considering all scales/zeros for different models with negligible loss.
Additionally, they design an asynchronous dequantization and fuse the dequantization and GEMV kernels during inference.
For Llama2-7B, it achieves 45.2 tokens/s on RTX 3090 GPU and 34.0 tokens/s on RTX 2080 GPU.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS3.p3">
<p class="ltx_p" id="S3.SS1.SSS3.p3.1">LUT-GEMM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib108" title="">108</a>]</cite> proposes an efficient LUT-based GPU kernel for quantized matrix multiplication, which not only eliminates the resource-intensive dequantization process but also reduces computational costs compared to previous kernels for weight-only quantization.
The impact of LUT-GEMM is facilitated by implementing high compression ratios through low-bit quantization and efficient LUT-based operations.
For Llama-7B with 4-bit quantization, it achieves 163.9 tokens/s on A100 GPU, achieving a remarkable 1.64<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p3.1.m1.1"><semantics id="S3.SS1.SSS3.p3.1.m1.1a"><mo id="S3.SS1.SSS3.p3.1.m1.1.1" xref="S3.SS1.SSS3.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p3.1.m1.1b"><times id="S3.SS1.SSS3.p3.1.m1.1.1.cmml" xref="S3.SS1.SSS3.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p3.1.m1.1d">×</annotation></semantics></math> token generation latency improvement compared to the pytorch FP16 baseline.
FLUTE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib109" title="">109</a>]</cite> is a flexible lookup table engine for LUT-quantized LLMs, which uses offline restructuring of the quantized weight matrix to minimize bit manipulations associated with unpacking, and vectorization and duplication of the lookup table to mitigate shared memory bandwidth constraints.
For Llama3-8B with 4-bit quantization, it achieves 91.3-99.8 tokens/s and 113.7-121.7 tokens/s on NVIDIA A6000 and A100 GPUs, respectively.
For Llama3-8B with 3-bit quantization, it achieves 91.9-110.0 tokens/s and 117.7-135.5 tokens/s on NVIDIA A6000 and A100 GPUs, respectively.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS3.p4">
<p class="ltx_p" id="S3.SS1.SSS3.p4.1">To effectively reduce the size of LLMs and preserve the model accuracy, FP6-LLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib110" title="">110</a>]</cite> proposes FP6 quantization on GPUs with TC-FPx, the first full-stack GPU kernel design scheme with unified Tensor Core support of float-point weights for various quantization bit-width.
It solves the unfriendly memory access of model weights with irregular bit-width and high runtime overhead of weight de-quantization.
Experimental results shows that for Llama2-13B with FP6 quantization, it achieves about 55 tokens/s on NVIDIA A100 GPU.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS3.p5">
<p class="ltx_p" id="S3.SS1.SSS3.p5.5"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS3.p5.5.1">Weight-Activation Quantization.</span>
In addition to hardware units that support FP16 computations, NVIDIA GPUs also provide hardware units that support INT4, INT8, and FP8 computations.
The number of these computation units can be 2<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p5.1.m1.1"><semantics id="S3.SS1.SSS3.p5.1.m1.1a"><mo id="S3.SS1.SSS3.p5.1.m1.1.1" xref="S3.SS1.SSS3.p5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p5.1.m1.1b"><times id="S3.SS1.SSS3.p5.1.m1.1.1.cmml" xref="S3.SS1.SSS3.p5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p5.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p5.1.m1.1d">×</annotation></semantics></math> and 4<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p5.2.m2.1"><semantics id="S3.SS1.SSS3.p5.2.m2.1a"><mo id="S3.SS1.SSS3.p5.2.m2.1.1" xref="S3.SS1.SSS3.p5.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p5.2.m2.1b"><times id="S3.SS1.SSS3.p5.2.m2.1.1.cmml" xref="S3.SS1.SSS3.p5.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p5.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p5.2.m2.1d">×</annotation></semantics></math> greater than FP16 on each chip.
Compared to weight-only quantization, weight-activation quantization can utilize INT4, INT8, and FP8 computations, thereby maximizing the peak computational performance of the GPU.
Since the prefill phase in LLM inference is compute-bound, weight-activation quantization can significantly enhance performance during this stage.
LLM.int8&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib111" title="">111</a>]</cite> uses vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features.
For the outliers, it isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9% of values are multiplied in 8-bit.
For BLOOM-176B model, LLM.int8 achieves 4.05 tokens/s, 30.3 tokens/s and 109.77 tokens/s for batch size 1, 8 and 32, respectively, on 3 A100 GPUs in decode phase.
The inference speed is slightly slower but close to 16-bit inference with less GPU consumption.
SmoothQuant&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib112" title="">112</a>]</cite> enables 8-bit weight and 8-bit activation (W8A8) quantization for LLMs.
Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation.
SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs.
SmoothQuant achieves up to 1.56<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p5.3.m3.1"><semantics id="S3.SS1.SSS3.p5.3.m3.1a"><mo id="S3.SS1.SSS3.p5.3.m3.1.1" xref="S3.SS1.SSS3.p5.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p5.3.m3.1b"><times id="S3.SS1.SSS3.p5.3.m3.1.1.cmml" xref="S3.SS1.SSS3.p5.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p5.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p5.3.m3.1d">×</annotation></semantics></math> speedup and 2<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p5.4.m4.1"><semantics id="S3.SS1.SSS3.p5.4.m4.1a"><mo id="S3.SS1.SSS3.p5.4.m4.1.1" xref="S3.SS1.SSS3.p5.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p5.4.m4.1b"><times id="S3.SS1.SSS3.p5.4.m4.1.1.cmml" xref="S3.SS1.SSS3.p5.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p5.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p5.4.m4.1d">×</annotation></semantics></math> memory reduction for LLMs with negligible loss in accuracy.
QUIK&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib113" title="">113</a>]</cite> is for the first time, that the majority of inference computations for LLMs can be performed with both weights and activations being cast to 4 bits.
QUIK compresses most of the weights and activations to 4-bit, while keeping some outlier weights and activations in higher-precision.
It also provides GPU kernels matching the QUIK format with highly-efficient layer-wise runtimes, which lead to practical end-to-end throughput improvements of up to 3.4<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p5.5.m5.1"><semantics id="S3.SS1.SSS3.p5.5.m5.1a"><mo id="S3.SS1.SSS3.p5.5.m5.1.1" xref="S3.SS1.SSS3.p5.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p5.5.m5.1b"><times id="S3.SS1.SSS3.p5.5.m5.1.1.cmml" xref="S3.SS1.SSS3.p5.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p5.5.m5.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p5.5.m5.1d">×</annotation></semantics></math> relative to FP16 execution in prefill phase.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS3.p6">
<p class="ltx_p" id="S3.SS1.SSS3.p6.2">Prevalent quantization schemes (e.g., W8A8) cannot fully leverage the capabilities of modern GPUs, such as 4-bit integer operators, resulting in sub-optimal performance.
To maximize the throughput, Atom&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib114" title="">114</a>]</cite> significantly boosts serving throughput by using low-bit operators and considerably reduces memory consumption via low-bit quantization.
It attains high accuracy by applying a novel mixed-precision and fine-grained quantization process.
For single batch inference, Atom can achieve about 30 tokens/s for on a NVIDIA RTX 4090 GPU.
Atom improves end-to-end throughput by up to 7.73<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p6.1.m1.1"><semantics id="S3.SS1.SSS3.p6.1.m1.1a"><mo id="S3.SS1.SSS3.p6.1.m1.1.1" xref="S3.SS1.SSS3.p6.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p6.1.m1.1b"><times id="S3.SS1.SSS3.p6.1.m1.1.1.cmml" xref="S3.SS1.SSS3.p6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p6.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p6.1.m1.1d">×</annotation></semantics></math> compared to the FP16 and by 2.53<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.SSS3.p6.2.m2.1"><semantics id="S3.SS1.SSS3.p6.2.m2.1a"><mo id="S3.SS1.SSS3.p6.2.m2.1.1" xref="S3.SS1.SSS3.p6.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p6.2.m2.1b"><times id="S3.SS1.SSS3.p6.2.m2.1.1.cmml" xref="S3.SS1.SSS3.p6.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p6.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS3.p6.2.m2.1d">×</annotation></semantics></math> compared to INT8 quantization, while maintaining the same latency target.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS3.p7">
<p class="ltx_p" id="S3.SS1.SSS3.p7.1">Compared to integer quantization, floating-point (FP) quantization can better handle long-tail or bell-shaped distributions, and it has emerged as a default choice in many hardware platforms.
LLM-FP4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib115" title="">115</a>]</cite> quantizes both weights and activations in LLMs down to 4-bit floating-point values (W4A4) with negligible accuracy loss.
Due to the lack of PF4 computing unit in GPUs, its decoding speed maybe slower than FP16 baseline.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.4 </span>FPGA</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS4.p1">
<p class="ltx_p" id="S3.SS1.SSS4.p1.3"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS4.p1.3.1">Weight-Activation Quantization.</span>
FlexRun&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib116" title="">116</a>]</cite> uses 8-bit quantization (W8A8), conducts an in-depth design space exploration to find the best accelerator architecture for a target LLM model, and automatically reconfigures the accelerator based on the exploration results.
With the implementation on Intel Stratix 10 GX and MX FPGAs, FlexRun outperforms the current state-of-the-art FPGA-based accelerator by 1.15<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.SSS4.p1.1.m1.1"><semantics id="S3.SS1.SSS4.p1.1.m1.1a"><mo id="S3.SS1.SSS4.p1.1.m1.1.1" xref="S3.SS1.SSS4.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS4.p1.1.m1.1b"><times id="S3.SS1.SSS4.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS4.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS4.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS4.p1.1.m1.1d">×</annotation></semantics></math>–1.50<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.SSS4.p1.2.m2.1"><semantics id="S3.SS1.SSS4.p1.2.m2.1a"><mo id="S3.SS1.SSS4.p1.2.m2.1.1" xref="S3.SS1.SSS4.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS4.p1.2.m2.1b"><times id="S3.SS1.SSS4.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS4.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS4.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS4.p1.2.m2.1d">×</annotation></semantics></math> for GPT2, respectively.
Compared to Nvidia’s V100 GPU, FlexRun achieves 2.69<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.SSS4.p1.3.m3.1"><semantics id="S3.SS1.SSS4.p1.3.m3.1a"><mo id="S3.SS1.SSS4.p1.3.m3.1.1" xref="S3.SS1.SSS4.p1.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS4.p1.3.m3.1b"><times id="S3.SS1.SSS4.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS4.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS4.p1.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS4.p1.3.m3.1d">×</annotation></semantics></math> higher performance on average for various GPT2 models.
HLSTransform&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib117" title="">117</a>]</cite> uses HLS to design a FPGA accelerator and synthesis combined with pipelining, memory unrolling, and memory partitioning and transfer optimizations, with the addition of 8-bit integer quantization (W8A8).
On a tiny model with 110 million parameters, HLSTransform achieves 57.11 tokens/s on Xilinx Virtex UltraScale+ VU9P FPGA.
SECDA-LLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib118" title="">118</a>]</cite> utilizes quantization (W3A8) and designs an efficient FPGA-based LLM accelerators for the llama.cpp inference framework.
By deploying on the PYNQ-Z1 board, it achieves 0.588 tokens/s for the TinyLlama model (1.1B).
Chen et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib119" title="">119</a>]</cite> investigate the feasibility and potential of model-speciic spatial acceleration for LLM inference on FPGAs.
They introduce a comprehensive analytical model to estimate the LLM inference performance of FPGA accelerator with W4A8 quantization, and provide a library of high-level synthesis (HLS) kernels that are composable and reusable.
For Llama2-7B, during prefilling phase, they can achieves about 213 tokens/s, 43 tokens/s, and 320 tokens/s on Xilinx Alveo U280, VCK5000, and VHK158 FPGAs, respectively.
During decode stage, they can achieves about 200 tokens/s, 40 tokens/s, and 333 tokens/s on Xilinx Alveo U280, VCK5000, and VHK158 FPGAs, respectively.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.5 </span>ASIC</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS5.p1">
<p class="ltx_p" id="S3.SS1.SSS5.p1.4"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS5.p1.4.1">Weight-Only Quantization.</span>
Despite the memory footprint reduction achieved by weight-only quantization, the actual computing performance is not really improved due to dequantization from integer to float.
FIGNA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib122" title="">122</a>]</cite> proposes dedicated FP-INT arithmetic units designed specifically for FP-INT MAC operations and integrates them on the accelerator.
FIGNA with FP16-INT4 provides 3.2768 TOPS computing power and 26.58W power consumption by considering all memory access in 28nm at 100MHz.
Estimated result shows that for OPT-6.7B it can achieve 21.332 tokens/s in decode stage.
Different from normal quantization methods, MECLA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib123" title="">123</a>]</cite> proposes a parameter-efficient scaling sub-matrix partition method (SSMP) to decompose large weight matrices into several tiny-scale source sub-matrices (SS) and derived sub-matrices (DS).
For memory issues, SSMP avoids accessing the full weight matrix but only requires small SS and DS scaling scalars.
For computation issues, the proposed accelerator fully exploits the intermediate data reuse of matrix multiplication via on-chip matrix regrouping, inner-product multiplication re-association, and outer-product partial sum reuse.
Totally, it can reduce 83.6% memory access and 72.2% computation.
MECLA provides 14.008 TOPS computing power and <math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS1.SSS5.p1.1.m1.1"><semantics id="S3.SS1.SSS5.p1.1.m1.1a"><mo id="S3.SS1.SSS5.p1.1.m1.1.1" xref="S3.SS1.SSS5.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS5.p1.1.m1.1b"><csymbol cd="latexml" id="S3.SS1.SSS5.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS5.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS5.p1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS5.p1.1.m1.1d">∼</annotation></semantics></math>96W (1.9763W+94W) power consumption by considering all memory access under 28nm.
For Llama2-7B and BLOOM-7B, compared to NVIDIA V100 GPU, MECLA achieves 6.74<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.SSS5.p1.2.m2.1"><semantics id="S3.SS1.SSS5.p1.2.m2.1a"><mo id="S3.SS1.SSS5.p1.2.m2.1.1" xref="S3.SS1.SSS5.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS5.p1.2.m2.1b"><times id="S3.SS1.SSS5.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS5.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS5.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS5.p1.2.m2.1d">×</annotation></semantics></math> and 5.91<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.SSS5.p1.3.m3.1"><semantics id="S3.SS1.SSS5.p1.3.m3.1a"><mo id="S3.SS1.SSS5.p1.3.m3.1.1" xref="S3.SS1.SSS5.p1.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS5.p1.3.m3.1b"><times id="S3.SS1.SSS5.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS5.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS5.p1.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS5.p1.3.m3.1d">×</annotation></semantics></math> inference speedup (<math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS1.SSS5.p1.4.m4.1"><semantics id="S3.SS1.SSS5.p1.4.m4.1a"><mo id="S3.SS1.SSS5.p1.4.m4.1.1" xref="S3.SS1.SSS5.p1.4.m4.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS5.p1.4.m4.1b"><csymbol cd="latexml" id="S3.SS1.SSS5.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS5.p1.4.m4.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS5.p1.4.m4.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS5.p1.4.m4.1d">∼</annotation></semantics></math>161 tokens/s and 141 tokens/s, respectively).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS5.p2">
<p class="ltx_p" id="S3.SS1.SSS5.p2.3"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS5.p2.3.1">Weight-Activation Quantization.</span>
Based on the key insight that outliers are important while the normal values next to them are not, OliVe&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib124" title="">124</a>]</cite> adopts an outlier-victim pair (OVP) quantization and handles outlier values locally with low hardware overheads.
This enables a memory-aligned W4A4/W8A8 quantization, which can be efficiently integrated to the existing hardware accelerators like systolic array and tensor core.
OliVe provides 0.71 TOPS computing power and <math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS1.SSS5.p2.1.m1.1"><semantics id="S3.SS1.SSS5.p2.1.m1.1a"><mo id="S3.SS1.SSS5.p2.1.m1.1.1" xref="S3.SS1.SSS5.p2.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS5.p2.1.m1.1b"><csymbol cd="latexml" id="S3.SS1.SSS5.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS5.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS5.p2.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS5.p2.1.m1.1d">∼</annotation></semantics></math>8W (0.2806W+7.9872W) power consumption by considering all memory access under 22nm.
Estimated results shows that for OPT-6.7B it can achieve 9.173 tokens/s in decode stage.
Li et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib125" title="">125</a>]</cite> uniformly group weights and activations to ensure workload balance for hardware, and propose two approaches called channel sorting and channel selection to enhance the performance of quantization.
It provides 1.43 TOPS computing power and <math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS1.SSS5.p2.2.m2.1"><semantics id="S3.SS1.SSS5.p2.2.m2.1a"><mo id="S3.SS1.SSS5.p2.2.m2.1.1" xref="S3.SS1.SSS5.p2.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS5.p2.2.m2.1b"><csymbol cd="latexml" id="S3.SS1.SSS5.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS5.p2.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS5.p2.2.m2.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS5.p2.2.m2.1d">∼</annotation></semantics></math>8.5W (0.472W+7.9872W) power consumption by considering all memory access under 65nm.
Estimated results shows that for OPT-6.7B it can achieve 19.733 tokens/s in decode stage.
Tender&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib126" title="">126</a>]</cite> decomposes weight and activation matrices by groups with different size to smooth the impact of outliers. And the format of scale factors are powers of two apart, which avoids explicit dequantization and extension to the commodity tensor compute hardware.
It is 7.174W (1.60W+5.574W) power consumption by considering HBM2 memory access under 28nm.
Result shows that for OPT-6.7B, Tender achieves 1.33<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.SSS5.p2.3.m3.1"><semantics id="S3.SS1.SSS5.p2.3.m3.1a"><mo id="S3.SS1.SSS5.p2.3.m3.1.1" xref="S3.SS1.SSS5.p2.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS5.p2.3.m3.1b"><times id="S3.SS1.SSS5.p2.3.m3.1.1.cmml" xref="S3.SS1.SSS5.p2.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS5.p2.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS5.p2.3.m3.1d">×</annotation></semantics></math> speedup (53.33 tokens/s) than NVIDIA A100 GPU.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.6 </span>PIM/NDP</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS6.p1">
<p class="ltx_p" id="S3.SS1.SSS6.p1.6">ReRAM-based analog PIM architectures perform integer MVMs using voltage, current, and conductance in the analog domain, limiting their application to the more accurate floating point (FP) data format.
Guo et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib127" title="">127</a>]</cite> propose an ReRAM and 3D-SRAM-based hybrid PIM architecture with non-uniform data format, achieving FP-based algorithm accuracy, high device utilization, and high energy efficiency.
At the software level, they first analyze the impact of quantization errors on the accuracy of attention-free LLMs.
For the quantization error-insensitive MVM operations, they propose the PIM-oriented exponent-free non-uniform (PN) data format.
The proposed PN format can be flexibly adjusted to fit the data distribution and approach the accuracy of the FP format using bit-slicing-based full INT operations.
For the quantization error-sensitive EWM operations, they introduce the multiplication free approximated FP multiplications to reduce the additional hardware overhead for PIM.
At the hardware level, they propose a hybrid PIM architecture, including an ReRAM analog PIM using shift-and-add for PN-based MVMs, and a 3D-SRAM digital PIM with high utilization for multiplication-free FP-based element-wise operations.
Extensive experiments show that the proposed PIM architecture achieves up to 89<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.SSS6.p1.1.m1.1"><semantics id="S3.SS1.SSS6.p1.1.m1.1a"><mo id="S3.SS1.SSS6.p1.1.m1.1.1" xref="S3.SS1.SSS6.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS6.p1.1.m1.1b"><times id="S3.SS1.SSS6.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS6.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS6.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS6.p1.1.m1.1d">×</annotation></semantics></math> and 16<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.SSS6.p1.2.m2.1"><semantics id="S3.SS1.SSS6.p1.2.m2.1a"><mo id="S3.SS1.SSS6.p1.2.m2.1.1" xref="S3.SS1.SSS6.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS6.p1.2.m2.1b"><times id="S3.SS1.SSS6.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS6.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS6.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS6.p1.2.m2.1d">×</annotation></semantics></math> speedup with 2537<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.SSS6.p1.3.m3.1"><semantics id="S3.SS1.SSS6.p1.3.m3.1a"><mo id="S3.SS1.SSS6.p1.3.m3.1.1" xref="S3.SS1.SSS6.p1.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS6.p1.3.m3.1b"><times id="S3.SS1.SSS6.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS6.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS6.p1.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS6.p1.3.m3.1d">×</annotation></semantics></math> and 12<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.SSS6.p1.4.m4.1"><semantics id="S3.SS1.SSS6.p1.4.m4.1a"><mo id="S3.SS1.SSS6.p1.4.m4.1.1" xref="S3.SS1.SSS6.p1.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS6.p1.4.m4.1b"><times id="S3.SS1.SSS6.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS6.p1.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS6.p1.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS6.p1.4.m4.1d">×</annotation></semantics></math> energy efficiency improvement compared with GPU and PIM-baseline, respectively.
TransPIM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib128" title="">128</a>]</cite> is a memory-based acceleration for Transformer using software and hardware co-design.
In the software-level, TransPIM adopts a token-based dataflow to avoid the expensive inter-layer data movements introduced by previous layer-based dataflow.
In the hardware-level, TransPIM introduces lightweight modifications in the conventional HBM architecture to support PIM-NMC hybrid processing and efficient data communication for accelerating Transformer-based models.
TransPIM system uses the 8GB HBM as the memory with 2.15<math alttext="mm^{2}" class="ltx_Math" display="inline" id="S3.SS1.SSS6.p1.5.m5.1"><semantics id="S3.SS1.SSS6.p1.5.m5.1a"><mrow id="S3.SS1.SSS6.p1.5.m5.1.1" xref="S3.SS1.SSS6.p1.5.m5.1.1.cmml"><mi id="S3.SS1.SSS6.p1.5.m5.1.1.2" xref="S3.SS1.SSS6.p1.5.m5.1.1.2.cmml">m</mi><mo id="S3.SS1.SSS6.p1.5.m5.1.1.1" xref="S3.SS1.SSS6.p1.5.m5.1.1.1.cmml">⁢</mo><msup id="S3.SS1.SSS6.p1.5.m5.1.1.3" xref="S3.SS1.SSS6.p1.5.m5.1.1.3.cmml"><mi id="S3.SS1.SSS6.p1.5.m5.1.1.3.2" xref="S3.SS1.SSS6.p1.5.m5.1.1.3.2.cmml">m</mi><mn id="S3.SS1.SSS6.p1.5.m5.1.1.3.3" xref="S3.SS1.SSS6.p1.5.m5.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS6.p1.5.m5.1b"><apply id="S3.SS1.SSS6.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS6.p1.5.m5.1.1"><times id="S3.SS1.SSS6.p1.5.m5.1.1.1.cmml" xref="S3.SS1.SSS6.p1.5.m5.1.1.1"></times><ci id="S3.SS1.SSS6.p1.5.m5.1.1.2.cmml" xref="S3.SS1.SSS6.p1.5.m5.1.1.2">𝑚</ci><apply id="S3.SS1.SSS6.p1.5.m5.1.1.3.cmml" xref="S3.SS1.SSS6.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS6.p1.5.m5.1.1.3.1.cmml" xref="S3.SS1.SSS6.p1.5.m5.1.1.3">superscript</csymbol><ci id="S3.SS1.SSS6.p1.5.m5.1.1.3.2.cmml" xref="S3.SS1.SSS6.p1.5.m5.1.1.3.2">𝑚</ci><cn id="S3.SS1.SSS6.p1.5.m5.1.1.3.3.cmml" type="integer" xref="S3.SS1.SSS6.p1.5.m5.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS6.p1.5.m5.1c">mm^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS6.p1.5.m5.1d">italic_m italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> area overhead and about 40.01W power consumption.
Experimental results show that for GPT2 models, TransPIM achieves at least 22.1<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.SSS6.p1.6.m6.1"><semantics id="S3.SS1.SSS6.p1.6.m6.1a"><mo id="S3.SS1.SSS6.p1.6.m6.1.1" xref="S3.SS1.SSS6.p1.6.m6.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS6.p1.6.m6.1b"><times id="S3.SS1.SSS6.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS6.p1.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS6.p1.6.m6.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS6.p1.6.m6.1d">×</annotation></semantics></math> speedup than NVIDIA RTX 2080Ti GPU.
Other PIM/NDP accelerators like TransPIM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib128" title="">128</a>]</cite> and Sharda’s method&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib129" title="">129</a>]</cite> also involve the quantization to further improve LLM inference.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="692" id="S3.F4.g1" src="https://arxiv.org/html/2410.04466v1/x4.png" width="813">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>LLM (<math alttext="\sim" class="ltx_Math" display="inline" id="S3.F4.2.m1.1"><semantics id="S3.F4.2.m1.1b"><mo id="S3.F4.2.m1.1.1" xref="S3.F4.2.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.F4.2.m1.1c"><csymbol cd="latexml" id="S3.F4.2.m1.1.1.cmml" xref="S3.F4.2.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.2.m1.1d">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.F4.2.m1.1e">∼</annotation></semantics></math> 7 billion parameters) decode stage throughput (batch size 1) vs power on different platforms with quantization.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS7">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.7 </span>Quantitative Comparison</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS7.p1">
<p class="ltx_p" id="S3.SS1.SSS7.p1.1">We first compare the power consumption, inference speed and energy efficiency for different hardware platforms, in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.F4" title="Figure 4 ‣ 3.1.6 PIM/NDP ‣ 3.1 Quantization ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_tag">4</span></a>.
For quantization, power consumption ranges from 3W to 450W, with inference speeds between 3 tokens/s and 1998 tokens/s.
The energy efficiency ranges from 0.0167 tokens/J to 46.66 tokens/J.
T-MAC&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib98" title="">98</a>]</cite> (CPU) achieves the lowest power consumption with 3W and Guo et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib127" title="">127</a>]</cite> (PIM/NDP) achieves the highest throughput with batch size 1 (pre-silicon simulation result).
And Guo et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib127" title="">127</a>]</cite> also achieves the highest energy efficiency with 46.66 tokens/J.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">For CPUs, power consumption ranges from 3W to 385W, with inference speeds between 3 tokens/s and 50 tokens/s, located in the bottom part of the figure. The energy efficiency ranges from 0.0167 tokens/J to 2.38 token/J. Additionally, we observe edge CPUs (including CPU SoCs) with 3W to 6W power consumption exhibit higher energy efficiency (0.544 tokens/J to 2.38 tokens/J).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">For GPUs, power consumption ranges from 40W to 450W, with inference speeds between 18 tokens/s and 194 tokens/s, situated in the upper right part of the figure. The energy efficiency ranges from 0.0667 tokens/J to 0.825 token/J. Compared to other hardware, GPUs can achieve higher absolute inference speeds due to their high computing power and high bandwidth. When quantization methods are used, the memory access bottlenecks in LLM inference are alleviated, further unlocking computing power.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">For FPGAs, power consumption ranges from 45W to 225W, with inference speeds between 40 tokens/s and 333 tokens/s, also in the upper right part of the figure. The energy efficiency ranges from over 0.178 tokens/J to 1.85 tokens/J, which is higher than GPUs and server CPUs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1">For ASICs, power consumption ranges from 6.3W to 96.66W, with inference speeds between 9.173 tokens/s and 161.086 tokens/s, found in the upper left section of the figure. The energy efficiency ranges from 0.803 tokens/J to over 7.434 tokens/J, outperforming CPU, GPU and FPGA hardware platforms.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I1.i5.p1">
<p class="ltx_p" id="S3.I1.i5.p1.1">For PIM/NDPs, power consumption ranges from 11.516W to 42.819W, with inference speeds between 481 tokens/s and 1998 tokens/s, found in the upper left section of the graph. The energy efficiency outperforms other hardware platforms.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS7.p2">
<p class="ltx_p" id="S3.SS1.SSS7.p2.1">Overall, both weight-only quantization and weight-activation quantization methods can enhance absolute inference speed and improve energy efficiency.
Weight-only quantization reduces bandwidth requirements but introduces additional dequantization operations, which can increase hardware power consumption while improving absolute speed.
On the other hand, weight-activation quantization reduces the hardware compute unit area and power consumption by using smaller-width computation units, leading to improved absolute speed while lowering overall hardware power consumption.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Sparsity</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Overview</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">Sparsity reduces the number of non-zero elements and skip the multiplication and addition with zero to improve efficiency of computation and storage.
Due to the presence of attention computations in standard transformer-based large models, sparsification methods include not only weight sparsity and activation sparsity but also attention sparsity.
<span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p1.1.1">Weight sparsity</span> is primarily achieved through pruning methods, including global pruning, layer-wise pruning, and structured pruning, which reduce the size of weight matrices and leverage sparse matrix libraries for optimization. <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p1.1.2">Activation sparsity</span> focuses on reducing the computation of activation values by employing techniques such as activation pruning (e.g., threshold pruning) and dynamic sparsity, with hardware optimizations utilizing sparse data structures to enhance efficiency. <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p1.1.3">Attention sparsity</span> addresses the optimization of computations in self-attention mechanisms, employing methods like local attention, block-wise attention, and sparse attention matrices, which reduce computational load by limiting the calculation scope or using sparse matrix storage. These sparsity strategies help improve model inference efficiency, particularly when dealing with large-scale data and complex tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1">Sparsity patterns can be categorized into random and structured sparsity as shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.F5" title="Figure 5 ‣ 3.2.1 Overview ‣ 3.2 Sparsity ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_tag">5</span></a>. <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p2.1.1">Random pattern</span> involves a random distribution of zero elements within the matrix, achieving higher accuracy but potentially lower speed for computation. <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p2.1.2">Structured pattern</span> applies a specific pattern to the sparsity, improving computational efficiency by aligning with hardware optimizations. Within structured sparsity, common patterns include block-wise sparsity, N:M sparsity, channel-wise sparsity and some combinations of structured pattern sparsity. These structured patterns offer predictable and optimized computational benefits.
Block-wise sparsity involves dividing the weight matrix into smaller blocks and applying sparsity within each block.
N:M sparsity retains M non-zero elements out of every N elements, improving efficiency through hardware acceleration. NVIDIA’s 2:4 sparse Tensor Core is a representative hardware unit for N:M sparsity, capable of achieving up to 2<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.1.m1.1"><semantics id="S3.SS2.SSS1.p2.1.m1.1a"><mo id="S3.SS2.SSS1.p2.1.m1.1.1" xref="S3.SS2.SSS1.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.1.m1.1b"><times id="S3.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p2.1.m1.1d">×</annotation></semantics></math> computational acceleration.
Channel-wise sparsity aims to prune entire channels in a matrix, significantly reducing computation and storage needs.
Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.T4" title="Table 4 ‣ 3.2.1 Overview ‣ 3.2 Sparsity ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_tag">4</span></a> shows the usage of three sparsity methods across different hardware platforms.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="253" id="S3.F5.g1" src="https://arxiv.org/html/2410.04466v1/x5.png" width="664">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Sparsity and sparsity patterns.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S3.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Sparsity on CPU, GPU, FPGA, ASIC, and PIM/NDP</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T4.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T4.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.1.1.1.1">Hardware</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.1.1.1.2">Weight Sparsity</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.1.1.1.3">Activation Sparsity</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.1.1.1.4">Attention Sparsity</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.1.2.2.1">CPU</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.1.2.2.2">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.1.2.2.3">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.1.2.2.4">✗</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.1.3.3.1">GPU</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.1.3.3.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.1.3.3.3">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.1.3.3.4">✓</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.1.4.4.1">FPGA</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.1.4.4.2">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.1.4.4.3">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.1.4.4.4">✗</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.1.5.5.1">ASIC</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.1.5.5.2">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.1.5.5.3">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.1.5.5.4">✓</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.1.6.6.1">PIM/NDP</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T4.1.6.6.2">✗</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T4.1.6.6.3">✗</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T4.1.6.6.4">✗</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>CPU</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p1.1.1">Activation Sparsity.</span>
Activation sparsity is determined by activation functions.
Commonly using SwiGLU&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib215" title="">215</a>]</cite> and GeGLU&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib216" title="">216</a>]</cite> exhibits limited sparsity for LLMs, but simply replacing these functions with ReLU fails to achieve sufficient sparsity.
Turbo Sparse&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib130" title="">130</a>]</cite> proposes the dReLU activation function to improve LLM activation sparsity, along with a high-quality training data mixture ratio to facilitate effective sparsity.
By applying their sparsity method to the Mistral and Mixtral models, only 2.5 billion (35.7%) and 4.3 billion (9.2%) parameters are activated per inference iteration, respectively.
For Mistral-7B, Turbo Sparse achieves 8.71 tokens/s and 9.94 tokens/s on Intel i9-14900HX processor and Intel i7-12700K processor, respectively.
For Mixtral-47B with 4-bit quantization, Turbo Sparse achieves 16.1 tokens/s, 11.98 tokens/s and 11.1 tokens/s on Intel i9-14900HX, Intel i7-12700K and SnapDragon 8 Gen3, respectively.
ProSparse&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib131" title="">131</a>]</cite> also introduces activation function substitution, progressive sparsity regularization, and activation threshold shifting to help non-ReLU LLMs obtain high activation sparsity without performance degradation.
For Llama2-7B and Llama2-13B, ProSparse achieves high sparsity of 89.32% and 88.80%, and 16.3 tokens/s and 8.67 tokens/s, respectively, based on PowerInfer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib195" title="">195</a>]</cite> framework.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>GPU</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS3.p1.1.1">Weight Sparsity.</span>
LLM-pruner&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib132" title="">132</a>]</cite>, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM’s functionality.
To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data.
We validate the LLM-Pruner on three LLMs, including Llama, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS3.p2">
<p class="ltx_p" id="S3.SS2.SSS3.p2.5">LLMs can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy.
SparseGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib133" title="">133</a>]</cite> requires a sophisticated weight update procedure in an iterative pruning process.
Wanda&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib134" title="">134</a>]</cite> prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis.
Notably, Wanda requires no retraining or weight update, where pruning process is faster.
Besides unstructured pattern, these two methods generalizes to semi-structured N:M (2:4 and 4:8) patterns.
E-Sparse&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib135" title="">135</a>]</cite> introduces entropy to quantify the information richness within each channel (intra-channel) of the input features, and adopts it to enhance the feature norms (crosschannel) as a metric to evaluate parameter importance.
Furthermore, it proposes Channel Shuffle to reorder the information distribution in LLMs to obtain N:M Sparsity with less information loss.
2:4 sparsity as supported by NVIDIA GPUs of generation Ampere and newer theoretically offers 2<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.1.m1.1"><semantics id="S3.SS2.SSS3.p2.1.m1.1a"><mo id="S3.SS2.SSS3.p2.1.m1.1.1" xref="S3.SS2.SSS3.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.1.m1.1b"><times id="S3.SS2.SSS3.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.1.m1.1d">×</annotation></semantics></math> acceleration of matrix multiplications.
In practical, 2:4 sparsity can achieve 1.54<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.2.m2.1"><semantics id="S3.SS2.SSS3.p2.2.m2.1a"><mo id="S3.SS2.SSS3.p2.2.m2.1.1" xref="S3.SS2.SSS3.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.2.m2.1b"><times id="S3.SS2.SSS3.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.2.m2.1d">×</annotation></semantics></math>-1.79<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.3.m3.1"><semantics id="S3.SS2.SSS3.p2.3.m3.1a"><mo id="S3.SS2.SSS3.p2.3.m3.1.1" xref="S3.SS2.SSS3.p2.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.3.m3.1b"><times id="S3.SS2.SSS3.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS3.p2.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.3.m3.1d">×</annotation></semantics></math> speedup for MatMul, and end-to-end speedups are about 1.21<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.4.m4.1"><semantics id="S3.SS2.SSS3.p2.4.m4.1a"><mo id="S3.SS2.SSS3.p2.4.m4.1.1" xref="S3.SS2.SSS3.p2.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.4.m4.1b"><times id="S3.SS2.SSS3.p2.4.m4.1.1.cmml" xref="S3.SS2.SSS3.p2.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.4.m4.1d">×</annotation></semantics></math>-1.25<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.5.m5.1"><semantics id="S3.SS2.SSS3.p2.5.m5.1a"><mo id="S3.SS2.SSS3.p2.5.m5.1.1" xref="S3.SS2.SSS3.p2.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.5.m5.1b"><times id="S3.SS2.SSS3.p2.5.m5.1.1.cmml" xref="S3.SS2.SSS3.p2.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.5.m5.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.5.m5.1d">×</annotation></semantics></math> (due to some extra overheads from e.g. attention).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS3.p3">
<p class="ltx_p" id="S3.SS2.SSS3.p3.1">Based on the key observation that the bottleneck of LLM inference is the skinny matrix multiplications, Flash-LLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib136" title="">136</a>]</cite> proposes a general Load-as-Sparse and Compute-as-Dense methodology for unstructured sparse matrix multiplication.
Flash-LLM proposes a new sparse format called Tiled-CSL to relieve the memory bandwidth bottleneck and support the tile-by-tile SpMM execution with tensor cores.
For OPT-30B, Flash-LLM achieves 80% sparsity with 1.44% accuracy decrease and about 290 tokens/s, 500 tokens/s, 800 tokens/s, and 1187 tokens/s on single A100 GPU with batch sizes 8, 16, 32, and 64, respectively.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS3.p4">
<p class="ltx_p" id="S3.SS2.SSS3.p4.1">Agarwalla et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib137" title="">137</a>]</cite> combine the SparseGPT one-shot pruning method and sparse pretraining to pretrain a high sparsity LLM.
They deploy model on GPU and CPU by utilizing Neural Magic’s DeepSparse engine and Neural Magic’s nm-vllm engine, respectively.
For Llama-7B, on NVIDIA A10 GPU, they achieve 44.4 tokens/s and 47.9 tokens/s with 50% sparsity and 70% sparsity, respectively.
On AMD EPYC 9R14 Processor, they achieve 4.4 tokens/s and 6.9 tokens/s with 50% sparsity and 70% sparsity, respectively.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS3.p5">
<p class="ltx_p" id="S3.SS2.SSS3.p5.2">Existing methods require costly retraining, forgo LLM’s in-context learning ability, or do not yield wall-clock time speedup on modern hardware.
DejaVu&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib138" title="">138</a>]</cite> predicts contextual sparsity on the fly given inputs to each layer, along with an asynchronous and hardware-aware implementation that speeds up LLM inference.
For OPT-175B model, DejaVu achieves up to 75% sparsity and 50 tokens/s on 8 A100-80GB GPUs with batch size 1, which is over 2<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p5.1.m1.1"><semantics id="S3.SS2.SSS3.p5.1.m1.1a"><mo id="S3.SS2.SSS3.p5.1.m1.1.1" xref="S3.SS2.SSS3.p5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p5.1.m1.1b"><times id="S3.SS2.SSS3.p5.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p5.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p5.1.m1.1d">×</annotation></semantics></math> and 6<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p5.2.m2.1"><semantics id="S3.SS2.SSS3.p5.2.m2.1a"><mo id="S3.SS2.SSS3.p5.2.m2.1.1" xref="S3.SS2.SSS3.p5.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p5.2.m2.1b"><times id="S3.SS2.SSS3.p5.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p5.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p5.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p5.2.m2.1d">×</annotation></semantics></math> faster than FasterTransformer and Hugging Face implementation, respectively.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS3.p6">
<p class="ltx_p" id="S3.SS2.SSS3.p6.10"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS3.p6.10.1">Attention Sparsity.</span>
During the prefilling phase of LLM inference, attention computation complexity scales quadratically with input sequence length.
Given limited GPU computing and memory resources, attention sparsification can reduce the number of attention values to accelerate prefilling phase.
For static sparsity, Sparse Transformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib139" title="">139</a>]</cite>, StreamingLLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib141" title="">141</a>]</cite>, Bigbird&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib140" title="">140</a>]</cite>, and Longformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib142" title="">142</a>]</cite> use the manual combination of global and local patterns to replace the full attention patterns.
The local pattern captures the local context of each token within a fixed size or stride while the global pattern captures the relationship between the specific tokens to all other tokens.
For Llama2-7B and Llama2-13B models, StreamingLLM achieves 15.38-32.26 tokens/s and 9.43-20.83 tokens/s on single NVIDIA A6000 GPU, respectively.
For dynaimic sparsity, Adaptively Sparse Attention&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib143" title="">143</a>]</cite> replaces softmax with <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p6.1.m1.1"><semantics id="S3.SS2.SSS3.p6.1.m1.1a"><mi id="S3.SS2.SSS3.p6.1.m1.1.1" xref="S3.SS2.SSS3.p6.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p6.1.m1.1b"><ci id="S3.SS2.SSS3.p6.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p6.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p6.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p6.1.m1.1d">italic_α</annotation></semantics></math>-entmax, a differentiable generalization of softmax that allows low-scoring words to receive precisely zero weight and drops parts of the context that are no longer required for future generation.
Reformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib144" title="">144</a>]</cite> replaces dot-product attention by using locality-sensitive hashing, changing the complexity from <math alttext="O(L^{2})" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p6.2.m2.1"><semantics id="S3.SS2.SSS3.p6.2.m2.1a"><mrow id="S3.SS2.SSS3.p6.2.m2.1.1" xref="S3.SS2.SSS3.p6.2.m2.1.1.cmml"><mi id="S3.SS2.SSS3.p6.2.m2.1.1.3" xref="S3.SS2.SSS3.p6.2.m2.1.1.3.cmml">O</mi><mo id="S3.SS2.SSS3.p6.2.m2.1.1.2" xref="S3.SS2.SSS3.p6.2.m2.1.1.2.cmml">⁢</mo><mrow id="S3.SS2.SSS3.p6.2.m2.1.1.1.1" xref="S3.SS2.SSS3.p6.2.m2.1.1.1.1.1.cmml"><mo id="S3.SS2.SSS3.p6.2.m2.1.1.1.1.2" stretchy="false" xref="S3.SS2.SSS3.p6.2.m2.1.1.1.1.1.cmml">(</mo><msup id="S3.SS2.SSS3.p6.2.m2.1.1.1.1.1" xref="S3.SS2.SSS3.p6.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS2.SSS3.p6.2.m2.1.1.1.1.1.2" xref="S3.SS2.SSS3.p6.2.m2.1.1.1.1.1.2.cmml">L</mi><mn id="S3.SS2.SSS3.p6.2.m2.1.1.1.1.1.3" xref="S3.SS2.SSS3.p6.2.m2.1.1.1.1.1.3.cmml">2</mn></msup><mo id="S3.SS2.SSS3.p6.2.m2.1.1.1.1.3" stretchy="false" xref="S3.SS2.SSS3.p6.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p6.2.m2.1b"><apply id="S3.SS2.SSS3.p6.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p6.2.m2.1.1"><times id="S3.SS2.SSS3.p6.2.m2.1.1.2.cmml" xref="S3.SS2.SSS3.p6.2.m2.1.1.2"></times><ci id="S3.SS2.SSS3.p6.2.m2.1.1.3.cmml" xref="S3.SS2.SSS3.p6.2.m2.1.1.3">𝑂</ci><apply id="S3.SS2.SSS3.p6.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.SSS3.p6.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p6.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS3.p6.2.m2.1.1.1.1">superscript</csymbol><ci id="S3.SS2.SSS3.p6.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS3.p6.2.m2.1.1.1.1.1.2">𝐿</ci><cn id="S3.SS2.SSS3.p6.2.m2.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS2.SSS3.p6.2.m2.1.1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p6.2.m2.1c">O(L^{2})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p6.2.m2.1d">italic_O ( italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )</annotation></semantics></math> to <math alttext="O(LlogL)" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p6.3.m3.1"><semantics id="S3.SS2.SSS3.p6.3.m3.1a"><mrow id="S3.SS2.SSS3.p6.3.m3.1.1" xref="S3.SS2.SSS3.p6.3.m3.1.1.cmml"><mi id="S3.SS2.SSS3.p6.3.m3.1.1.3" xref="S3.SS2.SSS3.p6.3.m3.1.1.3.cmml">O</mi><mo id="S3.SS2.SSS3.p6.3.m3.1.1.2" xref="S3.SS2.SSS3.p6.3.m3.1.1.2.cmml">⁢</mo><mrow id="S3.SS2.SSS3.p6.3.m3.1.1.1.1" xref="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.cmml"><mo id="S3.SS2.SSS3.p6.3.m3.1.1.1.1.2" stretchy="false" xref="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1" xref="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.cmml"><mi id="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.2" xref="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.2.cmml">L</mi><mo id="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.1" xref="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.3" xref="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.3.cmml">l</mi><mo id="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.1a" xref="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.4" xref="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.4.cmml">o</mi><mo id="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.1b" xref="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.5" xref="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.5.cmml">g</mi><mo id="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.1c" xref="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.6" xref="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.6.cmml">L</mi></mrow><mo id="S3.SS2.SSS3.p6.3.m3.1.1.1.1.3" stretchy="false" xref="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p6.3.m3.1b"><apply id="S3.SS2.SSS3.p6.3.m3.1.1.cmml" xref="S3.SS2.SSS3.p6.3.m3.1.1"><times id="S3.SS2.SSS3.p6.3.m3.1.1.2.cmml" xref="S3.SS2.SSS3.p6.3.m3.1.1.2"></times><ci id="S3.SS2.SSS3.p6.3.m3.1.1.3.cmml" xref="S3.SS2.SSS3.p6.3.m3.1.1.3">𝑂</ci><apply id="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.cmml" xref="S3.SS2.SSS3.p6.3.m3.1.1.1.1"><times id="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.1"></times><ci id="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.2">𝐿</ci><ci id="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.3">𝑙</ci><ci id="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.4.cmml" xref="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.4">𝑜</ci><ci id="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.5.cmml" xref="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.5">𝑔</ci><ci id="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.6.cmml" xref="S3.SS2.SSS3.p6.3.m3.1.1.1.1.1.6">𝐿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p6.3.m3.1c">O(LlogL)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p6.3.m3.1d">italic_O ( italic_L italic_l italic_o italic_g italic_L )</annotation></semantics></math>, where <math alttext="L" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p6.4.m4.1"><semantics id="S3.SS2.SSS3.p6.4.m4.1a"><mi id="S3.SS2.SSS3.p6.4.m4.1.1" xref="S3.SS2.SSS3.p6.4.m4.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p6.4.m4.1b"><ci id="S3.SS2.SSS3.p6.4.m4.1.1.cmml" xref="S3.SS2.SSS3.p6.4.m4.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p6.4.m4.1c">L</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p6.4.m4.1d">italic_L</annotation></semantics></math> is the sequence length.
Sparse Flash Attention&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib145" title="">145</a>]</cite> extends FlashAttention&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib217" title="">217</a>]</cite> GPU kernel and encompasses key/query dropping and hashing-based attention.
Sparse Sinkhorn Attention&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib146" title="">146</a>]</cite> adopts a learned sorting network to align keys with their relevant query buckets, ensuring that attention is computed only between the corresponding query-key pairs.
H<sub class="ltx_sub" id="S3.SS2.SSS3.p6.10.2">2</sub>O&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib147" title="">147</a>]</cite> observes that a small portion of tokens (called Heavy Hitters, H<sub class="ltx_sub" id="S3.SS2.SSS3.p6.10.3">2</sub>) contributes most of the value when computing attention scores.
H<sub class="ltx_sub" id="S3.SS2.SSS3.p6.10.4">2</sub>O introduces a dynamic attention sparsification method to adopt KV cache eviction policy that dynamically retains a balance of recent and H<sub class="ltx_sub" id="S3.SS2.SSS3.p6.10.5">2</sub> tokens.
For OPT-6.7B model, H<sub class="ltx_sub" id="S3.SS2.SSS3.p6.10.6">2</sub>O with 20% H<sub class="ltx_sub" id="S3.SS2.SSS3.p6.10.7">2</sub> achieves 30.4 tokens/s on single NVIDIA T4 GPU.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.4 </span>FPGA</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS4.p1">
<p class="ltx_p" id="S3.SS2.SSS4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS4.p1.1.1">Weight Sparsity.</span>
FlightLLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib120" title="">120</a>]</cite> is the first real FPGA-based LLM accelerator which proposes a configurable sparse DSP chain to support different sparsity patterns with high computation efficiency.
Then, it proposes an always-on-chip decode scheme to boost memory bandwidth with mixed-precision support.
Finally, it proposes a length adaptive compilation method to reduce the compilation overhead.
For Llama2-7B model, FlightLLM achieves 55 tokens/s and 92.5 tokens/s with batch size 1 on the Xilinx Alveo U280 FPGA and Versal VHK158 FPGA, respectively.
EdgeLLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib121" title="">121</a>]</cite> integrates 4-bit weight-only quantization and utilizes log-scale structural sparsity for weight parameters in the matrix multiplication operator.
For ChatGLM2-6B model, it achieves average 67 tokens/s with 55.07W power consumption on AMD Xilinx VCU128 FPGA.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.5 </span>ASIC</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS5.p1">
<p class="ltx_p" id="S3.SS2.SSS5.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS5.p1.1.1">Attention Sparsity.</span>
Spatten&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib148" title="">148</a>]</cite> leverages token sparsity, head sparsity, and quantization opportunities to reduce the attention computation and memory access.
It assesses the cumulative importance of each word by aggregating the attention matrix columns, subsequently pruning tokens with minimal cumulative significance from the input in subsequent layers.
It provides 2.88 TOPS computing power and 8.3W power consumption by considering all memory access under 40nm.
Experimental results shows that for GPT2-Medium, it can achieve about 35.86 tokens/s in decode stage.
TF-MVP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib149" title="">149</a>]</cite> quantitatively analyzes sparsity patterns of pruned-transformer models with the cutting-edge fine-grained pruning scheme for the first time and presents the mixed-length vector pruning (MVP) procedure by utilizing this direction strength.
From hardware perspective, it introduces the TF-MVP architecture, a sparsity-aware cost-efficient accelerator design dedicated to the proposed pruned-transformer models.
Implemented in a 28nm CMOS technology at 400MHz, TF-MVP provides 0.835 TOPS with 1.721W on-chip power consumption for accelerating GPT-2 small model.
SOFA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib150" title="">150</a>]</cite> predicts attention sparsity by using log-based add-only operations to avoid the significant overhead of prediction.
Then, a distributed sorting and a sorted updating FlashAttention mechanism are proposed with a cross-stage coordinated tiling principle, which enables fine-grained and lightweight coordination among stages, helping optimize memory access and latency.
SOFA provides 24.423 TOPS computing power and 3.4W power consumption under 28nm.
For Llama2-7B, compared to NVIDIA A100 GPU, it achieves 9.5<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.SSS5.p1.1.m1.1"><semantics id="S3.SS2.SSS5.p1.1.m1.1a"><mo id="S3.SS2.SSS5.p1.1.m1.1.1" xref="S3.SS2.SSS5.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS5.p1.1.m1.1b"><times id="S3.SS2.SSS5.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS5.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS5.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS5.p1.1.m1.1d">×</annotation></semantics></math> inference speedup in prefill stage.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.6 </span>PIM/NDP</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS6.p1">
<p class="ltx_p" id="S3.SS2.SSS6.p1.2"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS6.p1.2.1">Weight Sparsity.</span>
LauWS&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib151" title="">151</a>]</cite> proposes an unstructured sparsity method for NDP systems and is evaluated on a practical GDDR6-based bank NDP.
Not only the overall features of the total matrix but the features of the local region are preserved by ignoring non-feature values as much as possible, which is beneficial to the trade-off between high sparsity and least accuracy loss.
Compared to dense models, it achieves 1.23<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.SSS6.p1.1.m1.1"><semantics id="S3.SS2.SSS6.p1.1.m1.1a"><mo id="S3.SS2.SSS6.p1.1.m1.1.1" xref="S3.SS2.SSS6.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS6.p1.1.m1.1b"><times id="S3.SS2.SSS6.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS6.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS6.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS6.p1.1.m1.1d">×</annotation></semantics></math> and 1.24<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.SSS6.p1.2.m2.1"><semantics id="S3.SS2.SSS6.p1.2.m2.1a"><mo id="S3.SS2.SSS6.p1.2.m2.1.1" xref="S3.SS2.SSS6.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS6.p1.2.m2.1b"><times id="S3.SS2.SSS6.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS6.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS6.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS6.p1.2.m2.1d">×</annotation></semantics></math> speedup for GPT-2 small model and OPT-125M at 50% sparsity, respectively.
Sharda et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib129" title="">129</a>]</cite> propose to use the capacitorless 3D stackable DRAM to store much larger LLMs compared to conventional DRAM at higher density.
To reduce the intermediate data size, they propose to use a layer-wise sparsity-quantization hybrid (LSQH) algorithm, which induces sparsity based on calculations performed using low-bit quantization to reduce both the energy consumption and the data storage requirements.
Finally, a 3D heterogeneously integrated accelerator is designed by stacking a 3D DRAM with logic dies designed in the 3nm technology node at 1GHz.
The evaluation shows that for Llama2-13B, it achieves 163k tokens/s in prefill stage with 193W power consumption.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS6.p2">
<p class="ltx_p" id="S3.SS2.SSS6.p2.2"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS6.p2.2.1">Attention Sparsity.</span>
HARDSEA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib152" title="">152</a>]</cite> proposes an attention sparsity method by predicting lightweight token relevance and design a hybrid analog-ReRAM and digital-SRAM in-memory computing accelerator.
It employs ReRAM-CIM, whose precision is sensitive to circuit non-idealities, to take charge of token relevance prediction where only computing monotonicity is demanded.
The SRAM-CIM, utilized for exact sparse attention computing, is reorganized as an on-memory-boundary computing scheme, thus adapting to irregular sparsity patterns.
Experimental results show that HARDSEA prunes BERT and GPT-2 small model to 20% sparsity without accuracy loss, achieving 5.8<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.SSS6.p2.1.m1.1"><semantics id="S3.SS2.SSS6.p2.1.m1.1a"><mo id="S3.SS2.SSS6.p2.1.m1.1.1" xref="S3.SS2.SSS6.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS6.p2.1.m1.1b"><times id="S3.SS2.SSS6.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS6.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS6.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS6.p2.1.m1.1d">×</annotation></semantics></math>–6.7<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.SSS6.p2.2.m2.1"><semantics id="S3.SS2.SSS6.p2.2.m2.1a"><mo id="S3.SS2.SSS6.p2.2.m2.1.1" xref="S3.SS2.SSS6.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS6.p2.2.m2.1b"><times id="S3.SS2.SSS6.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS6.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS6.p2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS6.p2.2.m2.1d">×</annotation></semantics></math> speedup over NVIDIA RTX 3090 GPU.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="683" id="S3.F6.g1" src="https://arxiv.org/html/2410.04466v1/x6.png" width="814">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>LLM (<math alttext="\sim" class="ltx_Math" display="inline" id="S3.F6.2.m1.1"><semantics id="S3.F6.2.m1.1b"><mo id="S3.F6.2.m1.1.1" xref="S3.F6.2.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.F6.2.m1.1c"><csymbol cd="latexml" id="S3.F6.2.m1.1.1.cmml" xref="S3.F6.2.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.F6.2.m1.1d">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.F6.2.m1.1e">∼</annotation></semantics></math> 7 billion parameters) decode stage throughput (batch size 1) vs power on different platforms with sparsity.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS7">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.7 </span>Comparison</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS7.p1">
<p class="ltx_p" id="S3.SS2.SSS7.p1.1">In Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.F6" title="Figure 6 ‣ 3.2.6 PIM/NDP ‣ 3.2 Sparsity ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_tag">6</span></a>, for sparsity, power consumption ranges from 30W to 400W, with inference speeds between 8.67 tokens/s and 92.5 tokens/s.
The energy efficiency ranges from 0.069 tokens/J to 1.421 tokens/J.
TF-MVP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib149" title="">149</a>]</cite> (ASIC) achieves the lowest power consumption with 30.0167W and FlightLLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib120" title="">120</a>]</cite> (FPGA) achieves the highest throughput with batch size 1 (though it also applies quantization).
And TF-MVP (ASIC) achieves the highest energy efficiency with 1.421 tokens/J.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1">For CPUs, power consumption ranges from 55W to 125W, with inference speeds between 8.67 tokens/s and 16.3 tokens/s, located in the bottom part of the figure. The energy efficiency ranges from 0.069 tokens/J to 0.158 token/J, which is much lower than FPGAs and ASICs. Currently, no edge-side CPUs have adopted sparsity methods to accelerate LLM inference.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1">For GPUs, power consumption ranges from 70W to 400W, with inference speeds between 20.83 tokens/s and 50 tokens/s, situated in the middle right part of the figure. The energy efficiency ranges from 0.069 tokens/J to 0.434 token/J. Compared to CPUs, GPUs can achieve higher absolute inference speeds due to their high computing power and high bandwidth. However, the energy efficiency difference between CPUs is not significant.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1">For FPGAs, power consumption ranges from 45W to 155W, with inference speeds between 55 tokens/s and 92.5 tokens/s, also in the upper right part of the figure. The energy efficiency ranges from over 0.597 tokens/J to 1.32 tokens/J, which is much higher than GPUs and CPUs.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Fast Decoding</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Overview</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">Traditional autoregressive decoding typically generates text token by token, choosing only the highest probability token at each step, known as greedy sampling.
While this approach is simple and easy to implement, it may lead to a lack of diversity and creativity in the generated results. Another autoregressive method called nucleus sampling (or top-p sampling)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib218" title="">218</a>]</cite> considers multiple candidates during generation by setting a cumulative probability threshold p, allowing for sampling within a certain range.
Although this method offers more diversity than greedy sampling, it still operates in a step-by-step generation manner.
Currently, fast decoding techniques can be mainly divided into two categories: speculative decoding and skip layer.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.p2">
<p class="ltx_p" id="S3.SS3.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p2.1.1">Speculative Decoding.</span>
Speculative decoding is a technique for enhancing the generation efficiency of large language models (LLMs). Its core principle lies in using a draft model to quickly generate candidate outputs, which are then evaluated in depth by a main model, thereby accelerating the text generation process.
In the implementation of speculative decoding, a smaller draft model is first used to quickly generate multiple candidate words. This model can evaluate the context in a short time and propose various possible output options. Subsequently, the main model performs parallel evaluations of these candidates, calculating their probabilities or scores, and ultimately selects the candidate with the highest score for actual generation. Through this approach, speculative decoding combines speed and accuracy, significantly reducing the computation time while maintaining the quality of the generated text.
Common choices for draft models include: one option is to directly use a specific layer from the Transformer model, leveraging the existing architecture to maintain a certain level of feature extraction capability while accelerating inference speed; another option is to train a separate small model, which typically has fewer parameters and a simpler structure, focusing on rapidly generating candidate words and optimizing performance for specific tasks. Both methods have their advantages, and the choice can be made based on specific application needs, quality requirements, and computational resource constraints.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.p3">
<p class="ltx_p" id="S3.SS3.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p3.1.1">Skip Layer.</span>
The working principle of skip layer technology is to dynamically and selectively skip certain layers during the model inference process, thereby reducing computational load and increasing generation speed. In practical implementation, the model evaluates the importance of each layer for the current task while processing input, and decides whether to execute the computation of specific layers based on preset heuristic rules or learned strategies.
In this method, the model typically consists of multiple layers, such as self-attention and feedforward layers in a Transformer. During inference, when the input features are relatively simple or the context complexity is low, the model can choose to skip the computation of certain intermediate layers. This choice may be based on real-time assessments, allowing the model to dynamically adjust its computation path according to different inputs.
To effectively implement skip layer, the model often requires optimization during the training phase, learning when to skip which layers. This can be achieved through methods such as policy gradients or reinforcement learning, enabling the model to adapt flexibly across various tasks.
By skipping unnecessary layers, skip layer technology can significantly accelerate inference speed while reducing computational resource consumption, making it particularly suitable for real-time systems and resource-constrained environments. Overall, this method enhances the efficiency and applicability of large language models by intelligently selecting the computation process.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="293" id="S3.F7.g1" src="https://arxiv.org/html/2410.04466v1/x7.png" width="498">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Fast decoding.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S3.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Fast decoding on CPU, GPU, FPGA, ASIC, and PIM/NDP</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T5.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T5.1.1.1.1">Hardware</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T5.1.1.1.2">Speculative Decoding</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T5.1.1.1.3">Skip Layer</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T5.1.2.2.1">CPU</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T5.1.2.2.2">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T5.1.2.2.3">✗</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T5.1.3.3.1">GPU</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T5.1.3.3.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T5.1.3.3.3">✓</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T5.1.4.4.1">FPGA</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T5.1.4.4.2">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T5.1.4.4.3">✗</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T5.1.5.5.1">ASIC</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T5.1.5.5.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T5.1.5.5.3">✗</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T5.1.6.6.1">PIM/NDP</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T5.1.6.6.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T5.1.6.6.3">✗</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>GPU</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS2.p1.1.1">Speculative Decoding.</span>
Speculative decoding&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib154" title="">154</a>]</cite> is proposed to overcome the inherently sequential process in the autoregressive decoding of LLM. The essential decoding mechanism is to make predictions (<span class="ltx_text ltx_font_italic" id="S3.SS3.SSS2.p1.1.2">i.e.</span>, draft tokens) parallelly for multiple time steps and then select the longest prefix verified by a scoring model as the final output.
Lookahead decoding&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib155" title="">155</a>]</cite> adopts the <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS2.p1.1.3">Guess-and-Verify</span> paradigm as the whole decoding mechanism which generates the draft tokens by <math alttext="n" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.1.m1.1"><semantics id="S3.SS3.SSS2.p1.1.m1.1a"><mi id="S3.SS3.SSS2.p1.1.m1.1.1" xref="S3.SS3.SSS2.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.1.m1.1b"><ci id="S3.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p1.1.m1.1d">italic_n</annotation></semantics></math>-gram method and verifies the draft tokens during the forward at the same time. For Llama-7B and Llama-13B models with different datasets, it achieves 65.12-94.51 tokens/s and 56.01-90.05 tokens/s on a single NVIDIA A100-80GB GPU. To improve the acceptance rate while maintaining generation quality, many works focus on the scheme of generating draft tokens.
Medusa&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib156" title="">156</a>]</cite> adds extra decoding heads to generate multiple subsequent tokens in parallel and uses a tree-based attention mechanism to construct multiple candidate continuations and verify them simultaneously in each decoding step. For Vicuna-7B and Vicuna-13B, it achieves 129.86 tokens/s and 98.54 tokens/s on a single NVIDIA A100-80GB GPU.
EAGLE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib157" title="">157</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib158" title="">158</a>]</cite> selects a single transformer layer with the same configuration as LLM as the draft model to make predictions autoregressively and combines the feature and token embedding as the input of the draft model. During the verification phase on the target model, EAGLE chooses the tree-based attention mechanism similar to Lookahead and Medusa to ensure the correct relationship between the draft tokens. For Vicuna-7B, Vicuna-13B, Llama2-Chat-7B, and Llama-2-Chat-13B models, it achieves 139.95 tokens/s, 132.31 tokens/s, 133.98 tokens/s, and 156.80 tokens/s on a single NVIDIA A100-80GB GPU. Based on the basic paradigm of speculative decoding for prediction by draft models and verification by target models, some studies explore the optimizations on the modules in it.
Ouroboros&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib159" title="">159</a>]</cite> constructs a phrase candidate pool from the verification process of LLMs to provide candidates for the draft token generation of the draft model. Different from Medusa and EAGLE, Ouroboros uses the smaller LLMs (<span class="ltx_text ltx_font_italic" id="S3.SS3.SSS2.p1.1.4">e.g.</span>, DeepSeek-7B) as the draft models for the target models (<span class="ltx_text ltx_font_italic" id="S3.SS3.SSS2.p1.1.5">e.g.</span>, DeepSeek-34B). For Yi-34B, DeepSeek-34B and CodeLlama-34B models, it achieves 61.20 tokens/s, 41.00 tokens/s and 39.2 tokens/s on a single NVIDIA A100-80GB GPU. During the prediction phase,
Sequoia&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib160" title="">160</a>]</cite> introduces a dynamic programming algorithm and a hardware-aware tree optimizer to find the optimal tree structure based on the runtime features and the given hardware platform. During the verification phase, Sequoia uses a novel sampling and verification method that outperforms prior work across different decoding temperatures. For Llama2-7B and Llama2-13B models, it achieves 169.68 tokens/s and 149.20 tokens/s on a single NVIDIA A100-80GB GPU.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p2">
<p class="ltx_p" id="S3.SS3.SSS2.p2.1">The studies mentioned above all require the help of an auxiliary model (<span class="ltx_text ltx_font_italic" id="S3.SS3.SSS2.p2.1.1">e.g.</span>, a single transformer layer in EAGLE, several Medusa heads in Medusa, or a smaller LLM like Llama2-7B in Ouroboros) or the statistical methods (<span class="ltx_text ltx_font_italic" id="S3.SS3.SSS2.p2.1.2">e.g.</span>, <math alttext="n" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p2.1.m1.1"><semantics id="S3.SS3.SSS2.p2.1.m1.1a"><mi id="S3.SS3.SSS2.p2.1.m1.1.1" xref="S3.SS3.SSS2.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p2.1.m1.1b"><ci id="S3.SS3.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS3.SSS2.p2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p2.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p2.1.m1.1d">italic_n</annotation></semantics></math>-gram in Lookahead) to generate the predicted tokens and then the target model is utilized to verify the predicted tokens. Some other studies&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib161" title="">161</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib162" title="">162</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib163" title="">163</a>]</cite> also explore the LLM inference acceleration without the need of auxiliary models, called self-speculative decoding. Draft&amp;Verify&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib161" title="">161</a>]</cite> generates draft tokens by selectively skipping certain intermediate layers of LLMs. Subsequently, the draft tokens will be verified in one forward pass. For the Llama2-13B model, it achieves 62.23 tokens/s on a single NVIDIA A100-40GB GPU. Kangaroo&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib162" title="">162</a>]</cite> adopts a fixed shallow sub-network of the LLM as the draft model, with the remaining layers serving as the target model. To enhance the representation ability of the draft model (<span class="ltx_text ltx_font_italic" id="S3.SS3.SSS2.p2.1.3">i.e.</span>, the shallow sub-network), it trains an adapter module to follow the sub-network. For Vicuna-7B and Vicuna-13B models, it achieves 138.14 tokens/s and 105.89 tokens/s on a single NVIDIA A100-80GB GPU. LayerSkip&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib163" title="">163</a>]</cite> proposes to exit at early layers and verify and correct with remaining layers of the model. During training, it applies layer dropout with low dropout rates for earlier layers and higher dropout rates for later layers, and adds an early exit loss to increase the accuracy of early exit at earlier layers. For the Llama2-13B model, it achieves 66.37 tokens/s on a single NVIDIA H100 GPU.
LLMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib153" title="">153</a>]</cite> is motivated by the observation that there are abundant identical text spans between the decoding result by an LLM and the reference that is available in many real-world scenarios (e.g., retrieved documents).
LLMA first selects a text span from the reference and copies its tokens to the decoder and then efficiently checks the tokens’ appropriateness as the decoding result in parallel within one decoding step.
For Llama-7B and Llama-13B models, it achieves 59.2 tokens/s and 41.1 tokens/s on a single NVIDIA V100 GPU, respectively.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p3">
<p class="ltx_p" id="S3.SS3.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS2.p3.1.1">Skip Layer.</span>
The skip layer method&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib164" title="">164</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib165" title="">165</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib166" title="">166</a>]</cite> is proposed based on the idea that not all layers of LLMs are necessary during inference. AdaInfer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib164" title="">164</a>]</cite> statistically analyzes the activated layers across tasks and proposes a simple algorithm to determine the inference termination moment based on the input instance adaptively. Due to the introduction of the overhead in each layer, which is not friendly to the decoding phase, AdaInfer only takes some Benchmarks for the Q&amp;A tasks and achieves 25.2 tokens/s for Llama2-7B on a single NVIDIA V100 GPU. Based on the basic dataflow of Adainfer, RAEE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib165" title="">165</a>]</cite> proposes to build the retrieval database to store the token information offline and leverages the information of the retrieved similar token by searching the pre-built retrieval database to guide the backbone model to exit at the layer. For the LM-BFF which is finetuned based on RoBERT-large-350M, RAEE achieves 26.23 tokens/s. The studies mentioned above both use continuous shallow layers for inference, called early exiting. MOD&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib166" title="">166</a>]</cite> decides whether to skip the current layer or not by pretraining the model to add a router in each layer like Mixture-of-Experts. This achieves a dynamic selection of partial layers for computation instead of forcing continuous layers. The model of MOD is not open source and no corresponding results on throughput are given in the paper.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3 </span>ASIC</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS3.p1">
<p class="ltx_p" id="S3.SS3.SSS3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS3.p1.1.1">Speculative Decoding.</span>
C-Transformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib167" title="">167</a>]</cite> adopts a big-little network, which is composed of the original GPT-2 big model and a 1/10<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS3.SSS3.p1.1.m1.1"><semantics id="S3.SS3.SSS3.p1.1.m1.1a"><mo id="S3.SS3.SSS3.p1.1.m1.1.1" xref="S3.SS3.SSS3.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p1.1.m1.1b"><times id="S3.SS3.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS3.p1.1.m1.1d">×</annotation></semantics></math> smaller model, and a reconfigurable homogeneous architecture to increase hardware utilization and energy efficiency.
During inference, only the little model computation is performed, and if the prediction probability of a specific token is over a predefined threshold, the big model computation is skipped leading to memory access reduction.
Then, workloads are divided into two domains: adder-based spike domain which is efficient for small input values, and multiplier-based non-spike domain which is efficient for large input values.
It has 1.431W power consumption and is fabricated in 28nm CMOS technology.
For GPT-2 large model, C-Transformer achieves 2146.75 tokens/s in prefill stage.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="681" id="S3.F8.g1" src="https://arxiv.org/html/2410.04466v1/x8.png" width="813">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>LLM (<math alttext="\sim" class="ltx_Math" display="inline" id="S3.F8.2.m1.1"><semantics id="S3.F8.2.m1.1b"><mo id="S3.F8.2.m1.1.1" xref="S3.F8.2.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.F8.2.m1.1c"><csymbol cd="latexml" id="S3.F8.2.m1.1.1.cmml" xref="S3.F8.2.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.F8.2.m1.1d">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.F8.2.m1.1e">∼</annotation></semantics></math> 7 billion parameters) decode stage throughput (batch size 1) vs power on different platforms with fast decoding.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.4 </span>PIM/NDP</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS4.p1">
<p class="ltx_p" id="S3.SS3.SSS4.p1.4"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS4.p1.4.1">Speculative Decoding.</span>
SpecPIM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib168" title="">168</a>]</cite> aims to accelerate speculative inference on the PIM-enabled system by extensively exploring the heterogeneity brought by both the algorithm and the architecture.
It constructs the architecture design space to satisfy each model’s disparate resource demands and dedicates the dataflow design space to fully utilize the system’s hardware resources.
Based on the co-design space, it also proposes a design space exploration framework to provide the optimal design under different target scenarios.
Compared with speculative inference on GPUs and existing PIM-based LLM accelerators, SpecPIM achieves 1.52<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS3.SSS4.p1.1.m1.1"><semantics id="S3.SS3.SSS4.p1.1.m1.1a"><mo id="S3.SS3.SSS4.p1.1.m1.1.1" xref="S3.SS3.SSS4.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS4.p1.1.m1.1b"><times id="S3.SS3.SSS4.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS4.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS4.p1.1.m1.1d">×</annotation></semantics></math>/2.02<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS3.SSS4.p1.2.m2.1"><semantics id="S3.SS3.SSS4.p1.2.m2.1a"><mo id="S3.SS3.SSS4.p1.2.m2.1.1" xref="S3.SS3.SSS4.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS4.p1.2.m2.1b"><times id="S3.SS3.SSS4.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS4.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS4.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS4.p1.2.m2.1d">×</annotation></semantics></math> geomean speedup and 6.67<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS3.SSS4.p1.3.m3.1"><semantics id="S3.SS3.SSS4.p1.3.m3.1a"><mo id="S3.SS3.SSS4.p1.3.m3.1.1" xref="S3.SS3.SSS4.p1.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS4.p1.3.m3.1b"><times id="S3.SS3.SSS4.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS4.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS4.p1.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS4.p1.3.m3.1d">×</annotation></semantics></math>/2.68<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS3.SSS4.p1.4.m4.1"><semantics id="S3.SS3.SSS4.p1.4.m4.1a"><mo id="S3.SS3.SSS4.p1.4.m4.1.1" xref="S3.SS3.SSS4.p1.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS4.p1.4.m4.1b"><times id="S3.SS3.SSS4.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS4.p1.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS4.p1.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS4.p1.4.m4.1d">×</annotation></semantics></math> geomean higher energy efficiency.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.5 </span>Comparison</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS5.p1">
<p class="ltx_p" id="S3.SS3.SSS5.p1.1">In Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.F8" title="Figure 8 ‣ 3.3.3 ASIC ‣ 3.3 Fast Decoding ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_tag">8</span></a>, for fast decoding, power consumption ranges from 17.499W to 700W, with inference speeds between 24.7 tokens/s and 174.018 tokens/s.
The energy efficiency ranges from 0.082 tokens/J to 9.944 tokens/J.
SpecPIM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib168" title="">168</a>]</cite> (PIM/NDP) achieves the lowest power consumption with 17.499W, the highest throughput with batch size 1, and the highest energy efficiency.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S3.I3">
<li class="ltx_item" id="S3.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i1.p1">
<p class="ltx_p" id="S3.I3.i1.p1.1">For GPUs, power consumption ranges from 267W to 700W, with inference speeds between 24.7 tokens/s and 169.68 tokens/s. The energy efficiency ranges from 0.082 tokens/J to 0.587 tokens/J.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I3.i2.p1">
<p class="ltx_p" id="S3.I3.i2.p1.1">For PIM/NDPs, SpecPIM is the only one using speculative decoding, which outperforms GPU platforms.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Operator Optimization</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S3.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>Overview</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS1.p1">
<p class="ltx_p" id="S3.SS4.SSS1.p1.1">Improving the execution efficiency of operators is crucial for LLM eras, which not only involves enhancing computational speed but also maximizing resource utilization. As the scale and complexity of models continue to increase, traditional operator execution methods become increasingly inefficient, prompting the need to explore various optimization strategies.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS1.p2">
<p class="ltx_p" id="S3.SS4.SSS1.p2.1">The following four methods provide effective solutions for operator optimization, significantly enhancing the performance and responsiveness of models across different hardware platforms.
<span class="ltx_text ltx_font_bold" id="S3.SS4.SSS1.p2.1.1">Fusion.</span>
Operator fusion reduces the storage and transmission needs of intermediate data by merging multiple independent operators into a single entity. This approach not only lowers I/O overhead but also reduces redundant computations during the execution process, thereby improving overall efficiency. Operator fusion enables hardware to utilize cache and bandwidth more effectively, significantly boosting computational performance.
<span class="ltx_text ltx_font_bold" id="S3.SS4.SSS1.p2.1.2">Nonlinear Function Approximation.</span>
Common nonlinear activation functions in deep learning models often require specialized hardware support, which can occupy substantial chip resources. By employing linear approximations, we can achieve computations using less expensive hardware while maintaining algorithmic accuracy. This optimization strategy makes complex nonlinear calculations more efficient, making it suitable for resource-constrained environments.
<span class="ltx_text ltx_font_bold" id="S3.SS4.SSS1.p2.1.3">Coarse-grained Processing.</span>
When handling large-scale matrix operations, fine-grained computational units may lead to frequent resource scheduling and contention, introducing additional overhead. Coarse-grained processing simplifies the scheduling process by merging multiple small computational units into larger ones, reducing resource contention. This method effectively enhances computational coherence and efficiency, particularly in parallel computing environments.
<span class="ltx_text ltx_font_bold" id="S3.SS4.SSS1.p2.1.4">Storage Optimization.</span>
Storage optimization strategies focus on the arrangement of data in memory, aiming to minimize latency caused by memory access during computation. By strategically organizing data storage locations and access patterns, we can significantly improve data access efficiency and enhance overall computational performance. This optimization is closely related not only to hardware performance but also to algorithm design.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="345" id="S3.F9.g1" src="https://arxiv.org/html/2410.04466v1/x9.png" width="565">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Operator optimizations.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS1.p3">
<p class="ltx_p" id="S3.SS4.SSS1.p3.1">Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.T6" title="Table 6 ‣ 3.4.1 Overview ‣ 3.4 Operator Optimization ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_tag">6</span></a> shows the usage of these four operator optimization methods across different hardware platforms. By comprehensively applying these four optimization methods, the efficiency of operator execution can be significantly improved, allowing deep learning models to handle complex tasks more effectively. Selecting and combining these optimization strategies appropriately for specific application scenarios and hardware platforms will be key to achieving high-performance computing.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S3.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Operator optimization on CPU, GPU, FPGA, ASIC, and PIM/NDP</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T6.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T6.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T6.1.1.1.1">Hardware</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.1.1.1.2">Fusion</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.1.1.1.3">Nonlinear Function Approximation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.1.1.1.4">Coarse-grained Processing</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.1.1.1.5">Storage Optimization</td>
</tr>
<tr class="ltx_tr" id="S3.T6.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T6.1.2.2.1">CPU</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.1.2.2.2">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.1.2.2.3">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.1.2.2.4">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.1.2.2.5">✗</td>
</tr>
<tr class="ltx_tr" id="S3.T6.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T6.1.3.3.1">GPU</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.1.3.3.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.1.3.3.3">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.1.3.3.4">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.1.3.3.5">✗</td>
</tr>
<tr class="ltx_tr" id="S3.T6.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T6.1.4.4.1">FPGA</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.1.4.4.2">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.1.4.4.3">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.1.4.4.4">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.1.4.4.5">✗</td>
</tr>
<tr class="ltx_tr" id="S3.T6.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T6.1.5.5.1">ASIC</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.1.5.5.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.1.5.5.3">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.1.5.5.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.1.5.5.5">✗</td>
</tr>
<tr class="ltx_tr" id="S3.T6.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T6.1.6.6.1">PIM/NDP</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T6.1.6.6.2">✗</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T6.1.6.6.3">✓</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T6.1.6.6.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T6.1.6.6.5">✓</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>GPU</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p1">
<p class="ltx_p" id="S3.SS4.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS2.p1.1.1">Fusion.</span>
To address the quadratic memory requirements in the attention computation, FlashAttention&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib169" title="">169</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib170" title="">170</a>]</cite> fuses the attention operation into a single operator by tiling input matrices (Q, K, V) and the attention matrix into blocks. Based on FlashAttention, FlashDecoding&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib171" title="">171</a>]</cite> proposes additionally the parallel computation along the feature dimension, improving performance for small batch size during the decoding phase. FlashDecoding achieves 95.07 tokens/s and 54.19 tokens/s for Llama2-7B on a single NVIDIA A100 and RTX 3090 GPU respectively. Subsequently, FlashDecoding++&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib172" title="">172</a>]</cite> optimizes the synchronization overhead in softmax computation by pre-determining a unified maximum based on statistical analysis in advance, enabling the parallel execution of subsequent operations and improving efficiency in typical LLMs like Llama2 and ChatGLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib33" title="">33</a>]</cite>. FlashDecoding++ achieves 115.57 tokens/s and 61.66 tokens/s for Llama2-7B on a single NVIDIA A100 and RTX 3090 GPU respectively.
The linear operator is widely used in deep learning. In the common framework for deep learning (<span class="ltx_text ltx_font_italic" id="S3.SS4.SSS2.p1.1.2">e.g.</span>, Pytorch), the backend of the linear operator is usually the General Matrix-Matrix Multiplication (GEMM) operation supported by NVIDIA. The naive implementation by HuggingFace achieves 44.60 tokens/s and 36.29 tokens/s for Llama2-7B on a single NVIDIA A100 and RTX 3090 GPU respectively. In the LLM framework (<span class="ltx_text ltx_font_italic" id="S3.SS4.SSS2.p1.1.3">e.g.</span>, DeepSpeed&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib173" title="">173</a>]</cite>, vLLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib174" title="">174</a>]</cite>, and OpenPPL&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib175" title="">175</a>]</cite>), the GEMM implementation provided by cuBLAS&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib176" title="">176</a>]</cite> APIs is generally optimized. For the Llama2-7B model on a single NVIDIA A100 GPU, they achieve 89.24 tokens/s, 88.45 tokens/s and 93.63 tokens/s. For the Llama2-7B model on a single NVIDIA RTX 3090 GPU, they achieve 51.28 tokens/s, 53.31 tokens/s and 55.99 tokens/s. To address the inefficiency of GEMM during decoding due to the reduced dimensions, TensorRT-LLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib177" title="">177</a>]</cite> introduces a dedicated General Matrix-Vector Multiplication (GEMV) implementation to support the decoding phase of LLM when batch size equals 1. TensorRT-LLM achieves 98.19 tokens/s for Llama2-7B on a single NVIDIA A100 GPU.
For the smaller batch size during the decoding phase, FlashDecoding++&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib172" title="">172</a>]</cite> introduces FlatGEMM to address the inefficiencies in cuBLAS&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib176" title="">176</a>]</cite> and CUTLASS&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib178" title="">178</a>]</cite> libraries and employs fine-grained tiling and double buffering techniques to improve parallelism and reduce the latency of memory access. Moreover, it adopts a heuristic selection mechanism to dynamically select the most efficient operator based on the input. The performance of FlashDecoding++ is shown above.
Operator fusion&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib169" title="">169</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib179" title="">179</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib173" title="">173</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib172" title="">172</a>]</cite> is a common and effective optimization to reduce the runtime memory access, eliminate kernel launching overhead and enhance parallelism.
ByteTransformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib179" title="">179</a>]</cite> and DeepSpeed&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib173" title="">173</a>]</cite> fuse the main operator (<span class="ltx_text ltx_font_italic" id="S3.SS4.SSS2.p1.1.4">e.g.</span>, GEMM) and the lightweight operators(<span class="ltx_text ltx_font_italic" id="S3.SS4.SSS2.p1.1.5">e.g.</span>, residual adding, layernorm and activation functions) into a single kernel to reduce the kernel launching overhead.
FlashAttention&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib169" title="">169</a>]</cite> mentioned above fuse the attention operator into one single kernel, reducing significantly the overhead of memory access and the memory requirements. FlashDecoding++&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib172" title="">172</a>]</cite> also achieves the integration of seven fused kernels in a transformer block.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.3 </span>ASIC</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS3.p1">
<p class="ltx_p" id="S3.SS4.SSS3.p1.6"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS3.p1.6.1">Fusion.</span>
In 2020, Groq company introduces the Tensor Streaming Processor (TSP) architecture&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib219" title="">219</a>]</cite>, a functionally-sliced microarchitecture with memory units interleaved with vector and matrix compute units in order to take advantage of dataflow locality.
The first TSP implementation yields a computational density of more than 1 TOPS/<math alttext="mm^{2}" class="ltx_Math" display="inline" id="S3.SS4.SSS3.p1.1.m1.1"><semantics id="S3.SS4.SSS3.p1.1.m1.1a"><mrow id="S3.SS4.SSS3.p1.1.m1.1.1" xref="S3.SS4.SSS3.p1.1.m1.1.1.cmml"><mi id="S3.SS4.SSS3.p1.1.m1.1.1.2" xref="S3.SS4.SSS3.p1.1.m1.1.1.2.cmml">m</mi><mo id="S3.SS4.SSS3.p1.1.m1.1.1.1" xref="S3.SS4.SSS3.p1.1.m1.1.1.1.cmml">⁢</mo><msup id="S3.SS4.SSS3.p1.1.m1.1.1.3" xref="S3.SS4.SSS3.p1.1.m1.1.1.3.cmml"><mi id="S3.SS4.SSS3.p1.1.m1.1.1.3.2" xref="S3.SS4.SSS3.p1.1.m1.1.1.3.2.cmml">m</mi><mn id="S3.SS4.SSS3.p1.1.m1.1.1.3.3" xref="S3.SS4.SSS3.p1.1.m1.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS3.p1.1.m1.1b"><apply id="S3.SS4.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS4.SSS3.p1.1.m1.1.1"><times id="S3.SS4.SSS3.p1.1.m1.1.1.1.cmml" xref="S3.SS4.SSS3.p1.1.m1.1.1.1"></times><ci id="S3.SS4.SSS3.p1.1.m1.1.1.2.cmml" xref="S3.SS4.SSS3.p1.1.m1.1.1.2">𝑚</ci><apply id="S3.SS4.SSS3.p1.1.m1.1.1.3.cmml" xref="S3.SS4.SSS3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.SSS3.p1.1.m1.1.1.3.1.cmml" xref="S3.SS4.SSS3.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS4.SSS3.p1.1.m1.1.1.3.2.cmml" xref="S3.SS4.SSS3.p1.1.m1.1.1.3.2">𝑚</ci><cn id="S3.SS4.SSS3.p1.1.m1.1.1.3.3.cmml" type="integer" xref="S3.SS4.SSS3.p1.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS3.p1.1.m1.1c">mm^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS3.p1.1.m1.1d">italic_m italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> for its 25<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS3.p1.2.m2.1"><semantics id="S3.SS4.SSS3.p1.2.m2.1a"><mo id="S3.SS4.SSS3.p1.2.m2.1.1" xref="S3.SS4.SSS3.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS3.p1.2.m2.1b"><times id="S3.SS4.SSS3.p1.2.m2.1.1.cmml" xref="S3.SS4.SSS3.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS3.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS3.p1.2.m2.1d">×</annotation></semantics></math>29 mm 14nm chip at 900 MHz.
In 2022, Groq company introduces a novel software-defined communication approach for large-scale interconnection networks of TSP elements&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib220" title="">220</a>]</cite>.
This scalable communication fabric is based on a software-defined dragonfly topology, ultimately yielding a parallel machine learning system with elasticity to support a variety of workloads.
Each TSP contributes 220 MB to the global memory capacity, with the maximum capacity limited only by the network’s scale.
The large-scale parallel system achieves up to 10,440 TSPs and more than 2 TB of global memory accessible in less than 3ms of end-to-end system latency.
Based on two previous works, Groq Language Processing Unit (LPU)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib181" title="">181</a>]</cite>
is fabricated in 14nm with 185W power consumption.
According to&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib221" title="">221</a>]</cite>, LPU achieves 814 tokens/s for Gemma-7B model.
Another commercial company, HyperAccel, also proposes a LLM inference chip with dataflow architecture.
Latency Processing Unit (LPU)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib180" title="">180</a>]</cite> introduces streamlined hardware that maximizes the effective memory bandwidth usage during end-to-end inference regardless of the model size to achieve up to 90% bandwidth utilization for high-speed text generation.
It consists of expandable synchronization link (ESL) that hides bulk of the data synchronization latency in a multi-device system to achieve nearperfect scalability.
Its on-chip power is 0.284W sythesised by Samsung 4nm and the total system power is 86W with 96GB HBM3.
For OPT-1.3B/6.7B/13B/30B, LPU achieves 769.23 tokens/s, 217.39 tokens/s, 112.40 tokens/s and 49.26 tokens/s, respectively.
The Wafer-Scale Engine (WSE-3)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib188" title="">188</a>]</cite>, which powers the Cerebras CS-3 system, is the largest chip ever built. The WSE-3 is 57<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS3.p1.3.m3.1"><semantics id="S3.SS4.SSS3.p1.3.m3.1a"><mo id="S3.SS4.SSS3.p1.3.m3.1.1" xref="S3.SS4.SSS3.p1.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS3.p1.3.m3.1b"><times id="S3.SS4.SSS3.p1.3.m3.1.1.cmml" xref="S3.SS4.SSS3.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS3.p1.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS3.p1.3.m3.1d">×</annotation></semantics></math> larger than the largest GPU, has 52<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS3.p1.4.m4.1"><semantics id="S3.SS4.SSS3.p1.4.m4.1a"><mo id="S3.SS4.SSS3.p1.4.m4.1.1" xref="S3.SS4.SSS3.p1.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS3.p1.4.m4.1b"><times id="S3.SS4.SSS3.p1.4.m4.1.1.cmml" xref="S3.SS4.SSS3.p1.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS3.p1.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS3.p1.4.m4.1d">×</annotation></semantics></math> more compute cores, and 880<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS3.p1.5.m5.1"><semantics id="S3.SS4.SSS3.p1.5.m5.1a"><mo id="S3.SS4.SSS3.p1.5.m5.1.1" xref="S3.SS4.SSS3.p1.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS3.p1.5.m5.1b"><times id="S3.SS4.SSS3.p1.5.m5.1.1.cmml" xref="S3.SS4.SSS3.p1.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS3.p1.5.m5.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS3.p1.5.m5.1d">×</annotation></semantics></math> more high performance on-chip memory. The only wafer scale processor ever produced, it contains 4 trillion transistors, 900,000 AI-optimized cores, and 44 gigabytes of high performance on-wafer memory.
Each wafer consists of 84 dies with 40GB on-chip memory and 97W power consumption.
For Llama3.1-8B, it can achieve about 1800 tokens/s, which is 20<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS3.p1.6.m6.1"><semantics id="S3.SS4.SSS3.p1.6.m6.1a"><mo id="S3.SS4.SSS3.p1.6.m6.1.1" xref="S3.SS4.SSS3.p1.6.m6.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS3.p1.6.m6.1b"><times id="S3.SS4.SSS3.p1.6.m6.1.1.cmml" xref="S3.SS4.SSS3.p1.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS3.p1.6.m6.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS3.p1.6.m6.1d">×</annotation></semantics></math> faster than hyperscale clouds.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS3.p2">
<p class="ltx_p" id="S3.SS4.SSS3.p2.2"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS3.p2.2.1">Nonlinear Function Approximation.</span>
Constant Softmax (ConSmax)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib182" title="">182</a>]</cite> is a software-hardware co-design as an efficient Softmax alternative.
ConSmax employs differentiable normalization parameters to remove the maximum searching and denominator summation in softmax.
It allows for massive parallelization while performing the critical tasks of softmax.
In addition, a scalable ConSmax hardware utilizing a bitwidth-split LUT can produce lossless non-linear operation and support mix-precision computing. It further facilitates efficient LLM inference.
Experimental results show that ConSmax achieves a minuscule power consumption of 0.43mW and area of 0.001<math alttext="mm^{2}" class="ltx_Math" display="inline" id="S3.SS4.SSS3.p2.1.m1.1"><semantics id="S3.SS4.SSS3.p2.1.m1.1a"><mrow id="S3.SS4.SSS3.p2.1.m1.1.1" xref="S3.SS4.SSS3.p2.1.m1.1.1.cmml"><mi id="S3.SS4.SSS3.p2.1.m1.1.1.2" xref="S3.SS4.SSS3.p2.1.m1.1.1.2.cmml">m</mi><mo id="S3.SS4.SSS3.p2.1.m1.1.1.1" xref="S3.SS4.SSS3.p2.1.m1.1.1.1.cmml">⁢</mo><msup id="S3.SS4.SSS3.p2.1.m1.1.1.3" xref="S3.SS4.SSS3.p2.1.m1.1.1.3.cmml"><mi id="S3.SS4.SSS3.p2.1.m1.1.1.3.2" xref="S3.SS4.SSS3.p2.1.m1.1.1.3.2.cmml">m</mi><mn id="S3.SS4.SSS3.p2.1.m1.1.1.3.3" xref="S3.SS4.SSS3.p2.1.m1.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS3.p2.1.m1.1b"><apply id="S3.SS4.SSS3.p2.1.m1.1.1.cmml" xref="S3.SS4.SSS3.p2.1.m1.1.1"><times id="S3.SS4.SSS3.p2.1.m1.1.1.1.cmml" xref="S3.SS4.SSS3.p2.1.m1.1.1.1"></times><ci id="S3.SS4.SSS3.p2.1.m1.1.1.2.cmml" xref="S3.SS4.SSS3.p2.1.m1.1.1.2">𝑚</ci><apply id="S3.SS4.SSS3.p2.1.m1.1.1.3.cmml" xref="S3.SS4.SSS3.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.SSS3.p2.1.m1.1.1.3.1.cmml" xref="S3.SS4.SSS3.p2.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS4.SSS3.p2.1.m1.1.1.3.2.cmml" xref="S3.SS4.SSS3.p2.1.m1.1.1.3.2">𝑚</ci><cn id="S3.SS4.SSS3.p2.1.m1.1.1.3.3.cmml" type="integer" xref="S3.SS4.SSS3.p2.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS3.p2.1.m1.1c">mm^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS3.p2.1.m1.1d">italic_m italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> at 1GHz working frequency and 22nm CMOS technology.
MARCA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib183" title="">183</a>]</cite> is the first accelerator with reconfigurable architecture tailored for Mamba model.
It proposes a reduction alternative PE array architecture for both linear and element-wise operations.
For linear operations, the reduction tree connected to PE arrays is enabled and executes the reduction operation.
For element-wise operations, the reduction tree is disabled and the output bypasses.
And it proposes a reusable nonlinear function unit based on the reconfigurable PE and decomposes the exponential function and activation function (SiLU) into element-wise operations to reuse the reconfigurable PEs.
Extensive experiments show that MARCA can achieve 23.78 tokens/s with batch size 1 for Mamba-2.8B model with 10.33W power consumption (11.89 tokens/s for <math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS4.SSS3.p2.2.m2.1"><semantics id="S3.SS4.SSS3.p2.2.m2.1a"><mo id="S3.SS4.SSS3.p2.2.m2.1.1" xref="S3.SS4.SSS3.p2.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS3.p2.2.m2.1b"><csymbol cd="latexml" id="S3.SS4.SSS3.p2.2.m2.1.1.cmml" xref="S3.SS4.SSS3.p2.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS3.p2.2.m2.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS3.p2.2.m2.1d">∼</annotation></semantics></math> 7B model).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS3.p3">
<p class="ltx_p" id="S3.SS4.SSS3.p3.3"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS3.p3.3.1">Coarse-grained Processing.</span>
Tensor Contraction Processor (TCP)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib184" title="">184</a>]</cite>, is composed of coarse-grained PEs, which are designed to be flexible enough to be configured as a large-scale single unit or a set of small independent compute units.
TCP chip is designed and fabricated in 5nm technology with 256MB SRAM and 1.5 TB/s 48GB HBM3 under 150W.
For Llama2-7B model, TCP achieves about 125 tokens/s with batch size 1 and 1176 tokens/s with batch size 8.
Habana Gaudi&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib185" title="">185</a>]</cite> and Gaudi2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib186" title="">186</a>]</cite> consists of two main compute engines, Matrix Multiplication Engine (MME) and Tensor Processor Core (TPC) cluster.
The TPC unit is a SIMD processor tailor-made for general deep learning operations.
The biggest difference between GPU and Gaudi architecture is the size of MME.
Gaudi architecture can handle 256<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS3.p3.1.m1.1"><semantics id="S3.SS4.SSS3.p3.1.m1.1a"><mo id="S3.SS4.SSS3.p3.1.m1.1.1" xref="S3.SS4.SSS3.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS3.p3.1.m1.1b"><times id="S3.SS4.SSS3.p3.1.m1.1.1.cmml" xref="S3.SS4.SSS3.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS3.p3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS3.p3.1.m1.1d">×</annotation></semantics></math>256 matrix multiplication, which requires 512 input elements per cycle while GPU architecture with 16<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS3.p3.2.m2.1"><semantics id="S3.SS4.SSS3.p3.2.m2.1a"><mo id="S3.SS4.SSS3.p3.2.m2.1.1" xref="S3.SS4.SSS3.p3.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS3.p3.2.m2.1b"><times id="S3.SS4.SSS3.p3.2.m2.1.1.cmml" xref="S3.SS4.SSS3.p3.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS3.p3.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS3.p3.2.m2.1d">×</annotation></semantics></math>16 Tensor Core requires 8K input elements.
Therefore, Gaudi architecture can save 16<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS3.p3.3.m3.1"><semantics id="S3.SS4.SSS3.p3.3.m3.1a"><mo id="S3.SS4.SSS3.p3.3.m3.1.1" xref="S3.SS4.SSS3.p3.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS3.p3.3.m3.1b"><times id="S3.SS4.SSS3.p3.3.m3.1.1.cmml" xref="S3.SS4.SSS3.p3.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS3.p3.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS3.p3.3.m3.1d">×</annotation></semantics></math> less read bandwidth requirements.
Besides compute engine, Gaudi2 includes 96 GB of HBM2E memories delivering 2.45 TB/sec bandwidth, in addition to a 48 MB of local SRAM.
Gaudi is fabricated in 16nm and Gaudi2 is fabricated in 7nm technique node with 600W power consumption.
For Llama2-7B and Llama2-13B models, Gaudi2 achieves from 81.97 to 111.11 tokens/s and from 49.02 to 64.52 tokens/s with batch size 1, respectively.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.4 </span>PIM/NDP</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS4.p1">
<p class="ltx_p" id="S3.SS4.SSS4.p1.7"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS4.p1.7.1">Fusion.</span>
Modern DRAMs have multiple banks to serve multiple memory requests in parallel. However, when two requests go to the same bank, they have to be served serially, exacerbating the high latency of off-chip memory.
Therefore, Kim et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib222" title="">222</a>]</cite> propose Subarray-Level Parallelism (SALP) to overlap the latencies of different requests that go to the same bank.
Based on SALP, SAL-PIM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib192" title="">192</a>]</cite> proposes a subarray-level processing-in-memory architecture includes three architectural features.
Two types of ALUs (SALU and C-ALU) are integrated into the subarray-level and the channel-level, respectively.
S-ALU utilizes higher bandwidth than bank-level PIM to compute memory bound operation, and C-ALU supports accumulation and reduce-sum operation for multiple banks.
The SAL-PIM architecture is implemented by HBM2 8GB in 28nm CMOS technology with 68.973W power consumption.
As a result, when the input size is from 32 to 128 and the output size is from 1 to 256, SAL-PIM achieves average 1.83<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS4.p1.1.m1.1"><semantics id="S3.SS4.SSS4.p1.1.m1.1a"><mo id="S3.SS4.SSS4.p1.1.m1.1.1" xref="S3.SS4.SSS4.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS4.p1.1.m1.1b"><times id="S3.SS4.SSS4.p1.1.m1.1.1.cmml" xref="S3.SS4.SSS4.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS4.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS4.p1.1.m1.1d">×</annotation></semantics></math> inference speedup for the text generation based on the GPT-2 medium model (345M) compared to the NVIDIA Titan RTX GPU.
PipePIM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib193" title="">193</a>]</cite> introduces pipelining and dual buffering to maximize CU utilization in a digital PIM.
PipePIM consists of two primary schemes: subarray-level pipelining (SAPI) and a dual vector buffer.
The key ideas are to process activation, computation, and precharging on different subarrays in a pipelined manner by SAPI and concurrently perform buffer writes and computation by a dual vector buffer.
It is simulated in 22nm CMOS technology with 88.82<math alttext="mm^{2}" class="ltx_Math" display="inline" id="S3.SS4.SSS4.p1.2.m2.1"><semantics id="S3.SS4.SSS4.p1.2.m2.1a"><mrow id="S3.SS4.SSS4.p1.2.m2.1.1" xref="S3.SS4.SSS4.p1.2.m2.1.1.cmml"><mi id="S3.SS4.SSS4.p1.2.m2.1.1.2" xref="S3.SS4.SSS4.p1.2.m2.1.1.2.cmml">m</mi><mo id="S3.SS4.SSS4.p1.2.m2.1.1.1" xref="S3.SS4.SSS4.p1.2.m2.1.1.1.cmml">⁢</mo><msup id="S3.SS4.SSS4.p1.2.m2.1.1.3" xref="S3.SS4.SSS4.p1.2.m2.1.1.3.cmml"><mi id="S3.SS4.SSS4.p1.2.m2.1.1.3.2" xref="S3.SS4.SSS4.p1.2.m2.1.1.3.2.cmml">m</mi><mn id="S3.SS4.SSS4.p1.2.m2.1.1.3.3" xref="S3.SS4.SSS4.p1.2.m2.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS4.p1.2.m2.1b"><apply id="S3.SS4.SSS4.p1.2.m2.1.1.cmml" xref="S3.SS4.SSS4.p1.2.m2.1.1"><times id="S3.SS4.SSS4.p1.2.m2.1.1.1.cmml" xref="S3.SS4.SSS4.p1.2.m2.1.1.1"></times><ci id="S3.SS4.SSS4.p1.2.m2.1.1.2.cmml" xref="S3.SS4.SSS4.p1.2.m2.1.1.2">𝑚</ci><apply id="S3.SS4.SSS4.p1.2.m2.1.1.3.cmml" xref="S3.SS4.SSS4.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.SSS4.p1.2.m2.1.1.3.1.cmml" xref="S3.SS4.SSS4.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS4.SSS4.p1.2.m2.1.1.3.2.cmml" xref="S3.SS4.SSS4.p1.2.m2.1.1.3.2">𝑚</ci><cn id="S3.SS4.SSS4.p1.2.m2.1.1.3.3.cmml" type="integer" xref="S3.SS4.SSS4.p1.2.m2.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS4.p1.2.m2.1c">mm^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS4.p1.2.m2.1d">italic_m italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> and 86.36<math alttext="mm^{2}" class="ltx_Math" display="inline" id="S3.SS4.SSS4.p1.3.m3.1"><semantics id="S3.SS4.SSS4.p1.3.m3.1a"><mrow id="S3.SS4.SSS4.p1.3.m3.1.1" xref="S3.SS4.SSS4.p1.3.m3.1.1.cmml"><mi id="S3.SS4.SSS4.p1.3.m3.1.1.2" xref="S3.SS4.SSS4.p1.3.m3.1.1.2.cmml">m</mi><mo id="S3.SS4.SSS4.p1.3.m3.1.1.1" xref="S3.SS4.SSS4.p1.3.m3.1.1.1.cmml">⁢</mo><msup id="S3.SS4.SSS4.p1.3.m3.1.1.3" xref="S3.SS4.SSS4.p1.3.m3.1.1.3.cmml"><mi id="S3.SS4.SSS4.p1.3.m3.1.1.3.2" xref="S3.SS4.SSS4.p1.3.m3.1.1.3.2.cmml">m</mi><mn id="S3.SS4.SSS4.p1.3.m3.1.1.3.3" xref="S3.SS4.SSS4.p1.3.m3.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS4.p1.3.m3.1b"><apply id="S3.SS4.SSS4.p1.3.m3.1.1.cmml" xref="S3.SS4.SSS4.p1.3.m3.1.1"><times id="S3.SS4.SSS4.p1.3.m3.1.1.1.cmml" xref="S3.SS4.SSS4.p1.3.m3.1.1.1"></times><ci id="S3.SS4.SSS4.p1.3.m3.1.1.2.cmml" xref="S3.SS4.SSS4.p1.3.m3.1.1.2">𝑚</ci><apply id="S3.SS4.SSS4.p1.3.m3.1.1.3.cmml" xref="S3.SS4.SSS4.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.SSS4.p1.3.m3.1.1.3.1.cmml" xref="S3.SS4.SSS4.p1.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS4.SSS4.p1.3.m3.1.1.3.2.cmml" xref="S3.SS4.SSS4.p1.3.m3.1.1.3.2">𝑚</ci><cn id="S3.SS4.SSS4.p1.3.m3.1.1.3.3.cmml" type="integer" xref="S3.SS4.SSS4.p1.3.m3.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS4.p1.3.m3.1c">mm^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS4.p1.3.m3.1d">italic_m italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> area.
For LLMs like LLaMA and Mistral, the results show 1.2<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS4.p1.4.m4.1"><semantics id="S3.SS4.SSS4.p1.4.m4.1a"><mo id="S3.SS4.SSS4.p1.4.m4.1.1" xref="S3.SS4.SSS4.p1.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS4.p1.4.m4.1b"><times id="S3.SS4.SSS4.p1.4.m4.1.1.cmml" xref="S3.SS4.SSS4.p1.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS4.p1.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS4.p1.4.m4.1d">×</annotation></semantics></math> and 1.14<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS4.p1.5.m5.1"><semantics id="S3.SS4.SSS4.p1.5.m5.1a"><mo id="S3.SS4.SSS4.p1.5.m5.1.1" xref="S3.SS4.SSS4.p1.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS4.p1.5.m5.1b"><times id="S3.SS4.SSS4.p1.5.m5.1.1.cmml" xref="S3.SS4.SSS4.p1.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS4.p1.5.m5.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS4.p1.5.m5.1d">×</annotation></semantics></math> speedup in Newton while 1.21<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS4.p1.6.m6.1"><semantics id="S3.SS4.SSS4.p1.6.m6.1a"><mo id="S3.SS4.SSS4.p1.6.m6.1.1" xref="S3.SS4.SSS4.p1.6.m6.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS4.p1.6.m6.1b"><times id="S3.SS4.SSS4.p1.6.m6.1.1.cmml" xref="S3.SS4.SSS4.p1.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS4.p1.6.m6.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS4.p1.6.m6.1d">×</annotation></semantics></math> and 1.15<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS4.p1.7.m7.1"><semantics id="S3.SS4.SSS4.p1.7.m7.1a"><mo id="S3.SS4.SSS4.p1.7.m7.1.1" xref="S3.SS4.SSS4.p1.7.m7.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS4.p1.7.m7.1b"><times id="S3.SS4.SSS4.p1.7.m7.1.1.cmml" xref="S3.SS4.SSS4.p1.7.m7.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS4.p1.7.m7.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS4.p1.7.m7.1d">×</annotation></semantics></math> speedup in HBM-PIM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib223" title="">223</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS4.p2">
<p class="ltx_p" id="S3.SS4.SSS4.p2.2"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS4.p2.2.1">Nonlinear Function Approximation.</span>
AttentionLego&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib190" title="">190</a>]</cite> proposes a PIM-based matrix-vector multiplication and look-up table-based Softmax design.
PIM-GPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib191" title="">191</a>]</cite>, which achieves end-to-end acceleration of GPT inference with high performance and high energy efficiency.
At the hardware level, PIM-GPT is a hybrid system that includes DRAM-based PIM chips to accelerate VMM near data and an application-specific integrated circuit (ASIC) to support other functions that are too expensive for PIM including necessary nonlinear functions, data communication, and instructions for the PIM chips.
At the software level, the mapping scheme is optimized to efficiently support the GPT dataflow.
Overall, PIM-GPT achieves 89<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS4.p2.1.m1.1"><semantics id="S3.SS4.SSS4.p2.1.m1.1a"><mo id="S3.SS4.SSS4.p2.1.m1.1.1" xref="S3.SS4.SSS4.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS4.p2.1.m1.1b"><times id="S3.SS4.SSS4.p2.1.m1.1.1.cmml" xref="S3.SS4.SSS4.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS4.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS4.p2.1.m1.1d">×</annotation></semantics></math> speedup and 221<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS4.p2.2.m2.1"><semantics id="S3.SS4.SSS4.p2.2.m2.1a"><mo id="S3.SS4.SSS4.p2.2.m2.1.1" xref="S3.SS4.SSS4.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS4.p2.2.m2.1b"><times id="S3.SS4.SSS4.p2.2.m2.1.1.cmml" xref="S3.SS4.SSS4.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS4.p2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS4.p2.2.m2.1d">×</annotation></semantics></math> energy efficiency over NVIDIA T4 GPU on 8 GPT models with up to 1.4 billion parameters.
SAL-PIM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib192" title="">192</a>]</cite> also adopts LUT-based linear interpolation to perform complex nonlinear functions in PIM.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="671" id="S3.F10.g1" src="https://arxiv.org/html/2410.04466v1/x10.png" width="814">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>LLM (<math alttext="\sim" class="ltx_Math" display="inline" id="S3.F10.2.m1.1"><semantics id="S3.F10.2.m1.1b"><mo id="S3.F10.2.m1.1.1" xref="S3.F10.2.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.F10.2.m1.1c"><csymbol cd="latexml" id="S3.F10.2.m1.1.1.cmml" xref="S3.F10.2.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.F10.2.m1.1d">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.F10.2.m1.1e">∼</annotation></semantics></math> 7 billion parameters) decode stage throughput (batch size 1) vs power on different platforms with operator optimization.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS4.p3">
<p class="ltx_p" id="S3.SS4.SSS4.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS4.p3.1.1">Storage Optimization.</span>
By observing that a key impediment to truly harness PIM acceleration is deducing optimal data-placement to place the matrix in memory banks, PIMnast&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib189" title="">189</a>]</cite> proposes matrix tiling/ordering algorithms to tackle these factors and identify orchestration knobs that impact PIM acceleration.
For OPT-6.7B model, compared to the SoC baseline (AMD Ryzen PRO 7040 Series processors comprising eight CPU cores, 12 compute units of GPU cores, 16 AIE tiles, and eight channels of LPDDR5 memory for a peak memory bandwidth of 120 GB/s), PIMnast achieves 4.5<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS4.p3.1.m1.1"><semantics id="S3.SS4.SSS4.p3.1.m1.1a"><mo id="S3.SS4.SSS4.p3.1.m1.1.1" xref="S3.SS4.SSS4.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS4.p3.1.m1.1b"><times id="S3.SS4.SSS4.p3.1.m1.1.1.cmml" xref="S3.SS4.SSS4.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS4.p3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS4.p3.1.m1.1d">×</annotation></semantics></math> speedup for per-token latency.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.5 </span>Comparison</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS5.p1">
<p class="ltx_p" id="S3.SS4.SSS5.p1.1">In Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.F10" title="Figure 10 ‣ 3.4.4 PIM/NDP ‣ 3.4 Operator Optimization ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_tag">10</span></a>, for operator optimization, power consumption ranges from 10.44W to 600W, with inference speeds between 4.13 tokens/s and 1800 tokens/s.
The energy efficiency ranges from 0.092 tokens/J to 18.556 tokens/J.
MARCA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib183" title="">183</a>]</cite> (ASIC) achieves the lowest power consumption with 10.44W.
Cerebras WSE-3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib188" title="">188</a>]</cite> (ASIC) achieves the highest throughput with batch size 1 (post-silicon results) and the highest energy efficiency.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S3.I4">
<li class="ltx_item" id="S3.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I4.i1.p1">
<p class="ltx_p" id="S3.I4.i1.p1.1">For GPUs, power consumption ranges from 32W to 400W, with inference speeds between 4.13 tokens/s and 145.04 tokens/s. The energy efficiency ranges from 0.092 tokens/J to 0.522 tokens/J.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I4.i2.p1">
<p class="ltx_p" id="S3.I4.i2.p1.1">For ASICs, power consumption ranges from 10.44W to 600W, with inference speeds between 11.89 tokens/s and 1800 tokens/s, found in the upper left section of the figure. The energy efficiency ranges from 0.108 tokens/J to over 18.556 tokens/J, outperforming the GPU platform.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Heterogeneous Cooperation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S3.SS5.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.1 </span>Overview</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS5.SSS1.p1">
<p class="ltx_p" id="S3.SS5.SSS1.p1.1">Heterogeneous cooperation combines different types of computing platforms, such as CPUs, GPUs, FPGAs, and PIM/NDPs, to optimize system performance, energy efficiency, and flexibility.
Each computing unit has unique strengths for specific tasks; for example, GPUs excel at parallel processing, FPGAs offer customizable hardware acceleration, and PIM/NDPs are specialized for memory-bound operations.
By distributing tasks to the most suitable hardware, heterogeneous cooperation enhances computing efficiency, reduces power consumption, and lowers latency.
It can be broadly categorized into two types, computing enhancement and memory enhancement.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="168" id="S3.F11.g1" src="https://arxiv.org/html/2410.04466v1/x11.png" width="730">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Heterogeneous cooperation.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS5.SSS1.p2">
<p class="ltx_p" id="S3.SS5.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS5.SSS1.p2.1.1">Computing Enhancement.</span>
Commonly, CPUs and PIMs may struggle when handling large-scale computation tasks.
For instance, while CPUs offer versatility and flexibility, they are not well-suited for highly parallel processing.
PIM reduces data transfer by performing computations within memory, but its computational power is limited.
In such scenarios, powerful parallel computing hardware such as GPUs, NPUs, or TPUs are introduced to assist.
In LLM inference, computation-intensive computations such as attention calculations are placed on GPUs, NPUs, or TPUs, while other computations can be handled by CPUs or PIMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS5.SSS1.p3">
<p class="ltx_p" id="S3.SS5.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS5.SSS1.p3.1.1">Memory Enhancement.</span> focuses on two key aspects: storage capacity enhancement and bandwidth enhancement. As we move into the LLM eras, computational tasks are becoming increasingly complex, leading to greater demands on memory capacity and bandwidth. From the perspective of storage capacity, memory enhancement is achieved by integrating more on-chip memory by using 3D storage stacks on the same chip area. This allows more data and models to be totally stored in a limited on-chip space, reducing the need to access external storage and improving overall efficiency.
Another memory enhancement is to accelerate memory-bound computations by increasing data transfer speeds. For heterogeneous cooperation, bandwidth often restricts computational performance. To address this issue, much methods are proposed to support high-speed on-chip&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib224" title="">224</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib225" title="">225</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib226" title="">226</a>]</cite> and chip-to-chip interconnect, such as Compute Express Link (CXL)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib227" title="">227</a>]</cite> and NVLink&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib63" title="">63</a>]</cite>. These techniques enable low-latency, high-bandwidth data transmission between different chips, significantly speeding up data exchange processes.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S3.T7">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Heterogeneous cooperation on CPU, GPU, FPGA, ASIC, and PIM/NDP</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T7.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T7.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T7.1.1.1.1">Hardware</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.1.2">Computing Enhancement</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.1.1.3">Memory Enhancement</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T7.1.2.2.1">CPU</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.2.2.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.2.2.3">✗</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T7.1.3.3.1">GPU</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.3.3.2">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.3.3.3">✗</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T7.1.4.4.1">FPGA</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.4.4.2">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.4.4.3">✗</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T7.1.5.5.1">ASIC</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.5.5.2">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T7.1.5.5.3">✗</td>
</tr>
<tr class="ltx_tr" id="S3.T7.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T7.1.6.6.1">PIM/NDP</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T7.1.6.6.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T7.1.6.6.3">✓</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S3.SS5.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.2 </span>CPU</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS5.SSS2.p1">
<p class="ltx_p" id="S3.SS5.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS5.SSS2.p1.1.1">Computing Enhancement.</span>
<span class="ltx_text ltx_font_bold" id="S3.SS5.SSS2.p1.1.2">(1) CPU-GPU.</span>
Only using the CPU may result in slower performance, so many methods employ a combination of CPU and GPU to enhance LLM inference speed.
For personal computers, PowerInfer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib195" title="">195</a>]</cite> proposes that the hot-activated neurons should be preloaded onto the GPU for fast access, while cold-activated neurons are computed on the CPU, thus significantly reducing GPU memory demands and CPU-GPU data transfers.
PowerInfer further integrates adaptive predictors and neuron-aware sparse operators.
For various models (OPT-30B/66B, Falcon-40B, and Llama-70B) on a Intel i9-13900K processor equipped with an NVIDIA RTX 4090, it achieves 8.32 tokens/s on average.
On a Intel i7-12700K processor equipped with an NVIDIA RTX 2080Ti, it achieves 5.77 tokens/s on average.
For smartphones, PowerInfer-2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib196" title="">196</a>]</cite> further leverages the highly heterogeneous XPUs present in smartphone SoCs, such as asymmetric big.LITTLE CPU cores, GPU, and NPU.
On OnePlus 12 equipped with SnapDragon 8 Gen 3, PowerInfer-2 achieves 11.7 tokens/s and 10.5 tokens/s on Llama-7B and Mistral-7B, respectively.
On OnePlus Ace 2 equipped with SnapDragon 8+ Gen 1, PowerInfer-2 achieves 6.5 tokens/s and 6.3 tokens/s on Llama-7B and Mistral-7B, respectively.
For servers, Kim et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib194" title="">194</a>]</cite> propose an adaptive model to determine the LLM layers to be run on CPU and GPU, respectively, based on the memory capacity requirement and arithmetic intensity.
They then propose CPU-GPU cooperative computing that exploits the AMX extensions of the latest Intel CPU.
The evaluation demonstrates that for OPT-30B model, CPU-GPU cooperative computing achieves 336 tokens/s with batch size 1 and input tokens 2016 in prefill stage.
In decode stage, it achieves about 25 tokens/s with batch size 90 and input tokens 2016, and about 300 tokens/s with batch size 1400 and input tokens 64.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS5.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.3 </span>PIM/NDP</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS5.SSS3.p1">
<p class="ltx_p" id="S3.SS5.SSS3.p1.14"><span class="ltx_text ltx_font_bold" id="S3.SS5.SSS3.p1.14.1">Computing Enhancement.</span>
<span class="ltx_text ltx_font_bold" id="S3.SS5.SSS3.p1.14.2">(1) PIM-NPU.</span>
NeuPIMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib197" title="">197</a>]</cite> and IANUS&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib198" title="">198</a>]</cite> are heterogeneous PIM acceleration systems that jointly exploits a conventional GEMM-focused NPU and GEMV-optimized PIM devices.
NeuPIMs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib197" title="">197</a>]</cite> first proposes dual row buffers in each bank, facilitating the simultaneous management of memory read/write operations and PIM commands, to enable concurrent operations on both NPU and PIM platforms.
Further, it employs a runtime sub-batch interleaving technique to maximize concurrent execution for the inherent dependencies between GEMM and GEMV in LLMs.
Our evaluation demonstrates that NeuPIMs with <math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS5.SSS3.p1.1.m1.1"><semantics id="S3.SS5.SSS3.p1.1.m1.1a"><mo id="S3.SS5.SSS3.p1.1.m1.1.1" xref="S3.SS5.SSS3.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS3.p1.1.m1.1b"><csymbol cd="latexml" id="S3.SS5.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS5.SSS3.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS3.p1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS3.p1.1.m1.1d">∼</annotation></semantics></math>76W (0.6348W+75W) power consumption (32GB HBM) achieves <math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS5.SSS3.p1.2.m2.1"><semantics id="S3.SS5.SSS3.p1.2.m2.1a"><mo id="S3.SS5.SSS3.p1.2.m2.1.1" xref="S3.SS5.SSS3.p1.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS3.p1.2.m2.1b"><csymbol cd="latexml" id="S3.SS5.SSS3.p1.2.m2.1.1.cmml" xref="S3.SS5.SSS3.p1.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS3.p1.2.m2.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS3.p1.2.m2.1d">∼</annotation></semantics></math>3k tokens/s with batch size 8 for GPT3-7B.
IANUS&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib198" title="">198</a>]</cite> proposes novel PIM access scheduling that manages not only the scheduling of normal memory accesses and PIM computations but also workload mapping across the PIM and the NPU.
The evaluations show that for GPT-6.7B model, IANUS achieves 127.1 tokens/s with about 240W power consumption.
Cambricon-LLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib209" title="">209</a>]</cite> proposes a chiplet-based hybrid architecture with NPU and a dedicated NAND flash chip to enable efficient LLM inference.
It utilizes both the high computing capability of NPU and the data capacity of the NAND flash chip.
The NAND flash chip is enhanced by in-flash computing to perform precise lightweight on-die processing, and the NPU performs matrix operations and handles special function computations.
Experimental results show that Cambricon-LLM with <math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS5.SSS3.p1.3.m3.1"><semantics id="S3.SS5.SSS3.p1.3.m3.1a"><mo id="S3.SS5.SSS3.p1.3.m3.1.1" xref="S3.SS5.SSS3.p1.3.m3.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS3.p1.3.m3.1b"><csymbol cd="latexml" id="S3.SS5.SSS3.p1.3.m3.1.1.cmml" xref="S3.SS5.SSS3.p1.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS3.p1.3.m3.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS3.p1.3.m3.1d">∼</annotation></semantics></math>37W power consumption achieves 3.44 tokens/s (<math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS5.SSS3.p1.4.m4.1"><semantics id="S3.SS5.SSS3.p1.4.m4.1a"><mo id="S3.SS5.SSS3.p1.4.m4.1.1" xref="S3.SS5.SSS3.p1.4.m4.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS3.p1.4.m4.1b"><csymbol cd="latexml" id="S3.SS5.SSS3.p1.4.m4.1.1.cmml" xref="S3.SS5.SSS3.p1.4.m4.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS3.p1.4.m4.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS3.p1.4.m4.1d">∼</annotation></semantics></math>) and 36.34 tokens/s for 70B and 7B LLMs, which is over 22<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS5.SSS3.p1.5.m5.1"><semantics id="S3.SS5.SSS3.p1.5.m5.1a"><mo id="S3.SS5.SSS3.p1.5.m5.1.1" xref="S3.SS5.SSS3.p1.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS3.p1.5.m5.1b"><times id="S3.SS5.SSS3.p1.5.m5.1.1.cmml" xref="S3.SS5.SSS3.p1.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS3.p1.5.m5.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS3.p1.5.m5.1d">×</annotation></semantics></math> to 45<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS5.SSS3.p1.6.m6.1"><semantics id="S3.SS5.SSS3.p1.6.m6.1a"><mo id="S3.SS5.SSS3.p1.6.m6.1.1" xref="S3.SS5.SSS3.p1.6.m6.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS3.p1.6.m6.1b"><times id="S3.SS5.SSS3.p1.6.m6.1.1.cmml" xref="S3.SS5.SSS3.p1.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS3.p1.6.m6.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS3.p1.6.m6.1d">×</annotation></semantics></math> faster than existing flash-offloading technologies, respectively.
<span class="ltx_text ltx_font_bold" id="S3.SS5.SSS3.p1.14.3">(2) PIM-GPU.</span>
MoNDE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib199" title="">199</a>]</cite> is a near-data computing solution that efficiently enables Mixture-of-Experts (MoE) LLM inference on heterogeneous hardwares.
MoNDE reduces the volume of MoE parameter movement by transferring only the hot experts to the GPU, while computing the remaining cold experts inside the host memory device.
MoNDE can achieve inference latency comparable to an ideal GPU system with infinite memory.
AttAcc&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib200" title="">200</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib201" title="">201</a>]</cite> is also an heterogeneous system equipped with GPUs to accelerate the attention layers.
Compared to the monolithic state-of-the-art GPU system, AttAcc achieves significantly higher throughput (up to 2.81<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS5.SSS3.p1.7.m7.1"><semantics id="S3.SS5.SSS3.p1.7.m7.1a"><mo id="S3.SS5.SSS3.p1.7.m7.1.1" xref="S3.SS5.SSS3.p1.7.m7.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS3.p1.7.m7.1b"><times id="S3.SS5.SSS3.p1.7.m7.1.1.cmml" xref="S3.SS5.SSS3.p1.7.m7.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS3.p1.7.m7.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS3.p1.7.m7.1d">×</annotation></semantics></math>) and energy efficiency (up to 2.67<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS5.SSS3.p1.8.m8.1"><semantics id="S3.SS5.SSS3.p1.8.m8.1a"><mo id="S3.SS5.SSS3.p1.8.m8.1.1" xref="S3.SS5.SSS3.p1.8.m8.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS3.p1.8.m8.1b"><times id="S3.SS5.SSS3.p1.8.m8.1.1.cmml" xref="S3.SS5.SSS3.p1.8.m8.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS3.p1.8.m8.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS3.p1.8.m8.1d">×</annotation></semantics></math>) for GPT3-175B model.
In 2024, SK Hynix proposes LPDDR6-based heterogeneous LLM system called AiMX-xPU<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib208" title="">208</a>]</cite>, which consists 1 NVIDIA H100 GPU and 3 AiMX, achieving 167 tokens/s, 220 tokens/s, and 900 tokens/s with batch size 1, 8, and 32 for OPT-30B.
<span class="ltx_text ltx_font_bold" id="S3.SS5.SSS3.p1.14.4">(3) PIM-FPGA.</span>
Kang et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib202" title="">202</a>]</cite> put the memory-bound GEMV calculations including projections, feed-forward-network layer, and the last fully-connected layer with pre-trained weight on HBM2-PIM and evaluate system with PIM-powered Xilinx Alveo U280 board.
For GPT-1.3B model, after scaling the bandwidth by 8<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS5.SSS3.p1.9.m9.1"><semantics id="S3.SS5.SSS3.p1.9.m9.1a"><mo id="S3.SS5.SSS3.p1.9.m9.1.1" xref="S3.SS5.SSS3.p1.9.m9.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS3.p1.9.m9.1b"><times id="S3.SS5.SSS3.p1.9.m9.1.1.cmml" xref="S3.SS5.SSS3.p1.9.m9.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS3.p1.9.m9.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS3.p1.9.m9.1d">×</annotation></semantics></math> for PIM technology, it can achieve about 347.83 tokens/s, which is 1.6<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS5.SSS3.p1.10.m10.1"><semantics id="S3.SS5.SSS3.p1.10.m10.1a"><mo id="S3.SS5.SSS3.p1.10.m10.1.1" xref="S3.SS5.SSS3.p1.10.m10.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS3.p1.10.m10.1b"><times id="S3.SS5.SSS3.p1.10.m10.1.1.cmml" xref="S3.SS5.SSS3.p1.10.m10.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS3.p1.10.m10.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS3.p1.10.m10.1d">×</annotation></semantics></math> faster than the NVIDIA A100 GPU.
From 2022 to 2023, SK Hynix proposes GDDR6-based accelerator-in-memory named AiM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib228" title="">228</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib229" title="">229</a>]</cite> and integrates it with 2 XLINX XCVU9P FPGA chips for control and communication as a prototype called AiMX&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib207" title="">207</a>]</cite> to achieve 330 tokens/s for OPT-6.7B with batch size 1.
<span class="ltx_text ltx_font_bold" id="S3.SS5.SSS3.p1.14.5">(4) PIM-TPU.</span>
H3D-Transformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib204" title="">204</a>]</cite> proposes a heterogeneous 3D-based accelerator design for transformer models, which adopts an interposer substrate with multiple 3D memory/logic hybrid cubes optimized for accelerating different MatMul workloads.
An approximate computing scheme is proposed to take advantage of heterogeneous computing paradigms of mixed-signal compute-in-memory (CIM) and digital tensor processing units (TPU).
From the system-level evaluation results, 10 TOPS/W energy efficiency is achieved for the GPT2 model, which is about 2.6<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS5.SSS3.p1.11.m11.1"><semantics id="S3.SS5.SSS3.p1.11.m11.1a"><mo id="S3.SS5.SSS3.p1.11.m11.1.1" xref="S3.SS5.SSS3.p1.11.m11.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS3.p1.11.m11.1b"><times id="S3.SS5.SSS3.p1.11.m11.1.1.cmml" xref="S3.SS5.SSS3.p1.11.m11.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS3.p1.11.m11.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS3.p1.11.m11.1d">×</annotation></semantics></math>-3.1<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS5.SSS3.p1.12.m12.1"><semantics id="S3.SS5.SSS3.p1.12.m12.1a"><mo id="S3.SS5.SSS3.p1.12.m12.1.1" xref="S3.SS5.SSS3.p1.12.m12.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS3.p1.12.m12.1b"><times id="S3.SS5.SSS3.p1.12.m12.1.1.cmml" xref="S3.SS5.SSS3.p1.12.m12.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS3.p1.12.m12.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS3.p1.12.m12.1d">×</annotation></semantics></math> higher than the baseline with 7nm TPU and stacked FeFET memory.
3D-HI&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib206" title="">206</a>]</cite> leverage chiplet-based heterogeneous integration to design a Network-on-Interposer (NoI) architecture to accelerate LLM inference.
3D-HI uses the streaming multiprocessors along with the associated memory controllers (SM-MCs) for multi-head attention and ReRAM cores for the feed-forward network, which optimize both achievable energy efficiency and throughput.
Further, vertical integration on top of a 2.5D interposer helps to enhance overall system performance and alleviates the issue of memory bottlenecks.
Experimental results demonstrate that 3D-HI lowers the latency and energy consumption by up to 22.8<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS5.SSS3.p1.13.m13.1"><semantics id="S3.SS5.SSS3.p1.13.m13.1a"><mo id="S3.SS5.SSS3.p1.13.m13.1.1" xref="S3.SS5.SSS3.p1.13.m13.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS3.p1.13.m13.1b"><times id="S3.SS5.SSS3.p1.13.m13.1.1.cmml" xref="S3.SS5.SSS3.p1.13.m13.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS3.p1.13.m13.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS3.p1.13.m13.1d">×</annotation></semantics></math> and 5.36<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS5.SSS3.p1.14.m14.1"><semantics id="S3.SS5.SSS3.p1.14.m14.1a"><mo id="S3.SS5.SSS3.p1.14.m14.1.1" xref="S3.SS5.SSS3.p1.14.m14.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS3.p1.14.m14.1b"><times id="S3.SS5.SSS3.p1.14.m14.1.1.cmml" xref="S3.SS5.SSS3.p1.14.m14.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS3.p1.14.m14.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS3.p1.14.m14.1d">×</annotation></semantics></math> with respect to an equivalent state-of-the-art chiplet-based platform.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="680" id="S3.F12.g1" src="https://arxiv.org/html/2410.04466v1/x12.png" width="814">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>LLM (<math alttext="\sim" class="ltx_Math" display="inline" id="S3.F12.2.m1.1"><semantics id="S3.F12.2.m1.1b"><mo id="S3.F12.2.m1.1.1" xref="S3.F12.2.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.F12.2.m1.1c"><csymbol cd="latexml" id="S3.F12.2.m1.1.1.cmml" xref="S3.F12.2.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.F12.2.m1.1d">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.F12.2.m1.1e">∼</annotation></semantics></math> 7 billion parameters) decode stage throughput (batch size 1) vs power on different platforms with heterogeneous cooperation.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS5.SSS3.p2">
<p class="ltx_p" id="S3.SS5.SSS3.p2.4"><span class="ltx_text ltx_font_bold" id="S3.SS5.SSS3.p2.4.1">Memory Enhancement.</span>
<span class="ltx_text ltx_font_bold" id="S3.SS5.SSS3.p2.4.2">(1) PIM-CXL.</span>
As the frequent transfers of these model parameters and intermediate values are performed over relatively slow device-to-device interconnects such as PCIe or NVLink, they become the key bottleneck for efficient acceleration of LLMs.
Kim et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib203" title="">203</a>]</cite> exploit PIM, which uses bank-level parallelization to provide higher internal memory bandwidth compared to traditional DRAM, resulting in a significant increase in on-DRAM compute bandwidth.
In addition to achieving high capacity through Compute eXpress Link (CXL) memory expansion, CXL-PNM demonstrates performance improvements by integrating computational logic into memory products, consequently increasing internal memory bandwidth.
Evaluation results show that the performance of memory-bounded LLMs was significantly improved with PIM and PNM by up to 4.5<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS5.SSS3.p2.1.m1.1"><semantics id="S3.SS5.SSS3.p2.1.m1.1a"><mo id="S3.SS5.SSS3.p2.1.m1.1.1" xref="S3.SS5.SSS3.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS3.p2.1.m1.1b"><times id="S3.SS5.SSS3.p2.1.m1.1.1.cmml" xref="S3.SS5.SSS3.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS3.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS3.p2.1.m1.1d">×</annotation></semantics></math> and 4.4<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS5.SSS3.p2.2.m2.1"><semantics id="S3.SS5.SSS3.p2.2.m2.1a"><mo id="S3.SS5.SSS3.p2.2.m2.1.1" xref="S3.SS5.SSS3.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS3.p2.2.m2.1b"><times id="S3.SS5.SSS3.p2.2.m2.1.1.cmml" xref="S3.SS5.SSS3.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS3.p2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS3.p2.2.m2.1d">×</annotation></semantics></math>, respectively.
CXL-PNM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib205" title="">205</a>]</cite> first devises an LPDDR5X-based CXL memory architecture with 512GB of capacity and 1.1TB/s of bandwidth, which boasts 16<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS5.SSS3.p2.3.m3.1"><semantics id="S3.SS5.SSS3.p2.3.m3.1a"><mo id="S3.SS5.SSS3.p2.3.m3.1.1" xref="S3.SS5.SSS3.p2.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS3.p2.3.m3.1b"><times id="S3.SS5.SSS3.p2.3.m3.1.1.cmml" xref="S3.SS5.SSS3.p2.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS3.p2.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS3.p2.3.m3.1d">×</annotation></semantics></math> larger capacity and 10<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS5.SSS3.p2.4.m4.1"><semantics id="S3.SS5.SSS3.p2.4.m4.1a"><mo id="S3.SS5.SSS3.p2.4.m4.1.1" xref="S3.SS5.SSS3.p2.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS3.p2.4.m4.1b"><times id="S3.SS5.SSS3.p2.4.m4.1.1.cmml" xref="S3.SS5.SSS3.p2.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS3.p2.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.SSS3.p2.4.m4.1d">×</annotation></semantics></math> higher bandwidth than GDDR6 and DDR5-based CXL memory architectures, respectively, under a module form-factor constraint.
Second, it designs a CXL-PNM controller architecture integrated with an LLM inference accelerator, exploiting the unique capabilities of such CXL memory to overcome the disadvantages of competing technologies such as HBM-PIM and AxDIMM.
The evaluation shows that for OPT-13B model, CXL-PNM achieves 42.68 tokens/s with 77.6W power consumption.
<span class="ltx_text ltx_font_bold" id="S3.SS5.SSS3.p2.4.3">(2) PIM-3D Stack.</span>
Sharda et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib129" title="">129</a>]</cite> propose to use the capacitorless 3D stackable DRAM to store much larger LLMs compared to conventional DRAM at higher density.
To reduce the intermediate data size, they propose to use a layer-wise sparsity-quantization hybrid (LSQH) algorithm, which induces sparsity based on calculations performed using low-bit quantization to reduce both the energy consumption and the data storage requirements.
Finally, a 3D heterogeneously integrated accelerator is designed by stacking a 3D DRAM with logic dies designed in the 3nm technology node at 1GHz.
The evaluation shows that for Llama2-13B, it achieves 163k tokens/s in prefill stage with 193W power consumption.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS5.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.4 </span>Comparison</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS5.SSS4.p1">
<p class="ltx_p" id="S3.SS5.SSS4.p1.1">In Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S3.F12" title="Figure 12 ‣ 3.5.3 PIM/NDP ‣ 3.5 Heterogeneous Cooperation ‣ 3 Optimizations on Hardware Platforms ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_tag">12</span></a>, for heterogeneous cooperation, power consumption ranges from 6.3W to 575W, with inference speeds between 4.9 tokens/s and 330 tokens/s.
The energy efficiency ranges from 0.0145 tokens/J to 4.342 tokens/J.
PowerInfer-2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib196" title="">196</a>]</cite> with Snapdragon 8 Gen 3 (CPU) achieves the lowest power consumption with 6.3W.
SK Hynix AiMX&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib207" title="">207</a>]</cite> (PIM/NDP) achieves the highest throughput with batch size 1 and the highest energy efficiency.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S3.I5">
<li class="ltx_item" id="S3.I5.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I5.i1.p1">
<p class="ltx_p" id="S3.I5.i1.p1.1">For CPUs, power consumption ranges from 6.3W to 575W, with inference speeds between 4.9 tokens/s and 11.7 tokens/s, situated in the bottom part of the figure. The energy efficiency ranges from 0.0145 tokens/J to 1.857 tokens/J. In the end-side scenario, the CPU can achieve higher energy efficiency.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I5.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I5.i2.p1">
<p class="ltx_p" id="S3.I5.i2.p1.1">For PIM/NDPs, power consumption ranges from 37W to 240W, with inference speeds between 36.34 tokens/s and 330 tokens/s, found in the upper part of the graph. The energy efficiency ranges from 0.55 tokens/J to 4.342 tokens/J, similar with edge-side CPU platforms.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Homogeneous Cooperation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S3.SS6.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.6.1 </span>Overview</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS6.SSS1.p1">
<p class="ltx_p" id="S3.SS6.SSS1.p1.1">Due to the large size and high computational demands of LLMs, homogeneous cooperation can also enhance LLM inference.
Distributed computing like model parallelism is aimed at addressing memory limitations associated with LLMs. As model sizes continue to grow, a single hardware unit may not be able to accommodate the entire model.
Model parallelism splits the model into multiple parts, with different hardware units processing different segments of the model concurrently.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS6.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.6.2 </span>CPU</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS6.SSS2.p1">
<p class="ltx_p" id="S3.SS6.SSS2.p1.1">Distributed computing emerges as a prevalent strategy to mitigate single-node memory constraints and expedite LLM inference performance.
He et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib210" title="">210</a>]</cite> propose an efficient distributed inference optimization solution for LLMs on CPUs.
On four 5th Gen Intel Xeon Scalable Processors the result shows the generation speed on Qwen-72B is 7.14 tokens/s.
He et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib211" title="">211</a>]</cite> also propose new attention flow, SlimAttention, to reduce the KV cache size and ensure precision for efficient LLM inference on CPUs.
The experimental results on Llama2-70b shows the latency of token generation is 4 tokens/s with 2 sockets and 11.4 tokens/s with 8 sockets on Intel Xeon 8563C CPUs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS6.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.6.3 </span>FPGA</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS6.SSS3.p1">
<p class="ltx_p" id="S3.SS6.SSS3.p1.1">DFX&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib212" title="">212</a>]</cite> is a multi-FPGA acceleration which uses model parallelism and optimized dataflow to improve LLM inference speed in both prefilling and decoding phases.
With the implementation on four Xilinx Alveo U280 FPGAs, DFX achieves about 120 tokens/s for GPT2-1.5B model.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Setup</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">In this section, we compare the performance of state-of-the-art optimization methods on hardware accelerators including CPUs, GPUs, FPGAs, ASICs and PIM/NDPs for generative LLM.
In traditional hardware capability comparisons, the focus is usually on peak computational performance and power consumption.
However, in the context of generative large models, the emphasis shifts to inference speed (<span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">tokens per second, tokens/s</span>) and hardware power consumption (<span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.2">Watt, W</span>).
The slope of the curve representing inference speed on the Y-axis versus hardware power consumption on the X-axis indicates the hardware efficiency in terms of tokens per joule (<span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.3">tokens/J</span>).
Therefore, we have collected data on each hardware platform along with all optimization methods, highlighting their inference speed (mentioned explicitly or estimated) and actual power consumption while running generative large models (<math alttext="\sim" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mo id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><csymbol cd="latexml" id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">∼</annotation></semantics></math> 7 billion parameters) with batch size 1 and 8, as shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S4.F13" title="Figure 13 ‣ 4.2.1 Small batch size (bs=1) ‣ 4.2 Hardware Comparison ‣ 4 Discussion ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_tag">13</span></a> and Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S4.F14" title="Figure 14 ‣ 4.2.1 Small batch size (bs=1) ‣ 4.2 Hardware Comparison ‣ 4 Discussion ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_tag">14</span></a>, respectively.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">In collecting the data, the sources for absolute inference speed are categorized into five types:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">For most CPU and GPU hardware platforms, the absolute inference speed is typically reported in the literature alongside the specific model being run.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">For GPU platforms with open-source and reproducible code, we directly measure the absolute inference speed through actual execution.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">There is limited work on FPGA hardware platforms. Some papers provide verified absolute inference data through physical testing, while others offer estimates without hardware validation. Consequently, we also include estimated absolute inference speeds for these cases.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1">For most ASIC platforms, we first define a post-silicon work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib126" title="">126</a>]</cite> with an absolute inference speed as the baseline and scale the results by the peak computational performance (TOPS or GOPS).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S4.I1.i5.p1">
<p class="ltx_p" id="S4.I1.i5.p1.1">Some works provide acceleration ratios relative to GPUs or other hardware platforms. In these cases, we scale existing data according to the reported acceleration ratios to estimate the absolute inference speed.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">And the sources of power consumption are categorized into three types:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1">For most CPU, and some GPU and FPGA hardware platforms, the papers typically specify the hardware model and quantity used. We use these specifications to determine the actual power consumption.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1">For GPU platforms with open-source and reproducible code, we directly measure the actual power consumption through real-world execution.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1">For most ASIC and PIM/NDP platforms, we consider both on-chip and off-chip memory access power consumption. On-chip power consumption is often reported in the literature. Off-chip power consumption is calculated by multiplying the energy per bit of the off-chip memory type by the model parameter count and then by the absolute inference speed.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Hardware Comparison</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Small batch size (bs=1)</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">In Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S4.F13" title="Figure 13 ‣ 4.2.1 Small batch size (bs=1) ‣ 4.2 Hardware Comparison ‣ 4 Discussion ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_tag">13</span></a>, for <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p1.1.1">CPU</span> platforms, power consumption ranges from 3W to 575W, with inference speeds between 3 tokens/s and 50 tokens/s, located in the bottom part of the figure. The energy efficiency ranges from 0.014 tokens/J to 2.38 token/J.
Additionally, we observe edge-side CPUs (including CPU SoCs) with 3W to 6W power consumption exhibit higher energy efficiency (0.544 tokens/J to 2.38 tokens/J) compared to GPUs.
For <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p1.1.2">GPU</span> platforms, power consumption ranges from 40W to 700W, with inference speeds between 18 tokens/s and 194 tokens/s, situated in the middle right part of the figure. The energy efficiency ranges from 0.067 tokens/J to 0.825 token/J.
For <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p1.1.3">FPGA</span> platforms, power consumption ranges from 45W to 225W, with inference speeds between 40 tokens/s and 333 tokens/s, also in the middle part of the figure. The energy efficiency ranges from over 0.178 tokens/J to 1.85 tokens/J, which is higher than GPU platforms and similar with edge-side CPUs.
For <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p1.1.4">ASIC</span> platforms, power consumption ranges from 6.3W to 600W, with inference speeds between 9.173 tokens/s and 1800 tokens/s, found in the upper left part of the figure. The energy efficiency ranges from 0.108 tokens/J to over 18.557 tokens/J, outperforming CPU, GPU and FPGA platforms.
For <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p1.1.5">PIM/NDP</span> platforms, power consumption ranges from 11.516W to 240W, with inference speeds between 42.68 tokens/s and 1998 tokens/s, located in the upper left part of the figure. The energy efficiency ranges from 0.53 tokens/J to 46.66 tokens/J, outperforming other hardware platforms.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="663" id="S4.F13.g1" src="https://arxiv.org/html/2410.04466v1/x13.png" width="789">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>LLM (<math alttext="\sim" class="ltx_Math" display="inline" id="S4.F13.2.m1.1"><semantics id="S4.F13.2.m1.1b"><mo id="S4.F13.2.m1.1.1" xref="S4.F13.2.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.F13.2.m1.1c"><csymbol cd="latexml" id="S4.F13.2.m1.1.1.cmml" xref="S4.F13.2.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.F13.2.m1.1d">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.F13.2.m1.1e">∼</annotation></semantics></math> 7 billion parameters) decode stage throughput (batch size 1) vs power on different platforms with different optimization methods.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S4.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="653" id="S4.F14.g1" src="https://arxiv.org/html/2410.04466v1/x14.png" width="789">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>LLM (<math alttext="\sim" class="ltx_Math" display="inline" id="S4.F14.2.m1.1"><semantics id="S4.F14.2.m1.1b"><mo id="S4.F14.2.m1.1.1" xref="S4.F14.2.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.F14.2.m1.1c"><csymbol cd="latexml" id="S4.F14.2.m1.1.1.cmml" xref="S4.F14.2.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.F14.2.m1.1d">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.F14.2.m1.1e">∼</annotation></semantics></math> 7 billion parameters) decode stage throughput (batch size 8) vs power on different platforms with different optimization methods.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Large batch size (bs=8)</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.4">Compared to small batch size, the results of inference throughput on large batch size are limited. We can only collect seldom results in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S4.F14" title="Figure 14 ‣ 4.2.1 Small batch size (bs=1) ‣ 4.2 Hardware Comparison ‣ 4 Discussion ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_tag">14</span></a>.
For <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS2.p1.4.1">GPU</span> platforms, power consumption ranges from 45W to 464W, with inference speeds between 46 tokens/s and 1033 tokens/s. The energy efficiency ranges from 0.527 tokens/J to 3.155 token/J.
For <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS2.p1.4.2">FPGA</span> platforms, power consumption ranges from 180W to 255W, with inference speeds between 220 tokens/s and 370 tokens/s, also in the middle right part of the figure. The energy efficiency ranges from over 0.978 tokens/J to 2.056 tokens/J.
For <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS2.p1.4.3">ASIC</span> platforms, power consumption ranges from 150W to 600W, with inference speeds between 516 tokens/s and 1176 tokens/s. The energy efficiency ranges from 0.86 tokens/J to over 7.84 tokens/J.
For <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS2.p1.4.4">PIM/NDP</span> platforms, power consumption is about 75W, with inference speeds about 3000 tokens/s. The energy efficiency is up to 40 tokens/J.
Compared to small batch size 1, larger batch size 8 can achieve significantly higher throughput. For example, on a GPU, throughput increases from 18-194 tokens/s to 46-1033 tokens/s, representing an improvement of 2.56<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p1.1.m1.1"><semantics id="S4.SS2.SSS2.p1.1.m1.1a"><mo id="S4.SS2.SSS2.p1.1.m1.1.1" xref="S4.SS2.SSS2.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.1.m1.1b"><times id="S4.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p1.1.m1.1d">×</annotation></semantics></math>-5.32<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p1.2.m2.1"><semantics id="S4.SS2.SSS2.p1.2.m2.1a"><mo id="S4.SS2.SSS2.p1.2.m2.1.1" xref="S4.SS2.SSS2.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.2.m2.1b"><times id="S4.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p1.2.m2.1d">×</annotation></semantics></math>. Similarly, energy efficiency improves from 0.067-0.825 tokens/J to 0.527-3.155 tokens/J, an increase of 3.82<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p1.3.m3.1"><semantics id="S4.SS2.SSS2.p1.3.m3.1a"><mo id="S4.SS2.SSS2.p1.3.m3.1.1" xref="S4.SS2.SSS2.p1.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.3.m3.1b"><times id="S4.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS2.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p1.3.m3.1d">×</annotation></semantics></math>-7.87<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p1.4.m4.1"><semantics id="S4.SS2.SSS2.p1.4.m4.1a"><mo id="S4.SS2.SSS2.p1.4.m4.1.1" xref="S4.SS2.SSS2.p1.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.4.m4.1b"><times id="S4.SS2.SSS2.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS2.p1.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p1.4.m4.1d">×</annotation></semantics></math>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Optimization Method Comparison</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Small batch size (bs=1)</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">We then compare different optimization methods on the same platforms.
In Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S4.F13" title="Figure 13 ‣ 4.2.1 Small batch size (bs=1) ‣ 4.2 Hardware Comparison ‣ 4 Discussion ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_tag">13</span></a>, the methods employed on <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.1.1">CPU</span> platforms include quantization, sparsity, and heterogeneous cooperation. Quantization can achieve higher absolute inference speeds, reaching approximately 50 tokens/s, while sparsity and heterogeneous cooperation achieve speeds of 16.3 tokens/s and 11.7 tokens/s, respectively. Quantization and heterogeneous cooperation can achieve higher energy efficiency with 2.38 tokens/J and 1.86 tokens/J, respectively, compared to 0.16 tokens/J for sparsity.
On <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.1.2">GPU</span> platforms, the methods used include quantization, sparsity, fast decoding, and operator optimization. Quantization, fast decoding, and operator optimization can achieve higher absolute inference speeds, reaching approximately 194 tokens/s, 169.68 tokens/s, and 145.04 tokens/s, respectively, while sparsity achieves only about 50 tokens/s. Regarding efficiency, these three methods show relatively small differences in performance compared to sparsity (0.16 tokens/J), with efficiencies of 0.825 tokens/J, 0.587 tokens/J, and 0.46 tokens/J, respectively.
On <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.1.3">FPGA</span> platforms, the methods employed include quantization and sparsity, which are often used together. The highest on-board deployed speed reaches 92.5 tokens/s, with an efficiency of up to 1.32 tokens/J. Some works estimate that FPGA can achieve a maximum inference speed of 333 tokens/s and an efficiency of 1.85 tokens/J.
On <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.1.4">ASIC</span> platforms, the methods employed include quantization, sparsity, and operator optimization. Operator optimization can achieve the highest absolute inference speeds, reaching approximately 1800 tokens/s. In comparison, sparsity and quantization achieve speeds of 42.664 tokens/s and 161.086 tokens/s, respectively. In terms of efficiency, quantization and operator optimization offer higher energy efficiency with 7.434 tokens/J and 18.557 tokens/J, respectively. Sparsity shows lower energy efficiency with only 1.421 tokens/J.
On <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.1.5">PIM/NDP</span> platforms, the main methods used are quantization, fast decoding, and heterogeneous cooperation. Quantization can achieve higher throughput with 1998 tokens/s while fast decoding and heterogeneous cooperation only achieve 174.018 tokens/s and 330 tokens/s, respectively. In terms of efficiency, quantization can achieve higher energy efficiency with 46.66 tokens/J while fast decoding and heterogeneous cooperation only achieve 9.94 tokens/J and 4.34 tokens/s, respectively.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Large batch size (bs=8)</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1">In Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S4.F14" title="Figure 14 ‣ 4.2.1 Small batch size (bs=1) ‣ 4.2 Hardware Comparison ‣ 4 Discussion ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_tag">14</span></a>, on <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS2.p1.1.1">GPU</span> platforms, the methods used include quantization, sparsity, and operator optimization. Quantization and operator optimization achieve higher absolute inference speeds, reaching approximately 1000 tokens/s and 1033 tokens/s, respectively, while sparsity achieves about 290 tokens/s. Regarding efficiency, quantization, and operator optimization also achieve higher energy efficiency with 3.155 tokens/J and 2.3275 tokens/s compared to sparsity (0.725 tokens/J).
On <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS2.p1.1.2">FPGA</span> platforms, the methods employed include quantization and sparsity, which are often used together. The highest inference speed reaches 370 tokens/s, with an efficiency of up to 2.056 tokens/J.
On <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS2.p1.1.3">ASIC</span> platforms, the methods employed include operator optimization. It can achieve higher absolute inference speeds, reaching approximately 1176.47 tokens/s with 7.843 tokens/J energy efficiency.
On <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS2.p1.1.4">PIM/NDP</span> platforms, the main methods used are heterogeneous cooperation. Heterogeneous cooperation can achieve an inference speed of about 3000 tokens/s and 40 tokens/J.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F15"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="668" id="S4.F15.g1" src="https://arxiv.org/html/2410.04466v1/x15.png" width="789">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>LLM (<math alttext="\sim" class="ltx_Math" display="inline" id="S4.F15.2.m1.1"><semantics id="S4.F15.2.m1.1b"><mo id="S4.F15.2.m1.1.1" xref="S4.F15.2.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.F15.2.m1.1c"><csymbol cd="latexml" id="S4.F15.2.m1.1.1.cmml" xref="S4.F15.2.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.F15.2.m1.1d">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.F15.2.m1.1e">∼</annotation></semantics></math> 7 billion parameters) inference for different edge-side scenarios.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Discussion on Edge-side Scenarios</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">In Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#S4.F15" title="Figure 15 ‣ 4.3.2 Large batch size (bs=8) ‣ 4.3 Optimization Method Comparison ‣ 4 Discussion ‣ Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"><span class="ltx_text ltx_ref_tag">15</span></a>, we also analyze two typical edge-side LLM inference application scenarios: AI smartphones and robots.
For the <span class="ltx_text ltx_font_bold" id="S4.SS4.p1.1.1">AI smartphones</span>, edge hardware typically requires power consumption to be limited to 10W or less, and with human reading speeds around 10 tokens/s&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib230" title="">230</a>]</cite>, the inference speed requirement is relatively low. The lower left corner of the plot corresponds to this scenario. A notable example is the Qualcomm Snapdragon 8 Gen 3 chip&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib65" title="">65</a>]</cite>, which utilizes quantization techniques to optimize the model, allowing it to achieve the necessary inference speed while staying under the 10W power limit. And MARCA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib183" title="">183</a>]</cite> and Tender&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib126" title="">126</a>]</cite> can also satisfy the inference demand.
Such optimization strategies have enabled some edge CPUs and ASICs to meet the inference needs of smartphone applications.
For the <span class="ltx_text ltx_font_bold" id="S4.SS4.p1.1.2">robots</span>, the requirements for inference speed are significantly higher. To support real-time systems with an operational frequency of 100-1000Hz&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib231" title="">231</a>]</cite>, the inference speed must reach 100-1000 tokens/s, while the hardware power consumption typically needs to reach around 20W. According to the data in the plot, although some pre-silicon designs meet this requirement, they are currently limited to the pre-simulation stage and have not yet undergone post-silicon validation. For instance, Guo et al.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib127" title="">127</a>]</cite> and SpecPIM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04466v1#bib.bib168" title="">168</a>]</cite> show promising results in simulation, indicating that they could meet the power and speed demands, but whether they can actually fulfill the requirements for robot applications still requires further post-silicon validation and tape-out verification.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Generative LLMs like GPT series and Llama series are currently the main focus due to their high algorithm performance.
The advancements in generative LLMs are closely intertwined with the development of hardware capabilities.
This paper presents a comprehensive survey of efficient generative LLM inference on different hardware platforms.
We provide an overview of the algorithm architecture of mainstream generative LLMs and summarize different optimization methods for different platforms such as CPU, GPU, FPGA, ASIC, and PIM/NDP.
Furthermore, we perform a qualitative and quantitative comparison of inference performance with batch sizes 1 and 8 on different hardware platforms by considering hardware power consumption, absolute inference speed (tokens/s), and energy efficiency (tokens/J).
And we also point to the future trends and potential developments of generative LLMs and hardware technology for edge-side scenarios.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuhong Mo, Hao Qin, Yushan Dong, Ziyi Zhu, and Zhenglin Li.

</span>
<span class="ltx_bibblock">Large language model (llm) ai text generation detection based on transformer deep learning algorithm.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2405.06652</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yonghui Wu.

</span>
<span class="ltx_bibblock">Large language model and text generation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Natural Language Processing in Biomedicine: A Practical Guide</span>, pages 265–297. Springer, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Junyi Li, Tianyi Tang, Wayne&nbsp;Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen.

</span>
<span class="ltx_bibblock">Pre-trained language models for text generation: A survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">ACM Computing Surveys</span>, 56(9):1–39, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lei Shu, Liangchen Luo, Jayakumar Hoskere, Yun Zhu, Yinxiao Liu, Simon Tong, Jindong Chen, and Lei Meng.

</span>
<span class="ltx_bibblock">Rewritelm: An instruction-tuned large language model for text rewriting.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</span>, volume&nbsp;38, pages 18970–18980, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shailja Thakur, Baleegh Ahmad, Hammond Pearce, Benjamin Tan, Brendan Dolan-Gavitt, Ramesh Karri, and Siddharth Garg.

</span>
<span class="ltx_bibblock">Verigen: A large language model for verilog code generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">ACM Transactions on Design Automation of Electronic Systems</span>, 29(3):1–31, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jacob Devlin.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:1810.04805</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:1907.11692</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kevin Clark, Minh-Thang Luong, Quoc&nbsp;V Le, and Christopher&nbsp;D Manning.

</span>
<span class="ltx_bibblock">Electra: Pre-training text encoders as discriminators rather than generators.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2003.10555</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Deberta: Decoding-enhanced bert with disentangled attention.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2006.03654</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et&nbsp;al.

</span>
<span class="ltx_bibblock">Improving language understanding by generative pre-training.

</span>
<span class="ltx_bibblock">2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et&nbsp;al.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">OpenAI blog</span>, 1(8):9, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tom&nbsp;B Brown.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">arXiv preprint ArXiv:2005.14165</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia&nbsp;Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et&nbsp;al.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2303.08774</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">Journal of machine learning research</span>, 21(140):1–67, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi&nbsp;Victoria Lin, et&nbsp;al.

</span>
<span class="ltx_bibblock">Opt: Open pre-trained transformer language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2205.01068</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Teven Le&nbsp;Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra&nbsp;Sasha Luccioni, François Yvon, Matthias Gallé, et&nbsp;al.

</span>
<span class="ltx_bibblock">Bloom: A 176b-parameter open-access multilingual language model.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
AI&nbsp;Meta.

</span>
<span class="ltx_bibblock">Introducing llama: A foundational, 65-billion-parameter large language model.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">Meta AI</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:2307.09288</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Meta AI.

</span>
<span class="ltx_bibblock">Introducing LLaMA 3: Meta’s latest large language model, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M&nbsp;Lewis.

</span>
<span class="ltx_bibblock">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:1910.13461</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.

</span>
<span class="ltx_bibblock">Glm: General language model pretraining with autoregressive blank infilling.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:2103.10360</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L&nbsp;Xue.

</span>
<span class="ltx_bibblock">mt5: A massively multilingual pre-trained text-to-text transformer.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2010.11934</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
William Fedus, Barret Zoph, and Noam Shazeer.

</span>
<span class="ltx_bibblock">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2101.03961</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi&nbsp;Liao, Zhiwei Wang, Xin Jiang, ZhenZhang Yang, Kaisheng Wang, Xiaoda Zhang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Pangu-<math alttext="alpha" class="ltx_Math" display="inline" id="bib.bib24.1.m1.1"><semantics id="bib.bib24.1.m1.1a"><mrow id="bib.bib24.1.m1.1.1" xref="bib.bib24.1.m1.1.1.cmml"><mi id="bib.bib24.1.m1.1.1.2" xref="bib.bib24.1.m1.1.1.2.cmml">a</mi><mo id="bib.bib24.1.m1.1.1.1" xref="bib.bib24.1.m1.1.1.1.cmml">⁢</mo><mi id="bib.bib24.1.m1.1.1.3" xref="bib.bib24.1.m1.1.1.3.cmml">l</mi><mo id="bib.bib24.1.m1.1.1.1a" xref="bib.bib24.1.m1.1.1.1.cmml">⁢</mo><mi id="bib.bib24.1.m1.1.1.4" xref="bib.bib24.1.m1.1.1.4.cmml">p</mi><mo id="bib.bib24.1.m1.1.1.1b" xref="bib.bib24.1.m1.1.1.1.cmml">⁢</mo><mi id="bib.bib24.1.m1.1.1.5" xref="bib.bib24.1.m1.1.1.5.cmml">h</mi><mo id="bib.bib24.1.m1.1.1.1c" xref="bib.bib24.1.m1.1.1.1.cmml">⁢</mo><mi id="bib.bib24.1.m1.1.1.6" xref="bib.bib24.1.m1.1.1.6.cmml">a</mi></mrow><annotation-xml encoding="MathML-Content" id="bib.bib24.1.m1.1b"><apply id="bib.bib24.1.m1.1.1.cmml" xref="bib.bib24.1.m1.1.1"><times id="bib.bib24.1.m1.1.1.1.cmml" xref="bib.bib24.1.m1.1.1.1"></times><ci id="bib.bib24.1.m1.1.1.2.cmml" xref="bib.bib24.1.m1.1.1.2">𝑎</ci><ci id="bib.bib24.1.m1.1.1.3.cmml" xref="bib.bib24.1.m1.1.1.3">𝑙</ci><ci id="bib.bib24.1.m1.1.1.4.cmml" xref="bib.bib24.1.m1.1.1.4">𝑝</ci><ci id="bib.bib24.1.m1.1.1.5.cmml" xref="bib.bib24.1.m1.1.1.5">ℎ</ci><ci id="bib.bib24.1.m1.1.1.6.cmml" xref="bib.bib24.1.m1.1.1.6">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib24.1.m1.1c">alpha</annotation><annotation encoding="application/x-llamapun" id="bib.bib24.1.m1.1d">italic_a italic_l italic_p italic_h italic_a</annotation></semantics></math>: Large-scale autoregressive pretrained chinese language models with auto-parallel computation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.2.1">arXiv preprint arXiv:2104.12369</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yu&nbsp;Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen, Yanbin Zhao, Yuxiang Lu, et&nbsp;al.

</span>
<span class="ltx_bibblock">Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2107.02137</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Victor Sanh, Albert Webson, Colin Raffel, Stephen&nbsp;H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven&nbsp;Le Scao, Arun Raja, et&nbsp;al.

</span>
<span class="ltx_bibblock">Multitask prompted training enables zero-shot task generalization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2110.08207</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nan Du, Yanping Huang, Andrew&nbsp;M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams&nbsp;Wei Yu, Orhan Firat, et&nbsp;al.

</span>
<span class="ltx_bibblock">Glam: Efficient scaling of language models with mixture-of-experts.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">International Conference on Machine Learning</span>, pages 5547–5569. PMLR, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shuohuan Wang, Yu&nbsp;Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong, Shikun Feng, Junyuan Shang, Yanbin Zhao, Chao Pang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Ernie 3.0 titan: Exploring larger-scale knowledge enhanced pre-training for language understanding and generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2112.12731</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung&nbsp;Won Chung, Charles Sutton, Sebastian Gehrmann, et&nbsp;al.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2204.02311</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de&nbsp;Las Casas, Lisa&nbsp;Anne Hendricks, Johannes Welbl, Aidan Clark, et&nbsp;al.

</span>
<span class="ltx_bibblock">Training compute-optimal large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:2203.15556</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Romal Thoppilan, Daniel De&nbsp;Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu&nbsp;Du, et&nbsp;al.

</span>
<span class="ltx_bibblock">Lamda: Language models for dialog applications.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2201.08239</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">About claude models, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da&nbsp;Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, et&nbsp;al.

</span>
<span class="ltx_bibblock">Chatglm: A family of large language models from glm-130b to glm-4 all tools.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2406.12793</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugging Face.

</span>
<span class="ltx_bibblock">Chatglm2-6b model documentation, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugging Face.

</span>
<span class="ltx_bibblock">Chatglm3-6b model documentation, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi&nbsp;Lin, Zhuohan Li, Dacheng Li, Eric Xing, et&nbsp;al.

</span>
<span class="ltx_bibblock">Judging llm-as-a-judge with mt-bench and chatbot arena.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">Advances in Neural Information Processing Systems</span>, 36:46595–46623, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et&nbsp;al.

</span>
<span class="ltx_bibblock">The falcon series of open language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2311.16867</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Albert&nbsp;Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra&nbsp;Singh Chaplot, Diego de&nbsp;las Casas, Emma&nbsp;Bou Hanna, Florian Bressand, et&nbsp;al.

</span>
<span class="ltx_bibblock">Mixtral of experts.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2401.04088</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Bo&nbsp;Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, et&nbsp;al.

</span>
<span class="ltx_bibblock">Rwkv: Reinventing rnns for the transformer era.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:2305.13048</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
x.ai.

</span>
<span class="ltx_bibblock">Announcing grok, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Albert&nbsp;Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra&nbsp;Singh Chaplot, Diego de&nbsp;las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et&nbsp;al.

</span>
<span class="ltx_bibblock">Mistral 7b.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2310.06825</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew&nbsp;M Dai, Anja Hauth, et&nbsp;al.

</span>
<span class="ltx_bibblock">Gemini: a family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2312.11805</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Albert Gu and Tri Dao.

</span>
<span class="ltx_bibblock">Mamba: Linear-time sequence modeling with selective state spaces.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:2312.00752</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tri Dao and Albert Gu.

</span>
<span class="ltx_bibblock">Transformers are ssms: Generalized models and efficient algorithms through structured state space duality.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:2405.21060</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gemma Team, Morgane Riviere, Shreya Pathak, Pier&nbsp;Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et&nbsp;al.

</span>
<span class="ltx_bibblock">Gemma 2: Improving open language models at a practical size.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">arXiv preprint arXiv:2408.00118</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Introducing GPT-4O, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu&nbsp;Han, Fei Huang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Qwen technical report.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">arXiv preprint arXiv:2309.16609</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Qwen Team.

</span>
<span class="ltx_bibblock">Qwen2.5: A party of foundation models, September 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
An&nbsp;Yang, Baosong Yang, Binyuan Hui, Bo&nbsp;Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Qwen2 technical report.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">arXiv preprint arXiv:2407.10671</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Qwen Team.

</span>
<span class="ltx_bibblock">Introducing qwen1.5, February 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Meta AI.

</span>
<span class="ltx_bibblock">Introducing LLaMA 3: Meta’s latest large language model, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Meta AI.

</span>
<span class="ltx_bibblock">Llama 3.2, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et&nbsp;al.

</span>
<span class="ltx_bibblock">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">arXiv preprint arXiv:2403.05530</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
DeepMind.

</span>
<span class="ltx_bibblock">Gemini ultra, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Claude 3 haiku: our fastest model yet, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-3.5 turbo model documentation, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Grok.

</span>
<span class="ltx_bibblock">Introducing grok-0, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
xAI.

</span>
<span class="ltx_bibblock">Open release of grok-1, 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://x.ai/blog/grok-os" title="">https://x.ai/blog/grok-os</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Robert&nbsp;R Schaller.

</span>
<span class="ltx_bibblock">Moore’s law: past, present and future.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib59.1.1">IEEE spectrum</span>, 34(6):52–59, 1997.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom&nbsp;B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.

</span>
<span class="ltx_bibblock">Scaling laws for neural language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib60.1.1">arXiv preprint arXiv:2001.08361</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Introducing openai o1, 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/o1/" title="">https://openai.com/o1/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shane Cook.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib62.1.1">CUDA programming: a developer’s guide to parallel computing with GPUs</span>.

</span>
<span class="ltx_bibblock">Newnes, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Denis Foley and John Danskin.

</span>
<span class="ltx_bibblock">Ultra-performance pascal gpu and nvlink interconnect.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib63.1.1">IEEE Micro</span>, 37(2):7–17, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Apple Inc.

</span>
<span class="ltx_bibblock">Apple introduces m2 ultra, 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.apple.com/newsroom/2023/06/apple-introduces-m2-ultra/" title="">https://www.apple.com/newsroom/2023/06/apple-introduces-m2-ultra/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Inc. Qualcomm&nbsp;Technologies.

</span>
<span class="ltx_bibblock">Snapdragon 8 gen 3 mobile platform, 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-8-gen-3-mobile-platform" title="">https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-8-gen-3-mobile-platform</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Intel Inc.

</span>
<span class="ltx_bibblock">4th gen intel xeon scalable processors with built-in accelerators, 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.intel.com/content/www/us/en/products/docs/processors/xeon-accelerated/4th-gen-xeon-scalable-processors.html" title="">https://www.intel.com/content/www/us/en/products/docs/processors/xeon-accelerated/4th-gen-xeon-scalable-processors.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
NVIDIA Inc.

</span>
<span class="ltx_bibblock">Nvidia tesla v100 gpu architecture, 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf" title="">https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
NVIDIA Inc.

</span>
<span class="ltx_bibblock">Nvidia a100 tensor core gpu architecture, 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf" title="">https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
NVIDIA Inc.

</span>
<span class="ltx_bibblock">Nvidia h100 tensor core gpu architecture, 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://resources.nvidia.com/en-us-data-center-overview/gtc22-whitepaper-hopper" title="">https://resources.nvidia.com/en-us-data-center-overview/gtc22-whitepaper-hopper</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
AMD Inc.

</span>
<span class="ltx_bibblock">Amd cdna architecture, 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.amd.com/content/dam/amd/en/documents/instinct-business-docs/white-papers/amd-cdna-white-paper.pdf" title="">https://www.amd.com/content/dam/amd/en/documents/instinct-business-docs/white-papers/amd-cdna-white-paper.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
AMD Inc.

</span>
<span class="ltx_bibblock">Amd cdna 2 architecture, 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.amd.com/content/dam/amd/en/documents/instinct-business-docs/white-papers/amd-cdna2-white-paper.pdf" title="">https://www.amd.com/content/dam/amd/en/documents/instinct-business-docs/white-papers/amd-cdna2-white-paper.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
AMD Inc.

</span>
<span class="ltx_bibblock">Amd cdna 3 architecture, 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/white-papers/amd-cdna-3-white-paper.pdf" title="">https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/white-papers/amd-cdna-3-white-paper.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Hongyi Jin, Tianqi Chen, and Zhihao Jia.

</span>
<span class="ltx_bibblock">Towards efficient generative large language model serving: A survey from algorithms to systems.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib73.1.1">arXiv preprint arXiv:2312.15234</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang.

</span>
<span class="ltx_bibblock">A survey on model compression for large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib74.1.1">arXiv preprint arXiv:2308.07633</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Guanqiao Qu, Qiyuan Chen, Wei Wei, Zheng Lin, Xianhao Chen, and Kaibin Huang.

</span>
<span class="ltx_bibblock">Mobile edge intelligence for large language models: A contemporary survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib75.1.1">arXiv preprint arXiv:2407.18921</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Seungcheol Park, Jaehyeon Choi, Sojin Lee, and U&nbsp;Kang.

</span>
<span class="ltx_bibblock">A comprehensive survey of compression algorithms for language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib76.1.1">arXiv preprint arXiv:2401.15347</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zixuan Zhou, Xuefei Ning, Ke&nbsp;Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, et&nbsp;al.

</span>
<span class="ltx_bibblock">A survey on efficient inference for large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib77.1.1">arXiv preprint arXiv:2404.14294</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Christoforos Kachris.

</span>
<span class="ltx_bibblock">A survey on hardware accelerators for large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib78.1.1">arXiv preprint arXiv:2401.09890</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nikoletta Koilia and Christoforos Kachris.

</span>
<span class="ltx_bibblock">Hardware acceleration of llms: A comprehensive survey and comparison.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib79.1.1">arXiv preprint arXiv:2409.03384</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Christopher Wolters, Xiaoxuan Yang, Ulf Schlichtmann, and Toyotaro Suzumura.

</span>
<span class="ltx_bibblock">Memory is all you need: An overview of compute-in-memory architectures for accelerating large language model inference.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib80.1.1">arXiv preprint arXiv:2406.08413</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Beom&nbsp;Jin Kang, Hae&nbsp;In Lee, Seok&nbsp;Kyu Yoon, Young&nbsp;Chan Kim, Sang&nbsp;Beom Jeong, Hyun Kim, et&nbsp;al.

</span>
<span class="ltx_bibblock">A survey of fpga and asic designs for transformer inference acceleration and optimization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib81.1.1">Journal of Systems Architecture</span>, page 103247, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A&nbsp;Vaswani.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib82.1.1">Advances in Neural Information Processing Systems</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc&nbsp;V Le, and Ruslan Salakhutdinov.

</span>
<span class="ltx_bibblock">Transformer-xl: Attentive language models beyond a fixed-length context.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib83.1.1">arXiv preprint arXiv:1901.02860</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret.

</span>
<span class="ltx_bibblock">Transformers are rnns: Fast autoregressive transformers with linear attention.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib84.1.1">International conference on machine learning</span>, pages 5156–5165. PMLR, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind.

</span>
<span class="ltx_bibblock">An attention free transformer.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib85.1.1">arXiv preprint arXiv:2105.14103</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Bo&nbsp;Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, et&nbsp;al.

</span>
<span class="ltx_bibblock">Rwkv: Reinventing rnns for the transformer era.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib86.1.1">arXiv preprint arXiv:2305.13048</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hanting Chen, Zhicheng Liu, Xutao Wang, Yuchuan Tian, and Yunhe Wang.

</span>
<span class="ltx_bibblock">Dijiang: Efficient large language models through compact kernelization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib87.1.1">arXiv preprint arXiv:2403.19928</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_tag_bibitem">[88]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Albert Gu, Karan Goel, and Christopher Ré.

</span>
<span class="ltx_bibblock">Efficiently modeling long sequences with structured state spaces.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib88.1.1">arXiv preprint arXiv:2111.00396</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_tag_bibitem">[89]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur.

</span>
<span class="ltx_bibblock">Long range language modeling via gated state spaces.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib89.1.1">arXiv preprint arXiv:2206.13947</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_tag_bibitem">[90]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel&nbsp;Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Ré.

</span>
<span class="ltx_bibblock">Hyena hierarchy: Towards larger convolutional language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib90.1.1">International Conference on Machine Learning</span>, pages 28043–28078. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_tag_bibitem">[91]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, and Yunhe Wang.

</span>
<span class="ltx_bibblock">Densemamba: State space models with dense hidden connection for efficient large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib91.1.1">arXiv preprint arXiv:2403.00818</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_tag_bibitem">[92]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jonathan Pilault, Mahan Fathi, Orhan Firat, Chris Pal, Pierre-Luc Bacon, and Ross Goroshin.

</span>
<span class="ltx_bibblock">Block-state transformers.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib92.1.1">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_tag_bibitem">[93]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Soham De, Samuel&nbsp;L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et&nbsp;al.

</span>
<span class="ltx_bibblock">Griffin: Mixing gated linear recurrences with local attention for efficient language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib93.1.1">arXiv preprint arXiv:2402.19427</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_tag_bibitem">[94]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et&nbsp;al.

</span>
<span class="ltx_bibblock">Jamba: A hybrid transformer-mamba language model.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib94.1.1">arXiv preprint arXiv:2403.19887</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_tag_bibitem">[95]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal.

</span>
<span class="ltx_bibblock">Leave no context behind: Efficient infinite context transformers with infini-attention.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib95.1.1">arXiv preprint arXiv:2404.07143</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_tag_bibitem">[96]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, and Chunting Zhou.

</span>
<span class="ltx_bibblock">Megalodon: Efficient llm pretraining and inference with unlimited context length.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib96.1.1">arXiv preprint arXiv:2404.08801</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_tag_bibitem">[97]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Haihao Shen, Hanwen Chang, Bo&nbsp;Dong, Yu&nbsp;Luo, and Hengyu Meng.

</span>
<span class="ltx_bibblock">Efficient llm inference on cpus.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib97.1.1">arXiv preprint arXiv:2311.00502</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_tag_bibitem">[98]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jianyu Wei, Shijie Cao, Ting Cao, Lingxiao Ma, Lei Wang, Yanyong Zhang, and Mao Yang.

</span>
<span class="ltx_bibblock">T-mac: Cpu renaissance via table lookup for low-bit llm deployment on edge.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib98.1.1">arXiv preprint arXiv:2407.00088</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_tag_bibitem">[99]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
ggerganov.

</span>
<span class="ltx_bibblock">llama.cpp, 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ggerganov/llama.cpp" title="">https://github.com/ggerganov/llama.cpp</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_tag_bibitem">[100]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tianyi Zhang, Jonah&nbsp;Wonkyu Yi, Bowen Yao, Zhaozhuo Xu, and Anshumali Shrivastava.

</span>
<span class="ltx_bibblock">Nomad-attention: Efficient llm inference on cpus through multiply-add-free attention.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib100.1.1">arXiv preprint arXiv:2403.01273</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_tag_bibitem">[101]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.

</span>
<span class="ltx_bibblock">Gptq: Accurate post-training quantization for generative pre-trained transformers.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib101.1.1">arXiv preprint arXiv:2210.17323</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_tag_bibitem">[102]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ji&nbsp;Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han.

</span>
<span class="ltx_bibblock">Awq: Activation-aware weight quantization for on-device llm compression and acceleration.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib102.1.1">Proceedings of Machine Learning and Systems</span>, 6:87–100, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_tag_bibitem">[103]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh.

</span>
<span class="ltx_bibblock">Spqr: A sparse-quantized representation for near-lossless llm weight compression.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib103.1.1">arXiv preprint arXiv:2306.03078</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_tag_bibitem">[104]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael&nbsp;W Mahoney, and Kurt Keutzer.

</span>
<span class="ltx_bibblock">Squeezellm: Dense-and-sparse quantization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib104.1.1">arXiv preprint arXiv:2306.07629</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_tag_bibitem">[105]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shiyao Li, Xuefei Ning, Ke&nbsp;Hong, Tengxuan Liu, Luning Wang, Xiuhong Li, Kai Zhong, Guohao Dai, Huazhong Yang, and Yu&nbsp;Wang.

</span>
<span class="ltx_bibblock">Llm-mq: Mixed-precision quantization for efficient llm deployment.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib105.1.1">The Efficient Natural Language and Speech Processing Workshop with NeurIPS</span>, volume&nbsp;9, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_tag_bibitem">[106]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ziyi Guan, Hantao Huang, Yupeng Su, Hong Huang, Ngai Wong, and Hao Yu.

</span>
<span class="ltx_bibblock">Aptq: Attention-aware post-training mixed-precision quantization for large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib106.1.1">arXiv preprint arXiv:2402.14866</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_tag_bibitem">[107]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jinhao Li, Shiyao Li, Jiaming Xu, Shan Huang, Yaoxiu Lian, Jun Liu, Yu&nbsp;Wang, and Guohao Dai.

</span>
<span class="ltx_bibblock">Enabling fast 2-bit llm on gpus: Memory alignment, sparse outlier, and asynchronous dequantization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib107.1.1">arXiv preprint arXiv:2311.16442</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_tag_bibitem">[108]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon Kim, Beomseok Kwon, Se&nbsp;Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee.

</span>
<span class="ltx_bibblock">Lut-gemm: Quantized matrix multiplication based on luts for efficient inference in large-scale generative language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib108.1.1">arXiv preprint arXiv:2206.09557</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_tag_bibitem">[109]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Han Guo, William Brandon, Radostin Cholakov, Jonathan Ragan-Kelley, Eric&nbsp;P Xing, and Yoon Kim.

</span>
<span class="ltx_bibblock">Fast matrix multiplications for lookup table-quantized llms.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib109.1.1">arXiv preprint arXiv:2407.10960</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_tag_bibitem">[110]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Haojun Xia, Zhen Zheng, Xiaoxia Wu, Shiyang Chen, Zhewei Yao, Stephen Youn, Arash Bakhtiari, Michael Wyatt, Donglin Zhuang, Zhongzhu Zhou, et&nbsp;al.

</span>
<span class="ltx_bibblock">Fp6-llm: Efficiently serving large language models through fp6-centric algorithm-system co-design.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib110.1.1">arXiv preprint arXiv:2401.14112</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_tag_bibitem">[111]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib111.1.1">Advances in Neural Information Processing Systems</span>, 35:30318–30332, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_tag_bibitem">[112]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Guangxuan Xiao, Ji&nbsp;Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.

</span>
<span class="ltx_bibblock">Smoothquant: Accurate and efficient post-training quantization for large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib112.1.1">International Conference on Machine Learning</span>, pages 38087–38099. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_tag_bibitem">[113]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler, and Dan Alistarh.

</span>
<span class="ltx_bibblock">Towards end-to-end 4-bit inference on generative large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib113.1.1">arXiv preprint arXiv:2310.09259</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_tag_bibitem">[114]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci.

</span>
<span class="ltx_bibblock">Atom: Low-bit quantization for efficient and accurate llm serving.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib114.1.1">Proceedings of Machine Learning and Systems</span>, 6:196–209, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_tag_bibitem">[115]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong, and Kwang-Ting Cheng.

</span>
<span class="ltx_bibblock">Llm-fp4: 4-bit floating-point quantized transformers.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib115.1.1">arXiv preprint arXiv:2310.16836</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_tag_bibitem">[116]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Suyeon Hur, Seongmin Na, Dongup Kwon, Joonsung Kim, Andrew Boutros, Eriko Nurvitadhi, and Jangwoo Kim.

</span>
<span class="ltx_bibblock">A fast and flexible fpga-based accelerator for natural language processing neural networks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib116.1.1">ACM Transactions on Architecture and Code Optimization</span>, 20(1):1–24, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_tag_bibitem">[117]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Andy He, Darren Key, Mason Bulling, Andrew Chang, Skyler Shapiro, and Everett Lee.

</span>
<span class="ltx_bibblock">Hlstransform: Energy-efficient llama 2 inference on fpgas via high level synthesis.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib117.1.1">arXiv preprint arXiv:2405.00738</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_tag_bibitem">[118]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jude Haris, Rappy Saha, Wenhao Hu, and José Cano.

</span>
<span class="ltx_bibblock">Designing efficient llm accelerators for edge devices.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib118.1.1">arXiv preprint arXiv:2408.00462</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_tag_bibitem">[119]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hongzheng Chen, Jiahao Zhang, Yixiao Du, Shaojie Xiang, Zichao Yue, Niansong Zhang, Yaohui Cai, and Zhiru Zhang.

</span>
<span class="ltx_bibblock">Understanding the potential of fpga-based spatial acceleration for large language model inference.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib119.1.1">ACM Transactions on Reconfigurable Technology and Systems</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_tag_bibitem">[120]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shulin Zeng, Jun Liu, Guohao Dai, Xinhao Yang, Tianyu Fu, Hongyi Wang, Wenheng Ma, Hanbo Sun, Shiyao Li, Zixiao Huang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Flightllm: Efficient large language model inference with a complete mapping flow on fpgas.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib120.1.1">Proceedings of the 2024 ACM/SIGDA International Symposium on Field Programmable Gate Arrays</span>, pages 223–234, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_tag_bibitem">[121]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mingqiang Huang, Ao&nbsp;Shen, Kai Li, Haoxiang Peng, Boyu Li, and Hao Yu.

</span>
<span class="ltx_bibblock">Edgellm: A highly efficient cpu-fpga heterogeneous edge accelerator for large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib121.1.1">arXiv preprint arXiv:2407.21325</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_tag_bibitem">[122]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jaeyong Jang, Yulhwa Kim, Juheun Lee, and Jae-Joon Kim.

</span>
<span class="ltx_bibblock">Figna: Integer unit-based accelerator design for fp-int gemm preserving numerical accuracy.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib122.1.1">2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA)</span>, pages 760–773. IEEE, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_tag_bibitem">[123]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yubin Qin, Yang Wang, Zhiren Zhao, Xiaolong Yang, Yang Zhou, Shaojun Wei, Yang Hu, and Shouyi Yin.

</span>
<span class="ltx_bibblock">Mecla: Memory-compute-efficient llm accelerator with scaling sub-matrix partition.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib123.1.1">2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)</span>, pages 1032–1047. IEEE, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_tag_bibitem">[124]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Cong Guo, Jiaming Tang, Weiming Hu, Jingwen Leng, Chen Zhang, Fan Yang, Yunxin Liu, Minyi Guo, and Yuhao Zhu.

</span>
<span class="ltx_bibblock">Olive: Accelerating large language models via hardware-friendly outlier-victim pair quantization.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib124.1.1">Proceedings of the 50th Annual International Symposium on Computer Architecture</span>, pages 1–15, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_tag_bibitem">[125]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wenjie Li, Aokun Hu, Ningyi Xu, and Guanghui He.

</span>
<span class="ltx_bibblock">Quantization and hardware architecture co-design for matrix-vector multiplications of large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib125.1.1">IEEE Transactions on Circuits and Systems I: Regular Papers</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_tag_bibitem">[126]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jungi Lee, Wonbeom Lee, and Jaewoong Sim.

</span>
<span class="ltx_bibblock">Tender: Accelerating large language models via tensor decomposition and runtime requantization.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib126.1.1">2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)</span>, pages 1048–1062, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_tag_bibitem">[127]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lidong Guo, Zhenhua Zhu, Tengxuan Liu, Xuefei Ning, Shiyao Li, Guohao Dai, Huazhong Yang, Wangyang Fu, and Yu&nbsp;Wang.

</span>
<span class="ltx_bibblock">Towards floating point-based attention-free llm: Hybrid pim with non-uniform data format and reduced multiplications.

</span>
<span class="ltx_bibblock">2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_tag_bibitem">[128]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Minxuan Zhou, Weihong Xu, Jaeyoung Kang, and Tajana Rosing.

</span>
<span class="ltx_bibblock">Transpim: A memory-based acceleration via software-hardware co-design for transformer.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib128.1.1">2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)</span>, pages 1071–1085. IEEE, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_tag_bibitem">[129]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Janak Sharda, Po-Kai Hsu, and Shimeng Yu.

</span>
<span class="ltx_bibblock">Accelerator design using 3d stacked capacitorless dram for large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib129.1.1">2024 IEEE 6th International Conference on AI Circuits and Systems (AICAS)</span>, pages 487–491. IEEE, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_tag_bibitem">[130]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yixin Song, Haotong Xie, Zhengyan Zhang, Bo&nbsp;Wen, Li&nbsp;Ma, Zeyu Mi, and Haibo Chen.

</span>
<span class="ltx_bibblock">Turbo sparse: Achieving llm sota performance with minimal activated parameters.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib130.1.1">arXiv preprint arXiv:2406.05955</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_tag_bibitem">[131]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chenyang Song, Xu&nbsp;Han, Zhengyan Zhang, Shengding Hu, Xiyu Shi, Kuai Li, Chen Chen, Zhiyuan Liu, Guangli Li, Tao Yang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Prosparse: Introducing and enhancing intrinsic activation sparsity within large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib131.1.1">arXiv preprint arXiv:2402.13516</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_tag_bibitem">[132]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xinyin Ma, Gongfan Fang, and Xinchao Wang.

</span>
<span class="ltx_bibblock">Llm-pruner: On the structural pruning of large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib132.1.1">Advances in neural information processing systems</span>, 36:21702–21720, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_tag_bibitem">[133]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Elias Frantar and Dan Alistarh.

</span>
<span class="ltx_bibblock">Sparsegpt: Massive language models can be accurately pruned in one-shot.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib133.1.1">International Conference on Machine Learning</span>, pages 10323–10337. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_tag_bibitem">[134]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mingjie Sun, Zhuang Liu, Anna Bair, and J.&nbsp;Zico Kolter.

</span>
<span class="ltx_bibblock">A simple and effective pruning approach for large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib134.1.1">arXiv preprint arXiv:2306.11695</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_tag_bibitem">[135]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yun Li, Lin Niu, Xipeng Zhang, Kai Liu, Jianchen Zhu, and Zhanhui Kang.

</span>
<span class="ltx_bibblock">E-sparse: Boosting the large language model inference through entropy-based n: M sparsity.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib135.1.1">arXiv preprint arXiv:2310.15929</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_tag_bibitem">[136]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Haojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang, Zhongzhu Zhou, Xiafei Qiu, Yong Li, Wei Lin, and Shuaiwen&nbsp;Leon Song.

</span>
<span class="ltx_bibblock">Flash-llm: Enabling cost-effective and highly-efficient large generative model inference with unstructured sparsity.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib136.1.1">Proceedings of the VLDB Endowment</span>, 17(2):211–224, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_tag_bibitem">[137]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Abhinav Agarwalla, Abhay Gupta, Alexandre Marques, Shubhra Pandit, Michael Goin, Eldar Kurtic, Kevin Leong, Tuan Nguyen, Mahmoud Salem, Dan Alistarh, et&nbsp;al.

</span>
<span class="ltx_bibblock">Enabling high-sparsity foundational llama models with efficient pretraining and deployment.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib137.1.1">arXiv preprint arXiv:2405.03594</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_tag_bibitem">[138]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce&nbsp;Zhang, Yuandong Tian, Christopher Re, et&nbsp;al.

</span>
<span class="ltx_bibblock">Deja vu: Contextual sparsity for efficient llms at inference time.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib138.1.1">International Conference on Machine Learning</span>, pages 22137–22176. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_tag_bibitem">[139]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Generating long sequences with sparse transformers.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib139.1.1">arXiv preprint arXiv:1904.10509</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_tag_bibitem">[140]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Manzil Zaheer, Guru Guruganesh, Kumar&nbsp;Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li&nbsp;Yang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Big bird: Transformers for longer sequences.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib140.1.1">Advances in neural information processing systems</span>, 33:17283–17297, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_tag_bibitem">[141]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis.

</span>
<span class="ltx_bibblock">Efficient streaming language models with attention sinks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib141.1.1">ICLR</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_tag_bibitem">[142]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Iz&nbsp;Beltagy, Matthew&nbsp;E Peters, and Arman Cohan.

</span>
<span class="ltx_bibblock">Longformer: The long-document transformer.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib142.1.1">arXiv preprint arXiv:2004.05150</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_tag_bibitem">[143]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gonçalo&nbsp;M Correia, Vlad Niculae, and André&nbsp;FT Martins.

</span>
<span class="ltx_bibblock">Adaptively sparse transformers.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib143.1.1">arXiv preprint arXiv:1909.00015</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_tag_bibitem">[144]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.

</span>
<span class="ltx_bibblock">Reformer: The efficient transformer.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib144.1.1">arXiv preprint arXiv:2001.04451</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_tag_bibitem">[145]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, and François Fleuret.

</span>
<span class="ltx_bibblock">Faster causal attention over large sequences through sparse flash attention.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib145.1.1">arXiv preprint arXiv:2306.01160</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_tag_bibitem">[146]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yi&nbsp;Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan.

</span>
<span class="ltx_bibblock">Sparse sinkhorn attention.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib146.1.1">International Conference on Machine Learning</span>, pages 9438–9447. PMLR, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_tag_bibitem">[147]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et&nbsp;al.

</span>
<span class="ltx_bibblock">H2o: Heavy-hitter oracle for efficient generative inference of large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib147.1.1">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_tag_bibitem">[148]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hanrui Wang, Zhekai Zhang, and Song Han.

</span>
<span class="ltx_bibblock">Spatten: Efficient sparse attention architecture with cascade token and head pruning.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib148.1.1">2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)</span>, pages 97–110. IEEE, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_tag_bibitem">[149]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Eunji Yoo, Gunho Park, Jung&nbsp;Gyu Min, Se&nbsp;Jung Kwon, Baeseong Park, Dongsoo Lee, and Youngjoo Lee.

</span>
<span class="ltx_bibblock">Tf-mvp: Novel sparsity-aware transformer accelerator with mixed-length vector pruning.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib149.1.1">2023 60th ACM/IEEE Design Automation Conference (DAC)</span>, pages 1–6. IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_tag_bibitem">[150]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Huizheng Wang, Jiahao Fang, Xinru Tang, Zhiheng Yue, Jinxi Li, Yubin Qin, Sihan Guan, Qize Yang, Yang Wang, Chao Li, et&nbsp;al.

</span>
<span class="ltx_bibblock">Sofa: A compute-memory optimized sparsity accelerator via cross-stage coordinated tiling.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib150.1.1">arXiv preprint arXiv:2407.10416</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_tag_bibitem">[151]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zixu Li, Wang Wang, Xin Zhong, Manni Li, Jiayu Yang, Yinyin Lin, Guhyun Kim, Yosub Song, Chengchen Wang, and Xiankui Xiong.

</span>
<span class="ltx_bibblock">Lauws: Local adaptive unstructured weight sparsity of load balance for dnn in near-data processing.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib151.1.1">2024 IEEE International Symposium on Circuits and Systems (ISCAS)</span>, pages 1–5. IEEE, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_tag_bibitem">[152]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shiwei Liu, Chen Mu, Hao Jiang, Yunzhengmao Wang, Jinshan Zhang, Feng Lin, Keji Zhou, Qi&nbsp;Liu, and Chixiao Chen.

</span>
<span class="ltx_bibblock">Hardsea: Hybrid analog-reram clustering and digital-sram in-memory computing accelerator for dynamic sparse self-attention in transformer.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib152.1.1">IEEE Transactions on Very Large Scale Integration (VLSI) Systems</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_tag_bibitem">[153]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan Majumder, and Furu Wei.

</span>
<span class="ltx_bibblock">Inference with reference: Lossless acceleration of large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib153.1.1">arXiv preprint arXiv:2304.04487</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_tag_bibitem">[154]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit.

</span>
<span class="ltx_bibblock">Blockwise parallel decoding for deep autoregressive models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib154.1.1">Advances in Neural Information Processing Systems</span>, 31, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_tag_bibitem">[155]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang.

</span>
<span class="ltx_bibblock">Break the sequential dependency of llm inference using lookahead decoding.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib155.1.1">arXiv preprint arXiv:2402.02057</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_tag_bibitem">[156]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason&nbsp;D Lee, Deming Chen, and Tri Dao.

</span>
<span class="ltx_bibblock">Medusa: Simple llm inference acceleration framework with multiple decoding heads.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib156.1.1">arXiv preprint arXiv:2401.10774</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_tag_bibitem">[157]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang.

</span>
<span class="ltx_bibblock">Eagle: Speculative sampling requires rethinking feature uncertainty.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib157.1.1">arXiv preprint arXiv:2401.15077</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_tag_bibitem">[158]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang.

</span>
<span class="ltx_bibblock">Eagle-2: Faster inference of language models with dynamic draft trees.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib158.1.1">arXiv preprint arXiv:2406.16858</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_tag_bibitem">[159]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Weilin Zhao, Yuxiang Huang, Xu&nbsp;Han, Chaojun Xiao, Zhiyuan Liu, and Maosong Sun.

</span>
<span class="ltx_bibblock">Ouroboros: Speculative decoding with large model enhanced drafting.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib159.1.1">arXiv preprint arXiv:2402.13720</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_tag_bibitem">[160]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhuoming Chen, Avner May, Ruslan Svirschevski, Yuhsun Huang, Max Ryabinin, Zhihao Jia, and Beidi Chen.

</span>
<span class="ltx_bibblock">Sequoia: Scalable, robust, and hardware-aware speculative decoding.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib160.1.1">arXiv preprint arXiv:2402.12374</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_tag_bibitem">[161]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke&nbsp;Chen, Gang Chen, and Sharad Mehrotra.

</span>
<span class="ltx_bibblock">Draft &amp; verify: Lossless large language model acceleration via self-speculative decoding.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib161.1.1">arXiv preprint arXiv:2309.08168</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_tag_bibitem">[162]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Fangcheng Liu, Yehui Tang, Zhenhua Liu, Yunsheng Ni, Kai Han, and Yunhe Wang.

</span>
<span class="ltx_bibblock">Kangaroo: Lossless self-speculative decoding via double early exiting.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib162.1.1">arXiv preprint arXiv:2404.18911</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_tag_bibitem">[163]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, et&nbsp;al.

</span>
<span class="ltx_bibblock">Layer skip: Enabling early exit inference and self-speculative decoding.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib163.1.1">arXiv preprint arXiv:2404.16710</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_tag_bibitem">[164]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Siqi Fan, Xin Jiang, Xiang Li, Xuying Meng, Peng Han, Shuo Shang, Aixin Sun, Yequan Wang, and Zhongyuan Wang.

</span>
<span class="ltx_bibblock">Not all layers of llms are necessary during inference.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib164.1.1">arXiv preprint arXiv:2403.02181</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_tag_bibitem">[165]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lianming Huang, Shangyu Wu, Yufei Cui, Ying Xiong, Xue Liu, Tei-Wei Kuo, Nan Guan, and Chun&nbsp;Jason Xue.

</span>
<span class="ltx_bibblock">Raee: A training-free retrieval-augmented early exiting framework for efficient inference.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib165.1.1">arXiv preprint arXiv:2405.15198</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_tag_bibitem">[166]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter&nbsp;Conway Humphreys, and Adam Santoro.

</span>
<span class="ltx_bibblock">Mixture-of-depths: Dynamically allocating compute in transformer-based language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib166.1.1">arXiv preprint arXiv:2404.02258</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_tag_bibitem">[167]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sangyeob Kim, Sangjin Kim, Wooyoung Jo, Soyeon Kim, Seongyon Hong, and Hoi-Jun Yoo.

</span>
<span class="ltx_bibblock">20.5 c-transformer: A 2.6-18.1 <math alttext="\mu" class="ltx_Math" display="inline" id="bib.bib167.1.m1.1"><semantics id="bib.bib167.1.m1.1a"><mi id="bib.bib167.1.m1.1.1" xref="bib.bib167.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="bib.bib167.1.m1.1b"><ci id="bib.bib167.1.m1.1.1.cmml" xref="bib.bib167.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib167.1.m1.1c">\mu</annotation><annotation encoding="application/x-llamapun" id="bib.bib167.1.m1.1d">italic_μ</annotation></semantics></math>j/token homogeneous dnn-transformer/spiking-transformer processor with big-little network and implicit weight generation for large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib167.2.1">2024 IEEE International Solid-State Circuits Conference (ISSCC)</span>, volume&nbsp;67, pages 368–370. IEEE, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_tag_bibitem">[168]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Cong Li, Zhe Zhou, Size Zheng, Jiaxi Zhang, Yun Liang, and Guangyu Sun.

</span>
<span class="ltx_bibblock">Specpim: Accelerating speculative inference on pim-enabled system via architecture-dataflow co-exploration.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib168.1.1">Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3</span>, pages 950–965, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_tag_bibitem">[169]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tri Dao et&nbsp;al.

</span>
<span class="ltx_bibblock">Flashattention: Fast and memory-efficient exact attention with io-awareness.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib169.1.1">Advances in Neural Information Processing Systems</span>, 35:16344–16359, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_tag_bibitem">[170]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tri Dao.

</span>
<span class="ltx_bibblock">Flashattention-2: Faster attention with better parallelism and work partitioning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib170.1.1">arXiv preprint arXiv:2307.08691</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_tag_bibitem">[171]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tri Dao et&nbsp;al.

</span>
<span class="ltx_bibblock">Flash-decoding for long-context inference.

</span>
<span class="ltx_bibblock">[Online], 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://crfm.stanford.edu/2023/10/12/flashdecoding.html" title="">https://crfm.stanford.edu/2023/10/12/flashdecoding.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_tag_bibitem">[172]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ke&nbsp;Hong et&nbsp;al.

</span>
<span class="ltx_bibblock">Flashdecoding++: Faster large language model inference on gpus, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_tag_bibitem">[173]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Reza&nbsp;Yazdani Aminabadi et&nbsp;al.

</span>
<span class="ltx_bibblock">Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib173.1.1">SC22: International Conference for High Performance Computing, Networking, Storage and Analysis</span>, pages 1–15. IEEE, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_tag_bibitem">[174]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Woosuk Kwon et&nbsp;al.

</span>
<span class="ltx_bibblock">Efficient memory management for large language model serving with pagedattention.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib174.1.1">Proceedings of the 29th Symposium on Operating Systems Principles</span>, pages 611–626, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_tag_bibitem">[175]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sensetime.

</span>
<span class="ltx_bibblock">Openppl: A high-performance deep learning inference platform.

</span>
<span class="ltx_bibblock">[Online], 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openppl.ai/home" title="">https://openppl.ai/home</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_tag_bibitem">[176]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
NVIDIA.

</span>
<span class="ltx_bibblock">cublas: Basic linear algebra on nvidia gpus.

</span>
<span class="ltx_bibblock">[Online], 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.nvidia.com/cublas" title="">https://developer.nvidia.com/cublas</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_tag_bibitem">[177]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Neal Vaidya et&nbsp;al.

</span>
<span class="ltx_bibblock">Optimizing inference on large language models with nvidia tensorrt-llm, now publicly available.

</span>
<span class="ltx_bibblock">[Online], 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/NVIDIA/TensorRT-LLM" title="">https://github.com/NVIDIA/TensorRT-LLM</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_tag_bibitem">[178]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
NVIDIA.

</span>
<span class="ltx_bibblock">Cutlass: Cuda templates for linear algebra subroutines.

</span>
<span class="ltx_bibblock">[Online], 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/NVIDIA/cutlass" title="">https://github.com/NVIDIA/cutlass</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_tag_bibitem">[179]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yujia Zhai et&nbsp;al.

</span>
<span class="ltx_bibblock">Bytetransformer: A high-performance transformer boosted for variable-length inputs.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib179.1.1">2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</span>, pages 344–355. IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_tag_bibitem">[180]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Seungjae Moon, Jung-Hoon Kim, Junsoo Kim, Seongmin Hong, Junseo Cha, Minsu Kim, Sukbin Lim, Gyubin Choi, Dongjin Seo, Jongho Kim, et&nbsp;al.

</span>
<span class="ltx_bibblock">Lpu: A latency-optimized and highly scalable processor for large language model inference.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib180.1.1">IEEE Micro</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_tag_bibitem">[181]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Groq Inc.

</span>
<span class="ltx_bibblock">What is a language processing unit?, 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://wow.groq.com/wp-content/uploads/2024/07/GroqThoughts_WhatIsALPU-vF.pdf" title="">https://wow.groq.com/wp-content/uploads/2024/07/GroqThoughts_WhatIsALPU-vF.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_tag_bibitem">[182]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shiwei Liu, Guanchen Tao, Yifei Zou, Derek Chow, Zichen Fan, Kauna Lei, Bangfei Pan, Dennis Sylvester, Gregory Kielian, and Mehdi Saligane.

</span>
<span class="ltx_bibblock">Consmax: Hardware-friendly alternative softmax with learnable parameters.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib182.1.1">arXiv preprint arXiv:2402.10930</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_tag_bibitem">[183]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jinhao Li, Shan Huang, Jiaming Xu, Jun Liu, Li&nbsp;Ding, Ningyi Xu, and Guohao Dai.

</span>
<span class="ltx_bibblock">Marca: Mamba accelerator with reconfigurable architecture, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_tag_bibitem">[184]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hanjoon Kim, Younggeun Choi, Junyoung Park, Byeongwook Bae, Hyunmin Jeong, Sang&nbsp;Min Lee, Jeseung Yeon, Minho Kim, Changjae Park, Boncheol Gu, et&nbsp;al.

</span>
<span class="ltx_bibblock">Tcp: A tensor contraction processor for ai workloads industrial product.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib184.1.1">2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)</span>, pages 890–902. IEEE, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_tag_bibitem">[185]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Intel Inc.

</span>
<span class="ltx_bibblock">Intel®gaudi® ai accelerator first generation deep learning training and inference processor, 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://habana.ai/products/gaudi/" title="">https://habana.ai/products/gaudi/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_tag_bibitem">[186]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Intel Inc.

</span>
<span class="ltx_bibblock">Intel®gaudi® 2 ai accelerator high performance acceleration for genai and llms, 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://habana.ai/products/gaudi2/" title="">https://habana.ai/products/gaudi2/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_tag_bibitem">[187]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Roman Kaplan.

</span>
<span class="ltx_bibblock">Intel gaudi 3 ai accelerator: Architected for gen ai training and inference.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib187.1.1">2024 IEEE Hot Chips 36 Symposium (HCS)</span>, pages 1–16. IEEE, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib188">
<span class="ltx_tag ltx_tag_bibitem">[188]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sean Lie.

</span>
<span class="ltx_bibblock">Wafer-scale ai: Gpu impossible performance.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib188.1.1">2024 IEEE Hot Chips 36 Symposium (HCS)</span>, pages 1–71. IEEE Computer Society, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib189">
<span class="ltx_tag ltx_tag_bibitem">[189]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mohamed&nbsp;Assem Ibrahim, Mahzabeen Islam, and Shaizeen Aga.

</span>
<span class="ltx_bibblock">Balanced data placement for gemv acceleration with processing-in-memory.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib189.1.1">arXiv preprint arXiv:2403.20297</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib190">
<span class="ltx_tag ltx_tag_bibitem">[190]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rongqing Cong, Wenyang He, Mingxuan Li, Bangning Luo, Zebin Yang, Yuchao Yang, Ru&nbsp;Huang, and Bonan Yan.

</span>
<span class="ltx_bibblock">Attentionlego: An open-source building block for spatially-scalable large language model accelerator with processing-in-memory technology.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib190.1.1">arXiv preprint arXiv:2401.11459</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib191">
<span class="ltx_tag ltx_tag_bibitem">[191]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuting Wu, Ziyu Wang, and Wei&nbsp;D Lu.

</span>
<span class="ltx_bibblock">Pim gpt a hybrid process in memory accelerator for autoregressive transformers.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib191.1.1">npj Unconventional Computing</span>, 1(1):4, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib192">
<span class="ltx_tag ltx_tag_bibitem">[192]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wontak Han, Hyunjun Cho, Donghyuk Kim, and Joo-Young Kim.

</span>
<span class="ltx_bibblock">Sal-pim: A subarray-level processing-in-memory architecture with lut-based linear interpolation for transformer-based text generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib192.1.1">arXiv preprint arXiv:2401.17005</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib193">
<span class="ltx_tag ltx_tag_bibitem">[193]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Taeyang Jeong and Eui-Young Chung.

</span>
<span class="ltx_bibblock">Pipepim: Maximizing computing unit utilization in ml-oriented digital pim by pipelining and dual buffering.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib193.1.1">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib194">
<span class="ltx_tag ltx_tag_bibitem">[194]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hyungyo Kim, Gaohan Ye, Nachuan Wang, Amir Yazdanbakhsh, and Nam&nbsp;Sung Kim.

</span>
<span class="ltx_bibblock">Exploiting intel® advanced matrix extensions (amx) for large language model inference.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib194.1.1">IEEE Computer Architecture Letters</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib195">
<span class="ltx_tag ltx_tag_bibitem">[195]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yixin Song, Zeyu Mi, Haotong Xie, and Haibo Chen.

</span>
<span class="ltx_bibblock">Powerinfer: Fast large language model serving with a consumer-grade gpu.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib195.1.1">arXiv preprint arXiv:2312.12456</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib196">
<span class="ltx_tag ltx_tag_bibitem">[196]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhenliang Xue, Yixin Song, Zeyu Mi, Le&nbsp;Chen, Yubin Xia, and Haibo Chen.

</span>
<span class="ltx_bibblock">Powerinfer-2: Fast large language model inference on a smartphone.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib196.1.1">arXiv preprint arXiv:2406.06282</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib197">
<span class="ltx_tag ltx_tag_bibitem">[197]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Guseul Heo, Sangyeop Lee, Jaehong Cho, Hyunmin Choi, Sanghyeon Lee, Hyungkyu Ham, Gwangsun Kim, Divya Mahajan, and Jongse Park.

</span>
<span class="ltx_bibblock">Neupims: Npu-pim heterogeneous acceleration for batched llm inferencing.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib197.1.1">Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3</span>, pages 722–737, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib198">
<span class="ltx_tag ltx_tag_bibitem">[198]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Minseok Seo, Xuan&nbsp;Truong Nguyen, Seok&nbsp;Joong Hwang, Yongkee Kwon, Guhyun Kim, Chanwook Park, Ilkon Kim, Jaehan Park, Jeongbin Kim, Woojae Shin, et&nbsp;al.

</span>
<span class="ltx_bibblock">Ianus: Integrated accelerator based on npu-pim unified memory system.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib198.1.1">Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3</span>, pages 545–560, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib199">
<span class="ltx_tag ltx_tag_bibitem">[199]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Taehyun Kim, Kwanseok Choi, Youngmock Cho, Jaehoon Cho, Hyuk-Jae Lee, and Jaewoong Sim.

</span>
<span class="ltx_bibblock">Monde: Mixture of near-data experts for large-scale sparse models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib199.1.1">arXiv preprint arXiv:2405.18832</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib200">
<span class="ltx_tag ltx_tag_bibitem">[200]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jaewan Choi, Jaehyun Park, Kwanhee Kyung, Nam&nbsp;Sung Kim, and Jung&nbsp;Ho Ahn.

</span>
<span class="ltx_bibblock">Unleashing the potential of pim: Accelerating large batched inference of transformer-based generative models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib200.1.1">IEEE Computer Architecture Letters</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib201">
<span class="ltx_tag ltx_tag_bibitem">[201]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jaehyun Park, Jaewan Choi, Kwanhee Kyung, Michael&nbsp;Jaemin Kim, Yongsuk Kwon, Nam&nbsp;Sung Kim, and Jung&nbsp;Ho Ahn.

</span>
<span class="ltx_bibblock">Attacc! unleashing the power of pim for batched transformer-based generative model inference.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib201.1.1">Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2</span>, pages 103–119, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib202">
<span class="ltx_tag ltx_tag_bibitem">[202]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shin-haeng Kang, Sukhan Lee, and Kyomin Sohn.

</span>
<span class="ltx_bibblock">The era of generative artificial intelligence: In-memory computing perspective.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib202.1.1">2023 International Electron Devices Meeting (IEDM)</span>, pages 1–4. IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib203">
<span class="ltx_tag ltx_tag_bibitem">[203]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Byeongho Kim, Sanghoon Cha, Sangsoo Park, Jieun Lee, Sukhan Lee, Shin-haeng Kang, Jinin So, Kyungsoo Kim, Jin Jung, Jong-Geon Lee, et&nbsp;al.

</span>
<span class="ltx_bibblock">The breakthrough memory solutions for improved performance on llm inference.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib203.1.1">IEEE Micro</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib204">
<span class="ltx_tag ltx_tag_bibitem">[204]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yandong Luo and Shimeng Yu.

</span>
<span class="ltx_bibblock">H3d-transformer: A heterogeneous 3d (h3d) computing platform for transformer model acceleration on edge devices.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib204.1.1">ACM Transactions on Design Automation of Electronic Systems</span>, 29(3):1–19, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib205">
<span class="ltx_tag ltx_tag_bibitem">[205]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sang-Soo Park, KyungSoo Kim, Jinin So, Jin Jung, Jonggeon Lee, Kyoungwan Woo, Nayeon Kim, Younghyun Lee, Hyungyo Kim, Yongsuk Kwon, et&nbsp;al.

</span>
<span class="ltx_bibblock">An lpddr-based cxl-pnm platform for tco-efficient inference of transformer-based large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib205.1.1">2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA)</span>, pages 970–982. IEEE, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib206">
<span class="ltx_tag ltx_tag_bibitem">[206]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Harsh Sharma, Pratyush Dhingra, Janardhan&nbsp;Rao Doppa, Umit Ogras, and Partha&nbsp;Pratim Pande.

</span>
<span class="ltx_bibblock">A heterogeneous chiplet architecture for accelerating end-to-end transformer models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib206.1.1">arXiv preprint arXiv:2312.11750</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib207">
<span class="ltx_tag ltx_tag_bibitem">[207]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
SK&nbsp;Hynix.

</span>
<span class="ltx_bibblock">Cost-effective llm inference solution using sk hynix’s aim (accelerator-in-memory), 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://sc23.supercomputing.org/proceedings/exhibitor_forum/exhibitor_forum_pages/exforum133.html" title="">https://sc23.supercomputing.org/proceedings/exhibitor_forum/exhibitor_forum_pages/exforum133.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib208">
<span class="ltx_tag ltx_tag_bibitem">[208]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Guhyun Kim, Jinkwon Kim, Nahsung Kim, Woojae Shin, Jongsoon Won, Hyunha Joo, Haerang Choi, Byeongju An, Gyeongcheol Shin, Dayeon Yun, et&nbsp;al.

</span>
<span class="ltx_bibblock">Sk hynix ai-specific computing memory solution: From aim device to heterogeneous aimx-xpu system for comprehensive llm inference.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib208.1.1">2024 IEEE Hot Chips 36 Symposium (HCS)</span>, pages 1–26. IEEE Computer Society, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib209">
<span class="ltx_tag ltx_tag_bibitem">[209]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhongkai Yu, Shengwen Liang, Tianyun Ma, Yunke Cai, Ziyuan Nan, Di&nbsp;Huang, Xinkai Song, Yifan Hao, Jie Zhang, Tian Zhi, et&nbsp;al.

</span>
<span class="ltx_bibblock">Cambricon-llm: A chiplet-based hybrid architecture for on-device inference of 70b llm.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib209.1.1">arXiv preprint arXiv:2409.15654</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib210">
<span class="ltx_tag ltx_tag_bibitem">[210]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Pujiang He, Shan Zhou, Changqing Li, Wenhuan Huang, Weifei Yu, Duyi Wang, Chen Meng, and Sheng Gui.

</span>
<span class="ltx_bibblock">Distributed inference performance optimization for llms on cpus.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib210.1.1">arXiv preprint arXiv:2407.00029</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib211">
<span class="ltx_tag ltx_tag_bibitem">[211]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Pujiang He, Shan Zhou, Wenhuan Huang, Changqing Li, Duyi Wang, Bin Guo, Chen Meng, Sheng Gui, Weifei Yu, and Yi&nbsp;Xie.

</span>
<span class="ltx_bibblock">Inference performance optimization for large language models on cpus.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib211.1.1">arXiv preprint arXiv:2407.07304</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib212">
<span class="ltx_tag ltx_tag_bibitem">[212]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Seongmin Hong, Seungjae Moon, Junsoo Kim, Sungjae Lee, Minsub Kim, Dongsoo Lee, and Joo-Young Kim.

</span>
<span class="ltx_bibblock">Dfx: A low-latency multi-fpga appliance for accelerating transformer-based text generation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib212.1.1">2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO)</span>, pages 616–630. IEEE, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib213">
<span class="ltx_tag ltx_tag_bibitem">[213]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wenhua Cheng, Yiyang Cai, Kaokao Lv, and Haihao Shen.

</span>
<span class="ltx_bibblock">Teq: Trainable equivalent transformation for quantization of llms.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib213.1.1">arXiv preprint arXiv:2310.10944</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib214">
<span class="ltx_tag ltx_tag_bibitem">[214]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Raspberry Pi.

</span>
<span class="ltx_bibblock">Raspberry pi 5, 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.raspberrypi.com/products/raspberry-pi-5/" title="">https://www.raspberrypi.com/products/raspberry-pi-5/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib215">
<span class="ltx_tag ltx_tag_bibitem">[215]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Noam Shazeer.

</span>
<span class="ltx_bibblock">Glu variants improve transformer.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib215.1.1">arXiv preprint arXiv:2002.05202</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib216">
<span class="ltx_tag ltx_tag_bibitem">[216]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dan Hendrycks and Kevin Gimpel.

</span>
<span class="ltx_bibblock">Gaussian error linear units (gelus).

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib216.1.1">arXiv preprint arXiv:1606.08415</span>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib217">
<span class="ltx_tag ltx_tag_bibitem">[217]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré.

</span>
<span class="ltx_bibblock">Flashattention: Fast and memory-efficient exact attention with io-awareness.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib217.1.1">Advances in Neural Information Processing Systems</span>, 35:16344–16359, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib218">
<span class="ltx_tag ltx_tag_bibitem">[218]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ari Holtzman, Jan Buys, Li&nbsp;Du, Maxwell Forbes, and Yejin Choi.

</span>
<span class="ltx_bibblock">The curious case of neural text degeneration.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib218.1.1">arXiv preprint arXiv:1904.09751</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib219">
<span class="ltx_tag ltx_tag_bibitem">[219]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dennis Abts, Jonathan Ross, Jonathan Sparling, Mark Wong-VanHaren, Max Baker, Tom Hawkins, Andrew Bell, John Thompson, Temesghen Kahsai, Garrin Kimmell, et&nbsp;al.

</span>
<span class="ltx_bibblock">Think fast: A tensor streaming processor (tsp) for accelerating deep learning workloads.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib219.1.1">2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)</span>, pages 145–158. IEEE, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib220">
<span class="ltx_tag ltx_tag_bibitem">[220]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dennis Abts, Garrin Kimmell, Andrew Ling, John Kim, Matt Boyd, Andrew Bitar, Sahil Parmar, Ibrahim Ahmed, Roberto DiCecco, David Han, et&nbsp;al.

</span>
<span class="ltx_bibblock">A software-defined tensor streaming multiprocessor for large-scale machine learning.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib220.1.1">Proceedings of the 49th Annual International Symposium on Computer Architecture</span>, pages 567–580, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib221">
<span class="ltx_tag ltx_tag_bibitem">[221]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Groq Inc.

</span>
<span class="ltx_bibblock">Groundbreaking gemma 7b performance running on the groq lpu™ inference engine, 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://wow.groq.com/groundbreaking-gemma-7b-performance-running-on-the-groq-lpu-inference-engine/" title="">https://wow.groq.com/groundbreaking-gemma-7b-performance-running-on-the-groq-lpu-inference-engine/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib222">
<span class="ltx_tag ltx_tag_bibitem">[222]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yoongu Kim, Vivek Seshadri, Donghyuk Lee, Jamie Liu, and Onur Mutlu.

</span>
<span class="ltx_bibblock">A case for exploiting subarray-level parallelism (salp) in dram.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib222.1.1">ACM SIGARCH Computer Architecture News</span>, 40(3):368–379, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib223">
<span class="ltx_tag ltx_tag_bibitem">[223]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sukhan Lee, Shin-haeng Kang, Jaehoon Lee, Hyeonsu Kim, Eojin Lee, Seungwoo Seo, Hosang Yoon, Seungwon Lee, Kyounghwan Lim, Hyunsung Shin, et&nbsp;al.

</span>
<span class="ltx_bibblock">Hardware architecture and software stack for pim based on commercial dram technology: Industrial product.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib223.1.1">2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)</span>, pages 43–56. IEEE, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib224">
<span class="ltx_tag ltx_tag_bibitem">[224]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jinhao Li, Chong Qu, Fan Wu, and Jianfei Jiang.

</span>
<span class="ltx_bibblock">A 4gbps dppm on-chip serial link based on pipelined vernier-tdc.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib224.1.1">2020 IEEE 15th International Conference on Solid-State &amp; Integrated Circuit Technology (ICSICT)</span>, pages 1–3. IEEE, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib225">
<span class="ltx_tag ltx_tag_bibitem">[225]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ang Li, Jianfei Jiang, Qin Wang, Naifeng Jing, Zizheng Dong, Shuya Ji, Xiulan Cheng, and Yuhang Zhao.

</span>
<span class="ltx_bibblock">A method to improve 3d interconnections resource utilization and reliability in hybrid bonding process considering the effects on signal integrity.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib225.1.1">2023 IEEE 73rd Electronic Components and Technology Conference (ECTC)</span>, pages 2131–2136. IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib226">
<span class="ltx_tag ltx_tag_bibitem">[226]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jinhao Li, Jianfei Jiang, Qin Wang, Naifeng Jing, Weiguang Sheng, and Guanghui He.

</span>
<span class="ltx_bibblock">A gain reconfigurable time difference amplifier with self-adaptive linearity control.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib226.1.1">Analog Integrated Circuits and Signal Processing</span>, 107:435–449, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib227">
<span class="ltx_tag ltx_tag_bibitem">[227]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Debendra&nbsp;Das Sharma.

</span>
<span class="ltx_bibblock">Compute express link®: An open industry-standard interconnect enabling heterogeneous data-centric computing.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib227.1.1">2022 IEEE Symposium on High-Performance Interconnects (HOTI)</span>, pages 5–12. IEEE, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib228">
<span class="ltx_tag ltx_tag_bibitem">[228]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Seongju Lee, Kyuyoung Kim, Sanghoon Oh, Joonhong Park, Gimoon Hong, Dongyoon Ka, Kyudong Hwang, Jeongje Park, Kyeongpil Kang, Jungyeon Kim, et&nbsp;al.

</span>
<span class="ltx_bibblock">A 1ynm 1.25 v 8gb, 16gb/s/pin gddr6-based accelerator-in-memory supporting 1tflops mac operation and various activation functions for deep-learning applications.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib228.1.1">2022 IEEE International Solid-State Circuits Conference (ISSCC)</span>, volume&nbsp;65, pages 1–3. IEEE, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib229">
<span class="ltx_tag ltx_tag_bibitem">[229]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yongkee Kwon, Kornijcuk Vladimir, Nahsung Kim, Woojae Shin, Jongsoon Won, Minkyu Lee, Hyunha Joo, Haerang Choi, Guhyun Kim, Byeongju An, et&nbsp;al.

</span>
<span class="ltx_bibblock">System architecture and software stack for gddr6-aim.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib229.1.1">2022 IEEE Hot Chips 34 Symposium (HCS)</span>, pages 1–25. IEEE, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib230">
<span class="ltx_tag ltx_tag_bibitem">[230]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Marc Brysbaert.

</span>
<span class="ltx_bibblock">How many words do we read per minute? a review and meta-analysis of reading rate.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib230.1.1">Journal of memory and language</span>, 109:104047, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib231">
<span class="ltx_tag ltx_tag_bibitem">[231]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
David&nbsp;B Stewart, Donald&nbsp;E Schmitz, and Pradeep&nbsp;K Khosla.

</span>
<span class="ltx_bibblock">Implementing real-time robotic systems using chimera ii.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib231.1.1">1990 IEEE International Conference on Systems Engineering</span>, pages 252–257. IEEE, 1990.

</span>
</li>
</ul>
</section>
</article>
</div>

</div>


<button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button><div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer></body></html>