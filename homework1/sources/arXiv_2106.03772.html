<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2106.03772] Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking</title><meta property="og:description" content="Multi-person pose estimation and tracking
serve as crucial steps for video understanding.
Most state-of-the-art approaches
rely on first estimating
poses in each frame and only then
implementing data association
and reâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2106.03772">

<!--Generated on Sat Mar  9 02:16:53 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Learning Dynamics via Graph Neural Networks for 
<br class="ltx_break">Human Pose Estimation and Tracking</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Yiding Yang<sup id="id10.2.id1" class="ltx_sup">1</sup><span id="footnotex1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The work is partially done when the author is an internship at
Wormpex AI Research.</span></span></span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Zhou Ren<sup id="id11.2.id1" class="ltx_sup">2</sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Haoxiang Li<sup id="id12.2.id1" class="ltx_sup">2</sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Chunluan Zhou<sup id="id13.2.id1" class="ltx_sup">2</sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Xinchao Wang<sup id="id14.2.id1" class="ltx_sup"><span id="id14.2.id1.1" class="ltx_text ltx_font_italic">1,3</span></sup><span id="footnotex2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Corresponding author.</span></span></span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Gang Hua<sup id="id15.3.id1" class="ltx_sup">2</sup>
<br class="ltx_break"><sup id="id16.4.id2" class="ltx_sup">1</sup>Stevens Institute of Technology
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<sup id="id17.2.id1" class="ltx_sup">2</sup>Wormpex AI Research
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<sup id="id18.2.id1" class="ltx_sup">3</sup>National University of Singapore
<br class="ltx_break"><span id="id19.3.id2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{yyang99, hli18, xinchao.wang}@stevens.edu,
renzhou200622@gmail.com,
<br class="ltx_break">czhou002@e.ntu.edu.sg,
ganghua@gmail.com
</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id20.id1" class="ltx_p">Multi-person pose estimation and tracking
serve as crucial steps for video understanding.
Most state-of-the-art approaches
rely on first estimating
poses in each frame and only then
implementing data association
and refinement.
Despite the promising results achieved,
such a strategy is inevitably
prone to missed detections
especially in heavily-cluttered scenes,
since this tracking-by-detection
paradigm is, by nature, largely dependent
on visual evidences that are absent
in the case of occlusion.
In this paper, we propose a
novel online approach to
learning the pose dynamics,
which are independent of
pose detections in current fame,
and hence may serve as a robust
estimation even in challenging scenarios
including occlusion.
Specifically, we derive this
prediction of dynamics through
a graph neural networkÂ (GNN)
that explicitly accounts for
both spatial-temporal and visual information.
It takes as input the historical pose tracklets and
directly predicts the corresponding poses
in the following frame for each tracklet.
The predicted poses will then
be aggregated with the
detected poses, if any, at the same frame
so as to produce the final pose,
potentially recovering the occluded
joints missed
by the estimator.
Experiments on
PoseTrack 2017 and PoseTrack 2018 datasets
demonstrate that
the proposed
method achieves results superior to
the state of the art
on both human pose estimation and tracking tasks.
</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2106.03772/assets/x1.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="452" height="350" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
By modeling the pose dynamics
from history poses
through a graph neural network,
our method learns a pose prediction
that is robust to challenging scenes,
such as motion blurÂ (top)
and occlusionÂ (bottom).
In both cases,
the visual-based
HRNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>
fails to locate the joints,
yet our approach delivers
dependable pose estimations.
</figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Multi-person pose estimation and tracking
find their applications in a wide spectrum
of scenarios including behavior analysis
and action recognition,
and have therefore received increasing attention
in recent yearsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
Despite often coupled together,
they focus on slightly different aspects:
the former
aims to locate human joints
in each frame of an input video,
while the latter one aims to
associate joints that belong
to the same human across frames.
It has been long considered as
a challenging task
due to various factors,
including but not limited to
camera motions,
complex backgrounds,
and mutual occlusions.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Thanks to the recent advances
of deep learning techniques,
pose estimation and tracking have
witnessed unprecedented results
in the past years.
Existing methods can be broadly
categorized into
two streams,
bottom-up methodsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>
and top-down methodsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.
Bottom-up methods first generate joint candidates and
then group the joints into a person detection.
The grouped joints are then associated across frames to generate
the final pose tracking results.
Top-down methods,
on the other hand,
first detect human candidates in a single frame and
then estimate the human poses for each candidate.
The estimated human poses are
associated across frames to achieve pose tracking.
Methods from both streams
have produced promising results
on various scenariosÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In spite of the encouraging results,
state-of-the-art pose estimation
and tracking approaches
remain prone to missed detections
especially in highly-cluttered
and fast-motion scenes.
This is not totally unexpected,
since by nature they rely on
first detecting either
joints or human bodies in
a scene using a visual-based
detector, and only then
carrying out data association
to link the detections into tracks.
In challenging scenarios
such as crowded or blurred
scenes, the joint- or human-detector
would inevitably fail
due to the absent image evidences.
Although some succeeding
refinement steps would
mildly remedy the flawed
estimations, they are
are still largely dependent on visual cues
and hence incompetent
to fully tackle missed detections.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We propose in this paper
a novel approach by explicitly
looking into the <em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">dynamics</em>
of human poses within image sequences.
In contrast to state-of-the-art approaches
that rely on first
detecting human or joints in each frame,
which is again prone to failures
in the absence of detection evidences,
our approach first <em id="S1.p4.1.2" class="ltx_emph ltx_font_italic">predicts</em>
poses in a frame
from a track of history
without looking at
any detection cue.
This strategy allows
us to free our dependency on
the detection evidences
and consequently produce
a legitimate state of human pose
at the very first place.
Specifically,
in our approach this prediction
step is accomplished
through a graph neural networkÂ (GNN)
that takes as input
a track of history poses
in previous frames.
Next, the predicted pose
is aggregated with the
detected poses, if any,
in the same frame
to produce the final pose,
in which way both
dynamical and visual
information are
exploited.
At a conceptual level,
our approach
follows a similar spirit of
Bayesian filters, expect that
in our approach
all parameters and features
are learned end to end.
A qualitative example is
shown in FigureÂ <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
where our dynamic-based
approach yields dependable
pose estimation results
in the cases of motion blur
and occlusion.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Apart from the strength
of recovering missed poses
from predictions,
the proposed approach also
enjoys other merits.
First,
prior approaches
match poses between
two <em id="S1.p5.1.1" class="ltx_emph ltx_font_italic">consecutive</em> frames,
which is brittle to
identify switches
due to factors such as
intersection of poses and fast motion.
Our approach, by contrast,
aggregates poses
within the <em id="S1.p5.1.2" class="ltx_emph ltx_font_italic">same frame</em>,
thanks to our prediction-based nature,
allowing us to significantly reduce the
mismatched rate.
Second,
as compared to state-of-the-art methods,
our approach tackles pose tracking
from an additional perspective, <em id="S1.p5.1.3" class="ltx_emph ltx_font_italic">i.e</em><span id="S1.p5.1.4" class="ltx_ERROR undefined">\onedot</span>the motion dynamics,
which complements the visual cues
that are in many cases absent,
resulting in gratifying final poses.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">We evaluate the effectiveness
of the proposed method on
two widely used benchmark datasets,
PoseTrack 2017 and PoseTrack 2018.
Empirical evaluations showcase that our method
outperforms state-of-the-art approaches
by a considerably large margin
on both pose estimation and tracking
tasks.
We also provide extensive
analyses on the impact
of each component in the proposed method,
and demonstrate the superiority
of learning pose dynamics using our method.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">We briefly review the following three related topics,
including
single-frame human pose estimation,
human pose tracking,
and graph neural networks.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Single-Frame Human Pose Estimation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Human pose estimation methods from single images can be
generally categorized into top-down methods and bottom-up methods.
Bottom-up methodsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> do not
rely on human detectors.
These methods first detect all the body joints and
then group them to form human poses.
The major challenges are robustly detecting joints in
complex situationsÂ (e.g. various scales, poses and cluttered background)
and correctly grouping joints from different persons particularly
in crowds with heavy occlusions.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Top-down methods first detect the human
bounding boxes from an image and then estimate the human pose within each bounding box.
Most top-down methods adopt off-the-shelf
human detectorsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>
and focus on designing efficient human pose
estimatorsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.
Pose estimation is confined for a single person
within a small area at a fixed scale. With a reliable human detector, the top-down methods can achieve accurate human pose estimation.</p>
</div>
<figure id="S2.F2" class="ltx_figure">
<div id="S2.F2.1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:367.6pt;height:194.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-20.4pt,10.8pt) scale(0.9,0.9) ;"><img src="/html/2106.03772/assets/x2.png" id="S2.F2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="428" height="230" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overall pipeline of the proposed method.
Given the history of poses and the current frame,
the GNN model predicts poses for each tracklet
in the history memory.
The predicted poses are then matched and merged with
the estimated poses to obtain the final poses in
the current frame.</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Human Pose Tracking</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Extending the pose estimation to video lead to
the human pose tracking problem, where the human poses
are estimated for each frame and
associated across frames. As a result, pose tracking
is often tackled together with human-location trackingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Bottom-up methodsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> in pose tracking associated
the joints spatially and temporally without detecting human bounding boxes.
For example, RaajÂ <span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_italic">et al.</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> extended the Part Affinity FieldÂ (PAF)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> designed for single image
pose estimation to include temporal modeling for pose tracking.
JinÂ <span id="S2.SS2.p2.1.2" class="ltx_text ltx_font_italic">et al.</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> proposed ST-Embed to learn the
Spatial-Temporal Embedding of joints based on
the idea of Associative EmbeddingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.
Both methods only model relationships of joints between two frames.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Top-down methods focus on improving single-frame pose estimation
by exploiting temporal context and associating the estimated
poses into human pose tracklets.
In the simple baseline methodÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>,
the estimated human poses are associated by the similarity computed based on the
optical flow between consecutive frames.
Detect-and-TrackÂ (DAT)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> utilizes
a 3D Mask R-CNN model to detect persons with key-points
from a video clip and then associates them
by comparing the locations of person detections.
CombDetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> extends a 3D network as the backbone for pose estimation
to generate a tube
of poses by directly propagating a bounding box to the neighboring
frames. KeyTrackÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> associates the estimated human poses
pose similarities. TKMRNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> matches
human poses by learning appearance embeddings of joints
and refines joints by exploiting temporal context from tracked poses.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">Although some of the prior methods utilize multiple consecutive frames
to help improve pose estimation and tracking, none of them explicitly
model the spatial-temporal and visual dynamics of human joints. Our method
models the pose tracking process with a Graph Neural Networks
to learn the dynamics across frames from data.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Graph Neural Networks</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Graph Neural NetworksÂ (GNNs) was first developed for graph analysis such as
node classificationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>
and link predictionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>.
It shows great potential in dealing with non-grid dataÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>
and has been applied to process point clouds and
imagesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>.
For example, DGMPNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>
utilize GNN to capture the long range dependence among pixels in images
to enhance the feature representation.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">GNN has been used to model human poses for pose-based action recognitionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>
and single-frame pose estimationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
For example, DGCNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> adopts several learnt graphs to
model the relations of different joints and
propagates among them to obtain the enhanced joint feature
for better human pose estimation.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">There are prior works that use GNNs for generic object trackingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
GaoÂ <span id="S2.SS3.p3.1.1" class="ltx_text ltx_font_italic">et al.</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> proposed to divide an object into several parts and learn a spatial-temporal template of the object for tracking. BaoÂ <span id="S2.SS3.p3.1.2" class="ltx_text ltx_font_italic">et al.</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> utilized GNN in their pose tracking method to exploit human structural relations to help associate human poses across frames.
This method relies on a strong human detector as well as a strong pose estimator to generate human poses for association.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">In this paper, we propose a GNN-based predictor to estimate a potential configuration for each human pose tracklet frame by frame via leveraging the tracked pose history.
The learnable predictor naturally models the pose tracking process
and captures the dynamics of pose tracklets across video frames.
Our proposed framework is capable of predicting the poses of missed human detections, which makes it robust to heavy occlusions and motion blur.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">FigureÂ <a href="#S2.F2" title="Figure 2 â€£ 2.1 Single-Frame Human Pose Estimation â€£ 2 Related Work â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the overall pipeline of the proposed method.
For each incoming frame, two sets of poses are computed
separately by the single-frame pose estimation module and
the GNN-based pose prediction module.
These two sets of poses are matched and merged together to generate
the final human poses for the current frame.
We introduce each components of the proposed method in the following sections.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Single-Frame Pose Estimation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.7" class="ltx_p">We follow the standard pipeline of recent top-down pose trackersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> to perform pose estimation for each frame.
Each human detection in a frame is first cropped
and rescaled to a fixed sizeÂ (e.g. <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mn id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><times id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">384</cn><cn type="integer" id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">384\times 288</annotation></semantics></math> when
HRNet is used as the backbone of human pose estimation).
The human pose estimator takes the scaled image as input
and outputs a set of feature maps
as well as a set of heatmaps <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{H}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">ğ‡</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">ğ‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\mathbf{H}</annotation></semantics></math>.
The size of the generated heatmaps is typically smaller than the
input imageÂ (e.g. <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="96\times 72" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mn id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">96</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.3.m3.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.cmml">Ã—</mo><mn id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">72</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><times id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1"></times><cn type="integer" id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">96</cn><cn type="integer" id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">72</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">96\times 72</annotation></semantics></math> with
HRNet as the backbone).
The number of heatmaps is set to be
the number of joints, which is 15 on PoseTrack 2017
and PoseTrack 2018 datasets.
Let <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="\mathbf{H}_{ijk}" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><msub id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">ğ‡</mi><mrow id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml"><mi id="S3.SS1.p1.4.m4.1.1.3.2" xref="S3.SS1.p1.4.m4.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.4.m4.1.1.3.1" xref="S3.SS1.p1.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.4.m4.1.1.3.3" xref="S3.SS1.p1.4.m4.1.1.3.3.cmml">j</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.4.m4.1.1.3.1a" xref="S3.SS1.p1.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.4.m4.1.1.3.4" xref="S3.SS1.p1.4.m4.1.1.3.4.cmml">k</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">ğ‡</ci><apply id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3"><times id="S3.SS1.p1.4.m4.1.1.3.1.cmml" xref="S3.SS1.p1.4.m4.1.1.3.1"></times><ci id="S3.SS1.p1.4.m4.1.1.3.2.cmml" xref="S3.SS1.p1.4.m4.1.1.3.2">ğ‘–</ci><ci id="S3.SS1.p1.4.m4.1.1.3.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3.3">ğ‘—</ci><ci id="S3.SS1.p1.4.m4.1.1.3.4.cmml" xref="S3.SS1.p1.4.m4.1.1.3.4">ğ‘˜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">\mathbf{H}_{ijk}</annotation></semantics></math> be the value
at the <math id="S3.SS1.p1.5.m5.2" class="ltx_Math" alttext="(i,j)" display="inline"><semantics id="S3.SS1.p1.5.m5.2a"><mrow id="S3.SS1.p1.5.m5.2.3.2" xref="S3.SS1.p1.5.m5.2.3.1.cmml"><mo stretchy="false" id="S3.SS1.p1.5.m5.2.3.2.1" xref="S3.SS1.p1.5.m5.2.3.1.cmml">(</mo><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">i</mi><mo id="S3.SS1.p1.5.m5.2.3.2.2" xref="S3.SS1.p1.5.m5.2.3.1.cmml">,</mo><mi id="S3.SS1.p1.5.m5.2.2" xref="S3.SS1.p1.5.m5.2.2.cmml">j</mi><mo stretchy="false" id="S3.SS1.p1.5.m5.2.3.2.3" xref="S3.SS1.p1.5.m5.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.2b"><interval closure="open" id="S3.SS1.p1.5.m5.2.3.1.cmml" xref="S3.SS1.p1.5.m5.2.3.2"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">ğ‘–</ci><ci id="S3.SS1.p1.5.m5.2.2.cmml" xref="S3.SS1.p1.5.m5.2.2">ğ‘—</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.2c">(i,j)</annotation></semantics></math> location of the <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><mi id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">k</annotation></semantics></math>-th heatmap.
The position of the <math id="S3.SS1.p1.7.m7.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.p1.7.m7.1a"><mi id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><ci id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">k</annotation></semantics></math>-th joint can be computed as</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.3" class="ltx_Math" alttext="l_{k}^{*}=\operatorname*{arg\,max}_{(i,j)}\mathbf{H}_{ijk}," display="block"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml"><msubsup id="S3.E1.m1.3.3.1.1.2" xref="S3.E1.m1.3.3.1.1.2.cmml"><mi id="S3.E1.m1.3.3.1.1.2.2.2" xref="S3.E1.m1.3.3.1.1.2.2.2.cmml">l</mi><mi id="S3.E1.m1.3.3.1.1.2.2.3" xref="S3.E1.m1.3.3.1.1.2.2.3.cmml">k</mi><mo id="S3.E1.m1.3.3.1.1.2.3" xref="S3.E1.m1.3.3.1.1.2.3.cmml">âˆ—</mo></msubsup><mo id="S3.E1.m1.3.3.1.1.1" xref="S3.E1.m1.3.3.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.3.3.1.1.3" xref="S3.E1.m1.3.3.1.1.3.cmml"><munder id="S3.E1.m1.3.3.1.1.3.1" xref="S3.E1.m1.3.3.1.1.3.1.cmml"><mrow id="S3.E1.m1.3.3.1.1.3.1.2" xref="S3.E1.m1.3.3.1.1.3.1.2.cmml"><mi id="S3.E1.m1.3.3.1.1.3.1.2.2" xref="S3.E1.m1.3.3.1.1.3.1.2.2.cmml">arg</mi><mo lspace="0.170em" rspace="0em" id="S3.E1.m1.3.3.1.1.3.1.2.1" xref="S3.E1.m1.3.3.1.1.3.1.2.1.cmml">â€‹</mo><mi id="S3.E1.m1.3.3.1.1.3.1.2.3" xref="S3.E1.m1.3.3.1.1.3.1.2.3.cmml">max</mi></mrow><mrow id="S3.E1.m1.2.2.2.4" xref="S3.E1.m1.2.2.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.2.4.1" xref="S3.E1.m1.2.2.2.3.cmml">(</mo><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">i</mi><mo id="S3.E1.m1.2.2.2.4.2" xref="S3.E1.m1.2.2.2.3.cmml">,</mo><mi id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml">j</mi><mo stretchy="false" id="S3.E1.m1.2.2.2.4.3" xref="S3.E1.m1.2.2.2.3.cmml">)</mo></mrow></munder><mo id="S3.E1.m1.3.3.1.1.3a" xref="S3.E1.m1.3.3.1.1.3.cmml">â¡</mo><msub id="S3.E1.m1.3.3.1.1.3.2" xref="S3.E1.m1.3.3.1.1.3.2.cmml"><mi id="S3.E1.m1.3.3.1.1.3.2.2" xref="S3.E1.m1.3.3.1.1.3.2.2.cmml">ğ‡</mi><mrow id="S3.E1.m1.3.3.1.1.3.2.3" xref="S3.E1.m1.3.3.1.1.3.2.3.cmml"><mi id="S3.E1.m1.3.3.1.1.3.2.3.2" xref="S3.E1.m1.3.3.1.1.3.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.3.2.3.1" xref="S3.E1.m1.3.3.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.3.3.1.1.3.2.3.3" xref="S3.E1.m1.3.3.1.1.3.2.3.3.cmml">j</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.3.2.3.1a" xref="S3.E1.m1.3.3.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.3.3.1.1.3.2.3.4" xref="S3.E1.m1.3.3.1.1.3.2.3.4.cmml">k</mi></mrow></msub></mrow></mrow><mo id="S3.E1.m1.3.3.1.2" xref="S3.E1.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1"><eq id="S3.E1.m1.3.3.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1"></eq><apply id="S3.E1.m1.3.3.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.2">superscript</csymbol><apply id="S3.E1.m1.3.3.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.2.1.cmml" xref="S3.E1.m1.3.3.1.1.2">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2">ğ‘™</ci><ci id="S3.E1.m1.3.3.1.1.2.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3">ğ‘˜</ci></apply><times id="S3.E1.m1.3.3.1.1.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.3"></times></apply><apply id="S3.E1.m1.3.3.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.3"><apply id="S3.E1.m1.3.3.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.3.1.1.cmml" xref="S3.E1.m1.3.3.1.1.3.1">subscript</csymbol><apply id="S3.E1.m1.3.3.1.1.3.1.2.cmml" xref="S3.E1.m1.3.3.1.1.3.1.2"><times id="S3.E1.m1.3.3.1.1.3.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.3.1.2.1"></times><ci id="S3.E1.m1.3.3.1.1.3.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.3.1.2.2">arg</ci><ci id="S3.E1.m1.3.3.1.1.3.1.2.3.cmml" xref="S3.E1.m1.3.3.1.1.3.1.2.3">max</ci></apply><interval closure="open" id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.4"><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">ğ‘–</ci><ci id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2">ğ‘—</ci></interval></apply><apply id="S3.E1.m1.3.3.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.3.2.1.cmml" xref="S3.E1.m1.3.3.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.3.2.2.cmml" xref="S3.E1.m1.3.3.1.1.3.2.2">ğ‡</ci><apply id="S3.E1.m1.3.3.1.1.3.2.3.cmml" xref="S3.E1.m1.3.3.1.1.3.2.3"><times id="S3.E1.m1.3.3.1.1.3.2.3.1.cmml" xref="S3.E1.m1.3.3.1.1.3.2.3.1"></times><ci id="S3.E1.m1.3.3.1.1.3.2.3.2.cmml" xref="S3.E1.m1.3.3.1.1.3.2.3.2">ğ‘–</ci><ci id="S3.E1.m1.3.3.1.1.3.2.3.3.cmml" xref="S3.E1.m1.3.3.1.1.3.2.3.3">ğ‘—</ci><ci id="S3.E1.m1.3.3.1.1.3.2.3.4.cmml" xref="S3.E1.m1.3.3.1.1.3.2.3.4">ğ‘˜</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">l_{k}^{*}=\operatorname*{arg\,max}_{(i,j)}\mathbf{H}_{ijk},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.8" class="ltx_p">where <math id="S3.SS1.p1.8.m1.1" class="ltx_Math" alttext="l_{k}^{*}" display="inline"><semantics id="S3.SS1.p1.8.m1.1a"><msubsup id="S3.SS1.p1.8.m1.1.1" xref="S3.SS1.p1.8.m1.1.1.cmml"><mi id="S3.SS1.p1.8.m1.1.1.2.2" xref="S3.SS1.p1.8.m1.1.1.2.2.cmml">l</mi><mi id="S3.SS1.p1.8.m1.1.1.2.3" xref="S3.SS1.p1.8.m1.1.1.2.3.cmml">k</mi><mo id="S3.SS1.p1.8.m1.1.1.3" xref="S3.SS1.p1.8.m1.1.1.3.cmml">âˆ—</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m1.1b"><apply id="S3.SS1.p1.8.m1.1.1.cmml" xref="S3.SS1.p1.8.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m1.1.1.1.cmml" xref="S3.SS1.p1.8.m1.1.1">superscript</csymbol><apply id="S3.SS1.p1.8.m1.1.1.2.cmml" xref="S3.SS1.p1.8.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m1.1.1.2.1.cmml" xref="S3.SS1.p1.8.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.8.m1.1.1.2.2.cmml" xref="S3.SS1.p1.8.m1.1.1.2.2">ğ‘™</ci><ci id="S3.SS1.p1.8.m1.1.1.2.3.cmml" xref="S3.SS1.p1.8.m1.1.1.2.3">ğ‘˜</ci></apply><times id="S3.SS1.p1.8.m1.1.1.3.cmml" xref="S3.SS1.p1.8.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m1.1c">l_{k}^{*}</annotation></semantics></math> is the position within the heatmap
and can be transformed to the position in the frame
according to the center and scale information of the
cropped image.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.5" class="ltx_p">The training loss of the single-frame
pose estimation model is computed against the heatmaps.
A cropped human example is first scaled to a
fixed size and the corresponding ground-truth joints are properly transformed to the coordinates in heatmaps.
Let <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="l_{k}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><msub id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">l</mi><mi id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">ğ‘™</ci><ci id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">l_{k}</annotation></semantics></math> be the ground-truth location of the <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">k</annotation></semantics></math>-th joint in the heatmap.
The ground truth heatmap is generated following
a 2D Gaussian distribution:
<math id="S3.SS1.p2.3.m3.5" class="ltx_Math" alttext="\mathbf{H}_{ijk}^{gt}=\exp(-\frac{||(i,j)-l_{k}||_{2}^{2}}{\sigma^{2}})" display="inline"><semantics id="S3.SS1.p2.3.m3.5a"><mrow id="S3.SS1.p2.3.m3.5.5" xref="S3.SS1.p2.3.m3.5.5.cmml"><msubsup id="S3.SS1.p2.3.m3.5.5.3" xref="S3.SS1.p2.3.m3.5.5.3.cmml"><mi id="S3.SS1.p2.3.m3.5.5.3.2.2" xref="S3.SS1.p2.3.m3.5.5.3.2.2.cmml">ğ‡</mi><mrow id="S3.SS1.p2.3.m3.5.5.3.2.3" xref="S3.SS1.p2.3.m3.5.5.3.2.3.cmml"><mi id="S3.SS1.p2.3.m3.5.5.3.2.3.2" xref="S3.SS1.p2.3.m3.5.5.3.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.3.m3.5.5.3.2.3.1" xref="S3.SS1.p2.3.m3.5.5.3.2.3.1.cmml">â€‹</mo><mi id="S3.SS1.p2.3.m3.5.5.3.2.3.3" xref="S3.SS1.p2.3.m3.5.5.3.2.3.3.cmml">j</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.3.m3.5.5.3.2.3.1a" xref="S3.SS1.p2.3.m3.5.5.3.2.3.1.cmml">â€‹</mo><mi id="S3.SS1.p2.3.m3.5.5.3.2.3.4" xref="S3.SS1.p2.3.m3.5.5.3.2.3.4.cmml">k</mi></mrow><mrow id="S3.SS1.p2.3.m3.5.5.3.3" xref="S3.SS1.p2.3.m3.5.5.3.3.cmml"><mi id="S3.SS1.p2.3.m3.5.5.3.3.2" xref="S3.SS1.p2.3.m3.5.5.3.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.3.m3.5.5.3.3.1" xref="S3.SS1.p2.3.m3.5.5.3.3.1.cmml">â€‹</mo><mi id="S3.SS1.p2.3.m3.5.5.3.3.3" xref="S3.SS1.p2.3.m3.5.5.3.3.3.cmml">t</mi></mrow></msubsup><mo id="S3.SS1.p2.3.m3.5.5.2" xref="S3.SS1.p2.3.m3.5.5.2.cmml">=</mo><mrow id="S3.SS1.p2.3.m3.5.5.1.1" xref="S3.SS1.p2.3.m3.5.5.1.2.cmml"><mi id="S3.SS1.p2.3.m3.4.4" xref="S3.SS1.p2.3.m3.4.4.cmml">exp</mi><mo id="S3.SS1.p2.3.m3.5.5.1.1a" xref="S3.SS1.p2.3.m3.5.5.1.2.cmml">â¡</mo><mrow id="S3.SS1.p2.3.m3.5.5.1.1.1" xref="S3.SS1.p2.3.m3.5.5.1.2.cmml"><mo stretchy="false" id="S3.SS1.p2.3.m3.5.5.1.1.1.2" xref="S3.SS1.p2.3.m3.5.5.1.2.cmml">(</mo><mrow id="S3.SS1.p2.3.m3.5.5.1.1.1.1" xref="S3.SS1.p2.3.m3.5.5.1.1.1.1.cmml"><mo id="S3.SS1.p2.3.m3.5.5.1.1.1.1a" xref="S3.SS1.p2.3.m3.5.5.1.1.1.1.cmml">âˆ’</mo><mfrac id="S3.SS1.p2.3.m3.3.3" xref="S3.SS1.p2.3.m3.3.3.cmml"><msubsup id="S3.SS1.p2.3.m3.3.3.3" xref="S3.SS1.p2.3.m3.3.3.3.cmml"><mrow id="S3.SS1.p2.3.m3.3.3.3.3.1.1" xref="S3.SS1.p2.3.m3.3.3.3.3.1.2.cmml"><mo maxsize="142%" minsize="142%" id="S3.SS1.p2.3.m3.3.3.3.3.1.1.2" xref="S3.SS1.p2.3.m3.3.3.3.3.1.2.1.cmml">â€–</mo><mrow id="S3.SS1.p2.3.m3.3.3.3.3.1.1.1" xref="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.cmml"><mrow id="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.2.2" xref="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.2.1.cmml"><mo stretchy="false" id="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.2.2.1" xref="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.2.1.cmml">(</mo><mi id="S3.SS1.p2.3.m3.1.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.1.cmml">i</mi><mo id="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.2.2.2" xref="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.2.1.cmml">,</mo><mi id="S3.SS1.p2.3.m3.2.2.2.2" xref="S3.SS1.p2.3.m3.2.2.2.2.cmml">j</mi><mo stretchy="false" id="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.2.2.3" xref="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.2.1.cmml">)</mo></mrow><mo id="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.1" xref="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.1.cmml">âˆ’</mo><msub id="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.3" xref="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.3.cmml"><mi id="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.3.2" xref="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.3.2.cmml">l</mi><mi id="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.3.3" xref="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.3.3.cmml">k</mi></msub></mrow><mo maxsize="142%" minsize="142%" id="S3.SS1.p2.3.m3.3.3.3.3.1.1.3" xref="S3.SS1.p2.3.m3.3.3.3.3.1.2.1.cmml">â€–</mo></mrow><mn id="S3.SS1.p2.3.m3.3.3.3.3.3" xref="S3.SS1.p2.3.m3.3.3.3.3.3.cmml">2</mn><mn id="S3.SS1.p2.3.m3.3.3.3.5" xref="S3.SS1.p2.3.m3.3.3.3.5.cmml">2</mn></msubsup><msup id="S3.SS1.p2.3.m3.3.3.5" xref="S3.SS1.p2.3.m3.3.3.5.cmml"><mi id="S3.SS1.p2.3.m3.3.3.5.2" xref="S3.SS1.p2.3.m3.3.3.5.2.cmml">Ïƒ</mi><mn id="S3.SS1.p2.3.m3.3.3.5.3" xref="S3.SS1.p2.3.m3.3.3.5.3.cmml">2</mn></msup></mfrac></mrow><mo stretchy="false" id="S3.SS1.p2.3.m3.5.5.1.1.1.3" xref="S3.SS1.p2.3.m3.5.5.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.5b"><apply id="S3.SS1.p2.3.m3.5.5.cmml" xref="S3.SS1.p2.3.m3.5.5"><eq id="S3.SS1.p2.3.m3.5.5.2.cmml" xref="S3.SS1.p2.3.m3.5.5.2"></eq><apply id="S3.SS1.p2.3.m3.5.5.3.cmml" xref="S3.SS1.p2.3.m3.5.5.3"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.5.5.3.1.cmml" xref="S3.SS1.p2.3.m3.5.5.3">superscript</csymbol><apply id="S3.SS1.p2.3.m3.5.5.3.2.cmml" xref="S3.SS1.p2.3.m3.5.5.3"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.5.5.3.2.1.cmml" xref="S3.SS1.p2.3.m3.5.5.3">subscript</csymbol><ci id="S3.SS1.p2.3.m3.5.5.3.2.2.cmml" xref="S3.SS1.p2.3.m3.5.5.3.2.2">ğ‡</ci><apply id="S3.SS1.p2.3.m3.5.5.3.2.3.cmml" xref="S3.SS1.p2.3.m3.5.5.3.2.3"><times id="S3.SS1.p2.3.m3.5.5.3.2.3.1.cmml" xref="S3.SS1.p2.3.m3.5.5.3.2.3.1"></times><ci id="S3.SS1.p2.3.m3.5.5.3.2.3.2.cmml" xref="S3.SS1.p2.3.m3.5.5.3.2.3.2">ğ‘–</ci><ci id="S3.SS1.p2.3.m3.5.5.3.2.3.3.cmml" xref="S3.SS1.p2.3.m3.5.5.3.2.3.3">ğ‘—</ci><ci id="S3.SS1.p2.3.m3.5.5.3.2.3.4.cmml" xref="S3.SS1.p2.3.m3.5.5.3.2.3.4">ğ‘˜</ci></apply></apply><apply id="S3.SS1.p2.3.m3.5.5.3.3.cmml" xref="S3.SS1.p2.3.m3.5.5.3.3"><times id="S3.SS1.p2.3.m3.5.5.3.3.1.cmml" xref="S3.SS1.p2.3.m3.5.5.3.3.1"></times><ci id="S3.SS1.p2.3.m3.5.5.3.3.2.cmml" xref="S3.SS1.p2.3.m3.5.5.3.3.2">ğ‘”</ci><ci id="S3.SS1.p2.3.m3.5.5.3.3.3.cmml" xref="S3.SS1.p2.3.m3.5.5.3.3.3">ğ‘¡</ci></apply></apply><apply id="S3.SS1.p2.3.m3.5.5.1.2.cmml" xref="S3.SS1.p2.3.m3.5.5.1.1"><exp id="S3.SS1.p2.3.m3.4.4.cmml" xref="S3.SS1.p2.3.m3.4.4"></exp><apply id="S3.SS1.p2.3.m3.5.5.1.1.1.1.cmml" xref="S3.SS1.p2.3.m3.5.5.1.1.1.1"><minus id="S3.SS1.p2.3.m3.5.5.1.1.1.1.1.cmml" xref="S3.SS1.p2.3.m3.5.5.1.1.1.1"></minus><apply id="S3.SS1.p2.3.m3.3.3.cmml" xref="S3.SS1.p2.3.m3.3.3"><divide id="S3.SS1.p2.3.m3.3.3.4.cmml" xref="S3.SS1.p2.3.m3.3.3"></divide><apply id="S3.SS1.p2.3.m3.3.3.3.cmml" xref="S3.SS1.p2.3.m3.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.3.3.3.4.cmml" xref="S3.SS1.p2.3.m3.3.3.3">superscript</csymbol><apply id="S3.SS1.p2.3.m3.3.3.3.3.cmml" xref="S3.SS1.p2.3.m3.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.3.3.3.3.2.cmml" xref="S3.SS1.p2.3.m3.3.3.3">subscript</csymbol><apply id="S3.SS1.p2.3.m3.3.3.3.3.1.2.cmml" xref="S3.SS1.p2.3.m3.3.3.3.3.1.1"><csymbol cd="latexml" id="S3.SS1.p2.3.m3.3.3.3.3.1.2.1.cmml" xref="S3.SS1.p2.3.m3.3.3.3.3.1.1.2">norm</csymbol><apply id="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.3.3.3.3.1.1.1"><minus id="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.1.cmml" xref="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.1"></minus><interval closure="open" id="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.2.1.cmml" xref="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.2.2"><ci id="S3.SS1.p2.3.m3.1.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1">ğ‘–</ci><ci id="S3.SS1.p2.3.m3.2.2.2.2.cmml" xref="S3.SS1.p2.3.m3.2.2.2.2">ğ‘—</ci></interval><apply id="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.3.cmml" xref="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.3.1.cmml" xref="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.3">subscript</csymbol><ci id="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.3.2.cmml" xref="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.3.2">ğ‘™</ci><ci id="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.3.3.cmml" xref="S3.SS1.p2.3.m3.3.3.3.3.1.1.1.3.3">ğ‘˜</ci></apply></apply></apply><cn type="integer" id="S3.SS1.p2.3.m3.3.3.3.3.3.cmml" xref="S3.SS1.p2.3.m3.3.3.3.3.3">2</cn></apply><cn type="integer" id="S3.SS1.p2.3.m3.3.3.3.5.cmml" xref="S3.SS1.p2.3.m3.3.3.3.5">2</cn></apply><apply id="S3.SS1.p2.3.m3.3.3.5.cmml" xref="S3.SS1.p2.3.m3.3.3.5"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.3.3.5.1.cmml" xref="S3.SS1.p2.3.m3.3.3.5">superscript</csymbol><ci id="S3.SS1.p2.3.m3.3.3.5.2.cmml" xref="S3.SS1.p2.3.m3.3.3.5.2">ğœ</ci><cn type="integer" id="S3.SS1.p2.3.m3.3.3.5.3.cmml" xref="S3.SS1.p2.3.m3.3.3.5.3">2</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.5c">\mathbf{H}_{ijk}^{gt}=\exp(-\frac{||(i,j)-l_{k}||_{2}^{2}}{\sigma^{2}})</annotation></semantics></math>.
<math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mi id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">Ïƒ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><ci id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">\sigma</annotation></semantics></math> is set to be <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><mn id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><cn type="integer" id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">3</annotation></semantics></math> in all our experiments.
We train the human pose estimation model by minimizing the following loss:
</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\mathcal{L}_{e}=\sum_{i}^{H}\sum_{j}^{W}\sum_{k}^{K}||\mathbf{H}_{ijk}^{pred}-\mathbf{H}_{ijk}^{gt}||_{2}^{2}," display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.3.2.cmml">â„’</mi><mi id="S3.E2.m1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.3.3.cmml">e</mi></msub><mo rspace="0.111em" id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml"><munderover id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S3.E2.m1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.1.1.1.1.1.2.2.2.cmml">âˆ‘</mo><mi id="S3.E2.m1.1.1.1.1.1.2.2.3" xref="S3.E2.m1.1.1.1.1.1.2.2.3.cmml">i</mi><mi id="S3.E2.m1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.2.3.cmml">H</mi></munderover><mrow id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><munderover id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.1.1.1.1.1.1.2.2.2.cmml">âˆ‘</mo><mi id="S3.E2.m1.1.1.1.1.1.1.2.2.3" xref="S3.E2.m1.1.1.1.1.1.1.2.2.3.cmml">j</mi><mi id="S3.E2.m1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.2.3.cmml">W</mi></munderover><mrow id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><munderover id="S3.E2.m1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.2.2.2.cmml">âˆ‘</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.2.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.2.2.3.cmml">k</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.2.3.cmml">K</mi></munderover><msubsup id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">â€–</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msubsup id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml">ğ‡</mi><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.3.cmml">j</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.1a" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.4" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.4.cmml">k</mi></mrow><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1a" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.4" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1b" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.5" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.5.cmml">d</mi></mrow></msubsup><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><msubsup id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml">ğ‡</mi><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.3.cmml">j</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.1a" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.4" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.4.cmml">k</mi></mrow><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml">t</mi></mrow></msubsup></mrow><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">â€–</mo></mrow><mn id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml">2</mn><mn id="S3.E2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml">2</mn></msubsup></mrow></mrow></mrow></mrow><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"></eq><apply id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2">â„’</ci><ci id="S3.E2.m1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3">ğ‘’</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><apply id="S3.E2.m1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.2">subscript</csymbol><sum id="S3.E2.m1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2.2.2"></sum><ci id="S3.E2.m1.1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.2.2.3">ğ‘–</ci></apply><ci id="S3.E2.m1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.2.3">ğ»</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><apply id="S3.E2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S3.E2.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.2.2"></sum><ci id="S3.E2.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.2.3">ğ‘—</ci></apply><ci id="S3.E2.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.3">ğ‘Š</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1"><apply id="S3.E2.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S3.E2.m1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2.2.2"></sum><ci id="S3.E2.m1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2.2.3">ğ‘˜</ci></apply><ci id="S3.E2.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2.3">ğ¾</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1"><minus id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2">ğ‡</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3"><times id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.1"></times><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.2">ğ‘–</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.3">ğ‘—</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.4.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.4">ğ‘˜</ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3"><times id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1"></times><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2">ğ‘</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3">ğ‘Ÿ</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.4.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.4">ğ‘’</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.5.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.5">ğ‘‘</ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2">ğ‡</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3"><times id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.1"></times><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.2">ğ‘–</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.3">ğ‘—</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.4.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.4">ğ‘˜</ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3"><times id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.1"></times><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.2">ğ‘”</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3">ğ‘¡</ci></apply></apply></apply></apply><cn type="integer" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3">2</cn></apply><cn type="integer" id="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\mathcal{L}_{e}=\sum_{i}^{H}\sum_{j}^{W}\sum_{k}^{K}||\mathbf{H}_{ijk}^{pred}-\mathbf{H}_{ijk}^{gt}||_{2}^{2},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p2.8" class="ltx_p">where <math id="S3.SS1.p2.6.m1.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S3.SS1.p2.6.m1.1a"><mi id="S3.SS1.p2.6.m1.1.1" xref="S3.SS1.p2.6.m1.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m1.1b"><ci id="S3.SS1.p2.6.m1.1.1.cmml" xref="S3.SS1.p2.6.m1.1.1">ğ»</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m1.1c">H</annotation></semantics></math> and <math id="S3.SS1.p2.7.m2.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S3.SS1.p2.7.m2.1a"><mi id="S3.SS1.p2.7.m2.1.1" xref="S3.SS1.p2.7.m2.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m2.1b"><ci id="S3.SS1.p2.7.m2.1.1.cmml" xref="S3.SS1.p2.7.m2.1.1">ğ‘Š</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m2.1c">W</annotation></semantics></math> represent the height and width of heatmaps, and <math id="S3.SS1.p2.8.m3.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS1.p2.8.m3.1a"><mi id="S3.SS1.p2.8.m3.1.1" xref="S3.SS1.p2.8.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m3.1b"><ci id="S3.SS1.p2.8.m3.1.1.cmml" xref="S3.SS1.p2.8.m3.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m3.1c">K</annotation></semantics></math> is the
number of joints.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Dynamics Modeling via GNN</h3>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2106.03772/assets/x3.png" id="S3.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="456" height="386" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Illustration of our GNN model. Nodes in the tracklet are the joints of poses,
while edges are the connections between joints within the same pose or across consecutive poses.
During the pose prediction,
we model each position in the current frame
as a node and generate the heatmaps
by classifying all the nodes. <math id="S3.F3.3.m1.1" class="ltx_Math" alttext="L_{2}" display="inline"><semantics id="S3.F3.3.m1.1b"><msub id="S3.F3.3.m1.1.1" xref="S3.F3.3.m1.1.1.cmml"><mi id="S3.F3.3.m1.1.1.2" xref="S3.F3.3.m1.1.1.2.cmml">L</mi><mn id="S3.F3.3.m1.1.1.3" xref="S3.F3.3.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.F3.3.m1.1c"><apply id="S3.F3.3.m1.1.1.cmml" xref="S3.F3.3.m1.1.1"><csymbol cd="ambiguous" id="S3.F3.3.m1.1.1.1.cmml" xref="S3.F3.3.m1.1.1">subscript</csymbol><ci id="S3.F3.3.m1.1.1.2.cmml" xref="S3.F3.3.m1.1.1.2">ğ¿</ci><cn type="integer" id="S3.F3.3.m1.1.1.3.cmml" xref="S3.F3.3.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.3.m1.1d">L_{2}</annotation></semantics></math> norm is used as the loss function to train the GNN model.</figcaption>
</figure>
<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">As shown in FigureÂ <a href="#S2.F2" title="Figure 2 â€£ 2.1 Single-Frame Human Pose Estimation â€£ 2 Related Work â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, given the tracked poses of the same identity from prior frames, we design a GNN-based model
to explicitly capture the spatial-temporal human motion dynamics from history poses and make prediction for the subjectâ€™s pose in current frame.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The GNN as a human pose dynamics model has joints of tracklets as the nodes.
Edges between all pair-wise joints within-frame and between consecutive frames help capture the relative location constraints between joints as well as human motion dynamics. When applied to history tracklets as shown in the Joint Aggregation part in FigureÂ <a href="#S3.F3" title="Figure 3 â€£ 3.2 Dynamics Modeling via GNN â€£ 3 Method â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the GNN updates features on the nodes with respective to the learned dynamics.
For pose prediction, each location in the current frame is considered as a node and is connected to the joints of the last pose in the tracklet. The GNN performs feature aggregation for the locations in the current frame and classifies each location by its aggregated features to determine the joint type of the location.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.5" class="ltx_p">Let <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">t</annotation></semantics></math> be the total number of frames involved in the GNN. A FIFO queue is used
to maintain the history poses with the same identity.
We denote a human pose as <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="\mathbf{P}_{r}" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><msub id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">ğ</mi><mi id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">ğ</ci><ci id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">\mathbf{P}_{r}</annotation></semantics></math>, where <math id="S3.SS2.p3.3.m3.3" class="ltx_Math" alttext="r\in\{1,\dots,t\}" display="inline"><semantics id="S3.SS2.p3.3.m3.3a"><mrow id="S3.SS2.p3.3.m3.3.4" xref="S3.SS2.p3.3.m3.3.4.cmml"><mi id="S3.SS2.p3.3.m3.3.4.2" xref="S3.SS2.p3.3.m3.3.4.2.cmml">r</mi><mo id="S3.SS2.p3.3.m3.3.4.1" xref="S3.SS2.p3.3.m3.3.4.1.cmml">âˆˆ</mo><mrow id="S3.SS2.p3.3.m3.3.4.3.2" xref="S3.SS2.p3.3.m3.3.4.3.1.cmml"><mo stretchy="false" id="S3.SS2.p3.3.m3.3.4.3.2.1" xref="S3.SS2.p3.3.m3.3.4.3.1.cmml">{</mo><mn id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml">1</mn><mo id="S3.SS2.p3.3.m3.3.4.3.2.2" xref="S3.SS2.p3.3.m3.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p3.3.m3.2.2" xref="S3.SS2.p3.3.m3.2.2.cmml">â€¦</mi><mo id="S3.SS2.p3.3.m3.3.4.3.2.3" xref="S3.SS2.p3.3.m3.3.4.3.1.cmml">,</mo><mi id="S3.SS2.p3.3.m3.3.3" xref="S3.SS2.p3.3.m3.3.3.cmml">t</mi><mo stretchy="false" id="S3.SS2.p3.3.m3.3.4.3.2.4" xref="S3.SS2.p3.3.m3.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.3b"><apply id="S3.SS2.p3.3.m3.3.4.cmml" xref="S3.SS2.p3.3.m3.3.4"><in id="S3.SS2.p3.3.m3.3.4.1.cmml" xref="S3.SS2.p3.3.m3.3.4.1"></in><ci id="S3.SS2.p3.3.m3.3.4.2.cmml" xref="S3.SS2.p3.3.m3.3.4.2">ğ‘Ÿ</ci><set id="S3.SS2.p3.3.m3.3.4.3.1.cmml" xref="S3.SS2.p3.3.m3.3.4.3.2"><cn type="integer" id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">1</cn><ci id="S3.SS2.p3.3.m3.2.2.cmml" xref="S3.SS2.p3.3.m3.2.2">â€¦</ci><ci id="S3.SS2.p3.3.m3.3.3.cmml" xref="S3.SS2.p3.3.m3.3.3">ğ‘¡</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.3c">r\in\{1,\dots,t\}</annotation></semantics></math>.
<math id="S3.SS2.p3.4.m4.3" class="ltx_Math" alttext="\mathbf{P}_{1,\dots,t-1}" display="inline"><semantics id="S3.SS2.p3.4.m4.3a"><msub id="S3.SS2.p3.4.m4.3.4" xref="S3.SS2.p3.4.m4.3.4.cmml"><mi id="S3.SS2.p3.4.m4.3.4.2" xref="S3.SS2.p3.4.m4.3.4.2.cmml">ğ</mi><mrow id="S3.SS2.p3.4.m4.3.3.3.3" xref="S3.SS2.p3.4.m4.3.3.3.4.cmml"><mn id="S3.SS2.p3.4.m4.1.1.1.1" xref="S3.SS2.p3.4.m4.1.1.1.1.cmml">1</mn><mo id="S3.SS2.p3.4.m4.3.3.3.3.2" xref="S3.SS2.p3.4.m4.3.3.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p3.4.m4.2.2.2.2" xref="S3.SS2.p3.4.m4.2.2.2.2.cmml">â€¦</mi><mo id="S3.SS2.p3.4.m4.3.3.3.3.3" xref="S3.SS2.p3.4.m4.3.3.3.4.cmml">,</mo><mrow id="S3.SS2.p3.4.m4.3.3.3.3.1" xref="S3.SS2.p3.4.m4.3.3.3.3.1.cmml"><mi id="S3.SS2.p3.4.m4.3.3.3.3.1.2" xref="S3.SS2.p3.4.m4.3.3.3.3.1.2.cmml">t</mi><mo id="S3.SS2.p3.4.m4.3.3.3.3.1.1" xref="S3.SS2.p3.4.m4.3.3.3.3.1.1.cmml">âˆ’</mo><mn id="S3.SS2.p3.4.m4.3.3.3.3.1.3" xref="S3.SS2.p3.4.m4.3.3.3.3.1.3.cmml">1</mn></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.3b"><apply id="S3.SS2.p3.4.m4.3.4.cmml" xref="S3.SS2.p3.4.m4.3.4"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m4.3.4.1.cmml" xref="S3.SS2.p3.4.m4.3.4">subscript</csymbol><ci id="S3.SS2.p3.4.m4.3.4.2.cmml" xref="S3.SS2.p3.4.m4.3.4.2">ğ</ci><list id="S3.SS2.p3.4.m4.3.3.3.4.cmml" xref="S3.SS2.p3.4.m4.3.3.3.3"><cn type="integer" id="S3.SS2.p3.4.m4.1.1.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1.1.1">1</cn><ci id="S3.SS2.p3.4.m4.2.2.2.2.cmml" xref="S3.SS2.p3.4.m4.2.2.2.2">â€¦</ci><apply id="S3.SS2.p3.4.m4.3.3.3.3.1.cmml" xref="S3.SS2.p3.4.m4.3.3.3.3.1"><minus id="S3.SS2.p3.4.m4.3.3.3.3.1.1.cmml" xref="S3.SS2.p3.4.m4.3.3.3.3.1.1"></minus><ci id="S3.SS2.p3.4.m4.3.3.3.3.1.2.cmml" xref="S3.SS2.p3.4.m4.3.3.3.3.1.2">ğ‘¡</ci><cn type="integer" id="S3.SS2.p3.4.m4.3.3.3.3.1.3.cmml" xref="S3.SS2.p3.4.m4.3.3.3.3.1.3">1</cn></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.3c">\mathbf{P}_{1,\dots,t-1}</annotation></semantics></math> are from history tracklets and <math id="S3.SS2.p3.5.m5.1" class="ltx_Math" alttext="\mathbf{P}_{t}" display="inline"><semantics id="S3.SS2.p3.5.m5.1a"><msub id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml"><mi id="S3.SS2.p3.5.m5.1.1.2" xref="S3.SS2.p3.5.m5.1.1.2.cmml">ğ</mi><mi id="S3.SS2.p3.5.m5.1.1.3" xref="S3.SS2.p3.5.m5.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><apply id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.5.m5.1.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p3.5.m5.1.1.2.cmml" xref="S3.SS2.p3.5.m5.1.1.2">ğ</ci><ci id="S3.SS2.p3.5.m5.1.1.3.cmml" xref="S3.SS2.p3.5.m5.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">\mathbf{P}_{t}</annotation></semantics></math>
represents the predicted pose in the current frame.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Nodes in the proposed GNN model</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.7" class="ltx_p">Joints of history tracklets and potential joints of the human pose in current frame
are used as nodes in our GNN model. For each frame, we incorporate three kinds of cues on each joint to construct
the input node feature,
the visual feature from the backbone CNN of our single-frame pose estimator as <math id="S3.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="v_{k}" display="inline"><semantics id="S3.SS2.SSS1.p1.1.m1.1a"><msub id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS1.p1.1.m1.1.1.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml">v</mi><mi id="S3.SS2.SSS1.p1.1.m1.1.1.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.1b"><apply id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2">ğ‘£</ci><ci id="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.1c">v_{k}</annotation></semantics></math>,
the encoding of its joint type with a learnable lookup tableÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> as <math id="S3.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="c_{k}" display="inline"><semantics id="S3.SS2.SSS1.p1.2.m2.1a"><msub id="S3.SS2.SSS1.p1.2.m2.1.1" xref="S3.SS2.SSS1.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSS1.p1.2.m2.1.1.2" xref="S3.SS2.SSS1.p1.2.m2.1.1.2.cmml">c</mi><mi id="S3.SS2.SSS1.p1.2.m2.1.1.3" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.2.m2.1b"><apply id="S3.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.2">ğ‘</ci><ci id="S3.SS2.SSS1.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.2.m2.1c">c_{k}</annotation></semantics></math>,
and its 2D position and confidence score from pose estimator as <math id="S3.SS2.SSS1.p1.3.m3.1" class="ltx_Math" alttext="p_{k}" display="inline"><semantics id="S3.SS2.SSS1.p1.3.m3.1a"><msub id="S3.SS2.SSS1.p1.3.m3.1.1" xref="S3.SS2.SSS1.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS1.p1.3.m3.1.1.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.2.cmml">p</mi><mi id="S3.SS2.SSS1.p1.3.m3.1.1.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.3.m3.1b"><apply id="S3.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.2">ğ‘</ci><ci id="S3.SS2.SSS1.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.3.m3.1c">p_{k}</annotation></semantics></math>.
For the potential joint in the current frame we set its confidence to <math id="S3.SS2.SSS1.p1.4.m4.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS2.SSS1.p1.4.m4.1a"><mn id="S3.SS2.SSS1.p1.4.m4.1.1" xref="S3.SS2.SSS1.p1.4.m4.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.4.m4.1b"><cn type="integer" id="S3.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.4.m4.1c">1</annotation></semantics></math>. All the 2D position of joints are normalized according to the center of the last tracked pose <math id="S3.SS2.SSS1.p1.5.m5.1" class="ltx_Math" alttext="\mathbf{P}_{t-1}" display="inline"><semantics id="S3.SS2.SSS1.p1.5.m5.1a"><msub id="S3.SS2.SSS1.p1.5.m5.1.1" xref="S3.SS2.SSS1.p1.5.m5.1.1.cmml"><mi id="S3.SS2.SSS1.p1.5.m5.1.1.2" xref="S3.SS2.SSS1.p1.5.m5.1.1.2.cmml">ğ</mi><mrow id="S3.SS2.SSS1.p1.5.m5.1.1.3" xref="S3.SS2.SSS1.p1.5.m5.1.1.3.cmml"><mi id="S3.SS2.SSS1.p1.5.m5.1.1.3.2" xref="S3.SS2.SSS1.p1.5.m5.1.1.3.2.cmml">t</mi><mo id="S3.SS2.SSS1.p1.5.m5.1.1.3.1" xref="S3.SS2.SSS1.p1.5.m5.1.1.3.1.cmml">âˆ’</mo><mn id="S3.SS2.SSS1.p1.5.m5.1.1.3.3" xref="S3.SS2.SSS1.p1.5.m5.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.5.m5.1b"><apply id="S3.SS2.SSS1.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.5.m5.1.1.1.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p1.5.m5.1.1.2.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1.2">ğ</ci><apply id="S3.SS2.SSS1.p1.5.m5.1.1.3.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1.3"><minus id="S3.SS2.SSS1.p1.5.m5.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1.3.1"></minus><ci id="S3.SS2.SSS1.p1.5.m5.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1.3.2">ğ‘¡</ci><cn type="integer" id="S3.SS2.SSS1.p1.5.m5.1.1.3.3.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.5.m5.1c">\mathbf{P}_{t-1}</annotation></semantics></math>.
Normalizing joint positions with respect to the same center help capture the full body movement.
Here <math id="S3.SS2.SSS1.p1.6.m6.3" class="ltx_Math" alttext="k\in{1,\dots,K}" display="inline"><semantics id="S3.SS2.SSS1.p1.6.m6.3a"><mrow id="S3.SS2.SSS1.p1.6.m6.3.4" xref="S3.SS2.SSS1.p1.6.m6.3.4.cmml"><mi id="S3.SS2.SSS1.p1.6.m6.3.4.2" xref="S3.SS2.SSS1.p1.6.m6.3.4.2.cmml">k</mi><mo id="S3.SS2.SSS1.p1.6.m6.3.4.1" xref="S3.SS2.SSS1.p1.6.m6.3.4.1.cmml">âˆˆ</mo><mrow id="S3.SS2.SSS1.p1.6.m6.3.4.3.2" xref="S3.SS2.SSS1.p1.6.m6.3.4.3.1.cmml"><mn id="S3.SS2.SSS1.p1.6.m6.1.1" xref="S3.SS2.SSS1.p1.6.m6.1.1.cmml">1</mn><mo id="S3.SS2.SSS1.p1.6.m6.3.4.3.2.1" xref="S3.SS2.SSS1.p1.6.m6.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.SSS1.p1.6.m6.2.2" xref="S3.SS2.SSS1.p1.6.m6.2.2.cmml">â€¦</mi><mo id="S3.SS2.SSS1.p1.6.m6.3.4.3.2.2" xref="S3.SS2.SSS1.p1.6.m6.3.4.3.1.cmml">,</mo><mi id="S3.SS2.SSS1.p1.6.m6.3.3" xref="S3.SS2.SSS1.p1.6.m6.3.3.cmml">K</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.6.m6.3b"><apply id="S3.SS2.SSS1.p1.6.m6.3.4.cmml" xref="S3.SS2.SSS1.p1.6.m6.3.4"><in id="S3.SS2.SSS1.p1.6.m6.3.4.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.3.4.1"></in><ci id="S3.SS2.SSS1.p1.6.m6.3.4.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.3.4.2">ğ‘˜</ci><list id="S3.SS2.SSS1.p1.6.m6.3.4.3.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.3.4.3.2"><cn type="integer" id="S3.SS2.SSS1.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1">1</cn><ci id="S3.SS2.SSS1.p1.6.m6.2.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.2.2">â€¦</ci><ci id="S3.SS2.SSS1.p1.6.m6.3.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.3.3">ğ¾</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.6.m6.3c">k\in{1,\dots,K}</annotation></semantics></math> denotes the <math id="S3.SS2.SSS1.p1.7.m7.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS2.SSS1.p1.7.m7.1a"><mi id="S3.SS2.SSS1.p1.7.m7.1.1" xref="S3.SS2.SSS1.p1.7.m7.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.7.m7.1b"><ci id="S3.SS2.SSS1.p1.7.m7.1.1.cmml" xref="S3.SS2.SSS1.p1.7.m7.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.7.m7.1c">k</annotation></semantics></math>-th joint type of a given human pose.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.1" class="ltx_p">We use Multilayer PerceptronÂ (MLP) to transform all the joint features
to have the same dimension and merge them with average pooling, i.e.
The final feature of the <math id="S3.SS2.SSS1.p2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS2.SSS1.p2.1.m1.1a"><mi id="S3.SS2.SSS1.p2.1.m1.1.1" xref="S3.SS2.SSS1.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.1.m1.1b"><ci id="S3.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.1.m1.1c">k</annotation></semantics></math>-th joint is computed as follows:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.29" class="ltx_Math" alttext="\begin{split}\mathbf{J}_{k}=\textbf{Pooling}\big{(}&amp;\mathbf{MLP}_{vis}(v_{k}),\mathbf{MLP}_{pos}(p_{k}),\\
&amp;\mathbf{MLP}_{type}(c_{k})\big{)}.\end{split}" display="block"><semantics id="S3.E3.m1.29a"><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt" id="S3.E3.m1.29.29.2"><mtr id="S3.E3.m1.29.29.2a"><mtd class="ltx_align_right" columnalign="right" id="S3.E3.m1.29.29.2b"><mrow id="S3.E3.m1.5.5.5.5.5"><msub id="S3.E3.m1.5.5.5.5.5.6"><mi id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml">ğ‰</mi><mi id="S3.E3.m1.2.2.2.2.2.2.1" xref="S3.E3.m1.2.2.2.2.2.2.1.cmml">k</mi></msub><mo id="S3.E3.m1.3.3.3.3.3.3" xref="S3.E3.m1.3.3.3.3.3.3.cmml">=</mo><mtext class="ltx_mathvariant_bold" id="S3.E3.m1.4.4.4.4.4.4" xref="S3.E3.m1.4.4.4.4.4.4a.cmml">Pooling</mtext><mo maxsize="120%" minsize="120%" id="S3.E3.m1.5.5.5.5.5.5" xref="S3.E3.m1.28.28.1.1.1.cmml">(</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E3.m1.29.29.2c"><mrow id="S3.E3.m1.29.29.2.28.20.15.15"><mrow id="S3.E3.m1.29.29.2.28.20.15.15.1"><mrow id="S3.E3.m1.29.29.2.28.20.15.15.1.1.1"><msub id="S3.E3.m1.29.29.2.28.20.15.15.1.1.1.3"><mi id="S3.E3.m1.6.6.6.6.1.1" xref="S3.E3.m1.6.6.6.6.1.1.cmml">ğŒğ‹ğ</mi><mrow id="S3.E3.m1.7.7.7.7.2.2.1" xref="S3.E3.m1.7.7.7.7.2.2.1.cmml"><mi id="S3.E3.m1.7.7.7.7.2.2.1.2" xref="S3.E3.m1.7.7.7.7.2.2.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.7.7.2.2.1.1" xref="S3.E3.m1.7.7.7.7.2.2.1.1.cmml">â€‹</mo><mi id="S3.E3.m1.7.7.7.7.2.2.1.3" xref="S3.E3.m1.7.7.7.7.2.2.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.7.7.2.2.1.1a" xref="S3.E3.m1.7.7.7.7.2.2.1.1.cmml">â€‹</mo><mi id="S3.E3.m1.7.7.7.7.2.2.1.4" xref="S3.E3.m1.7.7.7.7.2.2.1.4.cmml">s</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.29.29.2.28.20.15.15.1.1.1.2" xref="S3.E3.m1.28.28.1.1.1.cmml">â€‹</mo><mrow id="S3.E3.m1.29.29.2.28.20.15.15.1.1.1.1.1"><mo stretchy="false" id="S3.E3.m1.8.8.8.8.3.3" xref="S3.E3.m1.28.28.1.1.1.cmml">(</mo><msub id="S3.E3.m1.29.29.2.28.20.15.15.1.1.1.1.1.1"><mi id="S3.E3.m1.9.9.9.9.4.4" xref="S3.E3.m1.9.9.9.9.4.4.cmml">v</mi><mi id="S3.E3.m1.10.10.10.10.5.5.1" xref="S3.E3.m1.10.10.10.10.5.5.1.cmml">k</mi></msub><mo stretchy="false" id="S3.E3.m1.11.11.11.11.6.6" xref="S3.E3.m1.28.28.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.12.12.12.12.7.7" xref="S3.E3.m1.28.28.1.1.1.cmml">,</mo><mrow id="S3.E3.m1.29.29.2.28.20.15.15.1.2.2"><msub id="S3.E3.m1.29.29.2.28.20.15.15.1.2.2.3"><mi id="S3.E3.m1.13.13.13.13.8.8" xref="S3.E3.m1.13.13.13.13.8.8.cmml">ğŒğ‹ğ</mi><mrow id="S3.E3.m1.14.14.14.14.9.9.1" xref="S3.E3.m1.14.14.14.14.9.9.1.cmml"><mi id="S3.E3.m1.14.14.14.14.9.9.1.2" xref="S3.E3.m1.14.14.14.14.9.9.1.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.14.14.14.14.9.9.1.1" xref="S3.E3.m1.14.14.14.14.9.9.1.1.cmml">â€‹</mo><mi id="S3.E3.m1.14.14.14.14.9.9.1.3" xref="S3.E3.m1.14.14.14.14.9.9.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.14.14.14.14.9.9.1.1a" xref="S3.E3.m1.14.14.14.14.9.9.1.1.cmml">â€‹</mo><mi id="S3.E3.m1.14.14.14.14.9.9.1.4" xref="S3.E3.m1.14.14.14.14.9.9.1.4.cmml">s</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.29.29.2.28.20.15.15.1.2.2.2" xref="S3.E3.m1.28.28.1.1.1.cmml">â€‹</mo><mrow id="S3.E3.m1.29.29.2.28.20.15.15.1.2.2.1.1"><mo stretchy="false" id="S3.E3.m1.15.15.15.15.10.10" xref="S3.E3.m1.28.28.1.1.1.cmml">(</mo><msub id="S3.E3.m1.29.29.2.28.20.15.15.1.2.2.1.1.1"><mi id="S3.E3.m1.16.16.16.16.11.11" xref="S3.E3.m1.16.16.16.16.11.11.cmml">p</mi><mi id="S3.E3.m1.17.17.17.17.12.12.1" xref="S3.E3.m1.17.17.17.17.12.12.1.cmml">k</mi></msub><mo stretchy="false" id="S3.E3.m1.18.18.18.18.13.13" xref="S3.E3.m1.28.28.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.19.19.19.19.14.14" xref="S3.E3.m1.28.28.1.1.1.cmml">,</mo></mrow></mtd></mtr><mtr id="S3.E3.m1.29.29.2d"><mtd id="S3.E3.m1.29.29.2e" xref="S3.E3.m1.28.28.1.1.1.cmml"></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E3.m1.29.29.2f"><mrow id="S3.E3.m1.27.27.27.8.8"><msub id="S3.E3.m1.27.27.27.8.8.9"><mi id="S3.E3.m1.20.20.20.1.1.1" xref="S3.E3.m1.20.20.20.1.1.1.cmml">ğŒğ‹ğ</mi><mrow id="S3.E3.m1.21.21.21.2.2.2.1" xref="S3.E3.m1.21.21.21.2.2.2.1.cmml"><mi id="S3.E3.m1.21.21.21.2.2.2.1.2" xref="S3.E3.m1.21.21.21.2.2.2.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.21.21.21.2.2.2.1.1" xref="S3.E3.m1.21.21.21.2.2.2.1.1.cmml">â€‹</mo><mi id="S3.E3.m1.21.21.21.2.2.2.1.3" xref="S3.E3.m1.21.21.21.2.2.2.1.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.21.21.21.2.2.2.1.1a" xref="S3.E3.m1.21.21.21.2.2.2.1.1.cmml">â€‹</mo><mi id="S3.E3.m1.21.21.21.2.2.2.1.4" xref="S3.E3.m1.21.21.21.2.2.2.1.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.21.21.21.2.2.2.1.1b" xref="S3.E3.m1.21.21.21.2.2.2.1.1.cmml">â€‹</mo><mi id="S3.E3.m1.21.21.21.2.2.2.1.5" xref="S3.E3.m1.21.21.21.2.2.2.1.5.cmml">e</mi></mrow></msub><mrow id="S3.E3.m1.27.27.27.8.8.10"><mo stretchy="false" id="S3.E3.m1.22.22.22.3.3.3" xref="S3.E3.m1.28.28.1.1.1.cmml">(</mo><msub id="S3.E3.m1.27.27.27.8.8.10.1"><mi id="S3.E3.m1.23.23.23.4.4.4" xref="S3.E3.m1.23.23.23.4.4.4.cmml">c</mi><mi id="S3.E3.m1.24.24.24.5.5.5.1" xref="S3.E3.m1.24.24.24.5.5.5.1.cmml">k</mi></msub><mo stretchy="false" id="S3.E3.m1.25.25.25.6.6.6" xref="S3.E3.m1.28.28.1.1.1.cmml">)</mo></mrow><mo maxsize="120%" minsize="120%" id="S3.E3.m1.26.26.26.7.7.7" xref="S3.E3.m1.28.28.1.1.1.cmml">)</mo><mo lspace="0em" id="S3.E3.m1.27.27.27.8.8.8" xref="S3.E3.m1.28.28.1.1.1.cmml">.</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E3.m1.29b"><apply id="S3.E3.m1.28.28.1.1.1.cmml" xref="S3.E3.m1.5.5.5.5.5.5"><eq id="S3.E3.m1.3.3.3.3.3.3.cmml" xref="S3.E3.m1.3.3.3.3.3.3"></eq><apply id="S3.E3.m1.28.28.1.1.1.5.cmml" xref="S3.E3.m1.5.5.5.5.5.5"><csymbol cd="ambiguous" id="S3.E3.m1.28.28.1.1.1.5.1.cmml" xref="S3.E3.m1.5.5.5.5.5.5">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1">ğ‰</ci><ci id="S3.E3.m1.2.2.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.2.2.2.1">ğ‘˜</ci></apply><apply id="S3.E3.m1.28.28.1.1.1.3.cmml" xref="S3.E3.m1.5.5.5.5.5.5"><times id="S3.E3.m1.28.28.1.1.1.3.4.cmml" xref="S3.E3.m1.5.5.5.5.5.5"></times><ci id="S3.E3.m1.4.4.4.4.4.4a.cmml" xref="S3.E3.m1.4.4.4.4.4.4"><mtext class="ltx_mathvariant_bold" id="S3.E3.m1.4.4.4.4.4.4.cmml" xref="S3.E3.m1.4.4.4.4.4.4">Pooling</mtext></ci><vector id="S3.E3.m1.28.28.1.1.1.3.3.4.cmml" xref="S3.E3.m1.5.5.5.5.5.5"><apply id="S3.E3.m1.28.28.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.5.5.5.5.5.5"><times id="S3.E3.m1.28.28.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.5.5.5.5.5.5"></times><apply id="S3.E3.m1.28.28.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.5.5.5.5.5.5"><csymbol cd="ambiguous" id="S3.E3.m1.28.28.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.5.5.5.5.5.5">subscript</csymbol><ci id="S3.E3.m1.6.6.6.6.1.1.cmml" xref="S3.E3.m1.6.6.6.6.1.1">ğŒğ‹ğ</ci><apply id="S3.E3.m1.7.7.7.7.2.2.1.cmml" xref="S3.E3.m1.7.7.7.7.2.2.1"><times id="S3.E3.m1.7.7.7.7.2.2.1.1.cmml" xref="S3.E3.m1.7.7.7.7.2.2.1.1"></times><ci id="S3.E3.m1.7.7.7.7.2.2.1.2.cmml" xref="S3.E3.m1.7.7.7.7.2.2.1.2">ğ‘£</ci><ci id="S3.E3.m1.7.7.7.7.2.2.1.3.cmml" xref="S3.E3.m1.7.7.7.7.2.2.1.3">ğ‘–</ci><ci id="S3.E3.m1.7.7.7.7.2.2.1.4.cmml" xref="S3.E3.m1.7.7.7.7.2.2.1.4">ğ‘ </ci></apply></apply><apply id="S3.E3.m1.28.28.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.5.5.5.5.5.5"><csymbol cd="ambiguous" id="S3.E3.m1.28.28.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.5.5.5.5.5.5">subscript</csymbol><ci id="S3.E3.m1.9.9.9.9.4.4.cmml" xref="S3.E3.m1.9.9.9.9.4.4">ğ‘£</ci><ci id="S3.E3.m1.10.10.10.10.5.5.1.cmml" xref="S3.E3.m1.10.10.10.10.5.5.1">ğ‘˜</ci></apply></apply><apply id="S3.E3.m1.28.28.1.1.1.2.2.2.2.cmml" xref="S3.E3.m1.5.5.5.5.5.5"><times id="S3.E3.m1.28.28.1.1.1.2.2.2.2.2.cmml" xref="S3.E3.m1.5.5.5.5.5.5"></times><apply id="S3.E3.m1.28.28.1.1.1.2.2.2.2.3.cmml" xref="S3.E3.m1.5.5.5.5.5.5"><csymbol cd="ambiguous" id="S3.E3.m1.28.28.1.1.1.2.2.2.2.3.1.cmml" xref="S3.E3.m1.5.5.5.5.5.5">subscript</csymbol><ci id="S3.E3.m1.13.13.13.13.8.8.cmml" xref="S3.E3.m1.13.13.13.13.8.8">ğŒğ‹ğ</ci><apply id="S3.E3.m1.14.14.14.14.9.9.1.cmml" xref="S3.E3.m1.14.14.14.14.9.9.1"><times id="S3.E3.m1.14.14.14.14.9.9.1.1.cmml" xref="S3.E3.m1.14.14.14.14.9.9.1.1"></times><ci id="S3.E3.m1.14.14.14.14.9.9.1.2.cmml" xref="S3.E3.m1.14.14.14.14.9.9.1.2">ğ‘</ci><ci id="S3.E3.m1.14.14.14.14.9.9.1.3.cmml" xref="S3.E3.m1.14.14.14.14.9.9.1.3">ğ‘œ</ci><ci id="S3.E3.m1.14.14.14.14.9.9.1.4.cmml" xref="S3.E3.m1.14.14.14.14.9.9.1.4">ğ‘ </ci></apply></apply><apply id="S3.E3.m1.28.28.1.1.1.2.2.2.2.1.1.1.cmml" xref="S3.E3.m1.5.5.5.5.5.5"><csymbol cd="ambiguous" id="S3.E3.m1.28.28.1.1.1.2.2.2.2.1.1.1.1.cmml" xref="S3.E3.m1.5.5.5.5.5.5">subscript</csymbol><ci id="S3.E3.m1.16.16.16.16.11.11.cmml" xref="S3.E3.m1.16.16.16.16.11.11">ğ‘</ci><ci id="S3.E3.m1.17.17.17.17.12.12.1.cmml" xref="S3.E3.m1.17.17.17.17.12.12.1">ğ‘˜</ci></apply></apply><apply id="S3.E3.m1.28.28.1.1.1.3.3.3.3.cmml" xref="S3.E3.m1.5.5.5.5.5.5"><times id="S3.E3.m1.28.28.1.1.1.3.3.3.3.2.cmml" xref="S3.E3.m1.5.5.5.5.5.5"></times><apply id="S3.E3.m1.28.28.1.1.1.3.3.3.3.3.cmml" xref="S3.E3.m1.5.5.5.5.5.5"><csymbol cd="ambiguous" id="S3.E3.m1.28.28.1.1.1.3.3.3.3.3.1.cmml" xref="S3.E3.m1.5.5.5.5.5.5">subscript</csymbol><ci id="S3.E3.m1.20.20.20.1.1.1.cmml" xref="S3.E3.m1.20.20.20.1.1.1">ğŒğ‹ğ</ci><apply id="S3.E3.m1.21.21.21.2.2.2.1.cmml" xref="S3.E3.m1.21.21.21.2.2.2.1"><times id="S3.E3.m1.21.21.21.2.2.2.1.1.cmml" xref="S3.E3.m1.21.21.21.2.2.2.1.1"></times><ci id="S3.E3.m1.21.21.21.2.2.2.1.2.cmml" xref="S3.E3.m1.21.21.21.2.2.2.1.2">ğ‘¡</ci><ci id="S3.E3.m1.21.21.21.2.2.2.1.3.cmml" xref="S3.E3.m1.21.21.21.2.2.2.1.3">ğ‘¦</ci><ci id="S3.E3.m1.21.21.21.2.2.2.1.4.cmml" xref="S3.E3.m1.21.21.21.2.2.2.1.4">ğ‘</ci><ci id="S3.E3.m1.21.21.21.2.2.2.1.5.cmml" xref="S3.E3.m1.21.21.21.2.2.2.1.5">ğ‘’</ci></apply></apply><apply id="S3.E3.m1.28.28.1.1.1.3.3.3.3.1.1.1.cmml" xref="S3.E3.m1.5.5.5.5.5.5"><csymbol cd="ambiguous" id="S3.E3.m1.28.28.1.1.1.3.3.3.3.1.1.1.1.cmml" xref="S3.E3.m1.5.5.5.5.5.5">subscript</csymbol><ci id="S3.E3.m1.23.23.23.4.4.4.cmml" xref="S3.E3.m1.23.23.23.4.4.4">ğ‘</ci><ci id="S3.E3.m1.24.24.24.5.5.5.1.cmml" xref="S3.E3.m1.24.24.24.5.5.5.1">ğ‘˜</ci></apply></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.29c">\begin{split}\mathbf{J}_{k}=\textbf{Pooling}\big{(}&amp;\mathbf{MLP}_{vis}(v_{k}),\mathbf{MLP}_{pos}(p_{k}),\\
&amp;\mathbf{MLP}_{type}(c_{k})\big{)}.\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.SSS1.p3" class="ltx_para">
<p id="S3.SS2.SSS1.p3.6" class="ltx_p">The three <math id="S3.SS2.SSS1.p3.1.m1.1" class="ltx_Math" alttext="\mathbf{MLP}_{*}" display="inline"><semantics id="S3.SS2.SSS1.p3.1.m1.1a"><msub id="S3.SS2.SSS1.p3.1.m1.1.1" xref="S3.SS2.SSS1.p3.1.m1.1.1.cmml"><mi id="S3.SS2.SSS1.p3.1.m1.1.1.2" xref="S3.SS2.SSS1.p3.1.m1.1.1.2.cmml">ğŒğ‹ğ</mi><mo id="S3.SS2.SSS1.p3.1.m1.1.1.3" xref="S3.SS2.SSS1.p3.1.m1.1.1.3.cmml">âˆ—</mo></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.1.m1.1b"><apply id="S3.SS2.SSS1.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p3.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.2">ğŒğ‹ğ</ci><times id="S3.SS2.SSS1.p3.1.m1.1.1.3.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.1.m1.1c">\mathbf{MLP}_{*}</annotation></semantics></math> encoders above (<math id="S3.SS2.SSS1.p3.2.m2.1" class="ltx_Math" alttext="\mathbf{MLP}_{vis}" display="inline"><semantics id="S3.SS2.SSS1.p3.2.m2.1a"><msub id="S3.SS2.SSS1.p3.2.m2.1.1" xref="S3.SS2.SSS1.p3.2.m2.1.1.cmml"><mi id="S3.SS2.SSS1.p3.2.m2.1.1.2" xref="S3.SS2.SSS1.p3.2.m2.1.1.2.cmml">ğŒğ‹ğ</mi><mrow id="S3.SS2.SSS1.p3.2.m2.1.1.3" xref="S3.SS2.SSS1.p3.2.m2.1.1.3.cmml"><mi id="S3.SS2.SSS1.p3.2.m2.1.1.3.2" xref="S3.SS2.SSS1.p3.2.m2.1.1.3.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p3.2.m2.1.1.3.1" xref="S3.SS2.SSS1.p3.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS1.p3.2.m2.1.1.3.3" xref="S3.SS2.SSS1.p3.2.m2.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p3.2.m2.1.1.3.1a" xref="S3.SS2.SSS1.p3.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS1.p3.2.m2.1.1.3.4" xref="S3.SS2.SSS1.p3.2.m2.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.2.m2.1b"><apply id="S3.SS2.SSS1.p3.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p3.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.2">ğŒğ‹ğ</ci><apply id="S3.SS2.SSS1.p3.2.m2.1.1.3.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.3"><times id="S3.SS2.SSS1.p3.2.m2.1.1.3.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.3.1"></times><ci id="S3.SS2.SSS1.p3.2.m2.1.1.3.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.3.2">ğ‘£</ci><ci id="S3.SS2.SSS1.p3.2.m2.1.1.3.3.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.3.3">ğ‘–</ci><ci id="S3.SS2.SSS1.p3.2.m2.1.1.3.4.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.3.4">ğ‘ </ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.2.m2.1c">\mathbf{MLP}_{vis}</annotation></semantics></math>, <math id="S3.SS2.SSS1.p3.3.m3.1" class="ltx_Math" alttext="\mathbf{MLP}_{pos}" display="inline"><semantics id="S3.SS2.SSS1.p3.3.m3.1a"><msub id="S3.SS2.SSS1.p3.3.m3.1.1" xref="S3.SS2.SSS1.p3.3.m3.1.1.cmml"><mi id="S3.SS2.SSS1.p3.3.m3.1.1.2" xref="S3.SS2.SSS1.p3.3.m3.1.1.2.cmml">ğŒğ‹ğ</mi><mrow id="S3.SS2.SSS1.p3.3.m3.1.1.3" xref="S3.SS2.SSS1.p3.3.m3.1.1.3.cmml"><mi id="S3.SS2.SSS1.p3.3.m3.1.1.3.2" xref="S3.SS2.SSS1.p3.3.m3.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p3.3.m3.1.1.3.1" xref="S3.SS2.SSS1.p3.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS1.p3.3.m3.1.1.3.3" xref="S3.SS2.SSS1.p3.3.m3.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p3.3.m3.1.1.3.1a" xref="S3.SS2.SSS1.p3.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS1.p3.3.m3.1.1.3.4" xref="S3.SS2.SSS1.p3.3.m3.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.3.m3.1b"><apply id="S3.SS2.SSS1.p3.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.3.m3.1.1.1.cmml" xref="S3.SS2.SSS1.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p3.3.m3.1.1.2.cmml" xref="S3.SS2.SSS1.p3.3.m3.1.1.2">ğŒğ‹ğ</ci><apply id="S3.SS2.SSS1.p3.3.m3.1.1.3.cmml" xref="S3.SS2.SSS1.p3.3.m3.1.1.3"><times id="S3.SS2.SSS1.p3.3.m3.1.1.3.1.cmml" xref="S3.SS2.SSS1.p3.3.m3.1.1.3.1"></times><ci id="S3.SS2.SSS1.p3.3.m3.1.1.3.2.cmml" xref="S3.SS2.SSS1.p3.3.m3.1.1.3.2">ğ‘</ci><ci id="S3.SS2.SSS1.p3.3.m3.1.1.3.3.cmml" xref="S3.SS2.SSS1.p3.3.m3.1.1.3.3">ğ‘œ</ci><ci id="S3.SS2.SSS1.p3.3.m3.1.1.3.4.cmml" xref="S3.SS2.SSS1.p3.3.m3.1.1.3.4">ğ‘ </ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.3.m3.1c">\mathbf{MLP}_{pos}</annotation></semantics></math>, <math id="S3.SS2.SSS1.p3.4.m4.1" class="ltx_Math" alttext="\mathbf{MLP}_{type}" display="inline"><semantics id="S3.SS2.SSS1.p3.4.m4.1a"><msub id="S3.SS2.SSS1.p3.4.m4.1.1" xref="S3.SS2.SSS1.p3.4.m4.1.1.cmml"><mi id="S3.SS2.SSS1.p3.4.m4.1.1.2" xref="S3.SS2.SSS1.p3.4.m4.1.1.2.cmml">ğŒğ‹ğ</mi><mrow id="S3.SS2.SSS1.p3.4.m4.1.1.3" xref="S3.SS2.SSS1.p3.4.m4.1.1.3.cmml"><mi id="S3.SS2.SSS1.p3.4.m4.1.1.3.2" xref="S3.SS2.SSS1.p3.4.m4.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p3.4.m4.1.1.3.1" xref="S3.SS2.SSS1.p3.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS1.p3.4.m4.1.1.3.3" xref="S3.SS2.SSS1.p3.4.m4.1.1.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p3.4.m4.1.1.3.1a" xref="S3.SS2.SSS1.p3.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS1.p3.4.m4.1.1.3.4" xref="S3.SS2.SSS1.p3.4.m4.1.1.3.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p3.4.m4.1.1.3.1b" xref="S3.SS2.SSS1.p3.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS1.p3.4.m4.1.1.3.5" xref="S3.SS2.SSS1.p3.4.m4.1.1.3.5.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.4.m4.1b"><apply id="S3.SS2.SSS1.p3.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.4.m4.1.1.1.cmml" xref="S3.SS2.SSS1.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p3.4.m4.1.1.2.cmml" xref="S3.SS2.SSS1.p3.4.m4.1.1.2">ğŒğ‹ğ</ci><apply id="S3.SS2.SSS1.p3.4.m4.1.1.3.cmml" xref="S3.SS2.SSS1.p3.4.m4.1.1.3"><times id="S3.SS2.SSS1.p3.4.m4.1.1.3.1.cmml" xref="S3.SS2.SSS1.p3.4.m4.1.1.3.1"></times><ci id="S3.SS2.SSS1.p3.4.m4.1.1.3.2.cmml" xref="S3.SS2.SSS1.p3.4.m4.1.1.3.2">ğ‘¡</ci><ci id="S3.SS2.SSS1.p3.4.m4.1.1.3.3.cmml" xref="S3.SS2.SSS1.p3.4.m4.1.1.3.3">ğ‘¦</ci><ci id="S3.SS2.SSS1.p3.4.m4.1.1.3.4.cmml" xref="S3.SS2.SSS1.p3.4.m4.1.1.3.4">ğ‘</ci><ci id="S3.SS2.SSS1.p3.4.m4.1.1.3.5.cmml" xref="S3.SS2.SSS1.p3.4.m4.1.1.3.5">ğ‘’</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.4.m4.1c">\mathbf{MLP}_{type}</annotation></semantics></math>) for different cues do not share parameters.
When constructing <math id="S3.SS2.SSS1.p3.5.m5.1" class="ltx_Math" alttext="\mathbf{J}_{k}" display="inline"><semantics id="S3.SS2.SSS1.p3.5.m5.1a"><msub id="S3.SS2.SSS1.p3.5.m5.1.1" xref="S3.SS2.SSS1.p3.5.m5.1.1.cmml"><mi id="S3.SS2.SSS1.p3.5.m5.1.1.2" xref="S3.SS2.SSS1.p3.5.m5.1.1.2.cmml">ğ‰</mi><mi id="S3.SS2.SSS1.p3.5.m5.1.1.3" xref="S3.SS2.SSS1.p3.5.m5.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.5.m5.1b"><apply id="S3.SS2.SSS1.p3.5.m5.1.1.cmml" xref="S3.SS2.SSS1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.5.m5.1.1.1.cmml" xref="S3.SS2.SSS1.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p3.5.m5.1.1.2.cmml" xref="S3.SS2.SSS1.p3.5.m5.1.1.2">ğ‰</ci><ci id="S3.SS2.SSS1.p3.5.m5.1.1.3.cmml" xref="S3.SS2.SSS1.p3.5.m5.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.5.m5.1c">\mathbf{J}_{k}</annotation></semantics></math> for potential joints in the current frame
the <math id="S3.SS2.SSS1.p3.6.m6.1" class="ltx_Math" alttext="c_{k}" display="inline"><semantics id="S3.SS2.SSS1.p3.6.m6.1a"><msub id="S3.SS2.SSS1.p3.6.m6.1.1" xref="S3.SS2.SSS1.p3.6.m6.1.1.cmml"><mi id="S3.SS2.SSS1.p3.6.m6.1.1.2" xref="S3.SS2.SSS1.p3.6.m6.1.1.2.cmml">c</mi><mi id="S3.SS2.SSS1.p3.6.m6.1.1.3" xref="S3.SS2.SSS1.p3.6.m6.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.6.m6.1b"><apply id="S3.SS2.SSS1.p3.6.m6.1.1.cmml" xref="S3.SS2.SSS1.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p3.6.m6.1.1.1.cmml" xref="S3.SS2.SSS1.p3.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p3.6.m6.1.1.2.cmml" xref="S3.SS2.SSS1.p3.6.m6.1.1.2">ğ‘</ci><ci id="S3.SS2.SSS1.p3.6.m6.1.1.3.cmml" xref="S3.SS2.SSS1.p3.6.m6.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.6.m6.1c">c_{k}</annotation></semantics></math> part is ignored.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Edges in the proposed GNN model</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">The graph is constructed with two different types of edges:
the connections between joints within the same frame and
the connections across consecutive frames. Edges within the same frame
enable the GNN to capture relative movements and spatial structure of human joints
while the cross-frame edges model the temporal human pose dynamics.
We use two sets of GNN parameters
when aggregating features from these two types of edges.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Joint aggregation</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.8" class="ltx_p">In each layer of the GNN model, node features
are updated via massage passing, i.e.,</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.2" class="ltx_math_unparsed" alttext="\mathbf{J}_{k}^{l+1}={\mathbf{J}_{k}^{l}}+\mathbf{MLP}\Big{(}\big{[}{\mathbf{J}_{k}^{l}}||\mathbf{M}({\mathbf{J}_{k^{\prime},k^{\prime}\in\mathcal{N}_{\mathbf{J}_{k}^{l}}}^{l}}|\mathbf{J}_{k}^{l})\big{]}\Big{)}," display="block"><semantics id="S3.E4.m1.2a"><mrow id="S3.E4.m1.2b"><msubsup id="S3.E4.m1.2.3"><mi id="S3.E4.m1.2.3.2.2">ğ‰</mi><mi id="S3.E4.m1.2.3.2.3">k</mi><mrow id="S3.E4.m1.2.3.3"><mi id="S3.E4.m1.2.3.3.2">l</mi><mo id="S3.E4.m1.2.3.3.1">+</mo><mn id="S3.E4.m1.2.3.3.3">1</mn></mrow></msubsup><mo id="S3.E4.m1.2.4">=</mo><msubsup id="S3.E4.m1.2.5"><mi id="S3.E4.m1.2.5.2.2">ğ‰</mi><mi id="S3.E4.m1.2.5.2.3">k</mi><mi id="S3.E4.m1.2.5.3">l</mi></msubsup><mo id="S3.E4.m1.2.6">+</mo><mi id="S3.E4.m1.2.7">ğŒğ‹ğ</mi><mrow id="S3.E4.m1.2.8"><mo maxsize="160%" minsize="160%" id="S3.E4.m1.2.8.1">(</mo><mrow id="S3.E4.m1.2.8.2"><mo maxsize="120%" minsize="120%" id="S3.E4.m1.2.8.2.1">[</mo><msubsup id="S3.E4.m1.2.8.2.2"><mi id="S3.E4.m1.2.8.2.2.2.2">ğ‰</mi><mi id="S3.E4.m1.2.8.2.2.2.3">k</mi><mi id="S3.E4.m1.2.8.2.2.3">l</mi></msubsup><mo fence="false" rspace="0.167em" stretchy="false" id="S3.E4.m1.2.8.2.3">|</mo><mo fence="false" rspace="0.167em" stretchy="false" id="S3.E4.m1.2.8.2.4">|</mo><mi id="S3.E4.m1.2.8.2.5">ğŒ</mi><mrow id="S3.E4.m1.2.8.2.6"><mo stretchy="false" id="S3.E4.m1.2.8.2.6.1">(</mo><msubsup id="S3.E4.m1.2.8.2.6.2"><mi id="S3.E4.m1.2.8.2.6.2.2.2">ğ‰</mi><mrow id="S3.E4.m1.2.2.2"><mrow id="S3.E4.m1.2.2.2.2.2"><msup id="S3.E4.m1.1.1.1.1.1.1"><mi id="S3.E4.m1.1.1.1.1.1.1.2">k</mi><mo id="S3.E4.m1.1.1.1.1.1.1.3">â€²</mo></msup><mo id="S3.E4.m1.2.2.2.2.2.3">,</mo><msup id="S3.E4.m1.2.2.2.2.2.2"><mi id="S3.E4.m1.2.2.2.2.2.2.2">k</mi><mo id="S3.E4.m1.2.2.2.2.2.2.3">â€²</mo></msup></mrow><mo id="S3.E4.m1.2.2.2.3">âˆˆ</mo><msub id="S3.E4.m1.2.2.2.4"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.2.2.2.4.2">ğ’©</mi><msubsup id="S3.E4.m1.2.2.2.4.3"><mi id="S3.E4.m1.2.2.2.4.3.2.2">ğ‰</mi><mi id="S3.E4.m1.2.2.2.4.3.2.3">k</mi><mi id="S3.E4.m1.2.2.2.4.3.3">l</mi></msubsup></msub></mrow><mi id="S3.E4.m1.2.8.2.6.2.3">l</mi></msubsup><mo fence="false" rspace="0.167em" stretchy="false" id="S3.E4.m1.2.8.2.6.3">|</mo><msubsup id="S3.E4.m1.2.8.2.6.4"><mi id="S3.E4.m1.2.8.2.6.4.2.2">ğ‰</mi><mi id="S3.E4.m1.2.8.2.6.4.2.3">k</mi><mi id="S3.E4.m1.2.8.2.6.4.3">l</mi></msubsup><mo stretchy="false" id="S3.E4.m1.2.8.2.6.5">)</mo></mrow><mo maxsize="120%" minsize="120%" id="S3.E4.m1.2.8.2.7">]</mo></mrow><mo maxsize="160%" minsize="160%" id="S3.E4.m1.2.8.3">)</mo></mrow><mo id="S3.E4.m1.2.9">,</mo></mrow><annotation encoding="application/x-tex" id="S3.E4.m1.2c">\mathbf{J}_{k}^{l+1}={\mathbf{J}_{k}^{l}}+\mathbf{MLP}\Big{(}\big{[}{\mathbf{J}_{k}^{l}}||\mathbf{M}({\mathbf{J}_{k^{\prime},k^{\prime}\in\mathcal{N}_{\mathbf{J}_{k}^{l}}}^{l}}|\mathbf{J}_{k}^{l})\big{]}\Big{)},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS3.p1.7" class="ltx_p">where <math id="S3.SS2.SSS3.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{J}_{k}^{l}" display="inline"><semantics id="S3.SS2.SSS3.p1.1.m1.1a"><msubsup id="S3.SS2.SSS3.p1.1.m1.1.1" xref="S3.SS2.SSS3.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS3.p1.1.m1.1.1.2.2" xref="S3.SS2.SSS3.p1.1.m1.1.1.2.2.cmml">ğ‰</mi><mi id="S3.SS2.SSS3.p1.1.m1.1.1.2.3" xref="S3.SS2.SSS3.p1.1.m1.1.1.2.3.cmml">k</mi><mi id="S3.SS2.SSS3.p1.1.m1.1.1.3" xref="S3.SS2.SSS3.p1.1.m1.1.1.3.cmml">l</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.1.m1.1b"><apply id="S3.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1">superscript</csymbol><apply id="S3.SS2.SSS3.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.1.m1.1.1.2.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS3.p1.1.m1.1.1.2.2.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1.2.2">ğ‰</ci><ci id="S3.SS2.SSS3.p1.1.m1.1.1.2.3.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1.2.3">ğ‘˜</ci></apply><ci id="S3.SS2.SSS3.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.1.m1.1c">\mathbf{J}_{k}^{l}</annotation></semantics></math> represents the feature of the <math id="S3.SS2.SSS3.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS2.SSS3.p1.2.m2.1a"><mi id="S3.SS2.SSS3.p1.2.m2.1.1" xref="S3.SS2.SSS3.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.2.m2.1b"><ci id="S3.SS2.SSS3.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p1.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.2.m2.1c">k</annotation></semantics></math>-th joint at
the <math id="S3.SS2.SSS3.p1.3.m3.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S3.SS2.SSS3.p1.3.m3.1a"><mi id="S3.SS2.SSS3.p1.3.m3.1.1" xref="S3.SS2.SSS3.p1.3.m3.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.3.m3.1b"><ci id="S3.SS2.SSS3.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1">ğ‘™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.3.m3.1c">l</annotation></semantics></math>-th layer.
<math id="S3.SS2.SSS3.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{N}_{\mathbf{J}_{k}^{l}}" display="inline"><semantics id="S3.SS2.SSS3.p1.4.m4.1a"><msub id="S3.SS2.SSS3.p1.4.m4.1.1" xref="S3.SS2.SSS3.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS3.p1.4.m4.1.1.2" xref="S3.SS2.SSS3.p1.4.m4.1.1.2.cmml">ğ’©</mi><msubsup id="S3.SS2.SSS3.p1.4.m4.1.1.3" xref="S3.SS2.SSS3.p1.4.m4.1.1.3.cmml"><mi id="S3.SS2.SSS3.p1.4.m4.1.1.3.2.2" xref="S3.SS2.SSS3.p1.4.m4.1.1.3.2.2.cmml">ğ‰</mi><mi id="S3.SS2.SSS3.p1.4.m4.1.1.3.2.3" xref="S3.SS2.SSS3.p1.4.m4.1.1.3.2.3.cmml">k</mi><mi id="S3.SS2.SSS3.p1.4.m4.1.1.3.3" xref="S3.SS2.SSS3.p1.4.m4.1.1.3.3.cmml">l</mi></msubsup></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.4.m4.1b"><apply id="S3.SS2.SSS3.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS3.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.2">ğ’©</ci><apply id="S3.SS2.SSS3.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.3">superscript</csymbol><apply id="S3.SS2.SSS3.p1.4.m4.1.1.3.2.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.4.m4.1.1.3.2.1.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.3">subscript</csymbol><ci id="S3.SS2.SSS3.p1.4.m4.1.1.3.2.2.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.3.2.2">ğ‰</ci><ci id="S3.SS2.SSS3.p1.4.m4.1.1.3.2.3.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.3.2.3">ğ‘˜</ci></apply><ci id="S3.SS2.SSS3.p1.4.m4.1.1.3.3.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.3.3">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.4.m4.1c">\mathcal{N}_{\mathbf{J}_{k}^{l}}</annotation></semantics></math> represents
the set of neighbours of the <math id="S3.SS2.SSS3.p1.5.m5.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS2.SSS3.p1.5.m5.1a"><mi id="S3.SS2.SSS3.p1.5.m5.1.1" xref="S3.SS2.SSS3.p1.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.5.m5.1b"><ci id="S3.SS2.SSS3.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS3.p1.5.m5.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.5.m5.1c">k</annotation></semantics></math>-th joint,
<math id="S3.SS2.SSS3.p1.6.m6.1" class="ltx_Math" alttext="\mathbf{M}" display="inline"><semantics id="S3.SS2.SSS3.p1.6.m6.1a"><mi id="S3.SS2.SSS3.p1.6.m6.1.1" xref="S3.SS2.SSS3.p1.6.m6.1.1.cmml">ğŒ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.6.m6.1b"><ci id="S3.SS2.SSS3.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS3.p1.6.m6.1.1">ğŒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.6.m6.1c">\mathbf{M}</annotation></semantics></math> represents the message aggregating function
that takes all the neighbours as inputs and computes
the aggregated feature,
and <math id="S3.SS2.SSS3.p1.7.m7.1" class="ltx_math_unparsed" alttext="[\cdot||\cdot]" display="inline"><semantics id="S3.SS2.SSS3.p1.7.m7.1a"><mrow id="S3.SS2.SSS3.p1.7.m7.1b"><mo stretchy="false" id="S3.SS2.SSS3.p1.7.m7.1.1">[</mo><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p1.7.m7.1.2">â‹…</mo><mo fence="false" rspace="0.167em" stretchy="false" id="S3.SS2.SSS3.p1.7.m7.1.3">|</mo><mo fence="false" stretchy="false" id="S3.SS2.SSS3.p1.7.m7.1.4">|</mo><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p1.7.m7.1.5">â‹…</mo><mo stretchy="false" id="S3.SS2.SSS3.p1.7.m7.1.6">]</mo></mrow><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.7.m7.1c">[\cdot||\cdot]</annotation></semantics></math> represents the concatenation of vectors.</p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para">
<p id="S3.SS2.SSS3.p2.6" class="ltx_p">We use self-attentionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> mechanism
in function <math id="S3.SS2.SSS3.p2.1.m1.1" class="ltx_Math" alttext="\mathbf{M}" display="inline"><semantics id="S3.SS2.SSS3.p2.1.m1.1a"><mi id="S3.SS2.SSS3.p2.1.m1.1.1" xref="S3.SS2.SSS3.p2.1.m1.1.1.cmml">ğŒ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.1.m1.1b"><ci id="S3.SS2.SSS3.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p2.1.m1.1.1">ğŒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.1.m1.1c">\mathbf{M}</annotation></semantics></math> to compute the aggregated feature.
To aggregate the features from all the neighbours,
the query representation of <math id="S3.SS2.SSS3.p2.2.m2.1" class="ltx_Math" alttext="\mathbf{J_{k}}" display="inline"><semantics id="S3.SS2.SSS3.p2.2.m2.1a"><msub id="S3.SS2.SSS3.p2.2.m2.1.1" xref="S3.SS2.SSS3.p2.2.m2.1.1.cmml"><mi id="S3.SS2.SSS3.p2.2.m2.1.1.2" xref="S3.SS2.SSS3.p2.2.m2.1.1.2.cmml">ğ‰</mi><mi id="S3.SS2.SSS3.p2.2.m2.1.1.3" xref="S3.SS2.SSS3.p2.2.m2.1.1.3.cmml">ğ¤</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.2.m2.1b"><apply id="S3.SS2.SSS3.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p2.2.m2.1.1.1.cmml" xref="S3.SS2.SSS3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS3.p2.2.m2.1.1.2.cmml" xref="S3.SS2.SSS3.p2.2.m2.1.1.2">ğ‰</ci><ci id="S3.SS2.SSS3.p2.2.m2.1.1.3.cmml" xref="S3.SS2.SSS3.p2.2.m2.1.1.3">ğ¤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.2.m2.1c">\mathbf{J_{k}}</annotation></semantics></math> is computed as <math id="S3.SS2.SSS3.p2.3.m3.1" class="ltx_Math" alttext="\mathbf{J_{kq}}" display="inline"><semantics id="S3.SS2.SSS3.p2.3.m3.1a"><msub id="S3.SS2.SSS3.p2.3.m3.1.1" xref="S3.SS2.SSS3.p2.3.m3.1.1.cmml"><mi id="S3.SS2.SSS3.p2.3.m3.1.1.2" xref="S3.SS2.SSS3.p2.3.m3.1.1.2.cmml">ğ‰</mi><mi id="S3.SS2.SSS3.p2.3.m3.1.1.3" xref="S3.SS2.SSS3.p2.3.m3.1.1.3.cmml">ğ¤ğª</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.3.m3.1b"><apply id="S3.SS2.SSS3.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p2.3.m3.1.1.1.cmml" xref="S3.SS2.SSS3.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS3.p2.3.m3.1.1.2.cmml" xref="S3.SS2.SSS3.p2.3.m3.1.1.2">ğ‰</ci><ci id="S3.SS2.SSS3.p2.3.m3.1.1.3.cmml" xref="S3.SS2.SSS3.p2.3.m3.1.1.3">ğ¤ğª</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.3.m3.1c">\mathbf{J_{kq}}</annotation></semantics></math>
and then each joint <math id="S3.SS2.SSS3.p2.4.m4.1" class="ltx_Math" alttext="\mathbf{J_{k^{\prime}}}" display="inline"><semantics id="S3.SS2.SSS3.p2.4.m4.1a"><msub id="S3.SS2.SSS3.p2.4.m4.1.1" xref="S3.SS2.SSS3.p2.4.m4.1.1.cmml"><mi id="S3.SS2.SSS3.p2.4.m4.1.1.2" xref="S3.SS2.SSS3.p2.4.m4.1.1.2.cmml">ğ‰</mi><msup id="S3.SS2.SSS3.p2.4.m4.1.1.3" xref="S3.SS2.SSS3.p2.4.m4.1.1.3.cmml"><mi id="S3.SS2.SSS3.p2.4.m4.1.1.3.2" xref="S3.SS2.SSS3.p2.4.m4.1.1.3.2.cmml">ğ¤</mi><mo id="S3.SS2.SSS3.p2.4.m4.1.1.3.3" xref="S3.SS2.SSS3.p2.4.m4.1.1.3.3.cmml">â€²</mo></msup></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.4.m4.1b"><apply id="S3.SS2.SSS3.p2.4.m4.1.1.cmml" xref="S3.SS2.SSS3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p2.4.m4.1.1.1.cmml" xref="S3.SS2.SSS3.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS3.p2.4.m4.1.1.2.cmml" xref="S3.SS2.SSS3.p2.4.m4.1.1.2">ğ‰</ci><apply id="S3.SS2.SSS3.p2.4.m4.1.1.3.cmml" xref="S3.SS2.SSS3.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p2.4.m4.1.1.3.1.cmml" xref="S3.SS2.SSS3.p2.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS3.p2.4.m4.1.1.3.2.cmml" xref="S3.SS2.SSS3.p2.4.m4.1.1.3.2">ğ¤</ci><ci id="S3.SS2.SSS3.p2.4.m4.1.1.3.3.cmml" xref="S3.SS2.SSS3.p2.4.m4.1.1.3.3">â€²</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.4.m4.1c">\mathbf{J_{k^{\prime}}}</annotation></semantics></math> is first transformed to two different
representations include value <math id="S3.SS2.SSS3.p2.5.m5.1" class="ltx_Math" alttext="\mathbf{J_{k^{\prime}v}}" display="inline"><semantics id="S3.SS2.SSS3.p2.5.m5.1a"><msub id="S3.SS2.SSS3.p2.5.m5.1.1" xref="S3.SS2.SSS3.p2.5.m5.1.1.cmml"><mi id="S3.SS2.SSS3.p2.5.m5.1.1.2" xref="S3.SS2.SSS3.p2.5.m5.1.1.2.cmml">ğ‰</mi><mrow id="S3.SS2.SSS3.p2.5.m5.1.1.3" xref="S3.SS2.SSS3.p2.5.m5.1.1.3.cmml"><msup id="S3.SS2.SSS3.p2.5.m5.1.1.3.2" xref="S3.SS2.SSS3.p2.5.m5.1.1.3.2.cmml"><mi id="S3.SS2.SSS3.p2.5.m5.1.1.3.2.2" xref="S3.SS2.SSS3.p2.5.m5.1.1.3.2.2.cmml">ğ¤</mi><mo id="S3.SS2.SSS3.p2.5.m5.1.1.3.2.3" xref="S3.SS2.SSS3.p2.5.m5.1.1.3.2.3.cmml">â€²</mo></msup><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p2.5.m5.1.1.3.1" xref="S3.SS2.SSS3.p2.5.m5.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS3.p2.5.m5.1.1.3.3" xref="S3.SS2.SSS3.p2.5.m5.1.1.3.3.cmml">ğ¯</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.5.m5.1b"><apply id="S3.SS2.SSS3.p2.5.m5.1.1.cmml" xref="S3.SS2.SSS3.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p2.5.m5.1.1.1.cmml" xref="S3.SS2.SSS3.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS3.p2.5.m5.1.1.2.cmml" xref="S3.SS2.SSS3.p2.5.m5.1.1.2">ğ‰</ci><apply id="S3.SS2.SSS3.p2.5.m5.1.1.3.cmml" xref="S3.SS2.SSS3.p2.5.m5.1.1.3"><times id="S3.SS2.SSS3.p2.5.m5.1.1.3.1.cmml" xref="S3.SS2.SSS3.p2.5.m5.1.1.3.1"></times><apply id="S3.SS2.SSS3.p2.5.m5.1.1.3.2.cmml" xref="S3.SS2.SSS3.p2.5.m5.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p2.5.m5.1.1.3.2.1.cmml" xref="S3.SS2.SSS3.p2.5.m5.1.1.3.2">superscript</csymbol><ci id="S3.SS2.SSS3.p2.5.m5.1.1.3.2.2.cmml" xref="S3.SS2.SSS3.p2.5.m5.1.1.3.2.2">ğ¤</ci><ci id="S3.SS2.SSS3.p2.5.m5.1.1.3.2.3.cmml" xref="S3.SS2.SSS3.p2.5.m5.1.1.3.2.3">â€²</ci></apply><ci id="S3.SS2.SSS3.p2.5.m5.1.1.3.3.cmml" xref="S3.SS2.SSS3.p2.5.m5.1.1.3.3">ğ¯</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.5.m5.1c">\mathbf{J_{k^{\prime}v}}</annotation></semantics></math> and key <math id="S3.SS2.SSS3.p2.6.m6.1" class="ltx_Math" alttext="\mathbf{J_{k^{\prime}k}}" display="inline"><semantics id="S3.SS2.SSS3.p2.6.m6.1a"><msub id="S3.SS2.SSS3.p2.6.m6.1.1" xref="S3.SS2.SSS3.p2.6.m6.1.1.cmml"><mi id="S3.SS2.SSS3.p2.6.m6.1.1.2" xref="S3.SS2.SSS3.p2.6.m6.1.1.2.cmml">ğ‰</mi><mrow id="S3.SS2.SSS3.p2.6.m6.1.1.3" xref="S3.SS2.SSS3.p2.6.m6.1.1.3.cmml"><msup id="S3.SS2.SSS3.p2.6.m6.1.1.3.2" xref="S3.SS2.SSS3.p2.6.m6.1.1.3.2.cmml"><mi id="S3.SS2.SSS3.p2.6.m6.1.1.3.2.2" xref="S3.SS2.SSS3.p2.6.m6.1.1.3.2.2.cmml">ğ¤</mi><mo id="S3.SS2.SSS3.p2.6.m6.1.1.3.2.3" xref="S3.SS2.SSS3.p2.6.m6.1.1.3.2.3.cmml">â€²</mo></msup><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p2.6.m6.1.1.3.1" xref="S3.SS2.SSS3.p2.6.m6.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS3.p2.6.m6.1.1.3.3" xref="S3.SS2.SSS3.p2.6.m6.1.1.3.3.cmml">ğ¤</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.6.m6.1b"><apply id="S3.SS2.SSS3.p2.6.m6.1.1.cmml" xref="S3.SS2.SSS3.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p2.6.m6.1.1.1.cmml" xref="S3.SS2.SSS3.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.SSS3.p2.6.m6.1.1.2.cmml" xref="S3.SS2.SSS3.p2.6.m6.1.1.2">ğ‰</ci><apply id="S3.SS2.SSS3.p2.6.m6.1.1.3.cmml" xref="S3.SS2.SSS3.p2.6.m6.1.1.3"><times id="S3.SS2.SSS3.p2.6.m6.1.1.3.1.cmml" xref="S3.SS2.SSS3.p2.6.m6.1.1.3.1"></times><apply id="S3.SS2.SSS3.p2.6.m6.1.1.3.2.cmml" xref="S3.SS2.SSS3.p2.6.m6.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p2.6.m6.1.1.3.2.1.cmml" xref="S3.SS2.SSS3.p2.6.m6.1.1.3.2">superscript</csymbol><ci id="S3.SS2.SSS3.p2.6.m6.1.1.3.2.2.cmml" xref="S3.SS2.SSS3.p2.6.m6.1.1.3.2.2">ğ¤</ci><ci id="S3.SS2.SSS3.p2.6.m6.1.1.3.2.3.cmml" xref="S3.SS2.SSS3.p2.6.m6.1.1.3.2.3">â€²</ci></apply><ci id="S3.SS2.SSS3.p2.6.m6.1.1.3.3.cmml" xref="S3.SS2.SSS3.p2.6.m6.1.1.3.3">ğ¤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.6.m6.1c">\mathbf{J_{k^{\prime}k}}</annotation></semantics></math>.
The final aggregated feature can be computed as the
weighted average of all the values of the neighbours:</p>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.33" class="ltx_Math" alttext="\begin{split}\mathbf{M}({\mathbf{J}_{k^{\prime},k^{\prime}\in\mathcal{N}_{\mathbf{J}_{k}}}}|\mathbf{J}_{k})=\sum_{k^{\prime}\in\mathcal{N}_{\mathbf{J}_{k}}}\alpha_{kk^{\prime}}\mathbf{J}_{k^{\prime}v},\\
\text{where }\alpha_{kk^{\prime}}=\mathbf{Softmax}_{k^{\prime}}(\mathbf{J}_{kq}^{\top}\mathbf{J}_{k^{\prime}k}).\end{split}" display="block"><semantics id="S3.E5.m1.33a"><mtable displaystyle="true" rowspacing="0pt" id="S3.E5.m1.33.33.3"><mtr id="S3.E5.m1.33.33.3a"><mtd class="ltx_align_right" columnalign="right" id="S3.E5.m1.33.33.3b"><mrow id="S3.E5.m1.32.32.2.31.17.17.17"><mrow id="S3.E5.m1.32.32.2.31.17.17.17.1"><mrow id="S3.E5.m1.32.32.2.31.17.17.17.1.1"><mi id="S3.E5.m1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.cmml">ğŒ</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.32.32.2.31.17.17.17.1.1.2">â€‹</mo><mrow id="S3.E5.m1.32.32.2.31.17.17.17.1.1.1.1"><mo stretchy="false" id="S3.E5.m1.2.2.2.2.2.2">(</mo><mrow id="S3.E5.m1.32.32.2.31.17.17.17.1.1.1.1.1"><msub id="S3.E5.m1.32.32.2.31.17.17.17.1.1.1.1.1.1"><mi id="S3.E5.m1.3.3.3.3.3.3" xref="S3.E5.m1.3.3.3.3.3.3.cmml">ğ‰</mi><mrow id="S3.E5.m1.4.4.4.4.4.4.1" xref="S3.E5.m1.4.4.4.4.4.4.1.cmml"><mrow id="S3.E5.m1.4.4.4.4.4.4.1.2.2" xref="S3.E5.m1.4.4.4.4.4.4.1.2.3.cmml"><msup id="S3.E5.m1.4.4.4.4.4.4.1.1.1.1" xref="S3.E5.m1.4.4.4.4.4.4.1.1.1.1.cmml"><mi id="S3.E5.m1.4.4.4.4.4.4.1.1.1.1.2" xref="S3.E5.m1.4.4.4.4.4.4.1.1.1.1.2.cmml">k</mi><mo id="S3.E5.m1.4.4.4.4.4.4.1.1.1.1.3" xref="S3.E5.m1.4.4.4.4.4.4.1.1.1.1.3.cmml">â€²</mo></msup><mo id="S3.E5.m1.4.4.4.4.4.4.1.2.2.3" xref="S3.E5.m1.4.4.4.4.4.4.1.2.3.cmml">,</mo><msup id="S3.E5.m1.4.4.4.4.4.4.1.2.2.2" xref="S3.E5.m1.4.4.4.4.4.4.1.2.2.2.cmml"><mi id="S3.E5.m1.4.4.4.4.4.4.1.2.2.2.2" xref="S3.E5.m1.4.4.4.4.4.4.1.2.2.2.2.cmml">k</mi><mo id="S3.E5.m1.4.4.4.4.4.4.1.2.2.2.3" xref="S3.E5.m1.4.4.4.4.4.4.1.2.2.2.3.cmml">â€²</mo></msup></mrow><mo id="S3.E5.m1.4.4.4.4.4.4.1.3" xref="S3.E5.m1.4.4.4.4.4.4.1.3.cmml">âˆˆ</mo><msub id="S3.E5.m1.4.4.4.4.4.4.1.4" xref="S3.E5.m1.4.4.4.4.4.4.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m1.4.4.4.4.4.4.1.4.2" xref="S3.E5.m1.4.4.4.4.4.4.1.4.2.cmml">ğ’©</mi><msub id="S3.E5.m1.4.4.4.4.4.4.1.4.3" xref="S3.E5.m1.4.4.4.4.4.4.1.4.3.cmml"><mi id="S3.E5.m1.4.4.4.4.4.4.1.4.3.2" xref="S3.E5.m1.4.4.4.4.4.4.1.4.3.2.cmml">ğ‰</mi><mi id="S3.E5.m1.4.4.4.4.4.4.1.4.3.3" xref="S3.E5.m1.4.4.4.4.4.4.1.4.3.3.cmml">k</mi></msub></msub></mrow></msub><mo fence="false" id="S3.E5.m1.5.5.5.5.5.5" xref="S3.E5.m1.5.5.5.5.5.5.cmml">|</mo><msub id="S3.E5.m1.32.32.2.31.17.17.17.1.1.1.1.1.2"><mi id="S3.E5.m1.6.6.6.6.6.6" xref="S3.E5.m1.6.6.6.6.6.6.cmml">ğ‰</mi><mi id="S3.E5.m1.7.7.7.7.7.7.1" xref="S3.E5.m1.7.7.7.7.7.7.1.cmml">k</mi></msub></mrow><mo stretchy="false" id="S3.E5.m1.8.8.8.8.8.8">)</mo></mrow></mrow><mo rspace="0.111em" id="S3.E5.m1.9.9.9.9.9.9" xref="S3.E5.m1.9.9.9.9.9.9.cmml">=</mo><mrow id="S3.E5.m1.32.32.2.31.17.17.17.1.2"><munder id="S3.E5.m1.32.32.2.31.17.17.17.1.2.1"><mo movablelimits="false" id="S3.E5.m1.10.10.10.10.10.10" xref="S3.E5.m1.10.10.10.10.10.10.cmml">âˆ‘</mo><mrow id="S3.E5.m1.11.11.11.11.11.11.1" xref="S3.E5.m1.11.11.11.11.11.11.1.cmml"><msup id="S3.E5.m1.11.11.11.11.11.11.1.2" xref="S3.E5.m1.11.11.11.11.11.11.1.2.cmml"><mi id="S3.E5.m1.11.11.11.11.11.11.1.2.2" xref="S3.E5.m1.11.11.11.11.11.11.1.2.2.cmml">k</mi><mo id="S3.E5.m1.11.11.11.11.11.11.1.2.3" xref="S3.E5.m1.11.11.11.11.11.11.1.2.3.cmml">â€²</mo></msup><mo id="S3.E5.m1.11.11.11.11.11.11.1.1" xref="S3.E5.m1.11.11.11.11.11.11.1.1.cmml">âˆˆ</mo><msub id="S3.E5.m1.11.11.11.11.11.11.1.3" xref="S3.E5.m1.11.11.11.11.11.11.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m1.11.11.11.11.11.11.1.3.2" xref="S3.E5.m1.11.11.11.11.11.11.1.3.2.cmml">ğ’©</mi><msub id="S3.E5.m1.11.11.11.11.11.11.1.3.3" xref="S3.E5.m1.11.11.11.11.11.11.1.3.3.cmml"><mi id="S3.E5.m1.11.11.11.11.11.11.1.3.3.2" xref="S3.E5.m1.11.11.11.11.11.11.1.3.3.2.cmml">ğ‰</mi><mi id="S3.E5.m1.11.11.11.11.11.11.1.3.3.3" xref="S3.E5.m1.11.11.11.11.11.11.1.3.3.3.cmml">k</mi></msub></msub></mrow></munder><mrow id="S3.E5.m1.32.32.2.31.17.17.17.1.2.2"><msub id="S3.E5.m1.32.32.2.31.17.17.17.1.2.2.2"><mi id="S3.E5.m1.12.12.12.12.12.12" xref="S3.E5.m1.12.12.12.12.12.12.cmml">Î±</mi><mrow id="S3.E5.m1.13.13.13.13.13.13.1" xref="S3.E5.m1.13.13.13.13.13.13.1.cmml"><mi id="S3.E5.m1.13.13.13.13.13.13.1.2" xref="S3.E5.m1.13.13.13.13.13.13.1.2.cmml">k</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.13.13.13.13.13.13.1.1" xref="S3.E5.m1.13.13.13.13.13.13.1.1.cmml">â€‹</mo><msup id="S3.E5.m1.13.13.13.13.13.13.1.3" xref="S3.E5.m1.13.13.13.13.13.13.1.3.cmml"><mi id="S3.E5.m1.13.13.13.13.13.13.1.3.2" xref="S3.E5.m1.13.13.13.13.13.13.1.3.2.cmml">k</mi><mo id="S3.E5.m1.13.13.13.13.13.13.1.3.3" xref="S3.E5.m1.13.13.13.13.13.13.1.3.3.cmml">â€²</mo></msup></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E5.m1.32.32.2.31.17.17.17.1.2.2.1">â€‹</mo><msub id="S3.E5.m1.32.32.2.31.17.17.17.1.2.2.3"><mi id="S3.E5.m1.14.14.14.14.14.14" xref="S3.E5.m1.14.14.14.14.14.14.cmml">ğ‰</mi><mrow id="S3.E5.m1.15.15.15.15.15.15.1" xref="S3.E5.m1.15.15.15.15.15.15.1.cmml"><msup id="S3.E5.m1.15.15.15.15.15.15.1.2" xref="S3.E5.m1.15.15.15.15.15.15.1.2.cmml"><mi id="S3.E5.m1.15.15.15.15.15.15.1.2.2" xref="S3.E5.m1.15.15.15.15.15.15.1.2.2.cmml">k</mi><mo id="S3.E5.m1.15.15.15.15.15.15.1.2.3" xref="S3.E5.m1.15.15.15.15.15.15.1.2.3.cmml">â€²</mo></msup><mo lspace="0em" rspace="0em" id="S3.E5.m1.15.15.15.15.15.15.1.1" xref="S3.E5.m1.15.15.15.15.15.15.1.1.cmml">â€‹</mo><mi id="S3.E5.m1.15.15.15.15.15.15.1.3" xref="S3.E5.m1.15.15.15.15.15.15.1.3.cmml">v</mi></mrow></msub></mrow></mrow></mrow><mo id="S3.E5.m1.16.16.16.16.16.16">,</mo></mrow></mtd></mtr><mtr id="S3.E5.m1.33.33.3c"><mtd class="ltx_align_right" columnalign="right" id="S3.E5.m1.33.33.3d"><mrow id="S3.E5.m1.33.33.3.32.15.15.15"><mrow id="S3.E5.m1.33.33.3.32.15.15.15.1"><mrow id="S3.E5.m1.33.33.3.32.15.15.15.1.2"><mtext id="S3.E5.m1.17.17.17.1.1.1" xref="S3.E5.m1.17.17.17.1.1.1a.cmml">whereÂ </mtext><mo lspace="0em" rspace="0em" id="S3.E5.m1.33.33.3.32.15.15.15.1.2.1">â€‹</mo><msub id="S3.E5.m1.33.33.3.32.15.15.15.1.2.2"><mi id="S3.E5.m1.18.18.18.2.2.2" xref="S3.E5.m1.18.18.18.2.2.2.cmml">Î±</mi><mrow id="S3.E5.m1.19.19.19.3.3.3.1" xref="S3.E5.m1.19.19.19.3.3.3.1.cmml"><mi id="S3.E5.m1.19.19.19.3.3.3.1.2" xref="S3.E5.m1.19.19.19.3.3.3.1.2.cmml">k</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.19.19.19.3.3.3.1.1" xref="S3.E5.m1.19.19.19.3.3.3.1.1.cmml">â€‹</mo><msup id="S3.E5.m1.19.19.19.3.3.3.1.3" xref="S3.E5.m1.19.19.19.3.3.3.1.3.cmml"><mi id="S3.E5.m1.19.19.19.3.3.3.1.3.2" xref="S3.E5.m1.19.19.19.3.3.3.1.3.2.cmml">k</mi><mo id="S3.E5.m1.19.19.19.3.3.3.1.3.3" xref="S3.E5.m1.19.19.19.3.3.3.1.3.3.cmml">â€²</mo></msup></mrow></msub></mrow><mo id="S3.E5.m1.20.20.20.4.4.4" xref="S3.E5.m1.20.20.20.4.4.4.cmml">=</mo><mrow id="S3.E5.m1.33.33.3.32.15.15.15.1.1"><msub id="S3.E5.m1.33.33.3.32.15.15.15.1.1.3"><mi id="S3.E5.m1.21.21.21.5.5.5" xref="S3.E5.m1.21.21.21.5.5.5.cmml">ğ’ğ¨ğŸğ­ğ¦ğšğ±</mi><msup id="S3.E5.m1.22.22.22.6.6.6.1" xref="S3.E5.m1.22.22.22.6.6.6.1.cmml"><mi id="S3.E5.m1.22.22.22.6.6.6.1.2" xref="S3.E5.m1.22.22.22.6.6.6.1.2.cmml">k</mi><mo id="S3.E5.m1.22.22.22.6.6.6.1.3" xref="S3.E5.m1.22.22.22.6.6.6.1.3.cmml">â€²</mo></msup></msub><mo lspace="0em" rspace="0em" id="S3.E5.m1.33.33.3.32.15.15.15.1.1.2">â€‹</mo><mrow id="S3.E5.m1.33.33.3.32.15.15.15.1.1.1.1"><mo stretchy="false" id="S3.E5.m1.23.23.23.7.7.7">(</mo><mrow id="S3.E5.m1.33.33.3.32.15.15.15.1.1.1.1.1"><msubsup id="S3.E5.m1.33.33.3.32.15.15.15.1.1.1.1.1.2"><mi id="S3.E5.m1.24.24.24.8.8.8" xref="S3.E5.m1.24.24.24.8.8.8.cmml">ğ‰</mi><mrow id="S3.E5.m1.25.25.25.9.9.9.1" xref="S3.E5.m1.25.25.25.9.9.9.1.cmml"><mi id="S3.E5.m1.25.25.25.9.9.9.1.2" xref="S3.E5.m1.25.25.25.9.9.9.1.2.cmml">k</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.25.25.25.9.9.9.1.1" xref="S3.E5.m1.25.25.25.9.9.9.1.1.cmml">â€‹</mo><mi id="S3.E5.m1.25.25.25.9.9.9.1.3" xref="S3.E5.m1.25.25.25.9.9.9.1.3.cmml">q</mi></mrow><mo id="S3.E5.m1.26.26.26.10.10.10.1" xref="S3.E5.m1.26.26.26.10.10.10.1.cmml">âŠ¤</mo></msubsup><mo lspace="0em" rspace="0em" id="S3.E5.m1.33.33.3.32.15.15.15.1.1.1.1.1.1">â€‹</mo><msub id="S3.E5.m1.33.33.3.32.15.15.15.1.1.1.1.1.3"><mi id="S3.E5.m1.27.27.27.11.11.11" xref="S3.E5.m1.27.27.27.11.11.11.cmml">ğ‰</mi><mrow id="S3.E5.m1.28.28.28.12.12.12.1" xref="S3.E5.m1.28.28.28.12.12.12.1.cmml"><msup id="S3.E5.m1.28.28.28.12.12.12.1.2" xref="S3.E5.m1.28.28.28.12.12.12.1.2.cmml"><mi id="S3.E5.m1.28.28.28.12.12.12.1.2.2" xref="S3.E5.m1.28.28.28.12.12.12.1.2.2.cmml">k</mi><mo id="S3.E5.m1.28.28.28.12.12.12.1.2.3" xref="S3.E5.m1.28.28.28.12.12.12.1.2.3.cmml">â€²</mo></msup><mo lspace="0em" rspace="0em" id="S3.E5.m1.28.28.28.12.12.12.1.1" xref="S3.E5.m1.28.28.28.12.12.12.1.1.cmml">â€‹</mo><mi id="S3.E5.m1.28.28.28.12.12.12.1.3" xref="S3.E5.m1.28.28.28.12.12.12.1.3.cmml">k</mi></mrow></msub></mrow><mo stretchy="false" id="S3.E5.m1.29.29.29.13.13.13">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S3.E5.m1.30.30.30.14.14.14">.</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E5.m1.33b"><apply id="S3.E5.m1.31.31.1.1.1.3.cmml"><csymbol cd="ambiguous" id="S3.E5.m1.31.31.1.1.1.3a.cmml">formulae-sequence</csymbol><apply id="S3.E5.m1.31.31.1.1.1.1.1.cmml"><eq id="S3.E5.m1.9.9.9.9.9.9.cmml" xref="S3.E5.m1.9.9.9.9.9.9"></eq><apply id="S3.E5.m1.31.31.1.1.1.1.1.1.cmml"><times id="S3.E5.m1.31.31.1.1.1.1.1.1.2.cmml"></times><ci id="S3.E5.m1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1">ğŒ</ci><apply id="S3.E5.m1.31.31.1.1.1.1.1.1.1.1.1.cmml"><csymbol cd="latexml" id="S3.E5.m1.5.5.5.5.5.5.cmml" xref="S3.E5.m1.5.5.5.5.5.5">conditional</csymbol><apply id="S3.E5.m1.31.31.1.1.1.1.1.1.1.1.1.2.cmml"><csymbol cd="ambiguous" id="S3.E5.m1.31.31.1.1.1.1.1.1.1.1.1.2.1.cmml">subscript</csymbol><ci id="S3.E5.m1.3.3.3.3.3.3.cmml" xref="S3.E5.m1.3.3.3.3.3.3">ğ‰</ci><apply id="S3.E5.m1.4.4.4.4.4.4.1.cmml" xref="S3.E5.m1.4.4.4.4.4.4.1"><in id="S3.E5.m1.4.4.4.4.4.4.1.3.cmml" xref="S3.E5.m1.4.4.4.4.4.4.1.3"></in><list id="S3.E5.m1.4.4.4.4.4.4.1.2.3.cmml" xref="S3.E5.m1.4.4.4.4.4.4.1.2.2"><apply id="S3.E5.m1.4.4.4.4.4.4.1.1.1.1.cmml" xref="S3.E5.m1.4.4.4.4.4.4.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.4.4.4.4.4.4.1.1.1.1.1.cmml" xref="S3.E5.m1.4.4.4.4.4.4.1.1.1.1">superscript</csymbol><ci id="S3.E5.m1.4.4.4.4.4.4.1.1.1.1.2.cmml" xref="S3.E5.m1.4.4.4.4.4.4.1.1.1.1.2">ğ‘˜</ci><ci id="S3.E5.m1.4.4.4.4.4.4.1.1.1.1.3.cmml" xref="S3.E5.m1.4.4.4.4.4.4.1.1.1.1.3">â€²</ci></apply><apply id="S3.E5.m1.4.4.4.4.4.4.1.2.2.2.cmml" xref="S3.E5.m1.4.4.4.4.4.4.1.2.2.2"><csymbol cd="ambiguous" id="S3.E5.m1.4.4.4.4.4.4.1.2.2.2.1.cmml" xref="S3.E5.m1.4.4.4.4.4.4.1.2.2.2">superscript</csymbol><ci id="S3.E5.m1.4.4.4.4.4.4.1.2.2.2.2.cmml" xref="S3.E5.m1.4.4.4.4.4.4.1.2.2.2.2">ğ‘˜</ci><ci id="S3.E5.m1.4.4.4.4.4.4.1.2.2.2.3.cmml" xref="S3.E5.m1.4.4.4.4.4.4.1.2.2.2.3">â€²</ci></apply></list><apply id="S3.E5.m1.4.4.4.4.4.4.1.4.cmml" xref="S3.E5.m1.4.4.4.4.4.4.1.4"><csymbol cd="ambiguous" id="S3.E5.m1.4.4.4.4.4.4.1.4.1.cmml" xref="S3.E5.m1.4.4.4.4.4.4.1.4">subscript</csymbol><ci id="S3.E5.m1.4.4.4.4.4.4.1.4.2.cmml" xref="S3.E5.m1.4.4.4.4.4.4.1.4.2">ğ’©</ci><apply id="S3.E5.m1.4.4.4.4.4.4.1.4.3.cmml" xref="S3.E5.m1.4.4.4.4.4.4.1.4.3"><csymbol cd="ambiguous" id="S3.E5.m1.4.4.4.4.4.4.1.4.3.1.cmml" xref="S3.E5.m1.4.4.4.4.4.4.1.4.3">subscript</csymbol><ci id="S3.E5.m1.4.4.4.4.4.4.1.4.3.2.cmml" xref="S3.E5.m1.4.4.4.4.4.4.1.4.3.2">ğ‰</ci><ci id="S3.E5.m1.4.4.4.4.4.4.1.4.3.3.cmml" xref="S3.E5.m1.4.4.4.4.4.4.1.4.3.3">ğ‘˜</ci></apply></apply></apply></apply><apply id="S3.E5.m1.31.31.1.1.1.1.1.1.1.1.1.3.cmml"><csymbol cd="ambiguous" id="S3.E5.m1.31.31.1.1.1.1.1.1.1.1.1.3.1.cmml">subscript</csymbol><ci id="S3.E5.m1.6.6.6.6.6.6.cmml" xref="S3.E5.m1.6.6.6.6.6.6">ğ‰</ci><ci id="S3.E5.m1.7.7.7.7.7.7.1.cmml" xref="S3.E5.m1.7.7.7.7.7.7.1">ğ‘˜</ci></apply></apply></apply><apply id="S3.E5.m1.31.31.1.1.1.1.1.3.cmml"><apply id="S3.E5.m1.31.31.1.1.1.1.1.3.1.cmml"><csymbol cd="ambiguous" id="S3.E5.m1.31.31.1.1.1.1.1.3.1.1.cmml">subscript</csymbol><sum id="S3.E5.m1.10.10.10.10.10.10.cmml" xref="S3.E5.m1.10.10.10.10.10.10"></sum><apply id="S3.E5.m1.11.11.11.11.11.11.1.cmml" xref="S3.E5.m1.11.11.11.11.11.11.1"><in id="S3.E5.m1.11.11.11.11.11.11.1.1.cmml" xref="S3.E5.m1.11.11.11.11.11.11.1.1"></in><apply id="S3.E5.m1.11.11.11.11.11.11.1.2.cmml" xref="S3.E5.m1.11.11.11.11.11.11.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.11.11.11.11.11.11.1.2.1.cmml" xref="S3.E5.m1.11.11.11.11.11.11.1.2">superscript</csymbol><ci id="S3.E5.m1.11.11.11.11.11.11.1.2.2.cmml" xref="S3.E5.m1.11.11.11.11.11.11.1.2.2">ğ‘˜</ci><ci id="S3.E5.m1.11.11.11.11.11.11.1.2.3.cmml" xref="S3.E5.m1.11.11.11.11.11.11.1.2.3">â€²</ci></apply><apply id="S3.E5.m1.11.11.11.11.11.11.1.3.cmml" xref="S3.E5.m1.11.11.11.11.11.11.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.11.11.11.11.11.11.1.3.1.cmml" xref="S3.E5.m1.11.11.11.11.11.11.1.3">subscript</csymbol><ci id="S3.E5.m1.11.11.11.11.11.11.1.3.2.cmml" xref="S3.E5.m1.11.11.11.11.11.11.1.3.2">ğ’©</ci><apply id="S3.E5.m1.11.11.11.11.11.11.1.3.3.cmml" xref="S3.E5.m1.11.11.11.11.11.11.1.3.3"><csymbol cd="ambiguous" id="S3.E5.m1.11.11.11.11.11.11.1.3.3.1.cmml" xref="S3.E5.m1.11.11.11.11.11.11.1.3.3">subscript</csymbol><ci id="S3.E5.m1.11.11.11.11.11.11.1.3.3.2.cmml" xref="S3.E5.m1.11.11.11.11.11.11.1.3.3.2">ğ‰</ci><ci id="S3.E5.m1.11.11.11.11.11.11.1.3.3.3.cmml" xref="S3.E5.m1.11.11.11.11.11.11.1.3.3.3">ğ‘˜</ci></apply></apply></apply></apply><apply id="S3.E5.m1.31.31.1.1.1.1.1.3.2.cmml"><times id="S3.E5.m1.31.31.1.1.1.1.1.3.2.1.cmml"></times><apply id="S3.E5.m1.31.31.1.1.1.1.1.3.2.2.cmml"><csymbol cd="ambiguous" id="S3.E5.m1.31.31.1.1.1.1.1.3.2.2.1.cmml">subscript</csymbol><ci id="S3.E5.m1.12.12.12.12.12.12.cmml" xref="S3.E5.m1.12.12.12.12.12.12">ğ›¼</ci><apply id="S3.E5.m1.13.13.13.13.13.13.1.cmml" xref="S3.E5.m1.13.13.13.13.13.13.1"><times id="S3.E5.m1.13.13.13.13.13.13.1.1.cmml" xref="S3.E5.m1.13.13.13.13.13.13.1.1"></times><ci id="S3.E5.m1.13.13.13.13.13.13.1.2.cmml" xref="S3.E5.m1.13.13.13.13.13.13.1.2">ğ‘˜</ci><apply id="S3.E5.m1.13.13.13.13.13.13.1.3.cmml" xref="S3.E5.m1.13.13.13.13.13.13.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.13.13.13.13.13.13.1.3.1.cmml" xref="S3.E5.m1.13.13.13.13.13.13.1.3">superscript</csymbol><ci id="S3.E5.m1.13.13.13.13.13.13.1.3.2.cmml" xref="S3.E5.m1.13.13.13.13.13.13.1.3.2">ğ‘˜</ci><ci id="S3.E5.m1.13.13.13.13.13.13.1.3.3.cmml" xref="S3.E5.m1.13.13.13.13.13.13.1.3.3">â€²</ci></apply></apply></apply><apply id="S3.E5.m1.31.31.1.1.1.1.1.3.2.3.cmml"><csymbol cd="ambiguous" id="S3.E5.m1.31.31.1.1.1.1.1.3.2.3.1.cmml">subscript</csymbol><ci id="S3.E5.m1.14.14.14.14.14.14.cmml" xref="S3.E5.m1.14.14.14.14.14.14">ğ‰</ci><apply id="S3.E5.m1.15.15.15.15.15.15.1.cmml" xref="S3.E5.m1.15.15.15.15.15.15.1"><times id="S3.E5.m1.15.15.15.15.15.15.1.1.cmml" xref="S3.E5.m1.15.15.15.15.15.15.1.1"></times><apply id="S3.E5.m1.15.15.15.15.15.15.1.2.cmml" xref="S3.E5.m1.15.15.15.15.15.15.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.15.15.15.15.15.15.1.2.1.cmml" xref="S3.E5.m1.15.15.15.15.15.15.1.2">superscript</csymbol><ci id="S3.E5.m1.15.15.15.15.15.15.1.2.2.cmml" xref="S3.E5.m1.15.15.15.15.15.15.1.2.2">ğ‘˜</ci><ci id="S3.E5.m1.15.15.15.15.15.15.1.2.3.cmml" xref="S3.E5.m1.15.15.15.15.15.15.1.2.3">â€²</ci></apply><ci id="S3.E5.m1.15.15.15.15.15.15.1.3.cmml" xref="S3.E5.m1.15.15.15.15.15.15.1.3">ğ‘£</ci></apply></apply></apply></apply></apply><apply id="S3.E5.m1.31.31.1.1.1.2.2.cmml"><eq id="S3.E5.m1.20.20.20.4.4.4.cmml" xref="S3.E5.m1.20.20.20.4.4.4"></eq><apply id="S3.E5.m1.31.31.1.1.1.2.2.3.cmml"><times id="S3.E5.m1.31.31.1.1.1.2.2.3.1.cmml"></times><ci id="S3.E5.m1.17.17.17.1.1.1a.cmml" xref="S3.E5.m1.17.17.17.1.1.1"><mtext id="S3.E5.m1.17.17.17.1.1.1.cmml" xref="S3.E5.m1.17.17.17.1.1.1">whereÂ </mtext></ci><apply id="S3.E5.m1.31.31.1.1.1.2.2.3.3.cmml"><csymbol cd="ambiguous" id="S3.E5.m1.31.31.1.1.1.2.2.3.3.1.cmml">subscript</csymbol><ci id="S3.E5.m1.18.18.18.2.2.2.cmml" xref="S3.E5.m1.18.18.18.2.2.2">ğ›¼</ci><apply id="S3.E5.m1.19.19.19.3.3.3.1.cmml" xref="S3.E5.m1.19.19.19.3.3.3.1"><times id="S3.E5.m1.19.19.19.3.3.3.1.1.cmml" xref="S3.E5.m1.19.19.19.3.3.3.1.1"></times><ci id="S3.E5.m1.19.19.19.3.3.3.1.2.cmml" xref="S3.E5.m1.19.19.19.3.3.3.1.2">ğ‘˜</ci><apply id="S3.E5.m1.19.19.19.3.3.3.1.3.cmml" xref="S3.E5.m1.19.19.19.3.3.3.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.19.19.19.3.3.3.1.3.1.cmml" xref="S3.E5.m1.19.19.19.3.3.3.1.3">superscript</csymbol><ci id="S3.E5.m1.19.19.19.3.3.3.1.3.2.cmml" xref="S3.E5.m1.19.19.19.3.3.3.1.3.2">ğ‘˜</ci><ci id="S3.E5.m1.19.19.19.3.3.3.1.3.3.cmml" xref="S3.E5.m1.19.19.19.3.3.3.1.3.3">â€²</ci></apply></apply></apply></apply><apply id="S3.E5.m1.31.31.1.1.1.2.2.1.cmml"><times id="S3.E5.m1.31.31.1.1.1.2.2.1.2.cmml"></times><apply id="S3.E5.m1.31.31.1.1.1.2.2.1.3.cmml"><csymbol cd="ambiguous" id="S3.E5.m1.31.31.1.1.1.2.2.1.3.1.cmml">subscript</csymbol><ci id="S3.E5.m1.21.21.21.5.5.5.cmml" xref="S3.E5.m1.21.21.21.5.5.5">ğ’ğ¨ğŸğ­ğ¦ğšğ±</ci><apply id="S3.E5.m1.22.22.22.6.6.6.1.cmml" xref="S3.E5.m1.22.22.22.6.6.6.1"><csymbol cd="ambiguous" id="S3.E5.m1.22.22.22.6.6.6.1.1.cmml" xref="S3.E5.m1.22.22.22.6.6.6.1">superscript</csymbol><ci id="S3.E5.m1.22.22.22.6.6.6.1.2.cmml" xref="S3.E5.m1.22.22.22.6.6.6.1.2">ğ‘˜</ci><ci id="S3.E5.m1.22.22.22.6.6.6.1.3.cmml" xref="S3.E5.m1.22.22.22.6.6.6.1.3">â€²</ci></apply></apply><apply id="S3.E5.m1.31.31.1.1.1.2.2.1.1.1.1.cmml"><times id="S3.E5.m1.31.31.1.1.1.2.2.1.1.1.1.1.cmml"></times><apply id="S3.E5.m1.31.31.1.1.1.2.2.1.1.1.1.2.cmml"><csymbol cd="ambiguous" id="S3.E5.m1.31.31.1.1.1.2.2.1.1.1.1.2.1.cmml">superscript</csymbol><apply id="S3.E5.m1.31.31.1.1.1.2.2.1.1.1.1.2.2.cmml"><csymbol cd="ambiguous" id="S3.E5.m1.31.31.1.1.1.2.2.1.1.1.1.2.2.1.cmml">subscript</csymbol><ci id="S3.E5.m1.24.24.24.8.8.8.cmml" xref="S3.E5.m1.24.24.24.8.8.8">ğ‰</ci><apply id="S3.E5.m1.25.25.25.9.9.9.1.cmml" xref="S3.E5.m1.25.25.25.9.9.9.1"><times id="S3.E5.m1.25.25.25.9.9.9.1.1.cmml" xref="S3.E5.m1.25.25.25.9.9.9.1.1"></times><ci id="S3.E5.m1.25.25.25.9.9.9.1.2.cmml" xref="S3.E5.m1.25.25.25.9.9.9.1.2">ğ‘˜</ci><ci id="S3.E5.m1.25.25.25.9.9.9.1.3.cmml" xref="S3.E5.m1.25.25.25.9.9.9.1.3">ğ‘</ci></apply></apply><csymbol cd="latexml" id="S3.E5.m1.26.26.26.10.10.10.1.cmml" xref="S3.E5.m1.26.26.26.10.10.10.1">top</csymbol></apply><apply id="S3.E5.m1.31.31.1.1.1.2.2.1.1.1.1.3.cmml"><csymbol cd="ambiguous" id="S3.E5.m1.31.31.1.1.1.2.2.1.1.1.1.3.1.cmml">subscript</csymbol><ci id="S3.E5.m1.27.27.27.11.11.11.cmml" xref="S3.E5.m1.27.27.27.11.11.11">ğ‰</ci><apply id="S3.E5.m1.28.28.28.12.12.12.1.cmml" xref="S3.E5.m1.28.28.28.12.12.12.1"><times id="S3.E5.m1.28.28.28.12.12.12.1.1.cmml" xref="S3.E5.m1.28.28.28.12.12.12.1.1"></times><apply id="S3.E5.m1.28.28.28.12.12.12.1.2.cmml" xref="S3.E5.m1.28.28.28.12.12.12.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.28.28.28.12.12.12.1.2.1.cmml" xref="S3.E5.m1.28.28.28.12.12.12.1.2">superscript</csymbol><ci id="S3.E5.m1.28.28.28.12.12.12.1.2.2.cmml" xref="S3.E5.m1.28.28.28.12.12.12.1.2.2">ğ‘˜</ci><ci id="S3.E5.m1.28.28.28.12.12.12.1.2.3.cmml" xref="S3.E5.m1.28.28.28.12.12.12.1.2.3">â€²</ci></apply><ci id="S3.E5.m1.28.28.28.12.12.12.1.3.cmml" xref="S3.E5.m1.28.28.28.12.12.12.1.3">ğ‘˜</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.33c">\begin{split}\mathbf{M}({\mathbf{J}_{k^{\prime},k^{\prime}\in\mathcal{N}_{\mathbf{J}_{k}}}}|\mathbf{J}_{k})=\sum_{k^{\prime}\in\mathcal{N}_{\mathbf{J}_{k}}}\alpha_{kk^{\prime}}\mathbf{J}_{k^{\prime}v},\\
\text{where }\alpha_{kk^{\prime}}=\mathbf{Softmax}_{k^{\prime}}(\mathbf{J}_{kq}^{\top}\mathbf{J}_{k^{\prime}k}).\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.SSS3.p3" class="ltx_para">
<p id="S3.SS2.SSS3.p3.3" class="ltx_p"><math id="S3.SS2.SSS3.p3.1.m1.1" class="ltx_Math" alttext="\mathbf{J}^{\top}" display="inline"><semantics id="S3.SS2.SSS3.p3.1.m1.1a"><msup id="S3.SS2.SSS3.p3.1.m1.1.1" xref="S3.SS2.SSS3.p3.1.m1.1.1.cmml"><mi id="S3.SS2.SSS3.p3.1.m1.1.1.2" xref="S3.SS2.SSS3.p3.1.m1.1.1.2.cmml">ğ‰</mi><mo id="S3.SS2.SSS3.p3.1.m1.1.1.3" xref="S3.SS2.SSS3.p3.1.m1.1.1.3.cmml">âŠ¤</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p3.1.m1.1b"><apply id="S3.SS2.SSS3.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p3.1.m1.1.1.1.cmml" xref="S3.SS2.SSS3.p3.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.SSS3.p3.1.m1.1.1.2.cmml" xref="S3.SS2.SSS3.p3.1.m1.1.1.2">ğ‰</ci><csymbol cd="latexml" id="S3.SS2.SSS3.p3.1.m1.1.1.3.cmml" xref="S3.SS2.SSS3.p3.1.m1.1.1.3">top</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p3.1.m1.1c">\mathbf{J}^{\top}</annotation></semantics></math> represents the transpose of the feature vector <math id="S3.SS2.SSS3.p3.2.m2.1" class="ltx_Math" alttext="\mathbf{J}" display="inline"><semantics id="S3.SS2.SSS3.p3.2.m2.1a"><mi id="S3.SS2.SSS3.p3.2.m2.1.1" xref="S3.SS2.SSS3.p3.2.m2.1.1.cmml">ğ‰</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p3.2.m2.1b"><ci id="S3.SS2.SSS3.p3.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p3.2.m2.1.1">ğ‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p3.2.m2.1c">\mathbf{J}</annotation></semantics></math>
and the similarity is computed as the dot product between the query and keys.
<math id="S3.SS2.SSS3.p3.3.m3.1" class="ltx_Math" alttext="\alpha_{kk^{\prime}}" display="inline"><semantics id="S3.SS2.SSS3.p3.3.m3.1a"><msub id="S3.SS2.SSS3.p3.3.m3.1.1" xref="S3.SS2.SSS3.p3.3.m3.1.1.cmml"><mi id="S3.SS2.SSS3.p3.3.m3.1.1.2" xref="S3.SS2.SSS3.p3.3.m3.1.1.2.cmml">Î±</mi><mrow id="S3.SS2.SSS3.p3.3.m3.1.1.3" xref="S3.SS2.SSS3.p3.3.m3.1.1.3.cmml"><mi id="S3.SS2.SSS3.p3.3.m3.1.1.3.2" xref="S3.SS2.SSS3.p3.3.m3.1.1.3.2.cmml">k</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p3.3.m3.1.1.3.1" xref="S3.SS2.SSS3.p3.3.m3.1.1.3.1.cmml">â€‹</mo><msup id="S3.SS2.SSS3.p3.3.m3.1.1.3.3" xref="S3.SS2.SSS3.p3.3.m3.1.1.3.3.cmml"><mi id="S3.SS2.SSS3.p3.3.m3.1.1.3.3.2" xref="S3.SS2.SSS3.p3.3.m3.1.1.3.3.2.cmml">k</mi><mo id="S3.SS2.SSS3.p3.3.m3.1.1.3.3.3" xref="S3.SS2.SSS3.p3.3.m3.1.1.3.3.3.cmml">â€²</mo></msup></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p3.3.m3.1b"><apply id="S3.SS2.SSS3.p3.3.m3.1.1.cmml" xref="S3.SS2.SSS3.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p3.3.m3.1.1.1.cmml" xref="S3.SS2.SSS3.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS3.p3.3.m3.1.1.2.cmml" xref="S3.SS2.SSS3.p3.3.m3.1.1.2">ğ›¼</ci><apply id="S3.SS2.SSS3.p3.3.m3.1.1.3.cmml" xref="S3.SS2.SSS3.p3.3.m3.1.1.3"><times id="S3.SS2.SSS3.p3.3.m3.1.1.3.1.cmml" xref="S3.SS2.SSS3.p3.3.m3.1.1.3.1"></times><ci id="S3.SS2.SSS3.p3.3.m3.1.1.3.2.cmml" xref="S3.SS2.SSS3.p3.3.m3.1.1.3.2">ğ‘˜</ci><apply id="S3.SS2.SSS3.p3.3.m3.1.1.3.3.cmml" xref="S3.SS2.SSS3.p3.3.m3.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p3.3.m3.1.1.3.3.1.cmml" xref="S3.SS2.SSS3.p3.3.m3.1.1.3.3">superscript</csymbol><ci id="S3.SS2.SSS3.p3.3.m3.1.1.3.3.2.cmml" xref="S3.SS2.SSS3.p3.3.m3.1.1.3.3.2">ğ‘˜</ci><ci id="S3.SS2.SSS3.p3.3.m3.1.1.3.3.3.cmml" xref="S3.SS2.SSS3.p3.3.m3.1.1.3.3.3">â€²</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p3.3.m3.1c">\alpha_{kk^{\prime}}</annotation></semantics></math> is computed as the softmax normalization over the
similarities.</p>
</div>
<div id="S3.SS2.SSS3.p4" class="ltx_para">
<p id="S3.SS2.SSS3.p4.6" class="ltx_p">The information comes from the different types of edges plays different roles: edges within the same frame model the spatial dynamics while
edges across frames incorporate the temporal dynamics.
We keep separated parameters for the two dynamics. Specifically, the <math id="S3.SS2.SSS3.p4.1.m1.1" class="ltx_Math" alttext="\mathbf{MLP}(\boldsymbol{\cdot})" display="inline"><semantics id="S3.SS2.SSS3.p4.1.m1.1a"><mrow id="S3.SS2.SSS3.p4.1.m1.1.2" xref="S3.SS2.SSS3.p4.1.m1.1.2.cmml"><mi id="S3.SS2.SSS3.p4.1.m1.1.2.2" xref="S3.SS2.SSS3.p4.1.m1.1.2.2.cmml">ğŒğ‹ğ</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p4.1.m1.1.2.1" xref="S3.SS2.SSS3.p4.1.m1.1.2.1.cmml">â€‹</mo><mrow id="S3.SS2.SSS3.p4.1.m1.1.2.3.2" xref="S3.SS2.SSS3.p4.1.m1.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS3.p4.1.m1.1.2.3.2.1" xref="S3.SS2.SSS3.p4.1.m1.1.2.cmml">(</mo><mo class="ltx_mathvariant_bold" lspace="0em" mathvariant="bold" rspace="0em" id="S3.SS2.SSS3.p4.1.m1.1.1" xref="S3.SS2.SSS3.p4.1.m1.1.1.cmml">â‹…</mo><mo stretchy="false" id="S3.SS2.SSS3.p4.1.m1.1.2.3.2.2" xref="S3.SS2.SSS3.p4.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p4.1.m1.1b"><apply id="S3.SS2.SSS3.p4.1.m1.1.2.cmml" xref="S3.SS2.SSS3.p4.1.m1.1.2"><times id="S3.SS2.SSS3.p4.1.m1.1.2.1.cmml" xref="S3.SS2.SSS3.p4.1.m1.1.2.1"></times><ci id="S3.SS2.SSS3.p4.1.m1.1.2.2.cmml" xref="S3.SS2.SSS3.p4.1.m1.1.2.2">ğŒğ‹ğ</ci><ci id="S3.SS2.SSS3.p4.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p4.1.m1.1.1">bold-â‹…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p4.1.m1.1c">\mathbf{MLP}(\boldsymbol{\cdot})</annotation></semantics></math> in EquationÂ <a href="#S3.E4" title="In 3.2.3 Joint aggregation â€£ 3.2 Dynamics Modeling via GNN â€£ 3 Method â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> is switched between two implementations from layer to layer. In the <math id="S3.SS2.SSS3.p4.2.m2.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S3.SS2.SSS3.p4.2.m2.1a"><mi id="S3.SS2.SSS3.p4.2.m2.1.1" xref="S3.SS2.SSS3.p4.2.m2.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p4.2.m2.1b"><ci id="S3.SS2.SSS3.p4.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p4.2.m2.1.1">ğ‘™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p4.2.m2.1c">l</annotation></semantics></math>-th layer, the implementation is set to be <math id="S3.SS2.SSS3.p4.3.m3.1" class="ltx_Math" alttext="\mathbf{MLP}_{spatial}(\boldsymbol{\cdot})" display="inline"><semantics id="S3.SS2.SSS3.p4.3.m3.1a"><mrow id="S3.SS2.SSS3.p4.3.m3.1.2" xref="S3.SS2.SSS3.p4.3.m3.1.2.cmml"><msub id="S3.SS2.SSS3.p4.3.m3.1.2.2" xref="S3.SS2.SSS3.p4.3.m3.1.2.2.cmml"><mi id="S3.SS2.SSS3.p4.3.m3.1.2.2.2" xref="S3.SS2.SSS3.p4.3.m3.1.2.2.2.cmml">ğŒğ‹ğ</mi><mrow id="S3.SS2.SSS3.p4.3.m3.1.2.2.3" xref="S3.SS2.SSS3.p4.3.m3.1.2.2.3.cmml"><mi id="S3.SS2.SSS3.p4.3.m3.1.2.2.3.2" xref="S3.SS2.SSS3.p4.3.m3.1.2.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p4.3.m3.1.2.2.3.1" xref="S3.SS2.SSS3.p4.3.m3.1.2.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS3.p4.3.m3.1.2.2.3.3" xref="S3.SS2.SSS3.p4.3.m3.1.2.2.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p4.3.m3.1.2.2.3.1a" xref="S3.SS2.SSS3.p4.3.m3.1.2.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS3.p4.3.m3.1.2.2.3.4" xref="S3.SS2.SSS3.p4.3.m3.1.2.2.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p4.3.m3.1.2.2.3.1b" xref="S3.SS2.SSS3.p4.3.m3.1.2.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS3.p4.3.m3.1.2.2.3.5" xref="S3.SS2.SSS3.p4.3.m3.1.2.2.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p4.3.m3.1.2.2.3.1c" xref="S3.SS2.SSS3.p4.3.m3.1.2.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS3.p4.3.m3.1.2.2.3.6" xref="S3.SS2.SSS3.p4.3.m3.1.2.2.3.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p4.3.m3.1.2.2.3.1d" xref="S3.SS2.SSS3.p4.3.m3.1.2.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS3.p4.3.m3.1.2.2.3.7" xref="S3.SS2.SSS3.p4.3.m3.1.2.2.3.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p4.3.m3.1.2.2.3.1e" xref="S3.SS2.SSS3.p4.3.m3.1.2.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS3.p4.3.m3.1.2.2.3.8" xref="S3.SS2.SSS3.p4.3.m3.1.2.2.3.8.cmml">l</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p4.3.m3.1.2.1" xref="S3.SS2.SSS3.p4.3.m3.1.2.1.cmml">â€‹</mo><mrow id="S3.SS2.SSS3.p4.3.m3.1.2.3.2" xref="S3.SS2.SSS3.p4.3.m3.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS3.p4.3.m3.1.2.3.2.1" xref="S3.SS2.SSS3.p4.3.m3.1.2.cmml">(</mo><mo class="ltx_mathvariant_bold" lspace="0em" mathvariant="bold" rspace="0em" id="S3.SS2.SSS3.p4.3.m3.1.1" xref="S3.SS2.SSS3.p4.3.m3.1.1.cmml">â‹…</mo><mo stretchy="false" id="S3.SS2.SSS3.p4.3.m3.1.2.3.2.2" xref="S3.SS2.SSS3.p4.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p4.3.m3.1b"><apply id="S3.SS2.SSS3.p4.3.m3.1.2.cmml" xref="S3.SS2.SSS3.p4.3.m3.1.2"><times id="S3.SS2.SSS3.p4.3.m3.1.2.1.cmml" xref="S3.SS2.SSS3.p4.3.m3.1.2.1"></times><apply id="S3.SS2.SSS3.p4.3.m3.1.2.2.cmml" xref="S3.SS2.SSS3.p4.3.m3.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p4.3.m3.1.2.2.1.cmml" xref="S3.SS2.SSS3.p4.3.m3.1.2.2">subscript</csymbol><ci id="S3.SS2.SSS3.p4.3.m3.1.2.2.2.cmml" xref="S3.SS2.SSS3.p4.3.m3.1.2.2.2">ğŒğ‹ğ</ci><apply id="S3.SS2.SSS3.p4.3.m3.1.2.2.3.cmml" xref="S3.SS2.SSS3.p4.3.m3.1.2.2.3"><times id="S3.SS2.SSS3.p4.3.m3.1.2.2.3.1.cmml" xref="S3.SS2.SSS3.p4.3.m3.1.2.2.3.1"></times><ci id="S3.SS2.SSS3.p4.3.m3.1.2.2.3.2.cmml" xref="S3.SS2.SSS3.p4.3.m3.1.2.2.3.2">ğ‘ </ci><ci id="S3.SS2.SSS3.p4.3.m3.1.2.2.3.3.cmml" xref="S3.SS2.SSS3.p4.3.m3.1.2.2.3.3">ğ‘</ci><ci id="S3.SS2.SSS3.p4.3.m3.1.2.2.3.4.cmml" xref="S3.SS2.SSS3.p4.3.m3.1.2.2.3.4">ğ‘</ci><ci id="S3.SS2.SSS3.p4.3.m3.1.2.2.3.5.cmml" xref="S3.SS2.SSS3.p4.3.m3.1.2.2.3.5">ğ‘¡</ci><ci id="S3.SS2.SSS3.p4.3.m3.1.2.2.3.6.cmml" xref="S3.SS2.SSS3.p4.3.m3.1.2.2.3.6">ğ‘–</ci><ci id="S3.SS2.SSS3.p4.3.m3.1.2.2.3.7.cmml" xref="S3.SS2.SSS3.p4.3.m3.1.2.2.3.7">ğ‘</ci><ci id="S3.SS2.SSS3.p4.3.m3.1.2.2.3.8.cmml" xref="S3.SS2.SSS3.p4.3.m3.1.2.2.3.8">ğ‘™</ci></apply></apply><ci id="S3.SS2.SSS3.p4.3.m3.1.1.cmml" xref="S3.SS2.SSS3.p4.3.m3.1.1">bold-â‹…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p4.3.m3.1c">\mathbf{MLP}_{spatial}(\boldsymbol{\cdot})</annotation></semantics></math> working on neighbors defined by the edges within the same frame and in the next <math id="S3.SS2.SSS3.p4.4.m4.1" class="ltx_Math" alttext="(l+1)" display="inline"><semantics id="S3.SS2.SSS3.p4.4.m4.1a"><mrow id="S3.SS2.SSS3.p4.4.m4.1.1.1" xref="S3.SS2.SSS3.p4.4.m4.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.SSS3.p4.4.m4.1.1.1.2" xref="S3.SS2.SSS3.p4.4.m4.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS3.p4.4.m4.1.1.1.1" xref="S3.SS2.SSS3.p4.4.m4.1.1.1.1.cmml"><mi id="S3.SS2.SSS3.p4.4.m4.1.1.1.1.2" xref="S3.SS2.SSS3.p4.4.m4.1.1.1.1.2.cmml">l</mi><mo id="S3.SS2.SSS3.p4.4.m4.1.1.1.1.1" xref="S3.SS2.SSS3.p4.4.m4.1.1.1.1.1.cmml">+</mo><mn id="S3.SS2.SSS3.p4.4.m4.1.1.1.1.3" xref="S3.SS2.SSS3.p4.4.m4.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.SS2.SSS3.p4.4.m4.1.1.1.3" xref="S3.SS2.SSS3.p4.4.m4.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p4.4.m4.1b"><apply id="S3.SS2.SSS3.p4.4.m4.1.1.1.1.cmml" xref="S3.SS2.SSS3.p4.4.m4.1.1.1"><plus id="S3.SS2.SSS3.p4.4.m4.1.1.1.1.1.cmml" xref="S3.SS2.SSS3.p4.4.m4.1.1.1.1.1"></plus><ci id="S3.SS2.SSS3.p4.4.m4.1.1.1.1.2.cmml" xref="S3.SS2.SSS3.p4.4.m4.1.1.1.1.2">ğ‘™</ci><cn type="integer" id="S3.SS2.SSS3.p4.4.m4.1.1.1.1.3.cmml" xref="S3.SS2.SSS3.p4.4.m4.1.1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p4.4.m4.1c">(l+1)</annotation></semantics></math>-th layer, it is switched to <math id="S3.SS2.SSS3.p4.5.m5.1" class="ltx_Math" alttext="\mathbf{MLP}_{temporal}(\boldsymbol{\cdot})" display="inline"><semantics id="S3.SS2.SSS3.p4.5.m5.1a"><mrow id="S3.SS2.SSS3.p4.5.m5.1.2" xref="S3.SS2.SSS3.p4.5.m5.1.2.cmml"><msub id="S3.SS2.SSS3.p4.5.m5.1.2.2" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.cmml"><mi id="S3.SS2.SSS3.p4.5.m5.1.2.2.2" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.2.cmml">ğŒğ‹ğ</mi><mrow id="S3.SS2.SSS3.p4.5.m5.1.2.2.3" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.3.cmml"><mi id="S3.SS2.SSS3.p4.5.m5.1.2.2.3.2" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p4.5.m5.1.2.2.3.1" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS3.p4.5.m5.1.2.2.3.3" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p4.5.m5.1.2.2.3.1a" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS3.p4.5.m5.1.2.2.3.4" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.3.4.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p4.5.m5.1.2.2.3.1b" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS3.p4.5.m5.1.2.2.3.5" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.3.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p4.5.m5.1.2.2.3.1c" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS3.p4.5.m5.1.2.2.3.6" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.3.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p4.5.m5.1.2.2.3.1d" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS3.p4.5.m5.1.2.2.3.7" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.3.7.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p4.5.m5.1.2.2.3.1e" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS3.p4.5.m5.1.2.2.3.8" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.3.8.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p4.5.m5.1.2.2.3.1f" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.SSS3.p4.5.m5.1.2.2.3.9" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.3.9.cmml">l</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.SS2.SSS3.p4.5.m5.1.2.1" xref="S3.SS2.SSS3.p4.5.m5.1.2.1.cmml">â€‹</mo><mrow id="S3.SS2.SSS3.p4.5.m5.1.2.3.2" xref="S3.SS2.SSS3.p4.5.m5.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS3.p4.5.m5.1.2.3.2.1" xref="S3.SS2.SSS3.p4.5.m5.1.2.cmml">(</mo><mo class="ltx_mathvariant_bold" lspace="0em" mathvariant="bold" rspace="0em" id="S3.SS2.SSS3.p4.5.m5.1.1" xref="S3.SS2.SSS3.p4.5.m5.1.1.cmml">â‹…</mo><mo stretchy="false" id="S3.SS2.SSS3.p4.5.m5.1.2.3.2.2" xref="S3.SS2.SSS3.p4.5.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p4.5.m5.1b"><apply id="S3.SS2.SSS3.p4.5.m5.1.2.cmml" xref="S3.SS2.SSS3.p4.5.m5.1.2"><times id="S3.SS2.SSS3.p4.5.m5.1.2.1.cmml" xref="S3.SS2.SSS3.p4.5.m5.1.2.1"></times><apply id="S3.SS2.SSS3.p4.5.m5.1.2.2.cmml" xref="S3.SS2.SSS3.p4.5.m5.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p4.5.m5.1.2.2.1.cmml" xref="S3.SS2.SSS3.p4.5.m5.1.2.2">subscript</csymbol><ci id="S3.SS2.SSS3.p4.5.m5.1.2.2.2.cmml" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.2">ğŒğ‹ğ</ci><apply id="S3.SS2.SSS3.p4.5.m5.1.2.2.3.cmml" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.3"><times id="S3.SS2.SSS3.p4.5.m5.1.2.2.3.1.cmml" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.3.1"></times><ci id="S3.SS2.SSS3.p4.5.m5.1.2.2.3.2.cmml" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.3.2">ğ‘¡</ci><ci id="S3.SS2.SSS3.p4.5.m5.1.2.2.3.3.cmml" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.3.3">ğ‘’</ci><ci id="S3.SS2.SSS3.p4.5.m5.1.2.2.3.4.cmml" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.3.4">ğ‘š</ci><ci id="S3.SS2.SSS3.p4.5.m5.1.2.2.3.5.cmml" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.3.5">ğ‘</ci><ci id="S3.SS2.SSS3.p4.5.m5.1.2.2.3.6.cmml" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.3.6">ğ‘œ</ci><ci id="S3.SS2.SSS3.p4.5.m5.1.2.2.3.7.cmml" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.3.7">ğ‘Ÿ</ci><ci id="S3.SS2.SSS3.p4.5.m5.1.2.2.3.8.cmml" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.3.8">ğ‘</ci><ci id="S3.SS2.SSS3.p4.5.m5.1.2.2.3.9.cmml" xref="S3.SS2.SSS3.p4.5.m5.1.2.2.3.9">ğ‘™</ci></apply></apply><ci id="S3.SS2.SSS3.p4.5.m5.1.1.cmml" xref="S3.SS2.SSS3.p4.5.m5.1.1">bold-â‹…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p4.5.m5.1c">\mathbf{MLP}_{temporal}(\boldsymbol{\cdot})</annotation></semantics></math> working on neighbors defined by edges across frames, and so on so forth.
The aggregated features of joints from <math id="S3.SS2.SSS3.p4.6.m6.1" class="ltx_Math" alttext="\mathbf{P}_{t-1}" display="inline"><semantics id="S3.SS2.SSS3.p4.6.m6.1a"><msub id="S3.SS2.SSS3.p4.6.m6.1.1" xref="S3.SS2.SSS3.p4.6.m6.1.1.cmml"><mi id="S3.SS2.SSS3.p4.6.m6.1.1.2" xref="S3.SS2.SSS3.p4.6.m6.1.1.2.cmml">ğ</mi><mrow id="S3.SS2.SSS3.p4.6.m6.1.1.3" xref="S3.SS2.SSS3.p4.6.m6.1.1.3.cmml"><mi id="S3.SS2.SSS3.p4.6.m6.1.1.3.2" xref="S3.SS2.SSS3.p4.6.m6.1.1.3.2.cmml">t</mi><mo id="S3.SS2.SSS3.p4.6.m6.1.1.3.1" xref="S3.SS2.SSS3.p4.6.m6.1.1.3.1.cmml">âˆ’</mo><mn id="S3.SS2.SSS3.p4.6.m6.1.1.3.3" xref="S3.SS2.SSS3.p4.6.m6.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p4.6.m6.1b"><apply id="S3.SS2.SSS3.p4.6.m6.1.1.cmml" xref="S3.SS2.SSS3.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p4.6.m6.1.1.1.cmml" xref="S3.SS2.SSS3.p4.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.SSS3.p4.6.m6.1.1.2.cmml" xref="S3.SS2.SSS3.p4.6.m6.1.1.2">ğ</ci><apply id="S3.SS2.SSS3.p4.6.m6.1.1.3.cmml" xref="S3.SS2.SSS3.p4.6.m6.1.1.3"><minus id="S3.SS2.SSS3.p4.6.m6.1.1.3.1.cmml" xref="S3.SS2.SSS3.p4.6.m6.1.1.3.1"></minus><ci id="S3.SS2.SSS3.p4.6.m6.1.1.3.2.cmml" xref="S3.SS2.SSS3.p4.6.m6.1.1.3.2">ğ‘¡</ci><cn type="integer" id="S3.SS2.SSS3.p4.6.m6.1.1.3.3.cmml" xref="S3.SS2.SSS3.p4.6.m6.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p4.6.m6.1c">\mathbf{P}_{t-1}</annotation></semantics></math> are used for the pose prediction step.</p>
</div>
</section>
<section id="S3.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.4 </span>Pose predictionÂ </h4>

<div id="S3.SS2.SSS4.p1" class="ltx_para">
<p id="S3.SS2.SSS4.p1.3" class="ltx_p">This step aims to locate the poses in current frame by the GNN model, with neither human detection nor single-frame human pose estimator.
To reduce computation, we select potential joints only from a confined scope.
We propagate the bounding box of the last tracked pose <math id="S3.SS2.SSS4.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{P}_{t-1}" display="inline"><semantics id="S3.SS2.SSS4.p1.1.m1.1a"><msub id="S3.SS2.SSS4.p1.1.m1.1.1" xref="S3.SS2.SSS4.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS4.p1.1.m1.1.1.2" xref="S3.SS2.SSS4.p1.1.m1.1.1.2.cmml">ğ</mi><mrow id="S3.SS2.SSS4.p1.1.m1.1.1.3" xref="S3.SS2.SSS4.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS4.p1.1.m1.1.1.3.2" xref="S3.SS2.SSS4.p1.1.m1.1.1.3.2.cmml">t</mi><mo id="S3.SS2.SSS4.p1.1.m1.1.1.3.1" xref="S3.SS2.SSS4.p1.1.m1.1.1.3.1.cmml">âˆ’</mo><mn id="S3.SS2.SSS4.p1.1.m1.1.1.3.3" xref="S3.SS2.SSS4.p1.1.m1.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p1.1.m1.1b"><apply id="S3.SS2.SSS4.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS4.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS4.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS4.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS4.p1.1.m1.1.1.2">ğ</ci><apply id="S3.SS2.SSS4.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS4.p1.1.m1.1.1.3"><minus id="S3.SS2.SSS4.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS4.p1.1.m1.1.1.3.1"></minus><ci id="S3.SS2.SSS4.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS4.p1.1.m1.1.1.3.2">ğ‘¡</ci><cn type="integer" id="S3.SS2.SSS4.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.SSS4.p1.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p1.1.m1.1c">\mathbf{P}_{t-1}</annotation></semantics></math>
to current frame and scale it up by a factor of <math id="S3.SS2.SSS4.p1.2.m2.1" class="ltx_Math" alttext="1.5" display="inline"><semantics id="S3.SS2.SSS4.p1.2.m2.1a"><mn id="S3.SS2.SSS4.p1.2.m2.1.1" xref="S3.SS2.SSS4.p1.2.m2.1.1.cmml">1.5</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p1.2.m2.1b"><cn type="float" id="S3.SS2.SSS4.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS4.p1.2.m2.1.1">1.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p1.2.m2.1c">1.5</annotation></semantics></math>
vertically and <math id="S3.SS2.SSS4.p1.3.m3.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S3.SS2.SSS4.p1.3.m3.1a"><mn id="S3.SS2.SSS4.p1.3.m3.1.1" xref="S3.SS2.SSS4.p1.3.m3.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p1.3.m3.1b"><cn type="integer" id="S3.SS2.SSS4.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS4.p1.3.m3.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p1.3.m3.1c">2</annotation></semantics></math> horizontally at the same center to support fast-motion scenes, shown as the dotted orange box in
FigureÂ <a href="#S3.F3" title="Figure 3 â€£ 3.2 Dynamics Modeling via GNN â€£ 3 Method â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S3.SS2.SSS4.p2" class="ltx_para">
<p id="S3.SS2.SSS4.p2.1" class="ltx_p">A graph is constructed with potential joints in the current frame and joints from <math id="S3.SS2.SSS4.p2.1.m1.1" class="ltx_Math" alttext="\mathbf{P}_{t-1}" display="inline"><semantics id="S3.SS2.SSS4.p2.1.m1.1a"><msub id="S3.SS2.SSS4.p2.1.m1.1.1" xref="S3.SS2.SSS4.p2.1.m1.1.1.cmml"><mi id="S3.SS2.SSS4.p2.1.m1.1.1.2" xref="S3.SS2.SSS4.p2.1.m1.1.1.2.cmml">ğ</mi><mrow id="S3.SS2.SSS4.p2.1.m1.1.1.3" xref="S3.SS2.SSS4.p2.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS4.p2.1.m1.1.1.3.2" xref="S3.SS2.SSS4.p2.1.m1.1.1.3.2.cmml">t</mi><mo id="S3.SS2.SSS4.p2.1.m1.1.1.3.1" xref="S3.SS2.SSS4.p2.1.m1.1.1.3.1.cmml">âˆ’</mo><mn id="S3.SS2.SSS4.p2.1.m1.1.1.3.3" xref="S3.SS2.SSS4.p2.1.m1.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p2.1.m1.1b"><apply id="S3.SS2.SSS4.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS4.p2.1.m1.1.1.1.cmml" xref="S3.SS2.SSS4.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS4.p2.1.m1.1.1.2.cmml" xref="S3.SS2.SSS4.p2.1.m1.1.1.2">ğ</ci><apply id="S3.SS2.SSS4.p2.1.m1.1.1.3.cmml" xref="S3.SS2.SSS4.p2.1.m1.1.1.3"><minus id="S3.SS2.SSS4.p2.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS4.p2.1.m1.1.1.3.1"></minus><ci id="S3.SS2.SSS4.p2.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS4.p2.1.m1.1.1.3.2">ğ‘¡</ci><cn type="integer" id="S3.SS2.SSS4.p2.1.m1.1.1.3.3.cmml" xref="S3.SS2.SSS4.p2.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p2.1.m1.1c">\mathbf{P}_{t-1}</annotation></semantics></math>. The learned GNN model is then applied to this graph to update joint features via message passing as explained above.</p>
</div>
<div id="S3.SS2.SSS4.p3" class="ltx_para">
<p id="S3.SS2.SSS4.p3.1" class="ltx_p">On top of the final features from GNN as <math id="S3.SS2.SSS4.p3.1.m1.1" class="ltx_Math" alttext="\mathbf{J}" display="inline"><semantics id="S3.SS2.SSS4.p3.1.m1.1a"><mi id="S3.SS2.SSS4.p3.1.m1.1.1" xref="S3.SS2.SSS4.p3.1.m1.1.1.cmml">ğ‰</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p3.1.m1.1b"><ci id="S3.SS2.SSS4.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS4.p3.1.m1.1.1">ğ‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p3.1.m1.1c">\mathbf{J}</annotation></semantics></math>, the prediction is conducted via another MLP over each potential joint in current frame, i.e.,</p>
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E6.m1.2" class="ltx_Math" alttext="\mathbf{Prob}=\mathbf{MLP}_{pred}(\mathbf{J})," display="block"><semantics id="S3.E6.m1.2a"><mrow id="S3.E6.m1.2.2.1" xref="S3.E6.m1.2.2.1.1.cmml"><mrow id="S3.E6.m1.2.2.1.1" xref="S3.E6.m1.2.2.1.1.cmml"><mi id="S3.E6.m1.2.2.1.1.2" xref="S3.E6.m1.2.2.1.1.2.cmml">ğğ«ğ¨ğ›</mi><mo id="S3.E6.m1.2.2.1.1.1" xref="S3.E6.m1.2.2.1.1.1.cmml">=</mo><mrow id="S3.E6.m1.2.2.1.1.3" xref="S3.E6.m1.2.2.1.1.3.cmml"><msub id="S3.E6.m1.2.2.1.1.3.2" xref="S3.E6.m1.2.2.1.1.3.2.cmml"><mi id="S3.E6.m1.2.2.1.1.3.2.2" xref="S3.E6.m1.2.2.1.1.3.2.2.cmml">ğŒğ‹ğ</mi><mrow id="S3.E6.m1.2.2.1.1.3.2.3" xref="S3.E6.m1.2.2.1.1.3.2.3.cmml"><mi id="S3.E6.m1.2.2.1.1.3.2.3.2" xref="S3.E6.m1.2.2.1.1.3.2.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.2.2.1.1.3.2.3.1" xref="S3.E6.m1.2.2.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S3.E6.m1.2.2.1.1.3.2.3.3" xref="S3.E6.m1.2.2.1.1.3.2.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.2.2.1.1.3.2.3.1a" xref="S3.E6.m1.2.2.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S3.E6.m1.2.2.1.1.3.2.3.4" xref="S3.E6.m1.2.2.1.1.3.2.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.2.2.1.1.3.2.3.1b" xref="S3.E6.m1.2.2.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S3.E6.m1.2.2.1.1.3.2.3.5" xref="S3.E6.m1.2.2.1.1.3.2.3.5.cmml">d</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E6.m1.2.2.1.1.3.1" xref="S3.E6.m1.2.2.1.1.3.1.cmml">â€‹</mo><mrow id="S3.E6.m1.2.2.1.1.3.3.2" xref="S3.E6.m1.2.2.1.1.3.cmml"><mo stretchy="false" id="S3.E6.m1.2.2.1.1.3.3.2.1" xref="S3.E6.m1.2.2.1.1.3.cmml">(</mo><mi id="S3.E6.m1.1.1" xref="S3.E6.m1.1.1.cmml">ğ‰</mi><mo stretchy="false" id="S3.E6.m1.2.2.1.1.3.3.2.2" xref="S3.E6.m1.2.2.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E6.m1.2.2.1.2" xref="S3.E6.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.2b"><apply id="S3.E6.m1.2.2.1.1.cmml" xref="S3.E6.m1.2.2.1"><eq id="S3.E6.m1.2.2.1.1.1.cmml" xref="S3.E6.m1.2.2.1.1.1"></eq><ci id="S3.E6.m1.2.2.1.1.2.cmml" xref="S3.E6.m1.2.2.1.1.2">ğğ«ğ¨ğ›</ci><apply id="S3.E6.m1.2.2.1.1.3.cmml" xref="S3.E6.m1.2.2.1.1.3"><times id="S3.E6.m1.2.2.1.1.3.1.cmml" xref="S3.E6.m1.2.2.1.1.3.1"></times><apply id="S3.E6.m1.2.2.1.1.3.2.cmml" xref="S3.E6.m1.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.1.1.3.2.1.cmml" xref="S3.E6.m1.2.2.1.1.3.2">subscript</csymbol><ci id="S3.E6.m1.2.2.1.1.3.2.2.cmml" xref="S3.E6.m1.2.2.1.1.3.2.2">ğŒğ‹ğ</ci><apply id="S3.E6.m1.2.2.1.1.3.2.3.cmml" xref="S3.E6.m1.2.2.1.1.3.2.3"><times id="S3.E6.m1.2.2.1.1.3.2.3.1.cmml" xref="S3.E6.m1.2.2.1.1.3.2.3.1"></times><ci id="S3.E6.m1.2.2.1.1.3.2.3.2.cmml" xref="S3.E6.m1.2.2.1.1.3.2.3.2">ğ‘</ci><ci id="S3.E6.m1.2.2.1.1.3.2.3.3.cmml" xref="S3.E6.m1.2.2.1.1.3.2.3.3">ğ‘Ÿ</ci><ci id="S3.E6.m1.2.2.1.1.3.2.3.4.cmml" xref="S3.E6.m1.2.2.1.1.3.2.3.4">ğ‘’</ci><ci id="S3.E6.m1.2.2.1.1.3.2.3.5.cmml" xref="S3.E6.m1.2.2.1.1.3.2.3.5">ğ‘‘</ci></apply></apply><ci id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.1.1">ğ‰</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.2c">\mathbf{Prob}=\mathbf{MLP}_{pred}(\mathbf{J}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS4.p3.2" class="ltx_p">where <math id="S3.SS2.SSS4.p3.2.m1.1" class="ltx_Math" alttext="\mathbf{Prob}" display="inline"><semantics id="S3.SS2.SSS4.p3.2.m1.1a"><mi id="S3.SS2.SSS4.p3.2.m1.1.1" xref="S3.SS2.SSS4.p3.2.m1.1.1.cmml">ğğ«ğ¨ğ›</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p3.2.m1.1b"><ci id="S3.SS2.SSS4.p3.2.m1.1.1.cmml" xref="S3.SS2.SSS4.p3.2.m1.1.1">ğğ«ğ¨ğ›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p3.2.m1.1c">\mathbf{Prob}</annotation></semantics></math> denotes the probability distribution over all joint types of the input node. The predicted probability distributions of all potential joints in current frame generate the predicted heatmaps for all joints.</p>
</div>
</section>
<section id="S3.SS2.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.5 </span>Training</h4>

<div id="S3.SS2.SSS5.p1" class="ltx_para">
<p id="S3.SS2.SSS5.p1.1" class="ltx_p">As in EquationÂ <a href="#S3.E2" title="In 3.1 Single-Frame Pose Estimation â€£ 3 Method â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we generate ground-truth heatmaps from labeled human pose and compute <math id="S3.SS2.SSS5.p1.1.m1.1" class="ltx_Math" alttext="L_{2}" display="inline"><semantics id="S3.SS2.SSS5.p1.1.m1.1a"><msub id="S3.SS2.SSS5.p1.1.m1.1.1" xref="S3.SS2.SSS5.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS5.p1.1.m1.1.1.2" xref="S3.SS2.SSS5.p1.1.m1.1.1.2.cmml">L</mi><mn id="S3.SS2.SSS5.p1.1.m1.1.1.3" xref="S3.SS2.SSS5.p1.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS5.p1.1.m1.1b"><apply id="S3.SS2.SSS5.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS5.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS5.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS5.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS5.p1.1.m1.1.1.2">ğ¿</ci><cn type="integer" id="S3.SS2.SSS5.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS5.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS5.p1.1.m1.1c">L_{2}</annotation></semantics></math> loss against the predicted joint heatmaps.
Since the full GNN predictor is differentiable, we optimize the parameters and learn the dynamics from end to end.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Online Tracking Pipeline</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In the current frame, given the poses from the GNN-based predictor and the poses
from the single-frame pose estimator, we match and fuse them to obtain the final tracked human poses.
In this process, the poses from the predictor and that from the estimator
are complimentary to each other as the poses missed by the single-frame estimator due to occlusion and motion blur can be recovered by the predictor.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Specifically, we apply Hungarian matching to compute an one-to-one mapping
between the predicted poses and the estimated poses.
The similarity used in the Hungarian algorithm is the
object keypoint similarityÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> computed based on the positions of the joints.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">After matching, we propagate the tracking IDs from the predicted poses to
the estimated poses if they are matched.
A new ID is assigned to the estimated pose without a matched predicted pose,
which is likely to be a newly observed one.
For all the matched poses, the joint heatmaps of the two poses are
first aligned according to their centers and then
merged together by averaging the heatmaps. Refined poses are then decoded from the fused heatmaps.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">We store the tracked results in a FIFO manner
while keeping a fixed size of each tracklet.
The history tracklets are then used as inputs to the GNN model
for the following frame. The proposed framework is hence implemented to be an online tracker, as shown in FigureÂ <a href="#S2.F2" title="Figure 2 â€£ 2.1 Single-Frame Human Pose Estimation â€£ 2 Related Work â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We evaluate the proposed method on two widely used datasets for human
pose estimation and tracking, PoseTrack 2017
and PoseTrack 2018Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
These datasets contain several video sequences of articulated people
that perform various actions.
Specially, PoseTrack 2017 contains 250 video sequences for training
and 50 video sequences for validation,
PoseTrack 2018 increases the number of video sequences
and contains 593 for training and 170 for validations.
Both datasets are annotated with 15 joints, each of them
are associated with an ID for the corresponding person.
The training videos are annotated densely within the middle 30 frames
of each video sequences.
The validation videos are annotated every forth frame across the whole
video sequences beside the densely annotation of the middle.
We use the training set for training and validation set for testing,
which is a common setup in previous worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">The performance of the proposed method is evaluated from two
aspects: human pose estimation and human pose tracking.
We use mean Average Precision (mAP)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> to evaluate the performance of human pose estimation, and Multi Object Tracking Accuracy (MOTA) to evaluate human pose tracking.
MOTA is evaluated based on three kinds of errors:
missing rate, false positive rate, and switch rate.
Both metrics are computed independently for
each joint and then averaged across all joints.
Since the evaluation of human pose tracking requires
filtering the joints according to some certain thresholds,
we can either evaluated the performance of
human pose estimation independently or based on
the filtered joints.
The former one provides us an illustration of
of the trade-off between human pose tracking and
human pose estimation while the
latter one provides us the pure performance of human
pose estimation.
We report both results for pose estimation.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation Details</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">For the single-frame human pose estimation, we used HRNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>
as the backbone.
Following the training strategies ofÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>,
the HRNet is first trained on COCO dataset and then fine-tuned on
PoseTrack 2017 and PoseTrack 2018 independently.
For the fine-tuning process, we train the model for 20 epochs with Adam
optimizer. The initial learning rate is set to be 0.0001
and reduced by a factor of 10 at the 10th and 15th epochs.
We add several data augmentation strategies as used inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>,
including random rotation, random flip,
randomly using half of body, and random scale.
Flip test is used in our work as inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.
We adopt Faster R-CNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> with feature pyramid network and
deformable convolutional network as the human detectorÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>.
The human detector is pre-trained on COCO dataset and then fine-tuned
on PoseTrack 2017 and PoseTrack 2018 separately.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">For the human detector, Non-Maximum Suppresion (NMS) is applied
to remove duplicate detected bounding boxes which is a common
operation in detection.
Specifically, we use Soft-NMSÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and set the threshold
to 0.7. As articulated human pose tracking in a video
often involves complex interaction and heavy person-to-person occlusions, traditional NMS in object detection that merely rely
on the Intersection Over UnionÂ (IOU) of the bounding boxes is prone to
failÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>.
Since we have the pose information, Pose-based Non-Maximum
SuppresionÂ (pNMS)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> is adopted to help
further remove the duplicate human poses.
In pNMS, the IOU is not computed based on the bounding boxes
but the weighted sum of all the jointsâ€™ distances with respect
to the scale of the pose. The threshold of pNMS is set to be
0.5.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">For the training of the GNN pose prediction model,
the fine-tuned backbone model is used to compute the visual feature
of the joints. Specifically, we obtain the feature maps that are in the same
resolution as the heatmaps, from all the three stages
of the HRNet. The feature maps then are concatenated together and form
the final feature maps with depth of 144.
The visual feature of each joint can be obtained according to the joint position in the heatmap.
Several data augmentation strategies are used during the GNN training process, including random rotation of the tube, random flip,
random scale of the tube, and
randomly selecting the gap between consecutive frames in the tube.
We train the GNN model for 10 epochs with Adam
optimizer. The initial learning rate is set to be 0.0001
and reduced by a factor of 10 at the 5th and 8th epochs. The length of pose history is set to be three.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results on PoseTrack 2017</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We compare our proposed method with the state-of-the-art methods
in human pose estimation and human pose tracking,
which are shown in TableÂ <a href="#S4.T1" title="Table 1 â€£ 4.3 Results on PoseTrack 2017 â€£ 4 Experiments â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, TableÂ <a href="#S4.T2" title="Table 2 â€£ 4.3 Results on PoseTrack 2017 â€£ 4 Experiments â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, and TableÂ <a href="#S4.T3" title="Table 3 â€£ 4.3 Results on PoseTrack 2017 â€£ 4 Experiments â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
In TableÂ <a href="#S4.T2" title="Table 2 â€£ 4.3 Results on PoseTrack 2017 â€£ 4 Experiments â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and TableÂ <a href="#S4.T3" title="Table 3 â€£ 4.3 Results on PoseTrack 2017 â€£ 4 Experiments â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>,
the upper methods are bottom-up fashion and lower methods
are top-down fashion.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:289.6pt;height:51.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-59.2pt,10.4pt) scale(0.71,0.71) ;">
<table id="S4.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Method</th>
<th id="S4.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Head</th>
<th id="S4.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Shou</th>
<th id="S4.T1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Elb</th>
<th id="S4.T1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Wri</th>
<th id="S4.T1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Hip</th>
<th id="S4.T1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Knee</th>
<th id="S4.T1.1.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Ankl</th>
<th id="S4.T1.1.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Total</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.2.1" class="ltx_tr">
<th id="S4.T1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">PoseWarperÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</th>
<td id="S4.T1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">81.4</td>
<td id="S4.T1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">88.3</td>
<td id="S4.T1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">83.9</td>
<td id="S4.T1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">78.0</td>
<td id="S4.T1.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">82.4</td>
<td id="S4.T1.1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">80.5</td>
<td id="S4.T1.1.1.2.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">73.6</td>
<td id="S4.T1.1.1.2.1.9" class="ltx_td ltx_align_center ltx_border_t">81.2</td>
</tr>
<tr id="S4.T1.1.1.3.2" class="ltx_tr">
<th id="S4.T1.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">CombDetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>
</th>
<td id="S4.T1.1.1.3.2.2" class="ltx_td ltx_align_center">89.4</td>
<td id="S4.T1.1.1.3.2.3" class="ltx_td ltx_align_center">89.7</td>
<td id="S4.T1.1.1.3.2.4" class="ltx_td ltx_align_center">85.5</td>
<td id="S4.T1.1.1.3.2.5" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.3.2.5.1" class="ltx_text ltx_font_bold">79.5</span></td>
<td id="S4.T1.1.1.3.2.6" class="ltx_td ltx_align_center">82.4</td>
<td id="S4.T1.1.1.3.2.7" class="ltx_td ltx_align_center">80.8</td>
<td id="S4.T1.1.1.3.2.8" class="ltx_td ltx_align_center ltx_border_r">76.4</td>
<td id="S4.T1.1.1.3.2.9" class="ltx_td ltx_align_center">83.8</td>
</tr>
<tr id="S4.T1.1.1.4.3" class="ltx_tr">
<th id="S4.T1.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">Ours</th>
<td id="S4.T1.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.1.1.4.3.2.1" class="ltx_text ltx_font_bold">90.9</span></td>
<td id="S4.T1.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.1.1.4.3.3.1" class="ltx_text ltx_font_bold">90.7</span></td>
<td id="S4.T1.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.1.1.4.3.4.1" class="ltx_text ltx_font_bold">86.0</span></td>
<td id="S4.T1.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_b">79.2</td>
<td id="S4.T1.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.1.1.4.3.6.1" class="ltx_text ltx_font_bold">83.8</span></td>
<td id="S4.T1.1.1.4.3.7" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.1.1.4.3.7.1" class="ltx_text ltx_font_bold">82.7</span></td>
<td id="S4.T1.1.1.4.3.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T1.1.1.4.3.8.1" class="ltx_text ltx_font_bold">78.0</span></td>
<td id="S4.T1.1.1.4.3.9" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.1.1.4.3.9.1" class="ltx_text ltx_font_bold">84.9</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of state-of-the-art methods on pure human pose estimation (without filtering) on the PoseTrack 2017 validation set, where the
performance is evaluated as mAP and all joints are counted.</figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Human pose estimation.</span>
In TableÂ <a href="#S4.T1" title="Table 1 â€£ 4.3 Results on PoseTrack 2017 â€£ 4 Experiments â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and TableÂ <a href="#S4.T2" title="Table 2 â€£ 4.3 Results on PoseTrack 2017 â€£ 4 Experiments â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we evaluate pure human
pose estimation in videos where the estimated poses are directly evaluated without filtering, as well as the filtered human pose estimation performance in the context of pose tracking. As shown in TableÂ <a href="#S4.T1" title="Table 1 â€£ 4.3 Results on PoseTrack 2017 â€£ 4 Experiments â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the proposed method achieves the
best performance, outperforming the previous best methodÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> by 1.1 mAP.
Note that CombDetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> utilizes a heavier 3D convolutional backbone and uses 9 frames as input. Since human pose tracking needs to firstly filter some estimated joints, the mAP result in TableÂ <a href="#S4.T2" title="Table 2 â€£ 4.3 Results on PoseTrack 2017 â€£ 4 Experiments â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> is lower than that in TableÂ <a href="#S4.T1" title="Table 1 â€£ 4.3 Results on PoseTrack 2017 â€£ 4 Experiments â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. As shown in TableÂ <a href="#S4.T3" title="Table 3 â€£ 4.3 Results on PoseTrack 2017 â€£ 4 Experiments â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, our method outperforms the best top-down methodÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> by 1.6 mAP and the best bottom-up methodÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> by
4.1 mAP.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_bold">Human pose tracking.</span>
As shown in TableÂ <a href="#S4.T3" title="Table 3 â€£ 4.3 Results on PoseTrack 2017 â€£ 4 Experiments â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, our method achieves state-of-the-art pose tracking
performance and outperform the best top-down methodÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> by 1.2 MOTA, and the best bottom-up methodÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> by 1.6 MOTA.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p"><span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_bold">Qualitative samples.</span>
To provide an intuitive understanding of our method,
in FigureÂ <a href="#S4.F4" title="Figure 4 â€£ 4.5 Model Analysis â€£ 4 Experiments â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> we visualize some samples of the
history pose tracklets, the pose estimation result in the current frame, and
the final outputs of our full method. Different skeleton colors represents different person identity
and the red circles in the 4th column highlight the missed or incorrect estimated joints that are corrected by the proposed GNN model.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:282.4pt;height:140.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-57.7pt,28.7pt) scale(0.71,0.71) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Method</th>
<td id="S4.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t">Head</td>
<td id="S4.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t">Shou</td>
<td id="S4.T2.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">Elb</td>
<td id="S4.T2.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">Wri</td>
<td id="S4.T2.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t">Hip</td>
<td id="S4.T2.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t">Knee</td>
<td id="S4.T2.1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Ankl</td>
<td id="S4.T2.1.1.1.1.9" class="ltx_td ltx_align_center ltx_border_t">Total</td>
</tr>
<tr id="S4.T2.1.1.2.2" class="ltx_tr">
<th id="S4.T2.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">BUTDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</th>
<td id="S4.T2.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">79.1</td>
<td id="S4.T2.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">77.3</td>
<td id="S4.T2.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">69.9</td>
<td id="S4.T2.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">58.3</td>
<td id="S4.T2.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t">66.2</td>
<td id="S4.T2.1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t">63.5</td>
<td id="S4.T2.1.1.2.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">54.9</td>
<td id="S4.T2.1.1.2.2.9" class="ltx_td ltx_align_center ltx_border_t">67.8</td>
</tr>
<tr id="S4.T2.1.1.3.3" class="ltx_tr">
<th id="S4.T2.1.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">RPAFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>
</th>
<td id="S4.T2.1.1.3.3.2" class="ltx_td ltx_align_center">83.8</td>
<td id="S4.T2.1.1.3.3.3" class="ltx_td ltx_align_center">84.9</td>
<td id="S4.T2.1.1.3.3.4" class="ltx_td ltx_align_center">76.2</td>
<td id="S4.T2.1.1.3.3.5" class="ltx_td ltx_align_center">64.0</td>
<td id="S4.T2.1.1.3.3.6" class="ltx_td ltx_align_center">72.2</td>
<td id="S4.T2.1.1.3.3.7" class="ltx_td ltx_align_center">64.5</td>
<td id="S4.T2.1.1.3.3.8" class="ltx_td ltx_align_center ltx_border_r">56.6</td>
<td id="S4.T2.1.1.3.3.9" class="ltx_td ltx_align_center">72.6</td>
</tr>
<tr id="S4.T2.1.1.4.4" class="ltx_tr">
<th id="S4.T2.1.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ArtTrackÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</th>
<td id="S4.T2.1.1.4.4.2" class="ltx_td ltx_align_center">78.7</td>
<td id="S4.T2.1.1.4.4.3" class="ltx_td ltx_align_center">76.2</td>
<td id="S4.T2.1.1.4.4.4" class="ltx_td ltx_align_center">70.4</td>
<td id="S4.T2.1.1.4.4.5" class="ltx_td ltx_align_center">62.3</td>
<td id="S4.T2.1.1.4.4.6" class="ltx_td ltx_align_center">68.1</td>
<td id="S4.T2.1.1.4.4.7" class="ltx_td ltx_align_center">66.7</td>
<td id="S4.T2.1.1.4.4.8" class="ltx_td ltx_align_center ltx_border_r">58.4</td>
<td id="S4.T2.1.1.4.4.9" class="ltx_td ltx_align_center">68.7</td>
</tr>
<tr id="S4.T2.1.1.5.5" class="ltx_tr">
<th id="S4.T2.1.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">PoseFlowÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</th>
<td id="S4.T2.1.1.5.5.2" class="ltx_td ltx_align_center">66.7</td>
<td id="S4.T2.1.1.5.5.3" class="ltx_td ltx_align_center">73.3</td>
<td id="S4.T2.1.1.5.5.4" class="ltx_td ltx_align_center">68.3</td>
<td id="S4.T2.1.1.5.5.5" class="ltx_td ltx_align_center">61.1</td>
<td id="S4.T2.1.1.5.5.6" class="ltx_td ltx_align_center">67.5</td>
<td id="S4.T2.1.1.5.5.7" class="ltx_td ltx_align_center">67.0</td>
<td id="S4.T2.1.1.5.5.8" class="ltx_td ltx_align_center ltx_border_r">61.3</td>
<td id="S4.T2.1.1.5.5.9" class="ltx_td ltx_align_center">66.5</td>
</tr>
<tr id="S4.T2.1.1.6.6" class="ltx_tr">
<th id="S4.T2.1.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">STAFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</th>
<td id="S4.T2.1.1.6.6.2" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.1.6.6.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.1.6.6.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.1.6.6.5" class="ltx_td ltx_align_center">65.0</td>
<td id="S4.T2.1.1.6.6.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.1.6.6.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.1.1.6.6.8" class="ltx_td ltx_align_center ltx_border_r">62.7</td>
<td id="S4.T2.1.1.6.6.9" class="ltx_td ltx_align_center">72.6</td>
</tr>
<tr id="S4.T2.1.1.7.7" class="ltx_tr">
<th id="S4.T2.1.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ST-EmbedÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>
</th>
<td id="S4.T2.1.1.7.7.2" class="ltx_td ltx_align_center">83.8</td>
<td id="S4.T2.1.1.7.7.3" class="ltx_td ltx_align_center">81.6</td>
<td id="S4.T2.1.1.7.7.4" class="ltx_td ltx_align_center">77.1</td>
<td id="S4.T2.1.1.7.7.5" class="ltx_td ltx_align_center">70.0</td>
<td id="S4.T2.1.1.7.7.6" class="ltx_td ltx_align_center">77.4</td>
<td id="S4.T2.1.1.7.7.7" class="ltx_td ltx_align_center">74.5</td>
<td id="S4.T2.1.1.7.7.8" class="ltx_td ltx_align_center ltx_border_r">70.8</td>
<td id="S4.T2.1.1.7.7.9" class="ltx_td ltx_align_center">77.0</td>
</tr>
<tr id="S4.T2.1.1.8.8" class="ltx_tr">
<th id="S4.T2.1.1.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">DATÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>
</th>
<td id="S4.T2.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_t">67.5</td>
<td id="S4.T2.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_t">70.2</td>
<td id="S4.T2.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_t">62.0</td>
<td id="S4.T2.1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_t">51.7</td>
<td id="S4.T2.1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_t">60.7</td>
<td id="S4.T2.1.1.8.8.7" class="ltx_td ltx_align_center ltx_border_t">58.7</td>
<td id="S4.T2.1.1.8.8.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">49.8</td>
<td id="S4.T2.1.1.8.8.9" class="ltx_td ltx_align_center ltx_border_t">60.6</td>
</tr>
<tr id="S4.T2.1.1.9.9" class="ltx_tr">
<th id="S4.T2.1.1.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">FlowTrackÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</th>
<td id="S4.T2.1.1.9.9.2" class="ltx_td ltx_align_center">81.7</td>
<td id="S4.T2.1.1.9.9.3" class="ltx_td ltx_align_center">83.4</td>
<td id="S4.T2.1.1.9.9.4" class="ltx_td ltx_align_center">80.0</td>
<td id="S4.T2.1.1.9.9.5" class="ltx_td ltx_align_center">72.4</td>
<td id="S4.T2.1.1.9.9.6" class="ltx_td ltx_align_center">75.3</td>
<td id="S4.T2.1.1.9.9.7" class="ltx_td ltx_align_center">74.8</td>
<td id="S4.T2.1.1.9.9.8" class="ltx_td ltx_align_center ltx_border_r">67.1</td>
<td id="S4.T2.1.1.9.9.9" class="ltx_td ltx_align_center">76.9</td>
</tr>
<tr id="S4.T2.1.1.10.10" class="ltx_tr">
<th id="S4.T2.1.1.10.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">TKMRNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>
</th>
<td id="S4.T2.1.1.10.10.2" class="ltx_td ltx_align_center">85.3</td>
<td id="S4.T2.1.1.10.10.3" class="ltx_td ltx_align_center">88.2</td>
<td id="S4.T2.1.1.10.10.4" class="ltx_td ltx_align_center">79.5</td>
<td id="S4.T2.1.1.10.10.5" class="ltx_td ltx_align_center">71.6</td>
<td id="S4.T2.1.1.10.10.6" class="ltx_td ltx_align_center">76.9</td>
<td id="S4.T2.1.1.10.10.7" class="ltx_td ltx_align_center">76.9</td>
<td id="S4.T2.1.1.10.10.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.1.10.10.8.1" class="ltx_text ltx_font_bold">73.1</span></td>
<td id="S4.T2.1.1.10.10.9" class="ltx_td ltx_align_center">79.5</td>
</tr>
<tr id="S4.T2.1.1.11.11" class="ltx_tr">
<th id="S4.T2.1.1.11.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">Ours</th>
<td id="S4.T2.1.1.11.11.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.1.1.11.11.2.1" class="ltx_text ltx_font_bold">88.4</span></td>
<td id="S4.T2.1.1.11.11.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.1.1.11.11.3.1" class="ltx_text ltx_font_bold">88.4</span></td>
<td id="S4.T2.1.1.11.11.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.1.1.11.11.4.1" class="ltx_text ltx_font_bold">82.0</span></td>
<td id="S4.T2.1.1.11.11.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.1.1.11.11.5.1" class="ltx_text ltx_font_bold">74.5</span></td>
<td id="S4.T2.1.1.11.11.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.1.1.11.11.6.1" class="ltx_text ltx_font_bold">79.1</span></td>
<td id="S4.T2.1.1.11.11.7" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.1.1.11.11.7.1" class="ltx_text ltx_font_bold">78.3</span></td>
<td id="S4.T2.1.1.11.11.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T2.1.1.11.11.8.1" class="ltx_text ltx_font_bold">73.1</span></td>
<td id="S4.T2.1.1.11.11.9" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.1.1.11.11.9.1" class="ltx_text ltx_font_bold">81.1</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison with state-of-the-art methods on human pose estimation (with filtering) on the
PoseTrack 2017 Validation set, where thresholds are used to filtering
low confidence joints for pose tracking. Evaluated in mAP and all joints are counted. </figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:296.9pt;height:153.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-60.6pt,31.3pt) scale(0.71,0.71) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Method</th>
<td id="S4.T3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t">Head</td>
<td id="S4.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t">Shou</td>
<td id="S4.T3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">Elb</td>
<td id="S4.T3.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">Wri</td>
<td id="S4.T3.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t">Hip</td>
<td id="S4.T3.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t">Knee</td>
<td id="S4.T3.1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Ankl</td>
<td id="S4.T3.1.1.1.1.9" class="ltx_td ltx_align_center ltx_border_t">Total</td>
</tr>
<tr id="S4.T3.1.1.2.2" class="ltx_tr">
<th id="S4.T3.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">BUTDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</th>
<td id="S4.T3.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">71.5</td>
<td id="S4.T3.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">70.3</td>
<td id="S4.T3.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">56.3</td>
<td id="S4.T3.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">45.1</td>
<td id="S4.T3.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t">55.5</td>
<td id="S4.T3.1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t">50.8</td>
<td id="S4.T3.1.1.2.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">37.5</td>
<td id="S4.T3.1.1.2.2.9" class="ltx_td ltx_align_center ltx_border_t">56.4</td>
</tr>
<tr id="S4.T3.1.1.3.3" class="ltx_tr">
<th id="S4.T3.1.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ArtTrackÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</th>
<td id="S4.T3.1.1.3.3.2" class="ltx_td ltx_align_center">66.2</td>
<td id="S4.T3.1.1.3.3.3" class="ltx_td ltx_align_center">64.2</td>
<td id="S4.T3.1.1.3.3.4" class="ltx_td ltx_align_center">53.2</td>
<td id="S4.T3.1.1.3.3.5" class="ltx_td ltx_align_center">43.7</td>
<td id="S4.T3.1.1.3.3.6" class="ltx_td ltx_align_center">53.0</td>
<td id="S4.T3.1.1.3.3.7" class="ltx_td ltx_align_center">51.6</td>
<td id="S4.T3.1.1.3.3.8" class="ltx_td ltx_align_center ltx_border_r">41.7</td>
<td id="S4.T3.1.1.3.3.9" class="ltx_td ltx_align_center">53.4</td>
</tr>
<tr id="S4.T3.1.1.4.4" class="ltx_tr">
<th id="S4.T3.1.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">PoseFlowÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</th>
<td id="S4.T3.1.1.4.4.2" class="ltx_td ltx_align_center">59.8</td>
<td id="S4.T3.1.1.4.4.3" class="ltx_td ltx_align_center">67.0</td>
<td id="S4.T3.1.1.4.4.4" class="ltx_td ltx_align_center">59.8</td>
<td id="S4.T3.1.1.4.4.5" class="ltx_td ltx_align_center">51.6</td>
<td id="S4.T3.1.1.4.4.6" class="ltx_td ltx_align_center">60.0</td>
<td id="S4.T3.1.1.4.4.7" class="ltx_td ltx_align_center">58.4</td>
<td id="S4.T3.1.1.4.4.8" class="ltx_td ltx_align_center ltx_border_r">50.5</td>
<td id="S4.T3.1.1.4.4.9" class="ltx_td ltx_align_center">58.3</td>
</tr>
<tr id="S4.T3.1.1.5.5" class="ltx_tr">
<th id="S4.T3.1.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">STAFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</th>
<td id="S4.T3.1.1.5.5.2" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.5.5.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.5.5.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.5.5.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.5.5.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.5.5.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.1.1.5.5.8" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T3.1.1.5.5.9" class="ltx_td ltx_align_center">62.7</td>
</tr>
<tr id="S4.T3.1.1.6.6" class="ltx_tr">
<th id="S4.T3.1.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ST-EmbedÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>
</th>
<td id="S4.T3.1.1.6.6.2" class="ltx_td ltx_align_center">78.7</td>
<td id="S4.T3.1.1.6.6.3" class="ltx_td ltx_align_center">79.2</td>
<td id="S4.T3.1.1.6.6.4" class="ltx_td ltx_align_center">71.2</td>
<td id="S4.T3.1.1.6.6.5" class="ltx_td ltx_align_center">61.1</td>
<td id="S4.T3.1.1.6.6.6" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.6.6.6.1" class="ltx_text ltx_font_bold">74.5</span></td>
<td id="S4.T3.1.1.6.6.7" class="ltx_td ltx_align_center">69.7</td>
<td id="S4.T3.1.1.6.6.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.1.1.6.6.8.1" class="ltx_text ltx_font_bold">64.5</span></td>
<td id="S4.T3.1.1.6.6.9" class="ltx_td ltx_align_center">71.8</td>
</tr>
<tr id="S4.T3.1.1.7.7" class="ltx_tr">
<th id="S4.T3.1.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">DATÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>
</th>
<td id="S4.T3.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_t">61.7</td>
<td id="S4.T3.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_t">65.5</td>
<td id="S4.T3.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_t">57.3</td>
<td id="S4.T3.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_t">45.7</td>
<td id="S4.T3.1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_t">54.3</td>
<td id="S4.T3.1.1.7.7.7" class="ltx_td ltx_align_center ltx_border_t">53.1</td>
<td id="S4.T3.1.1.7.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">45.7</td>
<td id="S4.T3.1.1.7.7.9" class="ltx_td ltx_align_center ltx_border_t">55.2</td>
</tr>
<tr id="S4.T3.1.1.8.8" class="ltx_tr">
<th id="S4.T3.1.1.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">FlowTrackÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</th>
<td id="S4.T3.1.1.8.8.2" class="ltx_td ltx_align_center">73.9</td>
<td id="S4.T3.1.1.8.8.3" class="ltx_td ltx_align_center">75.9</td>
<td id="S4.T3.1.1.8.8.4" class="ltx_td ltx_align_center">63.7</td>
<td id="S4.T3.1.1.8.8.5" class="ltx_td ltx_align_center">56.1</td>
<td id="S4.T3.1.1.8.8.6" class="ltx_td ltx_align_center">65.5</td>
<td id="S4.T3.1.1.8.8.7" class="ltx_td ltx_align_center">65.1</td>
<td id="S4.T3.1.1.8.8.8" class="ltx_td ltx_align_center ltx_border_r">53.5</td>
<td id="S4.T3.1.1.8.8.9" class="ltx_td ltx_align_center">65.4</td>
</tr>
<tr id="S4.T3.1.1.9.9" class="ltx_tr">
<th id="S4.T3.1.1.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">PGPTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</th>
<td id="S4.T3.1.1.9.9.2" class="ltx_td ltx_align_center">75.4</td>
<td id="S4.T3.1.1.9.9.3" class="ltx_td ltx_align_center">77.2</td>
<td id="S4.T3.1.1.9.9.4" class="ltx_td ltx_align_center">69.4</td>
<td id="S4.T3.1.1.9.9.5" class="ltx_td ltx_align_center">71.5</td>
<td id="S4.T3.1.1.9.9.6" class="ltx_td ltx_align_center">65.8</td>
<td id="S4.T3.1.1.9.9.7" class="ltx_td ltx_align_center">67.2</td>
<td id="S4.T3.1.1.9.9.8" class="ltx_td ltx_align_center ltx_border_r">59.0</td>
<td id="S4.T3.1.1.9.9.9" class="ltx_td ltx_align_center">68.4</td>
</tr>
<tr id="S4.T3.1.1.10.10" class="ltx_tr">
<th id="S4.T3.1.1.10.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">TKMRNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>
</th>
<td id="S4.T3.1.1.10.10.2" class="ltx_td ltx_align_center">81.0</td>
<td id="S4.T3.1.1.10.10.3" class="ltx_td ltx_align_center">82.9</td>
<td id="S4.T3.1.1.10.10.4" class="ltx_td ltx_align_center">69.8</td>
<td id="S4.T3.1.1.10.10.5" class="ltx_td ltx_align_center">63.6</td>
<td id="S4.T3.1.1.10.10.6" class="ltx_td ltx_align_center">72.0</td>
<td id="S4.T3.1.1.10.10.7" class="ltx_td ltx_align_center">71.1</td>
<td id="S4.T3.1.1.10.10.8" class="ltx_td ltx_align_center ltx_border_r">60.8</td>
<td id="S4.T3.1.1.10.10.9" class="ltx_td ltx_align_center">72.2</td>
</tr>
<tr id="S4.T3.1.1.11.11" class="ltx_tr">
<th id="S4.T3.1.1.11.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">CombDetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>
</th>
<td id="S4.T3.1.1.11.11.2" class="ltx_td ltx_align_center">80.5</td>
<td id="S4.T3.1.1.11.11.3" class="ltx_td ltx_align_center">80.9</td>
<td id="S4.T3.1.1.11.11.4" class="ltx_td ltx_align_center">71.6</td>
<td id="S4.T3.1.1.11.11.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.11.11.5.1" class="ltx_text ltx_font_bold">63.8</span></td>
<td id="S4.T3.1.1.11.11.6" class="ltx_td ltx_align_center">70.1</td>
<td id="S4.T3.1.1.11.11.7" class="ltx_td ltx_align_center">68.2</td>
<td id="S4.T3.1.1.11.11.8" class="ltx_td ltx_align_center ltx_border_r">62.0</td>
<td id="S4.T3.1.1.11.11.9" class="ltx_td ltx_align_center">71.6</td>
</tr>
<tr id="S4.T3.1.1.12.12" class="ltx_tr">
<th id="S4.T3.1.1.12.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">Ours</th>
<td id="S4.T3.1.1.12.12.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.1.1.12.12.2.1" class="ltx_text ltx_font_bold">82.0</span></td>
<td id="S4.T3.1.1.12.12.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.1.1.12.12.3.1" class="ltx_text ltx_font_bold">83.1</span></td>
<td id="S4.T3.1.1.12.12.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.1.1.12.12.4.1" class="ltx_text ltx_font_bold">73.4</span></td>
<td id="S4.T3.1.1.12.12.5" class="ltx_td ltx_align_center ltx_border_b">63.5</td>
<td id="S4.T3.1.1.12.12.6" class="ltx_td ltx_align_center ltx_border_b">72.3</td>
<td id="S4.T3.1.1.12.12.7" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.1.1.12.12.7.1" class="ltx_text ltx_font_bold">71.3</span></td>
<td id="S4.T3.1.1.12.12.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">63.5</td>
<td id="S4.T3.1.1.12.12.9" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.1.1.12.12.9.1" class="ltx_text ltx_font_bold">73.4</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of state-of-the-art methods on human pose tracking on the PoseTrack 2017 validation set. The performance is evaluated as MOTA and all joints are counted.</figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Results on PoseTrack 2018</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We show in TableÂ <a href="#S4.T4" title="Table 4 â€£ 4.4 Results on PoseTrack 2018 â€£ 4 Experiments â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, TableÂ <a href="#S4.T5" title="Table 5 â€£ 4.4 Results on PoseTrack 2018 â€£ 4 Experiments â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>
and TableÂ <a href="#S4.T6" title="Table 6 â€£ 4.4 Results on PoseTrack 2018 â€£ 4 Experiments â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> the comparison of our proposed method
and existing methods on the PoseTrack 2018
validation set. Again, our method achieves the best performances
in pure human pose estimation, pose estimation with filtering, and human pose tracking.
Specifically, as in TableÂ <a href="#S4.T4" title="Table 4 â€£ 4.4 Results on PoseTrack 2018 â€£ 4 Experiments â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the proposed method improves pure human pose estimation without filtering by 1.1 mAP over the state-of-the-art methodÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. As shown in TableÂ <a href="#S4.T5" title="Table 5 â€£ 4.4 Results on PoseTrack 2018 â€£ 4 Experiments â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, our method outperforms the best existing human pose estimationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> with filtering for pose tracking by 1.2 mAP.
And for human pose tracking, the proposed method
also achieves the state-of-the-art performance improving the MOTA by 0.3 overÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, as shown in TableÂ <a href="#S4.T6" title="Table 6 â€£ 4.4 Results on PoseTrack 2018 â€£ 4 Experiments â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">The superior performance on both PoseTrack 2017 and 2018 datasets in all three tasks (pure pose estimation in video, pose estimation with filtering, and pose tracking) validates the effectiveness of modeling dynamics by GNN.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<div id="S4.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:298.4pt;height:63.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-60.9pt,13.1pt) scale(0.71,0.71) ;">
<table id="S4.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Method</th>
<th id="S4.T4.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Head</th>
<th id="S4.T4.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Shou</th>
<th id="S4.T4.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Elb</th>
<th id="S4.T4.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Wri</th>
<th id="S4.T4.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Hip</th>
<th id="S4.T4.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Knee</th>
<th id="S4.T4.1.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Ankl</th>
<th id="S4.T4.1.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Total</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.1.2.1" class="ltx_tr">
<th id="S4.T4.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">PT_CPN++Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>
</th>
<td id="S4.T4.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">82.4</td>
<td id="S4.T4.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">88.8</td>
<td id="S4.T4.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">86.2</td>
<td id="S4.T4.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">79.4</td>
<td id="S4.T4.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">72.0</td>
<td id="S4.T4.1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">80.6</td>
<td id="S4.T4.1.1.2.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">76.2</td>
<td id="S4.T4.1.1.2.1.9" class="ltx_td ltx_align_center ltx_border_t">80.9</td>
</tr>
<tr id="S4.T4.1.1.3.2" class="ltx_tr">
<th id="S4.T4.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">KeyTrackÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</th>
<td id="S4.T4.1.1.3.2.2" class="ltx_td ltx_align_center">84.1</td>
<td id="S4.T4.1.1.3.2.3" class="ltx_td ltx_align_center">87.2</td>
<td id="S4.T4.1.1.3.2.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.3.2.4.1" class="ltx_text ltx_font_bold">85.3</span></td>
<td id="S4.T4.1.1.3.2.5" class="ltx_td ltx_align_center">79.2</td>
<td id="S4.T4.1.1.3.2.6" class="ltx_td ltx_align_center">77.1</td>
<td id="S4.T4.1.1.3.2.7" class="ltx_td ltx_align_center">80.6</td>
<td id="S4.T4.1.1.3.2.8" class="ltx_td ltx_align_center ltx_border_r">76.5</td>
<td id="S4.T4.1.1.3.2.9" class="ltx_td ltx_align_center">81.6</td>
</tr>
<tr id="S4.T4.1.1.4.3" class="ltx_tr">
<th id="S4.T4.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">CombDetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>
</th>
<td id="S4.T4.1.1.4.3.2" class="ltx_td ltx_align_center">84.9</td>
<td id="S4.T4.1.1.4.3.3" class="ltx_td ltx_align_center">87.4</td>
<td id="S4.T4.1.1.4.3.4" class="ltx_td ltx_align_center">84.8</td>
<td id="S4.T4.1.1.4.3.5" class="ltx_td ltx_align_center">79.2</td>
<td id="S4.T4.1.1.4.3.6" class="ltx_td ltx_align_center">77.6</td>
<td id="S4.T4.1.1.4.3.7" class="ltx_td ltx_align_center">79.7</td>
<td id="S4.T4.1.1.4.3.8" class="ltx_td ltx_align_center ltx_border_r">75.3</td>
<td id="S4.T4.1.1.4.3.9" class="ltx_td ltx_align_center">81.5</td>
</tr>
<tr id="S4.T4.1.1.5.4" class="ltx_tr">
<th id="S4.T4.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">Ours</th>
<td id="S4.T4.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.1.1.5.4.2.1" class="ltx_text ltx_font_bold">85.1</span></td>
<td id="S4.T4.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.1.1.5.4.3.1" class="ltx_text ltx_font_bold">87.7</span></td>
<td id="S4.T4.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.1.1.5.4.4.1" class="ltx_text ltx_font_bold">85.3</span></td>
<td id="S4.T4.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.1.1.5.4.5.1" class="ltx_text ltx_font_bold">80.0</span></td>
<td id="S4.T4.1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.1.1.5.4.6.1" class="ltx_text ltx_font_bold">81.1</span></td>
<td id="S4.T4.1.1.5.4.7" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.1.1.5.4.7.1" class="ltx_text ltx_font_bold">81.6</span></td>
<td id="S4.T4.1.1.5.4.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T4.1.1.5.4.8.1" class="ltx_text ltx_font_bold">77.2</span></td>
<td id="S4.T4.1.1.5.4.9" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.1.1.5.4.9.1" class="ltx_text ltx_font_bold">82.7</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparison of state-of-the-art methods on pure human pose estimation (without filtering)
on the validation set of PoseTrack 2018. Evaluated in mAP and all joints are counted.</figcaption>
</figure>
<figure id="S4.T5" class="ltx_table">
<div id="S4.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:282.4pt;height:63.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-57.7pt,13.1pt) scale(0.71,0.71) ;">
<table id="S4.T5.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.1.1.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Method</th>
<th id="S4.T5.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Head</th>
<th id="S4.T5.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Shou</th>
<th id="S4.T5.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Elb</th>
<th id="S4.T5.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Wri</th>
<th id="S4.T5.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Hip</th>
<th id="S4.T5.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Knee</th>
<th id="S4.T5.1.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Ankl</th>
<th id="S4.T5.1.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Total</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.1.1.2.1" class="ltx_tr">
<th id="S4.T5.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">STAFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</th>
<td id="S4.T5.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T5.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T5.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T5.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">64.7</td>
<td id="S4.T5.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T5.1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T5.1.1.2.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">62.0</td>
<td id="S4.T5.1.1.2.1.9" class="ltx_td ltx_align_center ltx_border_t">70.4</td>
</tr>
<tr id="S4.T5.1.1.3.2" class="ltx_tr">
<th id="S4.T5.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">TML++Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</th>
<td id="S4.T5.1.1.3.2.2" class="ltx_td ltx_align_center">-</td>
<td id="S4.T5.1.1.3.2.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T5.1.1.3.2.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T5.1.1.3.2.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T5.1.1.3.2.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T5.1.1.3.2.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T5.1.1.3.2.8" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T5.1.1.3.2.9" class="ltx_td ltx_align_center">74.6</td>
</tr>
<tr id="S4.T5.1.1.4.3" class="ltx_tr">
<th id="S4.T5.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">TKMRNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>
</th>
<td id="S4.T5.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T5.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T5.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T5.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T5.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T5.1.1.4.3.7" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T5.1.1.4.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T5.1.1.4.3.9" class="ltx_td ltx_align_center ltx_border_t">76.7</td>
</tr>
<tr id="S4.T5.1.1.5.4" class="ltx_tr">
<th id="S4.T5.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">Ours</th>
<td id="S4.T5.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b">80.6</td>
<td id="S4.T5.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b">84.5</td>
<td id="S4.T5.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_b">80.6</td>
<td id="S4.T5.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_b">74.4</td>
<td id="S4.T5.1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_b">75.0</td>
<td id="S4.T5.1.1.5.4.7" class="ltx_td ltx_align_center ltx_border_b">76.7</td>
<td id="S4.T5.1.1.5.4.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">71.9</td>
<td id="S4.T5.1.1.5.4.9" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.1.1.5.4.9.1" class="ltx_text ltx_font_bold">77.9</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Comparison of state-of-the-art methods on human pose estimation (with filtering)
on the PoseTrack 2018 validation set, where thresholds are used to filtering
low confidence joints for pose tracking.</figcaption>
</figure>
<figure id="S4.T6" class="ltx_table">
<div id="S4.T6.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:298.4pt;height:102.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-60.9pt,20.9pt) scale(0.71,0.71) ;">
<table id="S4.T6.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T6.1.1.1.1" class="ltx_tr">
<th id="S4.T6.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Method</th>
<td id="S4.T6.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t">Head</td>
<td id="S4.T6.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t">Shou</td>
<td id="S4.T6.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">Elb</td>
<td id="S4.T6.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">Wri</td>
<td id="S4.T6.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t">Hip</td>
<td id="S4.T6.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t">Knee</td>
<td id="S4.T6.1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Ankl</td>
<td id="S4.T6.1.1.1.1.9" class="ltx_td ltx_align_center ltx_border_t">Total</td>
</tr>
<tr id="S4.T6.1.1.2.2" class="ltx_tr">
<th id="S4.T6.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">STAFÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</th>
<td id="S4.T6.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T6.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T6.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T6.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T6.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T6.1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T6.1.1.2.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T6.1.1.2.2.9" class="ltx_td ltx_align_center ltx_border_t">60.9</td>
</tr>
<tr id="S4.T6.1.1.3.3" class="ltx_tr">
<th id="S4.T6.1.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">TML++Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</th>
<td id="S4.T6.1.1.3.3.2" class="ltx_td ltx_align_center"><span id="S4.T6.1.1.3.3.2.1" class="ltx_text ltx_font_bold">76.0</span></td>
<td id="S4.T6.1.1.3.3.3" class="ltx_td ltx_align_center">76.9</td>
<td id="S4.T6.1.1.3.3.4" class="ltx_td ltx_align_center">66.1</td>
<td id="S4.T6.1.1.3.3.5" class="ltx_td ltx_align_center">56.4</td>
<td id="S4.T6.1.1.3.3.6" class="ltx_td ltx_align_center">65.1</td>
<td id="S4.T6.1.1.3.3.7" class="ltx_td ltx_align_center">61.6</td>
<td id="S4.T6.1.1.3.3.8" class="ltx_td ltx_align_center ltx_border_r">52.4</td>
<td id="S4.T6.1.1.3.3.9" class="ltx_td ltx_align_center">65.7</td>
</tr>
<tr id="S4.T6.1.1.4.4" class="ltx_tr">
<th id="S4.T6.1.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">PT_CPN++Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>
</th>
<td id="S4.T6.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_t">68.8</td>
<td id="S4.T6.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t">73.5</td>
<td id="S4.T6.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t">65.6</td>
<td id="S4.T6.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t">61.2</td>
<td id="S4.T6.1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_t">54.9</td>
<td id="S4.T6.1.1.4.4.7" class="ltx_td ltx_align_center ltx_border_t">64.6</td>
<td id="S4.T6.1.1.4.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">56.7</td>
<td id="S4.T6.1.1.4.4.9" class="ltx_td ltx_align_center ltx_border_t">64.0</td>
</tr>
<tr id="S4.T6.1.1.5.5" class="ltx_tr">
<th id="S4.T6.1.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">TKMRNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>
</th>
<td id="S4.T6.1.1.5.5.2" class="ltx_td ltx_align_center">-</td>
<td id="S4.T6.1.1.5.5.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T6.1.1.5.5.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T6.1.1.5.5.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T6.1.1.5.5.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T6.1.1.5.5.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T6.1.1.5.5.8" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T6.1.1.5.5.9" class="ltx_td ltx_align_center">68.9</td>
</tr>
<tr id="S4.T6.1.1.6.6" class="ltx_tr">
<th id="S4.T6.1.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">KeyTrackÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</th>
<td id="S4.T6.1.1.6.6.2" class="ltx_td ltx_align_center">-</td>
<td id="S4.T6.1.1.6.6.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T6.1.1.6.6.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T6.1.1.6.6.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T6.1.1.6.6.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T6.1.1.6.6.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T6.1.1.6.6.8" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T6.1.1.6.6.9" class="ltx_td ltx_align_center">66.6</td>
</tr>
<tr id="S4.T6.1.1.7.7" class="ltx_tr">
<th id="S4.T6.1.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">CombDetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>
</th>
<td id="S4.T6.1.1.7.7.2" class="ltx_td ltx_align_center">74.2</td>
<td id="S4.T6.1.1.7.7.3" class="ltx_td ltx_align_center">76.4</td>
<td id="S4.T6.1.1.7.7.4" class="ltx_td ltx_align_center">71.2</td>
<td id="S4.T6.1.1.7.7.5" class="ltx_td ltx_align_center">64.1</td>
<td id="S4.T6.1.1.7.7.6" class="ltx_td ltx_align_center">64.5</td>
<td id="S4.T6.1.1.7.7.7" class="ltx_td ltx_align_center">65.8</td>
<td id="S4.T6.1.1.7.7.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T6.1.1.7.7.8.1" class="ltx_text ltx_font_bold">61.9</span></td>
<td id="S4.T6.1.1.7.7.9" class="ltx_td ltx_align_center">68.7</td>
</tr>
<tr id="S4.T6.1.1.8.8" class="ltx_tr">
<th id="S4.T6.1.1.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">Ours</th>
<td id="S4.T6.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_b">74.3</td>
<td id="S4.T6.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T6.1.1.8.8.3.1" class="ltx_text ltx_font_bold">77.6</span></td>
<td id="S4.T6.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T6.1.1.8.8.4.1" class="ltx_text ltx_font_bold">71.4</span></td>
<td id="S4.T6.1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T6.1.1.8.8.5.1" class="ltx_text ltx_font_bold">64.3</span></td>
<td id="S4.T6.1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T6.1.1.8.8.6.1" class="ltx_text ltx_font_bold">65.6</span></td>
<td id="S4.T6.1.1.8.8.7" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T6.1.1.8.8.7.1" class="ltx_text ltx_font_bold">66.7</span></td>
<td id="S4.T6.1.1.8.8.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">61.7</td>
<td id="S4.T6.1.1.8.8.9" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T6.1.1.8.8.9.1" class="ltx_text ltx_font_bold">69.2</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Comparison of state-of-the-art methods on human pose tracking on the PoseTrack 2018 validation set. Evaluated in MOTA and all joints are counted.</figcaption>
</figure>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Model Analysis</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">We provide here analyses on the proposed method,
including ablation
studies, visualization of the attentions among
joints learnt from the GNN model,
and sensitively
analysis of the memory length and GNN model size.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2106.03772/assets/Figs/samples.jpg" id="S4.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="287" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Qualitative examples of the proposed method on
the PoseTrack 2017 validation set. The first three columns show
the poses in the memory, the fourth column shows the estimated poses
from HRNet, and the last column shows the final poses of our proposed method.
Red dot circles highlight the incorrect or missed poses that are corrected.</figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure">
<div id="S4.F5.1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:392.0pt;height:161.6pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-21.8pt,8.9pt) scale(0.9,0.9) ;"><img src="/html/2106.03772/assets/x4.png" id="S4.F5.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="190" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Visualization of the attention among different joints within
the GNN model.
Red nodes are the centers for aggregation and the colors of lines
indicate the attention values. We zoom the current frame for a better visualization.</figcaption>
</figure>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p"><span id="S4.SS5.p2.1.1" class="ltx_text ltx_font_bold">Ablation study.</span> We examine the effectiveness of the proposed method by conducting ablation experiments on several key components. As shown in Table.<a href="#S4.T7" title="Table 7 â€£ 4.5 Model Analysis â€£ 4 Experiments â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>,
Matching w/ IOU and Matching w/ OKS means we
associate the estimated poses between consecutive frames
using the IOU and OKS as the similarity measure. Matching w/ GNN means we only use the predicted poses for matching measure, and the final poses are not refined by the predicted poses.
Full model is our proposed model.
It can be seen that using the predicted poses for matching metric
can improve the MOTA performance and reduce the switch rate over IOU and OKS metrics,
the full model with pose refinement by pose merging can improve both the mAP and MOTA
further more.</p>
</div>
<figure id="S4.T7" class="ltx_table">
<div id="S4.T7.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:255.1pt;height:68.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-40.3pt,10.8pt) scale(0.76,0.76) ;">
<table id="S4.T7.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T7.1.1.1.1" class="ltx_tr">
<th id="S4.T7.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Method</th>
<th id="S4.T7.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAP</th>
<th id="S4.T7.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">MOTA</th>
<th id="S4.T7.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">MissÂ (%)</th>
<th id="S4.T7.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">SwitchÂ (%)</th>
<th id="S4.T7.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">FPÂ (%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T7.1.1.2.1" class="ltx_tr">
<td id="S4.T7.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Matching w/ IOU</td>
<td id="S4.T7.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">79.9</td>
<td id="S4.T7.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">71.8</td>
<td id="S4.T7.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">17.4</td>
<td id="S4.T7.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">1.8</td>
<td id="S4.T7.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">9.0</td>
</tr>
<tr id="S4.T7.1.1.3.2" class="ltx_tr">
<td id="S4.T7.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r">Matching w/ OKS</td>
<td id="S4.T7.1.1.3.2.2" class="ltx_td ltx_align_center">79.9</td>
<td id="S4.T7.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">72.1</td>
<td id="S4.T7.1.1.3.2.4" class="ltx_td ltx_align_center">17.4</td>
<td id="S4.T7.1.1.3.2.5" class="ltx_td ltx_align_center">1.6</td>
<td id="S4.T7.1.1.3.2.6" class="ltx_td ltx_align_center">8.9</td>
</tr>
<tr id="S4.T7.1.1.4.3" class="ltx_tr">
<td id="S4.T7.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_r">Matching w/ GNN</td>
<td id="S4.T7.1.1.4.3.2" class="ltx_td ltx_align_center">79.9</td>
<td id="S4.T7.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">73.1</td>
<td id="S4.T7.1.1.4.3.4" class="ltx_td ltx_align_center">17.1</td>
<td id="S4.T7.1.1.4.3.5" class="ltx_td ltx_align_center">1.4</td>
<td id="S4.T7.1.1.4.3.6" class="ltx_td ltx_align_center">8.4</td>
</tr>
<tr id="S4.T7.1.1.5.4" class="ltx_tr">
<td id="S4.T7.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Full model</td>
<td id="S4.T7.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b">81.1</td>
<td id="S4.T7.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">73.4</td>
<td id="S4.T7.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_b">16.9</td>
<td id="S4.T7.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_b">1.3</td>
<td id="S4.T7.1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_b">8.4</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Ablation studies on the PoseTrack 2017 validation set, where Miss, Switch, FP stand for the missing rate, switch rate and false positive rate (the lower the better) in MOTA.</figcaption>
</figure>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.1" class="ltx_p"><span id="S4.SS5.p3.1.1" class="ltx_text ltx_font_bold">Visualization of GNN model.</span>
In order to provide a thorough understanding of the GNN model, we visualize in FigureÂ <a href="#S4.F5" title="Figure 5 â€£ 4.5 Model Analysis â€£ 4 Experiments â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> the computed attention weights <math id="S4.SS5.p3.1.m1.1" class="ltx_Math" alttext="\alpha_{kk^{\prime}}" display="inline"><semantics id="S4.SS5.p3.1.m1.1a"><msub id="S4.SS5.p3.1.m1.1.1" xref="S4.SS5.p3.1.m1.1.1.cmml"><mi id="S4.SS5.p3.1.m1.1.1.2" xref="S4.SS5.p3.1.m1.1.1.2.cmml">Î±</mi><mrow id="S4.SS5.p3.1.m1.1.1.3" xref="S4.SS5.p3.1.m1.1.1.3.cmml"><mi id="S4.SS5.p3.1.m1.1.1.3.2" xref="S4.SS5.p3.1.m1.1.1.3.2.cmml">k</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p3.1.m1.1.1.3.1" xref="S4.SS5.p3.1.m1.1.1.3.1.cmml">â€‹</mo><msup id="S4.SS5.p3.1.m1.1.1.3.3" xref="S4.SS5.p3.1.m1.1.1.3.3.cmml"><mi id="S4.SS5.p3.1.m1.1.1.3.3.2" xref="S4.SS5.p3.1.m1.1.1.3.3.2.cmml">k</mi><mo id="S4.SS5.p3.1.m1.1.1.3.3.3" xref="S4.SS5.p3.1.m1.1.1.3.3.3.cmml">â€²</mo></msup></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.p3.1.m1.1b"><apply id="S4.SS5.p3.1.m1.1.1.cmml" xref="S4.SS5.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS5.p3.1.m1.1.1.1.cmml" xref="S4.SS5.p3.1.m1.1.1">subscript</csymbol><ci id="S4.SS5.p3.1.m1.1.1.2.cmml" xref="S4.SS5.p3.1.m1.1.1.2">ğ›¼</ci><apply id="S4.SS5.p3.1.m1.1.1.3.cmml" xref="S4.SS5.p3.1.m1.1.1.3"><times id="S4.SS5.p3.1.m1.1.1.3.1.cmml" xref="S4.SS5.p3.1.m1.1.1.3.1"></times><ci id="S4.SS5.p3.1.m1.1.1.3.2.cmml" xref="S4.SS5.p3.1.m1.1.1.3.2">ğ‘˜</ci><apply id="S4.SS5.p3.1.m1.1.1.3.3.cmml" xref="S4.SS5.p3.1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS5.p3.1.m1.1.1.3.3.1.cmml" xref="S4.SS5.p3.1.m1.1.1.3.3">superscript</csymbol><ci id="S4.SS5.p3.1.m1.1.1.3.3.2.cmml" xref="S4.SS5.p3.1.m1.1.1.3.3.2">ğ‘˜</ci><ci id="S4.SS5.p3.1.m1.1.1.3.3.3.cmml" xref="S4.SS5.p3.1.m1.1.1.3.3.3">â€²</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p3.1.m1.1c">\alpha_{kk^{\prime}}</annotation></semantics></math>
as computed in EquationÂ <a href="#S3.E5" title="In 3.2.3 Joint aggregation â€£ 3.2 Dynamics Modeling via GNN â€£ 3 Method â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
It can be observed that the hip in current frame is influenced
by the hip, shoulder, and knee in the consecutive pose mostly.
The ankle in the middle frame is influence mostly by the lower part of the previous pose.</p>
</div>
<div id="S4.SS5.p4" class="ltx_para">
<p id="S4.SS5.p4.1" class="ltx_p"><span id="S4.SS5.p4.1.1" class="ltx_text ltx_font_bold">Length of memory tube and model size.</span>
In TableÂ <a href="#S4.T8" title="Table 8 â€£ 4.5 Model Analysis â€£ 4 Experiments â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> we show the results with
different lengths of memory and different model size,
where smaller model means the dimension of the output of
<math id="S4.SS5.p4.1.m1.1" class="ltx_Math" alttext="\mathbf{MLP}_{*}" display="inline"><semantics id="S4.SS5.p4.1.m1.1a"><msub id="S4.SS5.p4.1.m1.1.1" xref="S4.SS5.p4.1.m1.1.1.cmml"><mi id="S4.SS5.p4.1.m1.1.1.2" xref="S4.SS5.p4.1.m1.1.1.2.cmml">ğŒğ‹ğ</mi><mo id="S4.SS5.p4.1.m1.1.1.3" xref="S4.SS5.p4.1.m1.1.1.3.cmml">âˆ—</mo></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.p4.1.m1.1b"><apply id="S4.SS5.p4.1.m1.1.1.cmml" xref="S4.SS5.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS5.p4.1.m1.1.1.1.cmml" xref="S4.SS5.p4.1.m1.1.1">subscript</csymbol><ci id="S4.SS5.p4.1.m1.1.1.2.cmml" xref="S4.SS5.p4.1.m1.1.1.2">ğŒğ‹ğ</ci><times id="S4.SS5.p4.1.m1.1.1.3.cmml" xref="S4.SS5.p4.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p4.1.m1.1c">\mathbf{MLP}_{*}</annotation></semantics></math> (as in EquationÂ <a href="#S3.E3" title="In 3.2.1 Nodes in the proposed GNN model â€£ 3.2 Dynamics Modeling via GNN â€£ 3 Method â€£ Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) is halved.
It can be seen that the performance is improved when changing the
memory length from two to four frames and being saturated when using more memory.
Enlarging the model size improves both mAP and MOTA.</p>
</div>
<figure id="S4.T8" class="ltx_table">
<div id="S4.T8.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:242.8pt;height:69.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-36.3pt,10.4pt) scale(0.77,0.77) ;">
<table id="S4.T8.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T8.1.1.1.1" class="ltx_tr">
<th id="S4.T8.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Method</th>
<th id="S4.T8.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAP</th>
<th id="S4.T8.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">MOTA</th>
<th id="S4.T8.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">MissÂ (%)</th>
<th id="S4.T8.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">SwitchÂ (%)</th>
<th id="S4.T8.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">FPÂ (%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T8.1.1.2.1" class="ltx_tr">
<th id="S4.T8.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Two frames</th>
<td id="S4.T8.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">80.6</td>
<td id="S4.T8.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">72.9</td>
<td id="S4.T8.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">17.2</td>
<td id="S4.T8.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">1.4</td>
<td id="S4.T8.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">8.5</td>
</tr>
<tr id="S4.T8.1.1.3.2" class="ltx_tr">
<th id="S4.T8.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Four frames</th>
<td id="S4.T8.1.1.3.2.2" class="ltx_td ltx_align_center">81.3</td>
<td id="S4.T8.1.1.3.2.3" class="ltx_td ltx_align_center">73.4</td>
<td id="S4.T8.1.1.3.2.4" class="ltx_td ltx_align_center">16.9</td>
<td id="S4.T8.1.1.3.2.5" class="ltx_td ltx_align_center">1.3</td>
<td id="S4.T8.1.1.3.2.6" class="ltx_td ltx_align_center">8.4</td>
</tr>
<tr id="S4.T8.1.1.4.3" class="ltx_tr">
<th id="S4.T8.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Smaller model</th>
<td id="S4.T8.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_t">80.8</td>
<td id="S4.T8.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t">73.2</td>
<td id="S4.T8.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_t">17.1</td>
<td id="S4.T8.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_t">1.3</td>
<td id="S4.T8.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_t">8.4</td>
</tr>
<tr id="S4.T8.1.1.5.4" class="ltx_tr">
<th id="S4.T8.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">Full model</th>
<td id="S4.T8.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">81.1</td>
<td id="S4.T8.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">73.4</td>
<td id="S4.T8.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">16.9</td>
<td id="S4.T8.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">1.3</td>
<td id="S4.T8.1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">8.4</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Influence of model capacity and length of memory.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We present in this paper a novel
approach for human pose estimation and tracking.
In our method, a GNN model is designed to
explicitly model the dynamics of the pose tracklets and predict
the corresponding poses in an incoming frame,
independent of the estimations.
When combining with the human pose estimation model,
the proposed method
takes advantages of
both the visual information
and the dynamics, thereby
enabling the recovery of missed poses
and refinement of estimated poses.
Extensive experiments on PoseTrack
2017 and PoseTrack 2018 datasets
validate the superiority of the proposed method in both
human pose estimation and human pose tracking tasks.
In our future work, we would like to explore
a more flexible manner to aggregate the
predicted results and the new observation,
making the whole pipeline even more adaptive.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Mykhaylo Andriluka, Umar Iqbal, Eldar Insafutdinov, Leonid Pishchulin, Anton
Milan, Juergen Gall, and Bernt Schiele.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Posetrack: A benchmark for human pose estimation and tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, pages 5167â€“5176, 2018.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Qian Bao, Wu Liu, Yuhao Cheng, Boyan Zhou, and Tao Mei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Pose-guided tracking-by-detection: Robust multi-person pose tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Multimedia</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Gedas Bertasius, Christoph Feichtenhofer, Du Tran, Jianbo Shi, and Lorenzo
Torresani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Learning temporal pose estimation from sparsely-labeled videos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, pages
3027â€“3038, 2019.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Yanrui Bin, Zhao-Min Chen, Xiu-Shen Wei, Xinya Chen, Changxin Gao, and Nong
Sang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Structure-aware human pose estimation with graph convolutional
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Pattern Recognition</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, page 107410, 2020.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Navaneeth Bodla, Bharat Singh, Rama Chellappa, and LarryÂ S Davis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Soft-nmsâ€“improving object detection with one line of code.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 5561â€“5569, 2017.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Realtime multi-person 2d pose estimation using part affinity fields.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages 7291â€“7299, 2017.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun,
Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Hybrid task cascade for instance segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, pages 4974â€“4983, 2019.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Bowen Cheng, Bin Xiao, Jingdong Wang, Honghui Shi, ThomasÂ S Huang, and Lei
Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Higherhrnet: Scale-aware representation learning for bottom-up human
pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, pages 5386â€“5395, 2020.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Ke Cheng, Yifan Zhang, Xiangyu He, Weihan Chen, Jian Cheng, and Hanqing Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Skeleton-based action recognition with shift graph convolutional
network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages 183â€“192, 2020.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Bert: Pre-training of deep bidirectional transformers for language
understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1810.04805</span><span id="bib.bib10.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Rmpe: Regional multi-person pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages 2334â€“2343, 2017.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Matthias Fey, JanÂ E. Lenssen, Frank Weichert, and Heinrich MÃ¼ller.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">SplineCNN: Fast geometric deep learning with continuous B-spline
kernels.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Junyu Gao, Tianzhu Zhang, and Changsheng Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Graph convolutional tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages 4649â€“4659, 2019.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Justin Gilmer, SamuelÂ S Schoenholz, PatrickÂ F Riley, Oriol Vinyals, and
GeorgeÂ E Dahl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Neural message passing for quantum chemistry.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Rohit Girdhar, Georgia Gkioxari, Lorenzo Torresani, Manohar Paluri, and Du
Tran.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Detect-and-track: Efficient pose estimation in videos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, pages 350â€“359, 2018.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Jihye Hwang, Jieun Lee, Sungheon Park, and Nojun Kwak.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Pose estimator and tracker using temporal flow maps for limbs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2019 International Joint Conference on Neural Networks
(IJCNN)</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages 1â€“8. IEEE, 2019.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Eldar Insafutdinov, Leonid Pishchulin, Bjoern Andres, Mykhaylo Andriluka, and
Bernt Schiele.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Deepercut: A deeper, stronger, and faster multi-person pose
estimation model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, pages 34â€“50.
Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Sheng Jin, Wentao Liu, Wanli Ouyang, and Chen Qian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Multi-person articulated tracking with spatial and temporal
embeddings.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, pages 5664â€“5673, 2019.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Sheng Jin, Xujie Ma, Zhipeng Han, Yue Wu, Wei Yang, Wentao Liu, Chen Qian, and
Wanli Ouyang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Towards multi-person pose tracking: Bottom-up and top-down methods.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV PoseTrack Workshop</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, volumeÂ 2, pageÂ 7, 2017.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
ThomasÂ N Kipf and Max Welling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Semi-supervised classification with graph convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1609.02907</span><span id="bib.bib20.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Long Lan, Xinchao Wang, Gang Hua, ThomasÂ S. Huang, and Dacheng Tao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Semi-online multi-people tracking by re-identification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal on Computer Vision</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">, 128:1937â€“1955, 2020.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr DollÃ¡r, and CÂ Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, pages 740â€“755.
Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Ziyu Liu, Hongwen Zhang, Zhenghao Chen, Zhiyong Wang, and Wanli Ouyang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Disentangling and unifying graph convolutions for skeleton-based
action recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, pages 143â€“152, 2020.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Andrii Maksai, Xinchao Wang, Francois Fleuret, and Pascal Fua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Non-markovian globally consistent multi-object tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Computer Vision</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Andrii Maksai, Xinchao Wang, and Pascal Fua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">What players do with the ball: A physically constrained interaction
modeling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Gonzalo Mena, David Belanger, Gonzalo Munoz, and Jasper Snoek.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Sinkhorn networks: Using optimal transport techniques to learn
permutations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS Workshop in Optimal Transport and Machine Learning</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">,
2017.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Alejandro Newell, Zhiao Huang, and Jia Deng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Associative embedding: End-to-end learning for joint detection and
grouping.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, pages
2277â€“2287, 2017.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Tomas Pfister, James Charles, and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Flowing convnets for human pose estimation in videos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, pages 1913â€“1921, 2015.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Leonid Pishchulin, Eldar Insafutdinov, Siyu Tang, Bjoern Andres, Mykhaylo
Andriluka, PeterÂ V Gehler, and Bernt Schiele.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Deepcut: Joint subset partition and labeling for multi person pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, pages 4929â€“4937, 2016.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Jiayan Qiu, Yiding Yang, Xinchao Wang, and Dacheng Tao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Hallucinating visual instances in total absentia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Zhongwei Qiu, Kai Qiu, Jianlong Fu, and Dongmei Fu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Dgcn: Dynamic graph convolutional network for efficient multi-person
pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, pages 11924â€“11931, 2020.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Yaadhav Raaj, Haroon Idrees, Gines Hidalgo, and Yaser Sheikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Efficient online multi-person 2d pose tracking with recurrent
spatio-temporal affinity fields.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, pages 4620â€“4628, 2019.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Faster r-cnn: Towards real-time object detection with region proposal
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</span><span id="bib.bib33.4.2" class="ltx_text" style="font-size:90%;">,
39(6):1137â€“1149, 2016.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Matteo RuggeroÂ Ronchi and Pietro Perona.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Benchmarking and error diagnosis in multi-instance pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, pages 369â€“378, 2017.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Chenyang Si, Wentao Chen, Wei Wang, Liang Wang, and Tieniu Tan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">An attention enhanced graph convolutional lstm network for
skeleton-based action recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, pages 1227â€“1236, 2019.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Michael Snower, Asim Kadav, Farley Lai, and HansÂ Peter Graf.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">15 keypoints is all you need.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, pages 6738â€“6748, 2020.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Deep high-resolution representation learning for human pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, pages 5693â€“5703, 2019.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
AidanÂ N Gomez, Åukasz Kaiser, and Illia Polosukhin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Attention is all you need.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, pages
5998â€“6008, 2017.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Jue Wang, Shaoli Huang, Xinchao Wang, and Dacheng Tao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Not all parts are created equal: 3d pose estimation by modelling
bi-directional dependencies of body parts.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Computer Vision</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Manchen Wang, Joseph Tighe, and Davide Modolo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Combining detection and tracking for human pose estimation in videos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, pages 11088â€“11096, 2020.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Rui Wang, Chenyang Huang, and Xiangyang Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Global relation reasoning graph convolutional networks for human pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Access</span><span id="bib.bib41.4.2" class="ltx_text" style="font-size:90%;">, 8:38472â€“38480, 2020.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Xinchao Wang, Engin Turetken, Francois Fleuret, and Pascal Fua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Tracking interacting objects optimally using integer programming.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Xinchao Wang, Engin Turetken, Francois Fleuret, and Pascal Fua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Tracking interacting objects using intertwined flows.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib43.4.2" class="ltx_text" style="font-size:90%;">,
38:2312â€“2326, 2016.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Yue Wang, Yongbin Sun, Ziwei Liu, SanjayÂ E. Sarma, MichaelÂ M. Bronstein, and
JustinÂ M. Solomon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Dynamic graph cnn for learning on point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM Transactions on Graphics (TOG)</span><span id="bib.bib44.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Bin Xiao, Haiping Wu, and Yichen Wei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Simple baselines for human pose estimation and tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European conference on computer vision
(ECCV)</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, pages 466â€“481, 2018.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Yuliang Xiu, Jiefeng Li, Haoyu Wang, Yinghong Fang, and Cewu Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Pose flow: Efficient online pose tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1802.00977</span><span id="bib.bib46.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Yiding Yang, Zunlei Feng, Mingli Song, and Xinchao Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Factorizable graph convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib47.4.2" class="ltx_text" style="font-size:90%;">, 33, 2020.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Yiding Yang, Jiayan Qiu, Mingli Song, Dacheng Tao, and Xinchao Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Distilling knowledge from graph convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, pages 7074â€“7083, 2020.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Yiding Yang, Xinchao Wang, Mingli Song, Junsong Yuan, and Dacheng Tao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Spagan: Shortest path graph attention network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the Twenty-Eighth International Joint
Conference on Artificial Intelligence, IJCAI-19</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, pages 4099â€“4105.
International Joint Conferences on Artificial Intelligence Organization, 7
2019.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Dongdong Yu, Kai Su, Jia Sun, and Changhu Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Multi-person pose estimation for pose tracking with enhanced cascaded
pyramid network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV)</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">, pages 0â€“0, 2018.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Li Zhang, Dan Xu, Anurag Arnab, and PhilipÂ HS Torr.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Dynamic graph message passing networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib51.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib51.5.3" class="ltx_text" style="font-size:90%;">, pages 3726â€“3735, 2020.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Muhan Zhang and Yixin Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Link prediction based on graph neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib52.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib52.5.3" class="ltx_text" style="font-size:90%;">, pages
5165â€“5175, 2018.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Chunluan Zhou, Zhou Ren, and Gang Hua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Temporal keypoint matching and refinement network for pose estimation
and tracking.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Deformable convnets v2: More deformable, better results.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib54.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib54.5.3" class="ltx_text" style="font-size:90%;">, pages 9308â€“9316, 2019.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Xiangyu Zhu, Yingying Jiang, and Zhenbo Luo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">Multi-person pose estimation for posetrack with enhanced part
affinity fields.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib55.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV PoseTrack Workshop</span><span id="bib.bib55.5.3" class="ltx_text" style="font-size:90%;">, volumeÂ 7, 2017.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2106.03771" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2106.03772" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2106.03772">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2106.03772" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2106.03773" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  9 02:16:53 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
