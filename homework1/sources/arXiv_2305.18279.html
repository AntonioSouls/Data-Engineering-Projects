<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2305.18279] Contextual Object Detection with Multimodal Large Language Models</title><meta property="og:description" content="Recent Multimodal Large Language Models (MLLMs) are remarkable in vision-language tasks, such as image captioning and question answering, but lack the essential perception ability, i.e., object detection. In this work,â€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Contextual Object Detection with Multimodal Large Language Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Contextual Object Detection with Multimodal Large Language Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2305.18279">

<!--Generated on Thu Feb 29 04:31:43 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Contextual Object Detection 
<br class="ltx_break">with Multimodal Large Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, Chen Change Loy<sup id="id3.3.id1" class="ltx_sup">ğŸ–‚</sup>

<br class="ltx_break">
S-Lab, Nanyang Technological University


<br class="ltx_break"><math id="id1.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="id1.1.m1.1a"><mo maxsize="90%" minsize="90%" id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><ci id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\{</annotation></semantics></math><span id="id2.2.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">zang0012, wei.l, hanj0030, kaiyang.zhou, ccloy<math id="id2.2.1.m1.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="id2.2.1.m1.1a"><mo stretchy="false" id="id2.2.1.m1.1.1" xref="id2.2.1.m1.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="id2.2.1.m1.1b"><ci id="id2.2.1.m1.1.1.cmml" xref="id2.2.1.m1.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="id2.2.1.m1.1c">\}</annotation></semantics></math>@ntu.edu.sg</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">Recent Multimodal Large Language Models (MLLMs) are remarkable in vision-language tasks, such as image captioning and question answering, but lack the essential perception ability, <span id="id4.id1.1" class="ltx_text ltx_font_italic">i.e.</span>, object detection. In this work, we address this limitation by introducing a novel research problem of <span id="id4.id1.2" class="ltx_text ltx_font_italic">contextual object detection</span>â€”understanding visible objects within different human-AI interactive contexts. Three representative scenarios are investigated, including the language cloze test, visual captioning, and question answering. Moreover, we present ContextDET, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction.
Our ContextDET involves three key submodels: (i) a visual encoder for extracting visual representations, (ii) a pre-trained LLM for multimodal context decoding, and (iii) a visual decoder for predicting bounding boxes given contextual object words. The new <span id="id4.id1.3" class="ltx_text ltx_font_italic">generate-then-detect</span> framework enables us to detect object words within human vocabulary. Extensive experiments show the advantages of ContextDET on our proposed CODE benchmark, open-vocabulary detection, and referring image segmentation.
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Github: <a target="_blank" href="https://github.com/yuhangzang/ContextDET" title="" class="ltx_ref ltx_href">https://github.com/yuhangzang/ContextDET</a>.</span></span></span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">â€œ<em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">For me context is the key - from that comes the understanding of everything.</em>â€</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">â€”Â Kenneth Noland</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">One indispensable cornerstone of computer visionâ€”object detectionâ€”is understanding visible objects within scenes, which empowers many applications, such as robotics, autonomous driving, and AR/VR systems. Recently, Multi-modal Language Models (MLLMs) trained with internet-scale visual-language data, including FlamingoÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, PaLM-EÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, and the superb OpenAIâ€™s GPT-4Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, have shown a revolutionary ability to allow humans to interact with AI models for various vision-language tasks, <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">e.g.</span>, image captioning and question answering.
Such an interactive human-AI circumstance requires modeling <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">contextual</span> information, <span id="S1.p3.1.3" class="ltx_text ltx_font_italic">i.e.</span>, relationships among visual objects, human words, phrases, and even dialogues. Therefore, it is desirable to advance MLLMs with the capability of locating, identifying, and associating visual objects with language inputs for human-AI interaction.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we study a new research problemâ€”contextual object detectionâ€”that is understanding visible objects within human-AI interactive contexts. In comparison with existing standard object detection, we consider four comprehensive objectives for such a new setting: (i) <span id="S1.p4.1.1" class="ltx_text ltx_font_bold ltx_font_italic">capacity</span>: being able to handle a human language vocabulary; (ii) <span id="S1.p4.1.2" class="ltx_text ltx_font_bold ltx_font_italic">description</span>: describing visual inputs from users with informative natural language statements; (iii) <span id="S1.p4.1.3" class="ltx_text ltx_font_bold ltx_font_italic">perception</span>: locating and associating visual objects with language queries; (iv) <span id="S1.p4.1.4" class="ltx_text ltx_font_bold ltx_font_italic">understanding</span>: complementing proper words with language hints. To cover these four objectives, we incorporate three representative tasks: language cloze test, visual captioning, and question answering, with object detection for MLLMs (see FigureÂ <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2305.18279/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="452" height="293" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S1.F1.2.1" class="ltx_text" style="font-size:90%;">We present a new <span id="S1.F1.2.1.1" class="ltx_text ltx_font_bold">contextual object detection</span> task include <span id="S1.F1.2.1.2" class="ltx_text ltx_font_bold">(a)</span> look at the image and complete the masked object names and locations; <span id="S1.F1.2.1.3" class="ltx_text ltx_font_bold">(b)</span> predict the caption and the boxes of objects existing in the caption; <span id="S1.F1.2.1.4" class="ltx_text ltx_font_bold">(c)</span> answer a question about the names and locations of objects.
Unlike the traditional object detection task that typically focuses on detecting a limited set of pre-defined object classes such as â€˜<span id="S1.F1.2.1.5" class="ltx_text" style="color:#9999FF;">person</span>â€™, our task requires predicting more specific names (<span id="S1.F1.2.1.6" class="ltx_text ltx_font_italic">e.g.</span>, â€˜<span id="S1.F1.2.1.7" class="ltx_text" style="color:#FA6180;">hockey goalie</span>â€™, â€˜<span id="S1.F1.2.1.8" class="ltx_text" style="color:#13E31A;">groom</span>â€™, or â€˜<span id="S1.F1.2.1.9" class="ltx_text" style="color:#FF8C1A;">bride</span>â€™) based on contextual understanding.
</span></figcaption>
</figure>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">While significant progress has been made in developing more accurate and faster object detection algorithms, it is impossible to directly integrate existing deep object detectors with MLLMs for contextual object detection, due to the following reasons. First, standard deep detectors, such as Mask-RCNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and DETRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, are trained with close-set classifiers and cannot generalize well in real-world scenarios, where object categories or classes are not pre-defined or limited to a closed set. Despite the very recent development of open-vocabulary object detectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib87" title="" class="ltx_ref">87</a>, <a href="#bib.bib81" title="" class="ltx_ref">81</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> that builds on state-of-the-art vision-language models (e.g., CLIPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> and ALIGNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>) can improve the zero-shot transfer ability for novel classes, they are constrained by the scale of pre-defined novel categories, making them incapable of detecting objects for a human language vocabulary. For example, these open-vocabulary detectors fail to handle out-of-distributed categories in FigureÂ <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, such as hockey goalie, groom, and cowboy. Second, the inherent <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">locate-then-classify</span> paradigm of existing deep detection models is unsuitable for contextual object detection. In generic human-AI interactive scenarios, both natural objects in visual scenes and human words in language inputs have various meanings in different contexts. In FigureÂ <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (a) and (b), the universal â€˜personâ€™ category will manifest as â€˜goalieâ€™, â€˜playerâ€™, â€˜cowboyâ€™, â€˜groomâ€™, â€˜brideâ€™, and â€˜workerâ€™ within distinct visual contexts. Also, as language contexts shift, the word â€˜labradorâ€™ supplants the representation of â€˜dogâ€™ (FigureÂ <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (c)).
Consequently, an innovative detection approach is required to cater to considerably varied and changing contextual object detection.
</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">To address the above challenges, in this work, we present ContextDET, a novel <span id="S1.p6.1.1" class="ltx_text ltx_font_italic">generate-then-detect</span> framework, specialized for contextual object detection. Specifically, it is an end-to-end model that consists of three key modules. First, a visual encoder extracts high-level image representations for given images and produces both local and full visual tokens for further contextual modeling. Second, to effectively model multimodal contexts, we employ a pre-trained LLM to perform text generation, with conditioned inputs of both local visual tokens and task-related language tokens as the multimodal prefix. Third, taking the LLM tokens as prior knowledge for visual detection, we introduce a visual decoder that consists of multiple cross-attention layers, within which we compute conditional object queries from contextual LLM tokens, and keys and values from full visual tokens, to predict the corresponding matching scores and bounding boxes. This allows us to detect contextual object words for a human vocabulary.</p>
</div>
<div id="S1.p7" class="ltx_para ltx_noindent">
<p id="S1.p7.1" class="ltx_p"><span id="S1.p7.1.1" class="ltx_text ltx_font_bold">Contributions.</span> In summary, our contributions are the following: (<span id="S1.p7.1.2" class="ltx_text ltx_font_bold">i</span>) We for the first time investigate contextual object detectionâ€”a new direction for visual object detection that improves MLLMs with a greater ability for human-AI interaction. (<span id="S1.p7.1.3" class="ltx_text ltx_font_bold">ii</span>) To open this area to empirical study, we present a new benchmark CODE with 10,346 unique object words to facilitate research on contextual object detection. (<span id="S1.p7.1.4" class="ltx_text ltx_font_bold">iii</span>) We propose a novel <span id="S1.p7.1.5" class="ltx_text ltx_font_italic">generate-then-detect</span> framework, ContextDET, dedicated to contextual object detection. (<span id="S1.p7.1.6" class="ltx_text ltx_font_bold">iv</span>) We demonstrate the advantages of our ContextDET not only on the CODE benchmark but also on open-vocabulary detection and referring image segmentation tasks. We hope our work can motivate future research in contextual object detection that benefits human-AI interaction.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Multimodal Large Language Models (MLLMs).</span> Large Language Models (LLMs) have been developed to comprehend and generate textual language, showcasing remarkable performance across a wide range of Natural Language Processing (NLP) tasks. Notable examples of LLMs include OpenAIâ€™s GPT series <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, Googleâ€™s T5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> and PaLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, as well as Metaâ€™s OPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> and LLaMA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>. More recently, there have been advancements in the field of MLLMs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, exemplified by the GPT-4 model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, which have expanded the capabilities of LLMs to comprehend both language and visual inputs. MLLMs have demonstrated impressive proficiency in a range of vision-language tasks, including image captioning and visual question answering. However, existing MLLMs are limited to generating textual outputs. In contrast, our ContextDET, built upon MLLMs, extends support to contextual object detection, providing bounding box outputs. Further comparisons are discussed in SectionÂ <a href="#S4.SS4" title="4.4 Qualitative Results â€£ 4 Experiments â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Prompting LLMs with Vision Experts</span>. Several recent papersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>, <a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite> have proposed systems that leverage the textual output generated by LLMs, such as ChatGPTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, as prompts to manipulate external vision expert models for various vision-related tasks. In the context of object detection, these vision expert models include DETRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, Grounding DINOÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, SAMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, and other algorithms integrated into the HuggingFace communityÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. However, due to the frozen parameters of both LLMs and expert models, the knowledge and representations from LLMs cannot be shared, potentially leading to sub-optimal performance. In contrast to these prompting-based methods, our ContextDET employs an end-to-end training pipeline. We utilize the latent features extracted from MLLMs as conditional inputs for a visual decoder, enabling the prediction of bounding boxes.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Object Detection with Contextual Understanding</span>. The term â€œcontextâ€ commonly refers to the neighboring pixels or surrounding regions within images and has been extensively explored in previous studies to enhance object detection algorithmsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. In this paper, the concept of contextual information encompasses multimodal patterns and relationships, involving both visual images and textual words. Our ContextDET leverages the robust contextual understanding capability of MLLMs and applies it to the downstream object detection task. Additionally, we propose the adoption of new evaluation tasks, such as the cloze test, to more effectively assess the contextual understanding ability.</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Object Detection on Novel Classes</span>. Despite significant advancements in deep learning techniquesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib83" title="" class="ltx_ref">83</a>, <a href="#bib.bib89" title="" class="ltx_ref">89</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>, object detection remains a challenging task in real-world scenarios, particularly in the case of zero-shot object detectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Zero-shot object detection requires models trained on <em id="S2.p4.1.2" class="ltx_emph ltx_font_italic">base</em> classes to detect <em id="S2.p4.1.3" class="ltx_emph ltx_font_italic">novel</em> classes that were not encountered during training. A recent variant of zero-shot detection, known as Open-Vocabulary Object Detection, allows for the utilization of additional image-text pairsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>, garnering significant attention from the research community. In this context, recent vision and language pre-trained modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib86" title="" class="ltx_ref">86</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>, such as CLIP, have been widely employed for open-vocabulary object detectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib87" title="" class="ltx_ref">87</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib81" title="" class="ltx_ref">81</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib74" title="" class="ltx_ref">74</a>, <a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>. Instead of relying solely on CLIP, our ContextDET demonstrates that MLLMs can also be applied effectively to the open-vocabulary setting. With the assistance of MLLMs, ContextDET is not constrained by pre-defined <em id="S2.p4.1.4" class="ltx_emph ltx_font_italic">base</em> or <em id="S2.p4.1.5" class="ltx_emph ltx_font_italic">novel</em> classes. Notably, the object names predicted by ContextDET can be generated as the most contextually valid English words by the MLLMs.</p>
</div>
<div id="S2.p5" class="ltx_para ltx_noindent">
<p id="S2.p5.1" class="ltx_p"><span id="S2.p5.1.1" class="ltx_text ltx_font_bold">Visual Grounding</span>. Visual grounding tasks, such as referring expression comprehensionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, involve combining object detection with language understanding abilities. In these tasks, a language query is provided to describe a specific object, and models are tasked with predicting the position of the referred object. State-of-the-art algorithmsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>, <a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite> commonly employ Transformer-based cross-modal structures or multimodal pre-trainingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Our proposed contextual object detection task presents even greater challenges compared to visual grounding. For example, in our cloze test, the language query is incomplete, and the object names are masked. Models are required to infer both the missing object name words and their positions based on contextual information. Furthermore, in our contextual captioning setting, no language query is given. Additionally, in our contextual QA setting, the objects are described using human language in an <em id="S2.p5.1.2" class="ltx_emph ltx_font_italic">interactive</em> environment.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Approach</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2305.18279/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="243" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span> <span id="S3.F2.6.3" class="ltx_text" style="font-size:90%;">Our ContextDET is a unified end-to-end framework, being capable of taking different language token inputs for different tasks, including <span id="S3.F2.6.3.1" class="ltx_text ltx_font_bold">(a)</span> cloze test <span id="S3.F2.6.3.2" class="ltx_text ltx_font_bold">(b)</span> captioning and <span id="S3.F2.6.3.3" class="ltx_text ltx_font_bold">(c)</span> question answering.
<img src="/html/2305.18279/assets/figures/snowflake.png" id="S3.F2.4.1.g1" class="ltx_graphics ltx_img_square" width="14" height="14" alt="Refer to caption">: frozen.
The symbol <math id="S3.F2.5.2.m1.1" class="ltx_Math" alttext="{\bm{e}}" display="inline"><semantics id="S3.F2.5.2.m1.1b"><mi id="S3.F2.5.2.m1.1.1" xref="S3.F2.5.2.m1.1.1.cmml">ğ’†</mi><annotation-xml encoding="MathML-Content" id="S3.F2.5.2.m1.1c"><ci id="S3.F2.5.2.m1.1.1.cmml" xref="S3.F2.5.2.m1.1.1">ğ’†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.5.2.m1.1d">{\bm{e}}</annotation></semantics></math> indicates latent embeddings of LLM (SectionÂ <a href="#S3.SS2" title="3.2 Multimodal Context Modeling with LLM â€£ 3 Approach â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>), and the symbol <math id="S3.F2.6.3.m2.1" class="ltx_Math" alttext="{\bm{q}}" display="inline"><semantics id="S3.F2.6.3.m2.1b"><mi id="S3.F2.6.3.m2.1.1" xref="S3.F2.6.3.m2.1.1.cmml">ğ’’</mi><annotation-xml encoding="MathML-Content" id="S3.F2.6.3.m2.1c"><ci id="S3.F2.6.3.m2.1.1.cmml" xref="S3.F2.6.3.m2.1.1">ğ’’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.6.3.m2.1d">{\bm{q}}</annotation></semantics></math> denotes object queries of the visual decoder (SectionÂ <a href="#S3.SS3" title="3.3 Visual Decoder â€£ 3 Approach â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>).
</span></figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This section describes our contextual object detection framework, ContextDET, which accepts images interleaved with human text as inputs and produces free-form text and corresponding bounding boxes as outputs. As illustrated in FigureÂ <a href="#S3.F2" title="Figure 2 â€£ 3 Approach â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, our ContextDET is end-to-end and consists of three key architectural components: (1) a visual encoder that extracts high-level image representations and computes visual tokens, (2) a pre-trained LLM that decodes multimodal contextual tokens with a task-related multimodal prefix, and (3) a visual decoder that predicts matching scores and bounding boxes for conditional queries linked to contextual object words.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Visual Encoder</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.6" class="ltx_p">Given an image input <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="{\bm{x}}\in\mathbb{R}^{3\times H\times W}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">ğ’™</mi><mo id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.3.2" xref="S3.SS1.p1.1.m1.1.1.3.2.cmml">â„</mi><mrow id="S3.SS1.p1.1.m1.1.1.3.3" xref="S3.SS1.p1.1.m1.1.1.3.3.cmml"><mn id="S3.SS1.p1.1.m1.1.1.3.3.2" xref="S3.SS1.p1.1.m1.1.1.3.3.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.1.m1.1.1.3.3.1" xref="S3.SS1.p1.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.SS1.p1.1.m1.1.1.3.3.3" xref="S3.SS1.p1.1.m1.1.1.3.3.3.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.1.m1.1.1.3.3.1a" xref="S3.SS1.p1.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.SS1.p1.1.m1.1.1.3.3.4" xref="S3.SS1.p1.1.m1.1.1.3.3.4.cmml">W</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><in id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></in><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">ğ’™</ci><apply id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3.2">â„</ci><apply id="S3.SS1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3"><times id="S3.SS1.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.1"></times><cn type="integer" id="S3.SS1.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.2">3</cn><ci id="S3.SS1.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.3">ğ»</ci><ci id="S3.SS1.p1.1.m1.1.1.3.3.4.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.4">ğ‘Š</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">{\bm{x}}\in\mathbb{R}^{3\times H\times W}</annotation></semantics></math>, we use a vision backbone parameterized by <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="\phi" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">Ï•</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">italic-Ï•</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\phi</annotation></semantics></math> to extract image-level spatial features <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="{\bm{v}}=f_{\phi}({\bm{x}})\in\mathbb{R}^{d\times h\times w}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.2" xref="S3.SS1.p1.3.m3.1.2.cmml"><mi id="S3.SS1.p1.3.m3.1.2.2" xref="S3.SS1.p1.3.m3.1.2.2.cmml">ğ’—</mi><mo id="S3.SS1.p1.3.m3.1.2.3" xref="S3.SS1.p1.3.m3.1.2.3.cmml">=</mo><mrow id="S3.SS1.p1.3.m3.1.2.4" xref="S3.SS1.p1.3.m3.1.2.4.cmml"><msub id="S3.SS1.p1.3.m3.1.2.4.2" xref="S3.SS1.p1.3.m3.1.2.4.2.cmml"><mi id="S3.SS1.p1.3.m3.1.2.4.2.2" xref="S3.SS1.p1.3.m3.1.2.4.2.2.cmml">f</mi><mi id="S3.SS1.p1.3.m3.1.2.4.2.3" xref="S3.SS1.p1.3.m3.1.2.4.2.3.cmml">Ï•</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p1.3.m3.1.2.4.1" xref="S3.SS1.p1.3.m3.1.2.4.1.cmml">â€‹</mo><mrow id="S3.SS1.p1.3.m3.1.2.4.3.2" xref="S3.SS1.p1.3.m3.1.2.4.cmml"><mo stretchy="false" id="S3.SS1.p1.3.m3.1.2.4.3.2.1" xref="S3.SS1.p1.3.m3.1.2.4.cmml">(</mo><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">ğ’™</mi><mo stretchy="false" id="S3.SS1.p1.3.m3.1.2.4.3.2.2" xref="S3.SS1.p1.3.m3.1.2.4.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p1.3.m3.1.2.5" xref="S3.SS1.p1.3.m3.1.2.5.cmml">âˆˆ</mo><msup id="S3.SS1.p1.3.m3.1.2.6" xref="S3.SS1.p1.3.m3.1.2.6.cmml"><mi id="S3.SS1.p1.3.m3.1.2.6.2" xref="S3.SS1.p1.3.m3.1.2.6.2.cmml">â„</mi><mrow id="S3.SS1.p1.3.m3.1.2.6.3" xref="S3.SS1.p1.3.m3.1.2.6.3.cmml"><mi id="S3.SS1.p1.3.m3.1.2.6.3.2" xref="S3.SS1.p1.3.m3.1.2.6.3.2.cmml">d</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.3.m3.1.2.6.3.1" xref="S3.SS1.p1.3.m3.1.2.6.3.1.cmml">Ã—</mo><mi id="S3.SS1.p1.3.m3.1.2.6.3.3" xref="S3.SS1.p1.3.m3.1.2.6.3.3.cmml">h</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.3.m3.1.2.6.3.1a" xref="S3.SS1.p1.3.m3.1.2.6.3.1.cmml">Ã—</mo><mi id="S3.SS1.p1.3.m3.1.2.6.3.4" xref="S3.SS1.p1.3.m3.1.2.6.3.4.cmml">w</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.2.cmml" xref="S3.SS1.p1.3.m3.1.2"><and id="S3.SS1.p1.3.m3.1.2a.cmml" xref="S3.SS1.p1.3.m3.1.2"></and><apply id="S3.SS1.p1.3.m3.1.2b.cmml" xref="S3.SS1.p1.3.m3.1.2"><eq id="S3.SS1.p1.3.m3.1.2.3.cmml" xref="S3.SS1.p1.3.m3.1.2.3"></eq><ci id="S3.SS1.p1.3.m3.1.2.2.cmml" xref="S3.SS1.p1.3.m3.1.2.2">ğ’—</ci><apply id="S3.SS1.p1.3.m3.1.2.4.cmml" xref="S3.SS1.p1.3.m3.1.2.4"><times id="S3.SS1.p1.3.m3.1.2.4.1.cmml" xref="S3.SS1.p1.3.m3.1.2.4.1"></times><apply id="S3.SS1.p1.3.m3.1.2.4.2.cmml" xref="S3.SS1.p1.3.m3.1.2.4.2"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.2.4.2.1.cmml" xref="S3.SS1.p1.3.m3.1.2.4.2">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.2.4.2.2.cmml" xref="S3.SS1.p1.3.m3.1.2.4.2.2">ğ‘“</ci><ci id="S3.SS1.p1.3.m3.1.2.4.2.3.cmml" xref="S3.SS1.p1.3.m3.1.2.4.2.3">italic-Ï•</ci></apply><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">ğ’™</ci></apply></apply><apply id="S3.SS1.p1.3.m3.1.2c.cmml" xref="S3.SS1.p1.3.m3.1.2"><in id="S3.SS1.p1.3.m3.1.2.5.cmml" xref="S3.SS1.p1.3.m3.1.2.5"></in><share href="#S3.SS1.p1.3.m3.1.2.4.cmml" id="S3.SS1.p1.3.m3.1.2d.cmml" xref="S3.SS1.p1.3.m3.1.2"></share><apply id="S3.SS1.p1.3.m3.1.2.6.cmml" xref="S3.SS1.p1.3.m3.1.2.6"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.2.6.1.cmml" xref="S3.SS1.p1.3.m3.1.2.6">superscript</csymbol><ci id="S3.SS1.p1.3.m3.1.2.6.2.cmml" xref="S3.SS1.p1.3.m3.1.2.6.2">â„</ci><apply id="S3.SS1.p1.3.m3.1.2.6.3.cmml" xref="S3.SS1.p1.3.m3.1.2.6.3"><times id="S3.SS1.p1.3.m3.1.2.6.3.1.cmml" xref="S3.SS1.p1.3.m3.1.2.6.3.1"></times><ci id="S3.SS1.p1.3.m3.1.2.6.3.2.cmml" xref="S3.SS1.p1.3.m3.1.2.6.3.2">ğ‘‘</ci><ci id="S3.SS1.p1.3.m3.1.2.6.3.3.cmml" xref="S3.SS1.p1.3.m3.1.2.6.3.3">â„</ci><ci id="S3.SS1.p1.3.m3.1.2.6.3.4.cmml" xref="S3.SS1.p1.3.m3.1.2.6.3.4">ğ‘¤</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">{\bm{v}}=f_{\phi}({\bm{x}})\in\mathbb{R}^{d\times h\times w}</annotation></semantics></math>, where <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">d</annotation></semantics></math> denotes the feature dimension. The vision backbone <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="\phi" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">Ï•</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">italic-Ï•</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">\phi</annotation></semantics></math> is pre-trained and frozen, which can be selected from various options, including ResNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, Vision TransformerÂ (ViT)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, or Swin TransformerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>.
Subsequently, the image-level features <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="{\bm{v}}" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><mi id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml">ğ’—</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">ğ’—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">{\bm{v}}</annotation></semantics></math> are transformed into two distinct representations.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.4" class="ltx_p"><span id="S3.SS1.p2.4.1" class="ltx_text ltx_font_bold">Local Visual Tokens.</span> We first divide the 2D spatial grid of features as <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">p</annotation></semantics></math> local bins and apply adaptive average pooling for each bin, followed by a linear projection then flattened to 1D: <math id="S3.SS1.p2.2.m2.2" class="ltx_Math" alttext="{\bm{z}}=\texttt{Linear}({\texttt{AvgPool}({\bm{v}})})" display="inline"><semantics id="S3.SS1.p2.2.m2.2a"><mrow id="S3.SS1.p2.2.m2.2.2" xref="S3.SS1.p2.2.m2.2.2.cmml"><mi id="S3.SS1.p2.2.m2.2.2.3" xref="S3.SS1.p2.2.m2.2.2.3.cmml">ğ’›</mi><mo id="S3.SS1.p2.2.m2.2.2.2" xref="S3.SS1.p2.2.m2.2.2.2.cmml">=</mo><mrow id="S3.SS1.p2.2.m2.2.2.1" xref="S3.SS1.p2.2.m2.2.2.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.SS1.p2.2.m2.2.2.1.3" xref="S3.SS1.p2.2.m2.2.2.1.3a.cmml">Linear</mtext><mo lspace="0em" rspace="0em" id="S3.SS1.p2.2.m2.2.2.1.2" xref="S3.SS1.p2.2.m2.2.2.1.2.cmml">â€‹</mo><mrow id="S3.SS1.p2.2.m2.2.2.1.1.1" xref="S3.SS1.p2.2.m2.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p2.2.m2.2.2.1.1.1.2" xref="S3.SS1.p2.2.m2.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p2.2.m2.2.2.1.1.1.1" xref="S3.SS1.p2.2.m2.2.2.1.1.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.SS1.p2.2.m2.2.2.1.1.1.1.2" xref="S3.SS1.p2.2.m2.2.2.1.1.1.1.2a.cmml">AvgPool</mtext><mo lspace="0em" rspace="0em" id="S3.SS1.p2.2.m2.2.2.1.1.1.1.1" xref="S3.SS1.p2.2.m2.2.2.1.1.1.1.1.cmml">â€‹</mo><mrow id="S3.SS1.p2.2.m2.2.2.1.1.1.1.3.2" xref="S3.SS1.p2.2.m2.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p2.2.m2.2.2.1.1.1.1.3.2.1" xref="S3.SS1.p2.2.m2.2.2.1.1.1.1.cmml">(</mo><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">ğ’—</mi><mo stretchy="false" id="S3.SS1.p2.2.m2.2.2.1.1.1.1.3.2.2" xref="S3.SS1.p2.2.m2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.SS1.p2.2.m2.2.2.1.1.1.3" xref="S3.SS1.p2.2.m2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.2b"><apply id="S3.SS1.p2.2.m2.2.2.cmml" xref="S3.SS1.p2.2.m2.2.2"><eq id="S3.SS1.p2.2.m2.2.2.2.cmml" xref="S3.SS1.p2.2.m2.2.2.2"></eq><ci id="S3.SS1.p2.2.m2.2.2.3.cmml" xref="S3.SS1.p2.2.m2.2.2.3">ğ’›</ci><apply id="S3.SS1.p2.2.m2.2.2.1.cmml" xref="S3.SS1.p2.2.m2.2.2.1"><times id="S3.SS1.p2.2.m2.2.2.1.2.cmml" xref="S3.SS1.p2.2.m2.2.2.1.2"></times><ci id="S3.SS1.p2.2.m2.2.2.1.3a.cmml" xref="S3.SS1.p2.2.m2.2.2.1.3"><mtext class="ltx_mathvariant_monospace" id="S3.SS1.p2.2.m2.2.2.1.3.cmml" xref="S3.SS1.p2.2.m2.2.2.1.3">Linear</mtext></ci><apply id="S3.SS1.p2.2.m2.2.2.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.1"><times id="S3.SS1.p2.2.m2.2.2.1.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.1.1.1"></times><ci id="S3.SS1.p2.2.m2.2.2.1.1.1.1.2a.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.1.1.2"><mtext class="ltx_mathvariant_monospace" id="S3.SS1.p2.2.m2.2.2.1.1.1.1.2.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.1.1.2">AvgPool</mtext></ci><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">ğ’—</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.2c">{\bm{z}}=\texttt{Linear}({\texttt{AvgPool}({\bm{v}})})</annotation></semantics></math>. As a result, fixed-sized visual tokens <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="{\bm{z}}\in\mathbb{R}^{d_{1}\times p}" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mrow id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">ğ’›</mi><mo id="S3.SS1.p2.3.m3.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml"><mi id="S3.SS1.p2.3.m3.1.1.3.2" xref="S3.SS1.p2.3.m3.1.1.3.2.cmml">â„</mi><mrow id="S3.SS1.p2.3.m3.1.1.3.3" xref="S3.SS1.p2.3.m3.1.1.3.3.cmml"><msub id="S3.SS1.p2.3.m3.1.1.3.3.2" xref="S3.SS1.p2.3.m3.1.1.3.3.2.cmml"><mi id="S3.SS1.p2.3.m3.1.1.3.3.2.2" xref="S3.SS1.p2.3.m3.1.1.3.3.2.2.cmml">d</mi><mn id="S3.SS1.p2.3.m3.1.1.3.3.2.3" xref="S3.SS1.p2.3.m3.1.1.3.3.2.3.cmml">1</mn></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.3.m3.1.1.3.3.1" xref="S3.SS1.p2.3.m3.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.SS1.p2.3.m3.1.1.3.3.3" xref="S3.SS1.p2.3.m3.1.1.3.3.3.cmml">p</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><in id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1"></in><ci id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2">ğ’›</ci><apply id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.3.1.cmml" xref="S3.SS1.p2.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.3.2.cmml" xref="S3.SS1.p2.3.m3.1.1.3.2">â„</ci><apply id="S3.SS1.p2.3.m3.1.1.3.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3"><times id="S3.SS1.p2.3.m3.1.1.3.3.1.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3.1"></times><apply id="S3.SS1.p2.3.m3.1.1.3.3.2.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.3.3.2.1.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3.2">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.3.3.2.2.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3.2.2">ğ‘‘</ci><cn type="integer" id="S3.SS1.p2.3.m3.1.1.3.3.2.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3.2.3">1</cn></apply><ci id="S3.SS1.p2.3.m3.1.1.3.3.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3.3">ğ‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">{\bm{z}}\in\mathbb{R}^{d_{1}\times p}</annotation></semantics></math> are obtained and fed to the LLM (SectionÂ <a href="#S3.SS2" title="3.2 Multimodal Context Modeling with LLM â€£ 3 Approach â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>), Here, <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="d_{1}" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><msub id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">d</mi><mn id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">ğ‘‘</ci><cn type="integer" id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">d_{1}</annotation></semantics></math> represents the input dimension of LLM.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.4" class="ltx_p"><span id="S3.SS1.p3.4.1" class="ltx_text ltx_font_bold">Full Visual Tokens.</span> We flatten the 2D spatial features <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="{\bm{v}}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mi id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">ğ’—</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">ğ’—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">{\bm{v}}</annotation></semantics></math> as 1D sequence with <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="m=h\times w" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mrow id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mi id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml">m</mi><mo id="S3.SS1.p3.2.m2.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.cmml">=</mo><mrow id="S3.SS1.p3.2.m2.1.1.3" xref="S3.SS1.p3.2.m2.1.1.3.cmml"><mi id="S3.SS1.p3.2.m2.1.1.3.2" xref="S3.SS1.p3.2.m2.1.1.3.2.cmml">h</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.2.m2.1.1.3.1" xref="S3.SS1.p3.2.m2.1.1.3.1.cmml">Ã—</mo><mi id="S3.SS1.p3.2.m2.1.1.3.3" xref="S3.SS1.p3.2.m2.1.1.3.3.cmml">w</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><eq id="S3.SS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1"></eq><ci id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2">ğ‘š</ci><apply id="S3.SS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3"><times id="S3.SS1.p3.2.m2.1.1.3.1.cmml" xref="S3.SS1.p3.2.m2.1.1.3.1"></times><ci id="S3.SS1.p3.2.m2.1.1.3.2.cmml" xref="S3.SS1.p3.2.m2.1.1.3.2">â„</ci><ci id="S3.SS1.p3.2.m2.1.1.3.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3.3">ğ‘¤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">m=h\times w</annotation></semantics></math> tokens and leverage six Transformer layers <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="\psi" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><mi id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml">Ïˆ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><ci id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">ğœ“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">\psi</annotation></semantics></math> to compute the encoded full visual tokens: <math id="S3.SS1.p3.4.m4.1" class="ltx_Math" alttext="{\bm{c}}=f_{\psi}({\bm{v}})\in\mathbb{R}^{d_{2}\times m}" display="inline"><semantics id="S3.SS1.p3.4.m4.1a"><mrow id="S3.SS1.p3.4.m4.1.2" xref="S3.SS1.p3.4.m4.1.2.cmml"><mi id="S3.SS1.p3.4.m4.1.2.2" xref="S3.SS1.p3.4.m4.1.2.2.cmml">ğ’„</mi><mo id="S3.SS1.p3.4.m4.1.2.3" xref="S3.SS1.p3.4.m4.1.2.3.cmml">=</mo><mrow id="S3.SS1.p3.4.m4.1.2.4" xref="S3.SS1.p3.4.m4.1.2.4.cmml"><msub id="S3.SS1.p3.4.m4.1.2.4.2" xref="S3.SS1.p3.4.m4.1.2.4.2.cmml"><mi id="S3.SS1.p3.4.m4.1.2.4.2.2" xref="S3.SS1.p3.4.m4.1.2.4.2.2.cmml">f</mi><mi id="S3.SS1.p3.4.m4.1.2.4.2.3" xref="S3.SS1.p3.4.m4.1.2.4.2.3.cmml">Ïˆ</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p3.4.m4.1.2.4.1" xref="S3.SS1.p3.4.m4.1.2.4.1.cmml">â€‹</mo><mrow id="S3.SS1.p3.4.m4.1.2.4.3.2" xref="S3.SS1.p3.4.m4.1.2.4.cmml"><mo stretchy="false" id="S3.SS1.p3.4.m4.1.2.4.3.2.1" xref="S3.SS1.p3.4.m4.1.2.4.cmml">(</mo><mi id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml">ğ’—</mi><mo stretchy="false" id="S3.SS1.p3.4.m4.1.2.4.3.2.2" xref="S3.SS1.p3.4.m4.1.2.4.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p3.4.m4.1.2.5" xref="S3.SS1.p3.4.m4.1.2.5.cmml">âˆˆ</mo><msup id="S3.SS1.p3.4.m4.1.2.6" xref="S3.SS1.p3.4.m4.1.2.6.cmml"><mi id="S3.SS1.p3.4.m4.1.2.6.2" xref="S3.SS1.p3.4.m4.1.2.6.2.cmml">â„</mi><mrow id="S3.SS1.p3.4.m4.1.2.6.3" xref="S3.SS1.p3.4.m4.1.2.6.3.cmml"><msub id="S3.SS1.p3.4.m4.1.2.6.3.2" xref="S3.SS1.p3.4.m4.1.2.6.3.2.cmml"><mi id="S3.SS1.p3.4.m4.1.2.6.3.2.2" xref="S3.SS1.p3.4.m4.1.2.6.3.2.2.cmml">d</mi><mn id="S3.SS1.p3.4.m4.1.2.6.3.2.3" xref="S3.SS1.p3.4.m4.1.2.6.3.2.3.cmml">2</mn></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.4.m4.1.2.6.3.1" xref="S3.SS1.p3.4.m4.1.2.6.3.1.cmml">Ã—</mo><mi id="S3.SS1.p3.4.m4.1.2.6.3.3" xref="S3.SS1.p3.4.m4.1.2.6.3.3.cmml">m</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><apply id="S3.SS1.p3.4.m4.1.2.cmml" xref="S3.SS1.p3.4.m4.1.2"><and id="S3.SS1.p3.4.m4.1.2a.cmml" xref="S3.SS1.p3.4.m4.1.2"></and><apply id="S3.SS1.p3.4.m4.1.2b.cmml" xref="S3.SS1.p3.4.m4.1.2"><eq id="S3.SS1.p3.4.m4.1.2.3.cmml" xref="S3.SS1.p3.4.m4.1.2.3"></eq><ci id="S3.SS1.p3.4.m4.1.2.2.cmml" xref="S3.SS1.p3.4.m4.1.2.2">ğ’„</ci><apply id="S3.SS1.p3.4.m4.1.2.4.cmml" xref="S3.SS1.p3.4.m4.1.2.4"><times id="S3.SS1.p3.4.m4.1.2.4.1.cmml" xref="S3.SS1.p3.4.m4.1.2.4.1"></times><apply id="S3.SS1.p3.4.m4.1.2.4.2.cmml" xref="S3.SS1.p3.4.m4.1.2.4.2"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.2.4.2.1.cmml" xref="S3.SS1.p3.4.m4.1.2.4.2">subscript</csymbol><ci id="S3.SS1.p3.4.m4.1.2.4.2.2.cmml" xref="S3.SS1.p3.4.m4.1.2.4.2.2">ğ‘“</ci><ci id="S3.SS1.p3.4.m4.1.2.4.2.3.cmml" xref="S3.SS1.p3.4.m4.1.2.4.2.3">ğœ“</ci></apply><ci id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">ğ’—</ci></apply></apply><apply id="S3.SS1.p3.4.m4.1.2c.cmml" xref="S3.SS1.p3.4.m4.1.2"><in id="S3.SS1.p3.4.m4.1.2.5.cmml" xref="S3.SS1.p3.4.m4.1.2.5"></in><share href="#S3.SS1.p3.4.m4.1.2.4.cmml" id="S3.SS1.p3.4.m4.1.2d.cmml" xref="S3.SS1.p3.4.m4.1.2"></share><apply id="S3.SS1.p3.4.m4.1.2.6.cmml" xref="S3.SS1.p3.4.m4.1.2.6"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.2.6.1.cmml" xref="S3.SS1.p3.4.m4.1.2.6">superscript</csymbol><ci id="S3.SS1.p3.4.m4.1.2.6.2.cmml" xref="S3.SS1.p3.4.m4.1.2.6.2">â„</ci><apply id="S3.SS1.p3.4.m4.1.2.6.3.cmml" xref="S3.SS1.p3.4.m4.1.2.6.3"><times id="S3.SS1.p3.4.m4.1.2.6.3.1.cmml" xref="S3.SS1.p3.4.m4.1.2.6.3.1"></times><apply id="S3.SS1.p3.4.m4.1.2.6.3.2.cmml" xref="S3.SS1.p3.4.m4.1.2.6.3.2"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.2.6.3.2.1.cmml" xref="S3.SS1.p3.4.m4.1.2.6.3.2">subscript</csymbol><ci id="S3.SS1.p3.4.m4.1.2.6.3.2.2.cmml" xref="S3.SS1.p3.4.m4.1.2.6.3.2.2">ğ‘‘</ci><cn type="integer" id="S3.SS1.p3.4.m4.1.2.6.3.2.3.cmml" xref="S3.SS1.p3.4.m4.1.2.6.3.2.3">2</cn></apply><ci id="S3.SS1.p3.4.m4.1.2.6.3.3.cmml" xref="S3.SS1.p3.4.m4.1.2.6.3.3">ğ‘š</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">{\bm{c}}=f_{\psi}({\bm{v}})\in\mathbb{R}^{d_{2}\times m}</annotation></semantics></math>, which will serve as inputs for the visual decoder (SectionÂ <a href="#S3.SS3" title="3.3 Visual Decoder â€£ 3 Approach â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Multimodal Context Modeling with LLM</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Motivated by the finding that LLMs are strong context generatorsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> for solving various knowledge-intensive tasks, it is thus appealing to model multimodal contexts with LLMs. We consider performing text generation with the LLM, conditioned on both the visual representations produced by the visual encoder described in SectionÂ <a href="#S3.SS1" title="3.1 Visual Encoder â€£ 3 Approach â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> and task-oriented human languages.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.4" class="ltx_p"><span id="S3.SS2.p2.4.1" class="ltx_text ltx_font_bold">Multimodal Tokens.</span> Given the visual context of input images, we generate language contexts that describe the visual information or complement missing words. Specifically, the inputs to the LLM consist of (1) the local visual tokens <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="{\bm{z}}\in\mathbb{R}^{d_{1}\times p}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">ğ’›</mi><mo id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml"><mi id="S3.SS2.p2.1.m1.1.1.3.2" xref="S3.SS2.p2.1.m1.1.1.3.2.cmml">â„</mi><mrow id="S3.SS2.p2.1.m1.1.1.3.3" xref="S3.SS2.p2.1.m1.1.1.3.3.cmml"><msub id="S3.SS2.p2.1.m1.1.1.3.3.2" xref="S3.SS2.p2.1.m1.1.1.3.3.2.cmml"><mi id="S3.SS2.p2.1.m1.1.1.3.3.2.2" xref="S3.SS2.p2.1.m1.1.1.3.3.2.2.cmml">d</mi><mn id="S3.SS2.p2.1.m1.1.1.3.3.2.3" xref="S3.SS2.p2.1.m1.1.1.3.3.2.3.cmml">1</mn></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.1.m1.1.1.3.3.1" xref="S3.SS2.p2.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.SS2.p2.1.m1.1.1.3.3.3" xref="S3.SS2.p2.1.m1.1.1.3.3.3.cmml">p</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><in id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></in><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">ğ’›</ci><apply id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.3.2">â„</ci><apply id="S3.SS2.p2.1.m1.1.1.3.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3"><times id="S3.SS2.p2.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.1"></times><apply id="S3.SS2.p2.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.3.3.2.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.2">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.3.3.2.2.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.2.2">ğ‘‘</ci><cn type="integer" id="S3.SS2.p2.1.m1.1.1.3.3.2.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.2.3">1</cn></apply><ci id="S3.SS2.p2.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.3">ğ‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">{\bm{z}}\in\mathbb{R}^{d_{1}\times p}</annotation></semantics></math>, and (2) a series of language tokens <math id="S3.SS2.p2.2.m2.3" class="ltx_Math" alttext="{\bm{t}}_{1:l}=\{{\bm{t}}_{1},\ldots,{\bm{t}}_{l}\}\in\mathbb{R}^{d_{1}\times l}" display="inline"><semantics id="S3.SS2.p2.2.m2.3a"><mrow id="S3.SS2.p2.2.m2.3.3" xref="S3.SS2.p2.2.m2.3.3.cmml"><msub id="S3.SS2.p2.2.m2.3.3.4" xref="S3.SS2.p2.2.m2.3.3.4.cmml"><mi id="S3.SS2.p2.2.m2.3.3.4.2" xref="S3.SS2.p2.2.m2.3.3.4.2.cmml">ğ’•</mi><mrow id="S3.SS2.p2.2.m2.3.3.4.3" xref="S3.SS2.p2.2.m2.3.3.4.3.cmml"><mn id="S3.SS2.p2.2.m2.3.3.4.3.2" xref="S3.SS2.p2.2.m2.3.3.4.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S3.SS2.p2.2.m2.3.3.4.3.1" xref="S3.SS2.p2.2.m2.3.3.4.3.1.cmml">:</mo><mi id="S3.SS2.p2.2.m2.3.3.4.3.3" xref="S3.SS2.p2.2.m2.3.3.4.3.3.cmml">l</mi></mrow></msub><mo id="S3.SS2.p2.2.m2.3.3.5" xref="S3.SS2.p2.2.m2.3.3.5.cmml">=</mo><mrow id="S3.SS2.p2.2.m2.3.3.2.2" xref="S3.SS2.p2.2.m2.3.3.2.3.cmml"><mo stretchy="false" id="S3.SS2.p2.2.m2.3.3.2.2.3" xref="S3.SS2.p2.2.m2.3.3.2.3.cmml">{</mo><msub id="S3.SS2.p2.2.m2.2.2.1.1.1" xref="S3.SS2.p2.2.m2.2.2.1.1.1.cmml"><mi id="S3.SS2.p2.2.m2.2.2.1.1.1.2" xref="S3.SS2.p2.2.m2.2.2.1.1.1.2.cmml">ğ’•</mi><mn id="S3.SS2.p2.2.m2.2.2.1.1.1.3" xref="S3.SS2.p2.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS2.p2.2.m2.3.3.2.2.4" xref="S3.SS2.p2.2.m2.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">â€¦</mi><mo id="S3.SS2.p2.2.m2.3.3.2.2.5" xref="S3.SS2.p2.2.m2.3.3.2.3.cmml">,</mo><msub id="S3.SS2.p2.2.m2.3.3.2.2.2" xref="S3.SS2.p2.2.m2.3.3.2.2.2.cmml"><mi id="S3.SS2.p2.2.m2.3.3.2.2.2.2" xref="S3.SS2.p2.2.m2.3.3.2.2.2.2.cmml">ğ’•</mi><mi id="S3.SS2.p2.2.m2.3.3.2.2.2.3" xref="S3.SS2.p2.2.m2.3.3.2.2.2.3.cmml">l</mi></msub><mo stretchy="false" id="S3.SS2.p2.2.m2.3.3.2.2.6" xref="S3.SS2.p2.2.m2.3.3.2.3.cmml">}</mo></mrow><mo id="S3.SS2.p2.2.m2.3.3.6" xref="S3.SS2.p2.2.m2.3.3.6.cmml">âˆˆ</mo><msup id="S3.SS2.p2.2.m2.3.3.7" xref="S3.SS2.p2.2.m2.3.3.7.cmml"><mi id="S3.SS2.p2.2.m2.3.3.7.2" xref="S3.SS2.p2.2.m2.3.3.7.2.cmml">â„</mi><mrow id="S3.SS2.p2.2.m2.3.3.7.3" xref="S3.SS2.p2.2.m2.3.3.7.3.cmml"><msub id="S3.SS2.p2.2.m2.3.3.7.3.2" xref="S3.SS2.p2.2.m2.3.3.7.3.2.cmml"><mi id="S3.SS2.p2.2.m2.3.3.7.3.2.2" xref="S3.SS2.p2.2.m2.3.3.7.3.2.2.cmml">d</mi><mn id="S3.SS2.p2.2.m2.3.3.7.3.2.3" xref="S3.SS2.p2.2.m2.3.3.7.3.2.3.cmml">1</mn></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.2.m2.3.3.7.3.1" xref="S3.SS2.p2.2.m2.3.3.7.3.1.cmml">Ã—</mo><mi id="S3.SS2.p2.2.m2.3.3.7.3.3" xref="S3.SS2.p2.2.m2.3.3.7.3.3.cmml">l</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.3b"><apply id="S3.SS2.p2.2.m2.3.3.cmml" xref="S3.SS2.p2.2.m2.3.3"><and id="S3.SS2.p2.2.m2.3.3a.cmml" xref="S3.SS2.p2.2.m2.3.3"></and><apply id="S3.SS2.p2.2.m2.3.3b.cmml" xref="S3.SS2.p2.2.m2.3.3"><eq id="S3.SS2.p2.2.m2.3.3.5.cmml" xref="S3.SS2.p2.2.m2.3.3.5"></eq><apply id="S3.SS2.p2.2.m2.3.3.4.cmml" xref="S3.SS2.p2.2.m2.3.3.4"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.3.3.4.1.cmml" xref="S3.SS2.p2.2.m2.3.3.4">subscript</csymbol><ci id="S3.SS2.p2.2.m2.3.3.4.2.cmml" xref="S3.SS2.p2.2.m2.3.3.4.2">ğ’•</ci><apply id="S3.SS2.p2.2.m2.3.3.4.3.cmml" xref="S3.SS2.p2.2.m2.3.3.4.3"><ci id="S3.SS2.p2.2.m2.3.3.4.3.1.cmml" xref="S3.SS2.p2.2.m2.3.3.4.3.1">:</ci><cn type="integer" id="S3.SS2.p2.2.m2.3.3.4.3.2.cmml" xref="S3.SS2.p2.2.m2.3.3.4.3.2">1</cn><ci id="S3.SS2.p2.2.m2.3.3.4.3.3.cmml" xref="S3.SS2.p2.2.m2.3.3.4.3.3">ğ‘™</ci></apply></apply><set id="S3.SS2.p2.2.m2.3.3.2.3.cmml" xref="S3.SS2.p2.2.m2.3.3.2.2"><apply id="S3.SS2.p2.2.m2.2.2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.2.2.1.1.1.1.cmml" xref="S3.SS2.p2.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.2.m2.2.2.1.1.1.2.cmml" xref="S3.SS2.p2.2.m2.2.2.1.1.1.2">ğ’•</ci><cn type="integer" id="S3.SS2.p2.2.m2.2.2.1.1.1.3.cmml" xref="S3.SS2.p2.2.m2.2.2.1.1.1.3">1</cn></apply><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">â€¦</ci><apply id="S3.SS2.p2.2.m2.3.3.2.2.2.cmml" xref="S3.SS2.p2.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.3.3.2.2.2.1.cmml" xref="S3.SS2.p2.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S3.SS2.p2.2.m2.3.3.2.2.2.2.cmml" xref="S3.SS2.p2.2.m2.3.3.2.2.2.2">ğ’•</ci><ci id="S3.SS2.p2.2.m2.3.3.2.2.2.3.cmml" xref="S3.SS2.p2.2.m2.3.3.2.2.2.3">ğ‘™</ci></apply></set></apply><apply id="S3.SS2.p2.2.m2.3.3c.cmml" xref="S3.SS2.p2.2.m2.3.3"><in id="S3.SS2.p2.2.m2.3.3.6.cmml" xref="S3.SS2.p2.2.m2.3.3.6"></in><share href="#S3.SS2.p2.2.m2.3.3.2.cmml" id="S3.SS2.p2.2.m2.3.3d.cmml" xref="S3.SS2.p2.2.m2.3.3"></share><apply id="S3.SS2.p2.2.m2.3.3.7.cmml" xref="S3.SS2.p2.2.m2.3.3.7"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.3.3.7.1.cmml" xref="S3.SS2.p2.2.m2.3.3.7">superscript</csymbol><ci id="S3.SS2.p2.2.m2.3.3.7.2.cmml" xref="S3.SS2.p2.2.m2.3.3.7.2">â„</ci><apply id="S3.SS2.p2.2.m2.3.3.7.3.cmml" xref="S3.SS2.p2.2.m2.3.3.7.3"><times id="S3.SS2.p2.2.m2.3.3.7.3.1.cmml" xref="S3.SS2.p2.2.m2.3.3.7.3.1"></times><apply id="S3.SS2.p2.2.m2.3.3.7.3.2.cmml" xref="S3.SS2.p2.2.m2.3.3.7.3.2"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.3.3.7.3.2.1.cmml" xref="S3.SS2.p2.2.m2.3.3.7.3.2">subscript</csymbol><ci id="S3.SS2.p2.2.m2.3.3.7.3.2.2.cmml" xref="S3.SS2.p2.2.m2.3.3.7.3.2.2">ğ‘‘</ci><cn type="integer" id="S3.SS2.p2.2.m2.3.3.7.3.2.3.cmml" xref="S3.SS2.p2.2.m2.3.3.7.3.2.3">1</cn></apply><ci id="S3.SS2.p2.2.m2.3.3.7.3.3.cmml" xref="S3.SS2.p2.2.m2.3.3.7.3.3">ğ‘™</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.3c">{\bm{t}}_{1:l}=\{{\bm{t}}_{1},\ldots,{\bm{t}}_{l}\}\in\mathbb{R}^{d_{1}\times l}</annotation></semantics></math>, where the symbol <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mi id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">ğ‘™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">l</annotation></semantics></math> is the sequence length of the language tokens. The language tokens <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="{\bm{t}}_{1:l}" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><msub id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mi id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">ğ’•</mi><mrow id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml"><mn id="S3.SS2.p2.4.m4.1.1.3.2" xref="S3.SS2.p2.4.m4.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S3.SS2.p2.4.m4.1.1.3.1" xref="S3.SS2.p2.4.m4.1.1.3.1.cmml">:</mo><mi id="S3.SS2.p2.4.m4.1.1.3.3" xref="S3.SS2.p2.4.m4.1.1.3.3.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2">ğ’•</ci><apply id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3"><ci id="S3.SS2.p2.4.m4.1.1.3.1.cmml" xref="S3.SS2.p2.4.m4.1.1.3.1">:</ci><cn type="integer" id="S3.SS2.p2.4.m4.1.1.3.2.cmml" xref="S3.SS2.p2.4.m4.1.1.3.2">1</cn><ci id="S3.SS2.p2.4.m4.1.1.3.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3.3">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">{\bm{t}}_{1:l}</annotation></semantics></math>
have different forms for different contextual object detection settings. For the cloze test, the language tokens are tokenized sentences with masked names, <span id="S3.SS2.p2.4.2" class="ltx_text ltx_font_italic">e.g.</span>, â€˜<span id="S3.SS2.p2.4.3" class="ltx_text ltx_font_typewriter">a [MASK] and her [MASK] kiss with their [MASK] at their wedding</span>â€™.
For the visual captioning, the language tokens are tokenized text promptsâ€”â€˜<span id="S3.SS2.p2.4.4" class="ltx_text ltx_font_typewriter">a photo of</span>â€™â€”to describe the image. For the question answering, the language tokens represent the tokenized sentences of questions, <span id="S3.SS2.p2.4.5" class="ltx_text ltx_font_italic">e.g.</span>, â€˜<span id="S3.SS2.p2.4.6" class="ltx_text ltx_font_typewriter">Question: what is the specie of the dog? Answer:</span>â€™.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.2" class="ltx_p"><span id="S3.SS2.p3.2.1" class="ltx_text ltx_font_bold">Multimodal Prefixed LLM Decoding.</span> A pre-trained LLM <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\theta</annotation></semantics></math> can be conditioned on a prefix
<math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="{\bm{w}}_{1:n}" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><msub id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">ğ’˜</mi><mrow id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml"><mn id="S3.SS2.p3.2.m2.1.1.3.2" xref="S3.SS2.p3.2.m2.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S3.SS2.p3.2.m2.1.1.3.1" xref="S3.SS2.p3.2.m2.1.1.3.1.cmml">:</mo><mi id="S3.SS2.p3.2.m2.1.1.3.3" xref="S3.SS2.p3.2.m2.1.1.3.3.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">ğ’˜</ci><apply id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3"><ci id="S3.SS2.p3.2.m2.1.1.3.1.cmml" xref="S3.SS2.p3.2.m2.1.1.3.1">:</ci><cn type="integer" id="S3.SS2.p3.2.m2.1.1.3.2.cmml" xref="S3.SS2.p3.2.m2.1.1.3.2">1</cn><ci id="S3.SS2.p3.2.m2.1.1.3.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3.3">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">{\bm{w}}_{1:n}</annotation></semantics></math>
that contains multimodal tokens to generate text in an autoregressive way:</p>
<table id="A5.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\displaystyle p({\bm{w}}_{n+1:L}|{\bm{w}}_{1:n})=\prod_{i=n+1}^{L}p_{\theta}({\bm{w}}_{i+1}|{\bm{w}}_{1:i})." display="inline"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.cmml">ğ’˜</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2.2.cmml">n</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2.1.cmml">+</mo><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2.3.cmml">1</mn></mrow><mo lspace="0.278em" rspace="0.278em" id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.1.cmml">:</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.3.cmml">L</mi></mrow></msub><mo fence="false" id="S3.E1.m1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml">|</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml">ğ’˜</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml"><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1.cmml">:</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.cmml">n</mi></mrow></msub></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml"><mstyle displaystyle="true" id="S3.E1.m1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.2.2.cmml"><munderover id="S3.E1.m1.1.1.1.1.2.2a" xref="S3.E1.m1.1.1.1.1.2.2.cmml"><mo movablelimits="false" id="S3.E1.m1.1.1.1.1.2.2.2.2" xref="S3.E1.m1.1.1.1.1.2.2.2.2.cmml">âˆ</mo><mrow id="S3.E1.m1.1.1.1.1.2.2.2.3" xref="S3.E1.m1.1.1.1.1.2.2.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.2.2.2.3.2" xref="S3.E1.m1.1.1.1.1.2.2.2.3.2.cmml">i</mi><mo id="S3.E1.m1.1.1.1.1.2.2.2.3.1" xref="S3.E1.m1.1.1.1.1.2.2.2.3.1.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.2.2.2.3.3" xref="S3.E1.m1.1.1.1.1.2.2.2.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.2.2.2.3.3.2" xref="S3.E1.m1.1.1.1.1.2.2.2.3.3.2.cmml">n</mi><mo id="S3.E1.m1.1.1.1.1.2.2.2.3.3.1" xref="S3.E1.m1.1.1.1.1.2.2.2.3.3.1.cmml">+</mo><mn id="S3.E1.m1.1.1.1.1.2.2.2.3.3.3" xref="S3.E1.m1.1.1.1.1.2.2.2.3.3.3.cmml">1</mn></mrow></mrow><mi id="S3.E1.m1.1.1.1.1.2.2.3" xref="S3.E1.m1.1.1.1.1.2.2.3.cmml">L</mi></munderover></mstyle><mrow id="S3.E1.m1.1.1.1.1.2.1" xref="S3.E1.m1.1.1.1.1.2.1.cmml"><msub id="S3.E1.m1.1.1.1.1.2.1.3" xref="S3.E1.m1.1.1.1.1.2.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.2.1.3.2" xref="S3.E1.m1.1.1.1.1.2.1.3.2.cmml">p</mi><mi id="S3.E1.m1.1.1.1.1.2.1.3.3" xref="S3.E1.m1.1.1.1.1.2.1.3.3.cmml">Î¸</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2.1.2" xref="S3.E1.m1.1.1.1.1.2.1.2.cmml">â€‹</mo><mrow id="S3.E1.m1.1.1.1.1.2.1.1.1" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.2.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.2.1.1.1.1" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.2.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.2.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.2.2.cmml">ğ’˜</mi><mrow id="S3.E1.m1.1.1.1.1.2.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.2.1.1.1.1.2.3.2" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.2.3.2.cmml">i</mi><mo id="S3.E1.m1.1.1.1.1.2.1.1.1.1.2.3.1" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.2.3.1.cmml">+</mo><mn id="S3.E1.m1.1.1.1.1.2.1.1.1.1.2.3.3" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.2.3.3.cmml">1</mn></mrow></msub><mo fence="false" id="S3.E1.m1.1.1.1.1.2.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.1.cmml">|</mo><msub id="S3.E1.m1.1.1.1.1.2.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.2.cmml">ğ’˜</mi><mrow id="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.3.cmml"><mn id="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.3.2" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.3.1" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.3.1.cmml">:</mo><mi id="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.3.3.cmml">i</mi></mrow></msub></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.2.1.1.1.3" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em" id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"></eq><apply id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.2"></times><ci id="S3.E1.m1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.3">ğ‘</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2">ğ’˜</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3"><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.1">:</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2"><plus id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2.1"></plus><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2.2">ğ‘›</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.2.3">1</cn></apply><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.3">ğ¿</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.2">ğ’˜</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3"><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1">:</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2">1</cn><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3">ğ‘›</ci></apply></apply></apply></apply><apply id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"><apply id="S3.E1.m1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2.2">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2.2">subscript</csymbol><csymbol cd="latexml" id="S3.E1.m1.1.1.1.1.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.2">product</csymbol><apply id="S3.E1.m1.1.1.1.1.2.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.3"><eq id="S3.E1.m1.1.1.1.1.2.2.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.3.1"></eq><ci id="S3.E1.m1.1.1.1.1.2.2.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.3.2">ğ‘–</ci><apply id="S3.E1.m1.1.1.1.1.2.2.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.3.3"><plus id="S3.E1.m1.1.1.1.1.2.2.2.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.3.3.1"></plus><ci id="S3.E1.m1.1.1.1.1.2.2.2.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.3.3.2">ğ‘›</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.2.2.2.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.3.3.3">1</cn></apply></apply></apply><ci id="S3.E1.m1.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.3">ğ¿</ci></apply><apply id="S3.E1.m1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1"><times id="S3.E1.m1.1.1.1.1.2.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2.1.2"></times><apply id="S3.E1.m1.1.1.1.1.2.1.3.cmml" xref="S3.E1.m1.1.1.1.1.2.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.2.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.2.1.3.2">ğ‘</ci><ci id="S3.E1.m1.1.1.1.1.2.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.2.1.3.3">ğœƒ</ci></apply><apply id="S3.E1.m1.1.1.1.1.2.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.1.1.1.1.2.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.1">conditional</csymbol><apply id="S3.E1.m1.1.1.1.1.2.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.2.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.2.2">ğ’˜</ci><apply id="S3.E1.m1.1.1.1.1.2.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.2.3"><plus id="S3.E1.m1.1.1.1.1.2.1.1.1.1.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.2.3.1"></plus><ci id="S3.E1.m1.1.1.1.1.2.1.1.1.1.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.2.3.2">ğ‘–</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.2.1.1.1.1.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.2.3.3">1</cn></apply></apply><apply id="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.2">ğ’˜</ci><apply id="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.3"><ci id="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.3.1">:</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.3.2">1</cn><ci id="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.3.3">ğ‘–</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\displaystyle p({\bm{w}}_{n+1:L}|{\bm{w}}_{1:n})=\prod_{i=n+1}^{L}p_{\theta}({\bm{w}}_{i+1}|{\bm{w}}_{1:i}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p3.7" class="ltx_p">Here, the prefix <math id="S3.SS2.p3.3.m1.3" class="ltx_Math" alttext="{\bm{w}}_{1:n}=[{\bm{z}},{\bm{t}}_{1:l}]\in\mathbb{R}^{d_{1}\times(p+l)}" display="inline"><semantics id="S3.SS2.p3.3.m1.3a"><mrow id="S3.SS2.p3.3.m1.3.3" xref="S3.SS2.p3.3.m1.3.3.cmml"><msub id="S3.SS2.p3.3.m1.3.3.3" xref="S3.SS2.p3.3.m1.3.3.3.cmml"><mi id="S3.SS2.p3.3.m1.3.3.3.2" xref="S3.SS2.p3.3.m1.3.3.3.2.cmml">ğ’˜</mi><mrow id="S3.SS2.p3.3.m1.3.3.3.3" xref="S3.SS2.p3.3.m1.3.3.3.3.cmml"><mn id="S3.SS2.p3.3.m1.3.3.3.3.2" xref="S3.SS2.p3.3.m1.3.3.3.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S3.SS2.p3.3.m1.3.3.3.3.1" xref="S3.SS2.p3.3.m1.3.3.3.3.1.cmml">:</mo><mi id="S3.SS2.p3.3.m1.3.3.3.3.3" xref="S3.SS2.p3.3.m1.3.3.3.3.3.cmml">n</mi></mrow></msub><mo id="S3.SS2.p3.3.m1.3.3.4" xref="S3.SS2.p3.3.m1.3.3.4.cmml">=</mo><mrow id="S3.SS2.p3.3.m1.3.3.1.1" xref="S3.SS2.p3.3.m1.3.3.1.2.cmml"><mo stretchy="false" id="S3.SS2.p3.3.m1.3.3.1.1.2" xref="S3.SS2.p3.3.m1.3.3.1.2.cmml">[</mo><mi id="S3.SS2.p3.3.m1.2.2" xref="S3.SS2.p3.3.m1.2.2.cmml">ğ’›</mi><mo id="S3.SS2.p3.3.m1.3.3.1.1.3" xref="S3.SS2.p3.3.m1.3.3.1.2.cmml">,</mo><msub id="S3.SS2.p3.3.m1.3.3.1.1.1" xref="S3.SS2.p3.3.m1.3.3.1.1.1.cmml"><mi id="S3.SS2.p3.3.m1.3.3.1.1.1.2" xref="S3.SS2.p3.3.m1.3.3.1.1.1.2.cmml">ğ’•</mi><mrow id="S3.SS2.p3.3.m1.3.3.1.1.1.3" xref="S3.SS2.p3.3.m1.3.3.1.1.1.3.cmml"><mn id="S3.SS2.p3.3.m1.3.3.1.1.1.3.2" xref="S3.SS2.p3.3.m1.3.3.1.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S3.SS2.p3.3.m1.3.3.1.1.1.3.1" xref="S3.SS2.p3.3.m1.3.3.1.1.1.3.1.cmml">:</mo><mi id="S3.SS2.p3.3.m1.3.3.1.1.1.3.3" xref="S3.SS2.p3.3.m1.3.3.1.1.1.3.3.cmml">l</mi></mrow></msub><mo stretchy="false" id="S3.SS2.p3.3.m1.3.3.1.1.4" xref="S3.SS2.p3.3.m1.3.3.1.2.cmml">]</mo></mrow><mo id="S3.SS2.p3.3.m1.3.3.5" xref="S3.SS2.p3.3.m1.3.3.5.cmml">âˆˆ</mo><msup id="S3.SS2.p3.3.m1.3.3.6" xref="S3.SS2.p3.3.m1.3.3.6.cmml"><mi id="S3.SS2.p3.3.m1.3.3.6.2" xref="S3.SS2.p3.3.m1.3.3.6.2.cmml">â„</mi><mrow id="S3.SS2.p3.3.m1.1.1.1" xref="S3.SS2.p3.3.m1.1.1.1.cmml"><msub id="S3.SS2.p3.3.m1.1.1.1.3" xref="S3.SS2.p3.3.m1.1.1.1.3.cmml"><mi id="S3.SS2.p3.3.m1.1.1.1.3.2" xref="S3.SS2.p3.3.m1.1.1.1.3.2.cmml">d</mi><mn id="S3.SS2.p3.3.m1.1.1.1.3.3" xref="S3.SS2.p3.3.m1.1.1.1.3.3.cmml">1</mn></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p3.3.m1.1.1.1.2" xref="S3.SS2.p3.3.m1.1.1.1.2.cmml">Ã—</mo><mrow id="S3.SS2.p3.3.m1.1.1.1.1.1" xref="S3.SS2.p3.3.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p3.3.m1.1.1.1.1.1.2" xref="S3.SS2.p3.3.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p3.3.m1.1.1.1.1.1.1" xref="S3.SS2.p3.3.m1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p3.3.m1.1.1.1.1.1.1.2" xref="S3.SS2.p3.3.m1.1.1.1.1.1.1.2.cmml">p</mi><mo id="S3.SS2.p3.3.m1.1.1.1.1.1.1.1" xref="S3.SS2.p3.3.m1.1.1.1.1.1.1.1.cmml">+</mo><mi id="S3.SS2.p3.3.m1.1.1.1.1.1.1.3" xref="S3.SS2.p3.3.m1.1.1.1.1.1.1.3.cmml">l</mi></mrow><mo stretchy="false" id="S3.SS2.p3.3.m1.1.1.1.1.1.3" xref="S3.SS2.p3.3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m1.3b"><apply id="S3.SS2.p3.3.m1.3.3.cmml" xref="S3.SS2.p3.3.m1.3.3"><and id="S3.SS2.p3.3.m1.3.3a.cmml" xref="S3.SS2.p3.3.m1.3.3"></and><apply id="S3.SS2.p3.3.m1.3.3b.cmml" xref="S3.SS2.p3.3.m1.3.3"><eq id="S3.SS2.p3.3.m1.3.3.4.cmml" xref="S3.SS2.p3.3.m1.3.3.4"></eq><apply id="S3.SS2.p3.3.m1.3.3.3.cmml" xref="S3.SS2.p3.3.m1.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m1.3.3.3.1.cmml" xref="S3.SS2.p3.3.m1.3.3.3">subscript</csymbol><ci id="S3.SS2.p3.3.m1.3.3.3.2.cmml" xref="S3.SS2.p3.3.m1.3.3.3.2">ğ’˜</ci><apply id="S3.SS2.p3.3.m1.3.3.3.3.cmml" xref="S3.SS2.p3.3.m1.3.3.3.3"><ci id="S3.SS2.p3.3.m1.3.3.3.3.1.cmml" xref="S3.SS2.p3.3.m1.3.3.3.3.1">:</ci><cn type="integer" id="S3.SS2.p3.3.m1.3.3.3.3.2.cmml" xref="S3.SS2.p3.3.m1.3.3.3.3.2">1</cn><ci id="S3.SS2.p3.3.m1.3.3.3.3.3.cmml" xref="S3.SS2.p3.3.m1.3.3.3.3.3">ğ‘›</ci></apply></apply><interval closure="closed" id="S3.SS2.p3.3.m1.3.3.1.2.cmml" xref="S3.SS2.p3.3.m1.3.3.1.1"><ci id="S3.SS2.p3.3.m1.2.2.cmml" xref="S3.SS2.p3.3.m1.2.2">ğ’›</ci><apply id="S3.SS2.p3.3.m1.3.3.1.1.1.cmml" xref="S3.SS2.p3.3.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m1.3.3.1.1.1.1.cmml" xref="S3.SS2.p3.3.m1.3.3.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.3.m1.3.3.1.1.1.2.cmml" xref="S3.SS2.p3.3.m1.3.3.1.1.1.2">ğ’•</ci><apply id="S3.SS2.p3.3.m1.3.3.1.1.1.3.cmml" xref="S3.SS2.p3.3.m1.3.3.1.1.1.3"><ci id="S3.SS2.p3.3.m1.3.3.1.1.1.3.1.cmml" xref="S3.SS2.p3.3.m1.3.3.1.1.1.3.1">:</ci><cn type="integer" id="S3.SS2.p3.3.m1.3.3.1.1.1.3.2.cmml" xref="S3.SS2.p3.3.m1.3.3.1.1.1.3.2">1</cn><ci id="S3.SS2.p3.3.m1.3.3.1.1.1.3.3.cmml" xref="S3.SS2.p3.3.m1.3.3.1.1.1.3.3">ğ‘™</ci></apply></apply></interval></apply><apply id="S3.SS2.p3.3.m1.3.3c.cmml" xref="S3.SS2.p3.3.m1.3.3"><in id="S3.SS2.p3.3.m1.3.3.5.cmml" xref="S3.SS2.p3.3.m1.3.3.5"></in><share href="#S3.SS2.p3.3.m1.3.3.1.cmml" id="S3.SS2.p3.3.m1.3.3d.cmml" xref="S3.SS2.p3.3.m1.3.3"></share><apply id="S3.SS2.p3.3.m1.3.3.6.cmml" xref="S3.SS2.p3.3.m1.3.3.6"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m1.3.3.6.1.cmml" xref="S3.SS2.p3.3.m1.3.3.6">superscript</csymbol><ci id="S3.SS2.p3.3.m1.3.3.6.2.cmml" xref="S3.SS2.p3.3.m1.3.3.6.2">â„</ci><apply id="S3.SS2.p3.3.m1.1.1.1.cmml" xref="S3.SS2.p3.3.m1.1.1.1"><times id="S3.SS2.p3.3.m1.1.1.1.2.cmml" xref="S3.SS2.p3.3.m1.1.1.1.2"></times><apply id="S3.SS2.p3.3.m1.1.1.1.3.cmml" xref="S3.SS2.p3.3.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m1.1.1.1.3.1.cmml" xref="S3.SS2.p3.3.m1.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p3.3.m1.1.1.1.3.2.cmml" xref="S3.SS2.p3.3.m1.1.1.1.3.2">ğ‘‘</ci><cn type="integer" id="S3.SS2.p3.3.m1.1.1.1.3.3.cmml" xref="S3.SS2.p3.3.m1.1.1.1.3.3">1</cn></apply><apply id="S3.SS2.p3.3.m1.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.3.m1.1.1.1.1.1"><plus id="S3.SS2.p3.3.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.3.m1.1.1.1.1.1.1.1"></plus><ci id="S3.SS2.p3.3.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.3.m1.1.1.1.1.1.1.2">ğ‘</ci><ci id="S3.SS2.p3.3.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.3.m1.1.1.1.1.1.1.3">ğ‘™</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m1.3c">{\bm{w}}_{1:n}=[{\bm{z}},{\bm{t}}_{1:l}]\in\mathbb{R}^{d_{1}\times(p+l)}</annotation></semantics></math> is obtained via concatenating the local visual tokens <math id="S3.SS2.p3.4.m2.1" class="ltx_Math" alttext="{\bm{z}}" display="inline"><semantics id="S3.SS2.p3.4.m2.1a"><mi id="S3.SS2.p3.4.m2.1.1" xref="S3.SS2.p3.4.m2.1.1.cmml">ğ’›</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m2.1b"><ci id="S3.SS2.p3.4.m2.1.1.cmml" xref="S3.SS2.p3.4.m2.1.1">ğ’›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m2.1c">{\bm{z}}</annotation></semantics></math> with a sequence of language tokens <math id="S3.SS2.p3.5.m3.1" class="ltx_Math" alttext="{\bm{t}}_{1:l}" display="inline"><semantics id="S3.SS2.p3.5.m3.1a"><msub id="S3.SS2.p3.5.m3.1.1" xref="S3.SS2.p3.5.m3.1.1.cmml"><mi id="S3.SS2.p3.5.m3.1.1.2" xref="S3.SS2.p3.5.m3.1.1.2.cmml">ğ’•</mi><mrow id="S3.SS2.p3.5.m3.1.1.3" xref="S3.SS2.p3.5.m3.1.1.3.cmml"><mn id="S3.SS2.p3.5.m3.1.1.3.2" xref="S3.SS2.p3.5.m3.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S3.SS2.p3.5.m3.1.1.3.1" xref="S3.SS2.p3.5.m3.1.1.3.1.cmml">:</mo><mi id="S3.SS2.p3.5.m3.1.1.3.3" xref="S3.SS2.p3.5.m3.1.1.3.3.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m3.1b"><apply id="S3.SS2.p3.5.m3.1.1.cmml" xref="S3.SS2.p3.5.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.5.m3.1.1.1.cmml" xref="S3.SS2.p3.5.m3.1.1">subscript</csymbol><ci id="S3.SS2.p3.5.m3.1.1.2.cmml" xref="S3.SS2.p3.5.m3.1.1.2">ğ’•</ci><apply id="S3.SS2.p3.5.m3.1.1.3.cmml" xref="S3.SS2.p3.5.m3.1.1.3"><ci id="S3.SS2.p3.5.m3.1.1.3.1.cmml" xref="S3.SS2.p3.5.m3.1.1.3.1">:</ci><cn type="integer" id="S3.SS2.p3.5.m3.1.1.3.2.cmml" xref="S3.SS2.p3.5.m3.1.1.3.2">1</cn><ci id="S3.SS2.p3.5.m3.1.1.3.3.cmml" xref="S3.SS2.p3.5.m3.1.1.3.3">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m3.1c">{\bm{t}}_{1:l}</annotation></semantics></math>.
Specifically, the LLM consists of multiple Transformer layers (<span id="S3.SS2.p3.7.1" class="ltx_text ltx_font_typewriter">TransLayers</span>) and a final Feed Forward Network (<span id="S3.SS2.p3.7.2" class="ltx_text ltx_font_typewriter">FFN</span>).
To generate new tokens, the LLM first predicts the latent embedding <math id="S3.SS2.p3.6.m4.1" class="ltx_Math" alttext="{\bm{e}}_{n+1}" display="inline"><semantics id="S3.SS2.p3.6.m4.1a"><msub id="S3.SS2.p3.6.m4.1.1" xref="S3.SS2.p3.6.m4.1.1.cmml"><mi id="S3.SS2.p3.6.m4.1.1.2" xref="S3.SS2.p3.6.m4.1.1.2.cmml">ğ’†</mi><mrow id="S3.SS2.p3.6.m4.1.1.3" xref="S3.SS2.p3.6.m4.1.1.3.cmml"><mi id="S3.SS2.p3.6.m4.1.1.3.2" xref="S3.SS2.p3.6.m4.1.1.3.2.cmml">n</mi><mo id="S3.SS2.p3.6.m4.1.1.3.1" xref="S3.SS2.p3.6.m4.1.1.3.1.cmml">+</mo><mn id="S3.SS2.p3.6.m4.1.1.3.3" xref="S3.SS2.p3.6.m4.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m4.1b"><apply id="S3.SS2.p3.6.m4.1.1.cmml" xref="S3.SS2.p3.6.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.6.m4.1.1.1.cmml" xref="S3.SS2.p3.6.m4.1.1">subscript</csymbol><ci id="S3.SS2.p3.6.m4.1.1.2.cmml" xref="S3.SS2.p3.6.m4.1.1.2">ğ’†</ci><apply id="S3.SS2.p3.6.m4.1.1.3.cmml" xref="S3.SS2.p3.6.m4.1.1.3"><plus id="S3.SS2.p3.6.m4.1.1.3.1.cmml" xref="S3.SS2.p3.6.m4.1.1.3.1"></plus><ci id="S3.SS2.p3.6.m4.1.1.3.2.cmml" xref="S3.SS2.p3.6.m4.1.1.3.2">ğ‘›</ci><cn type="integer" id="S3.SS2.p3.6.m4.1.1.3.3.cmml" xref="S3.SS2.p3.6.m4.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m4.1c">{\bm{e}}_{n+1}</annotation></semantics></math> for the new <math id="S3.SS2.p3.7.m5.1" class="ltx_Math" alttext="n+1" display="inline"><semantics id="S3.SS2.p3.7.m5.1a"><mrow id="S3.SS2.p3.7.m5.1.1" xref="S3.SS2.p3.7.m5.1.1.cmml"><mi id="S3.SS2.p3.7.m5.1.1.2" xref="S3.SS2.p3.7.m5.1.1.2.cmml">n</mi><mo id="S3.SS2.p3.7.m5.1.1.1" xref="S3.SS2.p3.7.m5.1.1.1.cmml">+</mo><mn id="S3.SS2.p3.7.m5.1.1.3" xref="S3.SS2.p3.7.m5.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.7.m5.1b"><apply id="S3.SS2.p3.7.m5.1.1.cmml" xref="S3.SS2.p3.7.m5.1.1"><plus id="S3.SS2.p3.7.m5.1.1.1.cmml" xref="S3.SS2.p3.7.m5.1.1.1"></plus><ci id="S3.SS2.p3.7.m5.1.1.2.cmml" xref="S3.SS2.p3.7.m5.1.1.2">ğ‘›</ci><cn type="integer" id="S3.SS2.p3.7.m5.1.1.3.cmml" xref="S3.SS2.p3.7.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.7.m5.1c">n+1</annotation></semantics></math>-th token:</p>
<table id="A5.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\displaystyle{\bm{e}}_{n+1}=\texttt{TransLayers}({\bm{w}}_{1:n})," display="inline"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.3.2.cmml">ğ’†</mi><mrow id="S3.E2.m1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.3.2" xref="S3.E2.m1.1.1.1.1.3.3.2.cmml">n</mi><mo id="S3.E2.m1.1.1.1.1.3.3.1" xref="S3.E2.m1.1.1.1.1.3.3.1.cmml">+</mo><mn id="S3.E2.m1.1.1.1.1.3.3.3" xref="S3.E2.m1.1.1.1.1.3.3.3.cmml">1</mn></mrow></msub><mo id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.3a.cmml">TransLayers</mtext><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml">ğ’˜</mi><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml"><mn id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S3.E2.m1.1.1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.1.cmml">:</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.3.cmml">n</mi></mrow></msub><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"></eq><apply id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2">ğ’†</ci><apply id="S3.E2.m1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3"><plus id="S3.E2.m1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.3.1"></plus><ci id="S3.E2.m1.1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.3.2">ğ‘›</ci><cn type="integer" id="S3.E2.m1.1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3">1</cn></apply></apply><apply id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2"></times><ci id="S3.E2.m1.1.1.1.1.1.3a.cmml" xref="S3.E2.m1.1.1.1.1.1.3"><mtext class="ltx_mathvariant_monospace" id="S3.E2.m1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3">TransLayers</mtext></ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2">ğ’˜</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3"><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.1">:</ci><cn type="integer" id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2">1</cn><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.3">ğ‘›</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\displaystyle{\bm{e}}_{n+1}=\texttt{TransLayers}({\bm{w}}_{1:n}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p3.9" class="ltx_p">which contains decoded multimodal contextual information.
Then, the <span id="S3.SS2.p3.9.1" class="ltx_text ltx_font_typewriter">FFN</span> computes the probability distribution <math id="S3.SS2.p3.8.m1.1" class="ltx_Math" alttext="p({\bm{w}}_{n+1})" display="inline"><semantics id="S3.SS2.p3.8.m1.1a"><mrow id="S3.SS2.p3.8.m1.1.1" xref="S3.SS2.p3.8.m1.1.1.cmml"><mi id="S3.SS2.p3.8.m1.1.1.3" xref="S3.SS2.p3.8.m1.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.8.m1.1.1.2" xref="S3.SS2.p3.8.m1.1.1.2.cmml">â€‹</mo><mrow id="S3.SS2.p3.8.m1.1.1.1.1" xref="S3.SS2.p3.8.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p3.8.m1.1.1.1.1.2" xref="S3.SS2.p3.8.m1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p3.8.m1.1.1.1.1.1" xref="S3.SS2.p3.8.m1.1.1.1.1.1.cmml"><mi id="S3.SS2.p3.8.m1.1.1.1.1.1.2" xref="S3.SS2.p3.8.m1.1.1.1.1.1.2.cmml">ğ’˜</mi><mrow id="S3.SS2.p3.8.m1.1.1.1.1.1.3" xref="S3.SS2.p3.8.m1.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p3.8.m1.1.1.1.1.1.3.2" xref="S3.SS2.p3.8.m1.1.1.1.1.1.3.2.cmml">n</mi><mo id="S3.SS2.p3.8.m1.1.1.1.1.1.3.1" xref="S3.SS2.p3.8.m1.1.1.1.1.1.3.1.cmml">+</mo><mn id="S3.SS2.p3.8.m1.1.1.1.1.1.3.3" xref="S3.SS2.p3.8.m1.1.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo stretchy="false" id="S3.SS2.p3.8.m1.1.1.1.1.3" xref="S3.SS2.p3.8.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.8.m1.1b"><apply id="S3.SS2.p3.8.m1.1.1.cmml" xref="S3.SS2.p3.8.m1.1.1"><times id="S3.SS2.p3.8.m1.1.1.2.cmml" xref="S3.SS2.p3.8.m1.1.1.2"></times><ci id="S3.SS2.p3.8.m1.1.1.3.cmml" xref="S3.SS2.p3.8.m1.1.1.3">ğ‘</ci><apply id="S3.SS2.p3.8.m1.1.1.1.1.1.cmml" xref="S3.SS2.p3.8.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.8.m1.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.8.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.8.m1.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.8.m1.1.1.1.1.1.2">ğ’˜</ci><apply id="S3.SS2.p3.8.m1.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.8.m1.1.1.1.1.1.3"><plus id="S3.SS2.p3.8.m1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p3.8.m1.1.1.1.1.1.3.1"></plus><ci id="S3.SS2.p3.8.m1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p3.8.m1.1.1.1.1.1.3.2">ğ‘›</ci><cn type="integer" id="S3.SS2.p3.8.m1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p3.8.m1.1.1.1.1.1.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.8.m1.1c">p({\bm{w}}_{n+1})</annotation></semantics></math> based on the latent embedding <math id="S3.SS2.p3.9.m2.1" class="ltx_Math" alttext="{\bm{e}}_{n+1}" display="inline"><semantics id="S3.SS2.p3.9.m2.1a"><msub id="S3.SS2.p3.9.m2.1.1" xref="S3.SS2.p3.9.m2.1.1.cmml"><mi id="S3.SS2.p3.9.m2.1.1.2" xref="S3.SS2.p3.9.m2.1.1.2.cmml">ğ’†</mi><mrow id="S3.SS2.p3.9.m2.1.1.3" xref="S3.SS2.p3.9.m2.1.1.3.cmml"><mi id="S3.SS2.p3.9.m2.1.1.3.2" xref="S3.SS2.p3.9.m2.1.1.3.2.cmml">n</mi><mo id="S3.SS2.p3.9.m2.1.1.3.1" xref="S3.SS2.p3.9.m2.1.1.3.1.cmml">+</mo><mn id="S3.SS2.p3.9.m2.1.1.3.3" xref="S3.SS2.p3.9.m2.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.9.m2.1b"><apply id="S3.SS2.p3.9.m2.1.1.cmml" xref="S3.SS2.p3.9.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.9.m2.1.1.1.cmml" xref="S3.SS2.p3.9.m2.1.1">subscript</csymbol><ci id="S3.SS2.p3.9.m2.1.1.2.cmml" xref="S3.SS2.p3.9.m2.1.1.2">ğ’†</ci><apply id="S3.SS2.p3.9.m2.1.1.3.cmml" xref="S3.SS2.p3.9.m2.1.1.3"><plus id="S3.SS2.p3.9.m2.1.1.3.1.cmml" xref="S3.SS2.p3.9.m2.1.1.3.1"></plus><ci id="S3.SS2.p3.9.m2.1.1.3.2.cmml" xref="S3.SS2.p3.9.m2.1.1.3.2">ğ‘›</ci><cn type="integer" id="S3.SS2.p3.9.m2.1.1.3.3.cmml" xref="S3.SS2.p3.9.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.9.m2.1c">{\bm{e}}_{n+1}</annotation></semantics></math>:</p>
<table id="A5.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3.m1.1" class="ltx_Math" alttext="\displaystyle p({\bm{w}}_{n+1})=\texttt{Softmax}(\texttt{FFN}({\bm{e}}_{n+1}))," display="inline"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E3.m1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml">ğ’˜</mi><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.cmml">n</mi><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.3.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.cmml">+</mo><mn id="S3.E3.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.3.cmml">=</mo><mrow id="S3.E3.m1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.E3.m1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.2.3a.cmml">Softmax</mtext><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.2.2.cmml">â€‹</mo><mrow id="S3.E3.m1.1.1.1.1.2.1.1" xref="S3.E3.m1.1.1.1.1.2.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.2.1.1.2" xref="S3.E3.m1.1.1.1.1.2.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.2.1.1.1" xref="S3.E3.m1.1.1.1.1.2.1.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.E3.m1.1.1.1.1.2.1.1.1.3" xref="S3.E3.m1.1.1.1.1.2.1.1.1.3a.cmml">FFN</mtext><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.2.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E3.m1.1.1.1.1.2.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.1.2.cmml">ğ’†</mi><mrow id="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.1.3.2.cmml">n</mi><mo id="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.1.3.1" xref="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.1.3.1.cmml">+</mo><mn id="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo stretchy="false" id="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E3.m1.1.1.1.1.2.1.1.3" xref="S3.E3.m1.1.1.1.1.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><eq id="S3.E3.m1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3"></eq><apply id="S3.E3.m1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"><times id="S3.E3.m1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.2"></times><ci id="S3.E3.m1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.3">ğ‘</ci><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2">ğ’˜</ci><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3"><plus id="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.1"></plus><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2">ğ‘›</ci><cn type="integer" id="S3.E3.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.3">1</cn></apply></apply></apply><apply id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2"><times id="S3.E3.m1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2"></times><ci id="S3.E3.m1.1.1.1.1.2.3a.cmml" xref="S3.E3.m1.1.1.1.1.2.3"><mtext class="ltx_mathvariant_monospace" id="S3.E3.m1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.3">Softmax</mtext></ci><apply id="S3.E3.m1.1.1.1.1.2.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.2.1.1"><times id="S3.E3.m1.1.1.1.1.2.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2.1.1.1.2"></times><ci id="S3.E3.m1.1.1.1.1.2.1.1.1.3a.cmml" xref="S3.E3.m1.1.1.1.1.2.1.1.1.3"><mtext class="ltx_mathvariant_monospace" id="S3.E3.m1.1.1.1.1.2.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.2.1.1.1.3">FFN</mtext></ci><apply id="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.2.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.1.2">ğ’†</ci><apply id="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.1.3"><plus id="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.1.3.1"></plus><ci id="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.1.3.2">ğ‘›</ci><cn type="integer" id="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.2.1.1.1.1.1.1.3.3">1</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\displaystyle p({\bm{w}}_{n+1})=\texttt{Softmax}(\texttt{FFN}({\bm{e}}_{n+1})),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p3.12" class="ltx_p">where the tokens <math id="S3.SS2.p3.10.m1.1" class="ltx_Math" alttext="{\bm{w}}_{n+1}" display="inline"><semantics id="S3.SS2.p3.10.m1.1a"><msub id="S3.SS2.p3.10.m1.1.1" xref="S3.SS2.p3.10.m1.1.1.cmml"><mi id="S3.SS2.p3.10.m1.1.1.2" xref="S3.SS2.p3.10.m1.1.1.2.cmml">ğ’˜</mi><mrow id="S3.SS2.p3.10.m1.1.1.3" xref="S3.SS2.p3.10.m1.1.1.3.cmml"><mi id="S3.SS2.p3.10.m1.1.1.3.2" xref="S3.SS2.p3.10.m1.1.1.3.2.cmml">n</mi><mo id="S3.SS2.p3.10.m1.1.1.3.1" xref="S3.SS2.p3.10.m1.1.1.3.1.cmml">+</mo><mn id="S3.SS2.p3.10.m1.1.1.3.3" xref="S3.SS2.p3.10.m1.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.10.m1.1b"><apply id="S3.SS2.p3.10.m1.1.1.cmml" xref="S3.SS2.p3.10.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.10.m1.1.1.1.cmml" xref="S3.SS2.p3.10.m1.1.1">subscript</csymbol><ci id="S3.SS2.p3.10.m1.1.1.2.cmml" xref="S3.SS2.p3.10.m1.1.1.2">ğ’˜</ci><apply id="S3.SS2.p3.10.m1.1.1.3.cmml" xref="S3.SS2.p3.10.m1.1.1.3"><plus id="S3.SS2.p3.10.m1.1.1.3.1.cmml" xref="S3.SS2.p3.10.m1.1.1.3.1"></plus><ci id="S3.SS2.p3.10.m1.1.1.3.2.cmml" xref="S3.SS2.p3.10.m1.1.1.3.2">ğ‘›</ci><cn type="integer" id="S3.SS2.p3.10.m1.1.1.3.3.cmml" xref="S3.SS2.p3.10.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.10.m1.1c">{\bm{w}}_{n+1}</annotation></semantics></math> are elements of a vocabulary <math id="S3.SS2.p3.11.m2.1" class="ltx_Math" alttext="\mathcal{W}" display="inline"><semantics id="S3.SS2.p3.11.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.11.m2.1.1" xref="S3.SS2.p3.11.m2.1.1.cmml">ğ’²</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.11.m2.1b"><ci id="S3.SS2.p3.11.m2.1.1.cmml" xref="S3.SS2.p3.11.m2.1.1">ğ’²</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.11.m2.1c">\mathcal{W}</annotation></semantics></math> that corresponding to human words in natural language. Such autoregressive generation ends when the generated language token <math id="S3.SS2.p3.12.m3.1" class="ltx_Math" alttext="{\bm{w}}_{L}" display="inline"><semantics id="S3.SS2.p3.12.m3.1a"><msub id="S3.SS2.p3.12.m3.1.1" xref="S3.SS2.p3.12.m3.1.1.cmml"><mi id="S3.SS2.p3.12.m3.1.1.2" xref="S3.SS2.p3.12.m3.1.1.2.cmml">ğ’˜</mi><mi id="S3.SS2.p3.12.m3.1.1.3" xref="S3.SS2.p3.12.m3.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.12.m3.1b"><apply id="S3.SS2.p3.12.m3.1.1.cmml" xref="S3.SS2.p3.12.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.12.m3.1.1.1.cmml" xref="S3.SS2.p3.12.m3.1.1">subscript</csymbol><ci id="S3.SS2.p3.12.m3.1.1.2.cmml" xref="S3.SS2.p3.12.m3.1.1.2">ğ’˜</ci><ci id="S3.SS2.p3.12.m3.1.1.3.cmml" xref="S3.SS2.p3.12.m3.1.1.3">ğ¿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.12.m3.1c">{\bm{w}}_{L}</annotation></semantics></math> hits the <span id="S3.SS2.p3.12.1" class="ltx_text ltx_font_typewriter">[EOS]</span> token, <span id="S3.SS2.p3.12.2" class="ltx_text ltx_font_italic">i.e.</span>, the ending of sentences.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Visual Decoder</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In order to associate object words with corresponding visual objects in given images, we propose a novel <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_italic">generate-then-detect</span> pipeline for contextual object detection. Unlike the common <span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_italic">detect-then-classify</span> pipeline in standard object detectors (<span id="S3.SS3.p1.1.3" class="ltx_text ltx_font_italic">e.g.</span>, Mask R-CNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and DETRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>) that exhaustively locate and recognize all possible objects as pre-defined categories, we consider using the LLM tokens as prior knowledge for visual detection. This allows us to detect contextual object words, while not being limited to a close set of object classes.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.5" class="ltx_p"><span id="S3.SS3.p2.5.1" class="ltx_text ltx_font_bold">Contextual LLM Tokens as Conditional Object Queries.</span>
From both language prefix <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="{\bm{t}}_{1:l}" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><msub id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">ğ’•</mi><mrow id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml"><mn id="S3.SS3.p2.1.m1.1.1.3.2" xref="S3.SS3.p2.1.m1.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S3.SS3.p2.1.m1.1.1.3.1" xref="S3.SS3.p2.1.m1.1.1.3.1.cmml">:</mo><mi id="S3.SS3.p2.1.m1.1.1.3.3" xref="S3.SS3.p2.1.m1.1.1.3.3.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">ğ’•</ci><apply id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3"><ci id="S3.SS3.p2.1.m1.1.1.3.1.cmml" xref="S3.SS3.p2.1.m1.1.1.3.1">:</ci><cn type="integer" id="S3.SS3.p2.1.m1.1.1.3.2.cmml" xref="S3.SS3.p2.1.m1.1.1.3.2">1</cn><ci id="S3.SS3.p2.1.m1.1.1.3.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3.3">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">{\bm{t}}_{1:l}</annotation></semantics></math> and generated tokens <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="{\bm{w}}_{n+1:L}" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><msub id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mi id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml">ğ’˜</mi><mrow id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml"><mrow id="S3.SS3.p2.2.m2.1.1.3.2" xref="S3.SS3.p2.2.m2.1.1.3.2.cmml"><mi id="S3.SS3.p2.2.m2.1.1.3.2.2" xref="S3.SS3.p2.2.m2.1.1.3.2.2.cmml">n</mi><mo id="S3.SS3.p2.2.m2.1.1.3.2.1" xref="S3.SS3.p2.2.m2.1.1.3.2.1.cmml">+</mo><mn id="S3.SS3.p2.2.m2.1.1.3.2.3" xref="S3.SS3.p2.2.m2.1.1.3.2.3.cmml">1</mn></mrow><mo lspace="0.278em" rspace="0.278em" id="S3.SS3.p2.2.m2.1.1.3.1" xref="S3.SS3.p2.2.m2.1.1.3.1.cmml">:</mo><mi id="S3.SS3.p2.2.m2.1.1.3.3" xref="S3.SS3.p2.2.m2.1.1.3.3.cmml">L</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2">ğ’˜</ci><apply id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3"><ci id="S3.SS3.p2.2.m2.1.1.3.1.cmml" xref="S3.SS3.p2.2.m2.1.1.3.1">:</ci><apply id="S3.SS3.p2.2.m2.1.1.3.2.cmml" xref="S3.SS3.p2.2.m2.1.1.3.2"><plus id="S3.SS3.p2.2.m2.1.1.3.2.1.cmml" xref="S3.SS3.p2.2.m2.1.1.3.2.1"></plus><ci id="S3.SS3.p2.2.m2.1.1.3.2.2.cmml" xref="S3.SS3.p2.2.m2.1.1.3.2.2">ğ‘›</ci><cn type="integer" id="S3.SS3.p2.2.m2.1.1.3.2.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3.2.3">1</cn></apply><ci id="S3.SS3.p2.2.m2.1.1.3.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3.3">ğ¿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">{\bm{w}}_{n+1:L}</annotation></semantics></math> (SectionÂ <a href="#S3.SS2" title="3.2 Multimodal Context Modeling with LLM â€£ 3 Approach â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>), we predict the binary-classification probability of noun object words.
Then, we automatically select those language tokens related to object words (<span id="S3.SS3.p2.5.2" class="ltx_text ltx_font_italic">e.g.</span>, â€˜brideâ€™, â€˜groomâ€™, â€˜dogâ€™) as contextual object tokens and
take their latent embeddings as conditional inputs for the visual decoder.
To be specific, we set up <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mi id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><ci id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">N</annotation></semantics></math> learnable object queries <math id="S3.SS3.p2.4.m4.1" class="ltx_Math" alttext="{\bm{q}}" display="inline"><semantics id="S3.SS3.p2.4.m4.1a"><mi id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml">ğ’’</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><ci id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">ğ’’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">{\bm{q}}</annotation></semantics></math>
as learnable positional embeddings in the visual decoder. For a contextual token, <span id="S3.SS3.p2.5.3" class="ltx_text ltx_font_italic">e.g.</span>, â€˜brideâ€™, we obtain the conditional object queries that linked to â€˜brideâ€™, by incorporating the corresponding latent embedding <math id="S3.SS3.p2.5.m5.1" class="ltx_Math" alttext="{\bm{e}}" display="inline"><semantics id="S3.SS3.p2.5.m5.1a"><mi id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml">ğ’†</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><ci id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1">ğ’†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">{\bm{e}}</annotation></semantics></math> from the LLM with the object queries:</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.2" class="ltx_Math" alttext="\bar{{\bm{q}}}={\bm{q}}+\texttt{Linear}(\texttt{Repeat}({\bm{e}}))." display="block"><semantics id="S3.E4.m1.2a"><mrow id="S3.E4.m1.2.2.1" xref="S3.E4.m1.2.2.1.1.cmml"><mrow id="S3.E4.m1.2.2.1.1" xref="S3.E4.m1.2.2.1.1.cmml"><mover accent="true" id="S3.E4.m1.2.2.1.1.3" xref="S3.E4.m1.2.2.1.1.3.cmml"><mi id="S3.E4.m1.2.2.1.1.3.2" xref="S3.E4.m1.2.2.1.1.3.2.cmml">ğ’’</mi><mo id="S3.E4.m1.2.2.1.1.3.1" xref="S3.E4.m1.2.2.1.1.3.1.cmml">Â¯</mo></mover><mo id="S3.E4.m1.2.2.1.1.2" xref="S3.E4.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E4.m1.2.2.1.1.1" xref="S3.E4.m1.2.2.1.1.1.cmml"><mi id="S3.E4.m1.2.2.1.1.1.3" xref="S3.E4.m1.2.2.1.1.1.3.cmml">ğ’’</mi><mo id="S3.E4.m1.2.2.1.1.1.2" xref="S3.E4.m1.2.2.1.1.1.2.cmml">+</mo><mrow id="S3.E4.m1.2.2.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.E4.m1.2.2.1.1.1.1.3" xref="S3.E4.m1.2.2.1.1.1.1.3a.cmml">Linear</mtext><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.1.1.2" xref="S3.E4.m1.2.2.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E4.m1.2.2.1.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.2.2.1.1.1.1.1.1.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.2.2.1.1.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2a.cmml">Repeat</mtext><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.cmml">â€‹</mo><mrow id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.2.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">ğ’†</mi><mo stretchy="false" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.2.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E4.m1.2.2.1.1.1.1.1.1.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em" id="S3.E4.m1.2.2.1.2" xref="S3.E4.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.2b"><apply id="S3.E4.m1.2.2.1.1.cmml" xref="S3.E4.m1.2.2.1"><eq id="S3.E4.m1.2.2.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.2"></eq><apply id="S3.E4.m1.2.2.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.3"><ci id="S3.E4.m1.2.2.1.1.3.1.cmml" xref="S3.E4.m1.2.2.1.1.3.1">Â¯</ci><ci id="S3.E4.m1.2.2.1.1.3.2.cmml" xref="S3.E4.m1.2.2.1.1.3.2">ğ’’</ci></apply><apply id="S3.E4.m1.2.2.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1"><plus id="S3.E4.m1.2.2.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1.2"></plus><ci id="S3.E4.m1.2.2.1.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.1.3">ğ’’</ci><apply id="S3.E4.m1.2.2.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1"><times id="S3.E4.m1.2.2.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.2"></times><ci id="S3.E4.m1.2.2.1.1.1.1.3a.cmml" xref="S3.E4.m1.2.2.1.1.1.1.3"><mtext class="ltx_mathvariant_monospace" id="S3.E4.m1.2.2.1.1.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.3">Linear</mtext></ci><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1"><times id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1"></times><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2a.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2"><mtext class="ltx_mathvariant_monospace" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2">Repeat</mtext></ci><ci id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1">ğ’†</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.2c">\bar{{\bm{q}}}={\bm{q}}+\texttt{Linear}(\texttt{Repeat}({\bm{e}})).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p2.8" class="ltx_p">Here, we repeat the latent embedding <math id="S3.SS3.p2.6.m1.1" class="ltx_Math" alttext="{\bm{e}}" display="inline"><semantics id="S3.SS3.p2.6.m1.1a"><mi id="S3.SS3.p2.6.m1.1.1" xref="S3.SS3.p2.6.m1.1.1.cmml">ğ’†</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m1.1b"><ci id="S3.SS3.p2.6.m1.1.1.cmml" xref="S3.SS3.p2.6.m1.1.1">ğ’†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m1.1c">{\bm{e}}</annotation></semantics></math> for â€˜brideâ€™ <math id="S3.SS3.p2.7.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS3.p2.7.m2.1a"><mi id="S3.SS3.p2.7.m2.1.1" xref="S3.SS3.p2.7.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.7.m2.1b"><ci id="S3.SS3.p2.7.m2.1.1.cmml" xref="S3.SS3.p2.7.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.7.m2.1c">N</annotation></semantics></math> times so as to align with the number of the object queries <math id="S3.SS3.p2.8.m3.1" class="ltx_Math" alttext="{\bm{q}}" display="inline"><semantics id="S3.SS3.p2.8.m3.1a"><mi id="S3.SS3.p2.8.m3.1.1" xref="S3.SS3.p2.8.m3.1.1.cmml">ğ’’</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.8.m3.1b"><ci id="S3.SS3.p2.8.m3.1.1.cmml" xref="S3.SS3.p2.8.m3.1.1">ğ’’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.8.m3.1c">{\bm{q}}</annotation></semantics></math>. Also, a linear layer is employed for dimension projection.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.2" class="ltx_p"><span id="S3.SS3.p3.2.1" class="ltx_text ltx_font_bold">Conditional Multimodal Context Decoding.</span> To model cross-modal contextual relationships, we employ six Transformer cross-attention layers in the visual decoder, in which the keys and values are obtained from the full visual tokens <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="{\bm{c}}" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mi id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">ğ’„</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">ğ’„</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">{\bm{c}}</annotation></semantics></math> extracted by the visual encoder (Section <a href="#S3.SS1" title="3.1 Visual Encoder â€£ 3 Approach â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>) while the queries are derived from the conditional object queries <math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="\bar{{\bm{q}}}" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><mover accent="true" id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml"><mi id="S3.SS3.p3.2.m2.1.1.2" xref="S3.SS3.p3.2.m2.1.1.2.cmml">ğ’’</mi><mo id="S3.SS3.p3.2.m2.1.1.1" xref="S3.SS3.p3.2.m2.1.1.1.cmml">Â¯</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><apply id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1"><ci id="S3.SS3.p3.2.m2.1.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1.1">Â¯</ci><ci id="S3.SS3.p3.2.m2.1.1.2.cmml" xref="S3.SS3.p3.2.m2.1.1.2">ğ’’</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">\bar{{\bm{q}}}</annotation></semantics></math> for computing cross-attention:</p>
<table id="A5.EGx4" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E5.m1.3" class="ltx_Math" alttext="\displaystyle\hat{{\bm{q}}}=\texttt{CrossAttenLayers}({\bm{c}},\bar{{\bm{q}}})." display="inline"><semantics id="S3.E5.m1.3a"><mrow id="S3.E5.m1.3.3.1" xref="S3.E5.m1.3.3.1.1.cmml"><mrow id="S3.E5.m1.3.3.1.1" xref="S3.E5.m1.3.3.1.1.cmml"><mover accent="true" id="S3.E5.m1.3.3.1.1.2" xref="S3.E5.m1.3.3.1.1.2.cmml"><mi id="S3.E5.m1.3.3.1.1.2.2" xref="S3.E5.m1.3.3.1.1.2.2.cmml">ğ’’</mi><mo id="S3.E5.m1.3.3.1.1.2.1" xref="S3.E5.m1.3.3.1.1.2.1.cmml">^</mo></mover><mo id="S3.E5.m1.3.3.1.1.1" xref="S3.E5.m1.3.3.1.1.1.cmml">=</mo><mrow id="S3.E5.m1.3.3.1.1.3" xref="S3.E5.m1.3.3.1.1.3.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.E5.m1.3.3.1.1.3.2" xref="S3.E5.m1.3.3.1.1.3.2a.cmml">CrossAttenLayers</mtext><mo lspace="0em" rspace="0em" id="S3.E5.m1.3.3.1.1.3.1" xref="S3.E5.m1.3.3.1.1.3.1.cmml">â€‹</mo><mrow id="S3.E5.m1.3.3.1.1.3.3.2" xref="S3.E5.m1.3.3.1.1.3.3.1.cmml"><mo stretchy="false" id="S3.E5.m1.3.3.1.1.3.3.2.1" xref="S3.E5.m1.3.3.1.1.3.3.1.cmml">(</mo><mi id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml">ğ’„</mi><mo id="S3.E5.m1.3.3.1.1.3.3.2.2" xref="S3.E5.m1.3.3.1.1.3.3.1.cmml">,</mo><mover accent="true" id="S3.E5.m1.2.2" xref="S3.E5.m1.2.2.cmml"><mi id="S3.E5.m1.2.2.2" xref="S3.E5.m1.2.2.2.cmml">ğ’’</mi><mo id="S3.E5.m1.2.2.1" xref="S3.E5.m1.2.2.1.cmml">Â¯</mo></mover><mo stretchy="false" id="S3.E5.m1.3.3.1.1.3.3.2.3" xref="S3.E5.m1.3.3.1.1.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S3.E5.m1.3.3.1.2" xref="S3.E5.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.3b"><apply id="S3.E5.m1.3.3.1.1.cmml" xref="S3.E5.m1.3.3.1"><eq id="S3.E5.m1.3.3.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.1"></eq><apply id="S3.E5.m1.3.3.1.1.2.cmml" xref="S3.E5.m1.3.3.1.1.2"><ci id="S3.E5.m1.3.3.1.1.2.1.cmml" xref="S3.E5.m1.3.3.1.1.2.1">^</ci><ci id="S3.E5.m1.3.3.1.1.2.2.cmml" xref="S3.E5.m1.3.3.1.1.2.2">ğ’’</ci></apply><apply id="S3.E5.m1.3.3.1.1.3.cmml" xref="S3.E5.m1.3.3.1.1.3"><times id="S3.E5.m1.3.3.1.1.3.1.cmml" xref="S3.E5.m1.3.3.1.1.3.1"></times><ci id="S3.E5.m1.3.3.1.1.3.2a.cmml" xref="S3.E5.m1.3.3.1.1.3.2"><mtext class="ltx_mathvariant_monospace" id="S3.E5.m1.3.3.1.1.3.2.cmml" xref="S3.E5.m1.3.3.1.1.3.2">CrossAttenLayers</mtext></ci><interval closure="open" id="S3.E5.m1.3.3.1.1.3.3.1.cmml" xref="S3.E5.m1.3.3.1.1.3.3.2"><ci id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1">ğ’„</ci><apply id="S3.E5.m1.2.2.cmml" xref="S3.E5.m1.2.2"><ci id="S3.E5.m1.2.2.1.cmml" xref="S3.E5.m1.2.2.1">Â¯</ci><ci id="S3.E5.m1.2.2.2.cmml" xref="S3.E5.m1.2.2.2">ğ’’</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.3c">\displaystyle\hat{{\bm{q}}}=\texttt{CrossAttenLayers}({\bm{c}},\bar{{\bm{q}}}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p3.3" class="ltx_p">By doing so, the visual decoder learns to focus on specific areas of the visual context that are relevant to the conditional query for â€˜brideâ€™.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para ltx_noindent">
<p id="S3.SS3.p4.2" class="ltx_p"><span id="S3.SS3.p4.2.1" class="ltx_text ltx_font_bold">Box and Matching Predictions for Contextual Words.</span>
Finally, we compute the binary matching score and box prediction from the output latent embedding <math id="S3.SS3.p4.1.m1.1" class="ltx_Math" alttext="\hat{{\bm{q}}}" display="inline"><semantics id="S3.SS3.p4.1.m1.1a"><mover accent="true" id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml"><mi id="S3.SS3.p4.1.m1.1.1.2" xref="S3.SS3.p4.1.m1.1.1.2.cmml">ğ’’</mi><mo id="S3.SS3.p4.1.m1.1.1.1" xref="S3.SS3.p4.1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><apply id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1"><ci id="S3.SS3.p4.1.m1.1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1.1">^</ci><ci id="S3.SS3.p4.1.m1.1.1.2.cmml" xref="S3.SS3.p4.1.m1.1.1.2">ğ’’</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">\hat{{\bm{q}}}</annotation></semantics></math> using two <span id="S3.SS3.p4.2.2" class="ltx_text ltx_markedasmath ltx_font_typewriter">FFN</span> prediction heads:</p>
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E6.m1.3" class="ltx_Math" alttext="{\bm{p}}=\texttt{FFN}_{\text{cls}}(\hat{{\bm{q}}})\in\mathbb{R}^{N\times 2},{\bm{b}}=\texttt{FFN}_{\text{box}}(\hat{{\bm{q}}})\in\mathbb{R}^{N\times 4}," display="block"><semantics id="S3.E6.m1.3a"><mrow id="S3.E6.m1.3.3.1"><mrow id="S3.E6.m1.3.3.1.1.2" xref="S3.E6.m1.3.3.1.1.3.cmml"><mrow id="S3.E6.m1.3.3.1.1.1.1" xref="S3.E6.m1.3.3.1.1.1.1.cmml"><mi id="S3.E6.m1.3.3.1.1.1.1.2" xref="S3.E6.m1.3.3.1.1.1.1.2.cmml">ğ’‘</mi><mo id="S3.E6.m1.3.3.1.1.1.1.3" xref="S3.E6.m1.3.3.1.1.1.1.3.cmml">=</mo><mrow id="S3.E6.m1.3.3.1.1.1.1.4" xref="S3.E6.m1.3.3.1.1.1.1.4.cmml"><msub id="S3.E6.m1.3.3.1.1.1.1.4.2" xref="S3.E6.m1.3.3.1.1.1.1.4.2.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.E6.m1.3.3.1.1.1.1.4.2.2" xref="S3.E6.m1.3.3.1.1.1.1.4.2.2a.cmml">FFN</mtext><mtext id="S3.E6.m1.3.3.1.1.1.1.4.2.3" xref="S3.E6.m1.3.3.1.1.1.1.4.2.3a.cmml">cls</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E6.m1.3.3.1.1.1.1.4.1" xref="S3.E6.m1.3.3.1.1.1.1.4.1.cmml">â€‹</mo><mrow id="S3.E6.m1.3.3.1.1.1.1.4.3.2" xref="S3.E6.m1.1.1.cmml"><mo stretchy="false" id="S3.E6.m1.3.3.1.1.1.1.4.3.2.1" xref="S3.E6.m1.1.1.cmml">(</mo><mover accent="true" id="S3.E6.m1.1.1" xref="S3.E6.m1.1.1.cmml"><mi id="S3.E6.m1.1.1.2" xref="S3.E6.m1.1.1.2.cmml">ğ’’</mi><mo id="S3.E6.m1.1.1.1" xref="S3.E6.m1.1.1.1.cmml">^</mo></mover><mo stretchy="false" id="S3.E6.m1.3.3.1.1.1.1.4.3.2.2" xref="S3.E6.m1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E6.m1.3.3.1.1.1.1.5" xref="S3.E6.m1.3.3.1.1.1.1.5.cmml">âˆˆ</mo><msup id="S3.E6.m1.3.3.1.1.1.1.6" xref="S3.E6.m1.3.3.1.1.1.1.6.cmml"><mi id="S3.E6.m1.3.3.1.1.1.1.6.2" xref="S3.E6.m1.3.3.1.1.1.1.6.2.cmml">â„</mi><mrow id="S3.E6.m1.3.3.1.1.1.1.6.3" xref="S3.E6.m1.3.3.1.1.1.1.6.3.cmml"><mi id="S3.E6.m1.3.3.1.1.1.1.6.3.2" xref="S3.E6.m1.3.3.1.1.1.1.6.3.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E6.m1.3.3.1.1.1.1.6.3.1" xref="S3.E6.m1.3.3.1.1.1.1.6.3.1.cmml">Ã—</mo><mn id="S3.E6.m1.3.3.1.1.1.1.6.3.3" xref="S3.E6.m1.3.3.1.1.1.1.6.3.3.cmml">2</mn></mrow></msup></mrow><mo id="S3.E6.m1.3.3.1.1.2.3" xref="S3.E6.m1.3.3.1.1.3a.cmml">,</mo><mrow id="S3.E6.m1.3.3.1.1.2.2" xref="S3.E6.m1.3.3.1.1.2.2.cmml"><mi id="S3.E6.m1.3.3.1.1.2.2.2" xref="S3.E6.m1.3.3.1.1.2.2.2.cmml">ğ’ƒ</mi><mo id="S3.E6.m1.3.3.1.1.2.2.3" xref="S3.E6.m1.3.3.1.1.2.2.3.cmml">=</mo><mrow id="S3.E6.m1.3.3.1.1.2.2.4" xref="S3.E6.m1.3.3.1.1.2.2.4.cmml"><msub id="S3.E6.m1.3.3.1.1.2.2.4.2" xref="S3.E6.m1.3.3.1.1.2.2.4.2.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.E6.m1.3.3.1.1.2.2.4.2.2" xref="S3.E6.m1.3.3.1.1.2.2.4.2.2a.cmml">FFN</mtext><mtext id="S3.E6.m1.3.3.1.1.2.2.4.2.3" xref="S3.E6.m1.3.3.1.1.2.2.4.2.3a.cmml">box</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E6.m1.3.3.1.1.2.2.4.1" xref="S3.E6.m1.3.3.1.1.2.2.4.1.cmml">â€‹</mo><mrow id="S3.E6.m1.3.3.1.1.2.2.4.3.2" xref="S3.E6.m1.2.2.cmml"><mo stretchy="false" id="S3.E6.m1.3.3.1.1.2.2.4.3.2.1" xref="S3.E6.m1.2.2.cmml">(</mo><mover accent="true" id="S3.E6.m1.2.2" xref="S3.E6.m1.2.2.cmml"><mi id="S3.E6.m1.2.2.2" xref="S3.E6.m1.2.2.2.cmml">ğ’’</mi><mo id="S3.E6.m1.2.2.1" xref="S3.E6.m1.2.2.1.cmml">^</mo></mover><mo stretchy="false" id="S3.E6.m1.3.3.1.1.2.2.4.3.2.2" xref="S3.E6.m1.2.2.cmml">)</mo></mrow></mrow><mo id="S3.E6.m1.3.3.1.1.2.2.5" xref="S3.E6.m1.3.3.1.1.2.2.5.cmml">âˆˆ</mo><msup id="S3.E6.m1.3.3.1.1.2.2.6" xref="S3.E6.m1.3.3.1.1.2.2.6.cmml"><mi id="S3.E6.m1.3.3.1.1.2.2.6.2" xref="S3.E6.m1.3.3.1.1.2.2.6.2.cmml">â„</mi><mrow id="S3.E6.m1.3.3.1.1.2.2.6.3" xref="S3.E6.m1.3.3.1.1.2.2.6.3.cmml"><mi id="S3.E6.m1.3.3.1.1.2.2.6.3.2" xref="S3.E6.m1.3.3.1.1.2.2.6.3.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E6.m1.3.3.1.1.2.2.6.3.1" xref="S3.E6.m1.3.3.1.1.2.2.6.3.1.cmml">Ã—</mo><mn id="S3.E6.m1.3.3.1.1.2.2.6.3.3" xref="S3.E6.m1.3.3.1.1.2.2.6.3.3.cmml">4</mn></mrow></msup></mrow></mrow><mo id="S3.E6.m1.3.3.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.3b"><apply id="S3.E6.m1.3.3.1.1.3.cmml" xref="S3.E6.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.3.3.1.1.3a.cmml" xref="S3.E6.m1.3.3.1.1.2.3">formulae-sequence</csymbol><apply id="S3.E6.m1.3.3.1.1.1.1.cmml" xref="S3.E6.m1.3.3.1.1.1.1"><and id="S3.E6.m1.3.3.1.1.1.1a.cmml" xref="S3.E6.m1.3.3.1.1.1.1"></and><apply id="S3.E6.m1.3.3.1.1.1.1b.cmml" xref="S3.E6.m1.3.3.1.1.1.1"><eq id="S3.E6.m1.3.3.1.1.1.1.3.cmml" xref="S3.E6.m1.3.3.1.1.1.1.3"></eq><ci id="S3.E6.m1.3.3.1.1.1.1.2.cmml" xref="S3.E6.m1.3.3.1.1.1.1.2">ğ’‘</ci><apply id="S3.E6.m1.3.3.1.1.1.1.4.cmml" xref="S3.E6.m1.3.3.1.1.1.1.4"><times id="S3.E6.m1.3.3.1.1.1.1.4.1.cmml" xref="S3.E6.m1.3.3.1.1.1.1.4.1"></times><apply id="S3.E6.m1.3.3.1.1.1.1.4.2.cmml" xref="S3.E6.m1.3.3.1.1.1.1.4.2"><csymbol cd="ambiguous" id="S3.E6.m1.3.3.1.1.1.1.4.2.1.cmml" xref="S3.E6.m1.3.3.1.1.1.1.4.2">subscript</csymbol><ci id="S3.E6.m1.3.3.1.1.1.1.4.2.2a.cmml" xref="S3.E6.m1.3.3.1.1.1.1.4.2.2"><mtext class="ltx_mathvariant_monospace" id="S3.E6.m1.3.3.1.1.1.1.4.2.2.cmml" xref="S3.E6.m1.3.3.1.1.1.1.4.2.2">FFN</mtext></ci><ci id="S3.E6.m1.3.3.1.1.1.1.4.2.3a.cmml" xref="S3.E6.m1.3.3.1.1.1.1.4.2.3"><mtext mathsize="70%" id="S3.E6.m1.3.3.1.1.1.1.4.2.3.cmml" xref="S3.E6.m1.3.3.1.1.1.1.4.2.3">cls</mtext></ci></apply><apply id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.3.3.1.1.1.1.4.3.2"><ci id="S3.E6.m1.1.1.1.cmml" xref="S3.E6.m1.1.1.1">^</ci><ci id="S3.E6.m1.1.1.2.cmml" xref="S3.E6.m1.1.1.2">ğ’’</ci></apply></apply></apply><apply id="S3.E6.m1.3.3.1.1.1.1c.cmml" xref="S3.E6.m1.3.3.1.1.1.1"><in id="S3.E6.m1.3.3.1.1.1.1.5.cmml" xref="S3.E6.m1.3.3.1.1.1.1.5"></in><share href="#S3.E6.m1.3.3.1.1.1.1.4.cmml" id="S3.E6.m1.3.3.1.1.1.1d.cmml" xref="S3.E6.m1.3.3.1.1.1.1"></share><apply id="S3.E6.m1.3.3.1.1.1.1.6.cmml" xref="S3.E6.m1.3.3.1.1.1.1.6"><csymbol cd="ambiguous" id="S3.E6.m1.3.3.1.1.1.1.6.1.cmml" xref="S3.E6.m1.3.3.1.1.1.1.6">superscript</csymbol><ci id="S3.E6.m1.3.3.1.1.1.1.6.2.cmml" xref="S3.E6.m1.3.3.1.1.1.1.6.2">â„</ci><apply id="S3.E6.m1.3.3.1.1.1.1.6.3.cmml" xref="S3.E6.m1.3.3.1.1.1.1.6.3"><times id="S3.E6.m1.3.3.1.1.1.1.6.3.1.cmml" xref="S3.E6.m1.3.3.1.1.1.1.6.3.1"></times><ci id="S3.E6.m1.3.3.1.1.1.1.6.3.2.cmml" xref="S3.E6.m1.3.3.1.1.1.1.6.3.2">ğ‘</ci><cn type="integer" id="S3.E6.m1.3.3.1.1.1.1.6.3.3.cmml" xref="S3.E6.m1.3.3.1.1.1.1.6.3.3">2</cn></apply></apply></apply></apply><apply id="S3.E6.m1.3.3.1.1.2.2.cmml" xref="S3.E6.m1.3.3.1.1.2.2"><and id="S3.E6.m1.3.3.1.1.2.2a.cmml" xref="S3.E6.m1.3.3.1.1.2.2"></and><apply id="S3.E6.m1.3.3.1.1.2.2b.cmml" xref="S3.E6.m1.3.3.1.1.2.2"><eq id="S3.E6.m1.3.3.1.1.2.2.3.cmml" xref="S3.E6.m1.3.3.1.1.2.2.3"></eq><ci id="S3.E6.m1.3.3.1.1.2.2.2.cmml" xref="S3.E6.m1.3.3.1.1.2.2.2">ğ’ƒ</ci><apply id="S3.E6.m1.3.3.1.1.2.2.4.cmml" xref="S3.E6.m1.3.3.1.1.2.2.4"><times id="S3.E6.m1.3.3.1.1.2.2.4.1.cmml" xref="S3.E6.m1.3.3.1.1.2.2.4.1"></times><apply id="S3.E6.m1.3.3.1.1.2.2.4.2.cmml" xref="S3.E6.m1.3.3.1.1.2.2.4.2"><csymbol cd="ambiguous" id="S3.E6.m1.3.3.1.1.2.2.4.2.1.cmml" xref="S3.E6.m1.3.3.1.1.2.2.4.2">subscript</csymbol><ci id="S3.E6.m1.3.3.1.1.2.2.4.2.2a.cmml" xref="S3.E6.m1.3.3.1.1.2.2.4.2.2"><mtext class="ltx_mathvariant_monospace" id="S3.E6.m1.3.3.1.1.2.2.4.2.2.cmml" xref="S3.E6.m1.3.3.1.1.2.2.4.2.2">FFN</mtext></ci><ci id="S3.E6.m1.3.3.1.1.2.2.4.2.3a.cmml" xref="S3.E6.m1.3.3.1.1.2.2.4.2.3"><mtext mathsize="70%" id="S3.E6.m1.3.3.1.1.2.2.4.2.3.cmml" xref="S3.E6.m1.3.3.1.1.2.2.4.2.3">box</mtext></ci></apply><apply id="S3.E6.m1.2.2.cmml" xref="S3.E6.m1.3.3.1.1.2.2.4.3.2"><ci id="S3.E6.m1.2.2.1.cmml" xref="S3.E6.m1.2.2.1">^</ci><ci id="S3.E6.m1.2.2.2.cmml" xref="S3.E6.m1.2.2.2">ğ’’</ci></apply></apply></apply><apply id="S3.E6.m1.3.3.1.1.2.2c.cmml" xref="S3.E6.m1.3.3.1.1.2.2"><in id="S3.E6.m1.3.3.1.1.2.2.5.cmml" xref="S3.E6.m1.3.3.1.1.2.2.5"></in><share href="#S3.E6.m1.3.3.1.1.2.2.4.cmml" id="S3.E6.m1.3.3.1.1.2.2d.cmml" xref="S3.E6.m1.3.3.1.1.2.2"></share><apply id="S3.E6.m1.3.3.1.1.2.2.6.cmml" xref="S3.E6.m1.3.3.1.1.2.2.6"><csymbol cd="ambiguous" id="S3.E6.m1.3.3.1.1.2.2.6.1.cmml" xref="S3.E6.m1.3.3.1.1.2.2.6">superscript</csymbol><ci id="S3.E6.m1.3.3.1.1.2.2.6.2.cmml" xref="S3.E6.m1.3.3.1.1.2.2.6.2">â„</ci><apply id="S3.E6.m1.3.3.1.1.2.2.6.3.cmml" xref="S3.E6.m1.3.3.1.1.2.2.6.3"><times id="S3.E6.m1.3.3.1.1.2.2.6.3.1.cmml" xref="S3.E6.m1.3.3.1.1.2.2.6.3.1"></times><ci id="S3.E6.m1.3.3.1.1.2.2.6.3.2.cmml" xref="S3.E6.m1.3.3.1.1.2.2.6.3.2">ğ‘</ci><cn type="integer" id="S3.E6.m1.3.3.1.1.2.2.6.3.3.cmml" xref="S3.E6.m1.3.3.1.1.2.2.6.3.3">4</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.3c">{\bm{p}}=\texttt{FFN}_{\text{cls}}(\hat{{\bm{q}}})\in\mathbb{R}^{N\times 2},{\bm{b}}=\texttt{FFN}_{\text{box}}(\hat{{\bm{q}}})\in\mathbb{R}^{N\times 4},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.2" class="ltx_p">where <math id="S3.SS3.p5.1.m1.1" class="ltx_Math" alttext="{\bm{p}}" display="inline"><semantics id="S3.SS3.p5.1.m1.1a"><mi id="S3.SS3.p5.1.m1.1.1" xref="S3.SS3.p5.1.m1.1.1.cmml">ğ’‘</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.1.m1.1b"><ci id="S3.SS3.p5.1.m1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1">ğ’‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.1.m1.1c">{\bm{p}}</annotation></semantics></math> refers to the probability of being matched or not matched given the conditional object word, and <math id="S3.SS3.p5.2.m2.1" class="ltx_Math" alttext="{\bm{b}}" display="inline"><semantics id="S3.SS3.p5.2.m2.1a"><mi id="S3.SS3.p5.2.m2.1.1" xref="S3.SS3.p5.2.m2.1.1.cmml">ğ’ƒ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.2.m2.1b"><ci id="S3.SS3.p5.2.m2.1.1.cmml" xref="S3.SS3.p5.2.m2.1.1">ğ’ƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.2.m2.1c">{\bm{b}}</annotation></semantics></math> indicates the predicted box coordinates.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para ltx_noindent">
<p id="S3.SS3.p6.1" class="ltx_p"><span id="S3.SS3.p6.1.1" class="ltx_text ltx_font_bold">Conditional Matching for Label Assignment.</span> We introduce a conditional modification to the default optimal bipartite matching in DETRÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> that finds the best match between the set of <math id="S3.SS3.p6.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS3.p6.1.m1.1a"><mi id="S3.SS3.p6.1.m1.1.1" xref="S3.SS3.p6.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.1.m1.1b"><ci id="S3.SS3.p6.1.m1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.1.m1.1c">N</annotation></semantics></math> predictions and the set of ground truth objects.
In our approach, only the ground-truth bounding boxes that match the conditional object words are involved in the loss computation.
This conditional matching ensures that the model focuses solely on the objects described by the language queries.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Training Details</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">We use multi-scale deformable attentionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite> and IoU-based label assignmentÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> to accelerate the convergence speed.
The vision encoder <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="\phi" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mi id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">Ï•</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">italic-Ï•</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">\phi</annotation></semantics></math> also supports the pre-trained weights from previous MLLM such as BLIP-2Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para ltx_noindent">
<p id="S3.SS4.p2.6" class="ltx_p"><span id="S3.SS4.p2.6.1" class="ltx_text ltx_font_bold">Loss Function.</span>
In SectionÂ <a href="#S3.SS3" title="3.3 Visual Decoder â€£ 3 Approach â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>, we use conditional matching to derive the label assignments, which include the ground-truth matching labels <math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="\bm{\hat{{\bm{p}}}}" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><mover accent="true" id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml"><mi id="S3.SS4.p2.1.m1.1.1.2" xref="S3.SS4.p2.1.m1.1.1.2.cmml">ğ’‘</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S3.SS4.p2.1.m1.1.1.1" xref="S3.SS4.p2.1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><apply id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1"><ci id="S3.SS4.p2.1.m1.1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1.1">bold-^</ci><ci id="S3.SS4.p2.1.m1.1.1.2.cmml" xref="S3.SS4.p2.1.m1.1.1.2">ğ’‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">\bm{\hat{{\bm{p}}}}</annotation></semantics></math> and the associated box coordinates <math id="S3.SS4.p2.2.m2.1" class="ltx_Math" alttext="\bm{\hat{b}}" display="inline"><semantics id="S3.SS4.p2.2.m2.1a"><mover accent="true" id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml"><mi id="S3.SS4.p2.2.m2.1.1.2" xref="S3.SS4.p2.2.m2.1.1.2.cmml">ğ’ƒ</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S3.SS4.p2.2.m2.1.1.1" xref="S3.SS4.p2.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><apply id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1"><ci id="S3.SS4.p2.2.m2.1.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1.1">bold-^</ci><ci id="S3.SS4.p2.2.m2.1.1.2.cmml" xref="S3.SS4.p2.2.m2.1.1.2">ğ’ƒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">\bm{\hat{b}}</annotation></semantics></math>.
For our predicted language token <math id="S3.SS4.p2.3.m3.1" class="ltx_Math" alttext="{\bm{w}}" display="inline"><semantics id="S3.SS4.p2.3.m3.1a"><mi id="S3.SS4.p2.3.m3.1.1" xref="S3.SS4.p2.3.m3.1.1.cmml">ğ’˜</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.3.m3.1b"><ci id="S3.SS4.p2.3.m3.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1">ğ’˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.3.m3.1c">{\bm{w}}</annotation></semantics></math>, we can straightforwardly get the annotated ground truth token <math id="S3.SS4.p2.4.m4.1" class="ltx_Math" alttext="\hat{{\bm{w}}}" display="inline"><semantics id="S3.SS4.p2.4.m4.1a"><mover accent="true" id="S3.SS4.p2.4.m4.1.1" xref="S3.SS4.p2.4.m4.1.1.cmml"><mi id="S3.SS4.p2.4.m4.1.1.2" xref="S3.SS4.p2.4.m4.1.1.2.cmml">ğ’˜</mi><mo id="S3.SS4.p2.4.m4.1.1.1" xref="S3.SS4.p2.4.m4.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.4.m4.1b"><apply id="S3.SS4.p2.4.m4.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1"><ci id="S3.SS4.p2.4.m4.1.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1.1">^</ci><ci id="S3.SS4.p2.4.m4.1.1.2.cmml" xref="S3.SS4.p2.4.m4.1.1.2">ğ’˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.4.m4.1c">\hat{{\bm{w}}}</annotation></semantics></math>, <span id="S3.SS4.p2.6.2" class="ltx_text ltx_font_italic">e.g.</span>, tokenized answers for the cloze test.
We can also obtain the annotated binary label <math id="S3.SS4.p2.5.m5.1" class="ltx_Math" alttext="\bar{{\bm{w}}}" display="inline"><semantics id="S3.SS4.p2.5.m5.1a"><mover accent="true" id="S3.SS4.p2.5.m5.1.1" xref="S3.SS4.p2.5.m5.1.1.cmml"><mi id="S3.SS4.p2.5.m5.1.1.2" xref="S3.SS4.p2.5.m5.1.1.2.cmml">ğ’˜</mi><mo id="S3.SS4.p2.5.m5.1.1.1" xref="S3.SS4.p2.5.m5.1.1.1.cmml">Â¯</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.5.m5.1b"><apply id="S3.SS4.p2.5.m5.1.1.cmml" xref="S3.SS4.p2.5.m5.1.1"><ci id="S3.SS4.p2.5.m5.1.1.1.cmml" xref="S3.SS4.p2.5.m5.1.1.1">Â¯</ci><ci id="S3.SS4.p2.5.m5.1.1.2.cmml" xref="S3.SS4.p2.5.m5.1.1.2">ğ’˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.5.m5.1c">\bar{{\bm{w}}}</annotation></semantics></math> indicating whether a token belongs to an object word or not.
Based on the label assignment results, the overall loss function <math id="S3.SS4.p2.6.m6.1" class="ltx_Math" alttext="\mathcal{L}" display="inline"><semantics id="S3.SS4.p2.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.6.m6.1.1" xref="S3.SS4.p2.6.m6.1.1.cmml">â„’</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.6.m6.1b"><ci id="S3.SS4.p2.6.m6.1.1.cmml" xref="S3.SS4.p2.6.m6.1.1">â„’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.6.m6.1c">\mathcal{L}</annotation></semantics></math> is defined as:</p>
<table id="S3.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E7.m1.8" class="ltx_Math" alttext="\mathcal{L}=\lambda_{\text{cls}}\mathcal{L}_{\text{cls}}\left({\bm{p}},\hat{{\bm{p}}}\right)+\lambda_{\text{box}}\mathcal{L}_{\text{box}}(\bm{b},\hat{\bm{b}})+\lambda_{\text{lm}}\mathcal{L}_{\text{lm}}(\bm{w},\hat{\bm{w}})+\lambda_{\text{noun}}\mathcal{L}_{\text{noun}}({\bm{w}},\bar{{\bm{w}}})" display="block"><semantics id="S3.E7.m1.8a"><mrow id="S3.E7.m1.8.9" xref="S3.E7.m1.8.9.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E7.m1.8.9.2" xref="S3.E7.m1.8.9.2.cmml">â„’</mi><mo id="S3.E7.m1.8.9.1" xref="S3.E7.m1.8.9.1.cmml">=</mo><mrow id="S3.E7.m1.8.9.3" xref="S3.E7.m1.8.9.3.cmml"><mrow id="S3.E7.m1.8.9.3.2" xref="S3.E7.m1.8.9.3.2.cmml"><msub id="S3.E7.m1.8.9.3.2.2" xref="S3.E7.m1.8.9.3.2.2.cmml"><mi id="S3.E7.m1.8.9.3.2.2.2" xref="S3.E7.m1.8.9.3.2.2.2.cmml">Î»</mi><mtext id="S3.E7.m1.8.9.3.2.2.3" xref="S3.E7.m1.8.9.3.2.2.3a.cmml">cls</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E7.m1.8.9.3.2.1" xref="S3.E7.m1.8.9.3.2.1.cmml">â€‹</mo><msub id="S3.E7.m1.8.9.3.2.3" xref="S3.E7.m1.8.9.3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E7.m1.8.9.3.2.3.2" xref="S3.E7.m1.8.9.3.2.3.2.cmml">â„’</mi><mtext id="S3.E7.m1.8.9.3.2.3.3" xref="S3.E7.m1.8.9.3.2.3.3a.cmml">cls</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E7.m1.8.9.3.2.1a" xref="S3.E7.m1.8.9.3.2.1.cmml">â€‹</mo><mrow id="S3.E7.m1.8.9.3.2.4.2" xref="S3.E7.m1.8.9.3.2.4.1.cmml"><mo id="S3.E7.m1.8.9.3.2.4.2.1" xref="S3.E7.m1.8.9.3.2.4.1.cmml">(</mo><mi id="S3.E7.m1.1.1" xref="S3.E7.m1.1.1.cmml">ğ’‘</mi><mo id="S3.E7.m1.8.9.3.2.4.2.2" xref="S3.E7.m1.8.9.3.2.4.1.cmml">,</mo><mover accent="true" id="S3.E7.m1.2.2" xref="S3.E7.m1.2.2.cmml"><mi id="S3.E7.m1.2.2.2" xref="S3.E7.m1.2.2.2.cmml">ğ’‘</mi><mo id="S3.E7.m1.2.2.1" xref="S3.E7.m1.2.2.1.cmml">^</mo></mover><mo id="S3.E7.m1.8.9.3.2.4.2.3" xref="S3.E7.m1.8.9.3.2.4.1.cmml">)</mo></mrow></mrow><mo id="S3.E7.m1.8.9.3.1" xref="S3.E7.m1.8.9.3.1.cmml">+</mo><mrow id="S3.E7.m1.8.9.3.3" xref="S3.E7.m1.8.9.3.3.cmml"><msub id="S3.E7.m1.8.9.3.3.2" xref="S3.E7.m1.8.9.3.3.2.cmml"><mi id="S3.E7.m1.8.9.3.3.2.2" xref="S3.E7.m1.8.9.3.3.2.2.cmml">Î»</mi><mtext id="S3.E7.m1.8.9.3.3.2.3" xref="S3.E7.m1.8.9.3.3.2.3a.cmml">box</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E7.m1.8.9.3.3.1" xref="S3.E7.m1.8.9.3.3.1.cmml">â€‹</mo><msub id="S3.E7.m1.8.9.3.3.3" xref="S3.E7.m1.8.9.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E7.m1.8.9.3.3.3.2" xref="S3.E7.m1.8.9.3.3.3.2.cmml">â„’</mi><mtext id="S3.E7.m1.8.9.3.3.3.3" xref="S3.E7.m1.8.9.3.3.3.3a.cmml">box</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E7.m1.8.9.3.3.1a" xref="S3.E7.m1.8.9.3.3.1.cmml">â€‹</mo><mrow id="S3.E7.m1.8.9.3.3.4.2" xref="S3.E7.m1.8.9.3.3.4.1.cmml"><mo stretchy="false" id="S3.E7.m1.8.9.3.3.4.2.1" xref="S3.E7.m1.8.9.3.3.4.1.cmml">(</mo><mi id="S3.E7.m1.3.3" xref="S3.E7.m1.3.3.cmml">ğ’ƒ</mi><mo id="S3.E7.m1.8.9.3.3.4.2.2" xref="S3.E7.m1.8.9.3.3.4.1.cmml">,</mo><mover accent="true" id="S3.E7.m1.4.4" xref="S3.E7.m1.4.4.cmml"><mi id="S3.E7.m1.4.4.2" xref="S3.E7.m1.4.4.2.cmml">ğ’ƒ</mi><mo id="S3.E7.m1.4.4.1" xref="S3.E7.m1.4.4.1.cmml">^</mo></mover><mo stretchy="false" id="S3.E7.m1.8.9.3.3.4.2.3" xref="S3.E7.m1.8.9.3.3.4.1.cmml">)</mo></mrow></mrow><mo id="S3.E7.m1.8.9.3.1a" xref="S3.E7.m1.8.9.3.1.cmml">+</mo><mrow id="S3.E7.m1.8.9.3.4" xref="S3.E7.m1.8.9.3.4.cmml"><msub id="S3.E7.m1.8.9.3.4.2" xref="S3.E7.m1.8.9.3.4.2.cmml"><mi id="S3.E7.m1.8.9.3.4.2.2" xref="S3.E7.m1.8.9.3.4.2.2.cmml">Î»</mi><mtext id="S3.E7.m1.8.9.3.4.2.3" xref="S3.E7.m1.8.9.3.4.2.3a.cmml">lm</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E7.m1.8.9.3.4.1" xref="S3.E7.m1.8.9.3.4.1.cmml">â€‹</mo><msub id="S3.E7.m1.8.9.3.4.3" xref="S3.E7.m1.8.9.3.4.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E7.m1.8.9.3.4.3.2" xref="S3.E7.m1.8.9.3.4.3.2.cmml">â„’</mi><mtext id="S3.E7.m1.8.9.3.4.3.3" xref="S3.E7.m1.8.9.3.4.3.3a.cmml">lm</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E7.m1.8.9.3.4.1a" xref="S3.E7.m1.8.9.3.4.1.cmml">â€‹</mo><mrow id="S3.E7.m1.8.9.3.4.4.2" xref="S3.E7.m1.8.9.3.4.4.1.cmml"><mo stretchy="false" id="S3.E7.m1.8.9.3.4.4.2.1" xref="S3.E7.m1.8.9.3.4.4.1.cmml">(</mo><mi id="S3.E7.m1.5.5" xref="S3.E7.m1.5.5.cmml">ğ’˜</mi><mo id="S3.E7.m1.8.9.3.4.4.2.2" xref="S3.E7.m1.8.9.3.4.4.1.cmml">,</mo><mover accent="true" id="S3.E7.m1.6.6" xref="S3.E7.m1.6.6.cmml"><mi id="S3.E7.m1.6.6.2" xref="S3.E7.m1.6.6.2.cmml">ğ’˜</mi><mo id="S3.E7.m1.6.6.1" xref="S3.E7.m1.6.6.1.cmml">^</mo></mover><mo stretchy="false" id="S3.E7.m1.8.9.3.4.4.2.3" xref="S3.E7.m1.8.9.3.4.4.1.cmml">)</mo></mrow></mrow><mo id="S3.E7.m1.8.9.3.1b" xref="S3.E7.m1.8.9.3.1.cmml">+</mo><mrow id="S3.E7.m1.8.9.3.5" xref="S3.E7.m1.8.9.3.5.cmml"><msub id="S3.E7.m1.8.9.3.5.2" xref="S3.E7.m1.8.9.3.5.2.cmml"><mi id="S3.E7.m1.8.9.3.5.2.2" xref="S3.E7.m1.8.9.3.5.2.2.cmml">Î»</mi><mtext id="S3.E7.m1.8.9.3.5.2.3" xref="S3.E7.m1.8.9.3.5.2.3a.cmml">noun</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E7.m1.8.9.3.5.1" xref="S3.E7.m1.8.9.3.5.1.cmml">â€‹</mo><msub id="S3.E7.m1.8.9.3.5.3" xref="S3.E7.m1.8.9.3.5.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E7.m1.8.9.3.5.3.2" xref="S3.E7.m1.8.9.3.5.3.2.cmml">â„’</mi><mtext id="S3.E7.m1.8.9.3.5.3.3" xref="S3.E7.m1.8.9.3.5.3.3a.cmml">noun</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E7.m1.8.9.3.5.1a" xref="S3.E7.m1.8.9.3.5.1.cmml">â€‹</mo><mrow id="S3.E7.m1.8.9.3.5.4.2" xref="S3.E7.m1.8.9.3.5.4.1.cmml"><mo stretchy="false" id="S3.E7.m1.8.9.3.5.4.2.1" xref="S3.E7.m1.8.9.3.5.4.1.cmml">(</mo><mi id="S3.E7.m1.7.7" xref="S3.E7.m1.7.7.cmml">ğ’˜</mi><mo id="S3.E7.m1.8.9.3.5.4.2.2" xref="S3.E7.m1.8.9.3.5.4.1.cmml">,</mo><mover accent="true" id="S3.E7.m1.8.8" xref="S3.E7.m1.8.8.cmml"><mi id="S3.E7.m1.8.8.2" xref="S3.E7.m1.8.8.2.cmml">ğ’˜</mi><mo id="S3.E7.m1.8.8.1" xref="S3.E7.m1.8.8.1.cmml">Â¯</mo></mover><mo stretchy="false" id="S3.E7.m1.8.9.3.5.4.2.3" xref="S3.E7.m1.8.9.3.5.4.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.8b"><apply id="S3.E7.m1.8.9.cmml" xref="S3.E7.m1.8.9"><eq id="S3.E7.m1.8.9.1.cmml" xref="S3.E7.m1.8.9.1"></eq><ci id="S3.E7.m1.8.9.2.cmml" xref="S3.E7.m1.8.9.2">â„’</ci><apply id="S3.E7.m1.8.9.3.cmml" xref="S3.E7.m1.8.9.3"><plus id="S3.E7.m1.8.9.3.1.cmml" xref="S3.E7.m1.8.9.3.1"></plus><apply id="S3.E7.m1.8.9.3.2.cmml" xref="S3.E7.m1.8.9.3.2"><times id="S3.E7.m1.8.9.3.2.1.cmml" xref="S3.E7.m1.8.9.3.2.1"></times><apply id="S3.E7.m1.8.9.3.2.2.cmml" xref="S3.E7.m1.8.9.3.2.2"><csymbol cd="ambiguous" id="S3.E7.m1.8.9.3.2.2.1.cmml" xref="S3.E7.m1.8.9.3.2.2">subscript</csymbol><ci id="S3.E7.m1.8.9.3.2.2.2.cmml" xref="S3.E7.m1.8.9.3.2.2.2">ğœ†</ci><ci id="S3.E7.m1.8.9.3.2.2.3a.cmml" xref="S3.E7.m1.8.9.3.2.2.3"><mtext mathsize="70%" id="S3.E7.m1.8.9.3.2.2.3.cmml" xref="S3.E7.m1.8.9.3.2.2.3">cls</mtext></ci></apply><apply id="S3.E7.m1.8.9.3.2.3.cmml" xref="S3.E7.m1.8.9.3.2.3"><csymbol cd="ambiguous" id="S3.E7.m1.8.9.3.2.3.1.cmml" xref="S3.E7.m1.8.9.3.2.3">subscript</csymbol><ci id="S3.E7.m1.8.9.3.2.3.2.cmml" xref="S3.E7.m1.8.9.3.2.3.2">â„’</ci><ci id="S3.E7.m1.8.9.3.2.3.3a.cmml" xref="S3.E7.m1.8.9.3.2.3.3"><mtext mathsize="70%" id="S3.E7.m1.8.9.3.2.3.3.cmml" xref="S3.E7.m1.8.9.3.2.3.3">cls</mtext></ci></apply><interval closure="open" id="S3.E7.m1.8.9.3.2.4.1.cmml" xref="S3.E7.m1.8.9.3.2.4.2"><ci id="S3.E7.m1.1.1.cmml" xref="S3.E7.m1.1.1">ğ’‘</ci><apply id="S3.E7.m1.2.2.cmml" xref="S3.E7.m1.2.2"><ci id="S3.E7.m1.2.2.1.cmml" xref="S3.E7.m1.2.2.1">^</ci><ci id="S3.E7.m1.2.2.2.cmml" xref="S3.E7.m1.2.2.2">ğ’‘</ci></apply></interval></apply><apply id="S3.E7.m1.8.9.3.3.cmml" xref="S3.E7.m1.8.9.3.3"><times id="S3.E7.m1.8.9.3.3.1.cmml" xref="S3.E7.m1.8.9.3.3.1"></times><apply id="S3.E7.m1.8.9.3.3.2.cmml" xref="S3.E7.m1.8.9.3.3.2"><csymbol cd="ambiguous" id="S3.E7.m1.8.9.3.3.2.1.cmml" xref="S3.E7.m1.8.9.3.3.2">subscript</csymbol><ci id="S3.E7.m1.8.9.3.3.2.2.cmml" xref="S3.E7.m1.8.9.3.3.2.2">ğœ†</ci><ci id="S3.E7.m1.8.9.3.3.2.3a.cmml" xref="S3.E7.m1.8.9.3.3.2.3"><mtext mathsize="70%" id="S3.E7.m1.8.9.3.3.2.3.cmml" xref="S3.E7.m1.8.9.3.3.2.3">box</mtext></ci></apply><apply id="S3.E7.m1.8.9.3.3.3.cmml" xref="S3.E7.m1.8.9.3.3.3"><csymbol cd="ambiguous" id="S3.E7.m1.8.9.3.3.3.1.cmml" xref="S3.E7.m1.8.9.3.3.3">subscript</csymbol><ci id="S3.E7.m1.8.9.3.3.3.2.cmml" xref="S3.E7.m1.8.9.3.3.3.2">â„’</ci><ci id="S3.E7.m1.8.9.3.3.3.3a.cmml" xref="S3.E7.m1.8.9.3.3.3.3"><mtext mathsize="70%" id="S3.E7.m1.8.9.3.3.3.3.cmml" xref="S3.E7.m1.8.9.3.3.3.3">box</mtext></ci></apply><interval closure="open" id="S3.E7.m1.8.9.3.3.4.1.cmml" xref="S3.E7.m1.8.9.3.3.4.2"><ci id="S3.E7.m1.3.3.cmml" xref="S3.E7.m1.3.3">ğ’ƒ</ci><apply id="S3.E7.m1.4.4.cmml" xref="S3.E7.m1.4.4"><ci id="S3.E7.m1.4.4.1.cmml" xref="S3.E7.m1.4.4.1">^</ci><ci id="S3.E7.m1.4.4.2.cmml" xref="S3.E7.m1.4.4.2">ğ’ƒ</ci></apply></interval></apply><apply id="S3.E7.m1.8.9.3.4.cmml" xref="S3.E7.m1.8.9.3.4"><times id="S3.E7.m1.8.9.3.4.1.cmml" xref="S3.E7.m1.8.9.3.4.1"></times><apply id="S3.E7.m1.8.9.3.4.2.cmml" xref="S3.E7.m1.8.9.3.4.2"><csymbol cd="ambiguous" id="S3.E7.m1.8.9.3.4.2.1.cmml" xref="S3.E7.m1.8.9.3.4.2">subscript</csymbol><ci id="S3.E7.m1.8.9.3.4.2.2.cmml" xref="S3.E7.m1.8.9.3.4.2.2">ğœ†</ci><ci id="S3.E7.m1.8.9.3.4.2.3a.cmml" xref="S3.E7.m1.8.9.3.4.2.3"><mtext mathsize="70%" id="S3.E7.m1.8.9.3.4.2.3.cmml" xref="S3.E7.m1.8.9.3.4.2.3">lm</mtext></ci></apply><apply id="S3.E7.m1.8.9.3.4.3.cmml" xref="S3.E7.m1.8.9.3.4.3"><csymbol cd="ambiguous" id="S3.E7.m1.8.9.3.4.3.1.cmml" xref="S3.E7.m1.8.9.3.4.3">subscript</csymbol><ci id="S3.E7.m1.8.9.3.4.3.2.cmml" xref="S3.E7.m1.8.9.3.4.3.2">â„’</ci><ci id="S3.E7.m1.8.9.3.4.3.3a.cmml" xref="S3.E7.m1.8.9.3.4.3.3"><mtext mathsize="70%" id="S3.E7.m1.8.9.3.4.3.3.cmml" xref="S3.E7.m1.8.9.3.4.3.3">lm</mtext></ci></apply><interval closure="open" id="S3.E7.m1.8.9.3.4.4.1.cmml" xref="S3.E7.m1.8.9.3.4.4.2"><ci id="S3.E7.m1.5.5.cmml" xref="S3.E7.m1.5.5">ğ’˜</ci><apply id="S3.E7.m1.6.6.cmml" xref="S3.E7.m1.6.6"><ci id="S3.E7.m1.6.6.1.cmml" xref="S3.E7.m1.6.6.1">^</ci><ci id="S3.E7.m1.6.6.2.cmml" xref="S3.E7.m1.6.6.2">ğ’˜</ci></apply></interval></apply><apply id="S3.E7.m1.8.9.3.5.cmml" xref="S3.E7.m1.8.9.3.5"><times id="S3.E7.m1.8.9.3.5.1.cmml" xref="S3.E7.m1.8.9.3.5.1"></times><apply id="S3.E7.m1.8.9.3.5.2.cmml" xref="S3.E7.m1.8.9.3.5.2"><csymbol cd="ambiguous" id="S3.E7.m1.8.9.3.5.2.1.cmml" xref="S3.E7.m1.8.9.3.5.2">subscript</csymbol><ci id="S3.E7.m1.8.9.3.5.2.2.cmml" xref="S3.E7.m1.8.9.3.5.2.2">ğœ†</ci><ci id="S3.E7.m1.8.9.3.5.2.3a.cmml" xref="S3.E7.m1.8.9.3.5.2.3"><mtext mathsize="70%" id="S3.E7.m1.8.9.3.5.2.3.cmml" xref="S3.E7.m1.8.9.3.5.2.3">noun</mtext></ci></apply><apply id="S3.E7.m1.8.9.3.5.3.cmml" xref="S3.E7.m1.8.9.3.5.3"><csymbol cd="ambiguous" id="S3.E7.m1.8.9.3.5.3.1.cmml" xref="S3.E7.m1.8.9.3.5.3">subscript</csymbol><ci id="S3.E7.m1.8.9.3.5.3.2.cmml" xref="S3.E7.m1.8.9.3.5.3.2">â„’</ci><ci id="S3.E7.m1.8.9.3.5.3.3a.cmml" xref="S3.E7.m1.8.9.3.5.3.3"><mtext mathsize="70%" id="S3.E7.m1.8.9.3.5.3.3.cmml" xref="S3.E7.m1.8.9.3.5.3.3">noun</mtext></ci></apply><interval closure="open" id="S3.E7.m1.8.9.3.5.4.1.cmml" xref="S3.E7.m1.8.9.3.5.4.2"><ci id="S3.E7.m1.7.7.cmml" xref="S3.E7.m1.7.7">ğ’˜</ci><apply id="S3.E7.m1.8.8.cmml" xref="S3.E7.m1.8.8"><ci id="S3.E7.m1.8.8.1.cmml" xref="S3.E7.m1.8.8.1">Â¯</ci><ci id="S3.E7.m1.8.8.2.cmml" xref="S3.E7.m1.8.8.2">ğ’˜</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.8c">\mathcal{L}=\lambda_{\text{cls}}\mathcal{L}_{\text{cls}}\left({\bm{p}},\hat{{\bm{p}}}\right)+\lambda_{\text{box}}\mathcal{L}_{\text{box}}(\bm{b},\hat{\bm{b}})+\lambda_{\text{lm}}\mathcal{L}_{\text{lm}}(\bm{w},\hat{\bm{w}})+\lambda_{\text{noun}}\mathcal{L}_{\text{noun}}({\bm{w}},\bar{{\bm{w}}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.p2.15" class="ltx_p">Here, the classification loss <math id="S3.SS4.p2.7.m1.1" class="ltx_Math" alttext="\mathcal{L}_{\text{cls}}" display="inline"><semantics id="S3.SS4.p2.7.m1.1a"><msub id="S3.SS4.p2.7.m1.1.1" xref="S3.SS4.p2.7.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.7.m1.1.1.2" xref="S3.SS4.p2.7.m1.1.1.2.cmml">â„’</mi><mtext id="S3.SS4.p2.7.m1.1.1.3" xref="S3.SS4.p2.7.m1.1.1.3a.cmml">cls</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.7.m1.1b"><apply id="S3.SS4.p2.7.m1.1.1.cmml" xref="S3.SS4.p2.7.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.7.m1.1.1.1.cmml" xref="S3.SS4.p2.7.m1.1.1">subscript</csymbol><ci id="S3.SS4.p2.7.m1.1.1.2.cmml" xref="S3.SS4.p2.7.m1.1.1.2">â„’</ci><ci id="S3.SS4.p2.7.m1.1.1.3a.cmml" xref="S3.SS4.p2.7.m1.1.1.3"><mtext mathsize="70%" id="S3.SS4.p2.7.m1.1.1.3.cmml" xref="S3.SS4.p2.7.m1.1.1.3">cls</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.7.m1.1c">\mathcal{L}_{\text{cls}}</annotation></semantics></math> is a binary softmax classification loss of two classes: matched <span id="S3.SS4.p2.15.1" class="ltx_text ltx_font_italic">vs.</span> not matched.
The box-related loss <math id="S3.SS4.p2.8.m2.1" class="ltx_Math" alttext="\mathcal{L}_{\text{box}}" display="inline"><semantics id="S3.SS4.p2.8.m2.1a"><msub id="S3.SS4.p2.8.m2.1.1" xref="S3.SS4.p2.8.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.8.m2.1.1.2" xref="S3.SS4.p2.8.m2.1.1.2.cmml">â„’</mi><mtext id="S3.SS4.p2.8.m2.1.1.3" xref="S3.SS4.p2.8.m2.1.1.3a.cmml">box</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.8.m2.1b"><apply id="S3.SS4.p2.8.m2.1.1.cmml" xref="S3.SS4.p2.8.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.8.m2.1.1.1.cmml" xref="S3.SS4.p2.8.m2.1.1">subscript</csymbol><ci id="S3.SS4.p2.8.m2.1.1.2.cmml" xref="S3.SS4.p2.8.m2.1.1.2">â„’</ci><ci id="S3.SS4.p2.8.m2.1.1.3a.cmml" xref="S3.SS4.p2.8.m2.1.1.3"><mtext mathsize="70%" id="S3.SS4.p2.8.m2.1.1.3.cmml" xref="S3.SS4.p2.8.m2.1.1.3">box</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.8.m2.1c">\mathcal{L}_{\text{box}}</annotation></semantics></math> is either L1 loss or GIoU lossÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>.
The language modeling loss <math id="S3.SS4.p2.9.m3.1" class="ltx_Math" alttext="\mathcal{L}_{\text{lm}}" display="inline"><semantics id="S3.SS4.p2.9.m3.1a"><msub id="S3.SS4.p2.9.m3.1.1" xref="S3.SS4.p2.9.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.9.m3.1.1.2" xref="S3.SS4.p2.9.m3.1.1.2.cmml">â„’</mi><mtext id="S3.SS4.p2.9.m3.1.1.3" xref="S3.SS4.p2.9.m3.1.1.3a.cmml">lm</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.9.m3.1b"><apply id="S3.SS4.p2.9.m3.1.1.cmml" xref="S3.SS4.p2.9.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.9.m3.1.1.1.cmml" xref="S3.SS4.p2.9.m3.1.1">subscript</csymbol><ci id="S3.SS4.p2.9.m3.1.1.2.cmml" xref="S3.SS4.p2.9.m3.1.1.2">â„’</ci><ci id="S3.SS4.p2.9.m3.1.1.3a.cmml" xref="S3.SS4.p2.9.m3.1.1.3"><mtext mathsize="70%" id="S3.SS4.p2.9.m3.1.1.3.cmml" xref="S3.SS4.p2.9.m3.1.1.3">lm</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.9.m3.1c">\mathcal{L}_{\text{lm}}</annotation></semantics></math> is softmax classification loss over the vocabulary size <math id="S3.SS4.p2.10.m4.1" class="ltx_Math" alttext="\mathcal{W}" display="inline"><semantics id="S3.SS4.p2.10.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.10.m4.1.1" xref="S3.SS4.p2.10.m4.1.1.cmml">ğ’²</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.10.m4.1b"><ci id="S3.SS4.p2.10.m4.1.1.cmml" xref="S3.SS4.p2.10.m4.1.1">ğ’²</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.10.m4.1c">\mathcal{W}</annotation></semantics></math> of the LLM Tokenizer.
The noun loss <math id="S3.SS4.p2.11.m5.1" class="ltx_Math" alttext="\mathcal{L}_{\text{noun}}" display="inline"><semantics id="S3.SS4.p2.11.m5.1a"><msub id="S3.SS4.p2.11.m5.1.1" xref="S3.SS4.p2.11.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.11.m5.1.1.2" xref="S3.SS4.p2.11.m5.1.1.2.cmml">â„’</mi><mtext id="S3.SS4.p2.11.m5.1.1.3" xref="S3.SS4.p2.11.m5.1.1.3a.cmml">noun</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.11.m5.1b"><apply id="S3.SS4.p2.11.m5.1.1.cmml" xref="S3.SS4.p2.11.m5.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.11.m5.1.1.1.cmml" xref="S3.SS4.p2.11.m5.1.1">subscript</csymbol><ci id="S3.SS4.p2.11.m5.1.1.2.cmml" xref="S3.SS4.p2.11.m5.1.1.2">â„’</ci><ci id="S3.SS4.p2.11.m5.1.1.3a.cmml" xref="S3.SS4.p2.11.m5.1.1.3"><mtext mathsize="70%" id="S3.SS4.p2.11.m5.1.1.3.cmml" xref="S3.SS4.p2.11.m5.1.1.3">noun</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.11.m5.1c">\mathcal{L}_{\text{noun}}</annotation></semantics></math> is a binary classification loss that determines whether a token is an object word or not.
We set the loss weighting hyper-parameters <math id="S3.SS4.p2.12.m6.1" class="ltx_Math" alttext="\lambda_{\text{cls}}=1" display="inline"><semantics id="S3.SS4.p2.12.m6.1a"><mrow id="S3.SS4.p2.12.m6.1.1" xref="S3.SS4.p2.12.m6.1.1.cmml"><msub id="S3.SS4.p2.12.m6.1.1.2" xref="S3.SS4.p2.12.m6.1.1.2.cmml"><mi id="S3.SS4.p2.12.m6.1.1.2.2" xref="S3.SS4.p2.12.m6.1.1.2.2.cmml">Î»</mi><mtext id="S3.SS4.p2.12.m6.1.1.2.3" xref="S3.SS4.p2.12.m6.1.1.2.3a.cmml">cls</mtext></msub><mo id="S3.SS4.p2.12.m6.1.1.1" xref="S3.SS4.p2.12.m6.1.1.1.cmml">=</mo><mn id="S3.SS4.p2.12.m6.1.1.3" xref="S3.SS4.p2.12.m6.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.12.m6.1b"><apply id="S3.SS4.p2.12.m6.1.1.cmml" xref="S3.SS4.p2.12.m6.1.1"><eq id="S3.SS4.p2.12.m6.1.1.1.cmml" xref="S3.SS4.p2.12.m6.1.1.1"></eq><apply id="S3.SS4.p2.12.m6.1.1.2.cmml" xref="S3.SS4.p2.12.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p2.12.m6.1.1.2.1.cmml" xref="S3.SS4.p2.12.m6.1.1.2">subscript</csymbol><ci id="S3.SS4.p2.12.m6.1.1.2.2.cmml" xref="S3.SS4.p2.12.m6.1.1.2.2">ğœ†</ci><ci id="S3.SS4.p2.12.m6.1.1.2.3a.cmml" xref="S3.SS4.p2.12.m6.1.1.2.3"><mtext mathsize="70%" id="S3.SS4.p2.12.m6.1.1.2.3.cmml" xref="S3.SS4.p2.12.m6.1.1.2.3">cls</mtext></ci></apply><cn type="integer" id="S3.SS4.p2.12.m6.1.1.3.cmml" xref="S3.SS4.p2.12.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.12.m6.1c">\lambda_{\text{cls}}=1</annotation></semantics></math>, <math id="S3.SS4.p2.13.m7.1" class="ltx_Math" alttext="\lambda_{\text{box}}=5" display="inline"><semantics id="S3.SS4.p2.13.m7.1a"><mrow id="S3.SS4.p2.13.m7.1.1" xref="S3.SS4.p2.13.m7.1.1.cmml"><msub id="S3.SS4.p2.13.m7.1.1.2" xref="S3.SS4.p2.13.m7.1.1.2.cmml"><mi id="S3.SS4.p2.13.m7.1.1.2.2" xref="S3.SS4.p2.13.m7.1.1.2.2.cmml">Î»</mi><mtext id="S3.SS4.p2.13.m7.1.1.2.3" xref="S3.SS4.p2.13.m7.1.1.2.3a.cmml">box</mtext></msub><mo id="S3.SS4.p2.13.m7.1.1.1" xref="S3.SS4.p2.13.m7.1.1.1.cmml">=</mo><mn id="S3.SS4.p2.13.m7.1.1.3" xref="S3.SS4.p2.13.m7.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.13.m7.1b"><apply id="S3.SS4.p2.13.m7.1.1.cmml" xref="S3.SS4.p2.13.m7.1.1"><eq id="S3.SS4.p2.13.m7.1.1.1.cmml" xref="S3.SS4.p2.13.m7.1.1.1"></eq><apply id="S3.SS4.p2.13.m7.1.1.2.cmml" xref="S3.SS4.p2.13.m7.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p2.13.m7.1.1.2.1.cmml" xref="S3.SS4.p2.13.m7.1.1.2">subscript</csymbol><ci id="S3.SS4.p2.13.m7.1.1.2.2.cmml" xref="S3.SS4.p2.13.m7.1.1.2.2">ğœ†</ci><ci id="S3.SS4.p2.13.m7.1.1.2.3a.cmml" xref="S3.SS4.p2.13.m7.1.1.2.3"><mtext mathsize="70%" id="S3.SS4.p2.13.m7.1.1.2.3.cmml" xref="S3.SS4.p2.13.m7.1.1.2.3">box</mtext></ci></apply><cn type="integer" id="S3.SS4.p2.13.m7.1.1.3.cmml" xref="S3.SS4.p2.13.m7.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.13.m7.1c">\lambda_{\text{box}}=5</annotation></semantics></math>, <math id="S3.SS4.p2.14.m8.1" class="ltx_Math" alttext="\lambda_{\text{lm}}=1" display="inline"><semantics id="S3.SS4.p2.14.m8.1a"><mrow id="S3.SS4.p2.14.m8.1.1" xref="S3.SS4.p2.14.m8.1.1.cmml"><msub id="S3.SS4.p2.14.m8.1.1.2" xref="S3.SS4.p2.14.m8.1.1.2.cmml"><mi id="S3.SS4.p2.14.m8.1.1.2.2" xref="S3.SS4.p2.14.m8.1.1.2.2.cmml">Î»</mi><mtext id="S3.SS4.p2.14.m8.1.1.2.3" xref="S3.SS4.p2.14.m8.1.1.2.3a.cmml">lm</mtext></msub><mo id="S3.SS4.p2.14.m8.1.1.1" xref="S3.SS4.p2.14.m8.1.1.1.cmml">=</mo><mn id="S3.SS4.p2.14.m8.1.1.3" xref="S3.SS4.p2.14.m8.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.14.m8.1b"><apply id="S3.SS4.p2.14.m8.1.1.cmml" xref="S3.SS4.p2.14.m8.1.1"><eq id="S3.SS4.p2.14.m8.1.1.1.cmml" xref="S3.SS4.p2.14.m8.1.1.1"></eq><apply id="S3.SS4.p2.14.m8.1.1.2.cmml" xref="S3.SS4.p2.14.m8.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p2.14.m8.1.1.2.1.cmml" xref="S3.SS4.p2.14.m8.1.1.2">subscript</csymbol><ci id="S3.SS4.p2.14.m8.1.1.2.2.cmml" xref="S3.SS4.p2.14.m8.1.1.2.2">ğœ†</ci><ci id="S3.SS4.p2.14.m8.1.1.2.3a.cmml" xref="S3.SS4.p2.14.m8.1.1.2.3"><mtext mathsize="70%" id="S3.SS4.p2.14.m8.1.1.2.3.cmml" xref="S3.SS4.p2.14.m8.1.1.2.3">lm</mtext></ci></apply><cn type="integer" id="S3.SS4.p2.14.m8.1.1.3.cmml" xref="S3.SS4.p2.14.m8.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.14.m8.1c">\lambda_{\text{lm}}=1</annotation></semantics></math>, and <math id="S3.SS4.p2.15.m9.1" class="ltx_Math" alttext="\lambda_{\text{noun}}=1" display="inline"><semantics id="S3.SS4.p2.15.m9.1a"><mrow id="S3.SS4.p2.15.m9.1.1" xref="S3.SS4.p2.15.m9.1.1.cmml"><msub id="S3.SS4.p2.15.m9.1.1.2" xref="S3.SS4.p2.15.m9.1.1.2.cmml"><mi id="S3.SS4.p2.15.m9.1.1.2.2" xref="S3.SS4.p2.15.m9.1.1.2.2.cmml">Î»</mi><mtext id="S3.SS4.p2.15.m9.1.1.2.3" xref="S3.SS4.p2.15.m9.1.1.2.3a.cmml">noun</mtext></msub><mo id="S3.SS4.p2.15.m9.1.1.1" xref="S3.SS4.p2.15.m9.1.1.1.cmml">=</mo><mn id="S3.SS4.p2.15.m9.1.1.3" xref="S3.SS4.p2.15.m9.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.15.m9.1b"><apply id="S3.SS4.p2.15.m9.1.1.cmml" xref="S3.SS4.p2.15.m9.1.1"><eq id="S3.SS4.p2.15.m9.1.1.1.cmml" xref="S3.SS4.p2.15.m9.1.1.1"></eq><apply id="S3.SS4.p2.15.m9.1.1.2.cmml" xref="S3.SS4.p2.15.m9.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p2.15.m9.1.1.2.1.cmml" xref="S3.SS4.p2.15.m9.1.1.2">subscript</csymbol><ci id="S3.SS4.p2.15.m9.1.1.2.2.cmml" xref="S3.SS4.p2.15.m9.1.1.2.2">ğœ†</ci><ci id="S3.SS4.p2.15.m9.1.1.2.3a.cmml" xref="S3.SS4.p2.15.m9.1.1.2.3"><mtext mathsize="70%" id="S3.SS4.p2.15.m9.1.1.2.3.cmml" xref="S3.SS4.p2.15.m9.1.1.2.3">noun</mtext></ci></apply><cn type="integer" id="S3.SS4.p2.15.m9.1.1.3.cmml" xref="S3.SS4.p2.15.m9.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.15.m9.1c">\lambda_{\text{noun}}=1</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We present the results of ContextDET on different tasks, including our proposed contextual object detection task discussed in SectionÂ <a href="#S4.SS1" title="4.1 Contextual Object Detection â€£ 4 Experiments â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>, open-vocabulary object detection described in SectionÂ <a href="#S4.SS3" title="4.3 Open-Vocabulary Object Detection â€£ 4 Experiments â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>, and the referring image segmentation task presented in the Appendix. For contextual object detection, we focus on providing quantitative and qualitative results for the cloze test setting, as inferring relevant object words from vast human vocabulary poses a significant challenge. Additionally, we provide qualitative results for both contextual captioning and contextual question-answering settings.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.2" class="ltx_p"><span id="S4.p2.2.1" class="ltx_text ltx_font_bold">Implementation Details.</span>
Our proposed method is implemented in PyTorch and all models are trained using a single machine with 4 NVIDIA A100 GPUs. During training, data augmentation techniques are applied including random horizontal flipping with a probability of 0.5 and large-scale jitteringÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. We set the batch size to 8 and train the model for 6 epochs. We use AdamWÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> optimizer with a learning rate of <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="1e^{-4}" display="inline"><semantics id="S4.p2.1.m1.1a"><mrow id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml"><mn id="S4.p2.1.m1.1.1.2" xref="S4.p2.1.m1.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.p2.1.m1.1.1.1" xref="S4.p2.1.m1.1.1.1.cmml">â€‹</mo><msup id="S4.p2.1.m1.1.1.3" xref="S4.p2.1.m1.1.1.3.cmml"><mi id="S4.p2.1.m1.1.1.3.2" xref="S4.p2.1.m1.1.1.3.2.cmml">e</mi><mrow id="S4.p2.1.m1.1.1.3.3" xref="S4.p2.1.m1.1.1.3.3.cmml"><mo id="S4.p2.1.m1.1.1.3.3a" xref="S4.p2.1.m1.1.1.3.3.cmml">âˆ’</mo><mn id="S4.p2.1.m1.1.1.3.3.2" xref="S4.p2.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><apply id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1"><times id="S4.p2.1.m1.1.1.1.cmml" xref="S4.p2.1.m1.1.1.1"></times><cn type="integer" id="S4.p2.1.m1.1.1.2.cmml" xref="S4.p2.1.m1.1.1.2">1</cn><apply id="S4.p2.1.m1.1.1.3.cmml" xref="S4.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.p2.1.m1.1.1.3.1.cmml" xref="S4.p2.1.m1.1.1.3">superscript</csymbol><ci id="S4.p2.1.m1.1.1.3.2.cmml" xref="S4.p2.1.m1.1.1.3.2">ğ‘’</ci><apply id="S4.p2.1.m1.1.1.3.3.cmml" xref="S4.p2.1.m1.1.1.3.3"><minus id="S4.p2.1.m1.1.1.3.3.1.cmml" xref="S4.p2.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.p2.1.m1.1.1.3.3.2.cmml" xref="S4.p2.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">1e^{-4}</annotation></semantics></math> and a weight decay of <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="0.05" display="inline"><semantics id="S4.p2.2.m2.1a"><mn id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml">0.05</mn><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><cn type="float" id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1">0.05</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">0.05</annotation></semantics></math>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Contextual Object Detection</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">CODE Dataset.</span>
To facilitate research on contextual object detection, we construct a Contextual Object DEtection (CODE) dataset. Specifically, we collected images, bounding boxes and captions annotations from Flickr30k Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> and Flickr30k EntitiesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>. We added annotations containing the position information of object names in the caption strings. These object names will be replaced with â€˜<span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_typewriter">[MASK]</span>â€™ tokens to serve as input in our <span id="S4.SS1.p1.1.3" class="ltx_text ltx_font_italic">cloze test</span> setting. CODE is divided into three splits: the <span id="S4.SS1.p1.1.4" class="ltx_text ltx_font_typewriter">train</span> split has 665,161 bounding boxes in 29,781 images, the <span id="S4.SS1.p1.1.5" class="ltx_text ltx_font_typewriter">val</span> split has 22,061 bounding boxes in 1,000 images, and the <span id="S4.SS1.p1.1.6" class="ltx_text ltx_font_typewriter">test</span> split has 21,641 bounding boxes in 999 images. In total, the CODE dataset has 10,346 unique object names, surpassing the number of object names in any previous detection dataset, such as COCOÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> (80 classes) and LVISÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> (1,203 classes). Please refer to Appendix for more details about our CODE dataset.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Evaluation Metrics.</span>
In our contextual cloze test setting, we compute accuracy by calculating the percentage of correctly predicted object words. However, evaluating this accuracy poses a challenge due to the presence of numerous synonyms and fine-grained object words in human language, which can be difficult for annotators to distinguish.
This is a problem similar to those faced by previous large vocabulary image-classification datasets, such as ImageNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, which use the top-5 accuracy metric as a supplementary metric to the top-1 accuracy.
Consequently, we also adopt both the top-1 accuracy (Acc@1) and the top-5 accuracy (Acc@5) as our evaluation metrics.
For box evaluation, we compute the mean Average Precision (mAP) metric based on the top-1 and top-5 predicted names, which are represented as AP@1 and AP@5.
In evaluation, we compared the object name words rather than pre-defined category IDs, which allows a flexible extension to accommodate a vast human vocabulary.
The Appendix contains more implementation details.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span> <span id="S4.T1.4.2" class="ltx_text" style="font-size:90%;">Benchmark results of ContextDET on our CODE dataset <span id="S4.T1.4.2.1" class="ltx_text ltx_font_typewriter">val</span> set.
We report four metrics for comparisons: Acc@1, Acc@5, AP@1, and AP@5.
We also report the total number of parameters, number of trainable parameters, training time <math id="S4.T1.3.1.m1.1" class="ltx_Math" alttext="\rm{T}_{\rm{train}}" display="inline"><semantics id="S4.T1.3.1.m1.1b"><msub id="S4.T1.3.1.m1.1.1" xref="S4.T1.3.1.m1.1.1.cmml"><mi mathvariant="normal" id="S4.T1.3.1.m1.1.1.2" xref="S4.T1.3.1.m1.1.1.2.cmml">T</mi><mi id="S4.T1.3.1.m1.1.1.3" xref="S4.T1.3.1.m1.1.1.3.cmml">train</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T1.3.1.m1.1c"><apply id="S4.T1.3.1.m1.1.1.cmml" xref="S4.T1.3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.3.1.m1.1.1.1.cmml" xref="S4.T1.3.1.m1.1.1">subscript</csymbol><ci id="S4.T1.3.1.m1.1.1.2.cmml" xref="S4.T1.3.1.m1.1.1.2">T</ci><ci id="S4.T1.3.1.m1.1.1.3.cmml" xref="S4.T1.3.1.m1.1.1.3">train</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.1.m1.1d">\rm{T}_{\rm{train}}</annotation></semantics></math> and testing time <math id="S4.T1.4.2.m2.1" class="ltx_Math" alttext="\rm{T}_{\rm{test}}" display="inline"><semantics id="S4.T1.4.2.m2.1b"><msub id="S4.T1.4.2.m2.1.1" xref="S4.T1.4.2.m2.1.1.cmml"><mi mathvariant="normal" id="S4.T1.4.2.m2.1.1.2" xref="S4.T1.4.2.m2.1.1.2.cmml">T</mi><mi id="S4.T1.4.2.m2.1.1.3" xref="S4.T1.4.2.m2.1.1.3.cmml">test</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T1.4.2.m2.1c"><apply id="S4.T1.4.2.m2.1.1.cmml" xref="S4.T1.4.2.m2.1.1"><csymbol cd="ambiguous" id="S4.T1.4.2.m2.1.1.1.cmml" xref="S4.T1.4.2.m2.1.1">subscript</csymbol><ci id="S4.T1.4.2.m2.1.1.2.cmml" xref="S4.T1.4.2.m2.1.1.2">T</ci><ci id="S4.T1.4.2.m2.1.1.3.cmml" xref="S4.T1.4.2.m2.1.1.3">test</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.2.m2.1d">\rm{T}_{\rm{test}}</annotation></semantics></math> for efficiency analysis.
</span></figcaption>
<div id="S4.T1.6" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:429.3pt;height:85.2pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-59.9pt,11.8pt) scale(0.781742941466793,0.781742941466793) ;">
<table id="S4.T1.6.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.6.2.2" class="ltx_tr">
<th id="S4.T1.6.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S4.T1.6.2.2.3.1" class="ltx_text" style="color:#0FE3FF;">#</span></th>
<th id="S4.T1.6.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Language</th>
<th id="S4.T1.6.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Vision</th>
<th id="S4.T1.6.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S4.T1.6.2.2.6.1" class="ltx_text">Acc@1</span></th>
<th id="S4.T1.6.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S4.T1.6.2.2.7.1" class="ltx_text">Acc@5</span></th>
<th id="S4.T1.6.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S4.T1.6.2.2.8.1" class="ltx_text">AP@1</span></th>
<th id="S4.T1.6.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T1.6.2.2.9.1" class="ltx_text">AP@5</span></th>
<th id="S4.T1.6.2.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Total</th>
<th id="S4.T1.6.2.2.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Learnable</th>
<th id="S4.T1.5.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S4.T1.5.1.1.1.m1.1" class="ltx_Math" alttext="\rm{T}_{\rm{train}}" display="inline"><semantics id="S4.T1.5.1.1.1.m1.1a"><msub id="S4.T1.5.1.1.1.m1.1.1" xref="S4.T1.5.1.1.1.m1.1.1.cmml"><mi mathvariant="normal" id="S4.T1.5.1.1.1.m1.1.1.2" xref="S4.T1.5.1.1.1.m1.1.1.2.cmml">T</mi><mi id="S4.T1.5.1.1.1.m1.1.1.3" xref="S4.T1.5.1.1.1.m1.1.1.3.cmml">train</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T1.5.1.1.1.m1.1b"><apply id="S4.T1.5.1.1.1.m1.1.1.cmml" xref="S4.T1.5.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.5.1.1.1.m1.1.1.1.cmml" xref="S4.T1.5.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T1.5.1.1.1.m1.1.1.2.cmml" xref="S4.T1.5.1.1.1.m1.1.1.2">T</ci><ci id="S4.T1.5.1.1.1.m1.1.1.3.cmml" xref="S4.T1.5.1.1.1.m1.1.1.3">train</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.1.1.1.m1.1c">\rm{T}_{\rm{train}}</annotation></semantics></math></th>
<th id="S4.T1.6.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><math id="S4.T1.6.2.2.2.m1.1" class="ltx_Math" alttext="\rm{T}_{\rm{test}}" display="inline"><semantics id="S4.T1.6.2.2.2.m1.1a"><msub id="S4.T1.6.2.2.2.m1.1.1" xref="S4.T1.6.2.2.2.m1.1.1.cmml"><mi mathvariant="normal" id="S4.T1.6.2.2.2.m1.1.1.2" xref="S4.T1.6.2.2.2.m1.1.1.2.cmml">T</mi><mi id="S4.T1.6.2.2.2.m1.1.1.3" xref="S4.T1.6.2.2.2.m1.1.1.3.cmml">test</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T1.6.2.2.2.m1.1b"><apply id="S4.T1.6.2.2.2.m1.1.1.cmml" xref="S4.T1.6.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.6.2.2.2.m1.1.1.1.cmml" xref="S4.T1.6.2.2.2.m1.1.1">subscript</csymbol><ci id="S4.T1.6.2.2.2.m1.1.1.2.cmml" xref="S4.T1.6.2.2.2.m1.1.1.2">T</ci><ci id="S4.T1.6.2.2.2.m1.1.1.3.cmml" xref="S4.T1.6.2.2.2.m1.1.1.3">test</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.2.2.2.m1.1c">\rm{T}_{\rm{test}}</annotation></semantics></math></th>
</tr>
<tr id="S4.T1.6.2.3.1" class="ltx_tr">
<th id="S4.T1.6.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">Model</th>
<th id="S4.T1.6.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">Backbone</th>
<th id="S4.T1.6.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">#Params (M)</th>
<th id="S4.T1.6.2.3.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">#Params (M) (%)</th>
<th id="S4.T1.6.2.3.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">(s/iter)</th>
<th id="S4.T1.6.2.3.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column">(s/iter)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.6.2.4.1" class="ltx_tr">
<td id="S4.T1.6.2.4.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.6.2.4.1.1.1" class="ltx_text" style="color:#0FE3FF;">1</span></td>
<td id="S4.T1.6.2.4.1.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S4.T1.6.2.4.1.2.1" class="ltx_text">OPT-2.7B</span></td>
<td id="S4.T1.6.2.4.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ResNet50</td>
<td id="S4.T1.6.2.4.1.4" class="ltx_td ltx_align_center ltx_border_t">48.7</td>
<td id="S4.T1.6.2.4.1.5" class="ltx_td ltx_align_center ltx_border_t">73.8</td>
<td id="S4.T1.6.2.4.1.6" class="ltx_td ltx_align_center ltx_border_t">10.2</td>
<td id="S4.T1.6.2.4.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20.5</td>
<td id="S4.T1.6.2.4.1.8" class="ltx_td ltx_align_center ltx_border_t">2835</td>
<td id="S4.T1.6.2.4.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">183</td>
<td id="S4.T1.6.2.4.1.10" class="ltx_td ltx_align_center ltx_border_t">0.437</td>
<td id="S4.T1.6.2.4.1.11" class="ltx_td ltx_align_center ltx_border_t">0.224</td>
</tr>
<tr id="S4.T1.6.2.5.2" class="ltx_tr">
<td id="S4.T1.6.2.5.2.1" class="ltx_td ltx_align_center"><span id="S4.T1.6.2.5.2.1.1" class="ltx_text" style="color:#0FE3FF;">2</span></td>
<td id="S4.T1.6.2.5.2.2" class="ltx_td ltx_align_center ltx_border_r">Swin-B</td>
<td id="S4.T1.6.2.5.2.3" class="ltx_td ltx_align_center">54.3</td>
<td id="S4.T1.6.2.5.2.4" class="ltx_td ltx_align_center">78.1</td>
<td id="S4.T1.6.2.5.2.5" class="ltx_td ltx_align_center">13.1</td>
<td id="S4.T1.6.2.5.2.6" class="ltx_td ltx_align_center ltx_border_r">25.3</td>
<td id="S4.T1.6.2.5.2.7" class="ltx_td ltx_align_center">2893</td>
<td id="S4.T1.6.2.5.2.8" class="ltx_td ltx_align_center ltx_border_r">241</td>
<td id="S4.T1.6.2.5.2.9" class="ltx_td ltx_align_center">0.625</td>
<td id="S4.T1.6.2.5.2.10" class="ltx_td ltx_align_center">0.241</td>
</tr>
<tr id="S4.T1.6.2.6.3" class="ltx_tr">
<td id="S4.T1.6.2.6.3.1" class="ltx_td ltx_align_center"><span id="S4.T1.6.2.6.3.1.1" class="ltx_text" style="color:#0FE3FF;">3</span></td>
<td id="S4.T1.6.2.6.3.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="2"><span id="S4.T1.6.2.6.3.2.1" class="ltx_text">OPT-6.7B</span></td>
<td id="S4.T1.6.2.6.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ResNet50</td>
<td id="S4.T1.6.2.6.3.4" class="ltx_td ltx_align_center ltx_border_t">49.2</td>
<td id="S4.T1.6.2.6.3.5" class="ltx_td ltx_align_center ltx_border_t">74.5</td>
<td id="S4.T1.6.2.6.3.6" class="ltx_td ltx_align_center ltx_border_t">11.1</td>
<td id="S4.T1.6.2.6.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">23.4</td>
<td id="S4.T1.6.2.6.3.8" class="ltx_td ltx_align_center ltx_border_t">6922</td>
<td id="S4.T1.6.2.6.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">263</td>
<td id="S4.T1.6.2.6.3.10" class="ltx_td ltx_align_center ltx_border_t">0.448</td>
<td id="S4.T1.6.2.6.3.11" class="ltx_td ltx_align_center ltx_border_t">0.248</td>
</tr>
<tr id="S4.T1.6.2.7.4" class="ltx_tr">
<td id="S4.T1.6.2.7.4.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.6.2.7.4.1.1" class="ltx_text" style="color:#0FE3FF;">4</span></td>
<td id="S4.T1.6.2.7.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">Swin-B</td>
<td id="S4.T1.6.2.7.4.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.6.2.7.4.3.1" class="ltx_text ltx_font_bold">54.8</span></td>
<td id="S4.T1.6.2.7.4.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.6.2.7.4.4.1" class="ltx_text ltx_font_bold">78.6</span></td>
<td id="S4.T1.6.2.7.4.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.6.2.7.4.5.1" class="ltx_text ltx_font_bold">13.7</span></td>
<td id="S4.T1.6.2.7.4.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T1.6.2.7.4.6.1" class="ltx_text ltx_font_bold">26.6</span></td>
<td id="S4.T1.6.2.7.4.7" class="ltx_td ltx_align_center ltx_border_bb">6979</td>
<td id="S4.T1.6.2.7.4.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">320</td>
<td id="S4.T1.6.2.7.4.9" class="ltx_td ltx_align_center ltx_border_bb">0.652</td>
<td id="S4.T1.6.2.7.4.10" class="ltx_td ltx_align_center ltx_border_bb">0.251</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2305.18279/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="452" height="268" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 3: </span> Qualitative examples predicted by ContextDET in our three contextual object detection settings include (a) cloze test, (b) captioning, and (c) question answering.
The â€˜harry potterâ€™, â€˜pikachuâ€™, and â€˜messiâ€™ are novel names that are not annotated in the CODE training set.
ContextDET shows plausible contextual understanding and generalization abilities.
</figcaption>
</figure>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.3" class="ltx_p"><span id="S4.SS1.p3.3.1" class="ltx_text ltx_font_bold">Results.</span>
We provide the results of ContextDET on the CODE dataset in TableÂ <a href="#S4.T1" title="Table 1 â€£ 4.1 Contextual Object Detection â€£ 4 Experiments â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
We first report the results using OPT-2.7BÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> as the language model and ResNet50Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> as the vision backbone (Row <span id="S4.SS1.p3.3.2" class="ltx_text" style="color:#0FE3FF;">#1</span>). Our results suggest that the contextual cloze test task is very challenging: the top-1 AP (AP@1) is just 10.2, which falls significantly below the performance of previous object detection datasets like COCO. Moreover, our study suggests that using more powerful language models and vision backbones can improve performance. When we replace ResNet50 with Swin-BÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> (Row <span id="S4.SS1.p3.3.3" class="ltx_text" style="color:#0FE3FF;">#2</span>), we observe a notable improvement from <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="10.2" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mn id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">10.2</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><cn type="float" id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">10.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">10.2</annotation></semantics></math> to <math id="S4.SS1.p3.2.m2.1" class="ltx_Math" alttext="13.1" display="inline"><semantics id="S4.SS1.p3.2.m2.1a"><mn id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml">13.1</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><cn type="float" id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">13.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">13.1</annotation></semantics></math> in AP@1. In addition, by replacing OPT-2.7B with the larger OPT-6.7B (Row <span id="S4.SS1.p3.3.4" class="ltx_text" style="color:#0FE3FF;">#4</span>), we achieve an even higher AP@1 performance of <math id="S4.SS1.p3.3.m3.1" class="ltx_Math" alttext="13.7" display="inline"><semantics id="S4.SS1.p3.3.m3.1a"><mn id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml">13.7</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.1b"><cn type="float" id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1">13.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.1c">13.7</annotation></semantics></math>.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold">Efficiency Analysis.</span>
The majority of parameters in our model, including the LLM component, are frozen, resulting in a small percentage of learnable parameters. As shown in TableÂ <a href="#S4.T1" title="Table 1 â€£ 4.1 Contextual Object Detection â€£ 4 Experiments â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> Row <span id="S4.SS1.p4.1.2" class="ltx_text" style="color:#0FE3FF;">#1</span>, when employing OPT-2.7B and the ResNet50 backbone, only 6.4% (183 out of 2,835) of parameters are trainable. Our design does not impose a significant computational burden and can be easily reproduced.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para ltx_noindent">
<p id="S4.SS1.p5.1" class="ltx_p"><span id="S4.SS1.p5.1.1" class="ltx_text ltx_font_bold">Visualization.</span>
Besides the quantitative evaluation on the CODE benchmark, we further qualitatively evaluate ContextDET using more diverse images and objects, as shown in FigureÂ <a href="#S4.F3" title="Figure 3 â€£ 4.1 Contextual Object Detection â€£ 4 Experiments â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
We observe the capacity of ContextDET for complex contextual understanding and generalization to open-world names. For example, as illustrated in FigureÂ <a href="#S4.F3" title="Figure 3 â€£ 4.1 Contextual Object Detection â€£ 4 Experiments â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (a), ContextDET can reasonably infer the object names to fill the masked tokens, and accurately connect the object names with bounding boxes. Moreover, ContextDET is capable of predicting the names and locations of open-world concepts (<span id="S4.SS1.p5.1.2" class="ltx_text ltx_font_italic">e.g.</span>, â€˜Harry Potterâ€™, â€˜Pikachuâ€™, â€˜Messiâ€™), which are difficult to detect using previous close-set object detectors. Finally, in FigureÂ <a href="#S4.F3" title="Figure 3 â€£ 4.1 Contextual Object Detection â€£ 4 Experiments â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (c), we show that ContextDET can engage in multi-round question-answering conversations, and predict the bounding boxes of objects mentioned in the dialog history. Please refer to the appendix for more qualitative examples including failure cases.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T3.3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:208.1pt;">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Table 2: </span> Ablation studies on the impact of using local visual tokens <math id="S4.T3.2.2.m1.1" class="ltx_Math" alttext="{\bm{z}}" display="inline"><semantics id="S4.T3.2.2.m1.1b"><mi id="S4.T3.2.2.m1.1.1" xref="S4.T3.2.2.m1.1.1.cmml">ğ’›</mi><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.m1.1c"><ci id="S4.T3.2.2.m1.1.1.cmml" xref="S4.T3.2.2.m1.1.1">ğ’›</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.m1.1d">{\bm{z}}</annotation></semantics></math>.
</figcaption>
<div id="S4.T3.3.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:368.6pt;height:104.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(88.7pt,-25.0pt) scale(1.9272113208342,1.9272113208342) ;">
<table id="S4.T3.3.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.3.3.1.1" class="ltx_tr">
<th id="S4.T3.3.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T3.3.3.1.1.2.1" class="ltx_text" style="font-size:90%;color:#0FE3FF;">#</span></th>
<th id="S4.T3.3.3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"><math id="S4.T3.3.3.1.1.1.m1.1" class="ltx_Math" alttext="{\bm{z}}" display="inline"><semantics id="S4.T3.3.3.1.1.1.m1.1a"><mi mathsize="90%" id="S4.T3.3.3.1.1.1.m1.1.1" xref="S4.T3.3.3.1.1.1.m1.1.1.cmml">ğ’›</mi><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.1.1.1.m1.1b"><ci id="S4.T3.3.3.1.1.1.m1.1.1.cmml" xref="S4.T3.3.3.1.1.1.m1.1.1">ğ’›</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.1.1.1.m1.1c">{\bm{z}}</annotation></semantics></math></th>
<th id="S4.T3.3.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.3.3.1.1.3.1" class="ltx_text" style="font-size:90%;">Acc@1</span></th>
<th id="S4.T3.3.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.3.3.1.1.4.1" class="ltx_text" style="font-size:90%;">Acc@5</span></th>
<th id="S4.T3.3.3.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.3.3.1.1.5.1" class="ltx_text" style="font-size:90%;">AP@1</span></th>
<th id="S4.T3.3.3.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.3.3.1.1.6.1" class="ltx_text" style="font-size:90%;">AP@5</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.3.3.1.2.1" class="ltx_tr">
<th id="S4.T3.3.3.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.3.3.1.2.1.1.1" class="ltx_text" style="font-size:90%;color:#0FE3FF;">1</span></th>
<th id="S4.T3.3.3.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T3.3.3.1.2.1.2.1" class="ltx_text" style="font-size:90%;">âœ—</span></th>
<td id="S4.T3.3.3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.3.1.2.1.3.1" class="ltx_text" style="font-size:90%;">30.9</span></td>
<td id="S4.T3.3.3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.3.1.2.1.4.1" class="ltx_text" style="font-size:90%;">57.1</span></td>
<td id="S4.T3.3.3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.3.1.2.1.5.1" class="ltx_text" style="font-size:90%;">4.0</span></td>
<td id="S4.T3.3.3.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.3.1.2.1.6.1" class="ltx_text" style="font-size:90%;">13.6</span></td>
</tr>
<tr id="S4.T3.3.3.1.3.2" class="ltx_tr" style="background-color:#CCCCCC;">
<th id="S4.T3.3.3.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S4.T3.3.3.1.3.2.1.1" class="ltx_text" style="font-size:90%;color:#0FE3FF;background-color:#CCCCCC;">2</span></th>
<th id="S4.T3.3.3.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S4.T3.3.3.1.3.2.2.1" class="ltx_text" style="font-size:90%;background-color:#CCCCCC;">âœ“</span></th>
<td id="S4.T3.3.3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.3.3.1.3.2.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#CCCCCC;">48.7</span></td>
<td id="S4.T3.3.3.1.3.2.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.3.3.1.3.2.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#CCCCCC;">73.9</span></td>
<td id="S4.T3.3.3.1.3.2.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.3.3.1.3.2.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#CCCCCC;">10.4</span></td>
<td id="S4.T3.3.3.1.3.2.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.3.3.1.3.2.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#CCCCCC;">21.6</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T3.6" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:208.1pt;">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Table 3: </span> Ablation study: varying values of <math id="S4.T3.5.2.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S4.T3.5.2.m1.1b"><mi id="S4.T3.5.2.m1.1.1" xref="S4.T3.5.2.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.T3.5.2.m1.1c"><ci id="S4.T3.5.2.m1.1.1.cmml" xref="S4.T3.5.2.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.2.m1.1d">p</annotation></semantics></math>.
</figcaption>
<div id="S4.T3.6.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:368.6pt;height:137.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(87.9pt,-32.8pt) scale(1.91221343498352,1.91221343498352) ;">
<table id="S4.T3.6.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.6.3.1.1" class="ltx_tr">
<th id="S4.T3.6.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T3.6.3.1.1.2.1" class="ltx_text" style="font-size:90%;color:#0FE3FF;">#</span></th>
<th id="S4.T3.6.3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"><math id="S4.T3.6.3.1.1.1.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S4.T3.6.3.1.1.1.m1.1a"><mi mathsize="90%" id="S4.T3.6.3.1.1.1.m1.1.1" xref="S4.T3.6.3.1.1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.T3.6.3.1.1.1.m1.1b"><ci id="S4.T3.6.3.1.1.1.m1.1.1.cmml" xref="S4.T3.6.3.1.1.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.3.1.1.1.m1.1c">p</annotation></semantics></math></th>
<th id="S4.T3.6.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.6.3.1.1.3.1" class="ltx_text" style="font-size:90%;">Acc@1</span></th>
<th id="S4.T3.6.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.6.3.1.1.4.1" class="ltx_text" style="font-size:90%;">Acc@5</span></th>
<th id="S4.T3.6.3.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.6.3.1.1.5.1" class="ltx_text" style="font-size:90%;">AP@1</span></th>
<th id="S4.T3.6.3.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.6.3.1.1.6.1" class="ltx_text" style="font-size:90%;">AP@5</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.6.3.1.2.1" class="ltx_tr">
<th id="S4.T3.6.3.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.6.3.1.2.1.1.1" class="ltx_text" style="font-size:90%;color:#0FE3FF;">1</span></th>
<th id="S4.T3.6.3.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T3.6.3.1.2.1.2.1" class="ltx_text" style="font-size:90%;">4</span></th>
<td id="S4.T3.6.3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.6.3.1.2.1.3.1" class="ltx_text" style="font-size:90%;">48.4</span></td>
<td id="S4.T3.6.3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.6.3.1.2.1.4.1" class="ltx_text" style="font-size:90%;">73.2</span></td>
<td id="S4.T3.6.3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.6.3.1.2.1.5.1" class="ltx_text" style="font-size:90%;">10.1</span></td>
<td id="S4.T3.6.3.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.6.3.1.2.1.6.1" class="ltx_text" style="font-size:90%;">20.1</span></td>
</tr>
<tr id="S4.T3.6.3.1.3.2" class="ltx_tr" style="background-color:#CCCCCC;">
<th id="S4.T3.6.3.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.6.3.1.3.2.1.1" class="ltx_text" style="font-size:90%;color:#0FE3FF;background-color:#CCCCCC;">2</span></th>
<th id="S4.T3.6.3.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.6.3.1.3.2.2.1" class="ltx_text" style="font-size:90%;background-color:#CCCCCC;">9</span></th>
<td id="S4.T3.6.3.1.3.2.3" class="ltx_td ltx_align_center"><span id="S4.T3.6.3.1.3.2.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#CCCCCC;">48.7</span></td>
<td id="S4.T3.6.3.1.3.2.4" class="ltx_td ltx_align_center"><span id="S4.T3.6.3.1.3.2.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#CCCCCC;">73.9</span></td>
<td id="S4.T3.6.3.1.3.2.5" class="ltx_td ltx_align_center"><span id="S4.T3.6.3.1.3.2.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#CCCCCC;">10.4</span></td>
<td id="S4.T3.6.3.1.3.2.6" class="ltx_td ltx_align_center"><span id="S4.T3.6.3.1.3.2.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#CCCCCC;">21.6</span></td>
</tr>
<tr id="S4.T3.6.3.1.4.3" class="ltx_tr">
<th id="S4.T3.6.3.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S4.T3.6.3.1.4.3.1.1" class="ltx_text" style="font-size:90%;color:#0FE3FF;">3</span></th>
<th id="S4.T3.6.3.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S4.T3.6.3.1.4.3.2.1" class="ltx_text" style="font-size:90%;">16</span></th>
<td id="S4.T3.6.3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.6.3.1.4.3.3.1" class="ltx_text" style="font-size:90%;">47.5</span></td>
<td id="S4.T3.6.3.1.4.3.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.6.3.1.4.3.4.1" class="ltx_text" style="font-size:90%;">72.9</span></td>
<td id="S4.T3.6.3.1.4.3.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.6.3.1.4.3.5.1" class="ltx_text" style="font-size:90%;">9.9</span></td>
<td id="S4.T3.6.3.1.4.3.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.6.3.1.4.3.6.1" class="ltx_text" style="font-size:90%;">19.4</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</div>
</div>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Ablation Studies</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.2" class="ltx_p">We investigate the effects of using local visual tokens <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="{\bm{z}}" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">ğ’›</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">ğ’›</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">{\bm{z}}</annotation></semantics></math> and the associated hyper-parameter <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mi id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><ci id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">p</annotation></semantics></math> that determines the number of local bins.
The experiments are conducted on the CODE <span id="S4.SS2.p1.2.1" class="ltx_text ltx_font_typewriter">val</span> set.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.3" class="ltx_p"><span id="S4.SS2.p2.3.1" class="ltx_text ltx_font_bold">LLM without Local Visual Tokens.</span>
In our contextual cloze test setting, LLM is capable of making predictions even without the presence of the local visual token input <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="{\bm{z}}" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mi id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">ğ’›</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">ğ’›</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">{\bm{z}}</annotation></semantics></math>. However, upon analyzing the results presented in TableÂ <a href="#S4.T3" title="Table 3 â€£ 4.1 Contextual Object Detection â€£ 4 Experiments â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we observe a significant performance drop. For example, the top-1 accuracy drops around 20 percent from 48.7 to 30.9 (%). This observation emphasizes the crucial role of adding visual local tokens in our method for contextual understanding. We also observe that the value of language modeling loss <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="\mathcal{L}_{\text{lm}}" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><msub id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml">â„’</mi><mtext id="S4.SS2.p2.2.m2.1.1.3" xref="S4.SS2.p2.2.m2.1.1.3a.cmml">lm</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2">â„’</ci><ci id="S4.SS2.p2.2.m2.1.1.3a.cmml" xref="S4.SS2.p2.2.m2.1.1.3"><mtext mathsize="70%" id="S4.SS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3">lm</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">\mathcal{L}_{\text{lm}}</annotation></semantics></math> barely decreases in the absence of <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="{\bm{z}}" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><mi id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml">ğ’›</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><ci id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">ğ’›</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">{\bm{z}}</annotation></semantics></math>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.4" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">Hyper-Parameter <math id="S4.SS2.p3.1.1.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S4.SS2.p3.1.1.m1.1a"><mi id="S4.SS2.p3.1.1.m1.1.1" xref="S4.SS2.p3.1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.1.m1.1b"><ci id="S4.SS2.p3.1.1.m1.1.1.cmml" xref="S4.SS2.p3.1.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.1.m1.1c">p</annotation></semantics></math>.</span>
As discussed in SectionÂ <a href="#S3.SS1" title="3.1 Visual Encoder â€£ 3 Approach â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, we have <math id="S4.SS2.p3.2.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S4.SS2.p3.2.m1.1a"><mi id="S4.SS2.p3.2.m1.1.1" xref="S4.SS2.p3.2.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m1.1b"><ci id="S4.SS2.p3.2.m1.1.1.cmml" xref="S4.SS2.p3.2.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m1.1c">p</annotation></semantics></math> visual local tokens that serve as prefix inputs for LLM decoding.
In TableÂ <a href="#S4.T3" title="Table 3 â€£ 4.1 Contextual Object Detection â€£ 4 Experiments â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we show the effects of using different values for <math id="S4.SS2.p3.3.m2.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S4.SS2.p3.3.m2.1a"><mi id="S4.SS2.p3.3.m2.1.1" xref="S4.SS2.p3.3.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m2.1b"><ci id="S4.SS2.p3.3.m2.1.1.cmml" xref="S4.SS2.p3.3.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m2.1c">p</annotation></semantics></math>.
We observe that selecting <math id="S4.SS2.p3.4.m3.1" class="ltx_Math" alttext="p=9" display="inline"><semantics id="S4.SS2.p3.4.m3.1a"><mrow id="S4.SS2.p3.4.m3.1.1" xref="S4.SS2.p3.4.m3.1.1.cmml"><mi id="S4.SS2.p3.4.m3.1.1.2" xref="S4.SS2.p3.4.m3.1.1.2.cmml">p</mi><mo id="S4.SS2.p3.4.m3.1.1.1" xref="S4.SS2.p3.4.m3.1.1.1.cmml">=</mo><mn id="S4.SS2.p3.4.m3.1.1.3" xref="S4.SS2.p3.4.m3.1.1.3.cmml">9</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.4.m3.1b"><apply id="S4.SS2.p3.4.m3.1.1.cmml" xref="S4.SS2.p3.4.m3.1.1"><eq id="S4.SS2.p3.4.m3.1.1.1.cmml" xref="S4.SS2.p3.4.m3.1.1.1"></eq><ci id="S4.SS2.p3.4.m3.1.1.2.cmml" xref="S4.SS2.p3.4.m3.1.1.2">ğ‘</ci><cn type="integer" id="S4.SS2.p3.4.m3.1.1.3.cmml" xref="S4.SS2.p3.4.m3.1.1.3">9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.4.m3.1c">p=9</annotation></semantics></math> (Row <span id="S4.SS2.p3.4.2" class="ltx_text" style="color:#0FE3FF;">#2</span>) yields the optimal results, making it our default choice.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Open-Vocabulary Object Detection</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We demonstrate that our proposed ContextDET can also be applied to the open-vocabulary object detection task, aiming to evaluate the generalization ability. Following previous worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>, we use the OV-COCO benchmark and divide 65 categories as the split of base/novel (48/17) classes. The model is trained on the base classes only but evaluated on the novel classes (unavailable during model training). We measure the performance with the Average Precision (AP) metric on the base, novel, and all classes.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">To adapt ContextDET into the open-vocabulary setting, we ask questions like â€˜<span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_typewriter">Does the [CLASS] appear in this picture?</span>â€™ for every class including base and novel classes. If MLLM responds with a positive answer â€˜<span id="S4.SS3.p2.1.2" class="ltx_text ltx_font_typewriter">Yes</span>â€™, we take the latent embedding <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="{\bm{e}}" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mi id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml">ğ’†</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><ci id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">ğ’†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">{\bm{e}}</annotation></semantics></math> of the corresponding class name as a conditional input for our visual decoder (SectionÂ <a href="#S3.SS3" title="3.3 Visual Decoder â€£ 3 Approach â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>).
We compare ContextDET with selected baseline methods including the state-of-the-art method BARONÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite> in TableÂ <a href="#S4.T4" title="Table 4 â€£ 4.3 Open-Vocabulary Object Detection â€£ 4 Experiments â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
We observe that ContextDET significantly outperforms BARON by large margins of 2.8%, 4.7%, and 4.2% on the novel, base, and all sets, respectively.
All the baseline methods rely on prior knowledge from the vision-language model CLIP.
In contrast, our ContextDET uses MLLM to detect novel objects.
The results show that MLLM trained on web-scale datasets has strong generalizability that could benefit the open-vocabulary task.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4: </span> Comparison with state-of-the-art open-vocabulary detection methods on OV-COCO benchmark.
</figcaption>
<div id="S4.T4.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:325.2pt;height:67.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-54.7pt,11.3pt) scale(0.748407649462748,0.748407649462748) ;">
<table id="S4.T4.3.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.3.3.3" class="ltx_tr">
<th id="S4.T4.3.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.3.4.1" class="ltx_text" style="font-size:90%;color:#0FE3FF;">#</span></th>
<th id="S4.T4.3.3.3.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.3.5.1" class="ltx_text" style="font-size:90%;">Method</span></th>
<th id="S4.T4.3.3.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.3.6.1" class="ltx_text" style="font-size:90%;">Venue</span></th>
<th id="S4.T4.3.3.3.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.3.7.1" class="ltx_text" style="font-size:90%;">CLIP</span></th>
<th id="S4.T4.3.3.3.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.3.8.1" class="ltx_text" style="font-size:90%;">MLLM</span></th>
<th id="S4.T4.3.3.3.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.3.9.1" class="ltx_text" style="font-size:90%;">Backbone</span></th>
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.45pt;padding-bottom:0.45pt;">
<span id="S4.T4.1.1.1.1.1" class="ltx_text" style="font-size:90%;">AP</span><math id="S4.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="{}_{50}^{\text{novel}}" display="inline"><semantics id="S4.T4.1.1.1.1.m1.1a"><mmultiscripts id="S4.T4.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.m1.1.1.cmml"><mi id="S4.T4.1.1.1.1.m1.1.1.2.2" xref="S4.T4.1.1.1.1.m1.1.1.2.2.cmml"></mi><mprescripts id="S4.T4.1.1.1.1.m1.1.1a" xref="S4.T4.1.1.1.1.m1.1.1.cmml"></mprescripts><mrow id="S4.T4.1.1.1.1.m1.1.1b" xref="S4.T4.1.1.1.1.m1.1.1.cmml"></mrow><mtext mathsize="90%" id="S4.T4.1.1.1.1.m1.1.1.3" xref="S4.T4.1.1.1.1.m1.1.1.3a.cmml">novel</mtext><mn mathsize="90%" id="S4.T4.1.1.1.1.m1.1.1.2.3" xref="S4.T4.1.1.1.1.m1.1.1.2.3.cmml">50</mn><mrow id="S4.T4.1.1.1.1.m1.1.1c" xref="S4.T4.1.1.1.1.m1.1.1.cmml"></mrow></mmultiscripts><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.m1.1b"><apply id="S4.T4.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.1.1.1.1.m1.1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1">superscript</csymbol><apply id="S4.T4.1.1.1.1.m1.1.1.2.cmml" xref="S4.T4.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.1.1.1.1.m1.1.1.2.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1">subscript</csymbol><csymbol cd="latexml" id="S4.T4.1.1.1.1.m1.1.1.2.2.cmml" xref="S4.T4.1.1.1.1.m1.1.1.2.2">absent</csymbol><cn type="integer" id="S4.T4.1.1.1.1.m1.1.1.2.3.cmml" xref="S4.T4.1.1.1.1.m1.1.1.2.3">50</cn></apply><ci id="S4.T4.1.1.1.1.m1.1.1.3a.cmml" xref="S4.T4.1.1.1.1.m1.1.1.3"><mtext mathsize="63%" id="S4.T4.1.1.1.1.m1.1.1.3.cmml" xref="S4.T4.1.1.1.1.m1.1.1.3">novel</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.m1.1c">{}_{50}^{\text{novel}}</annotation></semantics></math>
</th>
<th id="S4.T4.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.45pt;padding-bottom:0.45pt;">
<span id="S4.T4.2.2.2.2.1" class="ltx_text" style="font-size:90%;">AP</span><math id="S4.T4.2.2.2.2.m1.1" class="ltx_Math" alttext="{}_{50}^{\text{base}}" display="inline"><semantics id="S4.T4.2.2.2.2.m1.1a"><mmultiscripts id="S4.T4.2.2.2.2.m1.1.1" xref="S4.T4.2.2.2.2.m1.1.1.cmml"><mi id="S4.T4.2.2.2.2.m1.1.1.2.2" xref="S4.T4.2.2.2.2.m1.1.1.2.2.cmml"></mi><mprescripts id="S4.T4.2.2.2.2.m1.1.1a" xref="S4.T4.2.2.2.2.m1.1.1.cmml"></mprescripts><mrow id="S4.T4.2.2.2.2.m1.1.1b" xref="S4.T4.2.2.2.2.m1.1.1.cmml"></mrow><mtext mathsize="90%" id="S4.T4.2.2.2.2.m1.1.1.3" xref="S4.T4.2.2.2.2.m1.1.1.3a.cmml">base</mtext><mn mathsize="90%" id="S4.T4.2.2.2.2.m1.1.1.2.3" xref="S4.T4.2.2.2.2.m1.1.1.2.3.cmml">50</mn><mrow id="S4.T4.2.2.2.2.m1.1.1c" xref="S4.T4.2.2.2.2.m1.1.1.cmml"></mrow></mmultiscripts><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.2.m1.1b"><apply id="S4.T4.2.2.2.2.m1.1.1.cmml" xref="S4.T4.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.2.2.2.2.m1.1.1.1.cmml" xref="S4.T4.2.2.2.2.m1.1.1">superscript</csymbol><apply id="S4.T4.2.2.2.2.m1.1.1.2.cmml" xref="S4.T4.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T4.2.2.2.2.m1.1.1.2.1.cmml" xref="S4.T4.2.2.2.2.m1.1.1">subscript</csymbol><csymbol cd="latexml" id="S4.T4.2.2.2.2.m1.1.1.2.2.cmml" xref="S4.T4.2.2.2.2.m1.1.1.2.2">absent</csymbol><cn type="integer" id="S4.T4.2.2.2.2.m1.1.1.2.3.cmml" xref="S4.T4.2.2.2.2.m1.1.1.2.3">50</cn></apply><ci id="S4.T4.2.2.2.2.m1.1.1.3a.cmml" xref="S4.T4.2.2.2.2.m1.1.1.3"><mtext mathsize="63%" id="S4.T4.2.2.2.2.m1.1.1.3.cmml" xref="S4.T4.2.2.2.2.m1.1.1.3">base</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.2.m1.1c">{}_{50}^{\text{base}}</annotation></semantics></math>
</th>
<th id="S4.T4.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.45pt;padding-bottom:0.45pt;">
<span id="S4.T4.3.3.3.3.1" class="ltx_text" style="font-size:90%;">AP</span><sub id="S4.T4.3.3.3.3.2" class="ltx_sub"><span id="S4.T4.3.3.3.3.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">50</span></sub>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.3.3.4.1" class="ltx_tr">
<td id="S4.T4.3.3.4.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.4.1.1.1" class="ltx_text" style="font-size:90%;color:#0FE3FF;">1</span></td>
<td id="S4.T4.3.3.4.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;">
<span id="S4.T4.3.3.4.1.2.1" class="ltx_text" style="font-size:90%;">ViLDÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T4.3.3.4.1.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib21" title="" class="ltx_ref">21</a><span id="S4.T4.3.3.4.1.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T4.3.3.4.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.4.1.3.1" class="ltx_text" style="font-size:90%;">ICLRâ€™22</span></td>
<td id="S4.T4.3.3.4.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.4.1.4.1" class="ltx_text" style="font-size:90%;">âœ“</span></td>
<td id="S4.T4.3.3.4.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.4.1.5.1" class="ltx_text" style="font-size:90%;">âœ—</span></td>
<td id="S4.T4.3.3.4.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.4.1.6.1" class="ltx_text" style="font-size:90%;">ResNet50-FPN</span></td>
<td id="S4.T4.3.3.4.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.4.1.7.1" class="ltx_text" style="font-size:90%;">27.6</span></td>
<td id="S4.T4.3.3.4.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.4.1.8.1" class="ltx_text" style="font-size:90%;">59.5</span></td>
<td id="S4.T4.3.3.4.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.4.1.9.1" class="ltx_text" style="font-size:90%;">51.2</span></td>
</tr>
<tr id="S4.T4.3.3.5.2" class="ltx_tr">
<td id="S4.T4.3.3.5.2.1" class="ltx_td ltx_align_left" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.5.2.1.1" class="ltx_text" style="font-size:90%;color:#0FE3FF;">2</span></td>
<td id="S4.T4.3.3.5.2.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;">
<span id="S4.T4.3.3.5.2.2.1" class="ltx_text" style="font-size:90%;">OV-DETRÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T4.3.3.5.2.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib81" title="" class="ltx_ref">81</a><span id="S4.T4.3.3.5.2.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T4.3.3.5.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.5.2.3.1" class="ltx_text" style="font-size:90%;">ECCVâ€™22</span></td>
<td id="S4.T4.3.3.5.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.5.2.4.1" class="ltx_text" style="font-size:90%;">âœ“</span></td>
<td id="S4.T4.3.3.5.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.5.2.5.1" class="ltx_text" style="font-size:90%;">âœ—</span></td>
<td id="S4.T4.3.3.5.2.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.5.2.6.1" class="ltx_text" style="font-size:90%;">ResNet50</span></td>
<td id="S4.T4.3.3.5.2.7" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.5.2.7.1" class="ltx_text" style="font-size:90%;">29.4</span></td>
<td id="S4.T4.3.3.5.2.8" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.5.2.8.1" class="ltx_text" style="font-size:90%;">61.0</span></td>
<td id="S4.T4.3.3.5.2.9" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.5.2.9.1" class="ltx_text" style="font-size:90%;">52.7</span></td>
</tr>
<tr id="S4.T4.3.3.6.3" class="ltx_tr">
<td id="S4.T4.3.3.6.3.1" class="ltx_td ltx_align_left" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.6.3.1.1" class="ltx_text" style="font-size:90%;color:#0FE3FF;">2</span></td>
<td id="S4.T4.3.3.6.3.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;">
<span id="S4.T4.3.3.6.3.2.1" class="ltx_text" style="font-size:90%;">BARONÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T4.3.3.6.3.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib74" title="" class="ltx_ref">74</a><span id="S4.T4.3.3.6.3.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T4.3.3.6.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.6.3.3.1" class="ltx_text" style="font-size:90%;">CVPRâ€™23</span></td>
<td id="S4.T4.3.3.6.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.6.3.4.1" class="ltx_text" style="font-size:90%;">âœ“</span></td>
<td id="S4.T4.3.3.6.3.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.6.3.5.1" class="ltx_text" style="font-size:90%;">âœ—</span></td>
<td id="S4.T4.3.3.6.3.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.6.3.6.1" class="ltx_text" style="font-size:90%;">ResNet50-FPN</span></td>
<td id="S4.T4.3.3.6.3.7" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.6.3.7.1" class="ltx_text" style="font-size:90%;">34.0</span></td>
<td id="S4.T4.3.3.6.3.8" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.6.3.8.1" class="ltx_text" style="font-size:90%;">60.4</span></td>
<td id="S4.T4.3.3.6.3.9" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.6.3.9.1" class="ltx_text" style="font-size:90%;">53.5</span></td>
</tr>
<tr id="S4.T4.3.3.7.4" class="ltx_tr" style="background-color:#CCCCCC;">
<td id="S4.T4.3.3.7.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.7.4.1.1" class="ltx_text" style="font-size:90%;color:#0FE3FF;background-color:#CCCCCC;">4</span></td>
<td id="S4.T4.3.3.7.4.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.7.4.2.1" class="ltx_text" style="font-size:90%;background-color:#CCCCCC;">ContextDET</span></td>
<td id="S4.T4.3.3.7.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.7.4.3.1" class="ltx_text" style="font-size:90%;background-color:#CCCCCC;">-</span></td>
<td id="S4.T4.3.3.7.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.7.4.4.1" class="ltx_text" style="font-size:90%;background-color:#CCCCCC;">âœ—</span></td>
<td id="S4.T4.3.3.7.4.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.7.4.5.1" class="ltx_text" style="font-size:90%;background-color:#CCCCCC;">âœ“</span></td>
<td id="S4.T4.3.3.7.4.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.7.4.6.1" class="ltx_text" style="font-size:90%;background-color:#CCCCCC;">ResNet50</span></td>
<td id="S4.T4.3.3.7.4.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.7.4.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#CCCCCC;">36.8</span></td>
<td id="S4.T4.3.3.7.4.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.7.4.8.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#CCCCCC;">65.1</span></td>
<td id="S4.T4.3.3.7.4.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T4.3.3.7.4.9.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#CCCCCC;">57.7</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Qualitative Results</h3>

<figure id="S4.F4" class="ltx_figure"><img src="/html/2305.18279/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="452" height="173" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 4: </span> Qualitative examples comparing ContextDET with existing Multimodal Language Models (MLLMs), including GPT-4Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, BLIP-2Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, and LLaVAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>.
Our method predicts related bounding boxes for the object names mentioned in the text outputs,
(<span id="S4.F4.18.1" class="ltx_text ltx_font_italic">e.g.</span>, â€˜<span id="S4.F4.19.2" class="ltx_text" style="color:#13E31A;">man</span>â€™, â€˜<span id="S4.F4.20.3" class="ltx_text" style="color:#FF8C1A;">board of a taxi</span>â€™),
enabling a more comprehensive interpretation for visual-language tasks and paving the way for broader application areas.
</figcaption>
</figure>
<div id="S4.SS4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.p1.1" class="ltx_p"><span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_bold">Comparison with MLLMs.</span>
We present some visual examples in FigureÂ <a href="#S4.F4" title="Figure 4 â€£ 4.4 Qualitative Results â€£ 4 Experiments â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and compare our ContextDET with some popular MLLMs like GPT-4Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>.
Existing MLLMs can only generate textual outputs while our ContextDET pushes the boundaries further by providing bounding boxes of objects of interest. In particular, our method allows fine-grained localization of objects of interest specified in the text input, which offers a higher degree of interpretability for vision-language models. Broadly speaking, our method offers new possibilities for various applications requiring both object localization and conversational interaction, <span id="S4.SS4.p1.1.2" class="ltx_text ltx_font_italic">e.g.</span>, AR/VR systems and robotics.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Although recent MLLMs have demonstrated remarkable abilities in vision-language tasks such as question-answering, their potential in other perception tasks remains largely unexplored. Our ContextDET highlights the significant potential of MLLMs in diverse perception tasks, such as the proposed contextual object detection task, which predicts precise object names and their locations in images for human-AI interaction. To train our model, we needed to associate object words of bounding boxes with language captions, incurring a high annotation cost. Consequently, we used less training data compared to previous MLLM papers, which may limit our final performance. In future work, we plan to explore the use of semi-supervised or weakly-supervised learning techniques to reduce annotation costs. Additionally, apart from their contextual understanding ability, we believe that other abilities of MLLMs remain underexplored for downstream tasks, such as their interactive ability for instruction tuning. For instance, can MLLMs be utilized to post-process detection outputs based on human language instructions? By providing instructions such as â€œshift the predicted box slightly to the left,â€ â€œremove the redundant overlapped box,â€ or â€œcorrect the predicted class from eagle to falcon,â€ can MLLMs adjust the predictions accordingly to meet our expectations? We hope the insights presented in this paper could inspire further research into adapting MLLMs to revolutionize more computer vision tasks.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds,
Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina
Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock,
Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,
Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Flamingo: a visual language model for few-shot learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen
Gould, and Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Bottom-up and top-down attention for image captioning and visual
question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
CÂ Lawrence Zitnick, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">VQA: Visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Ankan Bansal, Karan Sikka, Gaurav Sharma, Rama Chellappa, and Ajay Divakaran.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Zero-shot object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, JaredÂ D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Language models are few-shot learners.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
Kirillov, and Sergey Zagoruyko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">End-to-end object detection with transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Jun Chen, Han Guo, Kai Yi, Boyang Li, and Mohamed Elhoseiny.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">VisualGPT: Data-efficient adaptation of pretrained language models
for image captioning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian Shao, Wei Liu, and
Tat-Seng Chua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">SCA-CNN: Spatial and channel-wise attention in convolutional
networks for image captioning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Ting Chen, Saurabh Saxena, Lala Li, DavidÂ J Fleet, and Geoffrey Hinton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Pix2Seq: A language modeling framework for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Zhe Chen, Shaoli Huang, and Dacheng Tao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Context refinement for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
Adam Roberts, Paul Barham, HyungÂ Won Chung, Charles Sutton, Sebastian
Gehrmann, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">PaLM: Scaling language modeling with pathways.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2204.02311</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and LiÂ Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">ImageNet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, 2009.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Henghui Ding, Chang Liu, Suchen Wang, and Xudong Jiang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Vision-language transformer and query generation for referring
segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
SantoshÂ K Divvala, Derek Hoiem, JamesÂ H Hays, AlexeiÂ A Efros, and Martial
Hebert.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">An empirical study of context in object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, 2009.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">An image is worth 16x16 words: Transformers for image recognition at
scale.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Danny Driess, Fei Xia, Mehdi S.Â M. Sajjadi, Corey Lynch, Aakanksha Chowdhery,
Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong
Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine,
Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng,
Igor Mordatch, and Pete Florence.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">PaLM-E: An embodied multimodal language model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2303.03378</span><span id="bib.bib16.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
YuÂ Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, Yue Gao, and Guoqi Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Learning to prompt for open-vocabulary object detection with
vision-language model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Mark Everingham, Luc VanÂ Gool, ChristopherÂ KI Williams, John Winn, and Andrew
Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">The PASCAL Visual Object Classes (VOC) Challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IJCV</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, 2010.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, EkinÂ D Cubuk,
QuocÂ V Le, and Barret Zoph.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Simple copy-paste is a strong data augmentation method for instance
segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Making the v in vqa matter: Elevating the role of image understanding
in visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Open-vocabulary object detection via vision and language knowledge
distillation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Agrim Gupta, Piotr Dollar, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">LVIS: A dataset for large vocabulary instance segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Mask R-CNN.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Sepp Hochreiter and JÃ¼rgen Schmidhuber.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Long short-term memory.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neural computation</span><span id="bib.bib25.4.2" class="ltx_text" style="font-size:90%;">, 1997.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Shaohan Huang, LiÂ Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma,
Tengchao Lv, Lei Cui, OwaisÂ Khan Mohammed, Qiang Liu, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Language is not all you need: Aligning perception with language
models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2302.14045</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
HuggingFace.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Huggingface.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://huggingface.co/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://huggingface.co/</a><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Chao Jia, Yinfei Yang, YeÂ Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Scaling up visual and vision-language representation learning with
noisy text supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and
Nicolas Carion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Mdetr-modulated detection for end-to-end multi-modal understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Andrej Karpathy and LiÂ Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Deep visual-semantic alignments for generating image descriptions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura
Gustafson, Tete Xiao, Spencer Whitehead, AlexanderÂ C Berg, Wan-Yen Lo, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Segment anything.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2304.02643</span><span id="bib.bib31.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
JingÂ Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Grounding language models to images for multimodal generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2301.13823</span><span id="bib.bib32.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Weicheng Kuo, Yin Cui, Xiuye Gu, AJÂ Piergiovanni, and Anelia Angelova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">F-VLM: Open-vocabulary object detection upon frozen vision and
language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2209.15639</span><span id="bib.bib33.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi
Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander
Kolesnikov, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">The open images dataset v4.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IJCV</span><span id="bib.bib34.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Hei Law and Jia Deng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">CornerNet: Detecting objects as paired keypoints.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">BLIP-2: Bootstrapping language-image pre-training with frozen image
encoders and large language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2301.12597</span><span id="bib.bib36.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
LiunianÂ Harold Li*, Pengchuan Zhang*, Haotian Zhang*, Jianwei Yang, Chunyuan
Li, Yiwu Zhong, Lijuan Wang, LuÂ Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei
Chang, and Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Grounded language-image pre-training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr DollÃ¡r, and CÂ Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Microsoft COCO: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Haotian Liu, Chunyuan Li, Qingyang Wu, and YongÂ Jae Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Visual instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2304.08485</span><span id="bib.bib39.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Jiang Liu, Hui Ding, Zhaowei Cai, Yuting Zhang, RaviÂ Kumar Satzoda, Vijay
Mahadevan, and RÂ Manmatha.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">PolyFormer: Referring image segmentation as sequential polygon
generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan
Li, Jianwei Yang, Hang Su, Jun Zhu, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Grounding DINO: Marrying dino with grounded pre-training for
open-set object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2303.05499</span><span id="bib.bib41.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,
Cheng-Yang Fu, and AlexanderÂ C Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">SSD: Single shot multibox detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
ZeÂ Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
Baining Guo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Swin Transformer: Hierarchical vision transformer using shifted
windows.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Ilya Loshchilov and Frank Hutter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Decoupled weight decay regularization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Eric Mitchell, Yoonho Lee, Alexander Khazatsky, ChristopherÂ D Manning, and
Chelsea Finn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">DetectGPT: Zero-shot machine-generated text detection using
probability curvature.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2301.11305</span><span id="bib.bib45.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Ron Mokady, Amir Hertz, and AmitÂ H Bermano.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">ClipCap: Clip prefix for image captioning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2111.09734</span><span id="bib.bib46.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja
Fidler, Raquel Urtasun, and Alan Yuille.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">The role of context for object detection and semantic segmentation in
the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
LiÂ Muchen and Sigal Leonid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Referring Transformer: A one-step approach to multi-task visual
grounding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
VarunÂ K Nagaraja, VladÂ I Morariu, and LarryÂ S Davis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Modeling context between objects for referring expression
understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
OpenAI.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Chatgpt: Optimizing language models for dialogue, 2022.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openai.com/blog/chatgpt" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://openai.com/blog/chatgpt</a><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
OpenAI.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">GPT-4 technical report.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2303.08774</span><span id="bib.bib51.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Jeffrey Ouyang-Zhang, JangÂ Hyun Cho, Xingyi Zhou, and Philipp
KrÃ¤henbÃ¼hl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">NMS strikes back.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2212.06137</span><span id="bib.bib52.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
BryanÂ A Plummer, Liwei Wang, ChrisÂ M Cervantes, JuanÂ C Caicedo, Julia
Hockenmaier, and Svetlana Lazebnik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Flickr30K Entities: Collecting region-to-phrase correspondences for
richer image-to-sentence models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib53.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib53.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Alec Radford, JongÂ Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Learning transferable visual models from natural language
supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib54.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib54.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">Improving language understanding by generative pre-training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">OpenAI blog</span><span id="bib.bib55.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
Sutskever, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Language models are unsupervised multitask learners.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">OpenAI blog</span><span id="bib.bib56.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and PeterÂ J Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">Exploring the limits of transfer learning with a unified text-to-text
transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">JMLR</span><span id="bib.bib57.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
Hanoona Rasheed, Muhammad Maaz, MuhammadÂ Uzair Khattak, Salman Khan, and
FahadÂ Shahbaz Khan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">Bridging the gap between object and image-level representations for
open-vocabulary detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib58.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib58.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:90%;">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.2.1" class="ltx_text" style="font-size:90%;">Faster R-CNN: Towards real-time object detection with region
proposal networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib59.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib59.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text" style="font-size:90%;">
Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and
Silvio Savarese.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.2.1" class="ltx_text" style="font-size:90%;">Generalized intersection over union: A metric and a loss for bounding
box regression.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib60.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib60.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text" style="font-size:90%;">
VinuÂ Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and
Soheil Feizi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.2.1" class="ltx_text" style="font-size:90%;">Can ai-generated text be reliably detected?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2303.11156</span><span id="bib.bib61.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text" style="font-size:90%;">
Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing
Li, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.2.1" class="ltx_text" style="font-size:90%;">Objects365: A large-scale, high-quality dataset for object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib62.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib62.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text" style="font-size:90%;">
Yongliang Shen, Kaitao Song, XuÂ Tan, Dongsheng Li, Weiming Lu, and Yueting
Zhuang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.2.1" class="ltx_text" style="font-size:90%;">HuggingGPT: Solving ai tasks with chatgpt and its friends in
huggingface.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2303.17580</span><span id="bib.bib63.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text" style="font-size:90%;">
Abhinav Shrivastava and Abhinav Gupta.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.2.1" class="ltx_text" style="font-size:90%;">Contextual priming and feedback for faster r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib64.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib64.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text" style="font-size:90%;">
Zhi Tian, Chunhua Shen, Hao Chen, and Tong He.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.2.1" class="ltx_text" style="font-size:90%;">FCOS: Fully convolutional one-stage object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib65.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib65.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock"><span id="bib.bib66.1.1" class="ltx_text" style="font-size:90%;">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric
Hambro, Faisal Azhar, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.2.1" class="ltx_text" style="font-size:90%;">LLaMA: Open and efficient foundation language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2302.13971</span><span id="bib.bib66.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text" style="font-size:90%;">
Maria Tsimpoukelli, JacobÂ L Menick, Serkan Cabi, SMÂ Eslami, Oriol Vinyals, and
Felix Hill.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.2.1" class="ltx_text" style="font-size:90%;">Multimodal few-shot learning with frozen language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib67.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib67.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock"><span id="bib.bib68.1.1" class="ltx_text" style="font-size:90%;">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
AidanÂ N Gomez, Åukasz Kaiser, and Illia Polosukhin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.2.1" class="ltx_text" style="font-size:90%;">Attention is all you need.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib68.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib68.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock"><span id="bib.bib69.1.1" class="ltx_text" style="font-size:90%;">
Jiaqi Wang, Pan Zhang, Tao Chu, Yuhang Cao, Yujie Zhou, Tong Wu, Bin Wang,
Conghui He, and Dahua Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.2.1" class="ltx_text" style="font-size:90%;">V3Det: Vast vocabulary visual detection dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2304.03752</span><span id="bib.bib69.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock"><span id="bib.bib70.1.1" class="ltx_text" style="font-size:90%;">
Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu,
Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.2.1" class="ltx_text" style="font-size:90%;">InternImage: Exploring large-scale vision foundation models with
deformable convolutions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib70.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib70.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock"><span id="bib.bib71.1.1" class="ltx_text" style="font-size:90%;">
Zhaoqing Wang, YuÂ Lu, Qiang Li, Xunqiang Tao, Yandong Guo, Mingming Gong, and
Tongliang Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.2.1" class="ltx_text" style="font-size:90%;">CRIS: Clip-driven referring image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib71.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib71.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock"><span id="bib.bib72.1.1" class="ltx_text" style="font-size:90%;">
Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan
Duan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.2.1" class="ltx_text" style="font-size:90%;">Visual ChatGPT: Talking, drawing and editing with visual foundation
models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2303.04671</span><span id="bib.bib72.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock"><span id="bib.bib73.1.1" class="ltx_text" style="font-size:90%;">
Jianzong Wu, Xiangtai Li, Henghui Ding, Xia Li, Guangliang Cheng, Yunhai Tong,
and ChenÂ Change Loy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.2.1" class="ltx_text" style="font-size:90%;">Betrayed by captions: Joint caption grounding and generation for open
vocabulary instance segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2301.00805</span><span id="bib.bib73.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock"><span id="bib.bib74.1.1" class="ltx_text" style="font-size:90%;">
Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, and ChenÂ Change Loy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.2.1" class="ltx_text" style="font-size:90%;">Aligning bag of regions for open-vocabulary object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib74.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib74.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock"><span id="bib.bib75.1.1" class="ltx_text" style="font-size:90%;">
Xiaoshi Wu, Feng Zhu, Rui Zhao, and Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.2.1" class="ltx_text" style="font-size:90%;">CORA: Adapting clip for open-vocabulary detection with region
prompting and anchor pre-matching.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib75.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib75.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock"><span id="bib.bib76.1.1" class="ltx_text" style="font-size:90%;">
Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Hengshuang Zhao, and PhilipÂ HS
Torr.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.2.1" class="ltx_text" style="font-size:90%;">LAVT: Language-aware vision transformer for referring image
segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib76.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib76.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock"><span id="bib.bib77.1.1" class="ltx_text" style="font-size:90%;">
Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal
Ahmed, Zicheng Liu, CeÂ Liu, Michael Zeng, and Lijuan Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.2.1" class="ltx_text" style="font-size:90%;">MM-REACT: Prompting chatgpt for multimodal reasoning and action.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2303.11381</span><span id="bib.bib77.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock"><span id="bib.bib78.1.1" class="ltx_text" style="font-size:90%;">
Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.2.1" class="ltx_text" style="font-size:90%;">From image descriptions to visual denotations: New similarity metrics
for semantic inference over event descriptions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">TACL</span><span id="bib.bib78.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock"><span id="bib.bib79.1.1" class="ltx_text" style="font-size:90%;">
Licheng Yu, Patrick Poirson, Shan Yang, AlexanderÂ C Berg, and TamaraÂ L Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.2.1" class="ltx_text" style="font-size:90%;">Modeling context in referring expressions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib79.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib79.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock"><span id="bib.bib80.1.1" class="ltx_text" style="font-size:90%;">
Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal,
Chenguang Zhu, Michael Zeng, and Meng Jiang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.2.1" class="ltx_text" style="font-size:90%;">Generate rather than retrieve: Large language models are strong
context generators.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib80.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib80.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock"><span id="bib.bib81.1.1" class="ltx_text" style="font-size:90%;">
Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and ChenÂ Change Loy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.2.1" class="ltx_text" style="font-size:90%;">Open-vocabulary detr with conditional matching.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib81.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib81.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock"><span id="bib.bib82.1.1" class="ltx_text" style="font-size:90%;">
Alireza Zareian, KevinÂ Dela Rosa, DerekÂ Hao Hu, and Shih-Fu Chang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.2.1" class="ltx_text" style="font-size:90%;">Open-vocabulary object detection using captions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib82.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib82.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock"><span id="bib.bib83.1.1" class="ltx_text" style="font-size:90%;">
Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, LionelÂ M Ni, and
Heung-Yeung Shum.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib83.2.1" class="ltx_text" style="font-size:90%;">DINO: Detr with improved denoising anchor boxes for end-to-end
object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib83.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib83.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib83.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock"><span id="bib.bib84.1.1" class="ltx_text" style="font-size:90%;">
Haotian* Zhang, Pengchuan* Zhang, Xiaowei Hu, Yen-Chun Chen, LiunianÂ Harold Li,
Xiyang Dai, Lijuan Wang, LuÂ Yuan, Jenq-Neng Hwang, and Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib84.2.1" class="ltx_text" style="font-size:90%;">GLIPv2: Unifying localization and vision-language understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib84.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib84.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib84.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock"><span id="bib.bib85.1.1" class="ltx_text" style="font-size:90%;">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
Chen, Christopher Dewan, Mona Diab, Xian Li, XiÂ Victoria Lin, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib85.2.1" class="ltx_text" style="font-size:90%;">OPT: Open pre-trained transformer language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib85.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2205.01068</span><span id="bib.bib85.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock"><span id="bib.bib86.1.1" class="ltx_text" style="font-size:90%;">
Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella,
LiunianÂ Harold Li, Luowei Zhou, Xiyang Dai, LuÂ Yuan, Yin Li, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib86.2.1" class="ltx_text" style="font-size:90%;">RegionCLIP: Region-based language-image pretraining.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib86.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib86.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib86.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock"><span id="bib.bib87.1.1" class="ltx_text" style="font-size:90%;">
Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp KrÃ¤henbÃ¼hl, and
Ishan Misra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib87.2.1" class="ltx_text" style="font-size:90%;">Detecting twenty-thousand classes using image-level supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib87.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib87.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib87.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock"><span id="bib.bib88.1.1" class="ltx_text" style="font-size:90%;">
Chaoyang Zhu, Yiyi Zhou, Yunhang Shen, Gen Luo, Xingjia Pan, Mingbao Lin, Chao
Chen, Liujuan Cao, Xiaoshuai Sun, and Rongrong Ji.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib88.2.1" class="ltx_text" style="font-size:90%;">SeqTR: A simple yet universal network for visual grounding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib88.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib88.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib88.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock"><span id="bib.bib89.1.1" class="ltx_text" style="font-size:90%;">
Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib89.2.1" class="ltx_text" style="font-size:90%;">Deformable DETR: Deformable transformers for end-to-end object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib89.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib89.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib89.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p ltx_align_center"><span id="p1.1.1" class="ltx_text ltx_font_bold" style="font-size:120%;">Appendix<span id="p1.1.1.1" class="ltx_text ltx_font_medium"></span></span></p>
</div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p">In this supplementary, we discuss more related works, dataset and evaluation details, more experimental results, and the broader impact of this work.</p>
<ul id="A0.I1" class="ltx_itemize">
<li id="A0.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="A0.I1.i1.1.1.1" class="ltx_text" style="color:#000000;">â€¢</span></span> 
<div id="A0.I1.i1.p1" class="ltx_para">
<p id="A0.I1.i1.p1.1" class="ltx_p">In SectionÂ <a href="#A1" title="Appendix A More Related Works â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>, we present more discussion about related works.</p>
</div>
</li>
<li id="A0.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="A0.I1.i2.1.1.1" class="ltx_text" style="color:#000000;">â€¢</span></span> 
<div id="A0.I1.i2.p1" class="ltx_para">
<p id="A0.I1.i2.p1.1" class="ltx_p">In SectionÂ <a href="#A2" title="Appendix B Details of CODE Benchmark â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>, we discuss more details about our proposed CODE benchmark.</p>
</div>
</li>
<li id="A0.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="A0.I1.i3.1.1.1" class="ltx_text" style="color:#000000;">â€¢</span></span> 
<div id="A0.I1.i3.p1" class="ltx_para">
<p id="A0.I1.i3.p1.1" class="ltx_p">In SectionÂ <a href="#A3" title="Appendix C Details of Evaluation for Contextual Cloze Test â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>, we provide the evaluation details on our proposed contextual cloze test setting.</p>
</div>
</li>
<li id="A0.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="A0.I1.i4.1.1.1" class="ltx_text" style="color:#000000;">â€¢</span></span> 
<div id="A0.I1.i4.p1" class="ltx_para">
<p id="A0.I1.i4.p1.1" class="ltx_p">In SectionÂ <a href="#A4" title="Appendix D More Experiments â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>, we show more experiments on the referring image segmentation task and qualitative results on the contextual object detection task.</p>
</div>
</li>
<li id="A0.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="A0.I1.i5.1.1.1" class="ltx_text" style="color:#000000;">â€¢</span></span> 
<div id="A0.I1.i5.p1" class="ltx_para">
<p id="A0.I1.i5.p1.1" class="ltx_p">In SectionÂ <a href="#A5" title="Appendix E Broader Impact â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>, we provide the discussion about the broader impact of this paper.</p>
</div>
</li>
</ul>
</div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>More Related Works</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">In this section, we discuss more related tasks that are not fully covered in the main paper, including image captioning and visual question answering. TableÂ <a href="#A2.T5" title="Table 5 â€£ Appendix B Details of CODE Benchmark â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> also summarizes the differences between our proposed three contextual object detection settings with previous related tasks.</p>
</div>
<div id="A1.p2" class="ltx_para ltx_noindent">
<p id="A1.p2.1" class="ltx_p"><span id="A1.p2.1.1" class="ltx_text ltx_font_bold">Image Captioning.</span> Image captioning focuses on generating descriptive sentences to understand given images. Typically, image captioning models first encode the input image as feature embeddings using pre-trained classificationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, object detectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> or vision language modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. Subsequently, submodules like LSTMsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> or TransformersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> are employed to decode feature embeddings into predicted sentences. In contrast, our <em id="A1.p2.1.2" class="ltx_emph ltx_font_italic">contextual captioning</em> task extends beyond language outputs by requiring the model to predict the locations of the bounding boxes containing the objects mentioned in the generated captions.</p>
</div>
<div id="A1.p3" class="ltx_para ltx_noindent">
<p id="A1.p3.1" class="ltx_p"><span id="A1.p3.1.1" class="ltx_text ltx_font_bold">Visual Question Answering (VQA).</span> Visual question answering tasks involve answering questions related to given imagesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. In traditional VQA, model inputs and outputs are comprised of question-answer pairs in natural language. However, in our <em id="A1.p3.1.2" class="ltx_emph ltx_font_italic">contextual QA</em> task, questions are specifically focused on inquiring about object names and locations, while corresponding answers are expected to include the corresponding referring bounding boxes.</p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Details of CODE Benchmark</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">In this section, we provide comprehensive details about how we collect the CODE dataset to facilitate research on contextual object detection.</p>
</div>
<div id="A2.p2" class="ltx_para ltx_noindent">
<p id="A2.p2.1" class="ltx_p"><span id="A2.p2.1.1" class="ltx_text ltx_font_bold">Data Format.</span>
Our CODE benchmark follows the data format of the COCO dataset and includes additional fields to facilitate the evaluation, as shown in FigureÂ <a href="#A2.F5" title="Figure 5 â€£ Appendix B Details of CODE Benchmark â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
The images and annotations used in our new benchmark are based on Flickr 30kÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> and Flickr30k EntitiesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>.
We tokenize the language caption using the LLM tokenizer and record the related language tokens.
For each object name that appears in the tokens generated by the tokenizer, we track the start and end indices, which will be replaced with the <span id="A2.p2.1.2" class="ltx_text ltx_font_typewriter">[MASK]</span> token for our contextual cloze test task.</p>
</div>
<div id="A2.p3" class="ltx_para ltx_noindent">
<p id="A2.p3.1" class="ltx_p"><span id="A2.p3.1.1" class="ltx_text ltx_font_bold">Word Clouds.</span>
In the contextual cloze test setting, our CODE dataset consists of 10,346 unique object words that are masked and required to be predicted. FigureÂ <a href="#A2.F6" title="Figure 6 â€£ Appendix B Details of CODE Benchmark â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> presents the word cloud visualizations of object words in our dataset.
We can observe both high-frequency words such as â€˜manâ€™ and â€˜woman,â€™ as well as low-frequency words such as â€˜playerâ€™, â€˜scootyâ€™, and â€˜breadstick,â€™ which pose challenges for accurate predictions. Therefore, achieving precise predictions for these object words requires understanding contextual information.</p>
</div>
<figure id="A2.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 5: </span>Comparison of our proposed three contextual object detection settings with previous related tasks.
</figcaption>
<div id="A2.T5.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:429.3pt;height:149.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-44.4pt,15.4pt) scale(0.828496376018297,0.828496376018297) ;">
<table id="A2.T5.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A2.T5.3.1.1.1" class="ltx_tr">
<th id="A2.T5.3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T5.3.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Tasks</span></th>
<td id="A2.T5.3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T5.3.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Language Input</span></td>
<td id="A2.T5.3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T5.3.1.1.1.3.1" class="ltx_text" style="font-size:90%;">Output(s)</span></td>
<td id="A2.T5.3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T5.3.1.1.1.4.1" class="ltx_text" style="font-size:90%;">Remark</span></td>
</tr>
<tr id="A2.T5.3.1.2.2" class="ltx_tr">
<th id="A2.T5.3.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T5.3.1.2.2.1.1" class="ltx_text" style="font-size:90%;">Object Detection</span></th>
<td id="A2.T5.3.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T5.3.1.2.2.2.1" class="ltx_text" style="font-size:90%;">âœ—</span></td>
<td id="A2.T5.3.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T5.3.1.2.2.3.1" class="ltx_text" style="font-size:90%;">box, class label</span></td>
<td id="A2.T5.3.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T5.3.1.2.2.4.1" class="ltx_text" style="font-size:90%;">pre-defined class labels</span></td>
</tr>
<tr id="A2.T5.3.1.3.3" class="ltx_tr">
<th id="A2.T5.3.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T5.3.1.3.3.1.1" class="ltx_text" style="font-size:90%;">Open-Vocabulary Object Detection</span></th>
<td id="A2.T5.3.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T5.3.1.3.3.2.1" class="ltx_text" style="font-size:90%;">(optional) class names for CLIP</span></td>
<td id="A2.T5.3.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T5.3.1.3.3.3.1" class="ltx_text" style="font-size:90%;">box, class label</span></td>
<td id="A2.T5.3.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T5.3.1.3.3.4.1" class="ltx_text" style="font-size:90%;">pre-defined class labels</span></td>
</tr>
<tr id="A2.T5.3.1.4.4" class="ltx_tr">
<th id="A2.T5.3.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T5.3.1.4.4.1.1" class="ltx_text" style="font-size:90%;">Referring Expression Comprehension</span></th>
<td id="A2.T5.3.1.4.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T5.3.1.4.4.2.1" class="ltx_text" style="font-size:90%;">complete referring expression</span></td>
<td id="A2.T5.3.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T5.3.1.4.4.3.1" class="ltx_text" style="font-size:90%;">box that expression refers to</span></td>
<td id="A2.T5.3.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T5.3.1.4.4.4.1" class="ltx_text" style="font-size:90%;">/</span></td>
</tr>
<tr id="A2.T5.3.1.5.5" class="ltx_tr">
<th id="A2.T5.3.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-left:4.5pt;padding-right:4.5pt;" rowspan="2"><span id="A2.T5.3.1.5.5.1.1" class="ltx_text" style="font-size:90%;"><span id="A2.T5.3.1.5.5.1.1.1" class="ltx_text ltx_font_bold">Contextual Cloze Test</span> (ours)</span></th>
<td id="A2.T5.3.1.5.5.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.5pt;padding-right:4.5pt;">
<span id="A2.T5.3.1.5.5.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#FF8000;">incomplete</span><span id="A2.T5.3.1.5.5.2.2" class="ltx_text" style="font-size:90%;"> expression</span>
</td>
<td id="A2.T5.3.1.5.5.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.5pt;padding-right:4.5pt;">
<span id="A2.T5.3.1.5.5.3.1" class="ltx_text" style="font-size:90%;">{box, </span><span id="A2.T5.3.1.5.5.3.2" class="ltx_text ltx_font_bold" style="font-size:90%;color:#FF8000;">name</span><span id="A2.T5.3.1.5.5.3.3" class="ltx_text" style="font-size:90%;">}</span>
</td>
<td id="A2.T5.3.1.5.5.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.5pt;padding-right:4.5pt;">
<span id="A2.T5.3.1.5.5.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#FF8000;">name</span><span id="A2.T5.3.1.5.5.4.2" class="ltx_text" style="font-size:90%;">Â could be</span>
</td>
</tr>
<tr id="A2.T5.3.1.6.6" class="ltx_tr">
<td id="A2.T5.3.1.6.6.1" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T5.3.1.6.6.1.1" class="ltx_text" style="font-size:90%;">object names are masked</span></td>
<td id="A2.T5.3.1.6.6.2" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T5.3.1.6.6.2.1" class="ltx_text" style="font-size:90%;">to complete the mask</span></td>
<td id="A2.T5.3.1.6.6.3" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T5.3.1.6.6.3.1" class="ltx_text" style="font-size:90%;">most valid English word</span></td>
</tr>
<tr id="A2.T5.3.1.7.7" class="ltx_tr">
<th id="A2.T5.3.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T5.3.1.7.7.1.1" class="ltx_text" style="font-size:90%;">Image Captioning</span></th>
<td id="A2.T5.3.1.7.7.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T5.3.1.7.7.2.1" class="ltx_text" style="font-size:90%;">âœ—</span></td>
<td id="A2.T5.3.1.7.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T5.3.1.7.7.3.1" class="ltx_text" style="font-size:90%;">language caption</span></td>
<td id="A2.T5.3.1.7.7.4" class="ltx_td ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
</tr>
<tr id="A2.T5.3.1.8.8" class="ltx_tr">
<th id="A2.T5.3.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.5pt;padding-right:4.5pt;">
<span id="A2.T5.3.1.8.8.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Contextual Captioning</span><span id="A2.T5.3.1.8.8.1.2" class="ltx_text" style="font-size:90%;"> (ours)</span>
</th>
<td id="A2.T5.3.1.8.8.2" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T5.3.1.8.8.2.1" class="ltx_text" style="font-size:90%;">âœ—</span></td>
<td id="A2.T5.3.1.8.8.3" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">
<span id="A2.T5.3.1.8.8.3.1" class="ltx_text" style="font-size:90%;">language caption, </span><span id="A2.T5.3.1.8.8.3.2" class="ltx_text ltx_font_bold" style="font-size:90%;color:#FF8000;">box</span>
</td>
<td id="A2.T5.3.1.8.8.4" class="ltx_td" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
</tr>
<tr id="A2.T5.3.1.9.9" class="ltx_tr">
<th id="A2.T5.3.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T5.3.1.9.9.1.1" class="ltx_text" style="font-size:90%;">Visual Question Answering</span></th>
<td id="A2.T5.3.1.9.9.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T5.3.1.9.9.2.1" class="ltx_text" style="font-size:90%;">language question</span></td>
<td id="A2.T5.3.1.9.9.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T5.3.1.9.9.3.1" class="ltx_text" style="font-size:90%;">language answer</span></td>
<td id="A2.T5.3.1.9.9.4" class="ltx_td ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
</tr>
<tr id="A2.T5.3.1.10.10" class="ltx_tr">
<th id="A2.T5.3.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:4.5pt;padding-right:4.5pt;">
<span id="A2.T5.3.1.10.10.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Contextual QA</span><span id="A2.T5.3.1.10.10.1.2" class="ltx_text" style="font-size:90%;"> (ours)</span>
</th>
<td id="A2.T5.3.1.10.10.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T5.3.1.10.10.2.1" class="ltx_text" style="font-size:90%;">language question</span></td>
<td id="A2.T5.3.1.10.10.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.5pt;padding-right:4.5pt;">
<span id="A2.T5.3.1.10.10.3.1" class="ltx_text" style="font-size:90%;">language answer, </span><span id="A2.T5.3.1.10.10.3.2" class="ltx_text ltx_font_bold" style="font-size:90%;color:#FF8000;">box</span>
</td>
<td id="A2.T5.3.1.10.10.4" class="ltx_td ltx_border_bb" style="padding-left:4.5pt;padding-right:4.5pt;"></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="A2.F5" class="ltx_figure"><img src="/html/2305.18279/assets/x5.png" id="A2.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="456" height="184" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>
<span id="A2.F5.2.1" class="ltx_text" style="font-size:90%;">Our CODE benchmark follows the data format of the COCO datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, with additional fields (<span id="A2.F5.2.1.1" class="ltx_text" style="color:#4D4DFF;">blue</span> color) including the language caption, token ids, and object name. Token ids record the start and end position index of the object name existing in the language tokens.
</span></figcaption>
</figure>
<figure id="A2.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<div id="A2.F6.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:138.8pt;">
<img src="/html/2305.18279/assets/x6.png" id="A2.F6.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="230" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<div id="A2.F6.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:138.8pt;">
<img src="/html/2305.18279/assets/x7.png" id="A2.F6.2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="230" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<div id="A2.F6.3" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:138.8pt;">
<img src="/html/2305.18279/assets/x8.png" id="A2.F6.3.g1" class="ltx_graphics ltx_img_landscape" width="461" height="230" alt="Refer to caption">
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>
<span id="A2.F6.5.1" class="ltx_text" style="font-size:90%;">Word clouds of object words presented in the CODE train set (<span id="A2.F6.5.1.1" class="ltx_text ltx_font_bold">left</span>) and test set (<span id="A2.F6.5.1.2" class="ltx_text ltx_font_bold">middle</span>, <span id="A2.F6.5.1.3" class="ltx_text ltx_font_bold">right</span>). The <span id="A2.F6.5.1.4" class="ltx_text ltx_font_bold">middle</span> figure represents the visualization of high-frequency words in the test set, while the <span id="A2.F6.5.1.5" class="ltx_text ltx_font_bold">right</span> figure showcases the visualization of low-frequency words.
</span></figcaption>
</figure>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Details of Evaluation for Contextual Cloze Test</h2>

<figure id="A3.F7" class="ltx_figure"><img src="/html/2305.18279/assets/x9.png" id="A3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="456" height="315" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>
<span id="A3.F7.2.1" class="ltx_text" style="font-size:90%;">The comparison of (<span id="A3.F7.2.1.1" class="ltx_text ltx_font_bold">a</span>) the evaluation criteria for the traditional object detection task, and (<span id="A3.F7.2.1.2" class="ltx_text ltx_font_bold">b</span>) evaluation of our contextual cloze test.
</span></figcaption>
</figure>
<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">Existing object detection datasets, such as Pascal VOCÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, Microsoft COCOÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, Open ImagesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, LVISÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, Objects365Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> and V3DetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>, rely on predefined mappings between label IDs and class names for evaluation purposes. For example, the COCO dataset uses a mapping like (1, person), (2, bicycle), <math id="A3.p1.1.m1.1" class="ltx_Math" alttext="\dotsc" display="inline"><semantics id="A3.p1.1.m1.1a"><mi mathvariant="normal" id="A3.p1.1.m1.1.1" xref="A3.p1.1.m1.1.1.cmml">â€¦</mi><annotation-xml encoding="MathML-Content" id="A3.p1.1.m1.1b"><ci id="A3.p1.1.m1.1.1.cmml" xref="A3.p1.1.m1.1.1">â€¦</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.1.m1.1c">\dotsc</annotation></semantics></math>, (80, toothbrush) for its 80 classes. As shown in Fig.<a href="#A3.F7" title="Figure 7 â€£ Appendix C Details of Evaluation for Contextual Cloze Test â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>(a), in order to be classified as true positives, predicted bounding boxes must exhibit both high IoU overlap and identical class IDs to the ground-truth boxes. In certain scenarios, such as zero-shot<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> or open-vocabularyÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite> object detection settings, the predefined classes are divided into two separate groups: <em id="A3.p1.1.1" class="ltx_emph ltx_font_italic">base</em> and <em id="A3.p1.1.2" class="ltx_emph ltx_font_italic">novel</em>, to evaluate the modelâ€™s generalization capability. However, these evaluations still rely on the predefined ID-name mappings, while objects with names not included in predefined mappings are impossible.</p>
</div>
<div id="A3.p2" class="ltx_para">
<p id="A3.p2.1" class="ltx_p">Human perception does not depend on pre-defined class IDs.
Therefore, for our proposed contextual cloze test task, we have established new evaluation criteria that use object names from human language. In this evaluation, given a masked language expression and the indexes of the masked words, we classify predicted boxes as true positives if they i) exhibit high IoU overlap, ii) share the same meaning, and iii) have an identical masked index as the ground truth boxes. Conversely, predictions are considered false positives. The masked indexes are employed to differentiate cases where multiple objects have the same name but are located at different <span id="A3.p2.1.1" class="ltx_text ltx_font_typewriter">[MASK]</span> token positions within a sentence. The object names correspond to the most valid English words decoded by the Tokenizer of LLMs.</p>
</div>
<div id="A3.p3" class="ltx_para">
<p id="A3.p3.1" class="ltx_p">After defining our name-based criteria as true-positive/false-positive metrics, we could compute the overall Average Precision (AP) metric for evaluation.
We follow the COCO dataset to set the IoU thresholds ranging from 0.5 to 0.95 with a step size of 0.05. The per-name AP is not computed because there are numerous long-tailed infrequent names, of which only a few examples are available for evaluation.</p>
</div>
<div id="A3.p4" class="ltx_para ltx_noindent">
<p id="A3.p4.1" class="ltx_p"><span id="A3.p4.1.1" class="ltx_text ltx_font_bold">AP@5 for Top-5 Predictied Names.</span>
In some cases, our evaluation metric can be overly stringent, particularly when dealing with numerous synonyms or fine-grained categories that are challenging for annotators to distinguish. Similar challenges have been encountered in previous image classification datasets like ImageNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, where the top-5 accuracy metric is used as a supplementary metric to the top-1 accuracy metric. Therefore, we also introduce a supplementary metric called top-5 AP (AP@5), which relaxes the definition of true positives. Under AP@5, if the ground-truth name is among the top-5 predictions, the predictions are considered true positives. In contrast, the AP metric calculated based on the top-1 prediction result is referred to as AP@1 to differentiate it from AP@5.</p>
</div>
<div id="A3.p5" class="ltx_para ltx_noindent">
<p id="A3.p5.1" class="ltx_p"><span id="A3.p5.1.1" class="ltx_text ltx_font_bold">Implementation Details.</span>
We modify the famous <span id="A3.p5.1.2" class="ltx_text ltx_font_italic">pycocotools</span> packageÂ <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://github.com/cocodataset/cocoapi</span></span></span> provided in the COCO dataset and create the evaluation script.</p>
</div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>More Experiments</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">In this section, we first demonstrate that our ContextDET could be extended to segmentation tasks, such as the referring image segmentation task discussed in SectionÂ <a href="#A4.SS1" title="D.1 Referring Image Segmentation â€£ Appendix D More Experiments â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D.1</span></a>. Next, we provide additional qualitative results, including failure cases, in SectionÂ <a href="#A4.SS2" title="D.2 More Qualitative Results â€£ Appendix D More Experiments â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D.2</span></a>.</p>
</div>
<section id="A4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.1 </span>Referring Image Segmentation</h3>

<figure id="A4.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 6: </span> Comparisons with state-of-the-art methods on three referring image segmentation benchmarks in terms of the mean Intersection over Union (mIoU) metric.
</figcaption>
<div id="A4.T6.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:429.3pt;height:103.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-83.7pt,20.2pt) scale(0.719376157581839,0.719376157581839) ;">
<table id="A4.T6.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.T6.3.1.1.1" class="ltx_tr">
<th id="A4.T6.3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-bottom:1.93748pt;padding-top:0.45pt;padding-bottom:0.45pt;" rowspan="2"><span id="A4.T6.3.1.1.1.1.1" class="ltx_text" style="font-size:90%;color:#0FE3FF;">#</span></th>
<th id="A4.T6.3.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-bottom:1.93748pt;padding-top:0.45pt;padding-bottom:0.45pt;" rowspan="2"><span id="A4.T6.3.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Method</span></th>
<th id="A4.T6.3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-bottom:1.93748pt;padding-top:0.45pt;padding-bottom:0.45pt;" rowspan="2"><span id="A4.T6.3.1.1.1.3.1" class="ltx_text" style="font-size:90%;">Venue</span></th>
<th id="A4.T6.3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-bottom:1.93748pt;padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.1.1.4.1" class="ltx_text" style="font-size:90%;">Language</span></th>
<th id="A4.T6.3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-bottom:1.93748pt;padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.1.1.5.1" class="ltx_text" style="font-size:90%;">Vision</span></th>
<th id="A4.T6.3.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="background-color:#FFD9B3;padding-bottom:1.93748pt;padding-top:0.45pt;padding-bottom:0.45pt;" colspan="3"><span id="A4.T6.3.1.1.1.6.1" class="ltx_text" style="font-size:90%;background-color:#FFD9B3;">RefCOCO</span></th>
<th id="A4.T6.3.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="background-color:#B0F6B3;padding-bottom:1.93748pt;padding-top:0.45pt;padding-bottom:0.45pt;" colspan="3"><span id="A4.T6.3.1.1.1.7.1" class="ltx_text" style="font-size:90%;background-color:#B0F6B3;">RefCOCO+</span></th>
<th id="A4.T6.3.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="background-color:#B3FFFF;padding-bottom:1.93748pt;padding-top:0.45pt;padding-bottom:0.45pt;" colspan="2"><span id="A4.T6.3.1.1.1.8.1" class="ltx_text" style="font-size:90%;background-color:#B3FFFF;">RefCOCOg</span></th>
</tr>
<tr id="A4.T6.3.1.2.2" class="ltx_tr">
<th id="A4.T6.3.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.2.2.1.1" class="ltx_text" style="font-size:90%;">Model</span></th>
<th id="A4.T6.3.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.2.2.2.1" class="ltx_text" style="font-size:90%;">Backbone</span></th>
<th id="A4.T6.3.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.2.2.3.1" class="ltx_text" style="font-size:90%;">val</span></th>
<th id="A4.T6.3.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.2.2.4.1" class="ltx_text" style="font-size:90%;">testA</span></th>
<th id="A4.T6.3.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.2.2.5.1" class="ltx_text" style="font-size:90%;">testB</span></th>
<th id="A4.T6.3.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.2.2.6.1" class="ltx_text" style="font-size:90%;">val</span></th>
<th id="A4.T6.3.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.2.2.7.1" class="ltx_text" style="font-size:90%;">testA</span></th>
<th id="A4.T6.3.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.2.2.8.1" class="ltx_text" style="font-size:90%;">testB</span></th>
<th id="A4.T6.3.1.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.2.2.9.1" class="ltx_text" style="font-size:90%;">val</span></th>
<th id="A4.T6.3.1.2.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.2.2.10.1" class="ltx_text" style="font-size:90%;">test</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.T6.3.1.3.1" class="ltx_tr">
<td id="A4.T6.3.1.3.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.3.1.1.1" class="ltx_text" style="font-size:90%;color:#0FE3FF;">1</span></td>
<td id="A4.T6.3.1.3.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;">
<span id="A4.T6.3.1.3.1.2.1" class="ltx_text" style="font-size:90%;">VLTÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A4.T6.3.1.3.1.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="A4.T6.3.1.3.1.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="A4.T6.3.1.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.3.1.3.1" class="ltx_text" style="font-size:90%;">ICCVâ€™21</span></td>
<td id="A4.T6.3.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.3.1.4.1" class="ltx_text" style="font-size:90%;">Bi-GRU</span></td>
<td id="A4.T6.3.1.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.3.1.5.1" class="ltx_text" style="font-size:90%;">DN53</span></td>
<td id="A4.T6.3.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.3.1.6.1" class="ltx_text" style="font-size:90%;">65.65</span></td>
<td id="A4.T6.3.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.3.1.7.1" class="ltx_text" style="font-size:90%;">68.29</span></td>
<td id="A4.T6.3.1.3.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.3.1.8.1" class="ltx_text" style="font-size:90%;">62.73</span></td>
<td id="A4.T6.3.1.3.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.3.1.9.1" class="ltx_text" style="font-size:90%;">55.50</span></td>
<td id="A4.T6.3.1.3.1.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.3.1.10.1" class="ltx_text" style="font-size:90%;">59.20</span></td>
<td id="A4.T6.3.1.3.1.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.3.1.11.1" class="ltx_text" style="font-size:90%;">49.36</span></td>
<td id="A4.T6.3.1.3.1.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.3.1.12.1" class="ltx_text" style="font-size:90%;">52.99</span></td>
<td id="A4.T6.3.1.3.1.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.3.1.13.1" class="ltx_text" style="font-size:90%;">56.65</span></td>
</tr>
<tr id="A4.T6.3.1.4.2" class="ltx_tr">
<td id="A4.T6.3.1.4.2.1" class="ltx_td ltx_align_left" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.4.2.1.1" class="ltx_text" style="font-size:90%;color:#0FE3FF;">2</span></td>
<td id="A4.T6.3.1.4.2.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;">
<span id="A4.T6.3.1.4.2.2.1" class="ltx_text" style="font-size:90%;">SeqTRÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A4.T6.3.1.4.2.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib88" title="" class="ltx_ref">88</a><span id="A4.T6.3.1.4.2.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="A4.T6.3.1.4.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.4.2.3.1" class="ltx_text" style="font-size:90%;">ECCVâ€™22</span></td>
<td id="A4.T6.3.1.4.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.4.2.4.1" class="ltx_text" style="font-size:90%;">Bi-GRU</span></td>
<td id="A4.T6.3.1.4.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.4.2.5.1" class="ltx_text" style="font-size:90%;">DN53</span></td>
<td id="A4.T6.3.1.4.2.6" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.4.2.6.1" class="ltx_text" style="font-size:90%;">71.70</span></td>
<td id="A4.T6.3.1.4.2.7" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.4.2.7.1" class="ltx_text" style="font-size:90%;">73.31</span></td>
<td id="A4.T6.3.1.4.2.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.4.2.8.1" class="ltx_text" style="font-size:90%;">69.82</span></td>
<td id="A4.T6.3.1.4.2.9" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.4.2.9.1" class="ltx_text" style="font-size:90%;">63.04</span></td>
<td id="A4.T6.3.1.4.2.10" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.4.2.10.1" class="ltx_text" style="font-size:90%;">66.73</span></td>
<td id="A4.T6.3.1.4.2.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.4.2.11.1" class="ltx_text" style="font-size:90%;">58.97</span></td>
<td id="A4.T6.3.1.4.2.12" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.4.2.12.1" class="ltx_text" style="font-size:90%;">64.69</span></td>
<td id="A4.T6.3.1.4.2.13" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.4.2.13.1" class="ltx_text" style="font-size:90%;">65.74</span></td>
</tr>
<tr id="A4.T6.3.1.5.3" class="ltx_tr">
<td id="A4.T6.3.1.5.3.1" class="ltx_td ltx_align_left" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.5.3.1.1" class="ltx_text" style="font-size:90%;color:#0FE3FF;">3</span></td>
<td id="A4.T6.3.1.5.3.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;">
<span id="A4.T6.3.1.5.3.2.1" class="ltx_text" style="font-size:90%;">RefTRÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A4.T6.3.1.5.3.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib48" title="" class="ltx_ref">48</a><span id="A4.T6.3.1.5.3.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="A4.T6.3.1.5.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.5.3.3.1" class="ltx_text" style="font-size:90%;">NeurIPSâ€™21</span></td>
<td id="A4.T6.3.1.5.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.5.3.4.1" class="ltx_text" style="font-size:90%;">BERT-base</span></td>
<td id="A4.T6.3.1.5.3.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.5.3.5.1" class="ltx_text" style="font-size:90%;">RN101</span></td>
<td id="A4.T6.3.1.5.3.6" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.5.3.6.1" class="ltx_text" style="font-size:90%;">74.34</span></td>
<td id="A4.T6.3.1.5.3.7" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.5.3.7.1" class="ltx_text" style="font-size:90%;">76.77</span></td>
<td id="A4.T6.3.1.5.3.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.5.3.8.1" class="ltx_text" style="font-size:90%;">70.87</span></td>
<td id="A4.T6.3.1.5.3.9" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.5.3.9.1" class="ltx_text" style="font-size:90%;">66.75</span></td>
<td id="A4.T6.3.1.5.3.10" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.5.3.10.1" class="ltx_text" style="font-size:90%;">70.58</span></td>
<td id="A4.T6.3.1.5.3.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.5.3.11.1" class="ltx_text" style="font-size:90%;">59.40</span></td>
<td id="A4.T6.3.1.5.3.12" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.5.3.12.1" class="ltx_text" style="font-size:90%;">66.63</span></td>
<td id="A4.T6.3.1.5.3.13" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.5.3.13.1" class="ltx_text" style="font-size:90%;">67.39</span></td>
</tr>
<tr id="A4.T6.3.1.6.4" class="ltx_tr">
<td id="A4.T6.3.1.6.4.1" class="ltx_td ltx_align_left" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.6.4.1.1" class="ltx_text" style="font-size:90%;color:#0FE3FF;">4</span></td>
<td id="A4.T6.3.1.6.4.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;">
<span id="A4.T6.3.1.6.4.2.1" class="ltx_text" style="font-size:90%;">LAVTÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A4.T6.3.1.6.4.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib76" title="" class="ltx_ref">76</a><span id="A4.T6.3.1.6.4.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="A4.T6.3.1.6.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.6.4.3.1" class="ltx_text" style="font-size:90%;">CVPRâ€™22</span></td>
<td id="A4.T6.3.1.6.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.6.4.4.1" class="ltx_text" style="font-size:90%;">BERT-base</span></td>
<td id="A4.T6.3.1.6.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.6.4.5.1" class="ltx_text" style="font-size:90%;">Swin-B</span></td>
<td id="A4.T6.3.1.6.4.6" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.6.4.6.1" class="ltx_text" style="font-size:90%;">74.46</span></td>
<td id="A4.T6.3.1.6.4.7" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.6.4.7.1" class="ltx_text" style="font-size:90%;">76.89</span></td>
<td id="A4.T6.3.1.6.4.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.6.4.8.1" class="ltx_text" style="font-size:90%;">70.94</span></td>
<td id="A4.T6.3.1.6.4.9" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.6.4.9.1" class="ltx_text" style="font-size:90%;">65.81</span></td>
<td id="A4.T6.3.1.6.4.10" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.6.4.10.1" class="ltx_text" style="font-size:90%;">70.97</span></td>
<td id="A4.T6.3.1.6.4.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.6.4.11.1" class="ltx_text" style="font-size:90%;">59.23</span></td>
<td id="A4.T6.3.1.6.4.12" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.6.4.12.1" class="ltx_text" style="font-size:90%;">63.34</span></td>
<td id="A4.T6.3.1.6.4.13" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.6.4.13.1" class="ltx_text" style="font-size:90%;">63.62</span></td>
</tr>
<tr id="A4.T6.3.1.7.5" class="ltx_tr">
<td id="A4.T6.3.1.7.5.1" class="ltx_td ltx_align_left" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.7.5.1.1" class="ltx_text" style="font-size:90%;color:#0FE3FF;">5</span></td>
<td id="A4.T6.3.1.7.5.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;">
<span id="A4.T6.3.1.7.5.2.1" class="ltx_text" style="font-size:90%;">PolyFormerÂ </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A4.T6.3.1.7.5.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib40" title="" class="ltx_ref">40</a><span id="A4.T6.3.1.7.5.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="A4.T6.3.1.7.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.7.5.3.1" class="ltx_text" style="font-size:90%;">CVPRâ€™23</span></td>
<td id="A4.T6.3.1.7.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.7.5.4.1" class="ltx_text" style="font-size:90%;">BERT-base</span></td>
<td id="A4.T6.3.1.7.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.7.5.5.1" class="ltx_text" style="font-size:90%;">Swin-B</span></td>
<td id="A4.T6.3.1.7.5.6" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.7.5.6.1" class="ltx_text" style="font-size:90%;">75.96</span></td>
<td id="A4.T6.3.1.7.5.7" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.7.5.7.1" class="ltx_text" style="font-size:90%;">77.09</span></td>
<td id="A4.T6.3.1.7.5.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.7.5.8.1" class="ltx_text" style="font-size:90%;">73.22</span></td>
<td id="A4.T6.3.1.7.5.9" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.7.5.9.1" class="ltx_text" style="font-size:90%;">70.65</span></td>
<td id="A4.T6.3.1.7.5.10" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.7.5.10.1" class="ltx_text" style="font-size:90%;">74.51</span></td>
<td id="A4.T6.3.1.7.5.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.7.5.11.1" class="ltx_text" style="font-size:90%;">64.64</span></td>
<td id="A4.T6.3.1.7.5.12" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.7.5.12.1" class="ltx_text" style="font-size:90%;">69.36</span></td>
<td id="A4.T6.3.1.7.5.13" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.7.5.13.1" class="ltx_text" style="font-size:90%;">69.88</span></td>
</tr>
<tr id="A4.T6.3.1.8.6" class="ltx_tr" style="background-color:#CCCCCC;">
<td id="A4.T6.3.1.8.6.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.8.6.1.1" class="ltx_text" style="font-size:90%;color:#0FE3FF;background-color:#CCCCCC;">6</span></td>
<td id="A4.T6.3.1.8.6.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.8.6.2.1" class="ltx_text" style="font-size:90%;background-color:#CCCCCC;">ContextDET</span></td>
<td id="A4.T6.3.1.8.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.8.6.3.1" class="ltx_text" style="font-size:90%;background-color:#CCCCCC;">-</span></td>
<td id="A4.T6.3.1.8.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.8.6.4.1" class="ltx_text" style="font-size:90%;background-color:#CCCCCC;">OPT-2.7B</span></td>
<td id="A4.T6.3.1.8.6.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.8.6.5.1" class="ltx_text" style="font-size:90%;background-color:#CCCCCC;">Swin-B</span></td>
<td id="A4.T6.3.1.8.6.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.8.6.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#CCCCCC;">76.40</span></td>
<td id="A4.T6.3.1.8.6.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.8.6.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#CCCCCC;">77.39</span></td>
<td id="A4.T6.3.1.8.6.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.8.6.8.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#CCCCCC;">74.16</span></td>
<td id="A4.T6.3.1.8.6.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.8.6.9.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#CCCCCC;">71.67</span></td>
<td id="A4.T6.3.1.8.6.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.8.6.10.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#CCCCCC;">75.14</span></td>
<td id="A4.T6.3.1.8.6.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.8.6.11.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#CCCCCC;">65.52</span></td>
<td id="A4.T6.3.1.8.6.12" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.8.6.12.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#CCCCCC;">69.89</span></td>
<td id="A4.T6.3.1.8.6.13" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="A4.T6.3.1.8.6.13.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#CCCCCC;">70.33</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="A4.SS1.p1" class="ltx_para">
<p id="A4.SS1.p1.3" class="ltx_p">Our ContextDET is not limited to object detection and can be extended to the image segmentation task, in which the goal is to assign a pixel-level label to each pixel in the input image.
To adapt our ContextDET framework for segmentation, we introduce an extra pixel-level segmentation head that takes the full visual tokens <math id="A4.SS1.p1.1.m1.1" class="ltx_Math" alttext="{\bm{c}}" display="inline"><semantics id="A4.SS1.p1.1.m1.1a"><mi id="A4.SS1.p1.1.m1.1.1" xref="A4.SS1.p1.1.m1.1.1.cmml">ğ’„</mi><annotation-xml encoding="MathML-Content" id="A4.SS1.p1.1.m1.1b"><ci id="A4.SS1.p1.1.m1.1.1.cmml" xref="A4.SS1.p1.1.m1.1.1">ğ’„</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.SS1.p1.1.m1.1c">{\bm{c}}</annotation></semantics></math> as inputs.
To train the segmentation model, we use a pixel-wise cross-entropy loss <math id="A4.SS1.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{L}_{\text{mask}}" display="inline"><semantics id="A4.SS1.p1.2.m2.1a"><msub id="A4.SS1.p1.2.m2.1.1" xref="A4.SS1.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A4.SS1.p1.2.m2.1.1.2" xref="A4.SS1.p1.2.m2.1.1.2.cmml">â„’</mi><mtext id="A4.SS1.p1.2.m2.1.1.3" xref="A4.SS1.p1.2.m2.1.1.3a.cmml">mask</mtext></msub><annotation-xml encoding="MathML-Content" id="A4.SS1.p1.2.m2.1b"><apply id="A4.SS1.p1.2.m2.1.1.cmml" xref="A4.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A4.SS1.p1.2.m2.1.1.1.cmml" xref="A4.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="A4.SS1.p1.2.m2.1.1.2.cmml" xref="A4.SS1.p1.2.m2.1.1.2">â„’</ci><ci id="A4.SS1.p1.2.m2.1.1.3a.cmml" xref="A4.SS1.p1.2.m2.1.1.3"><mtext mathsize="70%" id="A4.SS1.p1.2.m2.1.1.3.cmml" xref="A4.SS1.p1.2.m2.1.1.3">mask</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS1.p1.2.m2.1c">\mathcal{L}_{\text{mask}}</annotation></semantics></math> and Dice loss <math id="A4.SS1.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{L}_{\text{dice}}" display="inline"><semantics id="A4.SS1.p1.3.m3.1a"><msub id="A4.SS1.p1.3.m3.1.1" xref="A4.SS1.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A4.SS1.p1.3.m3.1.1.2" xref="A4.SS1.p1.3.m3.1.1.2.cmml">â„’</mi><mtext id="A4.SS1.p1.3.m3.1.1.3" xref="A4.SS1.p1.3.m3.1.1.3a.cmml">dice</mtext></msub><annotation-xml encoding="MathML-Content" id="A4.SS1.p1.3.m3.1b"><apply id="A4.SS1.p1.3.m3.1.1.cmml" xref="A4.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="A4.SS1.p1.3.m3.1.1.1.cmml" xref="A4.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="A4.SS1.p1.3.m3.1.1.2.cmml" xref="A4.SS1.p1.3.m3.1.1.2">â„’</ci><ci id="A4.SS1.p1.3.m3.1.1.3a.cmml" xref="A4.SS1.p1.3.m3.1.1.3"><mtext mathsize="70%" id="A4.SS1.p1.3.m3.1.1.3.cmml" xref="A4.SS1.p1.3.m3.1.1.3">dice</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS1.p1.3.m3.1c">\mathcal{L}_{\text{dice}}</annotation></semantics></math>, where ground-truth labels are pixel-level masks for matched objects in an image.</p>
</div>
<div id="A4.SS1.p2" class="ltx_para">
<p id="A4.SS1.p2.1" class="ltx_p">We choose the referring image segmentation task as a representative benchmark to evaluate the segmentation performance of ContextDET.
The referring image segmentation task aims to segment regions described by fine-grained input language query. Language queries will act as conditional inputs for the visual decoder in ContextDET.
We use three commonly-used datasets: RefCOCOÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>, RefCOCO+Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> and RefCOCOgÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>.
On RefCOCO and RefCOCO+, we follow the default training/validation/testA/testB data split in Yu <span id="A4.SS1.p2.1.1" class="ltx_text ltx_font_italic">et al</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>.
For RefCOCOg, we use the RefCOCO-umd splitsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>.
We report the mean Intersection over Union (mIoU), which is calculated by averaging the IoU scores across all test samples.</p>
</div>
<div id="A4.SS1.p3" class="ltx_para">
<p id="A4.SS1.p3.1" class="ltx_p">We compare ContextDET with some state-of-the-art methods in TableÂ <a href="#A4.T6" title="Table 6 â€£ D.1 Referring Image Segmentation â€£ Appendix D More Experiments â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. ContextDET achieves better results with mIoU gains of 0.63% and 0.45% on the validation/test splits over PolyFormerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.</p>
</div>
</section>
<section id="A4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.2 </span>More Qualitative Results</h3>

<figure id="A4.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2305.18279/assets/x10.png" id="A4.F9.g1" class="ltx_graphics ltx_figure_panel ltx_img_square" width="452" height="391" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>
<span id="A4.F9.2.1" class="ltx_text" style="font-size:90%;">Qualitative examples of the <span id="A4.F9.2.1.1" class="ltx_text ltx_font_bold">contextual cloze test</span>.
</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2305.18279/assets/x11.png" id="A4.F9.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="452" height="356" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>
<span id="A4.F9.4.1" class="ltx_text" style="font-size:90%;">Qualitative examples of the <span id="A4.F9.4.1.1" class="ltx_text ltx_font_bold">contextual captioning</span>.
</span></figcaption>
</figure>
<figure id="A4.F10" class="ltx_figure"><img src="/html/2305.18279/assets/x12.png" id="A4.F10.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="452" height="702" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>
<span id="A4.F10.2.1" class="ltx_text" style="font-size:90%;">Qualitative examples of the <span id="A4.F10.2.1.1" class="ltx_text ltx_font_bold">contextual QA</span>.
</span></figcaption>
</figure>
<div id="A4.SS2.p1" class="ltx_para">
<p id="A4.SS2.p1.1" class="ltx_p">We provide more qualitative results predicted by ContextDET in the contextual cloze test (FigureÂ <a href="#A4.F9" title="Figure 9 â€£ D.2 More Qualitative Results â€£ Appendix D More Experiments â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>), contextual captioning (FigureÂ <a href="#A4.F9" title="Figure 9 â€£ D.2 More Qualitative Results â€£ Appendix D More Experiments â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>), and contextual QA settingsÂ (FigureÂ <a href="#A4.F10" title="Figure 10 â€£ D.2 More Qualitative Results â€£ Appendix D More Experiments â€£ Contextual Object Detection with Multimodal Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>). The selected images are sourced randomly from the web and are not included in the training data. We observe that ContextDET effectively predicts contextual object words, including terms like â€˜teacherâ€™, â€˜studentâ€™, â€˜doctorâ€™, and â€˜nurseâ€™, along with their corresponding bounding boxes. In addition, we find some failure cases. For instance, the predicted object words may be incorrect, particularly for less common terms like â€˜earthâ€™.
Our ContextDET is less robust when it comes to occluded objects, such as â€˜sheepâ€™. We aim to address these limitations in future research.</p>
</div>
</section>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Broader Impact</h2>

<div id="A5.p1" class="ltx_para">
<p id="A5.p1.1" class="ltx_p">In this paper, we propose ContextDET that is capable of detecting objects within multimodal vision-language contexts. Our proposed ContextDET could facilitate the development of more real-world applications with human-AI interaction, such as AR/VR systems and robotics. However, relying on LLMs in our method may raise potential concerns when LLMs are not used properly. For example, LLMs could be used for harmful applications, such as plagiarism, spamming, and telemarketing fraud. Therefore, further research focused on detecting AI-generated textÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> is essential to address these issues and mitigate the negative impacts.</p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2305.18278" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2305.18279" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2305.18279">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2305.18279" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2305.18280" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 04:31:43 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
