<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2306.17829] Federated Ensemble YOLOv5 – A Better Generalized Object Detection Algorithm</title><meta property="og:description" content="Federated learning (FL) has gained significant traction as a privacy-preserving algorithm, but the underlying resemblances of federated learning algorithms like Federated averaging (FedAvg) or Federated SGD (Fed SGD) t…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated Ensemble YOLOv5 – A Better Generalized Object Detection Algorithm">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated Ensemble YOLOv5 – A Better Generalized Object Detection Algorithm">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2306.17829">

<!--Generated on Wed Feb 28 21:20:06 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Federated learning,  FedAvg,  Bagging,  Boosting,  Ensemble learning,  Object detection.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Federated Ensemble YOLOv5 – A Better Generalized Object Detection Algorithm
<br class="ltx_break">
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">1<sup id="id1.1.id1" class="ltx_sup">st</sup> Vinit Hegiste
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id2.2.id1" class="ltx_text ltx_font_italic">Chair of Machine Tools and Control Systems</span>
<br class="ltx_break"><span id="id3.3.id2" class="ltx_text ltx_font_italic">RPTU Kaiserslautern-Landau
<br class="ltx_break"></span>Kaiserslautern, Germany 
<br class="ltx_break">vinit.hegiste@rptu.de
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">2<sup id="id4.1.id1" class="ltx_sup">nd</sup> Tatjana Legler
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id5.2.id1" class="ltx_text ltx_font_italic">Chair of Machine Tools and Control Systems</span>
<br class="ltx_break"><span id="id6.3.id2" class="ltx_text ltx_font_italic">RPTU Kaiserslautern-Landau
<br class="ltx_break"></span>Kaiserslautern, Germany 
<br class="ltx_break">tatjana.legler@rptu.de
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">3<sup id="id7.1.id1" class="ltx_sup">rd</sup> Martin Ruskowski
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id8.2.id1" class="ltx_text ltx_align_center">Innovative Factory Systems (IFS)</span>
<br class="ltx_break">
<br class="ltx_break"><span id="id9.3.id2" class="ltx_text ltx_font_italic">German Research Center for Artificial Intelligence (DFKI)
<br class="ltx_break"></span>Kaiserslautern, Germany 
<br class="ltx_break">martin.ruskowski@dfki.de
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id10.id1" class="ltx_p">Federated learning (FL) has gained significant traction as a privacy-preserving algorithm, but the underlying resemblances of federated learning algorithms like Federated averaging (FedAvg) or Federated SGD (Fed SGD) to ensemble learning algorithms has not been fully explored. The purpose of this paper is to examine the application of FL to object detection as a method to enhance generalizability, and to compare its performance against a centralized training approach for an object detection algorithm.
Specifically, we investigate the performance of a YOLOv5 model trained using FL across multiple clients and employ a random sampling strategy without replacement, so each client holds a portion of the same dataset used for centralized training.
Our experimental results showcase the superior efficiency of the FL object detector’s global model in generating accurate bounding boxes for unseen objects, with the test set being a mixture of objects from two distinct clients not represented in the training dataset. These findings suggest that FL can be viewed from an ensemble algorithm perspective, akin to a synergistic blend of Bagging and Boosting techniques. As a result, FL can be seen not only as a method to enhance privacy, but also as a method to enhance the performance of a machine learning model.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Federated learning, FedAvg, Bagging, Boosting, Ensemble learning, Object detection.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In recent years, federated learning (FL) has emerged as a promising privacy-preserving algorithm that allows multiple clients to collaboratively train a global model without sharing their raw data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. The concept of FL involves aggregating local model updates from distributed clients to create a more robust and accurate global model. While FL has gained significant traction for its privacy benefits, its underlying resemblance to ensemble learning algorithms such as Bagging or Boosting remains unexplored.
The objective of this paper is to investigate the parallels between federated learning algorithms, such as Federated Averaging (FedAvg) and ensemble learning algorithms, and to explore the application of FL to object detection tasks.
The aggregation of weights to achieve a better global model can be compared with the Bagging algorithm, where multiple clients have data belonging to the same class/label, falling in the same feature space. And the process of sending the global model back to all clients to re-train as the starting weights is similar to the boosting algorithm, where the idea is to train weak classifiers to be able to perform better.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this paper, we take advantage of the collaborative nature of FL to enhance the overall detection capabilities of the model, therefore creating a better generalized and more stable model: federated ensemble learning YOLOv5 (FedEnsemble YOLOv5).
We examine the performance of the popular YOLOv5 (You Only Look Once version 5) algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> for object detection when trained using ensemble FL with multiple clients in comparison to the traditional centralized training approach.
To accomplish this, we propose a methodology where multiple clients participate in the federated training process, with each client possessing a subset of the same dataset used in centralized training. The data distribution among clients is achieved through random sampling without replacement. This approach allows us to assess the effectiveness of FL for object detection using the YOLOv5 model in an ensemble setting.
By leveraging the collective knowledge of diverse clients, we aim to demonstrate the potential of FL as an ensemble algorithm, exhibiting characteristics similar to Bagging and Boosting techniques.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Ensemble learning methods use different approaches to combine the outputs of many (often simpler) models into a single collective output that achieves better performance than any individual model alone <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Although the foundation for some of these methods can be traced back several decades <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, they are gaining renewed significance due to the importance of machine learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. The combination rule depends on the type of problem at hand, the model being used and may range from simple to complex: Calculating the mean value of every output (bagging) or taking the number of votes for a prediction class (voting) are the most intuitive methods, often resulting in already good results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. There are different types of boosting approaches, all of which aim to reinforce a feature of the outputs between the classifiers, such as giving more weight to previously misclassified samples <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
Stacking is usually a two-step process, that uses multiple base-level models to generate predictions, which are then used as input for a higher-level model to produce the final prediction
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Combining ensemble learning and federated learning can leverage the advantages of both methods, enabling efficient learning from decentralized datasets while benefiting from the predictive power of multiple learners.
There exist numerous approaches to combining FL and ensemble learning, each with different objectives:
FedBoost introduces boosting to FL for communication cost reduction, while assuming that pre-trained base predictors exist <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> utilizes knowledge distillation known from continual learning and further developed this into ensemble distillation of the clients, but requires labeled data to be available to the server.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> focuses on the improvement of performance of a non-IID image classification by following a stacking approach.
For object detection in our research, we adopted the YOLOv5 algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, which is an improved implementation of the YOLOv3 algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> using the PyTorch framework. The selection of YOLOv5 as our object detection algorithm aligns with the methodology employed in a previous study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, allowing for direct comparisons between the outcomes presented in both papers. This choice ensures consistency and facilitates a comprehensive analysis of the results obtained in our current research in relation to the earlier work.
Several others have also developed more advanced concepts, however, none have explored the fundamental analogies between ensemble and federated learning.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we elaborate on our approach to federated ensemble learning by structuring clients that operate as weak learners. We segment the principal dataset into a number of mutually exclusive subsets, each corresponding to a specific client.
This division of data, sets the stage for implementing FedAvg algorithm, which aims to empower these weak learners by enabling them to acquire distinctive features and characteristics by utilizing the global models. Interestingly, each round of communication serves to effectively boost the performance of these learners, echoing the mechanics of Boosting algorithms. This systematic approach culminates in the creation of a robust global model that surpasses its counterparts in terms of performance, displaying superior generalization capabilities in classifying and drawing bounding boxes over the target objects.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Inspired by the ensemble nature of federated learning, we designed an experiment revolving around the goal of achieving an effective object detection model. Our model leverages the YOLOv5 algorithm, utilizing a dataset identical to the one employed in centralized YOLOv5 training. In contrast to conventional FL, the centralized dataset is randomly divided into partitions equivalent to the number of clients. Each client then engages in training a federated global model, where active participation in every communication round must be ensured. This approach allows us to make the most of the inherent ensemble behavior of federated learning, crafting an innovative solution for object detection that builds upon the strengths of existing methodologies.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">The centralized dataset is divided into three subsets for training, validation, and testing purposes.
Specifically, 80% of the total dataset is allocated as the training dataset, while 10% each is reserved for validation and testing. When distributing the centralized dataset to multiple clients, the same images that were part of the training subset used for training the centralized YOLOv5 model are provided.
To simplify the process, we consider a fixed number of three clients and provide them with identical validation and test datasets that were originally utilized during the centralized training.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">This distribution strategy remains consistent for both the dataset discussed in Section <a href="#S3.SS2" title="III-B Datasets ‣ III Methodology ‣ Federated Ensemble YOLOv5 – A Better Generalized Object Detection Algorithm" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>.
In our preliminary experiments, we conducted a test using the federated ensemble (FedEnsemble) algorithm on a dataset comprising USB-Sticks, which was previously presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. The results indicated that the FedEnsemble technique yielded superior accuracy compared to a centrally trained USB quality classification model.
Encouraged by this outcome, we extended our work to include federated ensemble object detection, aiming to address practical applications beyond image classification.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2306.17829/assets/images/fedensemble_flowchart.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="165" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>FedEnsemble algorithm with example of cabin dataset</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Ensemble Algorithm</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In order to leverage the ensemble behavior of federated learning, we conducted experiments with a new algorithm to develop a federated ensemble object detection model based on YOLOv5. This algorithm utilizes the same dataset that was employed for the centralized training of YOLOv5. The dataset is divided into 3 clients and the distribution strategy is the same as mentioned in Section <a href="#S3" title="III Methodology ‣ Federated Ensemble YOLOv5 – A Better Generalized Object Detection Algorithm" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. The number of clients can be increased based on the size of the centralized training dataset.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The algorithm follows the steps outlined below:</p>
<div id="S3.SS1.p2.2" class="ltx_listing ltx_listing">
<div id="algx1.l1" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algx1.l1.1.1.1" class="ltx_text" style="font-size:80%;">1:</span></span>Shuffle the centralized data with YOLO annotations and divide it into ’n’ mutually exclusive datasets, where ’n’ represents the number of clients. The validation and test datasets remain unchanged.

</div>
<div id="algx1.l2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algx1.l2.1.1.1" class="ltx_text" style="font-size:80%;">2:</span></span>Utilize the same YOLOv5 model architecture (e.g., YOLOv5m or YOLOv5l) and consistent hyperparameters (e.g., optimizer, batch size, learning rate, local epochs) for all clients.

</div>
<div id="algx1.l3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algx1.l3.1.1.1" class="ltx_text" style="font-size:80%;">3:</span></span>Train each client individually using their respective local dataset for a specified number of epochs and save the weights of the last epoch.

</div>
<div id="algx1.l4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algx1.l4.1.1.1" class="ltx_text" style="font-size:80%;">4:</span></span>Send the weights of all clients to the server and perform federated averaging to create a global model. It is important to note that all clients actively participate in each communication round.

</div>
<div id="algx1.l5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algx1.l5.1.1.1" class="ltx_text" style="font-size:80%;">5:</span></span>Evaluate the accuracy of the global model on the test dataset.

</div>
<div id="algx1.l6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algx1.l6.1.1.1" class="ltx_text" style="font-size:80%;">6:</span></span>Terminate the process when the global model achieves the desired accuracy on the test dataset.

</div>
</div>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Figure <a href="#S3.F1" title="Figure 1 ‣ III Methodology ‣ Federated Ensemble YOLOv5 – A Better Generalized Object Detection Algorithm" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> depicts the process flow diagram of the above-mentioned algorithm.
The resulting global model is then compared to the normally trained YOLOv5 model, where identical hyperparameters are maintained. The outcomes of this comparison can be found in Section <a href="#S5" title="V Results ‣ Federated Ensemble YOLOv5 – A Better Generalized Object Detection Algorithm" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>.
To expedite the process, clients can be executed on parallel GPUs while running the server code on the same system. This configuration can significantly enhance the computational efficiency of the federated ensemble algorithm. Since the federated ensemble learning approach does not tackle privacy issues as the subset used for training by every client is a part of a centralized dataset; hence this parallelization strategy can be employed without any concerns in that regard.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Datasets</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In this paper, we experiment with two different custom datasets, which belong to our manufacturing setting, to analyze the robustness of the federated ensemble algorithm on multiple use-cases and scenarios. The first preliminary tests were done on the dataset of USB-Sticks, previously presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
The first dataset employed in our study is the truck cabin dataset, which encompasses two distinct design variations of miniature truck cabins in red and blue colors.
Figure <a href="#S3.F2" title="Figure 2 ‣ III-B Datasets ‣ III Methodology ‣ Federated Ensemble YOLOv5 – A Better Generalized Object Detection Algorithm" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates this dataset, which is identical to the one utilized in a previous study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. The centralized cabin dataset comprises a total of 1200 images, with 600 images categorized as ’Cabin_without_windshield’ and the remaining 600 images labeled as ’Cabin_with_windshield’.
The dataset comprises four different windshield designs. The training data encompasses blue cabins with blue windshields of type A and B, along with red cabins featuring red windshields of type C and D.
Following the distribution strategy outlined in the methodology section, this dataset is divided among three clients by shuffling the training subset originally used for training the centralized YOLOv5 model. Consequently, the clients do not possess an equal number of instances for each class, as they are randomly assigned.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2306.17829/assets/images/cabin_ensemble_dataset.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="287" height="288" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A small example of distribution of centralized cabin dataset into three clients</figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2306.17829/assets/images/trailer_dataset.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="186" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>A small example of distribution of centralized trailer dataset into three clients</figcaption>
</figure>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Training dataset distribution of trailer dataset (No. of images)</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Dataset</th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Training images</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">White trailer</th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Blue trailer</th>
<th id="S3.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Penholder trailer</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Centralized</th>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">720</td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">246</td>
<td id="S3.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">233</td>
<td id="S3.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">241</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<th id="S3.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Client1</th>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">241</td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78</td>
<td id="S3.T1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">79</td>
<td id="S3.T1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">84</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<th id="S3.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Client2</th>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">240</td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">95</td>
<td id="S3.T1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">60</td>
<td id="S3.T1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">85</td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<th id="S3.T1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Client3</th>
<td id="S3.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">239</td>
<td id="S3.T1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">73</td>
<td id="S3.T1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">94</td>
<td id="S3.T1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">72</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The trailer dataset used in this study comprises a total of 900 images. Each class in the dataset consists of 300 images, and there are three different backgrounds included. Additionally, the dataset includes some deliberately introduced blurry images to enhance the robustness of the trained model. Figure <a href="#S3.F3" title="Figure 3 ‣ III-B Datasets ‣ III Methodology ‣ Federated Ensemble YOLOv5 – A Better Generalized Object Detection Algorithm" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> showcases a small subset of the dataset, providing a glimpse into its content.
To facilitate the application of the federated ensemble algorithm, we partitioned the training dataset of the centralized trailer dataset into three disjoint subsets. This partitioning ensures that the validation and test datasets remain consistent across all experiments, enabling fair and accurate comparisons. It is important to note that during the capture of the trailer dataset, the images were obtained without including the chassis.
Therefore, when evaluating the model’s performance on trailers with chassis, interesting and informative results are observed.
For the purpose of data distribution analysis, the centralized training dataset comprises 80% of the total dataset, resulting in a selection of 720 images. The distribution of these images within each class is randomized. For a comprehensive understanding of the dataset’s detailed distribution, refer to Table <a href="#S3.T1" title="TABLE I ‣ III-B Datasets ‣ III Methodology ‣ Federated Ensemble YOLOv5 – A Better Generalized Object Detection Algorithm" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Implementation and Experiments</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we outline the implementation details of the FedEnsemble algorithm for training the federated YOLOv5 models. As mentioned in Section <a href="#S3" title="III Methodology ‣ Federated Ensemble YOLOv5 – A Better Generalized Object Detection Algorithm" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, we distribute a centralized dataset for YOLOv5 into multiple clients (in our case, three clients). Each client utilizes the YOLOv5m architecture with the same set of hyperparameters.
The training process begins by training each client’s model for 10 epochs on its local dataset. After completing the local training, the client’s weights are sent to a central server, where the FedAvg algorithm is applied to obtain the global weights. However, before sending the weights back to the clients, the global federated model is evaluated using the test dataset. This evaluation allows us to monitor the progress and determine if the model has achieved the desired metrics.
If the global model’s performance meets the desired target, the FedEnsemble learning process is stopped. However, if the metrics do not meet the target, the updated weights are sent back to each client, serving as the starting point for the subsequent round of training. It is important to note that in FedEnsemble learning, each client participates in every communication round.
To illustrate this process in a more visual manner, Figure <a href="#S3.F1" title="Figure 1 ‣ III Methodology ‣ Federated Ensemble YOLOv5 – A Better Generalized Object Detection Algorithm" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents a flowchart depicting the steps involved in the FedEnsemble algorithm.
For the cabin dataset, the centralized model was trained for 150 epochs, while for the trailer dataset, it was trained for 100 epochs (after several experimentation this epoch number was selected based on the best performing model). The FedEnsemble model for the cabin dataset yielded good results when trained with 15 local epochs and 5 communication rounds. On the other hand, the best results for the trailer dataset were achieved with 10 local epochs and 4 communication rounds. Once again, this local epochs and communication rounds number was a result of various experimentation. Also in Figure <a href="#S3.F1" title="Figure 1 ‣ III Methodology ‣ Federated Ensemble YOLOv5 – A Better Generalized Object Detection Algorithm" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the accuracy 96% was set due to the accuracy of YOLOv5 model achieved from normal training. These parameters can vary depending upon the requirement or custom use case.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">In the cabin dataset, both the centralized trained YOLOv5 model and the FedEnsemble model exhibit excellent performance on the validation and test datasets.
To facilitate comparison, we constructed test combinations consisting of cabins and windshields not present in the training dataset, such as blue cabins with blue windshields of type C and D, and red cabins with red windshields of type A and B (as previously utilized in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>).
Both models demonstrate high accuracy and accurately predict bounding boxes on this test dataset.
However, it is important to note that the test dataset was generated using a similar background and environmental setting as the training dataset, aiming to assess the models’ robustness in accurately classifying objects and drawing precise bounding boxes.
It is worth mentioning two key distinctions between the truck cabin and trailer datasets. Firstly, all cabin images in the truck cabin dataset feature cabins on top of a chassis, whereas the trailer dataset includes chassis images as well. Secondly, both datasets solely contain individual objects, and neither dataset includes fully assembled trucks comprising both cabins and trailers. Based on these considerations, we designed the following experiments for this study:</p>
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Conduct a comparison between the centralized trained YOLOv5 and FedEnsemble Cabin models using a mix of cabin combinations (with and without chassis) for live classification.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Evaluate the outputs of the centralized trained YOLOv5 and FedEnsemble Cabin models on cabins assembled with various trailer types (fully assembled trucks).</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Analyze the outputs of the centralized trained YOLOv5 and FedEnsemble Cabin models using images from the quality inspection module on the demonstrator.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p">Compare the performance of the centralized trained YOLOv5 and FedEnsemble Trailer models for trailers (with and without chassis) in live classification.</p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p">Examine the outputs of the centralized trained YOLOv5 and FedEnsemble Trailer models on cabins assembled with different trailer types (fully assembled trucks).</p>
</div>
</li>
<li id="S4.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="S4.I1.i6.p1" class="ltx_para">
<p id="S4.I1.i6.p1.1" class="ltx_p">Analyze the outputs of the centralized trained YOLOv5 and FedEnsemble Trailer models using images from the quality inspection module on the demonstrator.</p>
</div>
</li>
</ol>
<p id="S4.p2.2" class="ltx_p">The quality inspection module is an integral part of the demonstrator located at SmartFactory-Kaiserslautern (SF-KL), which showcases the future of manufacturing with production level 4 capabilities.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Results</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we present the results of the experiments conducted in this paper.
To facilitate better visualization, the model outputs are depicted with different color bounding boxes (BB). For the cabin models, the ’red color’ BB represents the ’Cabin_without_windshield’ class, while the ’pink color’ BB represents the ’Cabin_with_windshield’ class. Similarly, for the trailer model, the ’red color’ BB corresponds to the ’trailer_body_blue’ class, the ’pink color’ BB corresponds to the ’trailer_body_white’ class, and the ’orange color’ BB corresponds to the ’trailer_body_white_penholder’ class.
For experiments number 2 and 5, the test images remain consistent for the cabin and trailer combinations to enable better comparison <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The output images have been cropped and enlarged for improved visibility</span></span></span>.
Figures <a href="#S5.F4" title="Figure 4 ‣ V Results ‣ Federated Ensemble YOLOv5 – A Better Generalized Object Detection Algorithm" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and <a href="#S5.F5" title="Figure 5 ‣ V Results ‣ Federated Ensemble YOLOv5 – A Better Generalized Object Detection Algorithm" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> present the outcomes of experiment number 1. Both figures consist of two windows, wherein the left window displays the output from the centrally trained YOLOv5 model, while the right window showcases the output from the FedEnsemble model on a live video frame.
In Figure <a href="#S5.F4" title="Figure 4 ‣ V Results ‣ Federated Ensemble YOLOv5 – A Better Generalized Object Detection Algorithm" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the frames feature two cabins with a mixed combination (including a chassis), illustrating the effective classification and accurate bounding box generation by both models.
Moving on to Figure <a href="#S5.F5" title="Figure 5 ‣ V Results ‣ Federated Ensemble YOLOv5 – A Better Generalized Object Detection Algorithm" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, the results are noteworthy as the cabins lack a chassis, resulting in object orientations that differ from those in the training dataset.
While the centrally trained YOLOv5 model correctly classifies the objects, it produces imprecise bounding boxes that lead to partial cropping of the objects. In contrast, the FedEnsemble model exhibits exceptional performance by accurately classifying the objects with high confidence scores and generating precise bounding boxes around both objects in the frame.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2306.17829/assets/images/cabin_fed_og.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="118" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Comparison of models trained using normal YOLOv5 (left) vs Federated Ensemble YOLOv5 (right)when object has a similar orientation, but a different combination not present in the dataset (Cabin with different color type windshield with chassis)</figcaption>
</figure>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2306.17829/assets/images/cabin_fed_og2.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="117" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Comparison of models trained using normal YOLOv5 (left) vs FedEnsemble YOLOv5 (right) when object has a different combination not present in the dataset (Cabin with different color type windshield without chassis) </figcaption>
</figure>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">For experiment number 2, both cabins were assembled with different trailer types, and the results of this experiment can be seen in figure <a href="#S5.F6" title="Figure 6 ‣ V Results ‣ Federated Ensemble YOLOv5 – A Better Generalized Object Detection Algorithm" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and <a href="#S5.F7" title="Figure 7 ‣ V Results ‣ Federated Ensemble YOLOv5 – A Better Generalized Object Detection Algorithm" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. Figure <a href="#S5.F6" title="Figure 6 ‣ V Results ‣ Federated Ensemble YOLOv5 – A Better Generalized Object Detection Algorithm" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows output of YOLOv5 cabin model on various cabin and trailer combinations.
The model produces good results apart from the images with blue trailers. The model draws a BB over the blue trailer body and classifies it as ’Cabin_with_windshield’, leading to false positives.
Figure <a href="#S5.F7" title="Figure 7 ‣ V Results ‣ Federated Ensemble YOLOv5 – A Better Generalized Object Detection Algorithm" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows the output of FedEnsemble model on the same test images, and the model is able to predict precise BB without producing any false positives on trailer objects.</p>
</div>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2306.17829/assets/images/normal_cabin_trucktest.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="230" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Output of cabin model trained with Normal YOLOv5 algorithm on cabins assembled with different trailer types</figcaption>
</figure>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2306.17829/assets/images/fed_ens_cabin_trucktest.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="227" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Output of cabin model trained with Federated Ensemble YOLOv5 algorithm on cabins assembled with different trailer types</figcaption>
</figure>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">The results obtained from Experiment 3 are particularly intriguing due to the significant differences in image environment, background, and lighting conditions compared to the images present in the training dataset. Figure <a href="#S5.F8" title="Figure 8 ‣ V Results ‣ Federated Ensemble YOLOv5 – A Better Generalized Object Detection Algorithm" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> displays the output of the YOLOv5 cabin model on test images captured from the demonstrator. While the model performs well on cabin combinations similar to those in the training dataset, it struggles to detect the combination of blue cabin with red windshield and also produces false positives on trailer objects.
In contrast, the results of the FedEnsemble cabin model demonstrate significant improvements. The bounding boxes predicted by the model exhibit high accuracy, with no false positives on trailer objects. Notably, one test image showcases a remarkable outcome: despite the flashlight of the quality inspection module being off and the object being barely visible to the human eye at first glance, the FedEnsemble cabin model is able to correctly classify the object and draw a precise bounding box around it.</p>
</div>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2306.17829/assets/images/normal_cabin_demonstrator.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="122" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Output of model trained with Normal YOLOv5 algorithm on images from the Demonstrator (0: Cabin_without_windshield, 1: Cabin_with_windshield)</figcaption>
</figure>
<figure id="S5.F9" class="ltx_figure"><img src="/html/2306.17829/assets/images/fedensemble_cabin_demonstrator.png" id="S5.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="122" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Output of model trained with Federated Ensemble YOLOv5 algorithm on images from the Demonstrator (0: Cabin_without_windshield, 1: Cabin_with_windshield)</figcaption>
</figure>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">Experiments 4, 5, and 6 focus on the trailer dataset models, with an emphasis on maintaining consistency in the test dataset compared to the previous results. Figure <a href="#S5.F10" title="Figure 10 ‣ V Results ‣ Federated Ensemble YOLOv5 – A Better Generalized Object Detection Algorithm" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> and <a href="#S5.F11" title="Figure 11 ‣ V Results ‣ Federated Ensemble YOLOv5 – A Better Generalized Object Detection Algorithm" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> illustrate the output of the YOLOv5 and FedEnsemble trailer models, respectively.
Both figures exhibit different orientations of trailer bodies (with and without chassis) in the context of live classification.
It is observed that both models correctly classify and predict trailer objects without a chassis (image orientation similar to the training dataset).
However, as depicted in Figure <a href="#S5.F11" title="Figure 11 ‣ V Results ‣ Federated Ensemble YOLOv5 – A Better Generalized Object Detection Algorithm" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>, the YOLOv5 trailer model struggles to classify the blue trailer accurately and crops out a portion of the penholder trailer when predicting the bounding box. In contrast, the FedEnsemble model demonstrates superior performance by drawing precise bounding boxes around both objects in the frame, further highlighting its superiority over the normal centralized trained YOLOv5 model.</p>
</div>
<figure id="S5.F10" class="ltx_figure"><img src="/html/2306.17829/assets/images/trailer_fed_og5.png" id="S5.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="117" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Comparison of models trained using normal YOLOv5 (left) Federated Ensemble YOLOv5 (right) when the object (trailer) orientation is similar to the training dataset</figcaption>
</figure>
<figure id="S5.F11" class="ltx_figure"><img src="/html/2306.17829/assets/images/trailer_fed_og6.png" id="S5.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="240" height="98" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Comparison of models trained using normal YOLOv5 (left) Federated Ensemble YOLOv5 (right) when the object (trailer attached to a chassis) orientation is not similar to the training images</figcaption>
</figure>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">The test images used in Experiment 5 correspond to the same set as those employed in Experiment 2. Figure <a href="#S5.F12" title="Figure 12 ‣ V Results ‣ Federated Ensemble YOLOv5 – A Better Generalized Object Detection Algorithm" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> showcases the output of the YOLOv5 trailer model on these images, revealing its poor performance. For instances featuring a blue trailer with a blue cabin combination, the model erroneously predicts the entire truck as ’trailer_body_blue’. Furthermore, the predicted trailer bounding box encompasses the chassis and exhibits a significant number of false positives across various images. In stark contrast, the results obtained from the FedEnsemble trailer model are truly remarkable. The FedEnsemble model avoids generating any false positives and accurately predicts and draws precise bounding boxes exclusively around the trailer object, effectively excluding the chassis from the prediction, as shown in Figure <a href="#S5.F13" title="Figure 13 ‣ V Results ‣ Federated Ensemble YOLOv5 – A Better Generalized Object Detection Algorithm" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>.</p>
</div>
<figure id="S5.F12" class="ltx_figure"><img src="/html/2306.17829/assets/images/normal_trailer_trucktest.png" id="S5.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="229" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Output of trailer model trained with Normal YOLOv5 algorithm on cabins assembled with different trailer types</figcaption>
</figure>
<figure id="S5.F13" class="ltx_figure"><img src="/html/2306.17829/assets/images/fed_ens_trailer_trucktest.png" id="S5.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="231" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Output of trailer model trained with Federated Ensemble YOLOv5 algorithm on cabins assembled with different trailer types (0: Trailer_blue, 1: Trailer_white, 2: Trailer_white_penholder)</figcaption>
</figure>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.1" class="ltx_p">In the final experiment, the trailer models are subjected to testing using images from the demonstrator. Similar to Experiment 3, these test images exhibit distinct backgrounds and lighting conditions compared to the trailer’s training dataset. Once again, the performance of the YOLOv5 trailer model on these images is unsatisfactory. The model fails to predict the objects accurately, whereas only identifying the white trailer, that too with an imprecise bounding box. Additionally, the model produces a false positive on an image without any objects present.
In contrast, the FedEnsemble YOLOv5 trailer model excels in this challenging scenario. It not only correctly classifies the objects but also draws precise bounding boxes around them, even in an unfamiliar environment characterized by different lighting conditions that may affect the color appearance of the objects. The FedEnsemble model’s ability to generalize well and maintain accurate predictions in such conditions demonstrates its superior performance compared to the standard YOLOv5 model.</p>
</div>
<figure id="S5.F14" class="ltx_figure"><img src="/html/2306.17829/assets/images/normal_trailer_demonstrator.png" id="S5.F14.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="125" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Output of model trained with Normal YOLOv5 algorithm on images from the Demonstrator (0: Trailer_blue, 1: Trailer_white, 2: Trailer_white_penholder)</figcaption>
</figure>
<figure id="S5.F15" class="ltx_figure"><img src="/html/2306.17829/assets/images/fedensemble_trailer_demonstrator.png" id="S5.F15.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="122" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Output of model trained with Federated Ensemble YOLOv5 algorithm on images from the Demonstrator (0: Trailer_blue, 1: Trailer_white, 2: Trailer_white_penholder)</figcaption>
</figure>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Discussion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this section, we discuss the implications and significance of the results obtained from the experiments conducted in this study.
The experimental results clearly demonstrate the superiority of the federated ensemble approach over the centralized training approach in the context of object detection using the YOLOv5 algorithm. The federated ensemble YOLOv5 model consistently outperformed the centralized YOLOv5 model across various scenarios for both cabin and trailer datasets.
One key advantage of the federated ensemble approach is its ability to effectively leverage the collective knowledge of multiple decentralized models. By aggregating the predictions from individual models trained on local data, the federated ensemble model achieves higher accuracy, better bounding box predictions, and reduced false positives compared to the centralized trained model.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">The experiments involving different combinations of cabins and windshields, as well as trailers with and without chassis, showcased the robustness of the federated ensemble model. It successfully classified and accurately predicted objects, even when presented with combinations not present in the training dataset. This indicates the model’s ability to generalize well and handle variations in object appearance and orientation.
Moreover, the tests conducted on images from the demonstrator, which featured different background settings and lighting conditions, further demonstrated the effectiveness of the federated ensemble approach. The model consistently achieved accurate classifications and precise bounding box predictions, even in challenging and unfamiliar environments. This highlights the Federated ensemble algorithm’s adaptability and potential for real-world applications.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS1.4.1.1" class="ltx_text">VI-A</span> </span><span id="S6.SS1.5.2" class="ltx_text ltx_font_italic">Potential for Generalization and Future Research</span>
</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">The positive outcomes obtained from this research indicate the potential applicability of the federated ensemble approach beyond the YOLOv5 algorithm and object detection tasks. The concept of federated learning, combining the strengths of ensemble methods and preserving data privacy, can be extended to other architectural models and various use cases, including image classification, image segmentation, and small object detection.
Furthermore, the potential of applying federated learning to non-vision tasks, such as spam detection and anomaly detection, warrants exploration. By decentralizing the training process and aggregating models’ predictions, federated learning may provide improved performance and data privacy in these domains as well.
Validation of these results through future experiments on public datasets would further support the effectiveness of the federated ensemble approach and solidify its potential for practical implementation.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Following the realization, that federated learning acts as a combination of Bagging and Boosting algorithms.
This was empirically tested using a YOLOv5 algorithm, specifically tailored to our research context. The comparative results highlighted a superior performance of the federated ensemble YOLOv5 algorithm, in contrast to the centralized YOLOv5 model, particularly within the context of a custom dataset applied to manufacturing scenarios.
This approach is not limited to YOLOv5, but can be applied to other object detection algorithms as well. Future research will explore the potential of applying this federated ensemble approach to diverse architectural models and use-cases, including but not limited to, image classification, image segmentation, and small object detection. Moreover, we also aim to extend its applicability to non-vision use cases, such as spam detection, anomaly detection, and more. There is potential for the validation of these results through future application on public datasets, which would further substantiate our findings.
In conclusion, our research demonstrates that, in addition to the well-known advantages of preserving data sovereignty, FL also holds potential advantages in situations where access to a substantial portion of the data set is possible.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y. Arcas,
“Communication-efficient learning of deep networks from decentralized
data,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 20th International Conference on
Artificial Intelligence and Statistics</em>, ser. Proceedings of Machine Learning
Research, A. Singh and J. Zhu, Eds., vol. 54.   Fort Lauderdale, FL, USA: PMLR, 2017, pp. 1273–1282. [Online].
Available: http://proceedings.mlr.press/v54/mcmahan17a.html

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
G. J. et al., “ultralytics/yolov5: v3.1 - Bug Fixes and Performance
Improvements,” Oct. 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Z.-H. Zhou, <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Ensemble methods: Foundations and algorithms</em>, ser. Machine
Learning &amp; Pattern Recognition Series.   Boca Raton, Fla.: CRC Press Taylor &amp; Francis, 2012.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
H. Drucker, C. Cortes, L. D. Jackel, Y. LeCun, and V. Vapnik, “Boosting and
other ensemble methods,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Neural computation</em>, vol. 6, no. 6, pp.
1289–1301, 1994.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
R. E. Schapire, “The strength of weak learnability,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Machine Learning</em>,
vol. 5, no. 2, pp. 197–227, 1990.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
X. Dong, Z. Yu, W. Cao, Y. Shi, and Q. Ma, “A survey on ensemble learning,”
<em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Frontiers of Computer Science</em>, vol. 14, no. 2, pp. 241–258, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
S. Wan and H. Yang, “Comparison among methods of ensemble learning,” in
<em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Biometrics and Security Technologies (ISBAST), 2013 International
Symposium on</em>.   IEEE, 2013, pp.
286–290.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
V. Svetnik, T. Wang, C. Tong, A. Liaw, R. P. Sheridan, and Q. Song, “Boosting:
an ensemble learning tool for compound classification and qsar modeling,”
<em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Journal of chemical information and modeling</em>, vol. 45, no. 3, pp.
786–799, 2005.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
D. H. Wolpert, “Stacked generalization,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Neural Networks</em>, vol. 5,
no. 2, pp. 241–259, 1992.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J. Hamer, M. Mohri, and A. T. Suresh, “Fedboost: A communication-efficient
algorithm for federated learning,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 37th
International Conference on Machine Learning</em>, ser. Proceedings of Machine
Learning Research, H. D. III and A. Singh, Eds., vol. 119.   PMLR, 2020, pp. 3973–3983.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
S. Lin, G. Yang, and J. Zhang, “A collaborative learning framework via
federated meta-learning,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">2020 IEEE 40th International Conference
on Distributed Computing Systems</em>.   Piscataway, NJ: IEEE, 2020, pp. 289–299.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
H. Yu, C. Wu, H. Yu, X. Wei, S. Liu, and Y. Zhang, “A federated learning
algorithm using parallel-ensemble method on non-iid datasets,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Complex
&amp; Intelligent Systems</em>, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:
Unified, real-time object detection,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2016.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
V. Hegiste, T. Legler, K. Fridman, and M. Ruskowski, “Federated object
detection for quality inspection in shared production,” 2023, unpublished,
pre-print ArXiv.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
V. Hegiste, T. Legler, and M. Ruskowski, “Application of federated machine
learning in manufacturing,” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">2022 International Conference on
Industry 4.0 Technology (I4Tech)</em>.   IEEE, 2022, pp. 1–8.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2306.17828" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2306.17829" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2306.17829">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2306.17829" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2306.17830" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 21:20:06 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
