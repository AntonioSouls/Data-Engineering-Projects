<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2311.00068] View Classification and Object Detection in Cardiac Ultrasound to Localize Valves via Deep Learning</title><meta property="og:description" content="Echocardiography provides an important tool for clinicians to observe the function of the heart in real time, at low cost, and without harmful radiation. Automated localization and classification of heart valves enable…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="View Classification and Object Detection in Cardiac Ultrasound to Localize Valves via Deep Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="View Classification and Object Detection in Cardiac Ultrasound to Localize Valves via Deep Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2311.00068">

<!--Generated on Tue Feb 27 20:49:22 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">View Classification and Object Detection in Cardiac Ultrasound to Localize Valves via Deep Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Derya Gol Gungor, Bimba Rao, Cynthia Wolverton, Ismayil Guracar
<br class="ltx_break">Siemens Healthineers
<br class="ltx_break">685 E Middlefield Rd,
Mountain View, CA 94043
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{derya.gol, bimba.rao, cynthia.wolverton, ismayil.guracar}@siemens-healthineers.com</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Echocardiography provides an important tool for clinicians to observe the function of the heart in real time, at low cost, and without harmful radiation. Automated localization and classification of heart valves enables automatic extraction of quantities associated with heart mechanical function and related blood flow measurements. We propose a machine learning pipeline that uses deep neural networks for separate classification and localization steps. As the first step in the pipeline, we apply view classification to echocardiograms with ten unique anatomic views of the heart. In the second step, we apply deep learning-based object detection to both localize and identify the valves. Image segmentation based object detection in echocardiography has been shown in many earlier studies but, to the best of our knowledge, this is the first study that predicts the bounding boxes around the valves along with classification from 2D ultrasound images with the help of deep neural networks. Our object detection experiments applied to the Apical views suggest that it is possible to localize and identify multiple valves precisely.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Echocardiography (diagnostic cardiac ultrasound imaging) is routinely used to visualize the chambers and valves of the heart. Typically, it is combined with Doppler ultrasound to evaluate blood flow through valves and within chambers. High frequency sound waves are transmitted into the body, and the received echoes from tissue are processed to produce both 2D images, and blood flow velocity estimates.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">This study focuses on heart valves which are critical components of the heart, namely the valves that control circulation between the chambers and aorta: Tricuspid Valve (TV), Mitral Valve (MV) and Aortic Valve (AV). TV is located in the right side of the heart, and MV is in the left side. These valves are called atrioventricular valves, since they connect the atria to the ventricles. In an ideal heart, blood flows through the both valves during diastole with contraction of the corresponding atrium and both close during systole with contraction of the corresponding ventricle to prevent regurgitation of blood from the ventricle to the atria. On the other hand, the AV is responsible for controlling the blood circulation between the left ventricle and aorta, which is the main artery supplying oxygenated blood to the circulatory system.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In the case of a heart with pathology, the blood may flow backwards through the valve if the valve does not close completely (regurgitation or insufficiency of the valve). MV and AV regurgitation affect more than 200.000 people per year in United States.
Another significant valve abnormality is stenosis where the valve flaps become stiff resulting in narrowed valve openings and reduced blood flow. Similarly, Tricuspid atresia may limit blood flow because the valve is not formed properly, and a solid sheet of tissue blocks the passage between the chambers. Any significant valve insufficiency and abnormality can affect the quality of daily life, and may require significant treatment procedures. Untreated pathologies can result in enlargement of heart, heart rhythm problems (arrhythmia), heart failure or even death.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Classification and localization of anatomy are key enabling technologies that open up doors to many solutions for Ultrasound techs as well as clinicians. Training and placement guidance for new and/or inexperienced users is an application that would tremendously benefit from these technologies. Successful localization and identification of valves would allow automatic highlighting and enhancing these organs in the image to make accurate measurements, guide procedures, place devices, etc.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Related Work</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">Deep neural networks are beginning to assist Echocardiagram image analysis. Automatic view classification is a popular application area since it is a basis for many other applications. Machine learning and image processing techniques for ultrasound image classification have been explored by many papers. Here, we focus only on those papers using deep neural networks.
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, two convolutional neural networks (CNNs) are combined to classify eight different views, namely Apical 2, 3, 4, 5 chamber, parasternal long axis (PLAX), parasternal short axis at aortic valve (PSAX-AoV), PSAX of papillary (PSAX-LV) and PSAX at mitral valve (PSAX-MV). In addition to brightness mode (B-mode) images, the temporal acceleration images are processed by a different network and the results are fused to obtain a final decision. This combination provided an average of 92% accuracy for all views, with the lowest accuracy (71.4%) in Apical 5 (mostly mixed with Apical 3 class) . Another study on view classification was done by  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. In addition to the classes listed above, they included PLAX RV-inflow, subcostal four-chamber, subcostal inferior vena-cava, subcostal aorta, suprasternal aorta, pulse-wave Doppler (PW), continuous-wave Doppler (CW) and motion mode (M-mode). Using the VGG-16 network<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, they achieved 97.8% overall accuracy in 15 different views with the accuracy for PLAX RV-inflow, specifically 86% in video and 72% on still images.This exceeds the prediction accuracy of a board-certified echo cardiographer. A more extended classification study was performed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, which also included subclasses of certain views. For example, in addition to the typical Apical 2 Chamber (A2C) view, the study included A2C plus occluded left atrium, and A2C plus occluded left ventricle. Due to the high correlations between classes, their average accuracy was 84% on 23 views. However, if the results are considered in terms of broad classes such as PLAX, the accuracy they achieved was around 96%. The classification network used was the 15 layer VGG network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. In addition to the view classification, they used deep networks also for image segmentation of cardiac chambers and also disease classification. However, image segmentation and disease detection are outside the scope of this paper.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">In the field of machine learning, object detection refers to the obtaining of bounding box coordinates for sub-images to identify and localize multiple objects in a single image. This differs from image segmentation which performs pixel-wise classification to identify regions in an image. A detailed review of deep learning based object detection methods is given in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. From a high level perspective, there are two main approaches to object detection: region proposal based and regression/classification based. The first stage of the well-known region-based CNN technique (R-CNN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> is a region proposal generation technique. As an efficient alternative to exhaustive search, R-CNN uses a selective search algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> which iteratively merges small regions by hierarchical grouping according to their color spaces and similarity metrics. Then these regions are fed into a CNN for feature extraction. The features are fed into multiple SVM classifiers to provide class probabilities and also to a linear regressor to optimize the bounding box coordinates. Running multiple CNNs for each region proposal is computationally expensive. Therefore in Fast R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, the order of image processing components is altered such that the feature extraction CNN is executed first, followed by the region proposal network (RPN). The implication is that the CNN runs only one time over the entire input image to generate a feature vector. The output of CNN is connected to two fully-connected (FC) layers: one to produce bounding box coordinates (as regressor) and the other to produce object-ness probabilities (as classifier). Also the maximum number of regions is fixed a priori. In Faster RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, a pre-trained region proposal network (RPN) is used to avoid the expensive selective search algorithm. Faster RCNN provides a more accurate and efficient architecture. However, some region-specific components still need to be applied hundreds of times per region proposal <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. This is handled by R-FCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, where region crops are calculated on the final layer of the network. This provides a significant speed-up (2.5-20 times <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> in tests with respect to Faster R-CNN). On the other hand, regression/classification based techniques such as Single-shot Detector (SSD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and YOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> predict the bounding boxes and class probabilities all at once instead of a two step mechanism and thus may be more suitable for real-time applications.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p id="S1.SS1.p3.1" class="ltx_p">Several algebraic, signal processing and machine learning techniques have been proposed for tracking cardiac valves. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, a non-negative matrix approximation approach has been used to detect and track the mitral valve in Apical 4 chamber views. This algorithmic approach does not require any labeling of the data but is limited to detecting the mitral valve only. Reference <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> uses a machine learning approach for real-time tracking of mitral valve in 3D images for inteventional guidance. Their technique relies on the box estimator, based on <span id="S1.SS1.p3.1.1" class="ltx_text ltx_font_italic">marginal space learning (MSL) approach</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> that predicts the presence of the MV location, orientation and scale in 3D images. MSL is three-step detector that applies probabilistic boosted-tree based classifiers multiple times to estimate different parameters. None of these studies uses deep neural networks.</p>
</div>
<div id="S1.SS1.p4" class="ltx_para">
<p id="S1.SS1.p4.1" class="ltx_p">Convolutional networks in conjunction with object detection are commonly used in medical imaging for localization and segmentation of anatomical structures and organs. For example, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> uses a specialized convolutional network called BoBNet (a variation of VGG network<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>) to predict bounding boxes around organs such as liver, heart, aorta applied to 3D computerized tomography images (CT) images. On the other hand, there is a lot of effort in fetal ultrasound imaging to identify the imaging plane and detect the structures being imaged. For example, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, CNN based techniques are proposed to automatically localize a fetal heart. Another CNN based technique <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> is able to detect fetal standard planes and localize structures such as brain, spine, kidneys, lips, femur, etc. from 2D ultrasound images.</p>
</div>
<div id="S1.SS1.p5" class="ltx_para">
<p id="S1.SS1.p5.1" class="ltx_p">In this paper, we illustrate how deep neural networks developed for the object detection problem perform on cardiac ultrasound images to detect the valves in different cardiac views. In the first section, we explain the image preprocessing and view classification applied as the initial step. Then, in the second section, we concentrate on the annotated data specific to object detection and mention the selected network for training. In the final section, we show the results of our experiments.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Materials and Methods</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>View Classification</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">View Classification of selected echocardiogram views is the first stage in our proposed machine learning pipeline. Views classified are as follows: Apical 2, Apical 3, Apical 4, Apical 5, Parasternal-long-axis (PLAX), PLAX-RVinflow (PLAX-RVIF), PLAX-RVoutflow (PLAX-RVOT), Parasternal-short-axis (PSAX)<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>PSAX view can be from Left Ventricle level or Mitral Valve. We observed that those two views look very similar and object detection to isolate the valve would produce similar results, thus we merged these two sub-classes.</span></span></span>, PSAX at the aortic valve level (PSAX-AoV), Subcostal of four-chamber, and Noise. The noise images for training are created by capturing images where the ultrasound probe is in contact with air, or in contact with ultrasound coupling gel only. An example image for each cardiac image class is shown in Fig. <a href="#S2.F1" title="Figure 1 ‣ 2.1 View Classification ‣ 2 Materials and Methods ‣ View Classification and Object Detection in Cardiac Ultrasound to Localize Valves via Deep Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2311.00068/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="263" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.3.2" class="ltx_text" style="font-size:90%;">Example images for each class used in view classification</span></figcaption>
</figure>
<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Annotateted Data</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">The complete dataset includes 11,150 B-mode clips of 1-5 heartbeats from Acuson SC2000, Siemens Cardiac Ultrasound system (Mountain View, CA, USA). All the clips are anonymized to remove any patient specific information. To ensure that the view classification testing is valid, care was taken to ensure that B-Mode clips from the same patient do not appear in both the training and the test data. Because the data is anonymous, we have used acquisition date and time that was contained in the DICOM header files. We numbered the clips with a separate patient ID if there is 30 minutes gap between two closest acquisitions. This method may result in identifying two separate patients as the same person (if two studies were done back-to-back). However, it has low probability of splitting the images from a single patient into two parts. With this method, we identified the total number of subjects to be at least 525. We needed this separation also to make sure that we do not include same patient’s data in both training and validation/test to eliminate the bias. We partitioned 60% of patients for training, 20% for validation and 20% for testing.</p>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Preprocessing</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">The images were pre-processed in preparation for training. First, we divided the clips into frames (frame rate was 50-70 frames per second) and randomly selected 10 frames per heartbeat up to a maximum of 30 frames per clip. This allowed us to have 126,731 images for training, 38,148 for validation and 35,932 for testing. In typical ultrasound images, anatomical structures are shown in a polar coordinate system within a trapezoid-shaped area as shown in Fig. <a href="#S2.F2.sf1" title="In Figure 2 ‣ 2.1.2 Preprocessing ‣ 2.1 View Classification ‣ 2 Materials and Methods ‣ View Classification and Object Detection in Cardiac Ultrasound to Localize Valves via Deep Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(a)</span></a>. There also exists some text related to system, acquisition or patient related information on the left and right side of rectangular images. In order to provide only necessary information to our network, the data is converted from the display grid (the trapezoid shape) to a Cartesian grid in the first pre-processing step. To do this, the image scan depth and the trapezoid angles as shown in Fig. <a href="#S2.F2.sf1" title="In Figure 2 ‣ 2.1.2 Preprocessing ‣ 2.1 View Classification ‣ 2 Materials and Methods ‣ View Classification and Object Detection in Cardiac Ultrasound to Localize Valves via Deep Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(a)</span></a> are automatically identified in MATLAB®(Natick, MA, USA). The red dots illustrate the extracted corners of the image and the magenta regions show the area scanned to find the angle of the trapezoid. Then the image within the trapezoid is transformed into Cartesian coordinate system via linear interpolation to give us a converted image as shown in Fig. <a href="#S2.F2.sf2" title="In Figure 2 ‣ 2.1.2 Preprocessing ‣ 2.1 View Classification ‣ 2 Materials and Methods ‣ View Classification and Object Detection in Cardiac Ultrasound to Localize Valves via Deep Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a>. The images were resized to <math id="S2.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="S2.SS1.SSS2.p1.1.m1.1a"><mrow id="S2.SS1.SSS2.p1.1.m1.1.1" xref="S2.SS1.SSS2.p1.1.m1.1.1.cmml"><mn id="S2.SS1.SSS2.p1.1.m1.1.1.2" xref="S2.SS1.SSS2.p1.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS2.p1.1.m1.1.1.1" xref="S2.SS1.SSS2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS2.p1.1.m1.1.1.3" xref="S2.SS1.SSS2.p1.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.1.m1.1b"><apply id="S2.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1"><times id="S2.SS1.SSS2.p1.1.m1.1.1.1.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S2.SS1.SSS2.p1.1.m1.1.1.2.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1.2">256</cn><cn type="integer" id="S2.SS1.SSS2.p1.1.m1.1.1.3.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.1.m1.1c">256\times 256</annotation></semantics></math>. The mean calculated from the training set images is extracted from each image. The grayscale images (maximum value of 255) were normalized to have values between 0-1 before feeding into the network.</p>
</div>
<figure id="S2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.00068/assets/scan_converted.png" id="S2.F2.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="169" height="120" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S2.F2.sf1.3.2" class="ltx_text" style="font-size:90%;">Before</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.00068/assets/acoustic_converted.png" id="S2.F2.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="120" height="120" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S2.F2.sf2.3.2" class="ltx_text" style="font-size:90%;">After</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.3.2" class="ltx_text" style="font-size:90%;">An example input image (a) before and (b) after pre-processing</span></figcaption>
</figure>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2311.00068/assets/Inceptionv3_graph.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="259" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.3.2" class="ltx_text" style="font-size:90%;">InceptionV3 detailed diagram <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note"><span id="footnote3.1.1.1" class="ltx_text" style="font-size:111%;">3</span></span><a target="_blank" href="https://cloud.google.com/tpu/docs/inception-v3-advanced" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:111%;">https://cloud.google.com/tpu/docs/inception-v3-advanced</a></span></span></span></span></figcaption>
</figure>
</section>
<section id="S2.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3 </span>Network</h4>

<div id="S2.SS1.SSS3.p1" class="ltx_para">
<p id="S2.SS1.SSS3.p1.11" class="ltx_p">We have adopted the <span id="S2.SS1.SSS3.p1.11.1" class="ltx_text ltx_font_bold">InceptionV3</span> network as described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. The structure of InceptionV3 is shown in Figure <a href="#footnote3" title="footnote 3 ‣ Figure 3 ‣ 2.1.2 Preprocessing ‣ 2.1 View Classification ‣ 2 Materials and Methods ‣ View Classification and Object Detection in Cardiac Ultrasound to Localize Valves via Deep Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text" style="font-size:111%;">3</span></span></a>. Note that each convolutional layer follows batch normalization and activation units. Naive Inception modules include combinations of <math id="S2.SS1.SSS3.p1.1.m1.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S2.SS1.SSS3.p1.1.m1.1a"><mrow id="S2.SS1.SSS3.p1.1.m1.1.1" xref="S2.SS1.SSS3.p1.1.m1.1.1.cmml"><mn id="S2.SS1.SSS3.p1.1.m1.1.1.2" xref="S2.SS1.SSS3.p1.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS3.p1.1.m1.1.1.1" xref="S2.SS1.SSS3.p1.1.m1.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS3.p1.1.m1.1.1.3" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p1.1.m1.1b"><apply id="S2.SS1.SSS3.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS3.p1.1.m1.1.1"><times id="S2.SS1.SSS3.p1.1.m1.1.1.1.cmml" xref="S2.SS1.SSS3.p1.1.m1.1.1.1"></times><cn type="integer" id="S2.SS1.SSS3.p1.1.m1.1.1.2.cmml" xref="S2.SS1.SSS3.p1.1.m1.1.1.2">1</cn><cn type="integer" id="S2.SS1.SSS3.p1.1.m1.1.1.3.cmml" xref="S2.SS1.SSS3.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p1.1.m1.1c">1\times 1</annotation></semantics></math>, <math id="S2.SS1.SSS3.p1.2.m2.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S2.SS1.SSS3.p1.2.m2.1a"><mrow id="S2.SS1.SSS3.p1.2.m2.1.1" xref="S2.SS1.SSS3.p1.2.m2.1.1.cmml"><mn id="S2.SS1.SSS3.p1.2.m2.1.1.2" xref="S2.SS1.SSS3.p1.2.m2.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS3.p1.2.m2.1.1.1" xref="S2.SS1.SSS3.p1.2.m2.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS3.p1.2.m2.1.1.3" xref="S2.SS1.SSS3.p1.2.m2.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p1.2.m2.1b"><apply id="S2.SS1.SSS3.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS3.p1.2.m2.1.1"><times id="S2.SS1.SSS3.p1.2.m2.1.1.1.cmml" xref="S2.SS1.SSS3.p1.2.m2.1.1.1"></times><cn type="integer" id="S2.SS1.SSS3.p1.2.m2.1.1.2.cmml" xref="S2.SS1.SSS3.p1.2.m2.1.1.2">3</cn><cn type="integer" id="S2.SS1.SSS3.p1.2.m2.1.1.3.cmml" xref="S2.SS1.SSS3.p1.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p1.2.m2.1c">3\times 3</annotation></semantics></math>, <math id="S2.SS1.SSS3.p1.3.m3.1" class="ltx_Math" alttext="5\times 5" display="inline"><semantics id="S2.SS1.SSS3.p1.3.m3.1a"><mrow id="S2.SS1.SSS3.p1.3.m3.1.1" xref="S2.SS1.SSS3.p1.3.m3.1.1.cmml"><mn id="S2.SS1.SSS3.p1.3.m3.1.1.2" xref="S2.SS1.SSS3.p1.3.m3.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS3.p1.3.m3.1.1.1" xref="S2.SS1.SSS3.p1.3.m3.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS3.p1.3.m3.1.1.3" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p1.3.m3.1b"><apply id="S2.SS1.SSS3.p1.3.m3.1.1.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1"><times id="S2.SS1.SSS3.p1.3.m3.1.1.1.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.1"></times><cn type="integer" id="S2.SS1.SSS3.p1.3.m3.1.1.2.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.2">5</cn><cn type="integer" id="S2.SS1.SSS3.p1.3.m3.1.1.3.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p1.3.m3.1c">5\times 5</annotation></semantics></math> convolutional layers plus a <math id="S2.SS1.SSS3.p1.4.m4.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S2.SS1.SSS3.p1.4.m4.1a"><mrow id="S2.SS1.SSS3.p1.4.m4.1.1" xref="S2.SS1.SSS3.p1.4.m4.1.1.cmml"><mn id="S2.SS1.SSS3.p1.4.m4.1.1.2" xref="S2.SS1.SSS3.p1.4.m4.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS3.p1.4.m4.1.1.1" xref="S2.SS1.SSS3.p1.4.m4.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS3.p1.4.m4.1.1.3" xref="S2.SS1.SSS3.p1.4.m4.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p1.4.m4.1b"><apply id="S2.SS1.SSS3.p1.4.m4.1.1.cmml" xref="S2.SS1.SSS3.p1.4.m4.1.1"><times id="S2.SS1.SSS3.p1.4.m4.1.1.1.cmml" xref="S2.SS1.SSS3.p1.4.m4.1.1.1"></times><cn type="integer" id="S2.SS1.SSS3.p1.4.m4.1.1.2.cmml" xref="S2.SS1.SSS3.p1.4.m4.1.1.2">3</cn><cn type="integer" id="S2.SS1.SSS3.p1.4.m4.1.1.3.cmml" xref="S2.SS1.SSS3.p1.4.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p1.4.m4.1c">3\times 3</annotation></semantics></math> pooling layer. The output of all those layers are then concatenated to create the input to the next layer.
In InceptionV3, several factorization and dimensionality reduction techniques are used in Inception layers to increase computational efficiency. For example, most convolutional layers are preceded by a <math id="S2.SS1.SSS3.p1.5.m5.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S2.SS1.SSS3.p1.5.m5.1a"><mrow id="S2.SS1.SSS3.p1.5.m5.1.1" xref="S2.SS1.SSS3.p1.5.m5.1.1.cmml"><mn id="S2.SS1.SSS3.p1.5.m5.1.1.2" xref="S2.SS1.SSS3.p1.5.m5.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS3.p1.5.m5.1.1.1" xref="S2.SS1.SSS3.p1.5.m5.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS3.p1.5.m5.1.1.3" xref="S2.SS1.SSS3.p1.5.m5.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p1.5.m5.1b"><apply id="S2.SS1.SSS3.p1.5.m5.1.1.cmml" xref="S2.SS1.SSS3.p1.5.m5.1.1"><times id="S2.SS1.SSS3.p1.5.m5.1.1.1.cmml" xref="S2.SS1.SSS3.p1.5.m5.1.1.1"></times><cn type="integer" id="S2.SS1.SSS3.p1.5.m5.1.1.2.cmml" xref="S2.SS1.SSS3.p1.5.m5.1.1.2">1</cn><cn type="integer" id="S2.SS1.SSS3.p1.5.m5.1.1.3.cmml" xref="S2.SS1.SSS3.p1.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p1.5.m5.1c">1\times 1</annotation></semantics></math> block to reduce dimension along depth. Also, instead of using expensive large size convolutions (e.g. <math id="S2.SS1.SSS3.p1.6.m6.1" class="ltx_Math" alttext="5\times 5" display="inline"><semantics id="S2.SS1.SSS3.p1.6.m6.1a"><mrow id="S2.SS1.SSS3.p1.6.m6.1.1" xref="S2.SS1.SSS3.p1.6.m6.1.1.cmml"><mn id="S2.SS1.SSS3.p1.6.m6.1.1.2" xref="S2.SS1.SSS3.p1.6.m6.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS3.p1.6.m6.1.1.1" xref="S2.SS1.SSS3.p1.6.m6.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS3.p1.6.m6.1.1.3" xref="S2.SS1.SSS3.p1.6.m6.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p1.6.m6.1b"><apply id="S2.SS1.SSS3.p1.6.m6.1.1.cmml" xref="S2.SS1.SSS3.p1.6.m6.1.1"><times id="S2.SS1.SSS3.p1.6.m6.1.1.1.cmml" xref="S2.SS1.SSS3.p1.6.m6.1.1.1"></times><cn type="integer" id="S2.SS1.SSS3.p1.6.m6.1.1.2.cmml" xref="S2.SS1.SSS3.p1.6.m6.1.1.2">5</cn><cn type="integer" id="S2.SS1.SSS3.p1.6.m6.1.1.3.cmml" xref="S2.SS1.SSS3.p1.6.m6.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p1.6.m6.1c">5\times 5</annotation></semantics></math> or <math id="S2.SS1.SSS3.p1.7.m7.1" class="ltx_Math" alttext="7\times 7" display="inline"><semantics id="S2.SS1.SSS3.p1.7.m7.1a"><mrow id="S2.SS1.SSS3.p1.7.m7.1.1" xref="S2.SS1.SSS3.p1.7.m7.1.1.cmml"><mn id="S2.SS1.SSS3.p1.7.m7.1.1.2" xref="S2.SS1.SSS3.p1.7.m7.1.1.2.cmml">7</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS3.p1.7.m7.1.1.1" xref="S2.SS1.SSS3.p1.7.m7.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS3.p1.7.m7.1.1.3" xref="S2.SS1.SSS3.p1.7.m7.1.1.3.cmml">7</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p1.7.m7.1b"><apply id="S2.SS1.SSS3.p1.7.m7.1.1.cmml" xref="S2.SS1.SSS3.p1.7.m7.1.1"><times id="S2.SS1.SSS3.p1.7.m7.1.1.1.cmml" xref="S2.SS1.SSS3.p1.7.m7.1.1.1"></times><cn type="integer" id="S2.SS1.SSS3.p1.7.m7.1.1.2.cmml" xref="S2.SS1.SSS3.p1.7.m7.1.1.2">7</cn><cn type="integer" id="S2.SS1.SSS3.p1.7.m7.1.1.3.cmml" xref="S2.SS1.SSS3.p1.7.m7.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p1.7.m7.1c">7\times 7</annotation></semantics></math>), multiple cascaded small-size (e.g. <math id="S2.SS1.SSS3.p1.8.m8.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S2.SS1.SSS3.p1.8.m8.1a"><mrow id="S2.SS1.SSS3.p1.8.m8.1.1" xref="S2.SS1.SSS3.p1.8.m8.1.1.cmml"><mn id="S2.SS1.SSS3.p1.8.m8.1.1.2" xref="S2.SS1.SSS3.p1.8.m8.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS3.p1.8.m8.1.1.1" xref="S2.SS1.SSS3.p1.8.m8.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS3.p1.8.m8.1.1.3" xref="S2.SS1.SSS3.p1.8.m8.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p1.8.m8.1b"><apply id="S2.SS1.SSS3.p1.8.m8.1.1.cmml" xref="S2.SS1.SSS3.p1.8.m8.1.1"><times id="S2.SS1.SSS3.p1.8.m8.1.1.1.cmml" xref="S2.SS1.SSS3.p1.8.m8.1.1.1"></times><cn type="integer" id="S2.SS1.SSS3.p1.8.m8.1.1.2.cmml" xref="S2.SS1.SSS3.p1.8.m8.1.1.2">3</cn><cn type="integer" id="S2.SS1.SSS3.p1.8.m8.1.1.3.cmml" xref="S2.SS1.SSS3.p1.8.m8.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p1.8.m8.1c">3\times 3</annotation></semantics></math>) are used to greatly reduce the number of parameters without loss of expressiveness <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Another technique to reduce computation is to use asymmetric factorization of convolutional layers that takes advantage of the separable property of convolution. For example, <math id="S2.SS1.SSS3.p1.9.m9.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S2.SS1.SSS3.p1.9.m9.1a"><mrow id="S2.SS1.SSS3.p1.9.m9.1.1" xref="S2.SS1.SSS3.p1.9.m9.1.1.cmml"><mn id="S2.SS1.SSS3.p1.9.m9.1.1.2" xref="S2.SS1.SSS3.p1.9.m9.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS3.p1.9.m9.1.1.1" xref="S2.SS1.SSS3.p1.9.m9.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS3.p1.9.m9.1.1.3" xref="S2.SS1.SSS3.p1.9.m9.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p1.9.m9.1b"><apply id="S2.SS1.SSS3.p1.9.m9.1.1.cmml" xref="S2.SS1.SSS3.p1.9.m9.1.1"><times id="S2.SS1.SSS3.p1.9.m9.1.1.1.cmml" xref="S2.SS1.SSS3.p1.9.m9.1.1.1"></times><cn type="integer" id="S2.SS1.SSS3.p1.9.m9.1.1.2.cmml" xref="S2.SS1.SSS3.p1.9.m9.1.1.2">3</cn><cn type="integer" id="S2.SS1.SSS3.p1.9.m9.1.1.3.cmml" xref="S2.SS1.SSS3.p1.9.m9.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p1.9.m9.1c">3\times 3</annotation></semantics></math> can be factorized into <math id="S2.SS1.SSS3.p1.10.m10.1" class="ltx_Math" alttext="3\times 1" display="inline"><semantics id="S2.SS1.SSS3.p1.10.m10.1a"><mrow id="S2.SS1.SSS3.p1.10.m10.1.1" xref="S2.SS1.SSS3.p1.10.m10.1.1.cmml"><mn id="S2.SS1.SSS3.p1.10.m10.1.1.2" xref="S2.SS1.SSS3.p1.10.m10.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS3.p1.10.m10.1.1.1" xref="S2.SS1.SSS3.p1.10.m10.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS3.p1.10.m10.1.1.3" xref="S2.SS1.SSS3.p1.10.m10.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p1.10.m10.1b"><apply id="S2.SS1.SSS3.p1.10.m10.1.1.cmml" xref="S2.SS1.SSS3.p1.10.m10.1.1"><times id="S2.SS1.SSS3.p1.10.m10.1.1.1.cmml" xref="S2.SS1.SSS3.p1.10.m10.1.1.1"></times><cn type="integer" id="S2.SS1.SSS3.p1.10.m10.1.1.2.cmml" xref="S2.SS1.SSS3.p1.10.m10.1.1.2">3</cn><cn type="integer" id="S2.SS1.SSS3.p1.10.m10.1.1.3.cmml" xref="S2.SS1.SSS3.p1.10.m10.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p1.10.m10.1c">3\times 1</annotation></semantics></math> and <math id="S2.SS1.SSS3.p1.11.m11.1" class="ltx_Math" alttext="1\times 3" display="inline"><semantics id="S2.SS1.SSS3.p1.11.m11.1a"><mrow id="S2.SS1.SSS3.p1.11.m11.1.1" xref="S2.SS1.SSS3.p1.11.m11.1.1.cmml"><mn id="S2.SS1.SSS3.p1.11.m11.1.1.2" xref="S2.SS1.SSS3.p1.11.m11.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS3.p1.11.m11.1.1.1" xref="S2.SS1.SSS3.p1.11.m11.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS3.p1.11.m11.1.1.3" xref="S2.SS1.SSS3.p1.11.m11.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p1.11.m11.1b"><apply id="S2.SS1.SSS3.p1.11.m11.1.1.cmml" xref="S2.SS1.SSS3.p1.11.m11.1.1"><times id="S2.SS1.SSS3.p1.11.m11.1.1.1.cmml" xref="S2.SS1.SSS3.p1.11.m11.1.1.1"></times><cn type="integer" id="S2.SS1.SSS3.p1.11.m11.1.1.2.cmml" xref="S2.SS1.SSS3.p1.11.m11.1.1.2">1</cn><cn type="integer" id="S2.SS1.SSS3.p1.11.m11.1.1.3.cmml" xref="S2.SS1.SSS3.p1.11.m11.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p1.11.m11.1c">1\times 3</annotation></semantics></math> cascaded layers. The resulting two layer network can provide 33% computational efficiency improvement.</p>
</div>
<div id="S2.SS1.SSS3.p2" class="ltx_para">
<p id="S2.SS1.SSS3.p2.9" class="ltx_p">InceptionV3 starts with three cascaded convolutional layers, whose dimensions are <math id="S2.SS1.SSS3.p2.1.m1.1" class="ltx_Math" alttext="3\times 3\times 32" display="inline"><semantics id="S2.SS1.SSS3.p2.1.m1.1a"><mrow id="S2.SS1.SSS3.p2.1.m1.1.1" xref="S2.SS1.SSS3.p2.1.m1.1.1.cmml"><mn id="S2.SS1.SSS3.p2.1.m1.1.1.2" xref="S2.SS1.SSS3.p2.1.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS3.p2.1.m1.1.1.1" xref="S2.SS1.SSS3.p2.1.m1.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS3.p2.1.m1.1.1.3" xref="S2.SS1.SSS3.p2.1.m1.1.1.3.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS3.p2.1.m1.1.1.1a" xref="S2.SS1.SSS3.p2.1.m1.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS3.p2.1.m1.1.1.4" xref="S2.SS1.SSS3.p2.1.m1.1.1.4.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p2.1.m1.1b"><apply id="S2.SS1.SSS3.p2.1.m1.1.1.cmml" xref="S2.SS1.SSS3.p2.1.m1.1.1"><times id="S2.SS1.SSS3.p2.1.m1.1.1.1.cmml" xref="S2.SS1.SSS3.p2.1.m1.1.1.1"></times><cn type="integer" id="S2.SS1.SSS3.p2.1.m1.1.1.2.cmml" xref="S2.SS1.SSS3.p2.1.m1.1.1.2">3</cn><cn type="integer" id="S2.SS1.SSS3.p2.1.m1.1.1.3.cmml" xref="S2.SS1.SSS3.p2.1.m1.1.1.3">3</cn><cn type="integer" id="S2.SS1.SSS3.p2.1.m1.1.1.4.cmml" xref="S2.SS1.SSS3.p2.1.m1.1.1.4">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p2.1.m1.1c">3\times 3\times 32</annotation></semantics></math> (stride 2), <math id="S2.SS1.SSS3.p2.2.m2.1" class="ltx_Math" alttext="3\times 3\times 32" display="inline"><semantics id="S2.SS1.SSS3.p2.2.m2.1a"><mrow id="S2.SS1.SSS3.p2.2.m2.1.1" xref="S2.SS1.SSS3.p2.2.m2.1.1.cmml"><mn id="S2.SS1.SSS3.p2.2.m2.1.1.2" xref="S2.SS1.SSS3.p2.2.m2.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS3.p2.2.m2.1.1.1" xref="S2.SS1.SSS3.p2.2.m2.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS3.p2.2.m2.1.1.3" xref="S2.SS1.SSS3.p2.2.m2.1.1.3.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS3.p2.2.m2.1.1.1a" xref="S2.SS1.SSS3.p2.2.m2.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS3.p2.2.m2.1.1.4" xref="S2.SS1.SSS3.p2.2.m2.1.1.4.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p2.2.m2.1b"><apply id="S2.SS1.SSS3.p2.2.m2.1.1.cmml" xref="S2.SS1.SSS3.p2.2.m2.1.1"><times id="S2.SS1.SSS3.p2.2.m2.1.1.1.cmml" xref="S2.SS1.SSS3.p2.2.m2.1.1.1"></times><cn type="integer" id="S2.SS1.SSS3.p2.2.m2.1.1.2.cmml" xref="S2.SS1.SSS3.p2.2.m2.1.1.2">3</cn><cn type="integer" id="S2.SS1.SSS3.p2.2.m2.1.1.3.cmml" xref="S2.SS1.SSS3.p2.2.m2.1.1.3">3</cn><cn type="integer" id="S2.SS1.SSS3.p2.2.m2.1.1.4.cmml" xref="S2.SS1.SSS3.p2.2.m2.1.1.4">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p2.2.m2.1c">3\times 3\times 32</annotation></semantics></math> (stride 1), <math id="S2.SS1.SSS3.p2.3.m3.1" class="ltx_Math" alttext="3\times 3\times 64" display="inline"><semantics id="S2.SS1.SSS3.p2.3.m3.1a"><mrow id="S2.SS1.SSS3.p2.3.m3.1.1" xref="S2.SS1.SSS3.p2.3.m3.1.1.cmml"><mn id="S2.SS1.SSS3.p2.3.m3.1.1.2" xref="S2.SS1.SSS3.p2.3.m3.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS3.p2.3.m3.1.1.1" xref="S2.SS1.SSS3.p2.3.m3.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS3.p2.3.m3.1.1.3" xref="S2.SS1.SSS3.p2.3.m3.1.1.3.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS3.p2.3.m3.1.1.1a" xref="S2.SS1.SSS3.p2.3.m3.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS3.p2.3.m3.1.1.4" xref="S2.SS1.SSS3.p2.3.m3.1.1.4.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p2.3.m3.1b"><apply id="S2.SS1.SSS3.p2.3.m3.1.1.cmml" xref="S2.SS1.SSS3.p2.3.m3.1.1"><times id="S2.SS1.SSS3.p2.3.m3.1.1.1.cmml" xref="S2.SS1.SSS3.p2.3.m3.1.1.1"></times><cn type="integer" id="S2.SS1.SSS3.p2.3.m3.1.1.2.cmml" xref="S2.SS1.SSS3.p2.3.m3.1.1.2">3</cn><cn type="integer" id="S2.SS1.SSS3.p2.3.m3.1.1.3.cmml" xref="S2.SS1.SSS3.p2.3.m3.1.1.3">3</cn><cn type="integer" id="S2.SS1.SSS3.p2.3.m3.1.1.4.cmml" xref="S2.SS1.SSS3.p2.3.m3.1.1.4">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p2.3.m3.1c">3\times 3\times 64</annotation></semantics></math> (stride 1), respectively.
It is followed with a <math id="S2.SS1.SSS3.p2.4.m4.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S2.SS1.SSS3.p2.4.m4.1a"><mrow id="S2.SS1.SSS3.p2.4.m4.1.1" xref="S2.SS1.SSS3.p2.4.m4.1.1.cmml"><mn id="S2.SS1.SSS3.p2.4.m4.1.1.2" xref="S2.SS1.SSS3.p2.4.m4.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS3.p2.4.m4.1.1.1" xref="S2.SS1.SSS3.p2.4.m4.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS3.p2.4.m4.1.1.3" xref="S2.SS1.SSS3.p2.4.m4.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p2.4.m4.1b"><apply id="S2.SS1.SSS3.p2.4.m4.1.1.cmml" xref="S2.SS1.SSS3.p2.4.m4.1.1"><times id="S2.SS1.SSS3.p2.4.m4.1.1.1.cmml" xref="S2.SS1.SSS3.p2.4.m4.1.1.1"></times><cn type="integer" id="S2.SS1.SSS3.p2.4.m4.1.1.2.cmml" xref="S2.SS1.SSS3.p2.4.m4.1.1.2">3</cn><cn type="integer" id="S2.SS1.SSS3.p2.4.m4.1.1.3.cmml" xref="S2.SS1.SSS3.p2.4.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p2.4.m4.1c">3\times 3</annotation></semantics></math> (stride 2) maximum pooling layer. Then, two cascaded convolutional layers again with filter sizes <math id="S2.SS1.SSS3.p2.5.m5.1" class="ltx_Math" alttext="1\times 1\times 80" display="inline"><semantics id="S2.SS1.SSS3.p2.5.m5.1a"><mrow id="S2.SS1.SSS3.p2.5.m5.1.1" xref="S2.SS1.SSS3.p2.5.m5.1.1.cmml"><mn id="S2.SS1.SSS3.p2.5.m5.1.1.2" xref="S2.SS1.SSS3.p2.5.m5.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS3.p2.5.m5.1.1.1" xref="S2.SS1.SSS3.p2.5.m5.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS3.p2.5.m5.1.1.3" xref="S2.SS1.SSS3.p2.5.m5.1.1.3.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS3.p2.5.m5.1.1.1a" xref="S2.SS1.SSS3.p2.5.m5.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS3.p2.5.m5.1.1.4" xref="S2.SS1.SSS3.p2.5.m5.1.1.4.cmml">80</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p2.5.m5.1b"><apply id="S2.SS1.SSS3.p2.5.m5.1.1.cmml" xref="S2.SS1.SSS3.p2.5.m5.1.1"><times id="S2.SS1.SSS3.p2.5.m5.1.1.1.cmml" xref="S2.SS1.SSS3.p2.5.m5.1.1.1"></times><cn type="integer" id="S2.SS1.SSS3.p2.5.m5.1.1.2.cmml" xref="S2.SS1.SSS3.p2.5.m5.1.1.2">1</cn><cn type="integer" id="S2.SS1.SSS3.p2.5.m5.1.1.3.cmml" xref="S2.SS1.SSS3.p2.5.m5.1.1.3">1</cn><cn type="integer" id="S2.SS1.SSS3.p2.5.m5.1.1.4.cmml" xref="S2.SS1.SSS3.p2.5.m5.1.1.4">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p2.5.m5.1c">1\times 1\times 80</annotation></semantics></math> (stride 1) and <math id="S2.SS1.SSS3.p2.6.m6.1" class="ltx_Math" alttext="3\times 3\times 192" display="inline"><semantics id="S2.SS1.SSS3.p2.6.m6.1a"><mrow id="S2.SS1.SSS3.p2.6.m6.1.1" xref="S2.SS1.SSS3.p2.6.m6.1.1.cmml"><mn id="S2.SS1.SSS3.p2.6.m6.1.1.2" xref="S2.SS1.SSS3.p2.6.m6.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS3.p2.6.m6.1.1.1" xref="S2.SS1.SSS3.p2.6.m6.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS3.p2.6.m6.1.1.3" xref="S2.SS1.SSS3.p2.6.m6.1.1.3.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS3.p2.6.m6.1.1.1a" xref="S2.SS1.SSS3.p2.6.m6.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS3.p2.6.m6.1.1.4" xref="S2.SS1.SSS3.p2.6.m6.1.1.4.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p2.6.m6.1b"><apply id="S2.SS1.SSS3.p2.6.m6.1.1.cmml" xref="S2.SS1.SSS3.p2.6.m6.1.1"><times id="S2.SS1.SSS3.p2.6.m6.1.1.1.cmml" xref="S2.SS1.SSS3.p2.6.m6.1.1.1"></times><cn type="integer" id="S2.SS1.SSS3.p2.6.m6.1.1.2.cmml" xref="S2.SS1.SSS3.p2.6.m6.1.1.2">3</cn><cn type="integer" id="S2.SS1.SSS3.p2.6.m6.1.1.3.cmml" xref="S2.SS1.SSS3.p2.6.m6.1.1.3">3</cn><cn type="integer" id="S2.SS1.SSS3.p2.6.m6.1.1.4.cmml" xref="S2.SS1.SSS3.p2.6.m6.1.1.4">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p2.6.m6.1c">3\times 3\times 192</annotation></semantics></math> (stride 1); this again is followed with a maximum pooling layer with <math id="S2.SS1.SSS3.p2.7.m7.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S2.SS1.SSS3.p2.7.m7.1a"><mrow id="S2.SS1.SSS3.p2.7.m7.1.1" xref="S2.SS1.SSS3.p2.7.m7.1.1.cmml"><mn id="S2.SS1.SSS3.p2.7.m7.1.1.2" xref="S2.SS1.SSS3.p2.7.m7.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS3.p2.7.m7.1.1.1" xref="S2.SS1.SSS3.p2.7.m7.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS3.p2.7.m7.1.1.3" xref="S2.SS1.SSS3.p2.7.m7.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p2.7.m7.1b"><apply id="S2.SS1.SSS3.p2.7.m7.1.1.cmml" xref="S2.SS1.SSS3.p2.7.m7.1.1"><times id="S2.SS1.SSS3.p2.7.m7.1.1.1.cmml" xref="S2.SS1.SSS3.p2.7.m7.1.1.1"></times><cn type="integer" id="S2.SS1.SSS3.p2.7.m7.1.1.2.cmml" xref="S2.SS1.SSS3.p2.7.m7.1.1.2">3</cn><cn type="integer" id="S2.SS1.SSS3.p2.7.m7.1.1.3.cmml" xref="S2.SS1.SSS3.p2.7.m7.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p2.7.m7.1c">3\times 3</annotation></semantics></math> (stride 2). After this, 15 different Inception modules that use all the computational techniques explained above follow. For an input size <math id="S2.SS1.SSS3.p2.8.m8.1" class="ltx_Math" alttext="299\times 299\times 3" display="inline"><semantics id="S2.SS1.SSS3.p2.8.m8.1a"><mrow id="S2.SS1.SSS3.p2.8.m8.1.1" xref="S2.SS1.SSS3.p2.8.m8.1.1.cmml"><mn id="S2.SS1.SSS3.p2.8.m8.1.1.2" xref="S2.SS1.SSS3.p2.8.m8.1.1.2.cmml">299</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS3.p2.8.m8.1.1.1" xref="S2.SS1.SSS3.p2.8.m8.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS3.p2.8.m8.1.1.3" xref="S2.SS1.SSS3.p2.8.m8.1.1.3.cmml">299</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS3.p2.8.m8.1.1.1a" xref="S2.SS1.SSS3.p2.8.m8.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS3.p2.8.m8.1.1.4" xref="S2.SS1.SSS3.p2.8.m8.1.1.4.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p2.8.m8.1b"><apply id="S2.SS1.SSS3.p2.8.m8.1.1.cmml" xref="S2.SS1.SSS3.p2.8.m8.1.1"><times id="S2.SS1.SSS3.p2.8.m8.1.1.1.cmml" xref="S2.SS1.SSS3.p2.8.m8.1.1.1"></times><cn type="integer" id="S2.SS1.SSS3.p2.8.m8.1.1.2.cmml" xref="S2.SS1.SSS3.p2.8.m8.1.1.2">299</cn><cn type="integer" id="S2.SS1.SSS3.p2.8.m8.1.1.3.cmml" xref="S2.SS1.SSS3.p2.8.m8.1.1.3">299</cn><cn type="integer" id="S2.SS1.SSS3.p2.8.m8.1.1.4.cmml" xref="S2.SS1.SSS3.p2.8.m8.1.1.4">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p2.8.m8.1c">299\times 299\times 3</annotation></semantics></math>, the output size at the end of the complete network becomes <math id="S2.SS1.SSS3.p2.9.m9.1" class="ltx_Math" alttext="8\times 8\times 2048" display="inline"><semantics id="S2.SS1.SSS3.p2.9.m9.1a"><mrow id="S2.SS1.SSS3.p2.9.m9.1.1" xref="S2.SS1.SSS3.p2.9.m9.1.1.cmml"><mn id="S2.SS1.SSS3.p2.9.m9.1.1.2" xref="S2.SS1.SSS3.p2.9.m9.1.1.2.cmml">8</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS3.p2.9.m9.1.1.1" xref="S2.SS1.SSS3.p2.9.m9.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS3.p2.9.m9.1.1.3" xref="S2.SS1.SSS3.p2.9.m9.1.1.3.cmml">8</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.SSS3.p2.9.m9.1.1.1a" xref="S2.SS1.SSS3.p2.9.m9.1.1.1.cmml">×</mo><mn id="S2.SS1.SSS3.p2.9.m9.1.1.4" xref="S2.SS1.SSS3.p2.9.m9.1.1.4.cmml">2048</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p2.9.m9.1b"><apply id="S2.SS1.SSS3.p2.9.m9.1.1.cmml" xref="S2.SS1.SSS3.p2.9.m9.1.1"><times id="S2.SS1.SSS3.p2.9.m9.1.1.1.cmml" xref="S2.SS1.SSS3.p2.9.m9.1.1.1"></times><cn type="integer" id="S2.SS1.SSS3.p2.9.m9.1.1.2.cmml" xref="S2.SS1.SSS3.p2.9.m9.1.1.2">8</cn><cn type="integer" id="S2.SS1.SSS3.p2.9.m9.1.1.3.cmml" xref="S2.SS1.SSS3.p2.9.m9.1.1.3">8</cn><cn type="integer" id="S2.SS1.SSS3.p2.9.m9.1.1.4.cmml" xref="S2.SS1.SSS3.p2.9.m9.1.1.4">2048</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p2.9.m9.1c">8\times 8\times 2048</annotation></semantics></math>.</p>
</div>
<div id="S2.SS1.SSS3.p3" class="ltx_para">
<p id="S2.SS1.SSS3.p3.1" class="ltx_p">In our experiments, we have used Python 3.6.5 from Anaconda (Austin, TX, USA), Keras 2.2.2, Keras Applications 1.0.5 and Keras Preprocessing 1.0.3 packages. Keras has been set to work in Tensorflow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> backend. We have used tf-nightly-gpu (version 1.10.0) for Tensorflow (Google Inc, Mountain View, CA, USA). Training was run on a 64 bit Windows 10 system with Intel®Xeon®CPU E5-2640 processor, a single Nvidia®Geforce Titan X (12GB) GPU card and 32GB RAM. The training took around 24 hours for 20 epochs with a training batch size 64 (<math id="S2.SS1.SSS3.p3.1.m1.2" class="ltx_Math" alttext="\approx 40,000" display="inline"><semantics id="S2.SS1.SSS3.p3.1.m1.2a"><mrow id="S2.SS1.SSS3.p3.1.m1.2.3" xref="S2.SS1.SSS3.p3.1.m1.2.3.cmml"><mi id="S2.SS1.SSS3.p3.1.m1.2.3.2" xref="S2.SS1.SSS3.p3.1.m1.2.3.2.cmml"></mi><mo id="S2.SS1.SSS3.p3.1.m1.2.3.1" xref="S2.SS1.SSS3.p3.1.m1.2.3.1.cmml">≈</mo><mrow id="S2.SS1.SSS3.p3.1.m1.2.3.3.2" xref="S2.SS1.SSS3.p3.1.m1.2.3.3.1.cmml"><mn id="S2.SS1.SSS3.p3.1.m1.1.1" xref="S2.SS1.SSS3.p3.1.m1.1.1.cmml">40</mn><mo id="S2.SS1.SSS3.p3.1.m1.2.3.3.2.1" xref="S2.SS1.SSS3.p3.1.m1.2.3.3.1.cmml">,</mo><mn id="S2.SS1.SSS3.p3.1.m1.2.2" xref="S2.SS1.SSS3.p3.1.m1.2.2.cmml">000</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p3.1.m1.2b"><apply id="S2.SS1.SSS3.p3.1.m1.2.3.cmml" xref="S2.SS1.SSS3.p3.1.m1.2.3"><approx id="S2.SS1.SSS3.p3.1.m1.2.3.1.cmml" xref="S2.SS1.SSS3.p3.1.m1.2.3.1"></approx><csymbol cd="latexml" id="S2.SS1.SSS3.p3.1.m1.2.3.2.cmml" xref="S2.SS1.SSS3.p3.1.m1.2.3.2">absent</csymbol><list id="S2.SS1.SSS3.p3.1.m1.2.3.3.1.cmml" xref="S2.SS1.SSS3.p3.1.m1.2.3.3.2"><cn type="integer" id="S2.SS1.SSS3.p3.1.m1.1.1.cmml" xref="S2.SS1.SSS3.p3.1.m1.1.1">40</cn><cn type="integer" id="S2.SS1.SSS3.p3.1.m1.2.2.cmml" xref="S2.SS1.SSS3.p3.1.m1.2.2">000</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p3.1.m1.2c">\approx 40,000</annotation></semantics></math> iterations). After each epoch, prediction on a validation set was performed and the model with lowest validation loss among 20 iterations was used to determine the final model. The loss function was the categorical cross-entropy function, used along with the Adadelta optimizer (learning rate=1, rho=0.95, epsilon=1e-8, decay=0). The class with maximum score was used as the final prediction value.
We also applied random data augmentation with zoom range up to 15%, shear range up to 3%, height and width shift ranges up to 15% , rotation angles up to 10 degrees, contrast range from -100 to 40. Please see the Results section for the classification results.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Object Detection</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">In order to localize the heart valves, we have applied object detection training for Apical 2, Apical 3 and Apical 4 classes separately<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>The annotations for the other views are not yet complete.</span></span></span>. We have used Tensorflow’s Object Detection API <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, which includes implementation of well-known deep learning based networks (SSD, RFCN, Faster-RCNN) and the tools for easy training and testing. The system we used for training was the same system and same packages/software mentioned in the View Classification section.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">The number of classes for identification within the object detection portion of our pipeline depends on the view classification result obtained in in the first stage of the pipeline. In the Apical 2 view, we have only the Mitral valve (MV) we are trying to correctly identify. In Apical 3, we have MV and aortic valve (AV); and in Apical 4 we have MV and tricuspid valve (TV); and in Apical 5 we have left ventricle outflow tract (LVOT) just above AV. Thus, we train object detection networks separately for each cardiac view.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Annotated data</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">As a basis for object detection, the B-Mode DICOM clips were annotated. Only one frame within a B-Mode DICOM clip was annotated, however we used the derived bounding box as ground truth for neighboring frames in the clip as well. Annotations were done on the frame in the heart cycle where the valve is completely closed. The annotation specifies three coordinate locations within the image: the center point where valve flaps touch when fully closed, and the left and right points where the valve connects to heart wall tissue. Since the annotation did not include top and bottom coordinates to define a ground truth bounding box, we selected a fixed height for all bounding boxes which is large enough to encompass the valve when it is fully open. In addition, the width of the valves in annotations can have variation since there is no specific line to distinguish the valves from the connecting tissue. Precise annotations on each frame in the clip could have improved accuracy.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Network</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">The meta-structure, we adopted was Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, with a high-level diagram shown in Fig.<a href="#S2.F4" title="Figure 4 ‣ 2.2.2 Network ‣ 2.2 Object Detection ‣ 2 Materials and Methods ‣ View Classification and Object Detection in Cardiac Ultrasound to Localize Valves via Deep Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. R-CNN networks consists of two stages. The first stage is called <span id="S2.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_italic">region proposal network (RPN)</span>, where the features are extracted from the intermediate layers of the networks such as Inception, ResNet or VGG. These features are given to a <span id="S2.SS2.SSS2.p1.1.2" class="ltx_text ltx_font_italic">region proposal generator</span> that outputs the bounding box coordinates and object-ness scores of fixed number of regions (e.g.300). In the second stage, a cropped set of sub-images created using the region proposals is fed to the remainder of the feature extractor network to output the predicted class and refined bounding box coordinates. Since this operation is done separately for each proposed region, the speed of the Faster R-CNN is highly dependent on the number of region proposals selected. Details about the loss functions and speed/accuracy comparisons of different feature extractors and meta-structures can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2311.00068/assets/rcnn.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="267" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S2.F4.3.2" class="ltx_text" style="font-size:90%;">Diagram of R-CNN (image from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>)</span></figcaption>
</figure>
<figure id="S2.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F5.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.00068/assets/confusion_matrix.png" id="S2.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="369" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F5.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.00068/assets/confusion_matrix_norm.png" id="S2.F5.2.g1" class="ltx_graphics ltx_img_landscape" width="568" height="382" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F5.4.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S2.F5.5.2" class="ltx_text" style="font-size:90%;">Confusion matrix for test set: (a) distribution in frames (b) normalized by the number images per class</span></figcaption>
</figure>
<div id="S2.SS2.SSS2.p2" class="ltx_para">
<p id="S2.SS2.SSS2.p2.1" class="ltx_p">We specifically used <span id="S2.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Faster-RCNN with ResNet101</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. This network has been shown to be slower than other deep learning based networks such as SSD and R-FCN but provides more accurate results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> based on experiments done on the Microsoft COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. The speed and accuracy rates of this network are highly dependent on parameters such as input image size and the number of bounding box proposals. In our applications we used low-resolution images (<math id="S2.SS2.SSS2.p2.1.m1.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="S2.SS2.SSS2.p2.1.m1.1a"><mrow id="S2.SS2.SSS2.p2.1.m1.1.1" xref="S2.SS2.SSS2.p2.1.m1.1.1.cmml"><mn id="S2.SS2.SSS2.p2.1.m1.1.1.2" xref="S2.SS2.SSS2.p2.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.SSS2.p2.1.m1.1.1.1" xref="S2.SS2.SSS2.p2.1.m1.1.1.1.cmml">×</mo><mn id="S2.SS2.SSS2.p2.1.m1.1.1.3" xref="S2.SS2.SSS2.p2.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p2.1.m1.1b"><apply id="S2.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S2.SS2.SSS2.p2.1.m1.1.1"><times id="S2.SS2.SSS2.p2.1.m1.1.1.1.cmml" xref="S2.SS2.SSS2.p2.1.m1.1.1.1"></times><cn type="integer" id="S2.SS2.SSS2.p2.1.m1.1.1.2.cmml" xref="S2.SS2.SSS2.p2.1.m1.1.1.2">256</cn><cn type="integer" id="S2.SS2.SSS2.p2.1.m1.1.1.3.cmml" xref="S2.SS2.SSS2.p2.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p2.1.m1.1c">256\times 256</annotation></semantics></math>) but selected the number of proposals as 300.</p>
</div>
<div id="S2.SS2.SSS2.p3" class="ltx_para">
<p id="S2.SS2.SSS2.p3.1" class="ltx_p">We started with a pre-trained faster-RCNN network on the COCO dataset (the checkpoint was downloaded from the Model Zoo website<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md</a></span></span></span>). The data augmentation options we have used were rgb-to-gray, random-horizontal flip, random-adjust-brightness, random-adjust-contrast, random-crop-and-pad-image (min-area: 0.5, min-padded-size-ratio: [1,1], max-padded-size-ratio: [2,1]). Because random-crop-and-pad image augmentation option may result in a change of image size, we have provided batch size equal to 1 in training. Other parameters are kept unchanged from the values provided in the configuration file that comes with the pre-trained model but we also provided them in the supplementary material. The same images provided to the classification network were used as input to the object detection network. The results of the experiments are provided in the next section.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>View Classification</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The distribution of DICOM clips in our training, validation and test data belonging to each class are shown in Fig. <a href="#S3.F6" title="Figure 6 ‣ 3.1 View Classification ‣ 3 Results ‣ View Classification and Object Detection in Cardiac Ultrasound to Localize Valves via Deep Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. As can be seen there is a significant imbalance between classes. The exact numbers are 686 (Apical 2), 1061 (Apical 3), 2028 (Apical 4), 428 (Apical 5), 1416 (PLAX), 250 (PLAX-RVIF), 74 (PLAX-RVOT), 1882 (PSAX), 909 (PSAX-AoV), 451 (Subcostal).
The overall accuracy we obtained was 97.62% on the test set. The confusion matrix obtained from the test set are shown in Fig. <a href="#S2.F5" title="Figure 5 ‣ 2.2.2 Network ‣ 2.2 Object Detection ‣ 2 Materials and Methods ‣ View Classification and Object Detection in Cardiac Ultrasound to Localize Valves via Deep Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. The diagonal elements show the number of correct predictions whereas off-diagonal elements show the number of mis-classified images. The lowest accuracy we obtained was in the Apical 5 class, mostly because of high correlation to Apical 4 images. In addition, while the heart is contracting, the chamber appearing in the center (the aorta) can become very small in some frames, which makes the image look like an Apical 4 view. Also, we observed that a zoomed Apical 5 may look more like Apical 3. Next lowest accuracy values were obtained in the PLAX-RVIF and PLAX-RVOT views. Images in these classes exhibit large variations from patient to patient. Additional training data representing all these variations was yet not available.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2311.00068/assets/data_distribution.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="368" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S3.F6.3.2" class="ltx_text" style="font-size:90%;">Distribution of dicom clips in overall data</span></figcaption>
</figure>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.3.2" class="ltx_text" style="font-size:90%;">Evaluation results of object detection experiments</span></figcaption>
<table id="S3.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.4.1.1" class="ltx_tr">
<th id="S3.T1.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-bottom:2.15277pt;">Class</th>
<th id="S3.T1.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-bottom:2.15277pt;"># test images</th>
<th id="S3.T1.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-bottom:2.15277pt;">mAP (IoU:0.50:0.95)</th>
<th id="S3.T1.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-bottom:2.15277pt;">mAP (IoU:0.50)</th>
<th id="S3.T1.4.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-bottom:2.15277pt;">mAP (IoU:0.75)</th>
<th id="S3.T1.4.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-bottom:2.15277pt;">mAR (IoU:0.50:0.95)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.4.2.1" class="ltx_tr">
<th id="S3.T1.4.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Apical 2</th>
<th id="S3.T1.4.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">2164</th>
<td id="S3.T1.4.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.151</td>
<td id="S3.T1.4.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.493</td>
<td id="S3.T1.4.2.1.5" class="ltx_td ltx_align_center ltx_border_t">0.041</td>
<td id="S3.T1.4.2.1.6" class="ltx_td ltx_align_center ltx_border_t">0.451</td>
</tr>
<tr id="S3.T1.4.3.2" class="ltx_tr">
<th id="S3.T1.4.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Apical 3</th>
<th id="S3.T1.4.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">2819</th>
<td id="S3.T1.4.3.2.3" class="ltx_td ltx_align_center">0.170</td>
<td id="S3.T1.4.3.2.4" class="ltx_td ltx_align_center">0.547</td>
<td id="S3.T1.4.3.2.5" class="ltx_td ltx_align_center">0.042</td>
<td id="S3.T1.4.3.2.6" class="ltx_td ltx_align_center">0.450</td>
</tr>
<tr id="S3.T1.4.4.3" class="ltx_tr">
<th id="S3.T1.4.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" style="padding-bottom:4.30554pt;">Apical 4</th>
<th id="S3.T1.4.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" style="padding-bottom:4.30554pt;">5303</th>
<td id="S3.T1.4.4.3.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-bottom:4.30554pt;">0.343</td>
<td id="S3.T1.4.4.3.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-bottom:4.30554pt;">0.896</td>
<td id="S3.T1.4.4.3.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-bottom:4.30554pt;">0.146</td>
<td id="S3.T1.4.4.3.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-bottom:4.30554pt;">0.528</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2311.00068/assets/x2.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="226" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S3.F7.3.2" class="ltx_text" style="font-size:90%;">Predicted bounding boxes (top row) and ground truths (bottom row) for Apical 2 view test images. Green represents Mitral valve (MV) with prediction scores in percentage.</span></figcaption>
</figure>
<figure id="S3.F8" class="ltx_figure"><img src="/html/2311.00068/assets/x3.png" id="S3.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="220" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S3.F8.3.2" class="ltx_text" style="font-size:90%;">Predicted bounding boxes (top row) and ground truths (bottom row) for Apical 3 view test images. Green represents Mitral valve (MV) and cyan represents Aortic Valve (AV) with prediction scores in percentage.</span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Object Detection</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In the evaluation of the object detection step, we have used mean average precision (mAP) and mean average recall (mAR) values calculated for the given intersection-over-union ratios (IoU). The evaluation results are given in Table <a href="#S3.T1" title="Table 1 ‣ 3.1 View Classification ‣ 3 Results ‣ View Classification and Object Detection in Cardiac Ultrasound to Localize Valves via Deep Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Here, mAP (IoU:0.50:0.95) corresponds to average mAP calculated over a range of IoU = 0.50:0.05:0.95. This metric is MS COCO’s standard detection metric. On the other hand, mAP (IoU:0.50) corresponds to mAP calculated at IoU=0.50 (PASCAL VOC’s metric <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>). We have also shown results for mAP (IoU:0.75) and mAR (IoU:0.50:0.95). As can be seen from Fig. <a href="#S3.T1" title="Table 1 ‣ 3.1 View Classification ‣ 3 Results ‣ View Classification and Object Detection in Cardiac Ultrasound to Localize Valves via Deep Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the best results are obtained for Apical 4 views because there are two times more samples available in training, and also MV and TV are larger valves than AV, and therefore easier to detect. On the other hand, the Apical 2 view has the largest MV but we have obtained low mAP values. We believe that this is simply because of the variations in ground truths mentioned above and this is more pronounced in Apical 2, where the valve appears big.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">We also show some visual examples of bounding box detection applied to six different images selected from the test set per each cardiac view. These images were hand-picked to represent different B-mode dynamic range, shifts, zoom factors, rotation and noise levels. In all of the figures corresponding to different views, the top row represents the detection results of maximum score (<math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="&gt;0.5" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml"></mi><mo id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">&gt;</mo><mn id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><gt id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></gt><csymbol cd="latexml" id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">absent</csymbol><cn type="float" id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">&gt;0.5</annotation></semantics></math>) and the bottom row represents the ground truth. We used the same color scheme (green) for MV as it appears in all of the Apical views we examined. AV appearing in Apical 3 view is shown in purple, and TV in Apical 4 view shown in cyan.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Results for Apical 2 view are illustrated in Fig. <a href="#S3.F7" title="Figure 7 ‣ 3.1 View Classification ‣ 3 Results ‣ View Classification and Object Detection in Cardiac Ultrasound to Localize Valves via Deep Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. As can be seen, the valves were detected precisely in all the test images. The lowest score was 69% obtained in the second test image since there is more structure appearing around the valve, probably due to the imaging plane being close the wall of the heart.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">Results for Apical 3 view are shown in Fig. <a href="#S3.F8" title="Figure 8 ‣ 3.1 View Classification ‣ 3 Results ‣ View Classification and Object Detection in Cardiac Ultrasound to Localize Valves via Deep Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. The scores for the aortic valve are overall lower than the scores of mitral valve because it is a smaller valve and sometimes hardly visible in especially noisy images such as in the last column of Fig. <a href="#S3.F8" title="Figure 8 ‣ 3.1 View Classification ‣ 3 Results ‣ View Classification and Object Detection in Cardiac Ultrasound to Localize Valves via Deep Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">Results for Apical 4 view are shown in Fig. <a href="#S3.F9" title="Figure 9 ‣ 3.2 Object Detection ‣ 3 Results ‣ View Classification and Object Detection in Cardiac Ultrasound to Localize Valves via Deep Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. Our annotations did not cover the valves appearing only partially (e.g. images in forth and fifth column of Fig. <a href="#S3.F9" title="Figure 9 ‣ 3.2 Object Detection ‣ 3 Results ‣ View Classification and Object Detection in Cardiac Ultrasound to Localize Valves via Deep Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>). However, the network was able to detect these valves as there is crop-and-pad option in our data augmentation, that may create such examples of partial valve images in the training data. When the heart is on a rotated plane, the MV appears smaller in size, which also decreases its detection probability with very low scores as seen in the third and fifth columns of Fig. <a href="#S3.F9" title="Figure 9 ‣ 3.2 Object Detection ‣ 3 Results ‣ View Classification and Object Detection in Cardiac Ultrasound to Localize Valves via Deep Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<figure id="S3.F9" class="ltx_figure"><img src="/html/2311.00068/assets/x4.png" id="S3.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="220" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S3.F9.3.2" class="ltx_text" style="font-size:90%;">Predicted bounding boxes (top row) and ground truths (bottom row) for Apical 4 view test images. Green represents Mitral valve (MV) and cyan represents Tricuspid Valve (TV) with prediction scores in percentage.</span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We have presented an end-to-end deep learning based pipeline that includes classification and object detection modules for the localization and identification of valves in Apical views of cardiac ultrasound images. To the best of our knowledge, this is the first paper that uses state-of the art deep learning based object detection networks for this specific application whereas previous studies use deep networks for segmentation. Our results suggest that it is possible to accurately locate and classify the valves using object detection techniques. For future work, we plan to add more training data with more precise annotations to increase the accuracy in Apical views. We also plan to extend the view classification to cover more views (such as other subcostal views and suprasternal) and apply object detection to the other cardiac views such as PLAX, etc.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
S. Ghemawat, G. Irving, M. Isard, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Tensorflow: a system for large-scale machine learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">OSDI</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, volume 16, pages 265–283, 2016.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
C. F. Baumgartner, K. Kamnitsas, J. Matthew, T. P. Fletcher, S. Smith, L. M.
Koch, B. Kainz, and D. Rueckert.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Sononet: real-time detection and localisation of fetal standard scan
planes in freehand ultrasound.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on medical imaging</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 36(11):2204–2215, 2017.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
J. Dai, Y. Li, K. He, and J. Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">R-fcn: Object detection via region-based fully convolutional
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, pages
379–387, 2016.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
B. D. de Vos, J. M. Wolterink, P. A. de Jong, T. Leiner, M. A. Viergever, and
I. Isgum.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Convnet-based localization of anatomical structures in 3-d medical
images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Trans Med Imaging</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, 36(7):1470–1481, 2017.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Y. Dukler, Y. Ge, Y. Qian, S. Yamamoto, B. Yuan, L. Zhao, A. L. Bertozzi,
B. Hunter, R. Llerena, and J. T. Yen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Automatic valve segmentation in cardiac ultrasound time series data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Medical Imaging 2018: Image Processing</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, volume 10574, page
105741Y. International Society for Optics and Photonics, 2018.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">The PASCAL Visual Object Classes Challenge 2012 (VOC2012)
Results.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
X. Gao, W. Li, M. Loomes, and L. Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">A fused deep learning architecture for viewpoint classification of
echocardiography.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Information Fusion</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, 36:103–113, 2017.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
R. Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Fast r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, pages 1440–1448, 2015.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
R. Girshick, J. Donahue, T. Darrell, and J. Malik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Rich feature hierarchies for accurate object detection and semantic
segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages 580–587, 2014.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
K. He, X. Zhang, S. Ren, and J. Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 770–778, 2016.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer,
Z. Wojna, Y. Song, S. Guadarrama, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Speed/accuracy trade-offs for modern convolutional object detectors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE CVPR</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, volume 4, 2017.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
W. Huang, C. P. Bridge, J. A. Noble, and A. Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Temporal heartnet: towards human-level automatic analysis of fetal
cardiac screening video.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Medical Image Computing and
Computer-Assisted Intervention</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, pages 341–349. Springer, 2017.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
T. Lin, M. Maire, S. J. Belongie, L. D. Bourdev, R. B. Girshick, J. Hays,
P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Microsoft COCO: common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">, abs/1405.0312, 2014.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. E. Reed, C. Fu, and A. C. Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">SSD: single shot multibox detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">, abs/1512.02325, 2015.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
A. Madani, R. Arnaout, M. Mofrad, and R. Arnaout.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Fast and accurate view classification of echocardiograms using deep
learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">npj Digital Medicine</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">, 1(1):6, 2018.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
J. Redmon, S. Divvala, R. Girshick, and A. Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">You only look once: Unified, real-time object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages 779–788, 2016.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
S. Ren, K. He, R. Girshick, and J. Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Faster r-cnn: Towards real-time object detection with region proposal
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, pages
91–99, 2015.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
K. Simonyan and A. Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Very deep convolutional networks for large-scale image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1409.1556</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
V. Sundaresan, C. P. Bridge, C. Ioannou, and J. A. Noble.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Automated characterization of the fetal heart in ultrasound images
using fully convolutional neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Biomedical Imaging (ISBI 2017), 2017 IEEE 14th International
Symposium on</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, pages 671–674. IEEE, 2017.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Rethinking the inception architecture for computer vision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, pages 2818–2826, 2016.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
J. R. Uijlings, K. E. Van De Sande, T. Gevers, and A. W. Smeulders.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Selective search for object recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International journal of computer vision</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">, 104(2):154–171,
2013.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
I. Voigt, M. Scutaru, T. Mansi, B. Georgescu, N. El-Zehiry, H. Houle, and
D. Comaniciu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Robust live tracking of mitral valve annulus for minimally-invasive
intervention guidance.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Medical Image Computing and
Computer-Assisted Intervention</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, pages 439–446. Springer, 2015.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
J. Zhang, S. Gajjala, P. Agrawal, G. H. Tison, L. A. Hallock,
L. Beussink-Nelson, M. H. Lassen, E. Fan, M. A. Aras, C. Jordan, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Fully automated echocardiogram interpretation in clinical practice:
feasibility and diagnostic accuracy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Circulation</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">, 138(16):1623–1635, 2018.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Z.-Q. Zhao, P. Zheng, S.-t. Xu, and X. Wu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Object detection with deep learning: A review.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1807.05511</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Y. Zheng, B. Georgescu, H. Ling, S. K. Zhou, M. Scheuering, and D. Comaniciu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Constrained marginal space learning for efficient 3d anatomical
structure detection in medical images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision and Pattern Recognition, 2009. CVPR 2009.
IEEE Conference on</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, pages 194–201. IEEE, 2009.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2311.00067" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2311.00068" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2311.00068">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2311.00068" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2311.00069" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 20:49:22 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
