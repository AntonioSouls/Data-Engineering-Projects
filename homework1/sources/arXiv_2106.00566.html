<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2106.00566] Full-Resolution Encoderâ€“Decoder Networks with Multi-Scale Feature Fusion for Human Pose Estimation</title><meta property="og:description" content="To achieve more accurate 2D human pose estimation, we extend the successful encoder-decoder network, simple baseline network (SBN), in three ways. To reduce the quantization errors caused by the large output stride sizâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Full-Resolution Encoderâ€“Decoder Networks with Multi-Scale Feature Fusion for Human Pose Estimation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Full-Resolution Encoderâ€“Decoder Networks with Multi-Scale Feature Fusion for Human Pose Estimation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2106.00566">

<!--Generated on Fri Mar  1 14:45:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Encoder-Decoder,  Full Output Resolution,  Spatial Attention,  Multi-Scale Feature Fusion,  Human Pose Estimation.">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Full-Resolution Encoderâ€“Decoder Networks with Multi-Scale Feature Fusion for Human Pose Estimation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jie Ou
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">University of Electronic Science and Technology of China</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_city">Chengdu</span><span id="id3.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:oujieww6@gmail.com">oujieww6@gmail.com</a>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mingjian Chen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id4.1.id1" class="ltx_text ltx_affiliation_institution">University of Electronic Science and Technology of China</span><span id="id5.2.id2" class="ltx_text ltx_affiliation_city">Chengdu</span><span id="id6.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:starvingnow@163.com">starvingnow@163.com</a>
</span></span></span>
<span class="ltx_author_before">Â andÂ </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hong Wu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_affiliation_institution">University of Electronic Science and Technology of China</span><span id="id8.2.id2" class="ltx_text ltx_affiliation_city">Chengdu</span><span id="id9.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:hwu@uestc.edu.cn">hwu@uestc.edu.cn</a>
</span></span></span>
</div>
<div class="ltx_dates">(2021)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id10.id1" class="ltx_p">To achieve more accurate 2D human pose estimation, we extend the successful encoder-decoder network, simple baseline network (SBN), in three ways. To reduce the quantization errors caused by the large output stride size, two more decoder modules are appended to the end of the simple baseline network to get full output resolution. Then, the global context blocks (GCBs) are added to the encoder and decoder modules to enhance them with global context features. Furthermore, we propose a novel spatial-attention-based multi-scale feature collection and distribution module (SA-MFCD) to fuse and distribute multi-scale features to boost the pose estimation. Experimental results on the MS COCO dataset indicate that our network can remarkably improve the accuracy of human pose estimation over SBN, our network using ResNet34 as the backbone network can even achieve the same accuracy as SBN with ResNet152, and our networks can achieve superior results with big backbone networks.</p>
</div>
<div class="ltx_keywords">Encoder-Decoder, Full Output Resolution, Spatial Attention, Multi-Scale Feature Fusion, Human Pose Estimation.
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">journalyear: </span>2021</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">copyright: </span>acmcopyright</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">conference: </span>ACM Multimedia
Asia; March 7â€“9, 2021; Virtual Event, Singapore</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_booktitle"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">booktitle: </span>ACM Multimedia Asia (MMAsia â€™20), March 7â€“9, 2021, Virtual Event,
Singapore</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_price"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">price: </span>15.00</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">doi: </span>10.1145/3444685.3446282</span></span></span><span id="id7" class="ltx_note ltx_note_frontmatter ltx_role_isbn"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">isbn: </span>978-1-4503-8308-0/21/03</span></span></span><span id="id8" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">ccs: </span>Computing methodologiesÂ Activity recognition and understanding</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">2D human pose estimation, which aims to predict locations of human body joints in an image, is a very important and challenging computer vision task. It has been studied for decades and the traditional methods mainly rely on hand-craft featuresÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2015</a>)</cite>. With the application of convolutional neural networks, the accuracy of human pose estimation has been greatly improvedÂ <cite class="ltx_cite ltx_citemacro_citep">(Wei
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2016</a>; Newell
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2016</a>; Chen
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2018</a>; Moon
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2019</a>; Xiao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Most multi-person pose estimation methods can be divided into two categories: top-down and bottom-up methods. The top-down methodsÂ <cite class="ltx_cite ltx_citemacro_citep">(Papandreou etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2017</a>; Fang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2017</a>; Xiao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2018</a>; Sun
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2019</a>)</cite> first use a human detector to detect all person instances in an image, then perform single-person pose estimation for each of them respectively. The top-down methods is inefficient in case of crowds of persons because the run-time of the second step is proportional to the number of persons. On the contrary, the bottom-up methodsÂ <cite class="ltx_cite ltx_citemacro_citep">(Papandreou etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2018</a>; Pishchulin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2016</a>; Insafutdinov etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2017</a>; Cao
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2017</a>)</cite> detect the joints of all persons at once and then group the joints into each individual person by a grouping algorithm. The bottom-up methods suffer from high complexity of joint grouping step and the multi-scale problem in joint detection. Recently, most state-of-the-art results have been achieved by the top-down methods. In this paper, we follow the top-down strategy and develop an effective human pose estimation method.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Among the top-down methods, the simple baseline networkÂ <cite class="ltx_cite ltx_citemacro_citep">(Xiao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2018</a>)</cite> has achieved state-of-the-art results with a simple encoder-decoder architecture. Due to its simplicity and effectiveness, it can be used as a basic network to develop more advanced pose estimation methods. To achieve more accurate human pose estimation, we extend the simple baseline network in three ways. To reduce the quantization error caused by large output stride size, two more decoder modules are appended to the end of SBN to increase the output resolution to the same as input resolution. Then, the global context blocks (GCBs) are added to the encoder and decoder modules to enhance them with global context features. Furthermore, we develop a novel spatial attention based multi-scale feature collection and distribution module (SA-MFCD) to fuse the feature maps from different layers of encoders, refine them with spatial attention, and distribute them to different layers of decoders. Experimental results on the MS COCO dataset indicate that our networks can remarkably improve the accuracy of human pose estimation over SBN, our network with ResNet34 can even achieve the same accuracy as SBN with ResNet152, and our network can achieve superior results with big backbone networks.
</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>RelatedÂ Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Multi-Person Pose Estimation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Most multi-person pose estimation approaches can be classified into top-down and bottom-up approaches.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p"><span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_bold">Top-Down Multi-Person Pose Estimation.</span> Top-Down approaches first apply human detection to an image and then predicate the joint locations of each human instance respectively. Convolutional Pose Machine (CPM)Â <cite class="ltx_cite ltx_citemacro_citep">(Wei
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2016</a>)</cite> employs a multi-stage architecture to combine the prediction of previous stages with the input to refine the pose estimation. NewellÂ <em id="S2.SS1.p2.1.2" class="ltx_emph ltx_font_italic">etÂ al.</em>Â <cite class="ltx_cite ltx_citemacro_citep">(Newell
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite> propose a stacked hourglass network to predict heatmap of 2D joint location.
Mask-RCNNÂ <cite class="ltx_cite ltx_citemacro_citep">(He
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite> extends faster-RCNNÂ <cite class="ltx_cite ltx_citemacro_citep">(Ren
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2015</a>)</cite> to support keypoint predication and obtains very competitive results for human pose estimation. CPNÂ <cite class="ltx_cite ltx_citemacro_citep">(Chen
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite> uses a GlobalNet to localize the â€œsimpleâ€ keypoints and a RefineNet to address the â€œhardâ€ keypoints with online hard keypoint mining. XiaoÂ <em id="S2.SS1.p2.1.3" class="ltx_emph ltx_font_italic">etÂ al.</em>Â <cite class="ltx_cite ltx_citemacro_citep">(Xiao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2018</a>)</cite> propose the simple baseline network for human pose estimation which has achieved state-of-the-art results. Recently, most state-of-the-art results have been achieved by top-down approaches.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p"><span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_bold">Bottom-Up Multi-Person Pose Estimation.</span> Bottom-Up approaches predict all joints first and then group them into different human poses. Some worksÂ <cite class="ltx_cite ltx_citemacro_citep">(Pishchulin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2016</a>; Insafutdinov etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2016</a>, <a href="#bib.bib13" title="" class="ltx_ref">2017</a>)</cite> formulate joint grouping via a Linear Program. CaoÂ <em id="S2.SS1.p3.1.2" class="ltx_emph ltx_font_italic">etÂ al.</em>Â <cite class="ltx_cite ltx_citemacro_citep">(Cao
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2017</a>)</cite> use part affinity fields to encode the relation between different joints to help joint grouping. NewellÂ <em id="S2.SS1.p3.1.3" class="ltx_emph ltx_font_italic">etÂ al.</em>Â <cite class="ltx_cite ltx_citemacro_citep">(Newell
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite> use a detection heatmap and a tagging heatmap for supervising the joint detection and grouping, and then group joints with similar tags into an individual person. PapandreouÂ <em id="S2.SS1.p3.1.4" class="ltx_emph ltx_font_italic">etÂ al.</em>Â <cite class="ltx_cite ltx_citemacro_citep">(Papandreou etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite> develop a convolutional network to predict body joints and their relative displacements for joint grouping. Bottom-up approaches suffer from high complexity of joint grouping step and the multi-scale problem in joint detection. Although some efficient grouping methods have been proposed, bottom-up pose estimation approaches still can not win the competition against top-down approaches.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Attention Mechanism and Global Context Modeling</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Attention mechanism is widely used in computer vision, which focuses on important features and suppresses unnecessary ones. It can be divided into two categories in terms of the dimension considered, channel-wise attention and spatial-wise attention. SENetÂ <cite class="ltx_cite ltx_citemacro_citep">(Hu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2018</a>)</cite> uses a bottleneck transform to generate channel attention weights for image classification.
ZhuÂ <em id="S2.SS2.p1.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>Â <cite class="ltx_cite ltx_citemacro_citep">(Zhu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2019</a>)</cite> propose a channel attention mechanism in their residual attention block to refine features for image super-resolution. CBAMÂ <cite class="ltx_cite ltx_citemacro_citep">(Woo
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2018</a>)</cite> and BAMÂ <cite class="ltx_cite ltx_citemacro_citep">(Park
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2018</a>)</cite> apply both channel and spatial attention modules to refine features for various visual tasks. ChuÂ <em id="S2.SS2.p1.1.2" class="ltx_emph ltx_font_italic">etÂ al.</em>Â <cite class="ltx_cite ltx_citemacro_citep">(Chu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2017</a>)</cite> use attention models for human pose estimation and model the attention from different context and resolution.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Spatial attention mechanism has also been used to model global context. The non-local networkÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2018</a>)</cite> uses a query-specific spatial attention map for each query position to aggregate context features. GCNetÂ <cite class="ltx_cite ltx_citemacro_citep">(Cao
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2019</a>)</cite> achieves more efficient global context modeling by using a query-independent attention map to generate a global context feature for all query positions and further re-calibrating the global context feature with channel attention.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Methodology</h2>

<figure id="S3.F1" class="ltx_figure"><img src="/html/2106.00566/assets/images/4.91.png" id="S3.F1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="385" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>(a) The full-resolution encoder-decoder network. The red arrow lines stand for the flows of spatial attention heatmaps (SAH) generated by GCBs, and black arrow lines are for feature maps. (b) The global context block (GCB). <math id="S3.F1.3.m1.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S3.F1.3.m1.1b"><mi id="S3.F1.3.m1.1.1" xref="S3.F1.3.m1.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.F1.3.m1.1c"><ci id="S3.F1.3.m1.1.1.cmml" xref="S3.F1.3.m1.1.1">ğ¹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.3.m1.1d">F</annotation></semantics></math> and <math id="S3.F1.4.m2.1" class="ltx_Math" alttext="F^{\prime}" display="inline"><semantics id="S3.F1.4.m2.1b"><msup id="S3.F1.4.m2.1.1" xref="S3.F1.4.m2.1.1.cmml"><mi id="S3.F1.4.m2.1.1.2" xref="S3.F1.4.m2.1.1.2.cmml">F</mi><mo id="S3.F1.4.m2.1.1.3" xref="S3.F1.4.m2.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S3.F1.4.m2.1c"><apply id="S3.F1.4.m2.1.1.cmml" xref="S3.F1.4.m2.1.1"><csymbol cd="ambiguous" id="S3.F1.4.m2.1.1.1.cmml" xref="S3.F1.4.m2.1.1">superscript</csymbol><ci id="S3.F1.4.m2.1.1.2.cmml" xref="S3.F1.4.m2.1.1.2">ğ¹</ci><ci id="S3.F1.4.m2.1.1.3.cmml" xref="S3.F1.4.m2.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.4.m2.1d">F^{\prime}</annotation></semantics></math> are the input and output feature maps of a GCB respectively. (c) The SA-MFCD Module. </figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our multi-person pose estimation approach uses faster-RCNNÂ <cite class="ltx_cite ltx_citemacro_citep">(Ren
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2015</a>)</cite> to detect each person in an image, and then predicates pose for each person instance. Our network is an extension of the successful simple baseline network (SBN), as show in Fig.Â <a href="#S3.F1" title="Figure 1 â€£ 3. Methodology â€£ Full-Resolution Encoderâ€“Decoder Networks with Multi-Scale Feature Fusion for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(a). Firstly, two more decoder modules are appended to the end of the SBN to increase the output resolution. The encoder and decoder modules are then enhanced by the global context blocks (GCBs)Â <cite class="ltx_cite ltx_citemacro_citep">(Cao
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2019</a>)</cite> which compute global context features to enhance the original feature maps. Furthermore, a spatial attention based multi-scale feature collection and distribution (SA-MFCD) module is proposed to fuse feature maps from different layers of encoders, refine them with spatial attention and distribute them to different layers of decoders. More detailed information are given in the following subsections.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Extend Simple Baseline Network to Full Output Resolution</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The encoder-decoder network structure is widely used in dense prediction, including semantic segmentationÂ <cite class="ltx_cite ltx_citemacro_citep">(Ronneberger
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2015</a>; Fu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite>, object detectionÂ <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2017</a>)</cite>, and human pose estimationÂ <cite class="ltx_cite ltx_citemacro_citep">(Newell
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2016</a>; Xiao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2018</a>)</cite>. The encoder-decoder networks contain encoders that gradually reduce the resolutions of feature maps while capturing higher semantic information, and decoders that gradually recover the spatial resolutions. The output resolutions of most popular encoder-decoder networks for human pose estimation are smaller than their input resolutions. For example, the output resolutions of the Hourglass network Â <cite class="ltx_cite ltx_citemacro_citep">(Newell
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite> and the simple baseline network Â <cite class="ltx_cite ltx_citemacro_citep">(Xiao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2018</a>)</cite> are only 1/4 of their input resolution. The outputs are then resized to the input resolution by simple transformation, which introduces quantization errors. InÂ <cite class="ltx_cite ltx_citemacro_citep">(Huang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite>, the common biased data processing for human pose estimation has been quantitatively analyzed, and it is found that the error of prediction is mainly due to the horizontal flip operation which is also related to the output stride size.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">To alleviate this problem, we append two more decoder modules to the end of SBN to achieve the full output resolution. Following SBN, the encoder part of our network is a modified version of the ResNetÂ <cite class="ltx_cite ltx_citemacro_citep">(He
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2016</a>)</cite>, where the average pooling layer and last fully-connected layers are removed and only the convolutional layers are used. It includes a convolutional layer and a pooling layer followed by four stages of ResBlocks, both the first convolutional layer and pooling layer halves the resolution, and at the beginning of each Res-stage (except for the first one), the feature map resolution is also halved by a convolutional layer with strides=2, while the number of filters is doubled. Each decoder is composed of a Deconvolution (Transport Convolution) layer followed by batch normalization and ReLU. The decoder part of our network includes five decoder modules each of which double the resolution, and finally gets output with the same resolution as that of input. Compared to SBN, the two additional decoder modules bring more parameters and increase the model size. Our previous study indicates that reducing the channel sizes of the two additional decoder modules can make a good trade-off between the prediction accuracy and the model size. In our study, we set the channel size of the 4th decoder to 128 and that of the 5th decoder to 32 respectively. We name this basic network as the full resolution version of SBN (FR-SBN).</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Enhance Encoders and Decoders by Global Context Blocks</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The combination of global context information has proven useful to various visual recognition tasks. However, a convolution layer only models pixel relationship in a local neighborhood, and the long-range dependencies are mainly modeled by deeply stacking convolution layers. Non-local operationÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2018</a>)</cite> is proposed to model the long-range dependencies using self-attention mechanismÂ <cite class="ltx_cite ltx_citemacro_citep">(Vaswani etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2017</a>)</cite>. The non-local network uses a query-specific attention map for each query position to aggregate context features and adds the context features to the feature of the corresponding query position. GCNetÂ <cite class="ltx_cite ltx_citemacro_citep">(Cao
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2019</a>)</cite> is a more effective and efficient global context modeling approach, which uses a query-independent attention map to generate a global context feature and add it to all query positions.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">In this paper, we apply the global context block (GCB)Â <cite class="ltx_cite ltx_citemacro_citep">(Cao
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2019</a>)</cite> to enhance the encoder and decoder modules. GCBs are introduced after the four Res-stages in the encoder part and the first three Decoder Models, as shown in Fig.Â <a href="#S3.F1" title="Figure 1 â€£ 3. Methodology â€£ Full-Resolution Encoderâ€“Decoder Networks with Multi-Scale Feature Fusion for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(a). The structure of GCB is shown in Fig.Â <a href="#S3.F1" title="Figure 1 â€£ 3. Methodology â€£ Full-Resolution Encoderâ€“Decoder Networks with Multi-Scale Feature Fusion for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(b). A <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mn id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><times id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></times><cn type="integer" id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">1</cn><cn type="integer" id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">1\times 1</annotation></semantics></math> convolution is first used to generate a spatial attention heatmap (SAH) and followed by softmax to generate attention weights. Then attention pooling is used to obtain a global context feature. After that, a bottleneck transform is applied to it to capture its channel-wise dependencies. Finally, the global context feature is added to features of all positions. The SAH is also used in SA-MFCD which is introduced in the next subsection.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Spatial Attention Based Multi-Scale Feature Collection and Distribution</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In previous encoder-decoder networks such as U-NetÂ <cite class="ltx_cite ltx_citemacro_citep">(Ronneberger
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2015</a>)</cite> and HourglassÂ <cite class="ltx_cite ltx_citemacro_citep">(Newell
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite>, skip connections are used to preserve spatial information at each resolution. Skip connections are formed between the encoders and the decoders having the same output resolution to transfer the spatial information across the network for better localization. Through the skip connections, feature maps from the encoder are concatenated (or summed) with the corresponding feature maps of the decoder. For simplicity, the simple baseline network Â <cite class="ltx_cite ltx_citemacro_citep">(Xiao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2018</a>)</cite> does not use skip connections, which may limited its performance.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.3" class="ltx_p">Different from the traditional skip connection, we propose a Spatial Attention based Multi-Scale Feature Collection and Distribution (SA-MFCD) module which merges multi-scale feature-maps from different layers of encoders, uses spatial attention to refine them, and finally distributes them to different layers of decoders. This scheme can bring multi-scale spatial and semantic information to the decoders for recovering high-resolution feature-maps. The details of this module is show in Fig.Â <a href="#S3.F1" title="Figure 1 â€£ 3. Methodology â€£ Full-Resolution Encoderâ€“Decoder Networks with Multi-Scale Feature Fusion for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(c). This module has two different inputs: feature maps and spatial attention maps. The feature maps generated from the GCBs after Res-stage1, Res-stage2 and Res-stage3 are sent to SA-MFCD, the feature maps with lower resolutions are up-sampled to match the highest one and concatenated together, and fused by a <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mn id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p2.1.m1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><times id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1"></times><cn type="integer" id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">3</cn><cn type="integer" id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">3\times 3</annotation></semantics></math> convolution. The SAHs generated from the three GCBs are also resized and concatenated, and fused by a <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mrow id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mn id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p2.2.m2.1.1.1" xref="S3.SS3.p2.2.m2.1.1.1.cmml">Ã—</mo><mn id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><times id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1.1"></times><cn type="integer" id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2">3</cn><cn type="integer" id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">3\times 3</annotation></semantics></math> convolution. The fused feature maps are then multiplied element-wise with the fused SAH. Finally, the refined feature maps are distributed to Decoder2, Decoder3 and Decoder4 after being resized to corresponding resolutions. The refined feature maps from SA-MFCD are concatenated with the output feature maps of the deconvolution layers in the corresponding decoders and fused by a <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mrow id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml"><mn id="S3.SS3.p2.3.m3.1.1.2" xref="S3.SS3.p2.3.m3.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p2.3.m3.1.1.1" xref="S3.SS3.p2.3.m3.1.1.1.cmml">Ã—</mo><mn id="S3.SS3.p2.3.m3.1.1.3" xref="S3.SS3.p2.3.m3.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><apply id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1"><times id="S3.SS3.p2.3.m3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1"></times><cn type="integer" id="S3.SS3.p2.3.m3.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2">3</cn><cn type="integer" id="S3.SS3.p2.3.m3.1.1.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">3\times 3</annotation></semantics></math> convolution to generate the outputs of the decoders. All of the convolution operations used here are followed by a batch normalization layer and ReLU function.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Via SA-MFCD, feature maps with different resolutions are fused together. The higher semantic information in lower resolution feature maps is combined with the more refined spatial information in the higher resolution feature maps. The spatial attention heatmaps (SAHs) with different resolutions can help extracting useful details and suppressing background information. And the fused feature maps are refined by the spatial attention mechanism to emphasize the positions related to pose estimation. When combining the refined feature maps from SA-MFCD model, the capacities of the corresponding decoders and thus the whole network are improved.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Ground-truth and Loss Function</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.10" class="ltx_p">Our human pose estimation network does not directly predict the coordinates, but do a pixel-level prediction by transforming joint position to heatmap. For each target-person patch cropped by human detection bounding-box, there might exist non-target person in it, we only generate ground-truth for the target one, by a 2D Gaussian function:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.5" class="ltx_Math" alttext="\boldsymbol{H}_{k}(x,y)=\exp\left(\frac{-\left[\left(x-x_{k}\right)^{2}+\left(y-y_{k}\right)^{2}\right]}{2\sigma^{2}}\right)," display="block"><semantics id="S3.E1.m1.5a"><mrow id="S3.E1.m1.5.5.1" xref="S3.E1.m1.5.5.1.1.cmml"><mrow id="S3.E1.m1.5.5.1.1" xref="S3.E1.m1.5.5.1.1.cmml"><mrow id="S3.E1.m1.5.5.1.1.2" xref="S3.E1.m1.5.5.1.1.2.cmml"><msub id="S3.E1.m1.5.5.1.1.2.2" xref="S3.E1.m1.5.5.1.1.2.2.cmml"><mi id="S3.E1.m1.5.5.1.1.2.2.2" xref="S3.E1.m1.5.5.1.1.2.2.2.cmml">ğ‘¯</mi><mi id="S3.E1.m1.5.5.1.1.2.2.3" xref="S3.E1.m1.5.5.1.1.2.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.1.1.2.1" xref="S3.E1.m1.5.5.1.1.2.1.cmml">â€‹</mo><mrow id="S3.E1.m1.5.5.1.1.2.3.2" xref="S3.E1.m1.5.5.1.1.2.3.1.cmml"><mo stretchy="false" id="S3.E1.m1.5.5.1.1.2.3.2.1" xref="S3.E1.m1.5.5.1.1.2.3.1.cmml">(</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">x</mi><mo id="S3.E1.m1.5.5.1.1.2.3.2.2" xref="S3.E1.m1.5.5.1.1.2.3.1.cmml">,</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">y</mi><mo stretchy="false" id="S3.E1.m1.5.5.1.1.2.3.2.3" xref="S3.E1.m1.5.5.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.5.5.1.1.1" xref="S3.E1.m1.5.5.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.5.5.1.1.3.2" xref="S3.E1.m1.5.5.1.1.3.1.cmml"><mi id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">exp</mi><mo id="S3.E1.m1.5.5.1.1.3.2a" xref="S3.E1.m1.5.5.1.1.3.1.cmml">â¡</mo><mrow id="S3.E1.m1.5.5.1.1.3.2.1" xref="S3.E1.m1.5.5.1.1.3.1.cmml"><mo id="S3.E1.m1.5.5.1.1.3.2.1.1" xref="S3.E1.m1.5.5.1.1.3.1.cmml">(</mo><mfrac id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><mo id="S3.E1.m1.1.1.1a" xref="S3.E1.m1.1.1.1.cmml">âˆ’</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.2.cmml"><mo id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.1.cmml">[</mo><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><msup id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">x</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">k</mi></msub></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml">2</mn></msup><mo id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml">+</mo><msup id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.2.1.1" xref="S3.E1.m1.1.1.1.1.1.1.2.1.1.1.cmml"><mo id="S3.E1.m1.1.1.1.1.1.1.2.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.2.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.2.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.1.1.1.2.cmml">y</mi><mo id="S3.E1.m1.1.1.1.1.1.1.2.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.2.1.1.1.1.cmml">âˆ’</mo><msub id="S3.E1.m1.1.1.1.1.1.1.2.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.2.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.2.1.1.1.3.2.cmml">y</mi><mi id="S3.E1.m1.1.1.1.1.1.1.2.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.2.1.1.1.3.3.cmml">k</mi></msub></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.2.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.2.1.1.1.cmml">)</mo></mrow><mn id="S3.E1.m1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.2.3.cmml">2</mn></msup></mrow><mo id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.2.1.cmml">]</mo></mrow></mrow><mrow id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mn id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.1" xref="S3.E1.m1.1.1.3.1.cmml">â€‹</mo><msup id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.3.3.2" xref="S3.E1.m1.1.1.3.3.2.cmml">Ïƒ</mi><mn id="S3.E1.m1.1.1.3.3.3" xref="S3.E1.m1.1.1.3.3.3.cmml">2</mn></msup></mrow></mfrac><mo id="S3.E1.m1.5.5.1.1.3.2.1.2" xref="S3.E1.m1.5.5.1.1.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.5.5.1.2" xref="S3.E1.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.5b"><apply id="S3.E1.m1.5.5.1.1.cmml" xref="S3.E1.m1.5.5.1"><eq id="S3.E1.m1.5.5.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1"></eq><apply id="S3.E1.m1.5.5.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.2"><times id="S3.E1.m1.5.5.1.1.2.1.cmml" xref="S3.E1.m1.5.5.1.1.2.1"></times><apply id="S3.E1.m1.5.5.1.1.2.2.cmml" xref="S3.E1.m1.5.5.1.1.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.2.2.1.cmml" xref="S3.E1.m1.5.5.1.1.2.2">subscript</csymbol><ci id="S3.E1.m1.5.5.1.1.2.2.2.cmml" xref="S3.E1.m1.5.5.1.1.2.2.2">ğ‘¯</ci><ci id="S3.E1.m1.5.5.1.1.2.2.3.cmml" xref="S3.E1.m1.5.5.1.1.2.2.3">ğ‘˜</ci></apply><interval closure="open" id="S3.E1.m1.5.5.1.1.2.3.1.cmml" xref="S3.E1.m1.5.5.1.1.2.3.2"><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">ğ‘¥</ci><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">ğ‘¦</ci></interval></apply><apply id="S3.E1.m1.5.5.1.1.3.1.cmml" xref="S3.E1.m1.5.5.1.1.3.2"><exp id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4"></exp><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><divide id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1"></divide><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><minus id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1"></minus><apply id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><plus id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3"></plus><apply id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1"><minus id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1"></minus><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2">ğ‘¥</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2">ğ‘¥</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3">ğ‘˜</ci></apply></apply><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3">2</cn></apply><apply id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.2.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.1.1"><minus id="S3.E1.m1.1.1.1.1.1.1.2.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.1.1.1.1"></minus><ci id="S3.E1.m1.1.1.1.1.1.1.2.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.1.1.1.2">ğ‘¦</ci><apply id="S3.E1.m1.1.1.1.1.1.1.2.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.2.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.2.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.1.1.1.3.2">ğ‘¦</ci><ci id="S3.E1.m1.1.1.1.1.1.1.2.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.1.1.1.3.3">ğ‘˜</ci></apply></apply><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.3">2</cn></apply></apply></apply></apply><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><times id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3.1"></times><cn type="integer" id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2">2</cn><apply id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3">superscript</csymbol><ci id="S3.E1.m1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.2">ğœ</ci><cn type="integer" id="S3.E1.m1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3">2</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.5c">\boldsymbol{H}_{k}(x,y)=\exp\left(\frac{-\left[\left(x-x_{k}\right)^{2}+\left(y-y_{k}\right)^{2}\right]}{2\sigma^{2}}\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS4.p1.9" class="ltx_p">where <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="H_{k}" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><msub id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><mi id="S3.SS4.p1.1.m1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.2.cmml">H</mi><mi id="S3.SS4.p1.1.m1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p1.1.m1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2">ğ»</ci><ci id="S3.SS4.p1.1.m1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">H_{k}</annotation></semantics></math> is the heatmap for the <math id="S3.SS4.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS4.p1.2.m2.1a"><mi id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><ci id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">k</annotation></semantics></math>-th joint<math id="S3.SS4.p1.3.m3.4" class="ltx_Math" alttext="(k\in\left\{1,...,K\right\})" display="inline"><semantics id="S3.SS4.p1.3.m3.4a"><mrow id="S3.SS4.p1.3.m3.4.4.1" xref="S3.SS4.p1.3.m3.4.4.1.1.cmml"><mo stretchy="false" id="S3.SS4.p1.3.m3.4.4.1.2" xref="S3.SS4.p1.3.m3.4.4.1.1.cmml">(</mo><mrow id="S3.SS4.p1.3.m3.4.4.1.1" xref="S3.SS4.p1.3.m3.4.4.1.1.cmml"><mi id="S3.SS4.p1.3.m3.4.4.1.1.2" xref="S3.SS4.p1.3.m3.4.4.1.1.2.cmml">k</mi><mo id="S3.SS4.p1.3.m3.4.4.1.1.1" xref="S3.SS4.p1.3.m3.4.4.1.1.1.cmml">âˆˆ</mo><mrow id="S3.SS4.p1.3.m3.4.4.1.1.3.2" xref="S3.SS4.p1.3.m3.4.4.1.1.3.1.cmml"><mo id="S3.SS4.p1.3.m3.4.4.1.1.3.2.1" xref="S3.SS4.p1.3.m3.4.4.1.1.3.1.cmml">{</mo><mn id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml">1</mn><mo id="S3.SS4.p1.3.m3.4.4.1.1.3.2.2" xref="S3.SS4.p1.3.m3.4.4.1.1.3.1.cmml">,</mo><mi mathvariant="normal" id="S3.SS4.p1.3.m3.2.2" xref="S3.SS4.p1.3.m3.2.2.cmml">â€¦</mi><mo id="S3.SS4.p1.3.m3.4.4.1.1.3.2.3" xref="S3.SS4.p1.3.m3.4.4.1.1.3.1.cmml">,</mo><mi id="S3.SS4.p1.3.m3.3.3" xref="S3.SS4.p1.3.m3.3.3.cmml">K</mi><mo id="S3.SS4.p1.3.m3.4.4.1.1.3.2.4" xref="S3.SS4.p1.3.m3.4.4.1.1.3.1.cmml">}</mo></mrow></mrow><mo stretchy="false" id="S3.SS4.p1.3.m3.4.4.1.3" xref="S3.SS4.p1.3.m3.4.4.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.4b"><apply id="S3.SS4.p1.3.m3.4.4.1.1.cmml" xref="S3.SS4.p1.3.m3.4.4.1"><in id="S3.SS4.p1.3.m3.4.4.1.1.1.cmml" xref="S3.SS4.p1.3.m3.4.4.1.1.1"></in><ci id="S3.SS4.p1.3.m3.4.4.1.1.2.cmml" xref="S3.SS4.p1.3.m3.4.4.1.1.2">ğ‘˜</ci><set id="S3.SS4.p1.3.m3.4.4.1.1.3.1.cmml" xref="S3.SS4.p1.3.m3.4.4.1.1.3.2"><cn type="integer" id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1">1</cn><ci id="S3.SS4.p1.3.m3.2.2.cmml" xref="S3.SS4.p1.3.m3.2.2">â€¦</ci><ci id="S3.SS4.p1.3.m3.3.3.cmml" xref="S3.SS4.p1.3.m3.3.3">ğ¾</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.4c">(k\in\left\{1,...,K\right\})</annotation></semantics></math>, <math id="S3.SS4.p1.4.m4.2" class="ltx_Math" alttext="(x_{k},y_{k})" display="inline"><semantics id="S3.SS4.p1.4.m4.2a"><mrow id="S3.SS4.p1.4.m4.2.2.2" xref="S3.SS4.p1.4.m4.2.2.3.cmml"><mo stretchy="false" id="S3.SS4.p1.4.m4.2.2.2.3" xref="S3.SS4.p1.4.m4.2.2.3.cmml">(</mo><msub id="S3.SS4.p1.4.m4.1.1.1.1" xref="S3.SS4.p1.4.m4.1.1.1.1.cmml"><mi id="S3.SS4.p1.4.m4.1.1.1.1.2" xref="S3.SS4.p1.4.m4.1.1.1.1.2.cmml">x</mi><mi id="S3.SS4.p1.4.m4.1.1.1.1.3" xref="S3.SS4.p1.4.m4.1.1.1.1.3.cmml">k</mi></msub><mo id="S3.SS4.p1.4.m4.2.2.2.4" xref="S3.SS4.p1.4.m4.2.2.3.cmml">,</mo><msub id="S3.SS4.p1.4.m4.2.2.2.2" xref="S3.SS4.p1.4.m4.2.2.2.2.cmml"><mi id="S3.SS4.p1.4.m4.2.2.2.2.2" xref="S3.SS4.p1.4.m4.2.2.2.2.2.cmml">y</mi><mi id="S3.SS4.p1.4.m4.2.2.2.2.3" xref="S3.SS4.p1.4.m4.2.2.2.2.3.cmml">k</mi></msub><mo stretchy="false" id="S3.SS4.p1.4.m4.2.2.2.5" xref="S3.SS4.p1.4.m4.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.4.m4.2b"><interval closure="open" id="S3.SS4.p1.4.m4.2.2.3.cmml" xref="S3.SS4.p1.4.m4.2.2.2"><apply id="S3.SS4.p1.4.m4.1.1.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.4.m4.1.1.1.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p1.4.m4.1.1.1.1.2.cmml" xref="S3.SS4.p1.4.m4.1.1.1.1.2">ğ‘¥</ci><ci id="S3.SS4.p1.4.m4.1.1.1.1.3.cmml" xref="S3.SS4.p1.4.m4.1.1.1.1.3">ğ‘˜</ci></apply><apply id="S3.SS4.p1.4.m4.2.2.2.2.cmml" xref="S3.SS4.p1.4.m4.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS4.p1.4.m4.2.2.2.2.1.cmml" xref="S3.SS4.p1.4.m4.2.2.2.2">subscript</csymbol><ci id="S3.SS4.p1.4.m4.2.2.2.2.2.cmml" xref="S3.SS4.p1.4.m4.2.2.2.2.2">ğ‘¦</ci><ci id="S3.SS4.p1.4.m4.2.2.2.2.3.cmml" xref="S3.SS4.p1.4.m4.2.2.2.2.3">ğ‘˜</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.4.m4.2c">(x_{k},y_{k})</annotation></semantics></math> is the coordinations of the <math id="S3.SS4.p1.5.m5.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS4.p1.5.m5.1a"><mi id="S3.SS4.p1.5.m5.1.1" xref="S3.SS4.p1.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.5.m5.1b"><ci id="S3.SS4.p1.5.m5.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.5.m5.1c">k</annotation></semantics></math>-th joint, <math id="S3.SS4.p1.6.m6.2" class="ltx_Math" alttext="(x,y)" display="inline"><semantics id="S3.SS4.p1.6.m6.2a"><mrow id="S3.SS4.p1.6.m6.2.3.2" xref="S3.SS4.p1.6.m6.2.3.1.cmml"><mo stretchy="false" id="S3.SS4.p1.6.m6.2.3.2.1" xref="S3.SS4.p1.6.m6.2.3.1.cmml">(</mo><mi id="S3.SS4.p1.6.m6.1.1" xref="S3.SS4.p1.6.m6.1.1.cmml">x</mi><mo id="S3.SS4.p1.6.m6.2.3.2.2" xref="S3.SS4.p1.6.m6.2.3.1.cmml">,</mo><mi id="S3.SS4.p1.6.m6.2.2" xref="S3.SS4.p1.6.m6.2.2.cmml">y</mi><mo stretchy="false" id="S3.SS4.p1.6.m6.2.3.2.3" xref="S3.SS4.p1.6.m6.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.6.m6.2b"><interval closure="open" id="S3.SS4.p1.6.m6.2.3.1.cmml" xref="S3.SS4.p1.6.m6.2.3.2"><ci id="S3.SS4.p1.6.m6.1.1.cmml" xref="S3.SS4.p1.6.m6.1.1">ğ‘¥</ci><ci id="S3.SS4.p1.6.m6.2.2.cmml" xref="S3.SS4.p1.6.m6.2.2">ğ‘¦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.6.m6.2c">(x,y)</annotation></semantics></math> specifies a pixel location in the heatmap and the hyper-parameter <math id="S3.SS4.p1.7.m7.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.SS4.p1.7.m7.1a"><mi id="S3.SS4.p1.7.m7.1.1" xref="S3.SS4.p1.7.m7.1.1.cmml">Ïƒ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.7.m7.1b"><ci id="S3.SS4.p1.7.m7.1.1.cmml" xref="S3.SS4.p1.7.m7.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.7.m7.1c">\sigma</annotation></semantics></math> denotes a pre-fixed spatial variance, which is set to 8 and 12 for <math id="S3.SS4.p1.8.m8.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S3.SS4.p1.8.m8.1a"><mrow id="S3.SS4.p1.8.m8.1.1" xref="S3.SS4.p1.8.m8.1.1.cmml"><mn id="S3.SS4.p1.8.m8.1.1.2" xref="S3.SS4.p1.8.m8.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p1.8.m8.1.1.1" xref="S3.SS4.p1.8.m8.1.1.1.cmml">Ã—</mo><mn id="S3.SS4.p1.8.m8.1.1.3" xref="S3.SS4.p1.8.m8.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.8.m8.1b"><apply id="S3.SS4.p1.8.m8.1.1.cmml" xref="S3.SS4.p1.8.m8.1.1"><times id="S3.SS4.p1.8.m8.1.1.1.cmml" xref="S3.SS4.p1.8.m8.1.1.1"></times><cn type="integer" id="S3.SS4.p1.8.m8.1.1.2.cmml" xref="S3.SS4.p1.8.m8.1.1.2">256</cn><cn type="integer" id="S3.SS4.p1.8.m8.1.1.3.cmml" xref="S3.SS4.p1.8.m8.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.8.m8.1c">256\times 192</annotation></semantics></math> and <math id="S3.SS4.p1.9.m9.1" class="ltx_Math" alttext="388\times 288" display="inline"><semantics id="S3.SS4.p1.9.m9.1a"><mrow id="S3.SS4.p1.9.m9.1.1" xref="S3.SS4.p1.9.m9.1.1.cmml"><mn id="S3.SS4.p1.9.m9.1.1.2" xref="S3.SS4.p1.9.m9.1.1.2.cmml">388</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p1.9.m9.1.1.1" xref="S3.SS4.p1.9.m9.1.1.1.cmml">Ã—</mo><mn id="S3.SS4.p1.9.m9.1.1.3" xref="S3.SS4.p1.9.m9.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.9.m9.1b"><apply id="S3.SS4.p1.9.m9.1.1.cmml" xref="S3.SS4.p1.9.m9.1.1"><times id="S3.SS4.p1.9.m9.1.1.1.cmml" xref="S3.SS4.p1.9.m9.1.1.1"></times><cn type="integer" id="S3.SS4.p1.9.m9.1.1.2.cmml" xref="S3.SS4.p1.9.m9.1.1.2">388</cn><cn type="integer" id="S3.SS4.p1.9.m9.1.1.3.cmml" xref="S3.SS4.p1.9.m9.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.9.m9.1c">388\times 288</annotation></semantics></math> input image size respectively.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.3" class="ltx_p">As in Â <cite class="ltx_cite ltx_citemacro_citep">(Xiao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2018</a>; Fang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2017</a>)</cite>, we also choose Mean-Squared Error (MSE) as the loss function. The MSE loss function is given as:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\mathcal{L}_{\mathrm{mse}}=\frac{1}{K}\sum_{k=1}^{K}\left\|\boldsymbol{H}_{k}-\hat{\boldsymbol{H}}_{k}\right\|_{2}^{2}," display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.3.2.cmml">â„’</mi><mi id="S3.E2.m1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.3.3.cmml">mse</mi></msub><mo id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml"><mfrac id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.3.cmml"><mn id="S3.E2.m1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.3.2.cmml">1</mn><mi id="S3.E2.m1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.3.3.cmml">K</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><munderover id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.1.1.1.1.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.2.2.3" xref="S3.E2.m1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.2.2.3.2" xref="S3.E2.m1.1.1.1.1.1.1.2.2.3.2.cmml">k</mi><mo id="S3.E2.m1.1.1.1.1.1.1.2.2.3.1" xref="S3.E2.m1.1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E2.m1.1.1.1.1.1.1.2.2.3.3" xref="S3.E2.m1.1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E2.m1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.2.3.cmml">K</mi></munderover><msubsup id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml"><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.1.cmml">â€–</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">ğ‘¯</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">k</mi></msub><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml">ğ‘¯</mi><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml">^</mo></mover><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">k</mi></msub></mrow><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.1.cmml">â€–</mo></mrow><mn id="S3.E2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml">2</mn><mn id="S3.E2.m1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.3.cmml">2</mn></msubsup></mrow></mrow></mrow><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"></eq><apply id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2">â„’</ci><ci id="S3.E2.m1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3">mse</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2"></times><apply id="S3.E2.m1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3"><divide id="S3.E2.m1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.3"></divide><cn type="integer" id="S3.E2.m1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.3.2">1</cn><ci id="S3.E2.m1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3.3">ğ¾</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><apply id="S3.E2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S3.E2.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.2.2"></sum><apply id="S3.E2.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.2.3"><eq id="S3.E2.m1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E2.m1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.2.3.2">ğ‘˜</ci><cn type="integer" id="S3.E2.m1.1.1.1.1.1.1.2.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E2.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.3">ğ¾</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1"><minus id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.2">ğ‘¯</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.3">ğ‘˜</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2"><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.1">^</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.2">ğ‘¯</ci></apply><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.3">ğ‘˜</ci></apply></apply></apply><cn type="integer" id="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3">2</cn></apply><cn type="integer" id="S3.E2.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\mathcal{L}_{\mathrm{mse}}=\frac{1}{K}\sum_{k=1}^{K}\left\|\boldsymbol{H}_{k}-\hat{\boldsymbol{H}}_{k}\right\|_{2}^{2},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS4.p2.2" class="ltx_p">where <math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="\hat{H}_{k}" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><msub id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml"><mover accent="true" id="S3.SS4.p2.1.m1.1.1.2" xref="S3.SS4.p2.1.m1.1.1.2.cmml"><mi id="S3.SS4.p2.1.m1.1.1.2.2" xref="S3.SS4.p2.1.m1.1.1.2.2.cmml">H</mi><mo id="S3.SS4.p2.1.m1.1.1.2.1" xref="S3.SS4.p2.1.m1.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS4.p2.1.m1.1.1.3" xref="S3.SS4.p2.1.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><apply id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.1.m1.1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">subscript</csymbol><apply id="S3.SS4.p2.1.m1.1.1.2.cmml" xref="S3.SS4.p2.1.m1.1.1.2"><ci id="S3.SS4.p2.1.m1.1.1.2.1.cmml" xref="S3.SS4.p2.1.m1.1.1.2.1">^</ci><ci id="S3.SS4.p2.1.m1.1.1.2.2.cmml" xref="S3.SS4.p2.1.m1.1.1.2.2">ğ»</ci></apply><ci id="S3.SS4.p2.1.m1.1.1.3.cmml" xref="S3.SS4.p2.1.m1.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">\hat{H}_{k}</annotation></semantics></math> refers to the predicted heatmap for the <math id="S3.SS4.p2.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS4.p2.2.m2.1a"><mi id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><ci id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">k</annotation></semantics></math>-th joint.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Experiment Setup</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Dataset.</span>Â <em id="S4.SS1.p1.1.2" class="ltx_emph ltx_font_italic">MS COCO</em> keypoint datasetÂ <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2014</a>)</cite> has more than 200k images in the wild, where more than 250k person instances are labeled with 17 human joints. We train our networks on the COCO 2017 training dataset, which has 57k images and 150k labeled person instances, no extra data involved. We evaluate our networks on the val2017 set (5k images) and test-dev2017 set (20k images) respectively.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.5" class="ltx_p"><span id="S4.SS1.p2.5.1" class="ltx_text ltx_font_bold">Performance Metrics.</span>
The OKS-based average precision and recall scores<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://cocodataset.org/#keypoints-eval.</span></span></span> are used to evaluate the accuracy of keypoint localization. The object keypoint similarity (OKS) is defined as the distance between the predicted points and the ground-truth normalized by scale of the person. The mean average precision (AP) over 10 OKS thresholds is used as the main evaluation metric. In our experiments, we report AP (the mean of AP scores at OKS = 0.50, 0.55, â€¦, 0.90, 0.95), AP<sup id="S4.SS1.p2.5.2" class="ltx_sup"><span id="S4.SS1.p2.5.2.1" class="ltx_text ltx_font_italic">50</span></sup> (AP at OKS = 0.50), AP<sup id="S4.SS1.p2.5.3" class="ltx_sup"><span id="S4.SS1.p2.5.3.1" class="ltx_text ltx_font_italic">75</span></sup>, AP<sup id="S4.SS1.p2.5.4" class="ltx_sup"><span id="S4.SS1.p2.5.4.1" class="ltx_text ltx_font_italic">M</span></sup> for medium objects, AP<sup id="S4.SS1.p2.5.5" class="ltx_sup"><span id="S4.SS1.p2.5.5.1" class="ltx_text ltx_font_italic">L</span></sup> for large objects, and AR (the mean of recalls).</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Implementation Details.</span> We implemented our networks via PyTorchÂ <cite class="ltx_cite ltx_citemacro_citep">(Paszke
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2019</a>)</cite> deep learning framework. ResNetÂ <cite class="ltx_cite ltx_citemacro_citep">(He
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2016</a>)</cite> pre-trained on ImageNet is used to construct the encoder part in our network. The base learning rate is 1e-3, droped by 0.1 at step 90 and 120, with 140 epochs in total. We utilize Adam optimizer for training. Data augmentation is a very important training strategy, we adopt random horizontal flip, random rotation (-40 to +40) degrees and random scale (0.7 to 1.3).</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Ablation Study</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We first study the effectiveness of each component used to extend SBN, including the two more decoder modules appended to SBN (FR-SBN), the GCBs used to enhance encoders and decoders, and our SA-MFCD module. All versions of networks use ResNet34 to build their encoder part. We evaluate these versions of networks on the COCO val2017 set, all results are obtained over the input size of <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><times id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">256</cn><cn type="integer" id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">256\times 192</annotation></semantics></math> and the same data augmentation strategies.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Comparison of networks with different additional components</figcaption>
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Method</th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">AP</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<td id="S4.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">SBN-34</td>
<td id="S4.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">69.8</td>
</tr>
<tr id="S4.T1.1.3.2" class="ltx_tr">
<td id="S4.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">FR-SBN</td>
<td id="S4.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">71.0</td>
</tr>
<tr id="S4.T1.1.4.3" class="ltx_tr">
<td id="S4.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">FR-SBN+GCB</td>
<td id="S4.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">71.6</td>
</tr>
<tr id="S4.T1.1.5.4" class="ltx_tr">
<td id="S4.T1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">FR-SBN+GCB+skip</td>
<td id="S4.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">71.7</td>
</tr>
<tr id="S4.T1.1.6.5" class="ltx_tr">
<td id="S4.T1.1.6.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">FR-SBN+GCB+SA-MFCD</td>
<td id="S4.T1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.1.6.5.2.1" class="ltx_text ltx_font_bold">72.5</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">TableÂ <a href="#S4.T1" title="Table 1 â€£ 4.2. Ablation Study â€£ 4. Experiments â€£ Full-Resolution Encoderâ€“Decoder Networks with Multi-Scale Feature Fusion for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the comparison results in detail. Our re-trained SBN-34 achieved an AP of 69.8. Comparing FR-SBN with SBN-34 (71.0 vs. 69.8), it can be seen that increase of the output resolution effectively improves AP by 1.2 points, which may due to the reduced quantization error. It can also be found that GCB can improve 0.6 AP points (71.6 vs. 71.0) over FR-SBN. SA-MFCD can further improve AP by 0.9 points (72.5 vs. 71.6), while the vanilla skip-connection (FR-SBN+GCB+skip) can only improve AP by 0.1 points (71.7 vs. 71.6). In total, our network (FR-SBN+GCB+SA-MFCD) gets 2.7 AP point improvements over the baseline (SBN-34).</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Comparison with the state of the art methods</h3>

<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>Comparisons on the COCO validation set. OHKM = online
hard keypoints mining</figcaption>
<table id="S4.T2.12" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.12.13.1" class="ltx_tr">
<td id="S4.T2.12.13.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Method</td>
<td id="S4.T2.12.13.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Backbone</td>
<td id="S4.T2.12.13.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Input size</td>
<td id="S4.T2.12.13.1.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">AP</td>
<td id="S4.T2.12.13.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">#Params</td>
<td id="S4.T2.12.13.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">GFLOPs</td>
<td id="S4.T2.12.13.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Time</td>
</tr>
<tr id="S4.T2.1.1" class="ltx_tr">
<td id="S4.T2.1.1.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt">HourglassÂ <cite class="ltx_cite ltx_citemacro_citep">(Newell
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite>
</td>
<td id="S4.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">-</td>
<td id="S4.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T2.1.1.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S4.T2.1.1.1.m1.1a"><mrow id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml"><mn id="S4.T2.1.1.1.m1.1.1.2" xref="S4.T2.1.1.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T2.1.1.1.m1.1.1.1" xref="S4.T2.1.1.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T2.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1"><times id="S4.T2.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1.1"></times><cn type="integer" id="S4.T2.1.1.1.m1.1.1.2.cmml" xref="S4.T2.1.1.1.m1.1.1.2">256</cn><cn type="integer" id="S4.T2.1.1.1.m1.1.1.3.cmml" xref="S4.T2.1.1.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">256\times 192</annotation></semantics></math></td>
<td id="S4.T2.1.1.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">66.9</td>
<td id="S4.T2.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">25M</td>
<td id="S4.T2.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">14.3G</td>
<td id="S4.T2.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">-</td>
</tr>
<tr id="S4.T2.2.2" class="ltx_tr">
<td id="S4.T2.2.2.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">CPNÂ <cite class="ltx_cite ltx_citemacro_citep">(Chen
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S4.T2.2.2.3" class="ltx_td ltx_align_center ltx_border_r">Resnet50</td>
<td id="S4.T2.2.2.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T2.2.2.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S4.T2.2.2.1.m1.1a"><mrow id="S4.T2.2.2.1.m1.1.1" xref="S4.T2.2.2.1.m1.1.1.cmml"><mn id="S4.T2.2.2.1.m1.1.1.2" xref="S4.T2.2.2.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T2.2.2.1.m1.1.1.1" xref="S4.T2.2.2.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T2.2.2.1.m1.1.1.3" xref="S4.T2.2.2.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.1.m1.1b"><apply id="S4.T2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.1.m1.1.1"><times id="S4.T2.2.2.1.m1.1.1.1.cmml" xref="S4.T2.2.2.1.m1.1.1.1"></times><cn type="integer" id="S4.T2.2.2.1.m1.1.1.2.cmml" xref="S4.T2.2.2.1.m1.1.1.2">256</cn><cn type="integer" id="S4.T2.2.2.1.m1.1.1.3.cmml" xref="S4.T2.2.2.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.1.m1.1c">256\times 192</annotation></semantics></math></td>
<td id="S4.T2.2.2.4" class="ltx_td ltx_align_center ltx_border_rr">68.7</td>
<td id="S4.T2.2.2.5" class="ltx_td ltx_align_center ltx_border_r">27M</td>
<td id="S4.T2.2.2.6" class="ltx_td ltx_align_center ltx_border_r">6.2G</td>
<td id="S4.T2.2.2.7" class="ltx_td ltx_align_center ltx_border_r">-</td>
</tr>
<tr id="S4.T2.3.3" class="ltx_tr">
<td id="S4.T2.3.3.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">CPN+OHKMÂ <cite class="ltx_cite ltx_citemacro_citep">(Chen
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S4.T2.3.3.3" class="ltx_td ltx_align_center ltx_border_r">Resnet50</td>
<td id="S4.T2.3.3.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T2.3.3.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S4.T2.3.3.1.m1.1a"><mrow id="S4.T2.3.3.1.m1.1.1" xref="S4.T2.3.3.1.m1.1.1.cmml"><mn id="S4.T2.3.3.1.m1.1.1.2" xref="S4.T2.3.3.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T2.3.3.1.m1.1.1.1" xref="S4.T2.3.3.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T2.3.3.1.m1.1.1.3" xref="S4.T2.3.3.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.1.m1.1b"><apply id="S4.T2.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.1.m1.1.1"><times id="S4.T2.3.3.1.m1.1.1.1.cmml" xref="S4.T2.3.3.1.m1.1.1.1"></times><cn type="integer" id="S4.T2.3.3.1.m1.1.1.2.cmml" xref="S4.T2.3.3.1.m1.1.1.2">256</cn><cn type="integer" id="S4.T2.3.3.1.m1.1.1.3.cmml" xref="S4.T2.3.3.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.1.m1.1c">256\times 192</annotation></semantics></math></td>
<td id="S4.T2.3.3.4" class="ltx_td ltx_align_center ltx_border_rr">69.4</td>
<td id="S4.T2.3.3.5" class="ltx_td ltx_align_center ltx_border_r">27M</td>
<td id="S4.T2.3.3.6" class="ltx_td ltx_align_center ltx_border_r">6.2G</td>
<td id="S4.T2.3.3.7" class="ltx_td ltx_align_center ltx_border_r">-</td>
</tr>
<tr id="S4.T2.4.4" class="ltx_tr">
<td id="S4.T2.4.4.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">PoseFixÂ <cite class="ltx_cite ltx_citemacro_citep">(Moon
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S4.T2.4.4.3" class="ltx_td ltx_align_center ltx_border_r">CPN</td>
<td id="S4.T2.4.4.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T2.4.4.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S4.T2.4.4.1.m1.1a"><mrow id="S4.T2.4.4.1.m1.1.1" xref="S4.T2.4.4.1.m1.1.1.cmml"><mn id="S4.T2.4.4.1.m1.1.1.2" xref="S4.T2.4.4.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T2.4.4.1.m1.1.1.1" xref="S4.T2.4.4.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T2.4.4.1.m1.1.1.3" xref="S4.T2.4.4.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.1.m1.1b"><apply id="S4.T2.4.4.1.m1.1.1.cmml" xref="S4.T2.4.4.1.m1.1.1"><times id="S4.T2.4.4.1.m1.1.1.1.cmml" xref="S4.T2.4.4.1.m1.1.1.1"></times><cn type="integer" id="S4.T2.4.4.1.m1.1.1.2.cmml" xref="S4.T2.4.4.1.m1.1.1.2">256</cn><cn type="integer" id="S4.T2.4.4.1.m1.1.1.3.cmml" xref="S4.T2.4.4.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.1.m1.1c">256\times 192</annotation></semantics></math></td>
<td id="S4.T2.4.4.4" class="ltx_td ltx_align_center ltx_border_rr">72.1</td>
<td id="S4.T2.4.4.5" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.4.4.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.4.4.7" class="ltx_td ltx_align_center ltx_border_r">-</td>
</tr>
<tr id="S4.T2.5.5" class="ltx_tr">
<td id="S4.T2.5.5.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">SBN-50Â <cite class="ltx_cite ltx_citemacro_citep">(Xiao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S4.T2.5.5.3" class="ltx_td ltx_align_center ltx_border_r">Resnet50</td>
<td id="S4.T2.5.5.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T2.5.5.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S4.T2.5.5.1.m1.1a"><mrow id="S4.T2.5.5.1.m1.1.1" xref="S4.T2.5.5.1.m1.1.1.cmml"><mn id="S4.T2.5.5.1.m1.1.1.2" xref="S4.T2.5.5.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T2.5.5.1.m1.1.1.1" xref="S4.T2.5.5.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T2.5.5.1.m1.1.1.3" xref="S4.T2.5.5.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.1.m1.1b"><apply id="S4.T2.5.5.1.m1.1.1.cmml" xref="S4.T2.5.5.1.m1.1.1"><times id="S4.T2.5.5.1.m1.1.1.1.cmml" xref="S4.T2.5.5.1.m1.1.1.1"></times><cn type="integer" id="S4.T2.5.5.1.m1.1.1.2.cmml" xref="S4.T2.5.5.1.m1.1.1.2">256</cn><cn type="integer" id="S4.T2.5.5.1.m1.1.1.3.cmml" xref="S4.T2.5.5.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.1.m1.1c">256\times 192</annotation></semantics></math></td>
<td id="S4.T2.5.5.4" class="ltx_td ltx_align_center ltx_border_rr">70.4</td>
<td id="S4.T2.5.5.5" class="ltx_td ltx_align_center ltx_border_r">34M</td>
<td id="S4.T2.5.5.6" class="ltx_td ltx_align_center ltx_border_r">8.9G</td>
<td id="S4.T2.5.5.7" class="ltx_td ltx_align_center ltx_border_r">2.3ms</td>
</tr>
<tr id="S4.T2.6.6" class="ltx_tr">
<td id="S4.T2.6.6.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">SBN-101Â <cite class="ltx_cite ltx_citemacro_citep">(Xiao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S4.T2.6.6.3" class="ltx_td ltx_align_center ltx_border_r">Resnet101</td>
<td id="S4.T2.6.6.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T2.6.6.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S4.T2.6.6.1.m1.1a"><mrow id="S4.T2.6.6.1.m1.1.1" xref="S4.T2.6.6.1.m1.1.1.cmml"><mn id="S4.T2.6.6.1.m1.1.1.2" xref="S4.T2.6.6.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T2.6.6.1.m1.1.1.1" xref="S4.T2.6.6.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T2.6.6.1.m1.1.1.3" xref="S4.T2.6.6.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.1.m1.1b"><apply id="S4.T2.6.6.1.m1.1.1.cmml" xref="S4.T2.6.6.1.m1.1.1"><times id="S4.T2.6.6.1.m1.1.1.1.cmml" xref="S4.T2.6.6.1.m1.1.1.1"></times><cn type="integer" id="S4.T2.6.6.1.m1.1.1.2.cmml" xref="S4.T2.6.6.1.m1.1.1.2">256</cn><cn type="integer" id="S4.T2.6.6.1.m1.1.1.3.cmml" xref="S4.T2.6.6.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.1.m1.1c">256\times 192</annotation></semantics></math></td>
<td id="S4.T2.6.6.4" class="ltx_td ltx_align_center ltx_border_rr">71.4</td>
<td id="S4.T2.6.6.5" class="ltx_td ltx_align_center ltx_border_r">53M</td>
<td id="S4.T2.6.6.6" class="ltx_td ltx_align_center ltx_border_r">12.4G</td>
<td id="S4.T2.6.6.7" class="ltx_td ltx_align_center ltx_border_r">3.3ms</td>
</tr>
<tr id="S4.T2.7.7" class="ltx_tr">
<td id="S4.T2.7.7.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">SBN-152Â <cite class="ltx_cite ltx_citemacro_citep">(Xiao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S4.T2.7.7.3" class="ltx_td ltx_align_center ltx_border_r">Resnet152</td>
<td id="S4.T2.7.7.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T2.7.7.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S4.T2.7.7.1.m1.1a"><mrow id="S4.T2.7.7.1.m1.1.1" xref="S4.T2.7.7.1.m1.1.1.cmml"><mn id="S4.T2.7.7.1.m1.1.1.2" xref="S4.T2.7.7.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T2.7.7.1.m1.1.1.1" xref="S4.T2.7.7.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T2.7.7.1.m1.1.1.3" xref="S4.T2.7.7.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.1.m1.1b"><apply id="S4.T2.7.7.1.m1.1.1.cmml" xref="S4.T2.7.7.1.m1.1.1"><times id="S4.T2.7.7.1.m1.1.1.1.cmml" xref="S4.T2.7.7.1.m1.1.1.1"></times><cn type="integer" id="S4.T2.7.7.1.m1.1.1.2.cmml" xref="S4.T2.7.7.1.m1.1.1.2">256</cn><cn type="integer" id="S4.T2.7.7.1.m1.1.1.3.cmml" xref="S4.T2.7.7.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.1.m1.1c">256\times 192</annotation></semantics></math></td>
<td id="S4.T2.7.7.4" class="ltx_td ltx_align_center ltx_border_rr">72.0</td>
<td id="S4.T2.7.7.5" class="ltx_td ltx_align_center ltx_border_r">68.6M</td>
<td id="S4.T2.7.7.6" class="ltx_td ltx_align_center ltx_border_r">15.7G</td>
<td id="S4.T2.7.7.7" class="ltx_td ltx_align_center ltx_border_r">4.8ms</td>
</tr>
<tr id="S4.T2.8.8" class="ltx_tr">
<td id="S4.T2.8.8.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.8.8.2.1" class="ltx_text ltx_font_bold">ours</span></td>
<td id="S4.T2.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Resnet34</td>
<td id="S4.T2.8.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.8.8.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S4.T2.8.8.1.m1.1a"><mrow id="S4.T2.8.8.1.m1.1.1" xref="S4.T2.8.8.1.m1.1.1.cmml"><mn id="S4.T2.8.8.1.m1.1.1.2" xref="S4.T2.8.8.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T2.8.8.1.m1.1.1.1" xref="S4.T2.8.8.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T2.8.8.1.m1.1.1.3" xref="S4.T2.8.8.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.8.8.1.m1.1b"><apply id="S4.T2.8.8.1.m1.1.1.cmml" xref="S4.T2.8.8.1.m1.1.1"><times id="S4.T2.8.8.1.m1.1.1.1.cmml" xref="S4.T2.8.8.1.m1.1.1.1"></times><cn type="integer" id="S4.T2.8.8.1.m1.1.1.2.cmml" xref="S4.T2.8.8.1.m1.1.1.2">256</cn><cn type="integer" id="S4.T2.8.8.1.m1.1.1.3.cmml" xref="S4.T2.8.8.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.8.1.m1.1c">256\times 192</annotation></semantics></math></td>
<td id="S4.T2.8.8.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S4.T2.8.8.4.1" class="ltx_text ltx_font_bold">72.5</span></td>
<td id="S4.T2.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">33M</td>
<td id="S4.T2.8.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">21.3G</td>
<td id="S4.T2.8.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.5ms</td>
</tr>
<tr id="S4.T2.9.9" class="ltx_tr">
<td id="S4.T2.9.9.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt">SBN-50Â <cite class="ltx_cite ltx_citemacro_citep">(Xiao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S4.T2.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Resnet50</td>
<td id="S4.T2.9.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S4.T2.9.9.1.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S4.T2.9.9.1.m1.1a"><mrow id="S4.T2.9.9.1.m1.1.1" xref="S4.T2.9.9.1.m1.1.1.cmml"><mn id="S4.T2.9.9.1.m1.1.1.2" xref="S4.T2.9.9.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T2.9.9.1.m1.1.1.1" xref="S4.T2.9.9.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T2.9.9.1.m1.1.1.3" xref="S4.T2.9.9.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.9.9.1.m1.1b"><apply id="S4.T2.9.9.1.m1.1.1.cmml" xref="S4.T2.9.9.1.m1.1.1"><times id="S4.T2.9.9.1.m1.1.1.1.cmml" xref="S4.T2.9.9.1.m1.1.1.1"></times><cn type="integer" id="S4.T2.9.9.1.m1.1.1.2.cmml" xref="S4.T2.9.9.1.m1.1.1.2">384</cn><cn type="integer" id="S4.T2.9.9.1.m1.1.1.3.cmml" xref="S4.T2.9.9.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.9.1.m1.1c">384\times 288</annotation></semantics></math></td>
<td id="S4.T2.9.9.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">72.2</td>
<td id="S4.T2.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">34M</td>
<td id="S4.T2.9.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">20G</td>
<td id="S4.T2.9.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">5.7ms</td>
</tr>
<tr id="S4.T2.10.10" class="ltx_tr">
<td id="S4.T2.10.10.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">SBN-101Â <cite class="ltx_cite ltx_citemacro_citep">(Xiao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S4.T2.10.10.3" class="ltx_td ltx_align_center ltx_border_r">Resnet101</td>
<td id="S4.T2.10.10.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T2.10.10.1.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S4.T2.10.10.1.m1.1a"><mrow id="S4.T2.10.10.1.m1.1.1" xref="S4.T2.10.10.1.m1.1.1.cmml"><mn id="S4.T2.10.10.1.m1.1.1.2" xref="S4.T2.10.10.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T2.10.10.1.m1.1.1.1" xref="S4.T2.10.10.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T2.10.10.1.m1.1.1.3" xref="S4.T2.10.10.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.10.10.1.m1.1b"><apply id="S4.T2.10.10.1.m1.1.1.cmml" xref="S4.T2.10.10.1.m1.1.1"><times id="S4.T2.10.10.1.m1.1.1.1.cmml" xref="S4.T2.10.10.1.m1.1.1.1"></times><cn type="integer" id="S4.T2.10.10.1.m1.1.1.2.cmml" xref="S4.T2.10.10.1.m1.1.1.2">384</cn><cn type="integer" id="S4.T2.10.10.1.m1.1.1.3.cmml" xref="S4.T2.10.10.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.10.1.m1.1c">384\times 288</annotation></semantics></math></td>
<td id="S4.T2.10.10.4" class="ltx_td ltx_align_center ltx_border_rr">73.6</td>
<td id="S4.T2.10.10.5" class="ltx_td ltx_align_center ltx_border_r">53M</td>
<td id="S4.T2.10.10.6" class="ltx_td ltx_align_center ltx_border_r">27.9G</td>
<td id="S4.T2.10.10.7" class="ltx_td ltx_align_center ltx_border_r">8.0ms</td>
</tr>
<tr id="S4.T2.11.11" class="ltx_tr">
<td id="S4.T2.11.11.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">SBN-152Â <cite class="ltx_cite ltx_citemacro_citep">(Xiao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S4.T2.11.11.3" class="ltx_td ltx_align_center ltx_border_r">Resnet152</td>
<td id="S4.T2.11.11.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T2.11.11.1.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S4.T2.11.11.1.m1.1a"><mrow id="S4.T2.11.11.1.m1.1.1" xref="S4.T2.11.11.1.m1.1.1.cmml"><mn id="S4.T2.11.11.1.m1.1.1.2" xref="S4.T2.11.11.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T2.11.11.1.m1.1.1.1" xref="S4.T2.11.11.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T2.11.11.1.m1.1.1.3" xref="S4.T2.11.11.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.11.11.1.m1.1b"><apply id="S4.T2.11.11.1.m1.1.1.cmml" xref="S4.T2.11.11.1.m1.1.1"><times id="S4.T2.11.11.1.m1.1.1.1.cmml" xref="S4.T2.11.11.1.m1.1.1.1"></times><cn type="integer" id="S4.T2.11.11.1.m1.1.1.2.cmml" xref="S4.T2.11.11.1.m1.1.1.2">384</cn><cn type="integer" id="S4.T2.11.11.1.m1.1.1.3.cmml" xref="S4.T2.11.11.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.11.1.m1.1c">384\times 288</annotation></semantics></math></td>
<td id="S4.T2.11.11.4" class="ltx_td ltx_align_center ltx_border_rr"><span id="S4.T2.11.11.4.1" class="ltx_text ltx_font_bold">74.3</span></td>
<td id="S4.T2.11.11.5" class="ltx_td ltx_align_center ltx_border_r">68.6M</td>
<td id="S4.T2.11.11.6" class="ltx_td ltx_align_center ltx_border_r">35.3G</td>
<td id="S4.T2.11.11.7" class="ltx_td ltx_align_center ltx_border_r">11.8ms</td>
</tr>
<tr id="S4.T2.12.12" class="ltx_tr">
<td id="S4.T2.12.12.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.12.12.2.1" class="ltx_text ltx_font_bold">ours</span></td>
<td id="S4.T2.12.12.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Resnet34</td>
<td id="S4.T2.12.12.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S4.T2.12.12.1.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S4.T2.12.12.1.m1.1a"><mrow id="S4.T2.12.12.1.m1.1.1" xref="S4.T2.12.12.1.m1.1.1.cmml"><mn id="S4.T2.12.12.1.m1.1.1.2" xref="S4.T2.12.12.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T2.12.12.1.m1.1.1.1" xref="S4.T2.12.12.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T2.12.12.1.m1.1.1.3" xref="S4.T2.12.12.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.12.12.1.m1.1b"><apply id="S4.T2.12.12.1.m1.1.1.cmml" xref="S4.T2.12.12.1.m1.1.1"><times id="S4.T2.12.12.1.m1.1.1.1.cmml" xref="S4.T2.12.12.1.m1.1.1.1"></times><cn type="integer" id="S4.T2.12.12.1.m1.1.1.2.cmml" xref="S4.T2.12.12.1.m1.1.1.2">384</cn><cn type="integer" id="S4.T2.12.12.1.m1.1.1.3.cmml" xref="S4.T2.12.12.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.12.1.m1.1c">384\times 288</annotation></semantics></math></td>
<td id="S4.T2.12.12.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t">74.2</td>
<td id="S4.T2.12.12.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">33M</td>
<td id="S4.T2.12.12.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">48.0G</td>
<td id="S4.T2.12.12.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">8.6ms</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3. </span>Comparisons on the COCO test-dev set. <sup id="S4.T3.31.1" class="ltx_sup"><span id="S4.T3.31.1.1" class="ltx_text ltx_font_italic">âˆ—</span></sup> means use extra data</figcaption>
<table id="S4.T3.29" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.6.4" class="ltx_tr">
<th id="S4.T3.6.4.5" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Method</th>
<th id="S4.T3.6.4.6" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Backbone</th>
<th id="S4.T3.6.4.7" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Input size</th>
<th id="S4.T3.6.4.8" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">AP</th>
<td id="S4.T3.3.1.1" class="ltx_td ltx_align_center ltx_border_t">AP<sup id="S4.T3.3.1.1.1" class="ltx_sup"><span id="S4.T3.3.1.1.1.1" class="ltx_text ltx_font_italic">50</span></sup>
</td>
<td id="S4.T3.4.2.2" class="ltx_td ltx_align_center ltx_border_t">AP<sup id="S4.T3.4.2.2.1" class="ltx_sup"><span id="S4.T3.4.2.2.1.1" class="ltx_text ltx_font_italic">75</span></sup>
</td>
<td id="S4.T3.5.3.3" class="ltx_td ltx_align_center ltx_border_t">AP<sup id="S4.T3.5.3.3.1" class="ltx_sup"><span id="S4.T3.5.3.3.1.1" class="ltx_text ltx_font_italic">M</span></sup>
</td>
<td id="S4.T3.6.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">AP<sup id="S4.T3.6.4.4.1" class="ltx_sup"><span id="S4.T3.6.4.4.1.1" class="ltx_text ltx_font_italic">L</span></sup>
</td>
<td id="S4.T3.6.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">AR</td>
</tr>
<tr id="S4.T3.29.28.1" class="ltx_tr">
<th id="S4.T3.29.28.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">Mask.Â <cite class="ltx_cite ltx_citemacro_citep">(He
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite>
</th>
<th id="S4.T3.29.28.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">Res.50-FPN</th>
<th id="S4.T3.29.28.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">-</th>
<th id="S4.T3.29.28.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">63.1</th>
<td id="S4.T3.29.28.1.5" class="ltx_td ltx_align_center ltx_border_tt">87.3</td>
<td id="S4.T3.29.28.1.6" class="ltx_td ltx_align_center ltx_border_tt">68.7</td>
<td id="S4.T3.29.28.1.7" class="ltx_td ltx_align_center ltx_border_tt">57.8</td>
<td id="S4.T3.29.28.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">71.4</td>
<td id="S4.T3.29.28.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">-</td>
</tr>
<tr id="S4.T3.7.5" class="ltx_tr">
<th id="S4.T3.7.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Integral.Â <cite class="ltx_cite ltx_citemacro_citep">(Sun
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2018</a>)</cite>
</th>
<th id="S4.T3.7.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Res.101</th>
<th id="S4.T3.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S4.T3.7.5.1.m1.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="S4.T3.7.5.1.m1.1a"><mrow id="S4.T3.7.5.1.m1.1.1" xref="S4.T3.7.5.1.m1.1.1.cmml"><mn id="S4.T3.7.5.1.m1.1.1.2" xref="S4.T3.7.5.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T3.7.5.1.m1.1.1.1" xref="S4.T3.7.5.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T3.7.5.1.m1.1.1.3" xref="S4.T3.7.5.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.7.5.1.m1.1b"><apply id="S4.T3.7.5.1.m1.1.1.cmml" xref="S4.T3.7.5.1.m1.1.1"><times id="S4.T3.7.5.1.m1.1.1.1.cmml" xref="S4.T3.7.5.1.m1.1.1.1"></times><cn type="integer" id="S4.T3.7.5.1.m1.1.1.2.cmml" xref="S4.T3.7.5.1.m1.1.1.2">256</cn><cn type="integer" id="S4.T3.7.5.1.m1.1.1.3.cmml" xref="S4.T3.7.5.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.7.5.1.m1.1c">256\times 256</annotation></semantics></math></th>
<th id="S4.T3.7.5.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">67.8</th>
<td id="S4.T3.7.5.5" class="ltx_td ltx_align_center">88.2</td>
<td id="S4.T3.7.5.6" class="ltx_td ltx_align_center">74.8</td>
<td id="S4.T3.7.5.7" class="ltx_td ltx_align_center">63.9</td>
<td id="S4.T3.7.5.8" class="ltx_td ltx_align_center ltx_border_r">74.0</td>
<td id="S4.T3.7.5.9" class="ltx_td ltx_align_center ltx_border_r">-</td>
</tr>
<tr id="S4.T3.8.6" class="ltx_tr">
<th id="S4.T3.8.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">CSANetÂ <cite class="ltx_cite ltx_citemacro_citep">(Yu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite>
</th>
<th id="S4.T3.8.6.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Res.101</th>
<th id="S4.T3.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S4.T3.8.6.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S4.T3.8.6.1.m1.1a"><mrow id="S4.T3.8.6.1.m1.1.1" xref="S4.T3.8.6.1.m1.1.1.cmml"><mn id="S4.T3.8.6.1.m1.1.1.2" xref="S4.T3.8.6.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T3.8.6.1.m1.1.1.1" xref="S4.T3.8.6.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T3.8.6.1.m1.1.1.3" xref="S4.T3.8.6.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.8.6.1.m1.1b"><apply id="S4.T3.8.6.1.m1.1.1.cmml" xref="S4.T3.8.6.1.m1.1.1"><times id="S4.T3.8.6.1.m1.1.1.1.cmml" xref="S4.T3.8.6.1.m1.1.1.1"></times><cn type="integer" id="S4.T3.8.6.1.m1.1.1.2.cmml" xref="S4.T3.8.6.1.m1.1.1.2">256</cn><cn type="integer" id="S4.T3.8.6.1.m1.1.1.3.cmml" xref="S4.T3.8.6.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.8.6.1.m1.1c">256\times 192</annotation></semantics></math></th>
<th id="S4.T3.8.6.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">72.3</th>
<td id="S4.T3.8.6.5" class="ltx_td ltx_align_center">91.2</td>
<td id="S4.T3.8.6.6" class="ltx_td ltx_align_center">80.2</td>
<td id="S4.T3.8.6.7" class="ltx_td ltx_align_center">69.3</td>
<td id="S4.T3.8.6.8" class="ltx_td ltx_align_center ltx_border_r">77.6</td>
<td id="S4.T3.8.6.9" class="ltx_td ltx_align_center ltx_border_r">79.1</td>
</tr>
<tr id="S4.T3.9.7" class="ltx_tr">
<th id="S4.T3.9.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">CASNetÂ <cite class="ltx_cite ltx_citemacro_citep">(Yu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite>
</th>
<th id="S4.T3.9.7.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Res.152</th>
<th id="S4.T3.9.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S4.T3.9.7.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S4.T3.9.7.1.m1.1a"><mrow id="S4.T3.9.7.1.m1.1.1" xref="S4.T3.9.7.1.m1.1.1.cmml"><mn id="S4.T3.9.7.1.m1.1.1.2" xref="S4.T3.9.7.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T3.9.7.1.m1.1.1.1" xref="S4.T3.9.7.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T3.9.7.1.m1.1.1.3" xref="S4.T3.9.7.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.9.7.1.m1.1b"><apply id="S4.T3.9.7.1.m1.1.1.cmml" xref="S4.T3.9.7.1.m1.1.1"><times id="S4.T3.9.7.1.m1.1.1.1.cmml" xref="S4.T3.9.7.1.m1.1.1.1"></times><cn type="integer" id="S4.T3.9.7.1.m1.1.1.2.cmml" xref="S4.T3.9.7.1.m1.1.1.2">256</cn><cn type="integer" id="S4.T3.9.7.1.m1.1.1.3.cmml" xref="S4.T3.9.7.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.9.7.1.m1.1c">256\times 192</annotation></semantics></math></th>
<th id="S4.T3.9.7.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">72.8</th>
<td id="S4.T3.9.7.5" class="ltx_td ltx_align_center">91.4</td>
<td id="S4.T3.9.7.6" class="ltx_td ltx_align_center">80.9</td>
<td id="S4.T3.9.7.7" class="ltx_td ltx_align_center">69.8</td>
<td id="S4.T3.9.7.8" class="ltx_td ltx_align_center ltx_border_r">78.3</td>
<td id="S4.T3.9.7.9" class="ltx_td ltx_align_center ltx_border_r">79.6</td>
</tr>
<tr id="S4.T3.10.8" class="ltx_tr">
<th id="S4.T3.10.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">UDP<cite class="ltx_cite ltx_citemacro_citep">(Huang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite>
</th>
<th id="S4.T3.10.8.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Res.152</th>
<th id="S4.T3.10.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S4.T3.10.8.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S4.T3.10.8.1.m1.1a"><mrow id="S4.T3.10.8.1.m1.1.1" xref="S4.T3.10.8.1.m1.1.1.cmml"><mn id="S4.T3.10.8.1.m1.1.1.2" xref="S4.T3.10.8.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T3.10.8.1.m1.1.1.1" xref="S4.T3.10.8.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T3.10.8.1.m1.1.1.3" xref="S4.T3.10.8.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.10.8.1.m1.1b"><apply id="S4.T3.10.8.1.m1.1.1.cmml" xref="S4.T3.10.8.1.m1.1.1"><times id="S4.T3.10.8.1.m1.1.1.1.cmml" xref="S4.T3.10.8.1.m1.1.1.1"></times><cn type="integer" id="S4.T3.10.8.1.m1.1.1.2.cmml" xref="S4.T3.10.8.1.m1.1.1.2">256</cn><cn type="integer" id="S4.T3.10.8.1.m1.1.1.3.cmml" xref="S4.T3.10.8.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.10.8.1.m1.1c">256\times 192</annotation></semantics></math></th>
<th id="S4.T3.10.8.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">72.9</th>
<td id="S4.T3.10.8.5" class="ltx_td ltx_align_center">91.6</td>
<td id="S4.T3.10.8.6" class="ltx_td ltx_align_center">80.9</td>
<td id="S4.T3.10.8.7" class="ltx_td ltx_align_center">70.0</td>
<td id="S4.T3.10.8.8" class="ltx_td ltx_align_center ltx_border_r">78.5</td>
<td id="S4.T3.10.8.9" class="ltx_td ltx_align_center ltx_border_r">78.4</td>
</tr>
<tr id="S4.T3.11.9" class="ltx_tr">
<th id="S4.T3.11.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">HRNet<cite class="ltx_cite ltx_citemacro_citep">(Sun
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2019</a>)</cite>
</th>
<th id="S4.T3.11.9.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">HRNet-w48</th>
<th id="S4.T3.11.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S4.T3.11.9.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S4.T3.11.9.1.m1.1a"><mrow id="S4.T3.11.9.1.m1.1.1" xref="S4.T3.11.9.1.m1.1.1.cmml"><mn id="S4.T3.11.9.1.m1.1.1.2" xref="S4.T3.11.9.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T3.11.9.1.m1.1.1.1" xref="S4.T3.11.9.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T3.11.9.1.m1.1.1.3" xref="S4.T3.11.9.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.11.9.1.m1.1b"><apply id="S4.T3.11.9.1.m1.1.1.cmml" xref="S4.T3.11.9.1.m1.1.1"><times id="S4.T3.11.9.1.m1.1.1.1.cmml" xref="S4.T3.11.9.1.m1.1.1.1"></times><cn type="integer" id="S4.T3.11.9.1.m1.1.1.2.cmml" xref="S4.T3.11.9.1.m1.1.1.2">256</cn><cn type="integer" id="S4.T3.11.9.1.m1.1.1.3.cmml" xref="S4.T3.11.9.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.11.9.1.m1.1c">256\times 192</annotation></semantics></math></th>
<th id="S4.T3.11.9.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.11.9.4.1" class="ltx_text ltx_font_bold">74.3</span></th>
<td id="S4.T3.11.9.5" class="ltx_td ltx_align_center"><span id="S4.T3.11.9.5.1" class="ltx_text ltx_font_bold">92.4</span></td>
<td id="S4.T3.11.9.6" class="ltx_td ltx_align_center"><span id="S4.T3.11.9.6.1" class="ltx_text ltx_font_bold">82.6</span></td>
<td id="S4.T3.11.9.7" class="ltx_td ltx_align_center"><span id="S4.T3.11.9.7.1" class="ltx_text ltx_font_bold">71.2</span></td>
<td id="S4.T3.11.9.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.11.9.8.1" class="ltx_text ltx_font_bold">79.6</span></td>
<td id="S4.T3.11.9.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.11.9.9.1" class="ltx_text ltx_font_bold">79.7</span></td>
</tr>
<tr id="S4.T3.12.10" class="ltx_tr">
<th id="S4.T3.12.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.12.10.2.1" class="ltx_text ltx_font_bold">ours</span></th>
<th id="S4.T3.12.10.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Res.34</th>
<th id="S4.T3.12.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><math id="S4.T3.12.10.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S4.T3.12.10.1.m1.1a"><mrow id="S4.T3.12.10.1.m1.1.1" xref="S4.T3.12.10.1.m1.1.1.cmml"><mn id="S4.T3.12.10.1.m1.1.1.2" xref="S4.T3.12.10.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T3.12.10.1.m1.1.1.1" xref="S4.T3.12.10.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T3.12.10.1.m1.1.1.3" xref="S4.T3.12.10.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.12.10.1.m1.1b"><apply id="S4.T3.12.10.1.m1.1.1.cmml" xref="S4.T3.12.10.1.m1.1.1"><times id="S4.T3.12.10.1.m1.1.1.1.cmml" xref="S4.T3.12.10.1.m1.1.1.1"></times><cn type="integer" id="S4.T3.12.10.1.m1.1.1.2.cmml" xref="S4.T3.12.10.1.m1.1.1.2">256</cn><cn type="integer" id="S4.T3.12.10.1.m1.1.1.3.cmml" xref="S4.T3.12.10.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.12.10.1.m1.1c">256\times 192</annotation></semantics></math></th>
<th id="S4.T3.12.10.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">72.0</th>
<td id="S4.T3.12.10.5" class="ltx_td ltx_align_center ltx_border_t">91.1</td>
<td id="S4.T3.12.10.6" class="ltx_td ltx_align_center ltx_border_t">79.9</td>
<td id="S4.T3.12.10.7" class="ltx_td ltx_align_center ltx_border_t">68.9</td>
<td id="S4.T3.12.10.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.7</td>
<td id="S4.T3.12.10.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.5</td>
</tr>
<tr id="S4.T3.13.11" class="ltx_tr">
<th id="S4.T3.13.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S4.T3.13.11.2.1" class="ltx_text ltx_font_bold">ours</span></th>
<th id="S4.T3.13.11.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Res.50</th>
<th id="S4.T3.13.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S4.T3.13.11.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S4.T3.13.11.1.m1.1a"><mrow id="S4.T3.13.11.1.m1.1.1" xref="S4.T3.13.11.1.m1.1.1.cmml"><mn id="S4.T3.13.11.1.m1.1.1.2" xref="S4.T3.13.11.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T3.13.11.1.m1.1.1.1" xref="S4.T3.13.11.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T3.13.11.1.m1.1.1.3" xref="S4.T3.13.11.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.13.11.1.m1.1b"><apply id="S4.T3.13.11.1.m1.1.1.cmml" xref="S4.T3.13.11.1.m1.1.1"><times id="S4.T3.13.11.1.m1.1.1.1.cmml" xref="S4.T3.13.11.1.m1.1.1.1"></times><cn type="integer" id="S4.T3.13.11.1.m1.1.1.2.cmml" xref="S4.T3.13.11.1.m1.1.1.2">256</cn><cn type="integer" id="S4.T3.13.11.1.m1.1.1.3.cmml" xref="S4.T3.13.11.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.13.11.1.m1.1c">256\times 192</annotation></semantics></math></th>
<th id="S4.T3.13.11.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">72.3</th>
<td id="S4.T3.13.11.5" class="ltx_td ltx_align_center">91.3</td>
<td id="S4.T3.13.11.6" class="ltx_td ltx_align_center">80.1</td>
<td id="S4.T3.13.11.7" class="ltx_td ltx_align_center">69.1</td>
<td id="S4.T3.13.11.8" class="ltx_td ltx_align_center ltx_border_r">78.2</td>
<td id="S4.T3.13.11.9" class="ltx_td ltx_align_center ltx_border_r">77.8</td>
</tr>
<tr id="S4.T3.14.12" class="ltx_tr">
<th id="S4.T3.14.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S4.T3.14.12.2.1" class="ltx_text ltx_font_bold">ours</span></th>
<th id="S4.T3.14.12.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Res.101</th>
<th id="S4.T3.14.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S4.T3.14.12.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S4.T3.14.12.1.m1.1a"><mrow id="S4.T3.14.12.1.m1.1.1" xref="S4.T3.14.12.1.m1.1.1.cmml"><mn id="S4.T3.14.12.1.m1.1.1.2" xref="S4.T3.14.12.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T3.14.12.1.m1.1.1.1" xref="S4.T3.14.12.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T3.14.12.1.m1.1.1.3" xref="S4.T3.14.12.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.14.12.1.m1.1b"><apply id="S4.T3.14.12.1.m1.1.1.cmml" xref="S4.T3.14.12.1.m1.1.1"><times id="S4.T3.14.12.1.m1.1.1.1.cmml" xref="S4.T3.14.12.1.m1.1.1.1"></times><cn type="integer" id="S4.T3.14.12.1.m1.1.1.2.cmml" xref="S4.T3.14.12.1.m1.1.1.2">256</cn><cn type="integer" id="S4.T3.14.12.1.m1.1.1.3.cmml" xref="S4.T3.14.12.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.14.12.1.m1.1c">256\times 192</annotation></semantics></math></th>
<th id="S4.T3.14.12.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">72.6</th>
<td id="S4.T3.14.12.5" class="ltx_td ltx_align_center">91.6</td>
<td id="S4.T3.14.12.6" class="ltx_td ltx_align_center">80.5</td>
<td id="S4.T3.14.12.7" class="ltx_td ltx_align_center">69.4</td>
<td id="S4.T3.14.12.8" class="ltx_td ltx_align_center ltx_border_r">78.4</td>
<td id="S4.T3.14.12.9" class="ltx_td ltx_align_center ltx_border_r">78.1</td>
</tr>
<tr id="S4.T3.15.13" class="ltx_tr">
<th id="S4.T3.15.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S4.T3.15.13.2.1" class="ltx_text ltx_font_bold">ours</span></th>
<th id="S4.T3.15.13.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Res.152</th>
<th id="S4.T3.15.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S4.T3.15.13.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S4.T3.15.13.1.m1.1a"><mrow id="S4.T3.15.13.1.m1.1.1" xref="S4.T3.15.13.1.m1.1.1.cmml"><mn id="S4.T3.15.13.1.m1.1.1.2" xref="S4.T3.15.13.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T3.15.13.1.m1.1.1.1" xref="S4.T3.15.13.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T3.15.13.1.m1.1.1.3" xref="S4.T3.15.13.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.15.13.1.m1.1b"><apply id="S4.T3.15.13.1.m1.1.1.cmml" xref="S4.T3.15.13.1.m1.1.1"><times id="S4.T3.15.13.1.m1.1.1.1.cmml" xref="S4.T3.15.13.1.m1.1.1.1"></times><cn type="integer" id="S4.T3.15.13.1.m1.1.1.2.cmml" xref="S4.T3.15.13.1.m1.1.1.2">256</cn><cn type="integer" id="S4.T3.15.13.1.m1.1.1.3.cmml" xref="S4.T3.15.13.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.15.13.1.m1.1c">256\times 192</annotation></semantics></math></th>
<th id="S4.T3.15.13.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">73.3</th>
<td id="S4.T3.15.13.5" class="ltx_td ltx_align_center">91.3</td>
<td id="S4.T3.15.13.6" class="ltx_td ltx_align_center">80.8</td>
<td id="S4.T3.15.13.7" class="ltx_td ltx_align_center">70.1</td>
<td id="S4.T3.15.13.8" class="ltx_td ltx_align_center ltx_border_r">78.9</td>
<td id="S4.T3.15.13.9" class="ltx_td ltx_align_center ltx_border_r">78.8</td>
</tr>
<tr id="S4.T3.16.14" class="ltx_tr">
<th id="S4.T3.16.14.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">CPNÂ <cite class="ltx_cite ltx_citemacro_citep">(Chen
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite>
</th>
<th id="S4.T3.16.14.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">Res.Inc.</th>
<th id="S4.T3.16.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt"><math id="S4.T3.16.14.1.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S4.T3.16.14.1.m1.1a"><mrow id="S4.T3.16.14.1.m1.1.1" xref="S4.T3.16.14.1.m1.1.1.cmml"><mn id="S4.T3.16.14.1.m1.1.1.2" xref="S4.T3.16.14.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T3.16.14.1.m1.1.1.1" xref="S4.T3.16.14.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T3.16.14.1.m1.1.1.3" xref="S4.T3.16.14.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.16.14.1.m1.1b"><apply id="S4.T3.16.14.1.m1.1.1.cmml" xref="S4.T3.16.14.1.m1.1.1"><times id="S4.T3.16.14.1.m1.1.1.1.cmml" xref="S4.T3.16.14.1.m1.1.1.1"></times><cn type="integer" id="S4.T3.16.14.1.m1.1.1.2.cmml" xref="S4.T3.16.14.1.m1.1.1.2">384</cn><cn type="integer" id="S4.T3.16.14.1.m1.1.1.3.cmml" xref="S4.T3.16.14.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.16.14.1.m1.1c">384\times 288</annotation></semantics></math></th>
<th id="S4.T3.16.14.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">72.1</th>
<td id="S4.T3.16.14.5" class="ltx_td ltx_align_center ltx_border_tt">91.4</td>
<td id="S4.T3.16.14.6" class="ltx_td ltx_align_center ltx_border_tt">80.0</td>
<td id="S4.T3.16.14.7" class="ltx_td ltx_align_center ltx_border_tt">68.7</td>
<td id="S4.T3.16.14.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">77.2</td>
<td id="S4.T3.16.14.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">78.5</td>
</tr>
<tr id="S4.T3.29.29.2" class="ltx_tr">
<th id="S4.T3.29.29.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">CFNÂ <cite class="ltx_cite ltx_citemacro_citep">(Huang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2017</a>)</cite>
</th>
<th id="S4.T3.29.29.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">-</th>
<th id="S4.T3.29.29.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">-</th>
<th id="S4.T3.29.29.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">72.6</th>
<td id="S4.T3.29.29.2.5" class="ltx_td ltx_align_center">86.1</td>
<td id="S4.T3.29.29.2.6" class="ltx_td ltx_align_center">69.7</td>
<td id="S4.T3.29.29.2.7" class="ltx_td ltx_align_center">78.3</td>
<td id="S4.T3.29.29.2.8" class="ltx_td ltx_align_center ltx_border_r">64.1</td>
<td id="S4.T3.29.29.2.9" class="ltx_td ltx_align_center ltx_border_r">-</td>
</tr>
<tr id="S4.T3.17.15" class="ltx_tr">
<th id="S4.T3.17.15.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">CPN (ens.)Â <cite class="ltx_cite ltx_citemacro_citep">(Chen
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite>
</th>
<th id="S4.T3.17.15.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Res.Inc.</th>
<th id="S4.T3.17.15.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S4.T3.17.15.1.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S4.T3.17.15.1.m1.1a"><mrow id="S4.T3.17.15.1.m1.1.1" xref="S4.T3.17.15.1.m1.1.1.cmml"><mn id="S4.T3.17.15.1.m1.1.1.2" xref="S4.T3.17.15.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T3.17.15.1.m1.1.1.1" xref="S4.T3.17.15.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T3.17.15.1.m1.1.1.3" xref="S4.T3.17.15.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.17.15.1.m1.1b"><apply id="S4.T3.17.15.1.m1.1.1.cmml" xref="S4.T3.17.15.1.m1.1.1"><times id="S4.T3.17.15.1.m1.1.1.1.cmml" xref="S4.T3.17.15.1.m1.1.1.1"></times><cn type="integer" id="S4.T3.17.15.1.m1.1.1.2.cmml" xref="S4.T3.17.15.1.m1.1.1.2">384</cn><cn type="integer" id="S4.T3.17.15.1.m1.1.1.3.cmml" xref="S4.T3.17.15.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.17.15.1.m1.1c">384\times 288</annotation></semantics></math></th>
<th id="S4.T3.17.15.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">73.0</th>
<td id="S4.T3.17.15.5" class="ltx_td ltx_align_center">91.7</td>
<td id="S4.T3.17.15.6" class="ltx_td ltx_align_center">80.9</td>
<td id="S4.T3.17.15.7" class="ltx_td ltx_align_center">69.5</td>
<td id="S4.T3.17.15.8" class="ltx_td ltx_align_center ltx_border_r">78.1</td>
<td id="S4.T3.17.15.9" class="ltx_td ltx_align_center ltx_border_r">79.0</td>
</tr>
<tr id="S4.T3.18.16" class="ltx_tr">
<th id="S4.T3.18.16.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">SBNÂ <cite class="ltx_cite ltx_citemacro_citep">(Xiao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2018</a>)</cite>
</th>
<th id="S4.T3.18.16.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Res.152</th>
<th id="S4.T3.18.16.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S4.T3.18.16.1.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S4.T3.18.16.1.m1.1a"><mrow id="S4.T3.18.16.1.m1.1.1" xref="S4.T3.18.16.1.m1.1.1.cmml"><mn id="S4.T3.18.16.1.m1.1.1.2" xref="S4.T3.18.16.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T3.18.16.1.m1.1.1.1" xref="S4.T3.18.16.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T3.18.16.1.m1.1.1.3" xref="S4.T3.18.16.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.18.16.1.m1.1b"><apply id="S4.T3.18.16.1.m1.1.1.cmml" xref="S4.T3.18.16.1.m1.1.1"><times id="S4.T3.18.16.1.m1.1.1.1.cmml" xref="S4.T3.18.16.1.m1.1.1.1"></times><cn type="integer" id="S4.T3.18.16.1.m1.1.1.2.cmml" xref="S4.T3.18.16.1.m1.1.1.2">384</cn><cn type="integer" id="S4.T3.18.16.1.m1.1.1.3.cmml" xref="S4.T3.18.16.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.18.16.1.m1.1c">384\times 288</annotation></semantics></math></th>
<th id="S4.T3.18.16.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">73.7</th>
<td id="S4.T3.18.16.5" class="ltx_td ltx_align_center">91.9</td>
<td id="S4.T3.18.16.6" class="ltx_td ltx_align_center">81.1</td>
<td id="S4.T3.18.16.7" class="ltx_td ltx_align_center">70.3</td>
<td id="S4.T3.18.16.8" class="ltx_td ltx_align_center ltx_border_r">80.0</td>
<td id="S4.T3.18.16.9" class="ltx_td ltx_align_center ltx_border_r">79.0</td>
</tr>
<tr id="S4.T3.19.17" class="ltx_tr">
<th id="S4.T3.19.17.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">PoseFixÂ <cite class="ltx_cite ltx_citemacro_citep">(Moon
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite>
</th>
<th id="S4.T3.19.17.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Res.152</th>
<th id="S4.T3.19.17.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S4.T3.19.17.1.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S4.T3.19.17.1.m1.1a"><mrow id="S4.T3.19.17.1.m1.1.1" xref="S4.T3.19.17.1.m1.1.1.cmml"><mn id="S4.T3.19.17.1.m1.1.1.2" xref="S4.T3.19.17.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T3.19.17.1.m1.1.1.1" xref="S4.T3.19.17.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T3.19.17.1.m1.1.1.3" xref="S4.T3.19.17.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.19.17.1.m1.1b"><apply id="S4.T3.19.17.1.m1.1.1.cmml" xref="S4.T3.19.17.1.m1.1.1"><times id="S4.T3.19.17.1.m1.1.1.1.cmml" xref="S4.T3.19.17.1.m1.1.1.1"></times><cn type="integer" id="S4.T3.19.17.1.m1.1.1.2.cmml" xref="S4.T3.19.17.1.m1.1.1.2">384</cn><cn type="integer" id="S4.T3.19.17.1.m1.1.1.3.cmml" xref="S4.T3.19.17.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.19.17.1.m1.1c">384\times 288</annotation></semantics></math></th>
<th id="S4.T3.19.17.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">73.6</th>
<td id="S4.T3.19.17.5" class="ltx_td ltx_align_center">90.8</td>
<td id="S4.T3.19.17.6" class="ltx_td ltx_align_center">81.0</td>
<td id="S4.T3.19.17.7" class="ltx_td ltx_align_center">70.3</td>
<td id="S4.T3.19.17.8" class="ltx_td ltx_align_center ltx_border_r">79.8</td>
<td id="S4.T3.19.17.9" class="ltx_td ltx_align_center ltx_border_r">79.0</td>
</tr>
<tr id="S4.T3.20.18" class="ltx_tr">
<th id="S4.T3.20.18.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">CSANetÂ <cite class="ltx_cite ltx_citemacro_citep">(Yu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite>
</th>
<th id="S4.T3.20.18.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Res.50</th>
<th id="S4.T3.20.18.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S4.T3.20.18.1.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S4.T3.20.18.1.m1.1a"><mrow id="S4.T3.20.18.1.m1.1.1" xref="S4.T3.20.18.1.m1.1.1.cmml"><mn id="S4.T3.20.18.1.m1.1.1.2" xref="S4.T3.20.18.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T3.20.18.1.m1.1.1.1" xref="S4.T3.20.18.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T3.20.18.1.m1.1.1.3" xref="S4.T3.20.18.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.20.18.1.m1.1b"><apply id="S4.T3.20.18.1.m1.1.1.cmml" xref="S4.T3.20.18.1.m1.1.1"><times id="S4.T3.20.18.1.m1.1.1.1.cmml" xref="S4.T3.20.18.1.m1.1.1.1"></times><cn type="integer" id="S4.T3.20.18.1.m1.1.1.2.cmml" xref="S4.T3.20.18.1.m1.1.1.2">384</cn><cn type="integer" id="S4.T3.20.18.1.m1.1.1.3.cmml" xref="S4.T3.20.18.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.20.18.1.m1.1c">384\times 288</annotation></semantics></math></th>
<th id="S4.T3.20.18.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">73.5</th>
<td id="S4.T3.20.18.5" class="ltx_td ltx_align_center">91.4</td>
<td id="S4.T3.20.18.6" class="ltx_td ltx_align_center">80.8</td>
<td id="S4.T3.20.18.7" class="ltx_td ltx_align_center">69.9</td>
<td id="S4.T3.20.18.8" class="ltx_td ltx_align_center ltx_border_r">79.4</td>
<td id="S4.T3.20.18.9" class="ltx_td ltx_align_center ltx_border_r">79.7</td>
</tr>
<tr id="S4.T3.21.19" class="ltx_tr">
<th id="S4.T3.21.19.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">CSANetÂ <cite class="ltx_cite ltx_citemacro_citep">(Yu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite>
</th>
<th id="S4.T3.21.19.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Res.101</th>
<th id="S4.T3.21.19.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S4.T3.21.19.1.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S4.T3.21.19.1.m1.1a"><mrow id="S4.T3.21.19.1.m1.1.1" xref="S4.T3.21.19.1.m1.1.1.cmml"><mn id="S4.T3.21.19.1.m1.1.1.2" xref="S4.T3.21.19.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T3.21.19.1.m1.1.1.1" xref="S4.T3.21.19.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T3.21.19.1.m1.1.1.3" xref="S4.T3.21.19.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.21.19.1.m1.1b"><apply id="S4.T3.21.19.1.m1.1.1.cmml" xref="S4.T3.21.19.1.m1.1.1"><times id="S4.T3.21.19.1.m1.1.1.1.cmml" xref="S4.T3.21.19.1.m1.1.1.1"></times><cn type="integer" id="S4.T3.21.19.1.m1.1.1.2.cmml" xref="S4.T3.21.19.1.m1.1.1.2">384</cn><cn type="integer" id="S4.T3.21.19.1.m1.1.1.3.cmml" xref="S4.T3.21.19.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.21.19.1.m1.1c">384\times 288</annotation></semantics></math></th>
<th id="S4.T3.21.19.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">74.1</th>
<td id="S4.T3.21.19.5" class="ltx_td ltx_align_center">91.6</td>
<td id="S4.T3.21.19.6" class="ltx_td ltx_align_center">81.6</td>
<td id="S4.T3.21.19.7" class="ltx_td ltx_align_center">70.7</td>
<td id="S4.T3.21.19.8" class="ltx_td ltx_align_center ltx_border_r">79.8</td>
<td id="S4.T3.21.19.9" class="ltx_td ltx_align_center ltx_border_r">80.4</td>
</tr>
<tr id="S4.T3.22.20" class="ltx_tr">
<th id="S4.T3.22.20.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">CSANetÂ <cite class="ltx_cite ltx_citemacro_citep">(Yu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite>
</th>
<th id="S4.T3.22.20.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Res.152</th>
<th id="S4.T3.22.20.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S4.T3.22.20.1.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S4.T3.22.20.1.m1.1a"><mrow id="S4.T3.22.20.1.m1.1.1" xref="S4.T3.22.20.1.m1.1.1.cmml"><mn id="S4.T3.22.20.1.m1.1.1.2" xref="S4.T3.22.20.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T3.22.20.1.m1.1.1.1" xref="S4.T3.22.20.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T3.22.20.1.m1.1.1.3" xref="S4.T3.22.20.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.22.20.1.m1.1b"><apply id="S4.T3.22.20.1.m1.1.1.cmml" xref="S4.T3.22.20.1.m1.1.1"><times id="S4.T3.22.20.1.m1.1.1.1.cmml" xref="S4.T3.22.20.1.m1.1.1.1"></times><cn type="integer" id="S4.T3.22.20.1.m1.1.1.2.cmml" xref="S4.T3.22.20.1.m1.1.1.2">384</cn><cn type="integer" id="S4.T3.22.20.1.m1.1.1.3.cmml" xref="S4.T3.22.20.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.22.20.1.m1.1c">384\times 288</annotation></semantics></math></th>
<th id="S4.T3.22.20.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">74.5</th>
<td id="S4.T3.22.20.5" class="ltx_td ltx_align_center">91.7</td>
<td id="S4.T3.22.20.6" class="ltx_td ltx_align_center">82.1</td>
<td id="S4.T3.22.20.7" class="ltx_td ltx_align_center">71.2</td>
<td id="S4.T3.22.20.8" class="ltx_td ltx_align_center ltx_border_r">80.2</td>
<td id="S4.T3.22.20.9" class="ltx_td ltx_align_center ltx_border_r">80.7</td>
</tr>
<tr id="S4.T3.23.21" class="ltx_tr">
<th id="S4.T3.23.21.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">HRNetÂ <cite class="ltx_cite ltx_citemacro_citep">(Sun
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2019</a>)</cite>
</th>
<th id="S4.T3.23.21.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">HRNet-w48</th>
<th id="S4.T3.23.21.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S4.T3.23.21.1.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S4.T3.23.21.1.m1.1a"><mrow id="S4.T3.23.21.1.m1.1.1" xref="S4.T3.23.21.1.m1.1.1.cmml"><mn id="S4.T3.23.21.1.m1.1.1.2" xref="S4.T3.23.21.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T3.23.21.1.m1.1.1.1" xref="S4.T3.23.21.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T3.23.21.1.m1.1.1.3" xref="S4.T3.23.21.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.23.21.1.m1.1b"><apply id="S4.T3.23.21.1.m1.1.1.cmml" xref="S4.T3.23.21.1.m1.1.1"><times id="S4.T3.23.21.1.m1.1.1.1.cmml" xref="S4.T3.23.21.1.m1.1.1.1"></times><cn type="integer" id="S4.T3.23.21.1.m1.1.1.2.cmml" xref="S4.T3.23.21.1.m1.1.1.2">384</cn><cn type="integer" id="S4.T3.23.21.1.m1.1.1.3.cmml" xref="S4.T3.23.21.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.23.21.1.m1.1c">384\times 288</annotation></semantics></math></th>
<th id="S4.T3.23.21.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.23.21.4.1" class="ltx_text ltx_font_bold">75.5</span></th>
<td id="S4.T3.23.21.5" class="ltx_td ltx_align_center"><span id="S4.T3.23.21.5.1" class="ltx_text ltx_font_bold">92.5</span></td>
<td id="S4.T3.23.21.6" class="ltx_td ltx_align_center"><span id="S4.T3.23.21.6.1" class="ltx_text ltx_font_bold">83.3</span></td>
<td id="S4.T3.23.21.7" class="ltx_td ltx_align_center"><span id="S4.T3.23.21.7.1" class="ltx_text ltx_font_bold">71.9</span></td>
<td id="S4.T3.23.21.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.23.21.8.1" class="ltx_text ltx_font_bold">81.5</span></td>
<td id="S4.T3.23.21.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.23.21.9.1" class="ltx_text ltx_font_bold">80.5</span></td>
</tr>
<tr id="S4.T3.24.22" class="ltx_tr">
<th id="S4.T3.24.22.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">UDPÂ <cite class="ltx_cite ltx_citemacro_citep">(Huang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite>
</th>
<th id="S4.T3.24.22.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Res.50</th>
<th id="S4.T3.24.22.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S4.T3.24.22.1.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S4.T3.24.22.1.m1.1a"><mrow id="S4.T3.24.22.1.m1.1.1" xref="S4.T3.24.22.1.m1.1.1.cmml"><mn id="S4.T3.24.22.1.m1.1.1.2" xref="S4.T3.24.22.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T3.24.22.1.m1.1.1.1" xref="S4.T3.24.22.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T3.24.22.1.m1.1.1.3" xref="S4.T3.24.22.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.24.22.1.m1.1b"><apply id="S4.T3.24.22.1.m1.1.1.cmml" xref="S4.T3.24.22.1.m1.1.1"><times id="S4.T3.24.22.1.m1.1.1.1.cmml" xref="S4.T3.24.22.1.m1.1.1.1"></times><cn type="integer" id="S4.T3.24.22.1.m1.1.1.2.cmml" xref="S4.T3.24.22.1.m1.1.1.2">384</cn><cn type="integer" id="S4.T3.24.22.1.m1.1.1.3.cmml" xref="S4.T3.24.22.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.24.22.1.m1.1c">384\times 288</annotation></semantics></math></th>
<th id="S4.T3.24.22.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">72.5</th>
<td id="S4.T3.24.22.5" class="ltx_td ltx_align_center">91.1</td>
<td id="S4.T3.24.22.6" class="ltx_td ltx_align_center">79.7</td>
<td id="S4.T3.24.22.7" class="ltx_td ltx_align_center">68.8</td>
<td id="S4.T3.24.22.8" class="ltx_td ltx_align_center ltx_border_r">79.1</td>
<td id="S4.T3.24.22.9" class="ltx_td ltx_align_center ltx_border_r">77.9</td>
</tr>
<tr id="S4.T3.25.23" class="ltx_tr">
<th id="S4.T3.25.23.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">UDPÂ <cite class="ltx_cite ltx_citemacro_citep">(Huang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite>
</th>
<th id="S4.T3.25.23.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Res.152</th>
<th id="S4.T3.25.23.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S4.T3.25.23.1.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S4.T3.25.23.1.m1.1a"><mrow id="S4.T3.25.23.1.m1.1.1" xref="S4.T3.25.23.1.m1.1.1.cmml"><mn id="S4.T3.25.23.1.m1.1.1.2" xref="S4.T3.25.23.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T3.25.23.1.m1.1.1.1" xref="S4.T3.25.23.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T3.25.23.1.m1.1.1.3" xref="S4.T3.25.23.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.25.23.1.m1.1b"><apply id="S4.T3.25.23.1.m1.1.1.cmml" xref="S4.T3.25.23.1.m1.1.1"><times id="S4.T3.25.23.1.m1.1.1.1.cmml" xref="S4.T3.25.23.1.m1.1.1.1"></times><cn type="integer" id="S4.T3.25.23.1.m1.1.1.2.cmml" xref="S4.T3.25.23.1.m1.1.1.2">384</cn><cn type="integer" id="S4.T3.25.23.1.m1.1.1.3.cmml" xref="S4.T3.25.23.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.25.23.1.m1.1c">384\times 288</annotation></semantics></math></th>
<th id="S4.T3.25.23.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">74.7</th>
<td id="S4.T3.25.23.5" class="ltx_td ltx_align_center">91.8</td>
<td id="S4.T3.25.23.6" class="ltx_td ltx_align_center">82.1</td>
<td id="S4.T3.25.23.7" class="ltx_td ltx_align_center">71.5</td>
<td id="S4.T3.25.23.8" class="ltx_td ltx_align_center ltx_border_r">80.8</td>
<td id="S4.T3.25.23.9" class="ltx_td ltx_align_center ltx_border_r">80.0</td>
</tr>
<tr id="S4.T3.26.24" class="ltx_tr">
<th id="S4.T3.26.24.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.26.24.2.1" class="ltx_text ltx_font_bold">ours</span></th>
<th id="S4.T3.26.24.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Res.34</th>
<th id="S4.T3.26.24.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><math id="S4.T3.26.24.1.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S4.T3.26.24.1.m1.1a"><mrow id="S4.T3.26.24.1.m1.1.1" xref="S4.T3.26.24.1.m1.1.1.cmml"><mn id="S4.T3.26.24.1.m1.1.1.2" xref="S4.T3.26.24.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T3.26.24.1.m1.1.1.1" xref="S4.T3.26.24.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T3.26.24.1.m1.1.1.3" xref="S4.T3.26.24.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.26.24.1.m1.1b"><apply id="S4.T3.26.24.1.m1.1.1.cmml" xref="S4.T3.26.24.1.m1.1.1"><times id="S4.T3.26.24.1.m1.1.1.1.cmml" xref="S4.T3.26.24.1.m1.1.1.1"></times><cn type="integer" id="S4.T3.26.24.1.m1.1.1.2.cmml" xref="S4.T3.26.24.1.m1.1.1.2">384</cn><cn type="integer" id="S4.T3.26.24.1.m1.1.1.3.cmml" xref="S4.T3.26.24.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.26.24.1.m1.1c">384\times 288</annotation></semantics></math></th>
<th id="S4.T3.26.24.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">73.7</th>
<td id="S4.T3.26.24.5" class="ltx_td ltx_align_center ltx_border_t">91.7</td>
<td id="S4.T3.26.24.6" class="ltx_td ltx_align_center ltx_border_t">81.1</td>
<td id="S4.T3.26.24.7" class="ltx_td ltx_align_center ltx_border_t">70.4</td>
<td id="S4.T3.26.24.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">79.7</td>
<td id="S4.T3.26.24.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78.9</td>
</tr>
<tr id="S4.T3.27.25" class="ltx_tr">
<th id="S4.T3.27.25.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S4.T3.27.25.2.1" class="ltx_text ltx_font_bold">ours</span></th>
<th id="S4.T3.27.25.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Res.50</th>
<th id="S4.T3.27.25.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S4.T3.27.25.1.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S4.T3.27.25.1.m1.1a"><mrow id="S4.T3.27.25.1.m1.1.1" xref="S4.T3.27.25.1.m1.1.1.cmml"><mn id="S4.T3.27.25.1.m1.1.1.2" xref="S4.T3.27.25.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T3.27.25.1.m1.1.1.1" xref="S4.T3.27.25.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T3.27.25.1.m1.1.1.3" xref="S4.T3.27.25.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.27.25.1.m1.1b"><apply id="S4.T3.27.25.1.m1.1.1.cmml" xref="S4.T3.27.25.1.m1.1.1"><times id="S4.T3.27.25.1.m1.1.1.1.cmml" xref="S4.T3.27.25.1.m1.1.1.1"></times><cn type="integer" id="S4.T3.27.25.1.m1.1.1.2.cmml" xref="S4.T3.27.25.1.m1.1.1.2">384</cn><cn type="integer" id="S4.T3.27.25.1.m1.1.1.3.cmml" xref="S4.T3.27.25.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.27.25.1.m1.1c">384\times 288</annotation></semantics></math></th>
<th id="S4.T3.27.25.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">73.9</th>
<td id="S4.T3.27.25.5" class="ltx_td ltx_align_center">91.7</td>
<td id="S4.T3.27.25.6" class="ltx_td ltx_align_center">81.3</td>
<td id="S4.T3.27.25.7" class="ltx_td ltx_align_center">70.4</td>
<td id="S4.T3.27.25.8" class="ltx_td ltx_align_center ltx_border_r">80.0</td>
<td id="S4.T3.27.25.9" class="ltx_td ltx_align_center ltx_border_r">79.0</td>
</tr>
<tr id="S4.T3.28.26" class="ltx_tr">
<th id="S4.T3.28.26.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S4.T3.28.26.2.1" class="ltx_text ltx_font_bold">ours</span></th>
<th id="S4.T3.28.26.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Res.101</th>
<th id="S4.T3.28.26.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S4.T3.28.26.1.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S4.T3.28.26.1.m1.1a"><mrow id="S4.T3.28.26.1.m1.1.1" xref="S4.T3.28.26.1.m1.1.1.cmml"><mn id="S4.T3.28.26.1.m1.1.1.2" xref="S4.T3.28.26.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T3.28.26.1.m1.1.1.1" xref="S4.T3.28.26.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T3.28.26.1.m1.1.1.3" xref="S4.T3.28.26.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.28.26.1.m1.1b"><apply id="S4.T3.28.26.1.m1.1.1.cmml" xref="S4.T3.28.26.1.m1.1.1"><times id="S4.T3.28.26.1.m1.1.1.1.cmml" xref="S4.T3.28.26.1.m1.1.1.1"></times><cn type="integer" id="S4.T3.28.26.1.m1.1.1.2.cmml" xref="S4.T3.28.26.1.m1.1.1.2">384</cn><cn type="integer" id="S4.T3.28.26.1.m1.1.1.3.cmml" xref="S4.T3.28.26.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.28.26.1.m1.1c">384\times 288</annotation></semantics></math></th>
<th id="S4.T3.28.26.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">74.3</th>
<td id="S4.T3.28.26.5" class="ltx_td ltx_align_center">92.0</td>
<td id="S4.T3.28.26.6" class="ltx_td ltx_align_center">82.0</td>
<td id="S4.T3.28.26.7" class="ltx_td ltx_align_center">71.0</td>
<td id="S4.T3.28.26.8" class="ltx_td ltx_align_center ltx_border_r">80.4</td>
<td id="S4.T3.28.26.9" class="ltx_td ltx_align_center ltx_border_r">79.6</td>
</tr>
<tr id="S4.T3.29.27" class="ltx_tr">
<th id="S4.T3.29.27.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r"><span id="S4.T3.29.27.2.1" class="ltx_text ltx_font_bold">ours</span></th>
<th id="S4.T3.29.27.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">Res.152</th>
<th id="S4.T3.29.27.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r"><math id="S4.T3.29.27.1.m1.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S4.T3.29.27.1.m1.1a"><mrow id="S4.T3.29.27.1.m1.1.1" xref="S4.T3.29.27.1.m1.1.1.cmml"><mn id="S4.T3.29.27.1.m1.1.1.2" xref="S4.T3.29.27.1.m1.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T3.29.27.1.m1.1.1.1" xref="S4.T3.29.27.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T3.29.27.1.m1.1.1.3" xref="S4.T3.29.27.1.m1.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.29.27.1.m1.1b"><apply id="S4.T3.29.27.1.m1.1.1.cmml" xref="S4.T3.29.27.1.m1.1.1"><times id="S4.T3.29.27.1.m1.1.1.1.cmml" xref="S4.T3.29.27.1.m1.1.1.1"></times><cn type="integer" id="S4.T3.29.27.1.m1.1.1.2.cmml" xref="S4.T3.29.27.1.m1.1.1.2">384</cn><cn type="integer" id="S4.T3.29.27.1.m1.1.1.3.cmml" xref="S4.T3.29.27.1.m1.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.29.27.1.m1.1c">384\times 288</annotation></semantics></math></th>
<th id="S4.T3.29.27.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">74.9</th>
<td id="S4.T3.29.27.5" class="ltx_td ltx_align_center ltx_border_b">92.2</td>
<td id="S4.T3.29.27.6" class="ltx_td ltx_align_center ltx_border_b">82.4</td>
<td id="S4.T3.29.27.7" class="ltx_td ltx_align_center ltx_border_b">71.6</td>
<td id="S4.T3.29.27.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">80.9</td>
<td id="S4.T3.29.27.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">80.2</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.2" class="ltx_p">Our method is compared to the state-of-the-art methods on the validation and test set with two different input sizes respectively. Most of the compared methods are built on ResNet. PoseFixÂ <cite class="ltx_cite ltx_citemacro_citep">(Moon
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite> uses CPN as the backbone, and UDPÂ <cite class="ltx_cite ltx_citemacro_citep">(Huang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite> uses SBN as the backbone. TableÂ <a href="#S4.T2" title="Table 2 â€£ 4.3. Comparison with the state of the art methods â€£ 4. Experiments â€£ Full-Resolution Encoderâ€“Decoder Networks with Multi-Scale Feature Fusion for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> gives the results on the validation set. It can be found that our method achieves the best results when the input size is <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mn id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p1.1.m1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><times id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">256</cn><cn type="integer" id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">256\times 192</annotation></semantics></math>. When the input size rises to <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mrow id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><mn id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p1.2.m2.1.1.1" xref="S4.SS3.p1.2.m2.1.1.1.cmml">Ã—</mo><mn id="S4.SS3.p1.2.m2.1.1.3" xref="S4.SS3.p1.2.m2.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><times id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS3.p1.2.m2.1.1.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2">384</cn><cn type="integer" id="S4.SS3.p1.2.m2.1.1.3.cmml" xref="S4.SS3.p1.2.m2.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">384\times 288</annotation></semantics></math>, our network with ResNet34 can still achieve almost the same AP of SBN-152 (74.2 vs. 74.3) with only half of parameters and less inference time (8.6ms vs. 11.8ms), but the GFLOPs of our network is higher than that of SBN-152. This is because that our backbone is relatively small and the SA-MFCD module can run in parallel with the encoder-decoder network. The experimental results on the validation dataset indicate our network with ResNet34 can achieve the same accuracy as SBN with ResNet152, but with less inference time.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.2" class="ltx_p">TableÂ <a href="#S4.T3" title="Table 3 â€£ 4.3. Comparison with the state of the art methods â€£ 4. Experiments â€£ Full-Resolution Encoderâ€“Decoder Networks with Multi-Scale Feature Fusion for Human Pose Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> gives the results on test-dev2017 set. When using the input of <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mn id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p2.1.m1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><times id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1"></times><cn type="integer" id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">256</cn><cn type="integer" id="S4.SS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">256\times 192</annotation></semantics></math>, our network outperforms CSANetÂ <cite class="ltx_cite ltx_citemacro_citep">(Yu
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite> and UDP<cite class="ltx_cite ltx_citemacro_citep">(Huang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite> with the same backbone networks. When using the input of <math id="S4.SS3.p2.2.m2.1" class="ltx_Math" alttext="384\times 288" display="inline"><semantics id="S4.SS3.p2.2.m2.1a"><mrow id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml"><mn id="S4.SS3.p2.2.m2.1.1.2" xref="S4.SS3.p2.2.m2.1.1.2.cmml">384</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p2.2.m2.1.1.1" xref="S4.SS3.p2.2.m2.1.1.1.cmml">Ã—</mo><mn id="S4.SS3.p2.2.m2.1.1.3" xref="S4.SS3.p2.2.m2.1.1.3.cmml">288</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><apply id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"><times id="S4.SS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1.1"></times><cn type="integer" id="S4.SS3.p2.2.m2.1.1.2.cmml" xref="S4.SS3.p2.2.m2.1.1.2">384</cn><cn type="integer" id="S4.SS3.p2.2.m2.1.1.3.cmml" xref="S4.SS3.p2.2.m2.1.1.3">288</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">384\times 288</annotation></semantics></math>, our network with ResNet34 outperforms CSANet and UDP with ResNet50. When ResNet50 is used as the backbone, our network is 0.4 points higher than CSANet and 1.4 points higher than UDP. When using ResNet101, our network reaches higher results than CSANet (74.3 vs. 74.1). When ResNet152 is used, our results (74.9) are also better than UDP (74.7) and CSANet (74.5). HRNet is the method outperforming our network at both input size settings, which is a more advanced and elaborate network recently refreshing the state-of-the-art of many computer visual tasks.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conclusions and Future Works</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This paper has proposed an advanced encoder-decoder networks for human pose estimation, which improves the effectiveness of the simple baseline network. To the best of our knowledge, we are the first to develop network with full output resolution for human pose estimation, which can reduce the quantization error and recover more accurate spatial information. Global context blocks are also used to enhance the encoders and decoders. Furthermore, the proposed spatial attention based multi-scale feature collection and distribution module can fuse multi-scale feature information to boost the pose estimation. Our experimental results show that our networks can remarkably improve the prediction accuracy over SBN, and our network with big backbone network can achieve top ranked performance among recent works. In this paper, our network uses ResNet to build the encoder part, and it is worth trying some more advanced backbones in the future.

</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Acknowledgement</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This work was supported in part by the National Natural Science Foundation of China (U20B2063), the Sichuan Science and Technology Program, China (2020YFS0057), and the Fundamental Research Funds for the Central Universities (ZYGX2019Z015).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao
etÂ al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Yue Cao, Jiarui Xu,
Stephen Lin, Fangyun Wei, and
Han Hu. 2019.

</span>
<span class="ltx_bibblock">Gcnet: Non-local networks meet squeeze-excitation
networks and beyond. In <em id="bib.bib2.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
International Conference on Computer Vision Workshops</em>.
0â€“0.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao
etÂ al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Zhe Cao, Tomas Simon,
Shih-En Wei, and Yaser Sheikh.
2017.

</span>
<span class="ltx_bibblock">Realtime multi-person 2d pose estimation using part
affinity fields. In <em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition</em>.
7291â€“7299.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
etÂ al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Yilun Chen, Zhicheng
Wang, Yuxiang Peng, Zhiqiang Zhang,
Gang Yu, and Jian Sun.
2018.

</span>
<span class="ltx_bibblock">Cascaded pyramid network for multi-person pose
estimation. In <em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition</em>. 7103â€“7112.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu
etÂ al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Xiao Chu, Wei Yang,
Wanli Ouyang, Cheng Ma,
AlanÂ L Yuille, and Xiaogang Wang.
2017.

</span>
<span class="ltx_bibblock">Multi-context attention for human pose estimation.
In <em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition</em>. 1831â€“1840.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang
etÂ al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Hao-Shu Fang, Shuqin Xie,
Yu-Wing Tai, and Cewu Lu.
2017.

</span>
<span class="ltx_bibblock">Rmpe: Regional multi-person pose estimation. In
<em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on
Computer Vision</em>. 2334â€“2343.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu
etÂ al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Jun Fu, Jing Liu,
Yuhang Wang, Jin Zhou,
Changyong Wang, and Hanqing Lu.
2019.

</span>
<span class="ltx_bibblock">Stacked deconvolutional network for semantic
segmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Image Processing</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He
etÂ al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Kaiming He, Georgia
Gkioxari, Piotr DollÃ¡r, and Ross
Girshick. 2017.

</span>
<span class="ltx_bibblock">Mask r-cnn. In
<em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on
Computer Vision</em>. 2961â€“2969.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He
etÂ al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu
Zhang, Shaoqing Ren, and Jian Sun.
2016.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition. In
<em id="bib.bib9.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition</em>. 770â€“778.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu etÂ al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Jie Hu, Li Shen, and
Gang Sun. 2018.

</span>
<span class="ltx_bibblock">Squeeze-and-excitation networks. In
<em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition</em>. 7132â€“7141.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang
etÂ al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Junjie Huang, Zheng Zhu,
Feng Guo, and Guan Huang.
2019.

</span>
<span class="ltx_bibblock">The Devil is in the Details: Delving into Unbiased
Data Processing for Human Pose Estimation.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.07524</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang
etÂ al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Shaoli Huang, Mingming
Gong, and Dacheng Tao. 2017.

</span>
<span class="ltx_bibblock">A coarse-fine network for keypoint localization.
In <em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference
on Computer Vision</em>. 3028â€“3037.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Insafutdinov etÂ al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Eldar Insafutdinov,
Mykhaylo Andriluka, Leonid Pishchulin,
Siyu Tang, Evgeny Levinkov,
Bjoern Andres, and Bernt Schiele.
2017.

</span>
<span class="ltx_bibblock">Arttrack: Articulated multi-person tracking in the
wild. In <em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition</em>. 6457â€“6465.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Insafutdinov etÂ al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Eldar Insafutdinov, Leonid
Pishchulin, Bjoern Andres, Mykhaylo
Andriluka, and Bernt Schiele.
2016.

</span>
<span class="ltx_bibblock">Deepercut: A deeper, stronger, and faster
multi-person pose estimation model. In <em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">European
Conference on Computer Vision</em>. Springer, 34â€“50.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Piotr
DollÃ¡r, Ross Girshick, Kaiming He,
Bharath Hariharan, and Serge Belongie.
2017.

</span>
<span class="ltx_bibblock">Feature pyramid networks for object detection. In
<em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition</em>. 2117â€“2125.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael
Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan,
Piotr DollÃ¡r, and CÂ Lawrence
Zitnick. 2014.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context. In
<em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>. Springer,
740â€“755.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu
etÂ al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Zhao Liu, Jianke Zhu,
Jiajun Bu, and Chun Chen.
2015.

</span>
<span class="ltx_bibblock">A survey of human pose estimation: the body parts
parsing based methods.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">Journal of Visual Communication and Image
Representation</em> 32 (2015),
10â€“19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moon
etÂ al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Gyeongsik Moon, JuÂ Yong
Chang, and KyoungÂ Mu Lee.
2019.

</span>
<span class="ltx_bibblock">Posefix: Model-agnostic general human pose
refinement network. In <em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition</em>.
Long Beach, 7773â€“7781.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Newell
etÂ al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Alejandro Newell, Zhiao
Huang, and Jia Deng. 2017.

</span>
<span class="ltx_bibblock">Associative embedding: End-to-end learning for
joint detection and grouping. In <em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">Advances in
Neural Information Processing Systems</em>. 2277â€“2287.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Newell
etÂ al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Alejandro Newell, Kaiyu
Yang, and Jia Deng. 2016.

</span>
<span class="ltx_bibblock">Stacked hourglass networks for human pose
estimation. In <em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">European Conference on Computer
Vision</em>. Springer, 483â€“499.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papandreou etÂ al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
George Papandreou, Tyler
Zhu, Liang-Chieh Chen, Spyros Gidaris,
Jonathan Tompson, and Kevin Murphy.
2018.

</span>
<span class="ltx_bibblock">Personlab: Person pose estimation and instance
segmentation with a bottom-up, part-based, geometric embedding model. In
<em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Computer
Vision</em>. 269â€“286.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papandreou etÂ al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
George Papandreou, Tyler
Zhu, Nori Kanazawa, Alexander Toshev,
Jonathan Tompson, Chris Bregler, and
Kevin Murphy. 2017.

</span>
<span class="ltx_bibblock">Towards accurate multi-person pose estimation in
the wild. In <em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition</em>. 4903â€“4911.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park
etÂ al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Jongchan Park, Sanghyun
Woo, Joon-Young Lee, and InÂ So Kweon.
2018.

</span>
<span class="ltx_bibblock">Bam: Bottleneck attention module.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.06514</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paszke
etÂ al<span id="bib.bib24.3.3.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Adam Paszke, Sam Gross,
Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin,
Natalia Gimelshein, Luca Antiga,
etÂ al<span id="bib.bib24.4.1" class="ltx_text">.</span> 2019.

</span>
<span class="ltx_bibblock">PyTorch: An imperative style, high-performance deep
learning library. In <em id="bib.bib24.5.1" class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems</em>. 8024â€“8035.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pishchulin etÂ al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Leonid Pishchulin, Eldar
Insafutdinov, Siyu Tang, Bjoern Andres,
Mykhaylo Andriluka, PeterÂ V Gehler, and
Bernt Schiele. 2016.

</span>
<span class="ltx_bibblock">Deepcut: Joint subset partition and labeling for
multi person pose estimation. In <em id="bib.bib25.3.1" class="ltx_emph ltx_font_italic">Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition</em>.
4929â€“4937.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren
etÂ al<span id="bib.bib26.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Shaoqing Ren, Kaiming He,
Ross Girshick, and Jian Sun.
2015.

</span>
<span class="ltx_bibblock">Faster r-cnn: Towards real-time object detection
with region proposal networks. In <em id="bib.bib26.3.1" class="ltx_emph ltx_font_italic">Advances in
Neural Information Processing Systems</em>. 91â€“99.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ronneberger
etÂ al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Olaf Ronneberger, Philipp
Fischer, and Thomas Brox.
2015.

</span>
<span class="ltx_bibblock">U-net: Convolutional networks for biomedical image
segmentation. In <em id="bib.bib27.3.1" class="ltx_emph ltx_font_italic">International Conference on
Medical Image Computing and Computer-Assisted Intervention</em>. Springer,
234â€“241.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun
etÂ al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Ke Sun, Bin Xiao,
Dong Liu, and Jingdong Wang.
2019.

</span>
<span class="ltx_bibblock">Deep high-resolution representation learning for
human pose estimation. In <em id="bib.bib28.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition</em>.
5693â€“5703.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun
etÂ al<span id="bib.bib29.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Xiao Sun, Bin Xiao,
Fangyin Wei, Shuang Liang, and
Yichen Wei. 2018.

</span>
<span class="ltx_bibblock">Integral human pose regression. In
<em id="bib.bib29.3.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Computer
Vision</em>. 529â€“545.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani etÂ al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam
Shazeer, Niki Parmar, Jakob Uszkoreit,
Llion Jones, AidanÂ N Gomez,
Åukasz Kaiser, and Illia
Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need. In
<em id="bib.bib30.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em>. 5998â€“6008.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang
etÂ al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Xiaolong Wang, Ross
Girshick, Abhinav Gupta, and Kaiming
He. 2018.

</span>
<span class="ltx_bibblock">Non-local neural networks. In
<em id="bib.bib31.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition</em>. 7794â€“7803.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei
etÂ al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Shih-En Wei, Varun
Ramakrishna, Takeo Kanade, and Yaser
Sheikh. 2016.

</span>
<span class="ltx_bibblock">Convolutional pose machines. In
<em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition</em>. 4724â€“4732.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Woo
etÂ al<span id="bib.bib33.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Sanghyun Woo, Jongchan
Park, Joon-Young Lee, and In
SoÂ Kweon. 2018.

</span>
<span class="ltx_bibblock">Cbam: Convolutional block attention module. In
<em id="bib.bib33.3.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Computer
Vision</em>. 3â€“19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao etÂ al<span id="bib.bib34.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Bin Xiao, Haiping Wu,
and Yichen Wei. 2018.

</span>
<span class="ltx_bibblock">Simple baselines for human pose estimation and
tracking. In <em id="bib.bib34.3.1" class="ltx_emph ltx_font_italic">Proceedings of the European
Conference on Computer Vision</em>. 466â€“481.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu
etÂ al<span id="bib.bib35.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Dongdong Yu, Kai Su,
Xin Geng, and Changhu Wang.
2019.

</span>
<span class="ltx_bibblock">A Context-and-Spatial Aware Network for
Multi-Person Pose Estimation.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.05355</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu
etÂ al<span id="bib.bib36.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Leilei Zhu, Shu Zhan,
and Haiyan Zhang. 2019.

</span>
<span class="ltx_bibblock">Stacked U-shape networks with channel-wise
attention for image super-resolution.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.3.1" class="ltx_emph ltx_font_italic">Neurocomputing</em> 345
(2019), 58â€“66.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2106.00565" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2106.00566" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2106.00566">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2106.00566" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2106.00567" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 14:45:09 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
