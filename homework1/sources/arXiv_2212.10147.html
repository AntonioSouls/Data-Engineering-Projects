<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2212.10147] Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection</title><meta property="og:description" content="Scaling object taxonomies is one of the important steps toward a robust real-world deployment of recognition systems.
We have faced remarkable progress in images since the introduction of the LVIS benchmark.
To continu…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2212.10147">

<!--Generated on Fri Mar  1 10:28:19 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Large Vocabulary Video Object Detection and Tracking">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>
KAIST </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Adobe Research</span></span></span>
<h1 class="ltx_title ltx_title_document">
Bridging Images and Videos: 
<br class="ltx_break">A Simple Learning Framework for Large Vocabulary Video Object Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Sanghyun Woo<sup id="id3.2.id1" class="ltx_sup"><span id="id3.2.id1.1" class="ltx_text ltx_font_italic">1∗</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kwanyong Park<sup id="id4.2.id1" class="ltx_sup"><span id="id4.2.id1.1" class="ltx_text ltx_font_italic">1∗</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Seoung Wug Oh
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">In So Kweon
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Joon-Young Lee
</span><span class="ltx_author_notes">22</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Scaling object taxonomies is one of the important steps toward a robust real-world deployment of recognition systems.
We have faced remarkable progress in images since the introduction of the LVIS benchmark.
To continue this success in videos, a new video benchmark, TAO, was recently presented.
Given the recent encouraging results from both detection and tracking communities, we are interested in marrying those two advances and building a strong large vocabulary video tracker.
However, supervisions in LVIS and TAO are inherently sparse or even missing, posing two new challenges for training the large vocabulary trackers.
First, no tracking supervisions are in LVIS, which leads to inconsistent learning of detection (with LVIS and TAO) and tracking (only with TAO).
Second, the detection supervisions in TAO are partial, which results in catastrophic forgetting of absent LVIS categories during video fine-tuning.
To resolve these challenges, we present a simple but effective learning framework that takes full advantage of all available training data to learn detection and tracking while not losing any LVIS categories to recognize.
With this new learning scheme, we show that consistent improvements of various large vocabulary trackers are capable, setting strong baseline results on the challenging TAO benchmarks.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Large Vocabulary Video Object Detection and Tracking
</div>
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><sup id="footnote1.1" class="ltx_sup">*</sup> Work done during an internship at Adobe Research.</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">A central goal of computer vision is to produce a general-purpose perception system that robustly works in the wild.
Towards this ambitious goal, extending the current short category regime is one of the essential key milestones.
As an initial effort in this direction, the large-scale image benchmark, LVIS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, was introduced and fostered significant progress in developing solid image domain solutions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>, <a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite>.
Recently, a video benchmark, TAO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, calls for a shift from image to video, opening the new task of detecting and tracking large vocabulary objects.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2212.10147/assets/figures/teaser.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="229" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
<span id="S1.F1.3.1" class="ltx_text ltx_font_bold">The Proposed Learning Framework for Large Vocabulary Tracker Training.</span>
While the current learning paradigm learns detection and tracking separately from LVIS and TAO (decoupled), our proposal takes all training data to learn detection and tracking jointly (<span id="S1.F1.4.2" class="ltx_text ltx_font_bold">unified</span>). This is achieved through missing supervision hallucination.
</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">With these new datasets of images and videos, LVIS and TAO, we are interested in building a strong large vocabulary video tracker.
However, as the annotation difficulty between images and videos is even more severe in large vocabulary datasets, the significant gap in dataset scale and label vocabularies naturally exists.
Therefore, pre-training the model on images for learning large vocabularies and then fine-tuning on video for seamless video domain adaptation is a standard learning protocol.
Given this context, can the current advances of large-vocabulary detection and multi-object tracking be successfully unified and tied into a single model?
In particular, we see there are two main challenges for the successful marriage of two streams:
<span id="S1.p2.1.1" class="ltx_text ltx_font_bold">First, no tracking supervisions are in LVIS.</span>
This essentially leads to inconsistent learning of detection (with LVIS and TAO) and tracking (only with TAO), resulting in sub-optimal video feature representations.
<span id="S1.p2.1.2" class="ltx_text ltx_font_bold">Second, detection supervisions in TAO are partial<span id="footnote1a" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span id="footnote1a.1.1.1" class="ltx_text ltx_font_medium">1</span></span><span id="footnote1a.5" class="ltx_text ltx_font_medium">
TAO dataset annotates 482 classes in total, which are the subset of LVIS dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="footnote1a.6.1" class="ltx_text ltx_font_medium">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="footnote1a.7.2" class="ltx_text ltx_font_medium">]</span></cite><span id="footnote1a.8" class="ltx_text ltx_font_medium">, and only 216 classes are in the training set.
</span></span></span></span>.</span>
Thus, catastrophic forgetting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> is inevitable if one naively fine-tunes the LVIS tracker directly on the TAO.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this work, we present simple, effective, and generic methods for hallucinating missing supervisions in each dataset.
Below, we describe the challenges and our solutions in turn.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">First, how can we simulate the tracking supervisions only with images in LVIS?
Given an image, our idea is to apply spatial jittering artifacts to mimic temporal changes in video and form a natural pair for tracking.
Here, we present two new spatial jittering methods.
The first is strong zoom-in/out augmentation, which has a large scale-jittering effect that can effectively simulate the low sampling rate test-time inputs in large vocabulary tracking.
It yields significant performance improvements over the conventional image affine augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib107" title="" class="ltx_ref">107</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite>.
Plus, our findings are in line with recent work that shows a large scale jittering is effective in image detection and segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, and here we examine this observation on video for robust large vocabulary tracker training.
The second is mosaicing augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, which is originally presented for object detection with enriched background.
We extend this augmentation for combining foreground objects in different images in a class-balanced manner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and simulate test-time hard, dense tracking scenarios suitable for “many object” trackers.
We show that both are effective and complementary to each other.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Second, how can we fill the missing detection supervisions in TAO?
The TAO training data partially spans the LVIS categories, and thus the direct fine-tuning of LVIS tracker on TAO causes catastrophic forgetting of absent categories.
A straightforward way to avoid this issue is to learn only the tracking part of the model with TAO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>.
However, this hinders full model training and abandons all the TAO detection labels, limiting the overall performance.
We instead approach this problem by combining the self-training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> with a teacher-student framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite>.
In practice, the teacher and student are identical copies of the LVIS pre-trained model, and we freeze the weights of the teacher during training.
The overall learning pipeline consists of two steps:
First, given an input, we predict pseudo labels using the teacher model.
The idea behind the pseudo labeling is to leverage the past knowledge acquired from LVIS and fill in the missing annotations in TAO.
Second, using the augmented labels, we train the student model with both distillation loss and ordinal detection loss.
Unlike the typical teacher-student schemes used in semi-supervised object detection studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>, we introduce two new adaptations suitable for large vocabulary learning setup.
The first is using <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">soft</span> pseudo labels, i.e., distilling class logits directly, to fire all the student’s classifier weights, rather than using common one-hot (hard) pseudo labels. This is crucial as standard hard pseudo labels tend to bias distillation only toward the frequent class objects due to inherent classifier calibration issue <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>.
The second is to use <span id="S1.p5.1.2" class="ltx_text ltx_font_italic">MSE</span> loss in order to equally impact all the classifier weights <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>, rather than using picky KL-divergence loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.
We found the type of the loss function is also very important for the successful large vocabulary classifier distillation.
Despite the simplicity, we empirically show that the distillation results are greatly improved with these adaptations.
We also show that our proposal works well on the common vocabulary setup, e.g., COCO, and can be easily extended to new class learning scenarios, COCO <math id="S1.p5.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S1.p5.1.m1.1a"><mo stretchy="false" id="S1.p5.1.m1.1.1" xref="S1.p5.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S1.p5.1.m1.1b"><ci id="S1.p5.1.m1.1.1.cmml" xref="S1.p5.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.1.m1.1c">\rightarrow</annotation></semantics></math> YTVIS.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Combining all these proposals together, unified learning of detection and tracking with both LVIS images and TAO videos becomes possible without forgetting any LVIS categories (see Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
Furthermore, we also introduce a new regularization objective, semantic consistency loss.
It aims to prevent the common tracking failure in large vocabulary tracking due to semantic flicker between similar classes.
We study the efficacy of our final framework on the TAO benchmark and achieve new state-of-the-art results.
Our extensive ablation studies confirm that the proposals are generic and effective.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Large vocabulary recognition.</span>
The object categories in natural images follow the Zipfian distribution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, and thus, the large vocabulary recognition is naturally tied with the long-tailed recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>.
Based on this connection, lots of solid approaches are introduced. The existing methods can be roughly categorized into <span id="S2.p1.1.2" class="ltx_text ltx_font_italic">data re-sampling</span> or <span id="S2.p1.1.3" class="ltx_text ltx_font_italic">loss re-weighting</span>.
The data re-sampling methods more often sample data from rare classes to balance the long-tailed training distribution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
The loss re-weighting aims at adjusting the loss of each data instance based on their labels or train-time accumulated statistics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.
Some approaches perform multi-staged training upon these methods, which first pre-train the model in a standard way and then fine-tune using either data re-sampling or loss re-weighting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib84" title="" class="ltx_ref">84</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib80" title="" class="ltx_ref">80</a>, <a href="#bib.bib81" title="" class="ltx_ref">81</a>, <a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite>. Also, there are new approaches based on data augmentations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib97" title="" class="ltx_ref">97</a>, <a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite> or test time calibration <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Apart from all these previous efforts on the image, we study a new video extension of the task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
We show that our proposal is generic and not sensitive to a specific method, data re-sampling or loss re-weighting, in successfully converting the current large vocabulary detectors to large vocabulary trackers.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Multi-object tracking.</span>
Most modern multi object trackers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> follow the tracking-by-detection paradigm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>.
An off-the-shelf object detector is first employed to localize all objects in each frame, and then track association is performed between adjacent frames.
The main difference among existing methods is in how they estimate the similarity between detected objects and previous tracks for the association.
To name a few, Kalman Filter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>, optical flow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>, displacement regression <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, and appearance similarities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib95" title="" class="ltx_ref">95</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib85" title="" class="ltx_ref">85</a>, <a href="#bib.bib102" title="" class="ltx_ref">102</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite> are the representatives.
On the other side, there are also efforts on joining detection and tracking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib103" title="" class="ltx_ref">103</a>, <a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>, and recently by transformer-based architectures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>.
We note that all these methods only focus on a few object categories such as people or vehicles, ignoring the vast majority of objects in the world.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Our work is an early attempt for extending the current short category regime of modern trackers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib82" title="" class="ltx_ref">82</a>, <a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite>.
In this paper, we build our proposal upon the tracking-by-detection paradigm.
We choose the state-of-the-art method, QDTrack <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, which adopts Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> and lightweight embedding head for detection and tracking, respectively.
The tracking is learned through a dense matching between quasi-dense samples on the pair of images and optimized with multiple positive contrastive learning.
Given the state-of-the-art large vocabulary detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> and multi-object tracking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> methods, we primarily investigate the new challenges in developing a strong large vocabulary tracker.</p>
</div>
<div id="S2.p5" class="ltx_para ltx_noindent">
<p id="S2.p5.1" class="ltx_p"><span id="S2.p5.1.1" class="ltx_text ltx_font_bold">Tracking without video annotations.</span>
There is a line of recent research on self-supervised learning for tracking, either using unlabeled videos <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>, <a href="#bib.bib83" title="" class="ltx_ref">83</a>, <a href="#bib.bib79" title="" class="ltx_ref">79</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite> or images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib107" title="" class="ltx_ref">107</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite>.
Our work belongs to the latter category.
Applying random affine augmentation to the original image provides a spatially jittered version, which mimics the temporal changes in the video.
By letting the model find the correspondence between those two images, meaningful tracking supervision can be provided <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite>.
Sio <span id="S2.p5.1.2" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p5.1.2.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib68" title="" class="ltx_ref">68</a><span id="S2.p5.1.2.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> present that an image and any cropped region of it can generate a similar effect.
Zheng <span id="S2.p5.1.3" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p5.1.3.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib104" title="" class="ltx_ref">104</a><span id="S2.p5.1.3.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> extend this idea to incorporate only the foreground objects in cropped regions for stable training.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">In this work, we explore this general idea under a more specific large vocabulary tracking setting.
First, we focus on the fact that conventional motion cues are not applicable for large-vocabulray trackers as the input are temporally distant (1FPS) due to the annotation difficulties and there are severe camera movements in natural videos. This motivate us to train the tracker’s vision feature matching more discriminative. To this end, we present a strong zoom-in/out augmentation that can not only simulate low sampling rate input but also includes large scale-jittering effect <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> which is known to be effective in the image domain vision tasks.
Second, we recast the image mosaicing augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, which was initially proposed for robust object detection with enriched backgrounds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite>, to simulate test-time dense tracking in the large vocabulary setting.
We show that both are complementary in providing discriminative tracking supervisions for this task.</p>
</div>
<div id="S2.p7" class="ltx_para ltx_noindent">
<p id="S2.p7.1" class="ltx_p"><span id="S2.p7.1.1" class="ltx_text ltx_font_bold">Catastrophic forgetting.</span>
The phenomenon wherein neural networks forget how to solve past tasks because of the exposure to new tasks is known as catastrophic forgetting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. It occurs because the model weights that contain important information for the old task are over-written by information relevant to the new one.
While the catastrophic forgetting can occur in various scenarios, many existing efforts are focused on the class incremental learning setup, where it incrementally adds new object categories phase-by-phase, in image classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib89" title="" class="ltx_ref">89</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>.
Also, there are some few approaches tackling incremental object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib105" title="" class="ltx_ref">105</a>, <a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>.</p>
</div>
<div id="S2.p8" class="ltx_para">
<p id="S2.p8.1" class="ltx_p">We target a different setup, transfer learning from image to video without forgetting.
Specifically, we aim to train the model on images covering the entire evaluation categories and then fine-tune it on videos, which partially covers the evaluation categories, without forgetting.
While lots of current video models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib95" title="" class="ltx_ref">95</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> are trained in this way for generic feature learning, the label difference issue between images and videos has been rarely studied and explored.
We study this issue, as this is a practical setup for training large vocabulary trackers using both images and videos.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2212.10147/assets/figures/main_fig.jpg" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="592" height="372" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S2.F2.3.1" class="ltx_text ltx_font_bold">Overview of the proposed learning framework.</span>
The <span id="S2.F2.4.2" class="ltx_text" style="color:#FF0000;">red</span> colored objective functions are generated supervisions with our proposals.
</figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We introduce a general learning framework that allows joint learning of detection and tracking from all training data, LVIS and TAO, for robust large vocabulary tracking.
The overview of our pipeline is shown in Fig. <a href="#S2.F2" title="Figure 2 ‣ 2 Related work ‣ Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
We first present how we can learn tracking from images through zoom-in/out and mosaicing augmentations in Sec. <a href="#S3.SS1" title="3.1 Learn to Track in LVIS ‣ 3 Proposed Method ‣ Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
We then describe how we avoid catastrophic forgetting when the videos for fine-tuning have fewer label vocabularies than pre-trained images in Sec. <a href="#S3.SS2" title="3.2 Learn to Unforget in TAO ‣ 3 Proposed Method ‣ Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.
Finally, we present a new regularization loss term, namely semantic consistency loss, for preventing semantic flicker in Sec. <a href="#S3.SS3" title="3.3 Regularizing Semantic Flickering ‣ 3 Proposed Method ‣ Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Learn to Track in LVIS</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Our approach is straightforward.
An original image and a transformed image with the spatial jittering artifacts can form a natural input pair for tracking.
For the jittering artifacts, we present two new augmentations, zoom-in/out and mosaicing (see Fig. <a href="#S2.F2" title="Figure 2 ‣ 2 Related work ‣ Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).
Note that tracking annotations come for free as we know the exact transformation relationship between the images.
We assign the same unique track-id to the same object in the transformed image.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Strong Zoom-in/out Track.</span>
Due to the annotation difficulty of the large-vocabulary tracking dataset, the train and test time inputs are temporally sparse, i.e., low sampling rate, which naturally results in conventional motion cues not applicable and rather rely on pure vision feature matching.
To make the vision feature matching more discriminative, and to effectively simulate test-time low sampling rate inputs, we present strong zoom-in/out augmentation.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.4" class="ltx_p">It is mainly composed of scaling and cropping operations, which essentially vary the scale and position of the objects.
Specifically, for an image <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="\mathrm{I}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mi mathvariant="normal" id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">I</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">\mathrm{I}</annotation></semantics></math>, we generate a input pair, <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="\mathrm{I_{t}}" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><msub id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml">I</mi><mi mathvariant="normal" id="S3.SS1.p3.2.m2.1.1.3" xref="S3.SS1.p3.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2">I</ci><ci id="S3.SS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3">t</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">\mathrm{I_{t}}</annotation></semantics></math> and <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="\mathrm{I_{t+\tau}}" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><msub id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p3.3.m3.1.1.2" xref="S3.SS1.p3.3.m3.1.1.2.cmml">I</mi><mrow id="S3.SS1.p3.3.m3.1.1.3" xref="S3.SS1.p3.3.m3.1.1.3.cmml"><mi mathvariant="normal" id="S3.SS1.p3.3.m3.1.1.3.2" xref="S3.SS1.p3.3.m3.1.1.3.2.cmml">t</mi><mo id="S3.SS1.p3.3.m3.1.1.3.1" xref="S3.SS1.p3.3.m3.1.1.3.1.cmml">+</mo><mi id="S3.SS1.p3.3.m3.1.1.3.3" xref="S3.SS1.p3.3.m3.1.1.3.3.cmml">τ</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><apply id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p3.3.m3.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2">I</ci><apply id="S3.SS1.p3.3.m3.1.1.3.cmml" xref="S3.SS1.p3.3.m3.1.1.3"><plus id="S3.SS1.p3.3.m3.1.1.3.1.cmml" xref="S3.SS1.p3.3.m3.1.1.3.1"></plus><ci id="S3.SS1.p3.3.m3.1.1.3.2.cmml" xref="S3.SS1.p3.3.m3.1.1.3.2">t</ci><ci id="S3.SS1.p3.3.m3.1.1.3.3.cmml" xref="S3.SS1.p3.3.m3.1.1.3.3">𝜏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">\mathrm{I_{t+\tau}}</annotation></semantics></math>, by applying the <math id="S3.SS1.p3.4.m4.1" class="ltx_Math" alttext="\mathrm{scale\_and\_crop(\cdot)}" display="inline"><semantics id="S3.SS1.p3.4.m4.1a"><mrow id="S3.SS1.p3.4.m4.1.2" xref="S3.SS1.p3.4.m4.1.2.cmml"><mi id="S3.SS1.p3.4.m4.1.2.2" xref="S3.SS1.p3.4.m4.1.2.2.cmml">scale</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.4.m4.1.2.1" xref="S3.SS1.p3.4.m4.1.2.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS1.p3.4.m4.1.2.3" xref="S3.SS1.p3.4.m4.1.2.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.4.m4.1.2.1a" xref="S3.SS1.p3.4.m4.1.2.1.cmml">​</mo><mi id="S3.SS1.p3.4.m4.1.2.4" xref="S3.SS1.p3.4.m4.1.2.4.cmml">and</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.4.m4.1.2.1b" xref="S3.SS1.p3.4.m4.1.2.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS1.p3.4.m4.1.2.5" xref="S3.SS1.p3.4.m4.1.2.5.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.4.m4.1.2.1c" xref="S3.SS1.p3.4.m4.1.2.1.cmml">​</mo><mi id="S3.SS1.p3.4.m4.1.2.6" xref="S3.SS1.p3.4.m4.1.2.6.cmml">crop</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.4.m4.1.2.1d" xref="S3.SS1.p3.4.m4.1.2.1.cmml">​</mo><mrow id="S3.SS1.p3.4.m4.1.2.7.2" xref="S3.SS1.p3.4.m4.1.2.cmml"><mo stretchy="false" id="S3.SS1.p3.4.m4.1.2.7.2.1" xref="S3.SS1.p3.4.m4.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS1.p3.4.m4.1.2.7.2.2" xref="S3.SS1.p3.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><apply id="S3.SS1.p3.4.m4.1.2.cmml" xref="S3.SS1.p3.4.m4.1.2"><times id="S3.SS1.p3.4.m4.1.2.1.cmml" xref="S3.SS1.p3.4.m4.1.2.1"></times><ci id="S3.SS1.p3.4.m4.1.2.2.cmml" xref="S3.SS1.p3.4.m4.1.2.2">scale</ci><ci id="S3.SS1.p3.4.m4.1.2.3.cmml" xref="S3.SS1.p3.4.m4.1.2.3">_</ci><ci id="S3.SS1.p3.4.m4.1.2.4.cmml" xref="S3.SS1.p3.4.m4.1.2.4">and</ci><ci id="S3.SS1.p3.4.m4.1.2.5.cmml" xref="S3.SS1.p3.4.m4.1.2.5">_</ci><ci id="S3.SS1.p3.4.m4.1.2.6.cmml" xref="S3.SS1.p3.4.m4.1.2.6">crop</ci><ci id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">\mathrm{scale\_and\_crop(\cdot)}</annotation></semantics></math> function to each image.
In practice, it scales an image up to 2 times and crops the image to have a minimum IoU of 0.4 or above with original bounding boxes to avoid heavy object truncation and ensure stable tracker training.
Prior works either adopt standard random affine transformation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite> or cropping without scaling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite>, which generally provide <span id="S3.SS1.p3.4.1" class="ltx_text ltx_font_italic">weak</span> scale-jittering effect.
Instead, we focus on enlarging the scaling effect and show that our proposal significantly outperforms the baselines.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.3" class="ltx_p"><span id="S3.SS1.p4.3.1" class="ltx_text ltx_font_bold">Mosaicing Track.</span>
While the zoom-in/out augmentation is already effective in providing tracking supervisions, it is limited in the tracking of a few objects due to the federated annotations of LVIS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.
To resolve the issue, we present to combine multiple images and perform tracking with the increased foreground objects.
We implement our idea by extending the image mosaicing augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, which stitches four random training images with certain ratios.
While it was originally presented for object detection with enriched background <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite>, we recast it to simulate hard, dense tracking scenarios in large vocabulary tracking.
In practice, four random images, <math id="S3.SS1.p4.1.m1.4" class="ltx_Math" alttext="\{\mathrm{I_{a},I_{b},I_{c},I_{d}}\}" display="inline"><semantics id="S3.SS1.p4.1.m1.4a"><mrow id="S3.SS1.p4.1.m1.4.4.4" xref="S3.SS1.p4.1.m1.4.4.5.cmml"><mo stretchy="false" id="S3.SS1.p4.1.m1.4.4.4.5" xref="S3.SS1.p4.1.m1.4.4.5.cmml">{</mo><msub id="S3.SS1.p4.1.m1.1.1.1.1" xref="S3.SS1.p4.1.m1.1.1.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p4.1.m1.1.1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.1.1.2.cmml">I</mi><mi mathvariant="normal" id="S3.SS1.p4.1.m1.1.1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.1.1.3.cmml">a</mi></msub><mo id="S3.SS1.p4.1.m1.4.4.4.6" xref="S3.SS1.p4.1.m1.4.4.5.cmml">,</mo><msub id="S3.SS1.p4.1.m1.2.2.2.2" xref="S3.SS1.p4.1.m1.2.2.2.2.cmml"><mi mathvariant="normal" id="S3.SS1.p4.1.m1.2.2.2.2.2" xref="S3.SS1.p4.1.m1.2.2.2.2.2.cmml">I</mi><mi mathvariant="normal" id="S3.SS1.p4.1.m1.2.2.2.2.3" xref="S3.SS1.p4.1.m1.2.2.2.2.3.cmml">b</mi></msub><mo id="S3.SS1.p4.1.m1.4.4.4.7" xref="S3.SS1.p4.1.m1.4.4.5.cmml">,</mo><msub id="S3.SS1.p4.1.m1.3.3.3.3" xref="S3.SS1.p4.1.m1.3.3.3.3.cmml"><mi mathvariant="normal" id="S3.SS1.p4.1.m1.3.3.3.3.2" xref="S3.SS1.p4.1.m1.3.3.3.3.2.cmml">I</mi><mi mathvariant="normal" id="S3.SS1.p4.1.m1.3.3.3.3.3" xref="S3.SS1.p4.1.m1.3.3.3.3.3.cmml">c</mi></msub><mo id="S3.SS1.p4.1.m1.4.4.4.8" xref="S3.SS1.p4.1.m1.4.4.5.cmml">,</mo><msub id="S3.SS1.p4.1.m1.4.4.4.4" xref="S3.SS1.p4.1.m1.4.4.4.4.cmml"><mi mathvariant="normal" id="S3.SS1.p4.1.m1.4.4.4.4.2" xref="S3.SS1.p4.1.m1.4.4.4.4.2.cmml">I</mi><mi mathvariant="normal" id="S3.SS1.p4.1.m1.4.4.4.4.3" xref="S3.SS1.p4.1.m1.4.4.4.4.3.cmml">d</mi></msub><mo stretchy="false" id="S3.SS1.p4.1.m1.4.4.4.9" xref="S3.SS1.p4.1.m1.4.4.5.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.4b"><set id="S3.SS1.p4.1.m1.4.4.5.cmml" xref="S3.SS1.p4.1.m1.4.4.4"><apply id="S3.SS1.p4.1.m1.1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p4.1.m1.1.1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.2">I</ci><ci id="S3.SS1.p4.1.m1.1.1.1.1.3.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.3">a</ci></apply><apply id="S3.SS1.p4.1.m1.2.2.2.2.cmml" xref="S3.SS1.p4.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.2.2.2.2.1.cmml" xref="S3.SS1.p4.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p4.1.m1.2.2.2.2.2.cmml" xref="S3.SS1.p4.1.m1.2.2.2.2.2">I</ci><ci id="S3.SS1.p4.1.m1.2.2.2.2.3.cmml" xref="S3.SS1.p4.1.m1.2.2.2.2.3">b</ci></apply><apply id="S3.SS1.p4.1.m1.3.3.3.3.cmml" xref="S3.SS1.p4.1.m1.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.3.3.3.3.1.cmml" xref="S3.SS1.p4.1.m1.3.3.3.3">subscript</csymbol><ci id="S3.SS1.p4.1.m1.3.3.3.3.2.cmml" xref="S3.SS1.p4.1.m1.3.3.3.3.2">I</ci><ci id="S3.SS1.p4.1.m1.3.3.3.3.3.cmml" xref="S3.SS1.p4.1.m1.3.3.3.3.3">c</ci></apply><apply id="S3.SS1.p4.1.m1.4.4.4.4.cmml" xref="S3.SS1.p4.1.m1.4.4.4.4"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.4.4.4.4.1.cmml" xref="S3.SS1.p4.1.m1.4.4.4.4">subscript</csymbol><ci id="S3.SS1.p4.1.m1.4.4.4.4.2.cmml" xref="S3.SS1.p4.1.m1.4.4.4.4.2">I</ci><ci id="S3.SS1.p4.1.m1.4.4.4.4.3.cmml" xref="S3.SS1.p4.1.m1.4.4.4.4.3">d</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.4c">\{\mathrm{I_{a},I_{b},I_{c},I_{d}}\}</annotation></semantics></math>, are sampled from RFS (Repeat Factor Sampling)-based dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> to maintain the class-balance.
Then, image stitching followed by random affine (with large scale jittering within a range of 0.1 to 2) and crop is applied.
We summarize these procedure as <math id="S3.SS1.p4.2.m2.1" class="ltx_Math" alttext="\mathrm{mosaic(\cdot)}" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><mrow id="S3.SS1.p4.2.m2.1.2" xref="S3.SS1.p4.2.m2.1.2.cmml"><mi id="S3.SS1.p4.2.m2.1.2.2" xref="S3.SS1.p4.2.m2.1.2.2.cmml">mosaic</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.2.m2.1.2.1" xref="S3.SS1.p4.2.m2.1.2.1.cmml">​</mo><mrow id="S3.SS1.p4.2.m2.1.2.3.2" xref="S3.SS1.p4.2.m2.1.2.cmml"><mo stretchy="false" id="S3.SS1.p4.2.m2.1.2.3.2.1" xref="S3.SS1.p4.2.m2.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS1.p4.2.m2.1.2.3.2.2" xref="S3.SS1.p4.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><apply id="S3.SS1.p4.2.m2.1.2.cmml" xref="S3.SS1.p4.2.m2.1.2"><times id="S3.SS1.p4.2.m2.1.2.1.cmml" xref="S3.SS1.p4.2.m2.1.2.1"></times><ci id="S3.SS1.p4.2.m2.1.2.2.cmml" xref="S3.SS1.p4.2.m2.1.2.2">mosaic</ci><ci id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">\mathrm{mosaic(\cdot)}</annotation></semantics></math>. The tracking pair then can be obtained by applying the <math id="S3.SS1.p4.3.m3.1" class="ltx_Math" alttext="\mathrm{mosaic(\cdot)}" display="inline"><semantics id="S3.SS1.p4.3.m3.1a"><mrow id="S3.SS1.p4.3.m3.1.2" xref="S3.SS1.p4.3.m3.1.2.cmml"><mi id="S3.SS1.p4.3.m3.1.2.2" xref="S3.SS1.p4.3.m3.1.2.2.cmml">mosaic</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.3.m3.1.2.1" xref="S3.SS1.p4.3.m3.1.2.1.cmml">​</mo><mrow id="S3.SS1.p4.3.m3.1.2.3.2" xref="S3.SS1.p4.3.m3.1.2.cmml"><mo stretchy="false" id="S3.SS1.p4.3.m3.1.2.3.2.1" xref="S3.SS1.p4.3.m3.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS1.p4.3.m3.1.2.3.2.2" xref="S3.SS1.p4.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><apply id="S3.SS1.p4.3.m3.1.2.cmml" xref="S3.SS1.p4.3.m3.1.2"><times id="S3.SS1.p4.3.m3.1.2.1.cmml" xref="S3.SS1.p4.3.m3.1.2.1"></times><ci id="S3.SS1.p4.3.m3.1.2.2.cmml" xref="S3.SS1.p4.3.m3.1.2.2">mosaic</ci><ci id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">\mathrm{mosaic(\cdot)}</annotation></semantics></math> function to the sampled images twice.
However, we see that unnatural layout pair results in train and test time inconsistency.
To this end, we propose to sample tracking input pairs in a mixed way from two different augmentations, zoom-in/out and mosaicing, with equal probability during training. We empirically confirm that this works well in practice.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">With our proposal, the model can receive tracking supervisions from all LVIS object categories.
The tracking objective function is adopted from the QDtrack <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> (see Fig. <a href="#S2.F2" title="Figure 2 ‣ 2 Related work ‣ Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>-top), and we call this model LVIS-Tracker.
While the model is only trained on LVIS dataset, it already outperforms the previous state-of-the-art tracker (trained with the standard decoupled learning scheme) significantly (see Table <a href="#S4.T2.sf1" title="In Table 2 ‣ 4.2 Ablation studies ‣ 4 Experiments ‣ Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2a</span></a>).</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Learn to Unforget in TAO</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Due to the fundamental annotation difficulties in videos, the images are in general bigger in dataset scale and larger in taxonomies.
Therefore, pre-training the model on images to acquire generic features and fine-tuning on videos for target domain adaptation has become a common protocol for obtaining satisfactory performance in various video tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib95" title="" class="ltx_ref">95</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.
This also applies to training the large vocabulary video trackers, where we first learn a large number of vocabulary from LVIS images and then adapt to the evaluation domain with TAO videos.
However, as TAO partially spans the full LVIS vocabularies, a naive transfer learning scheme results in catastrophic forgetting.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Here, our goal is to keep the ability to detect the previously seen object categories while also adapting to learn from new video labels.
We mainly focus on the catastrophic forgetting in the detector, as the tracking head is learned in a category-agnostic manner.
We detail the proposal using the standard two-staged Faster-RCNN detector (FPN backbone) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>.
Without loss of generality, the proposals can be extended to multi-staged architectures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>, where we apply the proposal for each RCNN head and average them.
In fact, the main issue is missing annotations for the seen, known object categories during the image to video transfer learning.
Since they are not annotated, we can neither provide detection supervision nor prevent them from being treated as background.
This basically perturbs the pre-trained classifier boundaries of both RPN and RCNN, leading to catastrophic forgetting.
We remedy this issue by presenting a pseudo-label guided teacher-student framework.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Our key idea is intuitive.
The pre-trained model already has sufficient knowledge to detect the seen, known categories.
Based on this fact, we first fill in the missing annotations by pseudo-labeling the input.
We adopt the basic pseudo-labeling scheme with a threshold of 0.3.
The redundant pseudo labels that highly overlap with the current labels are filtered out with NMS.
With these augmented labels, we 1) design a teacher-student network to provide (soft) supervisions, i.e., class logit, for preserving the past knowledge, and
2) update the incorrect background samples, i.e., negatives, in RPN and RCNN to prevent seen objects from being background (see Fig. <a href="#S2.F2" title="Figure 2 ‣ 2 Related work ‣ Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>-bottom).
Using soft class logit is important for the large vocabulary classifier distillation, as the hard pseudo labels bias the operation towards the frequent class objects.
Moreover, we use MSE loss instead of Kullback-Leibler (KL) divergence loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> for the logit matching. This is because the MSE loss treats all classes equally and thus it allows the rare classes with low probability also to be updated properly <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>. This two new adaptation leads to the successful distillation of the previous knowledge of the large vocabulary classifier (see Table <a href="#S4.T2.sf2" title="In Table 2 ‣ 4.2 Ablation studies ‣ 4 Experiments ‣ Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2b</span></a>).</p>
</div>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.1" class="ltx_p"><span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_bold">Teacher-Student Framework Setup.</span>
To effectively retain the previous knowledge, we design a teacher-student framework.
We first make identical copies of the image pre-trained model, teacher (T) and student (S).
The teacher model (T) is frozen to keep the previous knowledge and guide the student.
The student model (S) adapts to the new domain with incoming video labels (via detection loss) and also mimics the teacher model to preserve the past information (via distillation loss).
We detail the components in the following.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para ltx_noindent">
<p id="S3.SS2.p5.11" class="ltx_p"><span id="S3.SS2.p5.11.1" class="ltx_text ltx_font_bold">RPN Knowledge Distillation Loss.</span>
The RPN takes multi-level features from the ResNet feature pyramid <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>.
In particular, each feature map is embedded through the convolution layer, followed by two separate layers, one for objectness classification and the other for proposal regression.
We collect the outputs of both heads from the teacher and student to compute RPN distillation loss, which is defined as
<math id="S3.SS2.p5.1.m1.1" class="ltx_Math" alttext="\mathit{L}^{\mathrm{RPN}}_{\mathrm{KD}}=\frac{1}{\mathit{N}_{cls}}{\sum_{i=1}\mathit{L}_{cls}(u_{i},u_{i}^{*})}+\frac{1}{\mathit{N}_{reg}}{\sum_{i=1}\mathit{L}_{reg}(v_{i},v_{i}^{*})}." display="inline"><semantics id="S3.SS2.p5.1.m1.1a"><mrow id="S3.SS2.p5.1.m1.1.1.1" xref="S3.SS2.p5.1.m1.1.1.1.1.cmml"><mrow id="S3.SS2.p5.1.m1.1.1.1.1" xref="S3.SS2.p5.1.m1.1.1.1.1.cmml"><msubsup id="S3.SS2.p5.1.m1.1.1.1.1.6" xref="S3.SS2.p5.1.m1.1.1.1.1.6.cmml"><mi id="S3.SS2.p5.1.m1.1.1.1.1.6.2.2" xref="S3.SS2.p5.1.m1.1.1.1.1.6.2.2.cmml">L</mi><mi id="S3.SS2.p5.1.m1.1.1.1.1.6.3" xref="S3.SS2.p5.1.m1.1.1.1.1.6.3.cmml">KD</mi><mi id="S3.SS2.p5.1.m1.1.1.1.1.6.2.3" xref="S3.SS2.p5.1.m1.1.1.1.1.6.2.3.cmml">RPN</mi></msubsup><mo id="S3.SS2.p5.1.m1.1.1.1.1.5" xref="S3.SS2.p5.1.m1.1.1.1.1.5.cmml">=</mo><mrow id="S3.SS2.p5.1.m1.1.1.1.1.4" xref="S3.SS2.p5.1.m1.1.1.1.1.4.cmml"><mrow id="S3.SS2.p5.1.m1.1.1.1.1.2.2" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.cmml"><mfrac id="S3.SS2.p5.1.m1.1.1.1.1.2.2.4" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.cmml"><mn id="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.2" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.2.cmml">1</mn><msub id="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.cmml"><mi id="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.2" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.2.cmml">N</mi><mrow id="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.3" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.3.cmml"><mi id="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.3.2" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.3.1" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.3.1.cmml">​</mo><mi id="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.3.3" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.3.1a" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.3.1.cmml">​</mo><mi id="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.3.4" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.3.4.cmml">s</mi></mrow></msub></mfrac><mo lspace="0em" rspace="0em" id="S3.SS2.p5.1.m1.1.1.1.1.2.2.3" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.3.cmml">​</mo><mrow id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.cmml"><msub id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.3" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.3.cmml"><mo id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.3.2" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.3.2.cmml">∑</mo><mrow id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.3.3" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.3.3.cmml"><mi id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.3.3.2" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.3.3.2.cmml">i</mi><mo id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.3.3.1" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.3.3.1.cmml">=</mo><mn id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.3.3.3" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.3.3.3.cmml">1</mn></mrow></msub><mrow id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.cmml"><msub id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.cmml"><mi id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.2" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.2.cmml">L</mi><mrow id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.3" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.3.cmml"><mi id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.3.2" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.3.1" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.3.1.cmml">​</mo><mi id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.3.3" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.3.1a" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.3.1.cmml">​</mo><mi id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.3.4" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.3.4.cmml">s</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.3" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.3.cmml">​</mo><mrow id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.2" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.2.3" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.3.cmml">(</mo><msub id="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">u</mi><mi id="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.2.4" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.3.cmml">,</mo><msubsup id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.2.2" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.2.2.cmml"><mi id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.2.2.2.2" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.2.2.2.2.cmml">u</mi><mi id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.2.2.2.3" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.2.2.2.3.cmml">i</mi><mo id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.2.2.3" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.2.2.3.cmml">∗</mo></msubsup><mo stretchy="false" id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.2.5" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.SS2.p5.1.m1.1.1.1.1.4.5" xref="S3.SS2.p5.1.m1.1.1.1.1.4.5.cmml">+</mo><mrow id="S3.SS2.p5.1.m1.1.1.1.1.4.4" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.cmml"><mfrac id="S3.SS2.p5.1.m1.1.1.1.1.4.4.4" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.cmml"><mn id="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.2" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.2.cmml">1</mn><msub id="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.cmml"><mi id="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.2" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.2.cmml">N</mi><mrow id="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.3" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.3.cmml"><mi id="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.3.2" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.3.1" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.3.1.cmml">​</mo><mi id="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.3.3" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.3.1a" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.3.1.cmml">​</mo><mi id="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.3.4" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.3.4.cmml">g</mi></mrow></msub></mfrac><mo lspace="0em" rspace="0em" id="S3.SS2.p5.1.m1.1.1.1.1.4.4.3" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.3.cmml">​</mo><mrow id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.cmml"><msub id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.3" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.3.cmml"><mo id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.3.2" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.3.2.cmml">∑</mo><mrow id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.3.3" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.3.3.cmml"><mi id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.3.3.2" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.3.3.2.cmml">i</mi><mo id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.3.3.1" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.3.3.1.cmml">=</mo><mn id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.3.3.3" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.3.3.3.cmml">1</mn></mrow></msub><mrow id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.cmml"><msub id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.cmml"><mi id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.2" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.2.cmml">L</mi><mrow id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.3" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.3.cmml"><mi id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.3.2" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.3.1" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.3.1.cmml">​</mo><mi id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.3.3" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.3.1a" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.3.1.cmml">​</mo><mi id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.3.4" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.3.4.cmml">g</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.3" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.3.cmml">​</mo><mrow id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.2" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.2.3" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.3.cmml">(</mo><msub id="S3.SS2.p5.1.m1.1.1.1.1.3.3.1.1.1.1.1" xref="S3.SS2.p5.1.m1.1.1.1.1.3.3.1.1.1.1.1.cmml"><mi id="S3.SS2.p5.1.m1.1.1.1.1.3.3.1.1.1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.1.1.3.3.1.1.1.1.1.2.cmml">v</mi><mi id="S3.SS2.p5.1.m1.1.1.1.1.3.3.1.1.1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.1.1.3.3.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.2.4" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.3.cmml">,</mo><msubsup id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.2.2" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.2.2.cmml"><mi id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.2.2.2.2" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.2.2.2.2.cmml">v</mi><mi id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.2.2.2.3" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.2.2.2.3.cmml">i</mi><mo id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.2.2.3" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.2.2.3.cmml">∗</mo></msubsup><mo stretchy="false" id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.2.5" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo lspace="0em" id="S3.SS2.p5.1.m1.1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><apply id="S3.SS2.p5.1.m1.1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1"><eq id="S3.SS2.p5.1.m1.1.1.1.1.5.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.5"></eq><apply id="S3.SS2.p5.1.m1.1.1.1.1.6.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.6"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.1.6.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.6">subscript</csymbol><apply id="S3.SS2.p5.1.m1.1.1.1.1.6.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.6"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.1.6.2.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.6">superscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.1.1.6.2.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.6.2.2">𝐿</ci><ci id="S3.SS2.p5.1.m1.1.1.1.1.6.2.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.6.2.3">RPN</ci></apply><ci id="S3.SS2.p5.1.m1.1.1.1.1.6.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.6.3">KD</ci></apply><apply id="S3.SS2.p5.1.m1.1.1.1.1.4.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4"><plus id="S3.SS2.p5.1.m1.1.1.1.1.4.5.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.5"></plus><apply id="S3.SS2.p5.1.m1.1.1.1.1.2.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2"><times id="S3.SS2.p5.1.m1.1.1.1.1.2.2.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.3"></times><apply id="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.4"><divide id="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.4"></divide><cn type="integer" id="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.2">1</cn><apply id="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3">subscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.2">𝑁</ci><apply id="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.3"><times id="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.3.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.3.1"></times><ci id="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.3.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.3.2">𝑐</ci><ci id="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.3.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.3.3">𝑙</ci><ci id="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.3.4.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.4.3.3.4">𝑠</ci></apply></apply></apply><apply id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2"><apply id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.3.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.3">subscript</csymbol><sum id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.3.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.3.2"></sum><apply id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.3.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.3.3"><eq id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.3.3.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.3.3.1"></eq><ci id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.3.3.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.3.3.2">𝑖</ci><cn type="integer" id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.3.3.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.3.3.3">1</cn></apply></apply><apply id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2"><times id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.3"></times><apply id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4">subscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.2">𝐿</ci><apply id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.3"><times id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.3.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.3.1"></times><ci id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.3.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.3.2">𝑐</ci><ci id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.3.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.3.3">𝑙</ci><ci id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.3.4.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.4.3.4">𝑠</ci></apply></apply><interval closure="open" id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.2"><apply id="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1.1.1.2">𝑢</ci><ci id="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.2.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.2.2.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.2.2">superscript</csymbol><apply id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.2.2.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.2.2.2.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.2.2.2.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.2.2.2.2">𝑢</ci><ci id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.2.2.2.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.2.2.2.3">𝑖</ci></apply><times id="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.2.2.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.2.2.2.2.2.2.2.3"></times></apply></interval></apply></apply></apply><apply id="S3.SS2.p5.1.m1.1.1.1.1.4.4.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4"><times id="S3.SS2.p5.1.m1.1.1.1.1.4.4.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.3"></times><apply id="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.4"><divide id="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.4"></divide><cn type="integer" id="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.2">1</cn><apply id="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3">subscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.2">𝑁</ci><apply id="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.3"><times id="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.3.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.3.1"></times><ci id="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.3.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.3.2">𝑟</ci><ci id="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.3.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.3.3">𝑒</ci><ci id="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.3.4.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.4.3.3.4">𝑔</ci></apply></apply></apply><apply id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2"><apply id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.3"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.3.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.3">subscript</csymbol><sum id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.3.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.3.2"></sum><apply id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.3.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.3.3"><eq id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.3.3.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.3.3.1"></eq><ci id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.3.3.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.3.3.2">𝑖</ci><cn type="integer" id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.3.3.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.3.3.3">1</cn></apply></apply><apply id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2"><times id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.3"></times><apply id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4">subscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.2">𝐿</ci><apply id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.3"><times id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.3.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.3.1"></times><ci id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.3.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.3.2">𝑟</ci><ci id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.3.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.3.3">𝑒</ci><ci id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.3.4.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.4.3.4">𝑔</ci></apply></apply><interval closure="open" id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.2"><apply id="S3.SS2.p5.1.m1.1.1.1.1.3.3.1.1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.1.3.3.1.1.1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.3.3.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.1.1.3.3.1.1.1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.3.3.1.1.1.1.1.2">𝑣</ci><ci id="S3.SS2.p5.1.m1.1.1.1.1.3.3.1.1.1.1.1.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.3.3.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.2.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.2.2.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.2.2">superscript</csymbol><apply id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.2.2.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.2.2.2.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.2.2.2.2.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.2.2.2.2">𝑣</ci><ci id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.2.2.2.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.2.2.2.3">𝑖</ci></apply><times id="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.2.2.3.cmml" xref="S3.SS2.p5.1.m1.1.1.1.1.4.4.2.2.2.2.2.3"></times></apply></interval></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">\mathit{L}^{\mathrm{RPN}}_{\mathrm{KD}}=\frac{1}{\mathit{N}_{cls}}{\sum_{i=1}\mathit{L}_{cls}(u_{i},u_{i}^{*})}+\frac{1}{\mathit{N}_{reg}}{\sum_{i=1}\mathit{L}_{reg}(v_{i},v_{i}^{*})}.</annotation></semantics></math>
Here, <math id="S3.SS2.p5.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.p5.2.m2.1a"><mi id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><ci id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">i</annotation></semantics></math> is the index of an anchor.
<math id="S3.SS2.p5.3.m3.1" class="ltx_Math" alttext="u_{i}" display="inline"><semantics id="S3.SS2.p5.3.m3.1a"><msub id="S3.SS2.p5.3.m3.1.1" xref="S3.SS2.p5.3.m3.1.1.cmml"><mi id="S3.SS2.p5.3.m3.1.1.2" xref="S3.SS2.p5.3.m3.1.1.2.cmml">u</mi><mi id="S3.SS2.p5.3.m3.1.1.3" xref="S3.SS2.p5.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.3.m3.1b"><apply id="S3.SS2.p5.3.m3.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.3.m3.1.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p5.3.m3.1.1.2.cmml" xref="S3.SS2.p5.3.m3.1.1.2">𝑢</ci><ci id="S3.SS2.p5.3.m3.1.1.3.cmml" xref="S3.SS2.p5.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.3.m3.1c">u_{i}</annotation></semantics></math> and <math id="S3.SS2.p5.4.m4.1" class="ltx_Math" alttext="u_{i}^{*}" display="inline"><semantics id="S3.SS2.p5.4.m4.1a"><msubsup id="S3.SS2.p5.4.m4.1.1" xref="S3.SS2.p5.4.m4.1.1.cmml"><mi id="S3.SS2.p5.4.m4.1.1.2.2" xref="S3.SS2.p5.4.m4.1.1.2.2.cmml">u</mi><mi id="S3.SS2.p5.4.m4.1.1.2.3" xref="S3.SS2.p5.4.m4.1.1.2.3.cmml">i</mi><mo id="S3.SS2.p5.4.m4.1.1.3" xref="S3.SS2.p5.4.m4.1.1.3.cmml">∗</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.4.m4.1b"><apply id="S3.SS2.p5.4.m4.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.4.m4.1.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1">superscript</csymbol><apply id="S3.SS2.p5.4.m4.1.1.2.cmml" xref="S3.SS2.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.4.m4.1.1.2.1.cmml" xref="S3.SS2.p5.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p5.4.m4.1.1.2.2.cmml" xref="S3.SS2.p5.4.m4.1.1.2.2">𝑢</ci><ci id="S3.SS2.p5.4.m4.1.1.2.3.cmml" xref="S3.SS2.p5.4.m4.1.1.2.3">𝑖</ci></apply><times id="S3.SS2.p5.4.m4.1.1.3.cmml" xref="S3.SS2.p5.4.m4.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.4.m4.1c">u_{i}^{*}</annotation></semantics></math> are the mean subtracted objectness logits obtained from the student and the teacher, respectively.
<math id="S3.SS2.p5.5.m5.1" class="ltx_Math" alttext="v_{i}" display="inline"><semantics id="S3.SS2.p5.5.m5.1a"><msub id="S3.SS2.p5.5.m5.1.1" xref="S3.SS2.p5.5.m5.1.1.cmml"><mi id="S3.SS2.p5.5.m5.1.1.2" xref="S3.SS2.p5.5.m5.1.1.2.cmml">v</mi><mi id="S3.SS2.p5.5.m5.1.1.3" xref="S3.SS2.p5.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.5.m5.1b"><apply id="S3.SS2.p5.5.m5.1.1.cmml" xref="S3.SS2.p5.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.5.m5.1.1.1.cmml" xref="S3.SS2.p5.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p5.5.m5.1.1.2.cmml" xref="S3.SS2.p5.5.m5.1.1.2">𝑣</ci><ci id="S3.SS2.p5.5.m5.1.1.3.cmml" xref="S3.SS2.p5.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.5.m5.1c">v_{i}</annotation></semantics></math> and <math id="S3.SS2.p5.6.m6.1" class="ltx_Math" alttext="v_{i}^{*}" display="inline"><semantics id="S3.SS2.p5.6.m6.1a"><msubsup id="S3.SS2.p5.6.m6.1.1" xref="S3.SS2.p5.6.m6.1.1.cmml"><mi id="S3.SS2.p5.6.m6.1.1.2.2" xref="S3.SS2.p5.6.m6.1.1.2.2.cmml">v</mi><mi id="S3.SS2.p5.6.m6.1.1.2.3" xref="S3.SS2.p5.6.m6.1.1.2.3.cmml">i</mi><mo id="S3.SS2.p5.6.m6.1.1.3" xref="S3.SS2.p5.6.m6.1.1.3.cmml">∗</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.6.m6.1b"><apply id="S3.SS2.p5.6.m6.1.1.cmml" xref="S3.SS2.p5.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.6.m6.1.1.1.cmml" xref="S3.SS2.p5.6.m6.1.1">superscript</csymbol><apply id="S3.SS2.p5.6.m6.1.1.2.cmml" xref="S3.SS2.p5.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.6.m6.1.1.2.1.cmml" xref="S3.SS2.p5.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p5.6.m6.1.1.2.2.cmml" xref="S3.SS2.p5.6.m6.1.1.2.2">𝑣</ci><ci id="S3.SS2.p5.6.m6.1.1.2.3.cmml" xref="S3.SS2.p5.6.m6.1.1.2.3">𝑖</ci></apply><times id="S3.SS2.p5.6.m6.1.1.3.cmml" xref="S3.SS2.p5.6.m6.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.6.m6.1c">v_{i}^{*}</annotation></semantics></math> are four parameterized coordinates for the anchor refinement obtained from the student and teacher, respectively.
<math id="S3.SS2.p5.7.m7.1" class="ltx_Math" alttext="\mathit{L}_{cls}" display="inline"><semantics id="S3.SS2.p5.7.m7.1a"><msub id="S3.SS2.p5.7.m7.1.1" xref="S3.SS2.p5.7.m7.1.1.cmml"><mi id="S3.SS2.p5.7.m7.1.1.2" xref="S3.SS2.p5.7.m7.1.1.2.cmml">L</mi><mrow id="S3.SS2.p5.7.m7.1.1.3" xref="S3.SS2.p5.7.m7.1.1.3.cmml"><mi id="S3.SS2.p5.7.m7.1.1.3.2" xref="S3.SS2.p5.7.m7.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.7.m7.1.1.3.1" xref="S3.SS2.p5.7.m7.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p5.7.m7.1.1.3.3" xref="S3.SS2.p5.7.m7.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.7.m7.1.1.3.1a" xref="S3.SS2.p5.7.m7.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p5.7.m7.1.1.3.4" xref="S3.SS2.p5.7.m7.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.7.m7.1b"><apply id="S3.SS2.p5.7.m7.1.1.cmml" xref="S3.SS2.p5.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.7.m7.1.1.1.cmml" xref="S3.SS2.p5.7.m7.1.1">subscript</csymbol><ci id="S3.SS2.p5.7.m7.1.1.2.cmml" xref="S3.SS2.p5.7.m7.1.1.2">𝐿</ci><apply id="S3.SS2.p5.7.m7.1.1.3.cmml" xref="S3.SS2.p5.7.m7.1.1.3"><times id="S3.SS2.p5.7.m7.1.1.3.1.cmml" xref="S3.SS2.p5.7.m7.1.1.3.1"></times><ci id="S3.SS2.p5.7.m7.1.1.3.2.cmml" xref="S3.SS2.p5.7.m7.1.1.3.2">𝑐</ci><ci id="S3.SS2.p5.7.m7.1.1.3.3.cmml" xref="S3.SS2.p5.7.m7.1.1.3.3">𝑙</ci><ci id="S3.SS2.p5.7.m7.1.1.3.4.cmml" xref="S3.SS2.p5.7.m7.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.7.m7.1c">\mathit{L}_{cls}</annotation></semantics></math> and <math id="S3.SS2.p5.8.m8.1" class="ltx_Math" alttext="\mathit{L}_{reg}" display="inline"><semantics id="S3.SS2.p5.8.m8.1a"><msub id="S3.SS2.p5.8.m8.1.1" xref="S3.SS2.p5.8.m8.1.1.cmml"><mi id="S3.SS2.p5.8.m8.1.1.2" xref="S3.SS2.p5.8.m8.1.1.2.cmml">L</mi><mrow id="S3.SS2.p5.8.m8.1.1.3" xref="S3.SS2.p5.8.m8.1.1.3.cmml"><mi id="S3.SS2.p5.8.m8.1.1.3.2" xref="S3.SS2.p5.8.m8.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.8.m8.1.1.3.1" xref="S3.SS2.p5.8.m8.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p5.8.m8.1.1.3.3" xref="S3.SS2.p5.8.m8.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.8.m8.1.1.3.1a" xref="S3.SS2.p5.8.m8.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p5.8.m8.1.1.3.4" xref="S3.SS2.p5.8.m8.1.1.3.4.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.8.m8.1b"><apply id="S3.SS2.p5.8.m8.1.1.cmml" xref="S3.SS2.p5.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.8.m8.1.1.1.cmml" xref="S3.SS2.p5.8.m8.1.1">subscript</csymbol><ci id="S3.SS2.p5.8.m8.1.1.2.cmml" xref="S3.SS2.p5.8.m8.1.1.2">𝐿</ci><apply id="S3.SS2.p5.8.m8.1.1.3.cmml" xref="S3.SS2.p5.8.m8.1.1.3"><times id="S3.SS2.p5.8.m8.1.1.3.1.cmml" xref="S3.SS2.p5.8.m8.1.1.3.1"></times><ci id="S3.SS2.p5.8.m8.1.1.3.2.cmml" xref="S3.SS2.p5.8.m8.1.1.3.2">𝑟</ci><ci id="S3.SS2.p5.8.m8.1.1.3.3.cmml" xref="S3.SS2.p5.8.m8.1.1.3.3">𝑒</ci><ci id="S3.SS2.p5.8.m8.1.1.3.4.cmml" xref="S3.SS2.p5.8.m8.1.1.3.4">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.8.m8.1c">\mathit{L}_{reg}</annotation></semantics></math> are MSE loss and smooth L1 loss, respectively.
Here, we note that <math id="S3.SS2.p5.9.m9.1" class="ltx_Math" alttext="\mathit{L}_{reg}" display="inline"><semantics id="S3.SS2.p5.9.m9.1a"><msub id="S3.SS2.p5.9.m9.1.1" xref="S3.SS2.p5.9.m9.1.1.cmml"><mi id="S3.SS2.p5.9.m9.1.1.2" xref="S3.SS2.p5.9.m9.1.1.2.cmml">L</mi><mrow id="S3.SS2.p5.9.m9.1.1.3" xref="S3.SS2.p5.9.m9.1.1.3.cmml"><mi id="S3.SS2.p5.9.m9.1.1.3.2" xref="S3.SS2.p5.9.m9.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.9.m9.1.1.3.1" xref="S3.SS2.p5.9.m9.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p5.9.m9.1.1.3.3" xref="S3.SS2.p5.9.m9.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.9.m9.1.1.3.1a" xref="S3.SS2.p5.9.m9.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p5.9.m9.1.1.3.4" xref="S3.SS2.p5.9.m9.1.1.3.4.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.9.m9.1b"><apply id="S3.SS2.p5.9.m9.1.1.cmml" xref="S3.SS2.p5.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.9.m9.1.1.1.cmml" xref="S3.SS2.p5.9.m9.1.1">subscript</csymbol><ci id="S3.SS2.p5.9.m9.1.1.2.cmml" xref="S3.SS2.p5.9.m9.1.1.2">𝐿</ci><apply id="S3.SS2.p5.9.m9.1.1.3.cmml" xref="S3.SS2.p5.9.m9.1.1.3"><times id="S3.SS2.p5.9.m9.1.1.3.1.cmml" xref="S3.SS2.p5.9.m9.1.1.3.1"></times><ci id="S3.SS2.p5.9.m9.1.1.3.2.cmml" xref="S3.SS2.p5.9.m9.1.1.3.2">𝑟</ci><ci id="S3.SS2.p5.9.m9.1.1.3.3.cmml" xref="S3.SS2.p5.9.m9.1.1.3.3">𝑒</ci><ci id="S3.SS2.p5.9.m9.1.1.3.4.cmml" xref="S3.SS2.p5.9.m9.1.1.3.4">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.9.m9.1c">\mathit{L}_{reg}</annotation></semantics></math> is only computed for the positive anchors that have an IoU larger than 0.7 <span id="S3.SS2.p5.11.2" class="ltx_text ltx_font_italic">with the augmented ground-truth boxes</span>.
<math id="S3.SS2.p5.10.m10.1" class="ltx_Math" alttext="\mathit{N}_{cls}(=256)" display="inline"><semantics id="S3.SS2.p5.10.m10.1a"><mrow id="S3.SS2.p5.10.m10.1.1" xref="S3.SS2.p5.10.m10.1.1.cmml"><msub id="S3.SS2.p5.10.m10.1.1.3" xref="S3.SS2.p5.10.m10.1.1.3.cmml"><mi id="S3.SS2.p5.10.m10.1.1.3.2" xref="S3.SS2.p5.10.m10.1.1.3.2.cmml">N</mi><mrow id="S3.SS2.p5.10.m10.1.1.3.3" xref="S3.SS2.p5.10.m10.1.1.3.3.cmml"><mi id="S3.SS2.p5.10.m10.1.1.3.3.2" xref="S3.SS2.p5.10.m10.1.1.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.10.m10.1.1.3.3.1" xref="S3.SS2.p5.10.m10.1.1.3.3.1.cmml">​</mo><mi id="S3.SS2.p5.10.m10.1.1.3.3.3" xref="S3.SS2.p5.10.m10.1.1.3.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.10.m10.1.1.3.3.1a" xref="S3.SS2.p5.10.m10.1.1.3.3.1.cmml">​</mo><mi id="S3.SS2.p5.10.m10.1.1.3.3.4" xref="S3.SS2.p5.10.m10.1.1.3.3.4.cmml">s</mi></mrow></msub><mspace width="0.3888888888888889em" id="S3.SS2.p5.10.m10.1.1a" xref="S3.SS2.p5.10.m10.1.1.cmml"></mspace><mrow id="S3.SS2.p5.10.m10.1.1.1.1" xref="S3.SS2.p5.10.m10.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p5.10.m10.1.1.1.1.2" xref="S3.SS2.p5.10.m10.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p5.10.m10.1.1.1.1.1" xref="S3.SS2.p5.10.m10.1.1.1.1.1.cmml"><mi id="S3.SS2.p5.10.m10.1.1.1.1.1.2" xref="S3.SS2.p5.10.m10.1.1.1.1.1.2.cmml"></mi><mo id="S3.SS2.p5.10.m10.1.1.1.1.1.1" xref="S3.SS2.p5.10.m10.1.1.1.1.1.1.cmml">=</mo><mn id="S3.SS2.p5.10.m10.1.1.1.1.1.3" xref="S3.SS2.p5.10.m10.1.1.1.1.1.3.cmml">256</mn></mrow><mo stretchy="false" id="S3.SS2.p5.10.m10.1.1.1.1.3" xref="S3.SS2.p5.10.m10.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.10.m10.1b"><apply id="S3.SS2.p5.10.m10.1.1.cmml" xref="S3.SS2.p5.10.m10.1.1"><csymbol cd="latexml" id="S3.SS2.p5.10.m10.1.1.2.cmml" xref="S3.SS2.p5.10.m10.1.1">annotated</csymbol><apply id="S3.SS2.p5.10.m10.1.1.3.cmml" xref="S3.SS2.p5.10.m10.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p5.10.m10.1.1.3.1.cmml" xref="S3.SS2.p5.10.m10.1.1.3">subscript</csymbol><ci id="S3.SS2.p5.10.m10.1.1.3.2.cmml" xref="S3.SS2.p5.10.m10.1.1.3.2">𝑁</ci><apply id="S3.SS2.p5.10.m10.1.1.3.3.cmml" xref="S3.SS2.p5.10.m10.1.1.3.3"><times id="S3.SS2.p5.10.m10.1.1.3.3.1.cmml" xref="S3.SS2.p5.10.m10.1.1.3.3.1"></times><ci id="S3.SS2.p5.10.m10.1.1.3.3.2.cmml" xref="S3.SS2.p5.10.m10.1.1.3.3.2">𝑐</ci><ci id="S3.SS2.p5.10.m10.1.1.3.3.3.cmml" xref="S3.SS2.p5.10.m10.1.1.3.3.3">𝑙</ci><ci id="S3.SS2.p5.10.m10.1.1.3.3.4.cmml" xref="S3.SS2.p5.10.m10.1.1.3.3.4">𝑠</ci></apply></apply><apply id="S3.SS2.p5.10.m10.1.1.1.1.1.cmml" xref="S3.SS2.p5.10.m10.1.1.1.1"><eq id="S3.SS2.p5.10.m10.1.1.1.1.1.1.cmml" xref="S3.SS2.p5.10.m10.1.1.1.1.1.1"></eq><csymbol cd="latexml" id="S3.SS2.p5.10.m10.1.1.1.1.1.2.cmml" xref="S3.SS2.p5.10.m10.1.1.1.1.1.2">absent</csymbol><cn type="integer" id="S3.SS2.p5.10.m10.1.1.1.1.1.3.cmml" xref="S3.SS2.p5.10.m10.1.1.1.1.1.3">256</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.10.m10.1c">\mathit{N}_{cls}(=256)</annotation></semantics></math> and <math id="S3.SS2.p5.11.m11.1" class="ltx_Math" alttext="\mathit{N}_{reg}" display="inline"><semantics id="S3.SS2.p5.11.m11.1a"><msub id="S3.SS2.p5.11.m11.1.1" xref="S3.SS2.p5.11.m11.1.1.cmml"><mi id="S3.SS2.p5.11.m11.1.1.2" xref="S3.SS2.p5.11.m11.1.1.2.cmml">N</mi><mrow id="S3.SS2.p5.11.m11.1.1.3" xref="S3.SS2.p5.11.m11.1.1.3.cmml"><mi id="S3.SS2.p5.11.m11.1.1.3.2" xref="S3.SS2.p5.11.m11.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.11.m11.1.1.3.1" xref="S3.SS2.p5.11.m11.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p5.11.m11.1.1.3.3" xref="S3.SS2.p5.11.m11.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.11.m11.1.1.3.1a" xref="S3.SS2.p5.11.m11.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p5.11.m11.1.1.3.4" xref="S3.SS2.p5.11.m11.1.1.3.4.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.11.m11.1b"><apply id="S3.SS2.p5.11.m11.1.1.cmml" xref="S3.SS2.p5.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.11.m11.1.1.1.cmml" xref="S3.SS2.p5.11.m11.1.1">subscript</csymbol><ci id="S3.SS2.p5.11.m11.1.1.2.cmml" xref="S3.SS2.p5.11.m11.1.1.2">𝑁</ci><apply id="S3.SS2.p5.11.m11.1.1.3.cmml" xref="S3.SS2.p5.11.m11.1.1.3"><times id="S3.SS2.p5.11.m11.1.1.3.1.cmml" xref="S3.SS2.p5.11.m11.1.1.3.1"></times><ci id="S3.SS2.p5.11.m11.1.1.3.2.cmml" xref="S3.SS2.p5.11.m11.1.1.3.2">𝑟</ci><ci id="S3.SS2.p5.11.m11.1.1.3.3.cmml" xref="S3.SS2.p5.11.m11.1.1.3.3">𝑒</ci><ci id="S3.SS2.p5.11.m11.1.1.3.4.cmml" xref="S3.SS2.p5.11.m11.1.1.3.4">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.11.m11.1c">\mathit{N}_{reg}</annotation></semantics></math> are the effective number of anchors for the normalization.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para ltx_noindent">
<p id="S3.SS2.p6.11" class="ltx_p"><span id="S3.SS2.p6.11.1" class="ltx_text ltx_font_bold">RCNN Knowledge Distillation Loss.</span>
We perform RoIAlign <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> on top-scoring proposals from RPN, extracting the region features from each feature pyramid level.
Each region feature is embedded through two FC layers, one for classification and the other for bounding box regression. We collect the outputs of both heads from the teacher and student to compute RCNN distillation loss, which is defined as
<math id="S3.SS2.p6.1.m1.1" class="ltx_Math" alttext="\mathit{L}^{\mathrm{RCNN}}_{\mathrm{KD}}=\frac{1}{\mathit{M}_{cls}}{\sum_{j=1}\mathit{L}_{cls}(p_{j},p_{j}^{*})}+\frac{1}{\mathit{M}_{reg}}{\sum_{j=1}\mathit{L}_{reg}(t_{j},t_{j}^{*})}." display="inline"><semantics id="S3.SS2.p6.1.m1.1a"><mrow id="S3.SS2.p6.1.m1.1.1.1" xref="S3.SS2.p6.1.m1.1.1.1.1.cmml"><mrow id="S3.SS2.p6.1.m1.1.1.1.1" xref="S3.SS2.p6.1.m1.1.1.1.1.cmml"><msubsup id="S3.SS2.p6.1.m1.1.1.1.1.6" xref="S3.SS2.p6.1.m1.1.1.1.1.6.cmml"><mi id="S3.SS2.p6.1.m1.1.1.1.1.6.2.2" xref="S3.SS2.p6.1.m1.1.1.1.1.6.2.2.cmml">L</mi><mi id="S3.SS2.p6.1.m1.1.1.1.1.6.3" xref="S3.SS2.p6.1.m1.1.1.1.1.6.3.cmml">KD</mi><mi id="S3.SS2.p6.1.m1.1.1.1.1.6.2.3" xref="S3.SS2.p6.1.m1.1.1.1.1.6.2.3.cmml">RCNN</mi></msubsup><mo id="S3.SS2.p6.1.m1.1.1.1.1.5" xref="S3.SS2.p6.1.m1.1.1.1.1.5.cmml">=</mo><mrow id="S3.SS2.p6.1.m1.1.1.1.1.4" xref="S3.SS2.p6.1.m1.1.1.1.1.4.cmml"><mrow id="S3.SS2.p6.1.m1.1.1.1.1.2.2" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.cmml"><mfrac id="S3.SS2.p6.1.m1.1.1.1.1.2.2.4" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.cmml"><mn id="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.2" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.2.cmml">1</mn><msub id="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.cmml"><mi id="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.2" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.2.cmml">M</mi><mrow id="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.3" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.3.cmml"><mi id="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.3.2" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.3.1" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.3.1.cmml">​</mo><mi id="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.3.3" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.3.1a" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.3.1.cmml">​</mo><mi id="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.3.4" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.3.4.cmml">s</mi></mrow></msub></mfrac><mo lspace="0em" rspace="0em" id="S3.SS2.p6.1.m1.1.1.1.1.2.2.3" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.3.cmml">​</mo><mrow id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.cmml"><msub id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.3" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.3.cmml"><mo id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.3.2" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.3.2.cmml">∑</mo><mrow id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.3.3" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.3.3.cmml"><mi id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.3.3.2" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.3.3.2.cmml">j</mi><mo id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.3.3.1" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.3.3.1.cmml">=</mo><mn id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.3.3.3" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.3.3.3.cmml">1</mn></mrow></msub><mrow id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.cmml"><msub id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.cmml"><mi id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.2" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.2.cmml">L</mi><mrow id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.3" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.3.cmml"><mi id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.3.2" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.3.1" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.3.1.cmml">​</mo><mi id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.3.3" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.3.1a" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.3.1.cmml">​</mo><mi id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.3.4" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.3.4.cmml">s</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.3" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.3.cmml">​</mo><mrow id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.2" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.2.3" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.3.cmml">(</mo><msub id="S3.SS2.p6.1.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.SS2.p6.1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p6.1.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.SS2.p6.1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">p</mi><mi id="S3.SS2.p6.1.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.SS2.p6.1.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.2.4" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.3.cmml">,</mo><msubsup id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.2.2" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.2.2.cmml"><mi id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.2.2.2.2" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.2.2.2.2.cmml">p</mi><mi id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.2.2.2.3" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.2.2.2.3.cmml">j</mi><mo id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.2.2.3" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.2.2.3.cmml">∗</mo></msubsup><mo stretchy="false" id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.2.5" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.SS2.p6.1.m1.1.1.1.1.4.5" xref="S3.SS2.p6.1.m1.1.1.1.1.4.5.cmml">+</mo><mrow id="S3.SS2.p6.1.m1.1.1.1.1.4.4" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.cmml"><mfrac id="S3.SS2.p6.1.m1.1.1.1.1.4.4.4" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.cmml"><mn id="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.2" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.2.cmml">1</mn><msub id="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.cmml"><mi id="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.2" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.2.cmml">M</mi><mrow id="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.3" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.3.cmml"><mi id="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.3.2" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.3.1" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.3.1.cmml">​</mo><mi id="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.3.3" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.3.1a" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.3.1.cmml">​</mo><mi id="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.3.4" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.3.4.cmml">g</mi></mrow></msub></mfrac><mo lspace="0em" rspace="0em" id="S3.SS2.p6.1.m1.1.1.1.1.4.4.3" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.3.cmml">​</mo><mrow id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.cmml"><msub id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.3" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.3.cmml"><mo id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.3.2" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.3.2.cmml">∑</mo><mrow id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.3.3" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.3.3.cmml"><mi id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.3.3.2" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.3.3.2.cmml">j</mi><mo id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.3.3.1" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.3.3.1.cmml">=</mo><mn id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.3.3.3" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.3.3.3.cmml">1</mn></mrow></msub><mrow id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.cmml"><msub id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.cmml"><mi id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.2" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.2.cmml">L</mi><mrow id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.3" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.3.cmml"><mi id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.3.2" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.3.1" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.3.1.cmml">​</mo><mi id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.3.3" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.3.1a" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.3.1.cmml">​</mo><mi id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.3.4" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.3.4.cmml">g</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.3" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.3.cmml">​</mo><mrow id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.2" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.2.3" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.3.cmml">(</mo><msub id="S3.SS2.p6.1.m1.1.1.1.1.3.3.1.1.1.1.1" xref="S3.SS2.p6.1.m1.1.1.1.1.3.3.1.1.1.1.1.cmml"><mi id="S3.SS2.p6.1.m1.1.1.1.1.3.3.1.1.1.1.1.2" xref="S3.SS2.p6.1.m1.1.1.1.1.3.3.1.1.1.1.1.2.cmml">t</mi><mi id="S3.SS2.p6.1.m1.1.1.1.1.3.3.1.1.1.1.1.3" xref="S3.SS2.p6.1.m1.1.1.1.1.3.3.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.2.4" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.3.cmml">,</mo><msubsup id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.2.2" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.2.2.cmml"><mi id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.2.2.2.2" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.2.2.2.2.cmml">t</mi><mi id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.2.2.2.3" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.2.2.2.3.cmml">j</mi><mo id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.2.2.3" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.2.2.3.cmml">∗</mo></msubsup><mo stretchy="false" id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.2.5" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo lspace="0em" id="S3.SS2.p6.1.m1.1.1.1.2" xref="S3.SS2.p6.1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.1.m1.1b"><apply id="S3.SS2.p6.1.m1.1.1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1"><eq id="S3.SS2.p6.1.m1.1.1.1.1.5.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.5"></eq><apply id="S3.SS2.p6.1.m1.1.1.1.1.6.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.6"><csymbol cd="ambiguous" id="S3.SS2.p6.1.m1.1.1.1.1.6.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.6">subscript</csymbol><apply id="S3.SS2.p6.1.m1.1.1.1.1.6.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.6"><csymbol cd="ambiguous" id="S3.SS2.p6.1.m1.1.1.1.1.6.2.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.6">superscript</csymbol><ci id="S3.SS2.p6.1.m1.1.1.1.1.6.2.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.6.2.2">𝐿</ci><ci id="S3.SS2.p6.1.m1.1.1.1.1.6.2.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.6.2.3">RCNN</ci></apply><ci id="S3.SS2.p6.1.m1.1.1.1.1.6.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.6.3">KD</ci></apply><apply id="S3.SS2.p6.1.m1.1.1.1.1.4.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4"><plus id="S3.SS2.p6.1.m1.1.1.1.1.4.5.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.5"></plus><apply id="S3.SS2.p6.1.m1.1.1.1.1.2.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2"><times id="S3.SS2.p6.1.m1.1.1.1.1.2.2.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.3"></times><apply id="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.4"><divide id="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.4"></divide><cn type="integer" id="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.2">1</cn><apply id="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3"><csymbol cd="ambiguous" id="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3">subscript</csymbol><ci id="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.2">𝑀</ci><apply id="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.3"><times id="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.3.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.3.1"></times><ci id="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.3.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.3.2">𝑐</ci><ci id="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.3.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.3.3">𝑙</ci><ci id="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.3.4.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.4.3.3.4">𝑠</ci></apply></apply></apply><apply id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2"><apply id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.3.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.3">subscript</csymbol><sum id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.3.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.3.2"></sum><apply id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.3.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.3.3"><eq id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.3.3.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.3.3.1"></eq><ci id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.3.3.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.3.3.2">𝑗</ci><cn type="integer" id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.3.3.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.3.3.3">1</cn></apply></apply><apply id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2"><times id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.3"></times><apply id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4"><csymbol cd="ambiguous" id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4">subscript</csymbol><ci id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.2">𝐿</ci><apply id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.3"><times id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.3.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.3.1"></times><ci id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.3.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.3.2">𝑐</ci><ci id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.3.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.3.3">𝑙</ci><ci id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.3.4.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.4.3.4">𝑠</ci></apply></apply><interval closure="open" id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.2"><apply id="S3.SS2.p6.1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p6.1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.1.1.1.1.1.1.1.2">𝑝</ci><ci id="S3.SS2.p6.1.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.1.1.1.1.1.1.1.3">𝑗</ci></apply><apply id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.2.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.2.2.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.2.2">superscript</csymbol><apply id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.2.2.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.2.2.2.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.2.2.2.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.2.2.2.2">𝑝</ci><ci id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.2.2.2.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.2.2.2.3">𝑗</ci></apply><times id="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.2.2.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.2.2.2.2.2.2.2.3"></times></apply></interval></apply></apply></apply><apply id="S3.SS2.p6.1.m1.1.1.1.1.4.4.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4"><times id="S3.SS2.p6.1.m1.1.1.1.1.4.4.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.3"></times><apply id="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.4"><divide id="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.4"></divide><cn type="integer" id="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.2">1</cn><apply id="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3"><csymbol cd="ambiguous" id="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3">subscript</csymbol><ci id="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.2">𝑀</ci><apply id="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.3"><times id="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.3.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.3.1"></times><ci id="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.3.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.3.2">𝑟</ci><ci id="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.3.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.3.3">𝑒</ci><ci id="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.3.4.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.4.3.3.4">𝑔</ci></apply></apply></apply><apply id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2"><apply id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.3"><csymbol cd="ambiguous" id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.3.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.3">subscript</csymbol><sum id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.3.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.3.2"></sum><apply id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.3.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.3.3"><eq id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.3.3.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.3.3.1"></eq><ci id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.3.3.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.3.3.2">𝑗</ci><cn type="integer" id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.3.3.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.3.3.3">1</cn></apply></apply><apply id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2"><times id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.3"></times><apply id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4"><csymbol cd="ambiguous" id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4">subscript</csymbol><ci id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.2">𝐿</ci><apply id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.3"><times id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.3.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.3.1"></times><ci id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.3.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.3.2">𝑟</ci><ci id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.3.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.3.3">𝑒</ci><ci id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.3.4.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.4.3.4">𝑔</ci></apply></apply><interval closure="open" id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.2"><apply id="S3.SS2.p6.1.m1.1.1.1.1.3.3.1.1.1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.1.m1.1.1.1.1.3.3.1.1.1.1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.3.3.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p6.1.m1.1.1.1.1.3.3.1.1.1.1.1.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.3.3.1.1.1.1.1.2">𝑡</ci><ci id="S3.SS2.p6.1.m1.1.1.1.1.3.3.1.1.1.1.1.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.3.3.1.1.1.1.1.3">𝑗</ci></apply><apply id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.2.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.2.2.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.2.2">superscript</csymbol><apply id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.2.2.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.2.2.2.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.2.2.2.2.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.2.2.2.2">𝑡</ci><ci id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.2.2.2.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.2.2.2.3">𝑗</ci></apply><times id="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.2.2.3.cmml" xref="S3.SS2.p6.1.m1.1.1.1.1.4.4.2.2.2.2.2.3"></times></apply></interval></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.1.m1.1c">\mathit{L}^{\mathrm{RCNN}}_{\mathrm{KD}}=\frac{1}{\mathit{M}_{cls}}{\sum_{j=1}\mathit{L}_{cls}(p_{j},p_{j}^{*})}+\frac{1}{\mathit{M}_{reg}}{\sum_{j=1}\mathit{L}_{reg}(t_{j},t_{j}^{*})}.</annotation></semantics></math>
Here, <math id="S3.SS2.p6.2.m2.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS2.p6.2.m2.1a"><mi id="S3.SS2.p6.2.m2.1.1" xref="S3.SS2.p6.2.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.2.m2.1b"><ci id="S3.SS2.p6.2.m2.1.1.cmml" xref="S3.SS2.p6.2.m2.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.2.m2.1c">j</annotation></semantics></math> is the index of a proposal.
<math id="S3.SS2.p6.3.m3.1" class="ltx_Math" alttext="p_{j}" display="inline"><semantics id="S3.SS2.p6.3.m3.1a"><msub id="S3.SS2.p6.3.m3.1.1" xref="S3.SS2.p6.3.m3.1.1.cmml"><mi id="S3.SS2.p6.3.m3.1.1.2" xref="S3.SS2.p6.3.m3.1.1.2.cmml">p</mi><mi id="S3.SS2.p6.3.m3.1.1.3" xref="S3.SS2.p6.3.m3.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.3.m3.1b"><apply id="S3.SS2.p6.3.m3.1.1.cmml" xref="S3.SS2.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.3.m3.1.1.1.cmml" xref="S3.SS2.p6.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p6.3.m3.1.1.2.cmml" xref="S3.SS2.p6.3.m3.1.1.2">𝑝</ci><ci id="S3.SS2.p6.3.m3.1.1.3.cmml" xref="S3.SS2.p6.3.m3.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.3.m3.1c">p_{j}</annotation></semantics></math> and <math id="S3.SS2.p6.4.m4.1" class="ltx_Math" alttext="p_{j}^{*}" display="inline"><semantics id="S3.SS2.p6.4.m4.1a"><msubsup id="S3.SS2.p6.4.m4.1.1" xref="S3.SS2.p6.4.m4.1.1.cmml"><mi id="S3.SS2.p6.4.m4.1.1.2.2" xref="S3.SS2.p6.4.m4.1.1.2.2.cmml">p</mi><mi id="S3.SS2.p6.4.m4.1.1.2.3" xref="S3.SS2.p6.4.m4.1.1.2.3.cmml">j</mi><mo id="S3.SS2.p6.4.m4.1.1.3" xref="S3.SS2.p6.4.m4.1.1.3.cmml">∗</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.4.m4.1b"><apply id="S3.SS2.p6.4.m4.1.1.cmml" xref="S3.SS2.p6.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.4.m4.1.1.1.cmml" xref="S3.SS2.p6.4.m4.1.1">superscript</csymbol><apply id="S3.SS2.p6.4.m4.1.1.2.cmml" xref="S3.SS2.p6.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.4.m4.1.1.2.1.cmml" xref="S3.SS2.p6.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p6.4.m4.1.1.2.2.cmml" xref="S3.SS2.p6.4.m4.1.1.2.2">𝑝</ci><ci id="S3.SS2.p6.4.m4.1.1.2.3.cmml" xref="S3.SS2.p6.4.m4.1.1.2.3">𝑗</ci></apply><times id="S3.SS2.p6.4.m4.1.1.3.cmml" xref="S3.SS2.p6.4.m4.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.4.m4.1c">p_{j}^{*}</annotation></semantics></math> are the mean subtracted classification logits obtained from the student and the teacher, respectively.
<math id="S3.SS2.p6.5.m5.1" class="ltx_Math" alttext="t_{j}" display="inline"><semantics id="S3.SS2.p6.5.m5.1a"><msub id="S3.SS2.p6.5.m5.1.1" xref="S3.SS2.p6.5.m5.1.1.cmml"><mi id="S3.SS2.p6.5.m5.1.1.2" xref="S3.SS2.p6.5.m5.1.1.2.cmml">t</mi><mi id="S3.SS2.p6.5.m5.1.1.3" xref="S3.SS2.p6.5.m5.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.5.m5.1b"><apply id="S3.SS2.p6.5.m5.1.1.cmml" xref="S3.SS2.p6.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.5.m5.1.1.1.cmml" xref="S3.SS2.p6.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p6.5.m5.1.1.2.cmml" xref="S3.SS2.p6.5.m5.1.1.2">𝑡</ci><ci id="S3.SS2.p6.5.m5.1.1.3.cmml" xref="S3.SS2.p6.5.m5.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.5.m5.1c">t_{j}</annotation></semantics></math> and <math id="S3.SS2.p6.6.m6.1" class="ltx_Math" alttext="t_{j}^{*}" display="inline"><semantics id="S3.SS2.p6.6.m6.1a"><msubsup id="S3.SS2.p6.6.m6.1.1" xref="S3.SS2.p6.6.m6.1.1.cmml"><mi id="S3.SS2.p6.6.m6.1.1.2.2" xref="S3.SS2.p6.6.m6.1.1.2.2.cmml">t</mi><mi id="S3.SS2.p6.6.m6.1.1.2.3" xref="S3.SS2.p6.6.m6.1.1.2.3.cmml">j</mi><mo id="S3.SS2.p6.6.m6.1.1.3" xref="S3.SS2.p6.6.m6.1.1.3.cmml">∗</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.6.m6.1b"><apply id="S3.SS2.p6.6.m6.1.1.cmml" xref="S3.SS2.p6.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.6.m6.1.1.1.cmml" xref="S3.SS2.p6.6.m6.1.1">superscript</csymbol><apply id="S3.SS2.p6.6.m6.1.1.2.cmml" xref="S3.SS2.p6.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.6.m6.1.1.2.1.cmml" xref="S3.SS2.p6.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p6.6.m6.1.1.2.2.cmml" xref="S3.SS2.p6.6.m6.1.1.2.2">𝑡</ci><ci id="S3.SS2.p6.6.m6.1.1.2.3.cmml" xref="S3.SS2.p6.6.m6.1.1.2.3">𝑗</ci></apply><times id="S3.SS2.p6.6.m6.1.1.3.cmml" xref="S3.SS2.p6.6.m6.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.6.m6.1c">t_{j}^{*}</annotation></semantics></math> are four parameterized coordinates for the proposal refinement obtained from the student and teacher, respectively.
<math id="S3.SS2.p6.7.m7.1" class="ltx_Math" alttext="\mathit{L}_{cls}" display="inline"><semantics id="S3.SS2.p6.7.m7.1a"><msub id="S3.SS2.p6.7.m7.1.1" xref="S3.SS2.p6.7.m7.1.1.cmml"><mi id="S3.SS2.p6.7.m7.1.1.2" xref="S3.SS2.p6.7.m7.1.1.2.cmml">L</mi><mrow id="S3.SS2.p6.7.m7.1.1.3" xref="S3.SS2.p6.7.m7.1.1.3.cmml"><mi id="S3.SS2.p6.7.m7.1.1.3.2" xref="S3.SS2.p6.7.m7.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.7.m7.1.1.3.1" xref="S3.SS2.p6.7.m7.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p6.7.m7.1.1.3.3" xref="S3.SS2.p6.7.m7.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.7.m7.1.1.3.1a" xref="S3.SS2.p6.7.m7.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p6.7.m7.1.1.3.4" xref="S3.SS2.p6.7.m7.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.7.m7.1b"><apply id="S3.SS2.p6.7.m7.1.1.cmml" xref="S3.SS2.p6.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.7.m7.1.1.1.cmml" xref="S3.SS2.p6.7.m7.1.1">subscript</csymbol><ci id="S3.SS2.p6.7.m7.1.1.2.cmml" xref="S3.SS2.p6.7.m7.1.1.2">𝐿</ci><apply id="S3.SS2.p6.7.m7.1.1.3.cmml" xref="S3.SS2.p6.7.m7.1.1.3"><times id="S3.SS2.p6.7.m7.1.1.3.1.cmml" xref="S3.SS2.p6.7.m7.1.1.3.1"></times><ci id="S3.SS2.p6.7.m7.1.1.3.2.cmml" xref="S3.SS2.p6.7.m7.1.1.3.2">𝑐</ci><ci id="S3.SS2.p6.7.m7.1.1.3.3.cmml" xref="S3.SS2.p6.7.m7.1.1.3.3">𝑙</ci><ci id="S3.SS2.p6.7.m7.1.1.3.4.cmml" xref="S3.SS2.p6.7.m7.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.7.m7.1c">\mathit{L}_{cls}</annotation></semantics></math> and <math id="S3.SS2.p6.8.m8.1" class="ltx_Math" alttext="\mathit{L}_{reg}" display="inline"><semantics id="S3.SS2.p6.8.m8.1a"><msub id="S3.SS2.p6.8.m8.1.1" xref="S3.SS2.p6.8.m8.1.1.cmml"><mi id="S3.SS2.p6.8.m8.1.1.2" xref="S3.SS2.p6.8.m8.1.1.2.cmml">L</mi><mrow id="S3.SS2.p6.8.m8.1.1.3" xref="S3.SS2.p6.8.m8.1.1.3.cmml"><mi id="S3.SS2.p6.8.m8.1.1.3.2" xref="S3.SS2.p6.8.m8.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.8.m8.1.1.3.1" xref="S3.SS2.p6.8.m8.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p6.8.m8.1.1.3.3" xref="S3.SS2.p6.8.m8.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.8.m8.1.1.3.1a" xref="S3.SS2.p6.8.m8.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p6.8.m8.1.1.3.4" xref="S3.SS2.p6.8.m8.1.1.3.4.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.8.m8.1b"><apply id="S3.SS2.p6.8.m8.1.1.cmml" xref="S3.SS2.p6.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.8.m8.1.1.1.cmml" xref="S3.SS2.p6.8.m8.1.1">subscript</csymbol><ci id="S3.SS2.p6.8.m8.1.1.2.cmml" xref="S3.SS2.p6.8.m8.1.1.2">𝐿</ci><apply id="S3.SS2.p6.8.m8.1.1.3.cmml" xref="S3.SS2.p6.8.m8.1.1.3"><times id="S3.SS2.p6.8.m8.1.1.3.1.cmml" xref="S3.SS2.p6.8.m8.1.1.3.1"></times><ci id="S3.SS2.p6.8.m8.1.1.3.2.cmml" xref="S3.SS2.p6.8.m8.1.1.3.2">𝑟</ci><ci id="S3.SS2.p6.8.m8.1.1.3.3.cmml" xref="S3.SS2.p6.8.m8.1.1.3.3">𝑒</ci><ci id="S3.SS2.p6.8.m8.1.1.3.4.cmml" xref="S3.SS2.p6.8.m8.1.1.3.4">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.8.m8.1c">\mathit{L}_{reg}</annotation></semantics></math> are MSE loss and smooth L1 loss, respectively.
We only impose <math id="S3.SS2.p6.9.m9.1" class="ltx_Math" alttext="\mathit{L}_{reg}" display="inline"><semantics id="S3.SS2.p6.9.m9.1a"><msub id="S3.SS2.p6.9.m9.1.1" xref="S3.SS2.p6.9.m9.1.1.cmml"><mi id="S3.SS2.p6.9.m9.1.1.2" xref="S3.SS2.p6.9.m9.1.1.2.cmml">L</mi><mrow id="S3.SS2.p6.9.m9.1.1.3" xref="S3.SS2.p6.9.m9.1.1.3.cmml"><mi id="S3.SS2.p6.9.m9.1.1.3.2" xref="S3.SS2.p6.9.m9.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.9.m9.1.1.3.1" xref="S3.SS2.p6.9.m9.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p6.9.m9.1.1.3.3" xref="S3.SS2.p6.9.m9.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.9.m9.1.1.3.1a" xref="S3.SS2.p6.9.m9.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p6.9.m9.1.1.3.4" xref="S3.SS2.p6.9.m9.1.1.3.4.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.9.m9.1b"><apply id="S3.SS2.p6.9.m9.1.1.cmml" xref="S3.SS2.p6.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.9.m9.1.1.1.cmml" xref="S3.SS2.p6.9.m9.1.1">subscript</csymbol><ci id="S3.SS2.p6.9.m9.1.1.2.cmml" xref="S3.SS2.p6.9.m9.1.1.2">𝐿</ci><apply id="S3.SS2.p6.9.m9.1.1.3.cmml" xref="S3.SS2.p6.9.m9.1.1.3"><times id="S3.SS2.p6.9.m9.1.1.3.1.cmml" xref="S3.SS2.p6.9.m9.1.1.3.1"></times><ci id="S3.SS2.p6.9.m9.1.1.3.2.cmml" xref="S3.SS2.p6.9.m9.1.1.3.2">𝑟</ci><ci id="S3.SS2.p6.9.m9.1.1.3.3.cmml" xref="S3.SS2.p6.9.m9.1.1.3.3">𝑒</ci><ci id="S3.SS2.p6.9.m9.1.1.3.4.cmml" xref="S3.SS2.p6.9.m9.1.1.3.4">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.9.m9.1c">\mathit{L}_{reg}</annotation></semantics></math> for the positive proposals that have an IoU larger than 0.5 <span id="S3.SS2.p6.11.2" class="ltx_text ltx_font_italic">with the augmented ground-truth boxes</span>.
<math id="S3.SS2.p6.10.m10.1" class="ltx_Math" alttext="\mathit{M}_{cls}(=512)" display="inline"><semantics id="S3.SS2.p6.10.m10.1a"><mrow id="S3.SS2.p6.10.m10.1.1" xref="S3.SS2.p6.10.m10.1.1.cmml"><msub id="S3.SS2.p6.10.m10.1.1.3" xref="S3.SS2.p6.10.m10.1.1.3.cmml"><mi id="S3.SS2.p6.10.m10.1.1.3.2" xref="S3.SS2.p6.10.m10.1.1.3.2.cmml">M</mi><mrow id="S3.SS2.p6.10.m10.1.1.3.3" xref="S3.SS2.p6.10.m10.1.1.3.3.cmml"><mi id="S3.SS2.p6.10.m10.1.1.3.3.2" xref="S3.SS2.p6.10.m10.1.1.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.10.m10.1.1.3.3.1" xref="S3.SS2.p6.10.m10.1.1.3.3.1.cmml">​</mo><mi id="S3.SS2.p6.10.m10.1.1.3.3.3" xref="S3.SS2.p6.10.m10.1.1.3.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.10.m10.1.1.3.3.1a" xref="S3.SS2.p6.10.m10.1.1.3.3.1.cmml">​</mo><mi id="S3.SS2.p6.10.m10.1.1.3.3.4" xref="S3.SS2.p6.10.m10.1.1.3.3.4.cmml">s</mi></mrow></msub><mspace width="0.3888888888888889em" id="S3.SS2.p6.10.m10.1.1a" xref="S3.SS2.p6.10.m10.1.1.cmml"></mspace><mrow id="S3.SS2.p6.10.m10.1.1.1.1" xref="S3.SS2.p6.10.m10.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p6.10.m10.1.1.1.1.2" xref="S3.SS2.p6.10.m10.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p6.10.m10.1.1.1.1.1" xref="S3.SS2.p6.10.m10.1.1.1.1.1.cmml"><mi id="S3.SS2.p6.10.m10.1.1.1.1.1.2" xref="S3.SS2.p6.10.m10.1.1.1.1.1.2.cmml"></mi><mo id="S3.SS2.p6.10.m10.1.1.1.1.1.1" xref="S3.SS2.p6.10.m10.1.1.1.1.1.1.cmml">=</mo><mn id="S3.SS2.p6.10.m10.1.1.1.1.1.3" xref="S3.SS2.p6.10.m10.1.1.1.1.1.3.cmml">512</mn></mrow><mo stretchy="false" id="S3.SS2.p6.10.m10.1.1.1.1.3" xref="S3.SS2.p6.10.m10.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.10.m10.1b"><apply id="S3.SS2.p6.10.m10.1.1.cmml" xref="S3.SS2.p6.10.m10.1.1"><csymbol cd="latexml" id="S3.SS2.p6.10.m10.1.1.2.cmml" xref="S3.SS2.p6.10.m10.1.1">annotated</csymbol><apply id="S3.SS2.p6.10.m10.1.1.3.cmml" xref="S3.SS2.p6.10.m10.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p6.10.m10.1.1.3.1.cmml" xref="S3.SS2.p6.10.m10.1.1.3">subscript</csymbol><ci id="S3.SS2.p6.10.m10.1.1.3.2.cmml" xref="S3.SS2.p6.10.m10.1.1.3.2">𝑀</ci><apply id="S3.SS2.p6.10.m10.1.1.3.3.cmml" xref="S3.SS2.p6.10.m10.1.1.3.3"><times id="S3.SS2.p6.10.m10.1.1.3.3.1.cmml" xref="S3.SS2.p6.10.m10.1.1.3.3.1"></times><ci id="S3.SS2.p6.10.m10.1.1.3.3.2.cmml" xref="S3.SS2.p6.10.m10.1.1.3.3.2">𝑐</ci><ci id="S3.SS2.p6.10.m10.1.1.3.3.3.cmml" xref="S3.SS2.p6.10.m10.1.1.3.3.3">𝑙</ci><ci id="S3.SS2.p6.10.m10.1.1.3.3.4.cmml" xref="S3.SS2.p6.10.m10.1.1.3.3.4">𝑠</ci></apply></apply><apply id="S3.SS2.p6.10.m10.1.1.1.1.1.cmml" xref="S3.SS2.p6.10.m10.1.1.1.1"><eq id="S3.SS2.p6.10.m10.1.1.1.1.1.1.cmml" xref="S3.SS2.p6.10.m10.1.1.1.1.1.1"></eq><csymbol cd="latexml" id="S3.SS2.p6.10.m10.1.1.1.1.1.2.cmml" xref="S3.SS2.p6.10.m10.1.1.1.1.1.2">absent</csymbol><cn type="integer" id="S3.SS2.p6.10.m10.1.1.1.1.1.3.cmml" xref="S3.SS2.p6.10.m10.1.1.1.1.1.3">512</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.10.m10.1c">\mathit{M}_{cls}(=512)</annotation></semantics></math> and <math id="S3.SS2.p6.11.m11.1" class="ltx_Math" alttext="\mathit{M}_{reg}" display="inline"><semantics id="S3.SS2.p6.11.m11.1a"><msub id="S3.SS2.p6.11.m11.1.1" xref="S3.SS2.p6.11.m11.1.1.cmml"><mi id="S3.SS2.p6.11.m11.1.1.2" xref="S3.SS2.p6.11.m11.1.1.2.cmml">M</mi><mrow id="S3.SS2.p6.11.m11.1.1.3" xref="S3.SS2.p6.11.m11.1.1.3.cmml"><mi id="S3.SS2.p6.11.m11.1.1.3.2" xref="S3.SS2.p6.11.m11.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.11.m11.1.1.3.1" xref="S3.SS2.p6.11.m11.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p6.11.m11.1.1.3.3" xref="S3.SS2.p6.11.m11.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.11.m11.1.1.3.1a" xref="S3.SS2.p6.11.m11.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p6.11.m11.1.1.3.4" xref="S3.SS2.p6.11.m11.1.1.3.4.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.11.m11.1b"><apply id="S3.SS2.p6.11.m11.1.1.cmml" xref="S3.SS2.p6.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.11.m11.1.1.1.cmml" xref="S3.SS2.p6.11.m11.1.1">subscript</csymbol><ci id="S3.SS2.p6.11.m11.1.1.2.cmml" xref="S3.SS2.p6.11.m11.1.1.2">𝑀</ci><apply id="S3.SS2.p6.11.m11.1.1.3.cmml" xref="S3.SS2.p6.11.m11.1.1.3"><times id="S3.SS2.p6.11.m11.1.1.3.1.cmml" xref="S3.SS2.p6.11.m11.1.1.3.1"></times><ci id="S3.SS2.p6.11.m11.1.1.3.2.cmml" xref="S3.SS2.p6.11.m11.1.1.3.2">𝑟</ci><ci id="S3.SS2.p6.11.m11.1.1.3.3.cmml" xref="S3.SS2.p6.11.m11.1.1.3.3">𝑒</ci><ci id="S3.SS2.p6.11.m11.1.1.3.4.cmml" xref="S3.SS2.p6.11.m11.1.1.3.4">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.11.m11.1c">\mathit{M}_{reg}</annotation></semantics></math> are the effective number of proposals for the normalization.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S3.F3.4" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\vstretch</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.F3.1" class="ltx_p ltx_figure_panel ltx_align_center">0.95<img src="/html/2212.10147/assets/figures/im_to_vid_rel.png" id="S3.F3.1.g1" class="ltx_graphics ltx_img_square" width="568" height="465" alt="Refer to caption"></p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
<span id="S3.F3.6.2" class="ltx_text ltx_font_bold">(a) Two standard image to video transfer learning setups.</span>
In typical, a naive transfer learning from images to videos leads to catastrophic forgetting due to the missing annotations in the video.
We present a generic teacher-student scheme that works on both scenarios.
<span id="S3.F3.3.1" class="ltx_text ltx_font_bold">(b) Two-step approach for COCO <math id="S3.F3.3.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.F3.3.1.m1.1b"><mo stretchy="false" id="S3.F3.3.1.m1.1.1" xref="S3.F3.3.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.F3.3.1.m1.1c"><ci id="S3.F3.3.1.m1.1.1.cmml" xref="S3.F3.3.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.3.1.m1.1d">\rightarrow</annotation></semantics></math> YTVIS transfer learning setup.</span>
We first learn new object classifier weights with the pre-trained classifier as a fixed anchor and then fine-tune the whole classifier through the proposed teacher-student scheme.
The red-dotted line along the circle in the set relationship figure indicates the training data used in each stage.
The shape figures (e.g., square, triangle) and the separating line denote class instances and the associated classifier.
</figcaption>
</figure>
<div id="S3.SS2.p7" class="ltx_para ltx_noindent">
<p id="S3.SS2.p7.2" class="ltx_p"><span id="S3.SS2.p7.2.1" class="ltx_text ltx_font_bold">Correcting Negatives in Computing the Detection Loss.</span>
We avoid sampling the anchors or proposals that have significant IoU overlaps <span id="S3.SS2.p7.2.2" class="ltx_text ltx_font_italic">with the augmented ground-truth boxes</span> as a background (<math id="S3.SS2.p7.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.SS2.p7.1.m1.1a"><mo id="S3.SS2.p7.1.m1.1.1" xref="S3.SS2.p7.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.1.m1.1b"><gt id="S3.SS2.p7.1.m1.1.1.cmml" xref="S3.SS2.p7.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.1.m1.1c">&gt;</annotation></semantics></math> 0.7 for RPN and <math id="S3.SS2.p7.2.m2.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.SS2.p7.2.m2.1a"><mo id="S3.SS2.p7.2.m2.1.1" xref="S3.SS2.p7.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.2.m2.1b"><gt id="S3.SS2.p7.2.m2.1.1.cmml" xref="S3.SS2.p7.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.2.m2.1c">&gt;</annotation></semantics></math> 0.5 for RCNN).
We note that positives are only sampled based on the provided original ground truth labels.
This is because the detectors, especially the large vocabulary detectors, suffer from predicting the precise labels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> while they are good at recalling the objects. We empirically verify this in the experiment.</p>
</div>
<div id="S3.SS2.p8" class="ltx_para ltx_noindent">
<p id="S3.SS2.p8.2" class="ltx_p"><span id="S3.SS2.p8.2.1" class="ltx_text ltx_font_bold">Extension to other transfer learning setup.</span>
COCO to YTVIS is another important transfer learning setup (see Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.2 Learn to Unforget in TAO ‣ 3 Proposed Method ‣ Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>-(a)).
This is more challenging than LVIS to TAO, as the superset-subset relationship does not hold, and new object categories to learn are added.
To deal with this new pattern, we take a two-step approach (see Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.2 Learn to Unforget in TAO ‣ 3 Proposed Method ‣ Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>-(b)).
First, we adapt the RCNN classifier of the pre-trained model, increasing the number of output channels to accommodate newly added classes, and train on the videos, <math id="S3.SS2.p8.1.m1.1" class="ltx_Math" alttext="\mathrm{YTVIS}-\mathrm{COCO}" display="inline"><semantics id="S3.SS2.p8.1.m1.1a"><mrow id="S3.SS2.p8.1.m1.1.1" xref="S3.SS2.p8.1.m1.1.1.cmml"><mi id="S3.SS2.p8.1.m1.1.1.2" xref="S3.SS2.p8.1.m1.1.1.2.cmml">YTVIS</mi><mo id="S3.SS2.p8.1.m1.1.1.1" xref="S3.SS2.p8.1.m1.1.1.1.cmml">−</mo><mi id="S3.SS2.p8.1.m1.1.1.3" xref="S3.SS2.p8.1.m1.1.1.3.cmml">COCO</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.1.m1.1b"><apply id="S3.SS2.p8.1.m1.1.1.cmml" xref="S3.SS2.p8.1.m1.1.1"><minus id="S3.SS2.p8.1.m1.1.1.1.cmml" xref="S3.SS2.p8.1.m1.1.1.1"></minus><ci id="S3.SS2.p8.1.m1.1.1.2.cmml" xref="S3.SS2.p8.1.m1.1.1.2">YTVIS</ci><ci id="S3.SS2.p8.1.m1.1.1.3.cmml" xref="S3.SS2.p8.1.m1.1.1.3">COCO</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.1.m1.1c">\mathrm{YTVIS}-\mathrm{COCO}</annotation></semantics></math>, that contain new object categories.
In practice, we freeze the original detector, and thus the past information is intact, and only the newly added weight matrices are updated accordingly.
The key idea here is to use the original pretrained weight as an anchor and update the newly added weight to be compatible.
Second, after sufficient training of the new weights, we now unlock the original detector and update the whole weights with the remaining videos, <math id="S3.SS2.p8.2.m2.1" class="ltx_Math" alttext="\mathrm{YTVIS}\cap\mathrm{COCO}" display="inline"><semantics id="S3.SS2.p8.2.m2.1a"><mrow id="S3.SS2.p8.2.m2.1.1" xref="S3.SS2.p8.2.m2.1.1.cmml"><mi id="S3.SS2.p8.2.m2.1.1.2" xref="S3.SS2.p8.2.m2.1.1.2.cmml">YTVIS</mi><mo id="S3.SS2.p8.2.m2.1.1.1" xref="S3.SS2.p8.2.m2.1.1.1.cmml">∩</mo><mi id="S3.SS2.p8.2.m2.1.1.3" xref="S3.SS2.p8.2.m2.1.1.3.cmml">COCO</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.2.m2.1b"><apply id="S3.SS2.p8.2.m2.1.1.cmml" xref="S3.SS2.p8.2.m2.1.1"><intersect id="S3.SS2.p8.2.m2.1.1.1.cmml" xref="S3.SS2.p8.2.m2.1.1.1"></intersect><ci id="S3.SS2.p8.2.m2.1.1.2.cmml" xref="S3.SS2.p8.2.m2.1.1.2">YTVIS</ci><ci id="S3.SS2.p8.2.m2.1.1.3.cmml" xref="S3.SS2.p8.2.m2.1.1.3">COCO</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.2.m2.1c">\mathrm{YTVIS}\cap\mathrm{COCO}</annotation></semantics></math>, using the presented teacher-student scheme.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Regularizing Semantic Flickering</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.6" class="ltx_p">One of the common tracking failures in large vocabulary tracking is due to semantic flicker between similar object categories <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
To cope with this issue, we attempt to regularize the model during training with a new objective function, namely semantic consistency loss.
The proposal is motivated by the temporal consistency loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, which enforces the outputs of the model for corresponding pixels (or patches) in video frames to be consistent. It is often used in video processing tasks to ensure the output temporal smoothness at a pixel level.
The proposal extends this idea from pixels to instances; We enforce the class predictions of the same instances in two different frames to be equivalent.
In practice, we forward the ground truth bounding boxes of the same instance in two different frames to the RCNN head.
The mean subtracted classification logits, <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">p</annotation></semantics></math>, are used for the consistency regularization as,
<math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="\mathit{L}_{\mathrm{Semcon}}=|p^{t}-p^{t+\tau}|_{2}." display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mrow id="S3.SS3.p1.2.m2.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.1.cmml"><mrow id="S3.SS3.p1.2.m2.1.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.1.cmml"><msub id="S3.SS3.p1.2.m2.1.1.1.1.3" xref="S3.SS3.p1.2.m2.1.1.1.1.3.cmml"><mi id="S3.SS3.p1.2.m2.1.1.1.1.3.2" xref="S3.SS3.p1.2.m2.1.1.1.1.3.2.cmml">L</mi><mi id="S3.SS3.p1.2.m2.1.1.1.1.3.3" xref="S3.SS3.p1.2.m2.1.1.1.1.3.3.cmml">Semcon</mi></msub><mo id="S3.SS3.p1.2.m2.1.1.1.1.2" xref="S3.SS3.p1.2.m2.1.1.1.1.2.cmml">=</mo><msub id="S3.SS3.p1.2.m2.1.1.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.1.1.cmml"><mrow id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.2" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.2.1.cmml">|</mo><mrow id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.cmml"><msup id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.2" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.2.2" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.2.2.cmml">p</mi><mi id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.2.3" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.2.3.cmml">t</mi></msup><mo id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.1.cmml">−</mo><msup id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.3" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.3.2" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.3.2.cmml">p</mi><mrow id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.3.3" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.3.3.2" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.3.3.2.cmml">t</mi><mo id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.3.3.1" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.3.3.1.cmml">+</mo><mi id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.3.3.3" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.3.3.3.cmml">τ</mi></mrow></msup></mrow><mo stretchy="false" id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.3" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.2.1.cmml">|</mo></mrow><mn id="S3.SS3.p1.2.m2.1.1.1.1.1.3" xref="S3.SS3.p1.2.m2.1.1.1.1.1.3.cmml">2</mn></msub></mrow><mo lspace="0em" id="S3.SS3.p1.2.m2.1.1.1.2" xref="S3.SS3.p1.2.m2.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1"><eq id="S3.SS3.p1.2.m2.1.1.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.2"></eq><apply id="S3.SS3.p1.2.m2.1.1.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.1.3.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.3">subscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.1.1.3.2.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.3.2">𝐿</ci><ci id="S3.SS3.p1.2.m2.1.1.1.1.3.3.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.3.3">Semcon</ci></apply><apply id="S3.SS3.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1">subscript</csymbol><apply id="S3.SS3.p1.2.m2.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1"><abs id="S3.SS3.p1.2.m2.1.1.1.1.1.1.2.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.2"></abs><apply id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1"><minus id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.2">superscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.2.2">𝑝</ci><ci id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.2.3">𝑡</ci></apply><apply id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.3.2">𝑝</ci><apply id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.3.3"><plus id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.3.3.1"></plus><ci id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.3.3.2">𝑡</ci><ci id="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.1.1.1.3.3.3">𝜏</ci></apply></apply></apply></apply><cn type="integer" id="S3.SS3.p1.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">\mathit{L}_{\mathrm{Semcon}}=|p^{t}-p^{t+\tau}|_{2}.</annotation></semantics></math>
ere, <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="p^{t}" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><msup id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mi id="S3.SS3.p1.3.m3.1.1.2" xref="S3.SS3.p1.3.m3.1.1.2.cmml">p</mi><mi id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">superscript</csymbol><ci id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2">𝑝</ci><ci id="S3.SS3.p1.3.m3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">p^{t}</annotation></semantics></math> and <math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="p^{t+\tau}" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><msup id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml"><mi id="S3.SS3.p1.4.m4.1.1.2" xref="S3.SS3.p1.4.m4.1.1.2.cmml">p</mi><mrow id="S3.SS3.p1.4.m4.1.1.3" xref="S3.SS3.p1.4.m4.1.1.3.cmml"><mi id="S3.SS3.p1.4.m4.1.1.3.2" xref="S3.SS3.p1.4.m4.1.1.3.2.cmml">t</mi><mo id="S3.SS3.p1.4.m4.1.1.3.1" xref="S3.SS3.p1.4.m4.1.1.3.1.cmml">+</mo><mi id="S3.SS3.p1.4.m4.1.1.3.3" xref="S3.SS3.p1.4.m4.1.1.3.3.cmml">τ</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><apply id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m4.1.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">superscript</csymbol><ci id="S3.SS3.p1.4.m4.1.1.2.cmml" xref="S3.SS3.p1.4.m4.1.1.2">𝑝</ci><apply id="S3.SS3.p1.4.m4.1.1.3.cmml" xref="S3.SS3.p1.4.m4.1.1.3"><plus id="S3.SS3.p1.4.m4.1.1.3.1.cmml" xref="S3.SS3.p1.4.m4.1.1.3.1"></plus><ci id="S3.SS3.p1.4.m4.1.1.3.2.cmml" xref="S3.SS3.p1.4.m4.1.1.3.2">𝑡</ci><ci id="S3.SS3.p1.4.m4.1.1.3.3.cmml" xref="S3.SS3.p1.4.m4.1.1.3.3">𝜏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">p^{t+\tau}</annotation></semantics></math> denote the logits of the same instance in two different frames, <math id="S3.SS3.p1.5.m5.1" class="ltx_Math" alttext="I_{t}" display="inline"><semantics id="S3.SS3.p1.5.m5.1a"><msub id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml"><mi id="S3.SS3.p1.5.m5.1.1.2" xref="S3.SS3.p1.5.m5.1.1.2.cmml">I</mi><mi id="S3.SS3.p1.5.m5.1.1.3" xref="S3.SS3.p1.5.m5.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><apply id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.5.m5.1.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS3.p1.5.m5.1.1.2.cmml" xref="S3.SS3.p1.5.m5.1.1.2">𝐼</ci><ci id="S3.SS3.p1.5.m5.1.1.3.cmml" xref="S3.SS3.p1.5.m5.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">I_{t}</annotation></semantics></math> and <math id="S3.SS3.p1.6.m6.1" class="ltx_Math" alttext="I_{t+\tau}" display="inline"><semantics id="S3.SS3.p1.6.m6.1a"><msub id="S3.SS3.p1.6.m6.1.1" xref="S3.SS3.p1.6.m6.1.1.cmml"><mi id="S3.SS3.p1.6.m6.1.1.2" xref="S3.SS3.p1.6.m6.1.1.2.cmml">I</mi><mrow id="S3.SS3.p1.6.m6.1.1.3" xref="S3.SS3.p1.6.m6.1.1.3.cmml"><mi id="S3.SS3.p1.6.m6.1.1.3.2" xref="S3.SS3.p1.6.m6.1.1.3.2.cmml">t</mi><mo id="S3.SS3.p1.6.m6.1.1.3.1" xref="S3.SS3.p1.6.m6.1.1.3.1.cmml">+</mo><mi id="S3.SS3.p1.6.m6.1.1.3.3" xref="S3.SS3.p1.6.m6.1.1.3.3.cmml">τ</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m6.1b"><apply id="S3.SS3.p1.6.m6.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.6.m6.1.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.p1.6.m6.1.1.2.cmml" xref="S3.SS3.p1.6.m6.1.1.2">𝐼</ci><apply id="S3.SS3.p1.6.m6.1.1.3.cmml" xref="S3.SS3.p1.6.m6.1.1.3"><plus id="S3.SS3.p1.6.m6.1.1.3.1.cmml" xref="S3.SS3.p1.6.m6.1.1.3.1"></plus><ci id="S3.SS3.p1.6.m6.1.1.3.2.cmml" xref="S3.SS3.p1.6.m6.1.1.3.2">𝑡</ci><ci id="S3.SS3.p1.6.m6.1.1.3.3.cmml" xref="S3.SS3.p1.6.m6.1.1.3.3">𝜏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m6.1c">I_{t+\tau}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Unified Learning</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.5" class="ltx_p">Within our proposed learning framework (see Fig. <a href="#S2.F2" title="Figure 2 ‣ 2 Related work ‣ Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), we can train the whole video model, learning detection and tracking jointly, using all available image and video datasets.
The final objective function can be summarized as</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.24" class="ltx_Math" alttext="\begin{split}\mathit{L}=\lambda_{\mathrm{1}}\mathit{L}_{\mathrm{Det}}+\lambda_{\mathrm{2}}\mathit{L}_{\mathrm{Track}}+\lambda_{\mathrm{3}}\mathit{L}_{\mathrm{KD}}+\lambda_{\mathrm{4}}\mathit{L}_{\mathrm{Semcon}},\end{split}" display="block"><semantics id="S3.E1.m1.24a"><mtable displaystyle="true" id="S3.E1.m1.24.24.2"><mtr id="S3.E1.m1.24.24.2a"><mtd class="ltx_align_right" columnalign="right" id="S3.E1.m1.24.24.2b"><mrow id="S3.E1.m1.24.24.2.23.23.23.23"><mrow id="S3.E1.m1.24.24.2.23.23.23.23.1"><mi id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml">L</mi><mo id="S3.E1.m1.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.cmml">=</mo><mrow id="S3.E1.m1.24.24.2.23.23.23.23.1.1"><mrow id="S3.E1.m1.24.24.2.23.23.23.23.1.1.1"><msub id="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.2"><mi id="S3.E1.m1.3.3.3.3.3.3" xref="S3.E1.m1.3.3.3.3.3.3.cmml">λ</mi><mn id="S3.E1.m1.4.4.4.4.4.4.1" xref="S3.E1.m1.4.4.4.4.4.4.1.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.1" xref="S3.E1.m1.23.23.1.1.1.cmml">​</mo><msub id="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.3"><mi id="S3.E1.m1.5.5.5.5.5.5" xref="S3.E1.m1.5.5.5.5.5.5.cmml">L</mi><mi id="S3.E1.m1.6.6.6.6.6.6.1" xref="S3.E1.m1.6.6.6.6.6.6.1.cmml">Det</mi></msub></mrow><mo id="S3.E1.m1.7.7.7.7.7.7" xref="S3.E1.m1.7.7.7.7.7.7.cmml">+</mo><mrow id="S3.E1.m1.24.24.2.23.23.23.23.1.1.2"><msub id="S3.E1.m1.24.24.2.23.23.23.23.1.1.2.2"><mi id="S3.E1.m1.8.8.8.8.8.8" xref="S3.E1.m1.8.8.8.8.8.8.cmml">λ</mi><mn id="S3.E1.m1.9.9.9.9.9.9.1" xref="S3.E1.m1.9.9.9.9.9.9.1.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.24.24.2.23.23.23.23.1.1.2.1" xref="S3.E1.m1.23.23.1.1.1.cmml">​</mo><msub id="S3.E1.m1.24.24.2.23.23.23.23.1.1.2.3"><mi id="S3.E1.m1.10.10.10.10.10.10" xref="S3.E1.m1.10.10.10.10.10.10.cmml">L</mi><mi id="S3.E1.m1.11.11.11.11.11.11.1" xref="S3.E1.m1.11.11.11.11.11.11.1.cmml">Track</mi></msub></mrow><mo id="S3.E1.m1.7.7.7.7.7.7a" xref="S3.E1.m1.7.7.7.7.7.7.cmml">+</mo><mrow id="S3.E1.m1.24.24.2.23.23.23.23.1.1.3"><msub id="S3.E1.m1.24.24.2.23.23.23.23.1.1.3.2"><mi id="S3.E1.m1.13.13.13.13.13.13" xref="S3.E1.m1.13.13.13.13.13.13.cmml">λ</mi><mn id="S3.E1.m1.14.14.14.14.14.14.1" xref="S3.E1.m1.14.14.14.14.14.14.1.cmml">3</mn></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.24.24.2.23.23.23.23.1.1.3.1" xref="S3.E1.m1.23.23.1.1.1.cmml">​</mo><msub id="S3.E1.m1.24.24.2.23.23.23.23.1.1.3.3"><mi id="S3.E1.m1.15.15.15.15.15.15" xref="S3.E1.m1.15.15.15.15.15.15.cmml">L</mi><mi id="S3.E1.m1.16.16.16.16.16.16.1" xref="S3.E1.m1.16.16.16.16.16.16.1.cmml">KD</mi></msub></mrow><mo id="S3.E1.m1.7.7.7.7.7.7b" xref="S3.E1.m1.7.7.7.7.7.7.cmml">+</mo><mrow id="S3.E1.m1.24.24.2.23.23.23.23.1.1.4"><msub id="S3.E1.m1.24.24.2.23.23.23.23.1.1.4.2"><mi id="S3.E1.m1.18.18.18.18.18.18" xref="S3.E1.m1.18.18.18.18.18.18.cmml">λ</mi><mn id="S3.E1.m1.19.19.19.19.19.19.1" xref="S3.E1.m1.19.19.19.19.19.19.1.cmml">4</mn></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.24.24.2.23.23.23.23.1.1.4.1" xref="S3.E1.m1.23.23.1.1.1.cmml">​</mo><msub id="S3.E1.m1.24.24.2.23.23.23.23.1.1.4.3"><mi id="S3.E1.m1.20.20.20.20.20.20" xref="S3.E1.m1.20.20.20.20.20.20.cmml">L</mi><mi id="S3.E1.m1.21.21.21.21.21.21.1" xref="S3.E1.m1.21.21.21.21.21.21.1.cmml">Semcon</mi></msub></mrow></mrow></mrow><mo id="S3.E1.m1.22.22.22.22.22.22" xref="S3.E1.m1.23.23.1.1.1.cmml">,</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E1.m1.24b"><apply id="S3.E1.m1.23.23.1.1.1.cmml" xref="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.1"><eq id="S3.E1.m1.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2"></eq><ci id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1">𝐿</ci><apply id="S3.E1.m1.23.23.1.1.1.3.cmml" xref="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.1"><plus id="S3.E1.m1.7.7.7.7.7.7.cmml" xref="S3.E1.m1.7.7.7.7.7.7"></plus><apply id="S3.E1.m1.23.23.1.1.1.3.2.cmml" xref="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.1"><times id="S3.E1.m1.23.23.1.1.1.3.2.1.cmml" xref="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.1"></times><apply id="S3.E1.m1.23.23.1.1.1.3.2.2.cmml" xref="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.23.23.1.1.1.3.2.2.1.cmml" xref="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.3.3.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3.3.3">𝜆</ci><cn type="integer" id="S3.E1.m1.4.4.4.4.4.4.1.cmml" xref="S3.E1.m1.4.4.4.4.4.4.1">1</cn></apply><apply id="S3.E1.m1.23.23.1.1.1.3.2.3.cmml" xref="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.23.23.1.1.1.3.2.3.1.cmml" xref="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.5.5.5.5.5.5.cmml" xref="S3.E1.m1.5.5.5.5.5.5">𝐿</ci><ci id="S3.E1.m1.6.6.6.6.6.6.1.cmml" xref="S3.E1.m1.6.6.6.6.6.6.1">Det</ci></apply></apply><apply id="S3.E1.m1.23.23.1.1.1.3.3.cmml" xref="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.1"><times id="S3.E1.m1.23.23.1.1.1.3.3.1.cmml" xref="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.1"></times><apply id="S3.E1.m1.23.23.1.1.1.3.3.2.cmml" xref="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.23.23.1.1.1.3.3.2.1.cmml" xref="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.8.8.8.8.8.8.cmml" xref="S3.E1.m1.8.8.8.8.8.8">𝜆</ci><cn type="integer" id="S3.E1.m1.9.9.9.9.9.9.1.cmml" xref="S3.E1.m1.9.9.9.9.9.9.1">2</cn></apply><apply id="S3.E1.m1.23.23.1.1.1.3.3.3.cmml" xref="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.23.23.1.1.1.3.3.3.1.cmml" xref="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.10.10.10.10.10.10.cmml" xref="S3.E1.m1.10.10.10.10.10.10">𝐿</ci><ci id="S3.E1.m1.11.11.11.11.11.11.1.cmml" xref="S3.E1.m1.11.11.11.11.11.11.1">Track</ci></apply></apply><apply id="S3.E1.m1.23.23.1.1.1.3.4.cmml" xref="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.1"><times id="S3.E1.m1.23.23.1.1.1.3.4.1.cmml" xref="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.1"></times><apply id="S3.E1.m1.23.23.1.1.1.3.4.2.cmml" xref="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.23.23.1.1.1.3.4.2.1.cmml" xref="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.13.13.13.13.13.13.cmml" xref="S3.E1.m1.13.13.13.13.13.13">𝜆</ci><cn type="integer" id="S3.E1.m1.14.14.14.14.14.14.1.cmml" xref="S3.E1.m1.14.14.14.14.14.14.1">3</cn></apply><apply id="S3.E1.m1.23.23.1.1.1.3.4.3.cmml" xref="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.23.23.1.1.1.3.4.3.1.cmml" xref="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.15.15.15.15.15.15.cmml" xref="S3.E1.m1.15.15.15.15.15.15">𝐿</ci><ci id="S3.E1.m1.16.16.16.16.16.16.1.cmml" xref="S3.E1.m1.16.16.16.16.16.16.1">KD</ci></apply></apply><apply id="S3.E1.m1.23.23.1.1.1.3.5.cmml" xref="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.1"><times id="S3.E1.m1.23.23.1.1.1.3.5.1.cmml" xref="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.1"></times><apply id="S3.E1.m1.23.23.1.1.1.3.5.2.cmml" xref="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.23.23.1.1.1.3.5.2.1.cmml" xref="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.18.18.18.18.18.18.cmml" xref="S3.E1.m1.18.18.18.18.18.18">𝜆</ci><cn type="integer" id="S3.E1.m1.19.19.19.19.19.19.1.cmml" xref="S3.E1.m1.19.19.19.19.19.19.1">4</cn></apply><apply id="S3.E1.m1.23.23.1.1.1.3.5.3.cmml" xref="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.23.23.1.1.1.3.5.3.1.cmml" xref="S3.E1.m1.24.24.2.23.23.23.23.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.20.20.20.20.20.20.cmml" xref="S3.E1.m1.20.20.20.20.20.20">𝐿</ci><ci id="S3.E1.m1.21.21.21.21.21.21.1.cmml" xref="S3.E1.m1.21.21.21.21.21.21.1">Semcon</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.24c">\begin{split}\mathit{L}=\lambda_{\mathrm{1}}\mathit{L}_{\mathrm{Det}}+\lambda_{\mathrm{2}}\mathit{L}_{\mathrm{Track}}+\lambda_{\mathrm{3}}\mathit{L}_{\mathrm{KD}}+\lambda_{\mathrm{4}}\mathit{L}_{\mathrm{Semcon}},\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.p1.4" class="ltx_p">which consists of four loss terms in total.
The detection (<math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="\mathit{L}_{\mathrm{Det}}" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><msub id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><mi id="S3.SS4.p1.1.m1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.2.cmml">L</mi><mi id="S3.SS4.p1.1.m1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.3.cmml">Det</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p1.1.m1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2">𝐿</ci><ci id="S3.SS4.p1.1.m1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.3">Det</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">\mathit{L}_{\mathrm{Det}}</annotation></semantics></math>) and tracking losses (<math id="S3.SS4.p1.2.m2.1" class="ltx_Math" alttext="\mathit{L}_{\mathrm{Track}}" display="inline"><semantics id="S3.SS4.p1.2.m2.1a"><msub id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml"><mi id="S3.SS4.p1.2.m2.1.1.2" xref="S3.SS4.p1.2.m2.1.1.2.cmml">L</mi><mi id="S3.SS4.p1.2.m2.1.1.3" xref="S3.SS4.p1.2.m2.1.1.3.cmml">Track</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><apply id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.2.m2.1.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p1.2.m2.1.1.2.cmml" xref="S3.SS4.p1.2.m2.1.1.2">𝐿</ci><ci id="S3.SS4.p1.2.m2.1.1.3.cmml" xref="S3.SS4.p1.2.m2.1.1.3">Track</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">\mathit{L}_{\mathrm{Track}}</annotation></semantics></math>) are adopted from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>.
Note that the <math id="S3.SS4.p1.3.m3.1" class="ltx_Math" alttext="\mathit{L}_{\mathrm{KD}}(=\mathit{L}^{\mathrm{RPN}}_{\mathrm{KD}}+\mathit{L}^{\mathrm{RCNN}}_{\mathrm{KD}})" display="inline"><semantics id="S3.SS4.p1.3.m3.1a"><mrow id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml"><msub id="S3.SS4.p1.3.m3.1.1.3" xref="S3.SS4.p1.3.m3.1.1.3.cmml"><mi id="S3.SS4.p1.3.m3.1.1.3.2" xref="S3.SS4.p1.3.m3.1.1.3.2.cmml">L</mi><mi id="S3.SS4.p1.3.m3.1.1.3.3" xref="S3.SS4.p1.3.m3.1.1.3.3.cmml">KD</mi></msub><mspace width="0.3888888888888889em" id="S3.SS4.p1.3.m3.1.1a" xref="S3.SS4.p1.3.m3.1.1.cmml"></mspace><mrow id="S3.SS4.p1.3.m3.1.1.1.1" xref="S3.SS4.p1.3.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS4.p1.3.m3.1.1.1.1.2" xref="S3.SS4.p1.3.m3.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS4.p1.3.m3.1.1.1.1.1" xref="S3.SS4.p1.3.m3.1.1.1.1.1.cmml"><mi id="S3.SS4.p1.3.m3.1.1.1.1.1.2" xref="S3.SS4.p1.3.m3.1.1.1.1.1.2.cmml"></mi><mo id="S3.SS4.p1.3.m3.1.1.1.1.1.1" xref="S3.SS4.p1.3.m3.1.1.1.1.1.1.cmml">=</mo><mrow id="S3.SS4.p1.3.m3.1.1.1.1.1.3" xref="S3.SS4.p1.3.m3.1.1.1.1.1.3.cmml"><msubsup id="S3.SS4.p1.3.m3.1.1.1.1.1.3.2" xref="S3.SS4.p1.3.m3.1.1.1.1.1.3.2.cmml"><mi id="S3.SS4.p1.3.m3.1.1.1.1.1.3.2.2.2" xref="S3.SS4.p1.3.m3.1.1.1.1.1.3.2.2.2.cmml">L</mi><mi id="S3.SS4.p1.3.m3.1.1.1.1.1.3.2.3" xref="S3.SS4.p1.3.m3.1.1.1.1.1.3.2.3.cmml">KD</mi><mi id="S3.SS4.p1.3.m3.1.1.1.1.1.3.2.2.3" xref="S3.SS4.p1.3.m3.1.1.1.1.1.3.2.2.3.cmml">RPN</mi></msubsup><mo id="S3.SS4.p1.3.m3.1.1.1.1.1.3.1" xref="S3.SS4.p1.3.m3.1.1.1.1.1.3.1.cmml">+</mo><msubsup id="S3.SS4.p1.3.m3.1.1.1.1.1.3.3" xref="S3.SS4.p1.3.m3.1.1.1.1.1.3.3.cmml"><mi id="S3.SS4.p1.3.m3.1.1.1.1.1.3.3.2.2" xref="S3.SS4.p1.3.m3.1.1.1.1.1.3.3.2.2.cmml">L</mi><mi id="S3.SS4.p1.3.m3.1.1.1.1.1.3.3.3" xref="S3.SS4.p1.3.m3.1.1.1.1.1.3.3.3.cmml">KD</mi><mi id="S3.SS4.p1.3.m3.1.1.1.1.1.3.3.2.3" xref="S3.SS4.p1.3.m3.1.1.1.1.1.3.3.2.3.cmml">RCNN</mi></msubsup></mrow></mrow><mo stretchy="false" id="S3.SS4.p1.3.m3.1.1.1.1.3" xref="S3.SS4.p1.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.1b"><apply id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1"><csymbol cd="latexml" id="S3.SS4.p1.3.m3.1.1.2.cmml" xref="S3.SS4.p1.3.m3.1.1">annotated</csymbol><apply id="S3.SS4.p1.3.m3.1.1.3.cmml" xref="S3.SS4.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p1.3.m3.1.1.3.1.cmml" xref="S3.SS4.p1.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS4.p1.3.m3.1.1.3.2.cmml" xref="S3.SS4.p1.3.m3.1.1.3.2">𝐿</ci><ci id="S3.SS4.p1.3.m3.1.1.3.3.cmml" xref="S3.SS4.p1.3.m3.1.1.3.3">KD</ci></apply><apply id="S3.SS4.p1.3.m3.1.1.1.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1.1.1"><eq id="S3.SS4.p1.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1.1.1.1.1"></eq><csymbol cd="latexml" id="S3.SS4.p1.3.m3.1.1.1.1.1.2.cmml" xref="S3.SS4.p1.3.m3.1.1.1.1.1.2">absent</csymbol><apply id="S3.SS4.p1.3.m3.1.1.1.1.1.3.cmml" xref="S3.SS4.p1.3.m3.1.1.1.1.1.3"><plus id="S3.SS4.p1.3.m3.1.1.1.1.1.3.1.cmml" xref="S3.SS4.p1.3.m3.1.1.1.1.1.3.1"></plus><apply id="S3.SS4.p1.3.m3.1.1.1.1.1.3.2.cmml" xref="S3.SS4.p1.3.m3.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS4.p1.3.m3.1.1.1.1.1.3.2.1.cmml" xref="S3.SS4.p1.3.m3.1.1.1.1.1.3.2">subscript</csymbol><apply id="S3.SS4.p1.3.m3.1.1.1.1.1.3.2.2.cmml" xref="S3.SS4.p1.3.m3.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS4.p1.3.m3.1.1.1.1.1.3.2.2.1.cmml" xref="S3.SS4.p1.3.m3.1.1.1.1.1.3.2">superscript</csymbol><ci id="S3.SS4.p1.3.m3.1.1.1.1.1.3.2.2.2.cmml" xref="S3.SS4.p1.3.m3.1.1.1.1.1.3.2.2.2">𝐿</ci><ci id="S3.SS4.p1.3.m3.1.1.1.1.1.3.2.2.3.cmml" xref="S3.SS4.p1.3.m3.1.1.1.1.1.3.2.2.3">RPN</ci></apply><ci id="S3.SS4.p1.3.m3.1.1.1.1.1.3.2.3.cmml" xref="S3.SS4.p1.3.m3.1.1.1.1.1.3.2.3">KD</ci></apply><apply id="S3.SS4.p1.3.m3.1.1.1.1.1.3.3.cmml" xref="S3.SS4.p1.3.m3.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS4.p1.3.m3.1.1.1.1.1.3.3.1.cmml" xref="S3.SS4.p1.3.m3.1.1.1.1.1.3.3">subscript</csymbol><apply id="S3.SS4.p1.3.m3.1.1.1.1.1.3.3.2.cmml" xref="S3.SS4.p1.3.m3.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS4.p1.3.m3.1.1.1.1.1.3.3.2.1.cmml" xref="S3.SS4.p1.3.m3.1.1.1.1.1.3.3">superscript</csymbol><ci id="S3.SS4.p1.3.m3.1.1.1.1.1.3.3.2.2.cmml" xref="S3.SS4.p1.3.m3.1.1.1.1.1.3.3.2.2">𝐿</ci><ci id="S3.SS4.p1.3.m3.1.1.1.1.1.3.3.2.3.cmml" xref="S3.SS4.p1.3.m3.1.1.1.1.1.3.3.2.3">RCNN</ci></apply><ci id="S3.SS4.p1.3.m3.1.1.1.1.1.3.3.3.cmml" xref="S3.SS4.p1.3.m3.1.1.1.1.1.3.3.3">KD</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.1c">\mathit{L}_{\mathrm{KD}}(=\mathit{L}^{\mathrm{RPN}}_{\mathrm{KD}}+\mathit{L}^{\mathrm{RCNN}}_{\mathrm{KD}})</annotation></semantics></math> and <math id="S3.SS4.p1.4.m4.1" class="ltx_Math" alttext="\mathit{L}_{\mathrm{Semcon}}" display="inline"><semantics id="S3.SS4.p1.4.m4.1a"><msub id="S3.SS4.p1.4.m4.1.1" xref="S3.SS4.p1.4.m4.1.1.cmml"><mi id="S3.SS4.p1.4.m4.1.1.2" xref="S3.SS4.p1.4.m4.1.1.2.cmml">L</mi><mi id="S3.SS4.p1.4.m4.1.1.3" xref="S3.SS4.p1.4.m4.1.1.3.cmml">Semcon</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.4.m4.1b"><apply id="S3.SS4.p1.4.m4.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.4.m4.1.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS4.p1.4.m4.1.1.2.cmml" xref="S3.SS4.p1.4.m4.1.1.2">𝐿</ci><ci id="S3.SS4.p1.4.m4.1.1.3.cmml" xref="S3.SS4.p1.4.m4.1.1.3">Semcon</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.4.m4.1c">\mathit{L}_{\mathrm{Semcon}}</annotation></semantics></math> are used only when fine-tuning on the videos.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.2" class="ltx_p">In this section, we conduct extensive experiments to analyze our methods.
We investigate the results mainly in two aspects: image-level prediction and cross-frame association, which will be reflected in the BBox AP and Track AP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, respectively. Considering the task difficulty, we mainly focus on the Track AP of 50, 75 and their average.
For the TAO test, we provide Track AP*, a full Track AP average for IoU from 0.5 to 0.95 with a step size of 0.05.
We study the impact of unified learning on TAO dataset (Sec. <a href="#S4.SS1" title="4.1 Main Results ‣ 4 Experiments ‣ Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>). We consistently outperformed the current decoupled learning paradigm with healthy margins using various models, and pushed the state-of-the-art performance significantly.
Second, to investigate the importance of the major components in our proposals, we provide ablation studies on TAO validation set (Sec. <a href="#S4.SS2" title="4.2 Ablation studies ‣ 4 Experiments ‣ Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>).
Lastly, we evaluate our teacher-student scheme on two representative image-video transfer learning scenarios, LVIS <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.p1.1.m1.1a"><mo stretchy="false" id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><ci id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\rightarrow</annotation></semantics></math> TAO and COCO <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.p1.2.m2.1a"><mo stretchy="false" id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><ci id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">\rightarrow</annotation></semantics></math> YTVIS<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>For the experiment, we contact the authors for the YTVIS-<span id="footnote2.1" class="ltx_text ltx_font_italic">val</span> annotations.</span></span></span>(Sec. <a href="#S4.SS3" title="4.3 Image to Video Transfer Learning ‣ 4 Experiments ‣ Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>).
In the following, we provide experiment setups, evaluation protocol and results for each section. More details are in supplementary materials.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Main Results</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Upon the state-of-the-art tracking-by-detection framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, we instantiate various large vocabulary trackers.
In specific, we consider two important detection architecture, two-staged (Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>) and multi-staged (CenterNet2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite>), and three different long-tailed learning methods, Repeat Factor Sampling (RFS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, Equalization Loss V2 (EQLv2) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>, and Seesaw Loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>.
All the models use the same ResNet-101 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> with feature pyramid <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> backbone following the previous works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>.
Based on these baseline models, we compare our learning framework with the current standard learning protocol, decoupled learning.
The comparison is in Table <a href="#S4.T1" title="Table 1 ‣ 4.1 Main Results ‣ 4 Experiments ‣ Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
We observe that our unified learning scheme consistently outperforms the current decoupled learning paradigm on various models, showing the strong generalizabilty of the proposal.
With our method, we push the state-of-the-art performance significantly, achieving 21.6 and 20.1 Track AP50 on TAO-<span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">val</span> and TAO-<span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_italic">test</span>, respectively.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T1.sf1" class="ltx_table ltx_figure_panel ltx_align_center">
<div id="S4.T1.sf1.7" class="ltx_inline-block ltx_transformed_outer" style="width:203.8pt;height:107.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-85.9pt,45.3pt) scale(0.542707845950088,0.542707845950088) ;">
<table id="S4.T1.sf1.7.7" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.sf1.3.3.3" class="ltx_tr">
<th id="S4.T1.sf1.3.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-top:-1pt;padding-bottom:-1pt;">Method</th>
<th id="S4.T1.sf1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:-1pt;padding-bottom:-1pt;">Track <math id="S4.T1.sf1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathrm{AP}_{50}" display="inline"><semantics id="S4.T1.sf1.1.1.1.1.m1.1a"><msub id="S4.T1.sf1.1.1.1.1.m1.1.1" xref="S4.T1.sf1.1.1.1.1.m1.1.1.cmml"><mi id="S4.T1.sf1.1.1.1.1.m1.1.1.2" xref="S4.T1.sf1.1.1.1.1.m1.1.1.2.cmml">AP</mi><mn id="S4.T1.sf1.1.1.1.1.m1.1.1.3" xref="S4.T1.sf1.1.1.1.1.m1.1.1.3.cmml">50</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T1.sf1.1.1.1.1.m1.1b"><apply id="S4.T1.sf1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.sf1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.sf1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T1.sf1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T1.sf1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T1.sf1.1.1.1.1.m1.1.1.2">AP</ci><cn type="integer" id="S4.T1.sf1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T1.sf1.1.1.1.1.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.sf1.1.1.1.1.m1.1c">\mathrm{AP}_{50}</annotation></semantics></math>
</th>
<th id="S4.T1.sf1.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:-1pt;padding-bottom:-1pt;">Track <math id="S4.T1.sf1.2.2.2.2.m1.1" class="ltx_Math" alttext="\mathrm{AP}_{75}" display="inline"><semantics id="S4.T1.sf1.2.2.2.2.m1.1a"><msub id="S4.T1.sf1.2.2.2.2.m1.1.1" xref="S4.T1.sf1.2.2.2.2.m1.1.1.cmml"><mi id="S4.T1.sf1.2.2.2.2.m1.1.1.2" xref="S4.T1.sf1.2.2.2.2.m1.1.1.2.cmml">AP</mi><mn id="S4.T1.sf1.2.2.2.2.m1.1.1.3" xref="S4.T1.sf1.2.2.2.2.m1.1.1.3.cmml">75</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T1.sf1.2.2.2.2.m1.1b"><apply id="S4.T1.sf1.2.2.2.2.m1.1.1.cmml" xref="S4.T1.sf1.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.sf1.2.2.2.2.m1.1.1.1.cmml" xref="S4.T1.sf1.2.2.2.2.m1.1.1">subscript</csymbol><ci id="S4.T1.sf1.2.2.2.2.m1.1.1.2.cmml" xref="S4.T1.sf1.2.2.2.2.m1.1.1.2">AP</ci><cn type="integer" id="S4.T1.sf1.2.2.2.2.m1.1.1.3.cmml" xref="S4.T1.sf1.2.2.2.2.m1.1.1.3">75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.sf1.2.2.2.2.m1.1c">\mathrm{AP}_{75}</annotation></semantics></math>
</th>
<th id="S4.T1.sf1.3.3.3.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-top:-1pt;padding-bottom:-1pt;">Track <math id="S4.T1.sf1.3.3.3.3.m1.1" class="ltx_Math" alttext="\mathrm{AP}_{avg}" display="inline"><semantics id="S4.T1.sf1.3.3.3.3.m1.1a"><msub id="S4.T1.sf1.3.3.3.3.m1.1.1" xref="S4.T1.sf1.3.3.3.3.m1.1.1.cmml"><mi id="S4.T1.sf1.3.3.3.3.m1.1.1.2" xref="S4.T1.sf1.3.3.3.3.m1.1.1.2.cmml">AP</mi><mrow id="S4.T1.sf1.3.3.3.3.m1.1.1.3" xref="S4.T1.sf1.3.3.3.3.m1.1.1.3.cmml"><mi id="S4.T1.sf1.3.3.3.3.m1.1.1.3.2" xref="S4.T1.sf1.3.3.3.3.m1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T1.sf1.3.3.3.3.m1.1.1.3.1" xref="S4.T1.sf1.3.3.3.3.m1.1.1.3.1.cmml">​</mo><mi id="S4.T1.sf1.3.3.3.3.m1.1.1.3.3" xref="S4.T1.sf1.3.3.3.3.m1.1.1.3.3.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.T1.sf1.3.3.3.3.m1.1.1.3.1a" xref="S4.T1.sf1.3.3.3.3.m1.1.1.3.1.cmml">​</mo><mi id="S4.T1.sf1.3.3.3.3.m1.1.1.3.4" xref="S4.T1.sf1.3.3.3.3.m1.1.1.3.4.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T1.sf1.3.3.3.3.m1.1b"><apply id="S4.T1.sf1.3.3.3.3.m1.1.1.cmml" xref="S4.T1.sf1.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.sf1.3.3.3.3.m1.1.1.1.cmml" xref="S4.T1.sf1.3.3.3.3.m1.1.1">subscript</csymbol><ci id="S4.T1.sf1.3.3.3.3.m1.1.1.2.cmml" xref="S4.T1.sf1.3.3.3.3.m1.1.1.2">AP</ci><apply id="S4.T1.sf1.3.3.3.3.m1.1.1.3.cmml" xref="S4.T1.sf1.3.3.3.3.m1.1.1.3"><times id="S4.T1.sf1.3.3.3.3.m1.1.1.3.1.cmml" xref="S4.T1.sf1.3.3.3.3.m1.1.1.3.1"></times><ci id="S4.T1.sf1.3.3.3.3.m1.1.1.3.2.cmml" xref="S4.T1.sf1.3.3.3.3.m1.1.1.3.2">𝑎</ci><ci id="S4.T1.sf1.3.3.3.3.m1.1.1.3.3.cmml" xref="S4.T1.sf1.3.3.3.3.m1.1.1.3.3">𝑣</ci><ci id="S4.T1.sf1.3.3.3.3.m1.1.1.3.4.cmml" xref="S4.T1.sf1.3.3.3.3.m1.1.1.3.4">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.sf1.3.3.3.3.m1.1c">\mathrm{AP}_{avg}</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.sf1.7.7.8.1" class="ltx_tr">
<td id="S4.T1.sf1.7.7.8.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:-1pt;padding-bottom:-1pt;">SORT_TAO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</td>
<td id="S4.T1.sf1.7.7.8.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-1pt;padding-bottom:-1pt;">13.2</td>
<td id="S4.T1.sf1.7.7.8.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-1pt;padding-bottom:-1pt;">-</td>
<td id="S4.T1.sf1.7.7.8.1.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-1pt;padding-bottom:-1pt;">-</td>
</tr>
<tr id="S4.T1.sf1.7.7.9.2" class="ltx_tr">
<td id="S4.T1.sf1.7.7.9.2.1" class="ltx_td ltx_align_left" style="padding-top:-1pt;padding-bottom:-1pt;">FasterRCNN-RFS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>
</td>
<td id="S4.T1.sf1.7.7.9.2.2" class="ltx_td ltx_align_center" style="padding-top:-1pt;padding-bottom:-1pt;">16.1</td>
<td id="S4.T1.sf1.7.7.9.2.3" class="ltx_td ltx_align_center" style="padding-top:-1pt;padding-bottom:-1pt;">5.0</td>
<td id="S4.T1.sf1.7.7.9.2.4" class="ltx_td ltx_align_right" style="padding-top:-1pt;padding-bottom:-1pt;">10.6</td>
</tr>
<tr id="S4.T1.sf1.7.7.10.3" class="ltx_tr">
<td id="S4.T1.sf1.7.7.10.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:-1pt;padding-bottom:-1pt;">FasterRCNN-RFS*</td>
<td id="S4.T1.sf1.7.7.10.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-1pt;padding-bottom:-1pt;">13.4</td>
<td id="S4.T1.sf1.7.7.10.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-1pt;padding-bottom:-1pt;">4.9</td>
<td id="S4.T1.sf1.7.7.10.3.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-1pt;padding-bottom:-1pt;">9.2</td>
</tr>
<tr id="S4.T1.sf1.4.4.4" class="ltx_tr">
<td id="S4.T1.sf1.4.4.4.2" class="ltx_td ltx_align_left" style="padding-top:-1pt;padding-bottom:-1pt;">w/ SimLearn</td>
<td id="S4.T1.sf1.4.4.4.3" class="ltx_td ltx_align_center" style="padding-top:-1pt;padding-bottom:-1pt;"><span id="S4.T1.sf1.4.4.4.3.1" class="ltx_text ltx_font_bold">19.6</span></td>
<td id="S4.T1.sf1.4.4.4.4" class="ltx_td ltx_align_center" style="padding-top:-1pt;padding-bottom:-1pt;"><span id="S4.T1.sf1.4.4.4.4.1" class="ltx_text ltx_font_bold">7.3</span></td>
<td id="S4.T1.sf1.4.4.4.1" class="ltx_td ltx_align_right" style="padding-top:-1pt;padding-bottom:-1pt;">
<span id="S4.T1.sf1.4.4.4.1.1" class="ltx_text" style="color:#0000FF;">(<math id="S4.T1.sf1.4.4.4.1.1.m1.1" class="ltx_Math" alttext="+" display="inline"><semantics id="S4.T1.sf1.4.4.4.1.1.m1.1a"><mo mathcolor="#0000FF" id="S4.T1.sf1.4.4.4.1.1.m1.1.1" xref="S4.T1.sf1.4.4.4.1.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S4.T1.sf1.4.4.4.1.1.m1.1b"><plus id="S4.T1.sf1.4.4.4.1.1.m1.1.1.cmml" xref="S4.T1.sf1.4.4.4.1.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.sf1.4.4.4.1.1.m1.1c">+</annotation></semantics></math>4.4)</span> <span id="S4.T1.sf1.4.4.4.1.2" class="ltx_text ltx_font_bold">13.6</span>
</td>
</tr>
<tr id="S4.T1.sf1.7.7.11.4" class="ltx_tr">
<td id="S4.T1.sf1.7.7.11.4.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:-1pt;padding-bottom:-1pt;">FasterRCNN-EQLv2</td>
<td id="S4.T1.sf1.7.7.11.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-1pt;padding-bottom:-1pt;">14.2</td>
<td id="S4.T1.sf1.7.7.11.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-1pt;padding-bottom:-1pt;">5.5</td>
<td id="S4.T1.sf1.7.7.11.4.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-1pt;padding-bottom:-1pt;">10.1</td>
</tr>
<tr id="S4.T1.sf1.5.5.5" class="ltx_tr">
<td id="S4.T1.sf1.5.5.5.2" class="ltx_td ltx_align_left" style="padding-top:-1pt;padding-bottom:-1pt;">w/ SimLearn</td>
<td id="S4.T1.sf1.5.5.5.3" class="ltx_td ltx_align_center" style="padding-top:-1pt;padding-bottom:-1pt;"><span id="S4.T1.sf1.5.5.5.3.1" class="ltx_text ltx_font_bold">19.8</span></td>
<td id="S4.T1.sf1.5.5.5.4" class="ltx_td ltx_align_center" style="padding-top:-1pt;padding-bottom:-1pt;"><span id="S4.T1.sf1.5.5.5.4.1" class="ltx_text ltx_font_bold">8.8</span></td>
<td id="S4.T1.sf1.5.5.5.1" class="ltx_td ltx_align_right" style="padding-top:-1pt;padding-bottom:-1pt;">
<span id="S4.T1.sf1.5.5.5.1.1" class="ltx_text" style="color:#0000FF;">(<math id="S4.T1.sf1.5.5.5.1.1.m1.1" class="ltx_Math" alttext="+" display="inline"><semantics id="S4.T1.sf1.5.5.5.1.1.m1.1a"><mo mathcolor="#0000FF" id="S4.T1.sf1.5.5.5.1.1.m1.1.1" xref="S4.T1.sf1.5.5.5.1.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S4.T1.sf1.5.5.5.1.1.m1.1b"><plus id="S4.T1.sf1.5.5.5.1.1.m1.1.1.cmml" xref="S4.T1.sf1.5.5.5.1.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.sf1.5.5.5.1.1.m1.1c">+</annotation></semantics></math>4.1)</span> <span id="S4.T1.sf1.5.5.5.1.2" class="ltx_text ltx_font_bold">14.2</span>
</td>
</tr>
<tr id="S4.T1.sf1.7.7.12.5" class="ltx_tr">
<td id="S4.T1.sf1.7.7.12.5.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:-1pt;padding-bottom:-1pt;">FasterRCNN-Seesaw</td>
<td id="S4.T1.sf1.7.7.12.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-1pt;padding-bottom:-1pt;">15.4</td>
<td id="S4.T1.sf1.7.7.12.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-1pt;padding-bottom:-1pt;">5.7</td>
<td id="S4.T1.sf1.7.7.12.5.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-1pt;padding-bottom:-1pt;">10.5</td>
</tr>
<tr id="S4.T1.sf1.6.6.6" class="ltx_tr">
<td id="S4.T1.sf1.6.6.6.2" class="ltx_td ltx_align_left" style="padding-top:-1pt;padding-bottom:-1pt;">w/ SimLearn</td>
<td id="S4.T1.sf1.6.6.6.3" class="ltx_td ltx_align_center" style="padding-top:-1pt;padding-bottom:-1pt;"><span id="S4.T1.sf1.6.6.6.3.1" class="ltx_text ltx_font_bold">20.2</span></td>
<td id="S4.T1.sf1.6.6.6.4" class="ltx_td ltx_align_center" style="padding-top:-1pt;padding-bottom:-1pt;"><span id="S4.T1.sf1.6.6.6.4.1" class="ltx_text ltx_font_bold">9.4</span></td>
<td id="S4.T1.sf1.6.6.6.1" class="ltx_td ltx_align_right" style="padding-top:-1pt;padding-bottom:-1pt;">
<span id="S4.T1.sf1.6.6.6.1.1" class="ltx_text" style="color:#0000FF;">(<math id="S4.T1.sf1.6.6.6.1.1.m1.1" class="ltx_Math" alttext="+" display="inline"><semantics id="S4.T1.sf1.6.6.6.1.1.m1.1a"><mo mathcolor="#0000FF" id="S4.T1.sf1.6.6.6.1.1.m1.1.1" xref="S4.T1.sf1.6.6.6.1.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S4.T1.sf1.6.6.6.1.1.m1.1b"><plus id="S4.T1.sf1.6.6.6.1.1.m1.1.1.cmml" xref="S4.T1.sf1.6.6.6.1.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.sf1.6.6.6.1.1.m1.1c">+</annotation></semantics></math>4.3)</span> <span id="S4.T1.sf1.6.6.6.1.2" class="ltx_text ltx_font_bold">14.8</span>
</td>
</tr>
<tr id="S4.T1.sf1.7.7.13.6" class="ltx_tr">
<td id="S4.T1.sf1.7.7.13.6.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:-1pt;padding-bottom:-1pt;">CenterNet2-RFS</td>
<td id="S4.T1.sf1.7.7.13.6.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-1pt;padding-bottom:-1pt;">18.9</td>
<td id="S4.T1.sf1.7.7.13.6.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-1pt;padding-bottom:-1pt;">9.1</td>
<td id="S4.T1.sf1.7.7.13.6.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:-1pt;padding-bottom:-1pt;">14.0</td>
</tr>
<tr id="S4.T1.sf1.7.7.7" class="ltx_tr">
<td id="S4.T1.sf1.7.7.7.2" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:-1pt;padding-bottom:-1pt;">w/ SimLearn</td>
<td id="S4.T1.sf1.7.7.7.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:-1pt;padding-bottom:-1pt;"><span id="S4.T1.sf1.7.7.7.3.1" class="ltx_text ltx_font_bold">21.6</span></td>
<td id="S4.T1.sf1.7.7.7.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:-1pt;padding-bottom:-1pt;"><span id="S4.T1.sf1.7.7.7.4.1" class="ltx_text ltx_font_bold">10.4</span></td>
<td id="S4.T1.sf1.7.7.7.1" class="ltx_td ltx_align_right ltx_border_bb" style="padding-top:-1pt;padding-bottom:-1pt;">
<span id="S4.T1.sf1.7.7.7.1.1" class="ltx_text" style="color:#0000FF;">(<math id="S4.T1.sf1.7.7.7.1.1.m1.1" class="ltx_Math" alttext="+" display="inline"><semantics id="S4.T1.sf1.7.7.7.1.1.m1.1a"><mo mathcolor="#0000FF" id="S4.T1.sf1.7.7.7.1.1.m1.1.1" xref="S4.T1.sf1.7.7.7.1.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S4.T1.sf1.7.7.7.1.1.m1.1b"><plus id="S4.T1.sf1.7.7.7.1.1.m1.1.1.cmml" xref="S4.T1.sf1.7.7.7.1.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.sf1.7.7.7.1.1.m1.1c">+</annotation></semantics></math>2.1)</span> <span id="S4.T1.sf1.7.7.7.1.2" class="ltx_text ltx_font_bold">16.1</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(a) </span><span id="S4.T1.sf1.9.1" class="ltx_text ltx_font_bold" style="font-size:70%;">State-of-the-art results<span id="S4.T1.sf1.9.1.1" class="ltx_text ltx_font_medium"> in TAO-<span id="S4.T1.sf1.9.1.1.1" class="ltx_text ltx_font_italic">val</span>.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T1.sf2" class="ltx_table ltx_figure_panel ltx_align_center">
<div id="S4.T1.sf2.3" class="ltx_inline-block ltx_transformed_outer" style="width:221.1pt;height:73.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-79.2pt,26.3pt) scale(0.582668834937807,0.582668834937807) ;">
<table id="S4.T1.sf2.3.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.sf2.3.3.3" class="ltx_tr">
<th id="S4.T1.sf2.3.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-top:3.75pt;padding-bottom:3.75pt;">Method</th>
<th id="S4.T1.sf2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:3.75pt;padding-bottom:3.75pt;">Track <math id="S4.T1.sf2.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathrm{AP}_{50}" display="inline"><semantics id="S4.T1.sf2.1.1.1.1.m1.1a"><msub id="S4.T1.sf2.1.1.1.1.m1.1.1" xref="S4.T1.sf2.1.1.1.1.m1.1.1.cmml"><mi id="S4.T1.sf2.1.1.1.1.m1.1.1.2" xref="S4.T1.sf2.1.1.1.1.m1.1.1.2.cmml">AP</mi><mn id="S4.T1.sf2.1.1.1.1.m1.1.1.3" xref="S4.T1.sf2.1.1.1.1.m1.1.1.3.cmml">50</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T1.sf2.1.1.1.1.m1.1b"><apply id="S4.T1.sf2.1.1.1.1.m1.1.1.cmml" xref="S4.T1.sf2.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.sf2.1.1.1.1.m1.1.1.1.cmml" xref="S4.T1.sf2.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T1.sf2.1.1.1.1.m1.1.1.2.cmml" xref="S4.T1.sf2.1.1.1.1.m1.1.1.2">AP</ci><cn type="integer" id="S4.T1.sf2.1.1.1.1.m1.1.1.3.cmml" xref="S4.T1.sf2.1.1.1.1.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.sf2.1.1.1.1.m1.1c">\mathrm{AP}_{50}</annotation></semantics></math>
</th>
<th id="S4.T1.sf2.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:3.75pt;padding-bottom:3.75pt;">Track <math id="S4.T1.sf2.2.2.2.2.m1.1" class="ltx_Math" alttext="\mathrm{AP}_{75}" display="inline"><semantics id="S4.T1.sf2.2.2.2.2.m1.1a"><msub id="S4.T1.sf2.2.2.2.2.m1.1.1" xref="S4.T1.sf2.2.2.2.2.m1.1.1.cmml"><mi id="S4.T1.sf2.2.2.2.2.m1.1.1.2" xref="S4.T1.sf2.2.2.2.2.m1.1.1.2.cmml">AP</mi><mn id="S4.T1.sf2.2.2.2.2.m1.1.1.3" xref="S4.T1.sf2.2.2.2.2.m1.1.1.3.cmml">75</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T1.sf2.2.2.2.2.m1.1b"><apply id="S4.T1.sf2.2.2.2.2.m1.1.1.cmml" xref="S4.T1.sf2.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.sf2.2.2.2.2.m1.1.1.1.cmml" xref="S4.T1.sf2.2.2.2.2.m1.1.1">subscript</csymbol><ci id="S4.T1.sf2.2.2.2.2.m1.1.1.2.cmml" xref="S4.T1.sf2.2.2.2.2.m1.1.1.2">AP</ci><cn type="integer" id="S4.T1.sf2.2.2.2.2.m1.1.1.3.cmml" xref="S4.T1.sf2.2.2.2.2.m1.1.1.3">75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.sf2.2.2.2.2.m1.1c">\mathrm{AP}_{75}</annotation></semantics></math>
</th>
<th id="S4.T1.sf2.3.3.3.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-top:3.75pt;padding-bottom:3.75pt;">Track <math id="S4.T1.sf2.3.3.3.3.m1.1" class="ltx_Math" alttext="\mathrm{AP}_{50:95}" display="inline"><semantics id="S4.T1.sf2.3.3.3.3.m1.1a"><msub id="S4.T1.sf2.3.3.3.3.m1.1.1" xref="S4.T1.sf2.3.3.3.3.m1.1.1.cmml"><mi id="S4.T1.sf2.3.3.3.3.m1.1.1.2" xref="S4.T1.sf2.3.3.3.3.m1.1.1.2.cmml">AP</mi><mrow id="S4.T1.sf2.3.3.3.3.m1.1.1.3" xref="S4.T1.sf2.3.3.3.3.m1.1.1.3.cmml"><mn id="S4.T1.sf2.3.3.3.3.m1.1.1.3.2" xref="S4.T1.sf2.3.3.3.3.m1.1.1.3.2.cmml">50</mn><mo lspace="0.278em" rspace="0.278em" id="S4.T1.sf2.3.3.3.3.m1.1.1.3.1" xref="S4.T1.sf2.3.3.3.3.m1.1.1.3.1.cmml">:</mo><mn id="S4.T1.sf2.3.3.3.3.m1.1.1.3.3" xref="S4.T1.sf2.3.3.3.3.m1.1.1.3.3.cmml">95</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.T1.sf2.3.3.3.3.m1.1b"><apply id="S4.T1.sf2.3.3.3.3.m1.1.1.cmml" xref="S4.T1.sf2.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.sf2.3.3.3.3.m1.1.1.1.cmml" xref="S4.T1.sf2.3.3.3.3.m1.1.1">subscript</csymbol><ci id="S4.T1.sf2.3.3.3.3.m1.1.1.2.cmml" xref="S4.T1.sf2.3.3.3.3.m1.1.1.2">AP</ci><apply id="S4.T1.sf2.3.3.3.3.m1.1.1.3.cmml" xref="S4.T1.sf2.3.3.3.3.m1.1.1.3"><ci id="S4.T1.sf2.3.3.3.3.m1.1.1.3.1.cmml" xref="S4.T1.sf2.3.3.3.3.m1.1.1.3.1">:</ci><cn type="integer" id="S4.T1.sf2.3.3.3.3.m1.1.1.3.2.cmml" xref="S4.T1.sf2.3.3.3.3.m1.1.1.3.2">50</cn><cn type="integer" id="S4.T1.sf2.3.3.3.3.m1.1.1.3.3.cmml" xref="S4.T1.sf2.3.3.3.3.m1.1.1.3.3">95</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.sf2.3.3.3.3.m1.1c">\mathrm{AP}_{50:95}</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.sf2.3.3.4.1" class="ltx_tr">
<td id="S4.T1.sf2.3.3.4.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:3.75pt;padding-bottom:3.75pt;">SORT_TAO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</td>
<td id="S4.T1.sf2.3.3.4.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.75pt;padding-bottom:3.75pt;">10.2</td>
<td id="S4.T1.sf2.3.3.4.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.75pt;padding-bottom:3.75pt;">4.4</td>
<td id="S4.T1.sf2.3.3.4.1.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:3.75pt;padding-bottom:3.75pt;">4.9</td>
</tr>
<tr id="S4.T1.sf2.3.3.5.2" class="ltx_tr">
<td id="S4.T1.sf2.3.3.5.2.1" class="ltx_td ltx_align_left" style="padding-top:3.75pt;padding-bottom:3.75pt;">FasterRCNN-RFS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>
</td>
<td id="S4.T1.sf2.3.3.5.2.2" class="ltx_td ltx_align_center" style="padding-top:3.75pt;padding-bottom:3.75pt;">12.4</td>
<td id="S4.T1.sf2.3.3.5.2.3" class="ltx_td ltx_align_center" style="padding-top:3.75pt;padding-bottom:3.75pt;">4.5</td>
<td id="S4.T1.sf2.3.3.5.2.4" class="ltx_td ltx_align_right" style="padding-top:3.75pt;padding-bottom:3.75pt;">5.2</td>
</tr>
<tr id="S4.T1.sf2.3.3.6.3" class="ltx_tr">
<td id="S4.T1.sf2.3.3.6.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:3.75pt;padding-bottom:3.75pt;">FasterRCNN-RFS* + SimLearn.</td>
<td id="S4.T1.sf2.3.3.6.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.75pt;padding-bottom:3.75pt;">17.1</td>
<td id="S4.T1.sf2.3.3.6.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3.75pt;padding-bottom:3.75pt;">6.9</td>
<td id="S4.T1.sf2.3.3.6.3.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:3.75pt;padding-bottom:3.75pt;">7.8</td>
</tr>
<tr id="S4.T1.sf2.3.3.7.4" class="ltx_tr">
<td id="S4.T1.sf2.3.3.7.4.1" class="ltx_td ltx_align_left" style="padding-top:3.75pt;padding-bottom:3.75pt;">FasterRCNN-EQLv2 + SimLearn.</td>
<td id="S4.T1.sf2.3.3.7.4.2" class="ltx_td ltx_align_center" style="padding-top:3.75pt;padding-bottom:3.75pt;">16.8</td>
<td id="S4.T1.sf2.3.3.7.4.3" class="ltx_td ltx_align_center" style="padding-top:3.75pt;padding-bottom:3.75pt;">7.2</td>
<td id="S4.T1.sf2.3.3.7.4.4" class="ltx_td ltx_align_right" style="padding-top:3.75pt;padding-bottom:3.75pt;">8.0</td>
</tr>
<tr id="S4.T1.sf2.3.3.8.5" class="ltx_tr">
<td id="S4.T1.sf2.3.3.8.5.1" class="ltx_td ltx_align_left" style="padding-top:3.75pt;padding-bottom:3.75pt;">FasterRCNN-Seesaw + SimLearn.</td>
<td id="S4.T1.sf2.3.3.8.5.2" class="ltx_td ltx_align_center" style="padding-top:3.75pt;padding-bottom:3.75pt;">17.6</td>
<td id="S4.T1.sf2.3.3.8.5.3" class="ltx_td ltx_align_center" style="padding-top:3.75pt;padding-bottom:3.75pt;">8.0</td>
<td id="S4.T1.sf2.3.3.8.5.4" class="ltx_td ltx_align_right" style="padding-top:3.75pt;padding-bottom:3.75pt;">8.5</td>
</tr>
<tr id="S4.T1.sf2.3.3.9.6" class="ltx_tr">
<td id="S4.T1.sf2.3.3.9.6.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:3.75pt;padding-bottom:3.75pt;">CenterNet2-RFS + SimLearn.</td>
<td id="S4.T1.sf2.3.3.9.6.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:3.75pt;padding-bottom:3.75pt;">20.1</td>
<td id="S4.T1.sf2.3.3.9.6.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:3.75pt;padding-bottom:3.75pt;">9.6</td>
<td id="S4.T1.sf2.3.3.9.6.4" class="ltx_td ltx_align_right ltx_border_bb" style="padding-top:3.75pt;padding-bottom:3.75pt;"><span id="S4.T1.sf2.3.3.9.6.4.1" class="ltx_text ltx_font_bold">10.3</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(b) </span><span id="S4.T1.sf2.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">State-of-the-art results<span id="S4.T1.sf2.5.1.1" class="ltx_text ltx_font_medium"> in TAO-<span id="S4.T1.sf2.5.1.1.1" class="ltx_text ltx_font_italic">test</span>.</span></span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Our learning framework couples well with different model architectures and learning methods.
All the baseline scores are obtained after the decoupled training, i.e., training the detector and tracker on LVIS and TAO, respectively.
FasterRCNN-RFS* is a re-implementation of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> baseline.
</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Ablation studies</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">Impact of image spatial jitterings.</span>
The results are presented in Table <a href="#S4.T2.sf1" title="In Table 2 ‣ 4.2 Ablation studies ‣ 4 Experiments ‣ Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2a</span></a>.
Compared to the standard affine transformation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite> or simple cropping without scaling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite>, the presented strong zoom-in/out and mosaicing provides a large Track AP improvement.
This indicates both the low sampling rate input simulation (with large-scale jittering) and dense tracking simulation (with mosaicing) enables more accurate large vocabulary object associations at test-time.
To concretely investigate the scaling effects of zoom-in/out, we also provide its variant with small scale-jittering, Z-in/out*, and confirm that the large scale-jittering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> is indeed important for the performance.
We notice that mosaicing augmentation drops the Box AP.
We conjecture this happens due to the train and test time inconsistency of input pairs.
To this end, we present to form a tracking pair from two different augmentations in equal probability.
We found that this mixed sampling strategy provides the best Track AP.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T2.sf1" class="ltx_table ltx_figure_panel ltx_align_center">
<div id="S4.T2.sf1.2" class="ltx_inline-block ltx_transformed_outer" style="width:212.5pt;height:64.1pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-133.8pt,40.1pt) scale(0.44260457946116,0.44260457946116) ;">
<table id="S4.T2.sf1.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.sf1.2.2.2" class="ltx_tr">
<th id="S4.T2.sf1.2.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-top:0.75pt;padding-bottom:0.75pt;">Method</th>
<td id="S4.T2.sf1.2.2.2.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.75pt;padding-bottom:0.75pt;">Box AP</td>
<td id="S4.T2.sf1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.75pt;padding-bottom:0.75pt;">Track <math id="S4.T2.sf1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathrm{AP}_{50}" display="inline"><semantics id="S4.T2.sf1.1.1.1.1.m1.1a"><msub id="S4.T2.sf1.1.1.1.1.m1.1.1" xref="S4.T2.sf1.1.1.1.1.m1.1.1.cmml"><mi id="S4.T2.sf1.1.1.1.1.m1.1.1.2" xref="S4.T2.sf1.1.1.1.1.m1.1.1.2.cmml">AP</mi><mn id="S4.T2.sf1.1.1.1.1.m1.1.1.3" xref="S4.T2.sf1.1.1.1.1.m1.1.1.3.cmml">50</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T2.sf1.1.1.1.1.m1.1b"><apply id="S4.T2.sf1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.sf1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.sf1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.sf1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T2.sf1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T2.sf1.1.1.1.1.m1.1.1.2">AP</ci><cn type="integer" id="S4.T2.sf1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T2.sf1.1.1.1.1.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.sf1.1.1.1.1.m1.1c">\mathrm{AP}_{50}</annotation></semantics></math>
</td>
<td id="S4.T2.sf1.2.2.2.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.75pt;padding-bottom:0.75pt;">Track <math id="S4.T2.sf1.2.2.2.2.m1.1" class="ltx_Math" alttext="\mathrm{AP}_{75}" display="inline"><semantics id="S4.T2.sf1.2.2.2.2.m1.1a"><msub id="S4.T2.sf1.2.2.2.2.m1.1.1" xref="S4.T2.sf1.2.2.2.2.m1.1.1.cmml"><mi id="S4.T2.sf1.2.2.2.2.m1.1.1.2" xref="S4.T2.sf1.2.2.2.2.m1.1.1.2.cmml">AP</mi><mn id="S4.T2.sf1.2.2.2.2.m1.1.1.3" xref="S4.T2.sf1.2.2.2.2.m1.1.1.3.cmml">75</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T2.sf1.2.2.2.2.m1.1b"><apply id="S4.T2.sf1.2.2.2.2.m1.1.1.cmml" xref="S4.T2.sf1.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.sf1.2.2.2.2.m1.1.1.1.cmml" xref="S4.T2.sf1.2.2.2.2.m1.1.1">subscript</csymbol><ci id="S4.T2.sf1.2.2.2.2.m1.1.1.2.cmml" xref="S4.T2.sf1.2.2.2.2.m1.1.1.2">AP</ci><cn type="integer" id="S4.T2.sf1.2.2.2.2.m1.1.1.3.cmml" xref="S4.T2.sf1.2.2.2.2.m1.1.1.3">75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.sf1.2.2.2.2.m1.1c">\mathrm{AP}_{75}</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T2.sf1.2.2.3.1" class="ltx_tr">
<th id="S4.T2.sf1.2.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">Decoupled TAO-tracker <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>
</th>
<td id="S4.T2.sf1.2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">18.1</td>
<td id="S4.T2.sf1.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">16.1</td>
<td id="S4.T2.sf1.2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">5.0</td>
</tr>
<tr id="S4.T2.sf1.2.2.4.2" class="ltx_tr">
<th id="S4.T2.sf1.2.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">Random Affine <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite>
</th>
<td id="S4.T2.sf1.2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">17.4</td>
<td id="S4.T2.sf1.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">13.6</td>
<td id="S4.T2.sf1.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">5.0</td>
</tr>
<tr id="S4.T2.sf1.2.2.5.3" class="ltx_tr">
<th id="S4.T2.sf1.2.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:0.75pt;padding-bottom:0.75pt;">Random Crop <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite>
</th>
<td id="S4.T2.sf1.2.2.5.3.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">15.9</td>
<td id="S4.T2.sf1.2.2.5.3.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">11.5</td>
<td id="S4.T2.sf1.2.2.5.3.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">1.5</td>
</tr>
<tr id="S4.T2.sf1.2.2.6.4" class="ltx_tr">
<th id="S4.T2.sf1.2.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:0.75pt;padding-bottom:0.75pt;">Zoom in/out*</th>
<td id="S4.T2.sf1.2.2.6.4.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">18.1</td>
<td id="S4.T2.sf1.2.2.6.4.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">13.2</td>
<td id="S4.T2.sf1.2.2.6.4.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">4.4</td>
</tr>
<tr id="S4.T2.sf1.2.2.7.5" class="ltx_tr">
<th id="S4.T2.sf1.2.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">Zoom in/out</th>
<td id="S4.T2.sf1.2.2.7.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span id="S4.T2.sf1.2.2.7.5.2.1" class="ltx_text ltx_font_bold">19.0</span></td>
<td id="S4.T2.sf1.2.2.7.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">14.4</td>
<td id="S4.T2.sf1.2.2.7.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span id="S4.T2.sf1.2.2.7.5.4.1" class="ltx_text ltx_font_bold">6.0</span></td>
</tr>
<tr id="S4.T2.sf1.2.2.8.6" class="ltx_tr">
<th id="S4.T2.sf1.2.2.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:0.75pt;padding-bottom:0.75pt;">Mosaic</th>
<td id="S4.T2.sf1.2.2.8.6.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">16.2</td>
<td id="S4.T2.sf1.2.2.8.6.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">16.5</td>
<td id="S4.T2.sf1.2.2.8.6.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">4.7</td>
</tr>
<tr id="S4.T2.sf1.2.2.9.7" class="ltx_tr">
<th id="S4.T2.sf1.2.2.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-top:0.75pt;padding-bottom:0.75pt;">Both (LVIS-tracker)</th>
<td id="S4.T2.sf1.2.2.9.7.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.75pt;padding-bottom:0.75pt;">18.5</td>
<td id="S4.T2.sf1.2.2.9.7.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span id="S4.T2.sf1.2.2.9.7.3.1" class="ltx_text ltx_font_bold">17.8</span></td>
<td id="S4.T2.sf1.2.2.9.7.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.75pt;padding-bottom:0.75pt;">5.7</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(a) </span><span id="S4.T2.sf1.4.1" class="ltx_text ltx_font_bold ltx_align_center" style="font-size:70%;">Effect of Spatial Jittering Strategies<span id="S4.T2.sf1.4.1.1" class="ltx_text ltx_font_medium"> in LVIS pre-train.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T2.sf2" class="ltx_table ltx_figure_panel ltx_align_center">
<div id="S4.T2.sf2.2" class="ltx_inline-block ltx_transformed_outer" style="width:212.5pt;height:77pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-143.3pt,51.7pt) scale(0.425706476906676,0.425706476906676) ;">
<table id="S4.T2.sf2.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.sf2.2.2.2" class="ltx_tr">
<th id="S4.T2.sf2.2.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Method</th>
<td id="S4.T2.sf2.2.2.2.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Box AP</td>
<td id="S4.T2.sf2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Track <math id="S4.T2.sf2.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathrm{AP}_{50}" display="inline"><semantics id="S4.T2.sf2.1.1.1.1.m1.1a"><msub id="S4.T2.sf2.1.1.1.1.m1.1.1" xref="S4.T2.sf2.1.1.1.1.m1.1.1.cmml"><mi id="S4.T2.sf2.1.1.1.1.m1.1.1.2" xref="S4.T2.sf2.1.1.1.1.m1.1.1.2.cmml">AP</mi><mn id="S4.T2.sf2.1.1.1.1.m1.1.1.3" xref="S4.T2.sf2.1.1.1.1.m1.1.1.3.cmml">50</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T2.sf2.1.1.1.1.m1.1b"><apply id="S4.T2.sf2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.sf2.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.sf2.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.sf2.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T2.sf2.1.1.1.1.m1.1.1.2.cmml" xref="S4.T2.sf2.1.1.1.1.m1.1.1.2">AP</ci><cn type="integer" id="S4.T2.sf2.1.1.1.1.m1.1.1.3.cmml" xref="S4.T2.sf2.1.1.1.1.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.sf2.1.1.1.1.m1.1c">\mathrm{AP}_{50}</annotation></semantics></math>
</td>
<td id="S4.T2.sf2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Track <math id="S4.T2.sf2.2.2.2.2.m1.1" class="ltx_Math" alttext="\mathrm{AP}_{75}" display="inline"><semantics id="S4.T2.sf2.2.2.2.2.m1.1a"><msub id="S4.T2.sf2.2.2.2.2.m1.1.1" xref="S4.T2.sf2.2.2.2.2.m1.1.1.cmml"><mi id="S4.T2.sf2.2.2.2.2.m1.1.1.2" xref="S4.T2.sf2.2.2.2.2.m1.1.1.2.cmml">AP</mi><mn id="S4.T2.sf2.2.2.2.2.m1.1.1.3" xref="S4.T2.sf2.2.2.2.2.m1.1.1.3.cmml">75</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T2.sf2.2.2.2.2.m1.1b"><apply id="S4.T2.sf2.2.2.2.2.m1.1.1.cmml" xref="S4.T2.sf2.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.sf2.2.2.2.2.m1.1.1.1.cmml" xref="S4.T2.sf2.2.2.2.2.m1.1.1">subscript</csymbol><ci id="S4.T2.sf2.2.2.2.2.m1.1.1.2.cmml" xref="S4.T2.sf2.2.2.2.2.m1.1.1.2">AP</ci><cn type="integer" id="S4.T2.sf2.2.2.2.2.m1.1.1.3.cmml" xref="S4.T2.sf2.2.2.2.2.m1.1.1.3">75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.sf2.2.2.2.2.m1.1c">\mathrm{AP}_{75}</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T2.sf2.2.2.3.1" class="ltx_tr">
<th id="S4.T2.sf2.2.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">LVIS-tracker</th>
<td id="S4.T2.sf2.2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">18.5</td>
<td id="S4.T2.sf2.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">17.8</td>
<td id="S4.T2.sf2.2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">5.7</td>
</tr>
<tr id="S4.T2.sf2.2.2.4.2" class="ltx_tr">
<th id="S4.T2.sf2.2.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Naive-ft</th>
<td id="S4.T2.sf2.2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">11.7</td>
<td id="S4.T2.sf2.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">11.4</td>
<td id="S4.T2.sf2.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">2.7</td>
</tr>
<tr id="S4.T2.sf2.2.2.5.3" class="ltx_tr">
<th id="S4.T2.sf2.2.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Vanilla Teacher-Student</th>
<td id="S4.T2.sf2.2.2.5.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">18.0</td>
<td id="S4.T2.sf2.2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">15.9</td>
<td id="S4.T2.sf2.2.2.5.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">6.7</td>
</tr>
<tr id="S4.T2.sf2.2.2.6.4" class="ltx_tr">
<th id="S4.T2.sf2.2.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">+ Pseudo-distill target</th>
<td id="S4.T2.sf2.2.2.6.4.2" class="ltx_td ltx_align_center" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">18.6</td>
<td id="S4.T2.sf2.2.2.6.4.3" class="ltx_td ltx_align_center" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">17.8</td>
<td id="S4.T2.sf2.2.2.6.4.4" class="ltx_td ltx_align_center" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">7.2</td>
</tr>
<tr id="S4.T2.sf2.2.2.7.5" class="ltx_tr">
<th id="S4.T2.sf2.2.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">+ Pseudo-neg sample</th>
<td id="S4.T2.sf2.2.2.7.5.2" class="ltx_td ltx_align_center" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">18.8</td>
<td id="S4.T2.sf2.2.2.7.5.3" class="ltx_td ltx_align_center" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">17.9</td>
<td id="S4.T2.sf2.2.2.7.5.4" class="ltx_td ltx_align_center" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">7.3</td>
</tr>
<tr id="S4.T2.sf2.2.2.8.6" class="ltx_tr">
<th id="S4.T2.sf2.2.2.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">Both (Ours)</th>
<td id="S4.T2.sf2.2.2.8.6.2" class="ltx_td ltx_align_center" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">19.5</td>
<td id="S4.T2.sf2.2.2.8.6.3" class="ltx_td ltx_align_center" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">18.5</td>
<td id="S4.T2.sf2.2.2.8.6.4" class="ltx_td ltx_align_center" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span id="S4.T2.sf2.2.2.8.6.4.1" class="ltx_text ltx_font_bold">7.5</span></td>
</tr>
<tr id="S4.T2.sf2.2.2.9.7" class="ltx_tr">
<th id="S4.T2.sf2.2.2.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">w. Hard Pseudo label <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>
</th>
<td id="S4.T2.sf2.2.2.9.7.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">17.1</td>
<td id="S4.T2.sf2.2.2.9.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">15.9</td>
<td id="S4.T2.sf2.2.2.9.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">5.3</td>
</tr>
<tr id="S4.T2.sf2.2.2.10.8" class="ltx_tr">
<th id="S4.T2.sf2.2.2.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">w. KL-based distill <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>
</th>
<td id="S4.T2.sf2.2.2.10.8.2" class="ltx_td ltx_align_center" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">17.7</td>
<td id="S4.T2.sf2.2.2.10.8.3" class="ltx_td ltx_align_center" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">16.8</td>
<td id="S4.T2.sf2.2.2.10.8.4" class="ltx_td ltx_align_center" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">6.0</td>
</tr>
<tr id="S4.T2.sf2.2.2.11.9" class="ltx_tr">
<th id="S4.T2.sf2.2.2.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">w. Semcon. (Final Tracker)</th>
<td id="S4.T2.sf2.2.2.11.9.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span id="S4.T2.sf2.2.2.11.9.2.1" class="ltx_text ltx_font_bold">19.6</span></td>
<td id="S4.T2.sf2.2.2.11.9.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;"><span id="S4.T2.sf2.2.2.11.9.3.1" class="ltx_text ltx_font_bold">19.5</span></td>
<td id="S4.T2.sf2.2.2.11.9.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:-0.5pt;padding-bottom:-0.5pt;">7.3</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(b) </span><span id="S4.T2.sf2.4.1" class="ltx_text ltx_font_bold ltx_align_center" style="font-size:70%;">Effect of Teacher-Student Framework<span id="S4.T2.sf2.4.1.1" class="ltx_text ltx_font_medium"> in TAO fine-tune.</span></span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>(a) Zoom-in/out* and Zoom-in/out denote zoom-in/out augmentation with scaling range of [0.8, 1.25] and [0.1, 2.0], respectively.
(b)Pseudo Labeled Training denotes the standard (hard) pseudo label-based training.</figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Impact of teacher-student framework.</span>
In Table <a href="#S4.T2.sf2" title="In Table 2 ‣ 4.2 Ablation studies ‣ 4 Experiments ‣ Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2b</span></a>, we study the impact of the key proposals in teacher-student framework.
For the baselines, we provide the Naive-ft and Vanilla Teacher-Student schemes.
Naive-ft indicates fine-tuning on TAO videos without any proper regulation for forgetting, which results in a significant performance drop.
Vanilla Teacher-Student scheme samples the distillation targets only from the original ground truth labels, and no negative correction is performed. While it shows the past knowledge preservation effect to some extent, the performance is still worse than the LVIS-tracker.
The vanilla scheme starts to improve over the LVIS-tracker when our proposal is added.
This implies that pseudo labeling is essential, and 1) keeping the past knowledge of seen objects (by sampling distillation targets from the augmented labels) and 2) preventing the seen objects from being background (by correcting negatives using the augmented labels) are the key to avoid catastrophic forgetting.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">One may wonder if the standard (hard) pseudo-labeling approach can directly preserve the previous knowledge as typical teacher-student scheme do <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>.
However, as can be shown in the results, we instead observe inferior results than the baseline.
The large vocabulary classifier fundamentally suffers from the confidence calibration issue <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> as it is trained on the long-tailed class-imbalanced data.
It results in the classifier bias; predictions are made mainly toward the frequent object categories, missing rare objects in one-hot hard pseudo labels.
In contrast, the (soft) pseudo labels essentially affect all classes.
Furthermore, we suggest to employ MSE loss rather than standard KL-loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> as objective function in distillation.
As MSE loss treats all classes equally the impact of the gradient is not attenuated for the rare classes.
Recent study also reveals that MSE loss offers better generalization capability due to the direct matching of logits compared to the KL loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para ltx_noindent">
<p id="S4.SS2.p4.1" class="ltx_p"><span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_bold">Impact of semantic consistency loss.</span>
Finally, we study the impact of semantic consistency loss.
It regularizes the model’s class logits of the same instance in different frames to be the equivalent.
In Table <a href="#S4.T2.sf2" title="In Table 2 ‣ 4.2 Ablation studies ‣ 4 Experiments ‣ Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2b</span></a>, we observed meaningful improvement in Track AP.
This implies that semantic flicker regularization is indeed effective for the large vocabulary object tracking.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Image to Video Transfer Learning</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.2" class="ltx_p">Here, we evaluate our teacher-student scheme on two representative image to video transfer learning setups (see Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.2 Learn to Unforget in TAO ‣ 3 Proposed Method ‣ Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).
In LVIS <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mo stretchy="false" id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><ci id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">\rightarrow</annotation></semantics></math> TAO setup, we pre-train FasterRCNN-RFS tracker on LVIS (with 482 categories) and fine-tune on TAO (with 216 categories).
We evaluate the model on TAO-<span id="S4.SS3.p1.2.1" class="ltx_text ltx_font_italic">val</span> with Track AP metric.
In COCO <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mo stretchy="false" id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><ci id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">\rightarrow</annotation></semantics></math> YTVIS setup, we pre-train Mask-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> on COCO, transfer the weights to MaskTrack RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite>, add new randomly initialized classifier weights to accommodate newly added classes, and fine-tune on YTVIS.
More details of the setup are in supplementary materials.
We evaluate the model on YTVIS-<span id="S4.SS3.p1.2.2" class="ltx_text ltx_font_italic">val</span> with Mask AP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite> metric.
To quantitatively analyze whether the proposal properly preserves the past knowledge and benefits from the new video labels, we provide the scores of OLD and NEW.
Here, OLD indicates the classes that only reveal in the image pre-training stage.
NEW denotes the classes that appears in the video fine-tuning stage.
For each setup, we provide a baseline of naive fine-tuning, which results in a severe catastrophic forgetting.
The results are summarized in Table <a href="#S4.T3" title="Table 3 ‣ 4.3 Image to Video Transfer Learning ‣ 4 Experiments ‣ Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and Table <a href="#S4.T4" title="Table 4 ‣ 4.3 Image to Video Transfer Learning ‣ 4 Experiments ‣ Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.7" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:411.9pt;height:78.7pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-79.1pt,15.0pt) scale(0.722464342356198,0.722464342356198) ;">
<table id="S4.T3.7.7" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T3.1.1.1.2.1" class="ltx_text">Method</span></th>
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">OLD (LVIS <math id="S4.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S4.T3.1.1.1.1.m1.1a"><mo id="S4.T3.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><minus id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">-</annotation></semantics></math> TAO)</th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">NEW (TAO)</th>
<th id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">ALL</th>
</tr>
<tr id="S4.T3.7.7.7" class="ltx_tr">
<th id="S4.T3.7.7.7.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Box AP</th>
<th id="S4.T3.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><math id="S4.T3.2.2.2.1.m1.1" class="ltx_Math" alttext="\mathrm{TrackAP}_{50}" display="inline"><semantics id="S4.T3.2.2.2.1.m1.1a"><msub id="S4.T3.2.2.2.1.m1.1.1" xref="S4.T3.2.2.2.1.m1.1.1.cmml"><mi id="S4.T3.2.2.2.1.m1.1.1.2" xref="S4.T3.2.2.2.1.m1.1.1.2.cmml">TrackAP</mi><mn id="S4.T3.2.2.2.1.m1.1.1.3" xref="S4.T3.2.2.2.1.m1.1.1.3.cmml">50</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.1.m1.1b"><apply id="S4.T3.2.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.2.2.2.1.m1.1.1.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1">subscript</csymbol><ci id="S4.T3.2.2.2.1.m1.1.1.2.cmml" xref="S4.T3.2.2.2.1.m1.1.1.2">TrackAP</ci><cn type="integer" id="S4.T3.2.2.2.1.m1.1.1.3.cmml" xref="S4.T3.2.2.2.1.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.1.m1.1c">\mathrm{TrackAP}_{50}</annotation></semantics></math></th>
<th id="S4.T3.3.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><math id="S4.T3.3.3.3.2.m1.1" class="ltx_Math" alttext="\mathrm{TrackAP}_{75}" display="inline"><semantics id="S4.T3.3.3.3.2.m1.1a"><msub id="S4.T3.3.3.3.2.m1.1.1" xref="S4.T3.3.3.3.2.m1.1.1.cmml"><mi id="S4.T3.3.3.3.2.m1.1.1.2" xref="S4.T3.3.3.3.2.m1.1.1.2.cmml">TrackAP</mi><mn id="S4.T3.3.3.3.2.m1.1.1.3" xref="S4.T3.3.3.3.2.m1.1.1.3.cmml">75</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.2.m1.1b"><apply id="S4.T3.3.3.3.2.m1.1.1.cmml" xref="S4.T3.3.3.3.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.3.3.3.2.m1.1.1.1.cmml" xref="S4.T3.3.3.3.2.m1.1.1">subscript</csymbol><ci id="S4.T3.3.3.3.2.m1.1.1.2.cmml" xref="S4.T3.3.3.3.2.m1.1.1.2">TrackAP</ci><cn type="integer" id="S4.T3.3.3.3.2.m1.1.1.3.cmml" xref="S4.T3.3.3.3.2.m1.1.1.3">75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.2.m1.1c">\mathrm{TrackAP}_{75}</annotation></semantics></math></th>
<th id="S4.T3.7.7.7.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Box AP</th>
<th id="S4.T3.4.4.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><math id="S4.T3.4.4.4.3.m1.1" class="ltx_Math" alttext="\mathrm{TrackAP}_{50}" display="inline"><semantics id="S4.T3.4.4.4.3.m1.1a"><msub id="S4.T3.4.4.4.3.m1.1.1" xref="S4.T3.4.4.4.3.m1.1.1.cmml"><mi id="S4.T3.4.4.4.3.m1.1.1.2" xref="S4.T3.4.4.4.3.m1.1.1.2.cmml">TrackAP</mi><mn id="S4.T3.4.4.4.3.m1.1.1.3" xref="S4.T3.4.4.4.3.m1.1.1.3.cmml">50</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.3.m1.1b"><apply id="S4.T3.4.4.4.3.m1.1.1.cmml" xref="S4.T3.4.4.4.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.4.4.4.3.m1.1.1.1.cmml" xref="S4.T3.4.4.4.3.m1.1.1">subscript</csymbol><ci id="S4.T3.4.4.4.3.m1.1.1.2.cmml" xref="S4.T3.4.4.4.3.m1.1.1.2">TrackAP</ci><cn type="integer" id="S4.T3.4.4.4.3.m1.1.1.3.cmml" xref="S4.T3.4.4.4.3.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.3.m1.1c">\mathrm{TrackAP}_{50}</annotation></semantics></math></th>
<th id="S4.T3.5.5.5.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><math id="S4.T3.5.5.5.4.m1.1" class="ltx_Math" alttext="\mathrm{TrackAP}_{75}" display="inline"><semantics id="S4.T3.5.5.5.4.m1.1a"><msub id="S4.T3.5.5.5.4.m1.1.1" xref="S4.T3.5.5.5.4.m1.1.1.cmml"><mi id="S4.T3.5.5.5.4.m1.1.1.2" xref="S4.T3.5.5.5.4.m1.1.1.2.cmml">TrackAP</mi><mn id="S4.T3.5.5.5.4.m1.1.1.3" xref="S4.T3.5.5.5.4.m1.1.1.3.cmml">75</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.4.m1.1b"><apply id="S4.T3.5.5.5.4.m1.1.1.cmml" xref="S4.T3.5.5.5.4.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.5.5.5.4.m1.1.1.1.cmml" xref="S4.T3.5.5.5.4.m1.1.1">subscript</csymbol><ci id="S4.T3.5.5.5.4.m1.1.1.2.cmml" xref="S4.T3.5.5.5.4.m1.1.1.2">TrackAP</ci><cn type="integer" id="S4.T3.5.5.5.4.m1.1.1.3.cmml" xref="S4.T3.5.5.5.4.m1.1.1.3">75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.4.m1.1c">\mathrm{TrackAP}_{75}</annotation></semantics></math></th>
<th id="S4.T3.7.7.7.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Box AP</th>
<th id="S4.T3.6.6.6.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><math id="S4.T3.6.6.6.5.m1.1" class="ltx_Math" alttext="\mathrm{TrackAP}_{50}" display="inline"><semantics id="S4.T3.6.6.6.5.m1.1a"><msub id="S4.T3.6.6.6.5.m1.1.1" xref="S4.T3.6.6.6.5.m1.1.1.cmml"><mi id="S4.T3.6.6.6.5.m1.1.1.2" xref="S4.T3.6.6.6.5.m1.1.1.2.cmml">TrackAP</mi><mn id="S4.T3.6.6.6.5.m1.1.1.3" xref="S4.T3.6.6.6.5.m1.1.1.3.cmml">50</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.6.5.m1.1b"><apply id="S4.T3.6.6.6.5.m1.1.1.cmml" xref="S4.T3.6.6.6.5.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.6.6.6.5.m1.1.1.1.cmml" xref="S4.T3.6.6.6.5.m1.1.1">subscript</csymbol><ci id="S4.T3.6.6.6.5.m1.1.1.2.cmml" xref="S4.T3.6.6.6.5.m1.1.1.2">TrackAP</ci><cn type="integer" id="S4.T3.6.6.6.5.m1.1.1.3.cmml" xref="S4.T3.6.6.6.5.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.6.5.m1.1c">\mathrm{TrackAP}_{50}</annotation></semantics></math></th>
<th id="S4.T3.7.7.7.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><math id="S4.T3.7.7.7.6.m1.1" class="ltx_Math" alttext="\mathrm{TrackAP}_{75}" display="inline"><semantics id="S4.T3.7.7.7.6.m1.1a"><msub id="S4.T3.7.7.7.6.m1.1.1" xref="S4.T3.7.7.7.6.m1.1.1.cmml"><mi id="S4.T3.7.7.7.6.m1.1.1.2" xref="S4.T3.7.7.7.6.m1.1.1.2.cmml">TrackAP</mi><mn id="S4.T3.7.7.7.6.m1.1.1.3" xref="S4.T3.7.7.7.6.m1.1.1.3.cmml">75</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T3.7.7.7.6.m1.1b"><apply id="S4.T3.7.7.7.6.m1.1.1.cmml" xref="S4.T3.7.7.7.6.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.7.7.7.6.m1.1.1.1.cmml" xref="S4.T3.7.7.7.6.m1.1.1">subscript</csymbol><ci id="S4.T3.7.7.7.6.m1.1.1.2.cmml" xref="S4.T3.7.7.7.6.m1.1.1.2">TrackAP</ci><cn type="integer" id="S4.T3.7.7.7.6.m1.1.1.3.cmml" xref="S4.T3.7.7.7.6.m1.1.1.3">75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.7.7.7.6.m1.1c">\mathrm{TrackAP}_{75}</annotation></semantics></math></th>
</tr>
<tr id="S4.T3.7.7.8.1" class="ltx_tr">
<th id="S4.T3.7.7.8.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">LVIS-tracker</th>
<th id="S4.T3.7.7.8.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">15.7</th>
<th id="S4.T3.7.7.8.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">16.1</th>
<th id="S4.T3.7.7.8.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">5.7</th>
<th id="S4.T3.7.7.8.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">21.1</th>
<th id="S4.T3.7.7.8.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">17.2</th>
<th id="S4.T3.7.7.8.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">9.0</th>
<th id="S4.T3.7.7.8.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">18.5.</th>
<th id="S4.T3.7.7.8.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">17.8</th>
<th id="S4.T3.7.7.8.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">5.8</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.7.7.9.1" class="ltx_tr">
<th id="S4.T3.7.7.9.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Naive-ft</th>
<td id="S4.T3.7.7.9.1.2" class="ltx_td ltx_align_center ltx_border_t">7.1</td>
<td id="S4.T3.7.7.9.1.3" class="ltx_td ltx_align_center ltx_border_t">7.7</td>
<td id="S4.T3.7.7.9.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.3</td>
<td id="S4.T3.7.7.9.1.5" class="ltx_td ltx_align_center ltx_border_t">16.2</td>
<td id="S4.T3.7.7.9.1.6" class="ltx_td ltx_align_center ltx_border_t">14.9</td>
<td id="S4.T3.7.7.9.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.7</td>
<td id="S4.T3.7.7.9.1.8" class="ltx_td ltx_align_center ltx_border_t">11.7.</td>
<td id="S4.T3.7.7.9.1.9" class="ltx_td ltx_align_center ltx_border_t">11.4</td>
<td id="S4.T3.7.7.9.1.10" class="ltx_td ltx_align_center ltx_border_t">2.6</td>
</tr>
<tr id="S4.T3.7.7.10.2" class="ltx_tr">
<th id="S4.T3.7.7.10.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Track-only</th>
<td id="S4.T3.7.7.10.2.2" class="ltx_td ltx_align_center">15.7</td>
<td id="S4.T3.7.7.10.2.3" class="ltx_td ltx_align_center">15.3</td>
<td id="S4.T3.7.7.10.2.4" class="ltx_td ltx_align_center ltx_border_r">5.5</td>
<td id="S4.T3.7.7.10.2.5" class="ltx_td ltx_align_center">21.1</td>
<td id="S4.T3.7.7.10.2.6" class="ltx_td ltx_align_center">16.9</td>
<td id="S4.T3.7.7.10.2.7" class="ltx_td ltx_align_center ltx_border_r">7.1</td>
<td id="S4.T3.7.7.10.2.8" class="ltx_td ltx_align_center">18.5.</td>
<td id="S4.T3.7.7.10.2.9" class="ltx_td ltx_align_center">16.1</td>
<td id="S4.T3.7.7.10.2.10" class="ltx_td ltx_align_center">6.3</td>
</tr>
<tr id="S4.T3.7.7.11.3" class="ltx_tr">
<th id="S4.T3.7.7.11.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">Teacher-Student</th>
<td id="S4.T3.7.7.11.3.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.7.7.11.3.2.1" class="ltx_text ltx_font_bold">15.7</span></td>
<td id="S4.T3.7.7.11.3.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.7.7.11.3.3.1" class="ltx_text ltx_font_bold">16.3</span></td>
<td id="S4.T3.7.7.11.3.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">
<span id="S4.T3.7.7.11.3.4.1" class="ltx_text ltx_font_bold">6</span>.5</td>
<td id="S4.T3.7.7.11.3.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.7.7.11.3.5.1" class="ltx_text ltx_font_bold">23.1</span></td>
<td id="S4.T3.7.7.11.3.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.7.7.11.3.6.1" class="ltx_text ltx_font_bold">20.6</span></td>
<td id="S4.T3.7.7.11.3.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T3.7.7.11.3.7.1" class="ltx_text ltx_font_bold">9.0</span></td>
<td id="S4.T3.7.7.11.3.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.7.7.11.3.8.1" class="ltx_text ltx_font_bold">19.5</span></td>
<td id="S4.T3.7.7.11.3.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.7.7.11.3.9.1" class="ltx_text ltx_font_bold">18.5</span></td>
<td id="S4.T3.7.7.11.3.10" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.7.7.11.3.10.1" class="ltx_text ltx_font_bold">7.5</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>
Teacher-Student framework in <span id="S4.T3.9.1" class="ltx_text ltx_font_bold">LVIS <math id="S4.T3.9.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T3.9.1.m1.1b"><mo stretchy="false" id="S4.T3.9.1.m1.1.1" xref="S4.T3.9.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T3.9.1.m1.1c"><ci id="S4.T3.9.1.m1.1.1.cmml" xref="S4.T3.9.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.9.1.m1.1d">\rightarrow</annotation></semantics></math> TAO</span> transfer learning setup.
Evaluated on TAO-<span id="S4.T3.11.2" class="ltx_text ltx_font_italic">val</span>.
</figcaption>
</figure>
<figure id="S4.T4" class="ltx_table">
<div id="S4.T4.7" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:416.3pt;height:92.3pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-37.6pt,8.3pt) scale(0.847020516057442,0.847020516057442) ;">
<table id="S4.T4.7.7" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding:0.5pt 10.0pt;" rowspan="2"><span id="S4.T4.1.1.1.2.1" class="ltx_text">Method</span></th>
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:0.5pt 10.0pt;" colspan="3">OLD (COCO <math id="S4.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S4.T4.1.1.1.1.m1.1a"><mo id="S4.T4.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.m1.1b"><minus id="S4.T4.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.m1.1c">-</annotation></semantics></math> YTVIS)</th>
<th id="S4.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:0.5pt 10.0pt;" colspan="3">NEW (YTVIS)</th>
<th id="S4.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:0.5pt 10.0pt;" colspan="3">ALL</th>
</tr>
<tr id="S4.T4.7.7.7" class="ltx_tr">
<th id="S4.T4.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:0.5pt 10.0pt;">AP<sub id="S4.T4.2.2.2.1.1" class="ltx_sub"><span id="S4.T4.2.2.2.1.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</th>
<th id="S4.T4.3.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:0.5pt 10.0pt;">AP<sub id="S4.T4.3.3.3.2.1" class="ltx_sub"><span id="S4.T4.3.3.3.2.1.1" class="ltx_text ltx_font_italic">75</span></sub>
</th>
<th id="S4.T4.7.7.7.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding:0.5pt 10.0pt;">AP</th>
<th id="S4.T4.4.4.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:0.5pt 10.0pt;">AP<sub id="S4.T4.4.4.4.3.1" class="ltx_sub"><span id="S4.T4.4.4.4.3.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</th>
<th id="S4.T4.5.5.5.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:0.5pt 10.0pt;">AP<sub id="S4.T4.5.5.5.4.1" class="ltx_sub"><span id="S4.T4.5.5.5.4.1.1" class="ltx_text ltx_font_italic">75</span></sub>
</th>
<th id="S4.T4.7.7.7.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding:0.5pt 10.0pt;">AP</th>
<th id="S4.T4.6.6.6.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:0.5pt 10.0pt;">AP<sub id="S4.T4.6.6.6.5.1" class="ltx_sub"><span id="S4.T4.6.6.6.5.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</th>
<th id="S4.T4.7.7.7.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:0.5pt 10.0pt;">AP<sub id="S4.T4.7.7.7.6.1" class="ltx_sub"><span id="S4.T4.7.7.7.6.1.1" class="ltx_text ltx_font_italic">75</span></sub>
</th>
<th id="S4.T4.7.7.7.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:0.5pt 10.0pt;">AP</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.7.7.8.1" class="ltx_tr">
<th id="S4.T4.7.7.8.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:0.5pt 10.0pt;">Naive-ft</th>
<td id="S4.T4.7.7.8.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 10.0pt;">5.2</td>
<td id="S4.T4.7.7.8.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 10.0pt;">2.6</td>
<td id="S4.T4.7.7.8.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 10.0pt;">2.7</td>
<td id="S4.T4.7.7.8.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 10.0pt;">38.2</td>
<td id="S4.T4.7.7.8.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 10.0pt;">21.0</td>
<td id="S4.T4.7.7.8.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.5pt 10.0pt;">20.6</td>
<td id="S4.T4.7.7.8.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 10.0pt;">30.0</td>
<td id="S4.T4.7.7.8.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 10.0pt;">16.4</td>
<td id="S4.T4.7.7.8.1.10" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.5pt 10.0pt;">16.1</td>
</tr>
<tr id="S4.T4.7.7.9.2" class="ltx_tr">
<th id="S4.T4.7.7.9.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 10.0pt;">Teacher-Student (1-step)</th>
<td id="S4.T4.7.7.9.2.2" class="ltx_td ltx_align_center" style="padding:0.5pt 10.0pt;">38.9</td>
<td id="S4.T4.7.7.9.2.3" class="ltx_td ltx_align_center" style="padding:0.5pt 10.0pt;">31.2</td>
<td id="S4.T4.7.7.9.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 10.0pt;">26.2</td>
<td id="S4.T4.7.7.9.2.5" class="ltx_td ltx_align_center" style="padding:0.5pt 10.0pt;">36.2</td>
<td id="S4.T4.7.7.9.2.6" class="ltx_td ltx_align_center" style="padding:0.5pt 10.0pt;">19.4</td>
<td id="S4.T4.7.7.9.2.7" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 10.0pt;">20.1</td>
<td id="S4.T4.7.7.9.2.8" class="ltx_td ltx_align_center" style="padding:0.5pt 10.0pt;">36.9</td>
<td id="S4.T4.7.7.9.2.9" class="ltx_td ltx_align_center" style="padding:0.5pt 10.0pt;">22.4</td>
<td id="S4.T4.7.7.9.2.10" class="ltx_td ltx_align_center" style="padding:0.5pt 10.0pt;">21.6</td>
</tr>
<tr id="S4.T4.7.7.10.3" class="ltx_tr">
<th id="S4.T4.7.7.10.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:0.5pt 10.0pt;">Teacher-Student (2-step)</th>
<td id="S4.T4.7.7.10.3.2" class="ltx_td ltx_align_center" style="padding:0.5pt 10.0pt;">51.0</td>
<td id="S4.T4.7.7.10.3.3" class="ltx_td ltx_align_center" style="padding:0.5pt 10.0pt;">44.3</td>
<td id="S4.T4.7.7.10.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 10.0pt;">36.1</td>
<td id="S4.T4.7.7.10.3.5" class="ltx_td ltx_align_center" style="padding:0.5pt 10.0pt;"><span id="S4.T4.7.7.10.3.5.1" class="ltx_text ltx_font_bold">43.3</span></td>
<td id="S4.T4.7.7.10.3.6" class="ltx_td ltx_align_center" style="padding:0.5pt 10.0pt;">23.8</td>
<td id="S4.T4.7.7.10.3.7" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.5pt 10.0pt;"><span id="S4.T4.7.7.10.3.7.1" class="ltx_text ltx_font_bold">23.8</span></td>
<td id="S4.T4.7.7.10.3.8" class="ltx_td ltx_align_center" style="padding:0.5pt 10.0pt;"><span id="S4.T4.7.7.10.3.8.1" class="ltx_text ltx_font_bold">45.2</span></td>
<td id="S4.T4.7.7.10.3.9" class="ltx_td ltx_align_center" style="padding:0.5pt 10.0pt;">28.9</td>
<td id="S4.T4.7.7.10.3.10" class="ltx_td ltx_align_center" style="padding:0.5pt 10.0pt;">26.9</td>
</tr>
<tr id="S4.T4.7.7.11.4" class="ltx_tr">
<th id="S4.T4.7.7.11.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" style="padding:0.5pt 10.0pt;">Full-ft (Oracle)</th>
<td id="S4.T4.7.7.11.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.5pt 10.0pt;"><span id="S4.T4.7.7.11.4.2.1" class="ltx_text ltx_font_bold">54.9</span></td>
<td id="S4.T4.7.7.11.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.5pt 10.0pt;"><span id="S4.T4.7.7.11.4.3.1" class="ltx_text ltx_font_bold">46.9</span></td>
<td id="S4.T4.7.7.11.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding:0.5pt 10.0pt;"><span id="S4.T4.7.7.11.4.4.1" class="ltx_text ltx_font_bold">38.9</span></td>
<td id="S4.T4.7.7.11.4.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.5pt 10.0pt;">40.7</td>
<td id="S4.T4.7.7.11.4.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.5pt 10.0pt;"><span id="S4.T4.7.7.11.4.6.1" class="ltx_text ltx_font_bold">25.1</span></td>
<td id="S4.T4.7.7.11.4.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding:0.5pt 10.0pt;">23.3</td>
<td id="S4.T4.7.7.11.4.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.5pt 10.0pt;">44.3</td>
<td id="S4.T4.7.7.11.4.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.5pt 10.0pt;"><span id="S4.T4.7.7.11.4.9.1" class="ltx_text ltx_font_bold">30.6</span></td>
<td id="S4.T4.7.7.11.4.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.5pt 10.0pt;"><span id="S4.T4.7.7.11.4.10.1" class="ltx_text ltx_font_bold">27.2</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>
Teacher-Student framework in <span id="S4.T4.9.1" class="ltx_text ltx_font_bold">COCO <math id="S4.T4.9.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T4.9.1.m1.1b"><mo stretchy="false" id="S4.T4.9.1.m1.1.1" xref="S4.T4.9.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T4.9.1.m1.1c"><ci id="S4.T4.9.1.m1.1.1.cmml" xref="S4.T4.9.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.9.1.m1.1d">\rightarrow</annotation></semantics></math> YTVIS</span> transfer learning setup.
Evaluated on YTVIS-<span id="S4.T4.11.2" class="ltx_text ltx_font_italic">val</span>.
</figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">LVIS <math id="S4.SS3.p2.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS3.p2.1.1.m1.1a"><mo stretchy="false" id="S4.SS3.p2.1.1.m1.1.1" xref="S4.SS3.p2.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.1.m1.1b"><ci id="S4.SS3.p2.1.1.m1.1.1.cmml" xref="S4.SS3.p2.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.1.m1.1c">\rightarrow</annotation></semantics></math> TAO Transfer Learning.</span>
Especially in this setup, all necessary vocabularies are already learned at the image pre-training stage. Therefore, we can avoid catastrophic forgetting by fine-tuning only the tracking part.
However, as it only updates the video model partially, it leads to inconsistent video representations, and thus performance rather slightly drops from the baseline LVIS-tracker.
Instead, our method preserves the performance of OLD classes (preventing catastrophic forgetting) and significantly improves the NEW class performance (benefiting from labeled learning).</p>
</div>
<div id="S4.SS3.p3" class="ltx_para ltx_noindent">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_bold">COCO <math id="S4.SS3.p3.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS3.p3.1.1.m1.1a"><mo stretchy="false" id="S4.SS3.p3.1.1.m1.1.1" xref="S4.SS3.p3.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.1.m1.1b"><ci id="S4.SS3.p3.1.1.m1.1.1.cmml" xref="S4.SS3.p3.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.1.m1.1c">\rightarrow</annotation></semantics></math> YTVIS Transfer Learning.</span>
This setup is more challenging as the model is required to achieve two goals, new class learning and old class preserving simultaneously.
We decompose these goals and approach this setup in two-step as described in Sec. <a href="#S3.SS2" title="3.2 Learn to Unforget in TAO ‣ 3 Proposed Method ‣ Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.
As can be shown in the results, the proposed two-step approach performs better than the direct application of teacher student scheme.
The final performance is comparable with, and in NEW classes outperforms, the oracle setup that use all the YTVIS training videos.
This shows that our proposal is generic and effective for standard image to video transfer learning setups.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we tackle the challenging problem of learning a large vocabulary video tracker.
We present a simple learning framework that uses all LVIS images and TAO videos to jointly learn the detection and tracking.
In specific, first, two spatial jittering methods, strong zoom-in/out and mosaicing, which effectively simulate the test-time large vocabulary object tracking are presented to enable tracker training with LVIS.
Second, a generic teacher-student scheme is proposed to prevent catastrophic forgetting while fine-tuning the image pre-trained models on videos.
We show that two new adaptation of using soft labels with MSE loss is crucial for the large vocabulary classifier distillation.
We hope our new learning framework settles as a baseline learning scheme for many follow-up large-vocabulary trackers in the future.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Acknowledgement</span>
This work was supported in part supported by Samsung Electronics Co., Ltd (G01200447)</p>
</div>
</section>
<section id="Pt0.A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.A </span>Appendix</h2>

<div id="Pt0.A1.p1" class="ltx_para ltx_noindent">
<p id="Pt0.A1.p1.1" class="ltx_p">In this appendix, we provide,</p>
<ol id="Pt0.A1.I1" class="ltx_enumerate">
<li id="Pt0.A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">A1.</span> 
<div id="Pt0.A1.I1.i1.p1" class="ltx_para">
<p id="Pt0.A1.I1.i1.p1.1" class="ltx_p">Our view of the proposal from video data scaling perspectives,</p>
</div>
</li>
<li id="Pt0.A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">A2.</span> 
<div id="Pt0.A1.I1.i2.p1" class="ltx_para">
<p id="Pt0.A1.I1.i2.p1.1" class="ltx_p">Datasets specifics used in the experiments,</p>
</div>
</li>
<li id="Pt0.A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">A3.</span> 
<div id="Pt0.A1.I1.i3.p1" class="ltx_para">
<p id="Pt0.A1.I1.i3.p1.1" class="ltx_p">Implementation details including COCO <math id="Pt0.A1.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="Pt0.A1.I1.i3.p1.1.m1.1a"><mo stretchy="false" id="Pt0.A1.I1.i3.p1.1.m1.1.1" xref="Pt0.A1.I1.i3.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Pt0.A1.I1.i3.p1.1.m1.1b"><ci id="Pt0.A1.I1.i3.p1.1.m1.1.1.cmml" xref="Pt0.A1.I1.i3.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.I1.i3.p1.1.m1.1c">\rightarrow</annotation></semantics></math> YTVIS transfer learning setup,</p>
</div>
</li>
<li id="Pt0.A1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">A4.</span> 
<div id="Pt0.A1.I1.i4.p1" class="ltx_para">
<p id="Pt0.A1.I1.i4.p1.1" class="ltx_p">Oracle analyses to investigate the disentangled impact of the method on object classification and tracking,</p>
</div>
</li>
</ol>
</div>
<section id="Pt0.A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.A.1 </span>Bridging Images and Videos</h3>

<div id="Pt0.A1.SS1.p1" class="ltx_para">
<p id="Pt0.A1.SS1.p1.1" class="ltx_p">Applying deep learning in the video domain fundamentally suffers from the data-hungry issue, and the situation will become even more severe for more complex and challenging tasks.
One promising direction we believe is leveraging already well-curated large-scale image data to complement the insufficient video data.
However, jointly using multiple datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite>, image and video labels, leads to several issues, detailed below.</p>
</div>
<div id="Pt0.A1.SS1.p2" class="ltx_para">
<p id="Pt0.A1.SS1.p2.1" class="ltx_p">In this paper, we investigate the new problem of large vocabulary tracking, one of the essential milestones for dynamic world understanding AI agents.
The task naturally lacks training labels as the data collection and annotation procedure is extremely expensive.
As a remedy, leveraging the large-scale images is an attractive solution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
However, in doing so, we face three main issues: 1) lacking video supervision in images, 2) semantic label inconsistency between images and videos, and 3) the domain gap (e.g., explicit data styles or implicit data distributions are different) between images and videos.
The current learning paradigm bypasses the first two issues by independently training the detection head and tracking head with images and videos (<span id="Pt0.A1.SS1.p2.1.1" class="ltx_text ltx_font_italic">decoupled</span>).
Instead, our learning framework explicitly handles the former two issues by hallucinating the supervisions and enables end-to-end video model learning from all training data, leading to better feature representations (<span id="Pt0.A1.SS1.p2.1.2" class="ltx_text ltx_font_italic">unified</span>).
The last issue is implicitly handled by the two-step training of image pre-training and video fine-tuning.
In the preliminary experiments, we observe a slight performance drop when concatenating image and video datasets as a single dataset, possibly due to the weaker feature adaptation toward the video domain.</p>
</div>
<div id="Pt0.A1.SS1.p3" class="ltx_para">
<p id="Pt0.A1.SS1.p3.1" class="ltx_p">The abovementioned issues are fundamental and compounded when jointly using the image and video labels for video recognition models.
We thus see they should be carefully considered and adequately handled.
Our proposal is an initial effort in this direction, and we believe more clever and innovative solutions will be developed and presented in the future.</p>
</div>
</section>
<section id="Pt0.A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.A.2 </span>Data</h3>

<div id="Pt0.A1.SS2.p1" class="ltx_para ltx_noindent">
<p id="Pt0.A1.SS2.p1.1" class="ltx_p"><span id="Pt0.A1.SS2.p1.1.1" class="ltx_text ltx_font_bold">LVIS</span>  
LVIS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> is a large-scale benchmark for large vocabulary image recognition.
It provides precise bounding boxes and masks annotations for various categories with the long-tailed distribution.
To be consistent with the prior works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, we use LVIS v0.5 dataset and pre-train the model on 482 (out of 1230) LVIS categories that correspond to TAO categories.</p>
</div>
<div id="Pt0.A1.SS2.p2" class="ltx_para ltx_noindent">
<p id="Pt0.A1.SS2.p2.1" class="ltx_p"><span id="Pt0.A1.SS2.p2.1.1" class="ltx_text ltx_font_bold">TAO</span>  
TAO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> is the first video benchmark for large vocabulary video recognition.
TAO dataset annotates 482 classes in total, which are the subset of LVIS dataset.
It has 400 videos, 216 classes in the training set, 988 videos, 302 classes in the validation set, and 1419 videos, 369 classes in the test set.
The videos are annotated in 1 FPS.
We fine-tune the model on TAO-train and evaluate on TAO-val (or TAO-test).</p>
</div>
<div id="Pt0.A1.SS2.p3" class="ltx_para ltx_noindent">
<p id="Pt0.A1.SS2.p3.1" class="ltx_p">We additionally use COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> and YTVIS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite> to evaluate the proposed teacher-student scheme on COCO <math id="Pt0.A1.SS2.p3.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="Pt0.A1.SS2.p3.1.m1.1a"><mo stretchy="false" id="Pt0.A1.SS2.p3.1.m1.1.1" xref="Pt0.A1.SS2.p3.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Pt0.A1.SS2.p3.1.m1.1b"><ci id="Pt0.A1.SS2.p3.1.m1.1.1.cmml" xref="Pt0.A1.SS2.p3.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.SS2.p3.1.m1.1c">\rightarrow</annotation></semantics></math> YTVIS transfer learning setup.</p>
</div>
<div id="Pt0.A1.SS2.p4" class="ltx_para ltx_noindent">
<p id="Pt0.A1.SS2.p4.1" class="ltx_p"><span id="Pt0.A1.SS2.p4.1.1" class="ltx_text ltx_font_bold">COCO 2017</span>  
COCO contains 118k training images and 5k validation images.
We pre-train the model on 20 (out of 80) COCO categories, as the remaining 60 categories cannot be evaluated with YTVIS annotations.</p>
</div>
<div id="Pt0.A1.SS2.p5" class="ltx_para ltx_noindent">
<p id="Pt0.A1.SS2.p5.1" class="ltx_p"><span id="Pt0.A1.SS2.p5.1.1" class="ltx_text ltx_font_bold">YTVIS</span>  
YTVIS is largest video benchmark for video instance segmentation.
YTVIS annotates 40 classes in total. It has 2238 training, 302 validation, 343 test video clips.
The videos are annotated in 5 FPS.
We fine-tune the model on 30 (out of 40) YTVIS categories to simulate missing 10 categories and new 20 categories during transfer learning (see Fig. <a href="#Pt0.A1.F4" title="Figure 4 ‣ 0.A.2 Data ‣ Appendix 0.A Appendix ‣ Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).
We evaluate the model on YTVIS-val.</p>
</div>
<figure id="Pt0.A1.F4" class="ltx_figure"><img src="/html/2212.10147/assets/figures/coco_to_ytvis.png" id="Pt0.A1.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="228" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>
We provide the detailed category distribution setup of COCO <math id="Pt0.A1.F4.2.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="Pt0.A1.F4.2.m1.1b"><mo stretchy="false" id="Pt0.A1.F4.2.m1.1.1" xref="Pt0.A1.F4.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Pt0.A1.F4.2.m1.1c"><ci id="Pt0.A1.F4.2.m1.1.1.cmml" xref="Pt0.A1.F4.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.F4.2.m1.1d">\rightarrow</annotation></semantics></math> YTVIS transfer learning used in the experiments.
</figcaption>
</figure>
</section>
<section id="Pt0.A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.A.3 </span>Implementation Details</h3>

<div id="Pt0.A1.SS3.p1" class="ltx_para ltx_noindent">
<p id="Pt0.A1.SS3.p1.2" class="ltx_p"><span id="Pt0.A1.SS3.p1.2.1" class="ltx_text ltx_font_bold">Training.</span>  
The proposals are implemented under the MMdetection framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
The COCO-style training schedule of 2<math id="Pt0.A1.SS3.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="Pt0.A1.SS3.p1.1.m1.1a"><mo id="Pt0.A1.SS3.p1.1.m1.1.1" xref="Pt0.A1.SS3.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="Pt0.A1.SS3.p1.1.m1.1b"><times id="Pt0.A1.SS3.p1.1.m1.1.1.cmml" xref="Pt0.A1.SS3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.SS3.p1.1.m1.1c">\times</annotation></semantics></math> and 1<math id="Pt0.A1.SS3.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="Pt0.A1.SS3.p1.2.m2.1a"><mo id="Pt0.A1.SS3.p1.2.m2.1.1" xref="Pt0.A1.SS3.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="Pt0.A1.SS3.p1.2.m2.1b"><times id="Pt0.A1.SS3.p1.2.m2.1.1.cmml" xref="Pt0.A1.SS3.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.SS3.p1.2.m2.1c">\times</annotation></semantics></math> are adopted for LVIS pre-training and TAO fine-tuning, respectively.
We set the maximum number of predictions per image to 1000 for not losing correct predictions at frame-level <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, which makes the inter-frame object affinity matrix large and the subsequent tracking challenging. However, thanks to the proposed mosaic training, we see our tracker is robust to dense object tracking.
Batch size of 16 (2 per GPU) and an initial learning rate of 0.02 are used.
We randomly select a scale between 640 to 800 to resize the shorter side of images during training.
For the hyper-parameters of the models, we follow the original implementations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>, <a href="#bib.bib108" title="" class="ltx_ref">108</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>.
The standard learning protocol is first pre-training the model in LVIS and then fine-tuning on TAO, <span id="Pt0.A1.SS3.p1.2.2" class="ltx_text ltx_font_italic">i.e.</span>, decoupled learning.</p>
</div>
<div id="Pt0.A1.SS3.p2" class="ltx_para ltx_noindent">
<p id="Pt0.A1.SS3.p2.1" class="ltx_p"><span id="Pt0.A1.SS3.p2.1.1" class="ltx_text ltx_font_bold">Testing.</span>  
Our method processes video frames recursively, generating detection boxes and matching them with the candidate tracks from the past frames.
Apart from the conventional tracking algorithms, we see the motion is highly irregular for tracking the large vocabulary of objects.
Thus, the most reliable way is to link detection boxes only based on their appearance features.
Here, the main matching strategy is a bi-directional softmax operation that examines the two matched objects being each other’s nearest neighbor in the embedding space <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>.
For the unmatched tracks, we keep them until it remains for more than 30 frames.
We use resized frames of 1080<math id="Pt0.A1.SS3.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="Pt0.A1.SS3.p2.1.m1.1a"><mo id="Pt0.A1.SS3.p2.1.m1.1.1" xref="Pt0.A1.SS3.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="Pt0.A1.SS3.p2.1.m1.1b"><times id="Pt0.A1.SS3.p2.1.m1.1.1.cmml" xref="Pt0.A1.SS3.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.SS3.p2.1.m1.1c">\times</annotation></semantics></math>1080 for testing.</p>
</div>
<div id="Pt0.A1.SS3.p3" class="ltx_para ltx_noindent">
<p id="Pt0.A1.SS3.p3.2" class="ltx_p"><span id="Pt0.A1.SS3.p3.1.1" class="ltx_text ltx_font_bold">COCO <math id="Pt0.A1.SS3.p3.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="Pt0.A1.SS3.p3.1.1.m1.1a"><mo stretchy="false" id="Pt0.A1.SS3.p3.1.1.m1.1.1" xref="Pt0.A1.SS3.p3.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="Pt0.A1.SS3.p3.1.1.m1.1b"><ci id="Pt0.A1.SS3.p3.1.1.m1.1.1.cmml" xref="Pt0.A1.SS3.p3.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.SS3.p3.1.1.m1.1c">\rightarrow</annotation></semantics></math> YTVIS transfer learning setup.</span>  
We adopt MaskRCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> with ResNet-50 FPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> backbone for the COCO image pre-training.
We use 1<math id="Pt0.A1.SS3.p3.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="Pt0.A1.SS3.p3.2.m1.1a"><mo id="Pt0.A1.SS3.p3.2.m1.1.1" xref="Pt0.A1.SS3.p3.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="Pt0.A1.SS3.p3.2.m1.1b"><times id="Pt0.A1.SS3.p3.2.m1.1.1.cmml" xref="Pt0.A1.SS3.p3.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.SS3.p3.2.m1.1c">\times</annotation></semantics></math> training pipeline.
We transfer the pre-trained MaskRCNN model weights to MaskTrack RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite> for the YTVIS video fine-tuning.
At this stage, new weights are added to the class head, bounding box head, and mask head to accommodate newly added classes.
Also, the track head is appended to the model for object tracking.
We follow the original training schedules and hyper-parameters of MaskTrack RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite>.</p>
</div>
<div id="Pt0.A1.SS3.p4" class="ltx_para">
<p id="Pt0.A1.SS3.p4.1" class="ltx_p">The presented two-step teacher-student scheme is applied to the model during transfer learning (see Fig. <a href="#Pt0.A1.F4" title="Figure 4 ‣ 0.A.2 Data ‣ Appendix 0.A Appendix ‣ Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).
The distillation in the class head and the bounding box head follows the methods noted in the main paper.
For the mask head distillation, we collect teacher and student mask predictions and minimize their difference through MSE loss.</p>
</div>
<div id="Pt0.A1.SS3.p5" class="ltx_para">
<p id="Pt0.A1.SS3.p5.1" class="ltx_p">As the detailed class-wise evaluation in YTVIS is only possible for the 40 object categories, we simulate the pattern in Fig. <a href="#Pt0.A1.F4" title="Figure 4 ‣ 0.A.2 Data ‣ Appendix 0.A Appendix ‣ Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> by shrinking the original object categories in COCO and YTVIS.
In specific, for image pre-training, we trained the MaskRCNN on 20 COCO object categories.
For video fine-tuning, we trained the MaskTrack RCNN on YTVIS videos with 30 object categories, which consist of 10 overlapping object categories with the 20 COCO pre-trained categories and 20 new object categories.
The 10 overlapping object categories are selected based on the annotation frequency.
In this way, we can simulate the co-existence of missing object categories and new object categories during transfer learning.</p>
</div>
</section>
<section id="Pt0.A1.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.A.4 </span>Oracle Analysis</h3>

<div id="Pt0.A1.SS4.p1" class="ltx_para">
<p id="Pt0.A1.SS4.p1.1" class="ltx_p">To disentangle the impact of methods on object classification and tracking, we use two oracles: class oracle and track oracle on TAO validation set <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<div id="Pt0.A1.SS4.p2" class="ltx_para">
<p id="Pt0.A1.SS4.p2.1" class="ltx_p">For <span id="Pt0.A1.SS4.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">class oracle</span>, we first compute the best matching between predicted and ground truth tracks in each video.
The predicted tracks that match to a ground truth track with 3D IoU <math id="Pt0.A1.SS4.p2.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="Pt0.A1.SS4.p2.1.m1.1a"><mo id="Pt0.A1.SS4.p2.1.m1.1.1" xref="Pt0.A1.SS4.p2.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="Pt0.A1.SS4.p2.1.m1.1b"><gt id="Pt0.A1.SS4.p2.1.m1.1.1.cmml" xref="Pt0.A1.SS4.p2.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.SS4.p2.1.m1.1c">&gt;</annotation></semantics></math> 0.5 are assigned the category of their matched ground truth track.
Tracks that do not match to a ground truth track are treated as false positives.
This allows us to analyze the <span id="Pt0.A1.SS4.p2.1.2" class="ltx_text ltx_font_bold ltx_font_italic">pure tracking ability</span> of models assuming the classification task is solved.</p>
</div>
<div id="Pt0.A1.SS4.p3" class="ltx_para">
<p id="Pt0.A1.SS4.p3.1" class="ltx_p">For <span id="Pt0.A1.SS4.p3.1.1" class="ltx_text ltx_font_bold ltx_font_italic">track oracle</span>, we compute the best possible assignment of per-frame detection boxes to tracks, by comparing them with ground truth.
The class predictions for each detection are held constant. Any detection boxes that are not matched are removed.
This allows us to evaluate the <span id="Pt0.A1.SS4.p3.1.2" class="ltx_text ltx_font_bold ltx_font_italic">pure classification ability</span> of models given a perfectly linked per-frame detection boxes.</p>
</div>
<div id="Pt0.A1.SS4.p4" class="ltx_para">
<p id="Pt0.A1.SS4.p4.1" class="ltx_p">The results are summarized in Table <a href="#Pt0.A1.T5" title="Table 5 ‣ 0.A.4 Oracle Analysis ‣ Appendix 0.A Appendix ‣ Bridging Images and Videos: A Simple Learning Framework for Large Vocabulary Video Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
We use the same FasterRCNN-RFS tracker for the Decoupled and Unified methods.
We observe that our method outperforms the previous approaches in both oracle types.
This shows that the unified learning framework using all training data, LVIS and TAO, essentially improves the model’s tracking and classification ability significantly.</p>
</div>
<figure id="Pt0.A1.T5" class="ltx_table">
<div id="Pt0.A1.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:416.3pt;height:57.2pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-122.8pt,16.7pt) scale(0.62895335863714,0.62895335863714) ;">
<table id="Pt0.A1.T5.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Pt0.A1.T5.1.1.1.1" class="ltx_tr">
<th id="Pt0.A1.T5.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding:1pt 16.0pt;" rowspan="2">     <span id="Pt0.A1.T5.1.1.1.1.1.1" class="ltx_text">Method</span></th>
<th id="Pt0.A1.T5.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1pt 16.0pt;" colspan="3">     Oracle Class (pure tracking ability)</th>
<th id="Pt0.A1.T5.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1pt 16.0pt;" colspan="3">     Oracle Track (pure classification ability)</th>
</tr>
<tr id="Pt0.A1.T5.1.1.2.2" class="ltx_tr">
<th id="Pt0.A1.T5.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:1pt 16.0pt;">     Track AP50</th>
<th id="Pt0.A1.T5.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:1pt 16.0pt;">     Track AP75</th>
<th id="Pt0.A1.T5.1.1.2.2.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding:1pt 16.0pt;">     Track AP</th>
<th id="Pt0.A1.T5.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:1pt 16.0pt;">     Track AP50</th>
<th id="Pt0.A1.T5.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:1pt 16.0pt;">     Track AP75</th>
<th id="Pt0.A1.T5.1.1.2.2.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" style="padding:1pt 16.0pt;">     Track AP</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Pt0.A1.T5.1.1.3.1" class="ltx_tr">
<td id="Pt0.A1.T5.1.1.3.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding:1pt 16.0pt;">     SORT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite></td>
<td id="Pt0.A1.T5.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 16.0pt;">     30.2</td>
<td id="Pt0.A1.T5.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 16.0pt;">     -</td>
<td id="Pt0.A1.T5.1.1.3.1.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding:1pt 16.0pt;">     -</td>
<td id="Pt0.A1.T5.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 16.0pt;">     31.5</td>
<td id="Pt0.A1.T5.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 16.0pt;">     -</td>
<td id="Pt0.A1.T5.1.1.3.1.7" class="ltx_td ltx_align_right ltx_border_t" style="padding:1pt 16.0pt;">     -</td>
</tr>
<tr id="Pt0.A1.T5.1.1.4.2" class="ltx_tr">
<td id="Pt0.A1.T5.1.1.4.2.1" class="ltx_td ltx_align_left ltx_border_r" style="padding:1pt 16.0pt;">     Decoupled <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite></td>
<td id="Pt0.A1.T5.1.1.4.2.2" class="ltx_td ltx_align_center" style="padding:1pt 16.0pt;">     34.7</td>
<td id="Pt0.A1.T5.1.1.4.2.3" class="ltx_td ltx_align_center" style="padding:1pt 16.0pt;">     12.2</td>
<td id="Pt0.A1.T5.1.1.4.2.4" class="ltx_td ltx_align_right ltx_border_r" style="padding:1pt 16.0pt;">     15.1</td>
<td id="Pt0.A1.T5.1.1.4.2.5" class="ltx_td ltx_align_center" style="padding:1pt 16.0pt;">     32.1</td>
<td id="Pt0.A1.T5.1.1.4.2.6" class="ltx_td ltx_align_center" style="padding:1pt 16.0pt;">     12.1</td>
<td id="Pt0.A1.T5.1.1.4.2.7" class="ltx_td ltx_align_right" style="padding:1pt 16.0pt;">     14.8</td>
</tr>
<tr id="Pt0.A1.T5.1.1.5.3" class="ltx_tr">
<td id="Pt0.A1.T5.1.1.5.3.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding:1pt 16.0pt;">     <span id="Pt0.A1.T5.1.1.5.3.1.1" class="ltx_text ltx_font_bold">Unified (Ours)</span></td>
<td id="Pt0.A1.T5.1.1.5.3.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 16.0pt;">     <span id="Pt0.A1.T5.1.1.5.3.2.1" class="ltx_text ltx_font_bold">38.1</span></td>
<td id="Pt0.A1.T5.1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 16.0pt;">     <span id="Pt0.A1.T5.1.1.5.3.3.1" class="ltx_text ltx_font_bold">17.1</span></td>
<td id="Pt0.A1.T5.1.1.5.3.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" style="padding:1pt 16.0pt;">     <span id="Pt0.A1.T5.1.1.5.3.4.1" class="ltx_text ltx_font_bold">18.4</span></td>
<td id="Pt0.A1.T5.1.1.5.3.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 16.0pt;">     <span id="Pt0.A1.T5.1.1.5.3.5.1" class="ltx_text ltx_font_bold">43.1</span></td>
<td id="Pt0.A1.T5.1.1.5.3.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 16.0pt;">     <span id="Pt0.A1.T5.1.1.5.3.6.1" class="ltx_text ltx_font_bold">16.7</span></td>
<td id="Pt0.A1.T5.1.1.5.3.7" class="ltx_td ltx_align_right ltx_border_bb" style="padding:1pt 16.0pt;">     <span id="Pt0.A1.T5.1.1.5.3.7.1" class="ltx_text ltx_font_bold">19.9</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>
<span id="Pt0.A1.T5.3.1" class="ltx_text ltx_font_bold">Oracle analysis.</span>
We analyze the performance of two types of oracles: Oracle Class and Oracle Track.
The former provides the pure tracking ability of the model, and the latter allows us to analyze pure classification ability.
</figcaption>
</figure>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Aljundi, R., Lin, M., Goujaud, B., Bengio, Y.: Gradient based sample selection
for online continual learning. arXiv:1903.08671 (2019)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Barnes, C., Shechtman, E., Finkelstein, A., Goldman, D.B.: Patchmatch: A
randomized correspondence algorithm for structural image editing. ACM Trans.
Graph. <span id="bib.bib2.1.1" class="ltx_text ltx_font_bold">28</span>(3),  24 (2009)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Bergmann, P., Meinhardt, T., Leal-Taixe, L.: Tracking without bells and
whistles. In: ICCV. pp. 941–951 (2019)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Bewley, A., Ge, Z., Ott, L., Ramos, F., Upcroft, B.: Simple online and realtime
tracking. In: ICIP. pp. 3464–3468 (2016)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Bochkovskiy, A., Wang, C.Y., Liao, H.Y.M.: Yolov4: Optimal speed and accuracy
of object detection. arXiv:2004.10934 (2020)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Cai, Z., Vasconcelos, N.: Cascade r-cnn: Delving into high quality object
detection. In: CVPR. pp. 6154–6162 (2018)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Cai, Z., Vasconcelos, N.: Cascade r-cnn: High quality object detection and
instance segmentation. PAMI (2019)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Chang, N., Yu, Z., Wang, Y.X., Anandkumar, A., Fidler, S., Alvarez, J.M.:
Image-level or object-level? a tale of two resampling strategies for
long-tailed detection. arXiv:2104.05702 (2021)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Chaudhry, A., Dokania, P.K., Ajanthan, T., Torr, P.H.: Riemannian walk for
incremental learning: Understanding forgetting and intransigence. In: ECCV.
pp. 532–547 (2018)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Chen, K., Pang, J., Wang, J., Xiong, Y., Li, X., Sun, S., Feng, W., Liu, Z.,
Shi, J., Ouyang, W., et al.: Hybrid task cascade for instance segmentation.
In: CVPR. pp. 4974–4983 (2019)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Chen, K., Wang, J., Pang, J., Cao, Y., Xiong, Y., Li, X., Sun, S., Feng, W.,
Liu, Z., Xu, J., et al.: Mmdetection: Open mmlab detection toolbox and
benchmark. arXiv preprint arXiv:1906.07155 (2019)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Dave, A., Dollár, P., Ramanan, D., Kirillov, A., Girshick, R.: Evaluating
large-vocabulary object detectors: The devil is in the details.
arXiv:2102.01066 (2021)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Dave, A., Khurana, T., Tokmakov, P., Schmid, C., Ramanan, D.: Tao: A
large-scale benchmark for tracking any object. In: ECCV. pp. 436–454.
Springer (2020)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Feichtenhofer, C., Pinz, A., Zisserman, A.: Detect to track and track to
detect. In: ICCV. pp. 3038–3046 (2017)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Fu, Y., Liu, S., Iqbal, U., De Mello, S., Shi, H., Kautz, J.: Learning to track
instances without video annotations. In: CVPR. pp. 8680–8689 (2021)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Ghiasi, G., Cui, Y., Srinivas, A., Qian, R., Lin, T.Y., Cubuk, E.D., Le, Q.V.,
Zoph, B.: Simple copy-paste is a strong data augmentation method for instance
segmentation. In: CVPR. pp. 2918–2928 (2021)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Gupta, A., Dollar, P., Girshick, R.: Lvis: A dataset for large vocabulary
instance segmentation. In: CVPR. pp. 5356–5364 (2019)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
He, H., Garcia, E.A.: Learning from imbalanced data. IEEE Transactions on
knowledge and data engineering <span id="bib.bib18.1.1" class="ltx_text ltx_font_bold">21</span>(9), 1263–1284 (2009)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
He, K., Gkioxari, G., Dollár, P., Girshick, R.: Mask r-cnn. In: ICCV. pp.
2961–2969 (2017)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
recognition. In: CVPR. pp. 770–778 (2016)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Held, D., Thrun, S., Savarese, S.: Learning to track at 100 fps with deep
regression networks. In: ECCV. pp. 749–765. Springer (2016)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural
network. arXiv:1503.02531 (2015)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Hsieh, T.I., Robb, E., Chen, H.T., Huang, J.B.: Droploss for long-tail instance
segmentation. arXiv:2104.06402 (2021)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Hu, X., Jiang, Y., Tang, K., Chen, J., Miao, C., Zhang, H.: Learning to segment
the tail. In: CVPR. pp. 14045–14054 (2020)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Kang, B., Xie, S., Rohrbach, M., Yan, Z., Gordo, A., Feng, J., Kalantidis, Y.:
Decoupling representation and classifier for long-tailed recognition.
arXiv:1910.09217 (2019)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Kim, C., Li, F., Ciptadi, A., Rehg, J.M.: Multiple hypothesis tracking
revisited. In: ICCV. pp. 4696–4704 (2015)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Kim, D., Woo, S., Lee, J.Y., Kweon, I.S.: Deep video inpainting. In: CVPR. pp.
5792–5801 (2019)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Kim, D., Woo, S., Lee, J.Y., Kweon, I.S.: Video panoptic segmentation. In:
CVPR. pp. 9859–9868 (2020)

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Kim, T., Oh, J., Kim, N., Cho, S., Yun, S.Y.: Comparing kullback-leibler
divergence and mean squared error loss in knowledge distillation.
arXiv:2105.08919 (2021)

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu,
A.A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al.:
Overcoming catastrophic forgetting in neural networks. Proceedings of the
national academy of sciences <span id="bib.bib30.1.1" class="ltx_text ltx_font_bold">114</span>(13), 3521–3526 (2017)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Kuznetsova, A., Ju Hwang, S., Rosenhahn, B., Sigal, L.: Expanding object
detector’s horizon: Incremental learning framework for object detection in
videos. In: CVPR. pp. 28–36 (2015)

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Lai, W.S., Huang, J.B., Wang, O., Shechtman, E., Yumer, E., Yang, M.H.:
Learning blind video temporal consistency. In: ECCV. pp. 170–185 (2018)

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Lai, Z., Lu, E., Xie, W.: Mast: A memory-augmented self-supervised tracker. In:
CVPR. pp. 6479–6488 (2020)

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Lai, Z., Xie, W.: Self-supervised learning for video correspondence flow.
arXiv:1905.00875 (2019)

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Leal-Taixé, L., Canton-Ferrer, C., Schindler, K.: Learning by tracking:
Siamese cnn for robust target association. In: CVPR Workshops. pp. 33–40
(2016)

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Leal-Taixé, L., Milan, A., Schindler, K., Cremers, D., Reid, I., Roth, S.:
Tracking the trackers: an analysis of the state of the art in multiple object
tracking. arXiv:1704.02781 (2017)

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Lee, S.W., Kim, J.H., Jun, J., Ha, J.W., Zhang, B.T.: Overcoming catastrophic
forgetting by incremental moment matching. arXiv:1703.08475 (2017)

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Lei, C., Xing, Y., Chen, Q.: Blind video temporal consistency via deep video
prior. Advances in Neural Information Processing Systems <span id="bib.bib38.1.1" class="ltx_text ltx_font_bold">33</span> (2020)

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Li, X., Liu, S., De Mello, S., Wang, X., Kautz, J., Yang, M.H.: Joint-task
self-supervised learning for temporal correspondence. arXiv:1909.11895
(2019)

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Li, Y., Wang, T., Kang, B., Tang, S., Wang, C., Li, J., Feng, J.: Overcoming
classifier imbalance for long-tail object detection with balanced group
softmax. In: CVPR. pp. 10991–11000 (2020)

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S.:
Feature pyramid networks for object detection. In: CVPR. pp. 2117–2125
(2017)

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,
Dollár, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In:
ECCV. pp. 740–755. Springer (2014)

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Liu, Y., Zulfikar, I.E., Luiten, J., Dave, A., Ošep, A., Ramanan, D.,
Leibe, B., Leal-Taixé, L.: Opening up open-world tracking.
arXiv:2104.11221 (2021)

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Liu, Z., Miao, Z., Zhan, X., Wang, J., Gong, B., Yu, S.X.: Large-scale
long-tailed recognition in an open world. In: CVPR. pp. 2537–2546 (2019)

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Lu, Z., Rathod, V., Votel, R., Huang, J.: Retinatrack: Online single stage
joint detection and tracking. In: CVPR. pp. 14668–14678 (2020)

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Manning, C., Schutze, H.: Foundations of statistical natural language
processing. MIT press (1999)

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
McCloskey, M., Cohen, N.J.: Catastrophic interference in connectionist
networks: The sequential learning problem. In: Psychology of learning and
motivation, vol. 24, pp. 109–165. Elsevier (1989)

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Meinhardt, T., Kirillov, A., Leal-Taixe, L., Feichtenhofer, C.: Trackformer:
Multi-object tracking with transformers. arXiv:2101.02702 (2021)

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Milan, A., Rezatofighi, S.H., Dick, A., Reid, I., Schindler, K.: Online
multi-target tracking using recurrent neural networks. In: Thirty-First AAAI
Conference on Artificial Intelligence (2017)

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Oh, S.W., Lee, J.Y., Sunkavalli, K., Kim, S.J.: Fast video object segmentation
by reference-guided mask propagation. In: CVPR. pp. 7376–7385 (2018)

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Oh, S.W., Lee, J.Y., Xu, N., Kim, S.J.: Video object segmentation using
space-time memory networks. In: ICCV. pp. 9226–9235 (2019)

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Pan, T.Y., Zhang, C., Li, Y., Hu, H., Xuan, D., Changpinyo, S., Gong, B., Chao,
W.L.: On model calibration for long-tailed object detection and instance
segmentation. arXiv:2107.02170 (2021)

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Pang, J., Qiu, L., Li, X., Chen, H., Li, Q., Darrell, T., Yu, F.: Quasi-dense
similarity learning for multiple object tracking. In: CVPR. pp. 164–173
(2021)

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Park, K., Woo, S., Oh, S.W., Kweon, I.S., Lee, J.Y.: Per-clip video object
segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. pp. 1352–1361 (2022)

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Peng, J., Wang, C., Wan, F., Wu, Y., Wang, Y., Tai, Y., Wang, C., Li, J.,
Huang, F., Fu, Y.: Chained-tracker: Chaining paired attentive regression
results for end-to-end joint multiple-object detection and tracking. In:
ECCV. pp. 145–161. Springer (2020)

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Prabhu, A., Torr, P.H., Dokania, P.K.: Gdumb: A simple approach that questions
our progress in continual learning. In: ECCV. pp. 524–540. Springer (2020)

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Purushwalkam, S., Ye, T., Gupta, S., Gupta, A.: Aligning videos in space and
time. In: ECCV. pp. 262–278. Springer (2020)

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Ramanan, D., Forsyth, D.A.: Finding and tracking people from the bottom up. In:
CVPR. vol. 2, pp. II–II. IEEE (2003)

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Rebuffi, S.A., Kolesnikov, A., Sperl, G., Lampert, C.H.: icarl: Incremental
classifier and representation learning. In: CVPR. pp. 2001–2010 (2017)

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Ren, J., Yu, C., Sheng, S., Ma, X., Zhao, H., Yi, S., Li, H.: Balanced
meta-softmax for long-tailed visual recognition. arXiv:2007.10740 (2020)

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object
detection with region proposal networks. NIPS <span id="bib.bib61.1.1" class="ltx_text ltx_font_bold">28</span>, 91–99 (2015)

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Riloff, E.: Automatically generating extraction patterns from untagged text.
In: Proceedings of the national conference on artificial intelligence. pp.
1044–1049 (1996)

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Riloff, E., Wiebe, J.: Learning extraction patterns for subjective expressions.
In: Proceedings of the 2003 conference on Empirical methods in natural
language processing. pp. 105–112 (2003)

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Sadeghian, A., Alahi, A., Savarese, S.: Tracking the untrackable: Learning to
track multiple cues with long-term dependencies. In: ICCV. pp. 300–311
(2017)

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Scudder, H.: Probability of error of some adaptive pattern-recognition
machines. IEEE Transactions on Information Theory <span id="bib.bib65.1.1" class="ltx_text ltx_font_bold">11</span>(3), 363–371
(1965)

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Shin, H., Lee, J.K., Kim, J., Kim, J.: Continual learning with deep generative
replay. arXiv:1705.08690 (2017)

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Shmelkov, K., Schmid, C., Alahari, K.: Incremental learning of object detectors
without catastrophic forgetting. In: ICCV. pp. 3400–3409 (2017)

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Sio, C.H., Ma, Y.J., Shuai, H.H., Chen, J.C., Cheng, W.H.: S2siamfc:
Self-supervised fully convolutional siamese network for visual tracking. In:
Proceedings of ACM International Conference on Multimedia. pp. 1948–1957
(2020)

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Sohn, K., Zhang, Z., Li, C.L., Zhang, H., Lee, C.Y., Pfister, T.: A simple
semi-supervised learning framework for object detection. arXiv preprint
arXiv:2005.04757 (2020)

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Son, J., Baek, M., Cho, M., Han, B.: Multi-object tracking with quadruplet
convolutional neural networks. In: CVPR. pp. 5620–5629 (2017)

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Sun, P., Jiang, Y., Zhang, R., Xie, E., Cao, J., Hu, X., Kong, T., Yuan, Z.,
Wang, C., Luo, P.: Transtrack: Multiple-object tracking with transformer.
arXiv:2012.15460 (2020)

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Tan, J., Lu, X., Zhang, G., Yin, C., Li, Q.: Equalization loss v2: A new
gradient balance approach for long-tailed object detection. In: CVPR. pp.
1685–1694 (2021)

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Tan, J., Wang, C., Li, B., Li, Q., Ouyang, W., Yin, C., Yan, J.: Equalization
loss for long-tailed object recognition. In: CVPR. pp. 11662–11671 (2020)

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Tang, Y., Chen, W., Luo, Y., Zhang, Y.: Humble teachers teach better students
for semi-supervised object detection. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 3132–3141 (2021)

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Vondrick, C., Shrivastava, A., Fathi, A., Guadarrama, S., Murphy, K.: Tracking
emerges by colorizing videos. In: ECCV. pp. 391–408 (2018)

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Vu, T., Jang, H., Pham, T.X., Yoo, C.D.: Cascade rpn: Delving into high-quality
region proposal network with adaptive convolution. arXiv:1909.06720 (2019)

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Wang, J., Wang, X., Shang-Guan, Y., Gupta, A.: Wanderlust: Online continual
object detection in the real world. In: ICCV. pp. 10829–10838 (2021)

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Wang, J., Zhang, W., Zang, Y., Cao, Y., Pang, J., Gong, T., Chen, K., Liu, Z.,
Loy, C.C., Lin, D.: Seesaw loss for long-tailed instance segmentation. In:
CVPR. pp. 9695–9704 (2021)

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Wang, N., Song, Y., Ma, C., Zhou, W., Liu, W., Li, H.: Unsupervised deep
tracking. In: CVPR. pp. 1308–1317 (2019)

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Wang, T., Li, Y., Kang, B., Li, J., Liew, J., Tang, S., Hoi, S., Feng, J.: The
devil is in classification: A simple framework for long-tail instance
segmentation. In: ECCV. pp. 728–744. Springer (2020)

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Wang, T., Zhu, Y., Zhao, C., Zeng, W., Wang, J., Tang, M.: Adaptive class
suppression loss for long-tail object detection. In: CVPR. pp. 3103–3112
(2021)

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Wang, W., Feiszli, M., Wang, H., Tran, D.: Unidentified video objects: A
benchmark for dense, open-world segmentation. arXiv:2104.04691 (2021)

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Wang, X., Jabri, A., Efros, A.A.: Learning correspondence from the
cycle-consistency of time. In: CVPR. pp. 2566–2576 (2019)

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Wang, X., Huang, T.E., Darrell, T., Gonzalez, J.E., Yu, F.: Frustratingly
simple few-shot object detection. arXiv:2003.06957 (2020)

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Wang, Z., Zheng, L., Liu, Y., Li, Y., Wang, S.: Towards real-time multi-object
tracking. In: Computer Vision–ECCV 2020: 16th European Conference, Glasgow,
UK, August 23–28, 2020, Proceedings, Part XI 16. pp. 107–122. Springer
(2020)

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
Wojke, N., Bewley, A., Paulus, D.: Simple online and realtime tracking with a
deep association metric. In: ICIP. pp. 3645–3649. IEEE (2017)

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Wu, J., Cao, J., Song, L., Wang, Y., Yang, M., Yuan, J.: Track to detect and
segment: An online multi-object tracker. In: CVPR. pp. 12352–12361 (2021)

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Wu, T., Huang, Q., Liu, Z., Wang, Y., Lin, D.: Distribution-balanced loss for
multi-label classification in long-tailed datasets. In: ECCV. pp. 162–178.
Springer (2020)

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Wu, Y., Chen, Y., Wang, L., Ye, Y., Liu, Z., Guo, Y., Fu, Y.: Large scale
incremental learning. In: CVPR. pp. 374–382 (2019)

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
Xiao, B., Wu, H., Wei, Y.: Simple baselines for human pose estimation and
tracking. In: ECCV. pp. 466–481 (2018)

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
Xie, Q., Luong, M.T., Hovy, E., Le, Q.V.: Self-training with noisy student
improves imagenet classification. In: CVPR. pp. 10687–10698 (2020)

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Xu, J., Wang, X.: Rethinking self-supervised correspondence learning: A video
frame-level similarity perspective. arXiv:2103.17263 (2021)

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
Xu, M., Zhang, Z., Hu, H., Wang, J., Wang, L., Wei, F., Bai, X., Liu, Z.:
End-to-end semi-supervised object detection with soft teacher. In: ICCV. pp.
3060–3069 (2021)

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
Xu, M., Zhang, Z., Wei, F., Lin, Y., Cao, Y., Lin, S., Hu, H., Bai, X.:
Bootstrap your object detector via mixed training <span id="bib.bib94.1.1" class="ltx_text ltx_font_bold">34</span> (2021)

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
Yang, L., Fan, Y., Xu, N.: Video instance segmentation. In: ICCV. pp.
5188–5197 (2019)

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
Yun, S., Han, D., Oh, S.J., Chun, S., Choe, J., Yoo, Y.: Cutmix: Regularization
strategy to train strong classifiers with localizable features. In: ICCV. pp.
6023–6032 (2019)

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
Zang, Y., Huang, C., Loy, C.C.: Fasa: Feature augmentation and sampling
adaptation for long-tailed instance segmentation. arXiv:2102.12867 (2021)

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
Zeng, F., Dong, B., Wang, T., Zhang, X., Wei, Y.: Motr: End-to-end
multiple-object tracking with transformer. arXiv:2105.03247 (2021)

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
Zhang, C., Pan, T.Y., Li, Y., Hu, H., Xuan, D., Changpinyo, S., Gong, B., Chao,
W.L.: Mosaicos: A simple and effective use of object-centric images for
long-tailed object detection. arXiv:2102.08884 (2021)

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
Zhang, S., Li, Z., Yan, S., He, X., Sun, J.: Distribution alignment: A unified
framework for long-tail visual recognition. In: CVPR. pp. 2361–2370 (2021)

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
Zhang, Y., Sun, P., Jiang, Y., Yu, D., Yuan, Z., Luo, P., Liu, W., Wang, X.:
Bytetrack: Multi-object tracking by associating every detection box.
arXiv:2110.06864 (2021)

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
Zhang, Y., Wang, C., Wang, X., Zeng, W., Liu, W.: Fairmot: On the fairness of
detection and re-identification in multiple object tracking. International
Journal of Computer Vision pp. 1–19 (2021)

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
Zhang, Z., Cheng, D., Zhu, X., Lin, S., Dai, J.: Integrated object detection
and tracking with tracklet-conditioned detection. arXiv:1811.11167 (2018)

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
Zheng, J., Ma, C., Peng, H., Yang, X.: Learning to track objects from unlabeled
videos. In: ICCV. pp. 13546–13555 (2021)

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
Zhou, W., Chang, S., Sosa, N., Hamann, H., Cox, D.: Lifelong object detection.
arXiv:2009.01129 (2020)

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
Zhou, X., Girdhar, R., Joulin, A., Krähenbühl, P., Misra, I.: Detecting
twenty-thousand classes using image-level supervision. arXiv preprint
arXiv:2201.02605 (2022)

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
Zhou, X., Koltun, V., Krähenbühl, P.: Tracking objects as points. In:
ECCV. pp. 474–490. Springer (2020)

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
Zhou, X., Koltun, V., Krähenbühl, P.: Probabilistic two-stage
detection. arXiv:2103.07461 (2021)

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
Zhou, X., Koltun, V., Krähenbühl, P.: Simple multi-dataset detection.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 7571–7580 (2022)

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
Zhou, X., Yin, T., Koltun, V., Krähenbühl, P.: Global tracking
transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. pp. 8771–8780 (2022)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2212.10146" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2212.10147" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2212.10147">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2212.10147" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2212.10148" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 10:28:19 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
