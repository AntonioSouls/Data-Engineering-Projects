<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2211.15479] Object Detection in Aerial Imagery</title><meta property="og:description" content="Object detection in natural images has achieved remarkable results over the years. However, a similar progress has not yet been observed in aerial object detection due to several challenges, such as high resolution ima‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Object Detection in Aerial Imagery">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Object Detection in Aerial Imagery">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2211.15479">

<!--Generated on Thu Mar 14 05:27:15 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Object Detection in Aerial Imagery</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dmitry Demidov
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Mohamed bin Zayed University of Artificial Intelligence
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Salem AlMarri
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Mohamed bin Zayed University of Artificial Intelligence
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rushali Grandhe
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Mohamed bin Zayed University of Artificial Intelligence
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Object detection in natural images has achieved remarkable results over the years. However, a similar progress has not yet been observed in aerial object detection due to several challenges, such as high resolution images, instances scale variation, class imbalance etc. We show the performance of two-stage, one-stage and attention based object detectors on the iSAID dataset. Furthermore, we describe some modifications and analysis performed for different models - 
<br class="ltx_break"><span id="id1.id1.1" class="ltx_text ltx_font_bold">in two stage detector:</span> introduced weighted attention based FPN, class balanced sampler and density prediction head.
<br class="ltx_break"><span id="id1.id1.2" class="ltx_text ltx_font_bold">in one stage detector:</span> used weighted focal loss and introduced FPN
<br class="ltx_break"><span id="id1.id1.3" class="ltx_text ltx_font_bold">in attention based detector:</span> compare single,multi-scale attention and demonstrate effect of different backbones
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The pre-trained models and video can be found online: <a target="_blank" href="https://mbzuaiac-my.sharepoint.com/:f:/g/personal/dmitry_demidov_mbzuai_ac_ae/EkWESOWeAlZLpme5j_t2Ql8Bcfwn7gkiul_JDgZAJ16OqA?e=ssZjPd" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://mbzuaiac-my.sharepoint.com/:f:/g/personal/dmitry_demidov_mbzuai_ac_ae/EkWESOWeAlZLpme5j_t2Ql8Bcfwn7gkiul_JDgZAJ16OqA?e=ssZjPd</a></span></span></span>.
<br class="ltx_break">Finally, we show a comparative study highlighting the pros and cons of different models in aerial imagery setting.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Object detection is a very popular computer vision task which involves both objects locating and classifying in an image.
Though this task has been extensively studied for usual natural images, there is limited exploration in the field of aerial object detection. Aerial object detection brings various challenges such as large scale variation, uneven distribution of objects, class imbalance etc. which further makes the detection of objects much more demanding. However, the problem is particularly interesting due to its important applications such as defence, forestry, city planning etc.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this project we have performed experiments on the iSAID dataset which is a Large-scale Dataset for Object detection and Instance Segmentation in Aerial Images (iSAID) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. It includes 2,806 high-resolution images taken by satellites, and collects 665,451 densely annotated object instances, belonging to a total of 15 classes.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this work we, first, perform object detection in aerial images using two-stage, one-stage and attention-base detectors. Owing to the significant variation in the model architectures, it is natural that the same modification may not be suitable for all architectures. Hence, we describe modifications and analysis performed in each of the 3 models. <span id="S1.p3.1.1" class="ltx_text ltx_font_bold">For the two-stage model</span>, we have used the Faster-RCNN architecture where we introduced modifications such as weighted attention based FPN to better handle scal variation, simple class balanced sampler to handle the inherent long tail distribution and a density prediction head to tackle the large density variation observed in and among different images. <span id="S1.p3.1.2" class="ltx_text ltx_font_bold">For the one stage model</span>, we have used the YOLOv5 model and tried to analyse the performance with the addition of weighted focal loss to counter class imabalnce and FPN in attempt to deal with various scales. <span id="S1.p3.1.3" class="ltx_text ltx_font_bold">For attention based detector,</span> we have used DETR model. We tried to analyse the effect of different attention levels such as single scale, multi scale attention and the impact of different bakbones such as ResNet50,ResNet101 etc.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">And finally, we demonstrate a comparison between each of the considered detectors with respect to aerial object detection.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The used iSAID dataset and the architectures we experimented with: two-stage, one stage and attention based object detectors, are discussed in the following subsections.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Dataset</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">A Large-scale Dataset for Object Detection and Instance Segmentation in Aerial Images (iSAID) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> is the dataset which was specifically developed for the tasks of object detection, semantic and instance segmentation of aerial images. It includes 2,806 high-resolution images taken by satellites, and collects 665,451 densely annotated object instances, belonging to a total of 15 classes. The iSAID follows the same annotation format used in the popular MS COCO dataset and provides bounding boxes and pixel-level annotations, where each pixel represents a particular class (or absence of any). In order to leverage the limitations of input size of the model used, the original images were cropped into equal overlapping patches with 800x800px resolution.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2211.15479/assets/Images/Dmitry/1_Labels_per_class.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="314" height="167" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.3.2" class="ltx_text" style="font-size:90%;">Number of instances per each class in the train and validation sets of the iSAID dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Notations: ST - storage tank, BD - baseball diamond, TC - tennis court, BC - basketball court, GTF - ground track field, LV - large vehicle, SV - small vehicle, HC - helicopter, RA - roundabout, SBF - soccer ball field, SP - swimming pool.</span></figcaption>
</figure>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The dataset has several distinguishing characteristics, such as large number of images with high spatial resolution, large count of labelled instances per image (which might help in learning contextual information), noticeable scene variation depending on an object type, and imbalanced and uneven distribution of objects. One of the most important key characteristics of the dataset is its huge size and scale variation of the objects, providing the instance size range from 10x10px to 300x300px, which is also the case for instances of the same class. A representation for some of the above-mentioned properties can be seen in the figure <a href="#S2.F1" title="Figure 1 ‚Ä£ 2.1 Dataset ‚Ä£ 2 Methods ‚Ä£ Object Detection in Aerial Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Two stage detector</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Several two stage approaches have been developed for object detection. R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> used selective search algorithm to generate bounding box proposals. Fast R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> is an extension of R-CNN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> which enabled end-to-end training using shared convolutional features, thus improving train and inference speed while also increasing detection accuracy. Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> replaced selective search with Region Proposal Network (RPN) which generates multi-scale and translation-invariant region proposals based on the high-level features of images. It uses RoI pooling to crop the feature maps according to the proposals. This is followed by a small convolution network with classification and regression branch. We have carried out experiments using Faster-RCNN implemented in Detectron2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> framework with ResNet-101 FPN backbone. We introduced some modifications in the Faster-RCNN architecture inorder to address some issues specific to the dataset as explained below.
<br class="ltx_break"><span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_bold">Handling scale variation: </span> Feature pyramid network has been a popular approach used to handle scale variation. It includes a bottom up network and top-down network connected using 1x1 lateral connections. This is followed by a convolutional layer with large kernel to increase receptive field. However, FPN still struggles to deal with huge scale variation. This motivated us to introduce a modification in the FPN so that it can better handle varying size of objects.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">It is well known that small objects are more likely to be identified in the higher resolution layer P5, medium size objects in layer P4/P3 and large objects can be identified from the low resolution layers P2/P3 of FPN. This indicates that having access to more information i.e. from high resolution feature maps could be helpful for better dealing with varying sizes of objects.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.7" class="ltx_p">We transformed this intuition such that information from higher resolution feature maps be transferred to lower resolution feature maps. This information has been incorporated in the form of channel attention over the feature maps by drawing inspiration from works such as Squeeze-Excitation blocks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and CBAM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. However, owing to this change there was an improvement observed in the mAP but it was prominent only for medium and large objects which appears to be logical. Thus, inorder to better deal with the small objects as well, we introduced a small weighting factor for each of the output feature maps as shown in Fig <a href="#S2.F2" title="Figure 2 ‚Ä£ 2.2 Two stage detector ‚Ä£ 2 Methods ‚Ä£ Object Detection in Aerial Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, where <math id="S2.SS2.p3.1.m1.1" class="ltx_Math" alttext="wt_{4}" display="inline"><semantics id="S2.SS2.p3.1.m1.1a"><mrow id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml"><mi id="S2.SS2.p3.1.m1.1.1.2" xref="S2.SS2.p3.1.m1.1.1.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.1.m1.1.1.1" xref="S2.SS2.p3.1.m1.1.1.1.cmml">‚Äã</mo><msub id="S2.SS2.p3.1.m1.1.1.3" xref="S2.SS2.p3.1.m1.1.1.3.cmml"><mi id="S2.SS2.p3.1.m1.1.1.3.2" xref="S2.SS2.p3.1.m1.1.1.3.2.cmml">t</mi><mn id="S2.SS2.p3.1.m1.1.1.3.3" xref="S2.SS2.p3.1.m1.1.1.3.3.cmml">4</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><apply id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1"><times id="S2.SS2.p3.1.m1.1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1.1"></times><ci id="S2.SS2.p3.1.m1.1.1.2.cmml" xref="S2.SS2.p3.1.m1.1.1.2">ùë§</ci><apply id="S2.SS2.p3.1.m1.1.1.3.cmml" xref="S2.SS2.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p3.1.m1.1.1.3.1.cmml" xref="S2.SS2.p3.1.m1.1.1.3">subscript</csymbol><ci id="S2.SS2.p3.1.m1.1.1.3.2.cmml" xref="S2.SS2.p3.1.m1.1.1.3.2">ùë°</ci><cn type="integer" id="S2.SS2.p3.1.m1.1.1.3.3.cmml" xref="S2.SS2.p3.1.m1.1.1.3.3">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">wt_{4}</annotation></semantics></math> <math id="S2.SS2.p3.2.m2.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S2.SS2.p3.2.m2.1a"><mo id="S2.SS2.p3.2.m2.1.1" xref="S2.SS2.p3.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.2.m2.1b"><gt id="S2.SS2.p3.2.m2.1.1.cmml" xref="S2.SS2.p3.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.2.m2.1c">&gt;</annotation></semantics></math> <math id="S2.SS2.p3.3.m3.1" class="ltx_Math" alttext="wt_{3}" display="inline"><semantics id="S2.SS2.p3.3.m3.1a"><mrow id="S2.SS2.p3.3.m3.1.1" xref="S2.SS2.p3.3.m3.1.1.cmml"><mi id="S2.SS2.p3.3.m3.1.1.2" xref="S2.SS2.p3.3.m3.1.1.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.3.m3.1.1.1" xref="S2.SS2.p3.3.m3.1.1.1.cmml">‚Äã</mo><msub id="S2.SS2.p3.3.m3.1.1.3" xref="S2.SS2.p3.3.m3.1.1.3.cmml"><mi id="S2.SS2.p3.3.m3.1.1.3.2" xref="S2.SS2.p3.3.m3.1.1.3.2.cmml">t</mi><mn id="S2.SS2.p3.3.m3.1.1.3.3" xref="S2.SS2.p3.3.m3.1.1.3.3.cmml">3</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.3.m3.1b"><apply id="S2.SS2.p3.3.m3.1.1.cmml" xref="S2.SS2.p3.3.m3.1.1"><times id="S2.SS2.p3.3.m3.1.1.1.cmml" xref="S2.SS2.p3.3.m3.1.1.1"></times><ci id="S2.SS2.p3.3.m3.1.1.2.cmml" xref="S2.SS2.p3.3.m3.1.1.2">ùë§</ci><apply id="S2.SS2.p3.3.m3.1.1.3.cmml" xref="S2.SS2.p3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p3.3.m3.1.1.3.1.cmml" xref="S2.SS2.p3.3.m3.1.1.3">subscript</csymbol><ci id="S2.SS2.p3.3.m3.1.1.3.2.cmml" xref="S2.SS2.p3.3.m3.1.1.3.2">ùë°</ci><cn type="integer" id="S2.SS2.p3.3.m3.1.1.3.3.cmml" xref="S2.SS2.p3.3.m3.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.3.m3.1c">wt_{3}</annotation></semantics></math> <math id="S2.SS2.p3.4.m4.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S2.SS2.p3.4.m4.1a"><mo id="S2.SS2.p3.4.m4.1.1" xref="S2.SS2.p3.4.m4.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.4.m4.1b"><gt id="S2.SS2.p3.4.m4.1.1.cmml" xref="S2.SS2.p3.4.m4.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.4.m4.1c">&gt;</annotation></semantics></math> <math id="S2.SS2.p3.5.m5.1" class="ltx_Math" alttext="wt_{2}" display="inline"><semantics id="S2.SS2.p3.5.m5.1a"><mrow id="S2.SS2.p3.5.m5.1.1" xref="S2.SS2.p3.5.m5.1.1.cmml"><mi id="S2.SS2.p3.5.m5.1.1.2" xref="S2.SS2.p3.5.m5.1.1.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.5.m5.1.1.1" xref="S2.SS2.p3.5.m5.1.1.1.cmml">‚Äã</mo><msub id="S2.SS2.p3.5.m5.1.1.3" xref="S2.SS2.p3.5.m5.1.1.3.cmml"><mi id="S2.SS2.p3.5.m5.1.1.3.2" xref="S2.SS2.p3.5.m5.1.1.3.2.cmml">t</mi><mn id="S2.SS2.p3.5.m5.1.1.3.3" xref="S2.SS2.p3.5.m5.1.1.3.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.5.m5.1b"><apply id="S2.SS2.p3.5.m5.1.1.cmml" xref="S2.SS2.p3.5.m5.1.1"><times id="S2.SS2.p3.5.m5.1.1.1.cmml" xref="S2.SS2.p3.5.m5.1.1.1"></times><ci id="S2.SS2.p3.5.m5.1.1.2.cmml" xref="S2.SS2.p3.5.m5.1.1.2">ùë§</ci><apply id="S2.SS2.p3.5.m5.1.1.3.cmml" xref="S2.SS2.p3.5.m5.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p3.5.m5.1.1.3.1.cmml" xref="S2.SS2.p3.5.m5.1.1.3">subscript</csymbol><ci id="S2.SS2.p3.5.m5.1.1.3.2.cmml" xref="S2.SS2.p3.5.m5.1.1.3.2">ùë°</ci><cn type="integer" id="S2.SS2.p3.5.m5.1.1.3.3.cmml" xref="S2.SS2.p3.5.m5.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.5.m5.1c">wt_{2}</annotation></semantics></math> <math id="S2.SS2.p3.6.m6.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S2.SS2.p3.6.m6.1a"><mo id="S2.SS2.p3.6.m6.1.1" xref="S2.SS2.p3.6.m6.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.6.m6.1b"><gt id="S2.SS2.p3.6.m6.1.1.cmml" xref="S2.SS2.p3.6.m6.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.6.m6.1c">&gt;</annotation></semantics></math> <math id="S2.SS2.p3.7.m7.1" class="ltx_Math" alttext="wt_{1}" display="inline"><semantics id="S2.SS2.p3.7.m7.1a"><mrow id="S2.SS2.p3.7.m7.1.1" xref="S2.SS2.p3.7.m7.1.1.cmml"><mi id="S2.SS2.p3.7.m7.1.1.2" xref="S2.SS2.p3.7.m7.1.1.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.7.m7.1.1.1" xref="S2.SS2.p3.7.m7.1.1.1.cmml">‚Äã</mo><msub id="S2.SS2.p3.7.m7.1.1.3" xref="S2.SS2.p3.7.m7.1.1.3.cmml"><mi id="S2.SS2.p3.7.m7.1.1.3.2" xref="S2.SS2.p3.7.m7.1.1.3.2.cmml">t</mi><mn id="S2.SS2.p3.7.m7.1.1.3.3" xref="S2.SS2.p3.7.m7.1.1.3.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.7.m7.1b"><apply id="S2.SS2.p3.7.m7.1.1.cmml" xref="S2.SS2.p3.7.m7.1.1"><times id="S2.SS2.p3.7.m7.1.1.1.cmml" xref="S2.SS2.p3.7.m7.1.1.1"></times><ci id="S2.SS2.p3.7.m7.1.1.2.cmml" xref="S2.SS2.p3.7.m7.1.1.2">ùë§</ci><apply id="S2.SS2.p3.7.m7.1.1.3.cmml" xref="S2.SS2.p3.7.m7.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p3.7.m7.1.1.3.1.cmml" xref="S2.SS2.p3.7.m7.1.1.3">subscript</csymbol><ci id="S2.SS2.p3.7.m7.1.1.3.2.cmml" xref="S2.SS2.p3.7.m7.1.1.3.2">ùë°</ci><cn type="integer" id="S2.SS2.p3.7.m7.1.1.3.3.cmml" xref="S2.SS2.p3.7.m7.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.7.m7.1c">wt_{1}</annotation></semantics></math>.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2211.15479/assets/Images/FPN.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="479" height="549" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.3.2" class="ltx_text" style="font-size:90%;">Weighted FPN with channel attention</span></figcaption>
</figure>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p"><span id="S2.SS2.p4.1.1" class="ltx_text ltx_font_bold">Handling class imbalance: </span> In the iSAID dataset, a lot of class imbalance is observed which also been depicted in Fig <a href="#S2.F1" title="Figure 1 ‚Ä£ 2.1 Dataset ‚Ä£ 2 Methods ‚Ä£ Object Detection in Aerial Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. This corresponds to the problem of long tail distribution where some classes have large number of instances and some classes have very few number of instances. This can in turn cause the model to be biased towards the frequent classes and hence perform worse on the rare classes. We tried to address this issue by implementing a class balanced foreground sampler in the RoI head. Based on the statistics of the dataset, we segregated the classes into 3 different groups i.e. frequent, common and rare and adjusted the number of proposals for each group.
<br class="ltx_break"></p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.5" class="ltx_p"><span id="S2.SS2.p5.5.1" class="ltx_text ltx_font_bold">Handling uneven distribution/ modelling context: </span> From the dataset, it can be observed that there is a lot of variation in the distribution of instances within an image as well as across the dataset. This uneven distribution of instances could confuse the model due to the persistent variation. This motivated us to think about modeling the distribution/context about an object instance and introduce the density prediction head in the RPN. The density prediction head was first introduced in the Adapative NMS paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and was used to capture the surrounding density inorder to adaptively adjust the NMS threshold. However, we use the same density prediction head for a different objective.
It stacks the classification and bounding box regression branch feature maps along with the output of a 1x1 convolutional layer. This is followed by a convolutional layer with 5x5 kernel which effectively helps capture considerable information in the surroundings of an object as shown in Fig <a href="#S2.F3" title="Figure 3 ‚Ä£ 2.2 Two stage detector ‚Ä£ 2 Methods ‚Ä£ Object Detection in Aerial Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The corresponding ground truth density is calculated as <math id="S2.SS2.p5.1.m1.1" class="ltx_Math" alttext="d_{i}" display="inline"><semantics id="S2.SS2.p5.1.m1.1a"><msub id="S2.SS2.p5.1.m1.1.1" xref="S2.SS2.p5.1.m1.1.1.cmml"><mi id="S2.SS2.p5.1.m1.1.1.2" xref="S2.SS2.p5.1.m1.1.1.2.cmml">d</mi><mi id="S2.SS2.p5.1.m1.1.1.3" xref="S2.SS2.p5.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.1.m1.1b"><apply id="S2.SS2.p5.1.m1.1.1.cmml" xref="S2.SS2.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p5.1.m1.1.1.1.cmml" xref="S2.SS2.p5.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.p5.1.m1.1.1.2.cmml" xref="S2.SS2.p5.1.m1.1.1.2">ùëë</ci><ci id="S2.SS2.p5.1.m1.1.1.3.cmml" xref="S2.SS2.p5.1.m1.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.1.m1.1c">d_{i}</annotation></semantics></math> := max <math id="S2.SS2.p5.2.m2.1" class="ltx_Math" alttext="b_{j}\epsilon\mathcal{G}" display="inline"><semantics id="S2.SS2.p5.2.m2.1a"><mrow id="S2.SS2.p5.2.m2.1.1" xref="S2.SS2.p5.2.m2.1.1.cmml"><msub id="S2.SS2.p5.2.m2.1.1.2" xref="S2.SS2.p5.2.m2.1.1.2.cmml"><mi id="S2.SS2.p5.2.m2.1.1.2.2" xref="S2.SS2.p5.2.m2.1.1.2.2.cmml">b</mi><mi id="S2.SS2.p5.2.m2.1.1.2.3" xref="S2.SS2.p5.2.m2.1.1.2.3.cmml">j</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS2.p5.2.m2.1.1.1" xref="S2.SS2.p5.2.m2.1.1.1.cmml">‚Äã</mo><mi id="S2.SS2.p5.2.m2.1.1.3" xref="S2.SS2.p5.2.m2.1.1.3.cmml">œµ</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p5.2.m2.1.1.1a" xref="S2.SS2.p5.2.m2.1.1.1.cmml">‚Äã</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p5.2.m2.1.1.4" xref="S2.SS2.p5.2.m2.1.1.4.cmml">ùí¢</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.2.m2.1b"><apply id="S2.SS2.p5.2.m2.1.1.cmml" xref="S2.SS2.p5.2.m2.1.1"><times id="S2.SS2.p5.2.m2.1.1.1.cmml" xref="S2.SS2.p5.2.m2.1.1.1"></times><apply id="S2.SS2.p5.2.m2.1.1.2.cmml" xref="S2.SS2.p5.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.p5.2.m2.1.1.2.1.cmml" xref="S2.SS2.p5.2.m2.1.1.2">subscript</csymbol><ci id="S2.SS2.p5.2.m2.1.1.2.2.cmml" xref="S2.SS2.p5.2.m2.1.1.2.2">ùëè</ci><ci id="S2.SS2.p5.2.m2.1.1.2.3.cmml" xref="S2.SS2.p5.2.m2.1.1.2.3">ùëó</ci></apply><ci id="S2.SS2.p5.2.m2.1.1.3.cmml" xref="S2.SS2.p5.2.m2.1.1.3">italic-œµ</ci><ci id="S2.SS2.p5.2.m2.1.1.4.cmml" xref="S2.SS2.p5.2.m2.1.1.4">ùí¢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.2.m2.1c">b_{j}\epsilon\mathcal{G}</annotation></semantics></math> ,i<math id="S2.SS2.p5.3.m3.1" class="ltx_Math" alttext="\neq" display="inline"><semantics id="S2.SS2.p5.3.m3.1a"><mo id="S2.SS2.p5.3.m3.1.1" xref="S2.SS2.p5.3.m3.1.1.cmml">‚â†</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.3.m3.1b"><neq id="S2.SS2.p5.3.m3.1.1.cmml" xref="S2.SS2.p5.3.m3.1.1"></neq></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.3.m3.1c">\neq</annotation></semantics></math>j iou(<math id="S2.SS2.p5.4.m4.2" class="ltx_Math" alttext="b_{i},b_{j}" display="inline"><semantics id="S2.SS2.p5.4.m4.2a"><mrow id="S2.SS2.p5.4.m4.2.2.2" xref="S2.SS2.p5.4.m4.2.2.3.cmml"><msub id="S2.SS2.p5.4.m4.1.1.1.1" xref="S2.SS2.p5.4.m4.1.1.1.1.cmml"><mi id="S2.SS2.p5.4.m4.1.1.1.1.2" xref="S2.SS2.p5.4.m4.1.1.1.1.2.cmml">b</mi><mi id="S2.SS2.p5.4.m4.1.1.1.1.3" xref="S2.SS2.p5.4.m4.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.SS2.p5.4.m4.2.2.2.3" xref="S2.SS2.p5.4.m4.2.2.3.cmml">,</mo><msub id="S2.SS2.p5.4.m4.2.2.2.2" xref="S2.SS2.p5.4.m4.2.2.2.2.cmml"><mi id="S2.SS2.p5.4.m4.2.2.2.2.2" xref="S2.SS2.p5.4.m4.2.2.2.2.2.cmml">b</mi><mi id="S2.SS2.p5.4.m4.2.2.2.2.3" xref="S2.SS2.p5.4.m4.2.2.2.2.3.cmml">j</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.4.m4.2b"><list id="S2.SS2.p5.4.m4.2.2.3.cmml" xref="S2.SS2.p5.4.m4.2.2.2"><apply id="S2.SS2.p5.4.m4.1.1.1.1.cmml" xref="S2.SS2.p5.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p5.4.m4.1.1.1.1.1.cmml" xref="S2.SS2.p5.4.m4.1.1.1.1">subscript</csymbol><ci id="S2.SS2.p5.4.m4.1.1.1.1.2.cmml" xref="S2.SS2.p5.4.m4.1.1.1.1.2">ùëè</ci><ci id="S2.SS2.p5.4.m4.1.1.1.1.3.cmml" xref="S2.SS2.p5.4.m4.1.1.1.1.3">ùëñ</ci></apply><apply id="S2.SS2.p5.4.m4.2.2.2.2.cmml" xref="S2.SS2.p5.4.m4.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.p5.4.m4.2.2.2.2.1.cmml" xref="S2.SS2.p5.4.m4.2.2.2.2">subscript</csymbol><ci id="S2.SS2.p5.4.m4.2.2.2.2.2.cmml" xref="S2.SS2.p5.4.m4.2.2.2.2.2">ùëè</ci><ci id="S2.SS2.p5.4.m4.2.2.2.2.3.cmml" xref="S2.SS2.p5.4.m4.2.2.2.2.3">ùëó</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.4.m4.2c">b_{i},b_{j}</annotation></semantics></math>),
where the density of the object i is defined as the max bounding box IoU with other objects in the ground truth set <math id="S2.SS2.p5.5.m5.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S2.SS2.p5.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p5.5.m5.1.1" xref="S2.SS2.p5.5.m5.1.1.cmml">ùí¢</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.5.m5.1b"><ci id="S2.SS2.p5.5.m5.1.1.cmml" xref="S2.SS2.p5.5.m5.1.1">ùí¢</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.5.m5.1c">\mathcal{G}</annotation></semantics></math> and smooth L1 loss is used.
Furthermore, to the best of our knowledge we are the first to employ the density prediction head for aerial imagery.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2211.15479/assets/Images/density.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="168" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.3.2" class="ltx_text" style="font-size:90%;">Density prediction head for the Faster R-CNN</span></figcaption>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>One stage detector</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.4" class="ltx_p">In one-stage detectors, a one-shot configuration is proposed to replace region proposals, such that classification and regression take place immediately on candidate anchor boxes. With this improvement, architecture is simpler and inference time is more suitable for realistic applications. SSD, YOLO and RetinaNet are examples of one-stage detectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.YOLOv5 architecture consists of three parts: (1) Backbone: CSPDarknet, (2) Neck: PANet, and (3) Head: Yolo Layer. The images are first input to CSPDarknet for feature extraction, and then fed to PANet for feature fusion. Finally, Yolo layer outputs detection results (class, score, location, size).
<br class="ltx_break">We introduced some modifications in the YOLOv5 architecture inorder to address some issues specific to the dataset as explained below.
<br class="ltx_break"><span id="S2.SS3.p1.4.1" class="ltx_text ltx_font_bold">Focal Loss :</span> The iSAID dataset consists of multiple object classes which can be grouped based on the number of occurrences per image, certain object classes have higher occurrences rate such as Small vehicles when compared to other classes such as bridge, helicopter, roundabout etc. as shown in figure <a href="#S2.F1" title="Figure 1 ‚Ä£ 2.1 Dataset ‚Ä£ 2 Methods ‚Ä£ Object Detection in Aerial Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. For model training, it is expected to have a specific number of training examples per class, with an overall assumption of equal distribution of data, else the model could be biased. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> a work around for class imbalance is introduced, which is considered as an extension for cross-entropy loss function named as focal loss. There are two adjustable parameters for focal loss, <math id="S2.SS3.p1.1.m1.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S2.SS3.p1.1.m1.1a"><mi id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml">Œ≥</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><ci id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1">ùõæ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">\gamma</annotation></semantics></math> and <math id="S2.SS3.p1.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S2.SS3.p1.2.m2.1a"><mi id="S2.SS3.p1.2.m2.1.1" xref="S2.SS3.p1.2.m2.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.2.m2.1b"><ci id="S2.SS3.p1.2.m2.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.2.m2.1c">\alpha</annotation></semantics></math>. Increasing <math id="S2.SS3.p1.3.m3.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S2.SS3.p1.3.m3.1a"><mi id="S2.SS3.p1.3.m3.1.1" xref="S2.SS3.p1.3.m3.1.1.cmml">Œ≥</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.3.m3.1b"><ci id="S2.SS3.p1.3.m3.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1">ùõæ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.3.m3.1c">\gamma</annotation></semantics></math> value brings model‚Äôs attention towards object class which are difficult to classify, and increasing <math id="S2.SS3.p1.4.m4.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S2.SS3.p1.4.m4.1a"><mi id="S2.SS3.p1.4.m4.1.1" xref="S2.SS3.p1.4.m4.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.4.m4.1b"><ci id="S2.SS3.p1.4.m4.1.1.cmml" xref="S2.SS3.p1.4.m4.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.4.m4.1c">\alpha</annotation></semantics></math> value results in dedicating more weights for object classes with lower annotations.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p"><span id="S2.SS3.p2.1.1" class="ltx_text ltx_font_bold">Feature Pyramid Network :</span>
Motivated by scale-variation and the ability of FPNs to deal with multiple scales, YOLOv5 architecture was further enhanced with a Feature Pyramid Network. After adding FPN, an mAP of 0.422 was observed on the validation set.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Attention-based detector</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Another approach considered in this work is called Deformable DETR (Deformable Transformers for End-to-End Object Detection) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, which is an attention-based detector. This recently proposed method attempts to both eliminate the previous need of manually-designed components in object detection while still demonstrating good performance and efficiency.
As can be seen in the figure <a href="#S2.F4" title="Figure 4 ‚Ä£ 2.4 Attention-based detector ‚Ä£ 2 Methods ‚Ä£ Object Detection in Aerial Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, Deformable DETR combines two popular computer vision techniques, thereby leveraging their advantages in the way that they can be applied for solving drawbacks of each other.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">The first component is a transformer-based object detection model, DETR (End-to-End Object Detection with Transformers) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. This quite recently published method solves the object detection task as a direct set prediction problem. The approach provides a streamlined detection pipeline and effectively removes the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode prior knowledge about the task. The main ingredients of this framework, called DEtection TRansformer or simply DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR identifies the relations of the objects and the global image context in order to directly output the final set of predictions in parallel. According to the provided by the authors code and various experiments, the model is conceptually simple and efficient, unlike the majority of other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimised Faster R-CNN baseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S2.SS4.p3" class="ltx_para">
<p id="S2.SS4.p3.1" class="ltx_p">The second important part, used to build Deformable DETR, is Deformable Convolutional Networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. It was proved by the authors that the convolutional neural networks (CNNs) with original convolutional layers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> are inherently limited to model geometric transformations due to the fixed geometric structures. To solve this limitation by enhancing the transformation modeling capacity of CNNs, a deformable convolution module was introduced. It is based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from target tasks, without additional supervision. The new modules, as authors mentioned, can easily replace their plain counterparts in existing CNNs, and can be easily end-to-end trained by standard back-propagation. The high effectiveness of Deformable CNNs was also validated with extensive experiments on such complex vision tasks as object detection and semantic segmentation.</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2211.15479/assets/Images/Dmitry/1_Deformable.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="442" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S2.F4.3.2" class="ltx_text" style="font-size:90%;">Illustration of the deformable attention module (the aggregation part is not shown) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite></span></figcaption>
</figure>
<div id="S2.SS4.p4" class="ltx_para">
<p id="S2.SS4.p4.1" class="ltx_p">Therefore, the combination of both Deformable convolution, which helps to solve the problem of sparse spatial sampling, and DETR Transformer, which is responsible for relation modeling capability, results in a model that have such advantages as fast convergence, computational feasibility, and memory efficiency.
Moreover, the proposed multi-scale deformable attention architecture attends to only a small set of sampling locations on the feature map pixels, while still having adequate performance, what is presented by the authors as a reasonable replacement for manually-optimised FPN and computationally inefficient full-attention.</p>
</div>
<div id="S2.SS4.p5" class="ltx_para">
<p id="S2.SS4.p5.1" class="ltx_p">Provided in the original paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> results of extensive experiments with Deformable DETR, which was introduced with a goal to mitigate the slow convergence and high complexity issues of DETR, indeed show that on the MS COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> the benchmark demonstrates its superior effectiveness.
The visualisation of results indicates that the proposed combination of these above-mentioned modules in Deformable DETR looks at extreme points of the object to determine its bounding box, which is similar to the observation in DETR. However, Deformable DETR, more concretely, besides attending to left/right and top/bottom boundaries of the object, also attends to pixels inside the object for predicting its category, which is different to original DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
In addition, compared with its predecessor, Deformable DETR achieves better performance with significantly smaller number of training epochs. This effect is also especially noticeable on small objects, what is, at the same time, can be helpful for the considered iSAID dataset.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments and Results</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Faster R-CNN</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.4" class="ltx_p">To further verify the impact of the modifications, we have performed an ablation study and report the results on the iSAID validation set as shown in Fig <a href="#S3.F5" title="Figure 5 ‚Ä£ 3.1 Faster R-CNN ‚Ä£ 3 Experiments and Results ‚Ä£ Object Detection in Aerial Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. It can be observed that incrementally adding each modification resulted in better performance indicated by mAP.
The introduction of weighted FPN with channel attention was able to considerably improve the mAP indicating that using information from higher resolution layers is favorable when dealing with different scale objects. We found that using small weights was beneficial. Emperically, we set <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="wt_{1}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">‚Äã</mo><msub id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.3.2" xref="S3.SS1.p1.1.m1.1.1.3.2.cmml">t</mi><mn id="S3.SS1.p1.1.m1.1.1.3.3" xref="S3.SS1.p1.1.m1.1.1.3.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><times id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></times><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">ùë§</ci><apply id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3.2">ùë°</ci><cn type="integer" id="S3.SS1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">wt_{1}</annotation></semantics></math>=1.5, <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="wt_{2}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.2.m2.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.cmml">‚Äã</mo><msub id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml"><mi id="S3.SS1.p1.2.m2.1.1.3.2" xref="S3.SS1.p1.2.m2.1.1.3.2.cmml">t</mi><mn id="S3.SS1.p1.2.m2.1.1.3.3" xref="S3.SS1.p1.2.m2.1.1.3.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><times id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1"></times><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">ùë§</ci><apply id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.3.1.cmml" xref="S3.SS1.p1.2.m2.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.3.2.cmml" xref="S3.SS1.p1.2.m2.1.1.3.2">ùë°</ci><cn type="integer" id="S3.SS1.p1.2.m2.1.1.3.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">wt_{2}</annotation></semantics></math>=2, <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="wt_{3}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.3.m3.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.cmml">‚Äã</mo><msub id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml"><mi id="S3.SS1.p1.3.m3.1.1.3.2" xref="S3.SS1.p1.3.m3.1.1.3.2.cmml">t</mi><mn id="S3.SS1.p1.3.m3.1.1.3.3" xref="S3.SS1.p1.3.m3.1.1.3.3.cmml">3</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><times id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1"></times><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">ùë§</ci><apply id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.3.1.cmml" xref="S3.SS1.p1.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.3.2.cmml" xref="S3.SS1.p1.3.m3.1.1.3.2">ùë°</ci><cn type="integer" id="S3.SS1.p1.3.m3.1.1.3.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">wt_{3}</annotation></semantics></math>=2.5, <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="wt_{4}" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mrow id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.4.m4.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.cmml">‚Äã</mo><msub id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml"><mi id="S3.SS1.p1.4.m4.1.1.3.2" xref="S3.SS1.p1.4.m4.1.1.3.2.cmml">t</mi><mn id="S3.SS1.p1.4.m4.1.1.3.3" xref="S3.SS1.p1.4.m4.1.1.3.3.cmml">4</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><times id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1"></times><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">ùë§</ci><apply id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.3.1.cmml" xref="S3.SS1.p1.4.m4.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.3.2.cmml" xref="S3.SS1.p1.4.m4.1.1.3.2">ùë°</ci><cn type="integer" id="S3.SS1.p1.4.m4.1.1.3.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3.3">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">wt_{4}</annotation></semantics></math>=3.
<br class="ltx_break">Addition of the class balanced sampler in the RoI head was able to further improve the performance by 0.5 mAP. During training, we tried to ensure that the proposals are selected in a balanced way from each group thus reducing the possibility of bias towards a set of classes. We choose 256 proposals in the RoI head. Usually 25% (64) proposals are considered as foreground. We ensure that 24 proposals are selected from the rare classes and 20 proposals each from the common and frequent classes.
<br class="ltx_break">Introducing the density prediction head also led to increase in mAP, however the effect was more pronounced for small objects. This affirms that having knowledge of the context/ density around an instance can be very useful especially for small objects.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">All experiments have been carried out with the following settings - batch size of 2, learning rate of 0.0025, momentum of 0.9, weight decay of 0.0001 and <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">Œ≥</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">ùõæ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\gamma</annotation></semantics></math> = 0.1 run for 100,000 iterations.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2211.15479/assets/Images/det2_ablation.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="658" height="346" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.3.2" class="ltx_text" style="font-size:90%;">Faster R-CNN modifications ablation study, results on validation set</span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>YOLOv5</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Inorder to work with the dataset, we had to perform some preprocessing steps -</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Preprocessing: COCO JSON to YOLO TXT</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">Datasets dedicated for visual tasks such as object detection or segmentation consist of images and their metadata. The metadata file include label information for each image with annotations of bounding boxes specifying the location of each category found in every image. A widely used format is MS COCO format <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. iSAID metadata is provided in COCO JSON format. However, for YOLO it had to be converted into TXT format using an algorithm. Information related to each image was stored in a separate text file titled with respective image name. For generating an output json file, a Javascript code was used, which reads each image‚Äôs name in the directory and replaces it with entry id number.
</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Preprocessing: Background Images Reduction</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">In YOLO, unlabelled images are used as background images during training to reduce false positives and balance out the weights. This is unlike other models which neglect such images. An experiment was carried out to identify what percentage of background images would yield the highest accuracy and shortest training time. In <a href="#S3.T1" title="Table 1 ‚Ä£ 3.2.2 Preprocessing: Background Images Reduction ‚Ä£ 3.2 YOLOv5 ‚Ä£ 3 Experiments and Results ‚Ä£ Object Detection in Aerial Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the number of labelled/unlabelled images in training/validation sets are shown as per given iSAID dataset. When YOLOv5L was trained using a resolution of 640, batch size of 16, one epoch at full scale required 31 minutes, but when background images were reduced to 0, training only on labelled images, it took 6.24 minutes per epoch. Moreover, in table <a href="#S3.T2" title="Table 2 ‚Ä£ 3.2.2 Preprocessing: Background Images Reduction ‚Ä£ 3.2 YOLOv5 ‚Ä£ 3 Experiments and Results ‚Ä£ Object Detection in Aerial Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, three models were trained at full-scale, labelled + 10% unlabelled and labelled images only, where the latter achieved highest accuracy and fastest training time.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.2.1.1" class="ltx_tr">
<th id="S3.T1.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T1.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">iSAID</span></th>
<th id="S3.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T1.2.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Unlabelled</span></th>
<th id="S3.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T1.2.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Labelled</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.2.2.1" class="ltx_tr">
<th id="S3.T1.2.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T1.2.2.1.1.1" class="ltx_text" style="font-size:80%;">Training Set</span></th>
<td id="S3.T1.2.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.2.2.1.2.1" class="ltx_text" style="font-size:80%;">67703</span></td>
<td id="S3.T1.2.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.2.2.1.3.1" class="ltx_text" style="font-size:80%;">16384</span></td>
</tr>
<tr id="S3.T1.2.3.2" class="ltx_tr">
<th id="S3.T1.2.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T1.2.3.2.1.1" class="ltx_text" style="font-size:80%;">Validation Set</span></th>
<td id="S3.T1.2.3.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T1.2.3.2.2.1" class="ltx_text" style="font-size:80%;">22487</span></td>
<td id="S3.T1.2.3.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T1.2.3.2.3.1" class="ltx_text" style="font-size:80%;">6049</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.5.1.1" class="ltx_text" style="font-size:113%;">Table 1</span>: </span><span id="S3.T1.6.2" class="ltx_text" style="font-size:113%;">Comparison Table between labelled and unlabelled images</span></figcaption>
</figure>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.2.1.1" class="ltx_tr">
<th id="S3.T2.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T2.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Images</span></th>
<th id="S3.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T2.2.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Training time</span></th>
<th id="S3.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T2.2.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Epochs</span></th>
<th id="S3.T2.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T2.2.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">mAP@.5:.95</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.2.2.1" class="ltx_tr">
<td id="S3.T2.2.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T2.2.2.1.1.1" class="ltx_text" style="font-size:80%;">84087</span></td>
<td id="S3.T2.2.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.2.2.1.2.1" class="ltx_text" style="font-size:80%;">179m</span></td>
<td id="S3.T2.2.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.2.2.1.3.1" class="ltx_text" style="font-size:80%;">5</span></td>
<td id="S3.T2.2.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.2.2.1.4.1" class="ltx_text" style="font-size:80%;">0.3314</span></td>
</tr>
<tr id="S3.T2.2.3.2" class="ltx_tr">
<td id="S3.T2.2.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T2.2.3.2.1.1" class="ltx_text" style="font-size:80%;">18018</span></td>
<td id="S3.T2.2.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.2.3.2.2.1" class="ltx_text" style="font-size:80%;">43m</span></td>
<td id="S3.T2.2.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.2.3.2.3.1" class="ltx_text" style="font-size:80%;">5</span></td>
<td id="S3.T2.2.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.2.3.2.4.1" class="ltx_text" style="font-size:80%;">0.3448</span></td>
</tr>
<tr id="S3.T2.2.4.3" class="ltx_tr">
<td id="S3.T2.2.4.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T2.2.4.3.1.1" class="ltx_text" style="font-size:80%;">16384</span></td>
<td id="S3.T2.2.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T2.2.4.3.2.1" class="ltx_text" style="font-size:80%;">40m</span></td>
<td id="S3.T2.2.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T2.2.4.3.3.1" class="ltx_text" style="font-size:80%;">5</span></td>
<td id="S3.T2.2.4.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T2.2.4.3.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.3811</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.5.1.1" class="ltx_text" style="font-size:113%;">Table 2</span>: </span><span id="S3.T2.6.2" class="ltx_text" style="font-size:113%;">Model performance comparison based on background image percentage.</span></figcaption>
</figure>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.2" class="ltx_p">We tried playing with <math id="S3.SS2.SSS2.p2.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS2.SSS2.p2.1.m1.1a"><mi id="S3.SS2.SSS2.p2.1.m1.1.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.1.m1.1b"><ci id="S3.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.1.m1.1c">\alpha</annotation></semantics></math> and <math id="S3.SS2.SSS2.p2.2.m2.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S3.SS2.SSS2.p2.2.m2.1a"><mi id="S3.SS2.SSS2.p2.2.m2.1.1" xref="S3.SS2.SSS2.p2.2.m2.1.1.cmml">Œ≥</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.2.m2.1b"><ci id="S3.SS2.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1">ùõæ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.2.m2.1c">\gamma</annotation></semantics></math> parameters in the focal loss function. The bests results can be seen in Table <a href="#S3.T3" title="Table 3 ‚Ä£ 3.2.2 Preprocessing: Background Images Reduction ‚Ä£ 3.2 YOLOv5 ‚Ä£ 3 Experiments and Results ‚Ä£ Object Detection in Aerial Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<table id="S3.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.2.2" class="ltx_tr">
<th id="S3.T3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><math id="S3.T3.1.1.1.m1.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S3.T3.1.1.1.m1.1a"><mi mathsize="80%" id="S3.T3.1.1.1.m1.1.1" xref="S3.T3.1.1.1.m1.1.1.cmml">Œ≥</mi><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.m1.1b"><ci id="S3.T3.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.m1.1.1">ùõæ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.m1.1c">\gamma</annotation></semantics></math></th>
<th id="S3.T3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"><math id="S3.T3.2.2.2.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.T3.2.2.2.m1.1a"><mi mathsize="80%" id="S3.T3.2.2.2.m1.1.1" xref="S3.T3.2.2.2.m1.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.2.m1.1b"><ci id="S3.T3.2.2.2.m1.1.1.cmml" xref="S3.T3.2.2.2.m1.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.2.m1.1c">\alpha</annotation></semantics></math></th>
<th id="S3.T3.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T3.2.2.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">mAP@.5</span></th>
<th id="S3.T3.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T3.2.2.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">mAP@.5:.95</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.2.3.1" class="ltx_tr">
<th id="S3.T3.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T3.2.3.1.1.1" class="ltx_text" style="font-size:80%;">1.5</span></th>
<th id="S3.T3.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T3.2.3.1.2.1" class="ltx_text" style="font-size:80%;">0.25</span></th>
<td id="S3.T3.2.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.2.3.1.3.1" class="ltx_text" style="font-size:80%;">0.685</span></td>
<td id="S3.T3.2.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.2.3.1.4.1" class="ltx_text" style="font-size:80%;">0.466</span></td>
</tr>
<tr id="S3.T3.2.4.2" class="ltx_tr">
<th id="S3.T3.2.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T3.2.4.2.1.1" class="ltx_text" style="font-size:80%;">2.0</span></th>
<th id="S3.T3.2.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T3.2.4.2.2.1" class="ltx_text" style="font-size:80%;">0.25</span></th>
<td id="S3.T3.2.4.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T3.2.4.2.3.1" class="ltx_text" style="font-size:80%;">0.689</span></td>
<td id="S3.T3.2.4.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T3.2.4.2.4.1" class="ltx_text" style="font-size:80%;">0.469</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S3.T3.6.1.1" class="ltx_text" style="font-size:113%;">Table 3</span>: </span><span id="S3.T3.7.2" class="ltx_text" style="font-size:113%;">Focal loss</span></figcaption>
</figure>
<div id="S3.SS2.SSS2.p3" class="ltx_para">
<p id="S3.SS2.SSS2.p3.1" class="ltx_p">YOLOv5 Large architecture (normal), focal loss modified, and FPN modified models were run on iSAID‚Äôs validation set at resolution of 640, batch size of 16 and epochs of 50 and results are presented in figure <a href="#S3.F9" title="Figure 9 ‚Ä£ 3.3 Deformable DETR ‚Ä£ 3 Experiments and Results ‚Ä£ Object Detection in Aerial Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. In terms of precision and mAP@.5:.95, YOLOv5L scored the highest results when compared to other models. In terms of mAP@.5, the focal loss modified model showed better performance, while FPN modified model scored lowest of all models in all evaluation metrics. The YOLOv5 architecture is highly optimized for object detection tasks and receives consistent updates on uploaded repository.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2211.15479/assets/Images/yolo.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="658" height="347" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S3.F6.3.2" class="ltx_text" style="font-size:90%;">Results for validation set using YOLOv5</span></figcaption>
</figure>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Deformable DETR</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">At the moment, due to the recentness of the detection transformer, it has only been tested mainly on the popular, well-studied, and relatively balanced object detection datasets, such as MS COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, Pascal <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, etc.
In this subsection, we explore the performance of the Deformable DETR and its variations on the iSAID dataset.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">If not mentioned another setup, due to the limited time and available computing resources, the following models were trained with the following hyper-parameters: batch size of 6, 15 epochs, learning rate of 2e-4 for encoder-decoder, learning rate of 2e-5 for a backbone, learning rate decays by 0.5 multiplier at each 7th epoch for encoder-decoder and for fully connected classifiers. On average, training time with these parameters is approximately 18-25 hours, depending on the architecture.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p"><span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_bold">Singe-Scaled attention:</span>
First, as a baseline, we chose a single-scaled Deformable DETR model, described in the original paper. It uses an ImageNet pre-trained ResNet-50 convolutional neural network as a backbone for feature extraction. The extracted features from the last convolutional layer with the size of 7x7x2048 are used as an input for the encoder-decoder transformer.
Even this simplified architecture shows decent performance with the mAP value equal to <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="0.303" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mn id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">0.303</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><cn type="float" id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">0.303</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">0.303</annotation></semantics></math>, which is quite close to the Faster R-CNN baseline model. Taking into account that the model is trained from scratch for only 15 epochs, we suggest that it is able to reach satisfactory results in case of longer training time.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p"><span id="S3.SS3.p4.1.1" class="ltx_text ltx_font_bold">Multi-Scaled attention:</span>
Next, in order to confirm the suggestion, made in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> regarding the performance improvement for small and medium-sized objects, when a multi-scaled attention module is used, we also trained this model from scratch.
The multi-scaled encoder transformer module uses Conv3-5 ordinary layers from a CNN backbone, and also uses a Conv5 layer with a stride of 2 applied. All the number of feature channels are projected to 256 to have the same appropriate input size before being fed to the encoder transformer. Architecture of this module can be found in the figure <a href="#S3.F7" title="Figure 7 ‚Ä£ 3.3 Deformable DETR ‚Ä£ 3 Experiments and Results ‚Ä£ Object Detection in Aerial Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2211.15479/assets/Images/Dmitry/3.2_Multi.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="406" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S3.F7.3.2" class="ltx_text" style="font-size:90%;">Image feature maps, extracted with a CNN backbone are inputs for a multi-scaled attention module (encoder part); the architecture proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite></span></figcaption>
</figure>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">From the obtained results, one can observe that indeed, the mAP for small and medium objects increased by 20 %, comparing to the single-scaled architecture. This can be explained by the fact that iSAID dataset has much more corresponding-sized objects, rather than larger ones.
Since this architecture uses considerably more memory to store image features, the batch size value was decreased to 4.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.1" class="ltx_p"><span id="S3.SS3.p6.1.1" class="ltx_text ltx_font_bold">ResNet-101 Backbone:</span>
Another improvement we experimented with is changing the CNN backbone to a larger one, ResNet-101. Although it was not noticeably useful for the usual datasets as COCO and ImageNet where the medium objects are typically still covering significant part of the image frame, we assume that for the iSAID dataset, having larger image resolution, this may help to identify medium instances better. We also expect a certain minor performance increase for the small and large objects, but since the architecture of ResNet-50 and ResNet-101 differ only with the Conv3 layer depth, we assume that the effect should be mostly directed at the medium ones.</p>
</div>
<div id="S3.SS3.p7" class="ltx_para">
<p id="S3.SS3.p7.1" class="ltx_p">From our experimental observations, this assumption actually takes place, since in case of a single-scale version the accuracy indeed increased by 20 % for the middle-sized instances, and only by 7-9 % for the rest of them. In case of the multi-scale attention architecture, the increasing is 10 % for medium objects, and nearly 3 % for the rest.</p>
</div>
<div id="S3.SS3.p8" class="ltx_para">
<p id="S3.SS3.p8.1" class="ltx_p"><span id="S3.SS3.p8.1.1" class="ltx_text ltx_font_bold">Early backbone layers:</span>
As the next modification we decided to use earlier layers of the CNN backbone. We suggest that implementation of this idea may increase the performance for tiny, small and medium objects. This becomes obvious, that after resizing and passing through the first layers, receptive field of the backbone is for sure smaller, than on the latter layers. Therefore, we suggest that by the moment of reaching the Conv3 layer, the instances with the original resolution smaller than <math id="S3.SS3.p8.1.m1.1" class="ltx_Math" alttext="22x22" display="inline"><semantics id="S3.SS3.p8.1.m1.1a"><mrow id="S3.SS3.p8.1.m1.1.1" xref="S3.SS3.p8.1.m1.1.1.cmml"><mn id="S3.SS3.p8.1.m1.1.1.2" xref="S3.SS3.p8.1.m1.1.1.2.cmml">22</mn><mo lspace="0em" rspace="0em" id="S3.SS3.p8.1.m1.1.1.1" xref="S3.SS3.p8.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p8.1.m1.1.1.3" xref="S3.SS3.p8.1.m1.1.1.3.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p8.1.m1.1.1.1a" xref="S3.SS3.p8.1.m1.1.1.1.cmml">‚Äã</mo><mn id="S3.SS3.p8.1.m1.1.1.4" xref="S3.SS3.p8.1.m1.1.1.4.cmml">22</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p8.1.m1.1b"><apply id="S3.SS3.p8.1.m1.1.1.cmml" xref="S3.SS3.p8.1.m1.1.1"><times id="S3.SS3.p8.1.m1.1.1.1.cmml" xref="S3.SS3.p8.1.m1.1.1.1"></times><cn type="integer" id="S3.SS3.p8.1.m1.1.1.2.cmml" xref="S3.SS3.p8.1.m1.1.1.2">22</cn><ci id="S3.SS3.p8.1.m1.1.1.3.cmml" xref="S3.SS3.p8.1.m1.1.1.3">ùë•</ci><cn type="integer" id="S3.SS3.p8.1.m1.1.1.4.cmml" xref="S3.SS3.p8.1.m1.1.1.4">22</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p8.1.m1.1c">22x22</annotation></semantics></math> are almost impossible to classify, since they become a nearly 1x1 feature pixel by that layer. This might also explain the fact that the accuracy for small objects is relatively small in case of the considered ordinary Deformable DETR approach.</p>
</div>
<figure id="S3.F8" class="ltx_figure"><img src="/html/2211.15479/assets/Images/Dmitry/3.3_1layer.png" id="S3.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="321" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S3.F8.3.2" class="ltx_text" style="font-size:90%;">Modified encoder input architecture, connected with the backbone. The Conv2-5 convolutional layers, which outputs are projected to 256 channels, are then used as inputs for the encoder part of multi-scaled attention module.</span></figcaption>
</figure>
<div id="S3.SS3.p9" class="ltx_para">
<p id="S3.SS3.p9.1" class="ltx_p">Modified encoder input architecture, shown in the figure <a href="#S3.F8" title="Figure 8 ‚Ä£ 3.3 Deformable DETR ‚Ä£ 3 Experiments and Results ‚Ä£ Object Detection in Aerial Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, now includes the Conv2-5 backbone layers, omitting the last Conv5 layer with a stride of 2 applied previously. Since the size of added Conv2 layer is significantly larger than the size of the removed layer, the resulted model requires for more GPU memory during the training process. Having limited computing resources, we decided to decrease the batch size to 2, which significantly affected training time, increasing it from 0.8 of an hour to 3 hours per epoch.
<br class="ltx_break"></p>
</div>
<div id="S3.SS3.p10" class="ltx_para">
<p id="S3.SS3.p10.1" class="ltx_p"><span id="S3.SS3.p10.1.1" class="ltx_text ltx_font_bold">Optimised model:</span>
Finally, a model with the performance we were able to achieve is a combination of previously mentioned modifications to Deformable DETR. It includes Multi-scale attention module, ResNet-101 as a CNN backbone with earlier layers taken.
More specifically, the model was trained with these hyper-parameters: batch size of 6, 80 epochs, 2e-4 encoder-decoder learning rate, 2e-5 backbone learning rate, learning rate decay by 0.5 at each 7th epoch, and the loss function coefficients optimised for fine-tuning (decreased by 50 %).</p>
</div>
<figure id="S3.F9" class="ltx_figure"><img src="/html/2211.15479/assets/Images/Dmitry/3.5_Final.png" id="S3.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="406" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S3.F9.3.2" class="ltx_text" style="font-size:90%;">Results on the validation set using different modifications of Deformable DETR. The results for models with denoted as ‚Äô-1 layer‚Äô are expected extrapolated results after training for 30 epochs. Calculation is based on the thorough observations of similar fully-trained for 80 epochs models.</span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusions</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">After investigating of the conducted experiments with both original and modified architectures and summarising different performance metrics observed for all three considered approaches, we can provide a model-based conclusion for each of them, mentioning their successes and fails.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.2" class="ltx_p">Starting with Faster R-CNN we noticed that the architecture shows good precision and recall values on small objects and outstanding results on medium-sized [<math id="S4.p2.1.m1.2" class="ltx_Math" alttext="P_{m}:0.476,R_{m}:0.684" display="inline"><semantics id="S4.p2.1.m1.2a"><mrow id="S4.p2.1.m1.2.2" xref="S4.p2.1.m1.2.2.cmml"><msub id="S4.p2.1.m1.2.2.3" xref="S4.p2.1.m1.2.2.3.cmml"><mi id="S4.p2.1.m1.2.2.3.2" xref="S4.p2.1.m1.2.2.3.2.cmml">P</mi><mi id="S4.p2.1.m1.2.2.3.3" xref="S4.p2.1.m1.2.2.3.3.cmml">m</mi></msub><mo lspace="0.278em" rspace="0.278em" id="S4.p2.1.m1.2.2.4" xref="S4.p2.1.m1.2.2.4.cmml">:</mo><mrow id="S4.p2.1.m1.2.2.1.1" xref="S4.p2.1.m1.2.2.1.2.cmml"><mn id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">0.476</mn><mo id="S4.p2.1.m1.2.2.1.1.2" xref="S4.p2.1.m1.2.2.1.2.cmml">,</mo><msub id="S4.p2.1.m1.2.2.1.1.1" xref="S4.p2.1.m1.2.2.1.1.1.cmml"><mi id="S4.p2.1.m1.2.2.1.1.1.2" xref="S4.p2.1.m1.2.2.1.1.1.2.cmml">R</mi><mi id="S4.p2.1.m1.2.2.1.1.1.3" xref="S4.p2.1.m1.2.2.1.1.1.3.cmml">m</mi></msub></mrow><mo lspace="0.278em" rspace="0.278em" id="S4.p2.1.m1.2.2.5" xref="S4.p2.1.m1.2.2.5.cmml">:</mo><mn id="S4.p2.1.m1.2.2.6" xref="S4.p2.1.m1.2.2.6.cmml">0.684</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.2b"><apply id="S4.p2.1.m1.2.2.cmml" xref="S4.p2.1.m1.2.2"><and id="S4.p2.1.m1.2.2a.cmml" xref="S4.p2.1.m1.2.2"></and><apply id="S4.p2.1.m1.2.2b.cmml" xref="S4.p2.1.m1.2.2"><ci id="S4.p2.1.m1.2.2.4.cmml" xref="S4.p2.1.m1.2.2.4">:</ci><apply id="S4.p2.1.m1.2.2.3.cmml" xref="S4.p2.1.m1.2.2.3"><csymbol cd="ambiguous" id="S4.p2.1.m1.2.2.3.1.cmml" xref="S4.p2.1.m1.2.2.3">subscript</csymbol><ci id="S4.p2.1.m1.2.2.3.2.cmml" xref="S4.p2.1.m1.2.2.3.2">ùëÉ</ci><ci id="S4.p2.1.m1.2.2.3.3.cmml" xref="S4.p2.1.m1.2.2.3.3">ùëö</ci></apply><list id="S4.p2.1.m1.2.2.1.2.cmml" xref="S4.p2.1.m1.2.2.1.1"><cn type="float" id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">0.476</cn><apply id="S4.p2.1.m1.2.2.1.1.1.cmml" xref="S4.p2.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.p2.1.m1.2.2.1.1.1.1.cmml" xref="S4.p2.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S4.p2.1.m1.2.2.1.1.1.2.cmml" xref="S4.p2.1.m1.2.2.1.1.1.2">ùëÖ</ci><ci id="S4.p2.1.m1.2.2.1.1.1.3.cmml" xref="S4.p2.1.m1.2.2.1.1.1.3">ùëö</ci></apply></list></apply><apply id="S4.p2.1.m1.2.2c.cmml" xref="S4.p2.1.m1.2.2"><ci id="S4.p2.1.m1.2.2.5.cmml" xref="S4.p2.1.m1.2.2.5">:</ci><share href="#S4.p2.1.m1.2.2.1.cmml" id="S4.p2.1.m1.2.2d.cmml" xref="S4.p2.1.m1.2.2"></share><cn type="float" id="S4.p2.1.m1.2.2.6.cmml" xref="S4.p2.1.m1.2.2.6">0.684</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.2c">P_{m}:0.476,R_{m}:0.684</annotation></semantics></math>] (harbor, storage tank, bridge), however it performs quite bad on large ones [<math id="S4.p2.2.m2.1" class="ltx_Math" alttext="P_{l}:0.19" display="inline"><semantics id="S4.p2.2.m2.1a"><mrow id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml"><msub id="S4.p2.2.m2.1.1.2" xref="S4.p2.2.m2.1.1.2.cmml"><mi id="S4.p2.2.m2.1.1.2.2" xref="S4.p2.2.m2.1.1.2.2.cmml">P</mi><mi id="S4.p2.2.m2.1.1.2.3" xref="S4.p2.2.m2.1.1.2.3.cmml">l</mi></msub><mo lspace="0.278em" rspace="0.278em" id="S4.p2.2.m2.1.1.1" xref="S4.p2.2.m2.1.1.1.cmml">:</mo><mn id="S4.p2.2.m2.1.1.3" xref="S4.p2.2.m2.1.1.3.cmml">0.19</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><apply id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1"><ci id="S4.p2.2.m2.1.1.1.cmml" xref="S4.p2.2.m2.1.1.1">:</ci><apply id="S4.p2.2.m2.1.1.2.cmml" xref="S4.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.p2.2.m2.1.1.2.1.cmml" xref="S4.p2.2.m2.1.1.2">subscript</csymbol><ci id="S4.p2.2.m2.1.1.2.2.cmml" xref="S4.p2.2.m2.1.1.2.2">ùëÉ</ci><ci id="S4.p2.2.m2.1.1.2.3.cmml" xref="S4.p2.2.m2.1.1.2.3">ùëô</ci></apply><cn type="float" id="S4.p2.2.m2.1.1.3.cmml" xref="S4.p2.2.m2.1.1.3">0.19</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">P_{l}:0.19</annotation></semantics></math>].</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.2" class="ltx_p">Next, YOLOv5, in turn, demonstrates the best precision on small objects [<math id="S4.p3.1.m1.1" class="ltx_Math" alttext="P_{s}:0.46" display="inline"><semantics id="S4.p3.1.m1.1a"><mrow id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml"><msub id="S4.p3.1.m1.1.1.2" xref="S4.p3.1.m1.1.1.2.cmml"><mi id="S4.p3.1.m1.1.1.2.2" xref="S4.p3.1.m1.1.1.2.2.cmml">P</mi><mi id="S4.p3.1.m1.1.1.2.3" xref="S4.p3.1.m1.1.1.2.3.cmml">s</mi></msub><mo lspace="0.278em" rspace="0.278em" id="S4.p3.1.m1.1.1.1" xref="S4.p3.1.m1.1.1.1.cmml">:</mo><mn id="S4.p3.1.m1.1.1.3" xref="S4.p3.1.m1.1.1.3.cmml">0.46</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><apply id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1"><ci id="S4.p3.1.m1.1.1.1.cmml" xref="S4.p3.1.m1.1.1.1">:</ci><apply id="S4.p3.1.m1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.p3.1.m1.1.1.2.1.cmml" xref="S4.p3.1.m1.1.1.2">subscript</csymbol><ci id="S4.p3.1.m1.1.1.2.2.cmml" xref="S4.p3.1.m1.1.1.2.2">ùëÉ</ci><ci id="S4.p3.1.m1.1.1.2.3.cmml" xref="S4.p3.1.m1.1.1.2.3">ùë†</ci></apply><cn type="float" id="S4.p3.1.m1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.3">0.46</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">P_{s}:0.46</annotation></semantics></math>] (helicopter, large vehicle, baseball diamond) and good values for the small ones, but unacceptably low precision and recall for the large ones [<math id="S4.p3.2.m2.2" class="ltx_Math" alttext="P_{l}:0.13,R_{l}:0.25" display="inline"><semantics id="S4.p3.2.m2.2a"><mrow id="S4.p3.2.m2.2.2" xref="S4.p3.2.m2.2.2.cmml"><msub id="S4.p3.2.m2.2.2.3" xref="S4.p3.2.m2.2.2.3.cmml"><mi id="S4.p3.2.m2.2.2.3.2" xref="S4.p3.2.m2.2.2.3.2.cmml">P</mi><mi id="S4.p3.2.m2.2.2.3.3" xref="S4.p3.2.m2.2.2.3.3.cmml">l</mi></msub><mo lspace="0.278em" rspace="0.278em" id="S4.p3.2.m2.2.2.4" xref="S4.p3.2.m2.2.2.4.cmml">:</mo><mrow id="S4.p3.2.m2.2.2.1.1" xref="S4.p3.2.m2.2.2.1.2.cmml"><mn id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml">0.13</mn><mo id="S4.p3.2.m2.2.2.1.1.2" xref="S4.p3.2.m2.2.2.1.2.cmml">,</mo><msub id="S4.p3.2.m2.2.2.1.1.1" xref="S4.p3.2.m2.2.2.1.1.1.cmml"><mi id="S4.p3.2.m2.2.2.1.1.1.2" xref="S4.p3.2.m2.2.2.1.1.1.2.cmml">R</mi><mi id="S4.p3.2.m2.2.2.1.1.1.3" xref="S4.p3.2.m2.2.2.1.1.1.3.cmml">l</mi></msub></mrow><mo lspace="0.278em" rspace="0.278em" id="S4.p3.2.m2.2.2.5" xref="S4.p3.2.m2.2.2.5.cmml">:</mo><mn id="S4.p3.2.m2.2.2.6" xref="S4.p3.2.m2.2.2.6.cmml">0.25</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.2b"><apply id="S4.p3.2.m2.2.2.cmml" xref="S4.p3.2.m2.2.2"><and id="S4.p3.2.m2.2.2a.cmml" xref="S4.p3.2.m2.2.2"></and><apply id="S4.p3.2.m2.2.2b.cmml" xref="S4.p3.2.m2.2.2"><ci id="S4.p3.2.m2.2.2.4.cmml" xref="S4.p3.2.m2.2.2.4">:</ci><apply id="S4.p3.2.m2.2.2.3.cmml" xref="S4.p3.2.m2.2.2.3"><csymbol cd="ambiguous" id="S4.p3.2.m2.2.2.3.1.cmml" xref="S4.p3.2.m2.2.2.3">subscript</csymbol><ci id="S4.p3.2.m2.2.2.3.2.cmml" xref="S4.p3.2.m2.2.2.3.2">ùëÉ</ci><ci id="S4.p3.2.m2.2.2.3.3.cmml" xref="S4.p3.2.m2.2.2.3.3">ùëô</ci></apply><list id="S4.p3.2.m2.2.2.1.2.cmml" xref="S4.p3.2.m2.2.2.1.1"><cn type="float" id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1">0.13</cn><apply id="S4.p3.2.m2.2.2.1.1.1.cmml" xref="S4.p3.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.p3.2.m2.2.2.1.1.1.1.cmml" xref="S4.p3.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S4.p3.2.m2.2.2.1.1.1.2.cmml" xref="S4.p3.2.m2.2.2.1.1.1.2">ùëÖ</ci><ci id="S4.p3.2.m2.2.2.1.1.1.3.cmml" xref="S4.p3.2.m2.2.2.1.1.1.3">ùëô</ci></apply></list></apply><apply id="S4.p3.2.m2.2.2c.cmml" xref="S4.p3.2.m2.2.2"><ci id="S4.p3.2.m2.2.2.5.cmml" xref="S4.p3.2.m2.2.2.5">:</ci><share href="#S4.p3.2.m2.2.2.1.cmml" id="S4.p3.2.m2.2.2d.cmml" xref="S4.p3.2.m2.2.2"></share><cn type="float" id="S4.p3.2.m2.2.2.6.cmml" xref="S4.p3.2.m2.2.2.6">0.25</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.2c">P_{l}:0.13,R_{l}:0.25</annotation></semantics></math>].</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.2" class="ltx_p">And finally, attention-based Deformable DETR, shows the superior performance indicators for the large objects [<math id="S4.p4.1.m1.2" class="ltx_Math" alttext="P_{l}:0.476,R_{l}:0.684" display="inline"><semantics id="S4.p4.1.m1.2a"><mrow id="S4.p4.1.m1.2.2" xref="S4.p4.1.m1.2.2.cmml"><msub id="S4.p4.1.m1.2.2.3" xref="S4.p4.1.m1.2.2.3.cmml"><mi id="S4.p4.1.m1.2.2.3.2" xref="S4.p4.1.m1.2.2.3.2.cmml">P</mi><mi id="S4.p4.1.m1.2.2.3.3" xref="S4.p4.1.m1.2.2.3.3.cmml">l</mi></msub><mo lspace="0.278em" rspace="0.278em" id="S4.p4.1.m1.2.2.4" xref="S4.p4.1.m1.2.2.4.cmml">:</mo><mrow id="S4.p4.1.m1.2.2.1.1" xref="S4.p4.1.m1.2.2.1.2.cmml"><mn id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml">0.476</mn><mo id="S4.p4.1.m1.2.2.1.1.2" xref="S4.p4.1.m1.2.2.1.2.cmml">,</mo><msub id="S4.p4.1.m1.2.2.1.1.1" xref="S4.p4.1.m1.2.2.1.1.1.cmml"><mi id="S4.p4.1.m1.2.2.1.1.1.2" xref="S4.p4.1.m1.2.2.1.1.1.2.cmml">R</mi><mi id="S4.p4.1.m1.2.2.1.1.1.3" xref="S4.p4.1.m1.2.2.1.1.1.3.cmml">l</mi></msub></mrow><mo lspace="0.278em" rspace="0.278em" id="S4.p4.1.m1.2.2.5" xref="S4.p4.1.m1.2.2.5.cmml">:</mo><mn id="S4.p4.1.m1.2.2.6" xref="S4.p4.1.m1.2.2.6.cmml">0.684</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.2b"><apply id="S4.p4.1.m1.2.2.cmml" xref="S4.p4.1.m1.2.2"><and id="S4.p4.1.m1.2.2a.cmml" xref="S4.p4.1.m1.2.2"></and><apply id="S4.p4.1.m1.2.2b.cmml" xref="S4.p4.1.m1.2.2"><ci id="S4.p4.1.m1.2.2.4.cmml" xref="S4.p4.1.m1.2.2.4">:</ci><apply id="S4.p4.1.m1.2.2.3.cmml" xref="S4.p4.1.m1.2.2.3"><csymbol cd="ambiguous" id="S4.p4.1.m1.2.2.3.1.cmml" xref="S4.p4.1.m1.2.2.3">subscript</csymbol><ci id="S4.p4.1.m1.2.2.3.2.cmml" xref="S4.p4.1.m1.2.2.3.2">ùëÉ</ci><ci id="S4.p4.1.m1.2.2.3.3.cmml" xref="S4.p4.1.m1.2.2.3.3">ùëô</ci></apply><list id="S4.p4.1.m1.2.2.1.2.cmml" xref="S4.p4.1.m1.2.2.1.1"><cn type="float" id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1">0.476</cn><apply id="S4.p4.1.m1.2.2.1.1.1.cmml" xref="S4.p4.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.p4.1.m1.2.2.1.1.1.1.cmml" xref="S4.p4.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S4.p4.1.m1.2.2.1.1.1.2.cmml" xref="S4.p4.1.m1.2.2.1.1.1.2">ùëÖ</ci><ci id="S4.p4.1.m1.2.2.1.1.1.3.cmml" xref="S4.p4.1.m1.2.2.1.1.1.3">ùëô</ci></apply></list></apply><apply id="S4.p4.1.m1.2.2c.cmml" xref="S4.p4.1.m1.2.2"><ci id="S4.p4.1.m1.2.2.5.cmml" xref="S4.p4.1.m1.2.2.5">:</ci><share href="#S4.p4.1.m1.2.2.1.cmml" id="S4.p4.1.m1.2.2d.cmml" xref="S4.p4.1.m1.2.2"></share><cn type="float" id="S4.p4.1.m1.2.2.6.cmml" xref="S4.p4.1.m1.2.2.6">0.684</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.2c">P_{l}:0.476,R_{l}:0.684</annotation></semantics></math>] and good values for the medium-sized, however performing worse on the small ones [<math id="S4.p4.2.m2.2" class="ltx_Math" alttext="P_{s}:0.22,R_{s}:0.32" display="inline"><semantics id="S4.p4.2.m2.2a"><mrow id="S4.p4.2.m2.2.2" xref="S4.p4.2.m2.2.2.cmml"><msub id="S4.p4.2.m2.2.2.3" xref="S4.p4.2.m2.2.2.3.cmml"><mi id="S4.p4.2.m2.2.2.3.2" xref="S4.p4.2.m2.2.2.3.2.cmml">P</mi><mi id="S4.p4.2.m2.2.2.3.3" xref="S4.p4.2.m2.2.2.3.3.cmml">s</mi></msub><mo lspace="0.278em" rspace="0.278em" id="S4.p4.2.m2.2.2.4" xref="S4.p4.2.m2.2.2.4.cmml">:</mo><mrow id="S4.p4.2.m2.2.2.1.1" xref="S4.p4.2.m2.2.2.1.2.cmml"><mn id="S4.p4.2.m2.1.1" xref="S4.p4.2.m2.1.1.cmml">0.22</mn><mo id="S4.p4.2.m2.2.2.1.1.2" xref="S4.p4.2.m2.2.2.1.2.cmml">,</mo><msub id="S4.p4.2.m2.2.2.1.1.1" xref="S4.p4.2.m2.2.2.1.1.1.cmml"><mi id="S4.p4.2.m2.2.2.1.1.1.2" xref="S4.p4.2.m2.2.2.1.1.1.2.cmml">R</mi><mi id="S4.p4.2.m2.2.2.1.1.1.3" xref="S4.p4.2.m2.2.2.1.1.1.3.cmml">s</mi></msub></mrow><mo lspace="0.278em" rspace="0.278em" id="S4.p4.2.m2.2.2.5" xref="S4.p4.2.m2.2.2.5.cmml">:</mo><mn id="S4.p4.2.m2.2.2.6" xref="S4.p4.2.m2.2.2.6.cmml">0.32</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.2.m2.2b"><apply id="S4.p4.2.m2.2.2.cmml" xref="S4.p4.2.m2.2.2"><and id="S4.p4.2.m2.2.2a.cmml" xref="S4.p4.2.m2.2.2"></and><apply id="S4.p4.2.m2.2.2b.cmml" xref="S4.p4.2.m2.2.2"><ci id="S4.p4.2.m2.2.2.4.cmml" xref="S4.p4.2.m2.2.2.4">:</ci><apply id="S4.p4.2.m2.2.2.3.cmml" xref="S4.p4.2.m2.2.2.3"><csymbol cd="ambiguous" id="S4.p4.2.m2.2.2.3.1.cmml" xref="S4.p4.2.m2.2.2.3">subscript</csymbol><ci id="S4.p4.2.m2.2.2.3.2.cmml" xref="S4.p4.2.m2.2.2.3.2">ùëÉ</ci><ci id="S4.p4.2.m2.2.2.3.3.cmml" xref="S4.p4.2.m2.2.2.3.3">ùë†</ci></apply><list id="S4.p4.2.m2.2.2.1.2.cmml" xref="S4.p4.2.m2.2.2.1.1"><cn type="float" id="S4.p4.2.m2.1.1.cmml" xref="S4.p4.2.m2.1.1">0.22</cn><apply id="S4.p4.2.m2.2.2.1.1.1.cmml" xref="S4.p4.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.p4.2.m2.2.2.1.1.1.1.cmml" xref="S4.p4.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S4.p4.2.m2.2.2.1.1.1.2.cmml" xref="S4.p4.2.m2.2.2.1.1.1.2">ùëÖ</ci><ci id="S4.p4.2.m2.2.2.1.1.1.3.cmml" xref="S4.p4.2.m2.2.2.1.1.1.3">ùë†</ci></apply></list></apply><apply id="S4.p4.2.m2.2.2c.cmml" xref="S4.p4.2.m2.2.2"><ci id="S4.p4.2.m2.2.2.5.cmml" xref="S4.p4.2.m2.2.2.5">:</ci><share href="#S4.p4.2.m2.2.2.1.cmml" id="S4.p4.2.m2.2.2d.cmml" xref="S4.p4.2.m2.2.2"></share><cn type="float" id="S4.p4.2.m2.2.2.6.cmml" xref="S4.p4.2.m2.2.2.6">0.32</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.2.m2.2c">P_{s}:0.22,R_{s}:0.32</annotation></semantics></math>].</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">To conclude, the aerial object detection is certainly a challenging task, providing multiple complex questions to solve. Through our experiments, we tried to analyse the strengths and weaknesses of different types of detectors on the aerial images from iSAID dataset. The observed variety of outcomes and performance of models shows that the object detection problem already can be adequately solved with a certain highly dataset-depended solution, but it still does not have a single approach that can provide a general solution for every case.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Y.¬†Bengio and Y.¬†Lecun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Convolutional networks for images, speech, and time-series.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">11 1997.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
N.¬†Carion, F.¬†Massa, G.¬†Synnaeve, N.¬†Usunier, A.¬†Kirillov, and S.¬†Zagoruyko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">End-to-end object detection with transformers, 2020.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
J.¬†Dai, H.¬†Qi, Y.¬†Xiong, Y.¬†Li, G.¬†Zhang, H.¬†Hu, and Y.¬†Wei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Deformable convolutional networks, 2017.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
R.¬†Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Fast r-cnn, 2015.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
R.¬†Girshick, J.¬†Donahue, T.¬†Darrell, and J.¬†Malik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Rich feature hierarchies for accurate object detection and semantic
segmentation, 2014.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
J.¬†Hu, L.¬†Shen, S.¬†Albanie, G.¬†Sun, and E.¬†Wu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Squeeze-and-excitation networks, 2019.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
T.¬†Lin, P.¬†Goyal, R.¬†B. Girshick, K.¬†He, and P.¬†Doll√°r.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Focal loss for dense object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, abs/1708.02002, 2017.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
T.-Y. Lin, P.¬†Goyal, R.¬†Girshick, K.¬†He, and P.¬†Doll√°r.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Focal loss for dense object detection, 2018.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
T.-Y. Lin, M.¬†Maire, S.¬†Belongie, L.¬†Bourdev, R.¬†Girshick, J.¬†Hays, P.¬†Perona,
D.¬†Ramanan, C.¬†L. Zitnick, and P.¬†Doll√°r.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context, 2015.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
T.-Y. Lin, M.¬†Maire, S.¬†Belongie, J.¬†Hays, P.¬†Perona, D.¬†Ramanan,
P.¬†Doll√°r, and C.¬†L. Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In D.¬†Fleet, T.¬†Pajdla, B.¬†Schiele, and T.¬†Tuytelaars, editors, </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision ‚Äì ECCV 2014</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 740‚Äì755, Cham, 2014. Springer
International Publishing.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
S.¬†Liu, D.¬†Huang, and Y.¬†Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Adaptive nms: Refining pedestrian detection in a crowd, 2019.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
W.¬†Liu, D.¬†Anguelov, D.¬†Erhan, C.¬†Szegedy, S.¬†E. Reed, C.¬†Fu, and A.¬†C. Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">SSD: single shot multibox detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, abs/1512.02325, 2015.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
R.¬†Mottaghi, X.¬†Chen, X.¬†Liu, N.-G. Cho, S.-W. Lee, S.¬†Fidler, R.¬†Urtasun, and
A.¬†Yuille.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">The role of context for object detection and semantic segmentation in
the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
J.¬†Redmon, S.¬†Divvala, R.¬†Girshick, and A.¬†Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">You only look once: Unified, real-time object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, pages 779‚Äì788, 2016.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
S.¬†Ren, K.¬†He, R.¬†Girshick, and J.¬†Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Faster r-cnn: Towards real-time object detection with region proposal
networks, 2016.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
S.¬†Waqas¬†Zamir, A.¬†Arora, A.¬†Gupta, S.¬†Khan, G.¬†Sun, F.¬†Shahbaz¬†Khan, F.¬†Zhu,
L.¬†Shao, G.-S. Xia, and X.¬†Bai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">isaid: A large-scale dataset for instance segmentation in aerial
images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition Workshops</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages 28‚Äì37, 2019.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
S.¬†Woo, J.¬†Park, J.-Y. Lee, and I.¬†S. Kweon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Cbam: Convolutional block attention module, 2018.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Y.¬†Wu, A.¬†Kirillov, F.¬†Massa, W.-Y. Lo, and R.¬†Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Detectron2.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/facebookresearch/detectron2" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/facebookresearch/detectron2</a><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
X.¬†Zhu, W.¬†Su, L.¬†Lu, B.¬†Li, X.¬†Wang, and J.¬†Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Deformable detr: Deformable transformers for end-to-end object
detection, 2021.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2211.15478" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2211.15479" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2211.15479">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2211.15479" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2211.15480" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar 14 05:27:15 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
