<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2309.01574] Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data</title><meta property="og:description" content="Rising maintenance costs of ageing infrastructure necessitate innovative monitoring techniques. This paper presents a new approach for detecting axles, enabling real-time application of Bridge Weigh-In-Motion (BWIM) sy‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2309.01574">

<!--Generated on Wed Feb 28 08:23:37 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_fleqn">
<h1 class="ltx_title ltx_title_document">Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Henrik Riedel
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Steven Robert Lorenzen
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Clemens H√ºbler
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Rising maintenance costs of ageing infrastructure necessitate innovative monitoring techniques. This paper presents a new approach for detecting axles, enabling real-time application of Bridge Weigh-In-Motion (BWIM) systems without dedicated axle detectors. The proposed Virtual Axle Detector with Enhanced Receptive Field (VADER) is independent of bridge type and sensor placement while only using raw acceleration data as input. By using raw data instead of spectograms as input, the receptive field can be enhanced without increasing the number of parameters. We also introduce a novel receptive field (RF) rule for an object-size driven design of Convolutional Neural Network (CNN) architectures. We were able to show, that the RF rule has the potential to bridge the gap between physical boundary conditions and deep learning model development. Based on the RF rule, our results suggest that models using raw data could achieve better performance than those using spectrograms, offering a compelling reason to consider raw data as input. The proposed VADER achieves to detect 99.9¬†% of axles with a spatial error of 4.13¬†cm using only acceleration measurements, while cutting computational and memory costs by 99¬†% compared to the state-of-the-art using spectograms.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
Sound Event Detection , Continuous Wavelet Transformation , Moving Load Localisation , Nothing-on-Road , Free-of-Axle-Detector , Bridge Weigh-In-Motion , Structural Health Monitoring , Field Validation

</div>
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\affiliation</span>
<p id="p1.2" class="ltx_p">[label0]organization=Institute of Structural Mechanics and Design, Department of Civil and Environmental Engineering, Technical University of Darmstadt,addressline=Franziska-Braun-Str. 3,
city=Darmstadt,
postcode=64287,
state=Hesse,
country=Germany</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">As infrastructure ages, novel methods are imperative for efficient monitoring. Precise knowledge of the actual stress and loads on infrastructure enables more accurate determination of remaining service life, identification of overloaded vehicles, and more efficient planning for new constructions. Measuring loads directly is very difficult in many cases. Instead, loads can be reconstructed via the structural response <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Based on this, Weigh-In-Motion (WIM) systems are used to reconstruct the axle loads of trains or cars. WIM systems have the advantage of being able to determine the axle loads in the regular traffic flow. Conventional WIM systems necessitate installation directly into roadways or tracks, often requiring traffic disruption. Conventional WIM systems are therefore expensive and hardly suitable for a nationwide investigation of axle loads. In contrast, Bridge WIM systems (BWIM) are positioned beneath bridge structures, facilitating reuse and repositioning for multiple BWIM measurements <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Thus, the installation of a BWIM system is less expensive and can be used for a multitude of time-limited investigations.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">For large-scale investigations of axle loads, BWIM systems are therefore hardly dispensable. However, effective utilisation of BWIM systems typically demands knowledge of the axle positions <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. For this purpose, additional sensors currently need to be installed at specific locations (facing the roadway, on the cross girders, or near the supports) that depend on the bridge structure <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Only in the case of thin slab bridges can BWIM strain sensors be reliably used for axle detection <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. <cite class="ltx_cite ltx_citemacro_citet">Tan et¬†al. [<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> has proposed a BWIM system that works without axle detectors, but this system is dependent on strain measurements and only works for one vehicle at a time on the bridge. However, for a large scale application of BWIM measurements, a method for axle detection and localisation that is independent of bridge type, measured physical quantities and sensor position would be of great benefit, effectively reducing cost and risk. Therefore, our study aims at an axle detection method using existing sensors of arbitrary BWIM systems. To determine the axle positions, axles are usually detected at at least two locations. With the distance between the locations and the difference in time, the axle position can then be interpolated or extrapolated in time.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The term Sound Event Detection (SED) is predominantly used in the literature for the detection of events in time series <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. One of the tasks of SED is for example speech recognition. Here, events refer to words and audible sound is used as input. However, this methodology can also be applied to other vibration signals as input (such as accelerations in bridges) to find events like a train axle passing a bridge. For SED problems, time series are typically analysed in the form of 2D spectrograms using Convolutional Neural Networks (CNNs) <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> or, more recently, Transformers <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, despite the competitive performance of 1D CNNs <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. Consequently, research often concentrates on identifying the most suitable spectrogram type <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. Notably, in conventional tasks like speech recognition and acoustic scene classification, log mel spectrograms dominate <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, despite some studies showing better or comparable outcomes using raw data <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. In the context of axle localisation, Continuous Wavelet Transformations (CWTs) have demonstrated suitability as features for axle detection <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. Given that CWTs even compete well in speech recognition <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> and log mel spectrograms were tailored for human auditory perception, CWTs were adopted in our earlier work for axle detection <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. However, it is unclear whether CWTs are actually necessary, whether raw measurement data is sufficient as model input, and how this affects the model‚Äôs performance.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In addition, there are only a few guidelines for the hyperparameters and architectures for the development of neural networks and these usually refer to empirical values <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. In order to adapt models for new problems faster and more reliably, we propose in this study a receptive field rule based on object sizes and validate it on the virtual axle detector. The object size in this case refers to the minimum natural frequency of a bridge, but the basic principle should also be transferable to other domains such as sound or images.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this study, we propose a novel approach that enables real-time detection and localisation of axles using arbitrarily placed sensors. For each sensor, the axles are localised at the nearest point of the tracks to the sensor (perpendicular to the direction of travel). The position can also be determined multi-dimensionally using several sensors, making the concept presented here relevant for car bridges as well. Consequently, BWIM systems can now conduct real-time axle detection without the need for additional sensors. The concept is validated on a single track railway bridge. This means that in this study there is only one train at a time on the bridge on a ballasted track. To make it transferable to car bridges, the axles were always considered individually and not as part of a vehicle. The proposed concept determines the longitudinal position of axles using sensors distributed longitudinally. A transversal distribution of additional sensors could therefore determine the position of the axles in two directions. Thus, the approach is theoretically also applicable for multi-lane car bridges. To validate our approach, we adapted the VAD model <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> to handle raw measurement data and performed several tests on the dataset from <cite class="ltx_cite ltx_citemacro_citet">Lorenzen et¬†al. [<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methodology</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we outline the methodology used for the Virtual Axle Detector with Enhanced Receptive field (VADER) and the investigations on the influence of the input data in the form of spectograms or raw measurement data. The used data set <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> is briefly summarized. Finally, the training parameters and evaluation metrics are described.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data Acquisition</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The dataset from <cite class="ltx_cite ltx_citemacro_citet">Lorenzen et¬†al. [<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> was acquired on a single-span steel trough railway bridge situated on a long-distance traffic line in Germany (fig.¬†<a href="#S2.F1" title="Figure 1 ‚Ä£ 2.1 Data Acquisition ‚Ä£ 2 Methodology ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). This bridge is 18.4 meters long with a free span of 16.4 meters and a natural frequency of about 6.9¬†Hz for the first bending mode. The natural frequency is variable with a non-linear dependence on the load due to the ballast <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. Measurement data from ten seismic uniaxial accelerometers with a sampling rate of 600¬†Hz installed along the bridge are used as features.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Utilizing a supervised learning approach, our model requires labels as the ground truth from which to learn. For the creation of the labels, four rosette strain gauges directly installed on the rails (fig.¬†<a href="#S2.F1" title="Figure 1 ‚Ä£ 2.1 Data Acquisition ‚Ä£ 2 Methodology ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) were used to ensure that the accuracy of the assumed ground truth is sufficient to accurately evaluate the performance of the model. The rosette strain gauges were used to determine the axle positions and velocities of the passing trains. This was then used to determine the points in time at which the axles were closest to the individual acceleration sensors (perpendicular to the direction of travel). By using this as a label, the model is trained to use the acceleration sensors as a virtual light barrier that is triggered when a train axle is crossed <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. The label vector contains ones at the time points when a train axle is positioned above the respective sensor, and the rest are zeros (fig.¬†<a href="#S2.F2" title="Figure 2 ‚Ä£ 2.1 Data Acquisition ‚Ä£ 2 Methodology ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). Therefore, the sum of the label vector yields the number of axles of the corresponding train. The model then only receives the acceleration signals of a single sensor at a time and predicts the times at which an axle is located above this sensor. From a number of two acceleration sensors, the axle distances and velocities can thus also be determined. Due to the sampling rate, an inaccuracy of 6-38 cm is assumed for the label creation, depending on the train‚Äôs velocity and the sensor‚Äôs position. The dataset consist out of 3,745 train passages with one label vector per sensor. For more detailed information about the dataset and the measurement campaign, please refer to the paper by <cite class="ltx_cite ltx_citemacro_citet">Lorenzen et¬†al. [<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">To ensure that strain gauge measurements are not necessary for the application, two scenarios for labeling are investigated in this study: a) existing data from a failed axle detector and b) trains with a differential global positioning system (DGPS).</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2309.01574/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="145" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.4.2.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.2.1" class="ltx_text" style="font-size:90%;">Bridge and sensor setup a) side view b) top view with sensor labels, accelerometers <math id="S2.F1.2.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.F1.2.1.m1.1b"><mi id="S2.F1.2.1.m1.1.1" xref="S2.F1.2.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.F1.2.1.m1.1c"><ci id="S2.F1.2.1.m1.1.1.cmml" xref="S2.F1.2.1.m1.1.1">ùë•</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.2.1.m1.1d">x</annotation></semantics></math>-ordinate and strain gauge distances c) cross section. <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite></span></figcaption>
</figure>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2309.01574/assets/images/Label.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="449" height="239" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.3.2" class="ltx_text" style="font-size:90%;">The labels represent the moment when a train axle is located over the respective sensor. Here, as an example, potential label vectors are shown for three fictitious sensors and a train up to the time shown.</span></figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Data Transformation</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.2" class="ltx_p">To investigate the influence of data representation, two cases are considered: a) spectograms and b) raw measurement data. The spectograms are the same as proposed by <cite class="ltx_cite ltx_citemacro_citet">Lorenzen et¬†al. [<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> with three mother wavelets and two sets of 16 frequencies per mother wavelet as input per crossing and sensor (tab.¬†<a href="#S2.T1" title="Table 1 ‚Ä£ 2.2 Data Transformation ‚Ä£ 2 Methodology ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). This results in an input tensor of <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="16\times 6\times n_{\mathrm{s}}" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mrow id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml"><mn id="S2.SS2.p1.1.m1.1.1.2" xref="S2.SS2.p1.1.m1.1.1.2.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.p1.1.m1.1.1.1" xref="S2.SS2.p1.1.m1.1.1.1.cmml">√ó</mo><mn id="S2.SS2.p1.1.m1.1.1.3" xref="S2.SS2.p1.1.m1.1.1.3.cmml">6</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.p1.1.m1.1.1.1a" xref="S2.SS2.p1.1.m1.1.1.1.cmml">√ó</mo><msub id="S2.SS2.p1.1.m1.1.1.4" xref="S2.SS2.p1.1.m1.1.1.4.cmml"><mi id="S2.SS2.p1.1.m1.1.1.4.2" xref="S2.SS2.p1.1.m1.1.1.4.2.cmml">n</mi><mi mathvariant="normal" id="S2.SS2.p1.1.m1.1.1.4.3" xref="S2.SS2.p1.1.m1.1.1.4.3.cmml">s</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1"><times id="S2.SS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S2.SS2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.p1.1.m1.1.1.2">16</cn><cn type="integer" id="S2.SS2.p1.1.m1.1.1.3.cmml" xref="S2.SS2.p1.1.m1.1.1.3">6</cn><apply id="S2.SS2.p1.1.m1.1.1.4.cmml" xref="S2.SS2.p1.1.m1.1.1.4"><csymbol cd="ambiguous" id="S2.SS2.p1.1.m1.1.1.4.1.cmml" xref="S2.SS2.p1.1.m1.1.1.4">subscript</csymbol><ci id="S2.SS2.p1.1.m1.1.1.4.2.cmml" xref="S2.SS2.p1.1.m1.1.1.4.2">ùëõ</ci><ci id="S2.SS2.p1.1.m1.1.1.4.3.cmml" xref="S2.SS2.p1.1.m1.1.1.4.3">s</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">16\times 6\times n_{\mathrm{s}}</annotation></semantics></math> with <math id="S2.SS2.p1.2.m2.1" class="ltx_Math" alttext="n_{\mathrm{s}}" display="inline"><semantics id="S2.SS2.p1.2.m2.1a"><msub id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml"><mi id="S2.SS2.p1.2.m2.1.1.2" xref="S2.SS2.p1.2.m2.1.1.2.cmml">n</mi><mi mathvariant="normal" id="S2.SS2.p1.2.m2.1.1.3" xref="S2.SS2.p1.2.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><apply id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.p1.2.m2.1.1.2.cmml" xref="S2.SS2.p1.2.m2.1.1.2">ùëõ</ci><ci id="S2.SS2.p1.2.m2.1.1.3.cmml" xref="S2.SS2.p1.2.m2.1.1.3">s</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">n_{\mathrm{s}}</annotation></semantics></math> being the number of samples in the acceleration signal of a sensor. The raw measurement data was used unchanged as input and not even scaled or normalized.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S2.T1.2.1" class="ltx_tr">
<td id="S2.T1.2.1.1" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.2.1.2" class="ltx_td ltx_align_center" colspan="2">Scale Limit</td>
</tr>
<tr id="S2.T1.2.2" class="ltx_tr">
<td id="S2.T1.2.2.1" class="ltx_td ltx_align_left ltx_border_r">Wavelet</td>
<td id="S2.T1.2.2.2" class="ltx_td ltx_align_center">Lower</td>
<td id="S2.T1.2.2.3" class="ltx_td ltx_align_center">Upper</td>
</tr>
<tr id="S2.T1.2.3" class="ltx_tr">
<td id="S2.T1.2.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" rowspan="2"><span id="S2.T1.2.3.1.1" class="ltx_text">First Order Complex Gaussian Derivative</span></td>
<td id="S2.T1.2.3.2" class="ltx_td ltx_align_center ltx_border_tt">1</td>
<td id="S2.T1.2.3.3" class="ltx_td ltx_align_center ltx_border_tt">8</td>
</tr>
<tr id="S2.T1.2.4" class="ltx_tr">
<td id="S2.T1.2.4.1" class="ltx_td ltx_align_center">8</td>
<td id="S2.T1.2.4.2" class="ltx_td ltx_align_center">50</td>
</tr>
<tr id="S2.T1.2.5" class="ltx_tr">
<td id="S2.T1.2.5.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="2"><span id="S2.T1.2.5.1.1" class="ltx_text">First Order Gaussian Derivative</span></td>
<td id="S2.T1.2.5.2" class="ltx_td ltx_align_center ltx_border_t">0.6</td>
<td id="S2.T1.2.5.3" class="ltx_td ltx_align_center ltx_border_t">6.5</td>
</tr>
<tr id="S2.T1.2.6" class="ltx_tr">
<td id="S2.T1.2.6.1" class="ltx_td ltx_align_center">6.5</td>
<td id="S2.T1.2.6.2" class="ltx_td ltx_align_center">35</td>
</tr>
<tr id="S2.T1.2.7" class="ltx_tr">
<td id="S2.T1.2.7.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="2"><span id="S2.T1.2.7.1.1" class="ltx_text">Default Frequency B-Spline from <cite class="ltx_cite ltx_citemacro_citet">Lee et¬†al. [<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite></span></td>
<td id="S2.T1.2.7.2" class="ltx_td ltx_align_center ltx_border_t">1.5</td>
<td id="S2.T1.2.7.3" class="ltx_td ltx_align_center ltx_border_t">10</td>
</tr>
<tr id="S2.T1.2.8" class="ltx_tr">
<td id="S2.T1.2.8.1" class="ltx_td ltx_align_center">10</td>
<td id="S2.T1.2.8.2" class="ltx_td ltx_align_center">40</td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S2.T1.4.2" class="ltx_text" style="font-size:90%;">The transformation settings used per passage and per signal. Derived from <cite class="ltx_cite ltx_citemacro_citet">Lorenzen et¬†al. [<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.</span></figcaption>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Data Split</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The training and validation sets were implemented using a five-fold cross-validation approach with a holdout test set. Initially, one-sixth of the data was separated for the test set. The remaining data was divided into five folds. During training, a different fold was used as the validation set in each iteration, while the other four folds were used for training. Training concluded based on the validation set results, and the model was consistently tested on the same test set. This process allowed the model to be evaluated five times using the same data, providing a more accurate estimation of its generalization capability <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">To further investigate the applicability of the proposed concept, the dataset was split into training and testing based on the number of train axles in two different ways. In the first split, the model is intended to function as a surrogate for a malfunctioning axle detector. For this purpose, the data set was split for all folds and the test set in a stratified fashion (fig.¬†<a href="#S2.F3.sf1" title="In Figure 3 ‚Ä£ 2.3 Data Split ‚Ä£ 2 Methodology ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a>), resulting in 3,110 passages with 112,038 individual axles for training and 623 passages with 22,472 individual axles for testing.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">In the second split, the model was provided only with axle positions from trains of a single type. This simulates a scenario in which only one train type is equipped with a differential global positioning system (DGPS), but the axles of all other train types still need to be recognized. This allows the robustness of the method and the overfitting of the model to be examined. Since the exact construction series of the trains for determining the train types are not known, the train length in axles was used as a simplified criterion instead. Trains with the most frequent axle count were used for the five-fold cross-validation sets, while all other train lengths were included in the test set (fig.¬†<a href="#S2.F3.sf2" title="In Figure 3 ‚Ä£ 2.3 Data Split ‚Ä£ 2 Methodology ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(b)</span></a>). This results in 1,916 passages with 61,312 individual axles for training and 1,817 passages with 73,198 individual axles for testing.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">With the stratified splits, training is done with about 5/6 of the data and testing with 1/6 of the data, whereas the splits in the DGPS set for training and testing are about the same size. Thus, in the case of DGPS splits, in addition to having significantly less data available for training, these data are also not representative of the test data and therefore not for all trains. It should be noted that with non-representative data sets, inadequate results are usually to be expected <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. Thus, the DPGS splits are suitable for testing the models under particularly difficult conditions.</p>
</div>
<figure id="S2.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.01574/assets/images/fold1_stratified.png" id="S2.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S2.F3.sf1.3.2" class="ltx_text" style="font-size:90%;">Stratified data splits</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.01574/assets/images/fold1.png" id="S2.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S2.F3.sf2.3.2" class="ltx_text" style="font-size:90%;">DGPS data splits</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.3.2" class="ltx_text" style="font-size:90%;">Train lengths defined by number of axles in both splits for the first combination of folds of a five-fold cross-validation.</span></figcaption>
</figure>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Model Definition</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">The VADER model is implemented as a fully convolutional neural network (FCN) <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> to handle inputs of arbitrary lengths. This property is particularly advantageous for the detection of train axles, as the crossing time varies significantly based on the train‚Äôs speed and the number of axles.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">The U-Net concept <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> was chosen for the model architecture, originally developed for semantic segmentation of images, but also adaptable for segmenting time series data. The fundamental concept of the U-Net architecture is the encoder and decoder path. Even though new architectures have been developed, the U-Net architecture is still frequently used as backbone <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. In order to investigate the effects of the hyperparameters kernel size, max pooling size and pooling steps, no other model architectures are considered in this study. In the encoder path, the resolution is progressively reduced and information is compressed, while in the decoder path, the resolution is increased back to the original dimensions (fig.¬†<a href="#S2.F4.sf1" title="In Figure 4 ‚Ä£ 2.4 Model Definition ‚Ä£ 2 Methodology ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(a)</span></a>). The encoder path aims to capture details at various resolutions, while the decoder path integrates these details into the appropriate context. The intermediate results from the encoder and decoder paths at the same resolution are combined through skip connections.</p>
</div>
<div id="S2.SS4.p3" class="ltx_para">
<p id="S2.SS4.p3.1" class="ltx_p">Our implementation of the U-Net comprises of a well established combination <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> of convolution blocks (CBs), residual blocks (RBs) originally proposed by <cite class="ltx_cite ltx_citemacro_citet">He et¬†al. [<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, max pooling layers, concatenate layers, and transposed convolution layers (fig.¬†<a href="#S2.F4" title="Figure 4 ‚Ä£ 2.4 Model Definition ‚Ä£ 2 Methodology ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). A CB always consists of a convolution layer with Rectified Linear Unit (ReLU) activation and a group normalisation layer <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>. The first group normalisation layer has a single group (making it nearly identical to Layer Normalisation <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>) and remaining group norm layers have 16 channels per group. As a result, the measurement data is first normalised individually (as 1 channel per sensor) in the first group normalisation layer and the remaining CNN feature maps are normalised with the optimum group size according to <cite class="ltx_cite ltx_citemacro_citet">Wu and He [<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>. Group normalisation was shown to be less sensitive to other hyperparameters like batch size and was therefore chosen to effectively reduce the hyperparameter space. The RBs were implemented similarly to the 50-layer or larger variants of the ResNet <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. The final layer in both models is a convolution layer with only one filter and sigmoid activation.</p>
</div>
<figure id="S2.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.01574/assets/x2.png" id="S2.F4.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="99" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S2.F4.sf1.3.2" class="ltx_text" style="font-size:90%;">VADER with spectograms (VADERspec) as input and 4 pooling steps. Derived from <cite class="ltx_cite ltx_citemacro_citet">Lorenzen et¬†al. [<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.01574/assets/x3.png" id="S2.F4.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="69" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S2.F4.sf2.3.2" class="ltx_text" style="font-size:90%;">VADER with raw data (VADERraw) as input and 4 pooling steps.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F4.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.01574/assets/x4.png" id="S2.F4.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="289" height="73" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S2.F4.sf3.3.2" class="ltx_text" style="font-size:90%;">VADER with raw data (VADERraw) as input and 3 pooling steps.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.6.3.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S2.F4.4.2" class="ltx_text" style="font-size:90%;">Model definition with colored boxes corresponding to the following layers: CB (light purple), RB (yellow), max pooling (red), concatenate (green), transposed convolution (blue) and reshaping skip connection (purple arrow). Dimension of the feature maps at the corresponding boxes with <math id="S2.F4.3.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S2.F4.3.1.m1.1b"><mi id="S2.F4.3.1.m1.1.1" xref="S2.F4.3.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.F4.3.1.m1.1c"><ci id="S2.F4.3.1.m1.1.1.cmml" xref="S2.F4.3.1.m1.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F4.3.1.m1.1d">T</annotation></semantics></math> for samples and <math id="S2.F4.4.2.m2.1" class="ltx_Math" alttext="mp" display="inline"><semantics id="S2.F4.4.2.m2.1b"><mrow id="S2.F4.4.2.m2.1.1" xref="S2.F4.4.2.m2.1.1.cmml"><mi id="S2.F4.4.2.m2.1.1.2" xref="S2.F4.4.2.m2.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.F4.4.2.m2.1.1.1" xref="S2.F4.4.2.m2.1.1.1.cmml">‚Äã</mo><mi id="S2.F4.4.2.m2.1.1.3" xref="S2.F4.4.2.m2.1.1.3.cmml">p</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.F4.4.2.m2.1c"><apply id="S2.F4.4.2.m2.1.1.cmml" xref="S2.F4.4.2.m2.1.1"><times id="S2.F4.4.2.m2.1.1.1.cmml" xref="S2.F4.4.2.m2.1.1.1"></times><ci id="S2.F4.4.2.m2.1.1.2.cmml" xref="S2.F4.4.2.m2.1.1.2">ùëö</ci><ci id="S2.F4.4.2.m2.1.1.3.cmml" xref="S2.F4.4.2.m2.1.1.3">ùëù</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F4.4.2.m2.1d">mp</annotation></semantics></math> for max pooling size at the bottom right, feature maps at the bottom and frequencies at the left.</span></figcaption>
</figure>
<div id="S2.SS4.p4" class="ltx_para">
<p id="S2.SS4.p4.2" class="ltx_p">VADER with spectograms (VADERspec) as input (fig.¬†<a href="#S2.F4.sf1" title="In Figure 4 ‚Ä£ 2.4 Model Definition ‚Ä£ 2 Methodology ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(a)</span></a>) uses the default kernel size of <math id="S2.SS4.p4.1.m1.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S2.SS4.p4.1.m1.1a"><mrow id="S2.SS4.p4.1.m1.1.1" xref="S2.SS4.p4.1.m1.1.1.cmml"><mn id="S2.SS4.p4.1.m1.1.1.2" xref="S2.SS4.p4.1.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS4.p4.1.m1.1.1.1" xref="S2.SS4.p4.1.m1.1.1.1.cmml">√ó</mo><mn id="S2.SS4.p4.1.m1.1.1.3" xref="S2.SS4.p4.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p4.1.m1.1b"><apply id="S2.SS4.p4.1.m1.1.1.cmml" xref="S2.SS4.p4.1.m1.1.1"><times id="S2.SS4.p4.1.m1.1.1.1.cmml" xref="S2.SS4.p4.1.m1.1.1.1"></times><cn type="integer" id="S2.SS4.p4.1.m1.1.1.2.cmml" xref="S2.SS4.p4.1.m1.1.1.2">3</cn><cn type="integer" id="S2.SS4.p4.1.m1.1.1.3.cmml" xref="S2.SS4.p4.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p4.1.m1.1c">3\times 3</annotation></semantics></math> and the default pooling size of 2 for 2D inputs <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. This halves the temporal resolution with each pooling step. In the bottleneck layers, the temporal resolution is reduced by a factor of 16, giving the model a receptive field of <math id="S2.SS4.p4.2.m2.1" class="ltx_Math" alttext="16\times 3=48" display="inline"><semantics id="S2.SS4.p4.2.m2.1a"><mrow id="S2.SS4.p4.2.m2.1.1" xref="S2.SS4.p4.2.m2.1.1.cmml"><mrow id="S2.SS4.p4.2.m2.1.1.2" xref="S2.SS4.p4.2.m2.1.1.2.cmml"><mn id="S2.SS4.p4.2.m2.1.1.2.2" xref="S2.SS4.p4.2.m2.1.1.2.2.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS4.p4.2.m2.1.1.2.1" xref="S2.SS4.p4.2.m2.1.1.2.1.cmml">√ó</mo><mn id="S2.SS4.p4.2.m2.1.1.2.3" xref="S2.SS4.p4.2.m2.1.1.2.3.cmml">3</mn></mrow><mo id="S2.SS4.p4.2.m2.1.1.1" xref="S2.SS4.p4.2.m2.1.1.1.cmml">=</mo><mn id="S2.SS4.p4.2.m2.1.1.3" xref="S2.SS4.p4.2.m2.1.1.3.cmml">48</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p4.2.m2.1b"><apply id="S2.SS4.p4.2.m2.1.1.cmml" xref="S2.SS4.p4.2.m2.1.1"><eq id="S2.SS4.p4.2.m2.1.1.1.cmml" xref="S2.SS4.p4.2.m2.1.1.1"></eq><apply id="S2.SS4.p4.2.m2.1.1.2.cmml" xref="S2.SS4.p4.2.m2.1.1.2"><times id="S2.SS4.p4.2.m2.1.1.2.1.cmml" xref="S2.SS4.p4.2.m2.1.1.2.1"></times><cn type="integer" id="S2.SS4.p4.2.m2.1.1.2.2.cmml" xref="S2.SS4.p4.2.m2.1.1.2.2">16</cn><cn type="integer" id="S2.SS4.p4.2.m2.1.1.2.3.cmml" xref="S2.SS4.p4.2.m2.1.1.2.3">3</cn></apply><cn type="integer" id="S2.SS4.p4.2.m2.1.1.3.cmml" xref="S2.SS4.p4.2.m2.1.1.3">48</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p4.2.m2.1c">16\times 3=48</annotation></semantics></math> with a kernel size of 3. Thus, the model can analyse a time frame of at least 48 samples at once.</p>
</div>
<div id="S2.SS4.p5" class="ltx_para">
<p id="S2.SS4.p5.2" class="ltx_p">Since VADER with raw data (VADERraw) as input (fig.¬†<a href="#S2.F4.sf2" title="In Figure 4 ‚Ä£ 2.4 Model Definition ‚Ä£ 2 Methodology ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(b)</span></a>) processes only 1D data, the same number of parameters are required for a kernel size of <math id="S2.SS4.p5.1.m1.1" class="ltx_Math" alttext="9\times 1" display="inline"><semantics id="S2.SS4.p5.1.m1.1a"><mrow id="S2.SS4.p5.1.m1.1.1" xref="S2.SS4.p5.1.m1.1.1.cmml"><mn id="S2.SS4.p5.1.m1.1.1.2" xref="S2.SS4.p5.1.m1.1.1.2.cmml">9</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS4.p5.1.m1.1.1.1" xref="S2.SS4.p5.1.m1.1.1.1.cmml">√ó</mo><mn id="S2.SS4.p5.1.m1.1.1.3" xref="S2.SS4.p5.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p5.1.m1.1b"><apply id="S2.SS4.p5.1.m1.1.1.cmml" xref="S2.SS4.p5.1.m1.1.1"><times id="S2.SS4.p5.1.m1.1.1.1.cmml" xref="S2.SS4.p5.1.m1.1.1.1"></times><cn type="integer" id="S2.SS4.p5.1.m1.1.1.2.cmml" xref="S2.SS4.p5.1.m1.1.1.2">9</cn><cn type="integer" id="S2.SS4.p5.1.m1.1.1.3.cmml" xref="S2.SS4.p5.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p5.1.m1.1c">9\times 1</annotation></semantics></math> as for the kernel size of <math id="S2.SS4.p5.2.m2.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S2.SS4.p5.2.m2.1a"><mrow id="S2.SS4.p5.2.m2.1.1" xref="S2.SS4.p5.2.m2.1.1.cmml"><mn id="S2.SS4.p5.2.m2.1.1.2" xref="S2.SS4.p5.2.m2.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS4.p5.2.m2.1.1.1" xref="S2.SS4.p5.2.m2.1.1.1.cmml">√ó</mo><mn id="S2.SS4.p5.2.m2.1.1.3" xref="S2.SS4.p5.2.m2.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p5.2.m2.1b"><apply id="S2.SS4.p5.2.m2.1.1.cmml" xref="S2.SS4.p5.2.m2.1.1"><times id="S2.SS4.p5.2.m2.1.1.1.cmml" xref="S2.SS4.p5.2.m2.1.1.1"></times><cn type="integer" id="S2.SS4.p5.2.m2.1.1.2.cmml" xref="S2.SS4.p5.2.m2.1.1.2">3</cn><cn type="integer" id="S2.SS4.p5.2.m2.1.1.3.cmml" xref="S2.SS4.p5.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p5.2.m2.1c">3\times 3</annotation></semantics></math> in the VADERspec model. As the kernel size is larger, the pooling size can also be increased. In order to determine the optimal pooling size, kernel size and amount of pooling steps we propose the receptive field (RF) rule: To achieve good results with a CNN, the largest effective receptive field must be at least as large as the largest object of interest. The effective receptive field refers to the number of samples that the kernel could see at the original resolution/sampling rate. After a max pooling layer of size two, the effective receptive field would therefore double with the same kernel size. In our case, the RF rule can be used to calculate the required size of the largest receptive field to capture the lowest frequency of interest:</p>
</div>
<div id="S2.SS4.p6" class="ltx_para">
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S2.E1.m1.1" class="ltx_Math" alttext="y\geq\frac{f_{\mathrm{s}}}{f_{\mathrm{l}}}," display="block"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml"><mrow id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.2.cmml">y</mi><mo id="S2.E1.m1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.cmml">‚â•</mo><mfrac id="S2.E1.m1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.3.cmml"><msub id="S2.E1.m1.1.1.1.1.3.2" xref="S2.E1.m1.1.1.1.1.3.2.cmml"><mi id="S2.E1.m1.1.1.1.1.3.2.2" xref="S2.E1.m1.1.1.1.1.3.2.2.cmml">f</mi><mi mathvariant="normal" id="S2.E1.m1.1.1.1.1.3.2.3" xref="S2.E1.m1.1.1.1.1.3.2.3.cmml">s</mi></msub><msub id="S2.E1.m1.1.1.1.1.3.3" xref="S2.E1.m1.1.1.1.1.3.3.cmml"><mi id="S2.E1.m1.1.1.1.1.3.3.2" xref="S2.E1.m1.1.1.1.1.3.3.2.cmml">f</mi><mi mathvariant="normal" id="S2.E1.m1.1.1.1.1.3.3.3" xref="S2.E1.m1.1.1.1.1.3.3.3.cmml">l</mi></msub></mfrac></mrow><mo id="S2.E1.m1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"><geq id="S2.E1.m1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1"></geq><ci id="S2.E1.m1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.2">ùë¶</ci><apply id="S2.E1.m1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.3"><divide id="S2.E1.m1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.1.3"></divide><apply id="S2.E1.m1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.3.2.1.cmml" xref="S2.E1.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.3.2.2.cmml" xref="S2.E1.m1.1.1.1.1.3.2.2">ùëì</ci><ci id="S2.E1.m1.1.1.1.1.3.2.3.cmml" xref="S2.E1.m1.1.1.1.1.3.2.3">s</ci></apply><apply id="S2.E1.m1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.3.3.1.cmml" xref="S2.E1.m1.1.1.1.1.3.3">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.3.3.2.cmml" xref="S2.E1.m1.1.1.1.1.3.3.2">ùëì</ci><ci id="S2.E1.m1.1.1.1.1.3.3.3.cmml" xref="S2.E1.m1.1.1.1.1.3.3.3">l</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">y\geq\frac{f_{\mathrm{s}}}{f_{\mathrm{l}}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS4.p7" class="ltx_para">
<p id="S2.SS4.p7.2" class="ltx_p">with <math id="S2.SS4.p7.1.m1.1" class="ltx_Math" alttext="f_{\mathrm{s}}" display="inline"><semantics id="S2.SS4.p7.1.m1.1a"><msub id="S2.SS4.p7.1.m1.1.1" xref="S2.SS4.p7.1.m1.1.1.cmml"><mi id="S2.SS4.p7.1.m1.1.1.2" xref="S2.SS4.p7.1.m1.1.1.2.cmml">f</mi><mi mathvariant="normal" id="S2.SS4.p7.1.m1.1.1.3" xref="S2.SS4.p7.1.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.p7.1.m1.1b"><apply id="S2.SS4.p7.1.m1.1.1.cmml" xref="S2.SS4.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS4.p7.1.m1.1.1.1.cmml" xref="S2.SS4.p7.1.m1.1.1">subscript</csymbol><ci id="S2.SS4.p7.1.m1.1.1.2.cmml" xref="S2.SS4.p7.1.m1.1.1.2">ùëì</ci><ci id="S2.SS4.p7.1.m1.1.1.3.cmml" xref="S2.SS4.p7.1.m1.1.1.3">s</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p7.1.m1.1c">f_{\mathrm{s}}</annotation></semantics></math> as sample frequency, <math id="S2.SS4.p7.2.m2.1" class="ltx_Math" alttext="f_{\mathrm{l}}" display="inline"><semantics id="S2.SS4.p7.2.m2.1a"><msub id="S2.SS4.p7.2.m2.1.1" xref="S2.SS4.p7.2.m2.1.1.cmml"><mi id="S2.SS4.p7.2.m2.1.1.2" xref="S2.SS4.p7.2.m2.1.1.2.cmml">f</mi><mi mathvariant="normal" id="S2.SS4.p7.2.m2.1.1.3" xref="S2.SS4.p7.2.m2.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.p7.2.m2.1b"><apply id="S2.SS4.p7.2.m2.1.1.cmml" xref="S2.SS4.p7.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS4.p7.2.m2.1.1.1.cmml" xref="S2.SS4.p7.2.m2.1.1">subscript</csymbol><ci id="S2.SS4.p7.2.m2.1.1.2.cmml" xref="S2.SS4.p7.2.m2.1.1.2">ùëì</ci><ci id="S2.SS4.p7.2.m2.1.1.3.cmml" xref="S2.SS4.p7.2.m2.1.1.3">l</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p7.2.m2.1c">f_{\mathrm{l}}</annotation></semantics></math> as lowest frequency of interest and y as the necessary size of the models largest receptive field in samples. Hence, to cover signals of, e.g., 1¬†Hz at a sampling rate of 600¬†Hz, the receptive field must span at least 600¬†samples (eq.¬†<a href="#S2.E1" title="In 2.4 Model Definition ‚Ä£ 2 Methodology ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div id="S2.SS4.p8" class="ltx_para">
<p id="S2.SS4.p8.2" class="ltx_p">The bridge has a natural frequency of up to 6.9¬†Hz (depending on the amplitude), and to distinguish the contribution of the bridge from the axle loads‚Äô structural response, a receptive field of at least <math id="S2.SS4.p8.1.m1.1" class="ltx_Math" alttext="\frac{600~{}Hz}{6.9~{}Hz}\approx 87" display="inline"><semantics id="S2.SS4.p8.1.m1.1a"><mrow id="S2.SS4.p8.1.m1.1.1" xref="S2.SS4.p8.1.m1.1.1.cmml"><mfrac id="S2.SS4.p8.1.m1.1.1.2" xref="S2.SS4.p8.1.m1.1.1.2.cmml"><mrow id="S2.SS4.p8.1.m1.1.1.2.2" xref="S2.SS4.p8.1.m1.1.1.2.2.cmml"><mn id="S2.SS4.p8.1.m1.1.1.2.2.2" xref="S2.SS4.p8.1.m1.1.1.2.2.2.cmml">600</mn><mo lspace="0.230em" rspace="0em" id="S2.SS4.p8.1.m1.1.1.2.2.1" xref="S2.SS4.p8.1.m1.1.1.2.2.1.cmml">‚Äã</mo><mi id="S2.SS4.p8.1.m1.1.1.2.2.3" xref="S2.SS4.p8.1.m1.1.1.2.2.3.cmml">H</mi><mo lspace="0em" rspace="0em" id="S2.SS4.p8.1.m1.1.1.2.2.1a" xref="S2.SS4.p8.1.m1.1.1.2.2.1.cmml">‚Äã</mo><mi id="S2.SS4.p8.1.m1.1.1.2.2.4" xref="S2.SS4.p8.1.m1.1.1.2.2.4.cmml">z</mi></mrow><mrow id="S2.SS4.p8.1.m1.1.1.2.3" xref="S2.SS4.p8.1.m1.1.1.2.3.cmml"><mn id="S2.SS4.p8.1.m1.1.1.2.3.2" xref="S2.SS4.p8.1.m1.1.1.2.3.2.cmml">6.9</mn><mo lspace="0.230em" rspace="0em" id="S2.SS4.p8.1.m1.1.1.2.3.1" xref="S2.SS4.p8.1.m1.1.1.2.3.1.cmml">‚Äã</mo><mi id="S2.SS4.p8.1.m1.1.1.2.3.3" xref="S2.SS4.p8.1.m1.1.1.2.3.3.cmml">H</mi><mo lspace="0em" rspace="0em" id="S2.SS4.p8.1.m1.1.1.2.3.1a" xref="S2.SS4.p8.1.m1.1.1.2.3.1.cmml">‚Äã</mo><mi id="S2.SS4.p8.1.m1.1.1.2.3.4" xref="S2.SS4.p8.1.m1.1.1.2.3.4.cmml">z</mi></mrow></mfrac><mo id="S2.SS4.p8.1.m1.1.1.1" xref="S2.SS4.p8.1.m1.1.1.1.cmml">‚âà</mo><mn id="S2.SS4.p8.1.m1.1.1.3" xref="S2.SS4.p8.1.m1.1.1.3.cmml">87</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p8.1.m1.1b"><apply id="S2.SS4.p8.1.m1.1.1.cmml" xref="S2.SS4.p8.1.m1.1.1"><approx id="S2.SS4.p8.1.m1.1.1.1.cmml" xref="S2.SS4.p8.1.m1.1.1.1"></approx><apply id="S2.SS4.p8.1.m1.1.1.2.cmml" xref="S2.SS4.p8.1.m1.1.1.2"><divide id="S2.SS4.p8.1.m1.1.1.2.1.cmml" xref="S2.SS4.p8.1.m1.1.1.2"></divide><apply id="S2.SS4.p8.1.m1.1.1.2.2.cmml" xref="S2.SS4.p8.1.m1.1.1.2.2"><times id="S2.SS4.p8.1.m1.1.1.2.2.1.cmml" xref="S2.SS4.p8.1.m1.1.1.2.2.1"></times><cn type="integer" id="S2.SS4.p8.1.m1.1.1.2.2.2.cmml" xref="S2.SS4.p8.1.m1.1.1.2.2.2">600</cn><ci id="S2.SS4.p8.1.m1.1.1.2.2.3.cmml" xref="S2.SS4.p8.1.m1.1.1.2.2.3">ùêª</ci><ci id="S2.SS4.p8.1.m1.1.1.2.2.4.cmml" xref="S2.SS4.p8.1.m1.1.1.2.2.4">ùëß</ci></apply><apply id="S2.SS4.p8.1.m1.1.1.2.3.cmml" xref="S2.SS4.p8.1.m1.1.1.2.3"><times id="S2.SS4.p8.1.m1.1.1.2.3.1.cmml" xref="S2.SS4.p8.1.m1.1.1.2.3.1"></times><cn type="float" id="S2.SS4.p8.1.m1.1.1.2.3.2.cmml" xref="S2.SS4.p8.1.m1.1.1.2.3.2">6.9</cn><ci id="S2.SS4.p8.1.m1.1.1.2.3.3.cmml" xref="S2.SS4.p8.1.m1.1.1.2.3.3">ùêª</ci><ci id="S2.SS4.p8.1.m1.1.1.2.3.4.cmml" xref="S2.SS4.p8.1.m1.1.1.2.3.4">ùëß</ci></apply></apply><cn type="integer" id="S2.SS4.p8.1.m1.1.1.3.cmml" xref="S2.SS4.p8.1.m1.1.1.3">87</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p8.1.m1.1c">\frac{600~{}Hz}{6.9~{}Hz}\approx 87</annotation></semantics></math>¬†samples would be necessary. Taking into account that the resonance frequency decreases to about 5.6¬†Hz for big amplitudes <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, a minimum receptive field of <math id="S2.SS4.p8.2.m2.1" class="ltx_Math" alttext="\frac{600~{}Hz}{5~{}Hz}=120" display="inline"><semantics id="S2.SS4.p8.2.m2.1a"><mrow id="S2.SS4.p8.2.m2.1.1" xref="S2.SS4.p8.2.m2.1.1.cmml"><mfrac id="S2.SS4.p8.2.m2.1.1.2" xref="S2.SS4.p8.2.m2.1.1.2.cmml"><mrow id="S2.SS4.p8.2.m2.1.1.2.2" xref="S2.SS4.p8.2.m2.1.1.2.2.cmml"><mn id="S2.SS4.p8.2.m2.1.1.2.2.2" xref="S2.SS4.p8.2.m2.1.1.2.2.2.cmml">600</mn><mo lspace="0.230em" rspace="0em" id="S2.SS4.p8.2.m2.1.1.2.2.1" xref="S2.SS4.p8.2.m2.1.1.2.2.1.cmml">‚Äã</mo><mi id="S2.SS4.p8.2.m2.1.1.2.2.3" xref="S2.SS4.p8.2.m2.1.1.2.2.3.cmml">H</mi><mo lspace="0em" rspace="0em" id="S2.SS4.p8.2.m2.1.1.2.2.1a" xref="S2.SS4.p8.2.m2.1.1.2.2.1.cmml">‚Äã</mo><mi id="S2.SS4.p8.2.m2.1.1.2.2.4" xref="S2.SS4.p8.2.m2.1.1.2.2.4.cmml">z</mi></mrow><mrow id="S2.SS4.p8.2.m2.1.1.2.3" xref="S2.SS4.p8.2.m2.1.1.2.3.cmml"><mn id="S2.SS4.p8.2.m2.1.1.2.3.2" xref="S2.SS4.p8.2.m2.1.1.2.3.2.cmml">5</mn><mo lspace="0.230em" rspace="0em" id="S2.SS4.p8.2.m2.1.1.2.3.1" xref="S2.SS4.p8.2.m2.1.1.2.3.1.cmml">‚Äã</mo><mi id="S2.SS4.p8.2.m2.1.1.2.3.3" xref="S2.SS4.p8.2.m2.1.1.2.3.3.cmml">H</mi><mo lspace="0em" rspace="0em" id="S2.SS4.p8.2.m2.1.1.2.3.1a" xref="S2.SS4.p8.2.m2.1.1.2.3.1.cmml">‚Äã</mo><mi id="S2.SS4.p8.2.m2.1.1.2.3.4" xref="S2.SS4.p8.2.m2.1.1.2.3.4.cmml">z</mi></mrow></mfrac><mo id="S2.SS4.p8.2.m2.1.1.1" xref="S2.SS4.p8.2.m2.1.1.1.cmml">=</mo><mn id="S2.SS4.p8.2.m2.1.1.3" xref="S2.SS4.p8.2.m2.1.1.3.cmml">120</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p8.2.m2.1b"><apply id="S2.SS4.p8.2.m2.1.1.cmml" xref="S2.SS4.p8.2.m2.1.1"><eq id="S2.SS4.p8.2.m2.1.1.1.cmml" xref="S2.SS4.p8.2.m2.1.1.1"></eq><apply id="S2.SS4.p8.2.m2.1.1.2.cmml" xref="S2.SS4.p8.2.m2.1.1.2"><divide id="S2.SS4.p8.2.m2.1.1.2.1.cmml" xref="S2.SS4.p8.2.m2.1.1.2"></divide><apply id="S2.SS4.p8.2.m2.1.1.2.2.cmml" xref="S2.SS4.p8.2.m2.1.1.2.2"><times id="S2.SS4.p8.2.m2.1.1.2.2.1.cmml" xref="S2.SS4.p8.2.m2.1.1.2.2.1"></times><cn type="integer" id="S2.SS4.p8.2.m2.1.1.2.2.2.cmml" xref="S2.SS4.p8.2.m2.1.1.2.2.2">600</cn><ci id="S2.SS4.p8.2.m2.1.1.2.2.3.cmml" xref="S2.SS4.p8.2.m2.1.1.2.2.3">ùêª</ci><ci id="S2.SS4.p8.2.m2.1.1.2.2.4.cmml" xref="S2.SS4.p8.2.m2.1.1.2.2.4">ùëß</ci></apply><apply id="S2.SS4.p8.2.m2.1.1.2.3.cmml" xref="S2.SS4.p8.2.m2.1.1.2.3"><times id="S2.SS4.p8.2.m2.1.1.2.3.1.cmml" xref="S2.SS4.p8.2.m2.1.1.2.3.1"></times><cn type="integer" id="S2.SS4.p8.2.m2.1.1.2.3.2.cmml" xref="S2.SS4.p8.2.m2.1.1.2.3.2">5</cn><ci id="S2.SS4.p8.2.m2.1.1.2.3.3.cmml" xref="S2.SS4.p8.2.m2.1.1.2.3.3">ùêª</ci><ci id="S2.SS4.p8.2.m2.1.1.2.3.4.cmml" xref="S2.SS4.p8.2.m2.1.1.2.3.4">ùëß</ci></apply></apply><cn type="integer" id="S2.SS4.p8.2.m2.1.1.3.cmml" xref="S2.SS4.p8.2.m2.1.1.3">120</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p8.2.m2.1c">\frac{600~{}Hz}{5~{}Hz}=120</annotation></semantics></math>¬†samples ensures that more than the largest object size is captured.</p>
</div>
<div id="S2.SS4.p9" class="ltx_para">
<p id="S2.SS4.p9.1" class="ltx_p">Much like CNNs, the wavelet convolves over the samples, effectively encoding frequency information into individual sample values through the CWT transformation. Depending on the wavelet and the scale, different frequencies are filtered. With the parameters used, information on frequencies up to approximately 3.4¬†Hz is therefore encoded in individual samples. Therefore, the CWT transformation can effectively increase the receptive field for VADERspec to <math id="S2.SS4.p9.1.m1.1" class="ltx_Math" alttext="\frac{600~{}Hz}{3.4~{}Hz}\approx 176" display="inline"><semantics id="S2.SS4.p9.1.m1.1a"><mrow id="S2.SS4.p9.1.m1.1.1" xref="S2.SS4.p9.1.m1.1.1.cmml"><mfrac id="S2.SS4.p9.1.m1.1.1.2" xref="S2.SS4.p9.1.m1.1.1.2.cmml"><mrow id="S2.SS4.p9.1.m1.1.1.2.2" xref="S2.SS4.p9.1.m1.1.1.2.2.cmml"><mn id="S2.SS4.p9.1.m1.1.1.2.2.2" xref="S2.SS4.p9.1.m1.1.1.2.2.2.cmml">600</mn><mo lspace="0.230em" rspace="0em" id="S2.SS4.p9.1.m1.1.1.2.2.1" xref="S2.SS4.p9.1.m1.1.1.2.2.1.cmml">‚Äã</mo><mi id="S2.SS4.p9.1.m1.1.1.2.2.3" xref="S2.SS4.p9.1.m1.1.1.2.2.3.cmml">H</mi><mo lspace="0em" rspace="0em" id="S2.SS4.p9.1.m1.1.1.2.2.1a" xref="S2.SS4.p9.1.m1.1.1.2.2.1.cmml">‚Äã</mo><mi id="S2.SS4.p9.1.m1.1.1.2.2.4" xref="S2.SS4.p9.1.m1.1.1.2.2.4.cmml">z</mi></mrow><mrow id="S2.SS4.p9.1.m1.1.1.2.3" xref="S2.SS4.p9.1.m1.1.1.2.3.cmml"><mn id="S2.SS4.p9.1.m1.1.1.2.3.2" xref="S2.SS4.p9.1.m1.1.1.2.3.2.cmml">3.4</mn><mo lspace="0.230em" rspace="0em" id="S2.SS4.p9.1.m1.1.1.2.3.1" xref="S2.SS4.p9.1.m1.1.1.2.3.1.cmml">‚Äã</mo><mi id="S2.SS4.p9.1.m1.1.1.2.3.3" xref="S2.SS4.p9.1.m1.1.1.2.3.3.cmml">H</mi><mo lspace="0em" rspace="0em" id="S2.SS4.p9.1.m1.1.1.2.3.1a" xref="S2.SS4.p9.1.m1.1.1.2.3.1.cmml">‚Äã</mo><mi id="S2.SS4.p9.1.m1.1.1.2.3.4" xref="S2.SS4.p9.1.m1.1.1.2.3.4.cmml">z</mi></mrow></mfrac><mo id="S2.SS4.p9.1.m1.1.1.1" xref="S2.SS4.p9.1.m1.1.1.1.cmml">‚âà</mo><mn id="S2.SS4.p9.1.m1.1.1.3" xref="S2.SS4.p9.1.m1.1.1.3.cmml">176</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p9.1.m1.1b"><apply id="S2.SS4.p9.1.m1.1.1.cmml" xref="S2.SS4.p9.1.m1.1.1"><approx id="S2.SS4.p9.1.m1.1.1.1.cmml" xref="S2.SS4.p9.1.m1.1.1.1"></approx><apply id="S2.SS4.p9.1.m1.1.1.2.cmml" xref="S2.SS4.p9.1.m1.1.1.2"><divide id="S2.SS4.p9.1.m1.1.1.2.1.cmml" xref="S2.SS4.p9.1.m1.1.1.2"></divide><apply id="S2.SS4.p9.1.m1.1.1.2.2.cmml" xref="S2.SS4.p9.1.m1.1.1.2.2"><times id="S2.SS4.p9.1.m1.1.1.2.2.1.cmml" xref="S2.SS4.p9.1.m1.1.1.2.2.1"></times><cn type="integer" id="S2.SS4.p9.1.m1.1.1.2.2.2.cmml" xref="S2.SS4.p9.1.m1.1.1.2.2.2">600</cn><ci id="S2.SS4.p9.1.m1.1.1.2.2.3.cmml" xref="S2.SS4.p9.1.m1.1.1.2.2.3">ùêª</ci><ci id="S2.SS4.p9.1.m1.1.1.2.2.4.cmml" xref="S2.SS4.p9.1.m1.1.1.2.2.4">ùëß</ci></apply><apply id="S2.SS4.p9.1.m1.1.1.2.3.cmml" xref="S2.SS4.p9.1.m1.1.1.2.3"><times id="S2.SS4.p9.1.m1.1.1.2.3.1.cmml" xref="S2.SS4.p9.1.m1.1.1.2.3.1"></times><cn type="float" id="S2.SS4.p9.1.m1.1.1.2.3.2.cmml" xref="S2.SS4.p9.1.m1.1.1.2.3.2">3.4</cn><ci id="S2.SS4.p9.1.m1.1.1.2.3.3.cmml" xref="S2.SS4.p9.1.m1.1.1.2.3.3">ùêª</ci><ci id="S2.SS4.p9.1.m1.1.1.2.3.4.cmml" xref="S2.SS4.p9.1.m1.1.1.2.3.4">ùëß</ci></apply></apply><cn type="integer" id="S2.SS4.p9.1.m1.1.1.3.cmml" xref="S2.SS4.p9.1.m1.1.1.3">176</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p9.1.m1.1c">\frac{600~{}Hz}{3.4~{}Hz}\approx 176</annotation></semantics></math>¬†samples.</p>
</div>
<div id="S2.SS4.p10" class="ltx_para">
<p id="S2.SS4.p10.1" class="ltx_p">With a pooling size of 3, a kernel size of 9 and 4 pooling steps, VADERraw would result in a maximum receptive field of <math id="S2.SS4.p10.1.m1.1" class="ltx_Math" alttext="3^{4}\times 9=729" display="inline"><semantics id="S2.SS4.p10.1.m1.1a"><mrow id="S2.SS4.p10.1.m1.1.1" xref="S2.SS4.p10.1.m1.1.1.cmml"><mrow id="S2.SS4.p10.1.m1.1.1.2" xref="S2.SS4.p10.1.m1.1.1.2.cmml"><msup id="S2.SS4.p10.1.m1.1.1.2.2" xref="S2.SS4.p10.1.m1.1.1.2.2.cmml"><mn id="S2.SS4.p10.1.m1.1.1.2.2.2" xref="S2.SS4.p10.1.m1.1.1.2.2.2.cmml">3</mn><mn id="S2.SS4.p10.1.m1.1.1.2.2.3" xref="S2.SS4.p10.1.m1.1.1.2.2.3.cmml">4</mn></msup><mo lspace="0.222em" rspace="0.222em" id="S2.SS4.p10.1.m1.1.1.2.1" xref="S2.SS4.p10.1.m1.1.1.2.1.cmml">√ó</mo><mn id="S2.SS4.p10.1.m1.1.1.2.3" xref="S2.SS4.p10.1.m1.1.1.2.3.cmml">9</mn></mrow><mo id="S2.SS4.p10.1.m1.1.1.1" xref="S2.SS4.p10.1.m1.1.1.1.cmml">=</mo><mn id="S2.SS4.p10.1.m1.1.1.3" xref="S2.SS4.p10.1.m1.1.1.3.cmml">729</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p10.1.m1.1b"><apply id="S2.SS4.p10.1.m1.1.1.cmml" xref="S2.SS4.p10.1.m1.1.1"><eq id="S2.SS4.p10.1.m1.1.1.1.cmml" xref="S2.SS4.p10.1.m1.1.1.1"></eq><apply id="S2.SS4.p10.1.m1.1.1.2.cmml" xref="S2.SS4.p10.1.m1.1.1.2"><times id="S2.SS4.p10.1.m1.1.1.2.1.cmml" xref="S2.SS4.p10.1.m1.1.1.2.1"></times><apply id="S2.SS4.p10.1.m1.1.1.2.2.cmml" xref="S2.SS4.p10.1.m1.1.1.2.2"><csymbol cd="ambiguous" id="S2.SS4.p10.1.m1.1.1.2.2.1.cmml" xref="S2.SS4.p10.1.m1.1.1.2.2">superscript</csymbol><cn type="integer" id="S2.SS4.p10.1.m1.1.1.2.2.2.cmml" xref="S2.SS4.p10.1.m1.1.1.2.2.2">3</cn><cn type="integer" id="S2.SS4.p10.1.m1.1.1.2.2.3.cmml" xref="S2.SS4.p10.1.m1.1.1.2.2.3">4</cn></apply><cn type="integer" id="S2.SS4.p10.1.m1.1.1.2.3.cmml" xref="S2.SS4.p10.1.m1.1.1.2.3">9</cn></apply><cn type="integer" id="S2.SS4.p10.1.m1.1.1.3.cmml" xref="S2.SS4.p10.1.m1.1.1.3">729</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p10.1.m1.1c">3^{4}\times 9=729</annotation></semantics></math>¬†samples. Therefore, in this case the actual receptive field of VADERraw is over 15 times larger (729 vs. 48) and the effective receptive field is over 4 times larger (729 vs. 176) than that of VADERspec, while using the same number of parameters. As bigger a RF does not always lead to better results, two amounts of pooling steps, eight different kernel sizes and four different max pooling sizes were examined as a hyperparameter study (fig.¬†<a href="#S2.F5" title="Figure 5 ‚Ä£ 2.4 Model Definition ‚Ä£ 2 Methodology ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
<div id="S2.SS4.p11" class="ltx_para">
<p id="S2.SS4.p11.1" class="ltx_p">Depending on the hyperparameter combinations, there are large differences in the maximum repective field size of the respective model (fig.¬†<a href="#S2.F5" title="Figure 5 ‚Ä£ 2.4 Model Definition ‚Ä£ 2 Methodology ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). With a required minimum receptive field size of 87-120¬†samples (depending on whether the natural frequency is considered constant or not), a small proportion of the models are expected to perform significantly worse (dark purple in Fig.¬†<a href="#S2.F5" title="Figure 5 ‚Ä£ 2.4 Model Definition ‚Ä£ 2 Methodology ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). From a RF size above <math id="S2.SS4.p11.1.m1.1" class="ltx_Math" alttext="\frac{600~{}Hz}{1~{}Hz}\approx 600" display="inline"><semantics id="S2.SS4.p11.1.m1.1a"><mrow id="S2.SS4.p11.1.m1.1.1" xref="S2.SS4.p11.1.m1.1.1.cmml"><mfrac id="S2.SS4.p11.1.m1.1.1.2" xref="S2.SS4.p11.1.m1.1.1.2.cmml"><mrow id="S2.SS4.p11.1.m1.1.1.2.2" xref="S2.SS4.p11.1.m1.1.1.2.2.cmml"><mn id="S2.SS4.p11.1.m1.1.1.2.2.2" xref="S2.SS4.p11.1.m1.1.1.2.2.2.cmml">600</mn><mo lspace="0.230em" rspace="0em" id="S2.SS4.p11.1.m1.1.1.2.2.1" xref="S2.SS4.p11.1.m1.1.1.2.2.1.cmml">‚Äã</mo><mi id="S2.SS4.p11.1.m1.1.1.2.2.3" xref="S2.SS4.p11.1.m1.1.1.2.2.3.cmml">H</mi><mo lspace="0em" rspace="0em" id="S2.SS4.p11.1.m1.1.1.2.2.1a" xref="S2.SS4.p11.1.m1.1.1.2.2.1.cmml">‚Äã</mo><mi id="S2.SS4.p11.1.m1.1.1.2.2.4" xref="S2.SS4.p11.1.m1.1.1.2.2.4.cmml">z</mi></mrow><mrow id="S2.SS4.p11.1.m1.1.1.2.3" xref="S2.SS4.p11.1.m1.1.1.2.3.cmml"><mn id="S2.SS4.p11.1.m1.1.1.2.3.2" xref="S2.SS4.p11.1.m1.1.1.2.3.2.cmml">1</mn><mo lspace="0.230em" rspace="0em" id="S2.SS4.p11.1.m1.1.1.2.3.1" xref="S2.SS4.p11.1.m1.1.1.2.3.1.cmml">‚Äã</mo><mi id="S2.SS4.p11.1.m1.1.1.2.3.3" xref="S2.SS4.p11.1.m1.1.1.2.3.3.cmml">H</mi><mo lspace="0em" rspace="0em" id="S2.SS4.p11.1.m1.1.1.2.3.1a" xref="S2.SS4.p11.1.m1.1.1.2.3.1.cmml">‚Äã</mo><mi id="S2.SS4.p11.1.m1.1.1.2.3.4" xref="S2.SS4.p11.1.m1.1.1.2.3.4.cmml">z</mi></mrow></mfrac><mo id="S2.SS4.p11.1.m1.1.1.1" xref="S2.SS4.p11.1.m1.1.1.1.cmml">‚âà</mo><mn id="S2.SS4.p11.1.m1.1.1.3" xref="S2.SS4.p11.1.m1.1.1.3.cmml">600</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p11.1.m1.1b"><apply id="S2.SS4.p11.1.m1.1.1.cmml" xref="S2.SS4.p11.1.m1.1.1"><approx id="S2.SS4.p11.1.m1.1.1.1.cmml" xref="S2.SS4.p11.1.m1.1.1.1"></approx><apply id="S2.SS4.p11.1.m1.1.1.2.cmml" xref="S2.SS4.p11.1.m1.1.1.2"><divide id="S2.SS4.p11.1.m1.1.1.2.1.cmml" xref="S2.SS4.p11.1.m1.1.1.2"></divide><apply id="S2.SS4.p11.1.m1.1.1.2.2.cmml" xref="S2.SS4.p11.1.m1.1.1.2.2"><times id="S2.SS4.p11.1.m1.1.1.2.2.1.cmml" xref="S2.SS4.p11.1.m1.1.1.2.2.1"></times><cn type="integer" id="S2.SS4.p11.1.m1.1.1.2.2.2.cmml" xref="S2.SS4.p11.1.m1.1.1.2.2.2">600</cn><ci id="S2.SS4.p11.1.m1.1.1.2.2.3.cmml" xref="S2.SS4.p11.1.m1.1.1.2.2.3">ùêª</ci><ci id="S2.SS4.p11.1.m1.1.1.2.2.4.cmml" xref="S2.SS4.p11.1.m1.1.1.2.2.4">ùëß</ci></apply><apply id="S2.SS4.p11.1.m1.1.1.2.3.cmml" xref="S2.SS4.p11.1.m1.1.1.2.3"><times id="S2.SS4.p11.1.m1.1.1.2.3.1.cmml" xref="S2.SS4.p11.1.m1.1.1.2.3.1"></times><cn type="integer" id="S2.SS4.p11.1.m1.1.1.2.3.2.cmml" xref="S2.SS4.p11.1.m1.1.1.2.3.2">1</cn><ci id="S2.SS4.p11.1.m1.1.1.2.3.3.cmml" xref="S2.SS4.p11.1.m1.1.1.2.3.3">ùêª</ci><ci id="S2.SS4.p11.1.m1.1.1.2.3.4.cmml" xref="S2.SS4.p11.1.m1.1.1.2.3.4">ùëß</ci></apply></apply><cn type="integer" id="S2.SS4.p11.1.m1.1.1.3.cmml" xref="S2.SS4.p11.1.m1.1.1.3">600</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p11.1.m1.1c">\frac{600~{}Hz}{1~{}Hz}\approx 600</annotation></semantics></math>¬†samples there is expected to be no more improvement in the results (light orange in Fig.¬†<a href="#S2.F5" title="Figure 5 ‚Ä£ 2.4 Model Definition ‚Ä£ 2 Methodology ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). There are no objects with frequencies lower than 1¬†Hz that the model should be able to detect. An RF that can capture frequencies of 1¬†Hz already allows to capture several bridge oscillation periods in one RF. Beyond that, the risk increases that entire trains and thus axle configurations are learned by the model. This would therefore increase the probability of overfitting and reduce the generalisation capability.</p>
</div>
<figure id="S2.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.01574/assets/images/RF_MP_3.png" id="S2.F5.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="435" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F5.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S2.F5.sf1.3.2" class="ltx_text" style="font-size:90%;">Resulting maximum receptive fields for VADERraw with 3 pooling steps.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.01574/assets/images/RF_MP_4.png" id="S2.F5.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="435" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F5.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S2.F5.sf2.3.2" class="ltx_text" style="font-size:90%;">Resulting maximum receptive fields for VADERraw with 4 pooling steps.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S2.F5.3.2" class="ltx_text" style="font-size:90%;">Maximum receptive fields as a function of kernel size, max pooling size and pooling steps for all used combinations of the hyperparameter study.</span></figcaption>
</figure>
<div id="S2.SS4.p12" class="ltx_para">
<p id="S2.SS4.p12.1" class="ltx_p">The TensorFlow library <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> was used for implementation of the models and PlotNeuralNet <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> was used for visualising it.</p>
</div>
</section>
<section id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Training Parameter</h3>

<div id="S2.SS5.p1" class="ltx_para">
<p id="S2.SS5.p1.2" class="ltx_p">For the loss function, binary focal loss was chosen with an optimised <math id="S2.SS5.p1.1.m1.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S2.SS5.p1.1.m1.1a"><mi id="S2.SS5.p1.1.m1.1.1" xref="S2.SS5.p1.1.m1.1.1.cmml">Œ≥</mi><annotation-xml encoding="MathML-Content" id="S2.SS5.p1.1.m1.1b"><ci id="S2.SS5.p1.1.m1.1.1.cmml" xref="S2.SS5.p1.1.m1.1.1">ùõæ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p1.1.m1.1c">\gamma</annotation></semantics></math> value of 2.5 <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, as it is particularly suitable for imbalanced data sets <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>. Adam was used as the optimiser with the default initial learning rate of 0.001 <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>. To evaluate the model‚Äôs performance, the <math id="S2.SS5.p1.2.m2.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S2.SS5.p1.2.m2.1a"><msub id="S2.SS5.p1.2.m2.1.1" xref="S2.SS5.p1.2.m2.1.1.cmml"><mi id="S2.SS5.p1.2.m2.1.1.2" xref="S2.SS5.p1.2.m2.1.1.2.cmml">F</mi><mn id="S2.SS5.p1.2.m2.1.1.3" xref="S2.SS5.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS5.p1.2.m2.1b"><apply id="S2.SS5.p1.2.m2.1.1.cmml" xref="S2.SS5.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS5.p1.2.m2.1.1.1.cmml" xref="S2.SS5.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS5.p1.2.m2.1.1.2.cmml" xref="S2.SS5.p1.2.m2.1.1.2">ùêπ</ci><cn type="integer" id="S2.SS5.p1.2.m2.1.1.3.cmml" xref="S2.SS5.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p1.2.m2.1c">F_{1}</annotation></semantics></math> score (eq.¬†<a href="#S2.E2" title="In 2.5 Training Parameter ‚Ä£ 2 Methodology ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) was used as the standard metric for imbalanced data sets <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>:</p>
</div>
<div id="S2.SS5.p2" class="ltx_para">
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S2.E2.m1.1" class="ltx_Math" alttext="F_{1}=2\times\frac{p\times r}{p+r}\times 100" display="block"><semantics id="S2.E2.m1.1a"><mrow id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml"><msub id="S2.E2.m1.1.1.2" xref="S2.E2.m1.1.1.2.cmml"><mi id="S2.E2.m1.1.1.2.2" xref="S2.E2.m1.1.1.2.2.cmml">F</mi><mn id="S2.E2.m1.1.1.2.3" xref="S2.E2.m1.1.1.2.3.cmml">1</mn></msub><mo id="S2.E2.m1.1.1.1" xref="S2.E2.m1.1.1.1.cmml">=</mo><mrow id="S2.E2.m1.1.1.3" xref="S2.E2.m1.1.1.3.cmml"><mn id="S2.E2.m1.1.1.3.2" xref="S2.E2.m1.1.1.3.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S2.E2.m1.1.1.3.1" xref="S2.E2.m1.1.1.3.1.cmml">√ó</mo><mfrac id="S2.E2.m1.1.1.3.3" xref="S2.E2.m1.1.1.3.3.cmml"><mrow id="S2.E2.m1.1.1.3.3.2" xref="S2.E2.m1.1.1.3.3.2.cmml"><mi id="S2.E2.m1.1.1.3.3.2.2" xref="S2.E2.m1.1.1.3.3.2.2.cmml">p</mi><mo lspace="0.222em" rspace="0.222em" id="S2.E2.m1.1.1.3.3.2.1" xref="S2.E2.m1.1.1.3.3.2.1.cmml">√ó</mo><mi id="S2.E2.m1.1.1.3.3.2.3" xref="S2.E2.m1.1.1.3.3.2.3.cmml">r</mi></mrow><mrow id="S2.E2.m1.1.1.3.3.3" xref="S2.E2.m1.1.1.3.3.3.cmml"><mi id="S2.E2.m1.1.1.3.3.3.2" xref="S2.E2.m1.1.1.3.3.3.2.cmml">p</mi><mo id="S2.E2.m1.1.1.3.3.3.1" xref="S2.E2.m1.1.1.3.3.3.1.cmml">+</mo><mi id="S2.E2.m1.1.1.3.3.3.3" xref="S2.E2.m1.1.1.3.3.3.3.cmml">r</mi></mrow></mfrac><mo lspace="0.222em" rspace="0.222em" id="S2.E2.m1.1.1.3.1a" xref="S2.E2.m1.1.1.3.1.cmml">√ó</mo><mn id="S2.E2.m1.1.1.3.4" xref="S2.E2.m1.1.1.3.4.cmml">100</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.1b"><apply id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1"><eq id="S2.E2.m1.1.1.1.cmml" xref="S2.E2.m1.1.1.1"></eq><apply id="S2.E2.m1.1.1.2.cmml" xref="S2.E2.m1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.2.1.cmml" xref="S2.E2.m1.1.1.2">subscript</csymbol><ci id="S2.E2.m1.1.1.2.2.cmml" xref="S2.E2.m1.1.1.2.2">ùêπ</ci><cn type="integer" id="S2.E2.m1.1.1.2.3.cmml" xref="S2.E2.m1.1.1.2.3">1</cn></apply><apply id="S2.E2.m1.1.1.3.cmml" xref="S2.E2.m1.1.1.3"><times id="S2.E2.m1.1.1.3.1.cmml" xref="S2.E2.m1.1.1.3.1"></times><cn type="integer" id="S2.E2.m1.1.1.3.2.cmml" xref="S2.E2.m1.1.1.3.2">2</cn><apply id="S2.E2.m1.1.1.3.3.cmml" xref="S2.E2.m1.1.1.3.3"><divide id="S2.E2.m1.1.1.3.3.1.cmml" xref="S2.E2.m1.1.1.3.3"></divide><apply id="S2.E2.m1.1.1.3.3.2.cmml" xref="S2.E2.m1.1.1.3.3.2"><times id="S2.E2.m1.1.1.3.3.2.1.cmml" xref="S2.E2.m1.1.1.3.3.2.1"></times><ci id="S2.E2.m1.1.1.3.3.2.2.cmml" xref="S2.E2.m1.1.1.3.3.2.2">ùëù</ci><ci id="S2.E2.m1.1.1.3.3.2.3.cmml" xref="S2.E2.m1.1.1.3.3.2.3">ùëü</ci></apply><apply id="S2.E2.m1.1.1.3.3.3.cmml" xref="S2.E2.m1.1.1.3.3.3"><plus id="S2.E2.m1.1.1.3.3.3.1.cmml" xref="S2.E2.m1.1.1.3.3.3.1"></plus><ci id="S2.E2.m1.1.1.3.3.3.2.cmml" xref="S2.E2.m1.1.1.3.3.3.2">ùëù</ci><ci id="S2.E2.m1.1.1.3.3.3.3.cmml" xref="S2.E2.m1.1.1.3.3.3.3">ùëü</ci></apply></apply><cn type="integer" id="S2.E2.m1.1.1.3.4.cmml" xref="S2.E2.m1.1.1.3.4">100</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.1c">F_{1}=2\times\frac{p\times r}{p+r}\times 100</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS5.p3" class="ltx_para">
<p id="S2.SS5.p3.1" class="ltx_p">where,</p>
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S2.E3.m1.1" class="ltx_Math" alttext="p=\frac{tp}{tp+fp}" display="block"><semantics id="S2.E3.m1.1a"><mrow id="S2.E3.m1.1.1" xref="S2.E3.m1.1.1.cmml"><mi id="S2.E3.m1.1.1.2" xref="S2.E3.m1.1.1.2.cmml">p</mi><mo id="S2.E3.m1.1.1.1" xref="S2.E3.m1.1.1.1.cmml">=</mo><mfrac id="S2.E3.m1.1.1.3" xref="S2.E3.m1.1.1.3.cmml"><mrow id="S2.E3.m1.1.1.3.2" xref="S2.E3.m1.1.1.3.2.cmml"><mi id="S2.E3.m1.1.1.3.2.2" xref="S2.E3.m1.1.1.3.2.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.1.1.3.2.1" xref="S2.E3.m1.1.1.3.2.1.cmml">‚Äã</mo><mi id="S2.E3.m1.1.1.3.2.3" xref="S2.E3.m1.1.1.3.2.3.cmml">p</mi></mrow><mrow id="S2.E3.m1.1.1.3.3" xref="S2.E3.m1.1.1.3.3.cmml"><mrow id="S2.E3.m1.1.1.3.3.2" xref="S2.E3.m1.1.1.3.3.2.cmml"><mi id="S2.E3.m1.1.1.3.3.2.2" xref="S2.E3.m1.1.1.3.3.2.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.1.1.3.3.2.1" xref="S2.E3.m1.1.1.3.3.2.1.cmml">‚Äã</mo><mi id="S2.E3.m1.1.1.3.3.2.3" xref="S2.E3.m1.1.1.3.3.2.3.cmml">p</mi></mrow><mo id="S2.E3.m1.1.1.3.3.1" xref="S2.E3.m1.1.1.3.3.1.cmml">+</mo><mrow id="S2.E3.m1.1.1.3.3.3" xref="S2.E3.m1.1.1.3.3.3.cmml"><mi id="S2.E3.m1.1.1.3.3.3.2" xref="S2.E3.m1.1.1.3.3.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.1.1.3.3.3.1" xref="S2.E3.m1.1.1.3.3.3.1.cmml">‚Äã</mo><mi id="S2.E3.m1.1.1.3.3.3.3" xref="S2.E3.m1.1.1.3.3.3.3.cmml">p</mi></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.1b"><apply id="S2.E3.m1.1.1.cmml" xref="S2.E3.m1.1.1"><eq id="S2.E3.m1.1.1.1.cmml" xref="S2.E3.m1.1.1.1"></eq><ci id="S2.E3.m1.1.1.2.cmml" xref="S2.E3.m1.1.1.2">ùëù</ci><apply id="S2.E3.m1.1.1.3.cmml" xref="S2.E3.m1.1.1.3"><divide id="S2.E3.m1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.3"></divide><apply id="S2.E3.m1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.3.2"><times id="S2.E3.m1.1.1.3.2.1.cmml" xref="S2.E3.m1.1.1.3.2.1"></times><ci id="S2.E3.m1.1.1.3.2.2.cmml" xref="S2.E3.m1.1.1.3.2.2">ùë°</ci><ci id="S2.E3.m1.1.1.3.2.3.cmml" xref="S2.E3.m1.1.1.3.2.3">ùëù</ci></apply><apply id="S2.E3.m1.1.1.3.3.cmml" xref="S2.E3.m1.1.1.3.3"><plus id="S2.E3.m1.1.1.3.3.1.cmml" xref="S2.E3.m1.1.1.3.3.1"></plus><apply id="S2.E3.m1.1.1.3.3.2.cmml" xref="S2.E3.m1.1.1.3.3.2"><times id="S2.E3.m1.1.1.3.3.2.1.cmml" xref="S2.E3.m1.1.1.3.3.2.1"></times><ci id="S2.E3.m1.1.1.3.3.2.2.cmml" xref="S2.E3.m1.1.1.3.3.2.2">ùë°</ci><ci id="S2.E3.m1.1.1.3.3.2.3.cmml" xref="S2.E3.m1.1.1.3.3.2.3">ùëù</ci></apply><apply id="S2.E3.m1.1.1.3.3.3.cmml" xref="S2.E3.m1.1.1.3.3.3"><times id="S2.E3.m1.1.1.3.3.3.1.cmml" xref="S2.E3.m1.1.1.3.3.3.1"></times><ci id="S2.E3.m1.1.1.3.3.3.2.cmml" xref="S2.E3.m1.1.1.3.3.3.2">ùëì</ci><ci id="S2.E3.m1.1.1.3.3.3.3.cmml" xref="S2.E3.m1.1.1.3.3.3.3">ùëù</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.1c">p=\frac{tp}{tp+fp}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS5.p4" class="ltx_para">
<table id="S2.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S2.E4.m1.1" class="ltx_Math" alttext="r=\frac{tp}{tp+fn}" display="block"><semantics id="S2.E4.m1.1a"><mrow id="S2.E4.m1.1.1" xref="S2.E4.m1.1.1.cmml"><mi id="S2.E4.m1.1.1.2" xref="S2.E4.m1.1.1.2.cmml">r</mi><mo id="S2.E4.m1.1.1.1" xref="S2.E4.m1.1.1.1.cmml">=</mo><mfrac id="S2.E4.m1.1.1.3" xref="S2.E4.m1.1.1.3.cmml"><mrow id="S2.E4.m1.1.1.3.2" xref="S2.E4.m1.1.1.3.2.cmml"><mi id="S2.E4.m1.1.1.3.2.2" xref="S2.E4.m1.1.1.3.2.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.1.1.3.2.1" xref="S2.E4.m1.1.1.3.2.1.cmml">‚Äã</mo><mi id="S2.E4.m1.1.1.3.2.3" xref="S2.E4.m1.1.1.3.2.3.cmml">p</mi></mrow><mrow id="S2.E4.m1.1.1.3.3" xref="S2.E4.m1.1.1.3.3.cmml"><mrow id="S2.E4.m1.1.1.3.3.2" xref="S2.E4.m1.1.1.3.3.2.cmml"><mi id="S2.E4.m1.1.1.3.3.2.2" xref="S2.E4.m1.1.1.3.3.2.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.1.1.3.3.2.1" xref="S2.E4.m1.1.1.3.3.2.1.cmml">‚Äã</mo><mi id="S2.E4.m1.1.1.3.3.2.3" xref="S2.E4.m1.1.1.3.3.2.3.cmml">p</mi></mrow><mo id="S2.E4.m1.1.1.3.3.1" xref="S2.E4.m1.1.1.3.3.1.cmml">+</mo><mrow id="S2.E4.m1.1.1.3.3.3" xref="S2.E4.m1.1.1.3.3.3.cmml"><mi id="S2.E4.m1.1.1.3.3.3.2" xref="S2.E4.m1.1.1.3.3.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.1.1.3.3.3.1" xref="S2.E4.m1.1.1.3.3.3.1.cmml">‚Äã</mo><mi id="S2.E4.m1.1.1.3.3.3.3" xref="S2.E4.m1.1.1.3.3.3.3.cmml">n</mi></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.1b"><apply id="S2.E4.m1.1.1.cmml" xref="S2.E4.m1.1.1"><eq id="S2.E4.m1.1.1.1.cmml" xref="S2.E4.m1.1.1.1"></eq><ci id="S2.E4.m1.1.1.2.cmml" xref="S2.E4.m1.1.1.2">ùëü</ci><apply id="S2.E4.m1.1.1.3.cmml" xref="S2.E4.m1.1.1.3"><divide id="S2.E4.m1.1.1.3.1.cmml" xref="S2.E4.m1.1.1.3"></divide><apply id="S2.E4.m1.1.1.3.2.cmml" xref="S2.E4.m1.1.1.3.2"><times id="S2.E4.m1.1.1.3.2.1.cmml" xref="S2.E4.m1.1.1.3.2.1"></times><ci id="S2.E4.m1.1.1.3.2.2.cmml" xref="S2.E4.m1.1.1.3.2.2">ùë°</ci><ci id="S2.E4.m1.1.1.3.2.3.cmml" xref="S2.E4.m1.1.1.3.2.3">ùëù</ci></apply><apply id="S2.E4.m1.1.1.3.3.cmml" xref="S2.E4.m1.1.1.3.3"><plus id="S2.E4.m1.1.1.3.3.1.cmml" xref="S2.E4.m1.1.1.3.3.1"></plus><apply id="S2.E4.m1.1.1.3.3.2.cmml" xref="S2.E4.m1.1.1.3.3.2"><times id="S2.E4.m1.1.1.3.3.2.1.cmml" xref="S2.E4.m1.1.1.3.3.2.1"></times><ci id="S2.E4.m1.1.1.3.3.2.2.cmml" xref="S2.E4.m1.1.1.3.3.2.2">ùë°</ci><ci id="S2.E4.m1.1.1.3.3.2.3.cmml" xref="S2.E4.m1.1.1.3.3.2.3">ùëù</ci></apply><apply id="S2.E4.m1.1.1.3.3.3.cmml" xref="S2.E4.m1.1.1.3.3.3"><times id="S2.E4.m1.1.1.3.3.3.1.cmml" xref="S2.E4.m1.1.1.3.3.3.1"></times><ci id="S2.E4.m1.1.1.3.3.3.2.cmml" xref="S2.E4.m1.1.1.3.3.3.2">ùëì</ci><ci id="S2.E4.m1.1.1.3.3.3.3.cmml" xref="S2.E4.m1.1.1.3.3.3.3">ùëõ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.1c">r=\frac{tp}{tp+fn}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS5.p5" class="ltx_para">
<p id="S2.SS5.p5.5" class="ltx_p">with <math id="S2.SS5.p5.1.m1.1" class="ltx_Math" alttext="tp" display="inline"><semantics id="S2.SS5.p5.1.m1.1a"><mrow id="S2.SS5.p5.1.m1.1.1" xref="S2.SS5.p5.1.m1.1.1.cmml"><mi id="S2.SS5.p5.1.m1.1.1.2" xref="S2.SS5.p5.1.m1.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS5.p5.1.m1.1.1.1" xref="S2.SS5.p5.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S2.SS5.p5.1.m1.1.1.3" xref="S2.SS5.p5.1.m1.1.1.3.cmml">p</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS5.p5.1.m1.1b"><apply id="S2.SS5.p5.1.m1.1.1.cmml" xref="S2.SS5.p5.1.m1.1.1"><times id="S2.SS5.p5.1.m1.1.1.1.cmml" xref="S2.SS5.p5.1.m1.1.1.1"></times><ci id="S2.SS5.p5.1.m1.1.1.2.cmml" xref="S2.SS5.p5.1.m1.1.1.2">ùë°</ci><ci id="S2.SS5.p5.1.m1.1.1.3.cmml" xref="S2.SS5.p5.1.m1.1.1.3">ùëù</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p5.1.m1.1c">tp</annotation></semantics></math> as number of true positive results, <math id="S2.SS5.p5.2.m2.1" class="ltx_Math" alttext="fp" display="inline"><semantics id="S2.SS5.p5.2.m2.1a"><mrow id="S2.SS5.p5.2.m2.1.1" xref="S2.SS5.p5.2.m2.1.1.cmml"><mi id="S2.SS5.p5.2.m2.1.1.2" xref="S2.SS5.p5.2.m2.1.1.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.SS5.p5.2.m2.1.1.1" xref="S2.SS5.p5.2.m2.1.1.1.cmml">‚Äã</mo><mi id="S2.SS5.p5.2.m2.1.1.3" xref="S2.SS5.p5.2.m2.1.1.3.cmml">p</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS5.p5.2.m2.1b"><apply id="S2.SS5.p5.2.m2.1.1.cmml" xref="S2.SS5.p5.2.m2.1.1"><times id="S2.SS5.p5.2.m2.1.1.1.cmml" xref="S2.SS5.p5.2.m2.1.1.1"></times><ci id="S2.SS5.p5.2.m2.1.1.2.cmml" xref="S2.SS5.p5.2.m2.1.1.2">ùëì</ci><ci id="S2.SS5.p5.2.m2.1.1.3.cmml" xref="S2.SS5.p5.2.m2.1.1.3">ùëù</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p5.2.m2.1c">fp</annotation></semantics></math> as number of false positive results, <math id="S2.SS5.p5.3.m3.1" class="ltx_Math" alttext="fn" display="inline"><semantics id="S2.SS5.p5.3.m3.1a"><mrow id="S2.SS5.p5.3.m3.1.1" xref="S2.SS5.p5.3.m3.1.1.cmml"><mi id="S2.SS5.p5.3.m3.1.1.2" xref="S2.SS5.p5.3.m3.1.1.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.SS5.p5.3.m3.1.1.1" xref="S2.SS5.p5.3.m3.1.1.1.cmml">‚Äã</mo><mi id="S2.SS5.p5.3.m3.1.1.3" xref="S2.SS5.p5.3.m3.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS5.p5.3.m3.1b"><apply id="S2.SS5.p5.3.m3.1.1.cmml" xref="S2.SS5.p5.3.m3.1.1"><times id="S2.SS5.p5.3.m3.1.1.1.cmml" xref="S2.SS5.p5.3.m3.1.1.1"></times><ci id="S2.SS5.p5.3.m3.1.1.2.cmml" xref="S2.SS5.p5.3.m3.1.1.2">ùëì</ci><ci id="S2.SS5.p5.3.m3.1.1.3.cmml" xref="S2.SS5.p5.3.m3.1.1.3">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p5.3.m3.1c">fn</annotation></semantics></math> as number of false negative results, <math id="S2.SS5.p5.4.m4.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S2.SS5.p5.4.m4.1a"><mi id="S2.SS5.p5.4.m4.1.1" xref="S2.SS5.p5.4.m4.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S2.SS5.p5.4.m4.1b"><ci id="S2.SS5.p5.4.m4.1.1.cmml" xref="S2.SS5.p5.4.m4.1.1">ùëü</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p5.4.m4.1c">r</annotation></semantics></math> as recall and <math id="S2.SS5.p5.5.m5.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S2.SS5.p5.5.m5.1a"><mi id="S2.SS5.p5.5.m5.1.1" xref="S2.SS5.p5.5.m5.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S2.SS5.p5.5.m5.1b"><ci id="S2.SS5.p5.5.m5.1.1.cmml" xref="S2.SS5.p5.5.m5.1.1">ùëù</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p5.5.m5.1c">p</annotation></semantics></math> as precision.</p>
</div>
<div id="S2.SS5.p6" class="ltx_para">
<p id="S2.SS5.p6.2" class="ltx_p">To determine when an axle is considered correctly detected (<math id="S2.SS5.p6.1.m1.1" class="ltx_Math" alttext="TP" display="inline"><semantics id="S2.SS5.p6.1.m1.1a"><mrow id="S2.SS5.p6.1.m1.1.1" xref="S2.SS5.p6.1.m1.1.1.cmml"><mi id="S2.SS5.p6.1.m1.1.1.2" xref="S2.SS5.p6.1.m1.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S2.SS5.p6.1.m1.1.1.1" xref="S2.SS5.p6.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S2.SS5.p6.1.m1.1.1.3" xref="S2.SS5.p6.1.m1.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS5.p6.1.m1.1b"><apply id="S2.SS5.p6.1.m1.1.1.cmml" xref="S2.SS5.p6.1.m1.1.1"><times id="S2.SS5.p6.1.m1.1.1.1.cmml" xref="S2.SS5.p6.1.m1.1.1.1"></times><ci id="S2.SS5.p6.1.m1.1.1.2.cmml" xref="S2.SS5.p6.1.m1.1.1.2">ùëá</ci><ci id="S2.SS5.p6.1.m1.1.1.3.cmml" xref="S2.SS5.p6.1.m1.1.1.3">ùëÉ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p6.1.m1.1c">TP</annotation></semantics></math>), two spatial threshold values were defined. The first threshold of 200¬†cm corresponds to the minimum assumed distance between two axles, and the second threshold of 37¬†cm corresponds to the maximum expected error in label creation. Training was conducted for an arbitrary maximum of 300 epochs with a batch size of 16. Within an epoch, all training data is iterated through once. After three epochs without improvement in the <math id="S2.SS5.p6.2.m2.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S2.SS5.p6.2.m2.1a"><msub id="S2.SS5.p6.2.m2.1.1" xref="S2.SS5.p6.2.m2.1.1.cmml"><mi id="S2.SS5.p6.2.m2.1.1.2" xref="S2.SS5.p6.2.m2.1.1.2.cmml">F</mi><mn id="S2.SS5.p6.2.m2.1.1.3" xref="S2.SS5.p6.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS5.p6.2.m2.1b"><apply id="S2.SS5.p6.2.m2.1.1.cmml" xref="S2.SS5.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS5.p6.2.m2.1.1.1.cmml" xref="S2.SS5.p6.2.m2.1.1">subscript</csymbol><ci id="S2.SS5.p6.2.m2.1.1.2.cmml" xref="S2.SS5.p6.2.m2.1.1.2">ùêπ</ci><cn type="integer" id="S2.SS5.p6.2.m2.1.1.3.cmml" xref="S2.SS5.p6.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p6.2.m2.1c">F_{1}</annotation></semantics></math> score on the validation set, the learning rate was reduced by a factor of 0.3 (the default factor of 0.1 slowed down training to much), and the training was terminated after six epochs without improvement. For the hyperparameter study, each model was trained once instead of 5 times on the folds and the test data of the stratified splits.</p>
</div>
<div id="S2.SS5.p7" class="ltx_para">
<p id="S2.SS5.p7.3" class="ltx_p">The second metric to be determined is the mean absolute spatial error <math id="S2.SS5.p7.1.m1.1" class="ltx_Math" alttext="\bar{\Delta s}" display="inline"><semantics id="S2.SS5.p7.1.m1.1a"><mover accent="true" id="S2.SS5.p7.1.m1.1.1" xref="S2.SS5.p7.1.m1.1.1.cmml"><mrow id="S2.SS5.p7.1.m1.1.1.2" xref="S2.SS5.p7.1.m1.1.1.2.cmml"><mi mathvariant="normal" id="S2.SS5.p7.1.m1.1.1.2.2" xref="S2.SS5.p7.1.m1.1.1.2.2.cmml">Œî</mi><mo lspace="0em" rspace="0em" id="S2.SS5.p7.1.m1.1.1.2.1" xref="S2.SS5.p7.1.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S2.SS5.p7.1.m1.1.1.2.3" xref="S2.SS5.p7.1.m1.1.1.2.3.cmml">s</mi></mrow><mo id="S2.SS5.p7.1.m1.1.1.1" xref="S2.SS5.p7.1.m1.1.1.1.cmml">¬Ø</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS5.p7.1.m1.1b"><apply id="S2.SS5.p7.1.m1.1.1.cmml" xref="S2.SS5.p7.1.m1.1.1"><ci id="S2.SS5.p7.1.m1.1.1.1.cmml" xref="S2.SS5.p7.1.m1.1.1.1">¬Ø</ci><apply id="S2.SS5.p7.1.m1.1.1.2.cmml" xref="S2.SS5.p7.1.m1.1.1.2"><times id="S2.SS5.p7.1.m1.1.1.2.1.cmml" xref="S2.SS5.p7.1.m1.1.1.2.1"></times><ci id="S2.SS5.p7.1.m1.1.1.2.2.cmml" xref="S2.SS5.p7.1.m1.1.1.2.2">Œî</ci><ci id="S2.SS5.p7.1.m1.1.1.2.3.cmml" xref="S2.SS5.p7.1.m1.1.1.2.3">ùë†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p7.1.m1.1c">\bar{\Delta s}</annotation></semantics></math> between prediction and labeling. For this purpose, the temporal error is first determined in the form of the number of samples between the prediction <math id="S2.SS5.p7.2.m2.1" class="ltx_Math" alttext="\hat{t}" display="inline"><semantics id="S2.SS5.p7.2.m2.1a"><mover accent="true" id="S2.SS5.p7.2.m2.1.1" xref="S2.SS5.p7.2.m2.1.1.cmml"><mi id="S2.SS5.p7.2.m2.1.1.2" xref="S2.SS5.p7.2.m2.1.1.2.cmml">t</mi><mo id="S2.SS5.p7.2.m2.1.1.1" xref="S2.SS5.p7.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS5.p7.2.m2.1b"><apply id="S2.SS5.p7.2.m2.1.1.cmml" xref="S2.SS5.p7.2.m2.1.1"><ci id="S2.SS5.p7.2.m2.1.1.1.cmml" xref="S2.SS5.p7.2.m2.1.1.1">^</ci><ci id="S2.SS5.p7.2.m2.1.1.2.cmml" xref="S2.SS5.p7.2.m2.1.1.2">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p7.2.m2.1c">\hat{t}</annotation></semantics></math> and the label <math id="S2.SS5.p7.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.SS5.p7.3.m3.1a"><mi id="S2.SS5.p7.3.m3.1.1" xref="S2.SS5.p7.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS5.p7.3.m3.1b"><ci id="S2.SS5.p7.3.m3.1.1.cmml" xref="S2.SS5.p7.3.m3.1.1">ùë°</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p7.3.m3.1c">t</annotation></semantics></math> and this is then multiplied by the respective axle velocities:</p>
</div>
<div id="S2.SS5.p8" class="ltx_para">
<table id="S2.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S2.E5.m1.1" class="ltx_Math" alttext="\bar{\Delta s}=\frac{1}{n}\sum_{i=1}^{n}\lvert t_{i}-\hat{t}_{i}\rvert\times v_{i}" display="block"><semantics id="S2.E5.m1.1a"><mrow id="S2.E5.m1.1.1" xref="S2.E5.m1.1.1.cmml"><mover accent="true" id="S2.E5.m1.1.1.3" xref="S2.E5.m1.1.1.3.cmml"><mrow id="S2.E5.m1.1.1.3.2" xref="S2.E5.m1.1.1.3.2.cmml"><mi mathvariant="normal" id="S2.E5.m1.1.1.3.2.2" xref="S2.E5.m1.1.1.3.2.2.cmml">Œî</mi><mo lspace="0em" rspace="0em" id="S2.E5.m1.1.1.3.2.1" xref="S2.E5.m1.1.1.3.2.1.cmml">‚Äã</mo><mi id="S2.E5.m1.1.1.3.2.3" xref="S2.E5.m1.1.1.3.2.3.cmml">s</mi></mrow><mo id="S2.E5.m1.1.1.3.1" xref="S2.E5.m1.1.1.3.1.cmml">¬Ø</mo></mover><mo id="S2.E5.m1.1.1.2" xref="S2.E5.m1.1.1.2.cmml">=</mo><mrow id="S2.E5.m1.1.1.1" xref="S2.E5.m1.1.1.1.cmml"><mfrac id="S2.E5.m1.1.1.1.3" xref="S2.E5.m1.1.1.1.3.cmml"><mn id="S2.E5.m1.1.1.1.3.2" xref="S2.E5.m1.1.1.1.3.2.cmml">1</mn><mi id="S2.E5.m1.1.1.1.3.3" xref="S2.E5.m1.1.1.1.3.3.cmml">n</mi></mfrac><mo lspace="0em" rspace="0em" id="S2.E5.m1.1.1.1.2" xref="S2.E5.m1.1.1.1.2.cmml">‚Äã</mo><mrow id="S2.E5.m1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.cmml"><munderover id="S2.E5.m1.1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S2.E5.m1.1.1.1.1.2.2.2" xref="S2.E5.m1.1.1.1.1.2.2.2.cmml">‚àë</mo><mrow id="S2.E5.m1.1.1.1.1.2.2.3" xref="S2.E5.m1.1.1.1.1.2.2.3.cmml"><mi id="S2.E5.m1.1.1.1.1.2.2.3.2" xref="S2.E5.m1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S2.E5.m1.1.1.1.1.2.2.3.1" xref="S2.E5.m1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S2.E5.m1.1.1.1.1.2.2.3.3" xref="S2.E5.m1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S2.E5.m1.1.1.1.1.2.3" xref="S2.E5.m1.1.1.1.1.2.3.cmml">n</mi></munderover><mrow id="S2.E5.m1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.cmml"><mrow id="S2.E5.m1.1.1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E5.m1.1.1.1.1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.1.1.2.1.cmml">|</mo><mrow id="S2.E5.m1.1.1.1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.1.1.cmml"><msub id="S2.E5.m1.1.1.1.1.1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E5.m1.1.1.1.1.1.1.1.1.2.2" xref="S2.E5.m1.1.1.1.1.1.1.1.1.2.2.cmml">t</mi><mi id="S2.E5.m1.1.1.1.1.1.1.1.1.2.3" xref="S2.E5.m1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S2.E5.m1.1.1.1.1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.1.1.1.cmml">‚àí</mo><msub id="S2.E5.m1.1.1.1.1.1.1.1.1.3" xref="S2.E5.m1.1.1.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S2.E5.m1.1.1.1.1.1.1.1.1.3.2" xref="S2.E5.m1.1.1.1.1.1.1.1.1.3.2.cmml"><mi id="S2.E5.m1.1.1.1.1.1.1.1.1.3.2.2" xref="S2.E5.m1.1.1.1.1.1.1.1.1.3.2.2.cmml">t</mi><mo id="S2.E5.m1.1.1.1.1.1.1.1.1.3.2.1" xref="S2.E5.m1.1.1.1.1.1.1.1.1.3.2.1.cmml">^</mo></mover><mi id="S2.E5.m1.1.1.1.1.1.1.1.1.3.3" xref="S2.E5.m1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo rspace="0.055em" stretchy="false" id="S2.E5.m1.1.1.1.1.1.1.1.3" xref="S2.E5.m1.1.1.1.1.1.1.2.1.cmml">|</mo></mrow><mo rspace="0.222em" id="S2.E5.m1.1.1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.1.2.cmml">√ó</mo><msub id="S2.E5.m1.1.1.1.1.1.3" xref="S2.E5.m1.1.1.1.1.1.3.cmml"><mi id="S2.E5.m1.1.1.1.1.1.3.2" xref="S2.E5.m1.1.1.1.1.1.3.2.cmml">v</mi><mi id="S2.E5.m1.1.1.1.1.1.3.3" xref="S2.E5.m1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E5.m1.1b"><apply id="S2.E5.m1.1.1.cmml" xref="S2.E5.m1.1.1"><eq id="S2.E5.m1.1.1.2.cmml" xref="S2.E5.m1.1.1.2"></eq><apply id="S2.E5.m1.1.1.3.cmml" xref="S2.E5.m1.1.1.3"><ci id="S2.E5.m1.1.1.3.1.cmml" xref="S2.E5.m1.1.1.3.1">¬Ø</ci><apply id="S2.E5.m1.1.1.3.2.cmml" xref="S2.E5.m1.1.1.3.2"><times id="S2.E5.m1.1.1.3.2.1.cmml" xref="S2.E5.m1.1.1.3.2.1"></times><ci id="S2.E5.m1.1.1.3.2.2.cmml" xref="S2.E5.m1.1.1.3.2.2">Œî</ci><ci id="S2.E5.m1.1.1.3.2.3.cmml" xref="S2.E5.m1.1.1.3.2.3">ùë†</ci></apply></apply><apply id="S2.E5.m1.1.1.1.cmml" xref="S2.E5.m1.1.1.1"><times id="S2.E5.m1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.2"></times><apply id="S2.E5.m1.1.1.1.3.cmml" xref="S2.E5.m1.1.1.1.3"><divide id="S2.E5.m1.1.1.1.3.1.cmml" xref="S2.E5.m1.1.1.1.3"></divide><cn type="integer" id="S2.E5.m1.1.1.1.3.2.cmml" xref="S2.E5.m1.1.1.1.3.2">1</cn><ci id="S2.E5.m1.1.1.1.3.3.cmml" xref="S2.E5.m1.1.1.1.3.3">ùëõ</ci></apply><apply id="S2.E5.m1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1"><apply id="S2.E5.m1.1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.2.1.cmml" xref="S2.E5.m1.1.1.1.1.2">superscript</csymbol><apply id="S2.E5.m1.1.1.1.1.2.2.cmml" xref="S2.E5.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.2.2.1.cmml" xref="S2.E5.m1.1.1.1.1.2">subscript</csymbol><sum id="S2.E5.m1.1.1.1.1.2.2.2.cmml" xref="S2.E5.m1.1.1.1.1.2.2.2"></sum><apply id="S2.E5.m1.1.1.1.1.2.2.3.cmml" xref="S2.E5.m1.1.1.1.1.2.2.3"><eq id="S2.E5.m1.1.1.1.1.2.2.3.1.cmml" xref="S2.E5.m1.1.1.1.1.2.2.3.1"></eq><ci id="S2.E5.m1.1.1.1.1.2.2.3.2.cmml" xref="S2.E5.m1.1.1.1.1.2.2.3.2">ùëñ</ci><cn type="integer" id="S2.E5.m1.1.1.1.1.2.2.3.3.cmml" xref="S2.E5.m1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S2.E5.m1.1.1.1.1.2.3.cmml" xref="S2.E5.m1.1.1.1.1.2.3">ùëõ</ci></apply><apply id="S2.E5.m1.1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.1"><times id="S2.E5.m1.1.1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.1.1.2"></times><apply id="S2.E5.m1.1.1.1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1"><abs id="S2.E5.m1.1.1.1.1.1.1.2.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.2"></abs><apply id="S2.E5.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1"><minus id="S2.E5.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1.1"></minus><apply id="S2.E5.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.E5.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1.2.2">ùë°</ci><ci id="S2.E5.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1.2.3">ùëñ</ci></apply><apply id="S2.E5.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><apply id="S2.E5.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1.3.2"><ci id="S2.E5.m1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1.3.2.1">^</ci><ci id="S2.E5.m1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1.3.2.2">ùë°</ci></apply><ci id="S2.E5.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1.3.3">ùëñ</ci></apply></apply></apply><apply id="S2.E5.m1.1.1.1.1.1.3.cmml" xref="S2.E5.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.1.3.1.cmml" xref="S2.E5.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E5.m1.1.1.1.1.1.3.2.cmml" xref="S2.E5.m1.1.1.1.1.1.3.2">ùë£</ci><ci id="S2.E5.m1.1.1.1.1.1.3.3.cmml" xref="S2.E5.m1.1.1.1.1.1.3.3">ùëñ</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E5.m1.1c">\bar{\Delta s}=\frac{1}{n}\sum_{i=1}^{n}\lvert t_{i}-\hat{t}_{i}\rvert\times v_{i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS5.p9" class="ltx_para">
<p id="S2.SS5.p9.1" class="ltx_p">with n being the amount of axles, TE the temporal error and v the velocity.</p>
</div>
<div id="S2.SS5.p10" class="ltx_para">
<p id="S2.SS5.p10.2" class="ltx_p">The <math id="S2.SS5.p10.1.m1.1" class="ltx_Math" alttext="\bar{\Delta s}" display="inline"><semantics id="S2.SS5.p10.1.m1.1a"><mover accent="true" id="S2.SS5.p10.1.m1.1.1" xref="S2.SS5.p10.1.m1.1.1.cmml"><mrow id="S2.SS5.p10.1.m1.1.1.2" xref="S2.SS5.p10.1.m1.1.1.2.cmml"><mi mathvariant="normal" id="S2.SS5.p10.1.m1.1.1.2.2" xref="S2.SS5.p10.1.m1.1.1.2.2.cmml">Œî</mi><mo lspace="0em" rspace="0em" id="S2.SS5.p10.1.m1.1.1.2.1" xref="S2.SS5.p10.1.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S2.SS5.p10.1.m1.1.1.2.3" xref="S2.SS5.p10.1.m1.1.1.2.3.cmml">s</mi></mrow><mo id="S2.SS5.p10.1.m1.1.1.1" xref="S2.SS5.p10.1.m1.1.1.1.cmml">¬Ø</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS5.p10.1.m1.1b"><apply id="S2.SS5.p10.1.m1.1.1.cmml" xref="S2.SS5.p10.1.m1.1.1"><ci id="S2.SS5.p10.1.m1.1.1.1.cmml" xref="S2.SS5.p10.1.m1.1.1.1">¬Ø</ci><apply id="S2.SS5.p10.1.m1.1.1.2.cmml" xref="S2.SS5.p10.1.m1.1.1.2"><times id="S2.SS5.p10.1.m1.1.1.2.1.cmml" xref="S2.SS5.p10.1.m1.1.1.2.1"></times><ci id="S2.SS5.p10.1.m1.1.1.2.2.cmml" xref="S2.SS5.p10.1.m1.1.1.2.2">Œî</ci><ci id="S2.SS5.p10.1.m1.1.1.2.3.cmml" xref="S2.SS5.p10.1.m1.1.1.2.3">ùë†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p10.1.m1.1c">\bar{\Delta s}</annotation></semantics></math> is converted into mean spatial accuracy (MSA). Therefore, the second metric is transformed into the same scale like the <math id="S2.SS5.p10.2.m2.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S2.SS5.p10.2.m2.1a"><msub id="S2.SS5.p10.2.m2.1.1" xref="S2.SS5.p10.2.m2.1.1.cmml"><mi id="S2.SS5.p10.2.m2.1.1.2" xref="S2.SS5.p10.2.m2.1.1.2.cmml">F</mi><mn id="S2.SS5.p10.2.m2.1.1.3" xref="S2.SS5.p10.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS5.p10.2.m2.1b"><apply id="S2.SS5.p10.2.m2.1.1.cmml" xref="S2.SS5.p10.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS5.p10.2.m2.1.1.1.cmml" xref="S2.SS5.p10.2.m2.1.1">subscript</csymbol><ci id="S2.SS5.p10.2.m2.1.1.2.cmml" xref="S2.SS5.p10.2.m2.1.1.2">ùêπ</ci><cn type="integer" id="S2.SS5.p10.2.m2.1.1.3.cmml" xref="S2.SS5.p10.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p10.2.m2.1c">F_{1}</annotation></semantics></math> score and can be compared more intuitively:</p>
</div>
<div id="S2.SS5.p11" class="ltx_para">
<table id="S2.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S2.E6.m1.1" class="ltx_Math" alttext="MSA=\frac{2~{}m-\bar{\Delta s}}{2}\times 100" display="block"><semantics id="S2.E6.m1.1a"><mrow id="S2.E6.m1.1.1" xref="S2.E6.m1.1.1.cmml"><mrow id="S2.E6.m1.1.1.2" xref="S2.E6.m1.1.1.2.cmml"><mi id="S2.E6.m1.1.1.2.2" xref="S2.E6.m1.1.1.2.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S2.E6.m1.1.1.2.1" xref="S2.E6.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S2.E6.m1.1.1.2.3" xref="S2.E6.m1.1.1.2.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S2.E6.m1.1.1.2.1a" xref="S2.E6.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S2.E6.m1.1.1.2.4" xref="S2.E6.m1.1.1.2.4.cmml">A</mi></mrow><mo id="S2.E6.m1.1.1.1" xref="S2.E6.m1.1.1.1.cmml">=</mo><mrow id="S2.E6.m1.1.1.3" xref="S2.E6.m1.1.1.3.cmml"><mfrac id="S2.E6.m1.1.1.3.2" xref="S2.E6.m1.1.1.3.2.cmml"><mrow id="S2.E6.m1.1.1.3.2.2" xref="S2.E6.m1.1.1.3.2.2.cmml"><mrow id="S2.E6.m1.1.1.3.2.2.2" xref="S2.E6.m1.1.1.3.2.2.2.cmml"><mn id="S2.E6.m1.1.1.3.2.2.2.2" xref="S2.E6.m1.1.1.3.2.2.2.2.cmml">2</mn><mo lspace="0.330em" rspace="0em" id="S2.E6.m1.1.1.3.2.2.2.1" xref="S2.E6.m1.1.1.3.2.2.2.1.cmml">‚Äã</mo><mi id="S2.E6.m1.1.1.3.2.2.2.3" xref="S2.E6.m1.1.1.3.2.2.2.3.cmml">m</mi></mrow><mo id="S2.E6.m1.1.1.3.2.2.1" xref="S2.E6.m1.1.1.3.2.2.1.cmml">‚àí</mo><mover accent="true" id="S2.E6.m1.1.1.3.2.2.3" xref="S2.E6.m1.1.1.3.2.2.3.cmml"><mrow id="S2.E6.m1.1.1.3.2.2.3.2" xref="S2.E6.m1.1.1.3.2.2.3.2.cmml"><mi mathvariant="normal" id="S2.E6.m1.1.1.3.2.2.3.2.2" xref="S2.E6.m1.1.1.3.2.2.3.2.2.cmml">Œî</mi><mo lspace="0em" rspace="0em" id="S2.E6.m1.1.1.3.2.2.3.2.1" xref="S2.E6.m1.1.1.3.2.2.3.2.1.cmml">‚Äã</mo><mi id="S2.E6.m1.1.1.3.2.2.3.2.3" xref="S2.E6.m1.1.1.3.2.2.3.2.3.cmml">s</mi></mrow><mo id="S2.E6.m1.1.1.3.2.2.3.1" xref="S2.E6.m1.1.1.3.2.2.3.1.cmml">¬Ø</mo></mover></mrow><mn id="S2.E6.m1.1.1.3.2.3" xref="S2.E6.m1.1.1.3.2.3.cmml">2</mn></mfrac><mo lspace="0.222em" rspace="0.222em" id="S2.E6.m1.1.1.3.1" xref="S2.E6.m1.1.1.3.1.cmml">√ó</mo><mn id="S2.E6.m1.1.1.3.3" xref="S2.E6.m1.1.1.3.3.cmml">100</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E6.m1.1b"><apply id="S2.E6.m1.1.1.cmml" xref="S2.E6.m1.1.1"><eq id="S2.E6.m1.1.1.1.cmml" xref="S2.E6.m1.1.1.1"></eq><apply id="S2.E6.m1.1.1.2.cmml" xref="S2.E6.m1.1.1.2"><times id="S2.E6.m1.1.1.2.1.cmml" xref="S2.E6.m1.1.1.2.1"></times><ci id="S2.E6.m1.1.1.2.2.cmml" xref="S2.E6.m1.1.1.2.2">ùëÄ</ci><ci id="S2.E6.m1.1.1.2.3.cmml" xref="S2.E6.m1.1.1.2.3">ùëÜ</ci><ci id="S2.E6.m1.1.1.2.4.cmml" xref="S2.E6.m1.1.1.2.4">ùê¥</ci></apply><apply id="S2.E6.m1.1.1.3.cmml" xref="S2.E6.m1.1.1.3"><times id="S2.E6.m1.1.1.3.1.cmml" xref="S2.E6.m1.1.1.3.1"></times><apply id="S2.E6.m1.1.1.3.2.cmml" xref="S2.E6.m1.1.1.3.2"><divide id="S2.E6.m1.1.1.3.2.1.cmml" xref="S2.E6.m1.1.1.3.2"></divide><apply id="S2.E6.m1.1.1.3.2.2.cmml" xref="S2.E6.m1.1.1.3.2.2"><minus id="S2.E6.m1.1.1.3.2.2.1.cmml" xref="S2.E6.m1.1.1.3.2.2.1"></minus><apply id="S2.E6.m1.1.1.3.2.2.2.cmml" xref="S2.E6.m1.1.1.3.2.2.2"><times id="S2.E6.m1.1.1.3.2.2.2.1.cmml" xref="S2.E6.m1.1.1.3.2.2.2.1"></times><cn type="integer" id="S2.E6.m1.1.1.3.2.2.2.2.cmml" xref="S2.E6.m1.1.1.3.2.2.2.2">2</cn><ci id="S2.E6.m1.1.1.3.2.2.2.3.cmml" xref="S2.E6.m1.1.1.3.2.2.2.3">ùëö</ci></apply><apply id="S2.E6.m1.1.1.3.2.2.3.cmml" xref="S2.E6.m1.1.1.3.2.2.3"><ci id="S2.E6.m1.1.1.3.2.2.3.1.cmml" xref="S2.E6.m1.1.1.3.2.2.3.1">¬Ø</ci><apply id="S2.E6.m1.1.1.3.2.2.3.2.cmml" xref="S2.E6.m1.1.1.3.2.2.3.2"><times id="S2.E6.m1.1.1.3.2.2.3.2.1.cmml" xref="S2.E6.m1.1.1.3.2.2.3.2.1"></times><ci id="S2.E6.m1.1.1.3.2.2.3.2.2.cmml" xref="S2.E6.m1.1.1.3.2.2.3.2.2">Œî</ci><ci id="S2.E6.m1.1.1.3.2.2.3.2.3.cmml" xref="S2.E6.m1.1.1.3.2.2.3.2.3">ùë†</ci></apply></apply></apply><cn type="integer" id="S2.E6.m1.1.1.3.2.3.cmml" xref="S2.E6.m1.1.1.3.2.3">2</cn></apply><cn type="integer" id="S2.E6.m1.1.1.3.3.cmml" xref="S2.E6.m1.1.1.3.3">100</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E6.m1.1c">MSA=\frac{2~{}m-\bar{\Delta s}}{2}\times 100</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results and Discussion</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.2" class="ltx_p"><span id="S3.p1.2.1" class="ltx_text ltx_font_bold">Computational Efficiency:</span>
Generally using raw data instead of CWTs is a significant reduction in computation time and memory requirements. For the six CWTs with 16 scales each as proposed in <cite class="ltx_cite ltx_citemacro_citet">Lorenzen et¬†al. [<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, a 12-second traversal requires 1.4¬†seconds and about 5.7¬†MB of memory just for the transformation. In contrast, our approach takes only 0.022 seconds from raw data to prediction with about 61.1¬†KB memory usage. Therefore, when using raw data, inference becomes at least 65 times faster compared to using spectograms, while using only about 1¬†% of the memory for the input. In our case the axle detection for a train with 64 axles using 10 acceleration sensors would take <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="1.4~{}\mathrm{s}\times 10=140~{}\mathrm{s}" display="inline"><semantics id="S3.p1.1.m1.1a"><mrow id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml"><mrow id="S3.p1.1.m1.1.1.2" xref="S3.p1.1.m1.1.1.2.cmml"><mrow id="S3.p1.1.m1.1.1.2.2" xref="S3.p1.1.m1.1.1.2.2.cmml"><mn id="S3.p1.1.m1.1.1.2.2.2" xref="S3.p1.1.m1.1.1.2.2.2.cmml">1.4</mn><mo lspace="0.330em" rspace="0em" id="S3.p1.1.m1.1.1.2.2.1" xref="S3.p1.1.m1.1.1.2.2.1.cmml">‚Äã</mo><mi mathvariant="normal" id="S3.p1.1.m1.1.1.2.2.3" xref="S3.p1.1.m1.1.1.2.2.3.cmml">s</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S3.p1.1.m1.1.1.2.1" xref="S3.p1.1.m1.1.1.2.1.cmml">√ó</mo><mn id="S3.p1.1.m1.1.1.2.3" xref="S3.p1.1.m1.1.1.2.3.cmml">10</mn></mrow><mo id="S3.p1.1.m1.1.1.1" xref="S3.p1.1.m1.1.1.1.cmml">=</mo><mrow id="S3.p1.1.m1.1.1.3" xref="S3.p1.1.m1.1.1.3.cmml"><mn id="S3.p1.1.m1.1.1.3.2" xref="S3.p1.1.m1.1.1.3.2.cmml">140</mn><mo lspace="0.330em" rspace="0em" id="S3.p1.1.m1.1.1.3.1" xref="S3.p1.1.m1.1.1.3.1.cmml">‚Äã</mo><mi mathvariant="normal" id="S3.p1.1.m1.1.1.3.3" xref="S3.p1.1.m1.1.1.3.3.cmml">s</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><apply id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"><eq id="S3.p1.1.m1.1.1.1.cmml" xref="S3.p1.1.m1.1.1.1"></eq><apply id="S3.p1.1.m1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.2"><times id="S3.p1.1.m1.1.1.2.1.cmml" xref="S3.p1.1.m1.1.1.2.1"></times><apply id="S3.p1.1.m1.1.1.2.2.cmml" xref="S3.p1.1.m1.1.1.2.2"><times id="S3.p1.1.m1.1.1.2.2.1.cmml" xref="S3.p1.1.m1.1.1.2.2.1"></times><cn type="float" id="S3.p1.1.m1.1.1.2.2.2.cmml" xref="S3.p1.1.m1.1.1.2.2.2">1.4</cn><ci id="S3.p1.1.m1.1.1.2.2.3.cmml" xref="S3.p1.1.m1.1.1.2.2.3">s</ci></apply><cn type="integer" id="S3.p1.1.m1.1.1.2.3.cmml" xref="S3.p1.1.m1.1.1.2.3">10</cn></apply><apply id="S3.p1.1.m1.1.1.3.cmml" xref="S3.p1.1.m1.1.1.3"><times id="S3.p1.1.m1.1.1.3.1.cmml" xref="S3.p1.1.m1.1.1.3.1"></times><cn type="integer" id="S3.p1.1.m1.1.1.3.2.cmml" xref="S3.p1.1.m1.1.1.3.2">140</cn><ci id="S3.p1.1.m1.1.1.3.3.cmml" xref="S3.p1.1.m1.1.1.3.3">s</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">1.4~{}\mathrm{s}\times 10=140~{}\mathrm{s}</annotation></semantics></math> with VADERspec and <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="0.022~{}\mathrm{s}\times 10=0.22~{}\mathrm{s}" display="inline"><semantics id="S3.p1.2.m2.1a"><mrow id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml"><mrow id="S3.p1.2.m2.1.1.2" xref="S3.p1.2.m2.1.1.2.cmml"><mrow id="S3.p1.2.m2.1.1.2.2" xref="S3.p1.2.m2.1.1.2.2.cmml"><mn id="S3.p1.2.m2.1.1.2.2.2" xref="S3.p1.2.m2.1.1.2.2.2.cmml">0.022</mn><mo lspace="0.330em" rspace="0em" id="S3.p1.2.m2.1.1.2.2.1" xref="S3.p1.2.m2.1.1.2.2.1.cmml">‚Äã</mo><mi mathvariant="normal" id="S3.p1.2.m2.1.1.2.2.3" xref="S3.p1.2.m2.1.1.2.2.3.cmml">s</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S3.p1.2.m2.1.1.2.1" xref="S3.p1.2.m2.1.1.2.1.cmml">√ó</mo><mn id="S3.p1.2.m2.1.1.2.3" xref="S3.p1.2.m2.1.1.2.3.cmml">10</mn></mrow><mo id="S3.p1.2.m2.1.1.1" xref="S3.p1.2.m2.1.1.1.cmml">=</mo><mrow id="S3.p1.2.m2.1.1.3" xref="S3.p1.2.m2.1.1.3.cmml"><mn id="S3.p1.2.m2.1.1.3.2" xref="S3.p1.2.m2.1.1.3.2.cmml">0.22</mn><mo lspace="0.330em" rspace="0em" id="S3.p1.2.m2.1.1.3.1" xref="S3.p1.2.m2.1.1.3.1.cmml">‚Äã</mo><mi mathvariant="normal" id="S3.p1.2.m2.1.1.3.3" xref="S3.p1.2.m2.1.1.3.3.cmml">s</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><apply id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1"><eq id="S3.p1.2.m2.1.1.1.cmml" xref="S3.p1.2.m2.1.1.1"></eq><apply id="S3.p1.2.m2.1.1.2.cmml" xref="S3.p1.2.m2.1.1.2"><times id="S3.p1.2.m2.1.1.2.1.cmml" xref="S3.p1.2.m2.1.1.2.1"></times><apply id="S3.p1.2.m2.1.1.2.2.cmml" xref="S3.p1.2.m2.1.1.2.2"><times id="S3.p1.2.m2.1.1.2.2.1.cmml" xref="S3.p1.2.m2.1.1.2.2.1"></times><cn type="float" id="S3.p1.2.m2.1.1.2.2.2.cmml" xref="S3.p1.2.m2.1.1.2.2.2">0.022</cn><ci id="S3.p1.2.m2.1.1.2.2.3.cmml" xref="S3.p1.2.m2.1.1.2.2.3">s</ci></apply><cn type="integer" id="S3.p1.2.m2.1.1.2.3.cmml" xref="S3.p1.2.m2.1.1.2.3">10</cn></apply><apply id="S3.p1.2.m2.1.1.3.cmml" xref="S3.p1.2.m2.1.1.3"><times id="S3.p1.2.m2.1.1.3.1.cmml" xref="S3.p1.2.m2.1.1.3.1"></times><cn type="float" id="S3.p1.2.m2.1.1.3.2.cmml" xref="S3.p1.2.m2.1.1.3.2">0.22</cn><ci id="S3.p1.2.m2.1.1.3.3.cmml" xref="S3.p1.2.m2.1.1.3.3">s</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">0.022~{}\mathrm{s}\times 10=0.22~{}\mathrm{s}</annotation></semantics></math> with VADERraw. Even for more realistic scenarios with, e.g., three sensors VADERspec would take almost minutes while VADERraw takes under a second to detect the axles. This does not even include the other additional tracks and reduced computing power for on-site evaluation. With trains being a few minutes apart, the higher efficiency of VADERraw is essential for real-world application.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.4" class="ltx_p"><span id="S3.p2.4.1" class="ltx_text ltx_font_bold">Results of Hyperparameter Study:</span> VADERraw generally achieves increasingly better results with the size of the receptive field (fig.¬†<a href="#S3.F6" title="Figure 6 ‚Ä£ 3 Results and Discussion ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). For MSA (fig.¬†<a href="#S3.F6.sf2" title="In Figure 6 ‚Ä£ 3 Results and Discussion ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(b)</span></a>), the correlation is less clear compared to the <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.p2.1.m1.1a"><msub id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml"><mi id="S3.p2.1.m1.1.1.2" xref="S3.p2.1.m1.1.1.2.cmml">F</mi><mn id="S3.p2.1.m1.1.1.3" xref="S3.p2.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><apply id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p2.1.m1.1.1.1.cmml" xref="S3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.p2.1.m1.1.1.2.cmml" xref="S3.p2.1.m1.1.1.2">ùêπ</ci><cn type="integer" id="S3.p2.1.m1.1.1.3.cmml" xref="S3.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">F_{1}</annotation></semantics></math> score (fig.¬†<a href="#S3.F6.sf1" title="In Figure 6 ‚Ä£ 3 Results and Discussion ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(a)</span></a>), and the results are generally more scattered. This is because the U-Net architecture no longer works properly for certain hyperparameter combinations. In the event that kernel size <math id="S3.p2.2.m2.1" class="ltx_Math" alttext="\leq" display="inline"><semantics id="S3.p2.2.m2.1a"><mo id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">‚â§</mo><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><leq id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">\leq</annotation></semantics></math> max pooling size, the signal can no longer be processed correctly in the encoder path (or upsampling path) using transposed convolution. The max pooling size can be regarded as an upsampling factor in this context. With a max pooling size of three, the signal must therefore be increased to three times its length. This is achieved in this case by inserting two zeros between all input values. The signal is then filtered with the kernel. In this case, the kernel only ever stretches over one of the input values and two zeros. This means that the transposed convolution layer cannot interpolate between the input values. However, as soon as the kernel size is larger than the max pooling size, the kernel can always capture two values and therefore interpolate as expected. The influence is significantly greater with the MSA than with the <math id="S3.p2.3.m3.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.p2.3.m3.1a"><msub id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml"><mi id="S3.p2.3.m3.1.1.2" xref="S3.p2.3.m3.1.1.2.cmml">F</mi><mn id="S3.p2.3.m3.1.1.3" xref="S3.p2.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><apply id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p2.3.m3.1.1.1.cmml" xref="S3.p2.3.m3.1.1">subscript</csymbol><ci id="S3.p2.3.m3.1.1.2.cmml" xref="S3.p2.3.m3.1.1.2">ùêπ</ci><cn type="integer" id="S3.p2.3.m3.1.1.3.cmml" xref="S3.p2.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">F_{1}</annotation></semantics></math> score (fig.¬†<a href="#S3.F6.sf1" title="In Figure 6 ‚Ä£ 3 Results and Discussion ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(a)</span></a>). This could be explained because the results are shifted by three to five samples due to the lack of interpolation (depending on the max pooling size). However, this small shift is not relevant for <math id="S3.p2.4.m4.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.p2.4.m4.1a"><msub id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml"><mi id="S3.p2.4.m4.1.1.2" xref="S3.p2.4.m4.1.1.2.cmml">F</mi><mn id="S3.p2.4.m4.1.1.3" xref="S3.p2.4.m4.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><apply id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p2.4.m4.1.1.1.cmml" xref="S3.p2.4.m4.1.1">subscript</csymbol><ci id="S3.p2.4.m4.1.1.2.cmml" xref="S3.p2.4.m4.1.1.2">ùêπ</ci><cn type="integer" id="S3.p2.4.m4.1.1.3.cmml" xref="S3.p2.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">F_{1}</annotation></semantics></math> because it only measure, if an axle has been detected or not. In the MSA, however, every temporal and thus spatial error is directly included.</p>
</div>
<figure id="S3.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.01574/assets/images/f1_outlier.png" id="S3.F6.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="410" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.sf1.6.2.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><math id="S3.F6.sf1.3.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.F6.sf1.3.m1.1b"><msub id="S3.F6.sf1.3.m1.1.1" xref="S3.F6.sf1.3.m1.1.1.cmml"><mi mathsize="90%" id="S3.F6.sf1.3.m1.1.1.2" xref="S3.F6.sf1.3.m1.1.1.2.cmml">F</mi><mn mathsize="90%" id="S3.F6.sf1.3.m1.1.1.3" xref="S3.F6.sf1.3.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.F6.sf1.3.m1.1c"><apply id="S3.F6.sf1.3.m1.1.1.cmml" xref="S3.F6.sf1.3.m1.1.1"><csymbol cd="ambiguous" id="S3.F6.sf1.3.m1.1.1.1.cmml" xref="S3.F6.sf1.3.m1.1.1">subscript</csymbol><ci id="S3.F6.sf1.3.m1.1.1.2.cmml" xref="S3.F6.sf1.3.m1.1.1.2">ùêπ</ci><cn type="integer" id="S3.F6.sf1.3.m1.1.1.3.cmml" xref="S3.F6.sf1.3.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F6.sf1.3.m1.1d">F_{1}</annotation></semantics></math><span id="S3.F6.sf1.4.1" class="ltx_text" style="font-size:90%;"> scores with models as red dot where kernel size <math id="S3.F6.sf1.4.1.m1.1" class="ltx_Math" alttext="\leq" display="inline"><semantics id="S3.F6.sf1.4.1.m1.1b"><mo id="S3.F6.sf1.4.1.m1.1.1" xref="S3.F6.sf1.4.1.m1.1.1.cmml">‚â§</mo><annotation-xml encoding="MathML-Content" id="S3.F6.sf1.4.1.m1.1c"><leq id="S3.F6.sf1.4.1.m1.1.1.cmml" xref="S3.F6.sf1.4.1.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S3.F6.sf1.4.1.m1.1d">\leq</annotation></semantics></math> max pooling size.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.01574/assets/images/spatial_outlier.png" id="S3.F6.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="410" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.sf2.4.2.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F6.sf2.2.1" class="ltx_text" style="font-size:90%;">MSA with models as red dot where kernel size <math id="S3.F6.sf2.2.1.m1.1" class="ltx_Math" alttext="\leq" display="inline"><semantics id="S3.F6.sf2.2.1.m1.1b"><mo id="S3.F6.sf2.2.1.m1.1.1" xref="S3.F6.sf2.2.1.m1.1.1.cmml">‚â§</mo><annotation-xml encoding="MathML-Content" id="S3.F6.sf2.2.1.m1.1c"><leq id="S3.F6.sf2.2.1.m1.1.1.cmml" xref="S3.F6.sf2.2.1.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S3.F6.sf2.2.1.m1.1d">\leq</annotation></semantics></math> max pooling size.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F6.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.01574/assets/images/f1.png" id="S3.F6.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="410" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.sf3.4.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><math id="S3.F6.sf3.2.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.F6.sf3.2.m1.1b"><msub id="S3.F6.sf3.2.m1.1.1" xref="S3.F6.sf3.2.m1.1.1.cmml"><mi mathsize="90%" id="S3.F6.sf3.2.m1.1.1.2" xref="S3.F6.sf3.2.m1.1.1.2.cmml">F</mi><mn mathsize="90%" id="S3.F6.sf3.2.m1.1.1.3" xref="S3.F6.sf3.2.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.F6.sf3.2.m1.1c"><apply id="S3.F6.sf3.2.m1.1.1.cmml" xref="S3.F6.sf3.2.m1.1.1"><csymbol cd="ambiguous" id="S3.F6.sf3.2.m1.1.1.1.cmml" xref="S3.F6.sf3.2.m1.1.1">subscript</csymbol><ci id="S3.F6.sf3.2.m1.1.1.2.cmml" xref="S3.F6.sf3.2.m1.1.1.2">ùêπ</ci><cn type="integer" id="S3.F6.sf3.2.m1.1.1.3.cmml" xref="S3.F6.sf3.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F6.sf3.2.m1.1d">F_{1}</annotation></semantics></math><span id="S3.F6.sf3.5.2" class="ltx_text" style="font-size:90%;"> scores with only approved hyperparameter combinations.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F6.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.01574/assets/images/spatial.png" id="S3.F6.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="410" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S3.F6.sf4.3.2" class="ltx_text" style="font-size:90%;">MSA with only approved hyperparameter combinations.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.4.2.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S3.F6.2.1" class="ltx_text" style="font-size:90%;">Results of hyperparameter study: <math id="S3.F6.2.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.F6.2.1.m1.1b"><msub id="S3.F6.2.1.m1.1.1" xref="S3.F6.2.1.m1.1.1.cmml"><mi id="S3.F6.2.1.m1.1.1.2" xref="S3.F6.2.1.m1.1.1.2.cmml">F</mi><mn id="S3.F6.2.1.m1.1.1.3" xref="S3.F6.2.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.F6.2.1.m1.1c"><apply id="S3.F6.2.1.m1.1.1.cmml" xref="S3.F6.2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F6.2.1.m1.1.1.1.cmml" xref="S3.F6.2.1.m1.1.1">subscript</csymbol><ci id="S3.F6.2.1.m1.1.1.2.cmml" xref="S3.F6.2.1.m1.1.1.2">ùêπ</ci><cn type="integer" id="S3.F6.2.1.m1.1.1.3.cmml" xref="S3.F6.2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F6.2.1.m1.1d">F_{1}</annotation></semantics></math> scores and MSA in dependence of the maximum receptive field trained on DGPS splits.</span></figcaption>
</figure>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.3" class="ltx_p">All models with kernel size <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="\leq" display="inline"><semantics id="S3.p3.1.m1.1a"><mo id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">‚â§</mo><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><leq id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">\leq</annotation></semantics></math> max pooling size are therefore excluded from further analyses. With the remaining models, a clear correlation between receptive field size and result is evident for both <math id="S3.p3.2.m2.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.p3.2.m2.1a"><msub id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml"><mi id="S3.p3.2.m2.1.1.2" xref="S3.p3.2.m2.1.1.2.cmml">F</mi><mn id="S3.p3.2.m2.1.1.3" xref="S3.p3.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><apply id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p3.2.m2.1.1.1.cmml" xref="S3.p3.2.m2.1.1">subscript</csymbol><ci id="S3.p3.2.m2.1.1.2.cmml" xref="S3.p3.2.m2.1.1.2">ùêπ</ci><cn type="integer" id="S3.p3.2.m2.1.1.3.cmml" xref="S3.p3.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">F_{1}</annotation></semantics></math> score (fig.¬†<a href="#S3.F6.sf3" title="In Figure 6 ‚Ä£ 3 Results and Discussion ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(c)</span></a>) and MSA (fig.¬†<a href="#S3.F6.sf4" title="In Figure 6 ‚Ä£ 3 Results and Discussion ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(d)</span></a>). For the <math id="S3.p3.3.m3.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.p3.3.m3.1a"><msub id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml"><mi id="S3.p3.3.m3.1.1.2" xref="S3.p3.3.m3.1.1.2.cmml">F</mi><mn id="S3.p3.3.m3.1.1.3" xref="S3.p3.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><apply id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p3.3.m3.1.1.1.cmml" xref="S3.p3.3.m3.1.1">subscript</csymbol><ci id="S3.p3.3.m3.1.1.2.cmml" xref="S3.p3.3.m3.1.1.2">ùêπ</ci><cn type="integer" id="S3.p3.3.m3.1.1.3.cmml" xref="S3.p3.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">F_{1}</annotation></semantics></math> score, a disadvantage due to large receptive fields is not clearly recognisable. For MSA, on the other hand, a clear peak is recognisable at around 120 samples. For larger receptive fields, a plateau (stratified split) or a drop (DGPS split) in the results is evident. This could be explained by the larger max pooling size of the models with larger receptive fields, since it increasingly loses temporal information with size.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">To select the final hyperparameters for VADERraw, the harmonic mean was determined from the <math id="S3.p4.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.p4.1.m1.1a"><msub id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml"><mi id="S3.p4.1.m1.1.1.2" xref="S3.p4.1.m1.1.1.2.cmml">F</mi><mn id="S3.p4.1.m1.1.1.3" xref="S3.p4.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.1b"><apply id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p4.1.m1.1.1.1.cmml" xref="S3.p4.1.m1.1.1">subscript</csymbol><ci id="S3.p4.1.m1.1.1.2.cmml" xref="S3.p4.1.m1.1.1.2">ùêπ</ci><cn type="integer" id="S3.p4.1.m1.1.1.3.cmml" xref="S3.p4.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.1c">F_{1}</annotation></semantics></math> score and the MSA for both splits (fig.¬†<a href="#S3.F7" title="Figure 7 ‚Ä£ 3 Results and Discussion ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>). This is to ensure that the hyperparameters deliver good results in all metrics and scenarios. A closer look at the results shows that the best results are achieved with 4 pooling steps (fig.¬†<a href="#S3.F7.sf1" title="In Figure 7 ‚Ä£ 3 Results and Discussion ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7(a)</span></a>). This could be due to the fact that this increases the complexity of the model, as the data is additionally filtered twice (once in the encoder path and once in the decoder path) with each pooling step. The best results are achieved with a max pooling size of 3 (fig.¬†<a href="#S3.F7.sf2" title="In Figure 7 ‚Ä£ 3 Results and Discussion ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7(b)</span></a>). This could be because it allows a larger receptive field than a max pooling size of 2, but does not lose as much information as max pooling sizes of 4 or 5. The impact of the kernel size is the most ambiguous (fig.¬†<a href="#S3.F7.sf3" title="In Figure 7 ‚Ä£ 3 Results and Discussion ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7(c)</span></a>). There are slight tendencies towards medium kernel sizes of 7-15, but only extreme cases such as a kernel size of 27 can be ruled out.</p>
</div>
<figure id="S3.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F7.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.01574/assets/images/harmonic_pooling_steps.png" id="S3.F7.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="407" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F7.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F7.sf1.3.2" class="ltx_text" style="font-size:90%;">Importance of amount of pooling steps.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F7.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.01574/assets/images/harmonic_max_pooling.png" id="S3.F7.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="407" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F7.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F7.sf2.3.2" class="ltx_text" style="font-size:90%;">Importance of max pooling size. Only hyperparameter combinations with 4 pooling steps.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F7.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.01574/assets/images/harmonic_kernel_size.png" id="S3.F7.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="407" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F7.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S3.F7.sf3.3.2" class="ltx_text" style="font-size:90%;">Importance of kernel size. Only hyperparameter combinations with 4 pooling steps.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.F7.4" class="ltx_p ltx_figure_panel ltx_align_center">l</p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F7.5.2.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S3.F7.2.1" class="ltx_text" style="font-size:90%;">Harmonic mean of <math id="S3.F7.2.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.F7.2.1.m1.1b"><msub id="S3.F7.2.1.m1.1.1" xref="S3.F7.2.1.m1.1.1.cmml"><mi id="S3.F7.2.1.m1.1.1.2" xref="S3.F7.2.1.m1.1.1.2.cmml">F</mi><mn id="S3.F7.2.1.m1.1.1.3" xref="S3.F7.2.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.F7.2.1.m1.1c"><apply id="S3.F7.2.1.m1.1.1.cmml" xref="S3.F7.2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F7.2.1.m1.1.1.1.cmml" xref="S3.F7.2.1.m1.1.1">subscript</csymbol><ci id="S3.F7.2.1.m1.1.1.2.cmml" xref="S3.F7.2.1.m1.1.1.2">ùêπ</ci><cn type="integer" id="S3.F7.2.1.m1.1.1.3.cmml" xref="S3.F7.2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F7.2.1.m1.1d">F_{1}</annotation></semantics></math> score and MSA for both splits as a function of receptive field size.</span></figcaption>
</figure>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">For the final evaluation, the optimal hyperparameter combination with four pooling steps, max pooling size of three and a kernel size of nine, resulting in a receptive field size of 729 samples, is selected as the first VADERraw model. In order to test the hypothesis that the model must be able to map a maximum of 5¬†Hz with a load-dependent natural frequency of the bridge, the combination was selected that comes closest to this by changing only a single hyperparameter. The second VADERraw model thus differs only in the max pooling size with a value of two instead of three, yielding a receptive field size of 243 samples. From here on, the max pooling size is appended to the model name for traceability, so that the first model is called <span id="S3.p5.1.1" class="ltx_text ltx_font_bold">VADERraw3</span> and the second model <span id="S3.p5.1.2" class="ltx_text ltx_font_bold">VADERraw2</span>.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.2" class="ltx_p"><span id="S3.p6.2.1" class="ltx_text ltx_font_bold">Validation Results:</span>
The VADERraw models have similar results for both splits (fig.¬†<a href="#S3.F8" title="Figure 8 ‚Ä£ 3 Results and Discussion ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>). For <math id="S3.p6.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.p6.1.m1.1a"><msub id="S3.p6.1.m1.1.1" xref="S3.p6.1.m1.1.1.cmml"><mi id="S3.p6.1.m1.1.1.2" xref="S3.p6.1.m1.1.1.2.cmml">F</mi><mn id="S3.p6.1.m1.1.1.3" xref="S3.p6.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p6.1.m1.1b"><apply id="S3.p6.1.m1.1.1.cmml" xref="S3.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p6.1.m1.1.1.1.cmml" xref="S3.p6.1.m1.1.1">subscript</csymbol><ci id="S3.p6.1.m1.1.1.2.cmml" xref="S3.p6.1.m1.1.1.2">ùêπ</ci><cn type="integer" id="S3.p6.1.m1.1.1.3.cmml" xref="S3.p6.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.1.m1.1c">F_{1}</annotation></semantics></math> score, VADERraw2 is usually slightly worse than VADERraw3 for the training and validation data (fig.¬†<a href="#S3.F8.sf1" title="In Figure 8 ‚Ä£ 3 Results and Discussion ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8(a)</span></a>,¬†<a href="#S3.F8.sf3" title="In Figure 8 ‚Ä£ 3 Results and Discussion ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8(c)</span></a>). No clear differences are recognisable for the loss. VADERspec performs worse than the VADERraw models in all cases. It is particularly noticeable here that the loss of VADERspec on the training data is similar to that of the VADERraw models but significantly worse on the validation data (fig.¬†<a href="#S3.F8.sf2" title="In Figure 8 ‚Ä£ 3 Results and Discussion ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8(b)</span></a>,¬†<a href="#S3.F8.sf4" title="In Figure 8 ‚Ä£ 3 Results and Discussion ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8(d)</span></a>). Spectograms as input therefore appear to increase the risk of overfitting. A comparable tendency can also be seen in the <math id="S3.p6.2.m2.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.p6.2.m2.1a"><msub id="S3.p6.2.m2.1.1" xref="S3.p6.2.m2.1.1.cmml"><mi id="S3.p6.2.m2.1.1.2" xref="S3.p6.2.m2.1.1.2.cmml">F</mi><mn id="S3.p6.2.m2.1.1.3" xref="S3.p6.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p6.2.m2.1b"><apply id="S3.p6.2.m2.1.1.cmml" xref="S3.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p6.2.m2.1.1.1.cmml" xref="S3.p6.2.m2.1.1">subscript</csymbol><ci id="S3.p6.2.m2.1.1.2.cmml" xref="S3.p6.2.m2.1.1.2">ùêπ</ci><cn type="integer" id="S3.p6.2.m2.1.1.3.cmml" xref="S3.p6.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.2.m2.1c">F_{1}</annotation></semantics></math> score, where VADERspec would probably perform just as well on the training data with longer training, but performs significantly worse on the validation data than the VADERraw models.</p>
</div>
<figure id="S3.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F8.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.01574/assets/images/f1_vad_vader_single.png" id="S3.F8.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="409" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F8.sf1.4.2.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F8.sf1.2.1" class="ltx_text" style="font-size:90%;">DGPS splits: <math id="S3.F8.sf1.2.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.F8.sf1.2.1.m1.1b"><msub id="S3.F8.sf1.2.1.m1.1.1" xref="S3.F8.sf1.2.1.m1.1.1.cmml"><mi id="S3.F8.sf1.2.1.m1.1.1.2" xref="S3.F8.sf1.2.1.m1.1.1.2.cmml">F</mi><mn id="S3.F8.sf1.2.1.m1.1.1.3" xref="S3.F8.sf1.2.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.F8.sf1.2.1.m1.1c"><apply id="S3.F8.sf1.2.1.m1.1.1.cmml" xref="S3.F8.sf1.2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F8.sf1.2.1.m1.1.1.1.cmml" xref="S3.F8.sf1.2.1.m1.1.1">subscript</csymbol><ci id="S3.F8.sf1.2.1.m1.1.1.2.cmml" xref="S3.F8.sf1.2.1.m1.1.1.2">ùêπ</ci><cn type="integer" id="S3.F8.sf1.2.1.m1.1.1.3.cmml" xref="S3.F8.sf1.2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F8.sf1.2.1.m1.1d">F_{1}</annotation></semantics></math> scores in ¬†%.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F8.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.01574/assets/images/loss_vad_vader_single.png" id="S3.F8.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="396" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F8.sf2.4.2.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F8.sf2.2.1" class="ltx_text" style="font-size:90%;">DGPS splits: Binary Focal Loss with <math id="S3.F8.sf2.2.1.m1.1" class="ltx_Math" alttext="\gamma=2.5" display="inline"><semantics id="S3.F8.sf2.2.1.m1.1b"><mrow id="S3.F8.sf2.2.1.m1.1.1" xref="S3.F8.sf2.2.1.m1.1.1.cmml"><mi id="S3.F8.sf2.2.1.m1.1.1.2" xref="S3.F8.sf2.2.1.m1.1.1.2.cmml">Œ≥</mi><mo id="S3.F8.sf2.2.1.m1.1.1.1" xref="S3.F8.sf2.2.1.m1.1.1.1.cmml">=</mo><mn id="S3.F8.sf2.2.1.m1.1.1.3" xref="S3.F8.sf2.2.1.m1.1.1.3.cmml">2.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F8.sf2.2.1.m1.1c"><apply id="S3.F8.sf2.2.1.m1.1.1.cmml" xref="S3.F8.sf2.2.1.m1.1.1"><eq id="S3.F8.sf2.2.1.m1.1.1.1.cmml" xref="S3.F8.sf2.2.1.m1.1.1.1"></eq><ci id="S3.F8.sf2.2.1.m1.1.1.2.cmml" xref="S3.F8.sf2.2.1.m1.1.1.2">ùõæ</ci><cn type="float" id="S3.F8.sf2.2.1.m1.1.1.3.cmml" xref="S3.F8.sf2.2.1.m1.1.1.3">2.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F8.sf2.2.1.m1.1d">\gamma=2.5</annotation></semantics></math>.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F8.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.01574/assets/images/f1_vad_vader_all.png" id="S3.F8.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="409" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F8.sf3.4.2.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S3.F8.sf3.2.1" class="ltx_text" style="font-size:90%;">Stratified splits: <math id="S3.F8.sf3.2.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.F8.sf3.2.1.m1.1b"><msub id="S3.F8.sf3.2.1.m1.1.1" xref="S3.F8.sf3.2.1.m1.1.1.cmml"><mi id="S3.F8.sf3.2.1.m1.1.1.2" xref="S3.F8.sf3.2.1.m1.1.1.2.cmml">F</mi><mn id="S3.F8.sf3.2.1.m1.1.1.3" xref="S3.F8.sf3.2.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.F8.sf3.2.1.m1.1c"><apply id="S3.F8.sf3.2.1.m1.1.1.cmml" xref="S3.F8.sf3.2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F8.sf3.2.1.m1.1.1.1.cmml" xref="S3.F8.sf3.2.1.m1.1.1">subscript</csymbol><ci id="S3.F8.sf3.2.1.m1.1.1.2.cmml" xref="S3.F8.sf3.2.1.m1.1.1.2">ùêπ</ci><cn type="integer" id="S3.F8.sf3.2.1.m1.1.1.3.cmml" xref="S3.F8.sf3.2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F8.sf3.2.1.m1.1d">F_{1}</annotation></semantics></math> scores in ¬†%.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F8.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.01574/assets/images/loss_vad_vader_all.png" id="S3.F8.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F8.sf4.4.2.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S3.F8.sf4.2.1" class="ltx_text" style="font-size:90%;">Stratified splits: Binary Focal Loss with <math id="S3.F8.sf4.2.1.m1.1" class="ltx_Math" alttext="\gamma=2.5" display="inline"><semantics id="S3.F8.sf4.2.1.m1.1b"><mrow id="S3.F8.sf4.2.1.m1.1.1" xref="S3.F8.sf4.2.1.m1.1.1.cmml"><mi id="S3.F8.sf4.2.1.m1.1.1.2" xref="S3.F8.sf4.2.1.m1.1.1.2.cmml">Œ≥</mi><mo id="S3.F8.sf4.2.1.m1.1.1.1" xref="S3.F8.sf4.2.1.m1.1.1.1.cmml">=</mo><mn id="S3.F8.sf4.2.1.m1.1.1.3" xref="S3.F8.sf4.2.1.m1.1.1.3.cmml">2.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F8.sf4.2.1.m1.1c"><apply id="S3.F8.sf4.2.1.m1.1.1.cmml" xref="S3.F8.sf4.2.1.m1.1.1"><eq id="S3.F8.sf4.2.1.m1.1.1.1.cmml" xref="S3.F8.sf4.2.1.m1.1.1.1"></eq><ci id="S3.F8.sf4.2.1.m1.1.1.2.cmml" xref="S3.F8.sf4.2.1.m1.1.1.2">ùõæ</ci><cn type="float" id="S3.F8.sf4.2.1.m1.1.1.3.cmml" xref="S3.F8.sf4.2.1.m1.1.1.3">2.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F8.sf4.2.1.m1.1d">\gamma=2.5</annotation></semantics></math>.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S3.F8.3.2" class="ltx_text" style="font-size:90%;">Results on training and validation data per epoch with confidence interval with a coefficient of 95¬†% determined over the five folds.</span></figcaption>
</figure>
<figure id="S3.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F9.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.01574/assets/images/f1_2.png" id="S3.F9.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="410" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F9.sf1.4.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><math id="S3.F9.sf1.2.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.F9.sf1.2.m1.1b"><msub id="S3.F9.sf1.2.m1.1.1" xref="S3.F9.sf1.2.m1.1.1.cmml"><mi mathsize="90%" id="S3.F9.sf1.2.m1.1.1.2" xref="S3.F9.sf1.2.m1.1.1.2.cmml">F</mi><mn mathsize="90%" id="S3.F9.sf1.2.m1.1.1.3" xref="S3.F9.sf1.2.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.F9.sf1.2.m1.1c"><apply id="S3.F9.sf1.2.m1.1.1.cmml" xref="S3.F9.sf1.2.m1.1.1"><csymbol cd="ambiguous" id="S3.F9.sf1.2.m1.1.1.1.cmml" xref="S3.F9.sf1.2.m1.1.1">subscript</csymbol><ci id="S3.F9.sf1.2.m1.1.1.2.cmml" xref="S3.F9.sf1.2.m1.1.1.2">ùêπ</ci><cn type="integer" id="S3.F9.sf1.2.m1.1.1.3.cmml" xref="S3.F9.sf1.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F9.sf1.2.m1.1d">F_{1}</annotation></semantics></math><span id="S3.F9.sf1.5.2" class="ltx_text" style="font-size:90%;"> score</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F9.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.01574/assets/images/spatial_2.png" id="S3.F9.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="410" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F9.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F9.sf2.3.2" class="ltx_text" style="font-size:90%;">MSA</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F9.4.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><math id="S3.F9.2.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.F9.2.m1.1b"><msub id="S3.F9.2.m1.1.1" xref="S3.F9.2.m1.1.1.cmml"><mi mathsize="90%" id="S3.F9.2.m1.1.1.2" xref="S3.F9.2.m1.1.1.2.cmml">F</mi><mn mathsize="90%" id="S3.F9.2.m1.1.1.3" xref="S3.F9.2.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.F9.2.m1.1c"><apply id="S3.F9.2.m1.1.1.cmml" xref="S3.F9.2.m1.1.1"><csymbol cd="ambiguous" id="S3.F9.2.m1.1.1.1.cmml" xref="S3.F9.2.m1.1.1">subscript</csymbol><ci id="S3.F9.2.m1.1.1.2.cmml" xref="S3.F9.2.m1.1.1.2">ùêπ</ci><cn type="integer" id="S3.F9.2.m1.1.1.3.cmml" xref="S3.F9.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F9.2.m1.1d">F_{1}</annotation></semantics></math><span id="S3.F9.5.2" class="ltx_text" style="font-size:90%;"> score and MSA of the models for the two types of data split on the test sets. One data point was calculated per sensor, train and fold.</span></figcaption>
</figure>
<div id="S3.p7" class="ltx_para">
<p id="S3.p7.2" class="ltx_p">While the loss is calculated per sample and therefore only counts exact hits (exact time at which an axle passes the respective sensor), the <math id="S3.p7.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.p7.1.m1.1a"><msub id="S3.p7.1.m1.1.1" xref="S3.p7.1.m1.1.1.cmml"><mi id="S3.p7.1.m1.1.1.2" xref="S3.p7.1.m1.1.1.2.cmml">F</mi><mn id="S3.p7.1.m1.1.1.3" xref="S3.p7.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p7.1.m1.1b"><apply id="S3.p7.1.m1.1.1.cmml" xref="S3.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p7.1.m1.1.1.1.cmml" xref="S3.p7.1.m1.1.1">subscript</csymbol><ci id="S3.p7.1.m1.1.1.2.cmml" xref="S3.p7.1.m1.1.1.2">ùêπ</ci><cn type="integer" id="S3.p7.1.m1.1.1.3.cmml" xref="S3.p7.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.1.m1.1c">F_{1}</annotation></semantics></math> score is calculated per train axle. Thus, it can be interpreted that for all models and scenarios, the frequency of exact hits decreases again from about halfway through the training, but in return more axles are recognised. Therefore, depending on the boundary condition, either the loss or the <math id="S3.p7.2.m2.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.p7.2.m2.1a"><msub id="S3.p7.2.m2.1.1" xref="S3.p7.2.m2.1.1.cmml"><mi id="S3.p7.2.m2.1.1.2" xref="S3.p7.2.m2.1.1.2.cmml">F</mi><mn id="S3.p7.2.m2.1.1.3" xref="S3.p7.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p7.2.m2.1b"><apply id="S3.p7.2.m2.1.1.cmml" xref="S3.p7.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p7.2.m2.1.1.1.cmml" xref="S3.p7.2.m2.1.1">subscript</csymbol><ci id="S3.p7.2.m2.1.1.2.cmml" xref="S3.p7.2.m2.1.1.2">ùêπ</ci><cn type="integer" id="S3.p7.2.m2.1.1.3.cmml" xref="S3.p7.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.2.m2.1c">F_{1}</annotation></semantics></math> score can be selected as the criterion for terminating the training.</p>
</div>
<div id="S3.p8" class="ltx_para">
<p id="S3.p8.1" class="ltx_p"><span id="S3.p8.1.1" class="ltx_text ltx_font_bold">Test results: </span>
For the test results of the models, the learned parameters from the epoch with the best <math id="S3.p8.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.p8.1.m1.1a"><msub id="S3.p8.1.m1.1.1" xref="S3.p8.1.m1.1.1.cmml"><mi id="S3.p8.1.m1.1.1.2" xref="S3.p8.1.m1.1.1.2.cmml">F</mi><mn id="S3.p8.1.m1.1.1.3" xref="S3.p8.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p8.1.m1.1b"><apply id="S3.p8.1.m1.1.1.cmml" xref="S3.p8.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p8.1.m1.1.1.1.cmml" xref="S3.p8.1.m1.1.1">subscript</csymbol><ci id="S3.p8.1.m1.1.1.2.cmml" xref="S3.p8.1.m1.1.1.2">ùêπ</ci><cn type="integer" id="S3.p8.1.m1.1.1.3.cmml" xref="S3.p8.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p8.1.m1.1c">F_{1}</annotation></semantics></math> score on the validation set were chosen. Using these parameters, the models were then evaluated on the previously unseen test set. This procedure was repeated for all folds, splits (stratified and DGPS) and thresholds (200 cm and 37 cm) for each of the three models (VADERspec, VADERraw2 and VADERraw3). The VADERraw models achieve significantly higher accuracy for both threshold values (200¬†cm and 37¬†cm) and both splits (stratified and DGPS) (fig.¬†<a href="#S3.F9.sf1" title="In Figure 9 ‚Ä£ 3 Results and Discussion ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9(a)</span></a>,¬†<a href="#S3.F9.sf2" title="In Figure 9 ‚Ä£ 3 Results and Discussion ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9(b)</span></a>). In general, it can be observed that the accuracy of all models is considerably better for the stratified split compared to the DGPS split. The variance of the results is comparable for all models and as expected, the variance is generally higher for the more difficult DGPS scenario. VADERraw2 achieves the best results in all metrics for the DGPS splits while VADERraw3 achieves the best results for the stratified splits (tab.¬†<a href="#S3.T2" title="Table 2 ‚Ä£ 3 Results and Discussion ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). Since VADERraw3 has a significantly larger receptive field, it could be that the model learns entire train axle configurations, while VADERraw2 only learns individual axles or bogies. In this case, this would be advantageous for a representative data set (stratified splits), while it would increase the overfitting on the training data for non-representative data (DGPS splits). Generally VADERraw3 and VADERraw2 achieve similar results and only VADERspec performs significantly worse.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T2.2.2" class="ltx_tr">
<td id="S3.T2.2.2.3" class="ltx_td ltx_align_left ltx_border_tt">Type</td>
<td id="S3.T2.2.2.4" class="ltx_td ltx_align_left ltx_border_tt">Model</td>
<td id="S3.T2.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><math id="S3.T2.1.1.1.m1.1" class="ltx_Math" alttext="\bar{\Delta s}" display="inline"><semantics id="S3.T2.1.1.1.m1.1a"><mover accent="true" id="S3.T2.1.1.1.m1.1.1" xref="S3.T2.1.1.1.m1.1.1.cmml"><mrow id="S3.T2.1.1.1.m1.1.1.2" xref="S3.T2.1.1.1.m1.1.1.2.cmml"><mi mathvariant="normal" id="S3.T2.1.1.1.m1.1.1.2.2" xref="S3.T2.1.1.1.m1.1.1.2.2.cmml">Œî</mi><mo lspace="0em" rspace="0em" id="S3.T2.1.1.1.m1.1.1.2.1" xref="S3.T2.1.1.1.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S3.T2.1.1.1.m1.1.1.2.3" xref="S3.T2.1.1.1.m1.1.1.2.3.cmml">s</mi></mrow><mo id="S3.T2.1.1.1.m1.1.1.1" xref="S3.T2.1.1.1.m1.1.1.1.cmml">¬Ø</mo></mover><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.m1.1b"><apply id="S3.T2.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.m1.1.1"><ci id="S3.T2.1.1.1.m1.1.1.1.cmml" xref="S3.T2.1.1.1.m1.1.1.1">¬Ø</ci><apply id="S3.T2.1.1.1.m1.1.1.2.cmml" xref="S3.T2.1.1.1.m1.1.1.2"><times id="S3.T2.1.1.1.m1.1.1.2.1.cmml" xref="S3.T2.1.1.1.m1.1.1.2.1"></times><ci id="S3.T2.1.1.1.m1.1.1.2.2.cmml" xref="S3.T2.1.1.1.m1.1.1.2.2">Œî</ci><ci id="S3.T2.1.1.1.m1.1.1.2.3.cmml" xref="S3.T2.1.1.1.m1.1.1.2.3">ùë†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.m1.1c">\bar{\Delta s}</annotation></semantics></math></td>
<td id="S3.T2.2.2.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><math id="S3.T2.2.2.2.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.T2.2.2.2.m1.1a"><msub id="S3.T2.2.2.2.m1.1.1" xref="S3.T2.2.2.2.m1.1.1.cmml"><mi id="S3.T2.2.2.2.m1.1.1.2" xref="S3.T2.2.2.2.m1.1.1.2.cmml">F</mi><mn id="S3.T2.2.2.2.m1.1.1.3" xref="S3.T2.2.2.2.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.2.m1.1b"><apply id="S3.T2.2.2.2.m1.1.1.cmml" xref="S3.T2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S3.T2.2.2.2.m1.1.1.1.cmml" xref="S3.T2.2.2.2.m1.1.1">subscript</csymbol><ci id="S3.T2.2.2.2.m1.1.1.2.cmml" xref="S3.T2.2.2.2.m1.1.1.2">ùêπ</ci><cn type="integer" id="S3.T2.2.2.2.m1.1.1.3.cmml" xref="S3.T2.2.2.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.2.m1.1c">F_{1}</annotation></semantics></math></td>
</tr>
<tr id="S3.T2.2.3" class="ltx_tr">
<td id="S3.T2.2.3.1" class="ltx_td"></td>
<td id="S3.T2.2.3.2" class="ltx_td"></td>
<td id="S3.T2.2.3.3" class="ltx_td"></td>
<td id="S3.T2.2.3.4" class="ltx_td ltx_align_left">200¬†cm</td>
<td id="S3.T2.2.3.5" class="ltx_td ltx_align_left">37¬†cm</td>
</tr>
<tr id="S3.T2.2.4" class="ltx_tr">
<td id="S3.T2.2.4.1" class="ltx_td ltx_align_left ltx_border_t">DGPS</td>
<td id="S3.T2.2.4.2" class="ltx_td ltx_align_left ltx_border_t">VADERspec</td>
<td id="S3.T2.2.4.3" class="ltx_td ltx_align_left ltx_border_t">21.5¬†cm</td>
<td id="S3.T2.2.4.4" class="ltx_td ltx_align_left ltx_border_t">92.6¬†%</td>
<td id="S3.T2.2.4.5" class="ltx_td ltx_align_left ltx_border_t">75.6¬†%</td>
</tr>
<tr id="S3.T2.2.5" class="ltx_tr">
<td id="S3.T2.2.5.1" class="ltx_td"></td>
<td id="S3.T2.2.5.2" class="ltx_td ltx_align_left">VADERraw2</td>
<td id="S3.T2.2.5.3" class="ltx_td ltx_align_left">
<span id="S3.T2.2.5.3.1" class="ltx_text ltx_font_bold">16.1</span>¬†cm</td>
<td id="S3.T2.2.5.4" class="ltx_td ltx_align_left">
<span id="S3.T2.2.5.4.1" class="ltx_text ltx_font_bold">96.6</span>¬†%</td>
<td id="S3.T2.2.5.5" class="ltx_td ltx_align_left">
<span id="S3.T2.2.5.5.1" class="ltx_text ltx_font_bold">83.4</span>¬†%</td>
</tr>
<tr id="S3.T2.2.6" class="ltx_tr">
<td id="S3.T2.2.6.1" class="ltx_td"></td>
<td id="S3.T2.2.6.2" class="ltx_td ltx_align_left">VADERraw3</td>
<td id="S3.T2.2.6.3" class="ltx_td ltx_align_left">17.3¬†cm</td>
<td id="S3.T2.2.6.4" class="ltx_td ltx_align_left">
<span id="S3.T2.2.6.4.1" class="ltx_text ltx_font_bold">96.6</span>¬†%</td>
<td id="S3.T2.2.6.5" class="ltx_td ltx_align_left">82.7¬†%</td>
</tr>
<tr id="S3.T2.2.7" class="ltx_tr">
<td id="S3.T2.2.7.1" class="ltx_td ltx_align_left">stratified</td>
<td id="S3.T2.2.7.2" class="ltx_td ltx_align_left">VADERspec</td>
<td id="S3.T2.2.7.3" class="ltx_td ltx_align_left">6.95¬†cm</td>
<td id="S3.T2.2.7.4" class="ltx_td ltx_align_left">98.2¬†%</td>
<td id="S3.T2.2.7.5" class="ltx_td ltx_align_left">94.3¬†%</td>
</tr>
<tr id="S3.T2.2.8" class="ltx_tr">
<td id="S3.T2.2.8.1" class="ltx_td"></td>
<td id="S3.T2.2.8.2" class="ltx_td ltx_align_left">VADERraw2</td>
<td id="S3.T2.2.8.3" class="ltx_td ltx_align_left">5.36¬†cm</td>
<td id="S3.T2.2.8.4" class="ltx_td ltx_align_left">99.2¬†%</td>
<td id="S3.T2.2.8.5" class="ltx_td ltx_align_left">96.5¬†%</td>
</tr>
<tr id="S3.T2.2.9" class="ltx_tr">
<td id="S3.T2.2.9.1" class="ltx_td ltx_border_bb"></td>
<td id="S3.T2.2.9.2" class="ltx_td ltx_align_left ltx_border_bb">VADERraw3</td>
<td id="S3.T2.2.9.3" class="ltx_td ltx_align_left ltx_border_bb">
<span id="S3.T2.2.9.3.1" class="ltx_text ltx_font_bold">5.21</span>¬†cm</td>
<td id="S3.T2.2.9.4" class="ltx_td ltx_align_left ltx_border_bb">
<span id="S3.T2.2.9.4.1" class="ltx_text ltx_font_bold">99.5</span>¬†%</td>
<td id="S3.T2.2.9.5" class="ltx_td ltx_align_left ltx_border_bb">
<span id="S3.T2.2.9.5.1" class="ltx_text ltx_font_bold">96.8</span>¬†%</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.8.2.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><math id="S3.T2.5.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.T2.5.m1.1b"><msub id="S3.T2.5.m1.1.1" xref="S3.T2.5.m1.1.1.cmml"><mi mathsize="90%" id="S3.T2.5.m1.1.1.2" xref="S3.T2.5.m1.1.1.2.cmml">F</mi><mn mathsize="90%" id="S3.T2.5.m1.1.1.3" xref="S3.T2.5.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.T2.5.m1.1c"><apply id="S3.T2.5.m1.1.1.cmml" xref="S3.T2.5.m1.1.1"><csymbol cd="ambiguous" id="S3.T2.5.m1.1.1.1.cmml" xref="S3.T2.5.m1.1.1">subscript</csymbol><ci id="S3.T2.5.m1.1.1.2.cmml" xref="S3.T2.5.m1.1.1.2">ùêπ</ci><cn type="integer" id="S3.T2.5.m1.1.1.3.cmml" xref="S3.T2.5.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.5.m1.1d">F_{1}</annotation></semantics></math><span id="S3.T2.6.1" class="ltx_text" style="font-size:90%;"> score and <math id="S3.T2.6.1.m1.1" class="ltx_Math" alttext="\bar{\Delta s}" display="inline"><semantics id="S3.T2.6.1.m1.1b"><mover accent="true" id="S3.T2.6.1.m1.1.1" xref="S3.T2.6.1.m1.1.1.cmml"><mrow id="S3.T2.6.1.m1.1.1.2" xref="S3.T2.6.1.m1.1.1.2.cmml"><mi mathvariant="normal" id="S3.T2.6.1.m1.1.1.2.2" xref="S3.T2.6.1.m1.1.1.2.2.cmml">Œî</mi><mo lspace="0em" rspace="0em" id="S3.T2.6.1.m1.1.1.2.1" xref="S3.T2.6.1.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S3.T2.6.1.m1.1.1.2.3" xref="S3.T2.6.1.m1.1.1.2.3.cmml">s</mi></mrow><mo id="S3.T2.6.1.m1.1.1.1" xref="S3.T2.6.1.m1.1.1.1.cmml">¬Ø</mo></mover><annotation-xml encoding="MathML-Content" id="S3.T2.6.1.m1.1c"><apply id="S3.T2.6.1.m1.1.1.cmml" xref="S3.T2.6.1.m1.1.1"><ci id="S3.T2.6.1.m1.1.1.1.cmml" xref="S3.T2.6.1.m1.1.1.1">¬Ø</ci><apply id="S3.T2.6.1.m1.1.1.2.cmml" xref="S3.T2.6.1.m1.1.1.2"><times id="S3.T2.6.1.m1.1.1.2.1.cmml" xref="S3.T2.6.1.m1.1.1.2.1"></times><ci id="S3.T2.6.1.m1.1.1.2.2.cmml" xref="S3.T2.6.1.m1.1.1.2.2">Œî</ci><ci id="S3.T2.6.1.m1.1.1.2.3.cmml" xref="S3.T2.6.1.m1.1.1.2.3">ùë†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.6.1.m1.1d">\bar{\Delta s}</annotation></semantics></math> calculated for all samples of the corresponding folds.</span></figcaption>
</figure>
<div id="S3.p9" class="ltx_para">
<p id="S3.p9.1" class="ltx_p"><span id="S3.p9.1.1" class="ltx_text ltx_font_bold">Sensor dependent test results: </span>
It was previously observed that one of the sensors of the measurement campaign was degraded <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. To investigate the influence of sensor degradation for each model and split more closely, the <math id="S3.p9.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.p9.1.m1.1a"><msub id="S3.p9.1.m1.1.1" xref="S3.p9.1.m1.1.1.cmml"><mi id="S3.p9.1.m1.1.1.2" xref="S3.p9.1.m1.1.1.2.cmml">F</mi><mn id="S3.p9.1.m1.1.1.3" xref="S3.p9.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p9.1.m1.1b"><apply id="S3.p9.1.m1.1.1.cmml" xref="S3.p9.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p9.1.m1.1.1.1.cmml" xref="S3.p9.1.m1.1.1">subscript</csymbol><ci id="S3.p9.1.m1.1.1.2.cmml" xref="S3.p9.1.m1.1.1.2">ùêπ</ci><cn type="integer" id="S3.p9.1.m1.1.1.3.cmml" xref="S3.p9.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.1.m1.1c">F_{1}</annotation></semantics></math> score for both threshold values and both splits (fig.¬†<a href="#S3.F10.sf1" title="In Figure 10 ‚Ä£ 3 Results and Discussion ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10(a)</span></a> to <a href="#S3.F10.sf4" title="In Figure 10 ‚Ä£ 3 Results and Discussion ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10(d)</span></a>), and the MSA for both splits with a threshold value of 200¬†cm (fig.¬†<a href="#S3.F10.sf5" title="In Figure 10 ‚Ä£ 3 Results and Discussion ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10(e)</span></a>, <a href="#S3.F10.sf6" title="In Figure 10 ‚Ä£ 3 Results and Discussion ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10(f)</span></a>) were examined per sensor. Generally, VADERraw performs better than VADERspec in all quantiles across all plots. It is noteworthy that, for the DGPS splits, the difference from sensor R3 to the other sensors is smaller.</p>
</div>
<div id="S3.p10" class="ltx_para">
<p id="S3.p10.1" class="ltx_p">In general, it is clearly visible that worse results are achieved on sensor R3 (fig.¬†<a href="#S3.F10" title="Figure 10 ‚Ä£ 3 Results and Discussion ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>). The difference is most striking for the stratified splits, because in the majority of cases even the lower quartile is 100¬†% and the results only scatter widely for sensor R3. For the DGPS splits, the results are more scattered for all senors, but there are still large differences, especially in the median. it is also noticeable that VADERspec consistently performs worst and the VADERraw models are close to each other. For sensor R3, VADERraw3 almost always performs better than VADERraw2, but due to the high dispersion of the results, this is not necessarily due to the model. Evaluating the results again, excluding sensor R3, VADERraw2 now performs best in all metrics and scenarios (tab.¬†<a href="#S3.T3" title="Table 3 ‚Ä£ 3 Results and Discussion ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. At a threshold of 200¬†cm, 99.9¬†% of the axles are now recognized with a <math id="S3.p10.1.m1.1" class="ltx_Math" alttext="\bar{\Delta s}" display="inline"><semantics id="S3.p10.1.m1.1a"><mover accent="true" id="S3.p10.1.m1.1.1" xref="S3.p10.1.m1.1.1.cmml"><mrow id="S3.p10.1.m1.1.1.2" xref="S3.p10.1.m1.1.1.2.cmml"><mi mathvariant="normal" id="S3.p10.1.m1.1.1.2.2" xref="S3.p10.1.m1.1.1.2.2.cmml">Œî</mi><mo lspace="0em" rspace="0em" id="S3.p10.1.m1.1.1.2.1" xref="S3.p10.1.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S3.p10.1.m1.1.1.2.3" xref="S3.p10.1.m1.1.1.2.3.cmml">s</mi></mrow><mo id="S3.p10.1.m1.1.1.1" xref="S3.p10.1.m1.1.1.1.cmml">¬Ø</mo></mover><annotation-xml encoding="MathML-Content" id="S3.p10.1.m1.1b"><apply id="S3.p10.1.m1.1.1.cmml" xref="S3.p10.1.m1.1.1"><ci id="S3.p10.1.m1.1.1.1.cmml" xref="S3.p10.1.m1.1.1.1">¬Ø</ci><apply id="S3.p10.1.m1.1.1.2.cmml" xref="S3.p10.1.m1.1.1.2"><times id="S3.p10.1.m1.1.1.2.1.cmml" xref="S3.p10.1.m1.1.1.2.1"></times><ci id="S3.p10.1.m1.1.1.2.2.cmml" xref="S3.p10.1.m1.1.1.2.2">Œî</ci><ci id="S3.p10.1.m1.1.1.2.3.cmml" xref="S3.p10.1.m1.1.1.2.3">ùë†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p10.1.m1.1c">\bar{\Delta s}</annotation></semantics></math> of only 3.69¬†cm. Therefore, the RF rule seems to be confirmed for fully functioning sensors. In the case of sensor degradation, larger receptive fields appear to have some advantages (tab.¬†<a href="#S3.T2" title="Table 2 ‚Ä£ 3 Results and Discussion ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), although the differences here are dependent on the scenario.</p>
</div>
<figure id="S3.F10" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F10.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2309.01574/assets/images/SensorAllTrains2.png" id="S3.F10.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="409" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F10.sf1.4.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><math id="S3.F10.sf1.2.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.F10.sf1.2.m1.1b"><msub id="S3.F10.sf1.2.m1.1.1" xref="S3.F10.sf1.2.m1.1.1.cmml"><mi mathsize="90%" id="S3.F10.sf1.2.m1.1.1.2" xref="S3.F10.sf1.2.m1.1.1.2.cmml">F</mi><mn mathsize="90%" id="S3.F10.sf1.2.m1.1.1.3" xref="S3.F10.sf1.2.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.F10.sf1.2.m1.1c"><apply id="S3.F10.sf1.2.m1.1.1.cmml" xref="S3.F10.sf1.2.m1.1.1"><csymbol cd="ambiguous" id="S3.F10.sf1.2.m1.1.1.1.cmml" xref="S3.F10.sf1.2.m1.1.1">subscript</csymbol><ci id="S3.F10.sf1.2.m1.1.1.2.cmml" xref="S3.F10.sf1.2.m1.1.1.2">ùêπ</ci><cn type="integer" id="S3.F10.sf1.2.m1.1.1.3.cmml" xref="S3.F10.sf1.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F10.sf1.2.m1.1d">F_{1}</annotation></semantics></math><span id="S3.F10.sf1.5.2" class="ltx_text" style="font-size:90%;"> score with a threshold of 200 cm for each sensor trained with the stratified splits.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F10.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2309.01574/assets/images/SensorSingleTrain2.png" id="S3.F10.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="409" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F10.sf2.4.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><math id="S3.F10.sf2.2.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.F10.sf2.2.m1.1b"><msub id="S3.F10.sf2.2.m1.1.1" xref="S3.F10.sf2.2.m1.1.1.cmml"><mi mathsize="90%" id="S3.F10.sf2.2.m1.1.1.2" xref="S3.F10.sf2.2.m1.1.1.2.cmml">F</mi><mn mathsize="90%" id="S3.F10.sf2.2.m1.1.1.3" xref="S3.F10.sf2.2.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.F10.sf2.2.m1.1c"><apply id="S3.F10.sf2.2.m1.1.1.cmml" xref="S3.F10.sf2.2.m1.1.1"><csymbol cd="ambiguous" id="S3.F10.sf2.2.m1.1.1.1.cmml" xref="S3.F10.sf2.2.m1.1.1">subscript</csymbol><ci id="S3.F10.sf2.2.m1.1.1.2.cmml" xref="S3.F10.sf2.2.m1.1.1.2">ùêπ</ci><cn type="integer" id="S3.F10.sf2.2.m1.1.1.3.cmml" xref="S3.F10.sf2.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F10.sf2.2.m1.1d">F_{1}</annotation></semantics></math><span id="S3.F10.sf2.5.2" class="ltx_text" style="font-size:90%;"> score with a threshold of 200 cm for each sensor trained with the DGPS splits.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F10.sf3" class="ltx_figure ltx_figure_panel"><img src="/html/2309.01574/assets/images/SensorAllTrains37.png" id="S3.F10.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="409" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F10.sf3.4.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><math id="S3.F10.sf3.2.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.F10.sf3.2.m1.1b"><msub id="S3.F10.sf3.2.m1.1.1" xref="S3.F10.sf3.2.m1.1.1.cmml"><mi mathsize="90%" id="S3.F10.sf3.2.m1.1.1.2" xref="S3.F10.sf3.2.m1.1.1.2.cmml">F</mi><mn mathsize="90%" id="S3.F10.sf3.2.m1.1.1.3" xref="S3.F10.sf3.2.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.F10.sf3.2.m1.1c"><apply id="S3.F10.sf3.2.m1.1.1.cmml" xref="S3.F10.sf3.2.m1.1.1"><csymbol cd="ambiguous" id="S3.F10.sf3.2.m1.1.1.1.cmml" xref="S3.F10.sf3.2.m1.1.1">subscript</csymbol><ci id="S3.F10.sf3.2.m1.1.1.2.cmml" xref="S3.F10.sf3.2.m1.1.1.2">ùêπ</ci><cn type="integer" id="S3.F10.sf3.2.m1.1.1.3.cmml" xref="S3.F10.sf3.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F10.sf3.2.m1.1d">F_{1}</annotation></semantics></math><span id="S3.F10.sf3.5.2" class="ltx_text" style="font-size:90%;"> score with a threshold of 37 cm for each sensor trained with the stratified splits.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F10.sf4" class="ltx_figure ltx_figure_panel"><img src="/html/2309.01574/assets/images/SensorSingleTrain37.png" id="S3.F10.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="409" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F10.sf4.4.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><math id="S3.F10.sf4.2.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.F10.sf4.2.m1.1b"><msub id="S3.F10.sf4.2.m1.1.1" xref="S3.F10.sf4.2.m1.1.1.cmml"><mi mathsize="90%" id="S3.F10.sf4.2.m1.1.1.2" xref="S3.F10.sf4.2.m1.1.1.2.cmml">F</mi><mn mathsize="90%" id="S3.F10.sf4.2.m1.1.1.3" xref="S3.F10.sf4.2.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.F10.sf4.2.m1.1c"><apply id="S3.F10.sf4.2.m1.1.1.cmml" xref="S3.F10.sf4.2.m1.1.1"><csymbol cd="ambiguous" id="S3.F10.sf4.2.m1.1.1.1.cmml" xref="S3.F10.sf4.2.m1.1.1">subscript</csymbol><ci id="S3.F10.sf4.2.m1.1.1.2.cmml" xref="S3.F10.sf4.2.m1.1.1.2">ùêπ</ci><cn type="integer" id="S3.F10.sf4.2.m1.1.1.3.cmml" xref="S3.F10.sf4.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F10.sf4.2.m1.1d">F_{1}</annotation></semantics></math><span id="S3.F10.sf4.5.2" class="ltx_text" style="font-size:90%;"> score with a threshold of 37 cm for each sensor trained with DGPS splits.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F10.sf5" class="ltx_figure ltx_figure_panel"><img src="/html/2309.01574/assets/images/SensorAllTrains2SE.png" id="S3.F10.sf5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="409" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F10.sf5.2.1.1" class="ltx_text" style="font-size:90%;">(e)</span> </span><span id="S3.F10.sf5.3.2" class="ltx_text" style="font-size:90%;">MSA for each sensor trained with stratified splits.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F10.sf6" class="ltx_figure ltx_figure_panel"><img src="/html/2309.01574/assets/images/SensorSingleTrain2SE.png" id="S3.F10.sf6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="409" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F10.sf6.2.1.1" class="ltx_text" style="font-size:90%;">(f)</span> </span><span id="S3.F10.sf6.3.2" class="ltx_text" style="font-size:90%;">MSA for each sensor trained with DGPS splits.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F10.2.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S3.F10.3.2" class="ltx_text" style="font-size:90%;">Metrics evaluated per sensor.</span></figcaption>
</figure>
<figure id="S3.T3" class="ltx_table">
<table id="S3.T3.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T3.2.2" class="ltx_tr">
<td id="S3.T3.2.2.3" class="ltx_td ltx_align_left ltx_border_tt">Type</td>
<td id="S3.T3.2.2.4" class="ltx_td ltx_align_left ltx_border_tt">Model</td>
<td id="S3.T3.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><math id="S3.T3.1.1.1.m1.1" class="ltx_Math" alttext="\bar{\Delta s}" display="inline"><semantics id="S3.T3.1.1.1.m1.1a"><mover accent="true" id="S3.T3.1.1.1.m1.1.1" xref="S3.T3.1.1.1.m1.1.1.cmml"><mrow id="S3.T3.1.1.1.m1.1.1.2" xref="S3.T3.1.1.1.m1.1.1.2.cmml"><mi mathvariant="normal" id="S3.T3.1.1.1.m1.1.1.2.2" xref="S3.T3.1.1.1.m1.1.1.2.2.cmml">Œî</mi><mo lspace="0em" rspace="0em" id="S3.T3.1.1.1.m1.1.1.2.1" xref="S3.T3.1.1.1.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S3.T3.1.1.1.m1.1.1.2.3" xref="S3.T3.1.1.1.m1.1.1.2.3.cmml">s</mi></mrow><mo id="S3.T3.1.1.1.m1.1.1.1" xref="S3.T3.1.1.1.m1.1.1.1.cmml">¬Ø</mo></mover><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.m1.1b"><apply id="S3.T3.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.m1.1.1"><ci id="S3.T3.1.1.1.m1.1.1.1.cmml" xref="S3.T3.1.1.1.m1.1.1.1">¬Ø</ci><apply id="S3.T3.1.1.1.m1.1.1.2.cmml" xref="S3.T3.1.1.1.m1.1.1.2"><times id="S3.T3.1.1.1.m1.1.1.2.1.cmml" xref="S3.T3.1.1.1.m1.1.1.2.1"></times><ci id="S3.T3.1.1.1.m1.1.1.2.2.cmml" xref="S3.T3.1.1.1.m1.1.1.2.2">Œî</ci><ci id="S3.T3.1.1.1.m1.1.1.2.3.cmml" xref="S3.T3.1.1.1.m1.1.1.2.3">ùë†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.m1.1c">\bar{\Delta s}</annotation></semantics></math></td>
<td id="S3.T3.2.2.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><math id="S3.T3.2.2.2.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.T3.2.2.2.m1.1a"><msub id="S3.T3.2.2.2.m1.1.1" xref="S3.T3.2.2.2.m1.1.1.cmml"><mi id="S3.T3.2.2.2.m1.1.1.2" xref="S3.T3.2.2.2.m1.1.1.2.cmml">F</mi><mn id="S3.T3.2.2.2.m1.1.1.3" xref="S3.T3.2.2.2.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.2.m1.1b"><apply id="S3.T3.2.2.2.m1.1.1.cmml" xref="S3.T3.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S3.T3.2.2.2.m1.1.1.1.cmml" xref="S3.T3.2.2.2.m1.1.1">subscript</csymbol><ci id="S3.T3.2.2.2.m1.1.1.2.cmml" xref="S3.T3.2.2.2.m1.1.1.2">ùêπ</ci><cn type="integer" id="S3.T3.2.2.2.m1.1.1.3.cmml" xref="S3.T3.2.2.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.2.m1.1c">F_{1}</annotation></semantics></math></td>
</tr>
<tr id="S3.T3.2.3" class="ltx_tr">
<td id="S3.T3.2.3.1" class="ltx_td"></td>
<td id="S3.T3.2.3.2" class="ltx_td"></td>
<td id="S3.T3.2.3.3" class="ltx_td"></td>
<td id="S3.T3.2.3.4" class="ltx_td ltx_align_left">200¬†cm</td>
<td id="S3.T3.2.3.5" class="ltx_td ltx_align_left">37¬†cm</td>
</tr>
<tr id="S3.T3.2.4" class="ltx_tr">
<td id="S3.T3.2.4.1" class="ltx_td ltx_align_left ltx_border_t">DGPS</td>
<td id="S3.T3.2.4.2" class="ltx_td ltx_align_left ltx_border_t">VADERspec</td>
<td id="S3.T3.2.4.3" class="ltx_td ltx_align_left ltx_border_t">19.6¬†cm</td>
<td id="S3.T3.2.4.4" class="ltx_td ltx_align_left ltx_border_t">93.9¬†%</td>
<td id="S3.T3.2.4.5" class="ltx_td ltx_align_left ltx_border_t">78.4¬†%</td>
</tr>
<tr id="S3.T3.2.5" class="ltx_tr">
<td id="S3.T3.2.5.1" class="ltx_td"></td>
<td id="S3.T3.2.5.2" class="ltx_td ltx_align_left">VADERraw2</td>
<td id="S3.T3.2.5.3" class="ltx_td ltx_align_left">
<span id="S3.T3.2.5.3.1" class="ltx_text ltx_font_bold">14.5</span>¬†cm</td>
<td id="S3.T3.2.5.4" class="ltx_td ltx_align_left">
<span id="S3.T3.2.5.4.1" class="ltx_text ltx_font_bold">97.5</span>¬†%</td>
<td id="S3.T3.2.5.5" class="ltx_td ltx_align_left">
<span id="S3.T3.2.5.5.1" class="ltx_text ltx_font_bold">85.8</span>¬†%</td>
</tr>
<tr id="S3.T3.2.6" class="ltx_tr">
<td id="S3.T3.2.6.1" class="ltx_td"></td>
<td id="S3.T3.2.6.2" class="ltx_td ltx_align_left">VADERraw3</td>
<td id="S3.T3.2.6.3" class="ltx_td ltx_align_left">15.6¬†cm</td>
<td id="S3.T3.2.6.4" class="ltx_td ltx_align_left">97.1¬†%</td>
<td id="S3.T3.2.6.5" class="ltx_td ltx_align_left">85.0¬†%</td>
</tr>
<tr id="S3.T3.2.7" class="ltx_tr">
<td id="S3.T3.2.7.1" class="ltx_td ltx_align_left">stratified</td>
<td id="S3.T3.2.7.2" class="ltx_td ltx_align_left">VADERspec</td>
<td id="S3.T3.2.7.3" class="ltx_td ltx_align_left">5.00¬†cm</td>
<td id="S3.T3.2.7.4" class="ltx_td ltx_align_left">99.4¬†%</td>
<td id="S3.T3.2.7.5" class="ltx_td ltx_align_left">97.5¬†%</td>
</tr>
<tr id="S3.T3.2.8" class="ltx_tr">
<td id="S3.T3.2.8.1" class="ltx_td"></td>
<td id="S3.T3.2.8.2" class="ltx_td ltx_align_left">VADERraw2</td>
<td id="S3.T3.2.8.3" class="ltx_td ltx_align_left">
<span id="S3.T3.2.8.3.1" class="ltx_text ltx_font_bold">3.69</span>¬†cm</td>
<td id="S3.T3.2.8.4" class="ltx_td ltx_align_left">
<span id="S3.T3.2.8.4.1" class="ltx_text ltx_font_bold">99.9</span>¬†%</td>
<td id="S3.T3.2.8.5" class="ltx_td ltx_align_left">
<span id="S3.T3.2.8.5.1" class="ltx_text ltx_font_bold">99.1</span>¬†%</td>
</tr>
<tr id="S3.T3.2.9" class="ltx_tr">
<td id="S3.T3.2.9.1" class="ltx_td ltx_border_bb"></td>
<td id="S3.T3.2.9.2" class="ltx_td ltx_align_left ltx_border_bb">VADERraw3</td>
<td id="S3.T3.2.9.3" class="ltx_td ltx_align_left ltx_border_bb">3.71¬†cm</td>
<td id="S3.T3.2.9.4" class="ltx_td ltx_align_left ltx_border_bb">
<span id="S3.T3.2.9.4.1" class="ltx_text ltx_font_bold">99.9</span>¬†%</td>
<td id="S3.T3.2.9.5" class="ltx_td ltx_align_left ltx_border_bb">98.9¬†%</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T3.8.2.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><math id="S3.T3.5.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S3.T3.5.m1.1b"><msub id="S3.T3.5.m1.1.1" xref="S3.T3.5.m1.1.1.cmml"><mi mathsize="90%" id="S3.T3.5.m1.1.1.2" xref="S3.T3.5.m1.1.1.2.cmml">F</mi><mn mathsize="90%" id="S3.T3.5.m1.1.1.3" xref="S3.T3.5.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.T3.5.m1.1c"><apply id="S3.T3.5.m1.1.1.cmml" xref="S3.T3.5.m1.1.1"><csymbol cd="ambiguous" id="S3.T3.5.m1.1.1.1.cmml" xref="S3.T3.5.m1.1.1">subscript</csymbol><ci id="S3.T3.5.m1.1.1.2.cmml" xref="S3.T3.5.m1.1.1.2">ùêπ</ci><cn type="integer" id="S3.T3.5.m1.1.1.3.cmml" xref="S3.T3.5.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.5.m1.1d">F_{1}</annotation></semantics></math><span id="S3.T3.6.1" class="ltx_text" style="font-size:90%;"> score and <math id="S3.T3.6.1.m1.1" class="ltx_Math" alttext="\bar{\Delta s}" display="inline"><semantics id="S3.T3.6.1.m1.1b"><mover accent="true" id="S3.T3.6.1.m1.1.1" xref="S3.T3.6.1.m1.1.1.cmml"><mrow id="S3.T3.6.1.m1.1.1.2" xref="S3.T3.6.1.m1.1.1.2.cmml"><mi mathvariant="normal" id="S3.T3.6.1.m1.1.1.2.2" xref="S3.T3.6.1.m1.1.1.2.2.cmml">Œî</mi><mo lspace="0em" rspace="0em" id="S3.T3.6.1.m1.1.1.2.1" xref="S3.T3.6.1.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S3.T3.6.1.m1.1.1.2.3" xref="S3.T3.6.1.m1.1.1.2.3.cmml">s</mi></mrow><mo id="S3.T3.6.1.m1.1.1.1" xref="S3.T3.6.1.m1.1.1.1.cmml">¬Ø</mo></mover><annotation-xml encoding="MathML-Content" id="S3.T3.6.1.m1.1c"><apply id="S3.T3.6.1.m1.1.1.cmml" xref="S3.T3.6.1.m1.1.1"><ci id="S3.T3.6.1.m1.1.1.1.cmml" xref="S3.T3.6.1.m1.1.1.1">¬Ø</ci><apply id="S3.T3.6.1.m1.1.1.2.cmml" xref="S3.T3.6.1.m1.1.1.2"><times id="S3.T3.6.1.m1.1.1.2.1.cmml" xref="S3.T3.6.1.m1.1.1.2.1"></times><ci id="S3.T3.6.1.m1.1.1.2.2.cmml" xref="S3.T3.6.1.m1.1.1.2.2">Œî</ci><ci id="S3.T3.6.1.m1.1.1.2.3.cmml" xref="S3.T3.6.1.m1.1.1.2.3">ùë†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.6.1.m1.1d">\bar{\Delta s}</annotation></semantics></math> calculated for all samples of the corresponding folds without sensor R3.</span></figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We have demonstrated that an CNN with an adequate receptive field can achieve better results using raw data compared to using a spectrogram as input. This could be due to the loss of information when transforming into spectograms, whereas an CNN can directly learn to filter the data comparable to it when necessary. In order to achieve comparable or better results with raw data instead of spectograms, we propose the RF rule to calculate the required size of the largest receptive field of a model (eq.¬†<a href="#S2.E1" title="In 2.4 Model Definition ‚Ä£ 2 Methodology ‚Ä£ Object Size-Driven Design of Convolutional Neural Networks: Virtual Axle Detection based on Raw Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Based on our results we were able to show the potential of raw data as input considering the RF rule.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Furthermore, we demonstrated that acceleration data is suitable for axle detection and localisation. It is particularly noteworthy that even when training with only one type of train and thus a non-representative training set, we were able to achieve very good results on other train types, demonstrating that the model generalises exceptionally well. In this case, VADER was trained solely on trains with 32 axles but could still detect 97.5¬†% of the axles of all other train types, with an average spatial error of 14.5¬†cm. Trained on a representative training set, VADER was able to detect 99.9¬†% of the axles with an error of just 3.69¬†cm. In addition to improved accuracy, VADER using raw data allows for an inference that is 65 times faster while using only 1¬†% of the memory. This makes the use of virtual axle detectors in real time and using edge computing possible for the first time.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">It is evident that there is significant potential yet to be unlocked. The VADER model evaluates the signals of the acceleration sensors individually. A joint evaluation of the signals could further increase the accuracy. Another approach could be a combination of raw input data, FCN-based spectrogram-like data transformation, and transformer-based classification. This way, the FCN could learn how to optimally transform the data, while the transformer model would be utilised to identify even more complex correlations and relationships.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Author Contributions</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p"><span id="Sx1.p1.1.1" class="ltx_text ltx_font_bold">Henrik Riedel:</span> Conceptualization, investigation, methodology, software, validation, visualization and writing - original draft. <span id="Sx1.p1.1.2" class="ltx_text ltx_font_bold">Steven Robert Lorenzen:</span> Funding, supervision and writing - review &amp; editing <span id="Sx1.p1.1.3" class="ltx_text ltx_font_bold">Clemens H√ºbler:</span> Resources, supervision and writing - review &amp; editing.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">The research project ZEKISS (www.zekiss.de) is carried out in collaboration with the German railway company DB Netz AG, the W√∂lfel Engineering GmbH and the GMG Ingenieurgesellschaft mbH. It is funded by the mFund (mFund, 2020) promoted by the The Federal Ministry of Transport and Digital Infrastructure.</p>
</div>
<div id="Sx2.p2" class="ltx_para">
<p id="Sx2.p2.1" class="ltx_p">The research project DEEB-INFRA (www.deeb-infra.de) is carried out in collaboration with the the sub company DB Campus from the Deutschen Bahn AG, the AIT GmbH, the Revotec zt GmbH and the iSEA Tec GmbH. It is funded by the mFund (mFund, 2020) promoted by the The Federal Ministry of Transport and Digital Infrastructure.</p>
</div>
<figure id="Sx2.1" class="ltx_figure"><img src="/html/2309.01574/assets/x5.png" id="Sx2.1.g1" class="ltx_graphics ltx_img_landscape" width="332" height="98" alt="[Uncaptioned image]">
</figure>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Data and Source Code</h2>

<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.1" class="ltx_p">The data <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> as well as the source code <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> used in this paper is published and contains:</p>
<ol id="Sx3.I1" class="ltx_enumerate">
<li id="Sx3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="Sx3.I1.i1.p1" class="ltx_para">
<p id="Sx3.I1.i1.p1.1" class="ltx_p">All measurement data</p>
</div>
</li>
<li id="Sx3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="Sx3.I1.i2.p1" class="ltx_para">
<p id="Sx3.I1.i2.p1.1" class="ltx_p">Matlab code to label data and save as text files</p>
</div>
</li>
<li id="Sx3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="Sx3.I1.i3.p1" class="ltx_para">
<p id="Sx3.I1.i3.p1.1" class="ltx_p">Python code for transformation, training, evaluation and plotting.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">CHAN et¬†al. [2001]</span>
<span class="ltx_bibblock">
T.¬†CHAN, L.¬†YU, S.¬†LAW,
T.¬†YUNG,

</span>
<span class="ltx_bibblock">Moving force identification studies, i: Theory,

</span>
<span class="ltx_bibblock">Journal of Sound and Vibration
247 (2001) 59‚Äì76.
URL: <a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0022460X01936302" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.sciencedirect.com/science/article/pii/S0022460X01936302</a>.
doi:<a target="_blank" href="http://dx.doi.org/https://doi.org/10.1006/jsvi.2001.3630" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">https://doi.org/10.1006/jsvi.2001.3630</span></a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kazemi Amiri and Bucher [2017]</span>
<span class="ltx_bibblock">
A.¬†Kazemi Amiri, C.¬†Bucher,

</span>
<span class="ltx_bibblock">A procedure for in situ wind load reconstruction from
structural response only based on field testing data,

</span>
<span class="ltx_bibblock">Journal of Wind Engineering and Industrial
Aerodynamics 167 (2017)
75‚Äì86. URL: <a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0167610516303452" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.sciencedirect.com/science/article/pii/S0167610516303452</a>.
doi:<a target="_blank" href="http://dx.doi.org/https://doi.org/10.1016/j.jweia.2017.04.009" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">https://doi.org/10.1016/j.jweia.2017.04.009</span></a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hwang et¬†al. [2009]</span>
<span class="ltx_bibblock">
J.¬†Hwang, A.¬†Kareem,
W.¬†Kim,

</span>
<span class="ltx_bibblock">Estimation of modal loads using structural response,

</span>
<span class="ltx_bibblock">Journal of Sound and Vibration
326 (2009) 522‚Äì539.
URL: <a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0022460X09004271" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.sciencedirect.com/science/article/pii/S0022460X09004271</a>.
doi:<a target="_blank" href="http://dx.doi.org/https://doi.org/10.1016/j.jsv.2009.05.003" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">https://doi.org/10.1016/j.jsv.2009.05.003</span></a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lourens et¬†al. [2012]</span>
<span class="ltx_bibblock">
E.¬†Lourens, C.¬†Papadimitriou,
S.¬†Gillijns, E.¬†Reynders,
G.¬†De Roeck, G.¬†Lombaert,

</span>
<span class="ltx_bibblock">Joint input-response estimation for structural
systems based on reduced-order models and vibration data from a limited
number of sensors,

</span>
<span class="ltx_bibblock">Mechanical Systems and Signal Processing
29 (2012) 310‚Äì327.
URL: <a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S088832701200012X" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.sciencedirect.com/science/article/pii/S088832701200012X</a>.
doi:<a target="_blank" href="http://dx.doi.org/https://doi.org/10.1016/j.ymssp.2012.01.011" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">https://doi.org/10.1016/j.ymssp.2012.01.011</span></a>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Firus [2022]</span>
<span class="ltx_bibblock">
A.¬†Firus, A contribution to moving force
identification in bridge dynamics, Ph.D. thesis, Technische Universit√§t,
Darmstadt, 2022. URL: <a target="_blank" href="http://tuprints.ulb.tu-darmstadt.de/20293/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://tuprints.ulb.tu-darmstadt.de/20293/</a>.
doi:<a target="_blank" href="http://dx.doi.org/https://doi.org/10.26083/tuprints-00020293" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">https://doi.org/10.26083/tuprints-00020293</span></a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Firus et¬†al. [2022]</span>
<span class="ltx_bibblock">
A.¬†Firus, R.¬†Kemmler,
H.¬†Berthold, S.¬†Lorenzen,
J.¬†Schneider,

</span>
<span class="ltx_bibblock">A time domain method for reconstruction of pedestrian
induced loads on vibrating structures,

</span>
<span class="ltx_bibblock">Mechanical Systems and Signal Processing
171 (2022) 108887.
URL: <a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0888327022000772" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.sciencedirect.com/science/article/pii/S0888327022000772</a>.
doi:<a target="_blank" href="http://dx.doi.org/https://doi.org/10.1016/j.ymssp.2022.108887" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">https://doi.org/10.1016/j.ymssp.2022.108887</span></a>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lydon et¬†al. [2017]</span>
<span class="ltx_bibblock">
M.¬†Lydon, D.¬†Robinson,
S.¬†E. Taylor, G.¬†Amato,
E.¬†J.¬†O. Brien, N.¬†Uddin,

</span>
<span class="ltx_bibblock">Improved axle detection for bridge weigh-in-motion
systems using fiber optic sensors,

</span>
<span class="ltx_bibblock">Journal of Civil Structural Health Monitoring
7 (2017) 325‚Äì332.
URL: <a target="_blank" href="https://link.springer.com/article/10.1007/s13349-017-0229-4" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://link.springer.com/article/10.1007/s13349-017-0229-4</a>.
doi:<a target="_blank" href="http://dx.doi.org/10.1007/s13349-017-0229-4" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1007/s13349-017-0229-4</span></a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et¬†al. [2019]</span>
<span class="ltx_bibblock">
H.¬†Wang, Q.¬†Zhu, J.¬†Li,
J.¬†Mao, S.¬†Hu, X.¬†Zhao,

</span>
<span class="ltx_bibblock">Identification of moving train loads on railway
bridge based on strain monitoring,

</span>
<span class="ltx_bibblock">Smart Structures and Systems 23
(2019) 263‚Äì278. URL: <a target="_blank" href="https://koreascience.or.kr/article/JAKO201913457807828.page" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://koreascience.or.kr/article/JAKO201913457807828.page</a>.
doi:<a target="_blank" href="http://dx.doi.org/10.12989/sss.2019.23.3.263" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.12989/sss.2019.23.3.263</span></a>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et¬†al. [2016]</span>
<span class="ltx_bibblock">
Y.¬†Yu, C.¬†Cai, L.¬†Deng,

</span>
<span class="ltx_bibblock">State-of-the-art review on bridge weigh-in-motion
technology,

</span>
<span class="ltx_bibblock">Advances in Structural Engineering
19 (2016) 1514‚Äì1530.
URL: <a target="_blank" href="http://journals.sagepub.com/doi/10.1177/1369433216655922" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://journals.sagepub.com/doi/10.1177/1369433216655922</a>.
doi:<a target="_blank" href="http://dx.doi.org/10.1177/1369433216655922" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1177/1369433216655922</span></a>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et¬†al. [2019]</span>
<span class="ltx_bibblock">
W.¬†He, T.¬†Ling, E.¬†J.
OBrien, L.¬†Deng,

</span>
<span class="ltx_bibblock">Virtual axle method for bridge weigh-in-motion
systems requiring no axle detector,

</span>
<span class="ltx_bibblock">Journal of Bridge Engineering 24
(2019) 04019086. URL: <a href="%7Bhttps://ascelibrary.org/doi/abs/10.1061/%28ASCE%29BE.1943-5592.0001474%7D" title="" class="ltx_ref ltx_url ltx_font_typewriter">{https://ascelibrary.org/doi/abs/10.1061/%28ASCE%29BE.1943-5592.0001474}</a>.
doi:<a target="_blank" href="http://dx.doi.org/10.1061/(ASCE)BE.1943-5592.0001474" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1061/(ASCE)BE.1943-5592.0001474</span></a>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">O‚ÄôBrien et¬†al. [2012]</span>
<span class="ltx_bibblock">
E.¬†J. O‚ÄôBrien, D.¬†Hajializadeh,
N.¬†Uddin, D.¬†Robinson,
R.¬†Opitz,

</span>
<span class="ltx_bibblock">Strategies for Axle Detection in Bridge
Weigh-in-Motion Systems,

</span>
<span class="ltx_bibblock">in: Proceedings of the International Conference
on Weigh-In-Motion, June, 2012, pp.
79‚Äì88. URL: <a target="_blank" href="https://www.researchgate.net/publication/281031765_STRATEGIES_FOR_AXLE_DETECTION_IN_BRIDGE_WEIGH-IN-MOTION_SYSTEMS" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.researchgate.net/publication/281031765_STRATEGIES_FOR_AXLE_DETECTION_IN_BRIDGE_WEIGH-IN-MOTION_SYSTEMS</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et¬†al. [2020]</span>
<span class="ltx_bibblock">
H.¬†Zhao, C.¬†Tan, E.¬†J.
OBrien, N.¬†Uddin, B.¬†Zhang,

</span>
<span class="ltx_bibblock">Wavelet-based optimum identification of vehicle axles
using bridge measurements,

</span>
<span class="ltx_bibblock">Applied Sciences 10
(2020). URL: <a target="_blank" href="https://www.mdpi.com/2076-3417/10/21/7485" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.mdpi.com/2076-3417/10/21/7485</a>.
doi:<a target="_blank" href="http://dx.doi.org/10.3390/app10217485" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.3390/app10217485</span></a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thater et¬†al. [1998]</span>
<span class="ltx_bibblock">
G.¬†Thater, P.¬†Chang, D.¬†R.
Schelling, C.¬†C. Fu,

</span>
<span class="ltx_bibblock">Estimation of bridge static response and vehicle
weights by frequency response analysis,

</span>
<span class="ltx_bibblock">Canadian Journal of Civil Engineering
25 (1998) 631‚Äì639.
URL: <a target="_blank" href="https://cdnsciencepub.com/doi/10.1139/l97-128" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://cdnsciencepub.com/doi/10.1139/l97-128</a>.
doi:<a target="_blank" href="http://dx.doi.org/10.1139/l97-128" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1139/l97-128</span></a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zakharenko et¬†al. [2022]</span>
<span class="ltx_bibblock">
M.¬†Zakharenko, G.¬†T. Fr√∏seth,
A.¬†R√∂nnquist,

</span>
<span class="ltx_bibblock">Train classification using a weigh-in-motion system
and associated algorithms to determine fatigue loads,

</span>
<span class="ltx_bibblock">Sensors 22
(2022). URL: <a target="_blank" href="https://www.mdpi.com/1424-8220/22/5/1772" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.mdpi.com/1424-8220/22/5/1772</a>.
doi:<a target="_blank" href="http://dx.doi.org/10.3390/s22051772" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.3390/s22051772</span></a>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bernas et¬†al. [2018]</span>
<span class="ltx_bibblock">
M.¬†Bernas, B.¬†P≈Çaczek,
W.¬†Korski, P.¬†Loska,
J.¬†Smy≈Ça, P.¬†Szyma≈Ça,

</span>
<span class="ltx_bibblock">A survey and comparison of low-cost sensing
technologies for road traffic monitoring,

</span>
<span class="ltx_bibblock">Sensors 18
(2018). URL: <a target="_blank" href="https://www.mdpi.com/1424-8220/18/10/3243" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.mdpi.com/1424-8220/18/10/3243</a>.
doi:<a target="_blank" href="http://dx.doi.org/10.3390/s18103243" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.3390/s18103243</span></a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kouroussis et¬†al. [2015]</span>
<span class="ltx_bibblock">
G.¬†Kouroussis, C.¬†Caucheteur,
D.¬†Kinet, G.¬†Alexandrou,
O.¬†Verlinden, V.¬†Moeyaert,

</span>
<span class="ltx_bibblock">Review of trackside monitoring solutions: From strain
gages to optical fibre sensors,

</span>
<span class="ltx_bibblock">Sensors 15
(2015) 20115‚Äì20139. URL: <a target="_blank" href="https://www.mdpi.com/1424-8220/15/8/20115" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.mdpi.com/1424-8220/15/8/20115</a>.
doi:<a target="_blank" href="http://dx.doi.org/10.3390/s150820115" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.3390/s150820115</span></a>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lydon et¬†al. [2016]</span>
<span class="ltx_bibblock">
M.¬†Lydon, S.¬†E. Taylor,
D.¬†Robinson, A.¬†Mufti,
E.¬†J.¬†O. Brien,

</span>
<span class="ltx_bibblock">Recent developments in bridge weigh in motion
(b-wim),

</span>
<span class="ltx_bibblock">Journal of Civil Structural Health Monitoring
6 (2016) 69‚Äì81.
URL: <a target="_blank" href="https://link.springer.com/article/10.1007/s13349-015-0119-6" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://link.springer.com/article/10.1007/s13349-015-0119-6</a>.
doi:<a target="_blank" href="http://dx.doi.org/10.1007/s13349-015-0119-6" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1007/s13349-015-0119-6</span></a>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et¬†al. [2023]</span>
<span class="ltx_bibblock">
C.¬†Tan, B.¬†Zhang,
H.¬†Zhao, N.¬†Uddin,
H.¬†Guo, B.¬†Yan,

</span>
<span class="ltx_bibblock">An extended bridge weigh-in-motion system without
vehicular axles and speed detectors using nonnegative lasso regularization,

</span>
<span class="ltx_bibblock">Journal of Bridge Engineering 28
(2023) 04023022. URL: <a target="_blank" href="https://ascelibrary.org/doi/abs/10.1061/JBENF2.BEENG-5864" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ascelibrary.org/doi/abs/10.1061/JBENF2.BEENG-5864</a>.
doi:<a target="_blank" href="http://dx.doi.org/10.1061/JBENF2.BEENG-5864" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1061/JBENF2.BEENG-5864</span></a>.
<a target="_blank" href="http://arxiv.org/abs/https://ascelibrary.org/doi/pdf/10.1061/JBENF2.BEENG-5864" title="" class="ltx_ref ltx_href ltx_font_typewriter">arXiv:https://ascelibrary.org/doi/pdf/10.1061/JBENF2.BEENG-5864</a>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mesaros et¬†al. [2021]</span>
<span class="ltx_bibblock">
A.¬†Mesaros, T.¬†Heittola,
T.¬†Virtanen, M.¬†D. Plumbley,

</span>
<span class="ltx_bibblock">Sound event detection: A tutorial,

</span>
<span class="ltx_bibblock">IEEE Signal Processing Magazine
38 (2021) 67‚Äì83.
doi:<a target="_blank" href="http://dx.doi.org/10.1109/MSP.2021.3090678" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/MSP.2021.3090678</span></a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mesaros et¬†al. [2019]</span>
<span class="ltx_bibblock">
A.¬†Mesaros, A.¬†Diment,
B.¬†Elizalde, T.¬†Heittola,
E.¬†Vincent, B.¬†Raj,
T.¬†Virtanen,

</span>
<span class="ltx_bibblock">Sound event detection in the dcase 2017 challenge,

</span>
<span class="ltx_bibblock">IEEE/ACM Transactions on Audio, Speech, and
Language Processing 27 (2019)
992‚Äì1006. doi:<a target="_blank" href="http://dx.doi.org/10.1109/TASLP.2019.2907016" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/TASLP.2019.2907016</span></a>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chan and Chin [2020]</span>
<span class="ltx_bibblock">
T.¬†K. Chan, C.¬†S. Chin,

</span>
<span class="ltx_bibblock">A comprehensive review of polyphonic sound event
detection,

</span>
<span class="ltx_bibblock">IEEE Access 8
(2020) 103339‚Äì103373.
doi:<a target="_blank" href="http://dx.doi.org/10.1109/ACCESS.2020.2999388" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/ACCESS.2020.2999388</span></a>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et¬†al. [2020]</span>
<span class="ltx_bibblock">
Y.¬†Wang, J.¬†Salamon, N.¬†J.
Bryan, J.¬†Pablo¬†Bello,

</span>
<span class="ltx_bibblock">Few-shot sound event detection,

</span>
<span class="ltx_bibblock">in: ICASSP 2020 - 2020 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP),
2020, pp. 81‚Äì85.
doi:<a target="_blank" href="http://dx.doi.org/10.1109/ICASSP40776.2020.9054708" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/ICASSP40776.2020.9054708</span></a>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Latif et¬†al. [2021]</span>
<span class="ltx_bibblock">
S.¬†Latif, R.¬†Rana,
S.¬†Khalifa, R.¬†Jurdak,
J.¬†Qadir, B.¬†W. Schuller,
Deep representation learning in speech processing:
Challenges, recent advances, and future trends, 2021.
<a target="_blank" href="http://arxiv.org/abs/2001.00378" title="" class="ltx_ref ltx_href ltx_font_typewriter">arXiv:2001.00378</a>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Purwins et¬†al. [2019]</span>
<span class="ltx_bibblock">
H.¬†Purwins, B.¬†Li,
T.¬†Virtanen, J.¬†Schl√ºter,
S.-Y. Chang, T.¬†Sainath,

</span>
<span class="ltx_bibblock">Deep learning for audio signal processing,

</span>
<span class="ltx_bibblock">IEEE Journal of Selected Topics in Signal
Processing 13 (2019)
206‚Äì219. doi:<a target="_blank" href="http://dx.doi.org/10.1109/JSTSP.2019.2908700" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/JSTSP.2019.2908700</span></a>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et¬†al. [2023]</span>
<span class="ltx_bibblock">
A.¬†Radford, J.¬†W. Kim,
T.¬†Xu, G.¬†Brockman,
C.¬†Mcleavey, I.¬†Sutskever,

</span>
<span class="ltx_bibblock">Robust speech recognition via large-scale weak
supervision,

</span>
<span class="ltx_bibblock">in: A.¬†Krause, E.¬†Brunskill,
K.¬†Cho, B.¬†Engelhardt,
S.¬†Sabato, J.¬†Scarlett (Eds.),
Proceedings of the 40th International Conference on
Machine Learning, volume 202 of
<span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Proceedings of Machine Learning Research</span>,
PMLR, 2023, pp.
28492‚Äì28518. URL: <a target="_blank" href="https://proceedings.mlr.press/v202/radford23a.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.mlr.press/v202/radford23a.html</a>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kiranyaz et¬†al. [2021]</span>
<span class="ltx_bibblock">
S.¬†Kiranyaz, O.¬†Avci,
O.¬†Abdeljaber, T.¬†Ince,
M.¬†Gabbouj, D.¬†J. Inman,

</span>
<span class="ltx_bibblock">1d convolutional neural networks and applications: A
survey,

</span>
<span class="ltx_bibblock">Mechanical Systems and Signal Processing
151 (2021) 107398.
URL: <a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0888327020307846" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.sciencedirect.com/science/article/pii/S0888327020307846</a>.
doi:<a target="_blank" href="http://dx.doi.org/https://doi.org/10.1016/j.ymssp.2020.107398" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">https://doi.org/10.1016/j.ymssp.2020.107398</span></a>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arias-Vergara et¬†al. [2021]</span>
<span class="ltx_bibblock">
T.¬†Arias-Vergara, P.¬†Klumpp,
J.¬†C. Vasquez-Correa, E.¬†N√∂th,
J.¬†R. Orozco-Arroyave, M.¬†Schuster,

</span>
<span class="ltx_bibblock">Multi-channel spectrograms for speech processing
applications using deep learning methods,

</span>
<span class="ltx_bibblock">Pattern Analysis and Applications
24 (2021) 423‚Äì431.
URL: <a target="_blank" href="https://doi.org/10.1007/s10044-020-00921-5" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s10044-020-00921-5</a>.
doi:<a target="_blank" href="http://dx.doi.org/10.1007/s10044-020-00921-5" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1007/s10044-020-00921-5</span></a>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bozkurt et¬†al. [2018]</span>
<span class="ltx_bibblock">
B.¬†Bozkurt, I.¬†Germanakis,
Y.¬†Stylianou,

</span>
<span class="ltx_bibblock">A study of time-frequency features for cnn-based
automatic heart sound classification for pathology detection,

</span>
<span class="ltx_bibblock">Computers in Biology and Medicine
100 (2018) 132‚Äì143.
URL: <a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0010482518301744" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.sciencedirect.com/science/article/pii/S0010482518301744</a>.
doi:<a target="_blank" href="http://dx.doi.org/https://doi.org/10.1016/j.compbiomed.2018.06.026" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">https://doi.org/10.1016/j.compbiomed.2018.06.026</span></a>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mesaros et¬†al. [2019]</span>
<span class="ltx_bibblock">
A.¬†Mesaros, T.¬†Heittola,
T.¬†Virtanen,

</span>
<span class="ltx_bibblock">Acoustic scene classification in dcase 2019
challenge: Closed and open set classification and data mismatch setups,

</span>
<span class="ltx_bibblock">in: Workshop on Detection and Classification of
Acoustic Scenes and Events, 2019.
doi:<a target="_blank" href="http://dx.doi.org/10.33682/m5kp-fa97" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.33682/m5kp-fa97</span></a>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van¬†den Oord et¬†al. [2016]</span>
<span class="ltx_bibblock">
A.¬†van¬†den Oord, S.¬†Dieleman,
H.¬†Zen, K.¬†Simonyan,
O.¬†Vinyals, A.¬†Graves,
N.¬†Kalchbrenner, A.¬†Senior,
K.¬†Kavukcuoglu, Wavenet: A generative model
for raw audio, 2016.
<a target="_blank" href="http://arxiv.org/abs/1609.03499" title="" class="ltx_ref ltx_href ltx_font_typewriter">arXiv:1609.03499</a>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghahremani et¬†al. [2016]</span>
<span class="ltx_bibblock">
P.¬†Ghahremani, V.¬†Manohar,
D.¬†Povey, S.¬†Khudanpur,

</span>
<span class="ltx_bibblock">Acoustic modelling from the signal domain using
cnns,

</span>
<span class="ltx_bibblock">in: Interspeech 2016, 2016, pp.
3434‚Äì3438. URL: <a target="_blank" href="http://dx.doi.org/10.21437/Interspeech.2016-1495" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://dx.doi.org/10.21437/Interspeech.2016-1495</a>.
doi:<a target="_blank" href="http://dx.doi.org/10.21437/Interspeech.2016-1495" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.21437/Interspeech.2016-1495</span></a>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sailor and Patil [2016]</span>
<span class="ltx_bibblock">
H.¬†B. Sailor, H.¬†A. Patil,

</span>
<span class="ltx_bibblock">Novel unsupervised auditory filterbank learning using
convolutional rbm for speech recognition,

</span>
<span class="ltx_bibblock">IEEE/ACM Transactions on Audio, Speech, and
Language Processing 24 (2016)
2341‚Äì2353. doi:<a target="_blank" href="http://dx.doi.org/10.1109/TASLP.2016.2607341" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/TASLP.2016.2607341</span></a>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chatterjee et¬†al. [2006]</span>
<span class="ltx_bibblock">
P.¬†Chatterjee, E.¬†OBrien,
Y.¬†Li, A.¬†Gonz√°lez,

</span>
<span class="ltx_bibblock">Wavelet domain analysis for identification of vehicle
axles from bridge measurements,

</span>
<span class="ltx_bibblock">Computers &amp; Structures 84
(2006) 1792‚Äì1801. URL: <a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0045794906001933" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.sciencedirect.com/science/article/pii/S0045794906001933</a>.
doi:<a target="_blank" href="http://dx.doi.org/https://doi.org/10.1016/j.compstruc.2006.04.013" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">https://doi.org/10.1016/j.compstruc.2006.04.013</span></a>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kalhori et¬†al. [2017]</span>
<span class="ltx_bibblock">
H.¬†Kalhori, M.¬†M. Alamdari,
X.¬†Zhu, B.¬†Samali,
S.¬†Mustapha,

</span>
<span class="ltx_bibblock">Non-intrusive schemes for speed and axle
identification in bridge-weigh-in-motion systems,

</span>
<span class="ltx_bibblock">Measurement Science and Technology
28 (2017) 025102.
URL: <a target="_blank" href="https://doi.org/10.1088/1361-6501/aa52ec" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1088/1361-6501/aa52ec</a>.
doi:<a target="_blank" href="http://dx.doi.org/10.1088/1361-6501/aa52ec" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1088/1361-6501/aa52ec</span></a>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et¬†al. [2017]</span>
<span class="ltx_bibblock">
Y.¬†Yu, C.¬†Cai, L.¬†Deng,

</span>
<span class="ltx_bibblock">Vehicle axle identification using wavelet analysis of
bridge global responses,

</span>
<span class="ltx_bibblock">Journal of Vibration and Control
23 (2017) 2830‚Äì2840.
URL: <a target="_blank" href="https://journals.sagepub.com/doi/10.1177/1077546315623147" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://journals.sagepub.com/doi/10.1177/1077546315623147</a>.
doi:<a target="_blank" href="http://dx.doi.org/10.1177/1077546315623147" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1177/1077546315623147</span></a>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et¬†al. [2021]</span>
<span class="ltx_bibblock">
Y.¬†Zhu, H.¬†Sekiya,
T.¬†Okatani, I.¬†Yoshida,
S.¬†Hirano,

</span>
<span class="ltx_bibblock">Acceleration-based deep learning method for vehicle
monitoring,

</span>
<span class="ltx_bibblock">IEEE Sensors Journal 21
(2021) 17154‚Äì17161. URL: <a target="_blank" href="https://ieeexplore.ieee.org/document/9437183" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ieeexplore.ieee.org/document/9437183</a>.
doi:<a target="_blank" href="http://dx.doi.org/10.1109/JSEN.2021.3082145" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/JSEN.2021.3082145</span></a>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lorenzen et¬†al. [2022]</span>
<span class="ltx_bibblock">
S.¬†R. Lorenzen, H.¬†Riedel,
M.¬†M. Rupp, L.¬†Schmeiser,
H.¬†Berthold, A.¬†Firus,
J.¬†Schneider,

</span>
<span class="ltx_bibblock">Virtual axle detector based on analysis of bridge
acceleration measurements by fully convolutional network,

</span>
<span class="ltx_bibblock">Sensors 22
(2022). URL: <a target="_blank" href="https://www.mdpi.com/1424-8220/22/22/8963" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.mdpi.com/1424-8220/22/22/8963</a>.
doi:<a target="_blank" href="http://dx.doi.org/10.3390/s22228963" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.3390/s22228963</span></a>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">G√©ron [2019]</span>
<span class="ltx_bibblock">
A.¬†G√©ron, Hands-on Machine Learning with
Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build
Intelligent Systems, O‚ÄôReilly UK Ltd.,
Sebastopol, 2019. ISBN:
9781492032649.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lorenzen et¬†al. [2022]</span>
<span class="ltx_bibblock">
S.¬†R. Lorenzen, H.¬†Riedel,
M.¬†Rupp, L.¬†Schmeiser,
H.¬†Berthold, A.¬†Firus,
J.¬†Schneider, Virtual Axle Detector based
on Analysis of Bridge Acceleration Measurements by Fully Convolutional
Network, 2022. URL: <a target="_blank" href="https://doi.org/10.5281/zenodo.6782319" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5281/zenodo.6782319</a>.
doi:<a target="_blank" href="http://dx.doi.org/10.5281/zenodo.6782319" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.5281/zenodo.6782319</span></a>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reiterer and Firus [2022]</span>
<span class="ltx_bibblock">
M.¬†Reiterer, A.¬†Firus,

</span>
<span class="ltx_bibblock">Dynamische analyse der zug√ºberfahrt bei
eisenbahnbr√ºcken unter ber√ºcksichtigung von nichtlinearen effekten,

</span>
<span class="ltx_bibblock">Beton-und Stahlbetonbau 117
(2022) 90‚Äì98.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et¬†al. [2019]</span>
<span class="ltx_bibblock">
G.¬†R. Lee, R.¬†Gommers,
F.¬†Waselewski, K.¬†Wohlfahrt,
A.¬†O‚ÄôLeary,

</span>
<span class="ltx_bibblock">Pywavelets: A python package for wavelet analysis,

</span>
<span class="ltx_bibblock">Journal of Open Source Software
4 (2019) 1237. URL: <a target="_blank" href="https://doi.org/10.21105/joss.01237" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.21105/joss.01237</a>.
doi:<a target="_blank" href="http://dx.doi.org/10.21105/joss.01237" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.21105/joss.01237</span></a>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Long et¬†al. [2015]</span>
<span class="ltx_bibblock">
J.¬†Long, E.¬†Shelhamer,
T.¬†Darrell,

</span>
<span class="ltx_bibblock">Fully convolutional networks for semantic
segmentation,

</span>
<span class="ltx_bibblock">in: 2015 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2015, pp.
3431‚Äì3440. URL: <a target="_blank" href="https://ieeexplore.ieee.org/document/7298965" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ieeexplore.ieee.org/document/7298965</a>.
doi:<a target="_blank" href="http://dx.doi.org/10.1109/CVPR.2015.7298965" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/CVPR.2015.7298965</span></a>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ronneberger et¬†al. [2015]</span>
<span class="ltx_bibblock">
O.¬†Ronneberger, P.¬†Fischer,
T.¬†Brox,

</span>
<span class="ltx_bibblock">U-net: Convolutional networks for biomedical image
segmentation,

</span>
<span class="ltx_bibblock">in: Medical Image Computing and Computer-Assisted
Intervention ‚Äì MICCAI 2015, Springer International
Publishing, Cham, 2015, pp.
234‚Äì241. URL: <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28</a>.
doi:<a target="_blank" href="http://dx.doi.org/10.1007/978-3-319-24574-4_28" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1007/978-3-319-24574-4_28</span></a>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mo et¬†al. [2022]</span>
<span class="ltx_bibblock">
Y.¬†Mo, Y.¬†Wu, X.¬†Yang,
F.¬†Liu, Y.¬†Liao,

</span>
<span class="ltx_bibblock">Review the state-of-the-art technologies of semantic
segmentation based on deep learning,

</span>
<span class="ltx_bibblock">Neurocomputing 493
(2022) 626‚Äì646. URL: <a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0925231222000054" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.sciencedirect.com/science/article/pii/S0925231222000054</a>.
doi:<a target="_blank" href="http://dx.doi.org/https://doi.org/10.1016/j.neucom.2022.01.005" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">https://doi.org/10.1016/j.neucom.2022.01.005</span></a>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et¬†al. [2023]</span>
<span class="ltx_bibblock">
Y.¬†Yu, C.¬†Wang, Q.¬†Fu,
R.¬†Kou, F.¬†Huang,
B.¬†Yang, T.¬†Yang,
M.¬†Gao,

</span>
<span class="ltx_bibblock">Techniques and challenges of image segmentation: A
review,

</span>
<span class="ltx_bibblock">Electronics 12
(2023). URL: <a target="_blank" href="https://www.mdpi.com/2079-9292/12/5/1199" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.mdpi.com/2079-9292/12/5/1199</a>.
doi:<a target="_blank" href="http://dx.doi.org/10.3390/electronics12051199" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.3390/electronics12051199</span></a>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Siddique et¬†al. [2021]</span>
<span class="ltx_bibblock">
N.¬†Siddique, S.¬†Paheding,
C.¬†P. Elkin, V.¬†Devabhaktuni,

</span>
<span class="ltx_bibblock">U-net and its variants for medical image
segmentation: A review of theory and applications,

</span>
<span class="ltx_bibblock">IEEE Access 9
(2021) 82031‚Äì82057.
doi:<a target="_blank" href="http://dx.doi.org/10.1109/ACCESS.2021.3086020" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/ACCESS.2021.3086020</span></a>.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et¬†al. [2016]</span>
<span class="ltx_bibblock">
K.¬†He, X.¬†Zhang, S.¬†Ren,
J.¬†Sun,

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition,

</span>
<span class="ltx_bibblock">in: 2016 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2016, pp.
770‚Äì778. URL: <a target="_blank" href="https://ieeexplore.ieee.org/document/7780459" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ieeexplore.ieee.org/document/7780459</a>.
doi:<a target="_blank" href="http://dx.doi.org/https://doi.org/10.1109/CVPR.2016.90" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">https://doi.org/10.1109/CVPR.2016.90</span></a>.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu and He [2020]</span>
<span class="ltx_bibblock">
Y.¬†Wu, K.¬†He,

</span>
<span class="ltx_bibblock">Group normalization,

</span>
<span class="ltx_bibblock">International Journal of Computer Vision
128 (2020) 742‚Äì755.
URL: <a target="_blank" href="https://doi.org/10.1007/s11263-019-01198-w" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s11263-019-01198-w</a>.
doi:<a target="_blank" href="http://dx.doi.org/10.1007/s11263-019-01198-w" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1007/s11263-019-01198-w</span></a>.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lorenzen [2023]</span>
<span class="ltx_bibblock">
S.¬†R. Lorenzen, Railway Bridge Monitoring
with Minimal Sensor Deployment: Virtual Sensing and Resonance Curve-Based
Drive-by Monitoring, Ph.D. thesis, Technische Universit√§t Darmstadt,
Darmstadt, 2023. URL: <a target="_blank" href="http://tuprints.ulb.tu-darmstadt.de/26426/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://tuprints.ulb.tu-darmstadt.de/26426/</a>.
doi:<a target="_blank" href="http://dx.doi.org/https://doi.org/10.26083/tuprints-00026426" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">https://doi.org/10.26083/tuprints-00026426</span></a>.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abadi et¬†al. [2015]</span>
<span class="ltx_bibblock">
M.¬†Abadi, A.¬†Agarwal,
P.¬†Barham, E.¬†Brevdo,
Z.¬†Chen, C.¬†Citro, G.¬†S.
Corrado, A.¬†Davis, J.¬†Dean,
M.¬†Devin, S.¬†Ghemawat,
I.¬†Goodfellow, A.¬†Harp,
G.¬†Irving, M.¬†Isard,
Y.¬†Jia, R.¬†Jozefowicz,
L.¬†Kaiser, M.¬†Kudlur,
J.¬†Levenberg, D.¬†Man√©,
R.¬†Monga, S.¬†Moore,
D.¬†Murray, C.¬†Olah,
M.¬†Schuster, J.¬†Shlens,
B.¬†Steiner, I.¬†Sutskever,
K.¬†Talwar, P.¬†Tucker,
V.¬†Vanhoucke, V.¬†Vasudevan,
F.¬†Vi√©gas, O.¬†Vinyals,
P.¬†Warden, M.¬†Wattenberg,
M.¬†Wicke, Y.¬†Yu,
X.¬†Zheng, TensorFlow: Large-scale machine
learning on heterogeneous systems, 2015. URL: <a target="_blank" href="https://www.tensorflow.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.tensorflow.org/</a>, Accessed: 11.08.2021.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Iqbal [2018]</span>
<span class="ltx_bibblock">
H.¬†Iqbal, Harisiqbal88/plotneuralnet v1.0.0,
2018. URL: <a target="_blank" href="https://doi.org/10.5281/zenodo.2526396" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5281/zenodo.2526396</a>.
doi:<a target="_blank" href="http://dx.doi.org/10.5281/zenodo.2526396" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.5281/zenodo.2526396</span></a>.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et¬†al. [2017]</span>
<span class="ltx_bibblock">
T.-Y. Lin, P.¬†Goyal,
R.¬†Girshick, K.¬†He,
P.¬†Doll√°r,

</span>
<span class="ltx_bibblock">Focal loss for dense object detection,

</span>
<span class="ltx_bibblock">in: 2017 IEEE International Conference on
Computer Vision (ICCV), 2017, pp.
2999‚Äì3007. doi:<a target="_blank" href="http://dx.doi.org/10.1109/ICCV.2017.324" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.1109/ICCV.2017.324</span></a>.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba [2017]</span>
<span class="ltx_bibblock">
D.¬†P. Kingma, J.¬†Ba, Adam:
A method for stochastic optimization, 2017.
<a target="_blank" href="http://arxiv.org/abs/1412.6980" title="" class="ltx_ref ltx_href ltx_font_typewriter">arXiv:1412.6980</a>.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lorenzen et¬†al. [2022]</span>
<span class="ltx_bibblock">
S.¬†R. Lorenzen, H.¬†Riedel,
M.¬†Rupp, L.¬†Schmeiser,
H.¬†Berthold, A.¬†Firus,
J.¬†Schneider, Virtual Axle Detector based
on Analysis of Bridge Acceleration Measurements by Fully Convolutional
Network, 2022. URL: <a target="_blank" href="https://doi.org/10.5281/zenodo.6782319" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5281/zenodo.6782319</a>.
doi:<a target="_blank" href="http://dx.doi.org/10.5281/zenodo.6782319" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.5281/zenodo.6782319</span></a>.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Riedel [2023]</span>
<span class="ltx_bibblock">
H.¬†Riedel, hjhriedel/vader: v0.2-alpha,
2023. URL: <a target="_blank" href="https://doi.org/10.5281/zenodo.8296526" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5281/zenodo.8296526</a>.
doi:<a target="_blank" href="http://dx.doi.org/10.5281/zenodo.8296526" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">10.5281/zenodo.8296526</span></a>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2309.01573" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2309.01574" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2309.01574">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2309.01574" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2309.01575" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 08:23:37 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
