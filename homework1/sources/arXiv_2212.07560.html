<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2212.07560] Multi-level and Multi-modal Feature Fusion for Accurate 3D Object Detection in Connected and Automated Vehicles</title><meta property="og:description" content="Aiming at highly accurate object detection for connected and automated vehicles (CAVs), this paper presents a Deep Neural Network based 3D object detection model that leverages a three-stage feature extractor by develo…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Multi-level and Multi-modal Feature Fusion for Accurate 3D Object Detection in Connected and Automated Vehicles">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Multi-level and Multi-modal Feature Fusion for Accurate 3D Object Detection in Connected and Automated Vehicles">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2212.07560">

<!--Generated on Fri Mar  1 08:00:24 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
multi-sensor fusion,  autonomous vehicles,  vehicle detection,  multi-level feature fusion
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Multi-level and Multi-modal Feature Fusion for Accurate 3D Object Detection in Connected and Automated Vehicles
<br class="ltx_break">
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yiming Hou
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_font_italic">Institute for Transport Studies</span>
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_italic">University of Leeds
<br class="ltx_break"></span>Leeds, UK 
<br class="ltx_break">tsyh@leeds.ac.uk
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mahdi Rezaei
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id3.1.id1" class="ltx_text ltx_font_italic">Institute for Transport Studies</span>
<br class="ltx_break"><span id="id4.2.id2" class="ltx_text ltx_font_italic">University of Leeds
<br class="ltx_break"></span>Leeds, UK 
<br class="ltx_break">m.rezaei@leeds.ac.uk
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Richard Romano
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id5.1.id1" class="ltx_text ltx_font_italic">Institute for Transport Studies</span>
<br class="ltx_break"><span id="id6.2.id2" class="ltx_text ltx_font_italic">University of Leeds
<br class="ltx_break"></span>Leeds, UK 
<br class="ltx_break">r.romano@leeds.ac.uk
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id7.id1" class="ltx_p">Aiming at highly accurate object detection for connected and automated vehicles (CAVs), this paper presents a Deep Neural Network based 3D object detection model that leverages a three-stage feature extractor by developing a novel LIDAR-Camera fusion scheme. The proposed feature extractor extracts high-level features from two input sensory modalities and recovers the important features discarded during the convolutional process. The novel fusion scheme effectively fuses features across sensory modalities and convolutional layers to find the best representative global features. The fused features are shared by a two-stage network: the region proposal network (RPN) and the detection head (DH). The RPN generates high-recall proposals, and the DH produces final detection results. The experimental results show the proposed model outperforms more recent research on the KITTI 2D and 3D detection benchmark, particularly for distant and highly occluded instances.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
multi-sensor fusion, autonomous vehicles, vehicle detection, multi-level feature fusion

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Connected and Automated Vehicles (CAVs) are a transformative technology that could significantly change the current transportation modes by replacing human drivers with autonomous controllers. The technology is considered as a mitigator to road accidents and traffic pollution issues <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. One of the requirements of implementing autonomous vehicles is safe and collision-free driving, making a trustworthy perception system crucial. The perception system sees the surrounding environment for the autonomous vehicles via a variety of sensors (e.g., Cameras, LIDAR and RADAR) and must accurately detect vehicles, cyclists and pedestrians normally with the aid of the Deep Neural Network.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<p id="S1.F1.1" class="ltx_p ltx_align_center"><span id="S1.F1.1.1" class="ltx_text"><img src="/html/2212.07560/assets/002728.png" id="S1.F1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="330" height="202" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>2D and 3D vehicle detection results (green) of the proposed method, tested on the KITTI benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The Convolutional Neural Network (CNN) has proven a potent tool to extract and learn features from various sensory modalities. 2D road user detection methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> leverage CNNs to learn features from RGB camera images and have achieved significant and satisfactory results. CNNs can also produce accurate 3D object detection results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> by extracting 3D geometry information from the LIDAR point cloud. Some previous work proposes to discretise the 3D point clouds into voxel grids <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> or preprocess the point cloud into a bird’s eye view map <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> or range view maps <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> before inputting them to the CNN to mitigate the point cloud’s low resolution and sparsity drawbacks. Sensor-fusion-based road user detection methods, which leverage sensors with complementary characteristics, such as combining a LIDAR with a camera or combining a camera with a RADAR, are a hot research topic in improving 3D detection accuracy. This paper focuses on fusing RGB camera images with LIDAR point clouds to enhance 3D detection accuracy by leveraging sufficient RGB information and precise 3D geometry information. As a result, state-of-the-art LIDAR-Camera-fusion-based methods, such as PointPainting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, AVOD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, MVX-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, show better performance than some of the SORT LIDAR-based 3D detection methods, e.g. LaserNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and VoxelNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Fusing LIDAR point clouds with RGB images is not a trivial task, and the performance depends on the inputs to the fusion and when the fusion happens. There are three main sensor fusion types, early fusion, middle fusion and late fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. However, existing sensor-fusion-based detectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> cannot achieve error-free performance because they cannot fully exploit multi-view features obtained from different levels and the convolutional and downsampling process leads to loss of object features. Loss of details increases the probability of false-negative or false-positive detection and makes detecting partially-occluded or distant vehicles extremely challenging.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Aiming at these challenges, this paper (Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Multi-level and Multi-modal Feature Fusion for Accurate 3D Object Detection in Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) proposes a novel and comprehensive LIDAR-Camera-fusion-based 3D detection method, which effectively fuses features from both sensory modalities obtained at different levels of the proposed convolutional feature extractor. The architecture of the proposed detection network is illustrated in Fig. <a href="#S3.F2" title="Figure 2 ‣ III Methodology ‣ Multi-level and Multi-modal Feature Fusion for Accurate 3D Object Detection in Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The model takes BEV maps and RGB images as inputs. The novelty of the proposed method is combining a three-stage convolutional neural network (Fig. <a href="#S3.F3" title="Figure 3 ‣ III Methodology ‣ Multi-level and Multi-modal Feature Fusion for Accurate 3D Object Detection in Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) with a multi-modal and multi-level fusion scheme (Table 1) to extract the most representative object features across sensory modalities and convolutional layers. These features are shared by a two-stage detection network: the region proposal network (RPN) and the detection head (DH). The RPN projects pre-defined anchors onto feature maps and outputs high-recall 3D proposals. The DH utilises these proposals for the final object classification and oriented bounding box regression.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The proposed approach will be evaluated on the KITTI benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> for the 3D vehicle detection evaluation and bird’s eye view vehicle detection evaluation. We expect the proposed approach achieves a higher detection accuracy on both evaluation metrics and outperforms some of the more recent 3D detectors, such as VoxelNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, MV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, and MVX-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">3D object detection research mainly considers RGB sensors, LIDAR, or the fusion of both sensors.
The majority of existing 3D road user detection methods are LIDAR-based, taking point clouds as input or preprocessing the raw point cloud into a compact representation to improve its sparse, irregular, and borderless characteristics. Classified by the different point cloud representations, the three 3D road user detection methods categories are voxelisation representations, projection-based and point-based.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Point-based methods (e.g., PointNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and PointNet++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> extract point-wise features directly from the raw LIDAR point cloud without any preprocessing on the raw data.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Voxelisation-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> divide the point cloud into equally spaced voxels and extract voxel-wise features through a convolutional neural network. The voxel-wise features are aggregated and passed the region proposal network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> for the final object classification and 3D bounding box regression.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Projection-based methods transform the 3D point cloud into 2D representations and apply 2D convolutional neural networks to perform object detection. There are four 2D point cloud representation types: spherical projection (front view map), cylindrical projection, image-plane projection and bird’s eye view map projection. VeloFCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> projects the LIDAR point cloud into a 2D cylindrical map, and each point is encoded with its position on the 2D map. Squeezeseg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> projects the LIDAR point cloud onto a sphere and characterises each point by azimuth and zenith angles. PointFusion in MVX-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> applies a pre-trained 2D convolutional network to extract pixel-wise features. Subsequently, it projects the point cloud into the image plane and matches the positions of 3D points with 2D image pixels. According to the position match, each point is appended with the corresponding RGB pixel features. MV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> proposes the LIDAR BEV maps by discretising the 3D point cloud into a voxel grid, and BEV maps are encoded by points’ height, intensity and density. In the proposed approach, we follow MV3D’s method to preprocess the 3D point cloud into BEV maps and take it as an input to the detection network. LIDAR BEV maps are capable of mitigating the occlusion problem, enabling the proposed approach to extract highly-detailed features of partially-occluded vehicles.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Image-based 3D detection methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> utilise camera images as input and estimate the 3D bounding box from the 2D bounding box. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> proposes a 2D to 3D bounding box estimation by classifying the viewpoint and regressing the centre location projection of the bottom face of 3D bounding boxes. Recently, Traffic-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, the state-of-the-art method for traffic monitoring, develops a 2D to 3D model by feature extraction and matching from satellite and ground images.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">Sensor-fusion-based 3D detection methods refer to the methods that leverage sensors with complementary characteristics for enhancing detection accuracy under all weather and lighting conditions. PointPainting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and Complexer-YOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> employ a semantic segmentation network on the RGB images and generate a pixel-wise segmentation mask with scores. The fusion occurs when projecting the point cloud to the mask, and points are decorated according to scores. LaserNet++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> fuses the features extracted from the LIDAR range view map with the RGB image features. Frustum PointNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> generates region proposals from images and then extrudes the region proposals into 3D frustum region proposals. Then they apply a semantic segmentation network to filter the points, and the rest of the points are processed by the PointNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> for bounding box regression.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">MV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, a two-stage detection method, generates proposals based on the features extracted from the LIDAR BEV maps. Then, it uses these proposals to fuse element-wise features from LIDAR BEV maps, LIDAR range view maps and RGB images. However, points on LIDAR BEV maps are sparse, which is less suitable for detecting small or partially-occluded or distant road users. Therefore, MV3D’s RPN is limited from generating reliable proposals for small, distant or partially-occluded instances. Although it fuses multi-features in the detection stage, the performance of MV3D is severely limited by the performance of its RPN.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>

<figure id="S3.F2" class="ltx_figure">
<p id="S3.F2.1" class="ltx_p ltx_align_center"><span id="S3.F2.1.1" class="ltx_text"><img src="/html/2212.07560/assets/methdology_latex.png" id="S3.F2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="517" height="277" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The architecture of the proposed detection network. Yellow boxes: multi-level feature fusion. Orange boxes: multi-modal feature fusion. The blue dotted box: the region proposal network (RPN). The red dotted box: the detection head (DH).</figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure">
<p id="S3.F3.1" class="ltx_p ltx_align_center"><span id="S3.F3.1.1" class="ltx_text"><img src="/html/2212.07560/assets/feature_extractor_latex.png" id="S3.F3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="311" height="131" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The architecture of the proposed three-stage feature extractor contains two convolutional stages and one deconvolutional stage. The lateral connections are shown in green lines.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This paper proposes a two-stage 3D detector to achieve high-accuracy detection of distant and partially-occluded vehicles. As illustrated in Fig. <a href="#S3.F2" title="Figure 2 ‣ III Methodology ‣ Multi-level and Multi-modal Feature Fusion for Accurate 3D Object Detection in Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the proposed detection model takes RGB images and LIDAR point clouds as inputs. The model first generates LIDAR BEV maps from the LIDAR point cloud. The novelty of the proposed method is employing a three-stage feature extractor and a novel sensor fusion scheme to extract the most representative features from two input views effectively and enable the recovery of details discarded during the downsampling process. A two-stage network shares the extracted features: the 3D region proposal network (RPN) and the detection head (DH) module. The RPN generates high-recall 3D proposals by projecting pre-defined anchors onto the feature maps and employing feature pooling. Finally, the DH module utilises these proposals for the final object classification and oriented 3D bounding box regression. The detailed object features extracted by the feature extractor and the sensor fusion scheme increases the model’s accuracy in detecting partially occluded and distant vehicles.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">LIDAR BEV maps generator</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Six-channels LIDAR BEV maps are generated from the point cloud following the method proposed in MV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, and the BEV maps are encoded by the height and density of points. The point cloud is first divided equally into five slices along the z-axis and then projected into a 2D grid with a resolution of 0.1m. Each slice generates one height map by encoding each grid cell’s maximum height of points. Five height channels are generated for one point cloud input. The sixth channel of the BEV maps is the density map, which encodes the density of points in each cell, and is computed as shown in equation (1) for each cell, where N is the number of points in each cell.</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.3" class="ltx_Math" alttext="d=min(1.0,\frac{log(N+1)}{log(64)})" display="block"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.4" xref="S3.E1.m1.3.4.cmml"><mi id="S3.E1.m1.3.4.2" xref="S3.E1.m1.3.4.2.cmml">d</mi><mo id="S3.E1.m1.3.4.1" xref="S3.E1.m1.3.4.1.cmml">=</mo><mrow id="S3.E1.m1.3.4.3" xref="S3.E1.m1.3.4.3.cmml"><mi id="S3.E1.m1.3.4.3.2" xref="S3.E1.m1.3.4.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.4.3.1" xref="S3.E1.m1.3.4.3.1.cmml">​</mo><mi id="S3.E1.m1.3.4.3.3" xref="S3.E1.m1.3.4.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.4.3.1a" xref="S3.E1.m1.3.4.3.1.cmml">​</mo><mi id="S3.E1.m1.3.4.3.4" xref="S3.E1.m1.3.4.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.4.3.1b" xref="S3.E1.m1.3.4.3.1.cmml">​</mo><mrow id="S3.E1.m1.3.4.3.5.2" xref="S3.E1.m1.3.4.3.5.1.cmml"><mo stretchy="false" id="S3.E1.m1.3.4.3.5.2.1" xref="S3.E1.m1.3.4.3.5.1.cmml">(</mo><mn id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">1.0</mn><mo id="S3.E1.m1.3.4.3.5.2.2" xref="S3.E1.m1.3.4.3.5.1.cmml">,</mo><mfrac id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><mi id="S3.E1.m1.1.1.1.4" xref="S3.E1.m1.1.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2a" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><mi id="S3.E1.m1.1.1.1.5" xref="S3.E1.m1.1.1.1.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2b" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml">N</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">+</mo><mn id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><mi id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml">​</mo><mi id="S3.E1.m1.2.2.2.4" xref="S3.E1.m1.2.2.2.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2a" xref="S3.E1.m1.2.2.2.2.cmml">​</mo><mi id="S3.E1.m1.2.2.2.5" xref="S3.E1.m1.2.2.2.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2b" xref="S3.E1.m1.2.2.2.2.cmml">​</mo><mrow id="S3.E1.m1.2.2.2.6.2" xref="S3.E1.m1.2.2.2.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.2.6.2.1" xref="S3.E1.m1.2.2.2.cmml">(</mo><mn id="S3.E1.m1.2.2.2.1" xref="S3.E1.m1.2.2.2.1.cmml">64</mn><mo stretchy="false" id="S3.E1.m1.2.2.2.6.2.2" xref="S3.E1.m1.2.2.2.cmml">)</mo></mrow></mrow></mfrac><mo stretchy="false" id="S3.E1.m1.3.4.3.5.2.3" xref="S3.E1.m1.3.4.3.5.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.4.cmml" xref="S3.E1.m1.3.4"><eq id="S3.E1.m1.3.4.1.cmml" xref="S3.E1.m1.3.4.1"></eq><ci id="S3.E1.m1.3.4.2.cmml" xref="S3.E1.m1.3.4.2">𝑑</ci><apply id="S3.E1.m1.3.4.3.cmml" xref="S3.E1.m1.3.4.3"><times id="S3.E1.m1.3.4.3.1.cmml" xref="S3.E1.m1.3.4.3.1"></times><ci id="S3.E1.m1.3.4.3.2.cmml" xref="S3.E1.m1.3.4.3.2">𝑚</ci><ci id="S3.E1.m1.3.4.3.3.cmml" xref="S3.E1.m1.3.4.3.3">𝑖</ci><ci id="S3.E1.m1.3.4.3.4.cmml" xref="S3.E1.m1.3.4.3.4">𝑛</ci><interval closure="open" id="S3.E1.m1.3.4.3.5.1.cmml" xref="S3.E1.m1.3.4.3.5.2"><cn type="float" id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">1.0</cn><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2"><divide id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2"></divide><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><times id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.2"></times><ci id="S3.E1.m1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.3">𝑙</ci><ci id="S3.E1.m1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.4">𝑜</ci><ci id="S3.E1.m1.1.1.1.5.cmml" xref="S3.E1.m1.1.1.1.5">𝑔</ci><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><plus id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"></plus><ci id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2">𝑁</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3">1</cn></apply></apply><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><times id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2"></times><ci id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3">𝑙</ci><ci id="S3.E1.m1.2.2.2.4.cmml" xref="S3.E1.m1.2.2.2.4">𝑜</ci><ci id="S3.E1.m1.2.2.2.5.cmml" xref="S3.E1.m1.2.2.2.5">𝑔</ci><cn type="integer" id="S3.E1.m1.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.1">64</cn></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">d=min(1.0,\frac{log(N+1)}{log(64)})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">TABLE I: </span>The architectures of the proposed multi-level and multi-modal sensor fusion scheme.</figcaption>
<div id="S3.T1.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:312.2pt;height:136pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-29.8pt,13.0pt) scale(0.839514584072137,0.839514584072137) ;">
<table id="S3.T1.3.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.3.1.1.1" class="ltx_tr">
<td id="S3.T1.3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;" rowspan="6"><span id="S3.T1.3.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Multi-level Fusion</span></td>
<td id="S3.T1.3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.1.1.2.1" class="ltx_text" style="font-size:80%;">Bilinear-1</span></td>
<td id="S3.T1.3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.1.1.3.1" class="ltx_text" style="font-size:80%;">2×</span></td>
<td id="S3.T1.3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.1.1.4.1" class="ltx_text" style="font-size:80%;">180×600×128</span></td>
<td id="S3.T1.3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.1.1.5.1" class="ltx_text" style="font-size:80%;">352×400×128</span></td>
</tr>
<tr id="S3.T1.3.1.2.2" class="ltx_tr">
<td id="S3.T1.3.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.2.2.1.1" class="ltx_text" style="font-size:80%;">Conv-8</span></td>
<td id="S3.T1.3.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.2.2.2.1" class="ltx_text" style="font-size:80%;">3×3×64</span></td>
<td id="S3.T1.3.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.2.2.3.1" class="ltx_text" style="font-size:80%;">180×600×64</span></td>
<td id="S3.T1.3.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.2.2.4.1" class="ltx_text" style="font-size:80%;">352×400×64</span></td>
</tr>
<tr id="S3.T1.3.1.3.3" class="ltx_tr">
<td id="S3.T1.3.1.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.3.3.1.1" class="ltx_text" style="font-size:80%;">Element-wise-Max-1</span></td>
<td id="S3.T1.3.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.3.3.2.1" class="ltx_text" style="font-size:80%;">Conv-8+Conv-6</span></td>
<td id="S3.T1.3.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.3.3.3.1" class="ltx_text" style="font-size:80%;">180×600×64</span></td>
<td id="S3.T1.3.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.3.3.4.1" class="ltx_text" style="font-size:80%;">352×400×64</span></td>
</tr>
<tr id="S3.T1.3.1.4.4" class="ltx_tr">
<td id="S3.T1.3.1.4.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.4.4.1.1" class="ltx_text" style="font-size:80%;">Bilinear-2</span></td>
<td id="S3.T1.3.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.4.4.2.1" class="ltx_text" style="font-size:80%;">2×</span></td>
<td id="S3.T1.3.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.4.4.3.1" class="ltx_text" style="font-size:80%;">360×1200×64</span></td>
<td id="S3.T1.3.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.4.4.4.1" class="ltx_text" style="font-size:80%;">704×800×64</span></td>
</tr>
<tr id="S3.T1.3.1.5.5" class="ltx_tr">
<td id="S3.T1.3.1.5.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.5.5.1.1" class="ltx_text" style="font-size:80%;">Conv-9</span></td>
<td id="S3.T1.3.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.5.5.2.1" class="ltx_text" style="font-size:80%;">3×3×32</span></td>
<td id="S3.T1.3.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.5.5.3.1" class="ltx_text" style="font-size:80%;">360×1200×32</span></td>
<td id="S3.T1.3.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.5.5.4.1" class="ltx_text" style="font-size:80%;">704×800×32</span></td>
</tr>
<tr id="S3.T1.3.1.6.6" class="ltx_tr">
<td id="S3.T1.3.1.6.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.6.6.1.1" class="ltx_text" style="font-size:80%;">Element-wise-Max-2</span></td>
<td id="S3.T1.3.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.6.6.2.1" class="ltx_text" style="font-size:80%;">Conv-9+Conv-5</span></td>
<td id="S3.T1.3.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.6.6.3.1" class="ltx_text" style="font-size:80%;">360×1200×32</span></td>
<td id="S3.T1.3.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.6.6.4.1" class="ltx_text" style="font-size:80%;">704×800×32</span></td>
</tr>
<tr id="S3.T1.3.1.7.7" class="ltx_tr">
<td id="S3.T1.3.1.7.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.7.7.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Dimensionality reduction</span></td>
<td id="S3.T1.3.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.7.7.2.1" class="ltx_text" style="font-size:80%;">Conv-10</span></td>
<td id="S3.T1.3.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.7.7.3.1" class="ltx_text" style="font-size:80%;">1×1×1</span></td>
<td id="S3.T1.3.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.7.7.4.1" class="ltx_text" style="font-size:80%;">360×1200×1</span></td>
<td id="S3.T1.3.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="S3.T1.3.1.7.7.5.1" class="ltx_text" style="font-size:80%;">704×800×1</span></td>
</tr>
<tr id="S3.T1.3.1.8.8" class="ltx_tr">
<td id="S3.T1.3.1.8.8.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;" rowspan="2"><span id="S3.T1.3.1.8.8.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Multi-modal Fusion</span></td>
<td id="S3.T1.3.1.8.8.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;" colspan="4"><span id="S3.T1.3.1.8.8.2.1" class="ltx_text" style="font-size:80%;">Anchor/Proposal Projection + Feature Pooling</span></td>
</tr>
<tr id="S3.T1.3.1.9.9" class="ltx_tr">
<td id="S3.T1.3.1.9.9.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;" colspan="4"><span id="S3.T1.3.1.9.9.1.1" class="ltx_text" style="font-size:80%;">Element-wise mean fusion (Image feature vector + BEV feature vector)</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Feature extractor</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Inspired by the powerful 2D feature extractor proposed in U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, a three-stage feature extractor (Fig. <a href="#S3.F3" title="Figure 3 ‣ III Methodology ‣ Multi-level and Multi-modal Feature Fusion for Accurate 3D Object Detection in Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) is proposed to extract high-level object features from RGB images and BEV maps, including two convolutional stages and one deconvolutional stage. Given the high performance of VGG-16 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> in extracting object features, the first convolutional stage of the proposed feature extractor is built on modifying VGG-16’s neural network architecture, and the modifications include reducing the number of channels and dropping off the fifth set of layers. Reducing the number of channels can mitigate the computation and memory overhead. The fifth set of layers is discarded because hard-difficulty instances are struggling to be accurately detected on feature maps after being downsampled by sixteen times. In addition, inaccurate hard-difficulty object features are harmful to the details recovery process in the following two stages, leading to accuracy drops. Therefore, the first convolutional stage finally outputs a high-level feature map with one-eighth resolution.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">To recover object features from low resolution to full resolution, the proposed deconvolutional stage utilises transposed layers (deconvolutional layers) to achieve trainable upsampling. The one-eighth-resolution feature maps from the first convolutional stage are gradually upsampled to full resolution through three transposed layers. Compared with bilinear upsampling, which utilises linear interpolations to calculate the pixels’ values from the nearby pixels, transposed layers provide a more trainable and effective upsampling process.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">However, the details lost in the downsampling and convolutional process cannot be recovered by the deconvolutional layers, and inaccurate features are harmful to the convolutional process, increasing the probability of False-Positive detection. To this end, lateral connections are applied to concatenate feature maps of the same spatial sizes across three stages. As shown in Fig. <a href="#S3.F3" title="Figure 3 ‣ III Methodology ‣ Multi-level and Multi-modal Feature Fusion for Accurate 3D Object Detection in Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the outputs from the first convolutional stage are first upsampled by two times via the deconvolutional layer Upconv-1, and the outputs are concatenated with the feature maps from Conv-3 layers in the first stage along the channel dimensionalities. Then, Upconv-2, the next deconvolution layer, takes these concatenated feature maps as inputs and outputs double-sized feature maps after selecting more representative object features from feature maps of the same spatial sizes from different stages. Finally, the double-sized feature maps are concatenated with the ones from Conv-2 layers and upsampled to the full resolution via a deconvolutional layer. The gradual upsampling and lateral connections in this stage offer the deep neural network a chance to re-select more representative object features globally and discard inaccurate features by setting different weights to channels of feature maps.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">The second convolutional stage is designed to enhance features’ qualities and generate higher-level object features. Since features of distant or partially occluded objects may occupy less than one pixel on the eighth-resolution feature map, only three sets of convolutional layers and two max-pooling layers are employed in this stage. Lateral connections are employed again to retain features of distant or partially-occluded objects in the final outputs of the feature extractor. The outputs from the deconvolutional stage are first concatenated with feature maps from Conv-1 layers along channel dimensionalities. The concatenated features are processed by Conv-5 layers and downsampled by a max-pooling layer. The downsampled feature maps are concatenated with those from Conv-2 and Upconv-2 layers, and Conv-6 layers select accurate object features and discard inaccurate ones from concatenated feature maps. Finally, the outputs from Conv-6 layers are concatenated with those from Conv-3 and Upconv-1 layers and processed by Conv-7 layers. The final output of the feature extractor includes one full-resolution feature map, one half-resolution and one quarter-resolution.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">To be noted, the features from the first stage are concatenated to the later stages twice to enhance the performance of features recovery because these features are generated at an early period and preserve the initial and accurate object and background low-level information, making them essential to support achieving details recovery when high-level object features coexist with inaccurate features at a later period.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Multi-modal and Multi-level feature pooling and fusion</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The proposed approach projects 3D anchors and 3D proposals onto multi-view feature maps from the feature extractor to obtain region-wise features. Then the proposed method applies a novel multi-level and multi-modal fusion scheme to fuse these region-wise features across different convolutional layers and data types, enabling the model to find the most representative features element-wise.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The proposed sensor fusion scheme (Fig. <a href="#S3.F2" title="Figure 2 ‣ III Methodology ‣ Multi-level and Multi-modal Feature Fusion for Accurate 3D Object Detection in Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and Table <a href="#S3.T1" title="TABLE I ‣ III-A LIDAR BEV maps generator ‣ III Methodology ‣ Multi-level and Multi-modal Feature Fusion for Accurate 3D Object Detection in Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>) contains four blocks: multi-level feature fusion, dimensionality reduction, feature pooling and multi-modal feature fusion, and it finally outputs element-wise fused feature vectors to fully connected layers.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">The multi-level fusion scheme is designed to achieve feature fusion across convolutional layers for each input view and generate a full-resolution feature map containing the most representative object features. It takes the outputs from the feature extractor as inputs and hierarchically fuses them pixel-wise. 2× bilinear upsampling is first applied to upsample the quarter-resolution feature map to half-resolution, and the channel dimensionalities are reduced from 128 to 64 via Conv-8 layers. A bilinear upsampling operation is selected in this fusion scheme because it shows relatively stable performance and lower computational costs than the nearest-neighbour, bicubic and deconvolution methods. Afterwards, the upsampled feature map is fused with the half-resolution feature map from the feature extractor along channels by applying the element-wise max operation to extract the more representative pixel-wise features. Subsequently, a 2× bilinear upsampling is applied on the fused feature map to raise the size to full resolution, and the channel dimensionality is reduced from 64 to 32 through Conv-9 layers. Finally, this new feature map and the full-resolution feature map from the feature extractor are fused by applying the element-wise max operation to find the most representative pixel-wise features across different convolutional layers. The fused full-resolution feature map, containing pixel-wise maxima, is the output of the multi-level fusion scheme, representing the most representative object features extracted from each input view.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">In the RPN module, a 1×1×1 convolutional layer is applied after the multi-level fusion to reduce the channel dimensionality from thirty-two to one. This dimensionality reduction fuses the pixel-wise features across channels onto one feature map and reduces the memory and computational overhead.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">3D anchors are projected onto each view’s one-channel full resolution feature map, and they are transformed into 2D regions of interest (ROIs) in the RPN. The projection and transformation are achieved by transforming the LIDAR coordinates into BEV and image coordinates. The KITTI dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, used for training and testing the proposed model, provides the LIDAR coordinate system and the calibration matrix to support the transformation and projection between points to pixels. ROI pooling is employed inside each ROI to obtain a fixed-length feature vector element-wise.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.1" class="ltx_p">The element-wise mean operation is employed to fuse feature vectors from two input views, assigning both views with equal weights, and the output of the proposed feature fusion scheme is a fused feature vector element-wise.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">3D region proposal network (RPN)</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.13" class="ltx_p">Similar to the Region Proposal Network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, a set of 3D anchors based on the KITTI dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> is pre-designed and parametrised by (x, y, z, l, w, h). (x, y, z) refers to the centre coordinates of an anchor. z is computed as the distance from the LIDAR to the ground. x and y are varying positions sampled in the point cloud along the x-axis and y-axis at an interval of 0.5 meters. (l, w, h) refer to the dimensions of an anchor box, and they are computed by clustering the samples of each class in the KITTI training set. The proposed approach computed two sets of length and width values for the car class. By rotating each anchor box 90 degrees, four anchors are centred at an (x, y) point. The anchor boxes on the BEV map are parameterised as (<math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="x_{bev}" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><msub id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><mi id="S3.SS4.p1.1.m1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.2.cmml">x</mi><mrow id="S3.SS4.p1.1.m1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.3.cmml"><mi id="S3.SS4.p1.1.m1.1.1.3.2" xref="S3.SS4.p1.1.m1.1.1.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.1.m1.1.1.3.1" xref="S3.SS4.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p1.1.m1.1.1.3.3" xref="S3.SS4.p1.1.m1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.1.m1.1.1.3.1a" xref="S3.SS4.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p1.1.m1.1.1.3.4" xref="S3.SS4.p1.1.m1.1.1.3.4.cmml">v</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p1.1.m1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2">𝑥</ci><apply id="S3.SS4.p1.1.m1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.3"><times id="S3.SS4.p1.1.m1.1.1.3.1.cmml" xref="S3.SS4.p1.1.m1.1.1.3.1"></times><ci id="S3.SS4.p1.1.m1.1.1.3.2.cmml" xref="S3.SS4.p1.1.m1.1.1.3.2">𝑏</ci><ci id="S3.SS4.p1.1.m1.1.1.3.3.cmml" xref="S3.SS4.p1.1.m1.1.1.3.3">𝑒</ci><ci id="S3.SS4.p1.1.m1.1.1.3.4.cmml" xref="S3.SS4.p1.1.m1.1.1.3.4">𝑣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">x_{bev}</annotation></semantics></math>, <math id="S3.SS4.p1.2.m2.1" class="ltx_Math" alttext="y_{bev}" display="inline"><semantics id="S3.SS4.p1.2.m2.1a"><msub id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml"><mi id="S3.SS4.p1.2.m2.1.1.2" xref="S3.SS4.p1.2.m2.1.1.2.cmml">y</mi><mrow id="S3.SS4.p1.2.m2.1.1.3" xref="S3.SS4.p1.2.m2.1.1.3.cmml"><mi id="S3.SS4.p1.2.m2.1.1.3.2" xref="S3.SS4.p1.2.m2.1.1.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.2.m2.1.1.3.1" xref="S3.SS4.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p1.2.m2.1.1.3.3" xref="S3.SS4.p1.2.m2.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.2.m2.1.1.3.1a" xref="S3.SS4.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p1.2.m2.1.1.3.4" xref="S3.SS4.p1.2.m2.1.1.3.4.cmml">v</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><apply id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.2.m2.1.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p1.2.m2.1.1.2.cmml" xref="S3.SS4.p1.2.m2.1.1.2">𝑦</ci><apply id="S3.SS4.p1.2.m2.1.1.3.cmml" xref="S3.SS4.p1.2.m2.1.1.3"><times id="S3.SS4.p1.2.m2.1.1.3.1.cmml" xref="S3.SS4.p1.2.m2.1.1.3.1"></times><ci id="S3.SS4.p1.2.m2.1.1.3.2.cmml" xref="S3.SS4.p1.2.m2.1.1.3.2">𝑏</ci><ci id="S3.SS4.p1.2.m2.1.1.3.3.cmml" xref="S3.SS4.p1.2.m2.1.1.3.3">𝑒</ci><ci id="S3.SS4.p1.2.m2.1.1.3.4.cmml" xref="S3.SS4.p1.2.m2.1.1.3.4">𝑣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">y_{bev}</annotation></semantics></math>, <math id="S3.SS4.p1.3.m3.1" class="ltx_Math" alttext="l_{bev}" display="inline"><semantics id="S3.SS4.p1.3.m3.1a"><msub id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml"><mi id="S3.SS4.p1.3.m3.1.1.2" xref="S3.SS4.p1.3.m3.1.1.2.cmml">l</mi><mrow id="S3.SS4.p1.3.m3.1.1.3" xref="S3.SS4.p1.3.m3.1.1.3.cmml"><mi id="S3.SS4.p1.3.m3.1.1.3.2" xref="S3.SS4.p1.3.m3.1.1.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.3.m3.1.1.3.1" xref="S3.SS4.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p1.3.m3.1.1.3.3" xref="S3.SS4.p1.3.m3.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.3.m3.1.1.3.1a" xref="S3.SS4.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p1.3.m3.1.1.3.4" xref="S3.SS4.p1.3.m3.1.1.3.4.cmml">v</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.1b"><apply id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.3.m3.1.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS4.p1.3.m3.1.1.2.cmml" xref="S3.SS4.p1.3.m3.1.1.2">𝑙</ci><apply id="S3.SS4.p1.3.m3.1.1.3.cmml" xref="S3.SS4.p1.3.m3.1.1.3"><times id="S3.SS4.p1.3.m3.1.1.3.1.cmml" xref="S3.SS4.p1.3.m3.1.1.3.1"></times><ci id="S3.SS4.p1.3.m3.1.1.3.2.cmml" xref="S3.SS4.p1.3.m3.1.1.3.2">𝑏</ci><ci id="S3.SS4.p1.3.m3.1.1.3.3.cmml" xref="S3.SS4.p1.3.m3.1.1.3.3">𝑒</ci><ci id="S3.SS4.p1.3.m3.1.1.3.4.cmml" xref="S3.SS4.p1.3.m3.1.1.3.4">𝑣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.1c">l_{bev}</annotation></semantics></math>, <math id="S3.SS4.p1.4.m4.1" class="ltx_Math" alttext="w_{bev}" display="inline"><semantics id="S3.SS4.p1.4.m4.1a"><msub id="S3.SS4.p1.4.m4.1.1" xref="S3.SS4.p1.4.m4.1.1.cmml"><mi id="S3.SS4.p1.4.m4.1.1.2" xref="S3.SS4.p1.4.m4.1.1.2.cmml">w</mi><mrow id="S3.SS4.p1.4.m4.1.1.3" xref="S3.SS4.p1.4.m4.1.1.3.cmml"><mi id="S3.SS4.p1.4.m4.1.1.3.2" xref="S3.SS4.p1.4.m4.1.1.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.4.m4.1.1.3.1" xref="S3.SS4.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p1.4.m4.1.1.3.3" xref="S3.SS4.p1.4.m4.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.4.m4.1.1.3.1a" xref="S3.SS4.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p1.4.m4.1.1.3.4" xref="S3.SS4.p1.4.m4.1.1.3.4.cmml">v</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.4.m4.1b"><apply id="S3.SS4.p1.4.m4.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.4.m4.1.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS4.p1.4.m4.1.1.2.cmml" xref="S3.SS4.p1.4.m4.1.1.2">𝑤</ci><apply id="S3.SS4.p1.4.m4.1.1.3.cmml" xref="S3.SS4.p1.4.m4.1.1.3"><times id="S3.SS4.p1.4.m4.1.1.3.1.cmml" xref="S3.SS4.p1.4.m4.1.1.3.1"></times><ci id="S3.SS4.p1.4.m4.1.1.3.2.cmml" xref="S3.SS4.p1.4.m4.1.1.3.2">𝑏</ci><ci id="S3.SS4.p1.4.m4.1.1.3.3.cmml" xref="S3.SS4.p1.4.m4.1.1.3.3">𝑒</ci><ci id="S3.SS4.p1.4.m4.1.1.3.4.cmml" xref="S3.SS4.p1.4.m4.1.1.3.4">𝑣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.4.m4.1c">w_{bev}</annotation></semantics></math>) by discretising (x, y, l, w). The anchor boxes with no points inside are dropped. The rest of the anchors are projected onto multi-view feature maps, and we employ our multi-modal and multi-level feature pooling and fusion scheme to obtain a fused feature vector for each region of interest. The fused feature vector is fed to fully connected layers for the probability score (softmax output) and 3D bounding box regression tasks. Orientation regression is not considered in the RPN module, and the boxes only head towards 0 and 90 degrees. The model employs a binary cross-entropy loss function, which compares the predicted probabilities to the ground truth to measure the classification loss. The loss increases when the predicted probability becomes far from the ground truth. The 3D bounding box is regressed by computing (<math id="S3.SS4.p1.5.m5.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.SS4.p1.5.m5.1a"><mi mathvariant="normal" id="S3.SS4.p1.5.m5.1.1" xref="S3.SS4.p1.5.m5.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.5.m5.1b"><ci id="S3.SS4.p1.5.m5.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.5.m5.1c">\Delta</annotation></semantics></math>x, <math id="S3.SS4.p1.6.m6.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.SS4.p1.6.m6.1a"><mi mathvariant="normal" id="S3.SS4.p1.6.m6.1.1" xref="S3.SS4.p1.6.m6.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.6.m6.1b"><ci id="S3.SS4.p1.6.m6.1.1.cmml" xref="S3.SS4.p1.6.m6.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.6.m6.1c">\Delta</annotation></semantics></math>y, <math id="S3.SS4.p1.7.m7.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.SS4.p1.7.m7.1a"><mi mathvariant="normal" id="S3.SS4.p1.7.m7.1.1" xref="S3.SS4.p1.7.m7.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.7.m7.1b"><ci id="S3.SS4.p1.7.m7.1.1.cmml" xref="S3.SS4.p1.7.m7.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.7.m7.1c">\Delta</annotation></semantics></math>z, <math id="S3.SS4.p1.8.m8.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.SS4.p1.8.m8.1a"><mi mathvariant="normal" id="S3.SS4.p1.8.m8.1.1" xref="S3.SS4.p1.8.m8.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.8.m8.1b"><ci id="S3.SS4.p1.8.m8.1.1.cmml" xref="S3.SS4.p1.8.m8.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.8.m8.1c">\Delta</annotation></semantics></math>l, <math id="S3.SS4.p1.9.m9.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.SS4.p1.9.m9.1a"><mi mathvariant="normal" id="S3.SS4.p1.9.m9.1.1" xref="S3.SS4.p1.9.m9.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.9.m9.1b"><ci id="S3.SS4.p1.9.m9.1.1.cmml" xref="S3.SS4.p1.9.m9.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.9.m9.1c">\Delta</annotation></semantics></math>w, <math id="S3.SS4.p1.10.m10.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.SS4.p1.10.m10.1a"><mi mathvariant="normal" id="S3.SS4.p1.10.m10.1.1" xref="S3.SS4.p1.10.m10.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.10.m10.1b"><ci id="S3.SS4.p1.10.m10.1.1.cmml" xref="S3.SS4.p1.10.m10.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.10.m10.1c">\Delta</annotation></semantics></math>h), and the model employs the Smooth L1 loss function proposed in Fast-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> to compute the regression loss. Smooth L1 loss minimises the sum of the absolute values of differences between the ground truth and the predicted value and shows negligible sensitivity to outliers. In addition, (<math id="S3.SS4.p1.11.m11.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.SS4.p1.11.m11.1a"><mi mathvariant="normal" id="S3.SS4.p1.11.m11.1.1" xref="S3.SS4.p1.11.m11.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.11.m11.1b"><ci id="S3.SS4.p1.11.m11.1.1.cmml" xref="S3.SS4.p1.11.m11.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.11.m11.1c">\Delta</annotation></semantics></math>x, <math id="S3.SS4.p1.12.m12.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.SS4.p1.12.m12.1a"><mi mathvariant="normal" id="S3.SS4.p1.12.m12.1.1" xref="S3.SS4.p1.12.m12.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.12.m12.1b"><ci id="S3.SS4.p1.12.m12.1.1.cmml" xref="S3.SS4.p1.12.m12.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.12.m12.1c">\Delta</annotation></semantics></math>y) are normalised with the diagonal d and (<math id="S3.SS4.p1.13.m13.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.SS4.p1.13.m13.1a"><mi mathvariant="normal" id="S3.SS4.p1.13.m13.1.1" xref="S3.SS4.p1.13.m13.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.13.m13.1b"><ci id="S3.SS4.p1.13.m13.1.1.cmml" xref="S3.SS4.p1.13.m13.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.13.m13.1c">\Delta</annotation></semantics></math>z) is normalised with the anchor’s height.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_math_unparsed" alttext="d=\sqrt{{(l_{anchor)}}^{2}+{(w_{anchor)}}^{2}}" display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1"><mi id="S3.E2.m1.1.1.2">d</mi><mo id="S3.E2.m1.1.1.1">=</mo><msqrt id="S3.E2.m1.1.1.3"><mrow id="S3.E2.m1.1.1.3.2"><mo stretchy="false" id="S3.E2.m1.1.1.3.2.1">(</mo><mmultiscripts id="S3.E2.m1.1.1.3.2.2"><mi id="S3.E2.m1.1.1.3.2.2.2.2">l</mi><mrow id="S3.E2.m1.1.1.3.2.2.2.3"><mi id="S3.E2.m1.1.1.3.2.2.2.3.1">a</mi><mi id="S3.E2.m1.1.1.3.2.2.2.3.2">n</mi><mi id="S3.E2.m1.1.1.3.2.2.2.3.3">c</mi><mi id="S3.E2.m1.1.1.3.2.2.2.3.4">h</mi><mi id="S3.E2.m1.1.1.3.2.2.2.3.5">o</mi><mi id="S3.E2.m1.1.1.3.2.2.2.3.6">r</mi><mo stretchy="false" id="S3.E2.m1.1.1.3.2.2.2.3.7">)</mo></mrow><mrow id="S3.E2.m1.1.1.3.2.2a"></mrow><mrow id="S3.E2.m1.1.1.3.2.2b"></mrow><mn id="S3.E2.m1.1.1.3.2.2.3">2</mn></mmultiscripts><mo id="S3.E2.m1.1.1.3.2.3">+</mo><mrow id="S3.E2.m1.1.1.3.2.4"><mo stretchy="false" id="S3.E2.m1.1.1.3.2.4.1">(</mo><mmultiscripts id="S3.E2.m1.1.1.3.2.4.2"><mi id="S3.E2.m1.1.1.3.2.4.2.2.2">w</mi><mrow id="S3.E2.m1.1.1.3.2.4.2.2.3"><mi id="S3.E2.m1.1.1.3.2.4.2.2.3.1">a</mi><mi id="S3.E2.m1.1.1.3.2.4.2.2.3.2">n</mi><mi id="S3.E2.m1.1.1.3.2.4.2.2.3.3">c</mi><mi id="S3.E2.m1.1.1.3.2.4.2.2.3.4">h</mi><mi id="S3.E2.m1.1.1.3.2.4.2.2.3.5">o</mi><mi id="S3.E2.m1.1.1.3.2.4.2.2.3.6">r</mi><mo stretchy="false" id="S3.E2.m1.1.1.3.2.4.2.2.3.7">)</mo></mrow><mrow id="S3.E2.m1.1.1.3.2.4.2a"></mrow><mrow id="S3.E2.m1.1.1.3.2.4.2b"></mrow><mn id="S3.E2.m1.1.1.3.2.4.2.3">2</mn></mmultiscripts></mrow></mrow></msqrt></mrow><annotation encoding="application/x-tex" id="S3.E2.m1.1b">d=\sqrt{{(l_{anchor)}}^{2}+{(w_{anchor)}}^{2}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.5" class="ltx_Math" alttext="\Delta{x},\Delta{y}=\frac{x,y_{GT}-x,y_{anchor}}{d}" display="block"><semantics id="S3.E3.m1.5a"><mrow id="S3.E3.m1.5.5" xref="S3.E3.m1.5.5.cmml"><mrow id="S3.E3.m1.5.5.2.2" xref="S3.E3.m1.5.5.2.3.cmml"><mrow id="S3.E3.m1.4.4.1.1.1" xref="S3.E3.m1.4.4.1.1.1.cmml"><mi mathvariant="normal" id="S3.E3.m1.4.4.1.1.1.2" xref="S3.E3.m1.4.4.1.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.1.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.cmml">​</mo><mi id="S3.E3.m1.4.4.1.1.1.3" xref="S3.E3.m1.4.4.1.1.1.3.cmml">x</mi></mrow><mo id="S3.E3.m1.5.5.2.2.3" xref="S3.E3.m1.5.5.2.3.cmml">,</mo><mrow id="S3.E3.m1.5.5.2.2.2" xref="S3.E3.m1.5.5.2.2.2.cmml"><mi mathvariant="normal" id="S3.E3.m1.5.5.2.2.2.2" xref="S3.E3.m1.5.5.2.2.2.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.5.5.2.2.2.1" xref="S3.E3.m1.5.5.2.2.2.1.cmml">​</mo><mi id="S3.E3.m1.5.5.2.2.2.3" xref="S3.E3.m1.5.5.2.2.2.3.cmml">y</mi></mrow></mrow><mo id="S3.E3.m1.5.5.3" xref="S3.E3.m1.5.5.3.cmml">=</mo><mfrac id="S3.E3.m1.3.3" xref="S3.E3.m1.3.3.cmml"><mrow id="S3.E3.m1.3.3.3.3" xref="S3.E3.m1.3.3.3.4.cmml"><mi id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml">x</mi><mo id="S3.E3.m1.3.3.3.3.3" xref="S3.E3.m1.3.3.3.4.cmml">,</mo><mrow id="S3.E3.m1.2.2.2.2.1" xref="S3.E3.m1.2.2.2.2.1.cmml"><msub id="S3.E3.m1.2.2.2.2.1.2" xref="S3.E3.m1.2.2.2.2.1.2.cmml"><mi id="S3.E3.m1.2.2.2.2.1.2.2" xref="S3.E3.m1.2.2.2.2.1.2.2.cmml">y</mi><mrow id="S3.E3.m1.2.2.2.2.1.2.3" xref="S3.E3.m1.2.2.2.2.1.2.3.cmml"><mi id="S3.E3.m1.2.2.2.2.1.2.3.2" xref="S3.E3.m1.2.2.2.2.1.2.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.2.2.1.2.3.1" xref="S3.E3.m1.2.2.2.2.1.2.3.1.cmml">​</mo><mi id="S3.E3.m1.2.2.2.2.1.2.3.3" xref="S3.E3.m1.2.2.2.2.1.2.3.3.cmml">T</mi></mrow></msub><mo id="S3.E3.m1.2.2.2.2.1.1" xref="S3.E3.m1.2.2.2.2.1.1.cmml">−</mo><mi id="S3.E3.m1.2.2.2.2.1.3" xref="S3.E3.m1.2.2.2.2.1.3.cmml">x</mi></mrow><mo id="S3.E3.m1.3.3.3.3.4" xref="S3.E3.m1.3.3.3.4.cmml">,</mo><msub id="S3.E3.m1.3.3.3.3.2" xref="S3.E3.m1.3.3.3.3.2.cmml"><mi id="S3.E3.m1.3.3.3.3.2.2" xref="S3.E3.m1.3.3.3.3.2.2.cmml">y</mi><mrow id="S3.E3.m1.3.3.3.3.2.3" xref="S3.E3.m1.3.3.3.3.2.3.cmml"><mi id="S3.E3.m1.3.3.3.3.2.3.2" xref="S3.E3.m1.3.3.3.3.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.3.3.2.3.1" xref="S3.E3.m1.3.3.3.3.2.3.1.cmml">​</mo><mi id="S3.E3.m1.3.3.3.3.2.3.3" xref="S3.E3.m1.3.3.3.3.2.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.3.3.2.3.1a" xref="S3.E3.m1.3.3.3.3.2.3.1.cmml">​</mo><mi id="S3.E3.m1.3.3.3.3.2.3.4" xref="S3.E3.m1.3.3.3.3.2.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.3.3.2.3.1b" xref="S3.E3.m1.3.3.3.3.2.3.1.cmml">​</mo><mi id="S3.E3.m1.3.3.3.3.2.3.5" xref="S3.E3.m1.3.3.3.3.2.3.5.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.3.3.2.3.1c" xref="S3.E3.m1.3.3.3.3.2.3.1.cmml">​</mo><mi id="S3.E3.m1.3.3.3.3.2.3.6" xref="S3.E3.m1.3.3.3.3.2.3.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.3.3.2.3.1d" xref="S3.E3.m1.3.3.3.3.2.3.1.cmml">​</mo><mi id="S3.E3.m1.3.3.3.3.2.3.7" xref="S3.E3.m1.3.3.3.3.2.3.7.cmml">r</mi></mrow></msub></mrow><mi id="S3.E3.m1.3.3.5" xref="S3.E3.m1.3.3.5.cmml">d</mi></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.5b"><apply id="S3.E3.m1.5.5.cmml" xref="S3.E3.m1.5.5"><eq id="S3.E3.m1.5.5.3.cmml" xref="S3.E3.m1.5.5.3"></eq><list id="S3.E3.m1.5.5.2.3.cmml" xref="S3.E3.m1.5.5.2.2"><apply id="S3.E3.m1.4.4.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1"><times id="S3.E3.m1.4.4.1.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1"></times><ci id="S3.E3.m1.4.4.1.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.1.2">Δ</ci><ci id="S3.E3.m1.4.4.1.1.1.3.cmml" xref="S3.E3.m1.4.4.1.1.1.3">𝑥</ci></apply><apply id="S3.E3.m1.5.5.2.2.2.cmml" xref="S3.E3.m1.5.5.2.2.2"><times id="S3.E3.m1.5.5.2.2.2.1.cmml" xref="S3.E3.m1.5.5.2.2.2.1"></times><ci id="S3.E3.m1.5.5.2.2.2.2.cmml" xref="S3.E3.m1.5.5.2.2.2.2">Δ</ci><ci id="S3.E3.m1.5.5.2.2.2.3.cmml" xref="S3.E3.m1.5.5.2.2.2.3">𝑦</ci></apply></list><apply id="S3.E3.m1.3.3.cmml" xref="S3.E3.m1.3.3"><divide id="S3.E3.m1.3.3.4.cmml" xref="S3.E3.m1.3.3"></divide><list id="S3.E3.m1.3.3.3.4.cmml" xref="S3.E3.m1.3.3.3.3"><ci id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1">𝑥</ci><apply id="S3.E3.m1.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.2.1"><minus id="S3.E3.m1.2.2.2.2.1.1.cmml" xref="S3.E3.m1.2.2.2.2.1.1"></minus><apply id="S3.E3.m1.2.2.2.2.1.2.cmml" xref="S3.E3.m1.2.2.2.2.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.1.2.1.cmml" xref="S3.E3.m1.2.2.2.2.1.2">subscript</csymbol><ci id="S3.E3.m1.2.2.2.2.1.2.2.cmml" xref="S3.E3.m1.2.2.2.2.1.2.2">𝑦</ci><apply id="S3.E3.m1.2.2.2.2.1.2.3.cmml" xref="S3.E3.m1.2.2.2.2.1.2.3"><times id="S3.E3.m1.2.2.2.2.1.2.3.1.cmml" xref="S3.E3.m1.2.2.2.2.1.2.3.1"></times><ci id="S3.E3.m1.2.2.2.2.1.2.3.2.cmml" xref="S3.E3.m1.2.2.2.2.1.2.3.2">𝐺</ci><ci id="S3.E3.m1.2.2.2.2.1.2.3.3.cmml" xref="S3.E3.m1.2.2.2.2.1.2.3.3">𝑇</ci></apply></apply><ci id="S3.E3.m1.2.2.2.2.1.3.cmml" xref="S3.E3.m1.2.2.2.2.1.3">𝑥</ci></apply><apply id="S3.E3.m1.3.3.3.3.2.cmml" xref="S3.E3.m1.3.3.3.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.3.3.2.1.cmml" xref="S3.E3.m1.3.3.3.3.2">subscript</csymbol><ci id="S3.E3.m1.3.3.3.3.2.2.cmml" xref="S3.E3.m1.3.3.3.3.2.2">𝑦</ci><apply id="S3.E3.m1.3.3.3.3.2.3.cmml" xref="S3.E3.m1.3.3.3.3.2.3"><times id="S3.E3.m1.3.3.3.3.2.3.1.cmml" xref="S3.E3.m1.3.3.3.3.2.3.1"></times><ci id="S3.E3.m1.3.3.3.3.2.3.2.cmml" xref="S3.E3.m1.3.3.3.3.2.3.2">𝑎</ci><ci id="S3.E3.m1.3.3.3.3.2.3.3.cmml" xref="S3.E3.m1.3.3.3.3.2.3.3">𝑛</ci><ci id="S3.E3.m1.3.3.3.3.2.3.4.cmml" xref="S3.E3.m1.3.3.3.3.2.3.4">𝑐</ci><ci id="S3.E3.m1.3.3.3.3.2.3.5.cmml" xref="S3.E3.m1.3.3.3.3.2.3.5">ℎ</ci><ci id="S3.E3.m1.3.3.3.3.2.3.6.cmml" xref="S3.E3.m1.3.3.3.3.2.3.6">𝑜</ci><ci id="S3.E3.m1.3.3.3.3.2.3.7.cmml" xref="S3.E3.m1.3.3.3.3.2.3.7">𝑟</ci></apply></apply></list><ci id="S3.E3.m1.3.3.5.cmml" xref="S3.E3.m1.3.3.5">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.5c">\Delta{x},\Delta{y}=\frac{x,y_{GT}-x,y_{anchor}}{d}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.1" class="ltx_Math" alttext="\Delta{z}=\frac{z_{GT}-z_{anchor}}{h_{anchor}}" display="block"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><mrow id="S3.E4.m1.1.1.2" xref="S3.E4.m1.1.1.2.cmml"><mi mathvariant="normal" id="S3.E4.m1.1.1.2.2" xref="S3.E4.m1.1.1.2.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.2.1" xref="S3.E4.m1.1.1.2.1.cmml">​</mo><mi id="S3.E4.m1.1.1.2.3" xref="S3.E4.m1.1.1.2.3.cmml">z</mi></mrow><mo id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.cmml">=</mo><mfrac id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml"><mrow id="S3.E4.m1.1.1.3.2" xref="S3.E4.m1.1.1.3.2.cmml"><msub id="S3.E4.m1.1.1.3.2.2" xref="S3.E4.m1.1.1.3.2.2.cmml"><mi id="S3.E4.m1.1.1.3.2.2.2" xref="S3.E4.m1.1.1.3.2.2.2.cmml">z</mi><mrow id="S3.E4.m1.1.1.3.2.2.3" xref="S3.E4.m1.1.1.3.2.2.3.cmml"><mi id="S3.E4.m1.1.1.3.2.2.3.2" xref="S3.E4.m1.1.1.3.2.2.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.2.2.3.1" xref="S3.E4.m1.1.1.3.2.2.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.2.2.3.3" xref="S3.E4.m1.1.1.3.2.2.3.3.cmml">T</mi></mrow></msub><mo id="S3.E4.m1.1.1.3.2.1" xref="S3.E4.m1.1.1.3.2.1.cmml">−</mo><msub id="S3.E4.m1.1.1.3.2.3" xref="S3.E4.m1.1.1.3.2.3.cmml"><mi id="S3.E4.m1.1.1.3.2.3.2" xref="S3.E4.m1.1.1.3.2.3.2.cmml">z</mi><mrow id="S3.E4.m1.1.1.3.2.3.3" xref="S3.E4.m1.1.1.3.2.3.3.cmml"><mi id="S3.E4.m1.1.1.3.2.3.3.2" xref="S3.E4.m1.1.1.3.2.3.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.2.3.3.1" xref="S3.E4.m1.1.1.3.2.3.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.2.3.3.3" xref="S3.E4.m1.1.1.3.2.3.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.2.3.3.1a" xref="S3.E4.m1.1.1.3.2.3.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.2.3.3.4" xref="S3.E4.m1.1.1.3.2.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.2.3.3.1b" xref="S3.E4.m1.1.1.3.2.3.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.2.3.3.5" xref="S3.E4.m1.1.1.3.2.3.3.5.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.2.3.3.1c" xref="S3.E4.m1.1.1.3.2.3.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.2.3.3.6" xref="S3.E4.m1.1.1.3.2.3.3.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.2.3.3.1d" xref="S3.E4.m1.1.1.3.2.3.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.2.3.3.7" xref="S3.E4.m1.1.1.3.2.3.3.7.cmml">r</mi></mrow></msub></mrow><msub id="S3.E4.m1.1.1.3.3" xref="S3.E4.m1.1.1.3.3.cmml"><mi id="S3.E4.m1.1.1.3.3.2" xref="S3.E4.m1.1.1.3.3.2.cmml">h</mi><mrow id="S3.E4.m1.1.1.3.3.3" xref="S3.E4.m1.1.1.3.3.3.cmml"><mi id="S3.E4.m1.1.1.3.3.3.2" xref="S3.E4.m1.1.1.3.3.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.3.3.1" xref="S3.E4.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.3.3.3" xref="S3.E4.m1.1.1.3.3.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.3.3.1a" xref="S3.E4.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.3.3.4" xref="S3.E4.m1.1.1.3.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.3.3.1b" xref="S3.E4.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.3.3.5" xref="S3.E4.m1.1.1.3.3.3.5.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.3.3.1c" xref="S3.E4.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.3.3.6" xref="S3.E4.m1.1.1.3.3.3.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.3.3.1d" xref="S3.E4.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.3.3.7" xref="S3.E4.m1.1.1.3.3.3.7.cmml">r</mi></mrow></msub></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><eq id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"></eq><apply id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2"><times id="S3.E4.m1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.2.1"></times><ci id="S3.E4.m1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.2.2">Δ</ci><ci id="S3.E4.m1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.2.3">𝑧</ci></apply><apply id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3"><divide id="S3.E4.m1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.3"></divide><apply id="S3.E4.m1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.3.2"><minus id="S3.E4.m1.1.1.3.2.1.cmml" xref="S3.E4.m1.1.1.3.2.1"></minus><apply id="S3.E4.m1.1.1.3.2.2.cmml" xref="S3.E4.m1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.2.2.1.cmml" xref="S3.E4.m1.1.1.3.2.2">subscript</csymbol><ci id="S3.E4.m1.1.1.3.2.2.2.cmml" xref="S3.E4.m1.1.1.3.2.2.2">𝑧</ci><apply id="S3.E4.m1.1.1.3.2.2.3.cmml" xref="S3.E4.m1.1.1.3.2.2.3"><times id="S3.E4.m1.1.1.3.2.2.3.1.cmml" xref="S3.E4.m1.1.1.3.2.2.3.1"></times><ci id="S3.E4.m1.1.1.3.2.2.3.2.cmml" xref="S3.E4.m1.1.1.3.2.2.3.2">𝐺</ci><ci id="S3.E4.m1.1.1.3.2.2.3.3.cmml" xref="S3.E4.m1.1.1.3.2.2.3.3">𝑇</ci></apply></apply><apply id="S3.E4.m1.1.1.3.2.3.cmml" xref="S3.E4.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.2.3.1.cmml" xref="S3.E4.m1.1.1.3.2.3">subscript</csymbol><ci id="S3.E4.m1.1.1.3.2.3.2.cmml" xref="S3.E4.m1.1.1.3.2.3.2">𝑧</ci><apply id="S3.E4.m1.1.1.3.2.3.3.cmml" xref="S3.E4.m1.1.1.3.2.3.3"><times id="S3.E4.m1.1.1.3.2.3.3.1.cmml" xref="S3.E4.m1.1.1.3.2.3.3.1"></times><ci id="S3.E4.m1.1.1.3.2.3.3.2.cmml" xref="S3.E4.m1.1.1.3.2.3.3.2">𝑎</ci><ci id="S3.E4.m1.1.1.3.2.3.3.3.cmml" xref="S3.E4.m1.1.1.3.2.3.3.3">𝑛</ci><ci id="S3.E4.m1.1.1.3.2.3.3.4.cmml" xref="S3.E4.m1.1.1.3.2.3.3.4">𝑐</ci><ci id="S3.E4.m1.1.1.3.2.3.3.5.cmml" xref="S3.E4.m1.1.1.3.2.3.3.5">ℎ</ci><ci id="S3.E4.m1.1.1.3.2.3.3.6.cmml" xref="S3.E4.m1.1.1.3.2.3.3.6">𝑜</ci><ci id="S3.E4.m1.1.1.3.2.3.3.7.cmml" xref="S3.E4.m1.1.1.3.2.3.3.7">𝑟</ci></apply></apply></apply><apply id="S3.E4.m1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.3.1.cmml" xref="S3.E4.m1.1.1.3.3">subscript</csymbol><ci id="S3.E4.m1.1.1.3.3.2.cmml" xref="S3.E4.m1.1.1.3.3.2">ℎ</ci><apply id="S3.E4.m1.1.1.3.3.3.cmml" xref="S3.E4.m1.1.1.3.3.3"><times id="S3.E4.m1.1.1.3.3.3.1.cmml" xref="S3.E4.m1.1.1.3.3.3.1"></times><ci id="S3.E4.m1.1.1.3.3.3.2.cmml" xref="S3.E4.m1.1.1.3.3.3.2">𝑎</ci><ci id="S3.E4.m1.1.1.3.3.3.3.cmml" xref="S3.E4.m1.1.1.3.3.3.3">𝑛</ci><ci id="S3.E4.m1.1.1.3.3.3.4.cmml" xref="S3.E4.m1.1.1.3.3.3.4">𝑐</ci><ci id="S3.E4.m1.1.1.3.3.3.5.cmml" xref="S3.E4.m1.1.1.3.3.3.5">ℎ</ci><ci id="S3.E4.m1.1.1.3.3.3.6.cmml" xref="S3.E4.m1.1.1.3.3.3.6">𝑜</ci><ci id="S3.E4.m1.1.1.3.3.3.7.cmml" xref="S3.E4.m1.1.1.3.3.3.7">𝑟</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\Delta{z}=\frac{z_{GT}-z_{anchor}}{h_{anchor}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.p5" class="ltx_para">
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.9" class="ltx_Math" alttext="\Delta{l},\Delta{w},\Delta{h}=log(\frac{l,w,h_{GT}}{l,w,h_{anchor}})" display="block"><semantics id="S3.E5.m1.9a"><mrow id="S3.E5.m1.9.9" xref="S3.E5.m1.9.9.cmml"><mrow id="S3.E5.m1.9.9.3.3" xref="S3.E5.m1.9.9.3.4.cmml"><mrow id="S3.E5.m1.7.7.1.1.1" xref="S3.E5.m1.7.7.1.1.1.cmml"><mi mathvariant="normal" id="S3.E5.m1.7.7.1.1.1.2" xref="S3.E5.m1.7.7.1.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.7.7.1.1.1.1" xref="S3.E5.m1.7.7.1.1.1.1.cmml">​</mo><mi id="S3.E5.m1.7.7.1.1.1.3" xref="S3.E5.m1.7.7.1.1.1.3.cmml">l</mi></mrow><mo id="S3.E5.m1.9.9.3.3.4" xref="S3.E5.m1.9.9.3.4.cmml">,</mo><mrow id="S3.E5.m1.8.8.2.2.2" xref="S3.E5.m1.8.8.2.2.2.cmml"><mi mathvariant="normal" id="S3.E5.m1.8.8.2.2.2.2" xref="S3.E5.m1.8.8.2.2.2.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.8.8.2.2.2.1" xref="S3.E5.m1.8.8.2.2.2.1.cmml">​</mo><mi id="S3.E5.m1.8.8.2.2.2.3" xref="S3.E5.m1.8.8.2.2.2.3.cmml">w</mi></mrow><mo id="S3.E5.m1.9.9.3.3.5" xref="S3.E5.m1.9.9.3.4.cmml">,</mo><mrow id="S3.E5.m1.9.9.3.3.3" xref="S3.E5.m1.9.9.3.3.3.cmml"><mi mathvariant="normal" id="S3.E5.m1.9.9.3.3.3.2" xref="S3.E5.m1.9.9.3.3.3.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.9.9.3.3.3.1" xref="S3.E5.m1.9.9.3.3.3.1.cmml">​</mo><mi id="S3.E5.m1.9.9.3.3.3.3" xref="S3.E5.m1.9.9.3.3.3.3.cmml">h</mi></mrow></mrow><mo id="S3.E5.m1.9.9.4" xref="S3.E5.m1.9.9.4.cmml">=</mo><mrow id="S3.E5.m1.9.9.5" xref="S3.E5.m1.9.9.5.cmml"><mi id="S3.E5.m1.9.9.5.2" xref="S3.E5.m1.9.9.5.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.9.9.5.1" xref="S3.E5.m1.9.9.5.1.cmml">​</mo><mi id="S3.E5.m1.9.9.5.3" xref="S3.E5.m1.9.9.5.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.9.9.5.1a" xref="S3.E5.m1.9.9.5.1.cmml">​</mo><mi id="S3.E5.m1.9.9.5.4" xref="S3.E5.m1.9.9.5.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.9.9.5.1b" xref="S3.E5.m1.9.9.5.1.cmml">​</mo><mrow id="S3.E5.m1.9.9.5.5.2" xref="S3.E5.m1.6.6.cmml"><mo stretchy="false" id="S3.E5.m1.9.9.5.5.2.1" xref="S3.E5.m1.6.6.cmml">(</mo><mfrac id="S3.E5.m1.6.6" xref="S3.E5.m1.6.6.cmml"><mrow id="S3.E5.m1.3.3.3.3" xref="S3.E5.m1.3.3.3.4.cmml"><mi id="S3.E5.m1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.cmml">l</mi><mo id="S3.E5.m1.3.3.3.3.2" xref="S3.E5.m1.3.3.3.4.cmml">,</mo><mi id="S3.E5.m1.2.2.2.2" xref="S3.E5.m1.2.2.2.2.cmml">w</mi><mo id="S3.E5.m1.3.3.3.3.3" xref="S3.E5.m1.3.3.3.4.cmml">,</mo><msub id="S3.E5.m1.3.3.3.3.1" xref="S3.E5.m1.3.3.3.3.1.cmml"><mi id="S3.E5.m1.3.3.3.3.1.2" xref="S3.E5.m1.3.3.3.3.1.2.cmml">h</mi><mrow id="S3.E5.m1.3.3.3.3.1.3" xref="S3.E5.m1.3.3.3.3.1.3.cmml"><mi id="S3.E5.m1.3.3.3.3.1.3.2" xref="S3.E5.m1.3.3.3.3.1.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.3.3.3.3.1.3.1" xref="S3.E5.m1.3.3.3.3.1.3.1.cmml">​</mo><mi id="S3.E5.m1.3.3.3.3.1.3.3" xref="S3.E5.m1.3.3.3.3.1.3.3.cmml">T</mi></mrow></msub></mrow><mrow id="S3.E5.m1.6.6.6.3" xref="S3.E5.m1.6.6.6.4.cmml"><mi id="S3.E5.m1.4.4.4.1" xref="S3.E5.m1.4.4.4.1.cmml">l</mi><mo id="S3.E5.m1.6.6.6.3.2" xref="S3.E5.m1.6.6.6.4.cmml">,</mo><mi id="S3.E5.m1.5.5.5.2" xref="S3.E5.m1.5.5.5.2.cmml">w</mi><mo id="S3.E5.m1.6.6.6.3.3" xref="S3.E5.m1.6.6.6.4.cmml">,</mo><msub id="S3.E5.m1.6.6.6.3.1" xref="S3.E5.m1.6.6.6.3.1.cmml"><mi id="S3.E5.m1.6.6.6.3.1.2" xref="S3.E5.m1.6.6.6.3.1.2.cmml">h</mi><mrow id="S3.E5.m1.6.6.6.3.1.3" xref="S3.E5.m1.6.6.6.3.1.3.cmml"><mi id="S3.E5.m1.6.6.6.3.1.3.2" xref="S3.E5.m1.6.6.6.3.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.6.6.6.3.1.3.1" xref="S3.E5.m1.6.6.6.3.1.3.1.cmml">​</mo><mi id="S3.E5.m1.6.6.6.3.1.3.3" xref="S3.E5.m1.6.6.6.3.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.6.6.6.3.1.3.1a" xref="S3.E5.m1.6.6.6.3.1.3.1.cmml">​</mo><mi id="S3.E5.m1.6.6.6.3.1.3.4" xref="S3.E5.m1.6.6.6.3.1.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.6.6.6.3.1.3.1b" xref="S3.E5.m1.6.6.6.3.1.3.1.cmml">​</mo><mi id="S3.E5.m1.6.6.6.3.1.3.5" xref="S3.E5.m1.6.6.6.3.1.3.5.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.6.6.6.3.1.3.1c" xref="S3.E5.m1.6.6.6.3.1.3.1.cmml">​</mo><mi id="S3.E5.m1.6.6.6.3.1.3.6" xref="S3.E5.m1.6.6.6.3.1.3.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.6.6.6.3.1.3.1d" xref="S3.E5.m1.6.6.6.3.1.3.1.cmml">​</mo><mi id="S3.E5.m1.6.6.6.3.1.3.7" xref="S3.E5.m1.6.6.6.3.1.3.7.cmml">r</mi></mrow></msub></mrow></mfrac><mo stretchy="false" id="S3.E5.m1.9.9.5.5.2.2" xref="S3.E5.m1.6.6.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.9b"><apply id="S3.E5.m1.9.9.cmml" xref="S3.E5.m1.9.9"><eq id="S3.E5.m1.9.9.4.cmml" xref="S3.E5.m1.9.9.4"></eq><list id="S3.E5.m1.9.9.3.4.cmml" xref="S3.E5.m1.9.9.3.3"><apply id="S3.E5.m1.7.7.1.1.1.cmml" xref="S3.E5.m1.7.7.1.1.1"><times id="S3.E5.m1.7.7.1.1.1.1.cmml" xref="S3.E5.m1.7.7.1.1.1.1"></times><ci id="S3.E5.m1.7.7.1.1.1.2.cmml" xref="S3.E5.m1.7.7.1.1.1.2">Δ</ci><ci id="S3.E5.m1.7.7.1.1.1.3.cmml" xref="S3.E5.m1.7.7.1.1.1.3">𝑙</ci></apply><apply id="S3.E5.m1.8.8.2.2.2.cmml" xref="S3.E5.m1.8.8.2.2.2"><times id="S3.E5.m1.8.8.2.2.2.1.cmml" xref="S3.E5.m1.8.8.2.2.2.1"></times><ci id="S3.E5.m1.8.8.2.2.2.2.cmml" xref="S3.E5.m1.8.8.2.2.2.2">Δ</ci><ci id="S3.E5.m1.8.8.2.2.2.3.cmml" xref="S3.E5.m1.8.8.2.2.2.3">𝑤</ci></apply><apply id="S3.E5.m1.9.9.3.3.3.cmml" xref="S3.E5.m1.9.9.3.3.3"><times id="S3.E5.m1.9.9.3.3.3.1.cmml" xref="S3.E5.m1.9.9.3.3.3.1"></times><ci id="S3.E5.m1.9.9.3.3.3.2.cmml" xref="S3.E5.m1.9.9.3.3.3.2">Δ</ci><ci id="S3.E5.m1.9.9.3.3.3.3.cmml" xref="S3.E5.m1.9.9.3.3.3.3">ℎ</ci></apply></list><apply id="S3.E5.m1.9.9.5.cmml" xref="S3.E5.m1.9.9.5"><times id="S3.E5.m1.9.9.5.1.cmml" xref="S3.E5.m1.9.9.5.1"></times><ci id="S3.E5.m1.9.9.5.2.cmml" xref="S3.E5.m1.9.9.5.2">𝑙</ci><ci id="S3.E5.m1.9.9.5.3.cmml" xref="S3.E5.m1.9.9.5.3">𝑜</ci><ci id="S3.E5.m1.9.9.5.4.cmml" xref="S3.E5.m1.9.9.5.4">𝑔</ci><apply id="S3.E5.m1.6.6.cmml" xref="S3.E5.m1.9.9.5.5.2"><divide id="S3.E5.m1.6.6.7.cmml" xref="S3.E5.m1.9.9.5.5.2"></divide><list id="S3.E5.m1.3.3.3.4.cmml" xref="S3.E5.m1.3.3.3.3"><ci id="S3.E5.m1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1">𝑙</ci><ci id="S3.E5.m1.2.2.2.2.cmml" xref="S3.E5.m1.2.2.2.2">𝑤</ci><apply id="S3.E5.m1.3.3.3.3.1.cmml" xref="S3.E5.m1.3.3.3.3.1"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.3.3.1.1.cmml" xref="S3.E5.m1.3.3.3.3.1">subscript</csymbol><ci id="S3.E5.m1.3.3.3.3.1.2.cmml" xref="S3.E5.m1.3.3.3.3.1.2">ℎ</ci><apply id="S3.E5.m1.3.3.3.3.1.3.cmml" xref="S3.E5.m1.3.3.3.3.1.3"><times id="S3.E5.m1.3.3.3.3.1.3.1.cmml" xref="S3.E5.m1.3.3.3.3.1.3.1"></times><ci id="S3.E5.m1.3.3.3.3.1.3.2.cmml" xref="S3.E5.m1.3.3.3.3.1.3.2">𝐺</ci><ci id="S3.E5.m1.3.3.3.3.1.3.3.cmml" xref="S3.E5.m1.3.3.3.3.1.3.3">𝑇</ci></apply></apply></list><list id="S3.E5.m1.6.6.6.4.cmml" xref="S3.E5.m1.6.6.6.3"><ci id="S3.E5.m1.4.4.4.1.cmml" xref="S3.E5.m1.4.4.4.1">𝑙</ci><ci id="S3.E5.m1.5.5.5.2.cmml" xref="S3.E5.m1.5.5.5.2">𝑤</ci><apply id="S3.E5.m1.6.6.6.3.1.cmml" xref="S3.E5.m1.6.6.6.3.1"><csymbol cd="ambiguous" id="S3.E5.m1.6.6.6.3.1.1.cmml" xref="S3.E5.m1.6.6.6.3.1">subscript</csymbol><ci id="S3.E5.m1.6.6.6.3.1.2.cmml" xref="S3.E5.m1.6.6.6.3.1.2">ℎ</ci><apply id="S3.E5.m1.6.6.6.3.1.3.cmml" xref="S3.E5.m1.6.6.6.3.1.3"><times id="S3.E5.m1.6.6.6.3.1.3.1.cmml" xref="S3.E5.m1.6.6.6.3.1.3.1"></times><ci id="S3.E5.m1.6.6.6.3.1.3.2.cmml" xref="S3.E5.m1.6.6.6.3.1.3.2">𝑎</ci><ci id="S3.E5.m1.6.6.6.3.1.3.3.cmml" xref="S3.E5.m1.6.6.6.3.1.3.3">𝑛</ci><ci id="S3.E5.m1.6.6.6.3.1.3.4.cmml" xref="S3.E5.m1.6.6.6.3.1.3.4">𝑐</ci><ci id="S3.E5.m1.6.6.6.3.1.3.5.cmml" xref="S3.E5.m1.6.6.6.3.1.3.5">ℎ</ci><ci id="S3.E5.m1.6.6.6.3.1.3.6.cmml" xref="S3.E5.m1.6.6.6.3.1.3.6">𝑜</ci><ci id="S3.E5.m1.6.6.6.3.1.3.7.cmml" xref="S3.E5.m1.6.6.6.3.1.3.7">𝑟</ci></apply></apply></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.9c">\Delta{l},\Delta{w},\Delta{h}=log(\frac{l,w,h_{GT}}{l,w,h_{anchor}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.p5.4" class="ltx_p">Where <math id="S3.SS4.p5.1.m1.3" class="ltx_Math" alttext="l,w,h_{anchor}" display="inline"><semantics id="S3.SS4.p5.1.m1.3a"><mrow id="S3.SS4.p5.1.m1.3.3.1" xref="S3.SS4.p5.1.m1.3.3.2.cmml"><mi id="S3.SS4.p5.1.m1.1.1" xref="S3.SS4.p5.1.m1.1.1.cmml">l</mi><mo id="S3.SS4.p5.1.m1.3.3.1.2" xref="S3.SS4.p5.1.m1.3.3.2.cmml">,</mo><mi id="S3.SS4.p5.1.m1.2.2" xref="S3.SS4.p5.1.m1.2.2.cmml">w</mi><mo id="S3.SS4.p5.1.m1.3.3.1.3" xref="S3.SS4.p5.1.m1.3.3.2.cmml">,</mo><msub id="S3.SS4.p5.1.m1.3.3.1.1" xref="S3.SS4.p5.1.m1.3.3.1.1.cmml"><mi id="S3.SS4.p5.1.m1.3.3.1.1.2" xref="S3.SS4.p5.1.m1.3.3.1.1.2.cmml">h</mi><mrow id="S3.SS4.p5.1.m1.3.3.1.1.3" xref="S3.SS4.p5.1.m1.3.3.1.1.3.cmml"><mi id="S3.SS4.p5.1.m1.3.3.1.1.3.2" xref="S3.SS4.p5.1.m1.3.3.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p5.1.m1.3.3.1.1.3.1" xref="S3.SS4.p5.1.m1.3.3.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p5.1.m1.3.3.1.1.3.3" xref="S3.SS4.p5.1.m1.3.3.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p5.1.m1.3.3.1.1.3.1a" xref="S3.SS4.p5.1.m1.3.3.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p5.1.m1.3.3.1.1.3.4" xref="S3.SS4.p5.1.m1.3.3.1.1.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p5.1.m1.3.3.1.1.3.1b" xref="S3.SS4.p5.1.m1.3.3.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p5.1.m1.3.3.1.1.3.5" xref="S3.SS4.p5.1.m1.3.3.1.1.3.5.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p5.1.m1.3.3.1.1.3.1c" xref="S3.SS4.p5.1.m1.3.3.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p5.1.m1.3.3.1.1.3.6" xref="S3.SS4.p5.1.m1.3.3.1.1.3.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p5.1.m1.3.3.1.1.3.1d" xref="S3.SS4.p5.1.m1.3.3.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p5.1.m1.3.3.1.1.3.7" xref="S3.SS4.p5.1.m1.3.3.1.1.3.7.cmml">r</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p5.1.m1.3b"><list id="S3.SS4.p5.1.m1.3.3.2.cmml" xref="S3.SS4.p5.1.m1.3.3.1"><ci id="S3.SS4.p5.1.m1.1.1.cmml" xref="S3.SS4.p5.1.m1.1.1">𝑙</ci><ci id="S3.SS4.p5.1.m1.2.2.cmml" xref="S3.SS4.p5.1.m1.2.2">𝑤</ci><apply id="S3.SS4.p5.1.m1.3.3.1.1.cmml" xref="S3.SS4.p5.1.m1.3.3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p5.1.m1.3.3.1.1.1.cmml" xref="S3.SS4.p5.1.m1.3.3.1.1">subscript</csymbol><ci id="S3.SS4.p5.1.m1.3.3.1.1.2.cmml" xref="S3.SS4.p5.1.m1.3.3.1.1.2">ℎ</ci><apply id="S3.SS4.p5.1.m1.3.3.1.1.3.cmml" xref="S3.SS4.p5.1.m1.3.3.1.1.3"><times id="S3.SS4.p5.1.m1.3.3.1.1.3.1.cmml" xref="S3.SS4.p5.1.m1.3.3.1.1.3.1"></times><ci id="S3.SS4.p5.1.m1.3.3.1.1.3.2.cmml" xref="S3.SS4.p5.1.m1.3.3.1.1.3.2">𝑎</ci><ci id="S3.SS4.p5.1.m1.3.3.1.1.3.3.cmml" xref="S3.SS4.p5.1.m1.3.3.1.1.3.3">𝑛</ci><ci id="S3.SS4.p5.1.m1.3.3.1.1.3.4.cmml" xref="S3.SS4.p5.1.m1.3.3.1.1.3.4">𝑐</ci><ci id="S3.SS4.p5.1.m1.3.3.1.1.3.5.cmml" xref="S3.SS4.p5.1.m1.3.3.1.1.3.5">ℎ</ci><ci id="S3.SS4.p5.1.m1.3.3.1.1.3.6.cmml" xref="S3.SS4.p5.1.m1.3.3.1.1.3.6">𝑜</ci><ci id="S3.SS4.p5.1.m1.3.3.1.1.3.7.cmml" xref="S3.SS4.p5.1.m1.3.3.1.1.3.7">𝑟</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p5.1.m1.3c">l,w,h_{anchor}</annotation></semantics></math> are anchor’s length, width and height, <math id="S3.SS4.p5.2.m2.3" class="ltx_Math" alttext="l,w,h_{GT}" display="inline"><semantics id="S3.SS4.p5.2.m2.3a"><mrow id="S3.SS4.p5.2.m2.3.3.1" xref="S3.SS4.p5.2.m2.3.3.2.cmml"><mi id="S3.SS4.p5.2.m2.1.1" xref="S3.SS4.p5.2.m2.1.1.cmml">l</mi><mo id="S3.SS4.p5.2.m2.3.3.1.2" xref="S3.SS4.p5.2.m2.3.3.2.cmml">,</mo><mi id="S3.SS4.p5.2.m2.2.2" xref="S3.SS4.p5.2.m2.2.2.cmml">w</mi><mo id="S3.SS4.p5.2.m2.3.3.1.3" xref="S3.SS4.p5.2.m2.3.3.2.cmml">,</mo><msub id="S3.SS4.p5.2.m2.3.3.1.1" xref="S3.SS4.p5.2.m2.3.3.1.1.cmml"><mi id="S3.SS4.p5.2.m2.3.3.1.1.2" xref="S3.SS4.p5.2.m2.3.3.1.1.2.cmml">h</mi><mrow id="S3.SS4.p5.2.m2.3.3.1.1.3" xref="S3.SS4.p5.2.m2.3.3.1.1.3.cmml"><mi id="S3.SS4.p5.2.m2.3.3.1.1.3.2" xref="S3.SS4.p5.2.m2.3.3.1.1.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p5.2.m2.3.3.1.1.3.1" xref="S3.SS4.p5.2.m2.3.3.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p5.2.m2.3.3.1.1.3.3" xref="S3.SS4.p5.2.m2.3.3.1.1.3.3.cmml">T</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p5.2.m2.3b"><list id="S3.SS4.p5.2.m2.3.3.2.cmml" xref="S3.SS4.p5.2.m2.3.3.1"><ci id="S3.SS4.p5.2.m2.1.1.cmml" xref="S3.SS4.p5.2.m2.1.1">𝑙</ci><ci id="S3.SS4.p5.2.m2.2.2.cmml" xref="S3.SS4.p5.2.m2.2.2">𝑤</ci><apply id="S3.SS4.p5.2.m2.3.3.1.1.cmml" xref="S3.SS4.p5.2.m2.3.3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p5.2.m2.3.3.1.1.1.cmml" xref="S3.SS4.p5.2.m2.3.3.1.1">subscript</csymbol><ci id="S3.SS4.p5.2.m2.3.3.1.1.2.cmml" xref="S3.SS4.p5.2.m2.3.3.1.1.2">ℎ</ci><apply id="S3.SS4.p5.2.m2.3.3.1.1.3.cmml" xref="S3.SS4.p5.2.m2.3.3.1.1.3"><times id="S3.SS4.p5.2.m2.3.3.1.1.3.1.cmml" xref="S3.SS4.p5.2.m2.3.3.1.1.3.1"></times><ci id="S3.SS4.p5.2.m2.3.3.1.1.3.2.cmml" xref="S3.SS4.p5.2.m2.3.3.1.1.3.2">𝐺</ci><ci id="S3.SS4.p5.2.m2.3.3.1.1.3.3.cmml" xref="S3.SS4.p5.2.m2.3.3.1.1.3.3">𝑇</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p5.2.m2.3c">l,w,h_{GT}</annotation></semantics></math> are ground truth box’s length, width and height, <math id="S3.SS4.p5.3.m3.3" class="ltx_Math" alttext="x,y,z_{GT}" display="inline"><semantics id="S3.SS4.p5.3.m3.3a"><mrow id="S3.SS4.p5.3.m3.3.3.1" xref="S3.SS4.p5.3.m3.3.3.2.cmml"><mi id="S3.SS4.p5.3.m3.1.1" xref="S3.SS4.p5.3.m3.1.1.cmml">x</mi><mo id="S3.SS4.p5.3.m3.3.3.1.2" xref="S3.SS4.p5.3.m3.3.3.2.cmml">,</mo><mi id="S3.SS4.p5.3.m3.2.2" xref="S3.SS4.p5.3.m3.2.2.cmml">y</mi><mo id="S3.SS4.p5.3.m3.3.3.1.3" xref="S3.SS4.p5.3.m3.3.3.2.cmml">,</mo><msub id="S3.SS4.p5.3.m3.3.3.1.1" xref="S3.SS4.p5.3.m3.3.3.1.1.cmml"><mi id="S3.SS4.p5.3.m3.3.3.1.1.2" xref="S3.SS4.p5.3.m3.3.3.1.1.2.cmml">z</mi><mrow id="S3.SS4.p5.3.m3.3.3.1.1.3" xref="S3.SS4.p5.3.m3.3.3.1.1.3.cmml"><mi id="S3.SS4.p5.3.m3.3.3.1.1.3.2" xref="S3.SS4.p5.3.m3.3.3.1.1.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p5.3.m3.3.3.1.1.3.1" xref="S3.SS4.p5.3.m3.3.3.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p5.3.m3.3.3.1.1.3.3" xref="S3.SS4.p5.3.m3.3.3.1.1.3.3.cmml">T</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p5.3.m3.3b"><list id="S3.SS4.p5.3.m3.3.3.2.cmml" xref="S3.SS4.p5.3.m3.3.3.1"><ci id="S3.SS4.p5.3.m3.1.1.cmml" xref="S3.SS4.p5.3.m3.1.1">𝑥</ci><ci id="S3.SS4.p5.3.m3.2.2.cmml" xref="S3.SS4.p5.3.m3.2.2">𝑦</ci><apply id="S3.SS4.p5.3.m3.3.3.1.1.cmml" xref="S3.SS4.p5.3.m3.3.3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p5.3.m3.3.3.1.1.1.cmml" xref="S3.SS4.p5.3.m3.3.3.1.1">subscript</csymbol><ci id="S3.SS4.p5.3.m3.3.3.1.1.2.cmml" xref="S3.SS4.p5.3.m3.3.3.1.1.2">𝑧</ci><apply id="S3.SS4.p5.3.m3.3.3.1.1.3.cmml" xref="S3.SS4.p5.3.m3.3.3.1.1.3"><times id="S3.SS4.p5.3.m3.3.3.1.1.3.1.cmml" xref="S3.SS4.p5.3.m3.3.3.1.1.3.1"></times><ci id="S3.SS4.p5.3.m3.3.3.1.1.3.2.cmml" xref="S3.SS4.p5.3.m3.3.3.1.1.3.2">𝐺</ci><ci id="S3.SS4.p5.3.m3.3.3.1.1.3.3.cmml" xref="S3.SS4.p5.3.m3.3.3.1.1.3.3">𝑇</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p5.3.m3.3c">x,y,z_{GT}</annotation></semantics></math> are centre coordinates of the ground truth box, and <math id="S3.SS4.p5.4.m4.3" class="ltx_Math" alttext="x,y,z_{anchor}" display="inline"><semantics id="S3.SS4.p5.4.m4.3a"><mrow id="S3.SS4.p5.4.m4.3.3.1" xref="S3.SS4.p5.4.m4.3.3.2.cmml"><mi id="S3.SS4.p5.4.m4.1.1" xref="S3.SS4.p5.4.m4.1.1.cmml">x</mi><mo id="S3.SS4.p5.4.m4.3.3.1.2" xref="S3.SS4.p5.4.m4.3.3.2.cmml">,</mo><mi id="S3.SS4.p5.4.m4.2.2" xref="S3.SS4.p5.4.m4.2.2.cmml">y</mi><mo id="S3.SS4.p5.4.m4.3.3.1.3" xref="S3.SS4.p5.4.m4.3.3.2.cmml">,</mo><msub id="S3.SS4.p5.4.m4.3.3.1.1" xref="S3.SS4.p5.4.m4.3.3.1.1.cmml"><mi id="S3.SS4.p5.4.m4.3.3.1.1.2" xref="S3.SS4.p5.4.m4.3.3.1.1.2.cmml">z</mi><mrow id="S3.SS4.p5.4.m4.3.3.1.1.3" xref="S3.SS4.p5.4.m4.3.3.1.1.3.cmml"><mi id="S3.SS4.p5.4.m4.3.3.1.1.3.2" xref="S3.SS4.p5.4.m4.3.3.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p5.4.m4.3.3.1.1.3.1" xref="S3.SS4.p5.4.m4.3.3.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p5.4.m4.3.3.1.1.3.3" xref="S3.SS4.p5.4.m4.3.3.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p5.4.m4.3.3.1.1.3.1a" xref="S3.SS4.p5.4.m4.3.3.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p5.4.m4.3.3.1.1.3.4" xref="S3.SS4.p5.4.m4.3.3.1.1.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p5.4.m4.3.3.1.1.3.1b" xref="S3.SS4.p5.4.m4.3.3.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p5.4.m4.3.3.1.1.3.5" xref="S3.SS4.p5.4.m4.3.3.1.1.3.5.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p5.4.m4.3.3.1.1.3.1c" xref="S3.SS4.p5.4.m4.3.3.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p5.4.m4.3.3.1.1.3.6" xref="S3.SS4.p5.4.m4.3.3.1.1.3.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p5.4.m4.3.3.1.1.3.1d" xref="S3.SS4.p5.4.m4.3.3.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p5.4.m4.3.3.1.1.3.7" xref="S3.SS4.p5.4.m4.3.3.1.1.3.7.cmml">r</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p5.4.m4.3b"><list id="S3.SS4.p5.4.m4.3.3.2.cmml" xref="S3.SS4.p5.4.m4.3.3.1"><ci id="S3.SS4.p5.4.m4.1.1.cmml" xref="S3.SS4.p5.4.m4.1.1">𝑥</ci><ci id="S3.SS4.p5.4.m4.2.2.cmml" xref="S3.SS4.p5.4.m4.2.2">𝑦</ci><apply id="S3.SS4.p5.4.m4.3.3.1.1.cmml" xref="S3.SS4.p5.4.m4.3.3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p5.4.m4.3.3.1.1.1.cmml" xref="S3.SS4.p5.4.m4.3.3.1.1">subscript</csymbol><ci id="S3.SS4.p5.4.m4.3.3.1.1.2.cmml" xref="S3.SS4.p5.4.m4.3.3.1.1.2">𝑧</ci><apply id="S3.SS4.p5.4.m4.3.3.1.1.3.cmml" xref="S3.SS4.p5.4.m4.3.3.1.1.3"><times id="S3.SS4.p5.4.m4.3.3.1.1.3.1.cmml" xref="S3.SS4.p5.4.m4.3.3.1.1.3.1"></times><ci id="S3.SS4.p5.4.m4.3.3.1.1.3.2.cmml" xref="S3.SS4.p5.4.m4.3.3.1.1.3.2">𝑎</ci><ci id="S3.SS4.p5.4.m4.3.3.1.1.3.3.cmml" xref="S3.SS4.p5.4.m4.3.3.1.1.3.3">𝑛</ci><ci id="S3.SS4.p5.4.m4.3.3.1.1.3.4.cmml" xref="S3.SS4.p5.4.m4.3.3.1.1.3.4">𝑐</ci><ci id="S3.SS4.p5.4.m4.3.3.1.1.3.5.cmml" xref="S3.SS4.p5.4.m4.3.3.1.1.3.5">ℎ</ci><ci id="S3.SS4.p5.4.m4.3.3.1.1.3.6.cmml" xref="S3.SS4.p5.4.m4.3.3.1.1.3.6">𝑜</ci><ci id="S3.SS4.p5.4.m4.3.3.1.1.3.7.cmml" xref="S3.SS4.p5.4.m4.3.3.1.1.3.7">𝑟</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p5.4.m4.3c">x,y,z_{anchor}</annotation></semantics></math> are centre coordinates of the anchor.</p>
</div>
<div id="S3.SS4.p6" class="ltx_para">
<p id="S3.SS4.p6.1" class="ltx_p">Like the RPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, 2D IoU on the BEV map is computed between anchors and ground truth boxes, and background anchors are ignored when computing the regression loss. IoU threshold is set to 0.4 and 0.6 in terms of the car class. If IoU is above 0.6, anchors are positive. If IoU is below 0.4, anchors are negative. 2D non-maximal suppression (NMS) is applied to the predicted boxes to suppress redundant boxes, with an IoU threshold of 0.7.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS5.4.1.1" class="ltx_text">III-E</span> </span><span id="S3.SS5.5.2" class="ltx_text ltx_font_italic">Detection Head (DH)</span>
</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">3D proposals generated by the RPN are projected onto 32-channel full-resolution feature maps from the feature extractor. Dimensionality reduction is not implemented in the DH module, considering that the number of proposals is much lower than anchors. Following the same methods taken in the RPN module, a fused feature vector is obtained for each ROI, and the fully connected layers process the fused feature vectors for the final classification and 3D bounding box regression tasks.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.17" class="ltx_p">To regress the orientation of the 3D bounding box, we parametrise a 3D ground truth box as (x, y, z, l, w, h, <math id="S3.SS5.p2.1.m1.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS5.p2.1.m1.1a"><mi id="S3.SS5.p2.1.m1.1.1" xref="S3.SS5.p2.1.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.1.m1.1b"><ci id="S3.SS5.p2.1.m1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.1.m1.1c">\theta</annotation></semantics></math>), where <math id="S3.SS5.p2.2.m2.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS5.p2.2.m2.1a"><mi id="S3.SS5.p2.2.m2.1.1" xref="S3.SS5.p2.2.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.2.m2.1b"><ci id="S3.SS5.p2.2.m2.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.2.m2.1c">\theta</annotation></semantics></math> is the yaw rotation angle. The orientation of the 3D bounding box is estimated directly. Similar to the methods taken in the RPN module, the model applies the binary cross-entropy loss function to compute the classification loss and the SmoothL1 loss function <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> to compute the regression loss of (<math id="S3.SS5.p2.3.m3.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.SS5.p2.3.m3.1a"><mi mathvariant="normal" id="S3.SS5.p2.3.m3.1.1" xref="S3.SS5.p2.3.m3.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.3.m3.1b"><ci id="S3.SS5.p2.3.m3.1.1.cmml" xref="S3.SS5.p2.3.m3.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.3.m3.1c">\Delta</annotation></semantics></math>x, <math id="S3.SS5.p2.4.m4.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.SS5.p2.4.m4.1a"><mi mathvariant="normal" id="S3.SS5.p2.4.m4.1.1" xref="S3.SS5.p2.4.m4.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.4.m4.1b"><ci id="S3.SS5.p2.4.m4.1.1.cmml" xref="S3.SS5.p2.4.m4.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.4.m4.1c">\Delta</annotation></semantics></math>y, <math id="S3.SS5.p2.5.m5.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.SS5.p2.5.m5.1a"><mi mathvariant="normal" id="S3.SS5.p2.5.m5.1.1" xref="S3.SS5.p2.5.m5.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.5.m5.1b"><ci id="S3.SS5.p2.5.m5.1.1.cmml" xref="S3.SS5.p2.5.m5.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.5.m5.1c">\Delta</annotation></semantics></math>z, <math id="S3.SS5.p2.6.m6.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.SS5.p2.6.m6.1a"><mi mathvariant="normal" id="S3.SS5.p2.6.m6.1.1" xref="S3.SS5.p2.6.m6.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.6.m6.1b"><ci id="S3.SS5.p2.6.m6.1.1.cmml" xref="S3.SS5.p2.6.m6.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.6.m6.1c">\Delta</annotation></semantics></math>l, <math id="S3.SS5.p2.7.m7.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.SS5.p2.7.m7.1a"><mi mathvariant="normal" id="S3.SS5.p2.7.m7.1.1" xref="S3.SS5.p2.7.m7.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.7.m7.1b"><ci id="S3.SS5.p2.7.m7.1.1.cmml" xref="S3.SS5.p2.7.m7.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.7.m7.1c">\Delta</annotation></semantics></math>w, <math id="S3.SS5.p2.8.m8.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.SS5.p2.8.m8.1a"><mi mathvariant="normal" id="S3.SS5.p2.8.m8.1.1" xref="S3.SS5.p2.8.m8.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.8.m8.1b"><ci id="S3.SS5.p2.8.m8.1.1.cmml" xref="S3.SS5.p2.8.m8.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.8.m8.1c">\Delta</annotation></semantics></math>h, <math id="S3.SS5.p2.9.m9.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.SS5.p2.9.m9.1a"><mi mathvariant="normal" id="S3.SS5.p2.9.m9.1.1" xref="S3.SS5.p2.9.m9.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.9.m9.1b"><ci id="S3.SS5.p2.9.m9.1.1.cmml" xref="S3.SS5.p2.9.m9.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.9.m9.1c">\Delta</annotation></semantics></math><math id="S3.SS5.p2.10.m10.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS5.p2.10.m10.1a"><mi id="S3.SS5.p2.10.m10.1.1" xref="S3.SS5.p2.10.m10.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.10.m10.1b"><ci id="S3.SS5.p2.10.m10.1.1.cmml" xref="S3.SS5.p2.10.m10.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.10.m10.1c">\theta</annotation></semantics></math>). Regression of (<math id="S3.SS5.p2.11.m11.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.SS5.p2.11.m11.1a"><mi mathvariant="normal" id="S3.SS5.p2.11.m11.1.1" xref="S3.SS5.p2.11.m11.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.11.m11.1b"><ci id="S3.SS5.p2.11.m11.1.1.cmml" xref="S3.SS5.p2.11.m11.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.11.m11.1c">\Delta</annotation></semantics></math>x, <math id="S3.SS5.p2.12.m12.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.SS5.p2.12.m12.1a"><mi mathvariant="normal" id="S3.SS5.p2.12.m12.1.1" xref="S3.SS5.p2.12.m12.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.12.m12.1b"><ci id="S3.SS5.p2.12.m12.1.1.cmml" xref="S3.SS5.p2.12.m12.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.12.m12.1c">\Delta</annotation></semantics></math>y, <math id="S3.SS5.p2.13.m13.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.SS5.p2.13.m13.1a"><mi mathvariant="normal" id="S3.SS5.p2.13.m13.1.1" xref="S3.SS5.p2.13.m13.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.13.m13.1b"><ci id="S3.SS5.p2.13.m13.1.1.cmml" xref="S3.SS5.p2.13.m13.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.13.m13.1c">\Delta</annotation></semantics></math>z, <math id="S3.SS5.p2.14.m14.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.SS5.p2.14.m14.1a"><mi mathvariant="normal" id="S3.SS5.p2.14.m14.1.1" xref="S3.SS5.p2.14.m14.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.14.m14.1b"><ci id="S3.SS5.p2.14.m14.1.1.cmml" xref="S3.SS5.p2.14.m14.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.14.m14.1c">\Delta</annotation></semantics></math>l, <math id="S3.SS5.p2.15.m15.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.SS5.p2.15.m15.1a"><mi mathvariant="normal" id="S3.SS5.p2.15.m15.1.1" xref="S3.SS5.p2.15.m15.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.15.m15.1b"><ci id="S3.SS5.p2.15.m15.1.1.cmml" xref="S3.SS5.p2.15.m15.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.15.m15.1c">\Delta</annotation></semantics></math>w, <math id="S3.SS5.p2.16.m16.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.SS5.p2.16.m16.1a"><mi mathvariant="normal" id="S3.SS5.p2.16.m16.1.1" xref="S3.SS5.p2.16.m16.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.16.m16.1b"><ci id="S3.SS5.p2.16.m16.1.1.cmml" xref="S3.SS5.p2.16.m16.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.16.m16.1c">\Delta</annotation></semantics></math>h) follows the same equations utilised in the RPN module, and the regression of orientation <math id="S3.SS5.p2.17.m17.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS5.p2.17.m17.1a"><mi id="S3.SS5.p2.17.m17.1.1" xref="S3.SS5.p2.17.m17.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.17.m17.1b"><ci id="S3.SS5.p2.17.m17.1.1.cmml" xref="S3.SS5.p2.17.m17.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.17.m17.1c">\theta</annotation></semantics></math> is computed as:</p>
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E6.m1.1" class="ltx_Math" alttext="\Delta{\theta}=\theta_{GT}-\theta_{anchor}" display="block"><semantics id="S3.E6.m1.1a"><mrow id="S3.E6.m1.1.1" xref="S3.E6.m1.1.1.cmml"><mrow id="S3.E6.m1.1.1.2" xref="S3.E6.m1.1.1.2.cmml"><mi mathvariant="normal" id="S3.E6.m1.1.1.2.2" xref="S3.E6.m1.1.1.2.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.2.1" xref="S3.E6.m1.1.1.2.1.cmml">​</mo><mi id="S3.E6.m1.1.1.2.3" xref="S3.E6.m1.1.1.2.3.cmml">θ</mi></mrow><mo id="S3.E6.m1.1.1.1" xref="S3.E6.m1.1.1.1.cmml">=</mo><mrow id="S3.E6.m1.1.1.3" xref="S3.E6.m1.1.1.3.cmml"><msub id="S3.E6.m1.1.1.3.2" xref="S3.E6.m1.1.1.3.2.cmml"><mi id="S3.E6.m1.1.1.3.2.2" xref="S3.E6.m1.1.1.3.2.2.cmml">θ</mi><mrow id="S3.E6.m1.1.1.3.2.3" xref="S3.E6.m1.1.1.3.2.3.cmml"><mi id="S3.E6.m1.1.1.3.2.3.2" xref="S3.E6.m1.1.1.3.2.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.3.2.3.1" xref="S3.E6.m1.1.1.3.2.3.1.cmml">​</mo><mi id="S3.E6.m1.1.1.3.2.3.3" xref="S3.E6.m1.1.1.3.2.3.3.cmml">T</mi></mrow></msub><mo id="S3.E6.m1.1.1.3.1" xref="S3.E6.m1.1.1.3.1.cmml">−</mo><msub id="S3.E6.m1.1.1.3.3" xref="S3.E6.m1.1.1.3.3.cmml"><mi id="S3.E6.m1.1.1.3.3.2" xref="S3.E6.m1.1.1.3.3.2.cmml">θ</mi><mrow id="S3.E6.m1.1.1.3.3.3" xref="S3.E6.m1.1.1.3.3.3.cmml"><mi id="S3.E6.m1.1.1.3.3.3.2" xref="S3.E6.m1.1.1.3.3.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.3.3.3.1" xref="S3.E6.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.E6.m1.1.1.3.3.3.3" xref="S3.E6.m1.1.1.3.3.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.3.3.3.1a" xref="S3.E6.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.E6.m1.1.1.3.3.3.4" xref="S3.E6.m1.1.1.3.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.3.3.3.1b" xref="S3.E6.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.E6.m1.1.1.3.3.3.5" xref="S3.E6.m1.1.1.3.3.3.5.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.3.3.3.1c" xref="S3.E6.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.E6.m1.1.1.3.3.3.6" xref="S3.E6.m1.1.1.3.3.3.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.3.3.3.1d" xref="S3.E6.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.E6.m1.1.1.3.3.3.7" xref="S3.E6.m1.1.1.3.3.3.7.cmml">r</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.1b"><apply id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.1.1"><eq id="S3.E6.m1.1.1.1.cmml" xref="S3.E6.m1.1.1.1"></eq><apply id="S3.E6.m1.1.1.2.cmml" xref="S3.E6.m1.1.1.2"><times id="S3.E6.m1.1.1.2.1.cmml" xref="S3.E6.m1.1.1.2.1"></times><ci id="S3.E6.m1.1.1.2.2.cmml" xref="S3.E6.m1.1.1.2.2">Δ</ci><ci id="S3.E6.m1.1.1.2.3.cmml" xref="S3.E6.m1.1.1.2.3">𝜃</ci></apply><apply id="S3.E6.m1.1.1.3.cmml" xref="S3.E6.m1.1.1.3"><minus id="S3.E6.m1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.3.1"></minus><apply id="S3.E6.m1.1.1.3.2.cmml" xref="S3.E6.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.3.2.1.cmml" xref="S3.E6.m1.1.1.3.2">subscript</csymbol><ci id="S3.E6.m1.1.1.3.2.2.cmml" xref="S3.E6.m1.1.1.3.2.2">𝜃</ci><apply id="S3.E6.m1.1.1.3.2.3.cmml" xref="S3.E6.m1.1.1.3.2.3"><times id="S3.E6.m1.1.1.3.2.3.1.cmml" xref="S3.E6.m1.1.1.3.2.3.1"></times><ci id="S3.E6.m1.1.1.3.2.3.2.cmml" xref="S3.E6.m1.1.1.3.2.3.2">𝐺</ci><ci id="S3.E6.m1.1.1.3.2.3.3.cmml" xref="S3.E6.m1.1.1.3.2.3.3">𝑇</ci></apply></apply><apply id="S3.E6.m1.1.1.3.3.cmml" xref="S3.E6.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.3.3.1.cmml" xref="S3.E6.m1.1.1.3.3">subscript</csymbol><ci id="S3.E6.m1.1.1.3.3.2.cmml" xref="S3.E6.m1.1.1.3.3.2">𝜃</ci><apply id="S3.E6.m1.1.1.3.3.3.cmml" xref="S3.E6.m1.1.1.3.3.3"><times id="S3.E6.m1.1.1.3.3.3.1.cmml" xref="S3.E6.m1.1.1.3.3.3.1"></times><ci id="S3.E6.m1.1.1.3.3.3.2.cmml" xref="S3.E6.m1.1.1.3.3.3.2">𝑎</ci><ci id="S3.E6.m1.1.1.3.3.3.3.cmml" xref="S3.E6.m1.1.1.3.3.3.3">𝑛</ci><ci id="S3.E6.m1.1.1.3.3.3.4.cmml" xref="S3.E6.m1.1.1.3.3.3.4">𝑐</ci><ci id="S3.E6.m1.1.1.3.3.3.5.cmml" xref="S3.E6.m1.1.1.3.3.3.5">ℎ</ci><ci id="S3.E6.m1.1.1.3.3.3.6.cmml" xref="S3.E6.m1.1.1.3.3.3.6">𝑜</ci><ci id="S3.E6.m1.1.1.3.3.3.7.cmml" xref="S3.E6.m1.1.1.3.3.3.7">𝑟</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.1c">\Delta{\theta}=\theta_{GT}-\theta_{anchor}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="S3.SS5.p2.19" class="ltx_p">Where <math id="S3.SS5.p2.18.m1.1" class="ltx_Math" alttext="\theta_{GT}" display="inline"><semantics id="S3.SS5.p2.18.m1.1a"><msub id="S3.SS5.p2.18.m1.1.1" xref="S3.SS5.p2.18.m1.1.1.cmml"><mi id="S3.SS5.p2.18.m1.1.1.2" xref="S3.SS5.p2.18.m1.1.1.2.cmml">θ</mi><mrow id="S3.SS5.p2.18.m1.1.1.3" xref="S3.SS5.p2.18.m1.1.1.3.cmml"><mi id="S3.SS5.p2.18.m1.1.1.3.2" xref="S3.SS5.p2.18.m1.1.1.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.18.m1.1.1.3.1" xref="S3.SS5.p2.18.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS5.p2.18.m1.1.1.3.3" xref="S3.SS5.p2.18.m1.1.1.3.3.cmml">T</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.18.m1.1b"><apply id="S3.SS5.p2.18.m1.1.1.cmml" xref="S3.SS5.p2.18.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.18.m1.1.1.1.cmml" xref="S3.SS5.p2.18.m1.1.1">subscript</csymbol><ci id="S3.SS5.p2.18.m1.1.1.2.cmml" xref="S3.SS5.p2.18.m1.1.1.2">𝜃</ci><apply id="S3.SS5.p2.18.m1.1.1.3.cmml" xref="S3.SS5.p2.18.m1.1.1.3"><times id="S3.SS5.p2.18.m1.1.1.3.1.cmml" xref="S3.SS5.p2.18.m1.1.1.3.1"></times><ci id="S3.SS5.p2.18.m1.1.1.3.2.cmml" xref="S3.SS5.p2.18.m1.1.1.3.2">𝐺</ci><ci id="S3.SS5.p2.18.m1.1.1.3.3.cmml" xref="S3.SS5.p2.18.m1.1.1.3.3">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.18.m1.1c">\theta_{GT}</annotation></semantics></math> and <math id="S3.SS5.p2.19.m2.1" class="ltx_Math" alttext="\theta_{anchor}" display="inline"><semantics id="S3.SS5.p2.19.m2.1a"><msub id="S3.SS5.p2.19.m2.1.1" xref="S3.SS5.p2.19.m2.1.1.cmml"><mi id="S3.SS5.p2.19.m2.1.1.2" xref="S3.SS5.p2.19.m2.1.1.2.cmml">θ</mi><mrow id="S3.SS5.p2.19.m2.1.1.3" xref="S3.SS5.p2.19.m2.1.1.3.cmml"><mi id="S3.SS5.p2.19.m2.1.1.3.2" xref="S3.SS5.p2.19.m2.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.19.m2.1.1.3.1" xref="S3.SS5.p2.19.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS5.p2.19.m2.1.1.3.3" xref="S3.SS5.p2.19.m2.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.19.m2.1.1.3.1a" xref="S3.SS5.p2.19.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS5.p2.19.m2.1.1.3.4" xref="S3.SS5.p2.19.m2.1.1.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.19.m2.1.1.3.1b" xref="S3.SS5.p2.19.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS5.p2.19.m2.1.1.3.5" xref="S3.SS5.p2.19.m2.1.1.3.5.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.19.m2.1.1.3.1c" xref="S3.SS5.p2.19.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS5.p2.19.m2.1.1.3.6" xref="S3.SS5.p2.19.m2.1.1.3.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.19.m2.1.1.3.1d" xref="S3.SS5.p2.19.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS5.p2.19.m2.1.1.3.7" xref="S3.SS5.p2.19.m2.1.1.3.7.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.19.m2.1b"><apply id="S3.SS5.p2.19.m2.1.1.cmml" xref="S3.SS5.p2.19.m2.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.19.m2.1.1.1.cmml" xref="S3.SS5.p2.19.m2.1.1">subscript</csymbol><ci id="S3.SS5.p2.19.m2.1.1.2.cmml" xref="S3.SS5.p2.19.m2.1.1.2">𝜃</ci><apply id="S3.SS5.p2.19.m2.1.1.3.cmml" xref="S3.SS5.p2.19.m2.1.1.3"><times id="S3.SS5.p2.19.m2.1.1.3.1.cmml" xref="S3.SS5.p2.19.m2.1.1.3.1"></times><ci id="S3.SS5.p2.19.m2.1.1.3.2.cmml" xref="S3.SS5.p2.19.m2.1.1.3.2">𝑎</ci><ci id="S3.SS5.p2.19.m2.1.1.3.3.cmml" xref="S3.SS5.p2.19.m2.1.1.3.3">𝑛</ci><ci id="S3.SS5.p2.19.m2.1.1.3.4.cmml" xref="S3.SS5.p2.19.m2.1.1.3.4">𝑐</ci><ci id="S3.SS5.p2.19.m2.1.1.3.5.cmml" xref="S3.SS5.p2.19.m2.1.1.3.5">ℎ</ci><ci id="S3.SS5.p2.19.m2.1.1.3.6.cmml" xref="S3.SS5.p2.19.m2.1.1.3.6">𝑜</ci><ci id="S3.SS5.p2.19.m2.1.1.3.7.cmml" xref="S3.SS5.p2.19.m2.1.1.3.7">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.19.m2.1c">\theta_{anchor}</annotation></semantics></math> are orientation angles of the ground truth box and the anchor, respectively.</p>
</div>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.1" class="ltx_p">The model computes 2D IoU between proposals and ground truth boxes during training. The IoU threshold is set to 0.7 for the car class following the same instructions taken in Pyramid R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. The 3D proposals that have IoU above the threshold are considered positive, and proposals are considered negative when their IoU is below the threshold. Only positive proposals are taken into computing the regression loss. NMS is applied to remove redundant boxes, with an IoU threshold of 0.01.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiments and Results</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The experiment focuses on vehicle detection. The model is trained and tested on the challenging KITTI benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, containing 7481 training images, 7518 test images, as well as the corresponding point cloud of each image. The dataset includes 80256 labelled objects, covering three classes: car, pedestrian and cyclists. For each class, there are three difficulty levels of objects: easy, moderate and hard. They are determined based on the object’s size, occlusion level, visibility and truncation level. The KITTI dataset also provides the calibration matrix, which contains the spatial information between image pixels and 3D point cloud coordinates. As the KITTI test set does not provide ground truth boxes and labels, we divide the training set into two halves, training and validation sets, following the method proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. The training set is used for the training purpose, and the proposed approach is evaluated on the validation set to compare the performance with other SORT 3D detection methods.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">In the training phase, we applied the adaptive moment estimation (Adam) optimiser to update network weights, iteratively. Adam optimiser utilises the gradient descent with momentum and the Root Mean Square propagation algorithms and shows substantial advantages in efficient computation with low memory requirements. The proposed approach is trained for 120k iterations in total, which is the same total iterations used in the sensor-fusion-based 3D detection network, MV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. The initial learning rate is set as 0.0001 and exponentially decays with a factor of 0.8 at every 20k iteration, which is slightly modified based on the parameters used in MV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. This training strategy gives an intuitive comparison of performance between the proposed model and other similar research work. Specifically, we can directly compare the sensor fusion method and the feature extraction method with the methods used in MV3D, when two models take the same inputs and employ similar training strategies.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">We cluster the car class samples in the training set to compute two sets of length and width values for the car class, and these values are used for pre-defining the anchors. The height of the anchor is set to 1.65m, which is the camera height above the ground. The anchors are pre-generated above the ground plane for each RGB image in the training and validation sets based on the ground plane coefficients <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="(ax+by+cz+d)=0" display="inline"><semantics id="S4.p3.1.m1.1a"><mrow id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml"><mrow id="S4.p3.1.m1.1.1.1.1" xref="S4.p3.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.p3.1.m1.1.1.1.1.2" xref="S4.p3.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S4.p3.1.m1.1.1.1.1.1" xref="S4.p3.1.m1.1.1.1.1.1.cmml"><mrow id="S4.p3.1.m1.1.1.1.1.1.2" xref="S4.p3.1.m1.1.1.1.1.1.2.cmml"><mi id="S4.p3.1.m1.1.1.1.1.1.2.2" xref="S4.p3.1.m1.1.1.1.1.1.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p3.1.m1.1.1.1.1.1.2.1" xref="S4.p3.1.m1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S4.p3.1.m1.1.1.1.1.1.2.3" xref="S4.p3.1.m1.1.1.1.1.1.2.3.cmml">x</mi></mrow><mo id="S4.p3.1.m1.1.1.1.1.1.1" xref="S4.p3.1.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.p3.1.m1.1.1.1.1.1.3" xref="S4.p3.1.m1.1.1.1.1.1.3.cmml"><mi id="S4.p3.1.m1.1.1.1.1.1.3.2" xref="S4.p3.1.m1.1.1.1.1.1.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S4.p3.1.m1.1.1.1.1.1.3.1" xref="S4.p3.1.m1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S4.p3.1.m1.1.1.1.1.1.3.3" xref="S4.p3.1.m1.1.1.1.1.1.3.3.cmml">y</mi></mrow><mo id="S4.p3.1.m1.1.1.1.1.1.1a" xref="S4.p3.1.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.p3.1.m1.1.1.1.1.1.4" xref="S4.p3.1.m1.1.1.1.1.1.4.cmml"><mi id="S4.p3.1.m1.1.1.1.1.1.4.2" xref="S4.p3.1.m1.1.1.1.1.1.4.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.p3.1.m1.1.1.1.1.1.4.1" xref="S4.p3.1.m1.1.1.1.1.1.4.1.cmml">​</mo><mi id="S4.p3.1.m1.1.1.1.1.1.4.3" xref="S4.p3.1.m1.1.1.1.1.1.4.3.cmml">z</mi></mrow><mo id="S4.p3.1.m1.1.1.1.1.1.1b" xref="S4.p3.1.m1.1.1.1.1.1.1.cmml">+</mo><mi id="S4.p3.1.m1.1.1.1.1.1.5" xref="S4.p3.1.m1.1.1.1.1.1.5.cmml">d</mi></mrow><mo stretchy="false" id="S4.p3.1.m1.1.1.1.1.3" xref="S4.p3.1.m1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S4.p3.1.m1.1.1.2" xref="S4.p3.1.m1.1.1.2.cmml">=</mo><mn id="S4.p3.1.m1.1.1.3" xref="S4.p3.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><apply id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1"><eq id="S4.p3.1.m1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.2"></eq><apply id="S4.p3.1.m1.1.1.1.1.1.cmml" xref="S4.p3.1.m1.1.1.1.1"><plus id="S4.p3.1.m1.1.1.1.1.1.1.cmml" xref="S4.p3.1.m1.1.1.1.1.1.1"></plus><apply id="S4.p3.1.m1.1.1.1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.1.1.1.2"><times id="S4.p3.1.m1.1.1.1.1.1.2.1.cmml" xref="S4.p3.1.m1.1.1.1.1.1.2.1"></times><ci id="S4.p3.1.m1.1.1.1.1.1.2.2.cmml" xref="S4.p3.1.m1.1.1.1.1.1.2.2">𝑎</ci><ci id="S4.p3.1.m1.1.1.1.1.1.2.3.cmml" xref="S4.p3.1.m1.1.1.1.1.1.2.3">𝑥</ci></apply><apply id="S4.p3.1.m1.1.1.1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.1.1.1.3"><times id="S4.p3.1.m1.1.1.1.1.1.3.1.cmml" xref="S4.p3.1.m1.1.1.1.1.1.3.1"></times><ci id="S4.p3.1.m1.1.1.1.1.1.3.2.cmml" xref="S4.p3.1.m1.1.1.1.1.1.3.2">𝑏</ci><ci id="S4.p3.1.m1.1.1.1.1.1.3.3.cmml" xref="S4.p3.1.m1.1.1.1.1.1.3.3">𝑦</ci></apply><apply id="S4.p3.1.m1.1.1.1.1.1.4.cmml" xref="S4.p3.1.m1.1.1.1.1.1.4"><times id="S4.p3.1.m1.1.1.1.1.1.4.1.cmml" xref="S4.p3.1.m1.1.1.1.1.1.4.1"></times><ci id="S4.p3.1.m1.1.1.1.1.1.4.2.cmml" xref="S4.p3.1.m1.1.1.1.1.1.4.2">𝑐</ci><ci id="S4.p3.1.m1.1.1.1.1.1.4.3.cmml" xref="S4.p3.1.m1.1.1.1.1.1.4.3">𝑧</ci></apply><ci id="S4.p3.1.m1.1.1.1.1.1.5.cmml" xref="S4.p3.1.m1.1.1.1.1.1.5">𝑑</ci></apply><cn type="integer" id="S4.p3.1.m1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">(ax+by+cz+d)=0</annotation></semantics></math>.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">Finally, the proposed approach is tested on the KITTI validation set by setting the IoU threshold to 0.7 for the car class, and its 3D detection and BEV detection performance are compared with four more recent and extensively tested 3D detection methods: MV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, VoxelNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, MVX-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and F-PointNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, shown in Table <a href="#S4.T2" title="TABLE II ‣ IV Experiments and Results ‣ Multi-level and Multi-modal Feature Fusion for Accurate 3D Object Detection in Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> and  <a href="#S4.T3" title="TABLE III ‣ IV Experiments and Results ‣ Multi-level and Multi-modal Feature Fusion for Accurate 3D Object Detection in Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">The BEV detection accuracy evaluation result is illustrated in Table <a href="#S4.T2" title="TABLE II ‣ IV Experiments and Results ‣ Multi-level and Multi-modal Feature Fusion for Accurate 3D Object Detection in Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. The proposed approach outperforms the other four 3D detection methods in detecting easy and moderate vehicles. It achieves the second-best accuracy in detecting hard vehicles. It is noted that our approach outperforms the two-stage sensor-fusion-based model MV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> by 8.89% in detecting moderate vehicles and 3.03% in detecting hard vehicles. The proposed approach also outperforms the LIDAR-Point-Cloud-based method VoxelNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and achieves a 1.88% accuracy lead in detecting moderate vehicles. The success shows the proposed feature extractor and the proposed sensor fusion scheme can effectively extract and generate high-level object features from both input views and recover distant and partially-occluded vehicle features in the final output. The results also show the multi-level texture information from image planes can be efficiently fused with the multi-level spatial information from LIDAR BEV maps, leading to considerable enhancement in BEV detection accuracy.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Comparison of the proposed model with four 3D detection methods in terms of the BEV vehicle detection accuracy, evaluated on the KITTI validation set</figcaption>
<table id="S4.T2.1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Method</span></th>
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" colspan="3">
<span id="S4.T2.1.1.1.1.1" class="ltx_text" style="font-size:80%;">Car </span><math id="S4.T2.1.1.1.1.m1.1" class="ltx_math_unparsed" alttext="AP_{BEV}(\%)" display="inline"><semantics id="S4.T2.1.1.1.1.m1.1a"><mrow id="S4.T2.1.1.1.1.m1.1b"><mi mathsize="80%" id="S4.T2.1.1.1.1.m1.1.1">A</mi><msub id="S4.T2.1.1.1.1.m1.1.2"><mi mathsize="80%" id="S4.T2.1.1.1.1.m1.1.2.2">P</mi><mrow id="S4.T2.1.1.1.1.m1.1.2.3"><mi mathsize="80%" id="S4.T2.1.1.1.1.m1.1.2.3.2">B</mi><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.1.m1.1.2.3.1">​</mo><mi mathsize="80%" id="S4.T2.1.1.1.1.m1.1.2.3.3">E</mi><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.1.m1.1.2.3.1a">​</mo><mi mathsize="80%" id="S4.T2.1.1.1.1.m1.1.2.3.4">V</mi></mrow></msub><mrow id="S4.T2.1.1.1.1.m1.1.3"><mo maxsize="80%" minsize="80%" id="S4.T2.1.1.1.1.m1.1.3.1">(</mo><mo mathsize="80%" id="S4.T2.1.1.1.1.m1.1.3.2">%</mo><mo maxsize="80%" minsize="80%" id="S4.T2.1.1.1.1.m1.1.3.3">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">AP_{BEV}(\%)</annotation></semantics></math>
</th>
</tr>
<tr id="S4.T2.1.1.2.1" class="ltx_tr">
<td id="S4.T2.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.2.1.1.1" class="ltx_text" style="font-size:80%;">Easy</span></td>
<td id="S4.T2.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.2.1.2.1" class="ltx_text" style="font-size:80%;">Moderate</span></td>
<td id="S4.T2.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.2.1.3.1" class="ltx_text" style="font-size:80%;">Hard</span></td>
</tr>
<tr id="S4.T2.1.1.3.2" class="ltx_tr">
<td id="S4.T2.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.1.1.3.2.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">MV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite></span></td>
<td id="S4.T2.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.3.2.2.1" class="ltx_text" style="font-size:80%;">86.55</span></td>
<td id="S4.T2.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.3.2.3.1" class="ltx_text" style="font-size:80%;">78.10</span></td>
<td id="S4.T2.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.3.2.4.1" class="ltx_text" style="font-size:80%;">76.77</span></td>
</tr>
<tr id="S4.T2.1.1.4.3" class="ltx_tr">
<td id="S4.T2.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.1.1.4.3.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">VoxelNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite></span></td>
<td id="S4.T2.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.4.3.2.1" class="ltx_text" style="font-size:80%;">89.60</span></td>
<td id="S4.T2.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.4.3.3.1" class="ltx_text" style="font-size:80%;">84.81</span></td>
<td id="S4.T2.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.4.3.4.1" class="ltx_text" style="font-size:80%;">78.57</span></td>
</tr>
<tr id="S4.T2.1.1.5.4" class="ltx_tr">
<td id="S4.T2.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.1.1.5.4.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">MVX-Next <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite></span></td>
<td id="S4.T2.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.5.4.2.1" class="ltx_text" style="font-size:80%;">88.60</span></td>
<td id="S4.T2.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.5.4.3.1" class="ltx_text" style="font-size:80%;">84.60</span></td>
<td id="S4.T2.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.5.4.4.1" class="ltx_text" style="font-size:80%;">78.60</span></td>
</tr>
<tr id="S4.T2.1.1.6.5" class="ltx_tr">
<td id="S4.T2.1.1.6.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.1.1.6.5.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">F-PointNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite></span></td>
<td id="S4.T2.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.6.5.2.1" class="ltx_text" style="font-size:80%;">N/A</span></td>
<td id="S4.T2.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.6.5.3.1" class="ltx_text" style="font-size:80%;">N/A</span></td>
<td id="S4.T2.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.6.5.4.1" class="ltx_text" style="font-size:80%;">N/A</span></td>
</tr>
<tr id="S4.T2.1.1.7.6" class="ltx_tr">
<td id="S4.T2.1.1.7.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.1.1.7.6.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Ours</span></td>
<td id="S4.T2.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.1.1.7.6.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">89.62</span></td>
<td id="S4.T2.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.1.1.7.6.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">89.69</span></td>
<td id="S4.T2.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.1.1.7.6.4.1" class="ltx_text" style="font-size:80%;">79.80</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Comparison of the proposed model with four 3D detection methods in terms of the 3D vehicle detection accuracy, evaluated on the KITTI validation set</figcaption>
<table id="S4.T3.1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T3.1.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Method</span></th>
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" colspan="3">
<span id="S4.T3.1.1.1.1.1" class="ltx_text" style="font-size:80%;">Car </span><math id="S4.T3.1.1.1.1.m1.1" class="ltx_math_unparsed" alttext="AP_{3D}(\%)" display="inline"><semantics id="S4.T3.1.1.1.1.m1.1a"><mrow id="S4.T3.1.1.1.1.m1.1b"><mi mathsize="80%" id="S4.T3.1.1.1.1.m1.1.1">A</mi><msub id="S4.T3.1.1.1.1.m1.1.2"><mi mathsize="80%" id="S4.T3.1.1.1.1.m1.1.2.2">P</mi><mrow id="S4.T3.1.1.1.1.m1.1.2.3"><mn mathsize="80%" id="S4.T3.1.1.1.1.m1.1.2.3.2">3</mn><mo lspace="0em" rspace="0em" id="S4.T3.1.1.1.1.m1.1.2.3.1">​</mo><mi mathsize="80%" id="S4.T3.1.1.1.1.m1.1.2.3.3">D</mi></mrow></msub><mrow id="S4.T3.1.1.1.1.m1.1.3"><mo maxsize="80%" minsize="80%" id="S4.T3.1.1.1.1.m1.1.3.1">(</mo><mo mathsize="80%" id="S4.T3.1.1.1.1.m1.1.3.2">%</mo><mo maxsize="80%" minsize="80%" id="S4.T3.1.1.1.1.m1.1.3.3">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">AP_{3D}(\%)</annotation></semantics></math>
</th>
</tr>
<tr id="S4.T3.1.1.2.1" class="ltx_tr">
<td id="S4.T3.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.1.1.2.1.1.1" class="ltx_text" style="font-size:80%;">Easy</span></td>
<td id="S4.T3.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.1.1.2.1.2.1" class="ltx_text" style="font-size:80%;">Moderate</span></td>
<td id="S4.T3.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.1.1.2.1.3.1" class="ltx_text" style="font-size:80%;">Hard</span></td>
</tr>
<tr id="S4.T3.1.1.3.2" class="ltx_tr">
<td id="S4.T3.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.1.1.3.2.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">MV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite></span></td>
<td id="S4.T3.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.1.1.3.2.2.1" class="ltx_text" style="font-size:80%;">71.29</span></td>
<td id="S4.T3.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.1.1.3.2.3.1" class="ltx_text" style="font-size:80%;">62.68</span></td>
<td id="S4.T3.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.1.1.3.2.4.1" class="ltx_text" style="font-size:80%;">56.56</span></td>
</tr>
<tr id="S4.T3.1.1.4.3" class="ltx_tr">
<td id="S4.T3.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.1.1.4.3.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">VoxelNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite></span></td>
<td id="S4.T3.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.1.1.4.3.2.1" class="ltx_text" style="font-size:80%;">81.79</span></td>
<td id="S4.T3.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.1.1.4.3.3.1" class="ltx_text" style="font-size:80%;">65.46</span></td>
<td id="S4.T3.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.1.1.4.3.4.1" class="ltx_text" style="font-size:80%;">62.85</span></td>
</tr>
<tr id="S4.T3.1.1.5.4" class="ltx_tr">
<td id="S4.T3.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.1.1.5.4.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">MVX-Next <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite></span></td>
<td id="S4.T3.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.1.1.5.4.2.1" class="ltx_text" style="font-size:80%;">82.30</span></td>
<td id="S4.T3.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.1.1.5.4.3.1" class="ltx_text" style="font-size:80%;">72.20</span></td>
<td id="S4.T3.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.1.1.5.4.4.1" class="ltx_text" style="font-size:80%;">66.80</span></td>
</tr>
<tr id="S4.T3.1.1.6.5" class="ltx_tr">
<td id="S4.T3.1.1.6.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.1.1.6.5.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">F-PointNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite></span></td>
<td id="S4.T3.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.1.1.6.5.2.1" class="ltx_text" style="font-size:80%;">83.76</span></td>
<td id="S4.T3.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.1.1.6.5.3.1" class="ltx_text" style="font-size:80%;">70.92</span></td>
<td id="S4.T3.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.1.1.6.5.4.1" class="ltx_text" style="font-size:80%;">63.65</span></td>
</tr>
<tr id="S4.T3.1.1.7.6" class="ltx_tr">
<td id="S4.T3.1.1.7.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.1.1.7.6.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">Ours</span></td>
<td id="S4.T3.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.1.1.7.6.2.1" class="ltx_text" style="font-size:80%;">83.27</span></td>
<td id="S4.T3.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.1.1.7.6.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">73.99</span></td>
<td id="S4.T3.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.1.1.7.6.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">67.92</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F4" class="ltx_figure">
<p id="S4.F4.1" class="ltx_p ltx_align_center"><span id="S4.F4.1.1" class="ltx_text"><img src="/html/2212.07560/assets/000089.png" id="S4.F4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="280" height="171" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>2D and 3D vehicle detection results (green) of the proposed method in a standard driving scenario.</figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure">
<p id="S4.F5.1" class="ltx_p ltx_align_center"><span id="S4.F5.1.1" class="ltx_text"><img src="/html/2212.07560/assets/000420.png" id="S4.F5.1.1.g1" class="ltx_graphics ltx_img_landscape" width="280" height="171" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>2D and 3D vehicle detection results (green) of the proposed method in a partially-occluded driving scenario.</figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure">
<p id="S4.F6.1" class="ltx_p ltx_align_center"><span id="S4.F6.1.1" class="ltx_text"><img src="/html/2212.07560/assets/000315.png" id="S4.F6.1.1.g1" class="ltx_graphics ltx_img_landscape" width="280" height="171" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>2D and 3D vehicle detection results (green) of the proposed method in a driving scenario with distant vehicles.</figcaption>
</figure>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p">The evaluation results on the 3D vehicle detection accuracy are presented in Table <a href="#S4.T3" title="TABLE III ‣ IV Experiments and Results ‣ Multi-level and Multi-modal Feature Fusion for Accurate 3D Object Detection in Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. Although the proposed approach achieves the second-best 3D detection accuracy in detecting easy vehicles, it achieves the best performance in 3D detection of moderate and hard vehicles. The proposed approach outperforms the MV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> by 11.89%, 11.31% and 11.36% accuracy in detecting easy, moderate and hard vehicles, respectively. The notable leads in detection performance prove the significance of interactions between features from different stages of convolution layers and different sensory modalities. Compared the results with the LIDAR-solely-based VoxelNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, the proposed approach achieves a significant lead of 8.53% in detecting moderate vehicles. It is also noted that the proposed approach outperforms the state-of-the-art sensor-fusion-based method MVX-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> in detecting moderate and hard vehicles. The results prove the noteworthy improvements achieved by the proposed method in enhancing the detection accuracy of partially-occluded or distant vehicles. This improvement further proves that the proposed approach efficiently and effectively fuse images’ texture information and point cloud’s spatial information, by taking advantage of each modality and mitigating their drawbacks.
Some 2D and 3D detection results obtained in different driving scenarios can be visualised in Figure <a href="#S4.F4" title="Figure 4 ‣ IV Experiments and Results ‣ Multi-level and Multi-modal Feature Fusion for Accurate 3D Object Detection in Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> to <a href="#S4.F6" title="Figure 6 ‣ IV Experiments and Results ‣ Multi-level and Multi-modal Feature Fusion for Accurate 3D Object Detection in Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. The figure <a href="#S4.F4" title="Figure 4 ‣ IV Experiments and Results ‣ Multi-level and Multi-modal Feature Fusion for Accurate 3D Object Detection in Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows that the proposed model can accurately detect all vehicles in a standard driving scenario. Figure <a href="#S4.F5" title="Figure 5 ‣ IV Experiments and Results ‣ Multi-level and Multi-modal Feature Fusion for Accurate 3D Object Detection in Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows that the proposed approach can accurately detect all vehicles in a congested traffic scenario, including partially-occluded and low-visibility vehicles. In Figure <a href="#S4.F6" title="Figure 6 ‣ IV Experiments and Results ‣ Multi-level and Multi-modal Feature Fusion for Accurate 3D Object Detection in Connected and Automated Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, it can be visualised that the proposed approach can accurately detect distant vehicles, which appear small on the image plane.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This article proposed a two-stage object detector for driving scenarios that applies the convolutional neural network to extract features from RGB images and LIDAR BEV maps. A multi-modal and multi-level feature fusion scheme was developed to fuse high-level object features across input views and convolutional layers. It achieved taking advantage of each sensor and mitigating sensors’ drawbacks with complementary characteristics. A three-stage feature extractor, including two convolutional stages and one deconvolutional stage, was
designed to extract high-level object features from multi-views for the feature fusion scheme. The feature extractor employed lateral connections between stages to support the details recovery. The designed feature extractor and sensor fusion scheme led to an 11.36% improvement in 3D KITTI hard vehicles detection accuracy compared with the two-stage detector MV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, showing its satisfactory performance in detecting distant, partially occluded and low-visibility instances.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">The proposed approach was tested on the KITTI validation set and the 3D and BEV vehicle detection outperformed four more recent 3D detection methods, including MV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, MVX-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, F-PointNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> and VoxelNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. It is noted that the proposed model outperformed the MVX-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and F-PointNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> in 3D detection of KITTI’s moderate and hard vehicles. This performance lead can be attributed to employing the proposed three-stage feature extractor and the feature fusion scheme. The drawbacks of the proposed model, which prevent it from achieving higher accuracy, can be categorised into three causes: BEV maps, anchors, and ground planes. Specifically, the LIDAR BEV maps can
mitigate the occlusion issue but suffer from the sparsity of points. The dimensions of anchors
were pre-defined by clustering objects in the training set, preventing the RPN from detecting
objects that have considerably larger or smaller spatial sizes. In addition, the proposed approach requires a highly accurate ground plane estimation method to measure ground planes for pre-defining anchors, which is inefficient when implemented in the real world.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Aiming toward higher detection accuracy, we may suggest increasing BEV channels as a future work by generating more height maps to mitigate the sparsity of points on BEV maps. The availability of a larger-scale dataset can further increase detection accuracy by providing more training labels in various driving scenarios.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"> J. N. Bajpai, ”Emerging vehicle technologies &amp; the search for urban mobility solutions,” Urban, Planning and Transport Research, vol. 4, no. 1, pp. 83-100, 2016.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"> Y. E. Bigman and K. Gray, ”Life and death decisions of autonomous vehicles,” Nature, vol. 579, no. 7797, pp. E1-E2, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"> K. He, X. Zhang, S. Ren, and J. Sun, ”Deep residual learning for image recognition,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770-778.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"> Simonyan, K. and Zisserman, A. 2014. Very deep convolutional networks for large-scale
image recognition. arXiv preprint arXiv:1409.1556.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"> B. Singh, M. Najibi, and L. S. Davis, ”Sniper: Efficient multi-scale training,” Advances in neural information processing systems, vol. 31, 2018.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"> H. Li, B. Dai, S. Shi, W. Ouyang, and X. Wang, ”Feature intertwiner for object detection,” arXiv preprint arXiv:1903.11851, 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"> Z. Zhang, T. He, H. Zhang, Z. Zhang, J. Xie, and M. Li, ”Bag of freebies for training object detection neural networks,” arXiv preprint arXiv:1902.04103, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"> X. Zhao, Z. Liu, R. Hu, and K. Huang, ”3d object detection using scale invariant and feature reweighting networks,” in Proceedings of the AAAI Conference on Artificial Intelligence, 2019, vol. 33, no. 01, pp. 9267-9274.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"> S. Arshad, M. Shahzad, Q. Riaz, and M. M. Fraz, ”Dprnet: Deep 3d point based residual network for semantic segmentation and classification of 3d point clouds,” IEEE Access, vol. 7, pp. 68892-68904, 2019.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"> Y. Zhou and O. Tuzel, ”Voxelnet: End-to-end learning for point cloud based 3d object detection,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 4490-4499.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"> Y. Yan, Y. Mao, and B. Li, ”Second: Sparsely embedded convolutional detection,” Sensors, vol. 18, no. 10, p. 3337, 2018.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"> G. P. Meyer, J. Charland, D. Hegde, A. Laddha, and C. Vallespi-Gonzalez, ”Sensor fusion for joint 3d object detection and semantic segmentation,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2019, pp. 0-0.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"> G. P. Meyer, A. Laddha, E. Kee, C. Vallespi-Gonzalez, and C. K. Wellington, ”Lasernet: An efficient probabilistic 3d object detector for autonomous driving,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 12677-12686.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"> S. Vora, A. H. Lang, B. Helou, and O. Beijbom, ”Pointpainting: Sequential fusion for 3d object detection,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 4604-4612.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"> J. Ku, M. Mozifian, J. Lee, A. Harakeh, and S. L. Waslander, ”Joint 3d proposal generation and object detection from view aggregation,” in 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2018: IEEE, pp. 1-8.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"> V. A. Sindagi, Y. Zhou, and O. Tuzel, ”Mvx-net: Multimodal Voxelnet for 3d object detection,” in 2019 International Conference on Robotics and Automation (ICRA), 2019: IEEE, pp. 7276-7282.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"> Y. Cui, R. Chen, W. Chu, L. Chen, D. Tian, Y. Li, and D. Cao, ”Deep learning for image and point cloud fusion in autonomous driving: A review,” IEEE Transactions on Intelligent Transportation Systems, 2021.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"> A. Geiger, P. Lenz, and R. Urtasun, ”Are we ready for autonomous driving? the KITTI vision benchmark suite,” in 2012 IEEE conference on computer vision and pattern recognition, 2012: IEEE, pp. 3354-3361.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"> X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, ”Multi-view 3d object detection network for autonomous driving,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 1907-1915.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"> C. R. Qi, H. Su, K. Mo, and L. J. Guibas, ”Pointnet: Deep learning on point sets for 3d classification and segmentation,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 652-660.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"> C. R. Qi, L. Yi, H. Su, and L. J. Guibas, ”Pointnet++: Deep hierarchical feature learning on point sets in a metric space,” Advances in neural information processing systems, vol. 30, 2017.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"> S. Ren, K. He, R. Girshick, and J. Sun, ”Faster R-CNN: Towards real-time object detection with region proposal networks,” Advances in neural information processing systems, vol. 28, 2015.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"> B. Li, T. Zhang, and T. Xia, ”Vehicle detection from 3d lidar using fully convolutional network,” arXiv preprint arXiv:1608.07916, 2016.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"> B. Wu, A. Wan, X. Yue, and K. Keutzer, ”Squeezeseg: Convolutional neural nets with recurrent CRF for real-time road-object segmentation from 3d lidar point cloud,” in 2018 IEEE International Conference on Robotics and Automation (ICRA), 2018: IEEE, pp. 1887-1893.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"> M. Catà Villà, ”3D Bounding box detection from monocular images,” Universitat Politècnica de Catalunya, 2019.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"> A. Mousavian, D. Anguelov, J. Flynn, and J. Kosecka, ”3d bounding box estimation using deep learning and geometry,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 7074-7082.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"> J. Fang, L. Zhou, and G. Liu, ”3d bounding box estimation for autonomous vehicles by cascaded geometric constraints and depurated 2d detections using 3d results,” arXiv preprint arXiv:1909.01867, 2019.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"> M. Rezaei, M. Azarmi, and F. M. P. Mir, ”Traffic-Net: 3D Traffic Monitoring Using a Single Camera,” arXiv preprint arXiv:2109.09165, 2021.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"> M. Simon, K. Amende, A. Kraus, J. Honer, T. Samann, H. Kaulbersch, S. Milz, and H. Michael Gross, ”Complexer-yolo: Real-time 3d object detection and tracking on semantic point clouds,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2019, pp. 0-0.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"> C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, ”Frustum pointnets for 3d object detection from RGB-D data,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 918-927.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"> O. Ronneberger, P. Fischer, and T. Brox, ”U-net: Convolutional networks for biomedical image segmentation,” in International Conference on Medical image computing and computer-assisted intervention, 2015: Springer, pp. 234-241.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"> R. Girshick, ”Fast R-CNN,” in Proceedings of the IEEE international conference on computer vision, 2015, pp. 1440-1448.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"> J. Mao, M. Niu, H. Bai, X. Liang, H. Xu, and C. Xu, ”Pyramid R-CNN: Towards better performance and adaptability for 3d object detection,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 2723-2732.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"> T. Wang, X. Zhu, J. Pang, and D. Lin, ”Fcos3d: Fully convolutional one-stage monocular 3d object detection,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 913-922.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"> X. Chen, K. Kundu, Y. Zhu, A. G. Berneshawi, H. Ma, S. Fidler, and R. Urtasun, ”3d object proposals for accurate object class detection,” Advances in neural information processing systems, vol. 28, 2015.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2212.07559" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2212.07560" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2212.07560">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2212.07560" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2212.07561" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 08:00:24 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
