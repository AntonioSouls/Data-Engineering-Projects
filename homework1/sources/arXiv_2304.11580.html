<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2304.11580] A Framework for Benchmarking Real-Time Embedded Object Detection</title><meta property="og:description" content="Object detection is one of the key tasks in many applications of computer vision. Deep Neural Networks (DNNs) are undoubtedly a well-suited approach for object detection. However, such DNNs need highly adapted hardware…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Framework for Benchmarking Real-Time Embedded Object Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A Framework for Benchmarking Real-Time Embedded Object Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2304.11580">

<!--Generated on Thu Feb 29 13:36:50 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Generic Evaluation Optimization Efficient Deployment.">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>HENSOLDT Optronics GmbH, Oberkochen, Germany
<br class="ltx_break"><span id="id1.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>{michael.schlosser, daniel.koenig, michael.teutsch}@hensoldt.net</span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">A Framework for Benchmarking Real-Time Embedded Object Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Michael Schlosser
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Daniel König
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Michael Teutsch
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Object detection is one of the key tasks in many applications of computer vision. Deep Neural Networks (DNNs) are undoubtedly a well-suited approach for object detection. However, such DNNs need highly adapted hardware together with hardware-specific optimization to guarantee high efficiency during inference. This is especially the case when aiming for efficient object detection in video streaming applications on limited hardware such as edge devices. Comparing vendor-specific hardware and related optimization software pipelines in a fair experimental setup is a challenge. In this paper, we propose a framework that uses a host computer with a host software application together with a light-weight interface based on the Message Queuing Telemetry Transport (MQTT) protocol. Various different target devices with target apps can be connected via MQTT with this host computer. With well-defined and standardized MQTT messages, object detection results can be reported to the host computer, where the results are evaluated without harming or influencing the processing on the device. With this quite generic framework, we can measure the object detection performance, the runtime, and the energy efficiency at the same time. The effectiveness of this framework is demonstrated in multiple experiments that offer deep insights into the optimization of DNNs.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Generic Evaluation Optimization Efficient Deployment.
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Algorithm benchmarking is a crucial step during the development of computer vision systems and applications. In applications that have to meet requirements for minimum latency (i.e. real-time requirements) on limited hardware (i.e. edge devices), not only the algorithm effectiveness is important but also the efficiency. Facilitating recent state-of-the-art techniques such as deep learning and Deep Neural Networks (DNNs) as a very powerful tool in computer vision nowadays, authors try to find the sweet spot between processing speed and algorithm accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Relevant fields of application for finding such a trade-off range from robotics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and autonomous driving <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> to smartphones <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. Accompanying this progress, several vendors of specific inference hardware provide specialized software based optimization pipelines that enable scientists and developers to strongly reduce the latency of certain algorithms, while preserving the effective performance at the same time <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. Since these optimization software frameworks and pipelines are highly specific, however, benchmarking certain rather generic computer vision algorithms for certain applications with related hardware limitations such as miniaturized edge devices can be challenging.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this paper, we propose a generic framework for benchmarking low-latency computer vision algorithms for their application on vendor-specific inference hardware. We use object detection as our considered computer vision task. Utilizing the popular You Only Look Once (YOLO) object detector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> in its version YOLOv4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> as reference detection algorithm and MS Common Objects in Context (COCO) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> as reference dataset, we aim to measure detection performance and algorithm runtime simultaneously. This is achieved by separating data distribution and evaluation from data processing. A host app running on a desktop computer distributes the video data via the light-weight Message Queuing Telemetry Transport (MQTT) protocol. This data is processed by a target app on the target board. The results are then sent back via MQTT to the host app for evaluation and benchmarking. In this way, we can efficiently compare different vendor-specific hardware and optimization software pipelines, which usually contain software tools for quantization and pruning of DNNs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Furthermore, we can integrate new hardware and/or software updates easily into our framework to measure their performance and runtime gains, respectively.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Our proposed framework is not the first of its kind, of course. Several approaches exist for evaluating the performance of object detection, but most related literature either does not measure all important metrics relevant for the use on embedded systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, which are primarily the accuracy, the runtime and the power consumption, or the proposed frameworks are less generic compared to ours as they only refer to a single hardware <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> or vendor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. Other authors compare embedded hardware for certain computer vision tasks but they do not use or mention a unified evaluation framework at all <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. Another relevant aspect is that our MQTT based publish-subscribe approach is more flexible than server-client based architectures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>: in this way, we can evaluate a computer vision algorithm on multiple target platforms simultaneously. Server-client architectures instead usually have higher communication overhead since requests have to be sent to the host from each individual target device. Furthermore, MQTT is very light-weight, simpler, and with higher throughput compared to other protocols or interfaces such as Hypertext Transfer Protocol (HTTP) or Advanced Message Queuing Protocol (AMQP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. The application behind the framework is to provide a generic test bed for benchmarking optimized computer vision algorithms on highly efficient edge devices. In this way, we can effectively find the trade-off between processing speed and algorithm accuracy.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Our contributions are:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose a generic evaluation framework for different embedded devices using a lightweight concept that can be deployed with little effort and that provides remote access from the host app without the need to send requests from the target side.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">To the best of our knowledge this is the first framework of its kind that uses the highly efficient publish-subscribe protocol MQTT instead of less flexible or efficient communication protocols and/or interfaces such as HTTP or Representational State Transfer (REST). In this way, we can evaluate an algorithm on multiple target devices simultaneously and efficiently.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We demonstrate the usefulness of our framework by evaluating two different embedded devices from two vendors utilizing their specific optimization pipelines for the task of generic object detection.</p>
</div>
</li>
</ol>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The remainder of this paper is organized as follows: related work is presented in Section <a href="#S2" title="2 Related Work ‣ A Framework for Benchmarking Real-Time Embedded Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Our proposed framework together with the related methodology is presented in Section <a href="#S3" title="3 Proposed Evaluation Framework ‣ A Framework for Benchmarking Real-Time Embedded Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Experimental results are described in Section <a href="#S4" title="4 Experiments ‣ A Framework for Benchmarking Real-Time Embedded Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> followed by a discussion in Section <a href="#S4.SS4" title="4.4 Discussion ‣ 4 Experiments ‣ A Framework for Benchmarking Real-Time Embedded Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>. We conclude in Section <a href="#S5" title="5 Conclusion ‣ A Framework for Benchmarking Real-Time Embedded Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Several approaches exist for evaluating the performance of object detection, but most proposed works either do not measure all important metrics for use on embedded systems, which are primarily the accuracy, the runtime and the power consumption, or are not generic enough, because they only refer to a single hardware or manufacturer.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Stäcker et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> evaluate object detection on embedded systems using RetinaNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. The detection model is optimized using Nvidia TensorRT and deployed on an Nvidia Jetson AGX Xavier. Accuracy and runtime, but not power consumption, are measured during the experimental evaluation. The Robot Operating System (ROS) is used to communicate with the target and to receive the detection results. Only an Nvidia Jetson AGX Xavier is used as hardware platform and the comparison to other platforms is not considered here. Rungsuptaweekoon et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> utilize the object detector YOLOv2 on different embedded devices such as the Jetson TX1 and TX2. In addition to Frames Per Second (FPS) and mean Average Precision (mAP), the power consumption is also measured. A benchmark environment based on a server-client model is provided, where the target board acts as a client and requests the image data from a host PC. A system based on a client-server architecture is proposed, which requires the direct access of the target board to communicate with the server and the authors only consider Nvidia hardware.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Besides the evaluation of GPU-based hardware accelerators, there are also works dealing with the evaluation of object detection on Field Programmable Gate Arrays (FPGAs). Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> evaluate a YOLOv3 object detector, which is optimized using the recommended optimization framework Vitis AI on a Xilinx Zynq UltraScale+ MPSoC ZCU104 evaluation board. The performance results of the Xilinx MPSoC are compared to the performance of a Nvidia GeForce GTX 1080 GPU on a desktop computer. The power consumption and the FPS are measured, but no accuracy, which means that it is not verified whether the model still has an acceptable accuracy after optimization. Furthermore, there is no uniform evaluation framework for the different system environments that are compared, so implementation mismatches between the platforms are possible. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, Yu et al. propose a comparison of several object detectors such as YOLO, Faster RCNN and SSD, on multiple platforms, including Nvidia TK1, Xilinx Zynq 7045 and Xilinx KU115. They measure power consumption, throughput, and accuracy. However, they provide an incomplete comparison as not all models are evaluated on all boards. Furthermore, they also do not use an uniform evaluation framework for different implementations. Blott et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> propose a theoretical and experimental evaluation of different DNNs for several computer vision tasks on a variety of different acceleration platforms, such as FPGAs, GPUs and TPUs. The authors point out that measurement methods are often unclear and complicated by the large variety of deployment parameters. For this purpose, a large experimental study is conducted on how different DNN topologies behave with different deployment settings (e.g., batch size or power modes) and optimization methods (e.g., pruning and quantization), in terms of throughput, inference time, hardware utilization, power consumption, and accuracy. Accordingly, a detailed study on the comparison of DNNs, especially in the use case of image classification, is presented here. Lin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> provide a benchmark for different SSD models deployed on an Intel Arria 10 FPGA for traffic sign detection. In addition to an analysis of the training framework, they evaluate the models deployed on the FPGA in terms of inference time, accuracy, and power efficiency, varying critical parameters such as floating point precision and batch size. They found that the inference time on the GPU is faster in most cases and that the FPGA is better in terms of power efficiency. As with most of the publications just presented, the focus here is an experimental evaluation of the results for the given models, optimization methods and hardware platforms rather than the methodology of a generic evaluation framework. Verucchi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> propose a detailed comparison between object detectors, such as YOLOv3, CenterNet, and SSD deployed on an Nvidia Jetson AGX Xavier, a Xilinx Zynq UltraScale+ MPSoC ZCU102, and an industrial PC. High attention is paid to the fairness of the comparison between the detectors deployed on different hardware platforms. All important metrics mentioned before are considered, which include accuracy, runtime, and power consumption. However, no uniform evaluation framework is implemented and the accuracy is directly measured on the target device.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed Evaluation Framework</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our proposed framework enables the evaluation of object detection algorithms for such a stream on arbitrary target hardware and shifts the responsibility of providing input data and calculating evaluation measures to a separate host PC. The framework itself, however, is generic and thus not limited to the task of object detection.
We choose an MQTT-based approach, because it is a light-weight protocol and often used in embedded devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.
Since MQTT uses a publish-subscribe mechanism, it is even possible to evaluate object detection on multiple target devices simultaneously. The MQTT broker manages message distribution and decouples the communication of host and target. Hence, requests do not have to be sent directly to the host from each individual target device as is the case with a commonly used server-client architecture.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2304.11580/assets/fig-eval-framework-architecture.png" id="S3.F1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="244" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S3.F1.3.2" class="ltx_text" style="font-size:90%;">Architecture of the proposed evaluation framework consisting of the development PC with the host application and the embedded device with the target application. MQTT serves as communication protocol between the devices.</span></figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">As Fig. <a href="#S3.F1" title="Figure 1 ‣ 3 Proposed Evaluation Framework ‣ A Framework for Benchmarking Real-Time Embedded Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> depicts, with the proposed evaluation framework image data is sent from a host computer to the target via MQTT. The target is the platform that performs the inference, the associated pre-processing, which consists of resizing and normalizing the input image data, and post-processing, which consists of the conversion of the model output to bounding box coordinates and confidence values, and Non-Maximum Suppression (NMS). The results are sent back to the host PC for evaluation.
The use of such a framework allows to evaluate a DNN directly on a platform that is used for deployment in operational systems. Thus, we are able to verify the conversion from common deep learning frameworks such as PyTorch or TensorFlow into the proprietary optimization frameworks of the hardware vendors such as Xilinx or Nvidia. In this work, without limitation of generality, we focus on Xilinx Vitis AI and Nvidia TensorRT. Furthermore, we can provide accuracy measures of a final deep learning application deployed on the embedded device. Various proprietary profiler tools are provided by Xilinx and Nvidia that support measuring of runtime and throughput. However, to evaluate complete accuracy measurements locally, the entire test dataset would have to be installed on the target. This can be impractical especially for large datasets such as the COCO test dataset that is used in this work and consists of over <span id="S3.p2.1.1" class="ltx_text">20,000</span> images. We avoid this with the proposed evaluation framework. The responsibility of storing and providing the input data can thus be transferred to an external device (i.e. the host PC) together with the calculation of evaluation measures.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2304.11580/assets/fig-sequence-diagram.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="574" height="427" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">UML sequence diagram for an evaluation session.</span></figcaption>
</figure>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Figure <a href="#S3.F2" title="Figure 2 ‣ 3 Proposed Evaluation Framework ‣ A Framework for Benchmarking Real-Time Embedded Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> depicts the flow of an evaluation session as a Unified Modeling Language (UML) sequence diagram. In the first preparatory step, the host sends the desired configuration of the experiment setup to the MQTT broker. The target subscribes to this configuration topic, whereupon messages are distributed that contain, for example, the algorithm to be used and the topics to be subscribed to, on which the input data is sent. This procedure simplifies the setup of the target, because the target component only has to be started, which can also be automated in the boot process, and all further steps can be done remotely from the desktop PC. The host app sends the input data to the MQTT broker in a next step. For this purpose, the input image is read in as an OpenCV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> image matrix and serialized for transfer to the target device using Msgpack <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. These input messages are then successively forwarded to the target component, processed there and the results are sent back. These results include the measured times of pre- and post-processing and inference, as well as the determined bounding boxes. For demonstration purposes, the processed image with the bounding boxes plotted can be sent back to the host via MQTT. The host can then display the processed images at runtime if required. After all desired input data has been processed, the results can be further processed using the selected evaluation modules. For example, the received detection results can now be compared with the annotations of the test dataset that was just processed in order to measure the accuracy.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2304.11580/assets/fig-component-diagram.png" id="S3.F3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="139" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">UML component diagram for the evaluation framework.</span></figcaption>
</figure>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">Due to the decoupling and the modularized structure of the framework, as Fig. <a href="#S3.F3" title="Figure 3 ‣ 3 Proposed Evaluation Framework ‣ A Framework for Benchmarking Real-Time Embedded Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> depicts, it is possible to integrate further object detection algorithms or evaluators without much effort. Hence, no additional dependencies arise for the communication of the two components via MQTT and the actual object detection application. Functionality relevant to both target and host such as data transfer in particular, is implemented in the BaseClient component. By implementing the evaluator interface, a plugin can be developed for the host component that has access to the result message. This allows further processing of all existing measurements. Similarly, the object detector interface can be implemented on the target board, which receives and processes an OpenCV image matrix and provides access to various information such as the predicted bounding box coordinates and time measurements.
The target part of the evaluation framework only depends on an MQTT library, such as Eclipse Paho MQTT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and Msgpack, which is used for the standardization and de/serialization of the message. All other dependencies are dependent on the respective plugin. For example, the Vitis AI Runtime must be installed as a dependency for controlling an application for the Xilinx Deep Learning Processor Unit (DPU), which is a programmable engine for accelerating deep learning tasks on an FPGA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. Since host and target are completely decoupled from each other, modifications to the host component are not necessary when including a new target device.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this work, we choose a YOLOv4 object detector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> as baseline model. For the Xilinx Zynq UltraScale+ MPSoC ZCU104 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> it is optimized using Xilinx Vitis AI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and for the Nvidia Jetson AGX Xavier <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> we utilize Nvidia TensorRT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. By varying different parameters within the vendor-specific optimization frameworks, we can analyze the effects on the then deployed DNN. This for example includes steps such as quantization, which is the approximation of a neural network in floating point precision by means of a new neural network with a smaller bit width <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, and pruning, in which the size of the neural network is reduced by the systematical removal of elements <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. In addition, a cross-platform comparison between the models on different target devices is performed.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In order to achieve comparability between the optimization frameworks, we set up the conversion and optimization process as visualized in Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ A Framework for Benchmarking Real-Time Embedded Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2304.11580/assets/fig-opt-pipeline.png" id="S4.F4.g1" class="ltx_graphics ltx_img_landscape" width="598" height="250" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">Optimization pipelines for Xilinx Vitis AI and Nvidia TensorRT.</span></figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">We select a model trained with a deep learning framework compatible with both TensorRT and Vitis AI: TensorFlow. Accordingly, the first step is to convert a YOLOv4 model to TensorFlow, which was originally trained in Darknet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, to create a baseline for the optimization pipeline.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Vitis AI provides three separate optimization tools. The Vitis AI Optimizer, which applies the pruning, the Vitis AI Quantizer, which quantizes the model into INT8-precision and the Vitis AI Compiler, which compiles the model for the underlying DPU design.
Pruning with the Vitis AI Optimizer is not considered in the optimization pipeline directly, since the related software tool is not free of charge. Its effects are nevertheless analyzed in a separate experiment. If the validation results are satisfactory and only a slight or even no loss of accuracy can be noted, the quantized model must be compiled using the Vitis AI compiler.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">For optimization with TensorRT, we convert the TensorFlow model into ONNX format as a first step. For this purpose, the command line tool <span id="S4.SS1.p4.1.1" class="ltx_text">onnx-tf</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> is used. Subsequently, the ONNX model is optimized for utilization on inference hardware using TensorRT. According to the official documentation of the tool, <span id="S4.SS1.p4.1.2" class="ltx_text">onnx-tf</span> not only provides a portable intermediate format that increases the interoperability of the model, but it also performs optimization steps during the conversion that are intended to reduce the inference time. Pruning is already performed in this step: unused ONNX operators (e.g., neurons of the DNN) are removed and similar operators are merged.
The TensorRT Optimizer processes the converted ONNX model. Therefore, the optimizer must be executed directly on the target hardware, since various hardware-specific optimizations are performed.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">As evaluation measures, we choose the inference time, FPS, the energy consumption in watts, the energy efficiency in FPS/watt, and the mAP 0.5:0.95 for the accuracy.
For the measurement of the inference time, the object detector plugins were provided with time measurements, which are sent to the host along with the result message.
The measurement of power consumption in watts is realized by reading a multimeter, which is connected in series with the hardware platform to be evaluated. To measure the power in watts, the constant set voltage is multiplied by the current. A distinction is made between absolute and relative power consumption during the execution of object detection. To determine the relative power, the idle power, which is the power consumption of the target device without running an application, is subtracted from the absolute power used to actually execute the application. This increases the comparability between the inference hardware since the absolute power of the two platforms differs greatly as shown in Table <a href="#S4.T1" title="Table 1 ‣ 4.3 Experimental Results ‣ 4 Experiments ‣ A Framework for Benchmarking Real-Time Embedded Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
Energy efficiency indicates the FPS in relation to the power consumption. This enables us to analyze how efficiently the resources of the inference hardware are used in terms of power consumption. Due to the diversity of the compared hardware in terms of computing power and power consumption, measuring the energy efficiency allows for a cross-platform comparison, which puts the energy consumption of the platform in relation to the runtime performance.
Accuracy is measured by averaging the mAP for Intersection-over-Union (IoU) thresholds from .5 to .95 in steps of .05. mAP is a standard metric for evaluating object detectors and used for the COCO benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation Details</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.4" class="ltx_p">The baseline model is available in resolutions of <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="416\times 416" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">416</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">416</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><times id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">416</cn><cn type="integer" id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">416</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">416\times 416</annotation></semantics></math>, <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="512\times 512" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mn id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">512</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><times id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">512</cn><cn type="integer" id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">512\times 512</annotation></semantics></math>, and <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="608\times 608" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mrow id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mn id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml">608</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p1.3.m3.1.1.1" xref="S4.SS2.p1.3.m3.1.1.1.cmml">×</mo><mn id="S4.SS2.p1.3.m3.1.1.3" xref="S4.SS2.p1.3.m3.1.1.3.cmml">608</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><times id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1"></times><cn type="integer" id="S4.SS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2">608</cn><cn type="integer" id="S4.SS2.p1.3.m3.1.1.3.cmml" xref="S4.SS2.p1.3.m3.1.1.3">608</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">608\times 608</annotation></semantics></math> pixels. It was found that the <math id="S4.SS2.p1.4.m4.1" class="ltx_Math" alttext="512\times 512" display="inline"><semantics id="S4.SS2.p1.4.m4.1a"><mrow id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml"><mn id="S4.SS2.p1.4.m4.1.1.2" xref="S4.SS2.p1.4.m4.1.1.2.cmml">512</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p1.4.m4.1.1.1" xref="S4.SS2.p1.4.m4.1.1.1.cmml">×</mo><mn id="S4.SS2.p1.4.m4.1.1.3" xref="S4.SS2.p1.4.m4.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><apply id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1"><times id="S4.SS2.p1.4.m4.1.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1.1"></times><cn type="integer" id="S4.SS2.p1.4.m4.1.1.2.cmml" xref="S4.SS2.p1.4.m4.1.1.2">512</cn><cn type="integer" id="S4.SS2.p1.4.m4.1.1.3.cmml" xref="S4.SS2.p1.4.m4.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">512\times 512</annotation></semantics></math> resolution provides the best compromise between accuracy and runtime. Therefore, this model is used as the baseline model for the majority of the experiments. Nevertheless, in a separate experiment (see Fig. <a href="#S4.F6.sf1" title="In Figure 6 ‣ 4.4 Discussion ‣ 4 Experiments ‣ A Framework for Benchmarking Real-Time Embedded Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(a)</span></a>, <a href="#S4.F6.sf2" title="In Figure 6 ‣ 4.4 Discussion ‣ 4 Experiments ‣ A Framework for Benchmarking Real-Time Embedded Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(b)</span></a> and <a href="#S4.F6.sf3" title="In Figure 6 ‣ 4.4 Discussion ‣ 4 Experiments ‣ A Framework for Benchmarking Real-Time Embedded Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(c)</span></a>), the other two variants are also evaluated and included in the final comparison.
All executions of the object detection algorithm are performed with similar pre- and post-processing, which are based on reference implementations of the respective framework. For example, for Vitis AI, the pre- and post-processing is implemented following the example of the Vitis AI Library, which is a highly optimized variant. For TensorRT and ONNX, pre- and post-processing is implemented via Python numpy. For a fair comparison, we verify that all implementations are similar across all frameworks.
For each evaluation session, object detection with the COCO validation dataset consisting of 5,000 frames of image data is run six times in total and the evaluation results are averaged to ensure more stable results in terms of runtime. For the measurement of the accuracy, the confidence threshold is uniformly set to 0.25. For the execution of the NMS in post-processing, a threshold of 0.45 was set. The choice of threshold values was taken from existing implementations of a YOLOv4 object detection application <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.
Accuracy measurements on the test dataset, consisting of about 20,000 frames of image data, are performed only once.
The annotations of the test dataset are not freely available, since they are used for participation in the COCO challenge, and the related website allows to upload results to the COCO server just up to five times a day. In the context of this work, accuracy measurements have been performed with the test dataset for each optimization level in this way. For optimizations that have no influence on the accuracy, only the validation dataset, consisting of 5,000 frames of image data, is entered several times for the evaluation, since the annotations of the validation dataset are freely available and the accuracy can thus be determined using the proposed evaluation framework.
The confidence threshold was reduced to .05 for the measurements on the test dataset, since this setting is used to evaluate the original darknet YOLOv4. It is a common practice to set the confidence threshold for evaluating the accuracy that low to achieve a higher mAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Experimental Results</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">For the evaluation of the TensorRT optimizer, the model was quantized to all possible precisions such as Integer representations using 8 Bits (INT8) and floating point representations using 32 Bits (FP32) and 16 Bits (FP16), respectively.</p>
</div>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11580/assets/fig-bar-trt.png" id="S4.F5.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F5.sf1.3.2" class="ltx_text" style="font-size:90%;">TensorRT</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11580/assets/fig-bar-vai.png" id="S4.F5.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F5.sf2.3.2" class="ltx_text" style="font-size:90%;">Vitis AI</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;">Accuracy comparison of optimized DNN models after all optimization steps. A higher bar indicates higher mAP and thus better DNN performance. TF is the reference DNN run in TensorFlow at FP32 precision.</span></figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Figure <a href="#S4.F5.sf1" title="In Figure 5 ‣ 4.3 Experimental Results ‣ 4 Experiments ‣ A Framework for Benchmarking Real-Time Embedded Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(a)</span></a> shows the accuracy loss when quantizing to all available precisions. It could be observed that optimization with TensorRT in FP32 format results in accuracy losses. However, when quantizing the model converted to FP16 format, the same effects on the accuracy could be observed. But the inference time could be almost halved compared to the FP32 variant as we show in Table <a href="#S4.T1" title="Table 1 ‣ 4.3 Experimental Results ‣ 4 Experiments ‣ A Framework for Benchmarking Real-Time Embedded Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.6" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.6.7.1" class="ltx_tr">
<td id="S4.T1.6.7.1.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;" rowspan="2"><span id="S4.T1.6.7.1.1.1" class="ltx_text ltx_font_bold">Framework</span></td>
<td id="S4.T1.6.7.1.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;" rowspan="2"><span id="S4.T1.6.7.1.2.1" class="ltx_text ltx_font_bold">Device</span></td>
<td id="S4.T1.6.7.1.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;" rowspan="2"><span id="S4.T1.6.7.1.3.1" class="ltx_text ltx_font_bold">Precision</span></td>
<td id="S4.T1.6.7.1.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T1.6.7.1.4.1" class="ltx_text ltx_font_bold">Inference</span></td>
<td id="S4.T1.6.7.1.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T1.6.7.1.5.1" class="ltx_text ltx_font_bold">Absolute</span></td>
<td id="S4.T1.6.7.1.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T1.6.7.1.6.1" class="ltx_text ltx_font_bold">Relative</span></td>
<td id="S4.T1.6.7.1.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;" rowspan="2"><span id="S4.T1.6.7.1.7.1" class="ltx_text ltx_font_bold">Efficiency</span></td>
<td id="S4.T1.6.7.1.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;" rowspan="2"><span id="S4.T1.6.7.1.8.1" class="ltx_text ltx_font_bold">mAP</span></td>
</tr>
<tr id="S4.T1.6.8.2" class="ltx_tr">
<td id="S4.T1.6.8.2.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T1.6.8.2.1.1" class="ltx_text ltx_font_bold">Time</span></td>
<td id="S4.T1.6.8.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T1.6.8.2.2.1" class="ltx_text ltx_font_bold">Power</span></td>
<td id="S4.T1.6.8.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T1.6.8.2.3.1" class="ltx_text ltx_font_bold">Power</span></td>
</tr>
<tr id="S4.T1.6.9.3" class="ltx_tr">
<td id="S4.T1.6.9.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T1.6.9.3.1.1" class="ltx_text ltx_font_bold">TensorFlow</span></td>
<td id="S4.T1.6.9.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">CPU</td>
<td id="S4.T1.6.9.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">FP32</td>
<td id="S4.T1.6.9.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">767.16 ms</td>
<td id="S4.T1.6.9.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">N/A</td>
<td id="S4.T1.6.9.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">N/A</td>
<td id="S4.T1.6.9.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">N/A</td>
<td id="S4.T1.6.9.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">40.7</td>
</tr>
<tr id="S4.T1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T1.1.1.2.1" class="ltx_text ltx_font_bold">TensorRT</span></td>
<td id="S4.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Jetson</td>
<td id="S4.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">FP32</td>
<td id="S4.T1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">69.75 ms</td>
<td id="S4.T1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">28 W</td>
<td id="S4.T1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">18.5 W</td>
<td id="S4.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.45 <math id="S4.T1.1.1.1.m1.1" class="ltx_Math" alttext="\frac{FPS}{W}" display="inline"><semantics id="S4.T1.1.1.1.m1.1a"><mfrac id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml"><mrow id="S4.T1.1.1.1.m1.1.1.2" xref="S4.T1.1.1.1.m1.1.1.2.cmml"><mi id="S4.T1.1.1.1.m1.1.1.2.2" xref="S4.T1.1.1.1.m1.1.1.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.T1.1.1.1.m1.1.1.2.1" xref="S4.T1.1.1.1.m1.1.1.2.1.cmml">​</mo><mi id="S4.T1.1.1.1.m1.1.1.2.3" xref="S4.T1.1.1.1.m1.1.1.2.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.T1.1.1.1.m1.1.1.2.1a" xref="S4.T1.1.1.1.m1.1.1.2.1.cmml">​</mo><mi id="S4.T1.1.1.1.m1.1.1.2.4" xref="S4.T1.1.1.1.m1.1.1.2.4.cmml">S</mi></mrow><mi id="S4.T1.1.1.1.m1.1.1.3" xref="S4.T1.1.1.1.m1.1.1.3.cmml">W</mi></mfrac><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><apply id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1"><divide id="S4.T1.1.1.1.m1.1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1"></divide><apply id="S4.T1.1.1.1.m1.1.1.2.cmml" xref="S4.T1.1.1.1.m1.1.1.2"><times id="S4.T1.1.1.1.m1.1.1.2.1.cmml" xref="S4.T1.1.1.1.m1.1.1.2.1"></times><ci id="S4.T1.1.1.1.m1.1.1.2.2.cmml" xref="S4.T1.1.1.1.m1.1.1.2.2">𝐹</ci><ci id="S4.T1.1.1.1.m1.1.1.2.3.cmml" xref="S4.T1.1.1.1.m1.1.1.2.3">𝑃</ci><ci id="S4.T1.1.1.1.m1.1.1.2.4.cmml" xref="S4.T1.1.1.1.m1.1.1.2.4">𝑆</ci></apply><ci id="S4.T1.1.1.1.m1.1.1.3.cmml" xref="S4.T1.1.1.1.m1.1.1.3">𝑊</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">\frac{FPS}{W}</annotation></semantics></math>
</td>
<td id="S4.T1.1.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">39.8</td>
</tr>
<tr id="S4.T1.2.2" class="ltx_tr">
<td id="S4.T1.2.2.2" class="ltx_td ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S4.T1.2.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Jetson</td>
<td id="S4.T1.2.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">FP16</td>
<td id="S4.T1.2.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">35.32 ms</td>
<td id="S4.T1.2.2.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">20 W</td>
<td id="S4.T1.2.2.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">10.5 W</td>
<td id="S4.T1.2.2.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">1.17 <math id="S4.T1.2.2.1.m1.1" class="ltx_Math" alttext="\frac{FPS}{W}" display="inline"><semantics id="S4.T1.2.2.1.m1.1a"><mfrac id="S4.T1.2.2.1.m1.1.1" xref="S4.T1.2.2.1.m1.1.1.cmml"><mrow id="S4.T1.2.2.1.m1.1.1.2" xref="S4.T1.2.2.1.m1.1.1.2.cmml"><mi id="S4.T1.2.2.1.m1.1.1.2.2" xref="S4.T1.2.2.1.m1.1.1.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.T1.2.2.1.m1.1.1.2.1" xref="S4.T1.2.2.1.m1.1.1.2.1.cmml">​</mo><mi id="S4.T1.2.2.1.m1.1.1.2.3" xref="S4.T1.2.2.1.m1.1.1.2.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.T1.2.2.1.m1.1.1.2.1a" xref="S4.T1.2.2.1.m1.1.1.2.1.cmml">​</mo><mi id="S4.T1.2.2.1.m1.1.1.2.4" xref="S4.T1.2.2.1.m1.1.1.2.4.cmml">S</mi></mrow><mi id="S4.T1.2.2.1.m1.1.1.3" xref="S4.T1.2.2.1.m1.1.1.3.cmml">W</mi></mfrac><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.1.m1.1b"><apply id="S4.T1.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.1.m1.1.1"><divide id="S4.T1.2.2.1.m1.1.1.1.cmml" xref="S4.T1.2.2.1.m1.1.1"></divide><apply id="S4.T1.2.2.1.m1.1.1.2.cmml" xref="S4.T1.2.2.1.m1.1.1.2"><times id="S4.T1.2.2.1.m1.1.1.2.1.cmml" xref="S4.T1.2.2.1.m1.1.1.2.1"></times><ci id="S4.T1.2.2.1.m1.1.1.2.2.cmml" xref="S4.T1.2.2.1.m1.1.1.2.2">𝐹</ci><ci id="S4.T1.2.2.1.m1.1.1.2.3.cmml" xref="S4.T1.2.2.1.m1.1.1.2.3">𝑃</ci><ci id="S4.T1.2.2.1.m1.1.1.2.4.cmml" xref="S4.T1.2.2.1.m1.1.1.2.4">𝑆</ci></apply><ci id="S4.T1.2.2.1.m1.1.1.3.cmml" xref="S4.T1.2.2.1.m1.1.1.3">𝑊</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.1.m1.1c">\frac{FPS}{W}</annotation></semantics></math>
</td>
<td id="S4.T1.2.2.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">39.8</td>
</tr>
<tr id="S4.T1.3.3" class="ltx_tr">
<td id="S4.T1.3.3.2" class="ltx_td ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S4.T1.3.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Jetson</td>
<td id="S4.T1.3.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">INT8</td>
<td id="S4.T1.3.3.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">22.0 ms</td>
<td id="S4.T1.3.3.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">16.5 W</td>
<td id="S4.T1.3.3.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">7.0 W</td>
<td id="S4.T1.3.3.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">2.05 <math id="S4.T1.3.3.1.m1.1" class="ltx_Math" alttext="\frac{FPS}{W}" display="inline"><semantics id="S4.T1.3.3.1.m1.1a"><mfrac id="S4.T1.3.3.1.m1.1.1" xref="S4.T1.3.3.1.m1.1.1.cmml"><mrow id="S4.T1.3.3.1.m1.1.1.2" xref="S4.T1.3.3.1.m1.1.1.2.cmml"><mi id="S4.T1.3.3.1.m1.1.1.2.2" xref="S4.T1.3.3.1.m1.1.1.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.T1.3.3.1.m1.1.1.2.1" xref="S4.T1.3.3.1.m1.1.1.2.1.cmml">​</mo><mi id="S4.T1.3.3.1.m1.1.1.2.3" xref="S4.T1.3.3.1.m1.1.1.2.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.T1.3.3.1.m1.1.1.2.1a" xref="S4.T1.3.3.1.m1.1.1.2.1.cmml">​</mo><mi id="S4.T1.3.3.1.m1.1.1.2.4" xref="S4.T1.3.3.1.m1.1.1.2.4.cmml">S</mi></mrow><mi id="S4.T1.3.3.1.m1.1.1.3" xref="S4.T1.3.3.1.m1.1.1.3.cmml">W</mi></mfrac><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.1.m1.1b"><apply id="S4.T1.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.1.m1.1.1"><divide id="S4.T1.3.3.1.m1.1.1.1.cmml" xref="S4.T1.3.3.1.m1.1.1"></divide><apply id="S4.T1.3.3.1.m1.1.1.2.cmml" xref="S4.T1.3.3.1.m1.1.1.2"><times id="S4.T1.3.3.1.m1.1.1.2.1.cmml" xref="S4.T1.3.3.1.m1.1.1.2.1"></times><ci id="S4.T1.3.3.1.m1.1.1.2.2.cmml" xref="S4.T1.3.3.1.m1.1.1.2.2">𝐹</ci><ci id="S4.T1.3.3.1.m1.1.1.2.3.cmml" xref="S4.T1.3.3.1.m1.1.1.2.3">𝑃</ci><ci id="S4.T1.3.3.1.m1.1.1.2.4.cmml" xref="S4.T1.3.3.1.m1.1.1.2.4">𝑆</ci></apply><ci id="S4.T1.3.3.1.m1.1.1.3.cmml" xref="S4.T1.3.3.1.m1.1.1.3">𝑊</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.1.m1.1c">\frac{FPS}{W}</annotation></semantics></math>
</td>
<td id="S4.T1.3.3.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">38.6</td>
</tr>
<tr id="S4.T1.4.4" class="ltx_tr">
<td id="S4.T1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T1.4.4.2.1" class="ltx_text ltx_font_bold">Vitis AI</span></td>
<td id="S4.T1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">ZCU104</td>
<td id="S4.T1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">INT8</td>
<td id="S4.T1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">54.42 ms</td>
<td id="S4.T1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">23 W</td>
<td id="S4.T1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">7.0 W</td>
<td id="S4.T1.4.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">1.31 <math id="S4.T1.4.4.1.m1.1" class="ltx_Math" alttext="\frac{FPS}{W}" display="inline"><semantics id="S4.T1.4.4.1.m1.1a"><mfrac id="S4.T1.4.4.1.m1.1.1" xref="S4.T1.4.4.1.m1.1.1.cmml"><mrow id="S4.T1.4.4.1.m1.1.1.2" xref="S4.T1.4.4.1.m1.1.1.2.cmml"><mi id="S4.T1.4.4.1.m1.1.1.2.2" xref="S4.T1.4.4.1.m1.1.1.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.T1.4.4.1.m1.1.1.2.1" xref="S4.T1.4.4.1.m1.1.1.2.1.cmml">​</mo><mi id="S4.T1.4.4.1.m1.1.1.2.3" xref="S4.T1.4.4.1.m1.1.1.2.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.T1.4.4.1.m1.1.1.2.1a" xref="S4.T1.4.4.1.m1.1.1.2.1.cmml">​</mo><mi id="S4.T1.4.4.1.m1.1.1.2.4" xref="S4.T1.4.4.1.m1.1.1.2.4.cmml">S</mi></mrow><mi id="S4.T1.4.4.1.m1.1.1.3" xref="S4.T1.4.4.1.m1.1.1.3.cmml">W</mi></mfrac><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.1.m1.1b"><apply id="S4.T1.4.4.1.m1.1.1.cmml" xref="S4.T1.4.4.1.m1.1.1"><divide id="S4.T1.4.4.1.m1.1.1.1.cmml" xref="S4.T1.4.4.1.m1.1.1"></divide><apply id="S4.T1.4.4.1.m1.1.1.2.cmml" xref="S4.T1.4.4.1.m1.1.1.2"><times id="S4.T1.4.4.1.m1.1.1.2.1.cmml" xref="S4.T1.4.4.1.m1.1.1.2.1"></times><ci id="S4.T1.4.4.1.m1.1.1.2.2.cmml" xref="S4.T1.4.4.1.m1.1.1.2.2">𝐹</ci><ci id="S4.T1.4.4.1.m1.1.1.2.3.cmml" xref="S4.T1.4.4.1.m1.1.1.2.3">𝑃</ci><ci id="S4.T1.4.4.1.m1.1.1.2.4.cmml" xref="S4.T1.4.4.1.m1.1.1.2.4">𝑆</ci></apply><ci id="S4.T1.4.4.1.m1.1.1.3.cmml" xref="S4.T1.4.4.1.m1.1.1.3">𝑊</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.1.m1.1c">\frac{FPS}{W}</annotation></semantics></math>
</td>
<td id="S4.T1.4.4.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">37.5</td>
</tr>
<tr id="S4.T1.5.5" class="ltx_tr">
<td id="S4.T1.5.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T1.5.5.2.1" class="ltx_text ltx_font_bold">Vitis AI Zoo</span></td>
<td id="S4.T1.5.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">ZCU104</td>
<td id="S4.T1.5.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">INT8</td>
<td id="S4.T1.5.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">67.84 ms</td>
<td id="S4.T1.5.5.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">22.5 W</td>
<td id="S4.T1.5.5.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">6.5 W</td>
<td id="S4.T1.5.5.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">1.40 <math id="S4.T1.5.5.1.m1.1" class="ltx_Math" alttext="\frac{FPS}{W}" display="inline"><semantics id="S4.T1.5.5.1.m1.1a"><mfrac id="S4.T1.5.5.1.m1.1.1" xref="S4.T1.5.5.1.m1.1.1.cmml"><mrow id="S4.T1.5.5.1.m1.1.1.2" xref="S4.T1.5.5.1.m1.1.1.2.cmml"><mi id="S4.T1.5.5.1.m1.1.1.2.2" xref="S4.T1.5.5.1.m1.1.1.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.T1.5.5.1.m1.1.1.2.1" xref="S4.T1.5.5.1.m1.1.1.2.1.cmml">​</mo><mi id="S4.T1.5.5.1.m1.1.1.2.3" xref="S4.T1.5.5.1.m1.1.1.2.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.T1.5.5.1.m1.1.1.2.1a" xref="S4.T1.5.5.1.m1.1.1.2.1.cmml">​</mo><mi id="S4.T1.5.5.1.m1.1.1.2.4" xref="S4.T1.5.5.1.m1.1.1.2.4.cmml">S</mi></mrow><mi id="S4.T1.5.5.1.m1.1.1.3" xref="S4.T1.5.5.1.m1.1.1.3.cmml">W</mi></mfrac><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.1.m1.1b"><apply id="S4.T1.5.5.1.m1.1.1.cmml" xref="S4.T1.5.5.1.m1.1.1"><divide id="S4.T1.5.5.1.m1.1.1.1.cmml" xref="S4.T1.5.5.1.m1.1.1"></divide><apply id="S4.T1.5.5.1.m1.1.1.2.cmml" xref="S4.T1.5.5.1.m1.1.1.2"><times id="S4.T1.5.5.1.m1.1.1.2.1.cmml" xref="S4.T1.5.5.1.m1.1.1.2.1"></times><ci id="S4.T1.5.5.1.m1.1.1.2.2.cmml" xref="S4.T1.5.5.1.m1.1.1.2.2">𝐹</ci><ci id="S4.T1.5.5.1.m1.1.1.2.3.cmml" xref="S4.T1.5.5.1.m1.1.1.2.3">𝑃</ci><ci id="S4.T1.5.5.1.m1.1.1.2.4.cmml" xref="S4.T1.5.5.1.m1.1.1.2.4">𝑆</ci></apply><ci id="S4.T1.5.5.1.m1.1.1.3.cmml" xref="S4.T1.5.5.1.m1.1.1.3">𝑊</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.1.m1.1c">\frac{FPS}{W}</annotation></semantics></math>
</td>
<td id="S4.T1.5.5.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">38.3</td>
</tr>
<tr id="S4.T1.6.6" class="ltx_tr">
<td id="S4.T1.6.6.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T1.6.6.2.1" class="ltx_text ltx_font_bold">Zoo Pruned</span></td>
<td id="S4.T1.6.6.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">ZCU104</td>
<td id="S4.T1.6.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">INT8</td>
<td id="S4.T1.6.6.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">48.42 ms</td>
<td id="S4.T1.6.6.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">21.6 W</td>
<td id="S4.T1.6.6.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">5.6 W</td>
<td id="S4.T1.6.6.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">1.97 <math id="S4.T1.6.6.1.m1.1" class="ltx_Math" alttext="\frac{FPS}{W}" display="inline"><semantics id="S4.T1.6.6.1.m1.1a"><mfrac id="S4.T1.6.6.1.m1.1.1" xref="S4.T1.6.6.1.m1.1.1.cmml"><mrow id="S4.T1.6.6.1.m1.1.1.2" xref="S4.T1.6.6.1.m1.1.1.2.cmml"><mi id="S4.T1.6.6.1.m1.1.1.2.2" xref="S4.T1.6.6.1.m1.1.1.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.T1.6.6.1.m1.1.1.2.1" xref="S4.T1.6.6.1.m1.1.1.2.1.cmml">​</mo><mi id="S4.T1.6.6.1.m1.1.1.2.3" xref="S4.T1.6.6.1.m1.1.1.2.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.T1.6.6.1.m1.1.1.2.1a" xref="S4.T1.6.6.1.m1.1.1.2.1.cmml">​</mo><mi id="S4.T1.6.6.1.m1.1.1.2.4" xref="S4.T1.6.6.1.m1.1.1.2.4.cmml">S</mi></mrow><mi id="S4.T1.6.6.1.m1.1.1.3" xref="S4.T1.6.6.1.m1.1.1.3.cmml">W</mi></mfrac><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.1.m1.1b"><apply id="S4.T1.6.6.1.m1.1.1.cmml" xref="S4.T1.6.6.1.m1.1.1"><divide id="S4.T1.6.6.1.m1.1.1.1.cmml" xref="S4.T1.6.6.1.m1.1.1"></divide><apply id="S4.T1.6.6.1.m1.1.1.2.cmml" xref="S4.T1.6.6.1.m1.1.1.2"><times id="S4.T1.6.6.1.m1.1.1.2.1.cmml" xref="S4.T1.6.6.1.m1.1.1.2.1"></times><ci id="S4.T1.6.6.1.m1.1.1.2.2.cmml" xref="S4.T1.6.6.1.m1.1.1.2.2">𝐹</ci><ci id="S4.T1.6.6.1.m1.1.1.2.3.cmml" xref="S4.T1.6.6.1.m1.1.1.2.3">𝑃</ci><ci id="S4.T1.6.6.1.m1.1.1.2.4.cmml" xref="S4.T1.6.6.1.m1.1.1.2.4">𝑆</ci></apply><ci id="S4.T1.6.6.1.m1.1.1.3.cmml" xref="S4.T1.6.6.1.m1.1.1.3">𝑊</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.1.m1.1c">\frac{FPS}{W}</annotation></semantics></math>
</td>
<td id="S4.T1.6.6.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">36.5</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.8.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.9.2" class="ltx_text" style="font-size:90%;">Comparison of the optimized models across all used frameworks using all available quantization modes.</span></figcaption>
</figure>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">In addition, a lower energy consumption is evident. The INT8 quantization causes a loss of accuracy compared to the FP32 and FP16 models, but the average inference time was reduced. The Jetson AGX Xavier offers the possibility to configure the maximum clock rate, and therefore the maximum power consumption of the device, through so-called Jetson Power Modes. Mode 1, 2, and 5 budget the maximum power of the Jetson to 10, 15, and 30 watts, respectively. Mode 0 or mode MAXN is selected by default and sets the maximum possible power of the Jetson. The INT8 model optimized with TensorRT is used for the experiment as it shows the highest energy efficiency and by adjusting the power mode, this can be further optimized. As shown in Table <a href="#S4.T2" title="Table 2 ‣ 4.3 Experimental Results ‣ 4 Experiments ‣ A Framework for Benchmarking Real-Time Embedded Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the power modes have a direct impact on the overall runtime of the object detection application. Please note that there is a time difference for the same experiment reported in Table <a href="#S4.T2" title="Table 2 ‣ 4.3 Experimental Results ‣ 4 Experiments ‣ A Framework for Benchmarking Real-Time Embedded Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> with 70.23 ms and in Table <a href="#S4.T1" title="Table 1 ‣ 4.3 Experimental Results ‣ 4 Experiments ‣ A Framework for Benchmarking Real-Time Embedded Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> with 69.75 ms since we run the same experiment multiple times.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.2.1.1" class="ltx_tr">
<th id="S4.T2.2.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_r" style="padding:1.5pt 10.0pt;"></th>
<th id="S4.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding:1.5pt 10.0pt;"><span id="S4.T2.2.1.1.2.1" class="ltx_text ltx_font_bold">MAXN</span></th>
<th id="S4.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding:1.5pt 10.0pt;"><span id="S4.T2.2.1.1.3.1" class="ltx_text ltx_font_bold">30W</span></th>
<th id="S4.T2.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding:1.5pt 10.0pt;"><span id="S4.T2.2.1.1.4.1" class="ltx_text ltx_font_bold">15W</span></th>
<th id="S4.T2.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:1.5pt 10.0pt;"><span id="S4.T2.2.1.1.5.1" class="ltx_text ltx_font_bold">10W</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.2.2.1" class="ltx_tr">
<td id="S4.T2.2.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 10.0pt;"><span id="S4.T2.2.2.1.1.1" class="ltx_text ltx_font_bold">Pre-Process</span></td>
<td id="S4.T2.2.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 10.0pt;">8.78 ms</td>
<td id="S4.T2.2.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 10.0pt;">10.91 ms</td>
<td id="S4.T2.2.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 10.0pt;">16.43 ms</td>
<td id="S4.T2.2.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 10.0pt;">17.11 ms</td>
</tr>
<tr id="S4.T2.2.3.2" class="ltx_tr">
<td id="S4.T2.2.3.2.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 10.0pt;"><span id="S4.T2.2.3.2.1.1" class="ltx_text ltx_font_bold">Inference</span></td>
<td id="S4.T2.2.3.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 10.0pt;">22.0 ms</td>
<td id="S4.T2.2.3.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 10.0pt;">26.22 ms</td>
<td id="S4.T2.2.3.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 10.0pt;">33.75 ms</td>
<td id="S4.T2.2.3.2.5" class="ltx_td ltx_align_center" style="padding:1.5pt 10.0pt;">61.76 ms</td>
</tr>
<tr id="S4.T2.2.4.3" class="ltx_tr">
<td id="S4.T2.2.4.3.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 10.0pt;"><span id="S4.T2.2.4.3.1.1" class="ltx_text ltx_font_bold">Post-Process</span></td>
<td id="S4.T2.2.4.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 10.0pt;">39.45 ms</td>
<td id="S4.T2.2.4.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 10.0pt;">48.62 ms</td>
<td id="S4.T2.2.4.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 10.0pt;">68.12 ms</td>
<td id="S4.T2.2.4.3.5" class="ltx_td ltx_align_center" style="padding:1.5pt 10.0pt;">71.71 ms</td>
</tr>
<tr id="S4.T2.2.5.4" class="ltx_tr">
<td id="S4.T2.2.5.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 10.0pt;"><span id="S4.T2.2.5.4.1.1" class="ltx_text ltx_font_bold">Total</span></td>
<td id="S4.T2.2.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 10.0pt;">70.23 ms</td>
<td id="S4.T2.2.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 10.0pt;">85.75 ms</td>
<td id="S4.T2.2.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 10.0pt;">118.3 ms</td>
<td id="S4.T2.2.5.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 10.0pt;">150.58 ms</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.4.2" class="ltx_text" style="font-size:90%;">Runtime on Nvidia Jetson in different power modes.</span></figcaption>
</figure>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">For the optimization with Vitis AI, the Vitis AI Quantizer was applied first. The quantized model is also executed on the desktop PC and evaluated with respect to accuracy and inference time. Figure <a href="#S4.F5.sf2" title="In Figure 5 ‣ 4.3 Experimental Results ‣ 4 Experiments ‣ A Framework for Benchmarking Real-Time Embedded Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(b)</span></a> shows the accuracy loss after all optimization steps for Vitis AI. It was found that the Vitis AI Quantizer reduces the mAP by 2.2 %. This result can be verified by checking against reference instructions for optimizing a YOLOv4 model as similar results are obtained. Compared to the baseline model, a significantly higher runtime of the quantized model was observed on the desktop PC. However, it is noticeable that only one processor core is used when executing the model on the Desktop PC. When executing the baseline model, all available cores are used. Thus, the inference of the Vitis AI quantized model is not optimally parallelized. After compiling the model, using the Vitis AI compiler, it is subsequently installed on the ZCU104 and executed by the evaluation framework. It can be seen that the mAP is lower in comparison with the base model, but also inference time is reduced to 54.4 ms (see Table <a href="#S4.T1" title="Table 1 ‣ 4.3 Experimental Results ‣ 4 Experiments ‣ A Framework for Benchmarking Real-Time Embedded Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. For the optimization via Vitis AI Optimizer, already optimized models from the Vitis AI Model Zoo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> are used and their results are evaluated. We utilize a YOLOv4 model trained on COCO, which is provided pruned and unpruned with an input resolution of <math id="S4.SS3.p4.1.m1.1" class="ltx_Math" alttext="416\times 416" display="inline"><semantics id="S4.SS3.p4.1.m1.1a"><mrow id="S4.SS3.p4.1.m1.1.1" xref="S4.SS3.p4.1.m1.1.1.cmml"><mn id="S4.SS3.p4.1.m1.1.1.2" xref="S4.SS3.p4.1.m1.1.1.2.cmml">416</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p4.1.m1.1.1.1" xref="S4.SS3.p4.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS3.p4.1.m1.1.1.3" xref="S4.SS3.p4.1.m1.1.1.3.cmml">416</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.1.m1.1b"><apply id="S4.SS3.p4.1.m1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1"><times id="S4.SS3.p4.1.m1.1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1.1"></times><cn type="integer" id="S4.SS3.p4.1.m1.1.1.2.cmml" xref="S4.SS3.p4.1.m1.1.1.2">416</cn><cn type="integer" id="S4.SS3.p4.1.m1.1.1.3.cmml" xref="S4.SS3.p4.1.m1.1.1.3">416</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.1.m1.1c">416\times 416</annotation></semantics></math>. In the information provided about each model in Model Zoo, it is described that the number of operations in the model was reduced by 36 % using the Vitis AI Optimizer. The two models are also integrated and evaluated in this experiment.
When comparing the similarly optimized models in different input resolutions, it can be seen that the optimizations of the model in different resolutions result in similar improvements in terms of inference time and energy efficiency. Thus, after applying the optimizations, similar results are obtained as shown in Fig. <a href="#S4.F6.sf1" title="In Figure 6 ‣ 4.4 Discussion ‣ 4 Experiments ‣ A Framework for Benchmarking Real-Time Embedded Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(a)</span></a> and Fig. <a href="#S4.F6.sf2" title="In Figure 6 ‣ 4.4 Discussion ‣ 4 Experiments ‣ A Framework for Benchmarking Real-Time Embedded Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(b)</span></a>. No improvements in terms of power consumption could be identified, as shown in Fig. <a href="#S4.F6.sf3" title="In Figure 6 ‣ 4.4 Discussion ‣ 4 Experiments ‣ A Framework for Benchmarking Real-Time Embedded Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(c)</span></a>. It can be concluded that the input resolution has no direct influence on the effectiveness of the applied optimization tools. If a low inference time at an acceptable cost of accuracy is required, a smaller input resolution is preferable.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Discussion</h3>

<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11580/assets/fig-map_vs_infer.png" id="S4.F6.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F6.sf1.3.2" class="ltx_text" style="font-size:90%;">Accuracy vs. Inference Time</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11580/assets/fig-map_vs_eff.png" id="S4.F6.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F6.sf2.3.2" class="ltx_text" style="font-size:90%;">Accuracy vs. Energy Efficiency</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F6.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2304.11580/assets/fig-map_vs_watt.png" id="S4.F6.sf3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F6.sf3.3.2" class="ltx_text" style="font-size:90%;">Accuracy vs. Power Consumption</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.3.2" class="ltx_text" style="font-size:90%;">Comparison of all optimized models with respect to the chosen evaluation measures accuracy, inference time, power consumption and energy efficiency.</span></figcaption>
</figure>
<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">The results just described show that significant improvements in runtime and energy efficiency can be achieved by optimizing with TensorRT and Vitis AI. However, a small loss of accuracy can be expected for most optimization operations. Since FP16 quantization with TensorRT does not result in any loss of accuracy, it outperforms all other optimized models when the highest possible accuracy is required. The FP16 model already has a lower runtime than all optimizations in Vitis AI as shown in Fig. <a href="#S4.F6.sf1" title="In Figure 6 ‣ 4.4 Discussion ‣ 4 Experiments ‣ A Framework for Benchmarking Real-Time Embedded Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(a)</span></a>. For the INT8 quantization operations, similar accuracy losses could be observed in TensorRT and Vitis AI. Nevertheless, after the compilation process with the Vitis AI compiler, the accuracy is further reduced. Accordingly, all TensorRT models outperform the Vitis AI models in terms of accuracy.
The TensorRT optimized INT8 model with an input resolution of <math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="416\times 416" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mrow id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml"><mn id="S4.SS4.p1.1.m1.1.1.2" xref="S4.SS4.p1.1.m1.1.1.2.cmml">416</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS4.p1.1.m1.1.1.1" xref="S4.SS4.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS4.p1.1.m1.1.1.3" xref="S4.SS4.p1.1.m1.1.1.3.cmml">416</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><apply id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"><times id="S4.SS4.p1.1.m1.1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS4.p1.1.m1.1.1.2.cmml" xref="S4.SS4.p1.1.m1.1.1.2">416</cn><cn type="integer" id="S4.SS4.p1.1.m1.1.1.3.cmml" xref="S4.SS4.p1.1.m1.1.1.3">416</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">416\times 416</annotation></semantics></math> outperforms all other models in terms of inference time at the expense of an acceptable loss of accuracy. Significant improvements in energy efficiency and inference time can be observed with the Vitis AI Optimizer. On the other hand, the largest accuracy loss of all optimizations is noted here. The measurements of the pruned model in terms of energy efficiency are the only ones that provide similarly good results as the TensorRT-INT8 model. Figure <a href="#S4.F6.sf1" title="In Figure 6 ‣ 4.4 Discussion ‣ 4 Experiments ‣ A Framework for Benchmarking Real-Time Embedded Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(a)</span></a>,  <a href="#S4.F6.sf2" title="In Figure 6 ‣ 4.4 Discussion ‣ 4 Experiments ‣ A Framework for Benchmarking Real-Time Embedded Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(b)</span></a> and <a href="#S4.F6.sf3" title="In Figure 6 ‣ 4.4 Discussion ‣ 4 Experiments ‣ A Framework for Benchmarking Real-Time Embedded Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(c)</span></a> also show the measurements at different power budgets in the Jetson. Power budgeting can increase energy efficiency at the expense of inference time. Hence, a power budget of 15 watts is preferable for the requirement of high energy efficiency instead of 10W Mode, since the inference time is only 11.75 ms higher in comparison to MAXN Mode.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We described a generic evaluation framework for computer vision algorithms deployed on an embedded device. As an exemplary task we chose object detection. The framework is <em id="S5.p1.1.1" class="ltx_emph ltx_font_italic">flexible</em> as different vendor-specific optimization pipelines are supported, <em id="S5.p1.1.2" class="ltx_emph ltx_font_italic">scalable</em> as multiple edge devices can be connected to the host computer and evaluated in parallel, and <em id="S5.p1.1.3" class="ltx_emph ltx_font_italic">light-weight</em> as basically only some MQTT messages need to be supported (i.e. interpreted and replied) to integrate new devices. We demonstrate the effectiveness of the framework in multiple experiments using reference hardware and optimization software from Nvidia and Xilinx. Performance deterioration during optimization, the effective runtime reduction as well as energy efficiency can be measured simultaneously.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Blalock, D., Gonzalez Ortiz, J., Frankle, J., Guttag, J.: What is the State
of Neural Network Pruning? arXiv preprint arXiv:2004.10934 (2020)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Blott, M., Fraser, N.J., Gambardella, G., Halder, L., Kath, J., Neveu, Z.,
Umuroglu, Y., Vasilciuc, A., Leeser, M., Doyle, L.: Evaluation of Optimized
CNNs on Heterogeneous Accelerators Using a Novel Benchmarking Approach. IEEE
Transactions on Computers <span id="bib.bib2.1.1" class="ltx_text ltx_font_bold">70</span>(10), 1654–1669 (2021)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Bochkovskiy, A., Wang, C., Liao, H.: YOLOv4: Optimal Speed and Accuracy of
Object Detection. arXiv preprint arXiv:2004.10934 (2020)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Bradski, G.: The OpenCV Library. Dr. Dobb’s Journal of Software Tools (2000)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Cai, Z., He, X., Sun, J., Vasconcelos, N.: Deep Learning with Low Precision by
Half-wave Gaussian Quantization. In: IEEE CVPR (2017)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
david8862: keras-yolov3-model-set,
<a target="_blank" href="https://github.com/david8862/keras-YOLOv3-model-set/tree/v1.3.0" title="" class="ltx_ref ltx_url"><span id="bib.bib6.1.1.1" class="ltx_text" style="color:#0000FF;">https://github.com/david8862/keras-YOLOv3-model-set/tree/v1.3.0</span></a>

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Eclipse: Eclipse paho mqtt c++ client library,
<a target="_blank" href="https://github.com/eclipse/paho.mqtt.cpp" title="" class="ltx_ref ltx_url"><span id="bib.bib7.1.1.1" class="ltx_text" style="color:#0000FF;">https://github.com/eclipse/paho.mqtt.cpp</span></a>

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Gemirter, C., Senturca, C., Baydere, S.: A Comparative Evaluation of AMQP,
MQTT and HTTP Protocols Using Real-Time Public Smart City Data. In: 6th
International Conference on Computer Science and Engineering (UBMK) (2021)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M., Keutzer, K.: A survey of
quantization methods for efficient neural network inference. arXiv preprint
arXiv:2103.13630 (2021)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Gog, I., Kalra, S., Schafhalter, P., Wright, M., Gonzalez, J., Stoica, I.:
Pylot: A Modular Platform for Exploring Latency-Accuracy Tradeoffs in
Autonomous Vehicles. In: Proceedings of the IEEE International Conference on
Robotics and Automation (ICRA) (2021)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Gündogan, C., Kietzmann, P., Lenders, M., Petersen, H., Schmidt, T.,
Wählisch, M.: NDN, CoAP, and MQTT: A Comparative Measurement Study in
the IoT. In: Proceedings of the 5th ACM Conference on Information-Centric
Networking (ICN) (2018)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Hamerski, J.C., Domingues, A.R., Moraes, F.G., Amory, A.: Evaluating
serialization for a publish-subscribe based middleware for mpsocs. In: 2018
25th IEEE International Conference on Electronics, Circuits and Systems
(ICECS) (2018). https://doi.org/10.1109/ICECS.2018.8618003

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Huang, J., Rathod, V., Sun, C., Zhu, M., Korattikara, A., Fathi, A., Fischer,
I., Wojna, Z., Song, Y., Guadarrama, S., Murphy, K.: Speed/accuracy
trade-offs for modern convolutional object detectors. In: IEEE CVPR (2017)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Intel: OpenVINO Repository, <a target="_blank" href="https://github.com/openvinotoolkit/openvino" title="" class="ltx_ref ltx_url"><span id="bib.bib14.1.1.1" class="ltx_text" style="color:#0000FF;">https://github.com/openvinotoolkit/openvino</span></a>

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Jung, S., Hwang, S., Shin, H., Shim, D.: Perception, Guidance, and Navigation
for Indoor Autonomous Drone Racing Using Deep Learning. IEEE Robotics and
Automation Letters <span id="bib.bib15.1.1" class="ltx_text ltx_font_bold">3</span>(3), 2539–2544 (2018)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Lin, T., Goyal, P., Girshick, R., He, K., Dollar, P.: Focal Loss for Dense
Object Detection. In: IEEE ICCV (2017)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Lin, T., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar,
P., Zitnick, C.: Microsoft COCO: Common Objects in Context. In: ECCV (2014)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Lin, Z., Yih, M., Ota, J., Owens, J., Muyan-Özcelik, P.: Benchmarking Deep
Learning Frameworks and Investigating FPGA Deployment for Traffic Sign
Classification and Detection. IEEE Transactions on Intelligent Vehicles
<span id="bib.bib18.1.1" class="ltx_text ltx_font_bold">4</span>(3), 385–395 (2019)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Liu, D., Kong, H., Luo, X., Liu, W., Subramaniam, R.: Bringing AI to edge:
From deep learning’s perspective. Neurocomputing <span id="bib.bib19.1.1" class="ltx_text ltx_font_bold">485</span>, 297–320
(2022)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Mishra, B., Kertesz, A.: The Use of MQTT in M2M and IoT Systems: A Survey.
IEEE Access <span id="bib.bib20.1.1" class="ltx_text ltx_font_bold">8</span>, 201071–201086 (2021)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
NVIDIA: TensorRT Repository,
<a target="_blank" href="https://https://github.com/NVIDIA/TensorRT/" title="" class="ltx_ref ltx_url"><span id="bib.bib21.1.1.1" class="ltx_text" style="color:#0000FF;">https://https://github.com/NVIDIA/TensorRT/</span></a>

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Nvidia: Jetson agx xavier developer kit - user guide (2019),
<a target="_blank" href="https://developer.download.nvidia.com/embedded/L4T/r32-3-1_Release_v1.0/jetson_agx_xavier_developer_kit_user_guide.pdf" title="" class="ltx_ref ltx_url"><span id="bib.bib22.1.1.1" class="ltx_text" style="color:#0000FF;">https://developer.download.nvidia.com/embedded/L4T/r32-3-1˙Release˙v1.0/jetson˙agx˙xavier˙developer˙kit˙user˙guide.pdf</span></a>

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
ONNX: Tensorflow backend for onnx,
<a target="_blank" href="https://github.com/onnx/onnx-tensorflow/" title="" class="ltx_ref ltx_url"><span id="bib.bib23.1.1.1" class="ltx_text" style="color:#0000FF;">https://github.com/onnx/onnx-tensorflow/</span></a>

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Redmon, J.: Darknet: Open source neural networks in c.
<a target="_blank" href="http://pjreddie.com/darknet/" title="" class="ltx_ref ltx_url"><span id="bib.bib24.1.1.1" class="ltx_text" style="color:#0000FF;">http://pjreddie.com/darknet/</span></a> (2013–2016)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You Only Look Once:
Unified, Real-Time Object Detection. In: IEEE CVPR (2016)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Redmon, J., Farhadi, A.: Yolov3: An incremental improvement. CoRR
<span id="bib.bib26.1.1" class="ltx_text ltx_font_bold">abs/1804.02767</span> (2018), <a target="_blank" href="http://arxiv.org/abs/1804.02767" title="" class="ltx_ref ltx_url"><span id="bib.bib26.2.2.1" class="ltx_text" style="color:#0000FF;">http://arxiv.org/abs/1804.02767</span></a>

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Rungsuptaweekoon, K., Visoottiviseth, V., Takano, R.: Evaluating the power
efficiency of deep learning inference on embedded gpu systems. In:
International Conference on Information Technology (INCIT) (2017)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Stäcker, L., Fei, J., Heidenreich, P., Bonarens, F., Rambach, J., Stricker,
D., Stiller, C.: Deployment of Deep Neural Networks for Object Detection on
Edge AI Devices with Runtime Optimization. In: IEEE International Conference
on Computer Vision Workshops (ICCVW) (2021)

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Verucchi, M., Brilli, G., Sapienza, D., Verasani, M., Arena, M., Gatti, F.,
Capotondi, A., Cavicchioli, R., Bertogna, M., Solieri, M.: A Systematic
Assessment of Embedded Neural Networks for Object Detection. In: 2020 25th
IEEE International Conference on Emerging Technologies and Factory Automation
(ETFA). vol. 1, pp. 937–944 (2020). https://doi.org/10.1109/ETFA46521.2020.9212130

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Wang, J., Gu, S.: FPGA Implementation of Object Detection Accelerator Based on
Vitis-AI. 2021 11th International Conference on Information Science and
Technology (ICIST) pp. 571–577 (2021)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Xilinx: Vitis AI Repository, <a target="_blank" href="https://github.com/Xilinx/Vitis-AI/" title="" class="ltx_ref ltx_url"><span id="bib.bib31.1.1.1" class="ltx_text" style="color:#0000FF;">https://github.com/Xilinx/Vitis-AI/</span></a>

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Xilinx: Zcu104 board user guide (2018),
<a target="_blank" href="https://www.xilinx.com/support/documentation/boards_and_kits/zcu104/ug1267-zcu104-eval-bd.pdf" title="" class="ltx_ref ltx_url"><span id="bib.bib32.1.1.1" class="ltx_text" style="color:#0000FF;">https://www.xilinx.com/support/documentation/boards˙and˙kits/zcu104/ug1267-zcu104-eval-bd.pdf</span></a>

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Xilinx: Dpuczdx8g for zynq ultrascale+ mpsocs (2021),
<a target="_blank" href="https://www.xilinx.com/content/dam/xilinx/support/documentation/ip_documentation/dpu/v3_3/pg338-dpu.pdf" title="" class="ltx_ref ltx_url"><span id="bib.bib33.1.1.1" class="ltx_text" style="color:#0000FF;">https://www.xilinx.com/content/dam/xilinx/support/documentation/ip˙documentation/dpu/v3˙3/pg338-dpu.pdf</span></a>

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Xiong, Y., Liu, H., Gupta, S., Akin, B., Bender, G., Wang, Y., Kindermans, P.,
Tan, M., Singh, V., Chen, B.: MobileDets: Searching for Object Detection
Architectures for Mobile Accelerators. In: IEEE CVPR (2021)

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Yokotani, T., Sasaki, Y.: Comparison with HTTP and MQTT on required network
resources for IoT. In: International Conference on Control, Electronics,
Renewable Energy and Communications (ICCEREC) (2016)

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Yu, J., Guo, K., Hu, Y., Ning, X., Qiu, J., Mao, H., Yao, S., Tang, T., Li, B.,
Wang, Y., Yang, H.: Real-time object detection towards high power efficiency.
In: Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE)
(2018)

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2304.11579" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2304.11580" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2304.11580">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2304.11580" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2304.11581" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 13:36:50 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
