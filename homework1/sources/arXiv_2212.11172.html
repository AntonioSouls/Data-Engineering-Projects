<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2212.11172] A recurrent CNN for online object detection on raw radar frames</title><meta property="og:description" content="Automotive radar sensors provide valuable information for advanced driving assistance systems (ADAS). Radars can reliably estimate the distance to an object and the relative velocity, regardless of weather and light coâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A recurrent CNN for online object detection on raw radar frames">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A recurrent CNN for online object detection on raw radar frames">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2212.11172">

<!--Generated on Fri Mar  1 09:54:50 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Computer Vision and Pattern Recognition,  Radar Object Detection,  Autonomous Driving,  Radar imaging
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\useunder</span>
<p id="p1.2" class="ltx_p"><span id="p1.2.1" class="ltx_text ltx_ulem_uline"></span><span id="p1.2.2" class="ltx_text ltx_framed ltx_framed_underline"></span>
</p>
</div>
<h1 class="ltx_title ltx_title_document">A recurrent CNN for online object detection on raw radar frames</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Colin Decourt<sup id="id12.9.id1" class="ltx_sup"><span id="id12.9.id1.1" class="ltx_text ltx_font_italic">1,2,3,4</span></sup>, Rufin VanRullen<sup id="id13.10.id2" class="ltx_sup"><span id="id13.10.id2.1" class="ltx_text ltx_font_italic">1,2</span></sup>, Didier Salle<sup id="id14.11.id3" class="ltx_sup"><span id="id14.11.id3.1" class="ltx_text ltx_font_italic">1,4</span></sup>, Thomas Oberlin <sup id="id15.12.id4" class="ltx_sup"><span id="id15.12.id4.1" class="ltx_text ltx_font_italic">1,3</span></sup>
</span><span class="ltx_author_notes"><sup id="id16.13.id1" class="ltx_sup"><span id="id16.13.id1.1" class="ltx_text ltx_font_italic">1</span></sup>Artificial and Natural Intelligence Toulouse Institute, UniversitÃ© de Toulouse, France
<sup id="id17.14.id1" class="ltx_sup"><span id="id17.14.id1.1" class="ltx_text ltx_font_italic">2</span></sup>CerCO, CNRS UMR5549, Toulouse
<sup id="id18.15.id1" class="ltx_sup"><span id="id18.15.id1.1" class="ltx_text ltx_font_italic">3</span></sup>ISAE-SUPAERO, UniversitÃ© de Toulouse, 10 Avenue Edouard Belin, Toulouse 31400, France
<sup id="id19.16.id1" class="ltx_sup"><span id="id19.16.id1.1" class="ltx_text ltx_font_italic">4</span></sup>NXP Semiconductors, Toulouse, FranceThis work has been funded by the Institute for Artificial and Natural Intelligence Toulouse (ANITI) under grant agreement ANR-19-PI3A-0004.The code and the pre-trained weights will be available soon at <a target="_blank" href="https://github.com/colindecourt/record" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/colindecourt/record</a></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id20.id1" class="ltx_p">Automotive radar sensors provide valuable information for advanced driving assistance systems (ADAS). Radars can reliably estimate the distance to an object and the relative velocity, regardless of weather and light conditions. However, radar sensors suffer from low resolution and huge intra-class variations in the shape of objects. Exploiting the time information (e.g, multiple frames) has been shown to help to capture better the dynamics of objects and, therefore, the variation in the shape of objects. Most temporal radar object detectors use 3D convolutions to learn spatial and temporal information. However, these methods are often non-causal and unsuitable for real-time applications. This work presents RECORD, a new recurrent CNN architecture for online radar object detection. We propose an end-to-end trainable architecture mixing convolutions and ConvLSTMs to learn spatio-temporal dependencies between successive frames. Our model is causal and requires only the past information encoded in the memory of the ConvLSTMs to detect objects. Our experiments show such a methodâ€™s relevance for detecting objects in different radar representations (range-Doppler, range-angle) and outperform state-of-the-art models on the ROD2021 and CARRADA datasets while being less computationally expensive.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Computer Vision and Pattern Recognition, Radar Object Detection, Autonomous Driving, Radar imaging

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Today, most advanced driving assistance systems (ADAS) use cameras and LiDAR sensors to perceive and represent the surrounding environment. Cameras provide rich visual semantic information about the environment, while LiDARs provide high-resolution point clouds of surrounding targets. Large-scale image-based and LiDAR-based datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> have enabled the development of perception algorithms for object detection and segmentation.
Despite their high resolution, camera and LiDAR sensors suffer from high sensitivity to harsh weather (fog, snow, rain) and bad light conditions (night or sunny).
On the contrary, radar operates at a millimetre wavelength, which can penetrate or diffract around tiny particles, making it a robust and crucial sensor for ADAS applications. Radar sensors also provide accurate localisation of surrounding targets and can estimate the velocity of vehicles and pedestrians in a single capture. However, the small number of public radar datasets and the lack of uniformity between them have slowed down research in deep learning for radar.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.4" class="ltx_p">As shown in figure <a href="#S2.F1" title="Figure 1 â€£ II Radar background â€£ A recurrent CNN for online object detection on raw radar frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, radar data can be represented either as target lists or as raw data spectra (or tensors). Target lists are the default representation of radar data. They are obtained after several processing steps, including signal processing (Fourier transforms), threshold algorithms (CFAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>), target tracking (Kalman filtering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>), and classification algorithms. Targets lists representation contains low-level information such as position <math id="S1.p2.1.m1.2" class="ltx_Math" alttext="(x,y)" display="inline"><semantics id="S1.p2.1.m1.2a"><mrow id="S1.p2.1.m1.2.3.2" xref="S1.p2.1.m1.2.3.1.cmml"><mo stretchy="false" id="S1.p2.1.m1.2.3.2.1" xref="S1.p2.1.m1.2.3.1.cmml">(</mo><mi id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml">x</mi><mo id="S1.p2.1.m1.2.3.2.2" xref="S1.p2.1.m1.2.3.1.cmml">,</mo><mi id="S1.p2.1.m1.2.2" xref="S1.p2.1.m1.2.2.cmml">y</mi><mo stretchy="false" id="S1.p2.1.m1.2.3.2.3" xref="S1.p2.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.2b"><interval closure="open" id="S1.p2.1.m1.2.3.1.cmml" xref="S1.p2.1.m1.2.3.2"><ci id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1">ğ‘¥</ci><ci id="S1.p2.1.m1.2.2.cmml" xref="S1.p2.1.m1.2.2">ğ‘¦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.2c">(x,y)</annotation></semantics></math>, azimuth <math id="S1.p2.2.m2.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S1.p2.2.m2.1a"><mi id="S1.p2.2.m2.1.1" xref="S1.p2.2.m2.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S1.p2.2.m2.1b"><ci id="S1.p2.2.m2.1.1.cmml" xref="S1.p2.2.m2.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.2.m2.1c">\theta</annotation></semantics></math>, relative speed <math id="S1.p2.3.m3.1" class="ltx_Math" alttext="v_{r}" display="inline"><semantics id="S1.p2.3.m3.1a"><msub id="S1.p2.3.m3.1.1" xref="S1.p2.3.m3.1.1.cmml"><mi id="S1.p2.3.m3.1.1.2" xref="S1.p2.3.m3.1.1.2.cmml">v</mi><mi id="S1.p2.3.m3.1.1.3" xref="S1.p2.3.m3.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S1.p2.3.m3.1b"><apply id="S1.p2.3.m3.1.1.cmml" xref="S1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S1.p2.3.m3.1.1.1.cmml" xref="S1.p2.3.m3.1.1">subscript</csymbol><ci id="S1.p2.3.m3.1.1.2.cmml" xref="S1.p2.3.m3.1.1.2">ğ‘£</ci><ci id="S1.p2.3.m3.1.1.3.cmml" xref="S1.p2.3.m3.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.3.m3.1c">v_{r}</annotation></semantics></math> and radar cross section <math id="S1.p2.4.m4.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S1.p2.4.m4.1a"><mi id="S1.p2.4.m4.1.1" xref="S1.p2.4.m4.1.1.cmml">Ïƒ</mi><annotation-xml encoding="MathML-Content" id="S1.p2.4.m4.1b"><ci id="S1.p2.4.m4.1.1.cmml" xref="S1.p2.4.m4.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.4.m4.1c">\sigma</annotation></semantics></math> of targets. Classification or segmentation algorithms can be applied to these data, such as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, to provide semantic information about the target.
However, the target lists have suffered several pre-processing steps. They do not contain all the initial information, which might lower the performance of classification or segmentation (ghost targets, sparse point clouds). Instead of radar target lists, it is possible to consider raw radar data tensors (range-doppler (RD), range-azimuth (RA) or range-azimuth-doppler (RAD) spectra) to exploit all the information available in the radar signal. In the last two years, several raw data datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> have been released to perform classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> or segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> on raw data tensors. However, most detection or segmentation methods are static. In other words, they use only a single image as input without exploiting the correlations among successive frames.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">For automotive applications, time is key information which can be exploited to learn temporal patterns between successive frames in videos for example. In radar, given the modulation and the characteristic of the signal of FMCW (Frequency Modulated Continuous Wave) radars, the data includes temporal information (<span id="S1.p3.1.1" class="ltx_text ltx_font_italic">e.g.</span> Doppler effect), which is a crucial value in autonomous driving. The use of time in radar makes it possible to learn the dynamics of the objects held in the radar signal, handle the variation in the shape of the object over time, and reduce the noise between successive frames (induced by the movement of the surrounding object and the vehicle itself). Recent efforts have been made to exploit temporal relationships between raw data radar frames using multiple frames for detection or segmentation tasks. Mainly, Ouaknine <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> use 3D convolutions for multi-view radar semantic segmentation. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> also take advantage of 3D convolutions for object detection on RA maps. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, Major <span id="S1.p3.1.3" class="ltx_text ltx_font_italic">et al.</span> use ConvLSTM to detect cars in RA view and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> processes sequences of two successive radar frames to learn the temporal relationship between objects.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">This paper presents a new convolutional and recurrent neural network (CRNN) for radar spectra. Unlike most multi-frame radar object detectors, our model is causal, which means we only use past frames to detect objects. This characteristic is crucial for ADAS applications because such systems do not have access to future frames. To learn spatial and temporal dependencies, we introduce a model consisting of 2D convolutions and convolutional recurrent neural networks (ConvRNNs). Additionally, we use efficient convolutions and ConvRNNs (inverted residual blocks and Bottleneck LSTMs) to reduce the computational cost of our approach.
Our model is end-to-end trainable and does not require pretraining or multiple training steps. We present a generic method that can process either RA, RD or RAD spectra and outperforms state-of-the-art architectures on different tasks. To our knowledge, this is the first fully convolutional recurrent network for radar spectra.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Section <a href="#S2" title="II Radar background â€£ A recurrent CNN for online object detection on raw radar frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> introduces fundamental radar signal processing. Then in section <a href="#S3" title="III Spatio-temporal radar object detector â€£ A recurrent CNN for online object detection on raw radar frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, we describe our approach and the proposed model. <a href="#S4" title="IV Related work â€£ A recurrent CNN for online object detection on raw radar frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> presents the prior art to this work. We present the results of our experiments on the ROD2021 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and the CARRADA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> dataset in section <a href="#S5" title="V Experiments â€£ A recurrent CNN for online object detection on raw radar frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>. Finally, we discuss our results and conclude the paper in section <a href="#S6" title="VI Conclusion â€£ A recurrent CNN for online object detection on raw radar frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Radar background</span>
</h2>

<figure id="S2.F1" class="ltx_figure"><img src="/html/2212.11172/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="310" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>FMCW radar overview</figcaption>
</figure>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2212.11172/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="93" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Model architecture (RECORD). Rounded arrows on <span id="S2.F2.2.1" class="ltx_text ltx_font_italic">Bottleneck LSTMs</span> stand for a recurrent layer. Plus sign stands for the concatenation operation. We report the output size (left) and the number of output channels (right) for each layer.</figcaption>
</figure>
<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.4" class="ltx_p">Radar is an active sensor that transmits radio frequency (RF) electromagnetic (EM) waves and uses the reflected waves from objects to estimate the distance, velocity, and angle of arrival of these targets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. Automotive radars emit a particular waveform called FMCW. An FMCW radar periodically transmits <math id="S2.p1.1.m1.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S2.p1.1.m1.1a"><mi id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><ci id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">ğ‘ƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">P</annotation></semantics></math> chirps (a frequency-modulated pulse whose frequency increases linearly with time) over <math id="S2.p1.2.m2.1" class="ltx_Math" alttext="M_{Rx}" display="inline"><semantics id="S2.p1.2.m2.1a"><msub id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml"><mi id="S2.p1.2.m2.1.1.2" xref="S2.p1.2.m2.1.1.2.cmml">M</mi><mrow id="S2.p1.2.m2.1.1.3" xref="S2.p1.2.m2.1.1.3.cmml"><mi id="S2.p1.2.m2.1.1.3.2" xref="S2.p1.2.m2.1.1.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S2.p1.2.m2.1.1.3.1" xref="S2.p1.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S2.p1.2.m2.1.1.3.3" xref="S2.p1.2.m2.1.1.3.3.cmml">x</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><apply id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p1.2.m2.1.1.1.cmml" xref="S2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.p1.2.m2.1.1.2.cmml" xref="S2.p1.2.m2.1.1.2">ğ‘€</ci><apply id="S2.p1.2.m2.1.1.3.cmml" xref="S2.p1.2.m2.1.1.3"><times id="S2.p1.2.m2.1.1.3.1.cmml" xref="S2.p1.2.m2.1.1.3.1"></times><ci id="S2.p1.2.m2.1.1.3.2.cmml" xref="S2.p1.2.m2.1.1.3.2">ğ‘…</ci><ci id="S2.p1.2.m2.1.1.3.3.cmml" xref="S2.p1.2.m2.1.1.3.3">ğ‘¥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">M_{Rx}</annotation></semantics></math> receiving antennas and <math id="S2.p1.3.m3.1" class="ltx_Math" alttext="M_{Tx}" display="inline"><semantics id="S2.p1.3.m3.1a"><msub id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml"><mi id="S2.p1.3.m3.1.1.2" xref="S2.p1.3.m3.1.1.2.cmml">M</mi><mrow id="S2.p1.3.m3.1.1.3" xref="S2.p1.3.m3.1.1.3.cmml"><mi id="S2.p1.3.m3.1.1.3.2" xref="S2.p1.3.m3.1.1.3.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S2.p1.3.m3.1.1.3.1" xref="S2.p1.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S2.p1.3.m3.1.1.3.3" xref="S2.p1.3.m3.1.1.3.3.cmml">x</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><apply id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.p1.3.m3.1.1.1.cmml" xref="S2.p1.3.m3.1.1">subscript</csymbol><ci id="S2.p1.3.m3.1.1.2.cmml" xref="S2.p1.3.m3.1.1.2">ğ‘€</ci><apply id="S2.p1.3.m3.1.1.3.cmml" xref="S2.p1.3.m3.1.1.3"><times id="S2.p1.3.m3.1.1.3.1.cmml" xref="S2.p1.3.m3.1.1.3.1"></times><ci id="S2.p1.3.m3.1.1.3.2.cmml" xref="S2.p1.3.m3.1.1.3.2">ğ‘‡</ci><ci id="S2.p1.3.m3.1.1.3.3.cmml" xref="S2.p1.3.m3.1.1.3.3">ğ‘¥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">M_{Tx}</annotation></semantics></math> transmitting antennas to estimate the range, the velocity and the Direction-of-Arrival (DOA) of the targets. For the <math id="S2.p1.4.m4.1" class="ltx_Math" alttext="m^{th}" display="inline"><semantics id="S2.p1.4.m4.1a"><msup id="S2.p1.4.m4.1.1" xref="S2.p1.4.m4.1.1.cmml"><mi id="S2.p1.4.m4.1.1.2" xref="S2.p1.4.m4.1.1.2.cmml">m</mi><mrow id="S2.p1.4.m4.1.1.3" xref="S2.p1.4.m4.1.1.3.cmml"><mi id="S2.p1.4.m4.1.1.3.2" xref="S2.p1.4.m4.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.p1.4.m4.1.1.3.1" xref="S2.p1.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S2.p1.4.m4.1.1.3.3" xref="S2.p1.4.m4.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.1b"><apply id="S2.p1.4.m4.1.1.cmml" xref="S2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.p1.4.m4.1.1.1.cmml" xref="S2.p1.4.m4.1.1">superscript</csymbol><ci id="S2.p1.4.m4.1.1.2.cmml" xref="S2.p1.4.m4.1.1.2">ğ‘š</ci><apply id="S2.p1.4.m4.1.1.3.cmml" xref="S2.p1.4.m4.1.1.3"><times id="S2.p1.4.m4.1.1.3.1.cmml" xref="S2.p1.4.m4.1.1.3.1"></times><ci id="S2.p1.4.m4.1.1.3.2.cmml" xref="S2.p1.4.m4.1.1.3.2">ğ‘¡</ci><ci id="S2.p1.4.m4.1.1.3.3.cmml" xref="S2.p1.4.m4.1.1.3.3">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.1c">m^{th}</annotation></semantics></math> antenna, we express a single FMCW pulse as:</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.3" class="ltx_Math" alttext="s_{n}(t)=e^{j2\pi(f_{c}+0.5Kt)t}\&gt;\&gt;\&gt;0\leq t\leq T," display="block"><semantics id="S2.E1.m1.3a"><mrow id="S2.E1.m1.3.3.1" xref="S2.E1.m1.3.3.1.1.cmml"><mrow id="S2.E1.m1.3.3.1.1" xref="S2.E1.m1.3.3.1.1.cmml"><mrow id="S2.E1.m1.3.3.1.1.2" xref="S2.E1.m1.3.3.1.1.2.cmml"><msub id="S2.E1.m1.3.3.1.1.2.2" xref="S2.E1.m1.3.3.1.1.2.2.cmml"><mi id="S2.E1.m1.3.3.1.1.2.2.2" xref="S2.E1.m1.3.3.1.1.2.2.2.cmml">s</mi><mi id="S2.E1.m1.3.3.1.1.2.2.3" xref="S2.E1.m1.3.3.1.1.2.2.3.cmml">n</mi></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.1.1.2.1" xref="S2.E1.m1.3.3.1.1.2.1.cmml">â€‹</mo><mrow id="S2.E1.m1.3.3.1.1.2.3.2" xref="S2.E1.m1.3.3.1.1.2.cmml"><mo stretchy="false" id="S2.E1.m1.3.3.1.1.2.3.2.1" xref="S2.E1.m1.3.3.1.1.2.cmml">(</mo><mi id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">t</mi><mo stretchy="false" id="S2.E1.m1.3.3.1.1.2.3.2.2" xref="S2.E1.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.3.3.1.1.3" xref="S2.E1.m1.3.3.1.1.3.cmml">=</mo><mrow id="S2.E1.m1.3.3.1.1.4" xref="S2.E1.m1.3.3.1.1.4.cmml"><msup id="S2.E1.m1.3.3.1.1.4.2" xref="S2.E1.m1.3.3.1.1.4.2.cmml"><mi id="S2.E1.m1.3.3.1.1.4.2.2" xref="S2.E1.m1.3.3.1.1.4.2.2.cmml">e</mi><mrow id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.3" xref="S2.E1.m1.1.1.1.3.cmml">j</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.2" xref="S2.E1.m1.1.1.1.2.cmml">â€‹</mo><mn id="S2.E1.m1.1.1.1.4" xref="S2.E1.m1.1.1.1.4.cmml">2</mn><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.2a" xref="S2.E1.m1.1.1.1.2.cmml">â€‹</mo><mi id="S2.E1.m1.1.1.1.5" xref="S2.E1.m1.1.1.1.5.cmml">Ï€</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.2b" xref="S2.E1.m1.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E1.m1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml"><msub id="S2.E1.m1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.2.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.2.2" xref="S2.E1.m1.1.1.1.1.1.1.2.2.cmml">f</mi><mi id="S2.E1.m1.1.1.1.1.1.1.2.3" xref="S2.E1.m1.1.1.1.1.1.1.2.3.cmml">c</mi></msub><mo id="S2.E1.m1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S2.E1.m1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.3.cmml"><mn id="S2.E1.m1.1.1.1.1.1.1.3.2" xref="S2.E1.m1.1.1.1.1.1.1.3.2.cmml">0.5</mn><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.1.1.3.1" xref="S2.E1.m1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S2.E1.m1.1.1.1.1.1.1.3.3" xref="S2.E1.m1.1.1.1.1.1.1.3.3.cmml">K</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.1.1.3.1a" xref="S2.E1.m1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S2.E1.m1.1.1.1.1.1.1.3.4" xref="S2.E1.m1.1.1.1.1.1.1.3.4.cmml">t</mi></mrow></mrow><mo stretchy="false" id="S2.E1.m1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.2c" xref="S2.E1.m1.1.1.1.2.cmml">â€‹</mo><mi id="S2.E1.m1.1.1.1.6" xref="S2.E1.m1.1.1.1.6.cmml">t</mi></mrow></msup><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.1.1.4.1" xref="S2.E1.m1.3.3.1.1.4.1.cmml">â€‹</mo><mn id="S2.E1.m1.3.3.1.1.4.3" xref="S2.E1.m1.3.3.1.1.4.3.cmml">â€…â€…â€…0</mn></mrow><mo id="S2.E1.m1.3.3.1.1.5" xref="S2.E1.m1.3.3.1.1.5.cmml">â‰¤</mo><mi id="S2.E1.m1.3.3.1.1.6" xref="S2.E1.m1.3.3.1.1.6.cmml">t</mi><mo id="S2.E1.m1.3.3.1.1.7" xref="S2.E1.m1.3.3.1.1.7.cmml">â‰¤</mo><mi id="S2.E1.m1.3.3.1.1.8" xref="S2.E1.m1.3.3.1.1.8.cmml">T</mi></mrow><mo id="S2.E1.m1.3.3.1.2" xref="S2.E1.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.3b"><apply id="S2.E1.m1.3.3.1.1.cmml" xref="S2.E1.m1.3.3.1"><and id="S2.E1.m1.3.3.1.1a.cmml" xref="S2.E1.m1.3.3.1"></and><apply id="S2.E1.m1.3.3.1.1b.cmml" xref="S2.E1.m1.3.3.1"><eq id="S2.E1.m1.3.3.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.3"></eq><apply id="S2.E1.m1.3.3.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.2"><times id="S2.E1.m1.3.3.1.1.2.1.cmml" xref="S2.E1.m1.3.3.1.1.2.1"></times><apply id="S2.E1.m1.3.3.1.1.2.2.cmml" xref="S2.E1.m1.3.3.1.1.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.2.2.1.cmml" xref="S2.E1.m1.3.3.1.1.2.2">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.2.2.2.cmml" xref="S2.E1.m1.3.3.1.1.2.2.2">ğ‘ </ci><ci id="S2.E1.m1.3.3.1.1.2.2.3.cmml" xref="S2.E1.m1.3.3.1.1.2.2.3">ğ‘›</ci></apply><ci id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">ğ‘¡</ci></apply><apply id="S2.E1.m1.3.3.1.1.4.cmml" xref="S2.E1.m1.3.3.1.1.4"><times id="S2.E1.m1.3.3.1.1.4.1.cmml" xref="S2.E1.m1.3.3.1.1.4.1"></times><apply id="S2.E1.m1.3.3.1.1.4.2.cmml" xref="S2.E1.m1.3.3.1.1.4.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.4.2.1.cmml" xref="S2.E1.m1.3.3.1.1.4.2">superscript</csymbol><ci id="S2.E1.m1.3.3.1.1.4.2.2.cmml" xref="S2.E1.m1.3.3.1.1.4.2.2">ğ‘’</ci><apply id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"><times id="S2.E1.m1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.2"></times><ci id="S2.E1.m1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.3">ğ‘—</ci><cn type="integer" id="S2.E1.m1.1.1.1.4.cmml" xref="S2.E1.m1.1.1.1.4">2</cn><ci id="S2.E1.m1.1.1.1.5.cmml" xref="S2.E1.m1.1.1.1.5">ğœ‹</ci><apply id="S2.E1.m1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1"><plus id="S2.E1.m1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1"></plus><apply id="S2.E1.m1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.2.2">ğ‘“</ci><ci id="S2.E1.m1.1.1.1.1.1.1.2.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.2.3">ğ‘</ci></apply><apply id="S2.E1.m1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3"><times id="S2.E1.m1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3.1"></times><cn type="float" id="S2.E1.m1.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3.2">0.5</cn><ci id="S2.E1.m1.1.1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3.3">ğ¾</ci><ci id="S2.E1.m1.1.1.1.1.1.1.3.4.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3.4">ğ‘¡</ci></apply></apply><ci id="S2.E1.m1.1.1.1.6.cmml" xref="S2.E1.m1.1.1.1.6">ğ‘¡</ci></apply></apply><cn type="float" id="S2.E1.m1.3.3.1.1.4.3.cmml" xref="S2.E1.m1.3.3.1.1.4.3">â€…â€…â€…0</cn></apply></apply><apply id="S2.E1.m1.3.3.1.1c.cmml" xref="S2.E1.m1.3.3.1"><leq id="S2.E1.m1.3.3.1.1.5.cmml" xref="S2.E1.m1.3.3.1.1.5"></leq><share href="#S2.E1.m1.3.3.1.1.4.cmml" id="S2.E1.m1.3.3.1.1d.cmml" xref="S2.E1.m1.3.3.1"></share><ci id="S2.E1.m1.3.3.1.1.6.cmml" xref="S2.E1.m1.3.3.1.1.6">ğ‘¡</ci></apply><apply id="S2.E1.m1.3.3.1.1e.cmml" xref="S2.E1.m1.3.3.1"><leq id="S2.E1.m1.3.3.1.1.7.cmml" xref="S2.E1.m1.3.3.1.1.7"></leq><share href="#S2.E1.m1.3.3.1.1.6.cmml" id="S2.E1.m1.3.3.1.1f.cmml" xref="S2.E1.m1.3.3.1"></share><ci id="S2.E1.m1.3.3.1.1.8.cmml" xref="S2.E1.m1.3.3.1.1.8">ğ‘‡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.3c">s_{n}(t)=e^{j2\pi(f_{c}+0.5Kt)t}\&gt;\&gt;\&gt;0\leq t\leq T,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.p1.7" class="ltx_p">where <math id="S2.p1.5.m1.1" class="ltx_Math" alttext="f_{c}" display="inline"><semantics id="S2.p1.5.m1.1a"><msub id="S2.p1.5.m1.1.1" xref="S2.p1.5.m1.1.1.cmml"><mi id="S2.p1.5.m1.1.1.2" xref="S2.p1.5.m1.1.1.2.cmml">f</mi><mi id="S2.p1.5.m1.1.1.3" xref="S2.p1.5.m1.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.5.m1.1b"><apply id="S2.p1.5.m1.1.1.cmml" xref="S2.p1.5.m1.1.1"><csymbol cd="ambiguous" id="S2.p1.5.m1.1.1.1.cmml" xref="S2.p1.5.m1.1.1">subscript</csymbol><ci id="S2.p1.5.m1.1.1.2.cmml" xref="S2.p1.5.m1.1.1.2">ğ‘“</ci><ci id="S2.p1.5.m1.1.1.3.cmml" xref="S2.p1.5.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.5.m1.1c">f_{c}</annotation></semantics></math> is the carrier frequency, <math id="S2.p1.6.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S2.p1.6.m2.1a"><mi id="S2.p1.6.m2.1.1" xref="S2.p1.6.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.p1.6.m2.1b"><ci id="S2.p1.6.m2.1.1.cmml" xref="S2.p1.6.m2.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.6.m2.1c">K</annotation></semantics></math> is a modulation constant, and <math id="S2.p1.7.m3.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S2.p1.7.m3.1a"><mi id="S2.p1.7.m3.1.1" xref="S2.p1.7.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.p1.7.m3.1b"><ci id="S2.p1.7.m3.1.1.cmml" xref="S2.p1.7.m3.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.7.m3.1c">T</annotation></semantics></math> is the duration of the chirp (fast time).</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">As shown in figure <a href="#S2.F1" title="Figure 1 â€£ II Radar background â€£ A recurrent CNN for online object detection on raw radar frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, it is possible to estimate the distance to the target from the ADC data by applying a first discrete Fourier transform along the fast time index (<span id="S2.p2.1.1" class="ltx_text ltx_font_italic">i.e.</span> for every chirp). We obtain the velocity by computing a second discrete Fourier transform over the slow time index (<span id="S2.p2.1.2" class="ltx_text ltx_font_italic">i.e.</span> for each chirp index). This second Fourier transform allows measuring the frequency shift between received chirps resulting in the Doppler frequency and hence the velocity of targets. These two successive Fourier transforms result in a range-Doppler (or range-velocity) spectrum.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.4" class="ltx_p">From the RD spectrum, it is possible to obtain a list of targets which contains the position <math id="S2.p3.1.m1.2" class="ltx_Math" alttext="x,y" display="inline"><semantics id="S2.p3.1.m1.2a"><mrow id="S2.p3.1.m1.2.3.2" xref="S2.p3.1.m1.2.3.1.cmml"><mi id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml">x</mi><mo id="S2.p3.1.m1.2.3.2.1" xref="S2.p3.1.m1.2.3.1.cmml">,</mo><mi id="S2.p3.1.m1.2.2" xref="S2.p3.1.m1.2.2.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.2b"><list id="S2.p3.1.m1.2.3.1.cmml" xref="S2.p3.1.m1.2.3.2"><ci id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1">ğ‘¥</ci><ci id="S2.p3.1.m1.2.2.cmml" xref="S2.p3.1.m1.2.2">ğ‘¦</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.2c">x,y</annotation></semantics></math>, the radial velocity <math id="S2.p3.2.m2.1" class="ltx_Math" alttext="v_{r}" display="inline"><semantics id="S2.p3.2.m2.1a"><msub id="S2.p3.2.m2.1.1" xref="S2.p3.2.m2.1.1.cmml"><mi id="S2.p3.2.m2.1.1.2" xref="S2.p3.2.m2.1.1.2.cmml">v</mi><mi id="S2.p3.2.m2.1.1.3" xref="S2.p3.2.m2.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p3.2.m2.1b"><apply id="S2.p3.2.m2.1.1.cmml" xref="S2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p3.2.m2.1.1.1.cmml" xref="S2.p3.2.m2.1.1">subscript</csymbol><ci id="S2.p3.2.m2.1.1.2.cmml" xref="S2.p3.2.m2.1.1.2">ğ‘£</ci><ci id="S2.p3.2.m2.1.1.3.cmml" xref="S2.p3.2.m2.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.2.m2.1c">v_{r}</annotation></semantics></math>, the DOA <math id="S2.p3.3.m3.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.p3.3.m3.1a"><mi id="S2.p3.3.m3.1.1" xref="S2.p3.3.m3.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S2.p3.3.m3.1b"><ci id="S2.p3.3.m3.1.1.cmml" xref="S2.p3.3.m3.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.3.m3.1c">\theta</annotation></semantics></math> and the radar cross section <math id="S2.p3.4.m4.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S2.p3.4.m4.1a"><mi id="S2.p3.4.m4.1.1" xref="S2.p3.4.m4.1.1.cmml">Ïƒ</mi><annotation-xml encoding="MathML-Content" id="S2.p3.4.m4.1b"><ci id="S2.p3.4.m4.1.1.cmml" xref="S2.p3.4.m4.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.4.m4.1c">\sigma</annotation></semantics></math>. This is done by applying thresholding algorithms like CFAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, followed by the DOA estimation and post-processing steps (ego-motion compensation, Kalman filtering, classification) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. However, these operations reduce the amount of information in the signal.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">In multiple receiving antenna scenarios, each antenna sees the reflected signal with a slight time delay. Computing a third discrete Fourier transform along the antenna array allows estimating targetsâ€™ DOA and results in a range-azimuth-Doppler spectrum. Summing values along the Doppler dimension enable the computation of the range-angle spectrum. Compared to target lists, these representations contain much more information about the environment.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Spatio-temporal radar object detector</span>
</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">We aim to design a model to learn the implicit relationship between frames at different spatial and temporal levels recurrently. This section describes the architecture of our single-view spatio-temporal encoder and decoder. We also introduce a multi-view version of our model designed to learn spatial and temporal features in different views (<span id="S3.p1.1.1" class="ltx_text ltx_font_italic">e.g.</span> RD, AD, and RA) simultaneously.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Problem formulation</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.10" class="ltx_p">Let us consider a sequence <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">ğ‘…</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">R</annotation></semantics></math> of <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">N</annotation></semantics></math> radar frames ranging from time <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="k-N+1" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mrow id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2.2" xref="S3.SS1.p1.3.m3.1.1.2.2.cmml">k</mi><mo id="S3.SS1.p1.3.m3.1.1.2.1" xref="S3.SS1.p1.3.m3.1.1.2.1.cmml">âˆ’</mo><mi id="S3.SS1.p1.3.m3.1.1.2.3" xref="S3.SS1.p1.3.m3.1.1.2.3.cmml">N</mi></mrow><mo id="S3.SS1.p1.3.m3.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.cmml">+</mo><mn id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><plus id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1"></plus><apply id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2"><minus id="S3.SS1.p1.3.m3.1.1.2.1.cmml" xref="S3.SS1.p1.3.m3.1.1.2.1"></minus><ci id="S3.SS1.p1.3.m3.1.1.2.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2.2">ğ‘˜</ci><ci id="S3.SS1.p1.3.m3.1.1.2.3.cmml" xref="S3.SS1.p1.3.m3.1.1.2.3">ğ‘</ci></apply><cn type="integer" id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">k-N+1</annotation></semantics></math> to <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">k</annotation></semantics></math> such as: <math id="S3.SS1.p1.5.m5.3" class="ltx_Math" alttext="R=\{r_{k-N+1},\ldots,r_{k}\}" display="inline"><semantics id="S3.SS1.p1.5.m5.3a"><mrow id="S3.SS1.p1.5.m5.3.3" xref="S3.SS1.p1.5.m5.3.3.cmml"><mi id="S3.SS1.p1.5.m5.3.3.4" xref="S3.SS1.p1.5.m5.3.3.4.cmml">R</mi><mo id="S3.SS1.p1.5.m5.3.3.3" xref="S3.SS1.p1.5.m5.3.3.3.cmml">=</mo><mrow id="S3.SS1.p1.5.m5.3.3.2.2" xref="S3.SS1.p1.5.m5.3.3.2.3.cmml"><mo stretchy="false" id="S3.SS1.p1.5.m5.3.3.2.2.3" xref="S3.SS1.p1.5.m5.3.3.2.3.cmml">{</mo><msub id="S3.SS1.p1.5.m5.2.2.1.1.1" xref="S3.SS1.p1.5.m5.2.2.1.1.1.cmml"><mi id="S3.SS1.p1.5.m5.2.2.1.1.1.2" xref="S3.SS1.p1.5.m5.2.2.1.1.1.2.cmml">r</mi><mrow id="S3.SS1.p1.5.m5.2.2.1.1.1.3" xref="S3.SS1.p1.5.m5.2.2.1.1.1.3.cmml"><mrow id="S3.SS1.p1.5.m5.2.2.1.1.1.3.2" xref="S3.SS1.p1.5.m5.2.2.1.1.1.3.2.cmml"><mi id="S3.SS1.p1.5.m5.2.2.1.1.1.3.2.2" xref="S3.SS1.p1.5.m5.2.2.1.1.1.3.2.2.cmml">k</mi><mo id="S3.SS1.p1.5.m5.2.2.1.1.1.3.2.1" xref="S3.SS1.p1.5.m5.2.2.1.1.1.3.2.1.cmml">âˆ’</mo><mi id="S3.SS1.p1.5.m5.2.2.1.1.1.3.2.3" xref="S3.SS1.p1.5.m5.2.2.1.1.1.3.2.3.cmml">N</mi></mrow><mo id="S3.SS1.p1.5.m5.2.2.1.1.1.3.1" xref="S3.SS1.p1.5.m5.2.2.1.1.1.3.1.cmml">+</mo><mn id="S3.SS1.p1.5.m5.2.2.1.1.1.3.3" xref="S3.SS1.p1.5.m5.2.2.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S3.SS1.p1.5.m5.3.3.2.2.4" xref="S3.SS1.p1.5.m5.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">â€¦</mi><mo id="S3.SS1.p1.5.m5.3.3.2.2.5" xref="S3.SS1.p1.5.m5.3.3.2.3.cmml">,</mo><msub id="S3.SS1.p1.5.m5.3.3.2.2.2" xref="S3.SS1.p1.5.m5.3.3.2.2.2.cmml"><mi id="S3.SS1.p1.5.m5.3.3.2.2.2.2" xref="S3.SS1.p1.5.m5.3.3.2.2.2.2.cmml">r</mi><mi id="S3.SS1.p1.5.m5.3.3.2.2.2.3" xref="S3.SS1.p1.5.m5.3.3.2.2.2.3.cmml">k</mi></msub><mo stretchy="false" id="S3.SS1.p1.5.m5.3.3.2.2.6" xref="S3.SS1.p1.5.m5.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.3b"><apply id="S3.SS1.p1.5.m5.3.3.cmml" xref="S3.SS1.p1.5.m5.3.3"><eq id="S3.SS1.p1.5.m5.3.3.3.cmml" xref="S3.SS1.p1.5.m5.3.3.3"></eq><ci id="S3.SS1.p1.5.m5.3.3.4.cmml" xref="S3.SS1.p1.5.m5.3.3.4">ğ‘…</ci><set id="S3.SS1.p1.5.m5.3.3.2.3.cmml" xref="S3.SS1.p1.5.m5.3.3.2.2"><apply id="S3.SS1.p1.5.m5.2.2.1.1.1.cmml" xref="S3.SS1.p1.5.m5.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.2.2.1.1.1.1.cmml" xref="S3.SS1.p1.5.m5.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m5.2.2.1.1.1.2.cmml" xref="S3.SS1.p1.5.m5.2.2.1.1.1.2">ğ‘Ÿ</ci><apply id="S3.SS1.p1.5.m5.2.2.1.1.1.3.cmml" xref="S3.SS1.p1.5.m5.2.2.1.1.1.3"><plus id="S3.SS1.p1.5.m5.2.2.1.1.1.3.1.cmml" xref="S3.SS1.p1.5.m5.2.2.1.1.1.3.1"></plus><apply id="S3.SS1.p1.5.m5.2.2.1.1.1.3.2.cmml" xref="S3.SS1.p1.5.m5.2.2.1.1.1.3.2"><minus id="S3.SS1.p1.5.m5.2.2.1.1.1.3.2.1.cmml" xref="S3.SS1.p1.5.m5.2.2.1.1.1.3.2.1"></minus><ci id="S3.SS1.p1.5.m5.2.2.1.1.1.3.2.2.cmml" xref="S3.SS1.p1.5.m5.2.2.1.1.1.3.2.2">ğ‘˜</ci><ci id="S3.SS1.p1.5.m5.2.2.1.1.1.3.2.3.cmml" xref="S3.SS1.p1.5.m5.2.2.1.1.1.3.2.3">ğ‘</ci></apply><cn type="integer" id="S3.SS1.p1.5.m5.2.2.1.1.1.3.3.cmml" xref="S3.SS1.p1.5.m5.2.2.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">â€¦</ci><apply id="S3.SS1.p1.5.m5.3.3.2.2.2.cmml" xref="S3.SS1.p1.5.m5.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.3.3.2.2.2.1.cmml" xref="S3.SS1.p1.5.m5.3.3.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.5.m5.3.3.2.2.2.2.cmml" xref="S3.SS1.p1.5.m5.3.3.2.2.2.2">ğ‘Ÿ</ci><ci id="S3.SS1.p1.5.m5.3.3.2.2.2.3.cmml" xref="S3.SS1.p1.5.m5.3.3.2.2.2.3">ğ‘˜</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.3c">R=\{r_{k-N+1},\ldots,r_{k}\}</annotation></semantics></math>. We aim to find the locations of every object in the scene at time <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><mi id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">k</annotation></semantics></math>, <math id="S3.SS1.p1.7.m7.1" class="ltx_Math" alttext="P_{k}" display="inline"><semantics id="S3.SS1.p1.7.m7.1a"><msub id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml"><mi id="S3.SS1.p1.7.m7.1.1.2" xref="S3.SS1.p1.7.m7.1.1.2.cmml">P</mi><mi id="S3.SS1.p1.7.m7.1.1.3" xref="S3.SS1.p1.7.m7.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><apply id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p1.7.m7.1.1.2.cmml" xref="S3.SS1.p1.7.m7.1.1.2">ğ‘ƒ</ci><ci id="S3.SS1.p1.7.m7.1.1.3.cmml" xref="S3.SS1.p1.7.m7.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">P_{k}</annotation></semantics></math>, based on the <math id="S3.SS1.p1.8.m8.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p1.8.m8.1a"><mi id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><ci id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">N</annotation></semantics></math> past frames. We define our model with two functions, the encoder <math id="S3.SS1.p1.9.m9.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S3.SS1.p1.9.m9.1a"><mi id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.1b"><ci id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1">ğ¸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.1c">E</annotation></semantics></math> and the decoder <math id="S3.SS1.p1.10.m10.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS1.p1.10.m10.1a"><mi id="S3.SS1.p1.10.m10.1.1" xref="S3.SS1.p1.10.m10.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m10.1b"><ci id="S3.SS1.p1.10.m10.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m10.1c">D</annotation></semantics></math>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.7" class="ltx_p">We consider a causal model, which means it uses only the past to predict the next time step. For each time step <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">k</annotation></semantics></math> of the sequence, we consider a recurrent convolutional encoder taking as input the frame at time <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">k</annotation></semantics></math> and a set of <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">ğ¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">I</annotation></semantics></math> previous hidden states <math id="S3.SS1.p2.4.m4.5" class="ltx_Math" alttext="H_{k-1}=\{h^{0}_{k-1},\ldots,h^{i}_{k-1},\ldots,h^{I-1}_{k-1}\}" display="inline"><semantics id="S3.SS1.p2.4.m4.5a"><mrow id="S3.SS1.p2.4.m4.5.5" xref="S3.SS1.p2.4.m4.5.5.cmml"><msub id="S3.SS1.p2.4.m4.5.5.5" xref="S3.SS1.p2.4.m4.5.5.5.cmml"><mi id="S3.SS1.p2.4.m4.5.5.5.2" xref="S3.SS1.p2.4.m4.5.5.5.2.cmml">H</mi><mrow id="S3.SS1.p2.4.m4.5.5.5.3" xref="S3.SS1.p2.4.m4.5.5.5.3.cmml"><mi id="S3.SS1.p2.4.m4.5.5.5.3.2" xref="S3.SS1.p2.4.m4.5.5.5.3.2.cmml">k</mi><mo id="S3.SS1.p2.4.m4.5.5.5.3.1" xref="S3.SS1.p2.4.m4.5.5.5.3.1.cmml">âˆ’</mo><mn id="S3.SS1.p2.4.m4.5.5.5.3.3" xref="S3.SS1.p2.4.m4.5.5.5.3.3.cmml">1</mn></mrow></msub><mo id="S3.SS1.p2.4.m4.5.5.4" xref="S3.SS1.p2.4.m4.5.5.4.cmml">=</mo><mrow id="S3.SS1.p2.4.m4.5.5.3.3" xref="S3.SS1.p2.4.m4.5.5.3.4.cmml"><mo stretchy="false" id="S3.SS1.p2.4.m4.5.5.3.3.4" xref="S3.SS1.p2.4.m4.5.5.3.4.cmml">{</mo><msubsup id="S3.SS1.p2.4.m4.3.3.1.1.1" xref="S3.SS1.p2.4.m4.3.3.1.1.1.cmml"><mi id="S3.SS1.p2.4.m4.3.3.1.1.1.2.2" xref="S3.SS1.p2.4.m4.3.3.1.1.1.2.2.cmml">h</mi><mrow id="S3.SS1.p2.4.m4.3.3.1.1.1.3" xref="S3.SS1.p2.4.m4.3.3.1.1.1.3.cmml"><mi id="S3.SS1.p2.4.m4.3.3.1.1.1.3.2" xref="S3.SS1.p2.4.m4.3.3.1.1.1.3.2.cmml">k</mi><mo id="S3.SS1.p2.4.m4.3.3.1.1.1.3.1" xref="S3.SS1.p2.4.m4.3.3.1.1.1.3.1.cmml">âˆ’</mo><mn id="S3.SS1.p2.4.m4.3.3.1.1.1.3.3" xref="S3.SS1.p2.4.m4.3.3.1.1.1.3.3.cmml">1</mn></mrow><mn id="S3.SS1.p2.4.m4.3.3.1.1.1.2.3" xref="S3.SS1.p2.4.m4.3.3.1.1.1.2.3.cmml">0</mn></msubsup><mo id="S3.SS1.p2.4.m4.5.5.3.3.5" xref="S3.SS1.p2.4.m4.5.5.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">â€¦</mi><mo id="S3.SS1.p2.4.m4.5.5.3.3.6" xref="S3.SS1.p2.4.m4.5.5.3.4.cmml">,</mo><msubsup id="S3.SS1.p2.4.m4.4.4.2.2.2" xref="S3.SS1.p2.4.m4.4.4.2.2.2.cmml"><mi id="S3.SS1.p2.4.m4.4.4.2.2.2.2.2" xref="S3.SS1.p2.4.m4.4.4.2.2.2.2.2.cmml">h</mi><mrow id="S3.SS1.p2.4.m4.4.4.2.2.2.3" xref="S3.SS1.p2.4.m4.4.4.2.2.2.3.cmml"><mi id="S3.SS1.p2.4.m4.4.4.2.2.2.3.2" xref="S3.SS1.p2.4.m4.4.4.2.2.2.3.2.cmml">k</mi><mo id="S3.SS1.p2.4.m4.4.4.2.2.2.3.1" xref="S3.SS1.p2.4.m4.4.4.2.2.2.3.1.cmml">âˆ’</mo><mn id="S3.SS1.p2.4.m4.4.4.2.2.2.3.3" xref="S3.SS1.p2.4.m4.4.4.2.2.2.3.3.cmml">1</mn></mrow><mi id="S3.SS1.p2.4.m4.4.4.2.2.2.2.3" xref="S3.SS1.p2.4.m4.4.4.2.2.2.2.3.cmml">i</mi></msubsup><mo id="S3.SS1.p2.4.m4.5.5.3.3.7" xref="S3.SS1.p2.4.m4.5.5.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p2.4.m4.2.2" xref="S3.SS1.p2.4.m4.2.2.cmml">â€¦</mi><mo id="S3.SS1.p2.4.m4.5.5.3.3.8" xref="S3.SS1.p2.4.m4.5.5.3.4.cmml">,</mo><msubsup id="S3.SS1.p2.4.m4.5.5.3.3.3" xref="S3.SS1.p2.4.m4.5.5.3.3.3.cmml"><mi id="S3.SS1.p2.4.m4.5.5.3.3.3.2.2" xref="S3.SS1.p2.4.m4.5.5.3.3.3.2.2.cmml">h</mi><mrow id="S3.SS1.p2.4.m4.5.5.3.3.3.3" xref="S3.SS1.p2.4.m4.5.5.3.3.3.3.cmml"><mi id="S3.SS1.p2.4.m4.5.5.3.3.3.3.2" xref="S3.SS1.p2.4.m4.5.5.3.3.3.3.2.cmml">k</mi><mo id="S3.SS1.p2.4.m4.5.5.3.3.3.3.1" xref="S3.SS1.p2.4.m4.5.5.3.3.3.3.1.cmml">âˆ’</mo><mn id="S3.SS1.p2.4.m4.5.5.3.3.3.3.3" xref="S3.SS1.p2.4.m4.5.5.3.3.3.3.3.cmml">1</mn></mrow><mrow id="S3.SS1.p2.4.m4.5.5.3.3.3.2.3" xref="S3.SS1.p2.4.m4.5.5.3.3.3.2.3.cmml"><mi id="S3.SS1.p2.4.m4.5.5.3.3.3.2.3.2" xref="S3.SS1.p2.4.m4.5.5.3.3.3.2.3.2.cmml">I</mi><mo id="S3.SS1.p2.4.m4.5.5.3.3.3.2.3.1" xref="S3.SS1.p2.4.m4.5.5.3.3.3.2.3.1.cmml">âˆ’</mo><mn id="S3.SS1.p2.4.m4.5.5.3.3.3.2.3.3" xref="S3.SS1.p2.4.m4.5.5.3.3.3.2.3.3.cmml">1</mn></mrow></msubsup><mo stretchy="false" id="S3.SS1.p2.4.m4.5.5.3.3.9" xref="S3.SS1.p2.4.m4.5.5.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.5b"><apply id="S3.SS1.p2.4.m4.5.5.cmml" xref="S3.SS1.p2.4.m4.5.5"><eq id="S3.SS1.p2.4.m4.5.5.4.cmml" xref="S3.SS1.p2.4.m4.5.5.4"></eq><apply id="S3.SS1.p2.4.m4.5.5.5.cmml" xref="S3.SS1.p2.4.m4.5.5.5"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.5.5.5.1.cmml" xref="S3.SS1.p2.4.m4.5.5.5">subscript</csymbol><ci id="S3.SS1.p2.4.m4.5.5.5.2.cmml" xref="S3.SS1.p2.4.m4.5.5.5.2">ğ»</ci><apply id="S3.SS1.p2.4.m4.5.5.5.3.cmml" xref="S3.SS1.p2.4.m4.5.5.5.3"><minus id="S3.SS1.p2.4.m4.5.5.5.3.1.cmml" xref="S3.SS1.p2.4.m4.5.5.5.3.1"></minus><ci id="S3.SS1.p2.4.m4.5.5.5.3.2.cmml" xref="S3.SS1.p2.4.m4.5.5.5.3.2">ğ‘˜</ci><cn type="integer" id="S3.SS1.p2.4.m4.5.5.5.3.3.cmml" xref="S3.SS1.p2.4.m4.5.5.5.3.3">1</cn></apply></apply><set id="S3.SS1.p2.4.m4.5.5.3.4.cmml" xref="S3.SS1.p2.4.m4.5.5.3.3"><apply id="S3.SS1.p2.4.m4.3.3.1.1.1.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.3.3.1.1.1.1.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1.1">subscript</csymbol><apply id="S3.SS1.p2.4.m4.3.3.1.1.1.2.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.3.3.1.1.1.2.1.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1.1">superscript</csymbol><ci id="S3.SS1.p2.4.m4.3.3.1.1.1.2.2.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1.1.2.2">â„</ci><cn type="integer" id="S3.SS1.p2.4.m4.3.3.1.1.1.2.3.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1.1.2.3">0</cn></apply><apply id="S3.SS1.p2.4.m4.3.3.1.1.1.3.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1.1.3"><minus id="S3.SS1.p2.4.m4.3.3.1.1.1.3.1.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1.1.3.1"></minus><ci id="S3.SS1.p2.4.m4.3.3.1.1.1.3.2.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1.1.3.2">ğ‘˜</ci><cn type="integer" id="S3.SS1.p2.4.m4.3.3.1.1.1.3.3.cmml" xref="S3.SS1.p2.4.m4.3.3.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">â€¦</ci><apply id="S3.SS1.p2.4.m4.4.4.2.2.2.cmml" xref="S3.SS1.p2.4.m4.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.4.4.2.2.2.1.cmml" xref="S3.SS1.p2.4.m4.4.4.2.2.2">subscript</csymbol><apply id="S3.SS1.p2.4.m4.4.4.2.2.2.2.cmml" xref="S3.SS1.p2.4.m4.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.4.4.2.2.2.2.1.cmml" xref="S3.SS1.p2.4.m4.4.4.2.2.2">superscript</csymbol><ci id="S3.SS1.p2.4.m4.4.4.2.2.2.2.2.cmml" xref="S3.SS1.p2.4.m4.4.4.2.2.2.2.2">â„</ci><ci id="S3.SS1.p2.4.m4.4.4.2.2.2.2.3.cmml" xref="S3.SS1.p2.4.m4.4.4.2.2.2.2.3">ğ‘–</ci></apply><apply id="S3.SS1.p2.4.m4.4.4.2.2.2.3.cmml" xref="S3.SS1.p2.4.m4.4.4.2.2.2.3"><minus id="S3.SS1.p2.4.m4.4.4.2.2.2.3.1.cmml" xref="S3.SS1.p2.4.m4.4.4.2.2.2.3.1"></minus><ci id="S3.SS1.p2.4.m4.4.4.2.2.2.3.2.cmml" xref="S3.SS1.p2.4.m4.4.4.2.2.2.3.2">ğ‘˜</ci><cn type="integer" id="S3.SS1.p2.4.m4.4.4.2.2.2.3.3.cmml" xref="S3.SS1.p2.4.m4.4.4.2.2.2.3.3">1</cn></apply></apply><ci id="S3.SS1.p2.4.m4.2.2.cmml" xref="S3.SS1.p2.4.m4.2.2">â€¦</ci><apply id="S3.SS1.p2.4.m4.5.5.3.3.3.cmml" xref="S3.SS1.p2.4.m4.5.5.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.5.5.3.3.3.1.cmml" xref="S3.SS1.p2.4.m4.5.5.3.3.3">subscript</csymbol><apply id="S3.SS1.p2.4.m4.5.5.3.3.3.2.cmml" xref="S3.SS1.p2.4.m4.5.5.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.5.5.3.3.3.2.1.cmml" xref="S3.SS1.p2.4.m4.5.5.3.3.3">superscript</csymbol><ci id="S3.SS1.p2.4.m4.5.5.3.3.3.2.2.cmml" xref="S3.SS1.p2.4.m4.5.5.3.3.3.2.2">â„</ci><apply id="S3.SS1.p2.4.m4.5.5.3.3.3.2.3.cmml" xref="S3.SS1.p2.4.m4.5.5.3.3.3.2.3"><minus id="S3.SS1.p2.4.m4.5.5.3.3.3.2.3.1.cmml" xref="S3.SS1.p2.4.m4.5.5.3.3.3.2.3.1"></minus><ci id="S3.SS1.p2.4.m4.5.5.3.3.3.2.3.2.cmml" xref="S3.SS1.p2.4.m4.5.5.3.3.3.2.3.2">ğ¼</ci><cn type="integer" id="S3.SS1.p2.4.m4.5.5.3.3.3.2.3.3.cmml" xref="S3.SS1.p2.4.m4.5.5.3.3.3.2.3.3">1</cn></apply></apply><apply id="S3.SS1.p2.4.m4.5.5.3.3.3.3.cmml" xref="S3.SS1.p2.4.m4.5.5.3.3.3.3"><minus id="S3.SS1.p2.4.m4.5.5.3.3.3.3.1.cmml" xref="S3.SS1.p2.4.m4.5.5.3.3.3.3.1"></minus><ci id="S3.SS1.p2.4.m4.5.5.3.3.3.3.2.cmml" xref="S3.SS1.p2.4.m4.5.5.3.3.3.3.2">ğ‘˜</ci><cn type="integer" id="S3.SS1.p2.4.m4.5.5.3.3.3.3.3.cmml" xref="S3.SS1.p2.4.m4.5.5.3.3.3.3.3">1</cn></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.5c">H_{k-1}=\{h^{0}_{k-1},\ldots,h^{i}_{k-1},\ldots,h^{I-1}_{k-1}\}</annotation></semantics></math> with <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><mi id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><ci id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">i</annotation></semantics></math> the index of the recurrent unit if more than one are used. The encoder returns a set of features maps <math id="S3.SS1.p2.6.m6.1" class="ltx_Math" alttext="F_{k}" display="inline"><semantics id="S3.SS1.p2.6.m6.1a"><msub id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml"><mi id="S3.SS1.p2.6.m6.1.1.2" xref="S3.SS1.p2.6.m6.1.1.2.cmml">F</mi><mi id="S3.SS1.p2.6.m6.1.1.3" xref="S3.SS1.p2.6.m6.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><apply id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p2.6.m6.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.2">ğ¹</ci><ci id="S3.SS1.p2.6.m6.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">F_{k}</annotation></semantics></math> and a set of updated hidden states <math id="S3.SS1.p2.7.m7.5" class="ltx_Math" alttext="H_{k}=\{h^{0}_{k},\ldots,h^{i}_{k},\ldots,h^{I-1}_{k}\}" display="inline"><semantics id="S3.SS1.p2.7.m7.5a"><mrow id="S3.SS1.p2.7.m7.5.5" xref="S3.SS1.p2.7.m7.5.5.cmml"><msub id="S3.SS1.p2.7.m7.5.5.5" xref="S3.SS1.p2.7.m7.5.5.5.cmml"><mi id="S3.SS1.p2.7.m7.5.5.5.2" xref="S3.SS1.p2.7.m7.5.5.5.2.cmml">H</mi><mi id="S3.SS1.p2.7.m7.5.5.5.3" xref="S3.SS1.p2.7.m7.5.5.5.3.cmml">k</mi></msub><mo id="S3.SS1.p2.7.m7.5.5.4" xref="S3.SS1.p2.7.m7.5.5.4.cmml">=</mo><mrow id="S3.SS1.p2.7.m7.5.5.3.3" xref="S3.SS1.p2.7.m7.5.5.3.4.cmml"><mo stretchy="false" id="S3.SS1.p2.7.m7.5.5.3.3.4" xref="S3.SS1.p2.7.m7.5.5.3.4.cmml">{</mo><msubsup id="S3.SS1.p2.7.m7.3.3.1.1.1" xref="S3.SS1.p2.7.m7.3.3.1.1.1.cmml"><mi id="S3.SS1.p2.7.m7.3.3.1.1.1.2.2" xref="S3.SS1.p2.7.m7.3.3.1.1.1.2.2.cmml">h</mi><mi id="S3.SS1.p2.7.m7.3.3.1.1.1.3" xref="S3.SS1.p2.7.m7.3.3.1.1.1.3.cmml">k</mi><mn id="S3.SS1.p2.7.m7.3.3.1.1.1.2.3" xref="S3.SS1.p2.7.m7.3.3.1.1.1.2.3.cmml">0</mn></msubsup><mo id="S3.SS1.p2.7.m7.5.5.3.3.5" xref="S3.SS1.p2.7.m7.5.5.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml">â€¦</mi><mo id="S3.SS1.p2.7.m7.5.5.3.3.6" xref="S3.SS1.p2.7.m7.5.5.3.4.cmml">,</mo><msubsup id="S3.SS1.p2.7.m7.4.4.2.2.2" xref="S3.SS1.p2.7.m7.4.4.2.2.2.cmml"><mi id="S3.SS1.p2.7.m7.4.4.2.2.2.2.2" xref="S3.SS1.p2.7.m7.4.4.2.2.2.2.2.cmml">h</mi><mi id="S3.SS1.p2.7.m7.4.4.2.2.2.3" xref="S3.SS1.p2.7.m7.4.4.2.2.2.3.cmml">k</mi><mi id="S3.SS1.p2.7.m7.4.4.2.2.2.2.3" xref="S3.SS1.p2.7.m7.4.4.2.2.2.2.3.cmml">i</mi></msubsup><mo id="S3.SS1.p2.7.m7.5.5.3.3.7" xref="S3.SS1.p2.7.m7.5.5.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p2.7.m7.2.2" xref="S3.SS1.p2.7.m7.2.2.cmml">â€¦</mi><mo id="S3.SS1.p2.7.m7.5.5.3.3.8" xref="S3.SS1.p2.7.m7.5.5.3.4.cmml">,</mo><msubsup id="S3.SS1.p2.7.m7.5.5.3.3.3" xref="S3.SS1.p2.7.m7.5.5.3.3.3.cmml"><mi id="S3.SS1.p2.7.m7.5.5.3.3.3.2.2" xref="S3.SS1.p2.7.m7.5.5.3.3.3.2.2.cmml">h</mi><mi id="S3.SS1.p2.7.m7.5.5.3.3.3.3" xref="S3.SS1.p2.7.m7.5.5.3.3.3.3.cmml">k</mi><mrow id="S3.SS1.p2.7.m7.5.5.3.3.3.2.3" xref="S3.SS1.p2.7.m7.5.5.3.3.3.2.3.cmml"><mi id="S3.SS1.p2.7.m7.5.5.3.3.3.2.3.2" xref="S3.SS1.p2.7.m7.5.5.3.3.3.2.3.2.cmml">I</mi><mo id="S3.SS1.p2.7.m7.5.5.3.3.3.2.3.1" xref="S3.SS1.p2.7.m7.5.5.3.3.3.2.3.1.cmml">âˆ’</mo><mn id="S3.SS1.p2.7.m7.5.5.3.3.3.2.3.3" xref="S3.SS1.p2.7.m7.5.5.3.3.3.2.3.3.cmml">1</mn></mrow></msubsup><mo stretchy="false" id="S3.SS1.p2.7.m7.5.5.3.3.9" xref="S3.SS1.p2.7.m7.5.5.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.5b"><apply id="S3.SS1.p2.7.m7.5.5.cmml" xref="S3.SS1.p2.7.m7.5.5"><eq id="S3.SS1.p2.7.m7.5.5.4.cmml" xref="S3.SS1.p2.7.m7.5.5.4"></eq><apply id="S3.SS1.p2.7.m7.5.5.5.cmml" xref="S3.SS1.p2.7.m7.5.5.5"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.5.5.5.1.cmml" xref="S3.SS1.p2.7.m7.5.5.5">subscript</csymbol><ci id="S3.SS1.p2.7.m7.5.5.5.2.cmml" xref="S3.SS1.p2.7.m7.5.5.5.2">ğ»</ci><ci id="S3.SS1.p2.7.m7.5.5.5.3.cmml" xref="S3.SS1.p2.7.m7.5.5.5.3">ğ‘˜</ci></apply><set id="S3.SS1.p2.7.m7.5.5.3.4.cmml" xref="S3.SS1.p2.7.m7.5.5.3.3"><apply id="S3.SS1.p2.7.m7.3.3.1.1.1.cmml" xref="S3.SS1.p2.7.m7.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.3.3.1.1.1.1.cmml" xref="S3.SS1.p2.7.m7.3.3.1.1.1">subscript</csymbol><apply id="S3.SS1.p2.7.m7.3.3.1.1.1.2.cmml" xref="S3.SS1.p2.7.m7.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.3.3.1.1.1.2.1.cmml" xref="S3.SS1.p2.7.m7.3.3.1.1.1">superscript</csymbol><ci id="S3.SS1.p2.7.m7.3.3.1.1.1.2.2.cmml" xref="S3.SS1.p2.7.m7.3.3.1.1.1.2.2">â„</ci><cn type="integer" id="S3.SS1.p2.7.m7.3.3.1.1.1.2.3.cmml" xref="S3.SS1.p2.7.m7.3.3.1.1.1.2.3">0</cn></apply><ci id="S3.SS1.p2.7.m7.3.3.1.1.1.3.cmml" xref="S3.SS1.p2.7.m7.3.3.1.1.1.3">ğ‘˜</ci></apply><ci id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">â€¦</ci><apply id="S3.SS1.p2.7.m7.4.4.2.2.2.cmml" xref="S3.SS1.p2.7.m7.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.4.4.2.2.2.1.cmml" xref="S3.SS1.p2.7.m7.4.4.2.2.2">subscript</csymbol><apply id="S3.SS1.p2.7.m7.4.4.2.2.2.2.cmml" xref="S3.SS1.p2.7.m7.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.4.4.2.2.2.2.1.cmml" xref="S3.SS1.p2.7.m7.4.4.2.2.2">superscript</csymbol><ci id="S3.SS1.p2.7.m7.4.4.2.2.2.2.2.cmml" xref="S3.SS1.p2.7.m7.4.4.2.2.2.2.2">â„</ci><ci id="S3.SS1.p2.7.m7.4.4.2.2.2.2.3.cmml" xref="S3.SS1.p2.7.m7.4.4.2.2.2.2.3">ğ‘–</ci></apply><ci id="S3.SS1.p2.7.m7.4.4.2.2.2.3.cmml" xref="S3.SS1.p2.7.m7.4.4.2.2.2.3">ğ‘˜</ci></apply><ci id="S3.SS1.p2.7.m7.2.2.cmml" xref="S3.SS1.p2.7.m7.2.2">â€¦</ci><apply id="S3.SS1.p2.7.m7.5.5.3.3.3.cmml" xref="S3.SS1.p2.7.m7.5.5.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.5.5.3.3.3.1.cmml" xref="S3.SS1.p2.7.m7.5.5.3.3.3">subscript</csymbol><apply id="S3.SS1.p2.7.m7.5.5.3.3.3.2.cmml" xref="S3.SS1.p2.7.m7.5.5.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.5.5.3.3.3.2.1.cmml" xref="S3.SS1.p2.7.m7.5.5.3.3.3">superscript</csymbol><ci id="S3.SS1.p2.7.m7.5.5.3.3.3.2.2.cmml" xref="S3.SS1.p2.7.m7.5.5.3.3.3.2.2">â„</ci><apply id="S3.SS1.p2.7.m7.5.5.3.3.3.2.3.cmml" xref="S3.SS1.p2.7.m7.5.5.3.3.3.2.3"><minus id="S3.SS1.p2.7.m7.5.5.3.3.3.2.3.1.cmml" xref="S3.SS1.p2.7.m7.5.5.3.3.3.2.3.1"></minus><ci id="S3.SS1.p2.7.m7.5.5.3.3.3.2.3.2.cmml" xref="S3.SS1.p2.7.m7.5.5.3.3.3.2.3.2">ğ¼</ci><cn type="integer" id="S3.SS1.p2.7.m7.5.5.3.3.3.2.3.3.cmml" xref="S3.SS1.p2.7.m7.5.5.3.3.3.2.3.3">1</cn></apply></apply><ci id="S3.SS1.p2.7.m7.5.5.3.3.3.3.cmml" xref="S3.SS1.p2.7.m7.5.5.3.3.3.3">ğ‘˜</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.5c">H_{k}=\{h^{0}_{k},\ldots,h^{i}_{k},\ldots,h^{I-1}_{k}\}</annotation></semantics></math> such that:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="E(r_{k},H_{k-1})=(F_{k},H_{k})." display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.1.1.2.4" xref="S3.E2.m1.1.1.1.1.2.4.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.2.3.cmml">â€‹</mo><mrow id="S3.E2.m1.1.1.1.1.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.2.2.2.3" xref="S3.E2.m1.1.1.1.1.2.2.3.cmml">(</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml">r</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml">k</mi></msub><mo id="S3.E2.m1.1.1.1.1.2.2.2.4" xref="S3.E2.m1.1.1.1.1.2.2.3.cmml">,</mo><msub id="S3.E2.m1.1.1.1.1.2.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2.cmml">H</mi><mrow id="S3.E2.m1.1.1.1.1.2.2.2.2.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2.2.2.3.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2.3.2.cmml">k</mi><mo id="S3.E2.m1.1.1.1.1.2.2.2.2.3.1" xref="S3.E2.m1.1.1.1.1.2.2.2.2.3.1.cmml">âˆ’</mo><mn id="S3.E2.m1.1.1.1.1.2.2.2.2.3.3" xref="S3.E2.m1.1.1.1.1.2.2.2.2.3.3.cmml">1</mn></mrow></msub><mo stretchy="false" id="S3.E2.m1.1.1.1.1.2.2.2.5" xref="S3.E2.m1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.1.1.1.1.5" xref="S3.E2.m1.1.1.1.1.5.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.4.2" xref="S3.E2.m1.1.1.1.1.4.3.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.4.2.3" xref="S3.E2.m1.1.1.1.1.4.3.cmml">(</mo><msub id="S3.E2.m1.1.1.1.1.3.1.1" xref="S3.E2.m1.1.1.1.1.3.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.3.1.1.2" xref="S3.E2.m1.1.1.1.1.3.1.1.2.cmml">F</mi><mi id="S3.E2.m1.1.1.1.1.3.1.1.3" xref="S3.E2.m1.1.1.1.1.3.1.1.3.cmml">k</mi></msub><mo id="S3.E2.m1.1.1.1.1.4.2.4" xref="S3.E2.m1.1.1.1.1.4.3.cmml">,</mo><msub id="S3.E2.m1.1.1.1.1.4.2.2" xref="S3.E2.m1.1.1.1.1.4.2.2.cmml"><mi id="S3.E2.m1.1.1.1.1.4.2.2.2" xref="S3.E2.m1.1.1.1.1.4.2.2.2.cmml">H</mi><mi id="S3.E2.m1.1.1.1.1.4.2.2.3" xref="S3.E2.m1.1.1.1.1.4.2.2.3.cmml">k</mi></msub><mo stretchy="false" id="S3.E2.m1.1.1.1.1.4.2.5" xref="S3.E2.m1.1.1.1.1.4.3.cmml">)</mo></mrow></mrow><mo lspace="0em" id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.5.cmml" xref="S3.E2.m1.1.1.1.1.5"></eq><apply id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"><times id="S3.E2.m1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.3"></times><ci id="S3.E2.m1.1.1.1.1.2.4.cmml" xref="S3.E2.m1.1.1.1.1.2.4">ğ¸</ci><interval closure="open" id="S3.E2.m1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2"><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2">ğ‘Ÿ</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3">ğ‘˜</ci></apply><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.2">ğ»</ci><apply id="S3.E2.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.3"><minus id="S3.E2.m1.1.1.1.1.2.2.2.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.3.1"></minus><ci id="S3.E2.m1.1.1.1.1.2.2.2.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.3.2">ğ‘˜</ci><cn type="integer" id="S3.E2.m1.1.1.1.1.2.2.2.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2.3.3">1</cn></apply></apply></interval></apply><interval closure="open" id="S3.E2.m1.1.1.1.1.4.3.cmml" xref="S3.E2.m1.1.1.1.1.4.2"><apply id="S3.E2.m1.1.1.1.1.3.1.1.cmml" xref="S3.E2.m1.1.1.1.1.3.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.3.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.3.1.1.2">ğ¹</ci><ci id="S3.E2.m1.1.1.1.1.3.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3.1.1.3">ğ‘˜</ci></apply><apply id="S3.E2.m1.1.1.1.1.4.2.2.cmml" xref="S3.E2.m1.1.1.1.1.4.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.4.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.4.2.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.4.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.4.2.2.2">ğ»</ci><ci id="S3.E2.m1.1.1.1.1.4.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.4.2.2.3">ğ‘˜</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">E(r_{k},H_{k-1})=(F_{k},H_{k}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p2.11" class="ltx_p">Because our encoder encodes the past <math id="S3.SS1.p2.8.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p2.8.m1.1a"><mi id="S3.SS1.p2.8.m1.1.1" xref="S3.SS1.p2.8.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m1.1b"><ci id="S3.SS1.p2.8.m1.1.1.cmml" xref="S3.SS1.p2.8.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m1.1c">N</annotation></semantics></math> frames recurrently to predict the position of objects at time step <math id="S3.SS1.p2.9.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.p2.9.m2.1a"><mi id="S3.SS1.p2.9.m2.1.1" xref="S3.SS1.p2.9.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m2.1b"><ci id="S3.SS1.p2.9.m2.1.1.cmml" xref="S3.SS1.p2.9.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m2.1c">k</annotation></semantics></math>, our decoder is a fully convolutional decoder that takes as input the encoderâ€™s updated hidden states <math id="S3.SS1.p2.10.m3.1" class="ltx_Math" alttext="H_{k}" display="inline"><semantics id="S3.SS1.p2.10.m3.1a"><msub id="S3.SS1.p2.10.m3.1.1" xref="S3.SS1.p2.10.m3.1.1.cmml"><mi id="S3.SS1.p2.10.m3.1.1.2" xref="S3.SS1.p2.10.m3.1.1.2.cmml">H</mi><mi id="S3.SS1.p2.10.m3.1.1.3" xref="S3.SS1.p2.10.m3.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.10.m3.1b"><apply id="S3.SS1.p2.10.m3.1.1.cmml" xref="S3.SS1.p2.10.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.10.m3.1.1.1.cmml" xref="S3.SS1.p2.10.m3.1.1">subscript</csymbol><ci id="S3.SS1.p2.10.m3.1.1.2.cmml" xref="S3.SS1.p2.10.m3.1.1.2">ğ»</ci><ci id="S3.SS1.p2.10.m3.1.1.3.cmml" xref="S3.SS1.p2.10.m3.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.10.m3.1c">H_{k}</annotation></semantics></math> (the memory) and the set of feature maps <math id="S3.SS1.p2.11.m4.1" class="ltx_Math" alttext="F_{k}" display="inline"><semantics id="S3.SS1.p2.11.m4.1a"><msub id="S3.SS1.p2.11.m4.1.1" xref="S3.SS1.p2.11.m4.1.1.cmml"><mi id="S3.SS1.p2.11.m4.1.1.2" xref="S3.SS1.p2.11.m4.1.1.2.cmml">F</mi><mi id="S3.SS1.p2.11.m4.1.1.3" xref="S3.SS1.p2.11.m4.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.11.m4.1b"><apply id="S3.SS1.p2.11.m4.1.1.cmml" xref="S3.SS1.p2.11.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.11.m4.1.1.1.cmml" xref="S3.SS1.p2.11.m4.1.1">subscript</csymbol><ci id="S3.SS1.p2.11.m4.1.1.2.cmml" xref="S3.SS1.p2.11.m4.1.1.2">ğ¹</ci><ci id="S3.SS1.p2.11.m4.1.1.3.cmml" xref="S3.SS1.p2.11.m4.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.11.m4.1c">F_{k}</annotation></semantics></math> (spatio-temporal feature maps) such that:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.1" class="ltx_Math" alttext="D(F_{k},H_{k})=P_{k}." display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.1.1.1.1.2.4" xref="S3.E3.m1.1.1.1.1.2.4.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.2.3.cmml">â€‹</mo><mrow id="S3.E3.m1.1.1.1.1.2.2.2" xref="S3.E3.m1.1.1.1.1.2.2.3.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.2.2.2.3" xref="S3.E3.m1.1.1.1.1.2.2.3.cmml">(</mo><msub id="S3.E3.m1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml">F</mi><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml">k</mi></msub><mo id="S3.E3.m1.1.1.1.1.2.2.2.4" xref="S3.E3.m1.1.1.1.1.2.2.3.cmml">,</mo><msub id="S3.E3.m1.1.1.1.1.2.2.2.2" xref="S3.E3.m1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.E3.m1.1.1.1.1.2.2.2.2.2" xref="S3.E3.m1.1.1.1.1.2.2.2.2.2.cmml">H</mi><mi id="S3.E3.m1.1.1.1.1.2.2.2.2.3" xref="S3.E3.m1.1.1.1.1.2.2.2.2.3.cmml">k</mi></msub><mo stretchy="false" id="S3.E3.m1.1.1.1.1.2.2.2.5" xref="S3.E3.m1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.3.cmml">=</mo><msub id="S3.E3.m1.1.1.1.1.4" xref="S3.E3.m1.1.1.1.1.4.cmml"><mi id="S3.E3.m1.1.1.1.1.4.2" xref="S3.E3.m1.1.1.1.1.4.2.cmml">P</mi><mi id="S3.E3.m1.1.1.1.1.4.3" xref="S3.E3.m1.1.1.1.1.4.3.cmml">k</mi></msub></mrow><mo lspace="0em" id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><eq id="S3.E3.m1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3"></eq><apply id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2"><times id="S3.E3.m1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.3"></times><ci id="S3.E3.m1.1.1.1.1.2.4.cmml" xref="S3.E3.m1.1.1.1.1.2.4">ğ·</ci><interval closure="open" id="S3.E3.m1.1.1.1.1.2.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2"><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2">ğ¹</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3">ğ‘˜</ci></apply><apply id="S3.E3.m1.1.1.1.1.2.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.2.2">ğ»</ci><ci id="S3.E3.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.2.3">ğ‘˜</ci></apply></interval></apply><apply id="S3.E3.m1.1.1.1.1.4.cmml" xref="S3.E3.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.4.1.cmml" xref="S3.E3.m1.1.1.1.1.4">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.4.2.cmml" xref="S3.E3.m1.1.1.1.1.4.2">ğ‘ƒ</ci><ci id="S3.E3.m1.1.1.1.1.4.3.cmml" xref="S3.E3.m1.1.1.1.1.4.3">ğ‘˜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">D(F_{k},H_{k})=P_{k}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Spatio-temporal encoder</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.3" class="ltx_p">An overview of our single-view architecture is shown in figure <a href="#S2.F2" title="Figure 2 â€£ II Radar background â€£ A recurrent CNN for online object detection on raw radar frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We propose a fully convolutional recurrent encoder (left part of figure <a href="#S2.F2" title="Figure 2 â€£ II Radar background â€£ A recurrent CNN for online object detection on raw radar frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). In other words, our encoder mixes 2D convolutions and ConvRNNs.
We use 2D convolutions to learn spatial information and reduce the size of inputs. To reduce the number of parameters of the model and its computation time, we use inverted residual (IR) bottleneck blocks from the MobileNetV2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> architecture instead of classic 2D convolutions for most of the convolutional layers of the model. IR bottleneck is a residual block based on depthwise separable convolutions that use an inverted structure for efficiency reasons. Then, we propose inserting ConvLSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> cells between convolutional layers to learn the temporal relationship between frames. Similarly to the convolutions, we replace the classic ConvLSTM with an efficient one proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> by Liu and Zhu called bottleneck LSTM. Contrary to a classic ConvLSTM, authors replace convolutions with depthwise-separable convolutions, which reduces the required computation by a factor of eight to nine. Additionally, <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="tanh" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.1.m1.1.1.1a" xref="S3.SS2.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p1.1.m1.1.1.4" xref="S3.SS2.p1.1.m1.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.1.m1.1.1.1b" xref="S3.SS2.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p1.1.m1.1.1.5" xref="S3.SS2.p1.1.m1.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><times id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></times><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">ğ‘¡</ci><ci id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">ğ‘</ci><ci id="S3.SS2.p1.1.m1.1.1.4.cmml" xref="S3.SS2.p1.1.m1.1.1.4">ğ‘›</ci><ci id="S3.SS2.p1.1.m1.1.1.5.cmml" xref="S3.SS2.p1.1.m1.1.1.5">â„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">tanh</annotation></semantics></math> activation functions are replaced by <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="ReLU" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.1.1a" xref="S3.SS2.p1.2.m2.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p1.2.m2.1.1.4" xref="S3.SS2.p1.2.m2.1.1.4.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.1.1b" xref="S3.SS2.p1.2.m2.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p1.2.m2.1.1.5" xref="S3.SS2.p1.2.m2.1.1.5.cmml">U</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><times id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1"></times><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">ğ‘…</ci><ci id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">ğ‘’</ci><ci id="S3.SS2.p1.2.m2.1.1.4.cmml" xref="S3.SS2.p1.2.m2.1.1.4">ğ¿</ci><ci id="S3.SS2.p1.2.m2.1.1.5.cmml" xref="S3.SS2.p1.2.m2.1.1.5">ğ‘ˆ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">ReLU</annotation></semantics></math> activation functions. In this work, we use two bottleneck LSTMs, as a result, <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="I=2" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mrow id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">I</mi><mo id="S3.SS2.p1.3.m3.1.1.1" xref="S3.SS2.p1.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><eq id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1.1"></eq><ci id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">ğ¼</ci><cn type="integer" id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">I=2</annotation></semantics></math>. Such a layout enhances spatial features with temporal features and vice versa.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">We follow the MobileNetV2 structure by first applying a full convolution to increase the number of channels followed by a single IR bottleneck block. Except for the first IR bottleneck block, we set the expansion rate <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">Î³</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">ğ›¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\gamma</annotation></semantics></math> to four. Next, we apply two blocks composed of three IR bottleneck blocks followed by a bottleneck LSTM to learn spatio-temporal dependencies. Because the computational cost of bottleneck LSTMs is proportional to the input size, we use a stride of two in the first IR bottleneck block to reduce the input dimension. We insert these bottleneck LSTMs in the middle of the encoder to not alter the spatial information too much. Finally, we refine the spatio-temporal feature maps obtained from the bottleneck LSTMs by adding three additional IR bottleneck blocks.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.2" class="ltx_p">Because we treat data sequences, it is desirable to calculate normalisation statistics across all features and all elements for each instance independently instead of a batch of data (a batch can be composed of sequences from different scenes). As a result, we add layer normalisation before sigmoid activation on gates <math id="S3.SS2.p3.1.m1.2" class="ltx_Math" alttext="o_{t},i_{t}" display="inline"><semantics id="S3.SS2.p3.1.m1.2a"><mrow id="S3.SS2.p3.1.m1.2.2.2" xref="S3.SS2.p3.1.m1.2.2.3.cmml"><msub id="S3.SS2.p3.1.m1.1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.1.1.2.cmml">o</mi><mi id="S3.SS2.p3.1.m1.1.1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.SS2.p3.1.m1.2.2.2.3" xref="S3.SS2.p3.1.m1.2.2.3.cmml">,</mo><msub id="S3.SS2.p3.1.m1.2.2.2.2" xref="S3.SS2.p3.1.m1.2.2.2.2.cmml"><mi id="S3.SS2.p3.1.m1.2.2.2.2.2" xref="S3.SS2.p3.1.m1.2.2.2.2.2.cmml">i</mi><mi id="S3.SS2.p3.1.m1.2.2.2.2.3" xref="S3.SS2.p3.1.m1.2.2.2.2.3.cmml">t</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.2b"><list id="S3.SS2.p3.1.m1.2.2.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2"><apply id="S3.SS2.p3.1.m1.1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.2">ğ‘œ</ci><ci id="S3.SS2.p3.1.m1.1.1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.3">ğ‘¡</ci></apply><apply id="S3.SS2.p3.1.m1.2.2.2.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.2.2.2.2.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p3.1.m1.2.2.2.2.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.2">ğ‘–</ci><ci id="S3.SS2.p3.1.m1.2.2.2.2.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.3">ğ‘¡</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.2c">o_{t},i_{t}</annotation></semantics></math> and <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="f_{t}" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><msub id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">f</mi><mi id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">ğ‘“</ci><ci id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">f_{t}</annotation></semantics></math> in the bottleneck LSTM, and we adopt layer normalisation for all the layers in the model.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Decoder</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.3" class="ltx_p">As described in section <a href="#S3.SS1" title="III-A Problem formulation â€£ III Spatio-temporal radar object detector â€£ A recurrent CNN for online object detection on raw radar frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>, our decoder is a 2D convolutional decoder which takes as input the last feature maps of the encoder (denoted <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="F_{k}" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><msub id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">F</mi><mi id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">ğ¹</ci><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">F_{k}</annotation></semantics></math>) and a set of two hidden states <math id="S3.SS3.p1.2.m2.2" class="ltx_Math" alttext="H_{k}=\{h_{k}^{0},h_{k}^{1}\}" display="inline"><semantics id="S3.SS3.p1.2.m2.2a"><mrow id="S3.SS3.p1.2.m2.2.2" xref="S3.SS3.p1.2.m2.2.2.cmml"><msub id="S3.SS3.p1.2.m2.2.2.4" xref="S3.SS3.p1.2.m2.2.2.4.cmml"><mi id="S3.SS3.p1.2.m2.2.2.4.2" xref="S3.SS3.p1.2.m2.2.2.4.2.cmml">H</mi><mi id="S3.SS3.p1.2.m2.2.2.4.3" xref="S3.SS3.p1.2.m2.2.2.4.3.cmml">k</mi></msub><mo id="S3.SS3.p1.2.m2.2.2.3" xref="S3.SS3.p1.2.m2.2.2.3.cmml">=</mo><mrow id="S3.SS3.p1.2.m2.2.2.2.2" xref="S3.SS3.p1.2.m2.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.p1.2.m2.2.2.2.2.3" xref="S3.SS3.p1.2.m2.2.2.2.3.cmml">{</mo><msubsup id="S3.SS3.p1.2.m2.1.1.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS3.p1.2.m2.1.1.1.1.1.2.2" xref="S3.SS3.p1.2.m2.1.1.1.1.1.2.2.cmml">h</mi><mi id="S3.SS3.p1.2.m2.1.1.1.1.1.2.3" xref="S3.SS3.p1.2.m2.1.1.1.1.1.2.3.cmml">k</mi><mn id="S3.SS3.p1.2.m2.1.1.1.1.1.3" xref="S3.SS3.p1.2.m2.1.1.1.1.1.3.cmml">0</mn></msubsup><mo id="S3.SS3.p1.2.m2.2.2.2.2.4" xref="S3.SS3.p1.2.m2.2.2.2.3.cmml">,</mo><msubsup id="S3.SS3.p1.2.m2.2.2.2.2.2" xref="S3.SS3.p1.2.m2.2.2.2.2.2.cmml"><mi id="S3.SS3.p1.2.m2.2.2.2.2.2.2.2" xref="S3.SS3.p1.2.m2.2.2.2.2.2.2.2.cmml">h</mi><mi id="S3.SS3.p1.2.m2.2.2.2.2.2.2.3" xref="S3.SS3.p1.2.m2.2.2.2.2.2.2.3.cmml">k</mi><mn id="S3.SS3.p1.2.m2.2.2.2.2.2.3" xref="S3.SS3.p1.2.m2.2.2.2.2.2.3.cmml">1</mn></msubsup><mo stretchy="false" id="S3.SS3.p1.2.m2.2.2.2.2.5" xref="S3.SS3.p1.2.m2.2.2.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.2b"><apply id="S3.SS3.p1.2.m2.2.2.cmml" xref="S3.SS3.p1.2.m2.2.2"><eq id="S3.SS3.p1.2.m2.2.2.3.cmml" xref="S3.SS3.p1.2.m2.2.2.3"></eq><apply id="S3.SS3.p1.2.m2.2.2.4.cmml" xref="S3.SS3.p1.2.m2.2.2.4"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.2.2.4.1.cmml" xref="S3.SS3.p1.2.m2.2.2.4">subscript</csymbol><ci id="S3.SS3.p1.2.m2.2.2.4.2.cmml" xref="S3.SS3.p1.2.m2.2.2.4.2">ğ»</ci><ci id="S3.SS3.p1.2.m2.2.2.4.3.cmml" xref="S3.SS3.p1.2.m2.2.2.4.3">ğ‘˜</ci></apply><set id="S3.SS3.p1.2.m2.2.2.2.3.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2"><apply id="S3.SS3.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1">superscript</csymbol><apply id="S3.SS3.p1.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.1.1.2.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.1.1.1.2.2.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.2.2">â„</ci><ci id="S3.SS3.p1.2.m2.1.1.1.1.1.2.3.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.2.3">ğ‘˜</ci></apply><cn type="integer" id="S3.SS3.p1.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.1.3">0</cn></apply><apply id="S3.SS3.p1.2.m2.2.2.2.2.2.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.2.2.2.2.2.1.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2.2">superscript</csymbol><apply id="S3.SS3.p1.2.m2.2.2.2.2.2.2.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.2.2.2.2.2.2.1.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p1.2.m2.2.2.2.2.2.2.2.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2.2.2.2">â„</ci><ci id="S3.SS3.p1.2.m2.2.2.2.2.2.2.3.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2.2.2.3">ğ‘˜</ci></apply><cn type="integer" id="S3.SS3.p1.2.m2.2.2.2.2.2.3.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2.2.3">1</cn></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.2c">H_{k}=\{h_{k}^{0},h_{k}^{1}\}</annotation></semantics></math>. Our decoder is composed of three 2D transposed convolutions followed by a single IR block with an expansion factor <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mi id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">Î³</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><ci id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">ğ›¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">\gamma</annotation></semantics></math> set to one, and a layer normalisation layer. Each transposed convolution block upsample the input feature map by two. Finally, we use two 2D convolutions as a classification/segmentation head (depending on the task) which projects the upsampled feature map onto the desired output.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.2" class="ltx_p">The U-Net architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> has popularised skip connections between the encoder and decoder. It allows precise localisation by combining high-resolution and low-resolution features. We, therefore, adopt skip connections between our encoder and our decoder to improve the localisation precision. To prevent the loss of temporal information in the decoding stage, we use the hidden states of each bottleneck LSTM (denoted by <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="h_{k}^{0}" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><msubsup id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2.2" xref="S3.SS3.p2.1.m1.1.1.2.2.cmml">h</mi><mi id="S3.SS3.p2.1.m1.1.1.2.3" xref="S3.SS3.p2.1.m1.1.1.2.3.cmml">k</mi><mn id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">0</mn></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">superscript</csymbol><apply id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.2.1.cmml" xref="S3.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.2.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2.2">â„</ci><ci id="S3.SS3.p2.1.m1.1.1.2.3.cmml" xref="S3.SS3.p2.1.m1.1.1.2.3">ğ‘˜</ci></apply><cn type="integer" id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">h_{k}^{0}</annotation></semantics></math> and <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="h_{k}^{1}" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><msubsup id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mi id="S3.SS3.p2.2.m2.1.1.2.2" xref="S3.SS3.p2.2.m2.1.1.2.2.cmml">h</mi><mi id="S3.SS3.p2.2.m2.1.1.2.3" xref="S3.SS3.p2.2.m2.1.1.2.3.cmml">k</mi><mn id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml">1</mn></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">superscript</csymbol><apply id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.2.1.cmml" xref="S3.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.2.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2.2">â„</ci><ci id="S3.SS3.p2.2.m2.1.1.2.3.cmml" xref="S3.SS3.p2.2.m2.1.1.2.3">ğ‘˜</ci></apply><cn type="integer" id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">h_{k}^{1}</annotation></semantics></math> in figure <a href="#S2.F2" title="Figure 2 â€£ II Radar background â€£ A recurrent CNN for online object detection on raw radar frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) and concatenating them with the output of a transposed convolution operation to propagate in the decoder the temporal relationship learned by the encoder.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2212.11172/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="121" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Multi-view model architecture (MV-RECORD). We use the encoder described in figure <a href="#S3.SS2" title="III-B Spatio-temporal encoder â€£ III Spatio-temporal radar object detector â€£ A recurrent CNN for online object detection on raw radar frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a> for each view. Dashed boxes denote an optional operation applied only if the feature maps have different shapes. Gray arrows denote the same output.</figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.5.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.6.2" class="ltx_text ltx_font_italic">Multi-view spatio-temporal object detector</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The preceding sections describe a spatio-temporal radar object detection architecture for single view inputs (<span id="S3.SS4.p1.1.1" class="ltx_text ltx_font_italic">i.e.</span> RA or RD). However, using more than one view to represent targets in their entirety might be desirable. In other words, to simultaneously find the position (distance, angle), the velocity and the class of targets. In this section, we propose to extend the previous architecture to a multi-view approach. We follow the paradigm of Ouakine <span id="S3.SS4.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> by replicating three times the encoder proposed in section <a href="#S3.SS2" title="III-B Spatio-temporal encoder â€£ III Spatio-temporal radar object detector â€£ A recurrent CNN for online object detection on raw radar frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a> (one for RA view, one for RD view and one for AD view, see figure <a href="#S2.F2" title="Figure 2 â€£ II Radar background â€£ A recurrent CNN for online object detection on raw radar frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). Then, the latent space of each view is concatenated to create a multi-view latent space. We use two decoders to predict objectsâ€™ positions in all dimensions (RA and RD). One for the RA view and one for the RD view. The multi-view latent space is the input of these decoders.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.3" class="ltx_p">In section <a href="#S3.SS3" title="III-C Decoder â€£ III Spatio-temporal radar object detector â€£ A recurrent CNN for online object detection on raw radar frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a>, we use the hidden states of each bottleneck LSTMs for the skip connection to add the temporal information in the decoding part. For the multi-view approach, we want to take advantage of the multi-view and the spatio-temporal approaches in the skip connections to supplement decoders with data from other views (<span id="S3.SS4.p2.3.1" class="ltx_text ltx_font_italic">e.g.</span> add velocity information in the RA view). Similarly to the multi-view latent space, we concatenate the hidden states from RD, RA and AD views. This concatenation results in a set of concatenated hidden states <math id="S3.SS4.p2.1.m1.2" class="ltx_Math" alttext="H_{k}=\{h_{k_{skip}}^{0},h_{k_{skip}}^{1}\}" display="inline"><semantics id="S3.SS4.p2.1.m1.2a"><mrow id="S3.SS4.p2.1.m1.2.2" xref="S3.SS4.p2.1.m1.2.2.cmml"><msub id="S3.SS4.p2.1.m1.2.2.4" xref="S3.SS4.p2.1.m1.2.2.4.cmml"><mi id="S3.SS4.p2.1.m1.2.2.4.2" xref="S3.SS4.p2.1.m1.2.2.4.2.cmml">H</mi><mi id="S3.SS4.p2.1.m1.2.2.4.3" xref="S3.SS4.p2.1.m1.2.2.4.3.cmml">k</mi></msub><mo id="S3.SS4.p2.1.m1.2.2.3" xref="S3.SS4.p2.1.m1.2.2.3.cmml">=</mo><mrow id="S3.SS4.p2.1.m1.2.2.2.2" xref="S3.SS4.p2.1.m1.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS4.p2.1.m1.2.2.2.2.3" xref="S3.SS4.p2.1.m1.2.2.2.3.cmml">{</mo><msubsup id="S3.SS4.p2.1.m1.1.1.1.1.1" xref="S3.SS4.p2.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS4.p2.1.m1.1.1.1.1.1.2.2" xref="S3.SS4.p2.1.m1.1.1.1.1.1.2.2.cmml">h</mi><msub id="S3.SS4.p2.1.m1.1.1.1.1.1.2.3" xref="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.cmml"><mi id="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.2" xref="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.2.cmml">k</mi><mrow id="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.3" xref="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.3.cmml"><mi id="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.3.2" xref="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.3.1" xref="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.3.1.cmml">â€‹</mo><mi id="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.3.3" xref="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.3.3.cmml">k</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.3.1a" xref="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.3.1.cmml">â€‹</mo><mi id="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.3.4" xref="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.3.1b" xref="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.3.1.cmml">â€‹</mo><mi id="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.3.5" xref="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.3.5.cmml">p</mi></mrow></msub><mn id="S3.SS4.p2.1.m1.1.1.1.1.1.3" xref="S3.SS4.p2.1.m1.1.1.1.1.1.3.cmml">0</mn></msubsup><mo id="S3.SS4.p2.1.m1.2.2.2.2.4" xref="S3.SS4.p2.1.m1.2.2.2.3.cmml">,</mo><msubsup id="S3.SS4.p2.1.m1.2.2.2.2.2" xref="S3.SS4.p2.1.m1.2.2.2.2.2.cmml"><mi id="S3.SS4.p2.1.m1.2.2.2.2.2.2.2" xref="S3.SS4.p2.1.m1.2.2.2.2.2.2.2.cmml">h</mi><msub id="S3.SS4.p2.1.m1.2.2.2.2.2.2.3" xref="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.cmml"><mi id="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.2" xref="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.2.cmml">k</mi><mrow id="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.3" xref="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.3.cmml"><mi id="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.3.2" xref="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.3.1" xref="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.3.1.cmml">â€‹</mo><mi id="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.3.3" xref="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.3.3.cmml">k</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.3.1a" xref="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.3.1.cmml">â€‹</mo><mi id="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.3.4" xref="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.3.1b" xref="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.3.1.cmml">â€‹</mo><mi id="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.3.5" xref="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.3.5.cmml">p</mi></mrow></msub><mn id="S3.SS4.p2.1.m1.2.2.2.2.2.3" xref="S3.SS4.p2.1.m1.2.2.2.2.2.3.cmml">1</mn></msubsup><mo stretchy="false" id="S3.SS4.p2.1.m1.2.2.2.2.5" xref="S3.SS4.p2.1.m1.2.2.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.2b"><apply id="S3.SS4.p2.1.m1.2.2.cmml" xref="S3.SS4.p2.1.m1.2.2"><eq id="S3.SS4.p2.1.m1.2.2.3.cmml" xref="S3.SS4.p2.1.m1.2.2.3"></eq><apply id="S3.SS4.p2.1.m1.2.2.4.cmml" xref="S3.SS4.p2.1.m1.2.2.4"><csymbol cd="ambiguous" id="S3.SS4.p2.1.m1.2.2.4.1.cmml" xref="S3.SS4.p2.1.m1.2.2.4">subscript</csymbol><ci id="S3.SS4.p2.1.m1.2.2.4.2.cmml" xref="S3.SS4.p2.1.m1.2.2.4.2">ğ»</ci><ci id="S3.SS4.p2.1.m1.2.2.4.3.cmml" xref="S3.SS4.p2.1.m1.2.2.4.3">ğ‘˜</ci></apply><set id="S3.SS4.p2.1.m1.2.2.2.3.cmml" xref="S3.SS4.p2.1.m1.2.2.2.2"><apply id="S3.SS4.p2.1.m1.1.1.1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1.1.1.1">superscript</csymbol><apply id="S3.SS4.p2.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS4.p2.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.1.m1.1.1.1.1.1.2.1.cmml" xref="S3.SS4.p2.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p2.1.m1.1.1.1.1.1.2.2.cmml" xref="S3.SS4.p2.1.m1.1.1.1.1.1.2.2">â„</ci><apply id="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.cmml" xref="S3.SS4.p2.1.m1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.1.cmml" xref="S3.SS4.p2.1.m1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.2.cmml" xref="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.2">ğ‘˜</ci><apply id="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.3.cmml" xref="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.3"><times id="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.3.1.cmml" xref="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.3.1"></times><ci id="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.3.2.cmml" xref="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.3.2">ğ‘ </ci><ci id="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.3.3.cmml" xref="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.3.3">ğ‘˜</ci><ci id="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.3.4.cmml" xref="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.3.4">ğ‘–</ci><ci id="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.3.5.cmml" xref="S3.SS4.p2.1.m1.1.1.1.1.1.2.3.3.5">ğ‘</ci></apply></apply></apply><cn type="integer" id="S3.SS4.p2.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS4.p2.1.m1.1.1.1.1.1.3">0</cn></apply><apply id="S3.SS4.p2.1.m1.2.2.2.2.2.cmml" xref="S3.SS4.p2.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS4.p2.1.m1.2.2.2.2.2.1.cmml" xref="S3.SS4.p2.1.m1.2.2.2.2.2">superscript</csymbol><apply id="S3.SS4.p2.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS4.p2.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS4.p2.1.m1.2.2.2.2.2.2.1.cmml" xref="S3.SS4.p2.1.m1.2.2.2.2.2">subscript</csymbol><ci id="S3.SS4.p2.1.m1.2.2.2.2.2.2.2.cmml" xref="S3.SS4.p2.1.m1.2.2.2.2.2.2.2">â„</ci><apply id="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.cmml" xref="S3.SS4.p2.1.m1.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.1.cmml" xref="S3.SS4.p2.1.m1.2.2.2.2.2.2.3">subscript</csymbol><ci id="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.2.cmml" xref="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.2">ğ‘˜</ci><apply id="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.3.cmml" xref="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.3"><times id="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.3.1.cmml" xref="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.3.1"></times><ci id="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.3.2.cmml" xref="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.3.2">ğ‘ </ci><ci id="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.3.3.cmml" xref="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.3.3">ğ‘˜</ci><ci id="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.3.4.cmml" xref="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.3.4">ğ‘–</ci><ci id="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.3.5.cmml" xref="S3.SS4.p2.1.m1.2.2.2.2.2.2.3.3.5">ğ‘</ci></apply></apply></apply><cn type="integer" id="S3.SS4.p2.1.m1.2.2.2.2.2.3.cmml" xref="S3.SS4.p2.1.m1.2.2.2.2.2.3">1</cn></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.2c">H_{k}=\{h_{k_{skip}}^{0},h_{k_{skip}}^{1}\}</annotation></semantics></math>. We describe the operation to obtain <math id="S3.SS4.p2.2.m2.1" class="ltx_Math" alttext="H_{k}" display="inline"><semantics id="S3.SS4.p2.2.m2.1a"><msub id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml"><mi id="S3.SS4.p2.2.m2.1.1.2" xref="S3.SS4.p2.2.m2.1.1.2.cmml">H</mi><mi id="S3.SS4.p2.2.m2.1.1.3" xref="S3.SS4.p2.2.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><apply id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.2.m2.1.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p2.2.m2.1.1.2.cmml" xref="S3.SS4.p2.2.m2.1.1.2">ğ»</ci><ci id="S3.SS4.p2.2.m2.1.1.3.cmml" xref="S3.SS4.p2.2.m2.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">H_{k}</annotation></semantics></math> in figure <a href="#S3.F3" title="Figure 3 â€£ III-C Decoder â€£ III Spatio-temporal radar object detector â€£ A recurrent CNN for online object detection on raw radar frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>b. We concatenate <math id="S3.SS4.p2.3.m3.1" class="ltx_Math" alttext="H_{k}" display="inline"><semantics id="S3.SS4.p2.3.m3.1a"><msub id="S3.SS4.p2.3.m3.1.1" xref="S3.SS4.p2.3.m3.1.1.cmml"><mi id="S3.SS4.p2.3.m3.1.1.2" xref="S3.SS4.p2.3.m3.1.1.2.cmml">H</mi><mi id="S3.SS4.p2.3.m3.1.1.3" xref="S3.SS4.p2.3.m3.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.3.m3.1b"><apply id="S3.SS4.p2.3.m3.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.3.m3.1.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS4.p2.3.m3.1.1.2.cmml" xref="S3.SS4.p2.3.m3.1.1.2">ğ»</ci><ci id="S3.SS4.p2.3.m3.1.1.3.cmml" xref="S3.SS4.p2.3.m3.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.3.m3.1c">H_{k}</annotation></semantics></math> in the same way as in the single view approach. We call this operation Temporal Multi-View Skip Connections (TMVSC). Figure <a href="#S3.F3" title="Figure 3 â€£ III-C Decoder â€£ III Spatio-temporal radar object detector â€£ A recurrent CNN for online object detection on raw radar frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the multi-view architecture we propose.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Related work</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Sequential object detection and segmentation in computer vision</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Object detection and segmentation are fundamental problems in computer vision. However, the majority of object detection and segmentation algorithms have been developed on static images. For some applications (robotics, autonomous driving, earth observation), processing sequences of images is desirable. Due to motion blur or object occlusion, it is sub-optimal to directly apply classical object detectors, or segmentation algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> on successive frames. To exploit the temporal information in sequences, optical flows <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, recurrent networks with or without convolutions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, aggregation methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> or convolutions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> were widely used.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, authors propose to transform the SSD object detector in a recurrent model by adding ConvLSTM cells on the top of the regression and classification head for <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and between the last feature map of the feature extractor and the detection head for <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. However, these models predict bounding boxes (not segmentation masks) and learn to find temporal relationships only between feature maps. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, authors present a recurrent model for one-shot and zero-shot video object instance segmentation. Contrary to previous methods and ours, they use a fully recurrent decoder composed of upsampling ConvLSTM layers to predict instance segmentation masks. Another approach proposed by Sainte Fare Garnot and Landrieu in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> consists in using temporal self-attention to extract multi-scale spatio-temporal features for panoptic segmentation of satellite image time series.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Despite some models being trained online (<span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_italic">i.e.</span> no access to future frames during training), <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, some models using a sequence of images are non-causal in inference and use the video in its entirety. Thus, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> propose a causal recurrent flow-based method for online video object detection. Their method only uses the past and the current frame from a memory buffer and learns short-term temporal context using optical flow and long-term temporal context using a ConvLSTM. Nevertheless, this method needs to learn optical flow to get accurate results, which is not possible in radar.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Sequential object detection and segmentation in radar</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.3" class="ltx_p">In radar, object detection or segmentation algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> that do not use time suffer from low performances for similar classes such as pedestrians and bicyclists <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. According to the Doppler principle, motion information is held in the radar signal and should help to differentiate a pedestrian (non-rigid body, motion information widely distributed), and a car (rigid body, more consistent motion information) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. 3D convolutions are primarily used in radar to learn spatio-temporal dependencies between frames. Methods such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> adopt 3D encoder-decoder architectures where they predict the position of objects for <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">N</annotation></semantics></math> successive frames. Despite their performances, these methods require a buffer of <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mi id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><ci id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">N</annotation></semantics></math> frames in memory to work and are not really online methods in inference, as the convolutional kernel is applied over past and future frames. On the contrary, our approach doesnâ€™t access future frames either in training or inference. Additionally, the number of parameters of models using 3D convolutions is huge for real-time applications (34.5M for RODNet-CDC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, 104M for RAMP-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>). Consequently, Ju <span id="S4.SS2.p1.3.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> introduced Dimension Apart Module (DAM), a lightweight module for spatio-temporal feature extraction on RA maps that can be integrated into U-Net style network architecture. Alternatively, Ouaknine <span id="S4.SS2.p1.3.2" class="ltx_text ltx_font_italic">et al.</span> propose in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> to use 3D convolutions to encode the spatial information of the <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mi id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><ci id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">N</annotation></semantics></math> past frames in an online setting. Similarly to other 3D convolutions-based methods, TMVA-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> has a lot of parameters compared to our approach.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Kaul <span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> propose a model without 3D convolutions where the time information is stacked in the channel dimension. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> aggregate point clouds of different time steps to increase the resolution of the radar point cloud. More recently, Liu <span id="S4.SS2.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> propose an approach inspired by the transformer architecture and based on computer vision-based feature extractors to exploit temporal dependencies between objects in two successive frames. Finally, Major <span id="S4.SS2.p2.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> follow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> by using a ConvLSTM over the features of a multi-view convolutional encoder. Even though this model is similar to ours, the LSTM cell is applied only to the learned cartesian output before the detection head, and the proposed model only detects cars. Additionally, this model produces bounding boxes, which is not very accurate for radar, is not end-to-end trainable and requires pre-training of a non-recurrent version of it before.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.5.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.6.2" class="ltx_text ltx_font_italic">Single view object detection</span>
</h3>

<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Results obtained on the test set of the ROD2021 challenge for different driving scenarios (PL: Parking Lot, CR: Campus Road, CS: City Street and HW: Highway). We report the best results over five different seeds. The best results are in bold, and the second bests are underlined.</figcaption>
<div id="S5.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:91.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-125.2pt,26.4pt) scale(0.633907737297372,0.633907737297372) ;">
<table id="S5.T1.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T1.1.1.1.1" class="ltx_tr">
<td id="S5.T1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S5.T1.1.1.1.1.1.1" class="ltx_text">Model</span></td>
<td id="S5.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="5">AP</td>
<td id="S5.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="5">AR</td>
<td id="S5.T1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S5.T1.1.1.1.1.4.1" class="ltx_text">Params (M)</span></td>
<td id="S5.T1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S5.T1.1.1.1.1.5.1" class="ltx_text">GMACS</span></td>
<td id="S5.T1.1.1.1.1.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" rowspan="2"><span id="S5.T1.1.1.1.1.6.1" class="ltx_text">Runtime (ms)</span></td>
</tr>
<tr id="S5.T1.1.1.2.2" class="ltx_tr">
<td id="S5.T1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t">Mean</td>
<td id="S5.T1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">PL</td>
<td id="S5.T1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">CR</td>
<td id="S5.T1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">CS</td>
<td id="S5.T1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">HW</td>
<td id="S5.T1.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t">Mean</td>
<td id="S5.T1.1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t">PL</td>
<td id="S5.T1.1.1.2.2.8" class="ltx_td ltx_align_center ltx_border_t">CR</td>
<td id="S5.T1.1.1.2.2.9" class="ltx_td ltx_align_center ltx_border_t">CS</td>
<td id="S5.T1.1.1.2.2.10" class="ltx_td ltx_align_center ltx_border_t">HW</td>
</tr>
<tr id="S5.T1.1.1.3.3" class="ltx_tr">
<td id="S5.T1.1.1.3.3.1" class="ltx_td ltx_align_left ltx_border_t">RECORD (buffer, ours)</td>
<td id="S5.T1.1.1.3.3.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S5.T1.1.1.3.3.2.1" class="ltx_text ltx_framed ltx_framed_underline">7</span>2.8</td>
<td id="S5.T1.1.1.3.3.3" class="ltx_td ltx_align_left ltx_border_t">
<span id="S5.T1.1.1.3.3.3.1" class="ltx_text ltx_framed ltx_framed_underline">9</span>5.0</td>
<td id="S5.T1.1.1.3.3.4" class="ltx_td ltx_align_left ltx_border_t">
<span id="S5.T1.1.1.3.3.4.1" class="ltx_text ltx_framed ltx_framed_underline">6</span>7.7</td>
<td id="S5.T1.1.1.3.3.5" class="ltx_td ltx_align_left ltx_border_t">48.3</td>
<td id="S5.T1.1.1.3.3.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T1.1.1.3.3.6.1" class="ltx_text ltx_font_bold">77.4</span></td>
<td id="S5.T1.1.1.3.3.7" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T1.1.1.3.3.7.1" class="ltx_text ltx_font_bold">82.8</span></td>
<td id="S5.T1.1.1.3.3.8" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T1.1.1.3.3.8.1" class="ltx_text ltx_font_bold">96.7</span></td>
<td id="S5.T1.1.1.3.3.9" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T1.1.1.3.3.9.1" class="ltx_text ltx_font_bold">73.9</span></td>
<td id="S5.T1.1.1.3.3.10" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T1.1.1.3.3.10.1" class="ltx_text ltx_font_bold">72.8</span></td>
<td id="S5.T1.1.1.3.3.11" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T1.1.1.3.3.11.1" class="ltx_text ltx_font_bold">81.7</span></td>
<td id="S5.T1.1.1.3.3.12" class="ltx_td ltx_align_center ltx_border_t">0.69</td>
<td id="S5.T1.1.1.3.3.13" class="ltx_td ltx_align_center ltx_border_t">5.0</td>
<td id="S5.T1.1.1.3.3.14" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">61.6</td>
</tr>
<tr id="S5.T1.1.1.4.4" class="ltx_tr">
<td id="S5.T1.1.1.4.4.1" class="ltx_td ltx_align_left">RECORD (online, ours)</td>
<td id="S5.T1.1.1.4.4.2" class="ltx_td ltx_align_left"><span id="S5.T1.1.1.4.4.2.1" class="ltx_text ltx_font_bold">73.5</span></td>
<td id="S5.T1.1.1.4.4.3" class="ltx_td ltx_align_left"><span id="S5.T1.1.1.4.4.3.1" class="ltx_text ltx_font_bold">96.4</span></td>
<td id="S5.T1.1.1.4.4.4" class="ltx_td ltx_align_left"><span id="S5.T1.1.1.4.4.4.1" class="ltx_text ltx_font_bold">72.5</span></td>
<td id="S5.T1.1.1.4.4.5" class="ltx_td ltx_align_left">
<span id="S5.T1.1.1.4.4.5.1" class="ltx_text ltx_framed ltx_framed_underline">4</span>9.9</td>
<td id="S5.T1.1.1.4.4.6" class="ltx_td ltx_align_left">
<span id="S5.T1.1.1.4.4.6.1" class="ltx_text ltx_framed ltx_framed_underline">7</span>2.5</td>
<td id="S5.T1.1.1.4.4.7" class="ltx_td ltx_align_left">
<span id="S5.T1.1.1.4.4.7.1" class="ltx_text ltx_framed ltx_framed_underline">8</span>1.2</td>
<td id="S5.T1.1.1.4.4.8" class="ltx_td ltx_align_left">
<span id="S5.T1.1.1.4.4.8.1" class="ltx_text ltx_framed ltx_framed_underline">9</span>6.4</td>
<td id="S5.T1.1.1.4.4.9" class="ltx_td ltx_align_left">
<span id="S5.T1.1.1.4.4.9.1" class="ltx_text ltx_framed ltx_framed_underline">7</span>8.1</td>
<td id="S5.T1.1.1.4.4.10" class="ltx_td ltx_align_left">68.8</td>
<td id="S5.T1.1.1.4.4.11" class="ltx_td ltx_align_left">
<span id="S5.T1.1.1.4.4.11.1" class="ltx_text ltx_framed ltx_framed_underline">7</span>7.6</td>
<td id="S5.T1.1.1.4.4.12" class="ltx_td ltx_align_center">0.69</td>
<td id="S5.T1.1.1.4.4.13" class="ltx_td ltx_align_center">0.95</td>
<td id="S5.T1.1.1.4.4.14" class="ltx_td ltx_nopad_r ltx_align_center">6.2</td>
</tr>
<tr id="S5.T1.1.1.5.5" class="ltx_tr">
<td id="S5.T1.1.1.5.5.1" class="ltx_td ltx_align_left">RECORD (no lstm, multi)</td>
<td id="S5.T1.1.1.5.5.2" class="ltx_td ltx_align_left">65.5</td>
<td id="S5.T1.1.1.5.5.3" class="ltx_td ltx_align_left">89.9</td>
<td id="S5.T1.1.1.5.5.4" class="ltx_td ltx_align_left">57.3</td>
<td id="S5.T1.1.1.5.5.5" class="ltx_td ltx_align_left">43.1</td>
<td id="S5.T1.1.1.5.5.6" class="ltx_td ltx_align_left">68.9</td>
<td id="S5.T1.1.1.5.5.7" class="ltx_td ltx_align_left">78.9</td>
<td id="S5.T1.1.1.5.5.8" class="ltx_td ltx_align_left">93.1</td>
<td id="S5.T1.1.1.5.5.9" class="ltx_td ltx_align_left">68.2</td>
<td id="S5.T1.1.1.5.5.10" class="ltx_td ltx_align_left">71.5</td>
<td id="S5.T1.1.1.5.5.11" class="ltx_td ltx_align_left">75.7</td>
<td id="S5.T1.1.1.5.5.12" class="ltx_td ltx_align_center">0.47</td>
<td id="S5.T1.1.1.5.5.13" class="ltx_td ltx_align_center">0.76</td>
<td id="S5.T1.1.1.5.5.14" class="ltx_td ltx_nopad_r ltx_align_center">5.8</td>
</tr>
<tr id="S5.T1.1.1.6.6" class="ltx_tr">
<td id="S5.T1.1.1.6.6.1" class="ltx_td ltx_align_left">RECORD (no lstm, single)</td>
<td id="S5.T1.1.1.6.6.2" class="ltx_td ltx_align_left">59.5</td>
<td id="S5.T1.1.1.6.6.3" class="ltx_td ltx_align_left">85.7</td>
<td id="S5.T1.1.1.6.6.4" class="ltx_td ltx_align_left">48.5</td>
<td id="S5.T1.1.1.6.6.5" class="ltx_td ltx_align_left">39.11</td>
<td id="S5.T1.1.1.6.6.6" class="ltx_td ltx_align_left">64.4</td>
<td id="S5.T1.1.1.6.6.7" class="ltx_td ltx_align_left">75.1</td>
<td id="S5.T1.1.1.6.6.8" class="ltx_td ltx_align_left">90.8</td>
<td id="S5.T1.1.1.6.6.9" class="ltx_td ltx_align_left">62.4</td>
<td id="S5.T1.1.1.6.6.10" class="ltx_td ltx_align_left">68.9</td>
<td id="S5.T1.1.1.6.6.11" class="ltx_td ltx_align_left">69.6</td>
<td id="S5.T1.1.1.6.6.12" class="ltx_td ltx_align_center">0.44</td>
<td id="S5.T1.1.1.6.6.13" class="ltx_td ltx_align_center">0.35</td>
<td id="S5.T1.1.1.6.6.14" class="ltx_td ltx_nopad_r ltx_align_center">5.7</td>
</tr>
<tr id="S5.T1.1.1.7.7" class="ltx_tr">
<td id="S5.T1.1.1.7.7.1" class="ltx_td ltx_align_left">DANet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>
</td>
<td id="S5.T1.1.1.7.7.2" class="ltx_td ltx_align_left">71.9</td>
<td id="S5.T1.1.1.7.7.3" class="ltx_td ltx_align_left">94.7</td>
<td id="S5.T1.1.1.7.7.4" class="ltx_td ltx_align_left">65.7</td>
<td id="S5.T1.1.1.7.7.5" class="ltx_td ltx_align_left"><span id="S5.T1.1.1.7.7.5.1" class="ltx_text ltx_font_bold">51.9</span></td>
<td id="S5.T1.1.1.7.7.6" class="ltx_td ltx_align_left">70.0</td>
<td id="S5.T1.1.1.7.7.7" class="ltx_td ltx_align_left">80.7</td>
<td id="S5.T1.1.1.7.7.8" class="ltx_td ltx_align_left">96.2</td>
<td id="S5.T1.1.1.7.7.9" class="ltx_td ltx_align_left">75.1</td>
<td id="S5.T1.1.1.7.7.10" class="ltx_td ltx_align_left">
<span id="S5.T1.1.1.7.7.10.1" class="ltx_text ltx_framed ltx_framed_underline">7</span>2.8</td>
<td id="S5.T1.1.1.7.7.11" class="ltx_td ltx_align_left">73.0</td>
<td id="S5.T1.1.1.7.7.12" class="ltx_td ltx_align_center">0.74</td>
<td id="S5.T1.1.1.7.7.13" class="ltx_td ltx_align_center">9.1</td>
<td id="S5.T1.1.1.7.7.14" class="ltx_td ltx_nopad_r ltx_align_center">21.1</td>
</tr>
<tr id="S5.T1.1.1.8.8" class="ltx_tr">
<td id="S5.T1.1.1.8.8.1" class="ltx_td ltx_align_left ltx_border_bb">UTAE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>
</td>
<td id="S5.T1.1.1.8.8.2" class="ltx_td ltx_align_left ltx_border_bb">68.4</td>
<td id="S5.T1.1.1.8.8.3" class="ltx_td ltx_align_left ltx_border_bb">92.1</td>
<td id="S5.T1.1.1.8.8.4" class="ltx_td ltx_align_left ltx_border_bb">67.4</td>
<td id="S5.T1.1.1.8.8.5" class="ltx_td ltx_align_left ltx_border_bb">51.4</td>
<td id="S5.T1.1.1.8.8.6" class="ltx_td ltx_align_left ltx_border_bb">65.5</td>
<td id="S5.T1.1.1.8.8.7" class="ltx_td ltx_align_left ltx_border_bb">78.4</td>
<td id="S5.T1.1.1.8.8.8" class="ltx_td ltx_align_left ltx_border_bb">94.6</td>
<td id="S5.T1.1.1.8.8.9" class="ltx_td ltx_align_left ltx_border_bb">74.0</td>
<td id="S5.T1.1.1.8.8.10" class="ltx_td ltx_align_left ltx_border_bb">69.7</td>
<td id="S5.T1.1.1.8.8.11" class="ltx_td ltx_align_left ltx_border_bb">70.0</td>
<td id="S5.T1.1.1.8.8.12" class="ltx_td ltx_align_center ltx_border_bb">0.79</td>
<td id="S5.T1.1.1.8.8.13" class="ltx_td ltx_align_center ltx_border_bb">4.1</td>
<td id="S5.T1.1.1.8.8.14" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">7.5</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.p1.1" class="ltx_p"><span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_bold">Dataset.</span> We prototype and train RECORD on the ROD2021 challenge dataset<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://www.cruwdataset.org/rod2021</span></span></span>, a subset of the CRUW dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Due to its high frame rate (30 fps), this dataset is well-suited to evaluate the temporal models. The ROD2021 dataset contains 50 sequences (40 for training and 10 for testing) of synchronised cameras and raw radar frames. Each sequence contains around 800-1700 frames in four different driving scenarios, <span id="S5.SS1.p1.1.2" class="ltx_text ltx_font_italic">i.e.</span>, parking lot (PL), campus road (CR), city street (CS), and highway (HW).</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.3" class="ltx_p">The provided data of the ROD2021 challenge dataset are pre-processed sequences of RA spectra (or maps). Annotations are confidence maps (ConfMaps) in range-azimuth coordinates that represent object locations (see figure <a href="#S2.F2" title="Figure 2 â€£ II Radar background â€£ A recurrent CNN for online object detection on raw radar frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). According to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> one set of ConfMaps has multiple channels, each representing one specific class label, <span id="S5.SS1.p2.3.1" class="ltx_text ltx_font_italic">i.e.</span>, car, pedestrian, and cyclist. The pixel value in the <math id="S5.SS1.p2.1.m1.1" class="ltx_Math" alttext="cls" display="inline"><semantics id="S5.SS1.p2.1.m1.1a"><mrow id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml"><mi id="S5.SS1.p2.1.m1.1.1.2" xref="S5.SS1.p2.1.m1.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p2.1.m1.1.1.1" xref="S5.SS1.p2.1.m1.1.1.1.cmml">â€‹</mo><mi id="S5.SS1.p2.1.m1.1.1.3" xref="S5.SS1.p2.1.m1.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p2.1.m1.1.1.1a" xref="S5.SS1.p2.1.m1.1.1.1.cmml">â€‹</mo><mi id="S5.SS1.p2.1.m1.1.1.4" xref="S5.SS1.p2.1.m1.1.1.4.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><apply id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1"><times id="S5.SS1.p2.1.m1.1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1.1"></times><ci id="S5.SS1.p2.1.m1.1.1.2.cmml" xref="S5.SS1.p2.1.m1.1.1.2">ğ‘</ci><ci id="S5.SS1.p2.1.m1.1.1.3.cmml" xref="S5.SS1.p2.1.m1.1.1.3">ğ‘™</ci><ci id="S5.SS1.p2.1.m1.1.1.4.cmml" xref="S5.SS1.p2.1.m1.1.1.4">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">cls</annotation></semantics></math>-th channel represents the probability of an object with class <math id="S5.SS1.p2.2.m2.1" class="ltx_Math" alttext="cls" display="inline"><semantics id="S5.SS1.p2.2.m2.1a"><mrow id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml"><mi id="S5.SS1.p2.2.m2.1.1.2" xref="S5.SS1.p2.2.m2.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p2.2.m2.1.1.1" xref="S5.SS1.p2.2.m2.1.1.1.cmml">â€‹</mo><mi id="S5.SS1.p2.2.m2.1.1.3" xref="S5.SS1.p2.2.m2.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p2.2.m2.1.1.1a" xref="S5.SS1.p2.2.m2.1.1.1.cmml">â€‹</mo><mi id="S5.SS1.p2.2.m2.1.1.4" xref="S5.SS1.p2.2.m2.1.1.4.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><apply id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1"><times id="S5.SS1.p2.2.m2.1.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1.1"></times><ci id="S5.SS1.p2.2.m2.1.1.2.cmml" xref="S5.SS1.p2.2.m2.1.1.2">ğ‘</ci><ci id="S5.SS1.p2.2.m2.1.1.3.cmml" xref="S5.SS1.p2.2.m2.1.1.3">ğ‘™</ci><ci id="S5.SS1.p2.2.m2.1.1.4.cmml" xref="S5.SS1.p2.2.m2.1.1.4">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">cls</annotation></semantics></math> occurring at that range-azimuth location. We refer the reader to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> for more information about ConfMaps generation and post-processing. RA spectra and ConfMaps have dimensions <math id="S5.SS1.p2.3.m3.1" class="ltx_Math" alttext="\text{128}\times\text{128}" display="inline"><semantics id="S5.SS1.p2.3.m3.1a"><mrow id="S5.SS1.p2.3.m3.1.1" xref="S5.SS1.p2.3.m3.1.1.cmml"><mtext id="S5.SS1.p2.3.m3.1.1.2" xref="S5.SS1.p2.3.m3.1.1.2a.cmml">128</mtext><mo lspace="0.222em" rspace="0.222em" id="S5.SS1.p2.3.m3.1.1.1" xref="S5.SS1.p2.3.m3.1.1.1.cmml">Ã—</mo><mtext id="S5.SS1.p2.3.m3.1.1.3" xref="S5.SS1.p2.3.m3.1.1.3a.cmml">128</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.3.m3.1b"><apply id="S5.SS1.p2.3.m3.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1"><times id="S5.SS1.p2.3.m3.1.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1.1"></times><ci id="S5.SS1.p2.3.m3.1.1.2a.cmml" xref="S5.SS1.p2.3.m3.1.1.2"><mtext id="S5.SS1.p2.3.m3.1.1.2.cmml" xref="S5.SS1.p2.3.m3.1.1.2">128</mtext></ci><ci id="S5.SS1.p2.3.m3.1.1.3a.cmml" xref="S5.SS1.p2.3.m3.1.1.3"><mtext id="S5.SS1.p2.3.m3.1.1.3.cmml" xref="S5.SS1.p2.3.m3.1.1.3">128</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.3.m3.1c">\text{128}\times\text{128}</annotation></semantics></math>.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.5" class="ltx_p"><span id="S5.SS1.p3.5.1" class="ltx_text ltx_font_bold">Evaluation metrics.</span> We use the metric proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> to evaluate the models on the ROD2021 challenge datasets. In image-based object detection, intersection over union (IoU) is mostly used to estimate how close the prediction and the ground truth (GT) are. For our single-view approach, as we predict the location of objects, we utilise the object location similarity (OLS) to match detection and GT. The OLS is defined as:</p>
<table id="S5.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E4.m1.1" class="ltx_Math" alttext="OLS=\exp{\frac{-d^{2}}{2(s\kappa_{cls})^{2}}}" display="block"><semantics id="S5.E4.m1.1a"><mrow id="S5.E4.m1.1.2" xref="S5.E4.m1.1.2.cmml"><mrow id="S5.E4.m1.1.2.2" xref="S5.E4.m1.1.2.2.cmml"><mi id="S5.E4.m1.1.2.2.2" xref="S5.E4.m1.1.2.2.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.1.2.2.1" xref="S5.E4.m1.1.2.2.1.cmml">â€‹</mo><mi id="S5.E4.m1.1.2.2.3" xref="S5.E4.m1.1.2.2.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.1.2.2.1a" xref="S5.E4.m1.1.2.2.1.cmml">â€‹</mo><mi id="S5.E4.m1.1.2.2.4" xref="S5.E4.m1.1.2.2.4.cmml">S</mi></mrow><mo id="S5.E4.m1.1.2.1" xref="S5.E4.m1.1.2.1.cmml">=</mo><mrow id="S5.E4.m1.1.2.3" xref="S5.E4.m1.1.2.3.cmml"><mi id="S5.E4.m1.1.2.3.1" xref="S5.E4.m1.1.2.3.1.cmml">exp</mi><mo lspace="0.167em" id="S5.E4.m1.1.2.3a" xref="S5.E4.m1.1.2.3.cmml">â¡</mo><mfrac id="S5.E4.m1.1.1" xref="S5.E4.m1.1.1.cmml"><mrow id="S5.E4.m1.1.1.3" xref="S5.E4.m1.1.1.3.cmml"><mo id="S5.E4.m1.1.1.3a" xref="S5.E4.m1.1.1.3.cmml">âˆ’</mo><msup id="S5.E4.m1.1.1.3.2" xref="S5.E4.m1.1.1.3.2.cmml"><mi id="S5.E4.m1.1.1.3.2.2" xref="S5.E4.m1.1.1.3.2.2.cmml">d</mi><mn id="S5.E4.m1.1.1.3.2.3" xref="S5.E4.m1.1.1.3.2.3.cmml">2</mn></msup></mrow><mrow id="S5.E4.m1.1.1.1" xref="S5.E4.m1.1.1.1.cmml"><mn id="S5.E4.m1.1.1.1.3" xref="S5.E4.m1.1.1.1.3.cmml">2</mn><mo lspace="0em" rspace="0em" id="S5.E4.m1.1.1.1.2" xref="S5.E4.m1.1.1.1.2.cmml">â€‹</mo><msup id="S5.E4.m1.1.1.1.1" xref="S5.E4.m1.1.1.1.1.cmml"><mrow id="S5.E4.m1.1.1.1.1.1.1" xref="S5.E4.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.E4.m1.1.1.1.1.1.1.2" xref="S5.E4.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.E4.m1.1.1.1.1.1.1.1" xref="S5.E4.m1.1.1.1.1.1.1.1.cmml"><mi id="S5.E4.m1.1.1.1.1.1.1.1.2" xref="S5.E4.m1.1.1.1.1.1.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.1.1.1.1.1.1.1.1" xref="S5.E4.m1.1.1.1.1.1.1.1.1.cmml">â€‹</mo><msub id="S5.E4.m1.1.1.1.1.1.1.1.3" xref="S5.E4.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S5.E4.m1.1.1.1.1.1.1.1.3.2" xref="S5.E4.m1.1.1.1.1.1.1.1.3.2.cmml">Îº</mi><mrow id="S5.E4.m1.1.1.1.1.1.1.1.3.3" xref="S5.E4.m1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S5.E4.m1.1.1.1.1.1.1.1.3.3.2" xref="S5.E4.m1.1.1.1.1.1.1.1.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.1.1.1.1.1.1.1.3.3.1" xref="S5.E4.m1.1.1.1.1.1.1.1.3.3.1.cmml">â€‹</mo><mi id="S5.E4.m1.1.1.1.1.1.1.1.3.3.3" xref="S5.E4.m1.1.1.1.1.1.1.1.3.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.E4.m1.1.1.1.1.1.1.1.3.3.1a" xref="S5.E4.m1.1.1.1.1.1.1.1.3.3.1.cmml">â€‹</mo><mi id="S5.E4.m1.1.1.1.1.1.1.1.3.3.4" xref="S5.E4.m1.1.1.1.1.1.1.1.3.3.4.cmml">s</mi></mrow></msub></mrow><mo stretchy="false" id="S5.E4.m1.1.1.1.1.1.1.3" xref="S5.E4.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S5.E4.m1.1.1.1.1.3" xref="S5.E4.m1.1.1.1.1.3.cmml">2</mn></msup></mrow></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E4.m1.1b"><apply id="S5.E4.m1.1.2.cmml" xref="S5.E4.m1.1.2"><eq id="S5.E4.m1.1.2.1.cmml" xref="S5.E4.m1.1.2.1"></eq><apply id="S5.E4.m1.1.2.2.cmml" xref="S5.E4.m1.1.2.2"><times id="S5.E4.m1.1.2.2.1.cmml" xref="S5.E4.m1.1.2.2.1"></times><ci id="S5.E4.m1.1.2.2.2.cmml" xref="S5.E4.m1.1.2.2.2">ğ‘‚</ci><ci id="S5.E4.m1.1.2.2.3.cmml" xref="S5.E4.m1.1.2.2.3">ğ¿</ci><ci id="S5.E4.m1.1.2.2.4.cmml" xref="S5.E4.m1.1.2.2.4">ğ‘†</ci></apply><apply id="S5.E4.m1.1.2.3.cmml" xref="S5.E4.m1.1.2.3"><exp id="S5.E4.m1.1.2.3.1.cmml" xref="S5.E4.m1.1.2.3.1"></exp><apply id="S5.E4.m1.1.1.cmml" xref="S5.E4.m1.1.1"><divide id="S5.E4.m1.1.1.2.cmml" xref="S5.E4.m1.1.1"></divide><apply id="S5.E4.m1.1.1.3.cmml" xref="S5.E4.m1.1.1.3"><minus id="S5.E4.m1.1.1.3.1.cmml" xref="S5.E4.m1.1.1.3"></minus><apply id="S5.E4.m1.1.1.3.2.cmml" xref="S5.E4.m1.1.1.3.2"><csymbol cd="ambiguous" id="S5.E4.m1.1.1.3.2.1.cmml" xref="S5.E4.m1.1.1.3.2">superscript</csymbol><ci id="S5.E4.m1.1.1.3.2.2.cmml" xref="S5.E4.m1.1.1.3.2.2">ğ‘‘</ci><cn type="integer" id="S5.E4.m1.1.1.3.2.3.cmml" xref="S5.E4.m1.1.1.3.2.3">2</cn></apply></apply><apply id="S5.E4.m1.1.1.1.cmml" xref="S5.E4.m1.1.1.1"><times id="S5.E4.m1.1.1.1.2.cmml" xref="S5.E4.m1.1.1.1.2"></times><cn type="integer" id="S5.E4.m1.1.1.1.3.cmml" xref="S5.E4.m1.1.1.1.3">2</cn><apply id="S5.E4.m1.1.1.1.1.cmml" xref="S5.E4.m1.1.1.1.1"><csymbol cd="ambiguous" id="S5.E4.m1.1.1.1.1.2.cmml" xref="S5.E4.m1.1.1.1.1">superscript</csymbol><apply id="S5.E4.m1.1.1.1.1.1.1.1.cmml" xref="S5.E4.m1.1.1.1.1.1.1"><times id="S5.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="S5.E4.m1.1.1.1.1.1.1.1.1"></times><ci id="S5.E4.m1.1.1.1.1.1.1.1.2.cmml" xref="S5.E4.m1.1.1.1.1.1.1.1.2">ğ‘ </ci><apply id="S5.E4.m1.1.1.1.1.1.1.1.3.cmml" xref="S5.E4.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E4.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S5.E4.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S5.E4.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S5.E4.m1.1.1.1.1.1.1.1.3.2">ğœ…</ci><apply id="S5.E4.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S5.E4.m1.1.1.1.1.1.1.1.3.3"><times id="S5.E4.m1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S5.E4.m1.1.1.1.1.1.1.1.3.3.1"></times><ci id="S5.E4.m1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S5.E4.m1.1.1.1.1.1.1.1.3.3.2">ğ‘</ci><ci id="S5.E4.m1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S5.E4.m1.1.1.1.1.1.1.1.3.3.3">ğ‘™</ci><ci id="S5.E4.m1.1.1.1.1.1.1.1.3.3.4.cmml" xref="S5.E4.m1.1.1.1.1.1.1.1.3.3.4">ğ‘ </ci></apply></apply></apply><cn type="integer" id="S5.E4.m1.1.1.1.1.3.cmml" xref="S5.E4.m1.1.1.1.1.3">2</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E4.m1.1c">OLS=\exp{\frac{-d^{2}}{2(s\kappa_{cls})^{2}}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S5.SS1.p3.4" class="ltx_p">where <math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><mi id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><ci id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">d</annotation></semantics></math> is the distance (in meters) between the two points in the RA spectrum, <math id="S5.SS1.p3.2.m2.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S5.SS1.p3.2.m2.1a"><mi id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.1b"><ci id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.1c">s</annotation></semantics></math> is the object distance from the radar sensor (representing object scale information) and <math id="S5.SS1.p3.3.m3.1" class="ltx_Math" alttext="\kappa_{cls}" display="inline"><semantics id="S5.SS1.p3.3.m3.1a"><msub id="S5.SS1.p3.3.m3.1.1" xref="S5.SS1.p3.3.m3.1.1.cmml"><mi id="S5.SS1.p3.3.m3.1.1.2" xref="S5.SS1.p3.3.m3.1.1.2.cmml">Îº</mi><mrow id="S5.SS1.p3.3.m3.1.1.3" xref="S5.SS1.p3.3.m3.1.1.3.cmml"><mi id="S5.SS1.p3.3.m3.1.1.3.2" xref="S5.SS1.p3.3.m3.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p3.3.m3.1.1.3.1" xref="S5.SS1.p3.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S5.SS1.p3.3.m3.1.1.3.3" xref="S5.SS1.p3.3.m3.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p3.3.m3.1.1.3.1a" xref="S5.SS1.p3.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S5.SS1.p3.3.m3.1.1.3.4" xref="S5.SS1.p3.3.m3.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.3.m3.1b"><apply id="S5.SS1.p3.3.m3.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS1.p3.3.m3.1.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1">subscript</csymbol><ci id="S5.SS1.p3.3.m3.1.1.2.cmml" xref="S5.SS1.p3.3.m3.1.1.2">ğœ…</ci><apply id="S5.SS1.p3.3.m3.1.1.3.cmml" xref="S5.SS1.p3.3.m3.1.1.3"><times id="S5.SS1.p3.3.m3.1.1.3.1.cmml" xref="S5.SS1.p3.3.m3.1.1.3.1"></times><ci id="S5.SS1.p3.3.m3.1.1.3.2.cmml" xref="S5.SS1.p3.3.m3.1.1.3.2">ğ‘</ci><ci id="S5.SS1.p3.3.m3.1.1.3.3.cmml" xref="S5.SS1.p3.3.m3.1.1.3.3">ğ‘™</ci><ci id="S5.SS1.p3.3.m3.1.1.3.4.cmml" xref="S5.SS1.p3.3.m3.1.1.3.4">ğ‘ </ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.3.m3.1c">\kappa_{cls}</annotation></semantics></math> is a per-class constant that describes the error tolerance for class <math id="S5.SS1.p3.4.m4.1" class="ltx_Math" alttext="cls" display="inline"><semantics id="S5.SS1.p3.4.m4.1a"><mrow id="S5.SS1.p3.4.m4.1.1" xref="S5.SS1.p3.4.m4.1.1.cmml"><mi id="S5.SS1.p3.4.m4.1.1.2" xref="S5.SS1.p3.4.m4.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p3.4.m4.1.1.1" xref="S5.SS1.p3.4.m4.1.1.1.cmml">â€‹</mo><mi id="S5.SS1.p3.4.m4.1.1.3" xref="S5.SS1.p3.4.m4.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p3.4.m4.1.1.1a" xref="S5.SS1.p3.4.m4.1.1.1.cmml">â€‹</mo><mi id="S5.SS1.p3.4.m4.1.1.4" xref="S5.SS1.p3.4.m4.1.1.4.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.4.m4.1b"><apply id="S5.SS1.p3.4.m4.1.1.cmml" xref="S5.SS1.p3.4.m4.1.1"><times id="S5.SS1.p3.4.m4.1.1.1.cmml" xref="S5.SS1.p3.4.m4.1.1.1"></times><ci id="S5.SS1.p3.4.m4.1.1.2.cmml" xref="S5.SS1.p3.4.m4.1.1.2">ğ‘</ci><ci id="S5.SS1.p3.4.m4.1.1.3.cmml" xref="S5.SS1.p3.4.m4.1.1.3">ğ‘™</ci><ci id="S5.SS1.p3.4.m4.1.1.4.cmml" xref="S5.SS1.p3.4.m4.1.1.4">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.4.m4.1c">cls</annotation></semantics></math> (average object size of the corresponding class). First, OLS is computed between GT and detection. Then the average precision (AP) and the average recall (AR) are calculated using different OLS thresholds ranging from 0.5 to 0.9 with a step of 0.05, representing different localisation error tolerance for the detection results. In the rest of this section, AP and AR denote the average precision and recall for all the thresholds.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para ltx_noindent">
<p id="S5.SS1.p4.1" class="ltx_p"><span id="S5.SS1.p4.1.1" class="ltx_text ltx_font_bold">Competing methods.</span> We compare our approach with several radar-based and image-based methods using sequences of multiple radar frames. For the radar-based approach, we first benchmark our model against DANet<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The original implementation is not available so we implement it according to authorâ€™s guidelines. We do not use test augmentation and ensemble learning in this paper.</span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, a 3D convolutional model which won the ROD2021 challenge. Because image-based models are too heavy for our application, we finally contrast our recurrent approach against the attention-based model UTAE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, which is lighter than image-based approaches and which we can use causally. We found that decreasing the number of channels of UTAE and changing the positional encoding improved the performances (see tableÂ <a href="#S5.T4" title="TABLE IV â€£ V-A Single view object detection â€£ V Experiments â€£ A recurrent CNN for online object detection on raw radar frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>). We also consider two variants of our model without LSTMs, one using the time along the channel dimension (<span id="S5.SS1.p4.1.2" class="ltx_text ltx_font_italic">no lstm, multi)</span> and one using a single frame (<span id="S5.SS1.p4.1.3" class="ltx_text ltx_font_italic">no lstm, single)</span>.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para ltx_noindent">
<p id="S5.SS1.p5.2" class="ltx_p"><span id="S5.SS1.p5.2.1" class="ltx_text ltx_font_bold">Experimental settings.</span> In the ROD2021 dataset, annotations of test sequences are unavailable, so we select 36 sequences for training and 4 for validation from the training dataset. We evaluate all the models (including the baselines) on the ROD2021 evaluation platform using the same train/validation sets for a fair comparison. We use two evaluation settings for RECORD: online and buffer. Online stands for an online model in inference, where we never reset the hidden states. This is the most efficient approach (runtime and GMACS). We use sequences of 32 frames for the training. Buffer stands for a model which uses a buffer of 12 frames in training and inference. Here we reset the hidden states every 12 frames. We use this buffer approach for fair comparison with other baselines that also use a buffer, although it is less efficient than the online approach. We optimise our model using the Adam optimiser with learning rate (<math id="S5.SS1.p5.1.m1.1" class="ltx_Math" alttext="1\times 10^{-3}" display="inline"><semantics id="S5.SS1.p5.1.m1.1a"><mrow id="S5.SS1.p5.1.m1.1.1" xref="S5.SS1.p5.1.m1.1.1.cmml"><mn id="S5.SS1.p5.1.m1.1.1.2" xref="S5.SS1.p5.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS1.p5.1.m1.1.1.1" xref="S5.SS1.p5.1.m1.1.1.1.cmml">Ã—</mo><msup id="S5.SS1.p5.1.m1.1.1.3" xref="S5.SS1.p5.1.m1.1.1.3.cmml"><mn id="S5.SS1.p5.1.m1.1.1.3.2" xref="S5.SS1.p5.1.m1.1.1.3.2.cmml">10</mn><mrow id="S5.SS1.p5.1.m1.1.1.3.3" xref="S5.SS1.p5.1.m1.1.1.3.3.cmml"><mo id="S5.SS1.p5.1.m1.1.1.3.3a" xref="S5.SS1.p5.1.m1.1.1.3.3.cmml">âˆ’</mo><mn id="S5.SS1.p5.1.m1.1.1.3.3.2" xref="S5.SS1.p5.1.m1.1.1.3.3.2.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.1.m1.1b"><apply id="S5.SS1.p5.1.m1.1.1.cmml" xref="S5.SS1.p5.1.m1.1.1"><times id="S5.SS1.p5.1.m1.1.1.1.cmml" xref="S5.SS1.p5.1.m1.1.1.1"></times><cn type="integer" id="S5.SS1.p5.1.m1.1.1.2.cmml" xref="S5.SS1.p5.1.m1.1.1.2">1</cn><apply id="S5.SS1.p5.1.m1.1.1.3.cmml" xref="S5.SS1.p5.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p5.1.m1.1.1.3.1.cmml" xref="S5.SS1.p5.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S5.SS1.p5.1.m1.1.1.3.2.cmml" xref="S5.SS1.p5.1.m1.1.1.3.2">10</cn><apply id="S5.SS1.p5.1.m1.1.1.3.3.cmml" xref="S5.SS1.p5.1.m1.1.1.3.3"><minus id="S5.SS1.p5.1.m1.1.1.3.3.1.cmml" xref="S5.SS1.p5.1.m1.1.1.3.3"></minus><cn type="integer" id="S5.SS1.p5.1.m1.1.1.3.3.2.cmml" xref="S5.SS1.p5.1.m1.1.1.3.3.2">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.1.m1.1c">1\times 10^{-3}</annotation></semantics></math>) for the buffer method and <math id="S5.SS1.p5.2.m2.1" class="ltx_Math" alttext="3\times 10^{-4}" display="inline"><semantics id="S5.SS1.p5.2.m2.1a"><mrow id="S5.SS1.p5.2.m2.1.1" xref="S5.SS1.p5.2.m2.1.1.cmml"><mn id="S5.SS1.p5.2.m2.1.1.2" xref="S5.SS1.p5.2.m2.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS1.p5.2.m2.1.1.1" xref="S5.SS1.p5.2.m2.1.1.1.cmml">Ã—</mo><msup id="S5.SS1.p5.2.m2.1.1.3" xref="S5.SS1.p5.2.m2.1.1.3.cmml"><mn id="S5.SS1.p5.2.m2.1.1.3.2" xref="S5.SS1.p5.2.m2.1.1.3.2.cmml">10</mn><mrow id="S5.SS1.p5.2.m2.1.1.3.3" xref="S5.SS1.p5.2.m2.1.1.3.3.cmml"><mo id="S5.SS1.p5.2.m2.1.1.3.3a" xref="S5.SS1.p5.2.m2.1.1.3.3.cmml">âˆ’</mo><mn id="S5.SS1.p5.2.m2.1.1.3.3.2" xref="S5.SS1.p5.2.m2.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.2.m2.1b"><apply id="S5.SS1.p5.2.m2.1.1.cmml" xref="S5.SS1.p5.2.m2.1.1"><times id="S5.SS1.p5.2.m2.1.1.1.cmml" xref="S5.SS1.p5.2.m2.1.1.1"></times><cn type="integer" id="S5.SS1.p5.2.m2.1.1.2.cmml" xref="S5.SS1.p5.2.m2.1.1.2">3</cn><apply id="S5.SS1.p5.2.m2.1.1.3.cmml" xref="S5.SS1.p5.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p5.2.m2.1.1.3.1.cmml" xref="S5.SS1.p5.2.m2.1.1.3">superscript</csymbol><cn type="integer" id="S5.SS1.p5.2.m2.1.1.3.2.cmml" xref="S5.SS1.p5.2.m2.1.1.3.2">10</cn><apply id="S5.SS1.p5.2.m2.1.1.3.3.cmml" xref="S5.SS1.p5.2.m2.1.1.3.3"><minus id="S5.SS1.p5.2.m2.1.1.3.3.1.cmml" xref="S5.SS1.p5.2.m2.1.1.3.3"></minus><cn type="integer" id="S5.SS1.p5.2.m2.1.1.3.3.2.cmml" xref="S5.SS1.p5.2.m2.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.2.m2.1c">3\times 10^{-4}</annotation></semantics></math> for the online method. We decay the learning rate exponentially by a factor of 0.9 every ten epochs. We train all the models using a binary cross-entropy loss.</p>
</div>
<div id="S5.SS1.p6" class="ltx_para">
<p id="S5.SS1.p6.1" class="ltx_p">We use an early stopping strategy to stop training if the model does not improve for seven epochs. To avoid overfitting, we use a stride of four for the buffer model and eight for the online model (i.e., how many frames the model skips between each training iteration) in the training dataset. We apply different data augmentation techniques during training, such as horizontal, vertical and temporal flipping. We use these settings for all the baselines, except for DANet where we use the settings recommended by the authors. All the models were implemented using the Pytorch Lightning<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://www.pytorchlightning.ai/</span></span></span> framework and trained on an NVIDIA Quadro RTX 8000 GPU. We run all the models with five different seeds and report the best results in the next paragraph.</p>
</div>
<div id="S5.SS1.p7" class="ltx_para ltx_noindent">
<p id="S5.SS1.p7.1" class="ltx_p"><span id="S5.SS1.p7.1.1" class="ltx_text ltx_font_bold">Results.</span> TableÂ <a href="#S5.T1" title="TABLE I â€£ V-A Single view object detection â€£ V Experiments â€£ A recurrent CNN for online object detection on raw radar frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> presents the results of our model and the baselines on the test set of the ROD2021 dataset. Our recurrent approaches generally outperform baselines for both AP and AR metrics; this remains true for most scenarios. The online version of RECORD obtains the best trade-off between performances and computational complexity (parameters, number of multiplications and additions and runtime). Despite having less GMACs than UTAE and DANet, the buffer version of RECORD is the slowest one among all the models. Indeed, for each new frame we need to process the 11 previous ones, which is inefficient. Results show that the online version should be preferred for real-time applications. Additionally, RECORD methods exceed 3D and attention-based methods on static scenarios such as parking lot (PL) and campus road (CR). In PL and CR scenarios, the radar is static and the velocity of targets varies a lot, our recurrent models seem to learn variations of the targetâ€™s speed better than other approaches. Surprisingly the attention-based method UTAE, initially designed for the segmentation of satellite images, obtains very competitive results with our method and the DANet model. We notice that the approach using the time in the channel dimension reaches lower AP and AR than their counterpart, which explicitly uses time as a new dimension. Finally, training our model without the time and using only a 2D backbone (<span id="S5.SS1.p7.1.2" class="ltx_text ltx_font_italic">no lstm, single)</span> obtain the lower performance on the test set.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Comparison of different types of ConvRNN. We train all the models with the same loss and hyperparameters. Bottleneck LSTM achieves the best AP while having fewer parameters and GMACS.</figcaption>
<div id="S5.T2.6.6" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:72.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(1.1pt,-0.2pt) scale(1.00507309080386,1.00507309080386) ;">
<table id="S5.T2.6.6.6" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.6.6.6.7.1" class="ltx_tr">
<th id="S5.T2.6.6.6.7.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">ConvRNN type</th>
<th id="S5.T2.6.6.6.7.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">AP</th>
<th id="S5.T2.6.6.6.7.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">AR</th>
<th id="S5.T2.6.6.6.7.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Params (M)</th>
<th id="S5.T2.6.6.6.7.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">GMACS</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.2.2.2.2" class="ltx_tr">
<td id="S5.T2.2.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t">Bottleneck LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</td>
<td id="S5.T2.1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">69.8 <math id="S5.T2.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T2.1.1.1.1.1.1.m1.1a"><mo id="S5.T2.1.1.1.1.1.1.m1.1.1" xref="S5.T2.1.1.1.1.1.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T2.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.1.1.m1.1c">\pm</annotation></semantics></math> <span id="S5.T2.1.1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">2.2</span></span></td>
<td id="S5.T2.2.2.2.2.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S5.T2.2.2.2.2.2.1" class="ltx_text ltx_framed ltx_framed_underline">8</span>0.2 <math id="S5.T2.2.2.2.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T2.2.2.2.2.2.m1.1a"><mo id="S5.T2.2.2.2.2.2.m1.1.1" xref="S5.T2.2.2.2.2.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.2.2.m1.1b"><csymbol cd="latexml" id="S5.T2.2.2.2.2.2.m1.1.1.cmml" xref="S5.T2.2.2.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.2.2.m1.1c">\pm</annotation></semantics></math> <span id="S5.T2.2.2.2.2.2.2" class="ltx_text" style="font-size:90%;">1.5</span>
</td>
<td id="S5.T2.2.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t">0.69</td>
<td id="S5.T2.2.2.2.2.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">5.0</td>
</tr>
<tr id="S5.T2.4.4.4.4" class="ltx_tr">
<td id="S5.T2.4.4.4.4.3" class="ltx_td ltx_align_center">ConvLSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</td>
<td id="S5.T2.3.3.3.3.1" class="ltx_td ltx_align_left">66.63 <math id="S5.T2.3.3.3.3.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T2.3.3.3.3.1.m1.1a"><mo id="S5.T2.3.3.3.3.1.m1.1.1" xref="S5.T2.3.3.3.3.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.3.1.m1.1b"><csymbol cd="latexml" id="S5.T2.3.3.3.3.1.m1.1.1.cmml" xref="S5.T2.3.3.3.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.3.1.m1.1c">\pm</annotation></semantics></math> <span id="S5.T2.3.3.3.3.1.1" class="ltx_text" style="font-size:90%;">3.28</span>
</td>
<td id="S5.T2.4.4.4.4.2" class="ltx_td ltx_align_left">79.36 <math id="S5.T2.4.4.4.4.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T2.4.4.4.4.2.m1.1a"><mo id="S5.T2.4.4.4.4.2.m1.1.1" xref="S5.T2.4.4.4.4.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.4.4.2.m1.1b"><csymbol cd="latexml" id="S5.T2.4.4.4.4.2.m1.1.1.cmml" xref="S5.T2.4.4.4.4.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.4.4.2.m1.1c">\pm</annotation></semantics></math> <span id="S5.T2.4.4.4.4.2.1" class="ltx_text" style="font-size:90%;">2.39</span>
</td>
<td id="S5.T2.4.4.4.4.4" class="ltx_td ltx_align_center">1.0</td>
<td id="S5.T2.4.4.4.4.5" class="ltx_td ltx_nopad_r ltx_align_center">11.6</td>
</tr>
<tr id="S5.T2.6.6.6.6" class="ltx_tr">
<td id="S5.T2.6.6.6.6.3" class="ltx_td ltx_align_center ltx_border_bb">ConvGRU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</td>
<td id="S5.T2.5.5.5.5.1" class="ltx_td ltx_align_left ltx_border_bb">
<span id="S5.T2.5.5.5.5.1.1" class="ltx_text ltx_framed ltx_framed_underline">6</span>9.7 <math id="S5.T2.5.5.5.5.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T2.5.5.5.5.1.m1.1a"><mo id="S5.T2.5.5.5.5.1.m1.1.1" xref="S5.T2.5.5.5.5.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.5.5.5.5.1.m1.1b"><csymbol cd="latexml" id="S5.T2.5.5.5.5.1.m1.1.1.cmml" xref="S5.T2.5.5.5.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.5.5.5.1.m1.1c">\pm</annotation></semantics></math> <span id="S5.T2.5.5.5.5.1.2" class="ltx_text" style="font-size:90%;">2.4</span>
</td>
<td id="S5.T2.6.6.6.6.2" class="ltx_td ltx_align_left ltx_border_bb"><span id="S5.T2.6.6.6.6.2.1" class="ltx_text ltx_font_bold">81.2 <math id="S5.T2.6.6.6.6.2.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T2.6.6.6.6.2.1.m1.1a"><mo id="S5.T2.6.6.6.6.2.1.m1.1.1" xref="S5.T2.6.6.6.6.2.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.6.6.6.6.2.1.m1.1b"><csymbol cd="latexml" id="S5.T2.6.6.6.6.2.1.m1.1.1.cmml" xref="S5.T2.6.6.6.6.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.6.6.6.6.2.1.m1.1c">\pm</annotation></semantics></math> <span id="S5.T2.6.6.6.6.2.1.1" class="ltx_text" style="font-size:90%;">1.4</span></span></td>
<td id="S5.T2.6.6.6.6.4" class="ltx_td ltx_align_center ltx_border_bb">0.94</td>
<td id="S5.T2.6.6.6.6.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">9.8</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Comparison of different types of skip connections. Results are averaged over 5 different seeds on the ROD2021 test set. Concatenation is the RECORD model, addition stand for a model where we add the output of transposed convolutions to <math id="S5.T3.2.2.m1.1" class="ltx_Math" alttext="h_{k}^{i}" display="inline"><semantics id="S5.T3.2.2.m1.1b"><msubsup id="S5.T3.2.2.m1.1.1" xref="S5.T3.2.2.m1.1.1.cmml"><mi id="S5.T3.2.2.m1.1.1.2.2" xref="S5.T3.2.2.m1.1.1.2.2.cmml">h</mi><mi id="S5.T3.2.2.m1.1.1.2.3" xref="S5.T3.2.2.m1.1.1.2.3.cmml">k</mi><mi id="S5.T3.2.2.m1.1.1.3" xref="S5.T3.2.2.m1.1.1.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.m1.1c"><apply id="S5.T3.2.2.m1.1.1.cmml" xref="S5.T3.2.2.m1.1.1"><csymbol cd="ambiguous" id="S5.T3.2.2.m1.1.1.1.cmml" xref="S5.T3.2.2.m1.1.1">superscript</csymbol><apply id="S5.T3.2.2.m1.1.1.2.cmml" xref="S5.T3.2.2.m1.1.1"><csymbol cd="ambiguous" id="S5.T3.2.2.m1.1.1.2.1.cmml" xref="S5.T3.2.2.m1.1.1">subscript</csymbol><ci id="S5.T3.2.2.m1.1.1.2.2.cmml" xref="S5.T3.2.2.m1.1.1.2.2">â„</ci><ci id="S5.T3.2.2.m1.1.1.2.3.cmml" xref="S5.T3.2.2.m1.1.1.2.3">ğ‘˜</ci></apply><ci id="S5.T3.2.2.m1.1.1.3.cmml" xref="S5.T3.2.2.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.m1.1d">h_{k}^{i}</annotation></semantics></math>, and no skip connection stands for a model without skip connections. </figcaption>
<div id="S5.T3.8.8" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:115.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(81.9pt,-21.9pt) scale(1.6073201035713,1.6073201035713) ;">
<table id="S5.T3.8.8.6" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.8.8.6.7.1" class="ltx_tr">
<th id="S5.T3.8.8.6.7.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Skip connection</th>
<th id="S5.T3.8.8.6.7.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">AP</th>
<th id="S5.T3.8.8.6.7.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">AR</th>
<th id="S5.T3.8.8.6.7.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">Params (M)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.4.4.2.2" class="ltx_tr">
<td id="S5.T3.4.4.2.2.3" class="ltx_td ltx_align_center ltx_border_t">Concatenation</td>
<td id="S5.T3.3.3.1.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T3.3.3.1.1.1.1" class="ltx_text ltx_font_bold">69.8 <math id="S5.T3.3.3.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.3.3.1.1.1.1.m1.1a"><mo id="S5.T3.3.3.1.1.1.1.m1.1.1" xref="S5.T3.3.3.1.1.1.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T3.3.3.1.1.1.1.m1.1.1.cmml" xref="S5.T3.3.3.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.1.1.1.1.m1.1c">\pm</annotation></semantics></math> <span id="S5.T3.3.3.1.1.1.1.1" class="ltx_text" style="font-size:90%;">2.2</span></span></td>
<td id="S5.T3.4.4.2.2.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S5.T3.4.4.2.2.2.1" class="ltx_text ltx_framed ltx_framed_underline">8</span>0.2 <math id="S5.T3.4.4.2.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.4.4.2.2.2.m1.1a"><mo id="S5.T3.4.4.2.2.2.m1.1.1" xref="S5.T3.4.4.2.2.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.4.4.2.2.2.m1.1b"><csymbol cd="latexml" id="S5.T3.4.4.2.2.2.m1.1.1.cmml" xref="S5.T3.4.4.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.4.2.2.2.m1.1c">\pm</annotation></semantics></math> <span id="S5.T3.4.4.2.2.2.2" class="ltx_text" style="font-size:90%;">1.5</span>
</td>
<td id="S5.T3.4.4.2.2.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.69</td>
</tr>
<tr id="S5.T3.6.6.4.4" class="ltx_tr">
<td id="S5.T3.6.6.4.4.3" class="ltx_td ltx_align_center">Addition</td>
<td id="S5.T3.5.5.3.3.1" class="ltx_td ltx_align_left">
<span id="S5.T3.5.5.3.3.1.1" class="ltx_text ltx_framed ltx_framed_underline">6</span>4.4 <math id="S5.T3.5.5.3.3.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.5.5.3.3.1.m1.1a"><mo id="S5.T3.5.5.3.3.1.m1.1.1" xref="S5.T3.5.5.3.3.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.5.5.3.3.1.m1.1b"><csymbol cd="latexml" id="S5.T3.5.5.3.3.1.m1.1.1.cmml" xref="S5.T3.5.5.3.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.5.5.3.3.1.m1.1c">\pm</annotation></semantics></math> <span id="S5.T3.5.5.3.3.1.2" class="ltx_text" style="font-size:90%;">5.3</span>
</td>
<td id="S5.T3.6.6.4.4.2" class="ltx_td ltx_align_left"><span id="S5.T3.6.6.4.4.2.1" class="ltx_text ltx_font_bold">80.5 <math id="S5.T3.6.6.4.4.2.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.6.6.4.4.2.1.m1.1a"><mo id="S5.T3.6.6.4.4.2.1.m1.1.1" xref="S5.T3.6.6.4.4.2.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.6.6.4.4.2.1.m1.1b"><csymbol cd="latexml" id="S5.T3.6.6.4.4.2.1.m1.1.1.cmml" xref="S5.T3.6.6.4.4.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.6.6.4.4.2.1.m1.1c">\pm</annotation></semantics></math> <span id="S5.T3.6.6.4.4.2.1.1" class="ltx_text" style="font-size:90%;">1.1</span></span></td>
<td id="S5.T3.6.6.4.4.4" class="ltx_td ltx_nopad_r ltx_align_center">0.58</td>
</tr>
<tr id="S5.T3.8.8.6.6" class="ltx_tr">
<td id="S5.T3.8.8.6.6.3" class="ltx_td ltx_align_center ltx_border_bb">No skip connections</td>
<td id="S5.T3.7.7.5.5.1" class="ltx_td ltx_align_left ltx_border_bb">63.7 <math id="S5.T3.7.7.5.5.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.7.7.5.5.1.m1.1a"><mo id="S5.T3.7.7.5.5.1.m1.1.1" xref="S5.T3.7.7.5.5.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.7.7.5.5.1.m1.1b"><csymbol cd="latexml" id="S5.T3.7.7.5.5.1.m1.1.1.cmml" xref="S5.T3.7.7.5.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.7.7.5.5.1.m1.1c">\pm</annotation></semantics></math> <span id="S5.T3.7.7.5.5.1.1" class="ltx_text" style="font-size:90%;">6.2</span>
</td>
<td id="S5.T3.8.8.6.6.2" class="ltx_td ltx_align_left ltx_border_bb">78.7 <math id="S5.T3.8.8.6.6.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.8.8.6.6.2.m1.1a"><mo id="S5.T3.8.8.6.6.2.m1.1.1" xref="S5.T3.8.8.6.6.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.8.8.6.6.2.m1.1b"><csymbol cd="latexml" id="S5.T3.8.8.6.6.2.m1.1.1.cmml" xref="S5.T3.8.8.6.6.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.8.8.6.6.2.m1.1c">\pm</annotation></semantics></math> <span id="S5.T3.8.8.6.6.2.1" class="ltx_text" style="font-size:90%;">3.5</span>
</td>
<td id="S5.T3.8.8.6.6.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">0.58</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S5.SS1.p8" class="ltx_para ltx_noindent">
<p id="S5.SS1.p8.1" class="ltx_p"><span id="S5.SS1.p8.1.1" class="ltx_text ltx_font_bold">Ablation studies.</span> We demonstrate the relevance of using bottleneck LSTMs instead of classic ConvGRUs or ConvLSTMs in tableÂ <a href="#S5.T2" title="TABLE II â€£ V-A Single view object detection â€£ V Experiments â€£ A recurrent CNN for online object detection on raw radar frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. Bottleneck LSTMs reduce the number of parameters and GMACS and achieve higher AP and AR than classic ConvRNNs. Additionally, we show in tableÂ <a href="#S5.T3" title="TABLE III â€£ V-A Single view object detection â€£ V Experiments â€£ A recurrent CNN for online object detection on raw radar frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> the AP and the AR of our model with different skip connections. We show that concatenating temporal features with spatial features of the decoder (i.e., our RECORD model) reaches better AP and AR than a method without skip connections, or one where we add the temporal features to the spatial features of the decoder. Nevertheless, the concatenation of features increases the number of parameters and the number of GMACS of the model compared to other approaches.</p>
</div>
<div id="S5.SS1.p9" class="ltx_para ltx_noindent">
<p id="S5.SS1.p9.1" class="ltx_p"><span id="S5.SS1.p9.1.1" class="ltx_text ltx_font_bold">UTAE performances improvement.</span> TableÂ <a href="#S5.T4" title="TABLE IV â€£ V-A Single view object detection â€£ V Experiments â€£ A recurrent CNN for online object detection on raw radar frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> depicts the performance improvement of UTAE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> model with and without positional encoding and with a modified number of channels in the encoder and the decoder. We modify the number of channels of UTAE to match our architecture. We found that decreasing the model size and using positional encoding improve the modelâ€™s performance. We define positional encoding as the time between the first and the <math id="S5.SS1.p9.1.m1.1" class="ltx_Math" alttext="k^{th}" display="inline"><semantics id="S5.SS1.p9.1.m1.1a"><msup id="S5.SS1.p9.1.m1.1.1" xref="S5.SS1.p9.1.m1.1.1.cmml"><mi id="S5.SS1.p9.1.m1.1.1.2" xref="S5.SS1.p9.1.m1.1.1.2.cmml">k</mi><mrow id="S5.SS1.p9.1.m1.1.1.3" xref="S5.SS1.p9.1.m1.1.1.3.cmml"><mi id="S5.SS1.p9.1.m1.1.1.3.2" xref="S5.SS1.p9.1.m1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p9.1.m1.1.1.3.1" xref="S5.SS1.p9.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S5.SS1.p9.1.m1.1.1.3.3" xref="S5.SS1.p9.1.m1.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS1.p9.1.m1.1b"><apply id="S5.SS1.p9.1.m1.1.1.cmml" xref="S5.SS1.p9.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p9.1.m1.1.1.1.cmml" xref="S5.SS1.p9.1.m1.1.1">superscript</csymbol><ci id="S5.SS1.p9.1.m1.1.1.2.cmml" xref="S5.SS1.p9.1.m1.1.1.2">ğ‘˜</ci><apply id="S5.SS1.p9.1.m1.1.1.3.cmml" xref="S5.SS1.p9.1.m1.1.1.3"><times id="S5.SS1.p9.1.m1.1.1.3.1.cmml" xref="S5.SS1.p9.1.m1.1.1.3.1"></times><ci id="S5.SS1.p9.1.m1.1.1.3.2.cmml" xref="S5.SS1.p9.1.m1.1.1.3.2">ğ‘¡</ci><ci id="S5.SS1.p9.1.m1.1.1.3.3.cmml" xref="S5.SS1.p9.1.m1.1.1.3.3">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p9.1.m1.1c">k^{th}</annotation></semantics></math> frames.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Performances improvement of UTAE model with and without positional encoding and with the default architecture (underlined line). Results are obtained on the test set and on a single seed. </figcaption>
<div id="S5.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:115pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(47.1pt,-12.5pt) scale(1.27775191144226,1.27775191144226) ;">
<table id="S5.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.1.1.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" colspan="2"># channels</th>
<th id="S5.T4.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S5.T4.1.1.1.1.2.1" class="ltx_text">Pos. enc.</span></th>
<th id="S5.T4.1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S5.T4.1.1.1.1.3.1" class="ltx_text">AP</span></th>
<th id="S5.T4.1.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S5.T4.1.1.1.1.4.1" class="ltx_text">AR</span></th>
<th id="S5.T4.1.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S5.T4.1.1.1.1.5.1" class="ltx_text">Params (M)</span></th>
</tr>
<tr id="S5.T4.1.1.2.2" class="ltx_tr">
<th id="S5.T4.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">Encoder</th>
<th id="S5.T4.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">Decoder</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.1.1.3.1" class="ltx_tr">
<th id="S5.T4.1.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">16, 32, 64, 128</th>
<th id="S5.T4.1.1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">16, 32, 64, 128</th>
<td id="S5.T4.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">Yes</td>
<td id="S5.T4.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.1.1.3.1.4.1" class="ltx_text ltx_font_bold">68.4</span></td>
<td id="S5.T4.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.1.1.3.1.5.1" class="ltx_text ltx_font_bold">78.4</span></td>
<td id="S5.T4.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">0.79</td>
</tr>
<tr id="S5.T4.1.1.4.2" class="ltx_tr">
<th id="S5.T4.1.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">16, 32, 64, 128</th>
<th id="S5.T4.1.1.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">16, 32, 64, 128</th>
<td id="S5.T4.1.1.4.2.3" class="ltx_td ltx_align_center">No</td>
<td id="S5.T4.1.1.4.2.4" class="ltx_td ltx_align_center">46.9</td>
<td id="S5.T4.1.1.4.2.5" class="ltx_td ltx_align_center">64.3</td>
<td id="S5.T4.1.1.4.2.6" class="ltx_td ltx_align_center">0.79</td>
</tr>
<tr id="S5.T4.1.1.5.3" class="ltx_tr">
<th id="S5.T4.1.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">
<span id="S5.T4.1.1.5.3.1.1" class="ltx_text ltx_framed ltx_framed_underline">6</span>4, 64, 64, 128</th>
<th id="S5.T4.1.1.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">
<span id="S5.T4.1.1.5.3.2.1" class="ltx_text ltx_framed ltx_framed_underline">3</span>2, 32, 64, 128</th>
<td id="S5.T4.1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_bb">Yes</td>
<td id="S5.T4.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_bb">60.8</td>
<td id="S5.T4.1.1.5.3.5" class="ltx_td ltx_align_center ltx_border_bb">77.9</td>
<td id="S5.T4.1.1.5.3.6" class="ltx_td ltx_align_center ltx_border_bb">1.1</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.5.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.6.2" class="ltx_text ltx_font_italic">Multi-view semantic segmentation</span>
</h3>

<figure id="S5.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Results on the multi-view approach on CARRADA dataset. MV-RECORD stands for our multi-view approach. RECORD* stands for a single-view approach. The best results are in bold, and the second bests are underlined.</figcaption>
<table id="S5.T5.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T5.1.1.1" class="ltx_tr">
<td id="S5.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="2" rowspan="2"><span id="S5.T5.1.1.1.1.1" class="ltx_text">Model</span></td>
<td id="S5.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="5">IoU</td>
<td id="S5.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S5.T5.1.1.1.3.1" class="ltx_text">Params (M)</span></td>
<td id="S5.T5.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S5.T5.1.1.1.4.1" class="ltx_text">GMACS</span></td>
<td id="S5.T5.1.1.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" rowspan="2"><span id="S5.T5.1.1.1.5.1" class="ltx_text">Runtime (ms)</span></td>
</tr>
<tr id="S5.T5.1.2.2" class="ltx_tr">
<td id="S5.T5.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t">mIoU</td>
<td id="S5.T5.1.2.2.2" class="ltx_td ltx_align_left ltx_border_t">Bg</td>
<td id="S5.T5.1.2.2.3" class="ltx_td ltx_align_left ltx_border_t">Ped</td>
<td id="S5.T5.1.2.2.4" class="ltx_td ltx_align_left ltx_border_t">Cycl</td>
<td id="S5.T5.1.2.2.5" class="ltx_td ltx_align_left ltx_border_t">Car</td>
</tr>
<tr id="S5.T5.1.3.3" class="ltx_tr">
<td id="S5.T5.1.3.3.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="5"><span id="S5.T5.1.3.3.1.1" class="ltx_text">RA</span></td>
<td id="S5.T5.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">MV-RECORD (buffer, ours)</td>
<td id="S5.T5.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.1.3.3.3.1" class="ltx_text ltx_font_bold">44.5</span></td>
<td id="S5.T5.1.3.3.4" class="ltx_td ltx_align_left ltx_border_t">99.8</td>
<td id="S5.T5.1.3.3.5" class="ltx_td ltx_align_left ltx_border_t">
<span id="S5.T5.1.3.3.5.1" class="ltx_text ltx_framed ltx_framed_underline">2</span>4.2</td>
<td id="S5.T5.1.3.3.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T5.1.3.3.6.1" class="ltx_text ltx_font_bold">20.1</span></td>
<td id="S5.T5.1.3.3.7" class="ltx_td ltx_align_left ltx_border_t">
<span id="S5.T5.1.3.3.7.1" class="ltx_text ltx_framed ltx_framed_underline">3</span>4.1</td>
<td id="S5.T5.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t">1.9</td>
<td id="S5.T5.1.3.3.9" class="ltx_td ltx_align_center ltx_border_t">22.2</td>
<td id="S5.T5.1.3.3.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">281.8</td>
</tr>
<tr id="S5.T5.1.4.4" class="ltx_tr">
<td id="S5.T5.1.4.4.1" class="ltx_td ltx_align_center">MV-RECORD (online, ours)</td>
<td id="S5.T5.1.4.4.2" class="ltx_td ltx_align_center">
<span id="S5.T5.1.4.4.2.1" class="ltx_text ltx_framed ltx_framed_underline">4</span>2.4</td>
<td id="S5.T5.1.4.4.3" class="ltx_td ltx_align_left">99.8</td>
<td id="S5.T5.1.4.4.4" class="ltx_td ltx_align_left">22.1</td>
<td id="S5.T5.1.4.4.5" class="ltx_td ltx_align_left">
<span id="S5.T5.1.4.4.5.1" class="ltx_text ltx_framed ltx_framed_underline">1</span>1.1</td>
<td id="S5.T5.1.4.4.6" class="ltx_td ltx_align_left"><span id="S5.T5.1.4.4.6.1" class="ltx_text ltx_font_bold">36.4</span></td>
<td id="S5.T5.1.4.4.7" class="ltx_td ltx_align_center">1.9</td>
<td id="S5.T5.1.4.4.8" class="ltx_td ltx_align_center">3.7</td>
<td id="S5.T5.1.4.4.9" class="ltx_td ltx_nopad_r ltx_align_center">56.6</td>
</tr>
<tr id="S5.T5.1.5.5" class="ltx_tr">
<td id="S5.T5.1.5.5.1" class="ltx_td ltx_align_center">RECORD* (buffer, ours)</td>
<td id="S5.T5.1.5.5.2" class="ltx_td ltx_align_center">34.8</td>
<td id="S5.T5.1.5.5.3" class="ltx_td ltx_align_left">99.7</td>
<td id="S5.T5.1.5.5.4" class="ltx_td ltx_align_left">10.3</td>
<td id="S5.T5.1.5.5.5" class="ltx_td ltx_align_left">1.4</td>
<td id="S5.T5.1.5.5.6" class="ltx_td ltx_align_left">27.7</td>
<td id="S5.T5.1.5.5.7" class="ltx_td ltx_align_center">0.69</td>
<td id="S5.T5.1.5.5.8" class="ltx_td ltx_align_center">8.2</td>
<td id="S5.T5.1.5.5.9" class="ltx_td ltx_nopad_r ltx_align_center">116.1</td>
</tr>
<tr id="S5.T5.1.6.6" class="ltx_tr">
<td id="S5.T5.1.6.6.1" class="ltx_td ltx_align_center">RECORD* (online, ours)</td>
<td id="S5.T5.1.6.6.2" class="ltx_td ltx_align_center">36.3</td>
<td id="S5.T5.1.6.6.3" class="ltx_td ltx_align_left">99.8</td>
<td id="S5.T5.1.6.6.4" class="ltx_td ltx_align_left">12.1</td>
<td id="S5.T5.1.6.6.5" class="ltx_td ltx_align_left">3.1</td>
<td id="S5.T5.1.6.6.6" class="ltx_td ltx_align_left">30.4</td>
<td id="S5.T5.1.6.6.7" class="ltx_td ltx_align_center">0.69</td>
<td id="S5.T5.1.6.6.8" class="ltx_td ltx_align_center">2.38</td>
<td id="S5.T5.1.6.6.9" class="ltx_td ltx_nopad_r ltx_align_center">29.6</td>
</tr>
<tr id="S5.T5.1.7.7" class="ltx_tr">
<td id="S5.T5.1.7.7.1" class="ltx_td ltx_align_center">TMVA-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</td>
<td id="S5.T5.1.7.7.2" class="ltx_td ltx_align_center">41.3</td>
<td id="S5.T5.1.7.7.3" class="ltx_td ltx_align_left">99.8</td>
<td id="S5.T5.1.7.7.4" class="ltx_td ltx_align_left"><span id="S5.T5.1.7.7.4.1" class="ltx_text ltx_font_bold">26.0</span></td>
<td id="S5.T5.1.7.7.5" class="ltx_td ltx_align_left">8.6</td>
<td id="S5.T5.1.7.7.6" class="ltx_td ltx_align_left">30.7</td>
<td id="S5.T5.1.7.7.7" class="ltx_td ltx_align_center">5.6</td>
<td id="S5.T5.1.7.7.8" class="ltx_td ltx_align_center">98.0</td>
<td id="S5.T5.1.7.7.9" class="ltx_td ltx_nopad_r ltx_align_center">21.8</td>
</tr>
<tr id="S5.T5.1.8.8" class="ltx_tr">
<td id="S5.T5.1.8.8.1" class="ltx_td"></td>
<td id="S5.T5.1.8.8.2" class="ltx_td ltx_align_center">MV-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</td>
<td id="S5.T5.1.8.8.3" class="ltx_td ltx_align_center">26.8</td>
<td id="S5.T5.1.8.8.4" class="ltx_td ltx_align_left">99.8</td>
<td id="S5.T5.1.8.8.5" class="ltx_td ltx_align_left">0.1</td>
<td id="S5.T5.1.8.8.6" class="ltx_td ltx_align_left">1.1</td>
<td id="S5.T5.1.8.8.7" class="ltx_td ltx_align_left">6.2</td>
<td id="S5.T5.1.8.8.8" class="ltx_td ltx_align_center">2.4</td>
<td id="S5.T5.1.8.8.9" class="ltx_td ltx_align_center">53.3</td>
<td id="S5.T5.1.8.8.10" class="ltx_td ltx_nopad_r ltx_align_center">18.8</td>
</tr>
<tr id="S5.T5.1.9.9" class="ltx_tr">
<td id="S5.T5.1.9.9.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="5"><span id="S5.T5.1.9.9.1.1" class="ltx_text">RD</span></td>
<td id="S5.T5.1.9.9.2" class="ltx_td ltx_align_center ltx_border_t">MV-RECORD (buffer, ours)</td>
<td id="S5.T5.1.9.9.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.1.9.9.3.1" class="ltx_text ltx_font_bold">63.2</span></td>
<td id="S5.T5.1.9.9.4" class="ltx_td ltx_align_left ltx_border_t">99.6</td>
<td id="S5.T5.1.9.9.5" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T5.1.9.9.5.1" class="ltx_text ltx_font_bold">54.9</span></td>
<td id="S5.T5.1.9.9.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T5.1.9.9.6.1" class="ltx_text ltx_font_bold">39.3</span></td>
<td id="S5.T5.1.9.9.7" class="ltx_td ltx_align_left ltx_border_t">
<span id="S5.T5.1.9.9.7.1" class="ltx_text ltx_framed ltx_framed_underline">5</span>8.9</td>
<td id="S5.T5.1.9.9.8" class="ltx_td ltx_align_center ltx_border_t">1.9</td>
<td id="S5.T5.1.9.9.9" class="ltx_td ltx_align_center ltx_border_t">22.2</td>
<td id="S5.T5.1.9.9.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">281.8</td>
</tr>
<tr id="S5.T5.1.10.10" class="ltx_tr">
<td id="S5.T5.1.10.10.1" class="ltx_td ltx_align_center">MV-RECORD (online, ours)</td>
<td id="S5.T5.1.10.10.2" class="ltx_td ltx_align_center">58.5</td>
<td id="S5.T5.1.10.10.3" class="ltx_td ltx_align_left">99.7</td>
<td id="S5.T5.1.10.10.4" class="ltx_td ltx_align_left">49.4</td>
<td id="S5.T5.1.10.10.5" class="ltx_td ltx_align_left">26.3</td>
<td id="S5.T5.1.10.10.6" class="ltx_td ltx_align_left">58.6</td>
<td id="S5.T5.1.10.10.7" class="ltx_td ltx_align_center">0.69</td>
<td id="S5.T5.1.10.10.8" class="ltx_td ltx_align_center">3.7</td>
<td id="S5.T5.1.10.10.9" class="ltx_td ltx_nopad_r ltx_align_center">58.6</td>
</tr>
<tr id="S5.T5.1.11.11" class="ltx_tr">
<td id="S5.T5.1.11.11.1" class="ltx_td ltx_align_center">RECORD* (buffer, ours)</td>
<td id="S5.T5.1.11.11.2" class="ltx_td ltx_align_center">58.1</td>
<td id="S5.T5.1.11.11.3" class="ltx_td ltx_align_left">99.6</td>
<td id="S5.T5.1.11.11.4" class="ltx_td ltx_align_left">46.6</td>
<td id="S5.T5.1.11.11.5" class="ltx_td ltx_align_left">28.6</td>
<td id="S5.T5.1.11.11.6" class="ltx_td ltx_align_left">57.5</td>
<td id="S5.T5.1.11.11.7" class="ltx_td ltx_align_center">0.69</td>
<td id="S5.T5.1.11.11.8" class="ltx_td ltx_align_center">6.1</td>
<td id="S5.T5.1.11.11.9" class="ltx_td ltx_nopad_r ltx_align_center">58.5</td>
</tr>
<tr id="S5.T5.1.12.12" class="ltx_tr">
<td id="S5.T5.1.12.12.1" class="ltx_td ltx_align_center">RECORD* (online, ours)</td>
<td id="S5.T5.1.12.12.2" class="ltx_td ltx_align_center">
<span id="S5.T5.1.12.12.2.1" class="ltx_text ltx_framed ltx_framed_underline">6</span>1.7</td>
<td id="S5.T5.1.12.12.3" class="ltx_td ltx_align_left">99.7</td>
<td id="S5.T5.1.12.12.4" class="ltx_td ltx_align_left">52.1</td>
<td id="S5.T5.1.12.12.5" class="ltx_td ltx_align_left">
<span id="S5.T5.1.12.12.5.1" class="ltx_text ltx_framed ltx_framed_underline">3</span>3.6</td>
<td id="S5.T5.1.12.12.6" class="ltx_td ltx_align_left"><span id="S5.T5.1.12.12.6.1" class="ltx_text ltx_font_bold">61.4</span></td>
<td id="S5.T5.1.12.12.7" class="ltx_td ltx_align_center">0.69</td>
<td id="S5.T5.1.12.12.8" class="ltx_td ltx_align_center">0.59</td>
<td id="S5.T5.1.12.12.9" class="ltx_td ltx_nopad_r ltx_align_center">13.3</td>
</tr>
<tr id="S5.T5.1.13.13" class="ltx_tr">
<td id="S5.T5.1.13.13.1" class="ltx_td ltx_align_center">TMVA-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</td>
<td id="S5.T5.1.13.13.2" class="ltx_td ltx_align_center">58.7</td>
<td id="S5.T5.1.13.13.3" class="ltx_td ltx_align_left">99.7</td>
<td id="S5.T5.1.13.13.4" class="ltx_td ltx_align_left">
<span id="S5.T5.1.13.13.4.1" class="ltx_text ltx_framed ltx_framed_underline">5</span>2.6</td>
<td id="S5.T5.1.13.13.5" class="ltx_td ltx_align_left">29.0</td>
<td id="S5.T5.1.13.13.6" class="ltx_td ltx_align_left">53.4</td>
<td id="S5.T5.1.13.13.7" class="ltx_td ltx_align_center">5.6</td>
<td id="S5.T5.1.13.13.8" class="ltx_td ltx_align_center">98.0</td>
<td id="S5.T5.1.13.13.9" class="ltx_td ltx_nopad_r ltx_align_center">21.8</td>
</tr>
<tr id="S5.T5.1.14.14" class="ltx_tr">
<td id="S5.T5.1.14.14.1" class="ltx_td ltx_border_bb"></td>
<td id="S5.T5.1.14.14.2" class="ltx_td ltx_align_center ltx_border_bb">MV-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</td>
<td id="S5.T5.1.14.14.3" class="ltx_td ltx_align_center ltx_border_bb">29.0</td>
<td id="S5.T5.1.14.14.4" class="ltx_td ltx_align_left ltx_border_bb">98.0</td>
<td id="S5.T5.1.14.14.5" class="ltx_td ltx_align_left ltx_border_bb">0.0</td>
<td id="S5.T5.1.14.14.6" class="ltx_td ltx_align_left ltx_border_bb">3.8</td>
<td id="S5.T5.1.14.14.7" class="ltx_td ltx_align_left ltx_border_bb">14.1</td>
<td id="S5.T5.1.14.14.8" class="ltx_td ltx_align_center ltx_border_bb">2.4</td>
<td id="S5.T5.1.14.14.9" class="ltx_td ltx_align_center ltx_border_bb">53.3</td>
<td id="S5.T5.1.14.14.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">18.8</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.p1.3" class="ltx_p"><span id="S5.SS2.p1.3.1" class="ltx_text ltx_font_bold">Dataset.</span> To demonstrate the relevance of our method, we train our model for multi-view object segmentation on the CARRADA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. The CARRADA dataset contains 30 sequences of synchronised cameras and raw radar frames recorded in various scenarios with one or two moving objects. The CARRADA dataset provides RAD tensors and semantic segmentation masks for both RD and RA views. Contrary to the CRUW dataset, the CARRADA dataset only contains simple driving scenarios (static radar on an airport runway). The frame rate is 10Hz. The objects are separated into four categories: pedestrian, cyclist, car and background. The RAD tensors have dimensions <math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="\text{256}\times\text{256}\times\text{64}" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><mrow id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml"><mtext id="S5.SS2.p1.1.m1.1.1.2" xref="S5.SS2.p1.1.m1.1.1.2a.cmml">256</mtext><mo lspace="0.222em" rspace="0.222em" id="S5.SS2.p1.1.m1.1.1.1" xref="S5.SS2.p1.1.m1.1.1.1.cmml">Ã—</mo><mtext id="S5.SS2.p1.1.m1.1.1.3" xref="S5.SS2.p1.1.m1.1.1.3a.cmml">256</mtext><mo lspace="0.222em" rspace="0.222em" id="S5.SS2.p1.1.m1.1.1.1a" xref="S5.SS2.p1.1.m1.1.1.1.cmml">Ã—</mo><mtext id="S5.SS2.p1.1.m1.1.1.4" xref="S5.SS2.p1.1.m1.1.1.4a.cmml">64</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><apply id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"><times id="S5.SS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1.1"></times><ci id="S5.SS2.p1.1.m1.1.1.2a.cmml" xref="S5.SS2.p1.1.m1.1.1.2"><mtext id="S5.SS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.p1.1.m1.1.1.2">256</mtext></ci><ci id="S5.SS2.p1.1.m1.1.1.3a.cmml" xref="S5.SS2.p1.1.m1.1.1.3"><mtext id="S5.SS2.p1.1.m1.1.1.3.cmml" xref="S5.SS2.p1.1.m1.1.1.3">256</mtext></ci><ci id="S5.SS2.p1.1.m1.1.1.4a.cmml" xref="S5.SS2.p1.1.m1.1.1.4"><mtext id="S5.SS2.p1.1.m1.1.1.4.cmml" xref="S5.SS2.p1.1.m1.1.1.4">64</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">\text{256}\times\text{256}\times\text{64}</annotation></semantics></math> and the semantic segmentation masks have respectively dimensions <math id="S5.SS2.p1.2.m2.1" class="ltx_Math" alttext="\text{256}\times\text{256}" display="inline"><semantics id="S5.SS2.p1.2.m2.1a"><mrow id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml"><mtext id="S5.SS2.p1.2.m2.1.1.2" xref="S5.SS2.p1.2.m2.1.1.2a.cmml">256</mtext><mo lspace="0.222em" rspace="0.222em" id="S5.SS2.p1.2.m2.1.1.1" xref="S5.SS2.p1.2.m2.1.1.1.cmml">Ã—</mo><mtext id="S5.SS2.p1.2.m2.1.1.3" xref="S5.SS2.p1.2.m2.1.1.3a.cmml">256</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><apply id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1"><times id="S5.SS2.p1.2.m2.1.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1.1"></times><ci id="S5.SS2.p1.2.m2.1.1.2a.cmml" xref="S5.SS2.p1.2.m2.1.1.2"><mtext id="S5.SS2.p1.2.m2.1.1.2.cmml" xref="S5.SS2.p1.2.m2.1.1.2">256</mtext></ci><ci id="S5.SS2.p1.2.m2.1.1.3a.cmml" xref="S5.SS2.p1.2.m2.1.1.3"><mtext id="S5.SS2.p1.2.m2.1.1.3.cmml" xref="S5.SS2.p1.2.m2.1.1.3">256</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">\text{256}\times\text{256}</annotation></semantics></math> and <math id="S5.SS2.p1.3.m3.1" class="ltx_Math" alttext="\text{256}\times\text{64}" display="inline"><semantics id="S5.SS2.p1.3.m3.1a"><mrow id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml"><mtext id="S5.SS2.p1.3.m3.1.1.2" xref="S5.SS2.p1.3.m3.1.1.2a.cmml">256</mtext><mo lspace="0.222em" rspace="0.222em" id="S5.SS2.p1.3.m3.1.1.1" xref="S5.SS2.p1.3.m3.1.1.1.cmml">Ã—</mo><mtext id="S5.SS2.p1.3.m3.1.1.3" xref="S5.SS2.p1.3.m3.1.1.3a.cmml">64</mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><apply id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1"><times id="S5.SS2.p1.3.m3.1.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1.1"></times><ci id="S5.SS2.p1.3.m3.1.1.2a.cmml" xref="S5.SS2.p1.3.m3.1.1.2"><mtext id="S5.SS2.p1.3.m3.1.1.2.cmml" xref="S5.SS2.p1.3.m3.1.1.2">256</mtext></ci><ci id="S5.SS2.p1.3.m3.1.1.3a.cmml" xref="S5.SS2.p1.3.m3.1.1.3"><mtext id="S5.SS2.p1.3.m3.1.1.3.cmml" xref="S5.SS2.p1.3.m3.1.1.3">64</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">\text{256}\times\text{64}</annotation></semantics></math> for the RA and the RD spectra. For training, validation and testing, we use the dataset splits provided by the authors.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.p2.1" class="ltx_p"><span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_bold">Evaluation metrics.</span> We evaluate our multi-view model using the intersection over union (IoU). IoU is a common evaluation metric for semantic image segmentation, which quantifies the overlap between the target mask T and the predicted segmentation mask P. For a single class, IoU is defined as:</p>
<table id="S5.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E5.m1.2" class="ltx_Math" alttext="IoU=\big{|}\frac{T\cap P}{T\cup P}\big{|}." display="block"><semantics id="S5.E5.m1.2a"><mrow id="S5.E5.m1.2.2.1" xref="S5.E5.m1.2.2.1.1.cmml"><mrow id="S5.E5.m1.2.2.1.1" xref="S5.E5.m1.2.2.1.1.cmml"><mrow id="S5.E5.m1.2.2.1.1.2" xref="S5.E5.m1.2.2.1.1.2.cmml"><mi id="S5.E5.m1.2.2.1.1.2.2" xref="S5.E5.m1.2.2.1.1.2.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S5.E5.m1.2.2.1.1.2.1" xref="S5.E5.m1.2.2.1.1.2.1.cmml">â€‹</mo><mi id="S5.E5.m1.2.2.1.1.2.3" xref="S5.E5.m1.2.2.1.1.2.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.E5.m1.2.2.1.1.2.1a" xref="S5.E5.m1.2.2.1.1.2.1.cmml">â€‹</mo><mi id="S5.E5.m1.2.2.1.1.2.4" xref="S5.E5.m1.2.2.1.1.2.4.cmml">U</mi></mrow><mo id="S5.E5.m1.2.2.1.1.1" xref="S5.E5.m1.2.2.1.1.1.cmml">=</mo><mrow id="S5.E5.m1.2.2.1.1.3.2" xref="S5.E5.m1.2.2.1.1.3.1.cmml"><mo maxsize="120%" minsize="120%" id="S5.E5.m1.2.2.1.1.3.2.1" xref="S5.E5.m1.2.2.1.1.3.1.1.cmml">|</mo><mfrac id="S5.E5.m1.1.1" xref="S5.E5.m1.1.1.cmml"><mrow id="S5.E5.m1.1.1.2" xref="S5.E5.m1.1.1.2.cmml"><mi id="S5.E5.m1.1.1.2.2" xref="S5.E5.m1.1.1.2.2.cmml">T</mi><mo id="S5.E5.m1.1.1.2.1" xref="S5.E5.m1.1.1.2.1.cmml">âˆ©</mo><mi id="S5.E5.m1.1.1.2.3" xref="S5.E5.m1.1.1.2.3.cmml">P</mi></mrow><mrow id="S5.E5.m1.1.1.3" xref="S5.E5.m1.1.1.3.cmml"><mi id="S5.E5.m1.1.1.3.2" xref="S5.E5.m1.1.1.3.2.cmml">T</mi><mo id="S5.E5.m1.1.1.3.1" xref="S5.E5.m1.1.1.3.1.cmml">âˆª</mo><mi id="S5.E5.m1.1.1.3.3" xref="S5.E5.m1.1.1.3.3.cmml">P</mi></mrow></mfrac><mo maxsize="120%" minsize="120%" id="S5.E5.m1.2.2.1.1.3.2.2" xref="S5.E5.m1.2.2.1.1.3.1.1.cmml">|</mo></mrow></mrow><mo lspace="0em" id="S5.E5.m1.2.2.1.2" xref="S5.E5.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.E5.m1.2b"><apply id="S5.E5.m1.2.2.1.1.cmml" xref="S5.E5.m1.2.2.1"><eq id="S5.E5.m1.2.2.1.1.1.cmml" xref="S5.E5.m1.2.2.1.1.1"></eq><apply id="S5.E5.m1.2.2.1.1.2.cmml" xref="S5.E5.m1.2.2.1.1.2"><times id="S5.E5.m1.2.2.1.1.2.1.cmml" xref="S5.E5.m1.2.2.1.1.2.1"></times><ci id="S5.E5.m1.2.2.1.1.2.2.cmml" xref="S5.E5.m1.2.2.1.1.2.2">ğ¼</ci><ci id="S5.E5.m1.2.2.1.1.2.3.cmml" xref="S5.E5.m1.2.2.1.1.2.3">ğ‘œ</ci><ci id="S5.E5.m1.2.2.1.1.2.4.cmml" xref="S5.E5.m1.2.2.1.1.2.4">ğ‘ˆ</ci></apply><apply id="S5.E5.m1.2.2.1.1.3.1.cmml" xref="S5.E5.m1.2.2.1.1.3.2"><abs id="S5.E5.m1.2.2.1.1.3.1.1.cmml" xref="S5.E5.m1.2.2.1.1.3.2.1"></abs><apply id="S5.E5.m1.1.1.cmml" xref="S5.E5.m1.1.1"><divide id="S5.E5.m1.1.1.1.cmml" xref="S5.E5.m1.1.1"></divide><apply id="S5.E5.m1.1.1.2.cmml" xref="S5.E5.m1.1.1.2"><intersect id="S5.E5.m1.1.1.2.1.cmml" xref="S5.E5.m1.1.1.2.1"></intersect><ci id="S5.E5.m1.1.1.2.2.cmml" xref="S5.E5.m1.1.1.2.2">ğ‘‡</ci><ci id="S5.E5.m1.1.1.2.3.cmml" xref="S5.E5.m1.1.1.2.3">ğ‘ƒ</ci></apply><apply id="S5.E5.m1.1.1.3.cmml" xref="S5.E5.m1.1.1.3"><union id="S5.E5.m1.1.1.3.1.cmml" xref="S5.E5.m1.1.1.3.1"></union><ci id="S5.E5.m1.1.1.3.2.cmml" xref="S5.E5.m1.1.1.3.2">ğ‘‡</ci><ci id="S5.E5.m1.1.1.3.3.cmml" xref="S5.E5.m1.1.1.3.3">ğ‘ƒ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E5.m1.2c">IoU=\big{|}\frac{T\cap P}{T\cup P}\big{|}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S5.SS2.p2.2" class="ltx_p">We then average this metric over all classes to compute the mean IoU (mIoU).</p>
</div>
<div id="S5.SS2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.p3.1" class="ltx_p"><span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_bold">Competing methods.</span> We compare our multi-view model with state-of-the-art multi-view radar semantic segmentation models, namely MV-Net and TMVA-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Additionally, we train a buffer and an online single-view variant of our RECORD model. We train two different models, one for the RA view and one for the RD view.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para ltx_noindent">
<p id="S5.SS2.p4.2" class="ltx_p"><span id="S5.SS2.p4.2.1" class="ltx_text ltx_font_bold">Experimental settings.</span> As for the ROD2021 dataset, we use two evaluation settings for MV-RECORD: online and buffer. The CARRADA dataset has a significantly lower frame rate than the ROD2021 dataset. In order to match the same time as a single view model, we set the number of input frames to five for the buffer variant and ten for the online one, which corresponds to a time of respectively 0.5 and 1 second. We set the batch size to eight and optimise the model using Adam optimiser with a learning rate of <math id="S5.SS2.p4.1.m1.1" class="ltx_Math" alttext="1\times 10^{-3}" display="inline"><semantics id="S5.SS2.p4.1.m1.1a"><mrow id="S5.SS2.p4.1.m1.1.1" xref="S5.SS2.p4.1.m1.1.1.cmml"><mn id="S5.SS2.p4.1.m1.1.1.2" xref="S5.SS2.p4.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS2.p4.1.m1.1.1.1" xref="S5.SS2.p4.1.m1.1.1.1.cmml">Ã—</mo><msup id="S5.SS2.p4.1.m1.1.1.3" xref="S5.SS2.p4.1.m1.1.1.3.cmml"><mn id="S5.SS2.p4.1.m1.1.1.3.2" xref="S5.SS2.p4.1.m1.1.1.3.2.cmml">10</mn><mrow id="S5.SS2.p4.1.m1.1.1.3.3" xref="S5.SS2.p4.1.m1.1.1.3.3.cmml"><mo id="S5.SS2.p4.1.m1.1.1.3.3a" xref="S5.SS2.p4.1.m1.1.1.3.3.cmml">âˆ’</mo><mn id="S5.SS2.p4.1.m1.1.1.3.3.2" xref="S5.SS2.p4.1.m1.1.1.3.3.2.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.1.m1.1b"><apply id="S5.SS2.p4.1.m1.1.1.cmml" xref="S5.SS2.p4.1.m1.1.1"><times id="S5.SS2.p4.1.m1.1.1.1.cmml" xref="S5.SS2.p4.1.m1.1.1.1"></times><cn type="integer" id="S5.SS2.p4.1.m1.1.1.2.cmml" xref="S5.SS2.p4.1.m1.1.1.2">1</cn><apply id="S5.SS2.p4.1.m1.1.1.3.cmml" xref="S5.SS2.p4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.SS2.p4.1.m1.1.1.3.1.cmml" xref="S5.SS2.p4.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S5.SS2.p4.1.m1.1.1.3.2.cmml" xref="S5.SS2.p4.1.m1.1.1.3.2">10</cn><apply id="S5.SS2.p4.1.m1.1.1.3.3.cmml" xref="S5.SS2.p4.1.m1.1.1.3.3"><minus id="S5.SS2.p4.1.m1.1.1.3.3.1.cmml" xref="S5.SS2.p4.1.m1.1.1.3.3"></minus><cn type="integer" id="S5.SS2.p4.1.m1.1.1.3.3.2.cmml" xref="S5.SS2.p4.1.m1.1.1.3.3.2">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.1.m1.1c">1\times 10^{-3}</annotation></semantics></math> for both buffer and online methods except for the online multi-view model where the learning rate is set to <math id="S5.SS2.p4.2.m2.1" class="ltx_Math" alttext="3\times 10^{-}4" display="inline"><semantics id="S5.SS2.p4.2.m2.1a"><mrow id="S5.SS2.p4.2.m2.1.1" xref="S5.SS2.p4.2.m2.1.1.cmml"><mrow id="S5.SS2.p4.2.m2.1.1.2" xref="S5.SS2.p4.2.m2.1.1.2.cmml"><mn id="S5.SS2.p4.2.m2.1.1.2.2" xref="S5.SS2.p4.2.m2.1.1.2.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS2.p4.2.m2.1.1.2.1" xref="S5.SS2.p4.2.m2.1.1.2.1.cmml">Ã—</mo><msup id="S5.SS2.p4.2.m2.1.1.2.3" xref="S5.SS2.p4.2.m2.1.1.2.3.cmml"><mn id="S5.SS2.p4.2.m2.1.1.2.3.2" xref="S5.SS2.p4.2.m2.1.1.2.3.2.cmml">10</mn><mo id="S5.SS2.p4.2.m2.1.1.2.3.3" xref="S5.SS2.p4.2.m2.1.1.2.3.3.cmml">âˆ’</mo></msup></mrow><mo lspace="0em" rspace="0em" id="S5.SS2.p4.2.m2.1.1.1" xref="S5.SS2.p4.2.m2.1.1.1.cmml">â€‹</mo><mn id="S5.SS2.p4.2.m2.1.1.3" xref="S5.SS2.p4.2.m2.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.2.m2.1b"><apply id="S5.SS2.p4.2.m2.1.1.cmml" xref="S5.SS2.p4.2.m2.1.1"><times id="S5.SS2.p4.2.m2.1.1.1.cmml" xref="S5.SS2.p4.2.m2.1.1.1"></times><apply id="S5.SS2.p4.2.m2.1.1.2.cmml" xref="S5.SS2.p4.2.m2.1.1.2"><times id="S5.SS2.p4.2.m2.1.1.2.1.cmml" xref="S5.SS2.p4.2.m2.1.1.2.1"></times><cn type="integer" id="S5.SS2.p4.2.m2.1.1.2.2.cmml" xref="S5.SS2.p4.2.m2.1.1.2.2">3</cn><apply id="S5.SS2.p4.2.m2.1.1.2.3.cmml" xref="S5.SS2.p4.2.m2.1.1.2.3"><csymbol cd="ambiguous" id="S5.SS2.p4.2.m2.1.1.2.3.1.cmml" xref="S5.SS2.p4.2.m2.1.1.2.3">superscript</csymbol><cn type="integer" id="S5.SS2.p4.2.m2.1.1.2.3.2.cmml" xref="S5.SS2.p4.2.m2.1.1.2.3.2">10</cn><minus id="S5.SS2.p4.2.m2.1.1.2.3.3.cmml" xref="S5.SS2.p4.2.m2.1.1.2.3.3"></minus></apply></apply><cn type="integer" id="S5.SS2.p4.2.m2.1.1.3.cmml" xref="S5.SS2.p4.2.m2.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.2.m2.1c">3\times 10^{-}4</annotation></semantics></math>.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.1" class="ltx_p">We decay exponentially the learning rate every 20 epochs with a factor of 0.9. We use a combination of a weighted cross-entropy loss and a dice loss with the recommended parameters described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> to train our model as we find it provides the best results. To avoid overfitting, we apply horizontal and vertical flipping data augmentation. We also use an early stopping strategy to stop training if the modelâ€™s performance does not improve for 15 epochs. Training multi-view models is computationally expensive (around six days for TMVA-Net and five days for ours). As a result, we train models using the same seed as the baseline for a fair comparison. We use the pre-trained weights of TMVA-Net and MV-Net to evaluate baselines.</p>
</div>
<div id="S5.SS2.p6" class="ltx_para ltx_noindent">
<p id="S5.SS2.p6.1" class="ltx_p"><span id="S5.SS2.p6.1.1" class="ltx_text ltx_font_bold">Results.</span> TableÂ <a href="#S5.T5" title="TABLE V â€£ V-B Multi-view semantic segmentation â€£ V Experiments â€£ A recurrent CNN for online object detection on raw radar frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> shows the results we obtain on the CARRADA dataset. Our multi-view approaches beat the state-of-the-art model TMVA-Net on the multi-view radar semantic segmentation task while using two times fewer parameters and requiring significantly fewer GMACS. Our approach seems to correctly learn the variety of objectsâ€™ shapes without complex operations such as the atrous spatial pyramid pooling (ASPP) used in TMVA-Net. We notice that using recurrent units instead of 3D convolutions in a multi-view approach significantly helps to improve the classification of bicyclists and cars, especially on the RA view, where we double the IoU for bicyclists compared to TMVA-Net. However, bicyclists and pedestrians are very similar classes, and improving the detection performance of bicyclists leads to a loss in the detection performance of pedestrians for the RA view. In the RD view, MV-RECORD models outperform the TMVA-Net approach for all classes. We notice a huge gap in RA view performances between the CARRADA dataset and the CRUW dataset, as well as between the two views of the CARRADA dataset. We hypothesise that the small frame rate of the CARRADA dataset might cause these differences. Indeed, the RD view contains Doppler information, enabling one to learn the dynamics of targets. However, the RA view might not contain as much motion information as in the ROD2021 dataset, where the frame rate is higher, allowing the network to learn the dynamics of targets even in the RA view.
Unfortunately, we cannot share the same analysis for the online multi-view approach. Compared to the results on the ROD2021 dataset, where the online approach performs better than the buffer one, we could not find proper training settings for the online multi-view model. Despite MV-RECORD online reaching higher IoU than TMVA-Net on the RA view, this model performs similarly with TMVA-Net on the RD view but has significantly lower IoU than the MV-RECORD buffer approach. We think these differences are mostly optimisation problems. Indeed, we show the online training outperforms the buffer training when using a single view on both the ROD2021 (tableÂ <a href="#S5.T1" title="TABLE I â€£ V-A Single view object detection â€£ V Experiments â€£ A recurrent CNN for online object detection on raw radar frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>) and the CARRADA dataset. Especially on the RD view, our single view and online model outperforms TMVA-Net without using the angle information, with 8 times fewer parameters and less computations. This confirms that the low frame rate of the CARRADA dataset limits the motion information that the recurrent layers can learn. Finally, despite having fewer GMACS and parameters than TMVA-Net, our multi-view model (buffer) is much slower in inference than TMVA-Net and is unsuitable for real-time application. The online version is faster and should be preferred for real-time applications. Decreasing the size of the feature maps in the early layer of the network might help to increase the inference speed of the model. Also, we notice using a profiler that the LayerNorm operation takes up to 90% of the inference time for the multi-view models and up to 70% of the inference time for the single-view models. Replacing layer normalisation with batch normalisation should speed up the runtime of our approaches. Given the good results of the single-view approach (especially for the RD view), we recommend using our model for single-view inputs, as RECORD was originally designed for this single-view object detection.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.5.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.6.2" class="ltx_text ltx_font_italic">Discussion</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Here we discuss and analyse the results presented in sections <a href="#S5.SS1" title="V-A Single view object detection â€£ V Experiments â€£ A recurrent CNN for online object detection on raw radar frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-A</span></span></a> and <a href="#S5.SS2" title="V-B Multi-view semantic segmentation â€£ V Experiments â€£ A recurrent CNN for online object detection on raw radar frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-B</span></span></a>. First, radar data differs from LiDAR and images. The most critical differences being 1) the data is simpler in terms of variety, size, and complexity of the patterns; and 2) the datasets are smaller. We thus believe that our lighter architectures are flexible enough, while being less sensitive than huge backbones and less prone to overfitting for radar data. This mainly explains why Bottleneck LSTMs perform better than ConvLSTMs/ConvGRUs (see tableÂ <a href="#S5.T2" title="TABLE II â€£ V-A Single view object detection â€£ V Experiments â€£ A recurrent CNN for online object detection on raw radar frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>). Also, we think convolutional LSTMs are more adapted to radar sequences because 1) convLSTMs learn long-term spatio-temporal dependencies at multiple scales, which 3D convolution cannot do because of the limited size of the temporal kernel; 2) LSTMs can learn to weigh contributions of different frames which can be seen as an adaptive frame rate depending on the scenarios and the speed of vehicles; 3) Hidden states keep the position/velocity of objects in previous frames in memory and use it to predict the position in the next timesteps. Indeed, we show that, except for MV-RECORD, which is hard to optimise, online methods generally perform better than buffer ones while having lower computational cost (GMACs and inference time).</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">To conclude, although multi-view methods are interesting for research purposes, we find them difficult and long to optimise. Current radar generations generally detect targets in the range-Doppler view and compute the direction of arrival for each detected target to save computation time. As the RAD cube is cumbersome to compute and store in memory, we suggest using our model on single-view inputs (RD or RA), depending on the desired application.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">In this work, we tackled the problem of online object detection for radar using recurrent neural networks. Contrary to well-known radar object detectors, which use a single frame to detect objects in different radar representations, we learn spatial and temporal relationships between frames by leveraging characteristics of FMCW radar signal. We propose a new architecture type that iteratively learns spatial and temporal features throughout a recurrent convolutional encoder. We designed an end-to-end, efficient, causal and generic framework that can process different types of radar data and perform various detection tasks (key point detection, semantic segmentation). Our methods outperform competing methods on both CARRADA and ROD2021 datasets. Notably, our models help distinguish pedestrians and cyclists better and learn the target variations better than 3D approaches.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">The main challenge in the near future will be to embed this model onboard real cars. This will require a more involved training, a quantisation of the model, and improved data augmentation or domain adaptation strategies to cope with the limited amount of labelled data.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M.Â Maire, S.Â J. Belongie, J.Â Hays, P.Â Perona, D.Â Ramanan,
P.Â DollÃ¡r, and C.Â L. Zitnick, â€œMicrosoft COCO: Common Objects in
Context,â€ in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">ECCV</em>, 2014.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
R.Â Urtasun, P.Â Lenz, and A.Â Geiger, â€œAre we ready for autonomous driving? The
KITTI vision benchmark suite,â€ in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">2012 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR)</em>.Â Â Â Los Alamitos, CA, USA: IEEE Computer Society, jun 2012, pp.
3354â€“3361.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S.Â Blake, â€œOS-CFAR theory for multiple targets and nonuniform clutter,â€
<em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Aerospace and Electronic Systems</em>, vol.Â 24, no.Â 6,
pp. 785â€“790, 1988.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
R.Â E. Kalman, â€œA new approach to linear filtering and prediction problems,â€
<em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Transactions of the ASMEâ€“Journal of Basic Engineering</em>, vol.Â 82, no.
Series D, pp. 35â€“45, 1960.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A.Â Palffy, E.Â Pool, S.Â Baratam, J.Â F. Kooij, and D.Â M. Gavrila, â€œMulti-class
Road User Detection with 3+ 1D Radar in the View-of-Delft Dataset,â€
<em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IEEE Robotics and Automation Letters</em>, vol.Â 7, no.Â 2, pp. 4961â€“4968,
2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
N.Â Scheiner, O.Â Schumann, F.Â Kraus, N.Â Appenrodt, J.Â Dickmann, and B.Â Sick,
â€œOff-the-shelf sensor vs. experimental radar - How much resolution is
necessary in automotive radar classification?â€ in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">2020 IEEE 23rd
International Conference on Information Fusion (FUSION)</em>, 2020, pp. 1â€“8.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
A.Â Danzer, T.Â Griebel, M.Â Bach, and K.Â Dietmayer, â€œ2D Car Detection in
Radar Data with PointNets,â€ in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">2019 IEEE Intelligent
Transportation Systems Conference (ITSC)</em>, 2019, pp. 61â€“66.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
J.Â Liu, W.Â Xiong, L.Â Bai, Y.Â Xia, T.Â Huang, W.Â Ouyang, and B.Â Zhu, â€œDeep
Instance Segmentation with Automotive Radar Detection Points,â€
<em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Intelligent Vehicles</em>, pp. 1â€“1, 2022, conference
Name: IEEE Transactions on Intelligent Vehicles.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
A.Â Ouaknine, A.Â Newson, J.Â Rebut, F.Â Tupin, and P.Â PÃ©rez, â€œCARRADA Dataset:
Camera and Automotive Radar with Range- Angle- Doppler Annotations,â€ in
<em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">2020 25th International Conference on Pattern Recognition (ICPR)</em>,
2021, pp. 5068â€“5075.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Y.Â Wang, G.Â Wang, H.-M. Hsu, H.Â Liu, and J.-N. Hwang, â€œRethinking of Radarâ€™s
Role: A Camera-Radar Dataset and Systematic Annotator via Coordinate
Alignment,â€ in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) Workshops</em>, June 2021, pp. 2815â€“2824.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
J.Â Rebut, A.Â Ouaknine, W.Â Malik, and P.Â PÃ©rez, â€œRaw High-Definition Radar
for Multi-Task Learning,â€ in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR)</em>, June 2022, pp.
17â€‰021â€“17â€‰030.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
M.Â Ulrich, C.Â GlÃ¤ser, and F.Â Timm, â€œDeepReflecs: Deep Learning for
Automotive Object Classification with Radar Reflections,â€ in
<em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">2021 IEEE Radar Conference (RadarConf21)</em>, May 2021, pp. 1â€“6,
iSSN: 2375-5318.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
K.Â Patel, W.Â Beluch, K.Â Rambach, M.Â Pfeiffer, and B.Â Yang, â€œImproving
Uncertainty of Deep Learning-based Object Classification on Radar
Spectra using Label Smoothing,â€ in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">2022 IEEE Radar
Conference (RadarConf22)</em>, Mar. 2022, pp. 1â€“6.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
C.Â Decourt, R.Â VanRullen, D.Â Salle, and T.Â Oberlin, â€œDAROD: A Deep
Automotive Radar Object Detector on Range-Doppler maps,â€ in
<em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">2022 IEEE Intelligent Vehicles Symposium (IV)</em>, Jun. 2022,
pp. 112â€“118.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
M.Â Meyer, G.Â Kuschk, and S.Â Tomforde, â€œGraph Convolutional Networks for 3D
Object Detection on Radar Data,â€ in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV) Workshops</em>, October 2021,
pp. 3060â€“3069.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
R.Â Franceschi and D.Â Rachkov, â€œDeep learning-based Radar Detector for Complex
Automotive Scenarios,â€ in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">2022 IEEE Intelligent Vehicles Symposium
(IV)</em>.Â Â Â IEEE, 2022, pp. 303â€“308.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
A.Â Ouaknine, A.Â Newson, P.Â PÃ©rez, F.Â Tupin, and J.Â Rebut, â€œMulti-View Radar
Semantic Segmentation,â€ in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV)</em>, October 2021, pp. 15â€‰671â€“15â€‰680.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
P.Â Kaul, D.Â DeÂ Martini, M.Â Gadd, and P.Â Newman, â€œRSS-Net: weakly-supervised
multi-class semantic segmentation with FMCW radar,â€ in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">2020 IEEE
Intelligent Vehicles Symposium (IV)</em>.Â Â Â IEEE, 2020, pp. 431â€“436.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Y.Â Wang, Z.Â Jiang, Y.Â Li, J.-N. Hwang, G.Â Xing, and H.Â Liu, â€œRODNet: A
Real-Time Radar Object Detection Network Cross-Supervised by Camera-Radar
Fused Object 3D Localization,â€ <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in
Signal Processing</em>, vol.Â 15, no.Â 4, pp. 954â€“967, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
B.Â Ju, W.Â Yang, J.Â Jia, X.Â Ye, Q.Â Chen, X.Â Tan, H.Â Sun, Y.Â Shi, and E.Â Ding,
â€œDANet: Dimension Apart Network for Radar Object Detection,â€ in
<em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 International Conference on Multimedia
Retrieval</em>, 2021, pp. 533â€“539.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
B.Â Major, D.Â Fontijne, A.Â Ansari, R.Â TejaÂ Sukhavasi, R.Â Gowaikar, M.Â Hamilton,
S.Â Lee, S.Â Grzechnik, and S.Â Subramanian, â€œVehicle Detection With
Automotive Radar Using Deep Learning on Range-Azimuth-Doppler Tensors,â€ in
<em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision
(ICCV) Workshops</em>, Oct 2019.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
P.Â Li, P.Â Wang, K.Â Berntorp, and H.Â Liu, â€œExploiting Temporal Relations on
Radar Perception for Autonomous Driving,â€ in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, June
2022, pp. 17â€‰071â€“17â€‰080.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
S.Â M. Patole, M.Â Torlak, D.Â Wang, and M.Â Ali, â€œAutomotive Radars: A Review of
Signal Processing Techniques,â€ <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing Magazine</em>,
vol.Â 34, no.Â 2, pp. 22â€“35, 2017.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
M.Â Sandler, A.Â Howard, M.Â Zhu, A.Â Zhmoginov, and L.-C. Chen, â€œMobileNetV2:
Inverted Residuals and Linear Bottlenecks,â€ in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, June
2018.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
X.Â SHI, Z.Â Chen, H.Â Wang, D.-Y. Yeung, W.-k. Wong, and W.-c. WOO,
â€œConvolutional LSTM Network: A Machine Learning Approach for Precipitation
Nowcasting,â€ in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
vol.Â 28.Â Â Â Curran Associates, Inc.,
2015.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
M.Â Liu and M.Â Zhu, â€œMobile Video Object Detection With Temporally-Aware
Feature Maps,â€ in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR)</em>, June 2018.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
O.Â Ronneberger, P.Â Fischer, and T.Â Brox, â€œU-Net: Convolutional Networks for
Biomedical Image Segmentation,â€ in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Medical Image Computing and
Computer-Assisted Intervention â€“ MICCAI 2015</em>, N.Â Navab, J.Â Hornegger, W.Â M.
Wells, and A.Â F. Frangi, Eds.Â Â Â Springer
International Publishing, 2015, pp. 234â€“241.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
L.-C. Chen, Y.Â Zhu, G.Â Papandreou, F.Â Schroff, and H.Â Adam, â€œEncoder-Decoder
with Atrous Separable Convolution for Semantic Image Segmentation,â€ in
<em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Computer Vision (ECCV)</em>,
September 2018.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
X.Â Zhu, Y.Â Wang, J.Â Dai, L.Â Yuan, and Y.Â Wei, â€œFlow-Guided Feature
Aggregation for Video Object Detection,â€ in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
International Conference on Computer Vision (ICCV)</em>, Oct 2017.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Y.Â Yu, J.Â Yuan, G.Â Mittal, L.Â Fuxin, and M.Â Chen, â€œBATMAN: Bilateral
Attention Transformer in Motion-Appearance Neighboring Space for Video Object
Segmentation,â€ <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2208.01159</em>, 2022.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
X.Â Li, H.Â Zhao, and L.Â Zhang, â€œRecurrent RetinaNet: A Video Object Detection
Model based on Focal Loss,â€ in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">International conference on neural
information processing</em>.Â Â Â Springer,
2018, pp. 499â€“508.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
A.Â Pfeuffer, K.Â Schulz, and K.Â Dietmayer, â€œSemantic Segmentation of Video
Sequences with Convolutional LSTMs,â€ in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">2019 IEEE intelligent
vehicles symposium (IV)</em>.Â Â Â IEEE, 2019,
pp. 1441â€“1447.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
C.Â Ventura, M.Â Bellver, A.Â Girbau, A.Â Salvador, F.Â Marques, and X.Â Giro-i
Nieto, â€œRVOS: End-To-End Recurrent Network for Video Object
Segmentation,â€ in <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR)</em>, June 2019.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
C.Â Zhang and J.Â Kim, â€œModeling Long-and Short-Term Temporal Context for Video
Object Detection,â€ in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">2019 IEEE international conference on image
processing (ICIP)</em>.Â Â Â IEEE, 2019, pp.
71â€“75.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
V.Â S.Â F. Garnot and L.Â Landrieu, â€œPanoptic Segmentation of Satellite Image
Time Series With Convolutional Temporal Attention Networks,â€ in
<em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision
(ICCV)</em>, October 2021, pp. 4872â€“4881.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
B.Â Duke, A.Â Ahmed, C.Â Wolf, P.Â Aarabi, and G.Â W. Taylor, â€œSSTVOS: Sparse
Spatiotemporal Transformers for Video Object Segmentation,â€ in
<em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, June 2021, pp. 5912â€“5921.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Y.Â Chen, Y.Â Cao, H.Â Hu, and L.Â Wang, â€œMemory Enhanced Global-Local
Aggregation for Video Object Detection,â€ in <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>, June 2020.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
F.Â Xiao and Y.Â J. Lee, â€œVideo Object Detection with an Aligned
Spatial-Temporal Memory,â€ in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference
on Computer Vision (ECCV)</em>, September 2018.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
G.Â Bertasius, L.Â Torresani, and J.Â Shi, â€œObject detection in video with
spatiotemporal sampling networks,â€ in <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European
Conference on Computer Vision (ECCV)</em>, 2018, pp. 331â€“346.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
O.Â Schumann, M.Â Hahn, J.Â Dickmann, and C.Â WÃ¶hler, â€œSemantic Segmentation on
Radar Point Clouds,â€ in <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">2018 21st International Conference on
Information Fusion (FUSION)</em>, 2018, pp. 2179â€“2186.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
X.Â Gao, G.Â Xing, S.Â Roy, and H.Â Liu, â€œRAMP-CNN: A Novel Neural Network for
Enhanced Automotive Radar Object Recognition,â€ <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">IEEE Sensors Journal</em>,
vol.Â 21, no.Â 4, pp. 5119â€“5132, 2021.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
A.Â Zhang, F.Â E. Nowruzi, and R.Â Laganiere, â€œRADDet:
Range-Azimuth-Doppler based Radar Object Detection for Dynamic
Road Users,â€ in <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">2021 18th Conference on Robots and Vision (CRV)</em>,
2021, pp. 95â€“102.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Z.Â Zheng, X.Â Yue, K.Â Keutzer, and A.Â SangiovanniÂ Vincentelli, â€œScene-Aware
Learning Network for Radar Object Detection,â€ in <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
2021 International Conference on Multimedia Retrieval</em>, 2021, pp. 573â€“579.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
D.Â NiederlÃ¶hner, M.Â Ulrich, S.Â Braun, D.Â KÃ¶hler, F.Â Faion,
C.Â GlÃ¤ser, A.Â Treptow, and H.Â Blume, â€œSelf-Supervised Velocity
Estimation for Automotive Radar Object Detection Networks,â€ in <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">2022
IEEE Intelligent Vehicles Symposium (IV)</em>.Â Â Â IEEE, 2022, pp. 352â€“359.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
M.Â J. Jones, A.Â Broad, and T.-Y. Lee, â€œRecurrent Multi-frame Single Shot
Detector for Video Object Detection,â€ in <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">British Machine Vision
Conference (BMVC)</em>, Sep. 2018.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
N.Â Ballas, L.Â Yao, C.Â Pal, and A.Â C. Courville, â€œDelving Deeper into
Convolutional Networks for Learning Video Representations,â€ in <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">4th
International Conference on Learning Representations, ICLR 2016, San Juan,
Puerto Rico, May 2-4, 2016, Conference Track Proceedings</em>, 2016.

</span>
</li>
</ul>
</section>
<figure id="id9" class="ltx_float biography">
<table id="id9.1" class="ltx_tabular">
<tr id="id9.1.1" class="ltx_tr">
<td id="id9.1.1.1" class="ltx_td"><img src="/html/2212.11172/assets/images/sw.jpg" id="id9.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="89" height="125" alt="[Uncaptioned image]"></td>
<td id="id9.1.1.2" class="ltx_td">
<span id="id9.1.1.2.1" class="ltx_inline-block">
<span id="id9.1.1.2.1.1" class="ltx_p"><span id="id9.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Colin Decourt</span> 
is a PhD candidate as part of ANITI (Artificial Natural Intelligence Toulouse Institute). He received his Masterâ€™s degree in telecommunications and artificial intelligence at Bordeaux Institute of Technology in 2020.
He started his PhD with ISAE-SUPAERO, CerCo (CNRS UMR5549) and NXP Semiconductors in October 2020. His research focuses on scene understanding for automotive FMCW radars (targets detection, classification and tracking) using artificial intelligence.</span>
<span id="id9.1.1.2.1.2" class="ltx_p">His research interest includes image and radar processing, computer vision, and self-supervised learning.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id10" class="ltx_float biography">
<table id="id10.1" class="ltx_tabular">
<tr id="id10.1.1" class="ltx_tr">
<td id="id10.1.1.1" class="ltx_td"><img src="/html/2212.11172/assets/images/VanRullen.jpg" id="id10.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="94" height="125" alt="[Uncaptioned image]"></td>
<td id="id10.1.1.2" class="ltx_td">
<span id="id10.1.1.2.1" class="ltx_inline-block">
<span id="id10.1.1.2.1.1" class="ltx_p"><span id="id10.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Rufin VanRullen</span> 
studied Mathematics and Computer Science, then quickly turned to Cognitive Sciences. During his PhD from UniversitÃ© Paul Sabatier, Toulouse, France (2000), he worked on neural coding and rapid visual processing under the supervision of Simon J. Thorpe. As a post-doctoral researcher at the California Institute of technology with Cristof Koch, he became interested in the mechanisms of visual attention and consciousness. Since 2002, he is a Research Director at the CNRS in the Brain and Cognition Research Center (CerCo) of Toulouse. His work in experimental and computational neuroscience explored the role of brain oscillations in cognition. In particular, he demonstrated that rhythmic brain activity makes our perception periodicâ€“-a rapid sequence of perceptual cycles, akin to a video sequence. More recently, his research focuses on AI and deep neural networks. He holds a Research Chair in the Artificial and Natural Intelligence Toulouse Institute (ANITI) and has received several European grants (European Young Investigator Award, ERC Consolidator grant, ERC Advanced grant) as well as the CNRS bronze medal in 2007.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id11" class="ltx_float biography">
<table id="id11.1" class="ltx_tabular">
<tr id="id11.1.1" class="ltx_tr">
<td id="id11.1.1.1" class="ltx_td"><img src="/html/2212.11172/assets/images/to_2023_zoom.png" id="id11.1.1.1.g1" class="ltx_graphics ltx_img_square" width="100" height="115" alt="[Uncaptioned image]"></td>
<td id="id11.1.1.2" class="ltx_td">
<span id="id11.1.1.2.1" class="ltx_inline-block">
<span id="id11.1.1.2.1.1" class="ltx_p"><span id="id11.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Thomas Oberlin</span> 
received the M.S. degree in applied mathematics from UniversitÃ© Joseph Fourier, Grenoble, France, in 2010, as well as an engineerâ€™s degree from Grenoble Institute of Technology. In 2013, he received the Ph.D. in applied mathematics from the University of Grenoble. In 2014, he was a post-doctoral fellow in signal processing and medical imaging at Inria Rennes, France, before joining as an Assistant Professor INP Toulouse â€“ ENSEEIHT and the IRIT Laboratory, UniversitÃ© de Toulouse, France. Since 2019, he is an Associate Professor in artificial intelligence and image processing at ISAE-SUPAERO, UniversitÃ© de Toulouse, France.</span>
<span id="id11.1.1.2.1.2" class="ltx_p">His research interests are in signal, image and data processing and in particular time-frequency analysis, representation learning, and sparse/low-rank regularizations for inverse problems.</span>
<span id="id11.1.1.2.1.3" class="ltx_p">Since 2022, he serves as an Associate Editor for the IEEE Transactions on Signal Processing.</span>
</span>
</td>
</tr>
</table>
</figure>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2212.11171" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2212.11172" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2212.11172">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2212.11172" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2212.11173" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 09:54:50 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
