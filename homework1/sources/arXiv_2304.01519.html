<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2304.01519] LiDAR-Based 3D Object Detection via Hybrid 2D Semantic Scene Generation</title><meta property="og:description" content="Bird’s-Eye View (BEV) features are popular intermediate scene representations shared by the 3D backbone and the detector head in LiDAR-based object detectors. However, little research has been done to investigate how t…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="LiDAR-Based 3D Object Detection via Hybrid 2D Semantic Scene Generation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="LiDAR-Based 3D Object Detection via Hybrid 2D Semantic Scene Generation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2304.01519">

<!--Generated on Thu Feb 29 16:46:59 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_pruned_first" lang="en">
<h1 class="ltx_title ltx_title_document">LiDAR-Based 3D Object Detection via Hybrid 2D Semantic Scene Generation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Haitao Yang<sup id="id4.1.id1" class="ltx_sup">1</sup>      
Zaiwei Zhang<sup id="id5.2.id2" class="ltx_sup">2</sup>      
Xiangru Huang<sup id="id6.3.id3" class="ltx_sup">3</sup>      
Min Bai<sup id="id7.4.id4" class="ltx_sup">2</sup>      
Chen Song<sup id="id8.5.id5" class="ltx_sup">1</sup>

<br class="ltx_break">Bo Sun<sup id="id9.6.id6" class="ltx_sup">1</sup>        
Li Erran Li<sup id="id10.7.id7" class="ltx_sup">2</sup>        
Qixing Huang<sup id="id11.8.id8" class="ltx_sup">1</sup>


<br class="ltx_break"><sup id="id12.9.id9" class="ltx_sup">1</sup>The University of Texas at Austin        <sup id="id13.10.id10" class="ltx_sup">2</sup>AWS AI        <sup id="id14.11.id11" class="ltx_sup">3</sup>MIT CSAIL
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id15.id1" class="ltx_p"><span id="id15.id1.1" class="ltx_text">Bird’s-Eye View (BEV) features are popular intermediate scene representations shared by the 3D backbone and the detector head in LiDAR-based object detectors. However, little research has been done to investigate how to incorporate additional supervision on the BEV features to improve proposal generation in the detector head, while still balancing the number of powerful 3D layers and efficient 2D network operations. This paper proposes a novel scene representation that encodes both the semantics and geometry of the 3D environment in 2D, which serves as a dense supervision signal for better BEV feature learning. The key idea is to use auxiliary networks to predict a combination of explicit and implicit semantic probabilities by exploiting their complementary properties. Extensive experiments show that our simple yet effective design can be easily integrated into most state-of-the-art 3D object detectors and consistently improves upon baseline models.</span></p>
</div>
<div id="id3" class="ltx_logical-block">
<div id="id3.p1" class="ltx_para">
<table id="id2.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="id2.2.2" class="ltx_tr">
<td id="id1.1.1.1" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;"><img src="/html/2304.01519/assets/x1.png" id="id1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="230" height="71" alt="[Uncaptioned image]"></td>
<td id="id2.2.2.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;"><img src="/html/2304.01519/assets/x2.png" id="id2.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="230" height="71" alt="[Uncaptioned image]"></td>
</tr>
<tr id="id2.2.3.1" class="ltx_tr">
<td id="id2.2.3.1.1" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">(a) 2D Semantic Scene</td>
<td id="id2.2.3.1.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">(b) Pipeline Comparison</td>
</tr>
</tbody>
</table>
</div>
<figure id="S0.F1" class="ltx_figure ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>(a) We propose to integrate the 2D semantic scene representation into LiDAR-based 3D object detectors, which can be generated by either an explicit network (left) or an implicit network (middle) using the projection of 3D bounding boxes into the Bird’s Eye View (right) as supervision. (b) The proposed 2D semantic scene representation can be easily integrated into most existing detectors (top) as a 2D semantic scene generation module (bottom) with minimal engineering efforts.</figcaption>
</figure>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">LiDAR-based object detection is a critical task in 3D recognition with important applications in autonomous driving and robotics. Unlike the depth sensor commonly used indoors, a LiDAR device is often mounted on a moving vehicle and is subject to dynamic outdoor conditions. The differences in sampling densities, object sizes, and scene scales pose a set of unique challenges, stimulating a wave of research efforts to design 3D object detection models specifically for LiDAR point clouds.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Existing works in LiDAR-based 3D object detection typically use points <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, voxel grids <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, or their hybrids <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> as the input representation of the 3D scene. Point-based methods utilize PointNet and its variants <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> as the 3D backbone to extract point-wise features directly from the input. In contrast, grid-based methods voxelize the raw point cloud by either simple discretization or point-based feature aggregation within each cell <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Thanks to the sparse convolutional operation, grid-based methods are computationally efficient and usually outperform their point-based counterparts. At the time of paper writing, grid-based methods have a dominant role in LiDAR-based 3D object detection.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">This paper seeks to design an efficient and powerful LiDAR-based 3D object detector. A typical LiDAR-based 3D object detector first uses a 3D backbone to extract voxel features via sparse convolution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> or pillar features via PointNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> from the input. The extracted features are then projected to 2D, forming Bird’s-Eye View (BEV) features. Finally, the detection head takes the BEV features as input and generates object proposals. In this pipeline, the BEV features serve as condensed representations of the 3D scene, designed to balance the number of powerful 3D layers and efficient 2D network operations. The quality of BEV features directly affects proposal generation and dictates the final prediction accuracy. However, little research has been done to investigate how to incorporate additional supervision on the BEV features for better proposal generation. Among the limited number of existing works in this direction, we observe that 3D semantic information has an impressive ability to improve BEV feature qualities. Specifically, several works, including both point-based and grid-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, propose to integrate foreground-background segmentation predictions into the BEV representation. Instead of the per-point segmentation adopted by these works, Part-<math id="S1.p3.1.m1.1" class="ltx_Math" alttext="A^{2}" display="inline"><semantics id="S1.p3.1.m1.1a"><msup id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml"><mi id="S1.p3.1.m1.1.1.2" xref="S1.p3.1.m1.1.1.2.cmml">A</mi><mn id="S1.p3.1.m1.1.1.3" xref="S1.p3.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><apply id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S1.p3.1.m1.1.1.1.cmml" xref="S1.p3.1.m1.1.1">superscript</csymbol><ci id="S1.p3.1.m1.1.1.2.cmml" xref="S1.p3.1.m1.1.1.2">𝐴</ci><cn type="integer" id="S1.p3.1.m1.1.1.3.cmml" xref="S1.p3.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">A^{2}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> learns intra-object part locations, which has the additional advantage of utilizing the geometric information from the input point cloud. While beneficial, 3D semantic and geometric supervisions are sparse and fail to compensate for the problem of missing points. Each pixel in the BEV feature map is expected to propose a bounding box center in all commonly used detection heads such as SSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> and CenterPoint <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. When the 3D input contains very few points around the object center, which is surprisingly common since the LiDAR device only captures a sparse sample of partial surfaces in the environment, the detection head is unable to generate accurate proposals from the BEV features. Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ LiDAR-Based 3D Object Detection via Hybrid 2D Semantic Scene Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents an illustration of this problem.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2304.01519/assets/x3.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="237" height="133" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>When the ground-truth bounding box contains very few object points, the sparse supervision paradigm used by existing works (left) fails to provide strong enough regularization for object detection. In contrast, the proposed 2D semantic scene representation utilizes dense training supervision (right), covering interior locations without point samples.</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Recent research in 3D scene understanding has demonstrated the importance of dense supervision. For example, Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> improve the single-sweep LiDAR semantic segmentation performance using auxiliary scene completion, while Xu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> show that generating semantic points that faithfully recover occluded foreground regions enhances object detection accuracy. Unfortunately, obtaining ground-truth annotations of the dense 3D geometry is extremely challenging. To mitigate this issue, we introduce a novel 2D scene representation that encodes the 3D semantics for LiDAR-based object detection. Specifically, consider the direct projection of the 3D scene into the BEV. We model the 2D scene representation as a probability distribution map, where points inside foreground regions are assigned with higher probabilities than the background points. Examples of the proposed semantic scene representation are shown in Figure <a href="#S0.F1" title="Figure 1 ‣ LiDAR-Based 3D Object Detection via Hybrid 2D Semantic Scene Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (a). Unlike conventional segmentation masks, the 2D semantic scene representation consists of real-valued probabilities instead of binary labels. When generated by different methods, the probabilities display different properties. We use different colormaps to show three possible ways to generate the 2D semantic scene representation, including using an explicit network, using an implicit network, and directly projecting ground-truth 3D boxes into 2D. Notably, the 2D semantic scene representation provides dense semantic and geometric information, covering all pixels on the BEV feature map, regardless of the underlying generation method.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">A straightforward way to generate the 2D semantic scene representation is to use a discretized image grid. While high-resolution 2D representations are preferable for small objects such as pedestrians, concerns about computational efficiency and memory consumption impede their applications in large 3D scenes. To maintain a careful balance, we combine explicit and implicit representations. We use an explicit network and an implicit network to parallelly generate 2D semantic scenes from the raw BEV features. The refined BEV features are then obtained by fusing the explicit and implicit 2D semantic scenes. We conduct a detailed analysis to reveal how explicit and implicit representations possess complementary properties. Incorporating 2D semantic scene representations require very small modification to existing pipelines. As shown in Figure <a href="#S0.F1" title="Figure 1 ‣ LiDAR-Based 3D Object Detection via Hybrid 2D Semantic Scene Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (b), we introduce an innovative pipeline by adding an extra BEV feature refinement module, which explicitly refines the initial BEV features with 2D dense supervision, referred to as the Semantic Scene Generation Net (SSGNet).</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">We integrate SSGNet to various existing LiDAR-based object detectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> and evaluate SSGNet on both the Waymo Open Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> and nuScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Extensive experiments demonstrate consistent improvements to all the baseline approaches. SSGNet achieves 78.3%, 81.1%, and 74.6% mAP for the vehicle, pedestrian, and cyclist classes on Waymo (level 1) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, and 61.7% mAP for all classes on nuScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Codes will be available at <a target="_blank" href="https://github.com/yanghtr/SSGNet" title="" class="ltx_ref ltx_href">https://github.com/yanghtr/SSGNet</a>.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">In summary, we make the following contributions.</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We introduce a novel 2D semantic scene representation for LiDAR-based 3D object detection.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We show how to use explicit and implicit networks to jointly predict the proposed representation.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We utilize the predicted 2D semantic scene representation to obtain better BEV features, leading to significant improvement over existing detectors.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">We validate the benefits of refining BEV features using dense 2D supervision signals and advocate a new detection pipeline with direct BEV supervision.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The input to LiDAR-based 3D object detectors is usually represented by point clouds, voxel grids, range images, or their hybrids. We present a survey of recent developments in 3D object detection categorized by the input format.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Point-based methods.</span>
Point-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> typically apply PointNet and its variants <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> on unstructured point clouds to generate proposals from learned point features. Unlike grid-based methods, point-based methods do not suffer from information loss and nicely preserve structural details since they infer from the original LiDAR measurements without any quantization. However, point-based methods use a large number of costly sampling and grouping operations. Compared to methods based on other input representations, point-based methods are also more sensitive to sparsity variations, sampling irregularities, and occlusions.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Grid-based methods.</span> Grid-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> use discrete grids to represent the 3D scene. In addition to straightforward regular voxelization, cylindrical voxelization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> has been proven especially beneficial for semantic segmentation. This paper focuses on regular voxels since they are the most popular choices in object detection. Grid-based methods extract per-voxel features using a sparse convolution backbone <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. To compensate for the information loss due to voxelization, several works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> additionally utilize a light-weight PointNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> architecture for voxel feature encoding. Similar to existing works, our method adopts a grid-based representation for the input scene and uses out-of-the-box 3D grid-based backbones to obtain BEV features. We emphasize that existing grid-based methods fail to incorporate an understanding of the scene geometry into the BEV features, which motivates our design.</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Range-image-based methods.</span>
The raw output of a LiDAR sensor is a cylindrical range image. Taking its natural 2D representation as input, range-image-based methods typically apply well-established 2D backbones to extract features before projecting the feature maps to 3D for detection. Several approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> leverage the well-understood traditional 2D convolutional network to process range images directly for bounding box parameters regression. Closely related to our work, RSN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> uses range images for foreground point segmentation, followed by the projection of the learned features to 3D and the regression of bounding box parameters through 3D sparse convolution. While RSN relies on sparse 3D semantic supervision, our approach uses dense supervision.</p>
</div>
<div id="S2.p5" class="ltx_para ltx_noindent">
<p id="S2.p5.1" class="ltx_p"><span id="S2.p5.1.1" class="ltx_text ltx_font_bold">Hybrid representations.</span>
There is a growing interest in utilizing hybrid input representations for 3D perception tasks. As the pioneer, Frustum PointNets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> projects 2D images to the 3D frustum and combines image and point features for object detection. A few other approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> construct two separate network branches for point and voxel processing and fuse the extracted features for information aggregation. For example, HVPR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> concatenates point-based and voxel-based predictions as the final BEV features. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> combine BEV and the cylindrical view to obtain refined pillar features. Their important limitation is the lack of explicit supervision on BEV features. In contrast, we combine the 3D grid representation and 2D semantics, leading to higher efficiency and effectiveness than point-voxel hybrids.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Approach</h2>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2304.01519/assets/x4.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="528" height="146" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>SSGNet consists of three modules: the initial BEV feature extraction module, the BEV feature refinement module, and the detection head. The BEV feature refinement module generates hybrid BEV features via 2D semantic scene generation.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>2D Semantic Scene Representation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.3" class="ltx_p">The 2D semantic scene representation is the core of SSGNet, modeled as a 2D probability distribution map:</p>
<table id="S3.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex1.m1.2" class="ltx_Math" alttext="\psi:\mathbb{R}^{2}\to\mathbb{[}0,1]" display="block"><semantics id="S3.Ex1.m1.2a"><mrow id="S3.Ex1.m1.2.3" xref="S3.Ex1.m1.2.3.cmml"><mi id="S3.Ex1.m1.2.3.2" xref="S3.Ex1.m1.2.3.2.cmml">ψ</mi><mo lspace="0.278em" rspace="0.278em" id="S3.Ex1.m1.2.3.1" xref="S3.Ex1.m1.2.3.1.cmml">:</mo><mrow id="S3.Ex1.m1.2.3.3" xref="S3.Ex1.m1.2.3.3.cmml"><msup id="S3.Ex1.m1.2.3.3.2" xref="S3.Ex1.m1.2.3.3.2.cmml"><mi id="S3.Ex1.m1.2.3.3.2.2" xref="S3.Ex1.m1.2.3.3.2.2.cmml">ℝ</mi><mn id="S3.Ex1.m1.2.3.3.2.3" xref="S3.Ex1.m1.2.3.3.2.3.cmml">2</mn></msup><mo stretchy="false" id="S3.Ex1.m1.2.3.3.1" xref="S3.Ex1.m1.2.3.3.1.cmml">→</mo><mrow id="S3.Ex1.m1.2.3.3.3.2" xref="S3.Ex1.m1.2.3.3.3.1.cmml"><mo stretchy="false" id="S3.Ex1.m1.2.3.3.3.2.1" xref="S3.Ex1.m1.2.3.3.3.1.cmml">[</mo><mn id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml">0</mn><mo id="S3.Ex1.m1.2.3.3.3.2.2" xref="S3.Ex1.m1.2.3.3.3.1.cmml">,</mo><mn id="S3.Ex1.m1.2.2" xref="S3.Ex1.m1.2.2.cmml">1</mn><mo stretchy="false" id="S3.Ex1.m1.2.3.3.3.2.3" xref="S3.Ex1.m1.2.3.3.3.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.2b"><apply id="S3.Ex1.m1.2.3.cmml" xref="S3.Ex1.m1.2.3"><ci id="S3.Ex1.m1.2.3.1.cmml" xref="S3.Ex1.m1.2.3.1">:</ci><ci id="S3.Ex1.m1.2.3.2.cmml" xref="S3.Ex1.m1.2.3.2">𝜓</ci><apply id="S3.Ex1.m1.2.3.3.cmml" xref="S3.Ex1.m1.2.3.3"><ci id="S3.Ex1.m1.2.3.3.1.cmml" xref="S3.Ex1.m1.2.3.3.1">→</ci><apply id="S3.Ex1.m1.2.3.3.2.cmml" xref="S3.Ex1.m1.2.3.3.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.3.3.2.1.cmml" xref="S3.Ex1.m1.2.3.3.2">superscript</csymbol><ci id="S3.Ex1.m1.2.3.3.2.2.cmml" xref="S3.Ex1.m1.2.3.3.2.2">ℝ</ci><cn type="integer" id="S3.Ex1.m1.2.3.3.2.3.cmml" xref="S3.Ex1.m1.2.3.3.2.3">2</cn></apply><interval closure="closed" id="S3.Ex1.m1.2.3.3.3.1.cmml" xref="S3.Ex1.m1.2.3.3.3.2"><cn type="integer" id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1">0</cn><cn type="integer" id="S3.Ex1.m1.2.2.cmml" xref="S3.Ex1.m1.2.2">1</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.2c">\psi:\mathbb{R}^{2}\to\mathbb{[}0,1]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.2" class="ltx_p">where each 2D query point <math id="S3.SS1.p1.1.m1.2" class="ltx_Math" alttext="\boldsymbol{q}=(x,y)\in\mathbb{R}^{2}" display="inline"><semantics id="S3.SS1.p1.1.m1.2a"><mrow id="S3.SS1.p1.1.m1.2.3" xref="S3.SS1.p1.1.m1.2.3.cmml"><mi id="S3.SS1.p1.1.m1.2.3.2" xref="S3.SS1.p1.1.m1.2.3.2.cmml">𝒒</mi><mo id="S3.SS1.p1.1.m1.2.3.3" xref="S3.SS1.p1.1.m1.2.3.3.cmml">=</mo><mrow id="S3.SS1.p1.1.m1.2.3.4.2" xref="S3.SS1.p1.1.m1.2.3.4.1.cmml"><mo stretchy="false" id="S3.SS1.p1.1.m1.2.3.4.2.1" xref="S3.SS1.p1.1.m1.2.3.4.1.cmml">(</mo><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">x</mi><mo id="S3.SS1.p1.1.m1.2.3.4.2.2" xref="S3.SS1.p1.1.m1.2.3.4.1.cmml">,</mo><mi id="S3.SS1.p1.1.m1.2.2" xref="S3.SS1.p1.1.m1.2.2.cmml">y</mi><mo stretchy="false" id="S3.SS1.p1.1.m1.2.3.4.2.3" xref="S3.SS1.p1.1.m1.2.3.4.1.cmml">)</mo></mrow><mo id="S3.SS1.p1.1.m1.2.3.5" xref="S3.SS1.p1.1.m1.2.3.5.cmml">∈</mo><msup id="S3.SS1.p1.1.m1.2.3.6" xref="S3.SS1.p1.1.m1.2.3.6.cmml"><mi id="S3.SS1.p1.1.m1.2.3.6.2" xref="S3.SS1.p1.1.m1.2.3.6.2.cmml">ℝ</mi><mn id="S3.SS1.p1.1.m1.2.3.6.3" xref="S3.SS1.p1.1.m1.2.3.6.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.2b"><apply id="S3.SS1.p1.1.m1.2.3.cmml" xref="S3.SS1.p1.1.m1.2.3"><and id="S3.SS1.p1.1.m1.2.3a.cmml" xref="S3.SS1.p1.1.m1.2.3"></and><apply id="S3.SS1.p1.1.m1.2.3b.cmml" xref="S3.SS1.p1.1.m1.2.3"><eq id="S3.SS1.p1.1.m1.2.3.3.cmml" xref="S3.SS1.p1.1.m1.2.3.3"></eq><ci id="S3.SS1.p1.1.m1.2.3.2.cmml" xref="S3.SS1.p1.1.m1.2.3.2">𝒒</ci><interval closure="open" id="S3.SS1.p1.1.m1.2.3.4.1.cmml" xref="S3.SS1.p1.1.m1.2.3.4.2"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝑥</ci><ci id="S3.SS1.p1.1.m1.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2">𝑦</ci></interval></apply><apply id="S3.SS1.p1.1.m1.2.3c.cmml" xref="S3.SS1.p1.1.m1.2.3"><in id="S3.SS1.p1.1.m1.2.3.5.cmml" xref="S3.SS1.p1.1.m1.2.3.5"></in><share href="#S3.SS1.p1.1.m1.2.3.4.cmml" id="S3.SS1.p1.1.m1.2.3d.cmml" xref="S3.SS1.p1.1.m1.2.3"></share><apply id="S3.SS1.p1.1.m1.2.3.6.cmml" xref="S3.SS1.p1.1.m1.2.3.6"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.3.6.1.cmml" xref="S3.SS1.p1.1.m1.2.3.6">superscript</csymbol><ci id="S3.SS1.p1.1.m1.2.3.6.2.cmml" xref="S3.SS1.p1.1.m1.2.3.6.2">ℝ</ci><cn type="integer" id="S3.SS1.p1.1.m1.2.3.6.3.cmml" xref="S3.SS1.p1.1.m1.2.3.6.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.2c">\boldsymbol{q}=(x,y)\in\mathbb{R}^{2}</annotation></semantics></math> is associated with a probability <math id="S3.SS1.p1.2.m2.2" class="ltx_Math" alttext="p\in[0,1]" display="inline"><semantics id="S3.SS1.p1.2.m2.2a"><mrow id="S3.SS1.p1.2.m2.2.3" xref="S3.SS1.p1.2.m2.2.3.cmml"><mi id="S3.SS1.p1.2.m2.2.3.2" xref="S3.SS1.p1.2.m2.2.3.2.cmml">p</mi><mo id="S3.SS1.p1.2.m2.2.3.1" xref="S3.SS1.p1.2.m2.2.3.1.cmml">∈</mo><mrow id="S3.SS1.p1.2.m2.2.3.3.2" xref="S3.SS1.p1.2.m2.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.p1.2.m2.2.3.3.2.1" xref="S3.SS1.p1.2.m2.2.3.3.1.cmml">[</mo><mn id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">0</mn><mo id="S3.SS1.p1.2.m2.2.3.3.2.2" xref="S3.SS1.p1.2.m2.2.3.3.1.cmml">,</mo><mn id="S3.SS1.p1.2.m2.2.2" xref="S3.SS1.p1.2.m2.2.2.cmml">1</mn><mo stretchy="false" id="S3.SS1.p1.2.m2.2.3.3.2.3" xref="S3.SS1.p1.2.m2.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.2b"><apply id="S3.SS1.p1.2.m2.2.3.cmml" xref="S3.SS1.p1.2.m2.2.3"><in id="S3.SS1.p1.2.m2.2.3.1.cmml" xref="S3.SS1.p1.2.m2.2.3.1"></in><ci id="S3.SS1.p1.2.m2.2.3.2.cmml" xref="S3.SS1.p1.2.m2.2.3.2">𝑝</ci><interval closure="closed" id="S3.SS1.p1.2.m2.2.3.3.1.cmml" xref="S3.SS1.p1.2.m2.2.3.3.2"><cn type="integer" id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">0</cn><cn type="integer" id="S3.SS1.p1.2.m2.2.2.cmml" xref="S3.SS1.p1.2.m2.2.2">1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.2c">p\in[0,1]</annotation></semantics></math>. We assign higher probabilities to points inside foreground regions and lower probabilities to points in the background.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The benefits of the 2D semantic scene representation are three-fold. First, the 2D semantic scene provides an accurate approximation to the <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">complete</span> foreground silhouette in BEV. Contrary to sparse 3D representations such as point clouds and voxels, the 2D semantic scene representation is dense. The dense representation allows a powerful supervision signal covering all locations of interest, including the interior of object cuboids with very few or no LiDAR point captures. In addition, the spatial gradient of the probability distribution is closely related to surface boundaries and edges, which has been shown to enhance object detection thanks to their role in aligning align predicted bounding boxes with geometric cues <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Second, the 2D semantic scene representation naturally models the uncertainty and the structure of the environment. Although conventional segmentation masks are used as training supervision, the proposed 2D scene representation consists of real-valued probabilities instead of binary labels. While regions with high probabilities are more likely associated with objects of interest, a few background locations will have nonzero albeit low probabilities, indicating that the model suspects an object is around even though the confidence is low. Such ambiguities occur when the region of interest (RoI) does not contain a sufficient number of LiDAR points. Without contextual information, it is extremely difficult to accurately detect objects in these regions. By representing the entire scene as one single feature map, the proposed 2D semantic scene representation captures important information about the underlying 3D structure such as the relationship between the RoI and nearby objects.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">Third, we point out that training supervision can be easily obtained by the direct projection of 3D bounding boxes into the BEV. This allows the 2D semantic scene representation to connect 3D object detection with 2D image generation, which is a well-studied field with many state-of-the-art design choices.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Architecture</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">As shown in Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Approach ‣ LiDAR-Based 3D Object Detection via Hybrid 2D Semantic Scene Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the proposed SSGNet has a similar pipeline to existing grid-based detectors. Specifically, SSGNet consists of three modules: the initial BEV feature extraction module, the BEV feature refinement module, and the detection head. The main difference to existing pipelines is the use of a novel BEV feature refinement module that incorporates 2D semantic scene generation.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Initial BEV feature extraction module.</span>
Given an input scene, the first module in our pipeline extracts initial BEV features using a 3D backbone network. In principle, our SSGNet supports any 3D backbone that regresses BEV features. To demonstrate the generalizability of our design, we experiment with several existing 3D backbones, including CenterPoint <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> (sparse 3D Conv), PillarNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> (sparse 2D Conv), VoxSeT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> (transformer), and PV-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> (sparse 3D Conv, two-stage). In these 3D backbone, the input LiDAR points are discretized into small grids before a series of sparse convolution operations or attention layers are applied to extract multi-scale 3D features. The extracted 3D features are then compressed into the 2D BEV features.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.9" class="ltx_p"><span id="S3.SS2.p3.9.1" class="ltx_text ltx_font_bold">BEV feature refinement module.</span>
The BEV feature refinement module takes initial BEV features <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="X\in\mathbb{R}^{h\times w\times d}" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mrow id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">X</mi><mo id="S3.SS2.p3.1.m1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml"><mi id="S3.SS2.p3.1.m1.1.1.3.2" xref="S3.SS2.p3.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.p3.1.m1.1.1.3.3" xref="S3.SS2.p3.1.m1.1.1.3.3.cmml"><mi id="S3.SS2.p3.1.m1.1.1.3.3.2" xref="S3.SS2.p3.1.m1.1.1.3.3.2.cmml">h</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p3.1.m1.1.1.3.3.1" xref="S3.SS2.p3.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.p3.1.m1.1.1.3.3.3" xref="S3.SS2.p3.1.m1.1.1.3.3.3.cmml">w</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p3.1.m1.1.1.3.3.1a" xref="S3.SS2.p3.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.p3.1.m1.1.1.3.3.4" xref="S3.SS2.p3.1.m1.1.1.3.3.4.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><in id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1"></in><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">𝑋</ci><apply id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.3.1.cmml" xref="S3.SS2.p3.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.3.2.cmml" xref="S3.SS2.p3.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS2.p3.1.m1.1.1.3.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3.3"><times id="S3.SS2.p3.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.p3.1.m1.1.1.3.3.1"></times><ci id="S3.SS2.p3.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.p3.1.m1.1.1.3.3.2">ℎ</ci><ci id="S3.SS2.p3.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3.3.3">𝑤</ci><ci id="S3.SS2.p3.1.m1.1.1.3.3.4.cmml" xref="S3.SS2.p3.1.m1.1.1.3.3.4">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">X\in\mathbb{R}^{h\times w\times d}</annotation></semantics></math> as input, where <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><mi id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><ci id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">h</annotation></semantics></math> and <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="w" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><mi id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><ci id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">w</annotation></semantics></math> describe the spatial shape, and <math id="S3.SS2.p3.4.m4.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS2.p3.4.m4.1a"><mi id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><ci id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">d</annotation></semantics></math> denotes the dimension of the BEV features. The initial BEV features <math id="S3.SS2.p3.5.m5.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS2.p3.5.m5.1a"><mi id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><ci id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">X</annotation></semantics></math> are simultaneously presented to an explicit network and an implicit network to produce new BEV features <math id="S3.SS2.p3.6.m6.1" class="ltx_Math" alttext="X_{\text{exp}}" display="inline"><semantics id="S3.SS2.p3.6.m6.1a"><msub id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml"><mi id="S3.SS2.p3.6.m6.1.1.2" xref="S3.SS2.p3.6.m6.1.1.2.cmml">X</mi><mtext id="S3.SS2.p3.6.m6.1.1.3" xref="S3.SS2.p3.6.m6.1.1.3a.cmml">exp</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b"><apply id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.6.m6.1.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p3.6.m6.1.1.2.cmml" xref="S3.SS2.p3.6.m6.1.1.2">𝑋</ci><ci id="S3.SS2.p3.6.m6.1.1.3a.cmml" xref="S3.SS2.p3.6.m6.1.1.3"><mtext mathsize="70%" id="S3.SS2.p3.6.m6.1.1.3.cmml" xref="S3.SS2.p3.6.m6.1.1.3">exp</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">X_{\text{exp}}</annotation></semantics></math> and <math id="S3.SS2.p3.7.m7.1" class="ltx_Math" alttext="X_{\text{imp}}" display="inline"><semantics id="S3.SS2.p3.7.m7.1a"><msub id="S3.SS2.p3.7.m7.1.1" xref="S3.SS2.p3.7.m7.1.1.cmml"><mi id="S3.SS2.p3.7.m7.1.1.2" xref="S3.SS2.p3.7.m7.1.1.2.cmml">X</mi><mtext id="S3.SS2.p3.7.m7.1.1.3" xref="S3.SS2.p3.7.m7.1.1.3a.cmml">imp</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.7.m7.1b"><apply id="S3.SS2.p3.7.m7.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.1.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1">subscript</csymbol><ci id="S3.SS2.p3.7.m7.1.1.2.cmml" xref="S3.SS2.p3.7.m7.1.1.2">𝑋</ci><ci id="S3.SS2.p3.7.m7.1.1.3a.cmml" xref="S3.SS2.p3.7.m7.1.1.3"><mtext mathsize="70%" id="S3.SS2.p3.7.m7.1.1.3.cmml" xref="S3.SS2.p3.7.m7.1.1.3">imp</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.7.m7.1c">X_{\text{imp}}</annotation></semantics></math>, respectively. These new features are then concatenated with the initial BEV features <math id="S3.SS2.p3.8.m8.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS2.p3.8.m8.1a"><mi id="S3.SS2.p3.8.m8.1.1" xref="S3.SS2.p3.8.m8.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.8.m8.1b"><ci id="S3.SS2.p3.8.m8.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.8.m8.1c">X</annotation></semantics></math> to build the enriched hybrid BEV features <math id="S3.SS2.p3.9.m9.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S3.SS2.p3.9.m9.1a"><mi id="S3.SS2.p3.9.m9.1.1" xref="S3.SS2.p3.9.m9.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.9.m9.1b"><ci id="S3.SS2.p3.9.m9.1.1.cmml" xref="S3.SS2.p3.9.m9.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.9.m9.1c">F</annotation></semantics></math>.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.2" class="ltx_p"><span id="S3.SS2.p4.2.1" class="ltx_text ltx_font_bold">Explicit network.</span>
The explicit network represents the 2D semantic scene as a rectangular image grid <math id="S3.SS2.p4.1.m1.2" class="ltx_Math" alttext="S_{\text{exp}}\in[0,1]^{h\times w\times 1}" display="inline"><semantics id="S3.SS2.p4.1.m1.2a"><mrow id="S3.SS2.p4.1.m1.2.3" xref="S3.SS2.p4.1.m1.2.3.cmml"><msub id="S3.SS2.p4.1.m1.2.3.2" xref="S3.SS2.p4.1.m1.2.3.2.cmml"><mi id="S3.SS2.p4.1.m1.2.3.2.2" xref="S3.SS2.p4.1.m1.2.3.2.2.cmml">S</mi><mtext id="S3.SS2.p4.1.m1.2.3.2.3" xref="S3.SS2.p4.1.m1.2.3.2.3a.cmml">exp</mtext></msub><mo id="S3.SS2.p4.1.m1.2.3.1" xref="S3.SS2.p4.1.m1.2.3.1.cmml">∈</mo><msup id="S3.SS2.p4.1.m1.2.3.3" xref="S3.SS2.p4.1.m1.2.3.3.cmml"><mrow id="S3.SS2.p4.1.m1.2.3.3.2.2" xref="S3.SS2.p4.1.m1.2.3.3.2.1.cmml"><mo stretchy="false" id="S3.SS2.p4.1.m1.2.3.3.2.2.1" xref="S3.SS2.p4.1.m1.2.3.3.2.1.cmml">[</mo><mn id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">0</mn><mo id="S3.SS2.p4.1.m1.2.3.3.2.2.2" xref="S3.SS2.p4.1.m1.2.3.3.2.1.cmml">,</mo><mn id="S3.SS2.p4.1.m1.2.2" xref="S3.SS2.p4.1.m1.2.2.cmml">1</mn><mo stretchy="false" id="S3.SS2.p4.1.m1.2.3.3.2.2.3" xref="S3.SS2.p4.1.m1.2.3.3.2.1.cmml">]</mo></mrow><mrow id="S3.SS2.p4.1.m1.2.3.3.3" xref="S3.SS2.p4.1.m1.2.3.3.3.cmml"><mi id="S3.SS2.p4.1.m1.2.3.3.3.2" xref="S3.SS2.p4.1.m1.2.3.3.3.2.cmml">h</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p4.1.m1.2.3.3.3.1" xref="S3.SS2.p4.1.m1.2.3.3.3.1.cmml">×</mo><mi id="S3.SS2.p4.1.m1.2.3.3.3.3" xref="S3.SS2.p4.1.m1.2.3.3.3.3.cmml">w</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p4.1.m1.2.3.3.3.1a" xref="S3.SS2.p4.1.m1.2.3.3.3.1.cmml">×</mo><mn id="S3.SS2.p4.1.m1.2.3.3.3.4" xref="S3.SS2.p4.1.m1.2.3.3.3.4.cmml">1</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.2b"><apply id="S3.SS2.p4.1.m1.2.3.cmml" xref="S3.SS2.p4.1.m1.2.3"><in id="S3.SS2.p4.1.m1.2.3.1.cmml" xref="S3.SS2.p4.1.m1.2.3.1"></in><apply id="S3.SS2.p4.1.m1.2.3.2.cmml" xref="S3.SS2.p4.1.m1.2.3.2"><csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.2.3.2.1.cmml" xref="S3.SS2.p4.1.m1.2.3.2">subscript</csymbol><ci id="S3.SS2.p4.1.m1.2.3.2.2.cmml" xref="S3.SS2.p4.1.m1.2.3.2.2">𝑆</ci><ci id="S3.SS2.p4.1.m1.2.3.2.3a.cmml" xref="S3.SS2.p4.1.m1.2.3.2.3"><mtext mathsize="70%" id="S3.SS2.p4.1.m1.2.3.2.3.cmml" xref="S3.SS2.p4.1.m1.2.3.2.3">exp</mtext></ci></apply><apply id="S3.SS2.p4.1.m1.2.3.3.cmml" xref="S3.SS2.p4.1.m1.2.3.3"><csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.2.3.3.1.cmml" xref="S3.SS2.p4.1.m1.2.3.3">superscript</csymbol><interval closure="closed" id="S3.SS2.p4.1.m1.2.3.3.2.1.cmml" xref="S3.SS2.p4.1.m1.2.3.3.2.2"><cn type="integer" id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">0</cn><cn type="integer" id="S3.SS2.p4.1.m1.2.2.cmml" xref="S3.SS2.p4.1.m1.2.2">1</cn></interval><apply id="S3.SS2.p4.1.m1.2.3.3.3.cmml" xref="S3.SS2.p4.1.m1.2.3.3.3"><times id="S3.SS2.p4.1.m1.2.3.3.3.1.cmml" xref="S3.SS2.p4.1.m1.2.3.3.3.1"></times><ci id="S3.SS2.p4.1.m1.2.3.3.3.2.cmml" xref="S3.SS2.p4.1.m1.2.3.3.3.2">ℎ</ci><ci id="S3.SS2.p4.1.m1.2.3.3.3.3.cmml" xref="S3.SS2.p4.1.m1.2.3.3.3.3">𝑤</ci><cn type="integer" id="S3.SS2.p4.1.m1.2.3.3.3.4.cmml" xref="S3.SS2.p4.1.m1.2.3.3.3.4">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.2c">S_{\text{exp}}\in[0,1]^{h\times w\times 1}</annotation></semantics></math>, whose spatial resolution is the same as the initial BEV features <math id="S3.SS2.p4.2.m2.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS2.p4.2.m2.1a"><mi id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><ci id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">X</annotation></semantics></math>. The explicit network formulates 2D semantic scene generation as a classical 2D image regression problem. While SSGNet supports any out-of-the-box image generation network, our experiments apply the popular U-Net architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> for simplicity. Consider:</p>
<table id="A2.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\displaystyle S_{\text{exp}}" display="inline"><semantics id="S3.E1.m1.1a"><msub id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mi id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">S</mi><mtext id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3a.cmml">exp</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2">𝑆</ci><ci id="S3.E1.m1.1.1.3a.cmml" xref="S3.E1.m1.1.1.3"><mtext mathsize="70%" id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3">exp</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\displaystyle S_{\text{exp}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1.m2.2" class="ltx_Math" alttext="\displaystyle=f_{\text{exp}}(X)," display="inline"><semantics id="S3.E1.m2.2a"><mrow id="S3.E1.m2.2.2.1" xref="S3.E1.m2.2.2.1.1.cmml"><mrow id="S3.E1.m2.2.2.1.1" xref="S3.E1.m2.2.2.1.1.cmml"><mi id="S3.E1.m2.2.2.1.1.2" xref="S3.E1.m2.2.2.1.1.2.cmml"></mi><mo id="S3.E1.m2.2.2.1.1.1" xref="S3.E1.m2.2.2.1.1.1.cmml">=</mo><mrow id="S3.E1.m2.2.2.1.1.3" xref="S3.E1.m2.2.2.1.1.3.cmml"><msub id="S3.E1.m2.2.2.1.1.3.2" xref="S3.E1.m2.2.2.1.1.3.2.cmml"><mi id="S3.E1.m2.2.2.1.1.3.2.2" xref="S3.E1.m2.2.2.1.1.3.2.2.cmml">f</mi><mtext id="S3.E1.m2.2.2.1.1.3.2.3" xref="S3.E1.m2.2.2.1.1.3.2.3a.cmml">exp</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E1.m2.2.2.1.1.3.1" xref="S3.E1.m2.2.2.1.1.3.1.cmml">​</mo><mrow id="S3.E1.m2.2.2.1.1.3.3.2" xref="S3.E1.m2.2.2.1.1.3.cmml"><mo stretchy="false" id="S3.E1.m2.2.2.1.1.3.3.2.1" xref="S3.E1.m2.2.2.1.1.3.cmml">(</mo><mi id="S3.E1.m2.1.1" xref="S3.E1.m2.1.1.cmml">X</mi><mo stretchy="false" id="S3.E1.m2.2.2.1.1.3.3.2.2" xref="S3.E1.m2.2.2.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m2.2.2.1.2" xref="S3.E1.m2.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m2.2b"><apply id="S3.E1.m2.2.2.1.1.cmml" xref="S3.E1.m2.2.2.1"><eq id="S3.E1.m2.2.2.1.1.1.cmml" xref="S3.E1.m2.2.2.1.1.1"></eq><csymbol cd="latexml" id="S3.E1.m2.2.2.1.1.2.cmml" xref="S3.E1.m2.2.2.1.1.2">absent</csymbol><apply id="S3.E1.m2.2.2.1.1.3.cmml" xref="S3.E1.m2.2.2.1.1.3"><times id="S3.E1.m2.2.2.1.1.3.1.cmml" xref="S3.E1.m2.2.2.1.1.3.1"></times><apply id="S3.E1.m2.2.2.1.1.3.2.cmml" xref="S3.E1.m2.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m2.2.2.1.1.3.2.1.cmml" xref="S3.E1.m2.2.2.1.1.3.2">subscript</csymbol><ci id="S3.E1.m2.2.2.1.1.3.2.2.cmml" xref="S3.E1.m2.2.2.1.1.3.2.2">𝑓</ci><ci id="S3.E1.m2.2.2.1.1.3.2.3a.cmml" xref="S3.E1.m2.2.2.1.1.3.2.3"><mtext mathsize="70%" id="S3.E1.m2.2.2.1.1.3.2.3.cmml" xref="S3.E1.m2.2.2.1.1.3.2.3">exp</mtext></ci></apply><ci id="S3.E1.m2.1.1.cmml" xref="S3.E1.m2.1.1">𝑋</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m2.2c">\displaystyle=f_{\text{exp}}(X),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\displaystyle X_{\text{exp}}" display="inline"><semantics id="S3.E2.m1.1a"><msub id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mi id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">X</mi><mtext id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3a.cmml">exp</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2">𝑋</ci><ci id="S3.E2.m1.1.1.3a.cmml" xref="S3.E2.m1.1.1.3"><mtext mathsize="70%" id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3">exp</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\displaystyle X_{\text{exp}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E2.m2.1" class="ltx_Math" alttext="\displaystyle=X\oplus g_{\text{exp}}(S_{\text{exp}})," display="inline"><semantics id="S3.E2.m2.1a"><mrow id="S3.E2.m2.1.1.1" xref="S3.E2.m2.1.1.1.1.cmml"><mrow id="S3.E2.m2.1.1.1.1" xref="S3.E2.m2.1.1.1.1.cmml"><mi id="S3.E2.m2.1.1.1.1.3" xref="S3.E2.m2.1.1.1.1.3.cmml"></mi><mo id="S3.E2.m2.1.1.1.1.2" xref="S3.E2.m2.1.1.1.1.2.cmml">=</mo><mrow id="S3.E2.m2.1.1.1.1.1" xref="S3.E2.m2.1.1.1.1.1.cmml"><mi id="S3.E2.m2.1.1.1.1.1.3" xref="S3.E2.m2.1.1.1.1.1.3.cmml">X</mi><mo id="S3.E2.m2.1.1.1.1.1.2" xref="S3.E2.m2.1.1.1.1.1.2.cmml">⊕</mo><mrow id="S3.E2.m2.1.1.1.1.1.1" xref="S3.E2.m2.1.1.1.1.1.1.cmml"><msub id="S3.E2.m2.1.1.1.1.1.1.3" xref="S3.E2.m2.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m2.1.1.1.1.1.1.3.2" xref="S3.E2.m2.1.1.1.1.1.1.3.2.cmml">g</mi><mtext id="S3.E2.m2.1.1.1.1.1.1.3.3" xref="S3.E2.m2.1.1.1.1.1.1.3.3a.cmml">exp</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E2.m2.1.1.1.1.1.1.2" xref="S3.E2.m2.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m2.1.1.1.1.1.1.1.1" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m2.1.1.1.1.1.1.1.1.2" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E2.m2.1.1.1.1.1.1.1.1.1" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m2.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.2.cmml">S</mi><mtext id="S3.E2.m2.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.3a.cmml">exp</mtext></msub><mo stretchy="false" id="S3.E2.m2.1.1.1.1.1.1.1.1.3" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E2.m2.1.1.1.2" xref="S3.E2.m2.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m2.1b"><apply id="S3.E2.m2.1.1.1.1.cmml" xref="S3.E2.m2.1.1.1"><eq id="S3.E2.m2.1.1.1.1.2.cmml" xref="S3.E2.m2.1.1.1.1.2"></eq><csymbol cd="latexml" id="S3.E2.m2.1.1.1.1.3.cmml" xref="S3.E2.m2.1.1.1.1.3">absent</csymbol><apply id="S3.E2.m2.1.1.1.1.1.cmml" xref="S3.E2.m2.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m2.1.1.1.1.1.2.cmml" xref="S3.E2.m2.1.1.1.1.1.2">direct-sum</csymbol><ci id="S3.E2.m2.1.1.1.1.1.3.cmml" xref="S3.E2.m2.1.1.1.1.1.3">𝑋</ci><apply id="S3.E2.m2.1.1.1.1.1.1.cmml" xref="S3.E2.m2.1.1.1.1.1.1"><times id="S3.E2.m2.1.1.1.1.1.1.2.cmml" xref="S3.E2.m2.1.1.1.1.1.1.2"></times><apply id="S3.E2.m2.1.1.1.1.1.1.3.cmml" xref="S3.E2.m2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m2.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m2.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m2.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m2.1.1.1.1.1.1.3.2">𝑔</ci><ci id="S3.E2.m2.1.1.1.1.1.1.3.3a.cmml" xref="S3.E2.m2.1.1.1.1.1.1.3.3"><mtext mathsize="70%" id="S3.E2.m2.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m2.1.1.1.1.1.1.3.3">exp</mtext></ci></apply><apply id="S3.E2.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.2">𝑆</ci><ci id="S3.E2.m2.1.1.1.1.1.1.1.1.1.3a.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.3"><mtext mathsize="70%" id="S3.E2.m2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.3">exp</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m2.1c">\displaystyle=X\oplus g_{\text{exp}}(S_{\text{exp}}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p4.7" class="ltx_p">where <math id="S3.SS2.p4.3.m1.1" class="ltx_Math" alttext="f_{\text{exp}}" display="inline"><semantics id="S3.SS2.p4.3.m1.1a"><msub id="S3.SS2.p4.3.m1.1.1" xref="S3.SS2.p4.3.m1.1.1.cmml"><mi id="S3.SS2.p4.3.m1.1.1.2" xref="S3.SS2.p4.3.m1.1.1.2.cmml">f</mi><mtext id="S3.SS2.p4.3.m1.1.1.3" xref="S3.SS2.p4.3.m1.1.1.3a.cmml">exp</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m1.1b"><apply id="S3.SS2.p4.3.m1.1.1.cmml" xref="S3.SS2.p4.3.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.3.m1.1.1.1.cmml" xref="S3.SS2.p4.3.m1.1.1">subscript</csymbol><ci id="S3.SS2.p4.3.m1.1.1.2.cmml" xref="S3.SS2.p4.3.m1.1.1.2">𝑓</ci><ci id="S3.SS2.p4.3.m1.1.1.3a.cmml" xref="S3.SS2.p4.3.m1.1.1.3"><mtext mathsize="70%" id="S3.SS2.p4.3.m1.1.1.3.cmml" xref="S3.SS2.p4.3.m1.1.1.3">exp</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m1.1c">f_{\text{exp}}</annotation></semantics></math> denotes the U-Net, <math id="S3.SS2.p4.4.m2.1" class="ltx_Math" alttext="g_{\text{exp}}" display="inline"><semantics id="S3.SS2.p4.4.m2.1a"><msub id="S3.SS2.p4.4.m2.1.1" xref="S3.SS2.p4.4.m2.1.1.cmml"><mi id="S3.SS2.p4.4.m2.1.1.2" xref="S3.SS2.p4.4.m2.1.1.2.cmml">g</mi><mtext id="S3.SS2.p4.4.m2.1.1.3" xref="S3.SS2.p4.4.m2.1.1.3a.cmml">exp</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m2.1b"><apply id="S3.SS2.p4.4.m2.1.1.cmml" xref="S3.SS2.p4.4.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.4.m2.1.1.1.cmml" xref="S3.SS2.p4.4.m2.1.1">subscript</csymbol><ci id="S3.SS2.p4.4.m2.1.1.2.cmml" xref="S3.SS2.p4.4.m2.1.1.2">𝑔</ci><ci id="S3.SS2.p4.4.m2.1.1.3a.cmml" xref="S3.SS2.p4.4.m2.1.1.3"><mtext mathsize="70%" id="S3.SS2.p4.4.m2.1.1.3.cmml" xref="S3.SS2.p4.4.m2.1.1.3">exp</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m2.1c">g_{\text{exp}}</annotation></semantics></math> is a Conv-BN-ReLU layer that lifts the feature dimension from <math id="S3.SS2.p4.5.m3.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS2.p4.5.m3.1a"><mn id="S3.SS2.p4.5.m3.1.1" xref="S3.SS2.p4.5.m3.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.5.m3.1b"><cn type="integer" id="S3.SS2.p4.5.m3.1.1.cmml" xref="S3.SS2.p4.5.m3.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.5.m3.1c">1</annotation></semantics></math> to <math id="S3.SS2.p4.6.m4.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS2.p4.6.m4.1a"><mi id="S3.SS2.p4.6.m4.1.1" xref="S3.SS2.p4.6.m4.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.6.m4.1b"><ci id="S3.SS2.p4.6.m4.1.1.cmml" xref="S3.SS2.p4.6.m4.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.6.m4.1c">d</annotation></semantics></math>, and <math id="S3.SS2.p4.7.m5.1" class="ltx_Math" alttext="\oplus" display="inline"><semantics id="S3.SS2.p4.7.m5.1a"><mo id="S3.SS2.p4.7.m5.1.1" xref="S3.SS2.p4.7.m5.1.1.cmml">⊕</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.7.m5.1b"><csymbol cd="latexml" id="S3.SS2.p4.7.m5.1.1.cmml" xref="S3.SS2.p4.7.m5.1.1">direct-sum</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.7.m5.1c">\oplus</annotation></semantics></math> stands for channel-wise concatenation.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">By interpreting the network output as real-valued probabilities, the 2D semantic scene representation differs from a binary segmentation mask. Keeping the probabilities allows the enriched BEV features to preserve visual information in low-confidence regions. While a perfect binary segmentation is certainly desirable, predicting such a segmentation from the initial BEV features is practically impossible due to the problem of missing points. As an alternative, we use continuous probabilities instead of discrete binary labels to model the 2D semantic scene. Experimentally, Section <a href="#S4.SS4" title="4.4 Ablation Study ‣ 4 Experiments ‣ 3.2 Architecture ‣ 3 Approach ‣ LiDAR-Based 3D Object Detection via Hybrid 2D Semantic Scene Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a> demonstrates the effectiveness of this design choice through an ablation study.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p">We use 2D ground-truth segmentation masks to supervise all pixels densely. These ground-truth segmentation masks can be easily obtained by projecting the ground-truth bounding boxes from 3D into 2D. We adopt the focal loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> to calculate the difference between the predicted and ground-truth segmentations since the foreground pixels usually take up only a tiny fraction of the <math id="S3.SS2.p6.1.m1.1" class="ltx_Math" alttext="h\times w" display="inline"><semantics id="S3.SS2.p6.1.m1.1a"><mrow id="S3.SS2.p6.1.m1.1.1" xref="S3.SS2.p6.1.m1.1.1.cmml"><mi id="S3.SS2.p6.1.m1.1.1.2" xref="S3.SS2.p6.1.m1.1.1.2.cmml">h</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p6.1.m1.1.1.1" xref="S3.SS2.p6.1.m1.1.1.1.cmml">×</mo><mi id="S3.SS2.p6.1.m1.1.1.3" xref="S3.SS2.p6.1.m1.1.1.3.cmml">w</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.1.m1.1b"><apply id="S3.SS2.p6.1.m1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1"><times id="S3.SS2.p6.1.m1.1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1"></times><ci id="S3.SS2.p6.1.m1.1.1.2.cmml" xref="S3.SS2.p6.1.m1.1.1.2">ℎ</ci><ci id="S3.SS2.p6.1.m1.1.1.3.cmml" xref="S3.SS2.p6.1.m1.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.1.m1.1c">h\times w</annotation></semantics></math> grid.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2304.01519/assets/x5.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="211" height="192" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>While implicit network training uses both grid and importance samplings, only grid sampling is used in testing.</figcaption>
</figure>
<div id="S3.SS2.p7" class="ltx_para ltx_noindent">
<p id="S3.SS2.p7.3" class="ltx_p"><span id="S3.SS2.p7.3.1" class="ltx_text ltx_font_bold">Implicit network.</span> Inspired by the recent advances in neural implicit representations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, we propose to learn another instance of the probability distribution map using an implicit network. The architecture of the implicit network is shown in Figure <a href="#S3.F4" title="Figure 4 ‣ 3.2 Architecture ‣ 3 Approach ‣ LiDAR-Based 3D Object Detection via Hybrid 2D Semantic Scene Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. In addition to 2D coordinates, the implicit network <math id="S3.SS2.p7.1.m1.1" class="ltx_Math" alttext="\phi" display="inline"><semantics id="S3.SS2.p7.1.m1.1a"><mi id="S3.SS2.p7.1.m1.1.1" xref="S3.SS2.p7.1.m1.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.1.m1.1b"><ci id="S3.SS2.p7.1.m1.1.1.cmml" xref="S3.SS2.p7.1.m1.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.1.m1.1c">\phi</annotation></semantics></math> takes a latent code as input and uses the latent code as the source of scene-level information. Specifically, given the initial BEV features <math id="S3.SS2.p7.2.m2.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS2.p7.2.m2.1a"><mi id="S3.SS2.p7.2.m2.1.1" xref="S3.SS2.p7.2.m2.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.2.m2.1b"><ci id="S3.SS2.p7.2.m2.1.1.cmml" xref="S3.SS2.p7.2.m2.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.2.m2.1c">X</annotation></semantics></math>, we first embed <math id="S3.SS2.p7.3.m3.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS2.p7.3.m3.1a"><mi id="S3.SS2.p7.3.m3.1.1" xref="S3.SS2.p7.3.m3.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.3.m3.1b"><ci id="S3.SS2.p7.3.m3.1.1.cmml" xref="S3.SS2.p7.3.m3.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.3.m3.1c">X</annotation></semantics></math> into a latent space:</p>
<table id="A2.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3.m1.2" class="ltx_Math" alttext="\displaystyle L=h_{\text{imp}}(X)," display="inline"><semantics id="S3.E3.m1.2a"><mrow id="S3.E3.m1.2.2.1" xref="S3.E3.m1.2.2.1.1.cmml"><mrow id="S3.E3.m1.2.2.1.1" xref="S3.E3.m1.2.2.1.1.cmml"><mi id="S3.E3.m1.2.2.1.1.2" xref="S3.E3.m1.2.2.1.1.2.cmml">L</mi><mo id="S3.E3.m1.2.2.1.1.1" xref="S3.E3.m1.2.2.1.1.1.cmml">=</mo><mrow id="S3.E3.m1.2.2.1.1.3" xref="S3.E3.m1.2.2.1.1.3.cmml"><msub id="S3.E3.m1.2.2.1.1.3.2" xref="S3.E3.m1.2.2.1.1.3.2.cmml"><mi id="S3.E3.m1.2.2.1.1.3.2.2" xref="S3.E3.m1.2.2.1.1.3.2.2.cmml">h</mi><mtext id="S3.E3.m1.2.2.1.1.3.2.3" xref="S3.E3.m1.2.2.1.1.3.2.3a.cmml">imp</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.3.1" xref="S3.E3.m1.2.2.1.1.3.1.cmml">​</mo><mrow id="S3.E3.m1.2.2.1.1.3.3.2" xref="S3.E3.m1.2.2.1.1.3.cmml"><mo stretchy="false" id="S3.E3.m1.2.2.1.1.3.3.2.1" xref="S3.E3.m1.2.2.1.1.3.cmml">(</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">X</mi><mo stretchy="false" id="S3.E3.m1.2.2.1.1.3.3.2.2" xref="S3.E3.m1.2.2.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.2.2.1.2" xref="S3.E3.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.2b"><apply id="S3.E3.m1.2.2.1.1.cmml" xref="S3.E3.m1.2.2.1"><eq id="S3.E3.m1.2.2.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1"></eq><ci id="S3.E3.m1.2.2.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.2">𝐿</ci><apply id="S3.E3.m1.2.2.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.3"><times id="S3.E3.m1.2.2.1.1.3.1.cmml" xref="S3.E3.m1.2.2.1.1.3.1"></times><apply id="S3.E3.m1.2.2.1.1.3.2.cmml" xref="S3.E3.m1.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.3.2.1.cmml" xref="S3.E3.m1.2.2.1.1.3.2">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.3.2.2.cmml" xref="S3.E3.m1.2.2.1.1.3.2.2">ℎ</ci><ci id="S3.E3.m1.2.2.1.1.3.2.3a.cmml" xref="S3.E3.m1.2.2.1.1.3.2.3"><mtext mathsize="70%" id="S3.E3.m1.2.2.1.1.3.2.3.cmml" xref="S3.E3.m1.2.2.1.1.3.2.3">imp</mtext></ci></apply><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">𝑋</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.2c">\displaystyle L=h_{\text{imp}}(X),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p7.8" class="ltx_p">where <math id="S3.SS2.p7.4.m1.1" class="ltx_Math" alttext="h_{\text{imp}}" display="inline"><semantics id="S3.SS2.p7.4.m1.1a"><msub id="S3.SS2.p7.4.m1.1.1" xref="S3.SS2.p7.4.m1.1.1.cmml"><mi id="S3.SS2.p7.4.m1.1.1.2" xref="S3.SS2.p7.4.m1.1.1.2.cmml">h</mi><mtext id="S3.SS2.p7.4.m1.1.1.3" xref="S3.SS2.p7.4.m1.1.1.3a.cmml">imp</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.4.m1.1b"><apply id="S3.SS2.p7.4.m1.1.1.cmml" xref="S3.SS2.p7.4.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.4.m1.1.1.1.cmml" xref="S3.SS2.p7.4.m1.1.1">subscript</csymbol><ci id="S3.SS2.p7.4.m1.1.1.2.cmml" xref="S3.SS2.p7.4.m1.1.1.2">ℎ</ci><ci id="S3.SS2.p7.4.m1.1.1.3a.cmml" xref="S3.SS2.p7.4.m1.1.1.3"><mtext mathsize="70%" id="S3.SS2.p7.4.m1.1.1.3.cmml" xref="S3.SS2.p7.4.m1.1.1.3">imp</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.4.m1.1c">h_{\text{imp}}</annotation></semantics></math> is a Conv-BN-ReLU layer, and <math id="S3.SS2.p7.5.m2.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS2.p7.5.m2.1a"><mi id="S3.SS2.p7.5.m2.1.1" xref="S3.SS2.p7.5.m2.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.5.m2.1b"><ci id="S3.SS2.p7.5.m2.1.1.cmml" xref="S3.SS2.p7.5.m2.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.5.m2.1c">L</annotation></semantics></math> is the latent code. Notably, <math id="S3.SS2.p7.6.m3.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS2.p7.6.m3.1a"><mi id="S3.SS2.p7.6.m3.1.1" xref="S3.SS2.p7.6.m3.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.6.m3.1b"><ci id="S3.SS2.p7.6.m3.1.1.cmml" xref="S3.SS2.p7.6.m3.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.6.m3.1c">L</annotation></semantics></math> is not required to have the same spatial shape as <math id="S3.SS2.p7.7.m4.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS2.p7.7.m4.1a"><mi id="S3.SS2.p7.7.m4.1.1" xref="S3.SS2.p7.7.m4.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.7.m4.1b"><ci id="S3.SS2.p7.7.m4.1.1.cmml" xref="S3.SS2.p7.7.m4.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.7.m4.1c">X</annotation></semantics></math>. Our experiments demonstrate that increasing the spatial resolution of <math id="S3.SS2.p7.8.m5.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS2.p7.8.m5.1a"><mi id="S3.SS2.p7.8.m5.1.1" xref="S3.SS2.p7.8.m5.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.8.m5.1b"><ci id="S3.SS2.p7.8.m5.1.1.cmml" xref="S3.SS2.p7.8.m5.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.8.m5.1c">L</annotation></semantics></math> contributes to higher detection accuracy albeit introducing an overhead on GPU memory usage. The 2D semantic scene is represented implicitly as:</p>
<table id="A2.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E4.m1.3" class="ltx_Math" alttext="\displaystyle p=\phi(\boldsymbol{q},L)," display="inline"><semantics id="S3.E4.m1.3a"><mrow id="S3.E4.m1.3.3.1" xref="S3.E4.m1.3.3.1.1.cmml"><mrow id="S3.E4.m1.3.3.1.1" xref="S3.E4.m1.3.3.1.1.cmml"><mi id="S3.E4.m1.3.3.1.1.2" xref="S3.E4.m1.3.3.1.1.2.cmml">p</mi><mo id="S3.E4.m1.3.3.1.1.1" xref="S3.E4.m1.3.3.1.1.1.cmml">=</mo><mrow id="S3.E4.m1.3.3.1.1.3" xref="S3.E4.m1.3.3.1.1.3.cmml"><mi id="S3.E4.m1.3.3.1.1.3.2" xref="S3.E4.m1.3.3.1.1.3.2.cmml">ϕ</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.1.1.3.1" xref="S3.E4.m1.3.3.1.1.3.1.cmml">​</mo><mrow id="S3.E4.m1.3.3.1.1.3.3.2" xref="S3.E4.m1.3.3.1.1.3.3.1.cmml"><mo stretchy="false" id="S3.E4.m1.3.3.1.1.3.3.2.1" xref="S3.E4.m1.3.3.1.1.3.3.1.cmml">(</mo><mi id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">𝒒</mi><mo id="S3.E4.m1.3.3.1.1.3.3.2.2" xref="S3.E4.m1.3.3.1.1.3.3.1.cmml">,</mo><mi id="S3.E4.m1.2.2" xref="S3.E4.m1.2.2.cmml">L</mi><mo stretchy="false" id="S3.E4.m1.3.3.1.1.3.3.2.3" xref="S3.E4.m1.3.3.1.1.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E4.m1.3.3.1.2" xref="S3.E4.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.3b"><apply id="S3.E4.m1.3.3.1.1.cmml" xref="S3.E4.m1.3.3.1"><eq id="S3.E4.m1.3.3.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1"></eq><ci id="S3.E4.m1.3.3.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.2">𝑝</ci><apply id="S3.E4.m1.3.3.1.1.3.cmml" xref="S3.E4.m1.3.3.1.1.3"><times id="S3.E4.m1.3.3.1.1.3.1.cmml" xref="S3.E4.m1.3.3.1.1.3.1"></times><ci id="S3.E4.m1.3.3.1.1.3.2.cmml" xref="S3.E4.m1.3.3.1.1.3.2">italic-ϕ</ci><interval closure="open" id="S3.E4.m1.3.3.1.1.3.3.1.cmml" xref="S3.E4.m1.3.3.1.1.3.3.2"><ci id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1">𝒒</ci><ci id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2">𝐿</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.3c">\displaystyle p=\phi(\boldsymbol{q},L),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p7.11" class="ltx_p">where <math id="S3.SS2.p7.9.m1.1" class="ltx_Math" alttext="\boldsymbol{q}\in\mathbb{R}^{2}" display="inline"><semantics id="S3.SS2.p7.9.m1.1a"><mrow id="S3.SS2.p7.9.m1.1.1" xref="S3.SS2.p7.9.m1.1.1.cmml"><mi id="S3.SS2.p7.9.m1.1.1.2" xref="S3.SS2.p7.9.m1.1.1.2.cmml">𝒒</mi><mo id="S3.SS2.p7.9.m1.1.1.1" xref="S3.SS2.p7.9.m1.1.1.1.cmml">∈</mo><msup id="S3.SS2.p7.9.m1.1.1.3" xref="S3.SS2.p7.9.m1.1.1.3.cmml"><mi id="S3.SS2.p7.9.m1.1.1.3.2" xref="S3.SS2.p7.9.m1.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS2.p7.9.m1.1.1.3.3" xref="S3.SS2.p7.9.m1.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.9.m1.1b"><apply id="S3.SS2.p7.9.m1.1.1.cmml" xref="S3.SS2.p7.9.m1.1.1"><in id="S3.SS2.p7.9.m1.1.1.1.cmml" xref="S3.SS2.p7.9.m1.1.1.1"></in><ci id="S3.SS2.p7.9.m1.1.1.2.cmml" xref="S3.SS2.p7.9.m1.1.1.2">𝒒</ci><apply id="S3.SS2.p7.9.m1.1.1.3.cmml" xref="S3.SS2.p7.9.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p7.9.m1.1.1.3.1.cmml" xref="S3.SS2.p7.9.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.p7.9.m1.1.1.3.2.cmml" xref="S3.SS2.p7.9.m1.1.1.3.2">ℝ</ci><cn type="integer" id="S3.SS2.p7.9.m1.1.1.3.3.cmml" xref="S3.SS2.p7.9.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.9.m1.1c">\boldsymbol{q}\in\mathbb{R}^{2}</annotation></semantics></math> is the query point in BEV and <math id="S3.SS2.p7.10.m2.2" class="ltx_Math" alttext="p\in[0,1]" display="inline"><semantics id="S3.SS2.p7.10.m2.2a"><mrow id="S3.SS2.p7.10.m2.2.3" xref="S3.SS2.p7.10.m2.2.3.cmml"><mi id="S3.SS2.p7.10.m2.2.3.2" xref="S3.SS2.p7.10.m2.2.3.2.cmml">p</mi><mo id="S3.SS2.p7.10.m2.2.3.1" xref="S3.SS2.p7.10.m2.2.3.1.cmml">∈</mo><mrow id="S3.SS2.p7.10.m2.2.3.3.2" xref="S3.SS2.p7.10.m2.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS2.p7.10.m2.2.3.3.2.1" xref="S3.SS2.p7.10.m2.2.3.3.1.cmml">[</mo><mn id="S3.SS2.p7.10.m2.1.1" xref="S3.SS2.p7.10.m2.1.1.cmml">0</mn><mo id="S3.SS2.p7.10.m2.2.3.3.2.2" xref="S3.SS2.p7.10.m2.2.3.3.1.cmml">,</mo><mn id="S3.SS2.p7.10.m2.2.2" xref="S3.SS2.p7.10.m2.2.2.cmml">1</mn><mo stretchy="false" id="S3.SS2.p7.10.m2.2.3.3.2.3" xref="S3.SS2.p7.10.m2.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.10.m2.2b"><apply id="S3.SS2.p7.10.m2.2.3.cmml" xref="S3.SS2.p7.10.m2.2.3"><in id="S3.SS2.p7.10.m2.2.3.1.cmml" xref="S3.SS2.p7.10.m2.2.3.1"></in><ci id="S3.SS2.p7.10.m2.2.3.2.cmml" xref="S3.SS2.p7.10.m2.2.3.2">𝑝</ci><interval closure="closed" id="S3.SS2.p7.10.m2.2.3.3.1.cmml" xref="S3.SS2.p7.10.m2.2.3.3.2"><cn type="integer" id="S3.SS2.p7.10.m2.1.1.cmml" xref="S3.SS2.p7.10.m2.1.1">0</cn><cn type="integer" id="S3.SS2.p7.10.m2.2.2.cmml" xref="S3.SS2.p7.10.m2.2.2">1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.10.m2.2c">p\in[0,1]</annotation></semantics></math> is the probability of <math id="S3.SS2.p7.11.m3.1" class="ltx_Math" alttext="\boldsymbol{q}" display="inline"><semantics id="S3.SS2.p7.11.m3.1a"><mi id="S3.SS2.p7.11.m3.1.1" xref="S3.SS2.p7.11.m3.1.1.cmml">𝒒</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.11.m3.1b"><ci id="S3.SS2.p7.11.m3.1.1.cmml" xref="S3.SS2.p7.11.m3.1.1">𝒒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.11.m3.1c">\boldsymbol{q}</annotation></semantics></math> being a foreground location.</p>
</div>
<div id="S3.SS2.p8" class="ltx_para">
<p id="S3.SS2.p8.3" class="ltx_p">To obtain both local and global information, we extract multi-scale features from <math id="S3.SS2.p8.1.m1.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS2.p8.1.m1.1a"><mi id="S3.SS2.p8.1.m1.1.1" xref="S3.SS2.p8.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.1.m1.1b"><ci id="S3.SS2.p8.1.m1.1.1.cmml" xref="S3.SS2.p8.1.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.1.m1.1c">L</annotation></semantics></math>. For each 2D query point <math id="S3.SS2.p8.2.m2.1" class="ltx_Math" alttext="\boldsymbol{q}" display="inline"><semantics id="S3.SS2.p8.2.m2.1a"><mi id="S3.SS2.p8.2.m2.1.1" xref="S3.SS2.p8.2.m2.1.1.cmml">𝒒</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.2.m2.1b"><ci id="S3.SS2.p8.2.m2.1.1.cmml" xref="S3.SS2.p8.2.m2.1.1">𝒒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.2.m2.1c">\boldsymbol{q}</annotation></semantics></math>, we locate its position in the multi-scale feature maps and extract per-point features through bilinear interpolation, which are then concatenated and fed to an MLP network to predict the probability <math id="S3.SS2.p8.3.m3.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S3.SS2.p8.3.m3.1a"><mi id="S3.SS2.p8.3.m3.1.1" xref="S3.SS2.p8.3.m3.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.3.m3.1b"><ci id="S3.SS2.p8.3.m3.1.1.cmml" xref="S3.SS2.p8.3.m3.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.3.m3.1c">p</annotation></semantics></math>.</p>
</div>
<div id="S3.SS2.p9" class="ltx_para">
<p id="S3.SS2.p9.1" class="ltx_p">An appealing property of the implicit 2D semantic scene representation is that importance sampling can be naturally combined in the training process. The use of importance sampling has been repeatedly proven effective in LiDAR-based 3D object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Intuitively, we expect the model to sample more points around the foreground objects because they provide good guidance to object discovery.</p>
</div>
<figure id="S3.SS2.23" class="ltx_table">

<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Quantitative comparison of SSGNet with different baselines on the Waymo Open Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> (val split).</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S3.SS2.23.29" class="ltx_ERROR ltx_figure_panel undefined">{Tabular}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.SS2.23.30" class="ltx_p ltx_figure_panel"><span id="S3.SS2.23.30.1" class="ltx_text" style="font-size:80%;">c—c—cc—cc—cc—cc—cc—cc  Results When Trained With 20% Training Examples  
<br class="ltx_break"><span id="S3.SS2.23.30.1.1" class="ltx_text">Methods</span>  <span id="S3.SS2.23.30.1.2" class="ltx_text">Stages</span>   Veh. (L1) <span class="ltx_rule" style="background:black;display:inline-block;"> </span>   Veh. (L2) <span class="ltx_rule" style="background:black;display:inline-block;"> </span>   Ped. (L1) <span class="ltx_rule" style="background:black;display:inline-block;"> </span>   Ped. (L2) <span class="ltx_rule" style="background:black;display:inline-block;"> </span>   Cyc. (L1)    Cyc. (L2)  
<br class="ltx_break">  mAP  mAPH  mAP  mAPH  mAP  mAPH  mAP  mAPH  mAP  mAPH  mAP  mAPH 
<br class="ltx_break">SECOND <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>  One  71.0  70.3  62.6  62.0  65.2  54.2  57.2  47.5  57.1  55.6  55.0  53.5 
<br class="ltx_break">PointPillars <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>  One  70.4  69.8  62.2  61.6  66.2  46.3  58.2  40.6  55.3  51.8  53.2  49.8 
<br class="ltx_break">IA-SSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>  One  70.5  69.7  61.6  60.8  69.4  58.5  60.3  50.7  67.7  65.3  65.0  62.7 
<br class="ltx_break">SST (Center) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>  One  75.1  74.6  66.6  66.2  80.1  72.1  72.4  65.0  71.5  70.2  68.9  67.6 
<br class="ltx_break">CenterPoint-Voxel <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>  One  72.8  72.2  64.9  64.4  74.2  68.0  66.0  60.3  71.0  69.8  68.5  67.3 
<br class="ltx_break"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>+SSGNet (Ours)  One  75.0  74.4  67.0  66.5  75.6  69.0  67.9  61.9  71.8  70.7  69.2  68.1 
<br class="ltx_break"> <span id="S3.SS2.23.30.1.3" class="ltx_text ltx_font_bold">Absolute Improvements</span> <span class="ltx_rule" style="background:black;display:inline-block;"> </span>  <span id="S3.SS2.23.30.1.4" class="ltx_text ltx_font_bold">+2.2</span>  <span id="S3.SS2.23.30.1.5" class="ltx_text ltx_font_bold">+2.2</span>  <span id="S3.SS2.23.30.1.6" class="ltx_text ltx_font_bold">+2.1</span>  <span id="S3.SS2.23.30.1.7" class="ltx_text ltx_font_bold">+2.1</span>  <span id="S3.SS2.23.30.1.8" class="ltx_text ltx_font_bold">+1.4</span>  <span id="S3.SS2.23.30.1.9" class="ltx_text ltx_font_bold">+1.0</span>  <span id="S3.SS2.23.30.1.10" class="ltx_text ltx_font_bold">+1.9</span>  <span id="S3.SS2.23.30.1.11" class="ltx_text ltx_font_bold">+1.6</span>  <span id="S3.SS2.23.30.1.12" class="ltx_text ltx_font_bold">+0.8</span>  <span id="S3.SS2.23.30.1.13" class="ltx_text ltx_font_bold">+0.9</span>  <span id="S3.SS2.23.30.1.14" class="ltx_text ltx_font_bold">+0.7</span>  <span id="S3.SS2.23.30.1.15" class="ltx_text ltx_font_bold">+0.8</span> 
<br class="ltx_break">PillarNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>  One  74.0  73.4  66.0  65.5  72.3  63.8  64.5  56.7  67.8  66.5  65.2  64.0 
<br class="ltx_break"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>+SSGNet (Ours)  One  75.8  75.1  67.9  67.3  74.0  65.3  66.1  58.2  69.3  68.1  66.7  65.5 
<br class="ltx_break"> <span id="S3.SS2.23.30.1.16" class="ltx_text ltx_font_bold">Absolute Improvements</span> <span class="ltx_rule" style="background:black;display:inline-block;"> </span>  <span id="S3.SS2.23.30.1.17" class="ltx_text ltx_font_bold">+1.8</span>  <span id="S3.SS2.23.30.1.18" class="ltx_text ltx_font_bold">+1.7 </span> <span id="S3.SS2.23.30.1.19" class="ltx_text ltx_font_bold">+1.9</span>  <span id="S3.SS2.23.30.1.20" class="ltx_text ltx_font_bold">+1.8</span>  <span id="S3.SS2.23.30.1.21" class="ltx_text ltx_font_bold">+1.7</span>  <span id="S3.SS2.23.30.1.22" class="ltx_text ltx_font_bold">+1.5</span>  <span id="S3.SS2.23.30.1.23" class="ltx_text ltx_font_bold">+1.6</span>  <span id="S3.SS2.23.30.1.24" class="ltx_text ltx_font_bold">+1.5</span>  <span id="S3.SS2.23.30.1.25" class="ltx_text ltx_font_bold">+1.5</span>  <span id="S3.SS2.23.30.1.26" class="ltx_text ltx_font_bold">+1.6</span>  <span id="S3.SS2.23.30.1.27" class="ltx_text ltx_font_bold">+1.5</span>  <span id="S3.SS2.23.30.1.28" class="ltx_text ltx_font_bold">+1.5</span> 
<br class="ltx_break">VoxSeT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>  One  72.1  71.6  63.6  63.2  77.9  69.6  70.2  62.5  69.9  68.5  67.3  66.0 
<br class="ltx_break"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>+SSGNet (Ours)  One  76.0  75.5  67.9  67.4  79.4  71.3  71.8  64.3  72.1  70.9  69.4  68.3 
<br class="ltx_break"> <span id="S3.SS2.23.30.1.29" class="ltx_text ltx_font_bold">Absolute Improvements</span> <span class="ltx_rule" style="background:black;display:inline-block;"> </span>  <span id="S3.SS2.23.30.1.30" class="ltx_text ltx_font_bold">+3.9</span>  <span id="S3.SS2.23.30.1.31" class="ltx_text ltx_font_bold">+3.9</span>  <span id="S3.SS2.23.30.1.32" class="ltx_text ltx_font_bold">+4.3</span>  <span id="S3.SS2.23.30.1.33" class="ltx_text ltx_font_bold">+4.2</span>  <span id="S3.SS2.23.30.1.34" class="ltx_text ltx_font_bold">+1.5</span>  <span id="S3.SS2.23.30.1.35" class="ltx_text ltx_font_bold">+1.7</span>  <span id="S3.SS2.23.30.1.36" class="ltx_text ltx_font_bold">+1.6</span>  <span id="S3.SS2.23.30.1.37" class="ltx_text ltx_font_bold">+1.8</span>  <span id="S3.SS2.23.30.1.38" class="ltx_text ltx_font_bold">+2.2</span>  <span id="S3.SS2.23.30.1.39" class="ltx_text ltx_font_bold">+2.4</span>  <span id="S3.SS2.23.30.1.40" class="ltx_text ltx_font_bold">+2.1</span>  <span id="S3.SS2.23.30.1.41" class="ltx_text ltx_font_bold">+2.3</span> 
<br class="ltx_break">PV-RCNN (Center) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>  Two  76.0  75.4  68.0  67.5  75.9  69.4  67.7  61.6  70.2  69.0  67.7  66.6 
<br class="ltx_break"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>+SSGNet (Ours)  Two  78.3  77.8  70.1  69.6  79.6  73.3  71.4  65.4  71.5  70.4  68.9  67.9 
<br class="ltx_break"> <span id="S3.SS2.23.30.1.42" class="ltx_text ltx_font_bold">Absolute Improvements</span> <span class="ltx_rule" style="background:black;display:inline-block;"> </span>  <span id="S3.SS2.23.30.1.43" class="ltx_text ltx_font_bold">+2.3</span>  <span id="S3.SS2.23.30.1.44" class="ltx_text ltx_font_bold">+2.4</span>  <span id="S3.SS2.23.30.1.45" class="ltx_text ltx_font_bold">+2.1</span>  <span id="S3.SS2.23.30.1.46" class="ltx_text ltx_font_bold">+2.1</span>  <span id="S3.SS2.23.30.1.47" class="ltx_text ltx_font_bold">+3.7</span>  <span id="S3.SS2.23.30.1.48" class="ltx_text ltx_font_bold">+3.9</span>  <span id="S3.SS2.23.30.1.49" class="ltx_text ltx_font_bold">+3.7</span>  <span id="S3.SS2.23.30.1.50" class="ltx_text ltx_font_bold">+3.8</span>  <span id="S3.SS2.23.30.1.51" class="ltx_text ltx_font_bold">+1.3</span>  <span id="S3.SS2.23.30.1.52" class="ltx_text ltx_font_bold">+1.4</span>  <span id="S3.SS2.23.30.1.53" class="ltx_text ltx_font_bold">+1.2</span>  <span id="S3.SS2.23.30.1.54" class="ltx_text ltx_font_bold">+1.3</span> 
<br class="ltx_break"> Results When Trained With 100% Training Examples  
<br class="ltx_break"><span id="S3.SS2.23.30.1.55" class="ltx_text">Methods</span>  <span id="S3.SS2.23.30.1.56" class="ltx_text">Stages</span>   Veh. (L1) <span class="ltx_rule" style="background:black;display:inline-block;"> </span>   Veh. (L2) <span class="ltx_rule" style="background:black;display:inline-block;"> </span>   Ped. (L1) <span class="ltx_rule" style="background:black;display:inline-block;"> </span>   Ped. (L2) <span class="ltx_rule" style="background:black;display:inline-block;"> </span>   Cyc. (L1) <span class="ltx_rule" style="background:black;display:inline-block;"> </span>   Cyc. (L2)  
<br class="ltx_break">  mAP  mAPH  mAP  mAPH  mAP  mAPH  mAP  mAPH  mAP  mAPH  mAP  mAPH 
<br class="ltx_break">AFDetV2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>  One  77.6  77.1  69.7  69.2  80.2  74.6  72.2  67.0  73.7  72.7  71.0  70.1 
<br class="ltx_break">SWFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>  One  77.8  77.3  69.2  68.8  80.9  72.7  72.5  64.9  -  -  -  - 
<br class="ltx_break">CenterFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>  One  75.0  74.4  69.9  69.4  78.6  73.0  73.6  68.3  72.3  71.3  69.8  68.8 
<br class="ltx_break">PV-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>  Two  78.0  77.5  69.4  69.0  79.2  73.0  70.4  64.7  71.5  70.3  69.0  67.8 
<br class="ltx_break">Part-A2-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>  Two  77.1  76.5  68.5  68.0  75.2  66.9  66.2  58.6  68.6  67.4  66.1  64.9 
<br class="ltx_break">LiDAR-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>  Two  76.0  75.5  68.3  67.9  71.2  58.7  63.1  51.7  68.6  66.9  66.1  64.4 
<br class="ltx_break">VoxSeT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>  One  74.5  74.0  66.0  65.6  80.0  72.4  72.5  65.4  71.6  70.3  69.0  67.7 
<br class="ltx_break"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>+SSGNet (Ours)  One  78.3  77.8  70.2  69.8  81.1  74.1  73.6  67.1  74.6  73.4  71.9  70.7 
<br class="ltx_break"> <span id="S3.SS2.23.30.1.57" class="ltx_text ltx_font_bold">Absolute Improvements</span> <span class="ltx_rule" style="background:black;display:inline-block;"> </span>  <span id="S3.SS2.23.30.1.58" class="ltx_text ltx_font_bold">+3.8</span>  <span id="S3.SS2.23.30.1.59" class="ltx_text ltx_font_bold">+3.8</span>  <span id="S3.SS2.23.30.1.60" class="ltx_text ltx_font_bold">+4.2</span>  <span id="S3.SS2.23.30.1.61" class="ltx_text ltx_font_bold">+4.2</span>  <span id="S3.SS2.23.30.1.62" class="ltx_text ltx_font_bold">+1.1</span>  <span id="S3.SS2.23.30.1.63" class="ltx_text ltx_font_bold">+1.7</span>  <span id="S3.SS2.23.30.1.64" class="ltx_text ltx_font_bold">+1.1</span>  <span id="S3.SS2.23.30.1.65" class="ltx_text ltx_font_bold">+1.7</span>  <span id="S3.SS2.23.30.1.66" class="ltx_text ltx_font_bold">+3.0</span>  <span id="S3.SS2.23.30.1.67" class="ltx_text ltx_font_bold">+3.1</span>  <span id="S3.SS2.23.30.1.68" class="ltx_text ltx_font_bold">+2.9</span>  <span id="S3.SS2.23.30.1.69" class="ltx_text ltx_font_bold">+3.0</span> 
<br class="ltx_break"></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.SS2.23.23" class="ltx_table ltx_figure_panel">

<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Quantitative comparison of SSGNet with different baselines on the nuScenes dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S3.SS2.23.23.29" class="ltx_ERROR ltx_figure_panel undefined">{Tabular}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.SS2.23.23.30" class="ltx_p ltx_figure_panel"><span id="S3.SS2.23.23.30.1" class="ltx_text" style="font-size:80%;">c—cc—c—c—c—c—c—c—c—c—c—c
Methods  mAP  NDS  Car  Truck  Bus  Trailer  C.V.  Ped  Mot  Byc  T.C.  Bar </span>
<br class="ltx_break"><span id="S3.SS2.23.23.30.2" class="ltx_text" style="font-size:80%;">SECOND-CBGS </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.23.23.30.3.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib56" title="" class="ltx_ref">56</a><span id="S3.SS2.23.23.30.4.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S3.SS2.23.23.30.5" class="ltx_text" style="font-size:80%;">  50.6  62.3  -  -  -  -  -  -  -  -  -  - </span>
<br class="ltx_break"><span id="S3.SS2.23.23.30.6" class="ltx_text" style="font-size:80%;">PillarNet-18 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.23.23.30.7.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S3.SS2.23.23.30.8.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S3.SS2.23.23.30.9" class="ltx_text" style="font-size:80%;">  59.9  67.4  -  -  -  -  -  -  -  -  -  - </span>
<br class="ltx_break"><span id="S3.SS2.23.23.30.10" class="ltx_text" style="font-size:80%;">LargeKernel3D </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.23.23.30.11.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib6" title="" class="ltx_ref">6</a><span id="S3.SS2.23.23.30.12.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S3.SS2.23.23.30.13" class="ltx_text" style="font-size:80%;">  60.5  67.6  85.8  59.0  72.8  40.0  19.2  85.6  61.3  43.6  70.4  67.5 </span>
<br class="ltx_break"><span id="S3.SS2.23.23.30.14" class="ltx_text" style="font-size:80%;">Focals Conv </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.23.23.30.15.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S3.SS2.23.23.30.16.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S3.SS2.23.23.30.17" class="ltx_text" style="font-size:80%;">  61.2  68.1  86.6  60.2  72.3  40.8  </span><span id="S3.SS2.23.23.30.18" class="ltx_text ltx_font_bold" style="font-size:80%;">20.1</span><span id="S3.SS2.23.23.30.19" class="ltx_text" style="font-size:80%;">  </span><span id="S3.SS2.23.23.30.20" class="ltx_text ltx_font_bold" style="font-size:80%;">86.2</span><span id="S3.SS2.23.23.30.21" class="ltx_text" style="font-size:80%;">  61.3  45.6  70.2  </span><span id="S3.SS2.23.23.30.22" class="ltx_text ltx_font_bold" style="font-size:80%;">69.3</span><span id="S3.SS2.23.23.30.23" class="ltx_text" style="font-size:80%;"> </span>
<br class="ltx_break"><span id="S3.SS2.23.23.30.24" class="ltx_text" style="font-size:80%;">CenterPoint-Voxel </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.23.23.30.25.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib50" title="" class="ltx_ref">50</a><span id="S3.SS2.23.23.30.26.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S3.SS2.23.23.30.27" class="ltx_text" style="font-size:80%;">  59.0  66.4  85.6  57.2  71.2  37.3  16.2  85.1  58.4  41.0  69.2  68.2 </span>
<br class="ltx_break"><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.23.23.30.28.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib50" title="" class="ltx_ref">50</a><span id="S3.SS2.23.23.30.29.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S3.SS2.23.23.30.30" class="ltx_text" style="font-size:80%;">+SSGNet (Ours)  </span><span id="S3.SS2.23.23.30.31" class="ltx_text ltx_font_bold" style="font-size:80%;">61.7</span><span id="S3.SS2.23.23.30.32" class="ltx_text" style="font-size:80%;">  </span><span id="S3.SS2.23.23.30.33" class="ltx_text ltx_font_bold" style="font-size:80%;">68.3</span><span id="S3.SS2.23.23.30.34" class="ltx_text" style="font-size:80%;">  </span><span id="S3.SS2.23.23.30.35" class="ltx_text ltx_font_bold" style="font-size:80%;">87.1</span><span id="S3.SS2.23.23.30.36" class="ltx_text" style="font-size:80%;">  </span><span id="S3.SS2.23.23.30.37" class="ltx_text ltx_font_bold" style="font-size:80%;">60.5</span><span id="S3.SS2.23.23.30.38" class="ltx_text" style="font-size:80%;">  </span><span id="S3.SS2.23.23.30.39" class="ltx_text ltx_font_bold" style="font-size:80%;">73.9</span><span id="S3.SS2.23.23.30.40" class="ltx_text" style="font-size:80%;">  </span><span id="S3.SS2.23.23.30.41" class="ltx_text ltx_font_bold" style="font-size:80%;">43.3</span><span id="S3.SS2.23.23.30.42" class="ltx_text" style="font-size:80%;">  19.2  </span><span id="S3.SS2.23.23.30.43" class="ltx_text ltx_font_bold" style="font-size:80%;">86.2</span><span id="S3.SS2.23.23.30.44" class="ltx_text" style="font-size:80%;">  </span><span id="S3.SS2.23.23.30.45" class="ltx_text ltx_font_bold" style="font-size:80%;">63.0</span><span id="S3.SS2.23.23.30.46" class="ltx_text" style="font-size:80%;">  </span><span id="S3.SS2.23.23.30.47" class="ltx_text ltx_font_bold" style="font-size:80%;">45.7</span><span id="S3.SS2.23.23.30.48" class="ltx_text" style="font-size:80%;">  </span><span id="S3.SS2.23.23.30.49" class="ltx_text ltx_font_bold" style="font-size:80%;">72.1</span><span id="S3.SS2.23.23.30.50" class="ltx_text" style="font-size:80%;">  66.4 </span>
<br class="ltx_break"><span id="S3.SS2.23.23.30.51" class="ltx_text" style="font-size:80%;"></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.SS2.19.19.19" class="ltx_p ltx_figure_panel"><span id="S3.SS2.19.19.19.1" class="ltx_text" style="font-size:80%;">Computing the implicit training loss requires us to sample a large number of query points and compare the ground-truth mask and predicted probabilities, which is a time-consuming operation and typically involves additional data pre-processing </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.19.19.19.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib26" title="" class="ltx_ref">26</a><span id="S3.SS2.19.19.19.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S3.SS2.19.19.19.4" class="ltx_text" style="font-size:80%;">. We introduce a handy yet efficient point sampler built upon hyper-parameters </span><math id="S3.SS2.1.1.1.m1.1" class="ltx_Math" alttext="N_{s}\in\mathbb{N}" display="inline"><semantics id="S3.SS2.1.1.1.m1.1a"><mrow id="S3.SS2.1.1.1.m1.1.1" xref="S3.SS2.1.1.1.m1.1.1.cmml"><msub id="S3.SS2.1.1.1.m1.1.1.2" xref="S3.SS2.1.1.1.m1.1.1.2.cmml"><mi mathsize="80%" id="S3.SS2.1.1.1.m1.1.1.2.2" xref="S3.SS2.1.1.1.m1.1.1.2.2.cmml">N</mi><mi mathsize="80%" id="S3.SS2.1.1.1.m1.1.1.2.3" xref="S3.SS2.1.1.1.m1.1.1.2.3.cmml">s</mi></msub><mo mathsize="80%" id="S3.SS2.1.1.1.m1.1.1.1" xref="S3.SS2.1.1.1.m1.1.1.1.cmml">∈</mo><mi mathsize="80%" id="S3.SS2.1.1.1.m1.1.1.3" xref="S3.SS2.1.1.1.m1.1.1.3.cmml">ℕ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.1.1.1.m1.1b"><apply id="S3.SS2.1.1.1.m1.1.1.cmml" xref="S3.SS2.1.1.1.m1.1.1"><in id="S3.SS2.1.1.1.m1.1.1.1.cmml" xref="S3.SS2.1.1.1.m1.1.1.1"></in><apply id="S3.SS2.1.1.1.m1.1.1.2.cmml" xref="S3.SS2.1.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.1.1.1.m1.1.1.2.1.cmml" xref="S3.SS2.1.1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.1.1.1.m1.1.1.2.2.cmml" xref="S3.SS2.1.1.1.m1.1.1.2.2">𝑁</ci><ci id="S3.SS2.1.1.1.m1.1.1.2.3.cmml" xref="S3.SS2.1.1.1.m1.1.1.2.3">𝑠</ci></apply><ci id="S3.SS2.1.1.1.m1.1.1.3.cmml" xref="S3.SS2.1.1.1.m1.1.1.3">ℕ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.1.1.1.m1.1c">N_{s}\in\mathbb{N}</annotation></semantics></math><span id="S3.SS2.19.19.19.5" class="ltx_text" style="font-size:80%;">, </span><math id="S3.SS2.2.2.2.m2.2" class="ltx_Math" alttext="\alpha\in[0,1]" display="inline"><semantics id="S3.SS2.2.2.2.m2.2a"><mrow id="S3.SS2.2.2.2.m2.2.3" xref="S3.SS2.2.2.2.m2.2.3.cmml"><mi mathsize="80%" id="S3.SS2.2.2.2.m2.2.3.2" xref="S3.SS2.2.2.2.m2.2.3.2.cmml">α</mi><mo mathsize="80%" id="S3.SS2.2.2.2.m2.2.3.1" xref="S3.SS2.2.2.2.m2.2.3.1.cmml">∈</mo><mrow id="S3.SS2.2.2.2.m2.2.3.3.2" xref="S3.SS2.2.2.2.m2.2.3.3.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.SS2.2.2.2.m2.2.3.3.2.1" xref="S3.SS2.2.2.2.m2.2.3.3.1.cmml">[</mo><mn mathsize="80%" id="S3.SS2.2.2.2.m2.1.1" xref="S3.SS2.2.2.2.m2.1.1.cmml">0</mn><mo mathsize="80%" id="S3.SS2.2.2.2.m2.2.3.3.2.2" xref="S3.SS2.2.2.2.m2.2.3.3.1.cmml">,</mo><mn mathsize="80%" id="S3.SS2.2.2.2.m2.2.2" xref="S3.SS2.2.2.2.m2.2.2.cmml">1</mn><mo maxsize="80%" minsize="80%" id="S3.SS2.2.2.2.m2.2.3.3.2.3" xref="S3.SS2.2.2.2.m2.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.2.2.2.m2.2b"><apply id="S3.SS2.2.2.2.m2.2.3.cmml" xref="S3.SS2.2.2.2.m2.2.3"><in id="S3.SS2.2.2.2.m2.2.3.1.cmml" xref="S3.SS2.2.2.2.m2.2.3.1"></in><ci id="S3.SS2.2.2.2.m2.2.3.2.cmml" xref="S3.SS2.2.2.2.m2.2.3.2">𝛼</ci><interval closure="closed" id="S3.SS2.2.2.2.m2.2.3.3.1.cmml" xref="S3.SS2.2.2.2.m2.2.3.3.2"><cn type="integer" id="S3.SS2.2.2.2.m2.1.1.cmml" xref="S3.SS2.2.2.2.m2.1.1">0</cn><cn type="integer" id="S3.SS2.2.2.2.m2.2.2.cmml" xref="S3.SS2.2.2.2.m2.2.2">1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.2.2.2.m2.2c">\alpha\in[0,1]</annotation></semantics></math><span id="S3.SS2.19.19.19.6" class="ltx_text" style="font-size:80%;">, and </span><math id="S3.SS2.3.3.3.m3.2" class="ltx_Math" alttext="\beta\in[0,1]" display="inline"><semantics id="S3.SS2.3.3.3.m3.2a"><mrow id="S3.SS2.3.3.3.m3.2.3" xref="S3.SS2.3.3.3.m3.2.3.cmml"><mi mathsize="80%" id="S3.SS2.3.3.3.m3.2.3.2" xref="S3.SS2.3.3.3.m3.2.3.2.cmml">β</mi><mo mathsize="80%" id="S3.SS2.3.3.3.m3.2.3.1" xref="S3.SS2.3.3.3.m3.2.3.1.cmml">∈</mo><mrow id="S3.SS2.3.3.3.m3.2.3.3.2" xref="S3.SS2.3.3.3.m3.2.3.3.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.SS2.3.3.3.m3.2.3.3.2.1" xref="S3.SS2.3.3.3.m3.2.3.3.1.cmml">[</mo><mn mathsize="80%" id="S3.SS2.3.3.3.m3.1.1" xref="S3.SS2.3.3.3.m3.1.1.cmml">0</mn><mo mathsize="80%" id="S3.SS2.3.3.3.m3.2.3.3.2.2" xref="S3.SS2.3.3.3.m3.2.3.3.1.cmml">,</mo><mn mathsize="80%" id="S3.SS2.3.3.3.m3.2.2" xref="S3.SS2.3.3.3.m3.2.2.cmml">1</mn><mo maxsize="80%" minsize="80%" id="S3.SS2.3.3.3.m3.2.3.3.2.3" xref="S3.SS2.3.3.3.m3.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.3.3.3.m3.2b"><apply id="S3.SS2.3.3.3.m3.2.3.cmml" xref="S3.SS2.3.3.3.m3.2.3"><in id="S3.SS2.3.3.3.m3.2.3.1.cmml" xref="S3.SS2.3.3.3.m3.2.3.1"></in><ci id="S3.SS2.3.3.3.m3.2.3.2.cmml" xref="S3.SS2.3.3.3.m3.2.3.2">𝛽</ci><interval closure="closed" id="S3.SS2.3.3.3.m3.2.3.3.1.cmml" xref="S3.SS2.3.3.3.m3.2.3.3.2"><cn type="integer" id="S3.SS2.3.3.3.m3.1.1.cmml" xref="S3.SS2.3.3.3.m3.1.1">0</cn><cn type="integer" id="S3.SS2.3.3.3.m3.2.2.cmml" xref="S3.SS2.3.3.3.m3.2.2">1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.3.3.3.m3.2c">\beta\in[0,1]</annotation></semantics></math><span id="S3.SS2.19.19.19.7" class="ltx_text" style="font-size:80%;">. Here, </span><math id="S3.SS2.4.4.4.m4.1" class="ltx_Math" alttext="N_{s}" display="inline"><semantics id="S3.SS2.4.4.4.m4.1a"><msub id="S3.SS2.4.4.4.m4.1.1" xref="S3.SS2.4.4.4.m4.1.1.cmml"><mi mathsize="80%" id="S3.SS2.4.4.4.m4.1.1.2" xref="S3.SS2.4.4.4.m4.1.1.2.cmml">N</mi><mi mathsize="80%" id="S3.SS2.4.4.4.m4.1.1.3" xref="S3.SS2.4.4.4.m4.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.4.4.4.m4.1b"><apply id="S3.SS2.4.4.4.m4.1.1.cmml" xref="S3.SS2.4.4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.4.4.4.m4.1.1.1.cmml" xref="S3.SS2.4.4.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.4.4.4.m4.1.1.2.cmml" xref="S3.SS2.4.4.4.m4.1.1.2">𝑁</ci><ci id="S3.SS2.4.4.4.m4.1.1.3.cmml" xref="S3.SS2.4.4.4.m4.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.4.4.4.m4.1c">N_{s}</annotation></semantics></math><span id="S3.SS2.19.19.19.8" class="ltx_text" style="font-size:80%;"> is the total number of points to be sampled, and </span><math id="S3.SS2.5.5.5.m5.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS2.5.5.5.m5.1a"><mi mathsize="80%" id="S3.SS2.5.5.5.m5.1.1" xref="S3.SS2.5.5.5.m5.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.5.5.5.m5.1b"><ci id="S3.SS2.5.5.5.m5.1.1.cmml" xref="S3.SS2.5.5.5.m5.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.5.5.5.m5.1c">\alpha</annotation></semantics></math><span id="S3.SS2.19.19.19.9" class="ltx_text" style="font-size:80%;"> controls the ratio of uniform sampling to importance sampling. After obtaining </span><math id="S3.SS2.6.6.6.m6.1" class="ltx_Math" alttext="\alpha N_{s}" display="inline"><semantics id="S3.SS2.6.6.6.m6.1a"><mrow id="S3.SS2.6.6.6.m6.1.1" xref="S3.SS2.6.6.6.m6.1.1.cmml"><mi mathsize="80%" id="S3.SS2.6.6.6.m6.1.1.2" xref="S3.SS2.6.6.6.m6.1.1.2.cmml">α</mi><mo lspace="0em" rspace="0em" id="S3.SS2.6.6.6.m6.1.1.1" xref="S3.SS2.6.6.6.m6.1.1.1.cmml">​</mo><msub id="S3.SS2.6.6.6.m6.1.1.3" xref="S3.SS2.6.6.6.m6.1.1.3.cmml"><mi mathsize="80%" id="S3.SS2.6.6.6.m6.1.1.3.2" xref="S3.SS2.6.6.6.m6.1.1.3.2.cmml">N</mi><mi mathsize="80%" id="S3.SS2.6.6.6.m6.1.1.3.3" xref="S3.SS2.6.6.6.m6.1.1.3.3.cmml">s</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.6.6.6.m6.1b"><apply id="S3.SS2.6.6.6.m6.1.1.cmml" xref="S3.SS2.6.6.6.m6.1.1"><times id="S3.SS2.6.6.6.m6.1.1.1.cmml" xref="S3.SS2.6.6.6.m6.1.1.1"></times><ci id="S3.SS2.6.6.6.m6.1.1.2.cmml" xref="S3.SS2.6.6.6.m6.1.1.2">𝛼</ci><apply id="S3.SS2.6.6.6.m6.1.1.3.cmml" xref="S3.SS2.6.6.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.6.6.6.m6.1.1.3.1.cmml" xref="S3.SS2.6.6.6.m6.1.1.3">subscript</csymbol><ci id="S3.SS2.6.6.6.m6.1.1.3.2.cmml" xref="S3.SS2.6.6.6.m6.1.1.3.2">𝑁</ci><ci id="S3.SS2.6.6.6.m6.1.1.3.3.cmml" xref="S3.SS2.6.6.6.m6.1.1.3.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.6.6.6.m6.1c">\alpha N_{s}</annotation></semantics></math><span id="S3.SS2.19.19.19.10" class="ltx_text" style="font-size:80%;"> points by regular uniform sampling across the entire scene, the other </span><math id="S3.SS2.7.7.7.m7.1" class="ltx_Math" alttext="(1-\alpha)N_{s}" display="inline"><semantics id="S3.SS2.7.7.7.m7.1a"><mrow id="S3.SS2.7.7.7.m7.1.1" xref="S3.SS2.7.7.7.m7.1.1.cmml"><mrow id="S3.SS2.7.7.7.m7.1.1.1.1" xref="S3.SS2.7.7.7.m7.1.1.1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.SS2.7.7.7.m7.1.1.1.1.2" xref="S3.SS2.7.7.7.m7.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.7.7.7.m7.1.1.1.1.1" xref="S3.SS2.7.7.7.m7.1.1.1.1.1.cmml"><mn mathsize="80%" id="S3.SS2.7.7.7.m7.1.1.1.1.1.2" xref="S3.SS2.7.7.7.m7.1.1.1.1.1.2.cmml">1</mn><mo mathsize="80%" id="S3.SS2.7.7.7.m7.1.1.1.1.1.1" xref="S3.SS2.7.7.7.m7.1.1.1.1.1.1.cmml">−</mo><mi mathsize="80%" id="S3.SS2.7.7.7.m7.1.1.1.1.1.3" xref="S3.SS2.7.7.7.m7.1.1.1.1.1.3.cmml">α</mi></mrow><mo maxsize="80%" minsize="80%" id="S3.SS2.7.7.7.m7.1.1.1.1.3" xref="S3.SS2.7.7.7.m7.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.SS2.7.7.7.m7.1.1.2" xref="S3.SS2.7.7.7.m7.1.1.2.cmml">​</mo><msub id="S3.SS2.7.7.7.m7.1.1.3" xref="S3.SS2.7.7.7.m7.1.1.3.cmml"><mi mathsize="80%" id="S3.SS2.7.7.7.m7.1.1.3.2" xref="S3.SS2.7.7.7.m7.1.1.3.2.cmml">N</mi><mi mathsize="80%" id="S3.SS2.7.7.7.m7.1.1.3.3" xref="S3.SS2.7.7.7.m7.1.1.3.3.cmml">s</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.7.7.7.m7.1b"><apply id="S3.SS2.7.7.7.m7.1.1.cmml" xref="S3.SS2.7.7.7.m7.1.1"><times id="S3.SS2.7.7.7.m7.1.1.2.cmml" xref="S3.SS2.7.7.7.m7.1.1.2"></times><apply id="S3.SS2.7.7.7.m7.1.1.1.1.1.cmml" xref="S3.SS2.7.7.7.m7.1.1.1.1"><minus id="S3.SS2.7.7.7.m7.1.1.1.1.1.1.cmml" xref="S3.SS2.7.7.7.m7.1.1.1.1.1.1"></minus><cn type="integer" id="S3.SS2.7.7.7.m7.1.1.1.1.1.2.cmml" xref="S3.SS2.7.7.7.m7.1.1.1.1.1.2">1</cn><ci id="S3.SS2.7.7.7.m7.1.1.1.1.1.3.cmml" xref="S3.SS2.7.7.7.m7.1.1.1.1.1.3">𝛼</ci></apply><apply id="S3.SS2.7.7.7.m7.1.1.3.cmml" xref="S3.SS2.7.7.7.m7.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.7.7.7.m7.1.1.3.1.cmml" xref="S3.SS2.7.7.7.m7.1.1.3">subscript</csymbol><ci id="S3.SS2.7.7.7.m7.1.1.3.2.cmml" xref="S3.SS2.7.7.7.m7.1.1.3.2">𝑁</ci><ci id="S3.SS2.7.7.7.m7.1.1.3.3.cmml" xref="S3.SS2.7.7.7.m7.1.1.3.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.7.7.7.m7.1c">(1-\alpha)N_{s}</annotation></semantics></math><span id="S3.SS2.19.19.19.11" class="ltx_text" style="font-size:80%;"> points, referred to as </span><span id="S3.SS2.19.19.19.12" class="ltx_text ltx_font_italic" style="font-size:80%;">important points</span><span id="S3.SS2.19.19.19.13" class="ltx_text" style="font-size:80%;">, are sampled either inside or in close proximity to the foreground object bounding boxes. The number of points inside foreground boxes and the number of points outside but close to foreground boxes are balanced by the hyper-parameter </span><math id="S3.SS2.8.8.8.m8.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S3.SS2.8.8.8.m8.1a"><mi mathsize="80%" id="S3.SS2.8.8.8.m8.1.1" xref="S3.SS2.8.8.8.m8.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.8.8.8.m8.1b"><ci id="S3.SS2.8.8.8.m8.1.1.cmml" xref="S3.SS2.8.8.8.m8.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.8.8.8.m8.1c">\beta</annotation></semantics></math><span id="S3.SS2.19.19.19.14" class="ltx_text" style="font-size:80%;">. Let </span><math id="S3.SS2.9.9.9.m9.1" class="ltx_Math" alttext="K\in\mathbb{N}" display="inline"><semantics id="S3.SS2.9.9.9.m9.1a"><mrow id="S3.SS2.9.9.9.m9.1.1" xref="S3.SS2.9.9.9.m9.1.1.cmml"><mi mathsize="80%" id="S3.SS2.9.9.9.m9.1.1.2" xref="S3.SS2.9.9.9.m9.1.1.2.cmml">K</mi><mo mathsize="80%" id="S3.SS2.9.9.9.m9.1.1.1" xref="S3.SS2.9.9.9.m9.1.1.1.cmml">∈</mo><mi mathsize="80%" id="S3.SS2.9.9.9.m9.1.1.3" xref="S3.SS2.9.9.9.m9.1.1.3.cmml">ℕ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.9.9.9.m9.1b"><apply id="S3.SS2.9.9.9.m9.1.1.cmml" xref="S3.SS2.9.9.9.m9.1.1"><in id="S3.SS2.9.9.9.m9.1.1.1.cmml" xref="S3.SS2.9.9.9.m9.1.1.1"></in><ci id="S3.SS2.9.9.9.m9.1.1.2.cmml" xref="S3.SS2.9.9.9.m9.1.1.2">𝐾</ci><ci id="S3.SS2.9.9.9.m9.1.1.3.cmml" xref="S3.SS2.9.9.9.m9.1.1.3">ℕ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.9.9.9.m9.1c">K\in\mathbb{N}</annotation></semantics></math><span id="S3.SS2.19.19.19.15" class="ltx_text" style="font-size:80%;"> be the number of foreground objects in a specific scene. We first uniformly sample </span><math id="S3.SS2.10.10.10.m10.1" class="ltx_Math" alttext="\frac{(1-\alpha)N_{s}}{K}" display="inline"><semantics id="S3.SS2.10.10.10.m10.1a"><mfrac id="S3.SS2.10.10.10.m10.1.1" xref="S3.SS2.10.10.10.m10.1.1.cmml"><mrow id="S3.SS2.10.10.10.m10.1.1.1" xref="S3.SS2.10.10.10.m10.1.1.1.cmml"><mrow id="S3.SS2.10.10.10.m10.1.1.1.1.1" xref="S3.SS2.10.10.10.m10.1.1.1.1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.SS2.10.10.10.m10.1.1.1.1.1.2" xref="S3.SS2.10.10.10.m10.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.10.10.10.m10.1.1.1.1.1.1" xref="S3.SS2.10.10.10.m10.1.1.1.1.1.1.cmml"><mn mathsize="80%" id="S3.SS2.10.10.10.m10.1.1.1.1.1.1.2" xref="S3.SS2.10.10.10.m10.1.1.1.1.1.1.2.cmml">1</mn><mo mathsize="80%" id="S3.SS2.10.10.10.m10.1.1.1.1.1.1.1" xref="S3.SS2.10.10.10.m10.1.1.1.1.1.1.1.cmml">−</mo><mi mathsize="80%" id="S3.SS2.10.10.10.m10.1.1.1.1.1.1.3" xref="S3.SS2.10.10.10.m10.1.1.1.1.1.1.3.cmml">α</mi></mrow><mo maxsize="80%" minsize="80%" id="S3.SS2.10.10.10.m10.1.1.1.1.1.3" xref="S3.SS2.10.10.10.m10.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.SS2.10.10.10.m10.1.1.1.2" xref="S3.SS2.10.10.10.m10.1.1.1.2.cmml">​</mo><msub id="S3.SS2.10.10.10.m10.1.1.1.3" xref="S3.SS2.10.10.10.m10.1.1.1.3.cmml"><mi mathsize="80%" id="S3.SS2.10.10.10.m10.1.1.1.3.2" xref="S3.SS2.10.10.10.m10.1.1.1.3.2.cmml">N</mi><mi mathsize="80%" id="S3.SS2.10.10.10.m10.1.1.1.3.3" xref="S3.SS2.10.10.10.m10.1.1.1.3.3.cmml">s</mi></msub></mrow><mi mathsize="80%" id="S3.SS2.10.10.10.m10.1.1.3" xref="S3.SS2.10.10.10.m10.1.1.3.cmml">K</mi></mfrac><annotation-xml encoding="MathML-Content" id="S3.SS2.10.10.10.m10.1b"><apply id="S3.SS2.10.10.10.m10.1.1.cmml" xref="S3.SS2.10.10.10.m10.1.1"><divide id="S3.SS2.10.10.10.m10.1.1.2.cmml" xref="S3.SS2.10.10.10.m10.1.1"></divide><apply id="S3.SS2.10.10.10.m10.1.1.1.cmml" xref="S3.SS2.10.10.10.m10.1.1.1"><times id="S3.SS2.10.10.10.m10.1.1.1.2.cmml" xref="S3.SS2.10.10.10.m10.1.1.1.2"></times><apply id="S3.SS2.10.10.10.m10.1.1.1.1.1.1.cmml" xref="S3.SS2.10.10.10.m10.1.1.1.1.1"><minus id="S3.SS2.10.10.10.m10.1.1.1.1.1.1.1.cmml" xref="S3.SS2.10.10.10.m10.1.1.1.1.1.1.1"></minus><cn type="integer" id="S3.SS2.10.10.10.m10.1.1.1.1.1.1.2.cmml" xref="S3.SS2.10.10.10.m10.1.1.1.1.1.1.2">1</cn><ci id="S3.SS2.10.10.10.m10.1.1.1.1.1.1.3.cmml" xref="S3.SS2.10.10.10.m10.1.1.1.1.1.1.3">𝛼</ci></apply><apply id="S3.SS2.10.10.10.m10.1.1.1.3.cmml" xref="S3.SS2.10.10.10.m10.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.10.10.10.m10.1.1.1.3.1.cmml" xref="S3.SS2.10.10.10.m10.1.1.1.3">subscript</csymbol><ci id="S3.SS2.10.10.10.m10.1.1.1.3.2.cmml" xref="S3.SS2.10.10.10.m10.1.1.1.3.2">𝑁</ci><ci id="S3.SS2.10.10.10.m10.1.1.1.3.3.cmml" xref="S3.SS2.10.10.10.m10.1.1.1.3.3">𝑠</ci></apply></apply><ci id="S3.SS2.10.10.10.m10.1.1.3.cmml" xref="S3.SS2.10.10.10.m10.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.10.10.10.m10.1c">\frac{(1-\alpha)N_{s}}{K}</annotation></semantics></math><span id="S3.SS2.19.19.19.16" class="ltx_text" style="font-size:80%;"> points in a unit cube for each bounding box. The </span><math id="S3.SS2.11.11.11.m11.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS2.11.11.11.m11.1a"><mi mathsize="80%" id="S3.SS2.11.11.11.m11.1.1" xref="S3.SS2.11.11.11.m11.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.11.11.11.m11.1b"><ci id="S3.SS2.11.11.11.m11.1.1.cmml" xref="S3.SS2.11.11.11.m11.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.11.11.11.m11.1c">k</annotation></semantics></math><span id="S3.SS2.19.19.19.17" class="ltx_text" style="font-size:80%;">-th box (</span><math id="S3.SS2.12.12.12.m12.4" class="ltx_Math" alttext="k=\{1,2,...,K\}" display="inline"><semantics id="S3.SS2.12.12.12.m12.4a"><mrow id="S3.SS2.12.12.12.m12.4.5" xref="S3.SS2.12.12.12.m12.4.5.cmml"><mi mathsize="80%" id="S3.SS2.12.12.12.m12.4.5.2" xref="S3.SS2.12.12.12.m12.4.5.2.cmml">k</mi><mo mathsize="80%" id="S3.SS2.12.12.12.m12.4.5.1" xref="S3.SS2.12.12.12.m12.4.5.1.cmml">=</mo><mrow id="S3.SS2.12.12.12.m12.4.5.3.2" xref="S3.SS2.12.12.12.m12.4.5.3.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.SS2.12.12.12.m12.4.5.3.2.1" xref="S3.SS2.12.12.12.m12.4.5.3.1.cmml">{</mo><mn mathsize="80%" id="S3.SS2.12.12.12.m12.1.1" xref="S3.SS2.12.12.12.m12.1.1.cmml">1</mn><mo mathsize="80%" id="S3.SS2.12.12.12.m12.4.5.3.2.2" xref="S3.SS2.12.12.12.m12.4.5.3.1.cmml">,</mo><mn mathsize="80%" id="S3.SS2.12.12.12.m12.2.2" xref="S3.SS2.12.12.12.m12.2.2.cmml">2</mn><mo mathsize="80%" id="S3.SS2.12.12.12.m12.4.5.3.2.3" xref="S3.SS2.12.12.12.m12.4.5.3.1.cmml">,</mo><mi mathsize="80%" mathvariant="normal" id="S3.SS2.12.12.12.m12.3.3" xref="S3.SS2.12.12.12.m12.3.3.cmml">…</mi><mo mathsize="80%" id="S3.SS2.12.12.12.m12.4.5.3.2.4" xref="S3.SS2.12.12.12.m12.4.5.3.1.cmml">,</mo><mi mathsize="80%" id="S3.SS2.12.12.12.m12.4.4" xref="S3.SS2.12.12.12.m12.4.4.cmml">K</mi><mo maxsize="80%" minsize="80%" id="S3.SS2.12.12.12.m12.4.5.3.2.5" xref="S3.SS2.12.12.12.m12.4.5.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.12.12.12.m12.4b"><apply id="S3.SS2.12.12.12.m12.4.5.cmml" xref="S3.SS2.12.12.12.m12.4.5"><eq id="S3.SS2.12.12.12.m12.4.5.1.cmml" xref="S3.SS2.12.12.12.m12.4.5.1"></eq><ci id="S3.SS2.12.12.12.m12.4.5.2.cmml" xref="S3.SS2.12.12.12.m12.4.5.2">𝑘</ci><set id="S3.SS2.12.12.12.m12.4.5.3.1.cmml" xref="S3.SS2.12.12.12.m12.4.5.3.2"><cn type="integer" id="S3.SS2.12.12.12.m12.1.1.cmml" xref="S3.SS2.12.12.12.m12.1.1">1</cn><cn type="integer" id="S3.SS2.12.12.12.m12.2.2.cmml" xref="S3.SS2.12.12.12.m12.2.2">2</cn><ci id="S3.SS2.12.12.12.m12.3.3.cmml" xref="S3.SS2.12.12.12.m12.3.3">…</ci><ci id="S3.SS2.12.12.12.m12.4.4.cmml" xref="S3.SS2.12.12.12.m12.4.4">𝐾</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.12.12.12.m12.4c">k=\{1,2,...,K\}</annotation></semantics></math><span id="S3.SS2.19.19.19.18" class="ltx_text" style="font-size:80%;">) can be parameterized with </span><math id="S3.SS2.13.13.13.m13.14" class="ltx_Math" alttext="(c_{x}^{(k)},c_{y}^{(k)},c_{z}^{(k)},s_{x}^{(k)},s_{y}^{(k)},s_{z}^{(k)},r^{(k)})" display="inline"><semantics id="S3.SS2.13.13.13.m13.14a"><mrow id="S3.SS2.13.13.13.m13.14.14.7" xref="S3.SS2.13.13.13.m13.14.14.8.cmml"><mo maxsize="80%" minsize="80%" id="S3.SS2.13.13.13.m13.14.14.7.8" xref="S3.SS2.13.13.13.m13.14.14.8.cmml">(</mo><msubsup id="S3.SS2.13.13.13.m13.8.8.1.1" xref="S3.SS2.13.13.13.m13.8.8.1.1.cmml"><mi mathsize="80%" id="S3.SS2.13.13.13.m13.8.8.1.1.2.2" xref="S3.SS2.13.13.13.m13.8.8.1.1.2.2.cmml">c</mi><mi mathsize="80%" id="S3.SS2.13.13.13.m13.8.8.1.1.2.3" xref="S3.SS2.13.13.13.m13.8.8.1.1.2.3.cmml">x</mi><mrow id="S3.SS2.13.13.13.m13.1.1.1.3" xref="S3.SS2.13.13.13.m13.8.8.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.SS2.13.13.13.m13.1.1.1.3.1" xref="S3.SS2.13.13.13.m13.8.8.1.1.cmml">(</mo><mi mathsize="80%" id="S3.SS2.13.13.13.m13.1.1.1.1" xref="S3.SS2.13.13.13.m13.1.1.1.1.cmml">k</mi><mo maxsize="80%" minsize="80%" id="S3.SS2.13.13.13.m13.1.1.1.3.2" xref="S3.SS2.13.13.13.m13.8.8.1.1.cmml">)</mo></mrow></msubsup><mo mathsize="80%" id="S3.SS2.13.13.13.m13.14.14.7.9" xref="S3.SS2.13.13.13.m13.14.14.8.cmml">,</mo><msubsup id="S3.SS2.13.13.13.m13.9.9.2.2" xref="S3.SS2.13.13.13.m13.9.9.2.2.cmml"><mi mathsize="80%" id="S3.SS2.13.13.13.m13.9.9.2.2.2.2" xref="S3.SS2.13.13.13.m13.9.9.2.2.2.2.cmml">c</mi><mi mathsize="80%" id="S3.SS2.13.13.13.m13.9.9.2.2.2.3" xref="S3.SS2.13.13.13.m13.9.9.2.2.2.3.cmml">y</mi><mrow id="S3.SS2.13.13.13.m13.2.2.1.3" xref="S3.SS2.13.13.13.m13.9.9.2.2.cmml"><mo maxsize="80%" minsize="80%" id="S3.SS2.13.13.13.m13.2.2.1.3.1" xref="S3.SS2.13.13.13.m13.9.9.2.2.cmml">(</mo><mi mathsize="80%" id="S3.SS2.13.13.13.m13.2.2.1.1" xref="S3.SS2.13.13.13.m13.2.2.1.1.cmml">k</mi><mo maxsize="80%" minsize="80%" id="S3.SS2.13.13.13.m13.2.2.1.3.2" xref="S3.SS2.13.13.13.m13.9.9.2.2.cmml">)</mo></mrow></msubsup><mo mathsize="80%" id="S3.SS2.13.13.13.m13.14.14.7.10" xref="S3.SS2.13.13.13.m13.14.14.8.cmml">,</mo><msubsup id="S3.SS2.13.13.13.m13.10.10.3.3" xref="S3.SS2.13.13.13.m13.10.10.3.3.cmml"><mi mathsize="80%" id="S3.SS2.13.13.13.m13.10.10.3.3.2.2" xref="S3.SS2.13.13.13.m13.10.10.3.3.2.2.cmml">c</mi><mi mathsize="80%" id="S3.SS2.13.13.13.m13.10.10.3.3.2.3" xref="S3.SS2.13.13.13.m13.10.10.3.3.2.3.cmml">z</mi><mrow id="S3.SS2.13.13.13.m13.3.3.1.3" xref="S3.SS2.13.13.13.m13.10.10.3.3.cmml"><mo maxsize="80%" minsize="80%" id="S3.SS2.13.13.13.m13.3.3.1.3.1" xref="S3.SS2.13.13.13.m13.10.10.3.3.cmml">(</mo><mi mathsize="80%" id="S3.SS2.13.13.13.m13.3.3.1.1" xref="S3.SS2.13.13.13.m13.3.3.1.1.cmml">k</mi><mo maxsize="80%" minsize="80%" id="S3.SS2.13.13.13.m13.3.3.1.3.2" xref="S3.SS2.13.13.13.m13.10.10.3.3.cmml">)</mo></mrow></msubsup><mo mathsize="80%" id="S3.SS2.13.13.13.m13.14.14.7.11" xref="S3.SS2.13.13.13.m13.14.14.8.cmml">,</mo><msubsup id="S3.SS2.13.13.13.m13.11.11.4.4" xref="S3.SS2.13.13.13.m13.11.11.4.4.cmml"><mi mathsize="80%" id="S3.SS2.13.13.13.m13.11.11.4.4.2.2" xref="S3.SS2.13.13.13.m13.11.11.4.4.2.2.cmml">s</mi><mi mathsize="80%" id="S3.SS2.13.13.13.m13.11.11.4.4.2.3" xref="S3.SS2.13.13.13.m13.11.11.4.4.2.3.cmml">x</mi><mrow id="S3.SS2.13.13.13.m13.4.4.1.3" xref="S3.SS2.13.13.13.m13.11.11.4.4.cmml"><mo maxsize="80%" minsize="80%" id="S3.SS2.13.13.13.m13.4.4.1.3.1" xref="S3.SS2.13.13.13.m13.11.11.4.4.cmml">(</mo><mi mathsize="80%" id="S3.SS2.13.13.13.m13.4.4.1.1" xref="S3.SS2.13.13.13.m13.4.4.1.1.cmml">k</mi><mo maxsize="80%" minsize="80%" id="S3.SS2.13.13.13.m13.4.4.1.3.2" xref="S3.SS2.13.13.13.m13.11.11.4.4.cmml">)</mo></mrow></msubsup><mo mathsize="80%" id="S3.SS2.13.13.13.m13.14.14.7.12" xref="S3.SS2.13.13.13.m13.14.14.8.cmml">,</mo><msubsup id="S3.SS2.13.13.13.m13.12.12.5.5" xref="S3.SS2.13.13.13.m13.12.12.5.5.cmml"><mi mathsize="80%" id="S3.SS2.13.13.13.m13.12.12.5.5.2.2" xref="S3.SS2.13.13.13.m13.12.12.5.5.2.2.cmml">s</mi><mi mathsize="80%" id="S3.SS2.13.13.13.m13.12.12.5.5.2.3" xref="S3.SS2.13.13.13.m13.12.12.5.5.2.3.cmml">y</mi><mrow id="S3.SS2.13.13.13.m13.5.5.1.3" xref="S3.SS2.13.13.13.m13.12.12.5.5.cmml"><mo maxsize="80%" minsize="80%" id="S3.SS2.13.13.13.m13.5.5.1.3.1" xref="S3.SS2.13.13.13.m13.12.12.5.5.cmml">(</mo><mi mathsize="80%" id="S3.SS2.13.13.13.m13.5.5.1.1" xref="S3.SS2.13.13.13.m13.5.5.1.1.cmml">k</mi><mo maxsize="80%" minsize="80%" id="S3.SS2.13.13.13.m13.5.5.1.3.2" xref="S3.SS2.13.13.13.m13.12.12.5.5.cmml">)</mo></mrow></msubsup><mo mathsize="80%" id="S3.SS2.13.13.13.m13.14.14.7.13" xref="S3.SS2.13.13.13.m13.14.14.8.cmml">,</mo><msubsup id="S3.SS2.13.13.13.m13.13.13.6.6" xref="S3.SS2.13.13.13.m13.13.13.6.6.cmml"><mi mathsize="80%" id="S3.SS2.13.13.13.m13.13.13.6.6.2.2" xref="S3.SS2.13.13.13.m13.13.13.6.6.2.2.cmml">s</mi><mi mathsize="80%" id="S3.SS2.13.13.13.m13.13.13.6.6.2.3" xref="S3.SS2.13.13.13.m13.13.13.6.6.2.3.cmml">z</mi><mrow id="S3.SS2.13.13.13.m13.6.6.1.3" xref="S3.SS2.13.13.13.m13.13.13.6.6.cmml"><mo maxsize="80%" minsize="80%" id="S3.SS2.13.13.13.m13.6.6.1.3.1" xref="S3.SS2.13.13.13.m13.13.13.6.6.cmml">(</mo><mi mathsize="80%" id="S3.SS2.13.13.13.m13.6.6.1.1" xref="S3.SS2.13.13.13.m13.6.6.1.1.cmml">k</mi><mo maxsize="80%" minsize="80%" id="S3.SS2.13.13.13.m13.6.6.1.3.2" xref="S3.SS2.13.13.13.m13.13.13.6.6.cmml">)</mo></mrow></msubsup><mo mathsize="80%" id="S3.SS2.13.13.13.m13.14.14.7.14" xref="S3.SS2.13.13.13.m13.14.14.8.cmml">,</mo><msup id="S3.SS2.13.13.13.m13.14.14.7.7" xref="S3.SS2.13.13.13.m13.14.14.7.7.cmml"><mi mathsize="80%" id="S3.SS2.13.13.13.m13.14.14.7.7.2" xref="S3.SS2.13.13.13.m13.14.14.7.7.2.cmml">r</mi><mrow id="S3.SS2.13.13.13.m13.7.7.1.3" xref="S3.SS2.13.13.13.m13.14.14.7.7.cmml"><mo maxsize="80%" minsize="80%" id="S3.SS2.13.13.13.m13.7.7.1.3.1" xref="S3.SS2.13.13.13.m13.14.14.7.7.cmml">(</mo><mi mathsize="80%" id="S3.SS2.13.13.13.m13.7.7.1.1" xref="S3.SS2.13.13.13.m13.7.7.1.1.cmml">k</mi><mo maxsize="80%" minsize="80%" id="S3.SS2.13.13.13.m13.7.7.1.3.2" xref="S3.SS2.13.13.13.m13.14.14.7.7.cmml">)</mo></mrow></msup><mo maxsize="80%" minsize="80%" id="S3.SS2.13.13.13.m13.14.14.7.15" xref="S3.SS2.13.13.13.m13.14.14.8.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.13.13.13.m13.14b"><vector id="S3.SS2.13.13.13.m13.14.14.8.cmml" xref="S3.SS2.13.13.13.m13.14.14.7"><apply id="S3.SS2.13.13.13.m13.8.8.1.1.cmml" xref="S3.SS2.13.13.13.m13.8.8.1.1"><csymbol cd="ambiguous" id="S3.SS2.13.13.13.m13.8.8.1.1.1.cmml" xref="S3.SS2.13.13.13.m13.8.8.1.1">superscript</csymbol><apply id="S3.SS2.13.13.13.m13.8.8.1.1.2.cmml" xref="S3.SS2.13.13.13.m13.8.8.1.1"><csymbol cd="ambiguous" id="S3.SS2.13.13.13.m13.8.8.1.1.2.1.cmml" xref="S3.SS2.13.13.13.m13.8.8.1.1">subscript</csymbol><ci id="S3.SS2.13.13.13.m13.8.8.1.1.2.2.cmml" xref="S3.SS2.13.13.13.m13.8.8.1.1.2.2">𝑐</ci><ci id="S3.SS2.13.13.13.m13.8.8.1.1.2.3.cmml" xref="S3.SS2.13.13.13.m13.8.8.1.1.2.3">𝑥</ci></apply><ci id="S3.SS2.13.13.13.m13.1.1.1.1.cmml" xref="S3.SS2.13.13.13.m13.1.1.1.1">𝑘</ci></apply><apply id="S3.SS2.13.13.13.m13.9.9.2.2.cmml" xref="S3.SS2.13.13.13.m13.9.9.2.2"><csymbol cd="ambiguous" id="S3.SS2.13.13.13.m13.9.9.2.2.1.cmml" xref="S3.SS2.13.13.13.m13.9.9.2.2">superscript</csymbol><apply id="S3.SS2.13.13.13.m13.9.9.2.2.2.cmml" xref="S3.SS2.13.13.13.m13.9.9.2.2"><csymbol cd="ambiguous" id="S3.SS2.13.13.13.m13.9.9.2.2.2.1.cmml" xref="S3.SS2.13.13.13.m13.9.9.2.2">subscript</csymbol><ci id="S3.SS2.13.13.13.m13.9.9.2.2.2.2.cmml" xref="S3.SS2.13.13.13.m13.9.9.2.2.2.2">𝑐</ci><ci id="S3.SS2.13.13.13.m13.9.9.2.2.2.3.cmml" xref="S3.SS2.13.13.13.m13.9.9.2.2.2.3">𝑦</ci></apply><ci id="S3.SS2.13.13.13.m13.2.2.1.1.cmml" xref="S3.SS2.13.13.13.m13.2.2.1.1">𝑘</ci></apply><apply id="S3.SS2.13.13.13.m13.10.10.3.3.cmml" xref="S3.SS2.13.13.13.m13.10.10.3.3"><csymbol cd="ambiguous" id="S3.SS2.13.13.13.m13.10.10.3.3.1.cmml" xref="S3.SS2.13.13.13.m13.10.10.3.3">superscript</csymbol><apply id="S3.SS2.13.13.13.m13.10.10.3.3.2.cmml" xref="S3.SS2.13.13.13.m13.10.10.3.3"><csymbol cd="ambiguous" id="S3.SS2.13.13.13.m13.10.10.3.3.2.1.cmml" xref="S3.SS2.13.13.13.m13.10.10.3.3">subscript</csymbol><ci id="S3.SS2.13.13.13.m13.10.10.3.3.2.2.cmml" xref="S3.SS2.13.13.13.m13.10.10.3.3.2.2">𝑐</ci><ci id="S3.SS2.13.13.13.m13.10.10.3.3.2.3.cmml" xref="S3.SS2.13.13.13.m13.10.10.3.3.2.3">𝑧</ci></apply><ci id="S3.SS2.13.13.13.m13.3.3.1.1.cmml" xref="S3.SS2.13.13.13.m13.3.3.1.1">𝑘</ci></apply><apply id="S3.SS2.13.13.13.m13.11.11.4.4.cmml" xref="S3.SS2.13.13.13.m13.11.11.4.4"><csymbol cd="ambiguous" id="S3.SS2.13.13.13.m13.11.11.4.4.1.cmml" xref="S3.SS2.13.13.13.m13.11.11.4.4">superscript</csymbol><apply id="S3.SS2.13.13.13.m13.11.11.4.4.2.cmml" xref="S3.SS2.13.13.13.m13.11.11.4.4"><csymbol cd="ambiguous" id="S3.SS2.13.13.13.m13.11.11.4.4.2.1.cmml" xref="S3.SS2.13.13.13.m13.11.11.4.4">subscript</csymbol><ci id="S3.SS2.13.13.13.m13.11.11.4.4.2.2.cmml" xref="S3.SS2.13.13.13.m13.11.11.4.4.2.2">𝑠</ci><ci id="S3.SS2.13.13.13.m13.11.11.4.4.2.3.cmml" xref="S3.SS2.13.13.13.m13.11.11.4.4.2.3">𝑥</ci></apply><ci id="S3.SS2.13.13.13.m13.4.4.1.1.cmml" xref="S3.SS2.13.13.13.m13.4.4.1.1">𝑘</ci></apply><apply id="S3.SS2.13.13.13.m13.12.12.5.5.cmml" xref="S3.SS2.13.13.13.m13.12.12.5.5"><csymbol cd="ambiguous" id="S3.SS2.13.13.13.m13.12.12.5.5.1.cmml" xref="S3.SS2.13.13.13.m13.12.12.5.5">superscript</csymbol><apply id="S3.SS2.13.13.13.m13.12.12.5.5.2.cmml" xref="S3.SS2.13.13.13.m13.12.12.5.5"><csymbol cd="ambiguous" id="S3.SS2.13.13.13.m13.12.12.5.5.2.1.cmml" xref="S3.SS2.13.13.13.m13.12.12.5.5">subscript</csymbol><ci id="S3.SS2.13.13.13.m13.12.12.5.5.2.2.cmml" xref="S3.SS2.13.13.13.m13.12.12.5.5.2.2">𝑠</ci><ci id="S3.SS2.13.13.13.m13.12.12.5.5.2.3.cmml" xref="S3.SS2.13.13.13.m13.12.12.5.5.2.3">𝑦</ci></apply><ci id="S3.SS2.13.13.13.m13.5.5.1.1.cmml" xref="S3.SS2.13.13.13.m13.5.5.1.1">𝑘</ci></apply><apply id="S3.SS2.13.13.13.m13.13.13.6.6.cmml" xref="S3.SS2.13.13.13.m13.13.13.6.6"><csymbol cd="ambiguous" id="S3.SS2.13.13.13.m13.13.13.6.6.1.cmml" xref="S3.SS2.13.13.13.m13.13.13.6.6">superscript</csymbol><apply id="S3.SS2.13.13.13.m13.13.13.6.6.2.cmml" xref="S3.SS2.13.13.13.m13.13.13.6.6"><csymbol cd="ambiguous" id="S3.SS2.13.13.13.m13.13.13.6.6.2.1.cmml" xref="S3.SS2.13.13.13.m13.13.13.6.6">subscript</csymbol><ci id="S3.SS2.13.13.13.m13.13.13.6.6.2.2.cmml" xref="S3.SS2.13.13.13.m13.13.13.6.6.2.2">𝑠</ci><ci id="S3.SS2.13.13.13.m13.13.13.6.6.2.3.cmml" xref="S3.SS2.13.13.13.m13.13.13.6.6.2.3">𝑧</ci></apply><ci id="S3.SS2.13.13.13.m13.6.6.1.1.cmml" xref="S3.SS2.13.13.13.m13.6.6.1.1">𝑘</ci></apply><apply id="S3.SS2.13.13.13.m13.14.14.7.7.cmml" xref="S3.SS2.13.13.13.m13.14.14.7.7"><csymbol cd="ambiguous" id="S3.SS2.13.13.13.m13.14.14.7.7.1.cmml" xref="S3.SS2.13.13.13.m13.14.14.7.7">superscript</csymbol><ci id="S3.SS2.13.13.13.m13.14.14.7.7.2.cmml" xref="S3.SS2.13.13.13.m13.14.14.7.7.2">𝑟</ci><ci id="S3.SS2.13.13.13.m13.7.7.1.1.cmml" xref="S3.SS2.13.13.13.m13.7.7.1.1">𝑘</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.13.13.13.m13.14c">(c_{x}^{(k)},c_{y}^{(k)},c_{z}^{(k)},s_{x}^{(k)},s_{y}^{(k)},s_{z}^{(k)},r^{(k)})</annotation></semantics></math><span id="S3.SS2.19.19.19.19" class="ltx_text" style="font-size:80%;">, with elements representing the center, size, and orientation. We slightly enlarge the size </span><math id="S3.SS2.14.14.14.m14.1" class="ltx_Math" alttext="\sqrt{\frac{1}{\beta}}" display="inline"><semantics id="S3.SS2.14.14.14.m14.1a"><msqrt id="S3.SS2.14.14.14.m14.1.1" xref="S3.SS2.14.14.14.m14.1.1.cmml"><mfrac id="S3.SS2.14.14.14.m14.1.1.2" xref="S3.SS2.14.14.14.m14.1.1.2.cmml"><mn mathsize="80%" id="S3.SS2.14.14.14.m14.1.1.2.2" xref="S3.SS2.14.14.14.m14.1.1.2.2.cmml">1</mn><mi mathsize="80%" id="S3.SS2.14.14.14.m14.1.1.2.3" xref="S3.SS2.14.14.14.m14.1.1.2.3.cmml">β</mi></mfrac></msqrt><annotation-xml encoding="MathML-Content" id="S3.SS2.14.14.14.m14.1b"><apply id="S3.SS2.14.14.14.m14.1.1.cmml" xref="S3.SS2.14.14.14.m14.1.1"><root id="S3.SS2.14.14.14.m14.1.1a.cmml" xref="S3.SS2.14.14.14.m14.1.1"></root><apply id="S3.SS2.14.14.14.m14.1.1.2.cmml" xref="S3.SS2.14.14.14.m14.1.1.2"><divide id="S3.SS2.14.14.14.m14.1.1.2.1.cmml" xref="S3.SS2.14.14.14.m14.1.1.2"></divide><cn type="integer" id="S3.SS2.14.14.14.m14.1.1.2.2.cmml" xref="S3.SS2.14.14.14.m14.1.1.2.2">1</cn><ci id="S3.SS2.14.14.14.m14.1.1.2.3.cmml" xref="S3.SS2.14.14.14.m14.1.1.2.3">𝛽</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.14.14.14.m14.1c">\sqrt{\frac{1}{\beta}}</annotation></semantics></math><span id="S3.SS2.19.19.19.20" class="ltx_text" style="font-size:80%;"> times and generate a new box with parameters </span><math id="S3.SS2.15.15.15.m15.14" class="ltx_Math" alttext="(c_{x}^{(k)},c_{y}^{(k)},c_{z}^{(k)},\sqrt{\frac{1}{\beta}}s_{x}^{(k)},\sqrt{\frac{1}{\beta}}s_{y}^{(k)},s_{z}^{(k)},r^{(k)})" display="inline"><semantics id="S3.SS2.15.15.15.m15.14a"><mrow id="S3.SS2.15.15.15.m15.14.14.7" xref="S3.SS2.15.15.15.m15.14.14.8.cmml"><mo maxsize="80%" minsize="80%" id="S3.SS2.15.15.15.m15.14.14.7.8" xref="S3.SS2.15.15.15.m15.14.14.8.cmml">(</mo><msubsup id="S3.SS2.15.15.15.m15.8.8.1.1" xref="S3.SS2.15.15.15.m15.8.8.1.1.cmml"><mi mathsize="80%" id="S3.SS2.15.15.15.m15.8.8.1.1.2.2" xref="S3.SS2.15.15.15.m15.8.8.1.1.2.2.cmml">c</mi><mi mathsize="80%" id="S3.SS2.15.15.15.m15.8.8.1.1.2.3" xref="S3.SS2.15.15.15.m15.8.8.1.1.2.3.cmml">x</mi><mrow id="S3.SS2.15.15.15.m15.1.1.1.3" xref="S3.SS2.15.15.15.m15.8.8.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.SS2.15.15.15.m15.1.1.1.3.1" xref="S3.SS2.15.15.15.m15.8.8.1.1.cmml">(</mo><mi mathsize="80%" id="S3.SS2.15.15.15.m15.1.1.1.1" xref="S3.SS2.15.15.15.m15.1.1.1.1.cmml">k</mi><mo maxsize="80%" minsize="80%" id="S3.SS2.15.15.15.m15.1.1.1.3.2" xref="S3.SS2.15.15.15.m15.8.8.1.1.cmml">)</mo></mrow></msubsup><mo mathsize="80%" id="S3.SS2.15.15.15.m15.14.14.7.9" xref="S3.SS2.15.15.15.m15.14.14.8.cmml">,</mo><msubsup id="S3.SS2.15.15.15.m15.9.9.2.2" xref="S3.SS2.15.15.15.m15.9.9.2.2.cmml"><mi mathsize="80%" id="S3.SS2.15.15.15.m15.9.9.2.2.2.2" xref="S3.SS2.15.15.15.m15.9.9.2.2.2.2.cmml">c</mi><mi mathsize="80%" id="S3.SS2.15.15.15.m15.9.9.2.2.2.3" xref="S3.SS2.15.15.15.m15.9.9.2.2.2.3.cmml">y</mi><mrow id="S3.SS2.15.15.15.m15.2.2.1.3" xref="S3.SS2.15.15.15.m15.9.9.2.2.cmml"><mo maxsize="80%" minsize="80%" id="S3.SS2.15.15.15.m15.2.2.1.3.1" xref="S3.SS2.15.15.15.m15.9.9.2.2.cmml">(</mo><mi mathsize="80%" id="S3.SS2.15.15.15.m15.2.2.1.1" xref="S3.SS2.15.15.15.m15.2.2.1.1.cmml">k</mi><mo maxsize="80%" minsize="80%" id="S3.SS2.15.15.15.m15.2.2.1.3.2" xref="S3.SS2.15.15.15.m15.9.9.2.2.cmml">)</mo></mrow></msubsup><mo mathsize="80%" id="S3.SS2.15.15.15.m15.14.14.7.10" xref="S3.SS2.15.15.15.m15.14.14.8.cmml">,</mo><msubsup id="S3.SS2.15.15.15.m15.10.10.3.3" xref="S3.SS2.15.15.15.m15.10.10.3.3.cmml"><mi mathsize="80%" id="S3.SS2.15.15.15.m15.10.10.3.3.2.2" xref="S3.SS2.15.15.15.m15.10.10.3.3.2.2.cmml">c</mi><mi mathsize="80%" id="S3.SS2.15.15.15.m15.10.10.3.3.2.3" xref="S3.SS2.15.15.15.m15.10.10.3.3.2.3.cmml">z</mi><mrow id="S3.SS2.15.15.15.m15.3.3.1.3" xref="S3.SS2.15.15.15.m15.10.10.3.3.cmml"><mo maxsize="80%" minsize="80%" id="S3.SS2.15.15.15.m15.3.3.1.3.1" xref="S3.SS2.15.15.15.m15.10.10.3.3.cmml">(</mo><mi mathsize="80%" id="S3.SS2.15.15.15.m15.3.3.1.1" xref="S3.SS2.15.15.15.m15.3.3.1.1.cmml">k</mi><mo maxsize="80%" minsize="80%" id="S3.SS2.15.15.15.m15.3.3.1.3.2" xref="S3.SS2.15.15.15.m15.10.10.3.3.cmml">)</mo></mrow></msubsup><mo mathsize="80%" id="S3.SS2.15.15.15.m15.14.14.7.11" xref="S3.SS2.15.15.15.m15.14.14.8.cmml">,</mo><mrow id="S3.SS2.15.15.15.m15.11.11.4.4" xref="S3.SS2.15.15.15.m15.11.11.4.4.cmml"><msqrt id="S3.SS2.15.15.15.m15.11.11.4.4.2" xref="S3.SS2.15.15.15.m15.11.11.4.4.2.cmml"><mfrac id="S3.SS2.15.15.15.m15.11.11.4.4.2.2" xref="S3.SS2.15.15.15.m15.11.11.4.4.2.2.cmml"><mn mathsize="80%" id="S3.SS2.15.15.15.m15.11.11.4.4.2.2.2" xref="S3.SS2.15.15.15.m15.11.11.4.4.2.2.2.cmml">1</mn><mi mathsize="80%" id="S3.SS2.15.15.15.m15.11.11.4.4.2.2.3" xref="S3.SS2.15.15.15.m15.11.11.4.4.2.2.3.cmml">β</mi></mfrac></msqrt><mo lspace="0em" rspace="0em" id="S3.SS2.15.15.15.m15.11.11.4.4.1" xref="S3.SS2.15.15.15.m15.11.11.4.4.1.cmml">​</mo><msubsup id="S3.SS2.15.15.15.m15.11.11.4.4.3" xref="S3.SS2.15.15.15.m15.11.11.4.4.3.cmml"><mi mathsize="80%" id="S3.SS2.15.15.15.m15.11.11.4.4.3.2.2" xref="S3.SS2.15.15.15.m15.11.11.4.4.3.2.2.cmml">s</mi><mi mathsize="80%" id="S3.SS2.15.15.15.m15.11.11.4.4.3.2.3" xref="S3.SS2.15.15.15.m15.11.11.4.4.3.2.3.cmml">x</mi><mrow id="S3.SS2.15.15.15.m15.4.4.1.3" xref="S3.SS2.15.15.15.m15.11.11.4.4.3.cmml"><mo maxsize="80%" minsize="80%" id="S3.SS2.15.15.15.m15.4.4.1.3.1" xref="S3.SS2.15.15.15.m15.11.11.4.4.3.cmml">(</mo><mi mathsize="80%" id="S3.SS2.15.15.15.m15.4.4.1.1" xref="S3.SS2.15.15.15.m15.4.4.1.1.cmml">k</mi><mo maxsize="80%" minsize="80%" id="S3.SS2.15.15.15.m15.4.4.1.3.2" xref="S3.SS2.15.15.15.m15.11.11.4.4.3.cmml">)</mo></mrow></msubsup></mrow><mo mathsize="80%" id="S3.SS2.15.15.15.m15.14.14.7.12" xref="S3.SS2.15.15.15.m15.14.14.8.cmml">,</mo><mrow id="S3.SS2.15.15.15.m15.12.12.5.5" xref="S3.SS2.15.15.15.m15.12.12.5.5.cmml"><msqrt id="S3.SS2.15.15.15.m15.12.12.5.5.2" xref="S3.SS2.15.15.15.m15.12.12.5.5.2.cmml"><mfrac id="S3.SS2.15.15.15.m15.12.12.5.5.2.2" xref="S3.SS2.15.15.15.m15.12.12.5.5.2.2.cmml"><mn mathsize="80%" id="S3.SS2.15.15.15.m15.12.12.5.5.2.2.2" xref="S3.SS2.15.15.15.m15.12.12.5.5.2.2.2.cmml">1</mn><mi mathsize="80%" id="S3.SS2.15.15.15.m15.12.12.5.5.2.2.3" xref="S3.SS2.15.15.15.m15.12.12.5.5.2.2.3.cmml">β</mi></mfrac></msqrt><mo lspace="0em" rspace="0em" id="S3.SS2.15.15.15.m15.12.12.5.5.1" xref="S3.SS2.15.15.15.m15.12.12.5.5.1.cmml">​</mo><msubsup id="S3.SS2.15.15.15.m15.12.12.5.5.3" xref="S3.SS2.15.15.15.m15.12.12.5.5.3.cmml"><mi mathsize="80%" id="S3.SS2.15.15.15.m15.12.12.5.5.3.2.2" xref="S3.SS2.15.15.15.m15.12.12.5.5.3.2.2.cmml">s</mi><mi mathsize="80%" id="S3.SS2.15.15.15.m15.12.12.5.5.3.2.3" xref="S3.SS2.15.15.15.m15.12.12.5.5.3.2.3.cmml">y</mi><mrow id="S3.SS2.15.15.15.m15.5.5.1.3" xref="S3.SS2.15.15.15.m15.12.12.5.5.3.cmml"><mo maxsize="80%" minsize="80%" id="S3.SS2.15.15.15.m15.5.5.1.3.1" xref="S3.SS2.15.15.15.m15.12.12.5.5.3.cmml">(</mo><mi mathsize="80%" id="S3.SS2.15.15.15.m15.5.5.1.1" xref="S3.SS2.15.15.15.m15.5.5.1.1.cmml">k</mi><mo maxsize="80%" minsize="80%" id="S3.SS2.15.15.15.m15.5.5.1.3.2" xref="S3.SS2.15.15.15.m15.12.12.5.5.3.cmml">)</mo></mrow></msubsup></mrow><mo mathsize="80%" id="S3.SS2.15.15.15.m15.14.14.7.13" xref="S3.SS2.15.15.15.m15.14.14.8.cmml">,</mo><msubsup id="S3.SS2.15.15.15.m15.13.13.6.6" xref="S3.SS2.15.15.15.m15.13.13.6.6.cmml"><mi mathsize="80%" id="S3.SS2.15.15.15.m15.13.13.6.6.2.2" xref="S3.SS2.15.15.15.m15.13.13.6.6.2.2.cmml">s</mi><mi mathsize="80%" id="S3.SS2.15.15.15.m15.13.13.6.6.2.3" xref="S3.SS2.15.15.15.m15.13.13.6.6.2.3.cmml">z</mi><mrow id="S3.SS2.15.15.15.m15.6.6.1.3" xref="S3.SS2.15.15.15.m15.13.13.6.6.cmml"><mo maxsize="80%" minsize="80%" id="S3.SS2.15.15.15.m15.6.6.1.3.1" xref="S3.SS2.15.15.15.m15.13.13.6.6.cmml">(</mo><mi mathsize="80%" id="S3.SS2.15.15.15.m15.6.6.1.1" xref="S3.SS2.15.15.15.m15.6.6.1.1.cmml">k</mi><mo maxsize="80%" minsize="80%" id="S3.SS2.15.15.15.m15.6.6.1.3.2" xref="S3.SS2.15.15.15.m15.13.13.6.6.cmml">)</mo></mrow></msubsup><mo mathsize="80%" id="S3.SS2.15.15.15.m15.14.14.7.14" xref="S3.SS2.15.15.15.m15.14.14.8.cmml">,</mo><msup id="S3.SS2.15.15.15.m15.14.14.7.7" xref="S3.SS2.15.15.15.m15.14.14.7.7.cmml"><mi mathsize="80%" id="S3.SS2.15.15.15.m15.14.14.7.7.2" xref="S3.SS2.15.15.15.m15.14.14.7.7.2.cmml">r</mi><mrow id="S3.SS2.15.15.15.m15.7.7.1.3" xref="S3.SS2.15.15.15.m15.14.14.7.7.cmml"><mo maxsize="80%" minsize="80%" id="S3.SS2.15.15.15.m15.7.7.1.3.1" xref="S3.SS2.15.15.15.m15.14.14.7.7.cmml">(</mo><mi mathsize="80%" id="S3.SS2.15.15.15.m15.7.7.1.1" xref="S3.SS2.15.15.15.m15.7.7.1.1.cmml">k</mi><mo maxsize="80%" minsize="80%" id="S3.SS2.15.15.15.m15.7.7.1.3.2" xref="S3.SS2.15.15.15.m15.14.14.7.7.cmml">)</mo></mrow></msup><mo maxsize="80%" minsize="80%" id="S3.SS2.15.15.15.m15.14.14.7.15" xref="S3.SS2.15.15.15.m15.14.14.8.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.15.15.15.m15.14b"><vector id="S3.SS2.15.15.15.m15.14.14.8.cmml" xref="S3.SS2.15.15.15.m15.14.14.7"><apply id="S3.SS2.15.15.15.m15.8.8.1.1.cmml" xref="S3.SS2.15.15.15.m15.8.8.1.1"><csymbol cd="ambiguous" id="S3.SS2.15.15.15.m15.8.8.1.1.1.cmml" xref="S3.SS2.15.15.15.m15.8.8.1.1">superscript</csymbol><apply id="S3.SS2.15.15.15.m15.8.8.1.1.2.cmml" xref="S3.SS2.15.15.15.m15.8.8.1.1"><csymbol cd="ambiguous" id="S3.SS2.15.15.15.m15.8.8.1.1.2.1.cmml" xref="S3.SS2.15.15.15.m15.8.8.1.1">subscript</csymbol><ci id="S3.SS2.15.15.15.m15.8.8.1.1.2.2.cmml" xref="S3.SS2.15.15.15.m15.8.8.1.1.2.2">𝑐</ci><ci id="S3.SS2.15.15.15.m15.8.8.1.1.2.3.cmml" xref="S3.SS2.15.15.15.m15.8.8.1.1.2.3">𝑥</ci></apply><ci id="S3.SS2.15.15.15.m15.1.1.1.1.cmml" xref="S3.SS2.15.15.15.m15.1.1.1.1">𝑘</ci></apply><apply id="S3.SS2.15.15.15.m15.9.9.2.2.cmml" xref="S3.SS2.15.15.15.m15.9.9.2.2"><csymbol cd="ambiguous" id="S3.SS2.15.15.15.m15.9.9.2.2.1.cmml" xref="S3.SS2.15.15.15.m15.9.9.2.2">superscript</csymbol><apply id="S3.SS2.15.15.15.m15.9.9.2.2.2.cmml" xref="S3.SS2.15.15.15.m15.9.9.2.2"><csymbol cd="ambiguous" id="S3.SS2.15.15.15.m15.9.9.2.2.2.1.cmml" xref="S3.SS2.15.15.15.m15.9.9.2.2">subscript</csymbol><ci id="S3.SS2.15.15.15.m15.9.9.2.2.2.2.cmml" xref="S3.SS2.15.15.15.m15.9.9.2.2.2.2">𝑐</ci><ci id="S3.SS2.15.15.15.m15.9.9.2.2.2.3.cmml" xref="S3.SS2.15.15.15.m15.9.9.2.2.2.3">𝑦</ci></apply><ci id="S3.SS2.15.15.15.m15.2.2.1.1.cmml" xref="S3.SS2.15.15.15.m15.2.2.1.1">𝑘</ci></apply><apply id="S3.SS2.15.15.15.m15.10.10.3.3.cmml" xref="S3.SS2.15.15.15.m15.10.10.3.3"><csymbol cd="ambiguous" id="S3.SS2.15.15.15.m15.10.10.3.3.1.cmml" xref="S3.SS2.15.15.15.m15.10.10.3.3">superscript</csymbol><apply id="S3.SS2.15.15.15.m15.10.10.3.3.2.cmml" xref="S3.SS2.15.15.15.m15.10.10.3.3"><csymbol cd="ambiguous" id="S3.SS2.15.15.15.m15.10.10.3.3.2.1.cmml" xref="S3.SS2.15.15.15.m15.10.10.3.3">subscript</csymbol><ci id="S3.SS2.15.15.15.m15.10.10.3.3.2.2.cmml" xref="S3.SS2.15.15.15.m15.10.10.3.3.2.2">𝑐</ci><ci id="S3.SS2.15.15.15.m15.10.10.3.3.2.3.cmml" xref="S3.SS2.15.15.15.m15.10.10.3.3.2.3">𝑧</ci></apply><ci id="S3.SS2.15.15.15.m15.3.3.1.1.cmml" xref="S3.SS2.15.15.15.m15.3.3.1.1">𝑘</ci></apply><apply id="S3.SS2.15.15.15.m15.11.11.4.4.cmml" xref="S3.SS2.15.15.15.m15.11.11.4.4"><times id="S3.SS2.15.15.15.m15.11.11.4.4.1.cmml" xref="S3.SS2.15.15.15.m15.11.11.4.4.1"></times><apply id="S3.SS2.15.15.15.m15.11.11.4.4.2.cmml" xref="S3.SS2.15.15.15.m15.11.11.4.4.2"><root id="S3.SS2.15.15.15.m15.11.11.4.4.2a.cmml" xref="S3.SS2.15.15.15.m15.11.11.4.4.2"></root><apply id="S3.SS2.15.15.15.m15.11.11.4.4.2.2.cmml" xref="S3.SS2.15.15.15.m15.11.11.4.4.2.2"><divide id="S3.SS2.15.15.15.m15.11.11.4.4.2.2.1.cmml" xref="S3.SS2.15.15.15.m15.11.11.4.4.2.2"></divide><cn type="integer" id="S3.SS2.15.15.15.m15.11.11.4.4.2.2.2.cmml" xref="S3.SS2.15.15.15.m15.11.11.4.4.2.2.2">1</cn><ci id="S3.SS2.15.15.15.m15.11.11.4.4.2.2.3.cmml" xref="S3.SS2.15.15.15.m15.11.11.4.4.2.2.3">𝛽</ci></apply></apply><apply id="S3.SS2.15.15.15.m15.11.11.4.4.3.cmml" xref="S3.SS2.15.15.15.m15.11.11.4.4.3"><csymbol cd="ambiguous" id="S3.SS2.15.15.15.m15.11.11.4.4.3.1.cmml" xref="S3.SS2.15.15.15.m15.11.11.4.4.3">superscript</csymbol><apply id="S3.SS2.15.15.15.m15.11.11.4.4.3.2.cmml" xref="S3.SS2.15.15.15.m15.11.11.4.4.3"><csymbol cd="ambiguous" id="S3.SS2.15.15.15.m15.11.11.4.4.3.2.1.cmml" xref="S3.SS2.15.15.15.m15.11.11.4.4.3">subscript</csymbol><ci id="S3.SS2.15.15.15.m15.11.11.4.4.3.2.2.cmml" xref="S3.SS2.15.15.15.m15.11.11.4.4.3.2.2">𝑠</ci><ci id="S3.SS2.15.15.15.m15.11.11.4.4.3.2.3.cmml" xref="S3.SS2.15.15.15.m15.11.11.4.4.3.2.3">𝑥</ci></apply><ci id="S3.SS2.15.15.15.m15.4.4.1.1.cmml" xref="S3.SS2.15.15.15.m15.4.4.1.1">𝑘</ci></apply></apply><apply id="S3.SS2.15.15.15.m15.12.12.5.5.cmml" xref="S3.SS2.15.15.15.m15.12.12.5.5"><times id="S3.SS2.15.15.15.m15.12.12.5.5.1.cmml" xref="S3.SS2.15.15.15.m15.12.12.5.5.1"></times><apply id="S3.SS2.15.15.15.m15.12.12.5.5.2.cmml" xref="S3.SS2.15.15.15.m15.12.12.5.5.2"><root id="S3.SS2.15.15.15.m15.12.12.5.5.2a.cmml" xref="S3.SS2.15.15.15.m15.12.12.5.5.2"></root><apply id="S3.SS2.15.15.15.m15.12.12.5.5.2.2.cmml" xref="S3.SS2.15.15.15.m15.12.12.5.5.2.2"><divide id="S3.SS2.15.15.15.m15.12.12.5.5.2.2.1.cmml" xref="S3.SS2.15.15.15.m15.12.12.5.5.2.2"></divide><cn type="integer" id="S3.SS2.15.15.15.m15.12.12.5.5.2.2.2.cmml" xref="S3.SS2.15.15.15.m15.12.12.5.5.2.2.2">1</cn><ci id="S3.SS2.15.15.15.m15.12.12.5.5.2.2.3.cmml" xref="S3.SS2.15.15.15.m15.12.12.5.5.2.2.3">𝛽</ci></apply></apply><apply id="S3.SS2.15.15.15.m15.12.12.5.5.3.cmml" xref="S3.SS2.15.15.15.m15.12.12.5.5.3"><csymbol cd="ambiguous" id="S3.SS2.15.15.15.m15.12.12.5.5.3.1.cmml" xref="S3.SS2.15.15.15.m15.12.12.5.5.3">superscript</csymbol><apply id="S3.SS2.15.15.15.m15.12.12.5.5.3.2.cmml" xref="S3.SS2.15.15.15.m15.12.12.5.5.3"><csymbol cd="ambiguous" id="S3.SS2.15.15.15.m15.12.12.5.5.3.2.1.cmml" xref="S3.SS2.15.15.15.m15.12.12.5.5.3">subscript</csymbol><ci id="S3.SS2.15.15.15.m15.12.12.5.5.3.2.2.cmml" xref="S3.SS2.15.15.15.m15.12.12.5.5.3.2.2">𝑠</ci><ci id="S3.SS2.15.15.15.m15.12.12.5.5.3.2.3.cmml" xref="S3.SS2.15.15.15.m15.12.12.5.5.3.2.3">𝑦</ci></apply><ci id="S3.SS2.15.15.15.m15.5.5.1.1.cmml" xref="S3.SS2.15.15.15.m15.5.5.1.1">𝑘</ci></apply></apply><apply id="S3.SS2.15.15.15.m15.13.13.6.6.cmml" xref="S3.SS2.15.15.15.m15.13.13.6.6"><csymbol cd="ambiguous" id="S3.SS2.15.15.15.m15.13.13.6.6.1.cmml" xref="S3.SS2.15.15.15.m15.13.13.6.6">superscript</csymbol><apply id="S3.SS2.15.15.15.m15.13.13.6.6.2.cmml" xref="S3.SS2.15.15.15.m15.13.13.6.6"><csymbol cd="ambiguous" id="S3.SS2.15.15.15.m15.13.13.6.6.2.1.cmml" xref="S3.SS2.15.15.15.m15.13.13.6.6">subscript</csymbol><ci id="S3.SS2.15.15.15.m15.13.13.6.6.2.2.cmml" xref="S3.SS2.15.15.15.m15.13.13.6.6.2.2">𝑠</ci><ci id="S3.SS2.15.15.15.m15.13.13.6.6.2.3.cmml" xref="S3.SS2.15.15.15.m15.13.13.6.6.2.3">𝑧</ci></apply><ci id="S3.SS2.15.15.15.m15.6.6.1.1.cmml" xref="S3.SS2.15.15.15.m15.6.6.1.1">𝑘</ci></apply><apply id="S3.SS2.15.15.15.m15.14.14.7.7.cmml" xref="S3.SS2.15.15.15.m15.14.14.7.7"><csymbol cd="ambiguous" id="S3.SS2.15.15.15.m15.14.14.7.7.1.cmml" xref="S3.SS2.15.15.15.m15.14.14.7.7">superscript</csymbol><ci id="S3.SS2.15.15.15.m15.14.14.7.7.2.cmml" xref="S3.SS2.15.15.15.m15.14.14.7.7.2">𝑟</ci><ci id="S3.SS2.15.15.15.m15.7.7.1.1.cmml" xref="S3.SS2.15.15.15.m15.7.7.1.1">𝑘</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.15.15.15.m15.14c">(c_{x}^{(k)},c_{y}^{(k)},c_{z}^{(k)},\sqrt{\frac{1}{\beta}}s_{x}^{(k)},\sqrt{\frac{1}{\beta}}s_{y}^{(k)},s_{z}^{(k)},r^{(k)})</annotation></semantics></math><span id="S3.SS2.19.19.19.21" class="ltx_text" style="font-size:80%;">. We then scale the previously sampled points in the unit cube according to these new box parameters. An important property of this sampling method is that all foreground objects contain the same number of point samples, regardless of their respective sizes. Among all the important points, there are at least </span><math id="S3.SS2.16.16.16.m16.1" class="ltx_Math" alttext="(1-\alpha)\beta" display="inline"><semantics id="S3.SS2.16.16.16.m16.1a"><mrow id="S3.SS2.16.16.16.m16.1.1" xref="S3.SS2.16.16.16.m16.1.1.cmml"><mrow id="S3.SS2.16.16.16.m16.1.1.1.1" xref="S3.SS2.16.16.16.m16.1.1.1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.SS2.16.16.16.m16.1.1.1.1.2" xref="S3.SS2.16.16.16.m16.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.16.16.16.m16.1.1.1.1.1" xref="S3.SS2.16.16.16.m16.1.1.1.1.1.cmml"><mn mathsize="80%" id="S3.SS2.16.16.16.m16.1.1.1.1.1.2" xref="S3.SS2.16.16.16.m16.1.1.1.1.1.2.cmml">1</mn><mo mathsize="80%" id="S3.SS2.16.16.16.m16.1.1.1.1.1.1" xref="S3.SS2.16.16.16.m16.1.1.1.1.1.1.cmml">−</mo><mi mathsize="80%" id="S3.SS2.16.16.16.m16.1.1.1.1.1.3" xref="S3.SS2.16.16.16.m16.1.1.1.1.1.3.cmml">α</mi></mrow><mo maxsize="80%" minsize="80%" id="S3.SS2.16.16.16.m16.1.1.1.1.3" xref="S3.SS2.16.16.16.m16.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.SS2.16.16.16.m16.1.1.2" xref="S3.SS2.16.16.16.m16.1.1.2.cmml">​</mo><mi mathsize="80%" id="S3.SS2.16.16.16.m16.1.1.3" xref="S3.SS2.16.16.16.m16.1.1.3.cmml">β</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.16.16.16.m16.1b"><apply id="S3.SS2.16.16.16.m16.1.1.cmml" xref="S3.SS2.16.16.16.m16.1.1"><times id="S3.SS2.16.16.16.m16.1.1.2.cmml" xref="S3.SS2.16.16.16.m16.1.1.2"></times><apply id="S3.SS2.16.16.16.m16.1.1.1.1.1.cmml" xref="S3.SS2.16.16.16.m16.1.1.1.1"><minus id="S3.SS2.16.16.16.m16.1.1.1.1.1.1.cmml" xref="S3.SS2.16.16.16.m16.1.1.1.1.1.1"></minus><cn type="integer" id="S3.SS2.16.16.16.m16.1.1.1.1.1.2.cmml" xref="S3.SS2.16.16.16.m16.1.1.1.1.1.2">1</cn><ci id="S3.SS2.16.16.16.m16.1.1.1.1.1.3.cmml" xref="S3.SS2.16.16.16.m16.1.1.1.1.1.3">𝛼</ci></apply><ci id="S3.SS2.16.16.16.m16.1.1.3.cmml" xref="S3.SS2.16.16.16.m16.1.1.3">𝛽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.16.16.16.m16.1c">(1-\alpha)\beta</annotation></semantics></math><span id="S3.SS2.19.19.19.22" class="ltx_text" style="font-size:80%;"> samples inside the bounding boxes. Our experiments set </span><math id="S3.SS2.17.17.17.m17.1" class="ltx_Math" alttext="\alpha=\frac{1}{3}" display="inline"><semantics id="S3.SS2.17.17.17.m17.1a"><mrow id="S3.SS2.17.17.17.m17.1.1" xref="S3.SS2.17.17.17.m17.1.1.cmml"><mi mathsize="80%" id="S3.SS2.17.17.17.m17.1.1.2" xref="S3.SS2.17.17.17.m17.1.1.2.cmml">α</mi><mo mathsize="80%" id="S3.SS2.17.17.17.m17.1.1.1" xref="S3.SS2.17.17.17.m17.1.1.1.cmml">=</mo><mfrac id="S3.SS2.17.17.17.m17.1.1.3" xref="S3.SS2.17.17.17.m17.1.1.3.cmml"><mn mathsize="80%" id="S3.SS2.17.17.17.m17.1.1.3.2" xref="S3.SS2.17.17.17.m17.1.1.3.2.cmml">1</mn><mn mathsize="80%" id="S3.SS2.17.17.17.m17.1.1.3.3" xref="S3.SS2.17.17.17.m17.1.1.3.3.cmml">3</mn></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.17.17.17.m17.1b"><apply id="S3.SS2.17.17.17.m17.1.1.cmml" xref="S3.SS2.17.17.17.m17.1.1"><eq id="S3.SS2.17.17.17.m17.1.1.1.cmml" xref="S3.SS2.17.17.17.m17.1.1.1"></eq><ci id="S3.SS2.17.17.17.m17.1.1.2.cmml" xref="S3.SS2.17.17.17.m17.1.1.2">𝛼</ci><apply id="S3.SS2.17.17.17.m17.1.1.3.cmml" xref="S3.SS2.17.17.17.m17.1.1.3"><divide id="S3.SS2.17.17.17.m17.1.1.3.1.cmml" xref="S3.SS2.17.17.17.m17.1.1.3"></divide><cn type="integer" id="S3.SS2.17.17.17.m17.1.1.3.2.cmml" xref="S3.SS2.17.17.17.m17.1.1.3.2">1</cn><cn type="integer" id="S3.SS2.17.17.17.m17.1.1.3.3.cmml" xref="S3.SS2.17.17.17.m17.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.17.17.17.m17.1c">\alpha=\frac{1}{3}</annotation></semantics></math><span id="S3.SS2.19.19.19.23" class="ltx_text" style="font-size:80%;"> and </span><math id="S3.SS2.18.18.18.m18.1" class="ltx_Math" alttext="\beta=\frac{2}{3}" display="inline"><semantics id="S3.SS2.18.18.18.m18.1a"><mrow id="S3.SS2.18.18.18.m18.1.1" xref="S3.SS2.18.18.18.m18.1.1.cmml"><mi mathsize="80%" id="S3.SS2.18.18.18.m18.1.1.2" xref="S3.SS2.18.18.18.m18.1.1.2.cmml">β</mi><mo mathsize="80%" id="S3.SS2.18.18.18.m18.1.1.1" xref="S3.SS2.18.18.18.m18.1.1.1.cmml">=</mo><mfrac id="S3.SS2.18.18.18.m18.1.1.3" xref="S3.SS2.18.18.18.m18.1.1.3.cmml"><mn mathsize="80%" id="S3.SS2.18.18.18.m18.1.1.3.2" xref="S3.SS2.18.18.18.m18.1.1.3.2.cmml">2</mn><mn mathsize="80%" id="S3.SS2.18.18.18.m18.1.1.3.3" xref="S3.SS2.18.18.18.m18.1.1.3.3.cmml">3</mn></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.18.18.18.m18.1b"><apply id="S3.SS2.18.18.18.m18.1.1.cmml" xref="S3.SS2.18.18.18.m18.1.1"><eq id="S3.SS2.18.18.18.m18.1.1.1.cmml" xref="S3.SS2.18.18.18.m18.1.1.1"></eq><ci id="S3.SS2.18.18.18.m18.1.1.2.cmml" xref="S3.SS2.18.18.18.m18.1.1.2">𝛽</ci><apply id="S3.SS2.18.18.18.m18.1.1.3.cmml" xref="S3.SS2.18.18.18.m18.1.1.3"><divide id="S3.SS2.18.18.18.m18.1.1.3.1.cmml" xref="S3.SS2.18.18.18.m18.1.1.3"></divide><cn type="integer" id="S3.SS2.18.18.18.m18.1.1.3.2.cmml" xref="S3.SS2.18.18.18.m18.1.1.3.2">2</cn><cn type="integer" id="S3.SS2.18.18.18.m18.1.1.3.3.cmml" xref="S3.SS2.18.18.18.m18.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.18.18.18.m18.1c">\beta=\frac{2}{3}</annotation></semantics></math><span id="S3.SS2.19.19.19.24" class="ltx_text" style="font-size:80%;">. Uniform sampling (controlled by </span><math id="S3.SS2.19.19.19.m19.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS2.19.19.19.m19.1a"><mi mathsize="80%" id="S3.SS2.19.19.19.m19.1.1" xref="S3.SS2.19.19.19.m19.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.19.19.19.m19.1b"><ci id="S3.SS2.19.19.19.m19.1.1.cmml" xref="S3.SS2.19.19.19.m19.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.19.19.19.m19.1c">\alpha</annotation></semantics></math><span id="S3.SS2.19.19.19.25" class="ltx_text" style="font-size:80%;">) in the whole scene also produces a few points inside the bounding boxes. The percentage of foreground and background point samples are both close to 50%, leading to a nice balance between contextual and conceptual information. Notably, we point out that the sampling process discussed in this paragraph only occurs during the training phase as an intermediate step for loss computation.</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.SS2.23.23.23" class="ltx_p ltx_figure_panel"><span id="S3.SS2.23.23.23.1" class="ltx_text" style="font-size:80%;">The detection head expects the BEV features to be represented on a rectangular grid and not by an implicit function. During both the training and testing stages, we query all locations with integer coordinates on a </span><math id="S3.SS2.20.20.20.m1.1" class="ltx_Math" alttext="h\times w" display="inline"><semantics id="S3.SS2.20.20.20.m1.1a"><mrow id="S3.SS2.20.20.20.m1.1.1" xref="S3.SS2.20.20.20.m1.1.1.cmml"><mi mathsize="80%" id="S3.SS2.20.20.20.m1.1.1.2" xref="S3.SS2.20.20.20.m1.1.1.2.cmml">h</mi><mo lspace="0.222em" mathsize="80%" rspace="0.222em" id="S3.SS2.20.20.20.m1.1.1.1" xref="S3.SS2.20.20.20.m1.1.1.1.cmml">×</mo><mi mathsize="80%" id="S3.SS2.20.20.20.m1.1.1.3" xref="S3.SS2.20.20.20.m1.1.1.3.cmml">w</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.20.20.20.m1.1b"><apply id="S3.SS2.20.20.20.m1.1.1.cmml" xref="S3.SS2.20.20.20.m1.1.1"><times id="S3.SS2.20.20.20.m1.1.1.1.cmml" xref="S3.SS2.20.20.20.m1.1.1.1"></times><ci id="S3.SS2.20.20.20.m1.1.1.2.cmml" xref="S3.SS2.20.20.20.m1.1.1.2">ℎ</ci><ci id="S3.SS2.20.20.20.m1.1.1.3.cmml" xref="S3.SS2.20.20.20.m1.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.20.20.20.m1.1c">h\times w</annotation></semantics></math><span id="S3.SS2.23.23.23.2" class="ltx_text" style="font-size:80%;"> grid to obtain an explicit 2D probability distribution map </span><math id="S3.SS2.21.21.21.m2.1" class="ltx_Math" alttext="S_{\text{imp}}\in\mathbb{R}^{h\times w}" display="inline"><semantics id="S3.SS2.21.21.21.m2.1a"><mrow id="S3.SS2.21.21.21.m2.1.1" xref="S3.SS2.21.21.21.m2.1.1.cmml"><msub id="S3.SS2.21.21.21.m2.1.1.2" xref="S3.SS2.21.21.21.m2.1.1.2.cmml"><mi mathsize="80%" id="S3.SS2.21.21.21.m2.1.1.2.2" xref="S3.SS2.21.21.21.m2.1.1.2.2.cmml">S</mi><mtext mathsize="80%" id="S3.SS2.21.21.21.m2.1.1.2.3" xref="S3.SS2.21.21.21.m2.1.1.2.3a.cmml">imp</mtext></msub><mo mathsize="80%" id="S3.SS2.21.21.21.m2.1.1.1" xref="S3.SS2.21.21.21.m2.1.1.1.cmml">∈</mo><msup id="S3.SS2.21.21.21.m2.1.1.3" xref="S3.SS2.21.21.21.m2.1.1.3.cmml"><mi mathsize="80%" id="S3.SS2.21.21.21.m2.1.1.3.2" xref="S3.SS2.21.21.21.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.21.21.21.m2.1.1.3.3" xref="S3.SS2.21.21.21.m2.1.1.3.3.cmml"><mi mathsize="80%" id="S3.SS2.21.21.21.m2.1.1.3.3.2" xref="S3.SS2.21.21.21.m2.1.1.3.3.2.cmml">h</mi><mo lspace="0.222em" mathsize="80%" rspace="0.222em" id="S3.SS2.21.21.21.m2.1.1.3.3.1" xref="S3.SS2.21.21.21.m2.1.1.3.3.1.cmml">×</mo><mi mathsize="80%" id="S3.SS2.21.21.21.m2.1.1.3.3.3" xref="S3.SS2.21.21.21.m2.1.1.3.3.3.cmml">w</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.21.21.21.m2.1b"><apply id="S3.SS2.21.21.21.m2.1.1.cmml" xref="S3.SS2.21.21.21.m2.1.1"><in id="S3.SS2.21.21.21.m2.1.1.1.cmml" xref="S3.SS2.21.21.21.m2.1.1.1"></in><apply id="S3.SS2.21.21.21.m2.1.1.2.cmml" xref="S3.SS2.21.21.21.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.21.21.21.m2.1.1.2.1.cmml" xref="S3.SS2.21.21.21.m2.1.1.2">subscript</csymbol><ci id="S3.SS2.21.21.21.m2.1.1.2.2.cmml" xref="S3.SS2.21.21.21.m2.1.1.2.2">𝑆</ci><ci id="S3.SS2.21.21.21.m2.1.1.2.3a.cmml" xref="S3.SS2.21.21.21.m2.1.1.2.3"><mtext mathsize="56%" id="S3.SS2.21.21.21.m2.1.1.2.3.cmml" xref="S3.SS2.21.21.21.m2.1.1.2.3">imp</mtext></ci></apply><apply id="S3.SS2.21.21.21.m2.1.1.3.cmml" xref="S3.SS2.21.21.21.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.21.21.21.m2.1.1.3.1.cmml" xref="S3.SS2.21.21.21.m2.1.1.3">superscript</csymbol><ci id="S3.SS2.21.21.21.m2.1.1.3.2.cmml" xref="S3.SS2.21.21.21.m2.1.1.3.2">ℝ</ci><apply id="S3.SS2.21.21.21.m2.1.1.3.3.cmml" xref="S3.SS2.21.21.21.m2.1.1.3.3"><times id="S3.SS2.21.21.21.m2.1.1.3.3.1.cmml" xref="S3.SS2.21.21.21.m2.1.1.3.3.1"></times><ci id="S3.SS2.21.21.21.m2.1.1.3.3.2.cmml" xref="S3.SS2.21.21.21.m2.1.1.3.3.2">ℎ</ci><ci id="S3.SS2.21.21.21.m2.1.1.3.3.3.cmml" xref="S3.SS2.21.21.21.m2.1.1.3.3.3">𝑤</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.21.21.21.m2.1c">S_{\text{imp}}\in\mathbb{R}^{h\times w}</annotation></semantics></math><span id="S3.SS2.23.23.23.3" class="ltx_text" style="font-size:80%;">. Similar to the explicit representation, we generate the implicit BEV features </span><math id="S3.SS2.22.22.22.m3.1" class="ltx_Math" alttext="X_{\text{imp}}" display="inline"><semantics id="S3.SS2.22.22.22.m3.1a"><msub id="S3.SS2.22.22.22.m3.1.1" xref="S3.SS2.22.22.22.m3.1.1.cmml"><mi mathsize="80%" id="S3.SS2.22.22.22.m3.1.1.2" xref="S3.SS2.22.22.22.m3.1.1.2.cmml">X</mi><mtext mathsize="80%" id="S3.SS2.22.22.22.m3.1.1.3" xref="S3.SS2.22.22.22.m3.1.1.3a.cmml">imp</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.22.22.22.m3.1b"><apply id="S3.SS2.22.22.22.m3.1.1.cmml" xref="S3.SS2.22.22.22.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.22.22.22.m3.1.1.1.cmml" xref="S3.SS2.22.22.22.m3.1.1">subscript</csymbol><ci id="S3.SS2.22.22.22.m3.1.1.2.cmml" xref="S3.SS2.22.22.22.m3.1.1.2">𝑋</ci><ci id="S3.SS2.22.22.22.m3.1.1.3a.cmml" xref="S3.SS2.22.22.22.m3.1.1.3"><mtext mathsize="56%" id="S3.SS2.22.22.22.m3.1.1.3.cmml" xref="S3.SS2.22.22.22.m3.1.1.3">imp</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.22.22.22.m3.1c">X_{\text{imp}}</annotation></semantics></math><span id="S3.SS2.23.23.23.4" class="ltx_text" style="font-size:80%;"> through a Conv-BN-ReLU layer </span><math id="S3.SS2.23.23.23.m4.1" class="ltx_Math" alttext="g_{\text{imp}}" display="inline"><semantics id="S3.SS2.23.23.23.m4.1a"><msub id="S3.SS2.23.23.23.m4.1.1" xref="S3.SS2.23.23.23.m4.1.1.cmml"><mi mathsize="80%" id="S3.SS2.23.23.23.m4.1.1.2" xref="S3.SS2.23.23.23.m4.1.1.2.cmml">g</mi><mtext mathsize="80%" id="S3.SS2.23.23.23.m4.1.1.3" xref="S3.SS2.23.23.23.m4.1.1.3a.cmml">imp</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.23.23.23.m4.1b"><apply id="S3.SS2.23.23.23.m4.1.1.cmml" xref="S3.SS2.23.23.23.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.23.23.23.m4.1.1.1.cmml" xref="S3.SS2.23.23.23.m4.1.1">subscript</csymbol><ci id="S3.SS2.23.23.23.m4.1.1.2.cmml" xref="S3.SS2.23.23.23.m4.1.1.2">𝑔</ci><ci id="S3.SS2.23.23.23.m4.1.1.3a.cmml" xref="S3.SS2.23.23.23.m4.1.1.3"><mtext mathsize="56%" id="S3.SS2.23.23.23.m4.1.1.3.cmml" xref="S3.SS2.23.23.23.m4.1.1.3">imp</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.23.23.23.m4.1c">g_{\text{imp}}</annotation></semantics></math><span id="S3.SS2.23.23.23.5" class="ltx_text" style="font-size:80%;"> and channel-wise concatenation:</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="A2.EGx4" class="ltx_equationgroup ltx_eqn_align ltx_figure_panel ltx_eqn_table">

<tbody id="S3.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E5.m1.1" class="ltx_Math" alttext="\displaystyle X_{\text{imp}}=X\oplus g_{\text{imp}}(S_{\text{imp}})." display="inline"><semantics id="S3.E5.m1.1a"><mrow id="S3.E5.m1.1.1.1" xref="S3.E5.m1.1.1.1.1.cmml"><mrow id="S3.E5.m1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.cmml"><msub id="S3.E5.m1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.3.cmml"><mi mathsize="80%" id="S3.E5.m1.1.1.1.1.3.2" xref="S3.E5.m1.1.1.1.1.3.2.cmml">X</mi><mtext mathsize="80%" id="S3.E5.m1.1.1.1.1.3.3" xref="S3.E5.m1.1.1.1.1.3.3a.cmml">imp</mtext></msub><mo mathsize="80%" id="S3.E5.m1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E5.m1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.cmml"><mi mathsize="80%" id="S3.E5.m1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.3.cmml">X</mi><mo mathsize="80%" id="S3.E5.m1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.2.cmml">⊕</mo><mrow id="S3.E5.m1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.cmml"><msub id="S3.E5.m1.1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.3.cmml"><mi mathsize="80%" id="S3.E5.m1.1.1.1.1.1.1.3.2" xref="S3.E5.m1.1.1.1.1.1.1.3.2.cmml">g</mi><mtext mathsize="80%" id="S3.E5.m1.1.1.1.1.1.1.3.3" xref="S3.E5.m1.1.1.1.1.1.1.3.3a.cmml">imp</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E5.m1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E5.m1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.E5.m1.1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E5.m1.1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.cmml"><mi mathsize="80%" id="S3.E5.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.2.cmml">S</mi><mtext mathsize="80%" id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3a.cmml">imp</mtext></msub><mo maxsize="80%" minsize="80%" id="S3.E5.m1.1.1.1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em" mathsize="80%" id="S3.E5.m1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1"><eq id="S3.E5.m1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.2"></eq><apply id="S3.E5.m1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.1.1.3.2">𝑋</ci><ci id="S3.E5.m1.1.1.1.1.3.3a.cmml" xref="S3.E5.m1.1.1.1.1.3.3"><mtext mathsize="56%" id="S3.E5.m1.1.1.1.1.3.3.cmml" xref="S3.E5.m1.1.1.1.1.3.3">imp</mtext></ci></apply><apply id="S3.E5.m1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E5.m1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.2">direct-sum</csymbol><ci id="S3.E5.m1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.3">𝑋</ci><apply id="S3.E5.m1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1"><times id="S3.E5.m1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2"></times><apply id="S3.E5.m1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3.2">𝑔</ci><ci id="S3.E5.m1.1.1.1.1.1.1.3.3a.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3.3"><mtext mathsize="56%" id="S3.E5.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3.3">imp</mtext></ci></apply><apply id="S3.E5.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.2">𝑆</ci><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3a.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3"><mtext mathsize="56%" id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3">imp</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">\displaystyle X_{\text{imp}}=X\oplus g_{\text{imp}}(S_{\text{imp}}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.SS2.23.23.31" class="ltx_p ltx_figure_panel"><span id="S3.SS2.23.23.31.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Detection head.</span><span id="S3.SS2.23.23.31.2" class="ltx_text" style="font-size:80%;">
After obtaining the refined BEV features, we can use any existing detection head to generate object proposals. While our experiments focus on a few existing models </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.23.23.31.3.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a><span id="S3.SS2.23.23.31.4.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S3.SS2.23.23.31.5" class="ltx_text" style="font-size:80%;"> due to their state-of-the-art performance, we point out that SSGNet is also compatible with other designs.</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<section id="S4" class="ltx_section ltx_figure_panel">
<h2 class="ltx_title ltx_title_section" style="font-size:80%;">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<figure id="S4.2" class="ltx_table">
<div id="S4.2.2" class="ltx_block">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_block">Table 3: </span>Ablation study on the Waymo Open Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> (val split).</figcaption><span id="S4.2.2.8" class="ltx_ERROR ltx_centering undefined">{Tabular}</span>
<p id="S4.2.2.9" class="ltx_p ltx_align_center"><span id="S4.2.2.9.1" class="ltx_text" style="font-size:80%;">c—cc—cc—cc—cc—cc—cc </span><span id="S4.2.2.9.2" class="ltx_text" style="font-size:80%;">Method</span><span id="S4.2.2.9.3" class="ltx_text" style="font-size:80%;">   Veh. (L1) </span><span class="ltx_rule" style="background:black;display:inline-block;"> </span><span id="S4.2.2.9.4" class="ltx_text" style="font-size:80%;">   Veh. (L2) </span><span class="ltx_rule" style="background:black;display:inline-block;"> </span><span id="S4.2.2.9.5" class="ltx_text" style="font-size:80%;">   Ped. (L1) </span><span class="ltx_rule" style="background:black;display:inline-block;"> </span><span id="S4.2.2.9.6" class="ltx_text" style="font-size:80%;">   Ped. (L2) </span><span class="ltx_rule" style="background:black;display:inline-block;"> </span><span id="S4.2.2.9.7" class="ltx_text" style="font-size:80%;">   Cyc. (L1) </span><span class="ltx_rule" style="background:black;display:inline-block;"> </span><span id="S4.2.2.9.8" class="ltx_text" style="font-size:80%;">   Cyc. (L2)</span></p>
<p id="S4.2.2.10" class="ltx_p ltx_align_center"><span id="S4.2.2.10.1" class="ltx_text" style="font-size:80%;">mAP  mAPH  mAP  mAPH  mAP  mAPH  mAP  mAPH  mAP  mAPH  mAP  mAPH</span></p>
<p id="S4.2.2.11" class="ltx_p ltx_align_center"><span id="S4.2.2.11.1" class="ltx_text" style="font-size:80%;">CenterPoint-Voxel  72.8  72.2  64.9  64.4  74.2  68.0  66.0  60.3  71.0  69.8  68.5  67.3</span></p>
<p id="S4.1.1.1" class="ltx_p ltx_align_center"><span id="S4.1.1.1.1" class="ltx_text" style="font-size:80%;">CenterPoint-Voxel</span><math id="S4.1.1.1.m1.1" class="ltx_Math" alttext="\times 2" display="inline"><semantics id="S4.1.1.1.m1.1a"><mrow id="S4.1.1.1.m1.1.1" xref="S4.1.1.1.m1.1.1.cmml"><mi id="S4.1.1.1.m1.1.1.2" xref="S4.1.1.1.m1.1.1.2.cmml"></mi><mo lspace="0.222em" mathsize="80%" rspace="0.222em" id="S4.1.1.1.m1.1.1.1" xref="S4.1.1.1.m1.1.1.1.cmml">×</mo><mn mathsize="80%" id="S4.1.1.1.m1.1.1.3" xref="S4.1.1.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.1.1.1.m1.1b"><apply id="S4.1.1.1.m1.1.1.cmml" xref="S4.1.1.1.m1.1.1"><times id="S4.1.1.1.m1.1.1.1.cmml" xref="S4.1.1.1.m1.1.1.1"></times><csymbol cd="latexml" id="S4.1.1.1.m1.1.1.2.cmml" xref="S4.1.1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S4.1.1.1.m1.1.1.3.cmml" xref="S4.1.1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.1.1.1.m1.1c">\times 2</annotation></semantics></math><span id="S4.1.1.1.2" class="ltx_text" style="font-size:80%;">  72.9  72.3  65.0  64.5  74.0  67.8  66.0  60.3  71.2  70.0  68.6  67.4</span></p>
<p id="S4.2.2.2" class="ltx_p ltx_align_center"><span id="S4.2.2.2.1" class="ltx_text" style="font-size:80%;">CenterPoint-Voxel</span><math id="S4.2.2.2.m1.1" class="ltx_Math" alttext="\times 3" display="inline"><semantics id="S4.2.2.2.m1.1a"><mrow id="S4.2.2.2.m1.1.1" xref="S4.2.2.2.m1.1.1.cmml"><mi id="S4.2.2.2.m1.1.1.2" xref="S4.2.2.2.m1.1.1.2.cmml"></mi><mo lspace="0.222em" mathsize="80%" rspace="0.222em" id="S4.2.2.2.m1.1.1.1" xref="S4.2.2.2.m1.1.1.1.cmml">×</mo><mn mathsize="80%" id="S4.2.2.2.m1.1.1.3" xref="S4.2.2.2.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.2.2.2.m1.1b"><apply id="S4.2.2.2.m1.1.1.cmml" xref="S4.2.2.2.m1.1.1"><times id="S4.2.2.2.m1.1.1.1.cmml" xref="S4.2.2.2.m1.1.1.1"></times><csymbol cd="latexml" id="S4.2.2.2.m1.1.1.2.cmml" xref="S4.2.2.2.m1.1.1.2">absent</csymbol><cn type="integer" id="S4.2.2.2.m1.1.1.3.cmml" xref="S4.2.2.2.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.2.2.2.m1.1c">\times 3</annotation></semantics></math><span id="S4.2.2.2.2" class="ltx_text" style="font-size:80%;">  73.1  72.6  65.0  64.5  73.9  67.7  66.0  60.4  71.3  70.2  68.7  67.6</span></p>
<p id="S4.2.2.12" class="ltx_p ltx_align_center"><span id="S4.2.2.12.1" class="ltx_text" style="font-size:80%;">SSGNet-Voxel-Explicit  74.5  74.0  66.5  66.0  75.4  69.0  67.6  61.8  71.4  70.2  69.0  67.9 </span></p>
<p id="S4.2.2.13" class="ltx_p ltx_align_center"><span id="S4.2.2.13.1" class="ltx_text" style="font-size:80%;">SSGNet-Voxel-Implicit  74.1  73.6  66.0  65.5  74.8  68.3  66.9  61.0  71.1  69.9  68.7  67.6</span></p>
<p id="S4.2.2.14" class="ltx_p ltx_align_center"><span id="S4.2.2.14.1" class="ltx_text" style="font-size:80%;">SSGNet-Voxel-Hybrid-Binary  74.1  73.5  66.0  65.5  75.1  68.6  67.3  61.3  70.2  69.0  67.9  66.7</span></p>
<p id="S4.2.2.15" class="ltx_p ltx_align_center"><span id="S4.2.2.15.1" class="ltx_text" style="font-size:80%;">SSGNet-Voxel  </span><span id="S4.2.2.15.2" class="ltx_text ltx_font_bold" style="font-size:80%;">75.0</span><span id="S4.2.2.15.3" class="ltx_text" style="font-size:80%;">  </span><span id="S4.2.2.15.4" class="ltx_text ltx_font_bold" style="font-size:80%;">74.4</span><span id="S4.2.2.15.5" class="ltx_text" style="font-size:80%;">  </span><span id="S4.2.2.15.6" class="ltx_text ltx_font_bold" style="font-size:80%;">67.0</span><span id="S4.2.2.15.7" class="ltx_text" style="font-size:80%;">  </span><span id="S4.2.2.15.8" class="ltx_text ltx_font_bold" style="font-size:80%;">66.5</span><span id="S4.2.2.15.9" class="ltx_text" style="font-size:80%;">  </span><span id="S4.2.2.15.10" class="ltx_text ltx_font_bold" style="font-size:80%;">75.6</span><span id="S4.2.2.15.11" class="ltx_text" style="font-size:80%;">  </span><span id="S4.2.2.15.12" class="ltx_text ltx_font_bold" style="font-size:80%;">69.0</span><span id="S4.2.2.15.13" class="ltx_text" style="font-size:80%;">  </span><span id="S4.2.2.15.14" class="ltx_text ltx_font_bold" style="font-size:80%;">67.9</span><span id="S4.2.2.15.15" class="ltx_text" style="font-size:80%;">  </span><span id="S4.2.2.15.16" class="ltx_text ltx_font_bold" style="font-size:80%;">61.9</span><span id="S4.2.2.15.17" class="ltx_text" style="font-size:80%;">  </span><span id="S4.2.2.15.18" class="ltx_text ltx_font_bold" style="font-size:80%;">71.8</span><span id="S4.2.2.15.19" class="ltx_text" style="font-size:80%;">  </span><span id="S4.2.2.15.20" class="ltx_text ltx_font_bold" style="font-size:80%;">70.7</span><span id="S4.2.2.15.21" class="ltx_text" style="font-size:80%;">  </span><span id="S4.2.2.15.22" class="ltx_text ltx_font_bold" style="font-size:80%;">69.2</span><span id="S4.2.2.15.23" class="ltx_text" style="font-size:80%;">  </span><span id="S4.2.2.15.24" class="ltx_text ltx_font_bold" style="font-size:80%;">68.1</span><span id="S4.2.2.15.25" class="ltx_text" style="font-size:80%;"></span></p>
<section id="S4.SS1" class="ltx_subsection ltx_centering">
<h3 class="ltx_title ltx_title_subsection" style="font-size:80%;">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text" style="font-size:80%;">Waymo Open Dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p1.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib39" title="" class="ltx_ref">39</a><span id="S4.SS1.p1.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.SS1.p1.1.4" class="ltx_text" style="font-size:80%;"> is a large-scale dataset. We use version 1.2, which contains 158,081 training samples and 39,987 validation samples. There are two difficulties: levels 1 and 2, where the bounding boxes contain at least five and one LiDAR points, respectively. We use official metrics, including the mean Average Precision (mAP) and the mean Average Precision weighted by Heading (mAPH). We benchmark multi-class 3D object detection using only single-frame LiDAR point clouds. The IoU thresholds for vehicles, pedestrians, and cyclists are 0.7, 0.5, and 0.5.</span></p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text" style="font-size:80%;">nuScenes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p2.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib2" title="" class="ltx_ref">2</a><span id="S4.SS1.p2.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.SS1.p2.1.4" class="ltx_text" style="font-size:80%;"> is another popular benchmark dataset with a total of 1,000 driving sequences. Compared to Waymo </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p2.1.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib39" title="" class="ltx_ref">39</a><span id="S4.SS1.p2.1.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.SS1.p2.1.7" class="ltx_text" style="font-size:80%;">, nuScenes includes more object categories, covering different types of vehicles such as cars, trucks, buses, and trailers. We use the mean Average Precision (mAP) and the nuScenes Detection Score (NDS) to compare different baseline approaches.</span></p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection ltx_centering">
<h3 class="ltx_title ltx_title_subsection" style="font-size:80%;">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation Details</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.3" class="ltx_p"><span id="S4.SS2.p1.3.1" class="ltx_text" style="font-size:80%;">We integrate SSGNet into four different top-performing object detectors, including CenterPoint </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2.p1.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib50" title="" class="ltx_ref">50</a><span id="S4.SS2.p1.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.SS2.p1.3.4" class="ltx_text" style="font-size:80%;">, PillarNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2.p1.3.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S4.SS2.p1.3.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.SS2.p1.3.7" class="ltx_text" style="font-size:80%;">, VoxSeT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2.p1.3.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S4.SS2.p1.3.9.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.SS2.p1.3.10" class="ltx_text" style="font-size:80%;">, and PV-RCNN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2.p1.3.11.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib33" title="" class="ltx_ref">33</a><span id="S4.SS2.p1.3.12.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.SS2.p1.3.13" class="ltx_text" style="font-size:80%;">. We use a detection range of </span><span id="S4.SS2.p1.3.14" class="ltx_text ltx_markedasmath" style="font-size:80%;">[-75.2m, 75.2m]</span><span id="S4.SS2.p1.3.15" class="ltx_text" style="font-size:80%;"> for the X and Y axes and </span><span id="S4.SS2.p1.3.16" class="ltx_text ltx_markedasmath" style="font-size:80%;">[-2m, 4m]</span><span id="S4.SS2.p1.3.17" class="ltx_text" style="font-size:80%;"> for the Z axis. The voxel sizes are </span><span id="S4.SS2.p1.3.18" class="ltx_text ltx_markedasmath" style="font-size:80%;">(0.1m, 0.1m, 0.15m)</span><span id="S4.SS2.p1.3.19" class="ltx_text" style="font-size:80%;">. All of our implementations are built upon the OpenPCDet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2.p1.3.20.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib42" title="" class="ltx_ref">42</a><span id="S4.SS2.p1.3.21.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.SS2.p1.3.22" class="ltx_text" style="font-size:80%;"> code base. Following Zhang et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2.p1.3.23.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib51" title="" class="ltx_ref">51</a><span id="S4.SS2.p1.3.24.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.SS2.p1.3.25" class="ltx_text" style="font-size:80%;">, we mainly use 20% training frames but evaluate on whole validation set due to limited computation resources. This setup applies to all baselines. We provide more comprehensive evaluation using 100% training examples against VoxSeT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2.p1.3.26.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S4.SS2.p1.3.27.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.SS2.p1.3.28" class="ltx_text" style="font-size:80%;">.</span></p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text" style="font-size:80%;">For Waymo Open Dataset, we train the networks on 8 Tesla V100 GPUs for 30 epochs using the ADAM optimizer </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2.p2.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S4.SS2.p2.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.SS2.p2.1.4" class="ltx_text" style="font-size:80%;"> with the one-cycle policy. For nuScenes, we train the networks for 20 epochs. All methods use the same data pre-processing, augmentation, and post-processing steps. To establish fair comparisons, we keep all other settings as the default in OpenPCDet with class-agnostic NMS enabled.</span></p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection ltx_centering">
<h3 class="ltx_title ltx_title_subsection" style="font-size:80%;">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Analysis of Results</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text" style="font-size:80%;">Table </span><a href="#S3.SS2" title="3.2 Architecture ‣ 3 Approach ‣ LiDAR-Based 3D Object Detection via Hybrid 2D Semantic Scene Generation" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">3.2</span></a><span id="S4.SS3.p1.1.2" class="ltx_text" style="font-size:80%;"> presents a summary of the quantitative evaluation on the Waymo Open Dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p1.1.3.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib39" title="" class="ltx_ref">39</a><span id="S4.SS3.p1.1.4.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.SS3.p1.1.5" class="ltx_text" style="font-size:80%;">. We choose four recent top-performing object detectors, including three single-stage methods (CenterPoint </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p1.1.6.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib50" title="" class="ltx_ref">50</a><span id="S4.SS3.p1.1.7.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.SS3.p1.1.8" class="ltx_text" style="font-size:80%;">, PillarNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p1.1.9.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S4.SS3.p1.1.10.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.SS3.p1.1.11" class="ltx_text" style="font-size:80%;">, VoxSeT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p1.1.12.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S4.SS3.p1.1.13.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.SS3.p1.1.14" class="ltx_text" style="font-size:80%;">) and one two-stage method (PV-RCNN). We add a 2D semantic scene generation module to each of these detectors and compare the prediction quality with the primitive models. Additionally, we present the results from the state-of-the-art point-based detector that does not involve BEV features, IA-SSD </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p1.1.15.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib51" title="" class="ltx_ref">51</a><span id="S4.SS3.p1.1.16.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.SS3.p1.1.17" class="ltx_text" style="font-size:80%;">, and three other baseline methods </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p1.1.18.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a><span id="S4.SS3.p1.1.19.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.SS3.p1.1.20" class="ltx_text" style="font-size:80%;">.</span></p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text" style="font-size:80%;">The integration of SSGNet demonstrates consistent improvements from all baseline methods. Under the mAP metric, our method improves CenterPoint </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p2.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib50" title="" class="ltx_ref">50</a><span id="S4.SS3.p2.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.SS3.p2.1.4" class="ltx_text" style="font-size:80%;"> (the voxel-based sparse convolutional variant) by 2.2%, 1.4%, and 0.8% for level-1 vehicles, pedestrians, and cyclists, respectively. Compared to PillarNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p2.1.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S4.SS3.p2.1.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.SS3.p2.1.7" class="ltx_text" style="font-size:80%;">, the absolute improvements are 1.8%, 1.7%, and 1.5%. For VoxSeT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p2.1.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S4.SS3.p2.1.9.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.SS3.p2.1.10" class="ltx_text" style="font-size:80%;">, the improvements are 3.9%, 1.5%, and 2.2% using 20% training data, and 3.8%, 1.1%, and 3.0% using 100% training data. Compared to PV-RCNN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p2.1.11.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib33" title="" class="ltx_ref">33</a><span id="S4.SS3.p2.1.12.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.SS3.p2.1.13" class="ltx_text" style="font-size:80%;">, the incorporation of SSGNet leads to improvements of 2.3%, 3.7%, and 1.3%. Overall, we observe a more significant improvement in the vehicle class than the pedestrian and cyclist classes in the one-stage detectors. Our explanation is that the larger size of a typical vehicle results in a longer average distance from its center to the surface, making vehicles more susceptible to the problem of missing points. As shown in Figure </span><a href="#S4.F6" title="Figure 6 ‣ 4.3 Analysis of Results ‣ 4 Experiments ‣ 3.2 Architecture ‣ 3 Approach ‣ LiDAR-Based 3D Object Detection via Hybrid 2D Semantic Scene Generation" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">6</span></a><span id="S4.SS3.p2.1.14" class="ltx_text" style="font-size:80%;">, even in regions with very sparely sampled points, the proposed 2D semantic scene representation provides a faithful description of the object silhouettes thanks to the density of our supervision. Additionally, Figure </span><a href="#S4.F6" title="Figure 6 ‣ 4.3 Analysis of Results ‣ 4 Experiments ‣ 3.2 Architecture ‣ 3 Approach ‣ LiDAR-Based 3D Object Detection via Hybrid 2D Semantic Scene Generation" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">6</span></a><span id="S4.SS3.p2.1.15" class="ltx_text" style="font-size:80%;"> suggests that contextual information plays an important role. While our detected boxes follow a similar orientation, the detection from CenterPoint lacks such uniformity, leading to inaccuracies and lower IoUs.</span></p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text" style="font-size:80%;">In addition to Waymo  </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p3.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib39" title="" class="ltx_ref">39</a><span id="S4.SS3.p3.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.SS3.p3.1.4" class="ltx_text" style="font-size:80%;">, Table </span><a href="#S3.SS2" title="3.2 Architecture ‣ 3 Approach ‣ LiDAR-Based 3D Object Detection via Hybrid 2D Semantic Scene Generation" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">3.2</span></a><span id="S4.SS3.p3.1.5" class="ltx_text" style="font-size:80%;"> reports the quantitative results on nuScenes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p3.1.6.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib2" title="" class="ltx_ref">2</a><span id="S4.SS3.p3.1.7.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.SS3.p3.1.8" class="ltx_text" style="font-size:80%;">. Integrating SSGNet into CenterPoint </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p3.1.9.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib50" title="" class="ltx_ref">50</a><span id="S4.SS3.p3.1.10.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.SS3.p3.1.11" class="ltx_text" style="font-size:80%;"> achieves an mAP of 61.7% and an NDS of 68.3%, representing 2.7% and 1.9% improvements from the primitive model. This outperforms the state-of-the-art Focals Conv </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p3.1.12.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S4.SS3.p3.1.13.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.SS3.p3.1.14" class="ltx_text" style="font-size:80%;"> introduced in CVPR 2022.</span></p>
</div>
<figure id="S4.F5" class="ltx_figure"><span id="S4.F5.pic1" class="ltx_picture ltx_centering" style="width:396.0pt;height:193.7pt;">\begin{overpic}[width=397.48651pt]{Figures/Figure3.pdf}
\put(15.0,2.0){Input}
\put(37.0,2.0){Explicit 2DSS}
\put(60.0,2.0){Implicit 2DSS}
\put(83.0,2.0){GT 2DSS}
\end{overpic}</span>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Additional visualizations of 2D semantic scenes (2DSS) generated via different methods.</figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure"><span id="S4.F6.pic1" class="ltx_picture ltx_centering" style="width:223.3pt;height:135.9pt;">\begin{overpic}[width=223.58379pt]{Figures/Figure2.pdf}
\put(1.0,2.0){CenterPoint\leavevmode\nobreak\ \cite[cite]{[\@@bibref{}{Yin_2021_center}{}{}]}}
\put(35.0,2.0){Ours}
\put(60.0,2.0){GT}
\put(82.0,2.0){2DSS}
\end{overpic}</span>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>A failure cause in CenterPoint <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> successfully corrected by SSGNet. Contextual information provided by 2D semantic scene (2DSS) helps to infer the bounding box parameters from sparse input regions.</figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection ltx_centering">
<h3 class="ltx_title ltx_title_subsection" style="font-size:80%;">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation Study</h3>

<div id="S4.SS4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.p1.2" class="ltx_p"><span id="S4.SS4.p1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Effect of incorporating 2D semantic scene generation.</span><span id="S4.SS4.p1.2.2" class="ltx_text" style="font-size:80%;">
Starting from a basic CenterPoint </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS4.p1.2.3.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib50" title="" class="ltx_ref">50</a><span id="S4.SS4.p1.2.4.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.SS4.p1.2.5" class="ltx_text" style="font-size:80%;"> model, we gradually introduce 2D semantic scene generation networks. In Table  </span><a href="#S4" title="4 Experiments ‣ 3.2 Architecture ‣ 3 Approach ‣ LiDAR-Based 3D Object Detection via Hybrid 2D Semantic Scene Generation" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S4.SS4.p1.2.6" class="ltx_text" style="font-size:80%;">, SSGNet-Voxel-Explicit refers to the CenterPoint model after adding an explicit network, and SSGNet-Voxel-Implicit denotes the CenterPoint model after adding an implicit network. To compensate for the introduction of more trainable parameters, we compare the modified models with CenterPoint-Voxel</span><math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="\times 2" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mrow id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml"><mi id="S4.SS4.p1.1.m1.1.1.2" xref="S4.SS4.p1.1.m1.1.1.2.cmml"></mi><mo lspace="0.222em" mathsize="80%" rspace="0.222em" id="S4.SS4.p1.1.m1.1.1.1" xref="S4.SS4.p1.1.m1.1.1.1.cmml">×</mo><mn mathsize="80%" id="S4.SS4.p1.1.m1.1.1.3" xref="S4.SS4.p1.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><apply id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"><times id="S4.SS4.p1.1.m1.1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1.1"></times><csymbol cd="latexml" id="S4.SS4.p1.1.m1.1.1.2.cmml" xref="S4.SS4.p1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S4.SS4.p1.1.m1.1.1.3.cmml" xref="S4.SS4.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">\times 2</annotation></semantics></math><span id="S4.SS4.p1.2.7" class="ltx_text" style="font-size:80%;">, whose network size has been proportionally increased. Specifically, CenterPoint-Voxel</span><math id="S4.SS4.p1.2.m2.1" class="ltx_Math" alttext="\times 2" display="inline"><semantics id="S4.SS4.p1.2.m2.1a"><mrow id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml"><mi id="S4.SS4.p1.2.m2.1.1.2" xref="S4.SS4.p1.2.m2.1.1.2.cmml"></mi><mo lspace="0.222em" mathsize="80%" rspace="0.222em" id="S4.SS4.p1.2.m2.1.1.1" xref="S4.SS4.p1.2.m2.1.1.1.cmml">×</mo><mn mathsize="80%" id="S4.SS4.p1.2.m2.1.1.3" xref="S4.SS4.p1.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><apply id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1"><times id="S4.SS4.p1.2.m2.1.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1.1"></times><csymbol cd="latexml" id="S4.SS4.p1.2.m2.1.1.2.cmml" xref="S4.SS4.p1.2.m2.1.1.2">absent</csymbol><cn type="integer" id="S4.SS4.p1.2.m2.1.1.3.cmml" xref="S4.SS4.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">\times 2</annotation></semantics></math><span id="S4.SS4.p1.2.8" class="ltx_text" style="font-size:80%;"> uses two spare backbones to generate two sets of features and concatenates them as the final BEV feature.</span></p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p"><span id="S4.SS4.p2.1.1" class="ltx_text" style="font-size:80%;">We observe that using either of the two modified networks consistently improves the primitive model across all three object classes, although the explicit network shows a slightly greater boost. As suggested by Figure </span><a href="#S4.F5" title="Figure 5 ‣ 4.3 Analysis of Results ‣ 4 Experiments ‣ 3.2 Architecture ‣ 3 Approach ‣ LiDAR-Based 3D Object Detection via Hybrid 2D Semantic Scene Generation" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">5</span></a><span id="S4.SS4.p2.1.2" class="ltx_text" style="font-size:80%;">, the implicit network occasionally assigns high probabilities to background regions, creating false positives.</span></p>
</div>
<div id="S4.SS4.p3" class="ltx_para ltx_noindent">
<p id="S4.SS4.p3.1" class="ltx_p"><span id="S4.SS4.p3.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Effect of hybrid representation.</span><span id="S4.SS4.p3.1.2" class="ltx_text" style="font-size:80%;">
In Table </span><a href="#S4" title="4 Experiments ‣ 3.2 Architecture ‣ 3 Approach ‣ LiDAR-Based 3D Object Detection via Hybrid 2D Semantic Scene Generation" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S4.SS4.p3.1.3" class="ltx_text" style="font-size:80%;">, the comparisons of SSGNet-Voxel-Explicit and SSGNet-Voxel-Implicit with the hybrid model (SSGNet-Voxel) demonstrate that the implicit and explicit representations have complementary benefits to accurate object detection.
As shown in Figure </span><a href="#S4.F5" title="Figure 5 ‣ 4.3 Analysis of Results ‣ 4 Experiments ‣ 3.2 Architecture ‣ 3 Approach ‣ LiDAR-Based 3D Object Detection via Hybrid 2D Semantic Scene Generation" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">5</span></a><span id="S4.SS4.p3.1.4" class="ltx_text" style="font-size:80%;">, if we generate object proposals based on the predicted probabilities, the sensitivity of the implicit network will lead to a high recall, and the robustness of the explicit network will contribute to a high precision. In addition, the implicit scene contains valuable background information, such as the boundaries associated with land and walls.</span></p>
</div>
<div id="S4.SS4.p4" class="ltx_para ltx_noindent">
<p id="S4.SS4.p4.1" class="ltx_p"><span id="S4.SS4.p4.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Effects of probability distribution modeling.</span><span id="S4.SS4.p4.1.2" class="ltx_text" style="font-size:80%;">
By interpreting the network output as real-valued probabilities, the 2D semantic scene representation differs from a binary segmentation mask. While a perfect binary segmentation is certainly desirable, predicting such a segmentation from the initial BEV features is practically impossible due to the problem of missing points. As an alternative, we use continuous probabilities instead of discrete binary labels to model the 2D semantic scene. As shown in Table </span><a href="#S4" title="4 Experiments ‣ 3.2 Architecture ‣ 3 Approach ‣ LiDAR-Based 3D Object Detection via Hybrid 2D Semantic Scene Generation" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S4.SS4.p4.1.3" class="ltx_text" style="font-size:80%;">, SSGNet-Voxel-Hybrid-Binary denotes a variant of the full model (SSGNet-Voxel) where the 2D semantic scene representations are converted to binary masks under a threshold of 0.5. We observe that this binarization leads to a noticeable performance drop. This suggests that the probabilistic modeling naturally balances the number of object proposals with uncertainties. Leaving the ambiguities for the detection head to resolve is empirically better than using an arbitrary threshold to eliminate the ambiguities, which leads to an inevitable information loss.</span></p>
</div>
<div id="S4.SS4.p5" class="ltx_para ltx_noindent">
<p id="S4.SS4.p5.1" class="ltx_p"><span id="S4.SS4.p5.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Effects of supervision density.</span><span id="S4.SS4.p5.1.2" class="ltx_text" style="font-size:80%;">
The primitive VoxSeT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS4.p5.1.3.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S4.SS4.p5.1.4.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.SS4.p5.1.5" class="ltx_text" style="font-size:80%;"> has already utilized sparse per-point semantic segmentation. We modify VoxSeT+SSGNet by disabling the per-point segmentation and obtain close performance. Due to the space constraint, we refer interested readers to the supplementary material for more details about how it demonstrates the importance of dense supervision.</span></p>
</div>
</section>
<section id="S5" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section" style="font-size:80%;">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions and Future Work</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text" style="font-size:80%;">This paper introduces the Semantic Scene Generation Net (SSGNet), which encodes dense semantic and geometric information into Bird’s-Eye View (BEV) features under a hybrid of explicit and implicit representations. Experiments and detailed analysis demonstrate that SSGNet can be easily integrated into existing LiDAR-based 3D object detectors and achieve significant improvements in accuracy. In particular, the density of semantic scene supervision is an important contributor to the performance gain. In the future, we plan to investigate other possible dense supervision signals. Another promising direction is to explore whether there is a better 2D representation for the 3D semantics.</span></p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Acknowledgement.</span><span id="S5.p2.1.2" class="ltx_text" style="font-size:80%;"> We would like to acknowledge NSF IIS-2047677, HDR-1934932, and CCF-2019844.</span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography ltx_centering">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Alex Bewley, Pei Sun, Thomas Mensink, Dragomir Anguelov, and Cristian
Sminchisescu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Range conditioned dilated convolutions for scale invariant 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2005.09927</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong,
Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">nuscenes: A multimodal dataset for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, June 2020.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Chen Chen, Zhe Chen, Jing Zhang, and Dacheng Tao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Sasa: Semantics-augmented set abstraction for point-based 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI Conference on Artificial Intelligence</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, volume 1, 2022.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Multi-view 3d object detection network for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, July 2017.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Yukang Chen, Yanwei Li, Xiangyu Zhang, Jian Sun, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Focal sparse convolutional networks for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 5428–5437, June 2022.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Yukang Chen, Jianhui Liu, Xiaojuan Qi, Xiangyu Zhang, Jian Sun, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Scaling up kernels in 3d cnns.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">, abs/2206.10555, 2022.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Zhiqin Chen and Hao Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Learning implicit fields for generative shape modeling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, June 2019.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Shuai Li Chenhang He, Ruihuang Li and Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Voxel set transformer: A set-to-set approach to 3d object detection
from point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Jiajun Deng, Shaoshuai Shi, Peiwei Li, Wengang Zhou, Yanyong Zhang, and
Houqiang Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Voxel R-CNN: towards high performance voxel-based 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Thirty-Fifth AAAI Conference on Artificial Intelligence</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">,
pages 1201–1209, 2021.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Lue Fan, Ziqi Pang, Tianyuan Zhang, Yu-Xiong Wang, Hang Zhao, Feng Wang, Naiyan
Wang, and Zhaoxiang Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Embracing single stride 3d object detector with sparse transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 8458–8468, June 2022.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Chao Ma Guangsheng Shi, Ruifeng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Pillarnet: Real-time and high-performance pillar-based 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Chenhang He, Ruihuang Li, Shuai Li, and Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Voxel set transformer: A set-to-set approach to 3d object detection
from point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, pages 8417–8427, June 2022.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Chenhang He, Hui Zeng, Jianqiang Huang, Xian-Sheng Hua, and Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Structure aware single-stage 3d object detection from point cloud.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, June 2020.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Yihan Hu, Zhuangzhuang Ding, Runzhou Ge, Wenxin Shao, Li Huang, Kun Li, and
Qiang Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Afdetv2: Rethinking the necessity of the second stage for object
detection from point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial
Intelligence</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, volume 36, pages 969–979, 2022.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Diederik P. Kingma and Jimmy Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR (Poster)</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Alex H. Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar
Beijbom.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Pointpillars: Fast encoders for object detection from point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, June 2019.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Zhichao Li, Feng Wang, and Naiyan Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Lidar r-cnn: An efficient and universal 3d object detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, pages 7546–7555, 2021.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Zhidong Liang, Ming Zhang, Zehan Zhang, Xian Zhao, and Shiliang Pu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Rangercnn: Towards fast and accurate 3d object detection with range
image representation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2009.00206</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Focal loss for dense object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV)</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, March 2017.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Jiageng Mao, Minzhe Niu, Haoyue Bai, Xiaodan Liang, Hang Xu, and Chunjing Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Pyramid r-cnn: Towards better performance and adaptability for 3d
object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV)</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, pages 2723–2732, October 2021.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Jiageng Mao, Yujing Xue, Minzhe Niu, Haoyue Bai, Jiashi Feng, Xiaodan Liang,
Hang Xu, and Chunjing Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Voxel transformer for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV)</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, pages 3164–3173, October 2021.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and
Andreas Geiger.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Occupancy networks: Learning 3d reconstruction in function space.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, June 2019.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Gregory P Meyer, Ankit Laddha, Eric Kee, Carlos Vallespi-Gonzalez, and Carl K
Wellington.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Lasernet: An efficient probabilistic 3d object detector for
autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, pages 12677–12686, 2019.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Zhenwei Miao, Jikai Chen, Hongyu Pan, Ruiwen Zhang, Kaixuan Liu, Peihan Hao,
Jun Zhu, Yang Wang, and Xin Zhan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Pvgnet: A bottom-up one-stage 3d object detector with integrated
multi-level features.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, pages 3279–3288, 2021.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Jongyoun Noh, Sanghoon Lee, and Bumsub Ham.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Hvpr: Hybrid voxel-point representation for single-stage 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, pages 14605–14614, June 2021.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven
Lovegrove.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Deepsdf: Learning continuous signed distance functions for shape
representation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, pages 165–174, June 2019.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Charles R. Qi, Or Litany, Kaiming He, and Leonidas J. Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Deep hough voting for 3d object detection in point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV)</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, October 2019.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Charles R. Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J. Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Frustum pointnets for 3d object detection from rgb-d data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, June 2018.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Frustum pointnets for 3d object detection from rgb-d data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, pages 918–927, 2018.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Pointnet: Deep learning on point sets for 3d classification and
segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, July 2017.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Pointnet++: Deep hierarchical feature learning on point sets in a
metric space.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">,
volume 30, 2017.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">U-net: Convolutional networks for biomedical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Medical Image Computing and Computer-Assisted Intervention
(MICCAI)</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and
Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Pv-rcnn: Point-voxel feature set abstraction for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, June 2020.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Shaoshuai Shi, Li Jiang, Jiajun Deng, Zhe Wang, Chaoxu Guo, Jianping Shi,
Xiaogang Wang, and Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">PV-RCNN++: point-voxel feature set abstraction with local vector
representation for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib34.4.2" class="ltx_text" style="font-size:90%;">, abs/2102.00463, 2021.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Pointrcnn: 3d object proposal generation and detection from point
cloud.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, June 2019.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Shaoshuai Shi, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">From points to parts: 3d object detection from point cloud with
part-aware and part-aggregation network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</span><span id="bib.bib36.4.2" class="ltx_text" style="font-size:90%;">,
43(8):2647–2664, 2020.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Shaoshuai Shi, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">From points to parts: 3d object detection from point cloud with
part-aware and part-aggregation network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Trans. Pattern Anal. Mach. Intell.</span><span id="bib.bib37.4.2" class="ltx_text" style="font-size:90%;">, 43(8):2647–2664,
2021.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Weijing Shi and Raj Rajkumar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Point-gnn: Graph neural network for 3d object detection in a point
cloud.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, June 2020.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai
Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay
Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott
Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang, Jonathon Shlens,
Zhifeng Chen, and Dragomir Anguelov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Scalability in perception for autonomous driving: Waymo open dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, June 2020.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Pei Sun, Mingxing Tan, Weiyue Wang, Chenxi Liu, Fei Xia, Zhaoqi Leng, and
Dragomir Anguelov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Swformer: Sparse window transformer for 3d object detection in point
clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23–27, 2022, Proceedings, Part X</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, pages 426–442.
Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Pei Sun, Weiyue Wang, Yuning Chai, Gamaleldin Elsayed, Alex Bewley, Xiao Zhang,
Cristian Sminchisescu, and Dragomir Anguelov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Rsn: Range sparse net for efficient, accurate lidar 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, pages 5725–5734, 2021.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
OpenPCDet Development Team.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Openpcdet: An open-source toolbox for 3d object detection from point
clouds.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/open-mmlab/OpenPCDet" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/open-mmlab/OpenPCDet</a><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Yue Wang, Alireza Fathi, Abhijit Kundu, David A. Ross, Caroline Pantofaru,
Thomas A. Funkhouser, and Justin M. Solomon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Pillar-based object detection for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The European Conference on Computer Vision (ECCV)</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Qiangeng Xu, Yin Zhou, Weiyue Wang, Charles R Qi, and Dragomir Anguelov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Spg: Unsupervised domain adaptation for 3d object detection via
semantic point generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, pages 15446–15456, 2021.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Shaoqing Xu, Dingfu Zhou, Jin Fang, Junbo Yin, Zhou Bin, and Liangjun Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Fusionpainting: Multimodal fusion with adaptive attention for 3d
object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2021 IEEE International Intelligent Transportation Systems
Conference (ITSC)</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, pages 3047–3054. IEEE, 2021.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Xu Yan, Jiantao Gao, Jie Li, Ruimao Zhang, Zhen Li, Rui Huang, and Shuguang
Cui.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Sparse single sweep lidar point cloud segmentation via learning
contextual shape priors from scene completion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib46.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial
Intelligence</span><span id="bib.bib46.5.3" class="ltx_text" style="font-size:90%;">, volume 35, pages 3101–3109, 2021.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Yan Yan, Yuxing Mao, and Bo Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Second: Sparsely embedded convolutional detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Sensors</span><span id="bib.bib47.4.2" class="ltx_text" style="font-size:90%;">, 18(10), 2018.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Zetong Yang, Yanan Sun, Shu Liu, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">3dssd: Point-based 3d single stage object detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, June 2020.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Zetong Yang, Yanan Sun, Shu Liu, Xiaoyong Shen, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Std: Sparse-to-dense 3d object detector for point cloud.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV)</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, October 2019.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Center-based 3d object detection and tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">, pages 11784–11793, June 2021.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Yifan Zhang, Qingyong Hu, Guoquan Xu, Yanxin Ma, Jianwei Wan, and Yulan Guo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Not all points are equal: Learning highly efficient point-based
detectors for 3d lidar point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib51.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib51.5.3" class="ltx_text" style="font-size:90%;">, pages 18953–18962, June 2022.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Zaiwei Zhang, Bo Sun, Haitao Yang, and Qixing Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">H3dnet: 3d object detection using hybrid geometric primitives.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text" style="font-size:90%;">In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael
Frahm, editors, </span><span id="bib.bib52.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision - ECCV 2020 - 16th European
Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XII</span><span id="bib.bib52.5.3" class="ltx_text" style="font-size:90%;">, volume
12357 of </span><span id="bib.bib52.6.4" class="ltx_text ltx_font_italic" style="font-size:90%;">Lecture Notes in Computer Science</span><span id="bib.bib52.7.5" class="ltx_text" style="font-size:90%;">, pages 311–329. Springer,
2020.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Hui Zhou, Xinge Zhu, Xiao Song, Yuexin Ma, Zhe Wang, Hongsheng Li, and Dahua
Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Cylinder3d: An effective 3d framework for driving-scene lidar
semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2008.01550</span><span id="bib.bib53.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Yin Zhou and Oncel Tuzel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Voxelnet: End-to-end learning for point cloud based 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib54.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib54.5.3" class="ltx_text" style="font-size:90%;">, June 2018.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Zixiang Zhou, Xiangchen Zhao, Yu Wang, Panqu Wang, and Hassan Foroosh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">Centerformer: Center-based transformer for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib55.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXVIII</span><span id="bib.bib55.5.3" class="ltx_text" style="font-size:90%;">, pages
496–513. Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Benjin Zhu, Zhengkai Jiang, Xiangxin Zhou, Zeming Li, and Gang Yu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Class-balanced grouping and sampling for point cloud 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib56.4.2" class="ltx_text" style="font-size:90%;">, abs/1908.09492, 2019.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_centering ltx_role_newpage"></div>
<div class="ltx_pagination ltx_centering ltx_role_newpage"></div>
<p id="S4.2.2.16" class="ltx_p ltx_align_center"><span id="S4.2.2.16.1" class="ltx_text" style="font-size:80%;">The supplementary materials provide more results on the effects of supervision density in Section </span><a href="#A1" title="Appendix A Effects of Supervision Density ‣ 4 Experiments ‣ 3.2 Architecture ‣ 3 Approach ‣ LiDAR-Based 3D Object Detection via Hybrid 2D Semantic Scene Generation" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">A</span></a><span id="S4.2.2.16.2" class="ltx_text" style="font-size:80%;">, and additional implementation details in Section </span><a href="#A2" title="Appendix B Implementation Details ‣ Appendix A Effects of Supervision Density ‣ 4 Experiments ‣ 3.2 Architecture ‣ 3 Approach ‣ LiDAR-Based 3D Object Detection via Hybrid 2D Semantic Scene Generation" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">B</span></a><span id="S4.2.2.16.3" class="ltx_text" style="font-size:80%;">.</span></p>
<section id="A1" class="ltx_appendix ltx_centering">
<h2 class="ltx_title ltx_title_appendix" style="font-size:80%;">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Effects of Supervision Density</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.2" class="ltx_p"><span id="A1.p1.2.1" class="ltx_text" style="font-size:80%;">We provide an ablation study on the dense supervision in Table </span><a href="#A1" title="Appendix A Effects of Supervision Density ‣ 4 Experiments ‣ 3.2 Architecture ‣ 3 Approach ‣ LiDAR-Based 3D Object Detection via Hybrid 2D Semantic Scene Generation" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">A</span></a><span id="A1.p1.2.2" class="ltx_text" style="font-size:80%;">. VoxSeT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.p1.2.3.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="A1.p1.2.4.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="A1.p1.2.5" class="ltx_text" style="font-size:80%;"> has already utilized per-point semantic segmentation, which relies on sparse supervision. After directly adding the proposed BEV feature refinement module with dense supervision, VoxSeT+SSGNet improves VoxSeT significantly (</span><math id="A1.p1.1.m1.1" class="ltx_Math" alttext="+3.9\%" display="inline"><semantics id="A1.p1.1.m1.1a"><mrow id="A1.p1.1.m1.1.1" xref="A1.p1.1.m1.1.1.cmml"><mo mathsize="80%" id="A1.p1.1.m1.1.1a" xref="A1.p1.1.m1.1.1.cmml">+</mo><mrow id="A1.p1.1.m1.1.1.2" xref="A1.p1.1.m1.1.1.2.cmml"><mn mathsize="80%" id="A1.p1.1.m1.1.1.2.2" xref="A1.p1.1.m1.1.1.2.2.cmml">3.9</mn><mo mathsize="80%" id="A1.p1.1.m1.1.1.2.1" xref="A1.p1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.1.m1.1b"><apply id="A1.p1.1.m1.1.1.cmml" xref="A1.p1.1.m1.1.1"><plus id="A1.p1.1.m1.1.1.1.cmml" xref="A1.p1.1.m1.1.1"></plus><apply id="A1.p1.1.m1.1.1.2.cmml" xref="A1.p1.1.m1.1.1.2"><csymbol cd="latexml" id="A1.p1.1.m1.1.1.2.1.cmml" xref="A1.p1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="A1.p1.1.m1.1.1.2.2.cmml" xref="A1.p1.1.m1.1.1.2.2">3.9</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.1.m1.1c">+3.9\%</annotation></semantics></math><span id="A1.p1.2.6" class="ltx_text" style="font-size:80%;"> Veh. L1, </span><math id="A1.p1.2.m2.1" class="ltx_Math" alttext="+4.3\%" display="inline"><semantics id="A1.p1.2.m2.1a"><mrow id="A1.p1.2.m2.1.1" xref="A1.p1.2.m2.1.1.cmml"><mo mathsize="80%" id="A1.p1.2.m2.1.1a" xref="A1.p1.2.m2.1.1.cmml">+</mo><mrow id="A1.p1.2.m2.1.1.2" xref="A1.p1.2.m2.1.1.2.cmml"><mn mathsize="80%" id="A1.p1.2.m2.1.1.2.2" xref="A1.p1.2.m2.1.1.2.2.cmml">4.3</mn><mo mathsize="80%" id="A1.p1.2.m2.1.1.2.1" xref="A1.p1.2.m2.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.2.m2.1b"><apply id="A1.p1.2.m2.1.1.cmml" xref="A1.p1.2.m2.1.1"><plus id="A1.p1.2.m2.1.1.1.cmml" xref="A1.p1.2.m2.1.1"></plus><apply id="A1.p1.2.m2.1.1.2.cmml" xref="A1.p1.2.m2.1.1.2"><csymbol cd="latexml" id="A1.p1.2.m2.1.1.2.1.cmml" xref="A1.p1.2.m2.1.1.2.1">percent</csymbol><cn type="float" id="A1.p1.2.m2.1.1.2.2.cmml" xref="A1.p1.2.m2.1.1.2.2">4.3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.2.m2.1c">+4.3\%</annotation></semantics></math><span id="A1.p1.2.7" class="ltx_text" style="font-size:80%;"> Veh. L2). We then remove the per-point segmentation head from VoxSeT+SSGNet and obtain a new model VoxSeT-NoSparseSeg+SSGNet. From the results in Table </span><a href="#A1" title="Appendix A Effects of Supervision Density ‣ 4 Experiments ‣ 3.2 Architecture ‣ 3 Approach ‣ LiDAR-Based 3D Object Detection via Hybrid 2D Semantic Scene Generation" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">A</span></a><span id="A1.p1.2.8" class="ltx_text" style="font-size:80%;">, VoxSeT-NoSparseSeg+SSGNet has very close performance to VoxSeT+SSGNet, which indicates that the dense supervision, instead of the sparse supervision, is the key to the performance gains.</span></p>
</div>
<figure id="A1.2" class="ltx_table">
<div id="A1.2.2" class="ltx_block">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_block">Table 4: </span>Ablation study on the Waymo Open Dataset (val split, <math id="A1.2.2.2.m1.1" class="ltx_Math" alttext="20\%" display="inline"><semantics id="A1.2.2.2.m1.1a"><mrow id="A1.2.2.2.m1.1.1" xref="A1.2.2.2.m1.1.1.cmml"><mn id="A1.2.2.2.m1.1.1.2" xref="A1.2.2.2.m1.1.1.2.cmml">20</mn><mo id="A1.2.2.2.m1.1.1.1" xref="A1.2.2.2.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.2.2.2.m1.1b"><apply id="A1.2.2.2.m1.1.1.cmml" xref="A1.2.2.2.m1.1.1"><csymbol cd="latexml" id="A1.2.2.2.m1.1.1.1.cmml" xref="A1.2.2.2.m1.1.1.1">percent</csymbol><cn type="integer" id="A1.2.2.2.m1.1.1.2.cmml" xref="A1.2.2.2.m1.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.2.2.2.m1.1c">20\%</annotation></semantics></math> data).</figcaption><span id="A1.2.2.6" class="ltx_ERROR ltx_centering undefined">{Tabular}</span>
<p id="A1.2.2.7" class="ltx_p ltx_align_center"><span id="A1.2.2.7.1" class="ltx_text" style="font-size:80%;">c—cc—cc—cc—cc—cc—cc </span><span id="A1.2.2.7.2" class="ltx_text" style="font-size:80%;">Method</span><span id="A1.2.2.7.3" class="ltx_text" style="font-size:80%;">   Veh. (L1) </span><span class="ltx_rule" style="background:black;display:inline-block;"> </span><span id="A1.2.2.7.4" class="ltx_text" style="font-size:80%;">   Veh. (L2) </span><span class="ltx_rule" style="background:black;display:inline-block;"> </span><span id="A1.2.2.7.5" class="ltx_text" style="font-size:80%;">   Ped. (L1) </span><span class="ltx_rule" style="background:black;display:inline-block;"> </span><span id="A1.2.2.7.6" class="ltx_text" style="font-size:80%;">   Ped. (L2) </span><span class="ltx_rule" style="background:black;display:inline-block;"> </span><span id="A1.2.2.7.7" class="ltx_text" style="font-size:80%;">   Cyc. (L1) </span><span class="ltx_rule" style="background:black;display:inline-block;"> </span><span id="A1.2.2.7.8" class="ltx_text" style="font-size:80%;">   Cyc. (L2)</span></p>
<p id="A1.2.2.8" class="ltx_p ltx_align_center"><span id="A1.2.2.8.1" class="ltx_text" style="font-size:80%;">mAP  mAPH  mAP  mAPH  mAP  mAPH  mAP  mAPH  mAP  mAPH  mAP  mAPH</span></p>
<p id="A1.2.2.9" class="ltx_p ltx_align_center"><span id="A1.2.2.9.1" class="ltx_text" style="font-size:80%;">VoxSeT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.2.2.9.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="A1.2.2.9.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="A1.2.2.9.4" class="ltx_text" style="font-size:80%;">  72.1  71.6  63.6  63.2  77.9  69.6  70.2  62.5  69.9  68.5  67.3  66.0</span></p>
<p id="A1.2.2.10" class="ltx_p ltx_align_center"><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.2.2.10.1.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="A1.2.2.10.2.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="A1.2.2.10.3" class="ltx_text" style="font-size:80%;">+SSGNet  76.0  75.5  67.9  67.4  79.4  71.3  71.8  64.3  72.1  70.9  69.4  68.3</span></p>
<p id="A1.2.2.11" class="ltx_p ltx_align_center"><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.2.2.11.1.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="A1.2.2.11.2.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="A1.2.2.11.3" class="ltx_text" style="font-size:80%;">-NoSparseSeg+SSGNet 76.1  75.6  67.9  67.5  79.2  71.4  71.8  64.5  72.4  71.2  69.7  68.5</span></p>
<section id="A2" class="ltx_appendix ltx_centering">
<h2 class="ltx_title ltx_title_appendix" style="font-size:80%;">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Implementation Details</h2>

<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:80%;">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>More Details of the Implicit Network</h3>

<div id="A2.SS1.p1" class="ltx_para">
<p id="A2.SS1.p1.1" class="ltx_p"><span id="A2.SS1.p1.1.1" class="ltx_text" style="font-size:80%;">The query points from importance sampling is only used to compute loss during training and will not be used by the detection head. During training and testing, the query points from grid sampling will be fed to the implicit network to generate a grid of probability values </span><math id="A2.SS1.p1.1.m1.1" class="ltx_Math" alttext="S_{\text{imp}}" display="inline"><semantics id="A2.SS1.p1.1.m1.1a"><msub id="A2.SS1.p1.1.m1.1.1" xref="A2.SS1.p1.1.m1.1.1.cmml"><mi mathsize="80%" id="A2.SS1.p1.1.m1.1.1.2" xref="A2.SS1.p1.1.m1.1.1.2.cmml">S</mi><mtext mathsize="80%" id="A2.SS1.p1.1.m1.1.1.3" xref="A2.SS1.p1.1.m1.1.1.3a.cmml">imp</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.SS1.p1.1.m1.1b"><apply id="A2.SS1.p1.1.m1.1.1.cmml" xref="A2.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A2.SS1.p1.1.m1.1.1.1.cmml" xref="A2.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="A2.SS1.p1.1.m1.1.1.2.cmml" xref="A2.SS1.p1.1.m1.1.1.2">𝑆</ci><ci id="A2.SS1.p1.1.m1.1.1.3a.cmml" xref="A2.SS1.p1.1.m1.1.1.3"><mtext mathsize="56%" id="A2.SS1.p1.1.m1.1.1.3.cmml" xref="A2.SS1.p1.1.m1.1.1.3">imp</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p1.1.m1.1c">S_{\text{imp}}</annotation></semantics></math><span id="A2.SS1.p1.1.2" class="ltx_text" style="font-size:80%;">. Specifically, the loss of implicit network is</span></p>
<table id="A2.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.E6.m1.4" class="ltx_Math" alttext="L_{\text{imp}}=\frac{1}{|Q_{\text{IS}}|}\sum_{\boldsymbol{q}\in Q_{\text{IS}}}L_{\text{focal}}(\phi(\boldsymbol{q},L),p_{\text{gt}})," display="block"><semantics id="A2.E6.m1.4a"><mrow id="A2.E6.m1.4.4.1" xref="A2.E6.m1.4.4.1.1.cmml"><mrow id="A2.E6.m1.4.4.1.1" xref="A2.E6.m1.4.4.1.1.cmml"><msub id="A2.E6.m1.4.4.1.1.4" xref="A2.E6.m1.4.4.1.1.4.cmml"><mi mathsize="80%" id="A2.E6.m1.4.4.1.1.4.2" xref="A2.E6.m1.4.4.1.1.4.2.cmml">L</mi><mtext mathsize="80%" id="A2.E6.m1.4.4.1.1.4.3" xref="A2.E6.m1.4.4.1.1.4.3a.cmml">imp</mtext></msub><mo mathsize="80%" id="A2.E6.m1.4.4.1.1.3" xref="A2.E6.m1.4.4.1.1.3.cmml">=</mo><mrow id="A2.E6.m1.4.4.1.1.2" xref="A2.E6.m1.4.4.1.1.2.cmml"><mfrac id="A2.E6.m1.1.1" xref="A2.E6.m1.1.1.cmml"><mn mathsize="80%" id="A2.E6.m1.1.1.3" xref="A2.E6.m1.1.1.3.cmml">1</mn><mrow id="A2.E6.m1.1.1.1.1" xref="A2.E6.m1.1.1.1.2.cmml"><mo maxsize="80%" minsize="80%" id="A2.E6.m1.1.1.1.1.2" xref="A2.E6.m1.1.1.1.2.1.cmml">|</mo><msub id="A2.E6.m1.1.1.1.1.1" xref="A2.E6.m1.1.1.1.1.1.cmml"><mi mathsize="80%" id="A2.E6.m1.1.1.1.1.1.2" xref="A2.E6.m1.1.1.1.1.1.2.cmml">Q</mi><mtext mathsize="80%" id="A2.E6.m1.1.1.1.1.1.3" xref="A2.E6.m1.1.1.1.1.1.3a.cmml">IS</mtext></msub><mo maxsize="80%" minsize="80%" id="A2.E6.m1.1.1.1.1.3" xref="A2.E6.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo lspace="0em" rspace="0em" id="A2.E6.m1.4.4.1.1.2.3" xref="A2.E6.m1.4.4.1.1.2.3.cmml">​</mo><mrow id="A2.E6.m1.4.4.1.1.2.2" xref="A2.E6.m1.4.4.1.1.2.2.cmml"><munder id="A2.E6.m1.4.4.1.1.2.2.3" xref="A2.E6.m1.4.4.1.1.2.2.3.cmml"><mo maxsize="80%" minsize="80%" movablelimits="false" stretchy="true" id="A2.E6.m1.4.4.1.1.2.2.3.2" xref="A2.E6.m1.4.4.1.1.2.2.3.2.cmml">∑</mo><mrow id="A2.E6.m1.4.4.1.1.2.2.3.3" xref="A2.E6.m1.4.4.1.1.2.2.3.3.cmml"><mi mathsize="80%" id="A2.E6.m1.4.4.1.1.2.2.3.3.2" xref="A2.E6.m1.4.4.1.1.2.2.3.3.2.cmml">𝒒</mi><mo mathsize="80%" id="A2.E6.m1.4.4.1.1.2.2.3.3.1" xref="A2.E6.m1.4.4.1.1.2.2.3.3.1.cmml">∈</mo><msub id="A2.E6.m1.4.4.1.1.2.2.3.3.3" xref="A2.E6.m1.4.4.1.1.2.2.3.3.3.cmml"><mi mathsize="80%" id="A2.E6.m1.4.4.1.1.2.2.3.3.3.2" xref="A2.E6.m1.4.4.1.1.2.2.3.3.3.2.cmml">Q</mi><mtext mathsize="80%" id="A2.E6.m1.4.4.1.1.2.2.3.3.3.3" xref="A2.E6.m1.4.4.1.1.2.2.3.3.3.3a.cmml">IS</mtext></msub></mrow></munder><mrow id="A2.E6.m1.4.4.1.1.2.2.2" xref="A2.E6.m1.4.4.1.1.2.2.2.cmml"><msub id="A2.E6.m1.4.4.1.1.2.2.2.4" xref="A2.E6.m1.4.4.1.1.2.2.2.4.cmml"><mi mathsize="80%" id="A2.E6.m1.4.4.1.1.2.2.2.4.2" xref="A2.E6.m1.4.4.1.1.2.2.2.4.2.cmml">L</mi><mtext mathsize="80%" id="A2.E6.m1.4.4.1.1.2.2.2.4.3" xref="A2.E6.m1.4.4.1.1.2.2.2.4.3a.cmml">focal</mtext></msub><mo lspace="0em" rspace="0em" id="A2.E6.m1.4.4.1.1.2.2.2.3" xref="A2.E6.m1.4.4.1.1.2.2.2.3.cmml">​</mo><mrow id="A2.E6.m1.4.4.1.1.2.2.2.2.2" xref="A2.E6.m1.4.4.1.1.2.2.2.2.3.cmml"><mo maxsize="80%" minsize="80%" id="A2.E6.m1.4.4.1.1.2.2.2.2.2.3" xref="A2.E6.m1.4.4.1.1.2.2.2.2.3.cmml">(</mo><mrow id="A2.E6.m1.4.4.1.1.1.1.1.1.1.1" xref="A2.E6.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mi mathsize="80%" id="A2.E6.m1.4.4.1.1.1.1.1.1.1.1.2" xref="A2.E6.m1.4.4.1.1.1.1.1.1.1.1.2.cmml">ϕ</mi><mo lspace="0em" rspace="0em" id="A2.E6.m1.4.4.1.1.1.1.1.1.1.1.1" xref="A2.E6.m1.4.4.1.1.1.1.1.1.1.1.1.cmml">​</mo><mrow id="A2.E6.m1.4.4.1.1.1.1.1.1.1.1.3.2" xref="A2.E6.m1.4.4.1.1.1.1.1.1.1.1.3.1.cmml"><mo maxsize="80%" minsize="80%" id="A2.E6.m1.4.4.1.1.1.1.1.1.1.1.3.2.1" xref="A2.E6.m1.4.4.1.1.1.1.1.1.1.1.3.1.cmml">(</mo><mi mathsize="80%" id="A2.E6.m1.2.2" xref="A2.E6.m1.2.2.cmml">𝒒</mi><mo mathsize="80%" id="A2.E6.m1.4.4.1.1.1.1.1.1.1.1.3.2.2" xref="A2.E6.m1.4.4.1.1.1.1.1.1.1.1.3.1.cmml">,</mo><mi mathsize="80%" id="A2.E6.m1.3.3" xref="A2.E6.m1.3.3.cmml">L</mi><mo maxsize="80%" minsize="80%" id="A2.E6.m1.4.4.1.1.1.1.1.1.1.1.3.2.3" xref="A2.E6.m1.4.4.1.1.1.1.1.1.1.1.3.1.cmml">)</mo></mrow></mrow><mo mathsize="80%" id="A2.E6.m1.4.4.1.1.2.2.2.2.2.4" xref="A2.E6.m1.4.4.1.1.2.2.2.2.3.cmml">,</mo><msub id="A2.E6.m1.4.4.1.1.2.2.2.2.2.2" xref="A2.E6.m1.4.4.1.1.2.2.2.2.2.2.cmml"><mi mathsize="80%" id="A2.E6.m1.4.4.1.1.2.2.2.2.2.2.2" xref="A2.E6.m1.4.4.1.1.2.2.2.2.2.2.2.cmml">p</mi><mtext mathsize="80%" id="A2.E6.m1.4.4.1.1.2.2.2.2.2.2.3" xref="A2.E6.m1.4.4.1.1.2.2.2.2.2.2.3a.cmml">gt</mtext></msub><mo maxsize="80%" minsize="80%" id="A2.E6.m1.4.4.1.1.2.2.2.2.2.5" xref="A2.E6.m1.4.4.1.1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo mathsize="80%" id="A2.E6.m1.4.4.1.2" xref="A2.E6.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.E6.m1.4b"><apply id="A2.E6.m1.4.4.1.1.cmml" xref="A2.E6.m1.4.4.1"><eq id="A2.E6.m1.4.4.1.1.3.cmml" xref="A2.E6.m1.4.4.1.1.3"></eq><apply id="A2.E6.m1.4.4.1.1.4.cmml" xref="A2.E6.m1.4.4.1.1.4"><csymbol cd="ambiguous" id="A2.E6.m1.4.4.1.1.4.1.cmml" xref="A2.E6.m1.4.4.1.1.4">subscript</csymbol><ci id="A2.E6.m1.4.4.1.1.4.2.cmml" xref="A2.E6.m1.4.4.1.1.4.2">𝐿</ci><ci id="A2.E6.m1.4.4.1.1.4.3a.cmml" xref="A2.E6.m1.4.4.1.1.4.3"><mtext mathsize="56%" id="A2.E6.m1.4.4.1.1.4.3.cmml" xref="A2.E6.m1.4.4.1.1.4.3">imp</mtext></ci></apply><apply id="A2.E6.m1.4.4.1.1.2.cmml" xref="A2.E6.m1.4.4.1.1.2"><times id="A2.E6.m1.4.4.1.1.2.3.cmml" xref="A2.E6.m1.4.4.1.1.2.3"></times><apply id="A2.E6.m1.1.1.cmml" xref="A2.E6.m1.1.1"><divide id="A2.E6.m1.1.1.2.cmml" xref="A2.E6.m1.1.1"></divide><cn type="integer" id="A2.E6.m1.1.1.3.cmml" xref="A2.E6.m1.1.1.3">1</cn><apply id="A2.E6.m1.1.1.1.2.cmml" xref="A2.E6.m1.1.1.1.1"><abs id="A2.E6.m1.1.1.1.2.1.cmml" xref="A2.E6.m1.1.1.1.1.2"></abs><apply id="A2.E6.m1.1.1.1.1.1.cmml" xref="A2.E6.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="A2.E6.m1.1.1.1.1.1.1.cmml" xref="A2.E6.m1.1.1.1.1.1">subscript</csymbol><ci id="A2.E6.m1.1.1.1.1.1.2.cmml" xref="A2.E6.m1.1.1.1.1.1.2">𝑄</ci><ci id="A2.E6.m1.1.1.1.1.1.3a.cmml" xref="A2.E6.m1.1.1.1.1.1.3"><mtext mathsize="56%" id="A2.E6.m1.1.1.1.1.1.3.cmml" xref="A2.E6.m1.1.1.1.1.1.3">IS</mtext></ci></apply></apply></apply><apply id="A2.E6.m1.4.4.1.1.2.2.cmml" xref="A2.E6.m1.4.4.1.1.2.2"><apply id="A2.E6.m1.4.4.1.1.2.2.3.cmml" xref="A2.E6.m1.4.4.1.1.2.2.3"><csymbol cd="ambiguous" id="A2.E6.m1.4.4.1.1.2.2.3.1.cmml" xref="A2.E6.m1.4.4.1.1.2.2.3">subscript</csymbol><sum id="A2.E6.m1.4.4.1.1.2.2.3.2.cmml" xref="A2.E6.m1.4.4.1.1.2.2.3.2"></sum><apply id="A2.E6.m1.4.4.1.1.2.2.3.3.cmml" xref="A2.E6.m1.4.4.1.1.2.2.3.3"><in id="A2.E6.m1.4.4.1.1.2.2.3.3.1.cmml" xref="A2.E6.m1.4.4.1.1.2.2.3.3.1"></in><ci id="A2.E6.m1.4.4.1.1.2.2.3.3.2.cmml" xref="A2.E6.m1.4.4.1.1.2.2.3.3.2">𝒒</ci><apply id="A2.E6.m1.4.4.1.1.2.2.3.3.3.cmml" xref="A2.E6.m1.4.4.1.1.2.2.3.3.3"><csymbol cd="ambiguous" id="A2.E6.m1.4.4.1.1.2.2.3.3.3.1.cmml" xref="A2.E6.m1.4.4.1.1.2.2.3.3.3">subscript</csymbol><ci id="A2.E6.m1.4.4.1.1.2.2.3.3.3.2.cmml" xref="A2.E6.m1.4.4.1.1.2.2.3.3.3.2">𝑄</ci><ci id="A2.E6.m1.4.4.1.1.2.2.3.3.3.3a.cmml" xref="A2.E6.m1.4.4.1.1.2.2.3.3.3.3"><mtext mathsize="40%" id="A2.E6.m1.4.4.1.1.2.2.3.3.3.3.cmml" xref="A2.E6.m1.4.4.1.1.2.2.3.3.3.3">IS</mtext></ci></apply></apply></apply><apply id="A2.E6.m1.4.4.1.1.2.2.2.cmml" xref="A2.E6.m1.4.4.1.1.2.2.2"><times id="A2.E6.m1.4.4.1.1.2.2.2.3.cmml" xref="A2.E6.m1.4.4.1.1.2.2.2.3"></times><apply id="A2.E6.m1.4.4.1.1.2.2.2.4.cmml" xref="A2.E6.m1.4.4.1.1.2.2.2.4"><csymbol cd="ambiguous" id="A2.E6.m1.4.4.1.1.2.2.2.4.1.cmml" xref="A2.E6.m1.4.4.1.1.2.2.2.4">subscript</csymbol><ci id="A2.E6.m1.4.4.1.1.2.2.2.4.2.cmml" xref="A2.E6.m1.4.4.1.1.2.2.2.4.2">𝐿</ci><ci id="A2.E6.m1.4.4.1.1.2.2.2.4.3a.cmml" xref="A2.E6.m1.4.4.1.1.2.2.2.4.3"><mtext mathsize="56%" id="A2.E6.m1.4.4.1.1.2.2.2.4.3.cmml" xref="A2.E6.m1.4.4.1.1.2.2.2.4.3">focal</mtext></ci></apply><interval closure="open" id="A2.E6.m1.4.4.1.1.2.2.2.2.3.cmml" xref="A2.E6.m1.4.4.1.1.2.2.2.2.2"><apply id="A2.E6.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="A2.E6.m1.4.4.1.1.1.1.1.1.1.1"><times id="A2.E6.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="A2.E6.m1.4.4.1.1.1.1.1.1.1.1.1"></times><ci id="A2.E6.m1.4.4.1.1.1.1.1.1.1.1.2.cmml" xref="A2.E6.m1.4.4.1.1.1.1.1.1.1.1.2">italic-ϕ</ci><interval closure="open" id="A2.E6.m1.4.4.1.1.1.1.1.1.1.1.3.1.cmml" xref="A2.E6.m1.4.4.1.1.1.1.1.1.1.1.3.2"><ci id="A2.E6.m1.2.2.cmml" xref="A2.E6.m1.2.2">𝒒</ci><ci id="A2.E6.m1.3.3.cmml" xref="A2.E6.m1.3.3">𝐿</ci></interval></apply><apply id="A2.E6.m1.4.4.1.1.2.2.2.2.2.2.cmml" xref="A2.E6.m1.4.4.1.1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="A2.E6.m1.4.4.1.1.2.2.2.2.2.2.1.cmml" xref="A2.E6.m1.4.4.1.1.2.2.2.2.2.2">subscript</csymbol><ci id="A2.E6.m1.4.4.1.1.2.2.2.2.2.2.2.cmml" xref="A2.E6.m1.4.4.1.1.2.2.2.2.2.2.2">𝑝</ci><ci id="A2.E6.m1.4.4.1.1.2.2.2.2.2.2.3a.cmml" xref="A2.E6.m1.4.4.1.1.2.2.2.2.2.2.3"><mtext mathsize="56%" id="A2.E6.m1.4.4.1.1.2.2.2.2.2.2.3.cmml" xref="A2.E6.m1.4.4.1.1.2.2.2.2.2.2.3">gt</mtext></ci></apply></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.E6.m1.4c">L_{\text{imp}}=\frac{1}{|Q_{\text{IS}}|}\sum_{\boldsymbol{q}\in Q_{\text{IS}}}L_{\text{focal}}(\phi(\boldsymbol{q},L),p_{\text{gt}}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="A2.SS1.p1.5" class="ltx_p"><span id="A2.SS1.p1.5.1" class="ltx_text" style="font-size:80%;">where </span><math id="A2.SS1.p1.2.m1.1" class="ltx_Math" alttext="Q_{\text{IS}}" display="inline"><semantics id="A2.SS1.p1.2.m1.1a"><msub id="A2.SS1.p1.2.m1.1.1" xref="A2.SS1.p1.2.m1.1.1.cmml"><mi mathsize="80%" id="A2.SS1.p1.2.m1.1.1.2" xref="A2.SS1.p1.2.m1.1.1.2.cmml">Q</mi><mtext mathsize="80%" id="A2.SS1.p1.2.m1.1.1.3" xref="A2.SS1.p1.2.m1.1.1.3a.cmml">IS</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.SS1.p1.2.m1.1b"><apply id="A2.SS1.p1.2.m1.1.1.cmml" xref="A2.SS1.p1.2.m1.1.1"><csymbol cd="ambiguous" id="A2.SS1.p1.2.m1.1.1.1.cmml" xref="A2.SS1.p1.2.m1.1.1">subscript</csymbol><ci id="A2.SS1.p1.2.m1.1.1.2.cmml" xref="A2.SS1.p1.2.m1.1.1.2">𝑄</ci><ci id="A2.SS1.p1.2.m1.1.1.3a.cmml" xref="A2.SS1.p1.2.m1.1.1.3"><mtext mathsize="56%" id="A2.SS1.p1.2.m1.1.1.3.cmml" xref="A2.SS1.p1.2.m1.1.1.3">IS</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p1.2.m1.1c">Q_{\text{IS}}</annotation></semantics></math><span id="A2.SS1.p1.5.2" class="ltx_text" style="font-size:80%;"> is the set of points from importance sampling, </span><math id="A2.SS1.p1.3.m2.2" class="ltx_Math" alttext="L_{\text{focal}}(\cdot,\cdot)" display="inline"><semantics id="A2.SS1.p1.3.m2.2a"><mrow id="A2.SS1.p1.3.m2.2.3" xref="A2.SS1.p1.3.m2.2.3.cmml"><msub id="A2.SS1.p1.3.m2.2.3.2" xref="A2.SS1.p1.3.m2.2.3.2.cmml"><mi mathsize="80%" id="A2.SS1.p1.3.m2.2.3.2.2" xref="A2.SS1.p1.3.m2.2.3.2.2.cmml">L</mi><mtext mathsize="80%" id="A2.SS1.p1.3.m2.2.3.2.3" xref="A2.SS1.p1.3.m2.2.3.2.3a.cmml">focal</mtext></msub><mo lspace="0em" rspace="0em" id="A2.SS1.p1.3.m2.2.3.1" xref="A2.SS1.p1.3.m2.2.3.1.cmml">​</mo><mrow id="A2.SS1.p1.3.m2.2.3.3.2" xref="A2.SS1.p1.3.m2.2.3.3.1.cmml"><mo maxsize="80%" minsize="80%" id="A2.SS1.p1.3.m2.2.3.3.2.1" xref="A2.SS1.p1.3.m2.2.3.3.1.cmml">(</mo><mo lspace="0em" mathsize="80%" rspace="0em" id="A2.SS1.p1.3.m2.1.1" xref="A2.SS1.p1.3.m2.1.1.cmml">⋅</mo><mo mathsize="80%" rspace="0em" id="A2.SS1.p1.3.m2.2.3.3.2.2" xref="A2.SS1.p1.3.m2.2.3.3.1.cmml">,</mo><mo lspace="0em" mathsize="80%" rspace="0em" id="A2.SS1.p1.3.m2.2.2" xref="A2.SS1.p1.3.m2.2.2.cmml">⋅</mo><mo maxsize="80%" minsize="80%" id="A2.SS1.p1.3.m2.2.3.3.2.3" xref="A2.SS1.p1.3.m2.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p1.3.m2.2b"><apply id="A2.SS1.p1.3.m2.2.3.cmml" xref="A2.SS1.p1.3.m2.2.3"><times id="A2.SS1.p1.3.m2.2.3.1.cmml" xref="A2.SS1.p1.3.m2.2.3.1"></times><apply id="A2.SS1.p1.3.m2.2.3.2.cmml" xref="A2.SS1.p1.3.m2.2.3.2"><csymbol cd="ambiguous" id="A2.SS1.p1.3.m2.2.3.2.1.cmml" xref="A2.SS1.p1.3.m2.2.3.2">subscript</csymbol><ci id="A2.SS1.p1.3.m2.2.3.2.2.cmml" xref="A2.SS1.p1.3.m2.2.3.2.2">𝐿</ci><ci id="A2.SS1.p1.3.m2.2.3.2.3a.cmml" xref="A2.SS1.p1.3.m2.2.3.2.3"><mtext mathsize="56%" id="A2.SS1.p1.3.m2.2.3.2.3.cmml" xref="A2.SS1.p1.3.m2.2.3.2.3">focal</mtext></ci></apply><interval closure="open" id="A2.SS1.p1.3.m2.2.3.3.1.cmml" xref="A2.SS1.p1.3.m2.2.3.3.2"><ci id="A2.SS1.p1.3.m2.1.1.cmml" xref="A2.SS1.p1.3.m2.1.1">⋅</ci><ci id="A2.SS1.p1.3.m2.2.2.cmml" xref="A2.SS1.p1.3.m2.2.2">⋅</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p1.3.m2.2c">L_{\text{focal}}(\cdot,\cdot)</annotation></semantics></math><span id="A2.SS1.p1.5.3" class="ltx_text" style="font-size:80%;"> is the focal loss, </span><math id="A2.SS1.p1.4.m3.1" class="ltx_Math" alttext="p_{\text{gt}}" display="inline"><semantics id="A2.SS1.p1.4.m3.1a"><msub id="A2.SS1.p1.4.m3.1.1" xref="A2.SS1.p1.4.m3.1.1.cmml"><mi mathsize="80%" id="A2.SS1.p1.4.m3.1.1.2" xref="A2.SS1.p1.4.m3.1.1.2.cmml">p</mi><mtext mathsize="80%" id="A2.SS1.p1.4.m3.1.1.3" xref="A2.SS1.p1.4.m3.1.1.3a.cmml">gt</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.SS1.p1.4.m3.1b"><apply id="A2.SS1.p1.4.m3.1.1.cmml" xref="A2.SS1.p1.4.m3.1.1"><csymbol cd="ambiguous" id="A2.SS1.p1.4.m3.1.1.1.cmml" xref="A2.SS1.p1.4.m3.1.1">subscript</csymbol><ci id="A2.SS1.p1.4.m3.1.1.2.cmml" xref="A2.SS1.p1.4.m3.1.1.2">𝑝</ci><ci id="A2.SS1.p1.4.m3.1.1.3a.cmml" xref="A2.SS1.p1.4.m3.1.1.3"><mtext mathsize="56%" id="A2.SS1.p1.4.m3.1.1.3.cmml" xref="A2.SS1.p1.4.m3.1.1.3">gt</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p1.4.m3.1c">p_{\text{gt}}</annotation></semantics></math><span id="A2.SS1.p1.5.4" class="ltx_text" style="font-size:80%;"> is the ground truth probability, </span><math id="A2.SS1.p1.5.m4.1" class="ltx_Math" alttext="\phi" display="inline"><semantics id="A2.SS1.p1.5.m4.1a"><mi mathsize="80%" id="A2.SS1.p1.5.m4.1.1" xref="A2.SS1.p1.5.m4.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="A2.SS1.p1.5.m4.1b"><ci id="A2.SS1.p1.5.m4.1.1.cmml" xref="A2.SS1.p1.5.m4.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p1.5.m4.1c">\phi</annotation></semantics></math><span id="A2.SS1.p1.5.5" class="ltx_text" style="font-size:80%;"> is the implicit network. In addition,</span></p>
<table id="A2.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.E7.m1.2" class="ltx_Math" alttext="S_{\text{imp}}=\left[p_{ij}\right]_{h\times w},p_{ij}=\phi(\boldsymbol{q}_{ij},L),\boldsymbol{q}_{ij}\in Q_{\text{GS}}," display="block"><semantics id="A2.E7.m1.2a"><mrow id="A2.E7.m1.2.2.1"><mrow id="A2.E7.m1.2.2.1.1.2" xref="A2.E7.m1.2.2.1.1.3.cmml"><mrow id="A2.E7.m1.2.2.1.1.1.1" xref="A2.E7.m1.2.2.1.1.1.1.cmml"><msub id="A2.E7.m1.2.2.1.1.1.1.3" xref="A2.E7.m1.2.2.1.1.1.1.3.cmml"><mi mathsize="80%" id="A2.E7.m1.2.2.1.1.1.1.3.2" xref="A2.E7.m1.2.2.1.1.1.1.3.2.cmml">S</mi><mtext mathsize="80%" id="A2.E7.m1.2.2.1.1.1.1.3.3" xref="A2.E7.m1.2.2.1.1.1.1.3.3a.cmml">imp</mtext></msub><mo mathsize="80%" id="A2.E7.m1.2.2.1.1.1.1.2" xref="A2.E7.m1.2.2.1.1.1.1.2.cmml">=</mo><msub id="A2.E7.m1.2.2.1.1.1.1.1" xref="A2.E7.m1.2.2.1.1.1.1.1.cmml"><mrow id="A2.E7.m1.2.2.1.1.1.1.1.1.1" xref="A2.E7.m1.2.2.1.1.1.1.1.1.2.cmml"><mo id="A2.E7.m1.2.2.1.1.1.1.1.1.1.2" xref="A2.E7.m1.2.2.1.1.1.1.1.1.2.1.cmml">[</mo><msub id="A2.E7.m1.2.2.1.1.1.1.1.1.1.1" xref="A2.E7.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mi mathsize="80%" id="A2.E7.m1.2.2.1.1.1.1.1.1.1.1.2" xref="A2.E7.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">p</mi><mrow id="A2.E7.m1.2.2.1.1.1.1.1.1.1.1.3" xref="A2.E7.m1.2.2.1.1.1.1.1.1.1.1.3.cmml"><mi mathsize="80%" id="A2.E7.m1.2.2.1.1.1.1.1.1.1.1.3.2" xref="A2.E7.m1.2.2.1.1.1.1.1.1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="A2.E7.m1.2.2.1.1.1.1.1.1.1.1.3.1" xref="A2.E7.m1.2.2.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi mathsize="80%" id="A2.E7.m1.2.2.1.1.1.1.1.1.1.1.3.3" xref="A2.E7.m1.2.2.1.1.1.1.1.1.1.1.3.3.cmml">j</mi></mrow></msub><mo id="A2.E7.m1.2.2.1.1.1.1.1.1.1.3" xref="A2.E7.m1.2.2.1.1.1.1.1.1.2.1.cmml">]</mo></mrow><mrow id="A2.E7.m1.2.2.1.1.1.1.1.3" xref="A2.E7.m1.2.2.1.1.1.1.1.3.cmml"><mi mathsize="80%" id="A2.E7.m1.2.2.1.1.1.1.1.3.2" xref="A2.E7.m1.2.2.1.1.1.1.1.3.2.cmml">h</mi><mo lspace="0.222em" mathsize="80%" rspace="0.222em" id="A2.E7.m1.2.2.1.1.1.1.1.3.1" xref="A2.E7.m1.2.2.1.1.1.1.1.3.1.cmml">×</mo><mi mathsize="80%" id="A2.E7.m1.2.2.1.1.1.1.1.3.3" xref="A2.E7.m1.2.2.1.1.1.1.1.3.3.cmml">w</mi></mrow></msub></mrow><mo mathsize="80%" id="A2.E7.m1.2.2.1.1.2.3" xref="A2.E7.m1.2.2.1.1.3a.cmml">,</mo><mrow id="A2.E7.m1.2.2.1.1.2.2.2" xref="A2.E7.m1.2.2.1.1.2.2.3.cmml"><mrow id="A2.E7.m1.2.2.1.1.2.2.1.1" xref="A2.E7.m1.2.2.1.1.2.2.1.1.cmml"><msub id="A2.E7.m1.2.2.1.1.2.2.1.1.3" xref="A2.E7.m1.2.2.1.1.2.2.1.1.3.cmml"><mi mathsize="80%" id="A2.E7.m1.2.2.1.1.2.2.1.1.3.2" xref="A2.E7.m1.2.2.1.1.2.2.1.1.3.2.cmml">p</mi><mrow id="A2.E7.m1.2.2.1.1.2.2.1.1.3.3" xref="A2.E7.m1.2.2.1.1.2.2.1.1.3.3.cmml"><mi mathsize="80%" id="A2.E7.m1.2.2.1.1.2.2.1.1.3.3.2" xref="A2.E7.m1.2.2.1.1.2.2.1.1.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="A2.E7.m1.2.2.1.1.2.2.1.1.3.3.1" xref="A2.E7.m1.2.2.1.1.2.2.1.1.3.3.1.cmml">​</mo><mi mathsize="80%" id="A2.E7.m1.2.2.1.1.2.2.1.1.3.3.3" xref="A2.E7.m1.2.2.1.1.2.2.1.1.3.3.3.cmml">j</mi></mrow></msub><mo mathsize="80%" id="A2.E7.m1.2.2.1.1.2.2.1.1.2" xref="A2.E7.m1.2.2.1.1.2.2.1.1.2.cmml">=</mo><mrow id="A2.E7.m1.2.2.1.1.2.2.1.1.1" xref="A2.E7.m1.2.2.1.1.2.2.1.1.1.cmml"><mi mathsize="80%" id="A2.E7.m1.2.2.1.1.2.2.1.1.1.3" xref="A2.E7.m1.2.2.1.1.2.2.1.1.1.3.cmml">ϕ</mi><mo lspace="0em" rspace="0em" id="A2.E7.m1.2.2.1.1.2.2.1.1.1.2" xref="A2.E7.m1.2.2.1.1.2.2.1.1.1.2.cmml">​</mo><mrow id="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1" xref="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.2.cmml"><mo maxsize="80%" minsize="80%" id="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.2" xref="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.2.cmml">(</mo><msub id="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.1" xref="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.1.cmml"><mi mathsize="80%" id="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.1.2" xref="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.1.2.cmml">𝒒</mi><mrow id="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.1.3" xref="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.1.3.cmml"><mi mathsize="80%" id="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.1.3.2" xref="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.1.3.1" xref="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.1.3.1.cmml">​</mo><mi mathsize="80%" id="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.1.3.3" xref="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.1.3.3.cmml">j</mi></mrow></msub><mo mathsize="80%" id="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.3" xref="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.2.cmml">,</mo><mi mathsize="80%" id="A2.E7.m1.1.1" xref="A2.E7.m1.1.1.cmml">L</mi><mo maxsize="80%" minsize="80%" id="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.4" xref="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo mathsize="80%" id="A2.E7.m1.2.2.1.1.2.2.2.3" xref="A2.E7.m1.2.2.1.1.2.2.3a.cmml">,</mo><mrow id="A2.E7.m1.2.2.1.1.2.2.2.2" xref="A2.E7.m1.2.2.1.1.2.2.2.2.cmml"><msub id="A2.E7.m1.2.2.1.1.2.2.2.2.2" xref="A2.E7.m1.2.2.1.1.2.2.2.2.2.cmml"><mi mathsize="80%" id="A2.E7.m1.2.2.1.1.2.2.2.2.2.2" xref="A2.E7.m1.2.2.1.1.2.2.2.2.2.2.cmml">𝒒</mi><mrow id="A2.E7.m1.2.2.1.1.2.2.2.2.2.3" xref="A2.E7.m1.2.2.1.1.2.2.2.2.2.3.cmml"><mi mathsize="80%" id="A2.E7.m1.2.2.1.1.2.2.2.2.2.3.2" xref="A2.E7.m1.2.2.1.1.2.2.2.2.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="A2.E7.m1.2.2.1.1.2.2.2.2.2.3.1" xref="A2.E7.m1.2.2.1.1.2.2.2.2.2.3.1.cmml">​</mo><mi mathsize="80%" id="A2.E7.m1.2.2.1.1.2.2.2.2.2.3.3" xref="A2.E7.m1.2.2.1.1.2.2.2.2.2.3.3.cmml">j</mi></mrow></msub><mo mathsize="80%" id="A2.E7.m1.2.2.1.1.2.2.2.2.1" xref="A2.E7.m1.2.2.1.1.2.2.2.2.1.cmml">∈</mo><msub id="A2.E7.m1.2.2.1.1.2.2.2.2.3" xref="A2.E7.m1.2.2.1.1.2.2.2.2.3.cmml"><mi mathsize="80%" id="A2.E7.m1.2.2.1.1.2.2.2.2.3.2" xref="A2.E7.m1.2.2.1.1.2.2.2.2.3.2.cmml">Q</mi><mtext mathsize="80%" id="A2.E7.m1.2.2.1.1.2.2.2.2.3.3" xref="A2.E7.m1.2.2.1.1.2.2.2.2.3.3a.cmml">GS</mtext></msub></mrow></mrow></mrow><mo mathsize="80%" id="A2.E7.m1.2.2.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.E7.m1.2b"><apply id="A2.E7.m1.2.2.1.1.3.cmml" xref="A2.E7.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="A2.E7.m1.2.2.1.1.3a.cmml" xref="A2.E7.m1.2.2.1.1.2.3">formulae-sequence</csymbol><apply id="A2.E7.m1.2.2.1.1.1.1.cmml" xref="A2.E7.m1.2.2.1.1.1.1"><eq id="A2.E7.m1.2.2.1.1.1.1.2.cmml" xref="A2.E7.m1.2.2.1.1.1.1.2"></eq><apply id="A2.E7.m1.2.2.1.1.1.1.3.cmml" xref="A2.E7.m1.2.2.1.1.1.1.3"><csymbol cd="ambiguous" id="A2.E7.m1.2.2.1.1.1.1.3.1.cmml" xref="A2.E7.m1.2.2.1.1.1.1.3">subscript</csymbol><ci id="A2.E7.m1.2.2.1.1.1.1.3.2.cmml" xref="A2.E7.m1.2.2.1.1.1.1.3.2">𝑆</ci><ci id="A2.E7.m1.2.2.1.1.1.1.3.3a.cmml" xref="A2.E7.m1.2.2.1.1.1.1.3.3"><mtext mathsize="56%" id="A2.E7.m1.2.2.1.1.1.1.3.3.cmml" xref="A2.E7.m1.2.2.1.1.1.1.3.3">imp</mtext></ci></apply><apply id="A2.E7.m1.2.2.1.1.1.1.1.cmml" xref="A2.E7.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="A2.E7.m1.2.2.1.1.1.1.1.2.cmml" xref="A2.E7.m1.2.2.1.1.1.1.1">subscript</csymbol><apply id="A2.E7.m1.2.2.1.1.1.1.1.1.2.cmml" xref="A2.E7.m1.2.2.1.1.1.1.1.1.1"><csymbol cd="latexml" id="A2.E7.m1.2.2.1.1.1.1.1.1.2.1.cmml" xref="A2.E7.m1.2.2.1.1.1.1.1.1.1.2">delimited-[]</csymbol><apply id="A2.E7.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="A2.E7.m1.2.2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="A2.E7.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="A2.E7.m1.2.2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="A2.E7.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="A2.E7.m1.2.2.1.1.1.1.1.1.1.1.2">𝑝</ci><apply id="A2.E7.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="A2.E7.m1.2.2.1.1.1.1.1.1.1.1.3"><times id="A2.E7.m1.2.2.1.1.1.1.1.1.1.1.3.1.cmml" xref="A2.E7.m1.2.2.1.1.1.1.1.1.1.1.3.1"></times><ci id="A2.E7.m1.2.2.1.1.1.1.1.1.1.1.3.2.cmml" xref="A2.E7.m1.2.2.1.1.1.1.1.1.1.1.3.2">𝑖</ci><ci id="A2.E7.m1.2.2.1.1.1.1.1.1.1.1.3.3.cmml" xref="A2.E7.m1.2.2.1.1.1.1.1.1.1.1.3.3">𝑗</ci></apply></apply></apply><apply id="A2.E7.m1.2.2.1.1.1.1.1.3.cmml" xref="A2.E7.m1.2.2.1.1.1.1.1.3"><times id="A2.E7.m1.2.2.1.1.1.1.1.3.1.cmml" xref="A2.E7.m1.2.2.1.1.1.1.1.3.1"></times><ci id="A2.E7.m1.2.2.1.1.1.1.1.3.2.cmml" xref="A2.E7.m1.2.2.1.1.1.1.1.3.2">ℎ</ci><ci id="A2.E7.m1.2.2.1.1.1.1.1.3.3.cmml" xref="A2.E7.m1.2.2.1.1.1.1.1.3.3">𝑤</ci></apply></apply></apply><apply id="A2.E7.m1.2.2.1.1.2.2.3.cmml" xref="A2.E7.m1.2.2.1.1.2.2.2"><csymbol cd="ambiguous" id="A2.E7.m1.2.2.1.1.2.2.3a.cmml" xref="A2.E7.m1.2.2.1.1.2.2.2.3">formulae-sequence</csymbol><apply id="A2.E7.m1.2.2.1.1.2.2.1.1.cmml" xref="A2.E7.m1.2.2.1.1.2.2.1.1"><eq id="A2.E7.m1.2.2.1.1.2.2.1.1.2.cmml" xref="A2.E7.m1.2.2.1.1.2.2.1.1.2"></eq><apply id="A2.E7.m1.2.2.1.1.2.2.1.1.3.cmml" xref="A2.E7.m1.2.2.1.1.2.2.1.1.3"><csymbol cd="ambiguous" id="A2.E7.m1.2.2.1.1.2.2.1.1.3.1.cmml" xref="A2.E7.m1.2.2.1.1.2.2.1.1.3">subscript</csymbol><ci id="A2.E7.m1.2.2.1.1.2.2.1.1.3.2.cmml" xref="A2.E7.m1.2.2.1.1.2.2.1.1.3.2">𝑝</ci><apply id="A2.E7.m1.2.2.1.1.2.2.1.1.3.3.cmml" xref="A2.E7.m1.2.2.1.1.2.2.1.1.3.3"><times id="A2.E7.m1.2.2.1.1.2.2.1.1.3.3.1.cmml" xref="A2.E7.m1.2.2.1.1.2.2.1.1.3.3.1"></times><ci id="A2.E7.m1.2.2.1.1.2.2.1.1.3.3.2.cmml" xref="A2.E7.m1.2.2.1.1.2.2.1.1.3.3.2">𝑖</ci><ci id="A2.E7.m1.2.2.1.1.2.2.1.1.3.3.3.cmml" xref="A2.E7.m1.2.2.1.1.2.2.1.1.3.3.3">𝑗</ci></apply></apply><apply id="A2.E7.m1.2.2.1.1.2.2.1.1.1.cmml" xref="A2.E7.m1.2.2.1.1.2.2.1.1.1"><times id="A2.E7.m1.2.2.1.1.2.2.1.1.1.2.cmml" xref="A2.E7.m1.2.2.1.1.2.2.1.1.1.2"></times><ci id="A2.E7.m1.2.2.1.1.2.2.1.1.1.3.cmml" xref="A2.E7.m1.2.2.1.1.2.2.1.1.1.3">italic-ϕ</ci><interval closure="open" id="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.2.cmml" xref="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1"><apply id="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.1.cmml" xref="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.1.1.cmml" xref="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.1.2.cmml" xref="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.1.2">𝒒</ci><apply id="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.1.3.cmml" xref="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.1.3"><times id="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.1.3.1.cmml" xref="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.1.3.1"></times><ci id="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.1.3.2.cmml" xref="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.1.3.2">𝑖</ci><ci id="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.1.3.3.cmml" xref="A2.E7.m1.2.2.1.1.2.2.1.1.1.1.1.1.3.3">𝑗</ci></apply></apply><ci id="A2.E7.m1.1.1.cmml" xref="A2.E7.m1.1.1">𝐿</ci></interval></apply></apply><apply id="A2.E7.m1.2.2.1.1.2.2.2.2.cmml" xref="A2.E7.m1.2.2.1.1.2.2.2.2"><in id="A2.E7.m1.2.2.1.1.2.2.2.2.1.cmml" xref="A2.E7.m1.2.2.1.1.2.2.2.2.1"></in><apply id="A2.E7.m1.2.2.1.1.2.2.2.2.2.cmml" xref="A2.E7.m1.2.2.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="A2.E7.m1.2.2.1.1.2.2.2.2.2.1.cmml" xref="A2.E7.m1.2.2.1.1.2.2.2.2.2">subscript</csymbol><ci id="A2.E7.m1.2.2.1.1.2.2.2.2.2.2.cmml" xref="A2.E7.m1.2.2.1.1.2.2.2.2.2.2">𝒒</ci><apply id="A2.E7.m1.2.2.1.1.2.2.2.2.2.3.cmml" xref="A2.E7.m1.2.2.1.1.2.2.2.2.2.3"><times id="A2.E7.m1.2.2.1.1.2.2.2.2.2.3.1.cmml" xref="A2.E7.m1.2.2.1.1.2.2.2.2.2.3.1"></times><ci id="A2.E7.m1.2.2.1.1.2.2.2.2.2.3.2.cmml" xref="A2.E7.m1.2.2.1.1.2.2.2.2.2.3.2">𝑖</ci><ci id="A2.E7.m1.2.2.1.1.2.2.2.2.2.3.3.cmml" xref="A2.E7.m1.2.2.1.1.2.2.2.2.2.3.3">𝑗</ci></apply></apply><apply id="A2.E7.m1.2.2.1.1.2.2.2.2.3.cmml" xref="A2.E7.m1.2.2.1.1.2.2.2.2.3"><csymbol cd="ambiguous" id="A2.E7.m1.2.2.1.1.2.2.2.2.3.1.cmml" xref="A2.E7.m1.2.2.1.1.2.2.2.2.3">subscript</csymbol><ci id="A2.E7.m1.2.2.1.1.2.2.2.2.3.2.cmml" xref="A2.E7.m1.2.2.1.1.2.2.2.2.3.2">𝑄</ci><ci id="A2.E7.m1.2.2.1.1.2.2.2.2.3.3a.cmml" xref="A2.E7.m1.2.2.1.1.2.2.2.2.3.3"><mtext mathsize="56%" id="A2.E7.m1.2.2.1.1.2.2.2.2.3.3.cmml" xref="A2.E7.m1.2.2.1.1.2.2.2.2.3.3">GS</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.E7.m1.2c">S_{\text{imp}}=\left[p_{ij}\right]_{h\times w},p_{ij}=\phi(\boldsymbol{q}_{ij},L),\boldsymbol{q}_{ij}\in Q_{\text{GS}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p id="A2.SS1.p1.7" class="ltx_p"><span id="A2.SS1.p1.7.1" class="ltx_text" style="font-size:80%;">where </span><math id="A2.SS1.p1.6.m1.1" class="ltx_Math" alttext="Q_{\text{GS}}" display="inline"><semantics id="A2.SS1.p1.6.m1.1a"><msub id="A2.SS1.p1.6.m1.1.1" xref="A2.SS1.p1.6.m1.1.1.cmml"><mi mathsize="80%" id="A2.SS1.p1.6.m1.1.1.2" xref="A2.SS1.p1.6.m1.1.1.2.cmml">Q</mi><mtext mathsize="80%" id="A2.SS1.p1.6.m1.1.1.3" xref="A2.SS1.p1.6.m1.1.1.3a.cmml">GS</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.SS1.p1.6.m1.1b"><apply id="A2.SS1.p1.6.m1.1.1.cmml" xref="A2.SS1.p1.6.m1.1.1"><csymbol cd="ambiguous" id="A2.SS1.p1.6.m1.1.1.1.cmml" xref="A2.SS1.p1.6.m1.1.1">subscript</csymbol><ci id="A2.SS1.p1.6.m1.1.1.2.cmml" xref="A2.SS1.p1.6.m1.1.1.2">𝑄</ci><ci id="A2.SS1.p1.6.m1.1.1.3a.cmml" xref="A2.SS1.p1.6.m1.1.1.3"><mtext mathsize="56%" id="A2.SS1.p1.6.m1.1.1.3.cmml" xref="A2.SS1.p1.6.m1.1.1.3">GS</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p1.6.m1.1c">Q_{\text{GS}}</annotation></semantics></math><span id="A2.SS1.p1.7.2" class="ltx_text" style="font-size:80%;"> is the set of points from grid sampling. Note that there are no differences in generating implicit feature </span><math id="A2.SS1.p1.7.m2.1" class="ltx_Math" alttext="S_{\text{imp}}" display="inline"><semantics id="A2.SS1.p1.7.m2.1a"><msub id="A2.SS1.p1.7.m2.1.1" xref="A2.SS1.p1.7.m2.1.1.cmml"><mi mathsize="80%" id="A2.SS1.p1.7.m2.1.1.2" xref="A2.SS1.p1.7.m2.1.1.2.cmml">S</mi><mtext mathsize="80%" id="A2.SS1.p1.7.m2.1.1.3" xref="A2.SS1.p1.7.m2.1.1.3a.cmml">imp</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.SS1.p1.7.m2.1b"><apply id="A2.SS1.p1.7.m2.1.1.cmml" xref="A2.SS1.p1.7.m2.1.1"><csymbol cd="ambiguous" id="A2.SS1.p1.7.m2.1.1.1.cmml" xref="A2.SS1.p1.7.m2.1.1">subscript</csymbol><ci id="A2.SS1.p1.7.m2.1.1.2.cmml" xref="A2.SS1.p1.7.m2.1.1.2">𝑆</ci><ci id="A2.SS1.p1.7.m2.1.1.3a.cmml" xref="A2.SS1.p1.7.m2.1.1.3"><mtext mathsize="56%" id="A2.SS1.p1.7.m2.1.1.3.cmml" xref="A2.SS1.p1.7.m2.1.1.3">imp</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p1.7.m2.1c">S_{\text{imp}}</annotation></semantics></math><span id="A2.SS1.p1.7.3" class="ltx_text" style="font-size:80%;"> during training and testing (both from grid sampling).</span></p>
</div>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:80%;">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Overall Network Details</h3>

<div id="A2.SS2.p1" class="ltx_para">
<p id="A2.SS2.p1.1" class="ltx_p"><span id="A2.SS2.p1.1.1" class="ltx_text" style="font-size:80%;">In this section, we provide more details about the designs of CenterPoint-Voxel + SSGNet in the Waymo dataset. Similar designs also apply to other architectures. For the two-stage model PV-RCNN, we only modify its first stage. The voxel size is (0.1m, 0.1m, 0.15m) and the input spatial grid shape is [1504, 1504] for the X and Y axes. We use the down-sampled features with XY spatial grid shape [376, 376] as input to the BEV feature refinement module.</span></p>
</div>
<div id="A2.SS2.p2" class="ltx_para">
<p id="A2.SS2.p2.1" class="ltx_p"><span id="A2.SS2.p2.1.1" class="ltx_text" style="font-size:80%;">In the explicit network (U-Net), the input features are down-sampled four times. In the implicit network, the input features are down-sampled three times , and 3 MLP layers are used in the decoder. Note that although both the encoders of the explicit and implicit network use multi-scale features, the decoders are distinct (ConvNet </span><span id="A2.SS2.p2.1.2" class="ltx_text ltx_font_italic" style="font-size:80%;">vs.</span><span id="A2.SS2.p2.1.3" class="ltx_text" style="font-size:80%;"> MLP), which contributes to the complementary properties of the generated explicit and implicit 2D semantic scene.</span></p>
</div>
<div id="A2.SS2.p3" class="ltx_para">
<p id="A2.SS2.p3.5" class="ltx_p"><span id="A2.SS2.p3.5.1" class="ltx_text" style="font-size:80%;">The overall loss is</span></p>
<table id="A2.EGx5" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A2.E8"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A2.E8.m1.1" class="ltx_Math" alttext="\displaystyle L_{\text{total}}=L_{\text{rpn}}+L_{\text{exp}}+\lambda_{\text{imp}}L_{\text{imp}}," display="inline"><semantics id="A2.E8.m1.1a"><mrow id="A2.E8.m1.1.1.1" xref="A2.E8.m1.1.1.1.1.cmml"><mrow id="A2.E8.m1.1.1.1.1" xref="A2.E8.m1.1.1.1.1.cmml"><msub id="A2.E8.m1.1.1.1.1.2" xref="A2.E8.m1.1.1.1.1.2.cmml"><mi mathsize="80%" id="A2.E8.m1.1.1.1.1.2.2" xref="A2.E8.m1.1.1.1.1.2.2.cmml">L</mi><mtext mathsize="80%" id="A2.E8.m1.1.1.1.1.2.3" xref="A2.E8.m1.1.1.1.1.2.3a.cmml">total</mtext></msub><mo mathsize="80%" id="A2.E8.m1.1.1.1.1.1" xref="A2.E8.m1.1.1.1.1.1.cmml">=</mo><mrow id="A2.E8.m1.1.1.1.1.3" xref="A2.E8.m1.1.1.1.1.3.cmml"><msub id="A2.E8.m1.1.1.1.1.3.2" xref="A2.E8.m1.1.1.1.1.3.2.cmml"><mi mathsize="80%" id="A2.E8.m1.1.1.1.1.3.2.2" xref="A2.E8.m1.1.1.1.1.3.2.2.cmml">L</mi><mtext mathsize="80%" id="A2.E8.m1.1.1.1.1.3.2.3" xref="A2.E8.m1.1.1.1.1.3.2.3a.cmml">rpn</mtext></msub><mo mathsize="80%" id="A2.E8.m1.1.1.1.1.3.1" xref="A2.E8.m1.1.1.1.1.3.1.cmml">+</mo><msub id="A2.E8.m1.1.1.1.1.3.3" xref="A2.E8.m1.1.1.1.1.3.3.cmml"><mi mathsize="80%" id="A2.E8.m1.1.1.1.1.3.3.2" xref="A2.E8.m1.1.1.1.1.3.3.2.cmml">L</mi><mtext mathsize="80%" id="A2.E8.m1.1.1.1.1.3.3.3" xref="A2.E8.m1.1.1.1.1.3.3.3a.cmml">exp</mtext></msub><mo mathsize="80%" id="A2.E8.m1.1.1.1.1.3.1a" xref="A2.E8.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="A2.E8.m1.1.1.1.1.3.4" xref="A2.E8.m1.1.1.1.1.3.4.cmml"><msub id="A2.E8.m1.1.1.1.1.3.4.2" xref="A2.E8.m1.1.1.1.1.3.4.2.cmml"><mi mathsize="80%" id="A2.E8.m1.1.1.1.1.3.4.2.2" xref="A2.E8.m1.1.1.1.1.3.4.2.2.cmml">λ</mi><mtext mathsize="80%" id="A2.E8.m1.1.1.1.1.3.4.2.3" xref="A2.E8.m1.1.1.1.1.3.4.2.3a.cmml">imp</mtext></msub><mo lspace="0em" rspace="0em" id="A2.E8.m1.1.1.1.1.3.4.1" xref="A2.E8.m1.1.1.1.1.3.4.1.cmml">​</mo><msub id="A2.E8.m1.1.1.1.1.3.4.3" xref="A2.E8.m1.1.1.1.1.3.4.3.cmml"><mi mathsize="80%" id="A2.E8.m1.1.1.1.1.3.4.3.2" xref="A2.E8.m1.1.1.1.1.3.4.3.2.cmml">L</mi><mtext mathsize="80%" id="A2.E8.m1.1.1.1.1.3.4.3.3" xref="A2.E8.m1.1.1.1.1.3.4.3.3a.cmml">imp</mtext></msub></mrow></mrow></mrow><mo mathsize="80%" id="A2.E8.m1.1.1.1.2" xref="A2.E8.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.E8.m1.1b"><apply id="A2.E8.m1.1.1.1.1.cmml" xref="A2.E8.m1.1.1.1"><eq id="A2.E8.m1.1.1.1.1.1.cmml" xref="A2.E8.m1.1.1.1.1.1"></eq><apply id="A2.E8.m1.1.1.1.1.2.cmml" xref="A2.E8.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="A2.E8.m1.1.1.1.1.2.1.cmml" xref="A2.E8.m1.1.1.1.1.2">subscript</csymbol><ci id="A2.E8.m1.1.1.1.1.2.2.cmml" xref="A2.E8.m1.1.1.1.1.2.2">𝐿</ci><ci id="A2.E8.m1.1.1.1.1.2.3a.cmml" xref="A2.E8.m1.1.1.1.1.2.3"><mtext mathsize="56%" id="A2.E8.m1.1.1.1.1.2.3.cmml" xref="A2.E8.m1.1.1.1.1.2.3">total</mtext></ci></apply><apply id="A2.E8.m1.1.1.1.1.3.cmml" xref="A2.E8.m1.1.1.1.1.3"><plus id="A2.E8.m1.1.1.1.1.3.1.cmml" xref="A2.E8.m1.1.1.1.1.3.1"></plus><apply id="A2.E8.m1.1.1.1.1.3.2.cmml" xref="A2.E8.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="A2.E8.m1.1.1.1.1.3.2.1.cmml" xref="A2.E8.m1.1.1.1.1.3.2">subscript</csymbol><ci id="A2.E8.m1.1.1.1.1.3.2.2.cmml" xref="A2.E8.m1.1.1.1.1.3.2.2">𝐿</ci><ci id="A2.E8.m1.1.1.1.1.3.2.3a.cmml" xref="A2.E8.m1.1.1.1.1.3.2.3"><mtext mathsize="56%" id="A2.E8.m1.1.1.1.1.3.2.3.cmml" xref="A2.E8.m1.1.1.1.1.3.2.3">rpn</mtext></ci></apply><apply id="A2.E8.m1.1.1.1.1.3.3.cmml" xref="A2.E8.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="A2.E8.m1.1.1.1.1.3.3.1.cmml" xref="A2.E8.m1.1.1.1.1.3.3">subscript</csymbol><ci id="A2.E8.m1.1.1.1.1.3.3.2.cmml" xref="A2.E8.m1.1.1.1.1.3.3.2">𝐿</ci><ci id="A2.E8.m1.1.1.1.1.3.3.3a.cmml" xref="A2.E8.m1.1.1.1.1.3.3.3"><mtext mathsize="56%" id="A2.E8.m1.1.1.1.1.3.3.3.cmml" xref="A2.E8.m1.1.1.1.1.3.3.3">exp</mtext></ci></apply><apply id="A2.E8.m1.1.1.1.1.3.4.cmml" xref="A2.E8.m1.1.1.1.1.3.4"><times id="A2.E8.m1.1.1.1.1.3.4.1.cmml" xref="A2.E8.m1.1.1.1.1.3.4.1"></times><apply id="A2.E8.m1.1.1.1.1.3.4.2.cmml" xref="A2.E8.m1.1.1.1.1.3.4.2"><csymbol cd="ambiguous" id="A2.E8.m1.1.1.1.1.3.4.2.1.cmml" xref="A2.E8.m1.1.1.1.1.3.4.2">subscript</csymbol><ci id="A2.E8.m1.1.1.1.1.3.4.2.2.cmml" xref="A2.E8.m1.1.1.1.1.3.4.2.2">𝜆</ci><ci id="A2.E8.m1.1.1.1.1.3.4.2.3a.cmml" xref="A2.E8.m1.1.1.1.1.3.4.2.3"><mtext mathsize="56%" id="A2.E8.m1.1.1.1.1.3.4.2.3.cmml" xref="A2.E8.m1.1.1.1.1.3.4.2.3">imp</mtext></ci></apply><apply id="A2.E8.m1.1.1.1.1.3.4.3.cmml" xref="A2.E8.m1.1.1.1.1.3.4.3"><csymbol cd="ambiguous" id="A2.E8.m1.1.1.1.1.3.4.3.1.cmml" xref="A2.E8.m1.1.1.1.1.3.4.3">subscript</csymbol><ci id="A2.E8.m1.1.1.1.1.3.4.3.2.cmml" xref="A2.E8.m1.1.1.1.1.3.4.3.2">𝐿</ci><ci id="A2.E8.m1.1.1.1.1.3.4.3.3a.cmml" xref="A2.E8.m1.1.1.1.1.3.4.3.3"><mtext mathsize="56%" id="A2.E8.m1.1.1.1.1.3.4.3.3.cmml" xref="A2.E8.m1.1.1.1.1.3.4.3.3">imp</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.E8.m1.1c">\displaystyle L_{\text{total}}=L_{\text{rpn}}+L_{\text{exp}}+\lambda_{\text{imp}}L_{\text{imp}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
<p id="A2.SS2.p3.4" class="ltx_p"><span id="A2.SS2.p3.4.1" class="ltx_text" style="font-size:80%;">where </span><math id="A2.SS2.p3.1.m1.1" class="ltx_Math" alttext="L_{\text{rpn}}" display="inline"><semantics id="A2.SS2.p3.1.m1.1a"><msub id="A2.SS2.p3.1.m1.1.1" xref="A2.SS2.p3.1.m1.1.1.cmml"><mi mathsize="80%" id="A2.SS2.p3.1.m1.1.1.2" xref="A2.SS2.p3.1.m1.1.1.2.cmml">L</mi><mtext mathsize="80%" id="A2.SS2.p3.1.m1.1.1.3" xref="A2.SS2.p3.1.m1.1.1.3a.cmml">rpn</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.SS2.p3.1.m1.1b"><apply id="A2.SS2.p3.1.m1.1.1.cmml" xref="A2.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="A2.SS2.p3.1.m1.1.1.1.cmml" xref="A2.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="A2.SS2.p3.1.m1.1.1.2.cmml" xref="A2.SS2.p3.1.m1.1.1.2">𝐿</ci><ci id="A2.SS2.p3.1.m1.1.1.3a.cmml" xref="A2.SS2.p3.1.m1.1.1.3"><mtext mathsize="56%" id="A2.SS2.p3.1.m1.1.1.3.cmml" xref="A2.SS2.p3.1.m1.1.1.3">rpn</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p3.1.m1.1c">L_{\text{rpn}}</annotation></semantics></math><span id="A2.SS2.p3.4.2" class="ltx_text" style="font-size:80%;"> is the overall loss used in CenterPoint-Voxel, </span><math id="A2.SS2.p3.2.m2.1" class="ltx_Math" alttext="L_{\text{exp}}" display="inline"><semantics id="A2.SS2.p3.2.m2.1a"><msub id="A2.SS2.p3.2.m2.1.1" xref="A2.SS2.p3.2.m2.1.1.cmml"><mi mathsize="80%" id="A2.SS2.p3.2.m2.1.1.2" xref="A2.SS2.p3.2.m2.1.1.2.cmml">L</mi><mtext mathsize="80%" id="A2.SS2.p3.2.m2.1.1.3" xref="A2.SS2.p3.2.m2.1.1.3a.cmml">exp</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.SS2.p3.2.m2.1b"><apply id="A2.SS2.p3.2.m2.1.1.cmml" xref="A2.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="A2.SS2.p3.2.m2.1.1.1.cmml" xref="A2.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="A2.SS2.p3.2.m2.1.1.2.cmml" xref="A2.SS2.p3.2.m2.1.1.2">𝐿</ci><ci id="A2.SS2.p3.2.m2.1.1.3a.cmml" xref="A2.SS2.p3.2.m2.1.1.3"><mtext mathsize="56%" id="A2.SS2.p3.2.m2.1.1.3.cmml" xref="A2.SS2.p3.2.m2.1.1.3">exp</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p3.2.m2.1c">L_{\text{exp}}</annotation></semantics></math><span id="A2.SS2.p3.4.3" class="ltx_text" style="font-size:80%;"> and </span><math id="A2.SS2.p3.3.m3.1" class="ltx_Math" alttext="L_{\text{imp}}" display="inline"><semantics id="A2.SS2.p3.3.m3.1a"><msub id="A2.SS2.p3.3.m3.1.1" xref="A2.SS2.p3.3.m3.1.1.cmml"><mi mathsize="80%" id="A2.SS2.p3.3.m3.1.1.2" xref="A2.SS2.p3.3.m3.1.1.2.cmml">L</mi><mtext mathsize="80%" id="A2.SS2.p3.3.m3.1.1.3" xref="A2.SS2.p3.3.m3.1.1.3a.cmml">imp</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.SS2.p3.3.m3.1b"><apply id="A2.SS2.p3.3.m3.1.1.cmml" xref="A2.SS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="A2.SS2.p3.3.m3.1.1.1.cmml" xref="A2.SS2.p3.3.m3.1.1">subscript</csymbol><ci id="A2.SS2.p3.3.m3.1.1.2.cmml" xref="A2.SS2.p3.3.m3.1.1.2">𝐿</ci><ci id="A2.SS2.p3.3.m3.1.1.3a.cmml" xref="A2.SS2.p3.3.m3.1.1.3"><mtext mathsize="56%" id="A2.SS2.p3.3.m3.1.1.3.cmml" xref="A2.SS2.p3.3.m3.1.1.3">imp</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p3.3.m3.1c">L_{\text{imp}}</annotation></semantics></math><span id="A2.SS2.p3.4.4" class="ltx_text" style="font-size:80%;"> are the losses for the explicit and implicit 2D semantic scene generation, respectively. We set </span><math id="A2.SS2.p3.4.m4.1" class="ltx_Math" alttext="\lambda_{\text{imp}}=5" display="inline"><semantics id="A2.SS2.p3.4.m4.1a"><mrow id="A2.SS2.p3.4.m4.1.1" xref="A2.SS2.p3.4.m4.1.1.cmml"><msub id="A2.SS2.p3.4.m4.1.1.2" xref="A2.SS2.p3.4.m4.1.1.2.cmml"><mi mathsize="80%" id="A2.SS2.p3.4.m4.1.1.2.2" xref="A2.SS2.p3.4.m4.1.1.2.2.cmml">λ</mi><mtext mathsize="80%" id="A2.SS2.p3.4.m4.1.1.2.3" xref="A2.SS2.p3.4.m4.1.1.2.3a.cmml">imp</mtext></msub><mo mathsize="80%" id="A2.SS2.p3.4.m4.1.1.1" xref="A2.SS2.p3.4.m4.1.1.1.cmml">=</mo><mn mathsize="80%" id="A2.SS2.p3.4.m4.1.1.3" xref="A2.SS2.p3.4.m4.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS2.p3.4.m4.1b"><apply id="A2.SS2.p3.4.m4.1.1.cmml" xref="A2.SS2.p3.4.m4.1.1"><eq id="A2.SS2.p3.4.m4.1.1.1.cmml" xref="A2.SS2.p3.4.m4.1.1.1"></eq><apply id="A2.SS2.p3.4.m4.1.1.2.cmml" xref="A2.SS2.p3.4.m4.1.1.2"><csymbol cd="ambiguous" id="A2.SS2.p3.4.m4.1.1.2.1.cmml" xref="A2.SS2.p3.4.m4.1.1.2">subscript</csymbol><ci id="A2.SS2.p3.4.m4.1.1.2.2.cmml" xref="A2.SS2.p3.4.m4.1.1.2.2">𝜆</ci><ci id="A2.SS2.p3.4.m4.1.1.2.3a.cmml" xref="A2.SS2.p3.4.m4.1.1.2.3"><mtext mathsize="56%" id="A2.SS2.p3.4.m4.1.1.2.3.cmml" xref="A2.SS2.p3.4.m4.1.1.2.3">imp</mtext></ci></apply><cn type="integer" id="A2.SS2.p3.4.m4.1.1.3.cmml" xref="A2.SS2.p3.4.m4.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p3.4.m4.1c">\lambda_{\text{imp}}=5</annotation></semantics></math><span id="A2.SS2.p3.4.5" class="ltx_text" style="font-size:80%;">.</span></p>
</div>
</section>
</section>
</div>
</figure>
</section>
</div>
</figure>
</section>
</div>
</div>
</figure>
</div>
</div>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2304.01518" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2304.01519" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2304.01519">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2304.01519" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2304.01520" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 16:46:59 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
