<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification</title>
<!--Generated on Mon Oct  7 01:58:29 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Short-form Video Classification; Privileged Feature Distillation; Multi-Modal Classification" lang="en" name="keywords"/>
<base href="/html/2410.03038v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S1" title="In CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S2" title="In CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S2.SS1" title="In 2. Related Work ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Vision-Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S2.SS2" title="In 2. Related Work ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Privileged Features Distillation (PFD)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S2.SS3" title="In 2. Related Work ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Confidence-aware</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S3" title="In CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S3.SS1" title="In 3. Methodology ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Video Classification Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S3.SS2" title="In 3. Methodology ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Dense Feature enhanced Multimodal Model (DF-X-VLM)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S3.SS3" title="In 3. Methodology ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Privileged Feature Distillation (PFD) for Video Classification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S3.SS4" title="In 3. Methodology ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Confidence-aware Privileged Feature Distillation (CPFD)</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S3.SS4.SSS1" title="In 3.4. Confidence-aware Privileged Feature Distillation (CPFD) ‣ 3. Methodology ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span>Redesign PFD to Incorporate Teacher Confidence</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S3.SS4.SSS2" title="In 3.4. Confidence-aware Privileged Feature Distillation (CPFD) ‣ 3. Methodology ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span>Implementing CPFD</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S4" title="In CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S4.SS1" title="In 4. Experiments ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experiment Setting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S4.SS2" title="In 4. Experiments ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Offline Evaluation Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S4.SS2.SSS1" title="In 4.2. Offline Evaluation Results ‣ 4. Experiments ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Performance Comparison</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S4.SS2.SSS2" title="In 4.2. Offline Evaluation Results ‣ 4. Experiments ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Ablation Study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S4.SS2.SSS3" title="In 4.2. Offline Evaluation Results ‣ 4. Experiments ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>Computational Cost</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S4.SS3" title="In 4. Experiments ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Online Experiments</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S5" title="In CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jinghao Shi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0009-0003-0196-5111" title="ORCID identifier">0009-0003-0196-5111</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">ByteDance</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">San Jose</span><span class="ltx_text ltx_affiliation_state" id="id3.3.id3">CA</span><span class="ltx_text ltx_affiliation_country" id="id4.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:jinghao.shi@bytedance.com">jinghao.shi@bytedance.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiang Shen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-6457-0411" title="ORCID identifier">0000-0002-6457-0411</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id5.1.id1">ByteDance</span><span class="ltx_text ltx_affiliation_city" id="id6.2.id2">Bellevue</span><span class="ltx_text ltx_affiliation_state" id="id7.3.id3">WA</span><span class="ltx_text ltx_affiliation_country" id="id8.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:xiang.shen@bytedance.com">xiang.shen@bytedance.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kaili Zhao
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0009-0005-1572-2924" title="ORCID identifier">0009-0005-1572-2924</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id9.1.id1">ByteDance</span><span class="ltx_text ltx_affiliation_city" id="id10.2.id2">San Jose</span><span class="ltx_text ltx_affiliation_state" id="id11.3.id3">CA</span><span class="ltx_text ltx_affiliation_country" id="id12.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:kaili.zhao@bytedance.com">kaili.zhao@bytedance.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xuedong Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0009-0008-0527-8534" title="ORCID identifier">0009-0008-0527-8534</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">ByteDance</span><span class="ltx_text ltx_affiliation_city" id="id14.2.id2">San Jose</span><span class="ltx_text ltx_affiliation_state" id="id15.3.id3">CA</span><span class="ltx_text ltx_affiliation_country" id="id16.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:xuedong.wang@bytedance.com">xuedong.wang@bytedance.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vera Wen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0009-0000-7328-409X" title="ORCID identifier">0009-0000-7328-409X</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id17.1.id1">ByteDance</span><span class="ltx_text ltx_affiliation_city" id="id18.2.id2">San Jose</span><span class="ltx_text ltx_affiliation_state" id="id19.3.id3">CA</span><span class="ltx_text ltx_affiliation_country" id="id20.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:vera.wen@bytedance.com">vera.wen@bytedance.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zixuan Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0009-0001-5532-5932" title="ORCID identifier">0009-0001-5532-5932</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id21.1.id1">ByteDance</span><span class="ltx_text ltx_affiliation_city" id="id22.2.id2">San Jose</span><span class="ltx_text ltx_affiliation_state" id="id23.3.id3">CA</span><span class="ltx_text ltx_affiliation_country" id="id24.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:zixuan.wang1@bytedance.com">zixuan.wang1@bytedance.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yifan Wu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0009-0008-2798-8860" title="ORCID identifier">0009-0008-2798-8860</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id25.1.id1">ByteDance</span><span class="ltx_text ltx_affiliation_city" id="id26.2.id2">San Jose</span><span class="ltx_text ltx_affiliation_state" id="id27.3.id3">CA</span><span class="ltx_text ltx_affiliation_country" id="id28.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:yifan.wu@bytedance.com">yifan.wu@bytedance.com</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhixin Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0009-0004-3768-7325" title="ORCID identifier">0009-0004-3768-7325</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id29.1.id1">ByteDance</span><span class="ltx_text ltx_affiliation_city" id="id30.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id31.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:zhangzhixin.01@bytedance.com">zhangzhixin.01@bytedance.com</a>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id32.id1">Dense features, customized for different business scenarios, are essential in short video classification. However, their complexity, specific adaptation requirements, and high computational costs make them resource-intensive and less accessible during online inference. Consequently, these dense features are categorized as ‘Privileged Dense Features’.
Meanwhile, end-to-end multi-modal models have shown promising results in numerous computer vision tasks.
In industrial applications, prioritizing end-to-end multi-modal features, can enhance efficiency but often leads to the loss of valuable information from historical privileged dense features.
To integrate both features while maintaining efficiency and manageable resource costs, we present Confidence-aware Privileged Feature Distillation (CPFD), which empowers features of an end-to-end multi-modal model by adaptively distilling privileged features during training.
Unlike existing privileged feature distillation (PFD) methods, which apply uniform weights to all instances during distillation, potentially causing unstable performance across different business scenarios and a notable performance gap between teacher model (Dense Feature enhanced multimodal-model DF-X-VLM) and student model (multimodal-model only X-VLM), our CPFD leverages confidence scores derived from the teacher model to adaptively mitigate the performance variance with the student model.
We conducted extensive offline experiments on five diverse tasks demonstrating that CPFD improves the video classification F1 score by 6.76% compared with end-to-end multimodal-model (X-VLM) and by 2.31% with vanilla PFD on-average. And it reduces the performance gap by 84.6% and achieves results comparable to teacher model DF-X-VLM. The effectiveness of CPFD is further substantiated by online experiments, and our framework has been deployed in production systems for over a dozen models.</p>
</div>
<div class="ltx_keywords">Short-form Video Classification; Privileged Feature Distillation; Multi-Modal Classification
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Proceedings of the 33rd ACM International Conference on Information and Knowledge Management; October 21–25, 2024; Boise, ID, USA.</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_booktitle" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>Proceedings of the 33rd ACM International Conference on Information and Knowledge Management (CIKM ’24), October 21–25, 2024, Boise, ID, USA</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>979-8-4007-0436-9/24/10</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3627673.3680045</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Machine learning</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id8"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Data mining</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Recent years have witnessed the rapid expansion of short video platforms such as TikTok, Youtube Shorts, and Reels. And short video classification is an essential task for short video platforms with a wide range of applications. This task involves tagging videos based on predefined taxonomies, providing insights for user research and being utilized in recommender systems. Additionally, video classification models are instrumental in content moderation, helping to detect inappropriate content. For instance, Qi et al. <cite class="ltx_cite ltx_citemacro_citep">(Qi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib13" title="">2023</a>)</cite> curated a dataset from multiple short video platforms focusing on fake news and developed a multi-modal fake news detection model. Das et al. <cite class="ltx_cite ltx_citemacro_citep">(Das et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib3" title="">2023</a>)</cite> created a multi-modal dataset as a benchmark for hate video classification. Binh et al. <cite class="ltx_cite ltx_citemacro_citep">(Binh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib2" title="">2022</a>)</cite> developed a fusion model to identify inappropriate videos for young children using video titles and metadata.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Video classification predicts video category based on a predefined taxonomy and take a sequence of video frames as input.
Generally, text information such as title and sticker provides critical information to improve video classification, thus will be integrated to vision features using a multi-modal classification architectures. The performance of the model usually relies on extensive pre-training of foundation models such as CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib14" title="">2021</a>)</cite>, BLIP <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib8" title="">2022</a>)</cite>, and X-VLM <cite class="ltx_cite ltx_citemacro_citep">(Zeng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib21" title="">2022</a>)</cite> and followed by fine-tuning with task-specific data.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Besides, dense features are also crucial for prediction tasks in industrial applications. These dense features come from different sources. Content-understanding models such as video classification models looking for the existence of specific semantics/objects or text classification models focusing on specific keys words can be one type of source. User interaction features such as likes, shares, and downloads is another type of source. In this work, we enhance the X-VLM model with these dense features to form a Dense Feature-enhanced X-VLM (DF-X-VLM) to empower the model.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">However, these dense features are usually inaccessible or prohibitively expensive to access for online services. For example, when predicting the probability of the video containing inappropriate information, the count of user reports only exists after the video accumulates some views, but not immediately available upon video creation. Additionally, maintaining these historical features developed in the previous iterations require significant resources, including memory space for storing the features as well as computational power for inference. Those features are only available during training time but not available during inference and thus we define them as ”privileged dense features”. On the contrary, features that are available in both training and inference periods such as video frames are defined as non-privileged features. The existence of privileged features could be a severe issue in online machine learning systems at scale.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Xu and Yang et al. <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib19" title="">2020</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib20" title="">2022</a>)</cite> proposed privileged feature distillation (PFD) to address the issue under the deep neural network (DNN) model framework involving dense privileged features and dense non-privileged features. The PFD approach distill a student model from a teacher model armed with privileged features. However, prior efforts have not integrated PFD into a multi-modal framework.
In response, we introduce PFD in multi-modal classification. We employ X-VLM as a student model to learn from DF-X-VLM as a teacher model to harness the information within the dense features without incurring the cost of using those features during online inference.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Conventional PFD methods treat the distillation process uniformly across all instances or adapt solely to the presence of privileged information. However, it is recognized that the performance discrepancies exist across samples due to variations in teacher behavior <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib24" title="">2020</a>)</cite> for general distillation scenario, indicating the distillation performance could be significantly influenced by teacher confidence and thus cause the inherent limitation for conventional previous methods. In the context of distilling multi-modal video classification, we also observe such obvious discrepancies. To be more specific, we found that the performance of student varies a lot in different business tasks especially when the teacher model is empowered with a large number of privileged features. The paper introduces Confidence-aware Privileged Features Distillation for multi-modal video classification, a refined approach to the traditional knowledge distillation process, where the student model not only learns from the output of the teacher model but also leverages additional insights derived from the teacher’s confidence levels. The core idea is to condition the distillation process on the teacher’s confidence in its prediction and incorporate uncertainty learning. The shift not only facilitates a deeper understanding and utilization of privileged information but also addresses the performance variances and dependencies associated with traditional distillation and privileged feature approaches.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">We summarize our contributions as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We introduce the PFD approach to utilize dense features in a multi-modal classification framework for the first time.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We propose the CPFD algorithm to further address the performance discrepancies introduced by traditional PFD approach.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We validate the effectiveness of the CPFD approach in production data and deploy the model in production.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Vision-Language Models</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Video classification has been traditionally viewed as a vision-only task which aims to predict the class of a given input sequence of video frames. Recent work utilized additional available text and audio information to construct a multi-modal classification framework and achieve better performance. The video classification model usually follows pre-training fine-tuning paradigm and benefits from pre-training vision-language foundation models. CLIP CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib14" title="">2021</a>)</cite> represents a groundbreaking milestone in vision-language pretraining. , achieving remarkable success in simultaneously understanding images and text via contrastive learning approach. Following this, there has been a surge in the development of Vision-Language Pretraining (VLP) models <cite class="ltx_cite ltx_citemacro_citep">(Dou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib4" title="">2022</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib17" title="">2021</a>)</cite>, which have notably achieved SoTA performance across a range of vision-language benchmarks, such as visual question answering and image captioning assignments.ALBEF <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib9" title="">2021</a>)</cite> incorporates contrastive loss and cross-modal attention to enhance the alignment of image and text representations, facilitating more grounded vision and language representation learning. BLIP <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib8" title="">2022</a>)</cite> proposed multimodal mixture of encoder-decoder and jointed pretrained by 3 tasks on data precessed by captioning and filtering. X-VLM <cite class="ltx_cite ltx_citemacro_citep">(Zeng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib21" title="">2022</a>)</cite> performed multi-grained vision language pre-training to align the visual concepts in the images and the associated text under multi-granularity.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Privileged Features Distillation (PFD)</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">A privileged feature is one that is available during model training, but not available at test/online time. Such features naturally arise in merchandised recommendation systems; for instance, “user clicked this item” as a feature is predictive of “user purchased this item” in the offline data, but is clearly not available during online serving. Privileged features distillation (PFD) refers to a natural idea: train a teacher model using all features (including privileged ones) and then use it to train a student model that does not use the privileged features<cite class="ltx_cite ltx_citemacro_citep">(Lopez-Paz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib10" title="">2015</a>)</cite><cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib20" title="">2022</a>)</cite>.
Lopez-Paz et al. <cite class="ltx_cite ltx_citemacro_citep">(Lopez-Paz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib10" title="">2015</a>)</cite> marries the concepts of distillation and privileged information and proposes the initial idea of PFD for the first time. Then Chen et al <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib19" title="">2020</a>)</cite> implemented PFD first time in e-commerce recommendation systems and showcasing how distilling knowledge from privileged features like dwell time can significantly enhance model performance in real-world scenarios. Shuo et al. <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib20" title="">2022</a>)</cite> also explored PFD within the learning-to-rank framework, highlighting its efficacy over traditional methods and analyzing the intricate dynamics between the predictive power of privileged features and student model performance. Similarly, Xiaoqiang et al<cite class="ltx_cite ltx_citemacro_citep">(Gui et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib5" title="">2024</a>)</cite> addresses the challenge of distilling ranking ability for CTR prediction by proposing a calibration-compatible listwise distillation approach. Besides, PFD has also been studied in other areas such as cardinality estimation <cite class="ltx_cite ltx_citemacro_citep">(Page et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib12" title="">2023</a>)</cite>, noisy-label <cite class="ltx_cite ltx_citemacro_citep">(Ortiz-Jimenez et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib11" title="">2023</a>)</cite> and entity-relation extraction <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib25" title="">2023</a>)</cite>
Inspired by all previous works, we also study PFD but in the domain of content moderation and feed quality. We study how privileged features can help content-understand and content-moderation models</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Confidence-aware</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Although there is no previous methodology which shares the exact same definition of Confidence-aware with this project, there are some similar ideas which inspire our method. In Knowledge Distillation, the confidence of a teacher is used to adaptively assign sample-wise reliability for each teacher prediction with the help of ground-truth labels <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib22" title="">2022</a>)</cite>. Similarly, the reliability of teacher predictions is used to identify prime samples for distillation and introduce adaptive weighting <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib24" title="">2020</a>)</cite>. In the noisy label domain, Co-teaching designed a two teacher paradigm and used confidence to select the possible clean labels, letting them teach each other<cite class="ltx_cite ltx_citemacro_citep">(Han et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib6" title="">2018</a>)</cite>. JoCoR also applied a similar approach to utilize the teacher’s confidence/loss to adjust the distillation process <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib18" title="">2020</a>)</cite>. Besides, work in imitation learning used the idea of confidence-aware, though not specific for the purpose of distillation<cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib23" title="">2021</a>)</cite>. And Meta-Weight-Net designed a weighting function mapping from training loss to sample weight, which is similar to CPFD where we map teacher loss to weight <cite class="ltx_cite ltx_citemacro_citep">(Shu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib16" title="">2019</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we first provide an overview of common short video classification methods, then explain the application of Privileged Feature Distillation (PFD) for short video classification, and introduce our proposed Confidence-aware Privileged Feature Distillation (CPFD), showing its advantages over conventional PFD.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Video Classification Overview</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Short video classification requires multimodal understanding. In addition to vision content, text information such as title, sticker, ocr and even audio text also plays an critical role in comprehensively understanding videos. In the industry, video classification methodologies are generally categorized into two types: End-to-end multimodal model and Fusion models.
End-to-end Multimodal models have been one of the most popular areas in machine learning. The proposal of famous works such as ALBEF, BLIP, X-VLM and so on has greatly boosted the application of Multimodal models in video classification model. In this paper, we employed X-VLM<cite class="ltx_cite ltx_citemacro_citep">(Zeng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib21" title="">2022</a>)</cite> as the model architecture because of its outstanding performance in short video classification.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">The other type is fusion models which build separate models for each modality to produce dense features with different focus and granularity, and then train fusion models on these dense features. These dense features may focus on understanding content in each modality or specific characteristics, even including non-content-related features such as ”video view” (VV) or ”video action rate.”
Below, we elaborate on the exploration of efficient learning for dense and multimodal features, leading to our final proposals: confidence-aware privilege features (CPFD).</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Dense Feature enhanced Multimodal Model (DF-X-VLM)</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">It is a natural idea to integrate the multimodal model with the dense feature to use their joint power and make the model address different challenges across multiple content spaces.
We introduce DF-X-VLM. It contains two branchs as illustrated by Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S3.F1" title="Figure 1 ‣ 3.2. Dense Feature enhanced Multimodal Model (DF-X-VLM) ‣ 3. Methodology ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_tag">1</span></a>: one branch utilizes dense features as inputs (DF-Branch) and the other processes sequence of images and text tokens (X-VLM Branch).The DF branch to designed to memorize the bad contents while the X-VLM branch aims to generalize it. In other words, The DF branch delineate a clear decision boundary of the easy cases and the larger capacity X-VLM branch address the remaining hard cases.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="557" id="S3.F1.g1" src="extracted/5905908/DF-X-VLM.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>This framework comprises two primary branches. The X-VLM branch represents an end-to-end multi-modal architecture that processes raw images and texts as inputs. The DF branch handles privileged dense features. These two branches are fused together to generate the final prediction.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Privileged Feature Distillation (PFD) for Video Classification</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">DF-X-VLM approach is an effective method to boost performance However, we still want to use privilege feature distillation (PFD) to train an end-to-end student X-VLM model from DF-X-VLM teacher model for two main reasons:</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="145" id="S3.F2.g1" src="extracted/5905908/PFD.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Illustration of General PFD, General Distillation and PFD for Video Classification.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p2">
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Model Performance</span>: The X-VLM and DF branches use different coordinate systems and thus the decision boundary is drawn across different content spaces. Unifying the coordinate system may boost performance.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Online Pipeline Effciency</span> Dense features not only lead to more resource consumption, but also result in long chained dependencies which harm online stability. Besides, an end-to-end X-VLM model without any dense feature dependencies makes more rapid scaling across different business scenarios.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S3.T1.1.1.1.1">Mean Teacher Loss @</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S3.T1.1.1.1.2">Neg Sample</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S3.T1.1.1.1.3">Pos Sample</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S3.T1.1.1.1.4">Overall</th>
<td class="ltx_td" id="S3.T1.1.1.1.5"></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.2.2.1">Student Correct</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.2.2.2">0.0057</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.2.2.3">0.8154</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.2.2.4">0.0686</td>
<td class="ltx_td" id="S3.T1.1.2.2.5"></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_b" id="S3.T1.1.3.3.1">Student Incorrect</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S3.T1.1.3.3.2">0.2298</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S3.T1.1.3.3.3">4.1737</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S3.T1.1.3.3.4">0.3806</td>
<td class="ltx_td" id="S3.T1.1.3.3.5"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Correlation table between Teacher Loss and Student Prediction: This table presents the mean teacher loss, segmented by the correctness of the student’s prediction across negative and positive samples.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">Privileged Features Distillation (PFD) follows a natural distillation strategy: train a “teacher” model using all features (including privileged ones) and then use it to train a “student” model that does not use the privileged features. A comparison between General PFD, General Distillation and PFD for video classification is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S3.F2" title="Figure 2 ‣ 3.3. Privileged Feature Distillation (PFD) for Video Classification ‣ 3. Methodology ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_tag">2</span></a>. A privileged feature is one that is available during model training, but not available at test/online time. In the context of multimodal video classification, we define all historical dense features that is expensive to use as ”privileged features”. See more detailed definition in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S1" title="1. Introduction ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_tag">1</span></a>. For actual implementation, we distill an X-VLM model (student) from a DF-X-VLM model (teacher):</p>
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1">The student model will be trained using a paramatrized combination of classification loss and distillation loss, which is a common practice in knowledge distillation:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{student}=(1-\alpha)*\mathcal{L}_{cls}+\alpha*\mathcal{L}_{distill}" class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><msub id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml">ℒ</mi><mrow id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.3.3.2" xref="S3.E1.m1.1.1.3.3.2.cmml">s</mi><mo id="S3.E1.m1.1.1.3.3.1" xref="S3.E1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.3.3" xref="S3.E1.m1.1.1.3.3.3.cmml">t</mi><mo id="S3.E1.m1.1.1.3.3.1a" xref="S3.E1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.3.4" xref="S3.E1.m1.1.1.3.3.4.cmml">u</mi><mo id="S3.E1.m1.1.1.3.3.1b" xref="S3.E1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.3.5" xref="S3.E1.m1.1.1.3.3.5.cmml">d</mi><mo id="S3.E1.m1.1.1.3.3.1c" xref="S3.E1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.3.6" xref="S3.E1.m1.1.1.3.3.6.cmml">e</mi><mo id="S3.E1.m1.1.1.3.3.1d" xref="S3.E1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.3.7" xref="S3.E1.m1.1.1.3.3.7.cmml">n</mi><mo id="S3.E1.m1.1.1.3.3.1e" xref="S3.E1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.3.3.8" xref="S3.E1.m1.1.1.3.3.8.cmml">t</mi></mrow></msub><mo id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mn id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">−</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml">α</mi></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E1.m1.1.1.1.1.2" rspace="0.222em" xref="S3.E1.m1.1.1.1.1.2.cmml">∗</mo><msub id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.3.2.cmml">ℒ</mi><mrow id="S3.E1.m1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.3.2" xref="S3.E1.m1.1.1.1.1.3.3.2.cmml">c</mi><mo id="S3.E1.m1.1.1.1.1.3.3.1" xref="S3.E1.m1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.1.3.3.3.cmml">l</mi><mo id="S3.E1.m1.1.1.1.1.3.3.1a" xref="S3.E1.m1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.3.3.4" xref="S3.E1.m1.1.1.1.1.3.3.4.cmml">s</mi></mrow></msub></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.2.cmml">+</mo><mrow id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.3.2.cmml">α</mi><mo id="S3.E1.m1.1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.E1.m1.1.1.1.3.1.cmml">∗</mo><msub id="S3.E1.m1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.1.3.3.2" xref="S3.E1.m1.1.1.1.3.3.2.cmml">ℒ</mi><mrow id="S3.E1.m1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.3.3.3.cmml"><mi id="S3.E1.m1.1.1.1.3.3.3.2" xref="S3.E1.m1.1.1.1.3.3.3.2.cmml">d</mi><mo id="S3.E1.m1.1.1.1.3.3.3.1" xref="S3.E1.m1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.3.3.3.3" xref="S3.E1.m1.1.1.1.3.3.3.3.cmml">i</mi><mo id="S3.E1.m1.1.1.1.3.3.3.1a" xref="S3.E1.m1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.3.3.3.4" xref="S3.E1.m1.1.1.1.3.3.3.4.cmml">s</mi><mo id="S3.E1.m1.1.1.1.3.3.3.1b" xref="S3.E1.m1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.3.3.3.5" xref="S3.E1.m1.1.1.1.3.3.3.5.cmml">t</mi><mo id="S3.E1.m1.1.1.1.3.3.3.1c" xref="S3.E1.m1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.3.3.3.6" xref="S3.E1.m1.1.1.1.3.3.3.6.cmml">i</mi><mo id="S3.E1.m1.1.1.1.3.3.3.1d" xref="S3.E1.m1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.3.3.3.7" xref="S3.E1.m1.1.1.1.3.3.3.7.cmml">l</mi><mo id="S3.E1.m1.1.1.1.3.3.3.1e" xref="S3.E1.m1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.3.3.3.8" xref="S3.E1.m1.1.1.1.3.3.3.8.cmml">l</mi></mrow></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"></eq><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2">ℒ</ci><apply id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3"><times id="S3.E1.m1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3.1"></times><ci id="S3.E1.m1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.2">𝑠</ci><ci id="S3.E1.m1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3">𝑡</ci><ci id="S3.E1.m1.1.1.3.3.4.cmml" xref="S3.E1.m1.1.1.3.3.4">𝑢</ci><ci id="S3.E1.m1.1.1.3.3.5.cmml" xref="S3.E1.m1.1.1.3.3.5">𝑑</ci><ci id="S3.E1.m1.1.1.3.3.6.cmml" xref="S3.E1.m1.1.1.3.3.6">𝑒</ci><ci id="S3.E1.m1.1.1.3.3.7.cmml" xref="S3.E1.m1.1.1.3.3.7">𝑛</ci><ci id="S3.E1.m1.1.1.3.3.8.cmml" xref="S3.E1.m1.1.1.3.3.8">𝑡</ci></apply></apply><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><plus id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.2"></plus><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"></times><apply id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><minus id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1"></minus><cn id="S3.E1.m1.1.1.1.1.1.1.1.2.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.1.1.1.2">1</cn><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3">𝛼</ci></apply><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2">ℒ</ci><apply id="S3.E1.m1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3"><times id="S3.E1.m1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.1"></times><ci id="S3.E1.m1.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2">𝑐</ci><ci id="S3.E1.m1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3">𝑙</ci><ci id="S3.E1.m1.1.1.1.1.3.3.4.cmml" xref="S3.E1.m1.1.1.1.1.3.3.4">𝑠</ci></apply></apply></apply><apply id="S3.E1.m1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.3"><times id="S3.E1.m1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.3.1"></times><ci id="S3.E1.m1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.3.2">𝛼</ci><apply id="S3.E1.m1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.3.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.1.3.3.2">ℒ</ci><apply id="S3.E1.m1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.1.3.3.3"><times id="S3.E1.m1.1.1.1.3.3.3.1.cmml" xref="S3.E1.m1.1.1.1.3.3.3.1"></times><ci id="S3.E1.m1.1.1.1.3.3.3.2.cmml" xref="S3.E1.m1.1.1.1.3.3.3.2">𝑑</ci><ci id="S3.E1.m1.1.1.1.3.3.3.3.cmml" xref="S3.E1.m1.1.1.1.3.3.3.3">𝑖</ci><ci id="S3.E1.m1.1.1.1.3.3.3.4.cmml" xref="S3.E1.m1.1.1.1.3.3.3.4">𝑠</ci><ci id="S3.E1.m1.1.1.1.3.3.3.5.cmml" xref="S3.E1.m1.1.1.1.3.3.3.5">𝑡</ci><ci id="S3.E1.m1.1.1.1.3.3.3.6.cmml" xref="S3.E1.m1.1.1.1.3.3.3.6">𝑖</ci><ci id="S3.E1.m1.1.1.1.3.3.3.7.cmml" xref="S3.E1.m1.1.1.1.3.3.3.7">𝑙</ci><ci id="S3.E1.m1.1.1.1.3.3.3.8.cmml" xref="S3.E1.m1.1.1.1.3.3.3.8">𝑙</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\mathcal{L}_{student}=(1-\alpha)*\mathcal{L}_{cls}+\alpha*\mathcal{L}_{distill}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_s italic_t italic_u italic_d italic_e italic_n italic_t end_POSTSUBSCRIPT = ( 1 - italic_α ) ∗ caligraphic_L start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT + italic_α ∗ caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t italic_i italic_l italic_l end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{cls}=H(q_{s},y)=-\sum_{i}y_{i}\log(q_{s,i})" class="ltx_Math" display="block" id="S3.E2.m1.6"><semantics id="S3.E2.m1.6a"><mrow id="S3.E2.m1.6.6" xref="S3.E2.m1.6.6.cmml"><msub id="S3.E2.m1.6.6.4" xref="S3.E2.m1.6.6.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.6.6.4.2" xref="S3.E2.m1.6.6.4.2.cmml">ℒ</mi><mrow id="S3.E2.m1.6.6.4.3" xref="S3.E2.m1.6.6.4.3.cmml"><mi id="S3.E2.m1.6.6.4.3.2" xref="S3.E2.m1.6.6.4.3.2.cmml">c</mi><mo id="S3.E2.m1.6.6.4.3.1" xref="S3.E2.m1.6.6.4.3.1.cmml">⁢</mo><mi id="S3.E2.m1.6.6.4.3.3" xref="S3.E2.m1.6.6.4.3.3.cmml">l</mi><mo id="S3.E2.m1.6.6.4.3.1a" xref="S3.E2.m1.6.6.4.3.1.cmml">⁢</mo><mi id="S3.E2.m1.6.6.4.3.4" xref="S3.E2.m1.6.6.4.3.4.cmml">s</mi></mrow></msub><mo id="S3.E2.m1.6.6.5" xref="S3.E2.m1.6.6.5.cmml">=</mo><mrow id="S3.E2.m1.5.5.1" xref="S3.E2.m1.5.5.1.cmml"><mi id="S3.E2.m1.5.5.1.3" xref="S3.E2.m1.5.5.1.3.cmml">H</mi><mo id="S3.E2.m1.5.5.1.2" xref="S3.E2.m1.5.5.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.5.5.1.1.1" xref="S3.E2.m1.5.5.1.1.2.cmml"><mo id="S3.E2.m1.5.5.1.1.1.2" stretchy="false" xref="S3.E2.m1.5.5.1.1.2.cmml">(</mo><msub id="S3.E2.m1.5.5.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.cmml"><mi id="S3.E2.m1.5.5.1.1.1.1.2" xref="S3.E2.m1.5.5.1.1.1.1.2.cmml">q</mi><mi id="S3.E2.m1.5.5.1.1.1.1.3" xref="S3.E2.m1.5.5.1.1.1.1.3.cmml">s</mi></msub><mo id="S3.E2.m1.5.5.1.1.1.3" xref="S3.E2.m1.5.5.1.1.2.cmml">,</mo><mi id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">y</mi><mo id="S3.E2.m1.5.5.1.1.1.4" stretchy="false" xref="S3.E2.m1.5.5.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.6.6.6" xref="S3.E2.m1.6.6.6.cmml">=</mo><mrow id="S3.E2.m1.6.6.2" xref="S3.E2.m1.6.6.2.cmml"><mo id="S3.E2.m1.6.6.2a" xref="S3.E2.m1.6.6.2.cmml">−</mo><mrow id="S3.E2.m1.6.6.2.1" xref="S3.E2.m1.6.6.2.1.cmml"><munder id="S3.E2.m1.6.6.2.1.2" xref="S3.E2.m1.6.6.2.1.2.cmml"><mo id="S3.E2.m1.6.6.2.1.2.2" movablelimits="false" xref="S3.E2.m1.6.6.2.1.2.2.cmml">∑</mo><mi id="S3.E2.m1.6.6.2.1.2.3" xref="S3.E2.m1.6.6.2.1.2.3.cmml">i</mi></munder><mrow id="S3.E2.m1.6.6.2.1.1" xref="S3.E2.m1.6.6.2.1.1.cmml"><msub id="S3.E2.m1.6.6.2.1.1.3" xref="S3.E2.m1.6.6.2.1.1.3.cmml"><mi id="S3.E2.m1.6.6.2.1.1.3.2" xref="S3.E2.m1.6.6.2.1.1.3.2.cmml">y</mi><mi id="S3.E2.m1.6.6.2.1.1.3.3" xref="S3.E2.m1.6.6.2.1.1.3.3.cmml">i</mi></msub><mo id="S3.E2.m1.6.6.2.1.1.2" lspace="0.167em" xref="S3.E2.m1.6.6.2.1.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.6.6.2.1.1.1.1" xref="S3.E2.m1.6.6.2.1.1.1.2.cmml"><mi id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml">log</mi><mo id="S3.E2.m1.6.6.2.1.1.1.1a" xref="S3.E2.m1.6.6.2.1.1.1.2.cmml">⁡</mo><mrow id="S3.E2.m1.6.6.2.1.1.1.1.1" xref="S3.E2.m1.6.6.2.1.1.1.2.cmml"><mo id="S3.E2.m1.6.6.2.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.6.6.2.1.1.1.2.cmml">(</mo><msub id="S3.E2.m1.6.6.2.1.1.1.1.1.1" xref="S3.E2.m1.6.6.2.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.6.6.2.1.1.1.1.1.1.2" xref="S3.E2.m1.6.6.2.1.1.1.1.1.1.2.cmml">q</mi><mrow id="S3.E2.m1.2.2.2.4" xref="S3.E2.m1.2.2.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">s</mi><mo id="S3.E2.m1.2.2.2.4.1" xref="S3.E2.m1.2.2.2.3.cmml">,</mo><mi id="S3.E2.m1.2.2.2.2" xref="S3.E2.m1.2.2.2.2.cmml">i</mi></mrow></msub><mo id="S3.E2.m1.6.6.2.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.6.6.2.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.6b"><apply id="S3.E2.m1.6.6.cmml" xref="S3.E2.m1.6.6"><and id="S3.E2.m1.6.6a.cmml" xref="S3.E2.m1.6.6"></and><apply id="S3.E2.m1.6.6b.cmml" xref="S3.E2.m1.6.6"><eq id="S3.E2.m1.6.6.5.cmml" xref="S3.E2.m1.6.6.5"></eq><apply id="S3.E2.m1.6.6.4.cmml" xref="S3.E2.m1.6.6.4"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.4.1.cmml" xref="S3.E2.m1.6.6.4">subscript</csymbol><ci id="S3.E2.m1.6.6.4.2.cmml" xref="S3.E2.m1.6.6.4.2">ℒ</ci><apply id="S3.E2.m1.6.6.4.3.cmml" xref="S3.E2.m1.6.6.4.3"><times id="S3.E2.m1.6.6.4.3.1.cmml" xref="S3.E2.m1.6.6.4.3.1"></times><ci id="S3.E2.m1.6.6.4.3.2.cmml" xref="S3.E2.m1.6.6.4.3.2">𝑐</ci><ci id="S3.E2.m1.6.6.4.3.3.cmml" xref="S3.E2.m1.6.6.4.3.3">𝑙</ci><ci id="S3.E2.m1.6.6.4.3.4.cmml" xref="S3.E2.m1.6.6.4.3.4">𝑠</ci></apply></apply><apply id="S3.E2.m1.5.5.1.cmml" xref="S3.E2.m1.5.5.1"><times id="S3.E2.m1.5.5.1.2.cmml" xref="S3.E2.m1.5.5.1.2"></times><ci id="S3.E2.m1.5.5.1.3.cmml" xref="S3.E2.m1.5.5.1.3">𝐻</ci><interval closure="open" id="S3.E2.m1.5.5.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.1"><apply id="S3.E2.m1.5.5.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.5.5.1.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.2">𝑞</ci><ci id="S3.E2.m1.5.5.1.1.1.1.3.cmml" xref="S3.E2.m1.5.5.1.1.1.1.3">𝑠</ci></apply><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">𝑦</ci></interval></apply></apply><apply id="S3.E2.m1.6.6c.cmml" xref="S3.E2.m1.6.6"><eq id="S3.E2.m1.6.6.6.cmml" xref="S3.E2.m1.6.6.6"></eq><share href="https://arxiv.org/html/2410.03038v2#S3.E2.m1.5.5.1.cmml" id="S3.E2.m1.6.6d.cmml" xref="S3.E2.m1.6.6"></share><apply id="S3.E2.m1.6.6.2.cmml" xref="S3.E2.m1.6.6.2"><minus id="S3.E2.m1.6.6.2.2.cmml" xref="S3.E2.m1.6.6.2"></minus><apply id="S3.E2.m1.6.6.2.1.cmml" xref="S3.E2.m1.6.6.2.1"><apply id="S3.E2.m1.6.6.2.1.2.cmml" xref="S3.E2.m1.6.6.2.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.2.1.2.1.cmml" xref="S3.E2.m1.6.6.2.1.2">subscript</csymbol><sum id="S3.E2.m1.6.6.2.1.2.2.cmml" xref="S3.E2.m1.6.6.2.1.2.2"></sum><ci id="S3.E2.m1.6.6.2.1.2.3.cmml" xref="S3.E2.m1.6.6.2.1.2.3">𝑖</ci></apply><apply id="S3.E2.m1.6.6.2.1.1.cmml" xref="S3.E2.m1.6.6.2.1.1"><times id="S3.E2.m1.6.6.2.1.1.2.cmml" xref="S3.E2.m1.6.6.2.1.1.2"></times><apply id="S3.E2.m1.6.6.2.1.1.3.cmml" xref="S3.E2.m1.6.6.2.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.2.1.1.3.1.cmml" xref="S3.E2.m1.6.6.2.1.1.3">subscript</csymbol><ci id="S3.E2.m1.6.6.2.1.1.3.2.cmml" xref="S3.E2.m1.6.6.2.1.1.3.2">𝑦</ci><ci id="S3.E2.m1.6.6.2.1.1.3.3.cmml" xref="S3.E2.m1.6.6.2.1.1.3.3">𝑖</ci></apply><apply id="S3.E2.m1.6.6.2.1.1.1.2.cmml" xref="S3.E2.m1.6.6.2.1.1.1.1"><log id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4"></log><apply id="S3.E2.m1.6.6.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.6.6.2.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.6.6.2.1.1.1.1.1.1.2">𝑞</ci><list id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.4"><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">𝑠</ci><ci id="S3.E2.m1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2">𝑖</ci></list></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.6c">\mathcal{L}_{cls}=H(q_{s},y)=-\sum_{i}y_{i}\log(q_{s,i})</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.6d">caligraphic_L start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT = italic_H ( italic_q start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , italic_y ) = - ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_log ( italic_q start_POSTSUBSCRIPT italic_s , italic_i end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{distill}=D_{\text{KL}}(q_{t}||q_{s})=T^{2}\sum_{i}q_{t,i}(T)\log%
\left(\frac{q_{t,i}(T)}{q_{s,i}(T)}\right)" class="ltx_math_unparsed" display="block" id="S3.E3.m1.8"><semantics id="S3.E3.m1.8a"><mrow id="S3.E3.m1.8b"><msub id="S3.E3.m1.8.9"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.8.9.2">ℒ</mi><mrow id="S3.E3.m1.8.9.3"><mi id="S3.E3.m1.8.9.3.2">d</mi><mo id="S3.E3.m1.8.9.3.1">⁢</mo><mi id="S3.E3.m1.8.9.3.3">i</mi><mo id="S3.E3.m1.8.9.3.1a">⁢</mo><mi id="S3.E3.m1.8.9.3.4">s</mi><mo id="S3.E3.m1.8.9.3.1b">⁢</mo><mi id="S3.E3.m1.8.9.3.5">t</mi><mo id="S3.E3.m1.8.9.3.1c">⁢</mo><mi id="S3.E3.m1.8.9.3.6">i</mi><mo id="S3.E3.m1.8.9.3.1d">⁢</mo><mi id="S3.E3.m1.8.9.3.7">l</mi><mo id="S3.E3.m1.8.9.3.1e">⁢</mo><mi id="S3.E3.m1.8.9.3.8">l</mi></mrow></msub><mo id="S3.E3.m1.8.10">=</mo><msub id="S3.E3.m1.8.11"><mi id="S3.E3.m1.8.11.2">D</mi><mtext id="S3.E3.m1.8.11.3">KL</mtext></msub><mrow id="S3.E3.m1.8.12"><mo id="S3.E3.m1.8.12.1" stretchy="false">(</mo><msub id="S3.E3.m1.8.12.2"><mi id="S3.E3.m1.8.12.2.2">q</mi><mi id="S3.E3.m1.8.12.2.3">t</mi></msub><mo fence="false" id="S3.E3.m1.8.12.3" rspace="0.167em" stretchy="false">|</mo><mo fence="false" id="S3.E3.m1.8.12.4" rspace="0.167em" stretchy="false">|</mo><msub id="S3.E3.m1.8.12.5"><mi id="S3.E3.m1.8.12.5.2">q</mi><mi id="S3.E3.m1.8.12.5.3">s</mi></msub><mo id="S3.E3.m1.8.12.6" stretchy="false">)</mo></mrow><mo id="S3.E3.m1.8.13">=</mo><msup id="S3.E3.m1.8.14"><mi id="S3.E3.m1.8.14.2">T</mi><mn id="S3.E3.m1.8.14.3">2</mn></msup><munder id="S3.E3.m1.8.15"><mo id="S3.E3.m1.8.15.2" movablelimits="false">∑</mo><mi id="S3.E3.m1.8.15.3">i</mi></munder><msub id="S3.E3.m1.8.16"><mi id="S3.E3.m1.8.16.2">q</mi><mrow id="S3.E3.m1.2.2.2.4"><mi id="S3.E3.m1.1.1.1.1">t</mi><mo id="S3.E3.m1.2.2.2.4.1">,</mo><mi id="S3.E3.m1.2.2.2.2">i</mi></mrow></msub><mrow id="S3.E3.m1.8.17"><mo id="S3.E3.m1.8.17.1" stretchy="false">(</mo><mi id="S3.E3.m1.8.17.2">T</mi><mo id="S3.E3.m1.8.17.3" rspace="0.167em" stretchy="false">)</mo></mrow><mi id="S3.E3.m1.8.18">log</mi><mrow id="S3.E3.m1.8.19"><mo id="S3.E3.m1.8.19.1">(</mo><mfrac id="S3.E3.m1.8.8"><mrow id="S3.E3.m1.5.5.3"><msub id="S3.E3.m1.5.5.3.5"><mi id="S3.E3.m1.5.5.3.5.2">q</mi><mrow id="S3.E3.m1.4.4.2.2.2.4"><mi id="S3.E3.m1.3.3.1.1.1.1">t</mi><mo id="S3.E3.m1.4.4.2.2.2.4.1">,</mo><mi id="S3.E3.m1.4.4.2.2.2.2">i</mi></mrow></msub><mo id="S3.E3.m1.5.5.3.4">⁢</mo><mrow id="S3.E3.m1.5.5.3.6.2"><mo id="S3.E3.m1.5.5.3.6.2.1" stretchy="false">(</mo><mi id="S3.E3.m1.5.5.3.3">T</mi><mo id="S3.E3.m1.5.5.3.6.2.2" stretchy="false">)</mo></mrow></mrow><mrow id="S3.E3.m1.8.8.6"><msub id="S3.E3.m1.8.8.6.5"><mi id="S3.E3.m1.8.8.6.5.2">q</mi><mrow id="S3.E3.m1.7.7.5.2.2.4"><mi id="S3.E3.m1.6.6.4.1.1.1">s</mi><mo id="S3.E3.m1.7.7.5.2.2.4.1">,</mo><mi id="S3.E3.m1.7.7.5.2.2.2">i</mi></mrow></msub><mo id="S3.E3.m1.8.8.6.4">⁢</mo><mrow id="S3.E3.m1.8.8.6.6.2"><mo id="S3.E3.m1.8.8.6.6.2.1" stretchy="false">(</mo><mi id="S3.E3.m1.8.8.6.3">T</mi><mo id="S3.E3.m1.8.8.6.6.2.2" stretchy="false">)</mo></mrow></mrow></mfrac><mo id="S3.E3.m1.8.19.2">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.E3.m1.8c">\mathcal{L}_{distill}=D_{\text{KL}}(q_{t}||q_{s})=T^{2}\sum_{i}q_{t,i}(T)\log%
\left(\frac{q_{t,i}(T)}{q_{s,i}(T)}\right)</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.8d">caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t italic_i italic_l italic_l end_POSTSUBSCRIPT = italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | | italic_q start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) = italic_T start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT italic_t , italic_i end_POSTSUBSCRIPT ( italic_T ) roman_log ( divide start_ARG italic_q start_POSTSUBSCRIPT italic_t , italic_i end_POSTSUBSCRIPT ( italic_T ) end_ARG start_ARG italic_q start_POSTSUBSCRIPT italic_s , italic_i end_POSTSUBSCRIPT ( italic_T ) end_ARG )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1">Where:</p>
<ul class="ltx_itemize" id="S3.I2.i2.I1">
<li class="ltx_item" id="S3.I2.i2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S3.I2.i2.I1.i1.1.1.1">–</span></span>
<div class="ltx_para" id="S3.I2.i2.I1.i1.p1">
<p class="ltx_p" id="S3.I2.i2.I1.i1.p1.1"><math alttext="\alpha" class="ltx_Math" display="inline" id="S3.I2.i2.I1.i1.p1.1.m1.1"><semantics id="S3.I2.i2.I1.i1.p1.1.m1.1a"><mi id="S3.I2.i2.I1.i1.p1.1.m1.1.1" xref="S3.I2.i2.I1.i1.p1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i2.I1.i1.p1.1.m1.1b"><ci id="S3.I2.i2.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I2.i2.I1.i1.p1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.I1.i1.p1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.I1.i1.p1.1.m1.1d">italic_α</annotation></semantics></math> represents the scaling factor balancing two losses</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S3.I2.i2.I1.i2.1.1.1">–</span></span>
<div class="ltx_para" id="S3.I2.i2.I1.i2.p1">
<p class="ltx_p" id="S3.I2.i2.I1.i2.p1.1"><math alttext="T" class="ltx_Math" display="inline" id="S3.I2.i2.I1.i2.p1.1.m1.1"><semantics id="S3.I2.i2.I1.i2.p1.1.m1.1a"><mi id="S3.I2.i2.I1.i2.p1.1.m1.1.1" xref="S3.I2.i2.I1.i2.p1.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i2.I1.i2.p1.1.m1.1b"><ci id="S3.I2.i2.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I2.i2.I1.i2.p1.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.I1.i2.p1.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.I1.i2.p1.1.m1.1d">italic_T</annotation></semantics></math> is the temperature parameter</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S3.I2.i2.I1.i3.1.1.1">–</span></span>
<div class="ltx_para" id="S3.I2.i2.I1.i3.p1">
<p class="ltx_p" id="S3.I2.i2.I1.i3.p1.1"><math alttext="q_{s}" class="ltx_Math" display="inline" id="S3.I2.i2.I1.i3.p1.1.m1.1"><semantics id="S3.I2.i2.I1.i3.p1.1.m1.1a"><msub id="S3.I2.i2.I1.i3.p1.1.m1.1.1" xref="S3.I2.i2.I1.i3.p1.1.m1.1.1.cmml"><mi id="S3.I2.i2.I1.i3.p1.1.m1.1.1.2" xref="S3.I2.i2.I1.i3.p1.1.m1.1.1.2.cmml">q</mi><mi id="S3.I2.i2.I1.i3.p1.1.m1.1.1.3" xref="S3.I2.i2.I1.i3.p1.1.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i2.I1.i3.p1.1.m1.1b"><apply id="S3.I2.i2.I1.i3.p1.1.m1.1.1.cmml" xref="S3.I2.i2.I1.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I2.i2.I1.i3.p1.1.m1.1.1.1.cmml" xref="S3.I2.i2.I1.i3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I2.i2.I1.i3.p1.1.m1.1.1.2.cmml" xref="S3.I2.i2.I1.i3.p1.1.m1.1.1.2">𝑞</ci><ci id="S3.I2.i2.I1.i3.p1.1.m1.1.1.3.cmml" xref="S3.I2.i2.I1.i3.p1.1.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.I1.i3.p1.1.m1.1c">q_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.I1.i3.p1.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math> is the predicted probability distribution generated by the student.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S3.I2.i2.I1.i4.1.1.1">–</span></span>
<div class="ltx_para" id="S3.I2.i2.I1.i4.p1">
<p class="ltx_p" id="S3.I2.i2.I1.i4.p1.1"><math alttext="q_{t}" class="ltx_Math" display="inline" id="S3.I2.i2.I1.i4.p1.1.m1.1"><semantics id="S3.I2.i2.I1.i4.p1.1.m1.1a"><msub id="S3.I2.i2.I1.i4.p1.1.m1.1.1" xref="S3.I2.i2.I1.i4.p1.1.m1.1.1.cmml"><mi id="S3.I2.i2.I1.i4.p1.1.m1.1.1.2" xref="S3.I2.i2.I1.i4.p1.1.m1.1.1.2.cmml">q</mi><mi id="S3.I2.i2.I1.i4.p1.1.m1.1.1.3" xref="S3.I2.i2.I1.i4.p1.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i2.I1.i4.p1.1.m1.1b"><apply id="S3.I2.i2.I1.i4.p1.1.m1.1.1.cmml" xref="S3.I2.i2.I1.i4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I2.i2.I1.i4.p1.1.m1.1.1.1.cmml" xref="S3.I2.i2.I1.i4.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I2.i2.I1.i4.p1.1.m1.1.1.2.cmml" xref="S3.I2.i2.I1.i4.p1.1.m1.1.1.2">𝑞</ci><ci id="S3.I2.i2.I1.i4.p1.1.m1.1.1.3.cmml" xref="S3.I2.i2.I1.i4.p1.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.I1.i4.p1.1.m1.1c">q_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.I1.i4.p1.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is the soft target probability distribution generated by the teacher.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S3.I2.i2.I1.i5.1.1.1">–</span></span>
<div class="ltx_para" id="S3.I2.i2.I1.i5.p1">
<p class="ltx_p" id="S3.I2.i2.I1.i5.p1.1"><math alttext="y" class="ltx_Math" display="inline" id="S3.I2.i2.I1.i5.p1.1.m1.1"><semantics id="S3.I2.i2.I1.i5.p1.1.m1.1a"><mi id="S3.I2.i2.I1.i5.p1.1.m1.1.1" xref="S3.I2.i2.I1.i5.p1.1.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i2.I1.i5.p1.1.m1.1b"><ci id="S3.I2.i2.I1.i5.p1.1.m1.1.1.cmml" xref="S3.I2.i2.I1.i5.p1.1.m1.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.I1.i5.p1.1.m1.1c">y</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.I1.i5.p1.1.m1.1d">italic_y</annotation></semantics></math> is the hard label.</p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">Note that though we adopt X-VLM and DF-X-VLM as our use case, the PFD framework is suitable for any model architecture. With that said, the teacher and student can be any model architecture and are totally independent of one another.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Confidence-aware Privileged Feature Distillation (CPFD)</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Though PFD proves to be an effective method, there are still performance gaps between teacher and student, especially when the teacher model becomes more complicated. Below we analyze the causes of those performance gaps and propose a refined approach to address the gaps.</p>
<ul class="ltx_itemize" id="S3.I3">
<li class="ltx_item" id="S3.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i1.p1">
<p class="ltx_p" id="S3.I3.i1.p1.1">In traditional distillation framework, the student learning is often uniformly influenced by the teacher’s outputs across all examples. It does not take into account the varying levels of certainty or confidence the teacher possesses in its own predictions.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i2.p1">
<p class="ltx_p" id="S3.I3.i2.p1.1">To analyze how the confidence of the teacher influences the student’s behavior, we quantified the teacher’s confidence through the distribution of teacher loss associated with different student predictions (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S3.F3" title="Figure 3 ‣ 3.4. Confidence-aware Privileged Feature Distillation (CPFD) ‣ 3. Methodology ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_tag">3</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S3.T1" title="Table 1 ‣ 3.3. Privileged Feature Distillation (PFD) for Video Classification ‣ 3. Methodology ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_tag">1</span></a>). They highlights the varying impact of prediction accuracy on teacher loss, illustrating significant differences between correct and incorrect predictions by the student. We can conclude that the student prone to make mistakes when the teacher loss is high. And it is extremely obvious for false negative cases.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="427" id="S3.F3.g1" src="extracted/5905908/loss_correlation.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Correlation plot between Teacher Loss and Student Prediction: This table presents the teacher loss distribution, segmented by the correctness of the student’s prediction across negative and positive samples.</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="227" id="S3.F4.g1" src="extracted/5905908/CPFD.png" width="509"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Comparison between CPFD and Vanilla PFD. Vanilla PFD applies a uniform alpha value across all samples. In contrast, CPFD adaptively adjusts the alpha parameter based on the teacher’s loss value for each sample.</figcaption>
</figure>
<section class="ltx_subsubsection" id="S3.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1. </span>Redesign PFD to Incorporate Teacher Confidence</h4>
<div class="ltx_para" id="S3.SS4.SSS1.p1">
<p class="ltx_p" id="S3.SS4.SSS1.p1.3">Based on the above analysis, we propose Confidence-aware Privileged Feature Distillation (CPFD) as shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S3.F4" title="Figure 4 ‣ 3.4. Confidence-aware Privileged Feature Distillation (CPFD) ‣ 3. Methodology ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_tag">4</span></a>. CPFD is a refined approach to the traditional knowledge distillation process, where the student model not only learns from the output of the teacher model but also leverages additional insights derived from the teacher’s confidence levels. The core idea behind this methodology is to condition the distillation process on how certain the teacher is about its predictions and incorporating uncertainty learning. 
<br class="ltx_break"/>The proposed confidence-aware mechanism aims to tailor the student’s learning experience by weighting the distillation loss according to the teacher’s confidence, thereby mimicking the teacher more closely when it is more certain, and relying more on the ground truth when the teacher’s confidence is low.
This approach stems from the observation that not all knowledge from the teacher is equally reliable. By modulating the distillation process based on teacher confidence, the student can potentially avoid learning from the teacher’s less reliable cues, which may lead to a more robust and accurate student model.
The standard loss function for vanilla PFD is as below:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(4)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{student}=(1-\alpha)*\mathcal{L}_{cls}+\alpha*\mathcal{L}_{distill}" class="ltx_Math" display="block" id="S3.E4.m1.1"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><msub id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.1.1.3.2" xref="S3.E4.m1.1.1.3.2.cmml">ℒ</mi><mrow id="S3.E4.m1.1.1.3.3" xref="S3.E4.m1.1.1.3.3.cmml"><mi id="S3.E4.m1.1.1.3.3.2" xref="S3.E4.m1.1.1.3.3.2.cmml">s</mi><mo id="S3.E4.m1.1.1.3.3.1" xref="S3.E4.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.3.3.3" xref="S3.E4.m1.1.1.3.3.3.cmml">t</mi><mo id="S3.E4.m1.1.1.3.3.1a" xref="S3.E4.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.3.3.4" xref="S3.E4.m1.1.1.3.3.4.cmml">u</mi><mo id="S3.E4.m1.1.1.3.3.1b" xref="S3.E4.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.3.3.5" xref="S3.E4.m1.1.1.3.3.5.cmml">d</mi><mo id="S3.E4.m1.1.1.3.3.1c" xref="S3.E4.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.3.3.6" xref="S3.E4.m1.1.1.3.3.6.cmml">e</mi><mo id="S3.E4.m1.1.1.3.3.1d" xref="S3.E4.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.3.3.7" xref="S3.E4.m1.1.1.3.3.7.cmml">n</mi><mo id="S3.E4.m1.1.1.3.3.1e" xref="S3.E4.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.3.3.8" xref="S3.E4.m1.1.1.3.3.8.cmml">t</mi></mrow></msub><mo id="S3.E4.m1.1.1.2" xref="S3.E4.m1.1.1.2.cmml">=</mo><mrow id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.cmml"><mo id="S3.E4.m1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E4.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.cmml"><mn id="S3.E4.m1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E4.m1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml">−</mo><mi id="S3.E4.m1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.3.cmml">α</mi></mrow><mo id="S3.E4.m1.1.1.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="S3.E4.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E4.m1.1.1.1.1.2" rspace="0.222em" xref="S3.E4.m1.1.1.1.1.2.cmml">∗</mo><msub id="S3.E4.m1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.3.2.cmml">ℒ</mi><mrow id="S3.E4.m1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.3.3.cmml"><mi id="S3.E4.m1.1.1.1.1.3.3.2" xref="S3.E4.m1.1.1.1.1.3.3.2.cmml">c</mi><mo id="S3.E4.m1.1.1.1.1.3.3.1" xref="S3.E4.m1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.1.1.3.3.3" xref="S3.E4.m1.1.1.1.1.3.3.3.cmml">l</mi><mo id="S3.E4.m1.1.1.1.1.3.3.1a" xref="S3.E4.m1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.1.1.3.3.4" xref="S3.E4.m1.1.1.1.1.3.3.4.cmml">s</mi></mrow></msub></mrow><mo id="S3.E4.m1.1.1.1.2" xref="S3.E4.m1.1.1.1.2.cmml">+</mo><mrow id="S3.E4.m1.1.1.1.3" xref="S3.E4.m1.1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.3.2.cmml">α</mi><mo id="S3.E4.m1.1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.E4.m1.1.1.1.3.1.cmml">∗</mo><msub id="S3.E4.m1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.1.1.1.3.3.2" xref="S3.E4.m1.1.1.1.3.3.2.cmml">ℒ</mi><mrow id="S3.E4.m1.1.1.1.3.3.3" xref="S3.E4.m1.1.1.1.3.3.3.cmml"><mi id="S3.E4.m1.1.1.1.3.3.3.2" xref="S3.E4.m1.1.1.1.3.3.3.2.cmml">d</mi><mo id="S3.E4.m1.1.1.1.3.3.3.1" xref="S3.E4.m1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.1.3.3.3.3" xref="S3.E4.m1.1.1.1.3.3.3.3.cmml">i</mi><mo id="S3.E4.m1.1.1.1.3.3.3.1a" xref="S3.E4.m1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.1.3.3.3.4" xref="S3.E4.m1.1.1.1.3.3.3.4.cmml">s</mi><mo id="S3.E4.m1.1.1.1.3.3.3.1b" xref="S3.E4.m1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.1.3.3.3.5" xref="S3.E4.m1.1.1.1.3.3.3.5.cmml">t</mi><mo id="S3.E4.m1.1.1.1.3.3.3.1c" xref="S3.E4.m1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.1.3.3.3.6" xref="S3.E4.m1.1.1.1.3.3.3.6.cmml">i</mi><mo id="S3.E4.m1.1.1.1.3.3.3.1d" xref="S3.E4.m1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.1.3.3.3.7" xref="S3.E4.m1.1.1.1.3.3.3.7.cmml">l</mi><mo id="S3.E4.m1.1.1.1.3.3.3.1e" xref="S3.E4.m1.1.1.1.3.3.3.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.1.3.3.3.8" xref="S3.E4.m1.1.1.1.3.3.3.8.cmml">l</mi></mrow></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><eq id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2"></eq><apply id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.3.2">ℒ</ci><apply id="S3.E4.m1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.3.3"><times id="S3.E4.m1.1.1.3.3.1.cmml" xref="S3.E4.m1.1.1.3.3.1"></times><ci id="S3.E4.m1.1.1.3.3.2.cmml" xref="S3.E4.m1.1.1.3.3.2">𝑠</ci><ci id="S3.E4.m1.1.1.3.3.3.cmml" xref="S3.E4.m1.1.1.3.3.3">𝑡</ci><ci id="S3.E4.m1.1.1.3.3.4.cmml" xref="S3.E4.m1.1.1.3.3.4">𝑢</ci><ci id="S3.E4.m1.1.1.3.3.5.cmml" xref="S3.E4.m1.1.1.3.3.5">𝑑</ci><ci id="S3.E4.m1.1.1.3.3.6.cmml" xref="S3.E4.m1.1.1.3.3.6">𝑒</ci><ci id="S3.E4.m1.1.1.3.3.7.cmml" xref="S3.E4.m1.1.1.3.3.7">𝑛</ci><ci id="S3.E4.m1.1.1.3.3.8.cmml" xref="S3.E4.m1.1.1.3.3.8">𝑡</ci></apply></apply><apply id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"><plus id="S3.E4.m1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.2"></plus><apply id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1"><times id="S3.E4.m1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2"></times><apply id="S3.E4.m1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1"><minus id="S3.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1"></minus><cn id="S3.E4.m1.1.1.1.1.1.1.1.2.cmml" type="integer" xref="S3.E4.m1.1.1.1.1.1.1.1.2">1</cn><ci id="S3.E4.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.3">𝛼</ci></apply><apply id="S3.E4.m1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.3.2">ℒ</ci><apply id="S3.E4.m1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.3.3"><times id="S3.E4.m1.1.1.1.1.3.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3.3.1"></times><ci id="S3.E4.m1.1.1.1.1.3.3.2.cmml" xref="S3.E4.m1.1.1.1.1.3.3.2">𝑐</ci><ci id="S3.E4.m1.1.1.1.1.3.3.3.cmml" xref="S3.E4.m1.1.1.1.1.3.3.3">𝑙</ci><ci id="S3.E4.m1.1.1.1.1.3.3.4.cmml" xref="S3.E4.m1.1.1.1.1.3.3.4">𝑠</ci></apply></apply></apply><apply id="S3.E4.m1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.3"><times id="S3.E4.m1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.3.1"></times><ci id="S3.E4.m1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.3.2">𝛼</ci><apply id="S3.E4.m1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.3.3.1.cmml" xref="S3.E4.m1.1.1.1.3.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.3.3.2.cmml" xref="S3.E4.m1.1.1.1.3.3.2">ℒ</ci><apply id="S3.E4.m1.1.1.1.3.3.3.cmml" xref="S3.E4.m1.1.1.1.3.3.3"><times id="S3.E4.m1.1.1.1.3.3.3.1.cmml" xref="S3.E4.m1.1.1.1.3.3.3.1"></times><ci id="S3.E4.m1.1.1.1.3.3.3.2.cmml" xref="S3.E4.m1.1.1.1.3.3.3.2">𝑑</ci><ci id="S3.E4.m1.1.1.1.3.3.3.3.cmml" xref="S3.E4.m1.1.1.1.3.3.3.3">𝑖</ci><ci id="S3.E4.m1.1.1.1.3.3.3.4.cmml" xref="S3.E4.m1.1.1.1.3.3.3.4">𝑠</ci><ci id="S3.E4.m1.1.1.1.3.3.3.5.cmml" xref="S3.E4.m1.1.1.1.3.3.3.5">𝑡</ci><ci id="S3.E4.m1.1.1.1.3.3.3.6.cmml" xref="S3.E4.m1.1.1.1.3.3.3.6">𝑖</ci><ci id="S3.E4.m1.1.1.1.3.3.3.7.cmml" xref="S3.E4.m1.1.1.1.3.3.3.7">𝑙</ci><ci id="S3.E4.m1.1.1.1.3.3.3.8.cmml" xref="S3.E4.m1.1.1.1.3.3.3.8">𝑙</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\mathcal{L}_{student}=(1-\alpha)*\mathcal{L}_{cls}+\alpha*\mathcal{L}_{distill}</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_s italic_t italic_u italic_d italic_e italic_n italic_t end_POSTSUBSCRIPT = ( 1 - italic_α ) ∗ caligraphic_L start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT + italic_α ∗ caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_s italic_t italic_i italic_l italic_l end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS4.SSS1.p1.4">The combination of classification loss and distillation loss is a common practice in knowledge distillation. In the context of video understanding, a new angle to interpret the loss combination is to take it as a multi-teacher distillation.</p>
<ul class="ltx_itemize" id="S3.I4">
<li class="ltx_item" id="S3.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I4.i1.p1">
<p class="ltx_p" id="S3.I4.i1.p1.1">The ”distillation teacher” who is easier to learn from but also tends to have information loss. Moreover, the teacher is not same-level confident in all cases. To be more specific, the teacher can be good at some aspects while bad at others</p>
</div>
</li>
<li class="ltx_item" id="S3.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I4.i2.p1">
<p class="ltx_p" id="S3.I4.i2.p1.1">The ”label teacher” who is hard to learn from but has no information loss. However, this teacher would also suffer from noise-label problems, which is quite common in scenarios of video understanding.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S3.SS4.SSS1.p1.2">And this raises the natural questions to traditional PFD: To what extent we should trust each teacher? And the answer is we should adaptively trust them. Based on this, we adjusted the standard loss function to dynamically control the <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p1.1.m1.1"><semantics id="S3.SS4.SSS1.p1.1.m1.1a"><mi id="S3.SS4.SSS1.p1.1.m1.1.1" xref="S3.SS4.SSS1.p1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.1.m1.1b"><ci id="S3.SS4.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS4.SSS1.p1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS1.p1.1.m1.1d">italic_α</annotation></semantics></math> who balance the distillation loss and classification loss. For actual implementation of CPFD, we used teacher’s loss to represent confidence, which is used to control the <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p1.2.m2.1"><semantics id="S3.SS4.SSS1.p1.2.m2.1a"><mi id="S3.SS4.SSS1.p1.2.m2.1.1" xref="S3.SS4.SSS1.p1.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.2.m2.1b"><ci id="S3.SS4.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS4.SSS1.p1.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.2.m2.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS1.p1.2.m2.1d">italic_α</annotation></semantics></math>, representing how much we would learn from the teacher on this sample. A comparison between CPFD and PFD is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S3.F4" title="Figure 4 ‣ 3.4. Confidence-aware Privileged Feature Distillation (CPFD) ‣ 3. Methodology ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_tag">4</span></a></p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2. </span>Implementing CPFD</h4>
<div class="ltx_para" id="S3.SS4.SSS2.p1">
<p class="ltx_p" id="S3.SS4.SSS2.p1.1">To operationalize this concept, we designed several mapping mechanisms to allow the loss to control the <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p1.1.m1.1"><semantics id="S3.SS4.SSS2.p1.1.m1.1a"><mi id="S3.SS4.SSS2.p1.1.m1.1.1" xref="S3.SS4.SSS2.p1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p1.1.m1.1b"><ci id="S3.SS4.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS4.SSS2.p1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS2.p1.1.m1.1d">italic_α</annotation></semantics></math>. The detailed mapping function and distribution are outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S3.T2" title="Table 2 ‣ 3.4.2. Implementing CPFD ‣ 3.4. Confidence-aware Privileged Feature Distillation (CPFD) ‣ 3. Methodology ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_tag">2</span></a> and its distribution are shown Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S3.F5" title="Figure 5 ‣ 3.4.2. Implementing CPFD ‣ 3.4. Confidence-aware Privileged Feature Distillation (CPFD) ‣ 3. Methodology ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T2.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.5.6.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.5.6.1.1"><span class="ltx_text ltx_font_bold" id="S3.T2.5.6.1.1.1">Mapping</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T2.5.6.1.2"><span class="ltx_text ltx_font_bold" id="S3.T2.5.6.1.2.1">Description</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.1.2">Threshold</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.1"><math alttext="\alpha=0.9\text{ if }L_{\text{teacher}}&lt;\tau\text{ else }0.1" class="ltx_Math" display="inline" id="S3.T2.1.1.1.m1.1"><semantics id="S3.T2.1.1.1.m1.1a"><mrow id="S3.T2.1.1.1.m1.1.1" xref="S3.T2.1.1.1.m1.1.1.cmml"><mi id="S3.T2.1.1.1.m1.1.1.2" xref="S3.T2.1.1.1.m1.1.1.2.cmml">α</mi><mo id="S3.T2.1.1.1.m1.1.1.3" xref="S3.T2.1.1.1.m1.1.1.3.cmml">=</mo><mrow id="S3.T2.1.1.1.m1.1.1.4" xref="S3.T2.1.1.1.m1.1.1.4.cmml"><mn id="S3.T2.1.1.1.m1.1.1.4.2" xref="S3.T2.1.1.1.m1.1.1.4.2.cmml">0.9</mn><mo id="S3.T2.1.1.1.m1.1.1.4.1" xref="S3.T2.1.1.1.m1.1.1.4.1.cmml">⁢</mo><mtext id="S3.T2.1.1.1.m1.1.1.4.3" xref="S3.T2.1.1.1.m1.1.1.4.3a.cmml"> if </mtext><mo id="S3.T2.1.1.1.m1.1.1.4.1a" xref="S3.T2.1.1.1.m1.1.1.4.1.cmml">⁢</mo><msub id="S3.T2.1.1.1.m1.1.1.4.4" xref="S3.T2.1.1.1.m1.1.1.4.4.cmml"><mi id="S3.T2.1.1.1.m1.1.1.4.4.2" xref="S3.T2.1.1.1.m1.1.1.4.4.2.cmml">L</mi><mtext id="S3.T2.1.1.1.m1.1.1.4.4.3" xref="S3.T2.1.1.1.m1.1.1.4.4.3a.cmml">teacher</mtext></msub></mrow><mo id="S3.T2.1.1.1.m1.1.1.5" xref="S3.T2.1.1.1.m1.1.1.5.cmml">&lt;</mo><mrow id="S3.T2.1.1.1.m1.1.1.6" xref="S3.T2.1.1.1.m1.1.1.6.cmml"><mi id="S3.T2.1.1.1.m1.1.1.6.2" xref="S3.T2.1.1.1.m1.1.1.6.2.cmml">τ</mi><mo id="S3.T2.1.1.1.m1.1.1.6.1" xref="S3.T2.1.1.1.m1.1.1.6.1.cmml">⁢</mo><mtext id="S3.T2.1.1.1.m1.1.1.6.3" xref="S3.T2.1.1.1.m1.1.1.6.3a.cmml"> else </mtext><mo id="S3.T2.1.1.1.m1.1.1.6.1a" xref="S3.T2.1.1.1.m1.1.1.6.1.cmml">⁢</mo><mn id="S3.T2.1.1.1.m1.1.1.6.4" xref="S3.T2.1.1.1.m1.1.1.6.4.cmml">0.1</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.m1.1b"><apply id="S3.T2.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.m1.1.1"><and id="S3.T2.1.1.1.m1.1.1a.cmml" xref="S3.T2.1.1.1.m1.1.1"></and><apply id="S3.T2.1.1.1.m1.1.1b.cmml" xref="S3.T2.1.1.1.m1.1.1"><eq id="S3.T2.1.1.1.m1.1.1.3.cmml" xref="S3.T2.1.1.1.m1.1.1.3"></eq><ci id="S3.T2.1.1.1.m1.1.1.2.cmml" xref="S3.T2.1.1.1.m1.1.1.2">𝛼</ci><apply id="S3.T2.1.1.1.m1.1.1.4.cmml" xref="S3.T2.1.1.1.m1.1.1.4"><times id="S3.T2.1.1.1.m1.1.1.4.1.cmml" xref="S3.T2.1.1.1.m1.1.1.4.1"></times><cn id="S3.T2.1.1.1.m1.1.1.4.2.cmml" type="float" xref="S3.T2.1.1.1.m1.1.1.4.2">0.9</cn><ci id="S3.T2.1.1.1.m1.1.1.4.3a.cmml" xref="S3.T2.1.1.1.m1.1.1.4.3"><mtext id="S3.T2.1.1.1.m1.1.1.4.3.cmml" xref="S3.T2.1.1.1.m1.1.1.4.3"> if </mtext></ci><apply id="S3.T2.1.1.1.m1.1.1.4.4.cmml" xref="S3.T2.1.1.1.m1.1.1.4.4"><csymbol cd="ambiguous" id="S3.T2.1.1.1.m1.1.1.4.4.1.cmml" xref="S3.T2.1.1.1.m1.1.1.4.4">subscript</csymbol><ci id="S3.T2.1.1.1.m1.1.1.4.4.2.cmml" xref="S3.T2.1.1.1.m1.1.1.4.4.2">𝐿</ci><ci id="S3.T2.1.1.1.m1.1.1.4.4.3a.cmml" xref="S3.T2.1.1.1.m1.1.1.4.4.3"><mtext id="S3.T2.1.1.1.m1.1.1.4.4.3.cmml" mathsize="70%" xref="S3.T2.1.1.1.m1.1.1.4.4.3">teacher</mtext></ci></apply></apply></apply><apply id="S3.T2.1.1.1.m1.1.1c.cmml" xref="S3.T2.1.1.1.m1.1.1"><lt id="S3.T2.1.1.1.m1.1.1.5.cmml" xref="S3.T2.1.1.1.m1.1.1.5"></lt><share href="https://arxiv.org/html/2410.03038v2#S3.T2.1.1.1.m1.1.1.4.cmml" id="S3.T2.1.1.1.m1.1.1d.cmml" xref="S3.T2.1.1.1.m1.1.1"></share><apply id="S3.T2.1.1.1.m1.1.1.6.cmml" xref="S3.T2.1.1.1.m1.1.1.6"><times id="S3.T2.1.1.1.m1.1.1.6.1.cmml" xref="S3.T2.1.1.1.m1.1.1.6.1"></times><ci id="S3.T2.1.1.1.m1.1.1.6.2.cmml" xref="S3.T2.1.1.1.m1.1.1.6.2">𝜏</ci><ci id="S3.T2.1.1.1.m1.1.1.6.3a.cmml" xref="S3.T2.1.1.1.m1.1.1.6.3"><mtext id="S3.T2.1.1.1.m1.1.1.6.3.cmml" xref="S3.T2.1.1.1.m1.1.1.6.3"> else </mtext></ci><cn id="S3.T2.1.1.1.m1.1.1.6.4.cmml" type="float" xref="S3.T2.1.1.1.m1.1.1.6.4">0.1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.m1.1c">\alpha=0.9\text{ if }L_{\text{teacher}}&lt;\tau\text{ else }0.1</annotation><annotation encoding="application/x-llamapun" id="S3.T2.1.1.1.m1.1d">italic_α = 0.9 if italic_L start_POSTSUBSCRIPT teacher end_POSTSUBSCRIPT &lt; italic_τ else 0.1</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.2.2.2">Neg Sigmoid</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.2.1"><math alttext="\alpha=\frac{1}{1+e^{\beta\cdot(L_{\text{teacher}}-L_{\text{center}})}}" class="ltx_Math" display="inline" id="S3.T2.2.2.1.m1.1"><semantics id="S3.T2.2.2.1.m1.1a"><mrow id="S3.T2.2.2.1.m1.1.2" xref="S3.T2.2.2.1.m1.1.2.cmml"><mi id="S3.T2.2.2.1.m1.1.2.2" xref="S3.T2.2.2.1.m1.1.2.2.cmml">α</mi><mo id="S3.T2.2.2.1.m1.1.2.1" xref="S3.T2.2.2.1.m1.1.2.1.cmml">=</mo><mfrac id="S3.T2.2.2.1.m1.1.1" xref="S3.T2.2.2.1.m1.1.1.cmml"><mn id="S3.T2.2.2.1.m1.1.1.3" xref="S3.T2.2.2.1.m1.1.1.3.cmml">1</mn><mrow id="S3.T2.2.2.1.m1.1.1.1" xref="S3.T2.2.2.1.m1.1.1.1.cmml"><mn id="S3.T2.2.2.1.m1.1.1.1.3" xref="S3.T2.2.2.1.m1.1.1.1.3.cmml">1</mn><mo id="S3.T2.2.2.1.m1.1.1.1.2" xref="S3.T2.2.2.1.m1.1.1.1.2.cmml">+</mo><msup id="S3.T2.2.2.1.m1.1.1.1.4" xref="S3.T2.2.2.1.m1.1.1.1.4.cmml"><mi id="S3.T2.2.2.1.m1.1.1.1.4.2" xref="S3.T2.2.2.1.m1.1.1.1.4.2.cmml">e</mi><mrow id="S3.T2.2.2.1.m1.1.1.1.1.1" xref="S3.T2.2.2.1.m1.1.1.1.1.1.cmml"><mi id="S3.T2.2.2.1.m1.1.1.1.1.1.3" xref="S3.T2.2.2.1.m1.1.1.1.1.1.3.cmml">β</mi><mo id="S3.T2.2.2.1.m1.1.1.1.1.1.2" lspace="0.222em" rspace="0.222em" xref="S3.T2.2.2.1.m1.1.1.1.1.1.2.cmml">⋅</mo><mrow id="S3.T2.2.2.1.m1.1.1.1.1.1.1.1" xref="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1" xref="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.2" xref="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.2.2" xref="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.2.2.cmml">L</mi><mtext id="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.2.3" xref="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.2.3a.cmml">teacher</mtext></msub><mo id="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.1" xref="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.3" xref="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.3.2.cmml">L</mi><mtext id="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.3.3a.cmml">center</mtext></msub></mrow><mo id="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></msup></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.1.m1.1b"><apply id="S3.T2.2.2.1.m1.1.2.cmml" xref="S3.T2.2.2.1.m1.1.2"><eq id="S3.T2.2.2.1.m1.1.2.1.cmml" xref="S3.T2.2.2.1.m1.1.2.1"></eq><ci id="S3.T2.2.2.1.m1.1.2.2.cmml" xref="S3.T2.2.2.1.m1.1.2.2">𝛼</ci><apply id="S3.T2.2.2.1.m1.1.1.cmml" xref="S3.T2.2.2.1.m1.1.1"><divide id="S3.T2.2.2.1.m1.1.1.2.cmml" xref="S3.T2.2.2.1.m1.1.1"></divide><cn id="S3.T2.2.2.1.m1.1.1.3.cmml" type="integer" xref="S3.T2.2.2.1.m1.1.1.3">1</cn><apply id="S3.T2.2.2.1.m1.1.1.1.cmml" xref="S3.T2.2.2.1.m1.1.1.1"><plus id="S3.T2.2.2.1.m1.1.1.1.2.cmml" xref="S3.T2.2.2.1.m1.1.1.1.2"></plus><cn id="S3.T2.2.2.1.m1.1.1.1.3.cmml" type="integer" xref="S3.T2.2.2.1.m1.1.1.1.3">1</cn><apply id="S3.T2.2.2.1.m1.1.1.1.4.cmml" xref="S3.T2.2.2.1.m1.1.1.1.4"><csymbol cd="ambiguous" id="S3.T2.2.2.1.m1.1.1.1.4.1.cmml" xref="S3.T2.2.2.1.m1.1.1.1.4">superscript</csymbol><ci id="S3.T2.2.2.1.m1.1.1.1.4.2.cmml" xref="S3.T2.2.2.1.m1.1.1.1.4.2">𝑒</ci><apply id="S3.T2.2.2.1.m1.1.1.1.1.1.cmml" xref="S3.T2.2.2.1.m1.1.1.1.1.1"><ci id="S3.T2.2.2.1.m1.1.1.1.1.1.2.cmml" xref="S3.T2.2.2.1.m1.1.1.1.1.1.2">⋅</ci><ci id="S3.T2.2.2.1.m1.1.1.1.1.1.3.cmml" xref="S3.T2.2.2.1.m1.1.1.1.1.1.3">𝛽</ci><apply id="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.T2.2.2.1.m1.1.1.1.1.1.1.1"><minus id="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.2.2">𝐿</ci><ci id="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.2.3a.cmml" xref="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.2.3"><mtext id="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.2.3.cmml" mathsize="50%" xref="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.2.3">teacher</mtext></ci></apply><apply id="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.3.2">𝐿</ci><ci id="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.3.3a.cmml" xref="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.3.3"><mtext id="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.3.3.cmml" mathsize="50%" xref="S3.T2.2.2.1.m1.1.1.1.1.1.1.1.1.3.3">center</mtext></ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.1.m1.1c">\alpha=\frac{1}{1+e^{\beta\cdot(L_{\text{teacher}}-L_{\text{center}})}}</annotation><annotation encoding="application/x-llamapun" id="S3.T2.2.2.1.m1.1d">italic_α = divide start_ARG 1 end_ARG start_ARG 1 + italic_e start_POSTSUPERSCRIPT italic_β ⋅ ( italic_L start_POSTSUBSCRIPT teacher end_POSTSUBSCRIPT - italic_L start_POSTSUBSCRIPT center end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPT end_ARG</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.3.3.2">Tanh</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.3.3.1"><math alttext="\alpha=0.5\cdot(\tanh(-\beta\cdot(L_{\text{teacher}}-L_{\text{center}}))+1)" class="ltx_Math" display="inline" id="S3.T2.3.3.1.m1.2"><semantics id="S3.T2.3.3.1.m1.2a"><mrow id="S3.T2.3.3.1.m1.2.2" xref="S3.T2.3.3.1.m1.2.2.cmml"><mi id="S3.T2.3.3.1.m1.2.2.3" xref="S3.T2.3.3.1.m1.2.2.3.cmml">α</mi><mo id="S3.T2.3.3.1.m1.2.2.2" xref="S3.T2.3.3.1.m1.2.2.2.cmml">=</mo><mrow id="S3.T2.3.3.1.m1.2.2.1" xref="S3.T2.3.3.1.m1.2.2.1.cmml"><mn id="S3.T2.3.3.1.m1.2.2.1.3" xref="S3.T2.3.3.1.m1.2.2.1.3.cmml">0.5</mn><mo id="S3.T2.3.3.1.m1.2.2.1.2" lspace="0.222em" rspace="0.222em" xref="S3.T2.3.3.1.m1.2.2.1.2.cmml">⋅</mo><mrow id="S3.T2.3.3.1.m1.2.2.1.1.1" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.cmml"><mo id="S3.T2.3.3.1.m1.2.2.1.1.1.2" stretchy="false" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.T2.3.3.1.m1.2.2.1.1.1.1" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.cmml"><mrow id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.2.cmml"><mi id="S3.T2.3.3.1.m1.1.1" xref="S3.T2.3.3.1.m1.1.1.cmml">tanh</mi><mo id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1a" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.2.cmml">⁡</mo><mrow id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.2.cmml"><mo id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.2.cmml">(</mo><mrow id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mo id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1a" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.cmml">−</mo><mrow id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml">β</mi><mo id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.2" lspace="0.222em" rspace="0.222em" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml">⋅</mo><mrow id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">L</mi><mtext id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3a.cmml">teacher</mtext></msub><mo id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">L</mi><mtext id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.3a.cmml">center</mtext></msub></mrow><mo id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.T2.3.3.1.m1.2.2.1.1.1.1.2" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.2.cmml">+</mo><mn id="S3.T2.3.3.1.m1.2.2.1.1.1.1.3" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.T2.3.3.1.m1.2.2.1.1.1.3" stretchy="false" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.3.3.1.m1.2b"><apply id="S3.T2.3.3.1.m1.2.2.cmml" xref="S3.T2.3.3.1.m1.2.2"><eq id="S3.T2.3.3.1.m1.2.2.2.cmml" xref="S3.T2.3.3.1.m1.2.2.2"></eq><ci id="S3.T2.3.3.1.m1.2.2.3.cmml" xref="S3.T2.3.3.1.m1.2.2.3">𝛼</ci><apply id="S3.T2.3.3.1.m1.2.2.1.cmml" xref="S3.T2.3.3.1.m1.2.2.1"><ci id="S3.T2.3.3.1.m1.2.2.1.2.cmml" xref="S3.T2.3.3.1.m1.2.2.1.2">⋅</ci><cn id="S3.T2.3.3.1.m1.2.2.1.3.cmml" type="float" xref="S3.T2.3.3.1.m1.2.2.1.3">0.5</cn><apply id="S3.T2.3.3.1.m1.2.2.1.1.1.1.cmml" xref="S3.T2.3.3.1.m1.2.2.1.1.1"><plus id="S3.T2.3.3.1.m1.2.2.1.1.1.1.2.cmml" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.2"></plus><apply id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1"><tanh id="S3.T2.3.3.1.m1.1.1.cmml" xref="S3.T2.3.3.1.m1.1.1"></tanh><apply id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1"><minus id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1"></minus><apply id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1"><ci id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.2">⋅</ci><ci id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.3">𝛽</ci><apply id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1"><minus id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2">𝐿</ci><ci id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3a.cmml" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3"><mtext id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" mathsize="70%" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3">teacher</mtext></ci></apply><apply id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2">𝐿</ci><ci id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.3a.cmml" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.3"><mtext id="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" mathsize="70%" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.3">center</mtext></ci></apply></apply></apply></apply></apply><cn id="S3.T2.3.3.1.m1.2.2.1.1.1.1.3.cmml" type="integer" xref="S3.T2.3.3.1.m1.2.2.1.1.1.1.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.3.1.m1.2c">\alpha=0.5\cdot(\tanh(-\beta\cdot(L_{\text{teacher}}-L_{\text{center}}))+1)</annotation><annotation encoding="application/x-llamapun" id="S3.T2.3.3.1.m1.2d">italic_α = 0.5 ⋅ ( roman_tanh ( - italic_β ⋅ ( italic_L start_POSTSUBSCRIPT teacher end_POSTSUBSCRIPT - italic_L start_POSTSUBSCRIPT center end_POSTSUBSCRIPT ) ) + 1 )</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.4.4.2">Exp Decay</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.4.4.1"><math alttext="\alpha=\alpha_{\max}\cdot e^{-k\cdot(L_{\text{teacher}}-L_{\text{min}})}" class="ltx_Math" display="inline" id="S3.T2.4.4.1.m1.1"><semantics id="S3.T2.4.4.1.m1.1a"><mrow id="S3.T2.4.4.1.m1.1.2" xref="S3.T2.4.4.1.m1.1.2.cmml"><mi id="S3.T2.4.4.1.m1.1.2.2" xref="S3.T2.4.4.1.m1.1.2.2.cmml">α</mi><mo id="S3.T2.4.4.1.m1.1.2.1" xref="S3.T2.4.4.1.m1.1.2.1.cmml">=</mo><mrow id="S3.T2.4.4.1.m1.1.2.3" xref="S3.T2.4.4.1.m1.1.2.3.cmml"><msub id="S3.T2.4.4.1.m1.1.2.3.2" xref="S3.T2.4.4.1.m1.1.2.3.2.cmml"><mi id="S3.T2.4.4.1.m1.1.2.3.2.2" xref="S3.T2.4.4.1.m1.1.2.3.2.2.cmml">α</mi><mi id="S3.T2.4.4.1.m1.1.2.3.2.3" xref="S3.T2.4.4.1.m1.1.2.3.2.3.cmml">max</mi></msub><mo id="S3.T2.4.4.1.m1.1.2.3.1" lspace="0.222em" rspace="0.222em" xref="S3.T2.4.4.1.m1.1.2.3.1.cmml">⋅</mo><msup id="S3.T2.4.4.1.m1.1.2.3.3" xref="S3.T2.4.4.1.m1.1.2.3.3.cmml"><mi id="S3.T2.4.4.1.m1.1.2.3.3.2" xref="S3.T2.4.4.1.m1.1.2.3.3.2.cmml">e</mi><mrow id="S3.T2.4.4.1.m1.1.1.1" xref="S3.T2.4.4.1.m1.1.1.1.cmml"><mo id="S3.T2.4.4.1.m1.1.1.1a" xref="S3.T2.4.4.1.m1.1.1.1.cmml">−</mo><mrow id="S3.T2.4.4.1.m1.1.1.1.1" xref="S3.T2.4.4.1.m1.1.1.1.1.cmml"><mi id="S3.T2.4.4.1.m1.1.1.1.1.3" xref="S3.T2.4.4.1.m1.1.1.1.1.3.cmml">k</mi><mo id="S3.T2.4.4.1.m1.1.1.1.1.2" lspace="0.222em" rspace="0.222em" xref="S3.T2.4.4.1.m1.1.1.1.1.2.cmml">⋅</mo><mrow id="S3.T2.4.4.1.m1.1.1.1.1.1.1" xref="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.cmml"><mo id="S3.T2.4.4.1.m1.1.1.1.1.1.1.2" stretchy="false" xref="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.T2.4.4.1.m1.1.1.1.1.1.1.1" xref="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.cmml"><msub id="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.2" xref="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.2.2" xref="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.2.2.cmml">L</mi><mtext id="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.2.3" xref="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.2.3a.cmml">teacher</mtext></msub><mo id="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.1" xref="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.3" xref="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.3.2" xref="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.3.2.cmml">L</mi><mtext id="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.3.3" xref="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.3.3a.cmml">min</mtext></msub></mrow><mo id="S3.T2.4.4.1.m1.1.1.1.1.1.1.3" stretchy="false" xref="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.4.4.1.m1.1b"><apply id="S3.T2.4.4.1.m1.1.2.cmml" xref="S3.T2.4.4.1.m1.1.2"><eq id="S3.T2.4.4.1.m1.1.2.1.cmml" xref="S3.T2.4.4.1.m1.1.2.1"></eq><ci id="S3.T2.4.4.1.m1.1.2.2.cmml" xref="S3.T2.4.4.1.m1.1.2.2">𝛼</ci><apply id="S3.T2.4.4.1.m1.1.2.3.cmml" xref="S3.T2.4.4.1.m1.1.2.3"><ci id="S3.T2.4.4.1.m1.1.2.3.1.cmml" xref="S3.T2.4.4.1.m1.1.2.3.1">⋅</ci><apply id="S3.T2.4.4.1.m1.1.2.3.2.cmml" xref="S3.T2.4.4.1.m1.1.2.3.2"><csymbol cd="ambiguous" id="S3.T2.4.4.1.m1.1.2.3.2.1.cmml" xref="S3.T2.4.4.1.m1.1.2.3.2">subscript</csymbol><ci id="S3.T2.4.4.1.m1.1.2.3.2.2.cmml" xref="S3.T2.4.4.1.m1.1.2.3.2.2">𝛼</ci><max id="S3.T2.4.4.1.m1.1.2.3.2.3.cmml" xref="S3.T2.4.4.1.m1.1.2.3.2.3"></max></apply><apply id="S3.T2.4.4.1.m1.1.2.3.3.cmml" xref="S3.T2.4.4.1.m1.1.2.3.3"><csymbol cd="ambiguous" id="S3.T2.4.4.1.m1.1.2.3.3.1.cmml" xref="S3.T2.4.4.1.m1.1.2.3.3">superscript</csymbol><ci id="S3.T2.4.4.1.m1.1.2.3.3.2.cmml" xref="S3.T2.4.4.1.m1.1.2.3.3.2">𝑒</ci><apply id="S3.T2.4.4.1.m1.1.1.1.cmml" xref="S3.T2.4.4.1.m1.1.1.1"><minus id="S3.T2.4.4.1.m1.1.1.1.2.cmml" xref="S3.T2.4.4.1.m1.1.1.1"></minus><apply id="S3.T2.4.4.1.m1.1.1.1.1.cmml" xref="S3.T2.4.4.1.m1.1.1.1.1"><ci id="S3.T2.4.4.1.m1.1.1.1.1.2.cmml" xref="S3.T2.4.4.1.m1.1.1.1.1.2">⋅</ci><ci id="S3.T2.4.4.1.m1.1.1.1.1.3.cmml" xref="S3.T2.4.4.1.m1.1.1.1.1.3">𝑘</ci><apply id="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.T2.4.4.1.m1.1.1.1.1.1.1"><minus id="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.1"></minus><apply id="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.2.2">𝐿</ci><ci id="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.2.3a.cmml" xref="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.2.3"><mtext id="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.2.3.cmml" mathsize="50%" xref="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.2.3">teacher</mtext></ci></apply><apply id="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.3.2">𝐿</ci><ci id="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.3.3a.cmml" xref="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.3.3"><mtext id="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.3.3.cmml" mathsize="50%" xref="S3.T2.4.4.1.m1.1.1.1.1.1.1.1.3.3">min</mtext></ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.4.4.1.m1.1c">\alpha=\alpha_{\max}\cdot e^{-k\cdot(L_{\text{teacher}}-L_{\text{min}})}</annotation><annotation encoding="application/x-llamapun" id="S3.T2.4.4.1.m1.1d">italic_α = italic_α start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT ⋅ italic_e start_POSTSUPERSCRIPT - italic_k ⋅ ( italic_L start_POSTSUBSCRIPT teacher end_POSTSUBSCRIPT - italic_L start_POSTSUBSCRIPT min end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPT</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T2.5.5">
<th class="ltx_td ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" id="S3.T2.5.5.2"></th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T2.5.5.1">where <math alttext="k=-\log\left(\frac{\alpha_{\min}}{\alpha_{\max}}\right)/(L_{\max}-L_{\text{min%
}})" class="ltx_Math" display="inline" id="S3.T2.5.5.1.m1.3"><semantics id="S3.T2.5.5.1.m1.3a"><mrow id="S3.T2.5.5.1.m1.3.3" xref="S3.T2.5.5.1.m1.3.3.cmml"><mi id="S3.T2.5.5.1.m1.3.3.3" xref="S3.T2.5.5.1.m1.3.3.3.cmml">k</mi><mo id="S3.T2.5.5.1.m1.3.3.2" xref="S3.T2.5.5.1.m1.3.3.2.cmml">=</mo><mrow id="S3.T2.5.5.1.m1.3.3.1" xref="S3.T2.5.5.1.m1.3.3.1.cmml"><mo id="S3.T2.5.5.1.m1.3.3.1a" rspace="0.167em" xref="S3.T2.5.5.1.m1.3.3.1.cmml">−</mo><mrow id="S3.T2.5.5.1.m1.3.3.1.1" xref="S3.T2.5.5.1.m1.3.3.1.1.cmml"><mrow id="S3.T2.5.5.1.m1.3.3.1.1.3.2" xref="S3.T2.5.5.1.m1.3.3.1.1.3.1.cmml"><mi id="S3.T2.5.5.1.m1.1.1" xref="S3.T2.5.5.1.m1.1.1.cmml">log</mi><mo id="S3.T2.5.5.1.m1.3.3.1.1.3.2a" xref="S3.T2.5.5.1.m1.3.3.1.1.3.1.cmml">⁡</mo><mrow id="S3.T2.5.5.1.m1.3.3.1.1.3.2.1" xref="S3.T2.5.5.1.m1.3.3.1.1.3.1.cmml"><mo id="S3.T2.5.5.1.m1.3.3.1.1.3.2.1.1" xref="S3.T2.5.5.1.m1.3.3.1.1.3.1.cmml">(</mo><mfrac id="S3.T2.5.5.1.m1.2.2" xref="S3.T2.5.5.1.m1.2.2.cmml"><msub id="S3.T2.5.5.1.m1.2.2.2" xref="S3.T2.5.5.1.m1.2.2.2.cmml"><mi id="S3.T2.5.5.1.m1.2.2.2.2" xref="S3.T2.5.5.1.m1.2.2.2.2.cmml">α</mi><mi id="S3.T2.5.5.1.m1.2.2.2.3" xref="S3.T2.5.5.1.m1.2.2.2.3.cmml">min</mi></msub><msub id="S3.T2.5.5.1.m1.2.2.3" xref="S3.T2.5.5.1.m1.2.2.3.cmml"><mi id="S3.T2.5.5.1.m1.2.2.3.2" xref="S3.T2.5.5.1.m1.2.2.3.2.cmml">α</mi><mi id="S3.T2.5.5.1.m1.2.2.3.3" xref="S3.T2.5.5.1.m1.2.2.3.3.cmml">max</mi></msub></mfrac><mo id="S3.T2.5.5.1.m1.3.3.1.1.3.2.1.2" xref="S3.T2.5.5.1.m1.3.3.1.1.3.1.cmml">)</mo></mrow></mrow><mo id="S3.T2.5.5.1.m1.3.3.1.1.2" xref="S3.T2.5.5.1.m1.3.3.1.1.2.cmml">/</mo><mrow id="S3.T2.5.5.1.m1.3.3.1.1.1.1" xref="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.cmml"><mo id="S3.T2.5.5.1.m1.3.3.1.1.1.1.2" stretchy="false" xref="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.cmml">(</mo><mrow id="S3.T2.5.5.1.m1.3.3.1.1.1.1.1" xref="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.cmml"><msub id="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.2" xref="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.2.cmml"><mi id="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.2.2" xref="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.2.2.cmml">L</mi><mi id="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.2.3" xref="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.2.3.cmml">max</mi></msub><mo id="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.1" xref="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.1.cmml">−</mo><msub id="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.3" xref="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.3.cmml"><mi id="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.3.2" xref="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.3.2.cmml">L</mi><mtext id="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.3.3" xref="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.3.3a.cmml">min</mtext></msub></mrow><mo id="S3.T2.5.5.1.m1.3.3.1.1.1.1.3" stretchy="false" xref="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.5.5.1.m1.3b"><apply id="S3.T2.5.5.1.m1.3.3.cmml" xref="S3.T2.5.5.1.m1.3.3"><eq id="S3.T2.5.5.1.m1.3.3.2.cmml" xref="S3.T2.5.5.1.m1.3.3.2"></eq><ci id="S3.T2.5.5.1.m1.3.3.3.cmml" xref="S3.T2.5.5.1.m1.3.3.3">𝑘</ci><apply id="S3.T2.5.5.1.m1.3.3.1.cmml" xref="S3.T2.5.5.1.m1.3.3.1"><minus id="S3.T2.5.5.1.m1.3.3.1.2.cmml" xref="S3.T2.5.5.1.m1.3.3.1"></minus><apply id="S3.T2.5.5.1.m1.3.3.1.1.cmml" xref="S3.T2.5.5.1.m1.3.3.1.1"><divide id="S3.T2.5.5.1.m1.3.3.1.1.2.cmml" xref="S3.T2.5.5.1.m1.3.3.1.1.2"></divide><apply id="S3.T2.5.5.1.m1.3.3.1.1.3.1.cmml" xref="S3.T2.5.5.1.m1.3.3.1.1.3.2"><log id="S3.T2.5.5.1.m1.1.1.cmml" xref="S3.T2.5.5.1.m1.1.1"></log><apply id="S3.T2.5.5.1.m1.2.2.cmml" xref="S3.T2.5.5.1.m1.2.2"><divide id="S3.T2.5.5.1.m1.2.2.1.cmml" xref="S3.T2.5.5.1.m1.2.2"></divide><apply id="S3.T2.5.5.1.m1.2.2.2.cmml" xref="S3.T2.5.5.1.m1.2.2.2"><csymbol cd="ambiguous" id="S3.T2.5.5.1.m1.2.2.2.1.cmml" xref="S3.T2.5.5.1.m1.2.2.2">subscript</csymbol><ci id="S3.T2.5.5.1.m1.2.2.2.2.cmml" xref="S3.T2.5.5.1.m1.2.2.2.2">𝛼</ci><min id="S3.T2.5.5.1.m1.2.2.2.3.cmml" xref="S3.T2.5.5.1.m1.2.2.2.3"></min></apply><apply id="S3.T2.5.5.1.m1.2.2.3.cmml" xref="S3.T2.5.5.1.m1.2.2.3"><csymbol cd="ambiguous" id="S3.T2.5.5.1.m1.2.2.3.1.cmml" xref="S3.T2.5.5.1.m1.2.2.3">subscript</csymbol><ci id="S3.T2.5.5.1.m1.2.2.3.2.cmml" xref="S3.T2.5.5.1.m1.2.2.3.2">𝛼</ci><max id="S3.T2.5.5.1.m1.2.2.3.3.cmml" xref="S3.T2.5.5.1.m1.2.2.3.3"></max></apply></apply></apply><apply id="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.cmml" xref="S3.T2.5.5.1.m1.3.3.1.1.1.1"><minus id="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.1"></minus><apply id="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.2.1.cmml" xref="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.2">subscript</csymbol><ci id="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.2.2.cmml" xref="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.2.2">𝐿</ci><max id="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.2.3.cmml" xref="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.2.3"></max></apply><apply id="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.3.1.cmml" xref="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.3">subscript</csymbol><ci id="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.3.2.cmml" xref="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.3.2">𝐿</ci><ci id="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.3.3a.cmml" xref="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.3.3"><mtext id="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.3.3.cmml" mathsize="70%" xref="S3.T2.5.5.1.m1.3.3.1.1.1.1.1.3.3">min</mtext></ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.5.5.1.m1.3c">k=-\log\left(\frac{\alpha_{\min}}{\alpha_{\max}}\right)/(L_{\max}-L_{\text{min%
}})</annotation><annotation encoding="application/x-llamapun" id="S3.T2.5.5.1.m1.3d">italic_k = - roman_log ( divide start_ARG italic_α start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT end_ARG start_ARG italic_α start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT end_ARG ) / ( italic_L start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT - italic_L start_POSTSUBSCRIPT min end_POSTSUBSCRIPT )</annotation></semantics></math>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>Mapping Function Designs.</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="450" id="S3.F5.g1" src="extracted/5905908/mapping_func.jpeg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Alpha Distribution as a Function of Teacher Loss.</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we conduct experiments to evaluate both the offline and online A/B performance of the video classification models trained by PFD and CPFD.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.1">Task</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3" id="S4.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.2.1">X-VLM</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3" id="S4.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.3.1">DF-X-VLM (Teacher)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3" id="S4.T3.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.4.1">PFD-XVLM (Student)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3" id="S4.T3.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.5.1">CPFD-X-VLM (Student)</span></th>
</tr>
<tr class="ltx_tr" id="S4.T3.1.2.2">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r" id="S4.T3.1.2.2.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.1.2.2.2">ROC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.1.2.2.3">PR</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T3.1.2.2.4">F1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.1.2.2.5">ROC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.1.2.2.6">PR</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T3.1.2.2.7">F1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.1.2.2.8">ROC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.1.2.2.9">PR</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T3.1.2.2.10">F1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.1.2.2.11">ROC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.1.2.2.12">PR</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T3.1.2.2.13">F1</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.1.3.1.1">Task1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.2">0.9269</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.3">0.5621</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.3.1.4">0.5424</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.5">0.9493</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.6">0.6878</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.3.1.7">0.6277</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.8">0.9400</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.9">0.6584</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.3.1.10">0.6104</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.11"><span class="ltx_text ltx_font_bold" id="S4.T3.1.3.1.11.1">0.9460</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.12"><span class="ltx_text ltx_font_bold" id="S4.T3.1.3.1.12.1">0.6667</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.3.1.13"><span class="ltx_text ltx_font_bold" id="S4.T3.1.3.1.13.1">0.6266</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T3.1.4.2.1">Task2</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.2">0.9515</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.3">0.9646</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.4.2.4">0.8073</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.5">0.9616</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.6">0.9720</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.4.2.7">0.8223</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.8">0.9529</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.9">0.9657</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.4.2.10">0.8191</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.11"><span class="ltx_text ltx_font_bold" id="S4.T3.1.4.2.11.1">0.9555</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.12"><span class="ltx_text ltx_font_bold" id="S4.T3.1.4.2.12.1">0.9670</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.4.2.13"><span class="ltx_text ltx_font_bold" id="S4.T3.1.4.2.13.1">0.8303</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T3.1.5.3.1">Task3</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.2">0.9927</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.3">0.9739</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.5.3.4">0.9255</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.5">0.9962</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.6">0.9844</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.5.3.7">0.9443</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.8">0.9928</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.9">0.9744</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.5.3.10">0.9315</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.11"><span class="ltx_text ltx_font_bold" id="S4.T3.1.5.3.11.1">0.9935</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.12"><span class="ltx_text ltx_font_bold" id="S4.T3.1.5.3.12.1">0.9764</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.5.3.13"><span class="ltx_text ltx_font_bold" id="S4.T3.1.5.3.13.1">0.9337</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.6.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T3.1.6.4.1">Task4</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.4.2">0.9657</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.4.3">0.9541</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.6.4.4">0.7767</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.4.5">0.9728</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.4.6">0.9621</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.6.4.7">0.8009</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.4.8">0.9697</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.4.9">0.9587</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.6.4.10"><span class="ltx_text ltx_font_bold" id="S4.T3.1.6.4.10.1">0.7964</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.4.11"><span class="ltx_text ltx_font_bold" id="S4.T3.1.6.4.11.1">0.9716</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.4.12"><span class="ltx_text ltx_font_bold" id="S4.T3.1.6.4.12.1">0.9614</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.6.4.13">0.7962</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.7.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" id="S4.T3.1.7.5.1">Task5</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.7.5.2">0.9439</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.7.5.3">0.4100</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T3.1.7.5.4">0.4538</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.7.5.5">0.9556</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.7.5.6">0.4515</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T3.1.7.5.7">0.4942</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.7.5.8">0.9489</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.7.5.9">0.4418</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T3.1.7.5.10">0.4737</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.7.5.11"><span class="ltx_text ltx_font_bold" id="S4.T3.1.7.5.11.1">0.9550</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.7.5.12"><span class="ltx_text ltx_font_bold" id="S4.T3.1.7.5.12.1">0.4728</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T3.1.7.5.13"><span class="ltx_text ltx_font_bold" id="S4.T3.1.7.5.13.1">0.5083</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3. </span>Performance Metrics Across Different Models (ROC:ROC AUC, PR:PR AUC, F1:F1 Score). X-VLM refers to normally trained X-VLM model. PFD-X-VLM and CPFD-X-VLM refers to student X-VLM model distilled from teacher DF-X-VLM using vanilla PFD method and Confidence-aware PFD method. The bold number represents the best <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.3.1">student</span> model results.</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Experiment Setting</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">Data.</span> Since the proposed CPFD mainly focus on the industrial video classification models, we conduct the experiments on real-world industrial datasets. The dataset spans five diverse classification tasks in different domains. For each classification task, we collected 5 to 20 millions of videos in the past 3 months. The labels under different taxonomies are human labeled. We extract 5%-10% data as the evaluation dataset and leave the rest as training dataset based on the available amount of data of each task. 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.2">Model.</span> As mentioned, we employ X-VLM architecture <cite class="ltx_cite ltx_citemacro_citep">(Zeng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib21" title="">2022</a>)</cite> as our primary model for video classification. The model takes two type of inputs: visual and text. For visual input, we extract and concatenate video frames. For text input, we concatenate the video title with the overlay text within the video. The implementation is modified based on the code base released by X-VLM (https://github.com/zengyan-97/X-VLM). 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.3">Training.</span> Training commences from a in-house checkpoint pre-trained on over 5 billion short video data. We start with an initial learning rate of 1e-5, which decays over time. Models are trained on a single 8-A100 node, with training time ranging from 1 to 5 days depending on the data volume. 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.4">Baselines.</span> To verify the power of the proposed CPFD, Our proposed CPFD is benchmarked against the following models:</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">X-VLM</span>. Normally trained X-VLM model without any distillation. It is basically a reproduction of the paper X-VLM with some minor modification to adapt to business need.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">DF-X-VLM</span>. Dense Feature enhance X-VLM as described in Section 3.2 and Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S3.F1" title="Figure 1 ‣ 3.2. Dense Feature enhanced Multimodal Model (DF-X-VLM) ‣ 3. Methodology ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.1.1">PFD-X-VLM</span>. A X-VLM student model distilled from teacher DF-X-VLM using vanilla PFD method.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">To ensure a fair comparison, all methods are implemented on same code base, model starting checkpoint, learning rate, optimizer and other hyper-parameters based on empirical observations.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Offline Evaluation Results</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.1">Offline Evaluation Metrics.</span> To show a comprehensive comparison between proposed method and baselines, we compare ROC AUC, PR AUC and F1 score. PR AUC is particularly informative for evaluating binary classifiers on imbalanced datasets<cite class="ltx_cite ltx_citemacro_citep">(Saito and Rehmsmeier, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib15" title="">2015</a>)</cite>
<br class="ltx_break"/></p>
</div>
<figure class="ltx_table" id="S4.T4">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.1.1.1">Task</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.1.1.2">Task1</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.1.1.3">Task2</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.1.1.4">Task3</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.1.1.5">Task4</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.1.1.6">Task5</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S4.T4.1.2.1.1">Cost Ratio</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S4.T4.1.2.1.2">4.08</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S4.T4.1.2.1.3">3.5</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S4.T4.1.2.1.4">9</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S4.T4.1.2.1.5">4.5</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S4.T4.1.2.1.6">3.92</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4. </span>Additional Dense Feature Inference Cost compared with X-VLM model.</figcaption>
</figure>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1. </span>Performance Comparison</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">Results are detailed in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S4.T3" title="Table 3 ‣ 4. Experiments ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_tag">3</span></a>, from which we draw two main conclusions:</p>
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1">Proposed CPFD consistently outperforms other methods across all metrics and all tasks. Specifically, CPFD improves video classification F1 score by 6.76% over the standard X-VLM and by 2.31% over PFD.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1">Relatively, CPFD significantly reduced the performance F1 score gap between X-VLM and DF-X-VLM by 84.6% and that between PFD-X-VLM and DF-X-VLM by 62.2%. CPFD performs very closely with the teacher model DF-X-VLM, indicating minimum information loss removing the DF branch through distillation.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_table" id="S4.T5">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T5.1.1.1.1">Model</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T5.1.1.1.2">X-VLM</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T5.1.1.1.3">DF-X-VLM</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T5.1.1.1.4">PFD</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T5.1.1.1.5">CPFD</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.1.2.1.1">Hit Rate</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.1.2.1.2">19.82%</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.1.2.1.3">28.00%</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.1.2.1.4">26.88%</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.1.2.1.5">28.00%</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T5.1.3.2.1">Relative Difference</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T5.1.3.2.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T5.1.3.2.3">+41.27%</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T5.1.3.2.4">+35.62%</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T5.1.3.2.5">+41.27%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5. </span>Performance Metrics Across Different Models in Online A/B experiment. PFD represents PFD X-VLM and CPFD represents CPFD X-VLM.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2. </span>Ablation Study</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">In this section, we study the influence of different mapping function designs and temperature setting during distillation 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS2.p1.1.1">Mapping Functions.</span> We evaluated four empirically derived mapping functions, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S3.T2" title="Table 2 ‣ 3.4.2. Implementing CPFD ‣ 3.4. Confidence-aware Privileged Feature Distillation (CPFD) ‣ 3. Methodology ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_tag">2</span></a>. The four mapping functions including the thresholds and parameters we provided here are purely empirical, inspired by some popular activation function based on some basic facts:</p>
<ul class="ltx_itemize" id="S4.I3">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p" id="S4.I3.i1.p1.1">The output range needs to between 0 and 1</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i2.p1">
<p class="ltx_p" id="S4.I3.i2.p1.1">The function should be a monotonically increasing function with the loss input.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1">We experimented with all four mapping functions on Task1,2 and 3. he following observations can be drawn based on results in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S4.T6" title="Table 6 ‣ 4.2.2. Ablation Study ‣ 4.2. Offline Evaluation Results ‣ 4. Experiments ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_tag">6</span></a></p>
<ul class="ltx_itemize" id="S4.I4">
<li class="ltx_item" id="S4.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I4.i1.p1">
<p class="ltx_p" id="S4.I4.i1.p1.1">There is no definitive best performer, though Exponential Decay functions yields slightly better outcomes due to varying data distributions.</p>
</div>
</li>
<li class="ltx_item" id="S4.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I4.i2.p1">
<p class="ltx_p" id="S4.I4.i2.p1.1">The performance of different mapping design is relatively stable and consistently outperforms X-VLM and PFD-X-VLM</p>
</div>
</li>
<li class="ltx_item" id="S4.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I4.i3.p1">
<p class="ltx_p" id="S4.I4.i3.p1.1">Mapping functions and their parameters should be selected according to the specific data distribution and problem context. While we present several promising options here, there may be more optimal mapping function designs that could further enhance performance</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_table" id="S4.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T6.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T6.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T6.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.1.2.1">Threshold</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T6.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.1.3.1">Neg Sigmoid</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T6.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.1.4.1">Tanh</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T6.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.1.5.1">Exp Decay</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T6.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T6.1.2.1.1">Task1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.1.2">0.9453</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.1.3">0.9441</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.1.4">0.9439</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.1.5">0.9460</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.1.3.2.1">Task2</th>
<td class="ltx_td ltx_align_center" id="S4.T6.1.3.2.2">0.9933</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.3.2.3">0.9932</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.3.2.4">0.9933</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.3.2.5">0.9935</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T6.1.4.3.1">Task3</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T6.1.4.3.2">0.9550</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T6.1.4.3.3">0.9549</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T6.1.4.3.4">0.9550</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T6.1.4.3.5">0.9542</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6. </span>Comparison of Model AUC with Different Mapping Functions across Tasks.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS2.p3">
<p class="ltx_p" id="S4.SS2.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS2.p3.1.1">Temperature.</span> We also ablated the important parameter Temperature during distillation <cite class="ltx_cite ltx_citemacro_citep">(Hinton et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#bib.bib7" title="">2015</a>)</cite>. Due to space limitations, we only display the experimental results for Task1. The results in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S4.T7" title="Table 7 ‣ 4.2.2. Ablation Study ‣ 4.2. Offline Evaluation Results ‣ 4. Experiments ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_tag">7</span></a>. show that T=1 works the best in our setting. So to make a fair comparison, we used 1 as the default temperature across all our experiments.</p>
</div>
<figure class="ltx_table" id="S4.T7">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T7.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T7.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T7.1.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T7.1.1.1.2.1">T=0.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T7.1.1.1.3.1">T=1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T7.1.1.1.4.1">T=2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S4.T7.1.2.2.1">ROC AUC</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T7.1.2.2.2">0.9402</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T7.1.2.2.3">0.9460</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T7.1.2.2.4">0.9439</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7. </span>Comparison of Model AUC with Different Temperature on Task1.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3. </span>Computational Cost</h4>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">We calculate the required online resources to inference the dense features compared with the inference cost of X-VLM models among all 5 different tasks and present the results in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S4.T4" title="Table 4 ‣ 4.2. Offline Evaluation Results ‣ 4. Experiments ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_tag">4</span></a>. On average, the DF-X-VLM model spends 5 times more resources compared with the X-VLM model among the 5 tasks while some of the features are counted repeated when used in different tasks. It shows that CPFD is a better option due to comparable classification performance and lower computation resources.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p2">
<p class="ltx_p" id="S4.SS2.SSS3.p2.1">In addition to resource consumption, CPFD also significantly improve the online service stability. It liberates the model from the dependency on specific features during online inference, allowing reliance solely on raw inputs such as video, text, or audio and mitigating online-offline model performance difference. Further more, it largely enhanced the model capability by granting the flexibility to leverage a broader range of privileged features during training, without the complications associated with online service and deployment. Lastly, it reduces operational costs and simplifies the expansion of applications.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Online Experiments</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">We conduct an online A/B experiment to further evaluate the effectiveness of the proposed approach. Specifically, we deployed 4 different models targeting Task 3 in the production system and the models are evaluated in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S4.T5" title="Table 5 ‣ 4.2.1. Performance Comparison ‣ 4.2. Offline Evaluation Results ‣ 4. Experiments ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_tag">5</span></a>. We employ the X-VLM model as the control group and conduct the experiment for 5 consecutive days. We set specific model score thresholds for each model and created 4 different model based rules. New videos published with model scores higher than the thresholds will be recalled. Human reviewers will review all the recalled videos and decide if the videos are accurate. We adjust the thresholds to minimize the differences of total videos recalled per day between the baseline model and other models. During the experiment, we evaluate the results by the hit rate as the number of positive videos after human review within recalled videos by the number recalled videos. The result is presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03038v2#S4.T5" title="Table 5 ‣ 4.2.1. Performance Comparison ‣ 4.2. Offline Evaluation Results ‣ 4. Experiments ‣ CPFD: Confidence-aware Privileged Feature Distillation for Short Video Classification"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">From the result, we could observe PFD X-VLM significantly improves performance compared with X-VLM. CPFD XVLM further improves the performance and achieve same hit rate as the teacher model DF-X-VLM, again indicating almost no information loss during distillation</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we introduce a Confidence-aware Privileged Feature Distillation (CPFD) approach tailored for video classification. This method effectively utilizes information from privileged dense features without incurring additional inference costs. Experimented on real-world datasets, both offline and online evaluation results consolidate the effectiveness of CPFD. This framework not only enhances classification performance but also maintains operational efficiency. It has been successfully integrated into production systems with multiple deployed models.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
We extend our gratitude to Zeya Wang for his invaluable assistance during the initial stages of this project. We also thank Zhiqian Chen and Kenan Xiao for their contributions to the offline experiments. Special thanks go to Ardalan Mehrani, Zhongrong Zuo, and Chengkai Jin for their insightful technical discussions.

</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Binh et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Le Binh, Rajat Tandon, Chingis Oinar, Jeffrey Liu, Uma Durairaj, Jiani Guo, Spencer Zahabizadeh, Sanjana Ilango, Jeremy Tang, Fred Morstatter, et al<span class="ltx_text" id="bib.bib2.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Samba: Identifying Inappropriate Videos for Young Children on YouTube. In <em class="ltx_emph ltx_font_italic" id="bib.bib2.4.1">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</em>. 88–97.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Mithun Das, Rohit Raj, Punyajoy Saha, Binny Mathew, Manish Gupta, and Animesh Mukherjee. 2023.

</span>
<span class="ltx_bibblock">Hatemm: A multi-modal dataset for hate video classification. In <em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">Proceedings of the International AAAI Conference on Web and Social Media</em>, Vol. 17. 1014–1023.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dou et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, et al<span class="ltx_text" id="bib.bib4.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">An empirical study of training end-to-end vision-and-language transformers. In <em class="ltx_emph ltx_font_italic" id="bib.bib4.4.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 18166–18176.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gui et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xiaoqiang Gui, Yueyao Cheng, Xiang-Rong Sheng, Yunfeng Zhao, Guoxian Yu, Shuguang Han, Yuning Jiang, Jian Xu, and Bo Zheng. 2024.

</span>
<span class="ltx_bibblock">Calibration-compatible Listwise Distillation of Privileged Features for CTR Prediction. In <em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Proceedings of the 17th ACM International Conference on Web Search and Data Mining</em>. 247–256.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. 2018.

</span>
<span class="ltx_bibblock">Co-teaching: Robust training of deep neural networks with extremely noisy labels.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">Advances in neural information processing systems</em> 31 (2018), 8536–8546.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinton et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.

</span>
<span class="ltx_bibblock">Distilling the knowledge in a neural network.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">arXiv preprint arXiv:1503.02531</em> (2015).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022.

</span>
<span class="ltx_bibblock">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">International conference on machine learning</em>. PMLR, 12888–12900.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. 2021.

</span>
<span class="ltx_bibblock">Align before fuse: Vision and language representation learning with momentum distillation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">Advances in neural information processing systems</em> 34 (2021), 9694–9705.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lopez-Paz et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
David Lopez-Paz, Léon Bottou, Bernhard Schölkopf, and Vladimir Vapnik. 2015.

</span>
<span class="ltx_bibblock">Unifying distillation and privileged information.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">arXiv preprint arXiv:1511.03643</em> (2015).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ortiz-Jimenez et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Guillermo Ortiz-Jimenez, Mark Collier, Anant Nawalgaria, Alexander Nicholas D’Amour, Jesse Berent, Rodolphe Jenatton, and Efi Kokiopoulou. 2023.

</span>
<span class="ltx_bibblock">When does privileged information explain away label noise?. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">International Conference on Machine Learning</em>. PMLR, 26646–26669.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Page et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Pranav S Page, Anand S Siyote, Vivek S Borkar, and Gaurav S Kasbekar. 2023.

</span>
<span class="ltx_bibblock">Node Cardinality Estimation in the Internet of Things Using Privileged Feature Distillation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">arXiv preprint arXiv:2310.18664</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Peng Qi, Yuyan Bu, Juan Cao, Wei Ji, Ruihao Shui, Junbin Xiao, Danding Wang, and Tat-Seng Chua. 2023.

</span>
<span class="ltx_bibblock">Fakesv: A multimodal benchmark with rich social context for fake news detection on short video platforms. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, Vol. 37. 14444–14452.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al<span class="ltx_text" id="bib.bib14.3.1">.</span> 2021.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision. In <em class="ltx_emph ltx_font_italic" id="bib.bib14.4.1">International conference on machine learning</em>. PMLR, 8748–8763.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saito and Rehmsmeier (2015)</span>
<span class="ltx_bibblock">
Takaya Saito and Marc Rehmsmeier. 2015.

</span>
<span class="ltx_bibblock">The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">PloS one</em> 10, 3 (2015), e0118432.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shu et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. 2019.

</span>
<span class="ltx_bibblock">Meta-weight-net: Learning an explicit mapping for sample weighting.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">Advances in neural information processing systems</em> 32 (2019), 12 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. 2021.

</span>
<span class="ltx_bibblock">SimVLM: Simple Visual Language Model Pretraining with Weak Supervision. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Hongxin Wei, Lei Feng, Xiangyu Chen, and Bo An. 2020.

</span>
<span class="ltx_bibblock">Combating noisy labels by agreement: A joint training method with co-regularization. In <em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 13726–13735.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Chen Xu, Quan Li, Junfeng Ge, Jinyang Gao, Xiaoyong Yang, Changhua Pei, Fei Sun, Jian Wu, Hanxiao Sun, and Wenwu Ou. 2020.

</span>
<span class="ltx_bibblock">Privileged features distillation at taobao recommendations. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>. 2590–2598.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Shuo Yang, Sujay Sanghavi, Holakou Rahmanian, Jan Bakus, and Vishwanathan SVN. 2022.

</span>
<span class="ltx_bibblock">Toward understanding privileged features distillation in learning-to-rank.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">Advances in Neural Information Processing Systems</em> 35 (2022), 26658–26670.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yan Zeng, Xinsong Zhang, and Hang Li. 2022.

</span>
<span class="ltx_bibblock">Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts. In <em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">International Conference on Machine Learning</em>. PMLR, 25994–26009.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Hailin Zhang, Defang Chen, and Can Wang. 2022.

</span>
<span class="ltx_bibblock">Confidence-aware multi-teacher knowledge distillation. In <em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>. IEEE, 4498–4502.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Songyuan Zhang, Zhangjie Cao, Dorsa Sadigh, and Yanan Sui. 2021.

</span>
<span class="ltx_bibblock">Confidence-aware imitation learning from demonstrations with varying optimality.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">Advances in Neural Information Processing Systems</em> 34 (2021), 12340–12350.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Youcai Zhang, Zhonghao Lan, Yuchen Dai, Fangao Zeng, Yan Bai, Jie Chang, and Yichen Wei. 2020.

</span>
<span class="ltx_bibblock">Prime-aware adaptive distillation. In <em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIX 16</em>. Springer, 658–674.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xiaoyan Zhao, Min Yang, Qiang Qu, Ruifeng Xu, and Jieke Li. 2023.

</span>
<span class="ltx_bibblock">Exploring privileged features for relation extraction with contrastive student-teacher learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">IEEE Transactions on Knowledge and Data Engineering</em> 35, 8 (2023), 7953–7965.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Oct  7 01:58:29 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
