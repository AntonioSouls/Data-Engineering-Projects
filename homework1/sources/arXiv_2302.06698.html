<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2302.06698] An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection</title><meta property="og:description" content="Tree fruit breeding is a long-term activity involving repeated measurements of various fruit quality traits on a large number of samples. These traits are traditionally measured by manually counting the fruits, weighin…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2302.06698">

<!--Generated on Fri Mar  1 02:10:16 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Fazackerley, S., Jahagirdar, S., Lawrence, R., Liu, W., Long, S., Nagpal, R., Singh, A.">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\sidecaptionvpos</span>
<p id="p1.2" class="ltx_p">tfigure


</p>
</div>
<div id="p2" class="ltx_para">
<span id="p2.1" class="ltx_ERROR undefined">\tocauthor</span>
<p id="p2.2" class="ltx_p">Ritayu Nagpal, Sam Long, Shahid Jahagirdar, Weiwei Liu, Scott Fazackerley, Ramon Lawrence, Amritpal Singh</p>
</div>
<span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>
Department of Computer Science, University of British Columbia 
<br class="ltx_break">Kelowna, BC, Canada, V1V 2Z3
<br class="ltx_break"><span id="id1.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>ritayu.nagpal09@gmail.com, lsam8910@gmail.com, 
<br class="ltx_break">shahid.h.j@gmail.com, weiwei.liu2046@gmail.com, scott.fazackerley@alumni.ubc.ca, ramon.lawrence@ubc.ca</span></span></span>
</span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Agriculture and Agri-Food Canada
<br class="ltx_break">Summerland, BC, Canada, V0H 1Z0
<br class="ltx_break"><span id="id2.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>amritpal.singh@agr.gc.ca</span></span></span>
</span></span></span>
<h1 class="ltx_title ltx_title_document">An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ritayu Nagpal
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Sam Long
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Shahid Jahagirdar
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Weiwei Liu
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> 
<br class="ltx_break">Scott Fazackerley
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Ramon Lawrence
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Amritpal Singh
</span><span class="ltx_author_notes">22</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Tree fruit breeding is a long-term activity involving repeated measurements of various fruit quality traits on a large number of samples. These traits are traditionally measured by manually counting the fruits, weighing to indirectly measure the fruit size, and fruit colour is classified subjectively into different color categories using visual comparison to colour charts. These processes are slow, expensive and subject to evaluators’ bias and fatigue. Recent advancements in deep learning can help automate this process. Objective data can be generated for consistent characterization of germplasm, with greater speed and higher accuracy.
A method was developed to automatically count the number of sweet cherry fruits in a camera’s field of view in real time using YOLOv3. A system capable of analyzing the image data for other traits such as size and color was also developed using Python. The YOLO model obtained close to 99% accuracy in object detection and counting of cherries and 90% on the Intersection over Union metric for object localization when extracting size and colour information. The model surpasses human performance and offers a significant improvement compared to manual counting.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Deep Learning, Object Classification, Fruit, Cherry, High Throughput Phenotyping, YOLO, R-CNN, SSD, HAAR
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The Okanagan Valley in British Columbia, Canada is one of the largest cherry producing regions in the country <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> accounting for over 89% of Canada’s production of sweet cherries <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. The Agriculture and Agri-Food Canada (AAFC) research station located in Summerland, BC, has produced many of the premier cherry varieties currently in commercial production and has a large, ongoing research program to develop new varieties with traits such as improved fruit quality, later harvest, and self-fertility <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. New improved cultivars of fruits produced through selective breeding for desirable traits are known to significantly increase the economic growth and success of the horticulture industry.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The process of tree fruit breeding involves development of crosses (hybridizations) using distinct genotypes, followed by several stages of evaluation of the germplasm for numerous traits over multiple years and locations before the commercialization of a new cultivar. Germplasm is a piece of live plant tissue from which a new plant can be grown and is a common method used for crop propagation. Due to the long juvenile period and perennial nature of trees, this process can take more than 20 years. The resources and time required for evaluation of the germplasm for various traits is a major bottleneck in this process.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Traditionally, in each phase of germplasm evaluation, most of the traits are measured manually, which involves tedious and repetitive tasks such as counting fruits, size measurement, and color classification. The assessment of these traits is referred to as phenotyping. The tree fruit germplasm evaluation process is therefore vulnerable to worker fatigue and subjectivity, resulting in errors and inconsistencies in the collection of fruit trait data.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">With recent advancements in image analysis, computer vision and artificial intelligence, many of the aforementioned manual tasks involved in the process of tree fruit breeding can be fully or partially automated.
Open source tools and methods were explored and a Python-based application utilizing YOLO (You Only Look Once) and OpenCV for real-time sweet cherry detection was developed allowing for increased productivity and consistency in analysis.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">A variety of open-source object detection models were considered. These models varied from traditional image processing techniques such as Haar-Feature based classifiers to more modern deep learning architectures such as Fast-R-CNN, YOLO (You Only Look Once) and SSDs (Single-Shot Detectors). Techniques such as transfer learning were employed to train the models quickly and accurately on the labelled custom dataset. The final model was integrated with Tkinter and OpenCV libraries in Python to make a full-fledged desktop application with all the image statistics and analysis results such as size, color, and counts collected and stored in a spreadsheet for further analysis.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Automation can help breeding programs in improving the overall efficiency of the evaluation process by providing unbiased and precise data in a swift manner while avoiding costs arising from tedious procedures and possible human errors. Under controlled environments, the application is capable of generating accurate counts of sweet cherries, as well as their respective size and color information in an instant, surpassing the performance of humans on the same task in terms of both time and accuracy.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The contributions of this work are:</p>
</div>
<div id="S1.p8" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Evaluation of numerous image processing classifiers on the cherry detection and classification problem</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Development of an integrated application for statistics collection using the image processing classifier</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Analysis of the benefits of automation and the real-world impact on development of cherry cultivars in breeding programs</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">A complete application that offers significant improvements in time, data quality and testing results for cherry phenotyping.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">The paper outline is as follows. Section <a href="#S2" title="2 Background ‣ An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> discusses current techniques used for cherry phenotyping and presents different deep learning approaches for object classification. The dataset, model training and model selection are in Section <a href="#S3" title="3 Model Development and Data ‣ An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Information on models and analysis is in Section <a href="#S4" title="4 Analysis and Results ‣ An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The paper closes with future work and conclusions.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Properties such as fruit size and color of each genotype of cherry are among the key factors in the evaluation and selection of cherry germplasm. Traditionally, these properties are evaluated manually. The following steps are utilized:</p>
</div>
<div id="S2.p2" class="ltx_para">
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">Two sets of 100 cherries are counted from a selected genotype, and weighed to produce an estimation of fruit size in terms of average fruit weight.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">Cherries are assigned a color number on a scale of 1 to 7 by an evaluator based on a CTIFL standard cherry color card<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://www.ctifl.fr</span></span></span>.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">A sub-sample of cherries are put onto a firmness tester instrument in batches to measure the firmness and size distribution of each cherry.</p>
</div>
</li>
</ul>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Ensuring each cherry sample comprises exactly 100 cherries is crucial. The same amount of cherries ensures each genotype is evaluated on the same base line in the estimation of size and weight. Traditionally, cherries are counted manually one by one. This process is time consuming and prone to miscounts. It takes about 60 sec to count a sample of 100 cherries.</p>
</div>
<figure id="S2.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F1.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2302.06698/assets/img/CherryCountTray.png" id="S2.F1.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="397" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S2.F1.sf1.3.2" class="ltx_text" style="font-size:90%;">Cherry count tray </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F1.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2302.06698/assets/img/firmtest.png" id="S2.F1.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="359" height="441" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S2.F1.sf2.3.2" class="ltx_text" style="font-size:90%;">Cherry size and firmness measurement machine</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.3.2" class="ltx_text" style="font-size:90%;">Cherry measurement apparatus</span></figcaption>
</figure>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">To make this process more efficient, AAFC introduced a 3D printed tray with 25 cubicles (Figure <a href="#S2.F1.sf1" title="In Figure 1 ‣ 2 Background ‣ An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(a)</span></a>). Without counting, the operator only needs to put one cherry in each cubicle, and as long as all cubicles are filled, the tray will contain 25 cherries. Repeating the process four times results in a 100 cherry sample. This method has eliminated the requirement of counting cherries one by one, but it still takes more than 20 seconds to finish loading 4 batches of 25 cherries in a weighing tray. This method improved the accuracy, however, there can still be some accidental errors in getting exactly 100 cherries.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">To evaluate the color of each cherry sample, the evaluator observes the cherries collectively, and then compares the color of the cherries with the CTIFL cherry color chart to classify the fruits. There are seven classes of cherry colors in the CTIFL colour chart, and each cherry sample is categorized into one or two classes. The major drawback of this evaluation process is that the evaluation is subjective, as the samples could be categorized into different color categories by different evaluators, or by the same evaluator at a different time. This method is further impacted by ambient lighting conditions and evaluator fatigue.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">One method to estimate the cherry size is through weight averaging. Since the density of each cherry is approximately the same, the overall size of each cherry genotype is estimated by weight index. To get an accurate estimate of cherry size, the diameter of each fruit is measured using an instrument that also measures the fruit firmness (Figure <a href="#S2.F1.sf2" title="In Figure 1 ‣ 2 Background ‣ An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(b)</span></a>).</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">Object recognition is used to describe a collection of related computer vision tasks that involve identifying objects in digital images/videos. An object detection model identifies known objects (objects a model is trained on) and outputs information about their positions in terms of bounding box coordinates.</p>
</div>
<div id="S2.p8" class="ltx_para">
<p id="S2.p8.1" class="ltx_p">Image classification is a task involving the prediction of the object class for input by providing a probabilistic output for all the classes available. Object localization refers to determining the location of one or more objects in an image and drawing a bounding box around these identified objects. Another method for object localization is by marking/classifying all the pixels of an image and generating an image mask. The later method for object localization is also referred to as object segmentation. Object detection performs both these tasks and localizes and classifies one or more objects in an image.</p>
</div>
<div id="S2.p9" class="ltx_para">
<p id="S2.p9.1" class="ltx_p">An object detection model is trained by providing images and labels corresponding to those images having classification and location information. Models can be trained with images that contain various types of fruits, along with a label that specifies the location of objects in the image and class of fruit they represent. Given an image, the model outputs a list of the objects detected, the location of a bounding box that contains each object, and a score that indicates the confidence that detection was correct.</p>
</div>
<div id="S2.p10" class="ltx_para">
<p id="S2.p10.1" class="ltx_p">Several object detection models have been developed over the years, and this work evaluates the most suitable object detection models for the cherry detection problem. Most of the modern modeling architectures fall under One-Step or Two-Step Object Detection techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S2.p11" class="ltx_para">
<p id="S2.p11.1" class="ltx_p">Two-Step Object Detection involves algorithms that first identify bounding boxes that may potentially contain objects and then classify each bounding box separately in the second step. The first step requires a Region Proposal Network, providing many regions that are then passed to common Deep Learning based classification architectures for the second step.</p>
</div>
<div id="S2.p12" class="ltx_para">
<p id="S2.p12.1" class="ltx_p">One-Step Object Detection algorithms combine the detection and classification step by introducing the idea of regressing the bounding box predictions. These algorithms are the go-to choices for most of the real-time object detection tasks due to their speed of detection.</p>
</div>
<div id="S2.p13" class="ltx_para">
<p id="S2.p13.1" class="ltx_p">Neural networks have been used for the identification of unlabelled data based on labelled training data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.
Further improvements have been seen with the introduction of convolutional neural networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> in terms of overall prediction. They have been used for plant classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Previous work has examined using machine learning with deep neural networks for general fruit classification using EfficientNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Results indicate deep neural networks offer superior prediction abilities but the increased performance comes with higher computation cost. Detection, classification and mapping systems have been developed for coffee assessment on branch <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and during harvest <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Previous work has examined the assessment of fruit on trees for improvements on harvesting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. To our knowledge, there is no published work using deep learning to improve the assessment and classification of cherry phenotyping in cherry breeding programs.</p>
</div>
<div id="S2.p14" class="ltx_para">
<p id="S2.p14.1" class="ltx_p">Different object detection models proposed have their own strengths and weaknesses. Commonly investigated models are the HAAR Cascade Classifier <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, Region-Based Convolutional Neural Networks (R-CNN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, the Single Shot Detector (SSD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and You Only Look Once (YOLO) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> models.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>HAAR Cascade Classifier</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">HAAR Cascade is a machine learning object detection algorithm used to identify objects in an image based on the concept of features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. A cascade function is trained from a large collection of positive and negative images. Once trained, it is then used to detect objects in other images. The algorithm has four stages:</p>
<ul id="S2.I2" class="ltx_itemize">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p">HAAR feature selection</p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p id="S2.I2.i2.p1.1" class="ltx_p">Creating integral images</p>
</div>
</li>
<li id="S2.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i3.p1" class="ltx_para">
<p id="S2.I2.i3.p1.1" class="ltx_p">Adaboost training</p>
</div>
</li>
<li id="S2.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i4.p1" class="ltx_para">
<p id="S2.I2.i4.p1.1" class="ltx_p">Cascading classifiers</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">This method is well known for detecting faces and body parts in an image and can be trained to identify almost any object. The algorithm needs a large number of positive images of cherries and negative images without cherries to train the classifier. The model is then trained while implementing the four steps.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Faster R-CNN</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">R-CNN models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> are two-step object detection models known as <em id="S2.SS2.p1.1.1" class="ltx_emph ltx_font_italic">Regions with CNN Features</em> or <em id="S2.SS2.p1.1.2" class="ltx_emph ltx_font_italic">Region-Based Convolutional Neural Network</em>. The R-CNN model is comprised of three steps incorporating:</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<ul id="S2.I3" class="ltx_itemize">
<li id="S2.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i1.p1" class="ltx_para">
<p id="S2.I3.i1.p1.1" class="ltx_p">Region Proposal: Generate and extract category independent region proposals, e.g. possible bounding boxes</p>
</div>
</li>
<li id="S2.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i2.p1" class="ltx_para">
<p id="S2.I3.i2.p1.1" class="ltx_p">Feature Extractor: Extract features from all the proposed regions in the first step using a deep convolutional neural network</p>
</div>
</li>
<li id="S2.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i3.p1" class="ltx_para">
<p id="S2.I3.i3.p1.1" class="ltx_p">Classifier: Classify features as one of the known classes</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">The R-CNN and Fast R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> use selective search to find the region proposals. Selective search is a slow and time-consuming process affecting the performance of the network. Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> was introduced that eliminates the selective search algorithm and lets the network learn the region proposals. For the Faster R-CNN model the image is provided as an input to a convolutional network that provides a convolutional feature map. Instead of using a selective search algorithm
on the feature map to identify the region proposals, a separate network is used to predict the region proposals. The predicted region proposals are then reshaped using a RoI (Region of Interest) pooling layer that is then used to classify the image within the proposed region and predict the offset values for the bounding boxes.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">The architecture of the model takes a set of region proposals as input from the image that are passed through a deep convolutional neural network. The end of the deep CNN is a custom layer called RoI Pooling, which extracts features specific for a given input proposed regions. The model splits into two outputs, one for the class prediction step involving a softmax/sigmoid layer, and another with a linear output for the bounding box. This process is then repeated multiple times for each region of interest. Faster R-CNN is considerably faster than R-CNN and Fast R-CNN and is a potential option for real time object detection.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Single Shot Detectors</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Unlike the previous models of object detection which involved having one part of the network dedicated to providing region proposals followed by a high-quality classifier to classify these proposals in the second part, Single Shot Detector (SSD) methods are very accurate but come with a drawback of large computational cost (low frame rate). They are not suitable for real-time applications.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">An alternate strategy for object detection can be accomplished by combining these two tasks into one network. Rather than having a network provide region proposals, a model can have a set of predefined boxes to identify objects. Using convolutional features maps from later layers of a network, the model can use small CONV filters over these features maps to predict class scores and bounding box offsets. As the model only requires one single shot to detect multiple objects within the image, this model was termed as Single Shot Detectors.
In the final layers, each pixel represents a larger area of the input image and this is used to infer the object position. SSD has two components: a backbone model and SSD Head. The SSD head is just one or more convolutional layers added to the backbone and the outputs are interpreted as the bounding boxes and classes of objects in the spatial location of the final layer activations.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>You Only Look Once (YOLO)</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">YOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> is a convolutional neural network (CNN) for performing object detection tasks in real-time. The algorithm applies a single neural network to the full image, and then divides the image into regions and predicts bounding boxes and probabilities for each region. These bounding boxes are weighted by the predicted probabilities. The single neural network simultaneously predicts multiple bounding boxes and class probabilities for those boxes. YOLO trains on full images and directly optimizes detection performance. This model is extremely fast and learns generalizable representations of objects and outperforms its competitor models.
YOLOv3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> uses a variant of Darknet, which originally has a 53-layer network trained on ImageNet that offers significant improvements over previous YOLO models. For detection, 53 more layers are stacked onto it, forming a 106 layer fully convolutional architecture.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Model Development and Data</h2>

<figure id="S3.F2" class="ltx_figure">
<p id="S3.F2.1" class="ltx_p ltx_align_center"><span id="S3.F2.1.1" class="ltx_text"><img src="/html/2302.06698/assets/img/workflowDiagram.png" id="S3.F2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="479" height="308" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.4.2" class="ltx_text" style="font-size:90%;">Workflow diagram</span></figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">A variety of tools, software and frameworks were used for building the application and interface for the detection of cherries. Figure <a href="#S3.F2" title="Figure 2 ‣ 3 Model Development and Data ‣ An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> describes the complete workflow. The project was developed with Python. The system contains:</p>
</div>
<div id="S3.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">OpenCV<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://opencv.org/</span></span></span></span>: an open source project written in C++ with bindings for various other languages (Python, Java) (OpenCV version 4.3).</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">TensorFlow<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://www.tensorflow.org</span></span></span></span>: a deep learning library developed and maintained by Google. TensorFlow has primarily been used to train the model, define the architectures of different deep learning models and validate the performance of the model. The SSD and Faster R-CNN models were trained using TensorFlow.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Tkinter<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://wiki.python.org/moin/TkInter</span></span></span></span>: a Python library that helps with Graphical User Interface development (GUI) for desktop applications.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">Google Colab<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://colab.research.google.com</span></span></span></span>: a free virtual environment provided by Google with different virtual machines powered by GPUs that enable training and development of deep learning models. All models were trained with Google Colab.</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p">LabelImg<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://github.com/tzutalin/labelImg</span></span></span></span>: a Python-based library to manually annotate the images.</p>
</div>
</li>
</ul>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Agriculture and Agri-Food Canada provided test images. 360 raw JPEG images containing several cherries in addition to the label cards to help in classifying the cherries to a category were analyzed. A ruler (scale) was also placed on each image that gives the approximate size of each cherry.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">Pre-processing the images involved manually annotating the images using the LabelImg library. A bounding box was drawn around each cherry in the image. After drawing the boxes, the image is saved either in the YOLO format (an XML format) or PASCAL VOC format (a text file). After manual annotation of images, the data is usable for all of the deep learning architectures.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Model Training</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Training different deep learning architectures on the data is an iterative process. The process involves configuring the deep learning architecture, continuously tuning the hyperparameters, measuring loss after each epoch to see if it is improving and finally monitoring the metrics such as the accuracy, the number of false positives and false negatives. For this application, the data was used to train three different deep learning architectures using YOLOv3, Faster R-CNN and SSD. For SSD and Faster R-CNN, the models were trained using TensorFlow. Both the YOLOv3 base model and YOLOv3 with Efficient Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> as the backbone model were trained for comparison.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Model Performance Evaluation Metrics</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">When comparing the performance of the models, the Intersection over Union (IoU) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and average precision and accuracy were considered. Bounding box predictions are not very precise on the pixel level, and thus a metric is required for the extent of overlap between two bounding boxes (true and predicted boxes). Intersection over Union takes the area of intersection of the two bounding boxes and divides it with the area of their union (Equation <a href="#S3.E1" title="In 3.2 Model Performance Evaluation Metrics ‣ 3 Model Development and Data ‣ An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) as:</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<table id="S6.EGx1" class="ltx_equationgroup ltx_eqn_eqnarray ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="S3.E1.2.1.1.1.1" class="ltx_text ltx_markedasmath">IoU</span></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math id="S3.E1.m2.1" class="ltx_Math" alttext="\displaystyle=" display="inline"><semantics id="S3.E1.m2.1a"><mo id="S3.E1.m2.1.1" xref="S3.E1.m2.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S3.E1.m2.1b"><eq id="S3.E1.m2.1.1.cmml" xref="S3.E1.m2.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m2.1c">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1.m3.1" class="ltx_math_unparsed" alttext="\displaystyle\frac{\text{Area of Overlap}}{\text{Area of Union}.}" display="inline"><semantics id="S3.E1.m3.1a"><mstyle displaystyle="true" id="S3.E1.m3.1.1"><mfrac id="S3.E1.m3.1.1a"><mtext id="S3.E1.m3.1.1.2">Area of Overlap</mtext><mrow id="S3.E1.m3.1.1.3"><mtext id="S3.E1.m3.1.1.3.1">Area of Union</mtext><mo lspace="0em" id="S3.E1.m3.1.1.3.2">.</mo></mrow></mfrac></mstyle><annotation encoding="application/x-tex" id="S3.E1.m3.1b">\displaystyle\frac{\text{Area of Overlap}}{\text{Area of Union}.}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">IoU produces a score between 0 and 1 that represents the quality of overlap between the two boxes. A score of 1 means a perfect match between the two boxes and 0 means no match between the two boxes.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">Precision provides information on how accurate the model predictions are, and recall denotes whether it can detect all objects present in the image. Average Precision (AP) is a widely used metric in object detection evaluation. Accuracy in counting cherries in the image was used to measure model performance.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Model Selection</h3>

<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F3.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2302.06698/assets/img/HAAR.png" id="S3.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="366" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F3.sf1.3.2" class="ltx_text" style="font-size:90%;">HAAR model detection. </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F3.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2302.06698/assets/img/SSD.png" id="S3.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="366" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F3.sf2.3.2" class="ltx_text" style="font-size:90%;">SSD model detection</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F3.sf3" class="ltx_figure ltx_figure_panel"><img src="/html/2302.06698/assets/img/R-CNN.png" id="S3.F3.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="366" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S3.F3.sf3.3.2" class="ltx_text" style="font-size:90%;">R-CNN model detection</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F3.sf4" class="ltx_figure ltx_figure_panel"><img src="/html/2302.06698/assets/img/YOLO.png" id="S3.F3.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="366" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S3.F3.sf4.3.2" class="ltx_text" style="font-size:90%;">YOLO model detection</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">Prediction results of the four models on the sample cherry image from test set using different modelling techniques</span></figcaption>
</figure>
<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Four models were trained with different techniques to identify the most suitable modeling architecture for our custom cherry dataset: the HAAR – Feature Cascade Classifier, Faster R-CNN, Single Shot Detectors (SSD) and You Only Look Once (YOLO) models. The object detection results were analyzed on a testing set to identify which technique would be most suitable for the cherry image set.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Figure <a href="#S3.F3.sf1" title="In Figure 3 ‣ 3.3 Model Selection ‣ 3 Model Development and Data ‣ An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a> presents a sample image for the HAAR model. While the model is computationally efficient on prediction, the training is slow and accuracy is low. The model detects shadows as cherries as well as demonstrating a high variance based on image size and offers no multi-class detection.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Figure <a href="#S3.F3.sf2" title="In Figure 3 ‣ 3.3 Model Selection ‣ 3 Model Development and Data ‣ An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(b)</span></a> presents a sample for the SSD model. This neural network architecture takes a single shot to detect multiple objects within the image and combines region identification and class prediction steps into a single neural network. However, it fails to detect most of the objects. Additionally, the bounding boxes are not precise in terms of cherry size and location.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">Figure <a href="#S3.F3.sf3" title="In Figure 3 ‣ 3.3 Model Selection ‣ 3 Model Development and Data ‣ An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(c)</span></a> presents a sample for the R-CNN model using a 2-step approach to detect objects. For the cherry image set, the detection accuracy is very low. Further, it requires a large training data set and is computationally expensive.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">Figure <a href="#S3.F3.sf4" title="In Figure 3 ‣ 3.3 Model Selection ‣ 3 Model Development and Data ‣ An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(d)</span></a> presents a sample image for the YOLO model. While it utilizes a similar underlying concept to SSD models, YOLO uses a different neural network architecture. It is computationally efficient in real-time prediction. The model has a very high detection accuracy and precise detection of bounding boxes.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.1" class="ltx_p">From the comparison of the different models utilizing the cherry image set, the results indicate that the YOLO model is the suitable choice for cherry object detection. This model not only correctly identifies almost all of the cherries in the image but also provides fairly accurate bounding boxes for each cherry detected. Detailed results from subsequent training of YOLO models are in Section <a href="#S4" title="4 Analysis and Results ‣ An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Application Development</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The end-user application has a user interface and interactivity with Excel. The trained model was incorporated into the application with OpenCV and implements two steps:</p>
</div>
<section id="S3.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>Video Stream and Image Capture</h4>

<div id="S3.SS4.SSS1.p1" class="ltx_para">
<p id="S3.SS4.SSS1.p1.1" class="ltx_p">The application consists of switching on the USB web camera from OpenCV. Once the camera is switched on, each frame from the video is taken and passed to the YOLOv3 original and Efficient Net models. As each frame from the video is passed to the model, the model analyzes the frame and displays the corresponding bounding boxes on the screen where cherries are detected and the overall cherry count. Figure <a href="#S3.F4" title="Figure 4 ‣ 3.4.2 Extracting Feature Data ‣ 3.4 Application Development ‣ 3 Model Development and Data ‣ An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the display output. The reading at the top left is the cherry count from the YOLOv3 model.</p>
</div>
<div id="S3.SS4.SSS1.p2" class="ltx_para">
<p id="S3.SS4.SSS1.p2.1" class="ltx_p">Once the operator is satisfied with the count converging for both models, pressing the space bar generates a pop-up window for the operator allowing them to save the image from the video for additional analysis. The operator can enter the specific name of the genotype for that batch of cherries, and the image is saved in the images folder for that genotype and annotated with a timestamp.</p>
</div>
</section>
<section id="S3.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Extracting Feature Data</h4>

<div id="S3.SS4.SSS2.p1" class="ltx_para">
<p id="S3.SS4.SSS2.p1.1" class="ltx_p">The second part of the application consists of two scripts. The first script stores each image in the images folder and generates the predicted image with the bounding box using the YOLOv3 model. A CSV file is also created that gives confidence and bounding box coordinates for each cherry. This information is essential to crop each cherry from the actual image to get its size, color and various other properties. The second script is used to take each image in the images folder and run through a series of functions to populate the results spreadsheet. The size is measured using the largest dimension if it is a rectangle. The color classification is based on extracting the average red, green and blue content of each image and then comparing them with how close they are to the average intensity of different classes. This results in the generation of four spreadsheets. Table <a href="#S3.T1" title="Table 1 ‣ 3.4.2 Extracting Feature Data ‣ 3.4 Application Development ‣ 3 Model Development and Data ‣ An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes the information stored in each spreadsheet.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<p id="S3.F4.1" class="ltx_p ltx_align_center"><span id="S3.F4.1.1" class="ltx_text"><img src="/html/2302.06698/assets/img/process.png" id="S3.F4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="184" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.4.2" class="ltx_text" style="font-size:90%;">Application showing input and output images</span></figcaption>
</figure>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.3.2" class="ltx_text" style="font-size:90%;">Information captured from the application</span></figcaption>
<table id="S3.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.4.1.1" class="ltx_tr">
<th id="S3.T1.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T1.4.1.1.1.1" class="ltx_text ltx_font_bold">Sheet</span></th>
<th id="S3.T1.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T1.4.1.1.2.1" class="ltx_text ltx_font_bold">Details</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.4.2.1" class="ltx_tr">
<th id="S3.T1.4.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Summary</th>
<td id="S3.T1.4.2.1.2" class="ltx_td ltx_align_center ltx_border_t">
<table id="S3.T1.4.2.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.4.2.1.2.1.1" class="ltx_tr">
<td id="S3.T1.4.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">ImageID, Count, Avg size, Avg size - Top 50,</td>
</tr>
<tr id="S3.T1.4.2.1.2.1.2" class="ltx_tr">
<td id="S3.T1.4.2.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Average red/green/blue content, Average RGB content - Top50,</td>
</tr>
<tr id="S3.T1.4.2.1.2.1.3" class="ltx_tr">
<td id="S3.T1.4.2.1.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">Stem Avg Red, Stem Avg Green, Stem Avg Blue, Date and Time</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T1.4.3.2" class="ltx_tr">
<th id="S3.T1.4.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Cherry Size</th>
<td id="S3.T1.4.3.2.2" class="ltx_td ltx_align_center ltx_border_t">
<table id="S3.T1.4.3.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.4.3.2.2.1.1" class="ltx_tr">
<td id="S3.T1.4.3.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">ImageID, CherryID, Confidence, Cherry Size, Cherry Width/Height,</td>
</tr>
<tr id="S3.T1.4.3.2.2.1.2" class="ltx_tr">
<td id="S3.T1.4.3.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Cherry Size (mm), Cherry Width/Height (mm), Top 50,</td>
</tr>
<tr id="S3.T1.4.3.2.2.1.3" class="ltx_tr">
<td id="S3.T1.4.3.2.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">Box X/Y, Central Region, Scaled Box X/Y, Date, Time</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T1.4.4.3" class="ltx_tr">
<th id="S3.T1.4.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Cherry Colour</th>
<td id="S3.T1.4.4.3.2" class="ltx_td ltx_align_center ltx_border_t">
<table id="S3.T1.4.4.3.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.4.4.3.2.1.1" class="ltx_tr">
<td id="S3.T1.4.4.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Image ID, Cherry ID, Cherry Avg Red, Cherry Avg Green,</td>
</tr>
<tr id="S3.T1.4.4.3.2.1.2" class="ltx_tr">
<td id="S3.T1.4.4.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Cherry Avg Blue, Cherry Classification, Date, Time, Top 50</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T1.4.5.4" class="ltx_tr">
<th id="S3.T1.4.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">Stem Colour</th>
<td id="S3.T1.4.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">
<table id="S3.T1.4.5.4.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.4.5.4.2.1.1" class="ltx_tr">
<td id="S3.T1.4.5.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Image ID, Cherry ID, Stem Avg Red, Stem Avg Green,</td>
</tr>
<tr id="S3.T1.4.5.4.2.1.2" class="ltx_tr">
<td id="S3.T1.4.5.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Stem Avg Blue, Date, Time, Top 50</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS4.SSS2.p2" class="ltx_para">
<p id="S3.SS4.SSS2.p2.1" class="ltx_p">The summary sheet provides a summary of the entire image with the count and other traits. The second sheet is specific to each cherry in an image. The third sheet corresponds to the color details of each cherry in an image and the last sheet gives details about each stem of a cherry in an image. This is accomplished by cropping each cherry from the image using the coordinates produced by the text files script and extracting its average red, green and blue intensity. The average size, color and class of each cherry and its stem is recorded.</p>
</div>
<div id="S3.SS4.SSS2.p3" class="ltx_para">
<p id="S3.SS4.SSS2.p3.1" class="ltx_p">The entire code is packaged into two executable batch files. The first batch file will launch OpenCV and run the camera and save the images as required by the operator. The second batch file consists of the two scripts that are used to generate the text file for the images and update the Excel spreadsheet.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Analysis and Results</h2>

<figure id="S4.T3" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.2.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T3.3.2" class="ltx_text" style="font-size:90%;">Counting and IoU accuracy for the YOLOv3 original using Darknet-53 and the YOLOv3 Efficient Net B0 model for the training image set</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.T3.4" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.4.1.1" class="ltx_tr">
<th id="S4.T3.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.4.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S4.T3.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.4.1.1.2.1" class="ltx_text ltx_font_bold">Resize</span></th>
<th id="S4.T3.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.4.1.1.3.1" class="ltx_text ltx_font_bold">CT</span></th>
<th id="S4.T3.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.4.1.1.4.1" class="ltx_text ltx_font_bold">DC</span></th>
<th id="S4.T3.4.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.4.1.1.5.1" class="ltx_text ltx_font_bold">TC</span></th>
<th id="S4.T3.4.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.4.1.1.6.1" class="ltx_text ltx_font_bold">TP</span></th>
<th id="S4.T3.4.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.4.1.1.7.1" class="ltx_text ltx_font_bold">FP</span></th>
<th id="S4.T3.4.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.4.1.1.8.1" class="ltx_text ltx_font_bold">FN</span></th>
<th id="S4.T3.4.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.4.1.1.9.1" class="ltx_text ltx_font_bold">mAP @ 0.5</span></th>
<th id="S4.T3.4.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.4.1.1.10.1" class="ltx_text ltx_font_bold">mean IoU</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.4.2.1" class="ltx_tr">
<td id="S4.T3.4.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">original</td>
<td id="S4.T3.4.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">512x416</td>
<td id="S4.T3.4.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.5</td>
<td id="S4.T3.4.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6539</td>
<td id="S4.T3.4.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6498</td>
<td id="S4.T3.4.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6498</td>
<td id="S4.T3.4.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</td>
<td id="S4.T3.4.2.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S4.T3.4.2.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.98%</td>
<td id="S4.T3.4.2.1.10" class="ltx_td ltx_align_center ltx_border_t">94.98%</td>
</tr>
<tr id="S4.T3.4.3.2" class="ltx_tr">
<td id="S4.T3.4.3.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">efficientnet_B0</td>
<td id="S4.T3.4.3.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">480x480</td>
<td id="S4.T3.4.3.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.25</td>
<td id="S4.T3.4.3.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">6496</td>
<td id="S4.T3.4.3.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">6498</td>
<td id="S4.T3.4.3.2.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">6495</td>
<td id="S4.T3.4.3.2.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0</td>
<td id="S4.T3.4.3.2.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">3</td>
<td id="S4.T3.4.3.2.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">99.94%</td>
<td id="S4.T3.4.3.2.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">94.21%</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul id="S4.I1" class="ltx_itemize ltx_centering ltx_figure_panel">
<li id="S4.I1.1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S4.I1.1.p1" class="ltx_para">
<p id="S4.I1.1.p1.1" class="ltx_p"><span id="S4.I1.1.p1.1.1" class="ltx_text" style="font-size:90%;">Notes: Confidence Threshold (CT), Detection Counts with confidence threshold less than 0.1 (DC), True Counts (TC), True Positive (TP), False Positive (FP), False Negative (FN), mean accuracy precision with confidence threshold 0.5 mean (mAP@0.5), and mean intersection over union (mean IoU).
</span></p>
</div>
</li>
</ul>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.5.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.6.2" class="ltx_text" style="font-size:90%;">Counting and IoU accuracy for the YOLOv3 original using Darknet-53 and the YOLOv3 Efficient Net B0 model with the validation image set</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<table id="S4.T3.7" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.7.1.1" class="ltx_tr">
<th id="S4.T3.7.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.7.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S4.T3.7.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.7.1.1.2.1" class="ltx_text ltx_font_bold">Resize</span></th>
<th id="S4.T3.7.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.7.1.1.3.1" class="ltx_text ltx_font_bold">CT</span></th>
<th id="S4.T3.7.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.7.1.1.4.1" class="ltx_text ltx_font_bold">DC</span></th>
<th id="S4.T3.7.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.7.1.1.5.1" class="ltx_text ltx_font_bold">TC</span></th>
<th id="S4.T3.7.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.7.1.1.6.1" class="ltx_text ltx_font_bold">TP</span></th>
<th id="S4.T3.7.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.7.1.1.7.1" class="ltx_text ltx_font_bold">FP</span></th>
<th id="S4.T3.7.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.7.1.1.8.1" class="ltx_text ltx_font_bold">FN</span></th>
<th id="S4.T3.7.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.7.1.1.9.1" class="ltx_text ltx_font_bold">mAP @ 0.5</span></th>
<th id="S4.T3.7.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T3.7.1.1.10.1" class="ltx_text ltx_font_bold">mean IoU</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.7.2.1" class="ltx_tr">
<td id="S4.T3.7.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">original</td>
<td id="S4.T3.7.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">512x416</td>
<td id="S4.T3.7.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.5</td>
<td id="S4.T3.7.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3482</td>
<td id="S4.T3.7.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3455</td>
<td id="S4.T3.7.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3455</td>
<td id="S4.T3.7.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S4.T3.7.2.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S4.T3.7.2.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.97%</td>
<td id="S4.T3.7.2.1.10" class="ltx_td ltx_align_center ltx_border_t">95.16%</td>
</tr>
<tr id="S4.T3.7.3.2" class="ltx_tr">
<td id="S4.T3.7.3.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">efficientnet_B0</td>
<td id="S4.T3.7.3.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">480x480</td>
<td id="S4.T3.7.3.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.25</td>
<td id="S4.T3.7.3.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">3452</td>
<td id="S4.T3.7.3.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">3455</td>
<td id="S4.T3.7.3.2.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">3451</td>
<td id="S4.T3.7.3.2.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0</td>
<td id="S4.T3.7.3.2.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">4</td>
<td id="S4.T3.7.3.2.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">99.88%</td>
<td id="S4.T3.7.3.2.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">94.26%</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Three questions were considered when testing how the models performed:</p>
</div>
<div id="S4.p2" class="ltx_para">
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p">What is the counting and IoU accuracy of the models on static images?</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p">What is the counting accuracy and speed of the models in real time counting?</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p id="S4.I2.i3.p1.1" class="ltx_p">Is the extracted color and size accurate compared with traditional methods?</p>
</div>
</li>
</ul>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Two YOLO-based models evaluated: YOLOv3 Efficient Net B0 model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and YOLOv3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> original. In total, 364 images are in the data set. For most of these images, the cherry count is targeted to be 100, but as these images involved workers manually counting 100 cherries some variability in total count is observed. A total of 109 images out of these 364 images where randomly chosen and labelled using LabelImg. The models were trained with 72 randomly chosen images with the remaining 37 images used as a validation set. Finally, all models were tested with the complete 364 image set for performance comparison.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Counting and IoU Accuracy</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Table <a href="#S4.T3" title="Table 3 ‣ 4 Analysis and Results ‣ An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the results for YOLOv3 original and YOLOv3 Efficient Net B0 models for the 72 training images. Table <a href="#S4.T3" title="Table 3 ‣ 4 Analysis and Results ‣ An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the results for the models with the validation image set of 37 images. Table <a href="#S4.T4" title="Table 4 ‣ 4.1 Counting and IoU Accuracy ‣ 4 Analysis and Results ‣ An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents the overall performance of the trained models on the complete image set.</p>
</div>
<figure id="S4.T4" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.2.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.3.2" class="ltx_text" style="font-size:90%;">Counting accuracy for the YOLOv3 original using Darknet-53 and the YOLOv3 Efficient Net B0 model for complete image set</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.T4.4" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.4.1.1" class="ltx_tr">
<th id="S4.T4.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T4.4.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S4.T4.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T4.4.1.1.2.1" class="ltx_text ltx_font_bold">Resize</span></th>
<th id="S4.T4.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T4.4.1.1.3.1" class="ltx_text ltx_font_bold">CT</span></th>
<th id="S4.T4.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T4.4.1.1.4.1" class="ltx_text ltx_font_bold">TC</span></th>
<th id="S4.T4.4.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T4.4.1.1.5.1" class="ltx_text ltx_font_bold">TP</span></th>
<th id="S4.T4.4.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T4.4.1.1.6.1" class="ltx_text ltx_font_bold">FP</span></th>
<th id="S4.T4.4.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T4.4.1.1.7.1" class="ltx_text ltx_font_bold">FN</span></th>
<th id="S4.T4.4.1.1.8" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Human Count Error</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.4.2.1" class="ltx_tr">
<td id="S4.T4.4.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">original</td>
<td id="S4.T4.4.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">512x416</td>
<td id="S4.T4.4.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.5</td>
<td id="S4.T4.4.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">34963</td>
<td id="S4.T4.4.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">34961</td>
<td id="S4.T4.4.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</td>
<td id="S4.T4.4.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2</td>
<td id="S4.T4.4.2.1.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" rowspan="2"><span id="S4.T4.4.2.1.8.1" class="ltx_text">22</span></td>
</tr>
<tr id="S4.T4.4.3.2" class="ltx_tr">
<td id="S4.T4.4.3.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">efficientnet_B0</td>
<td id="S4.T4.4.3.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">480x480</td>
<td id="S4.T4.4.3.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.25</td>
<td id="S4.T4.4.3.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">34963</td>
<td id="S4.T4.4.3.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">34953</td>
<td id="S4.T4.4.3.2.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">2</td>
<td id="S4.T4.4.3.2.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">10</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul id="S4.I3" class="ltx_itemize ltx_centering ltx_figure_panel">
<li id="S4.I3.1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S4.I3.1.p1" class="ltx_para">
<p id="S4.I3.1.p1.1" class="ltx_p"><span id="S4.I3.1.p1.1.1" class="ltx_text" style="font-size:90%;">Notes: Confidence Threshold (CT), True Counts (TC), True Positive (TP), False Positive (FP), and False Negative (FN).
</span></p>
</div>
</li>
</ul>
</div>
</div>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">For the testing results, consideration was also given for reasons why the models predicted the counts incorrectly. Of all 364 images, the YOLOv3 Efficient Net B0 model predicts counts incorrectly for 11 images and 12 cherries missed. The reasons for missing cherries is that some cherries are on the frame edges of the images, some leaves are counted as cherries, and a few completely failed to be detected. The YOLOv3 model predicts three image counts wrong while incorrectly counting three cherries (one leaf counted as cherry and two cherries due to overlapping). There are a total of 22 images that were manually counted wrong with not exactly 100 cherries.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">From the training and validation results, it can be seen that both models achieved accuracy (based on AP metric) of over 99%, and the predicted bounding box around cherry accuracy (based on mean IoU metric) are over 94%. The results from the YOLOv3 original model perform slightly better in both metrics. It was decided to use YOLOv3 original as the model to generate results when dealing with static images. The testing results indicate that the models outperformed the traditional method of manually counting. The Efficient Net model predicted 11 wrong counts and the original model predicted only three wrong counts, while the manually counting method produced 22 wrong counts. From the testing results, wrong counts by the models are due to either cherries are on the camera frame edge or the cherries are overlapping. Both of these scenarios can be avoided when displaying cherries real time in the future for the operator to view. Both of the models can be used on the real time counting application to check with each other to ensure that the count is correct.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Real Time Counting Accuracy and Speed</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">After training, validating, tuning and testing the models, an application was developed to use the models to perform the real time counting. Models were tested with several videos provided by AAFC mimicking the lab real time counting situation. It takes approximately 60 seconds to manually count 100 cherries. With the application, utilizing both models at the same time, the time to process the image and perform the count is approximately one second utilizing a consumer laptop. The YOLOv3 original model takes around 0.5 seconds to finish one inference, and the YOLOv3 Efficient Net B0 model takes around 0.1 seconds.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Color and Size Information Extraction</h3>

<figure id="S4.T6" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.T6.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:433.6pt;">
<p id="S4.T6.1.1" class="ltx_p ltx_align_center"><span id="S4.T6.1.1.1" class="ltx_text"><img src="/html/2302.06698/assets/img/bivariate.png" id="S4.T6.1.1.1.g1" class="ltx_graphics ltx_img_square" width="374" height="305" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.T6.1.2.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S4.T6.1.3.2" class="ltx_text" style="font-size:90%;">Bivariate fit of YOLOv3</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.T6.3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:433.6pt;">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.T6.3.2.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="S4.T6.3.3.2" class="ltx_text" style="font-size:90%;">Summary Statistics for Bivariate fit of YOLOv3</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.T6.2.tab1" class="ltx_table ltx_figure_panel ltx_align_center">
<div id="S4.T6.2.tab1.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:103.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(65.4pt,-15.5pt) scale(1.43148024212393,1.43148024212393) ;">
<table id="S4.T6.2.tab1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T6.2.tab1.1.1.1.1" class="ltx_tr">
<th id="S4.T6.2.tab1.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="S4.T6.2.tab1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T6.2.tab1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Value</span></td>
<td id="S4.T6.2.tab1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T6.2.tab1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Lower 95%</span></td>
<td id="S4.T6.2.tab1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T6.2.tab1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Upper 95%</span></td>
<td id="S4.T6.2.tab1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.2.tab1.1.1.1.1.5.1" class="ltx_text ltx_font_bold">Signif. Prob</span></td>
</tr>
<tr id="S4.T6.2.tab1.1.1.2.2" class="ltx_tr">
<th id="S4.T6.2.tab1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Correlation</th>
<td id="S4.T6.2.tab1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.874896</td>
<td id="S4.T6.2.tab1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.848403</td>
<td id="S4.T6.2.tab1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.897017</td>
<td id="S4.T6.2.tab1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">&lt;.0001*</td>
</tr>
<tr id="S4.T6.2.tab1.1.1.3.3" class="ltx_tr">
<th id="S4.T6.2.tab1.1.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Covariance</th>
<td id="S4.T6.2.tab1.1.1.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">145.1858</td>
<td id="S4.T6.2.tab1.1.1.3.3.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T6.2.tab1.1.1.3.3.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T6.2.tab1.1.1.3.3.5" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S4.T6.2.tab1.1.1.4.4" class="ltx_tr">
<th id="S4.T6.2.tab1.1.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">Count</th>
<td id="S4.T6.2.tab1.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">364</td>
<td id="S4.T6.2.tab1.1.1.4.4.3" class="ltx_td ltx_border_b ltx_border_r ltx_border_t"></td>
<td id="S4.T6.2.tab1.1.1.4.4.4" class="ltx_td ltx_border_b ltx_border_r ltx_border_t"></td>
<td id="S4.T6.2.tab1.1.1.4.4.5" class="ltx_td ltx_border_b ltx_border_t"></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.T6.3.1.tab1" class="ltx_table ltx_figure_panel ltx_align_center">
<table id="S4.T6.3.1.tab1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T6.3.1.tab1.1.1.1" class="ltx_tr">
<th id="S4.T6.3.1.tab1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T6.3.1.tab1.1.1.1.1.1" class="ltx_text ltx_font_bold">Variable</span></th>
<td id="S4.T6.3.1.tab1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T6.3.1.tab1.1.1.1.2.1" class="ltx_text ltx_font_bold">Mean</span></td>
<td id="S4.T6.3.1.tab1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.3.1.tab1.1.1.1.3.1" class="ltx_text ltx_font_bold">Std Dev</span></td>
</tr>
<tr id="S4.T6.3.1.tab1.1.2.2" class="ltx_tr">
<th id="S4.T6.3.1.tab1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">img_geno.AFW_(g)</th>
<td id="S4.T6.3.1.tab1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1092.03</td>
<td id="S4.T6.3.1.tab1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">125.8303</td>
</tr>
<tr id="S4.T6.3.1.tab1.1.3.3" class="ltx_tr">
<th id="S4.T6.3.1.tab1.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">Avg Size (mm) - Al</th>
<td id="S4.T6.3.1.tab1.1.3.3.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_t">30.10884</td>
<td id="S4.T6.3.1.tab1.1.3.3.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_t">1.315497</td>
</tr>
</tbody>
</table>
</figure>
</div>
</div>
</figure>
</div>
</div>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">After verifying cherry counts using the models, a static image is captured. This image is then passed to YOLOv3 original model to get another inference to get the bounding box information for each cherry in the image. Based on the bounding box coordinates, the model generates information on the color, cherry size, and stem color. This information is stored in an Excel spreadsheet for additional analysis.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">To ensure that the two models produce color and size information suitable for use in plant phenotyping, the 364 static images were analyzed using this method. The results were compared to the results generated by traditional methods.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">To compare how much the proposed methods and the traditional method agree with each other, a bivariate plot, correlation analysis, and linear regression were conducted. The results from YOLOv3 original and YOLOv3 Efficient Net are very similar, so only the results from YOLOv3 original are shown in Figure <a href="#S4.T6" title="Table 6 ‣ 4.3 Color and Size Information Extraction ‣ 4 Analysis and Results ‣ An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. Table <a href="#S4.T6" title="Table 6 ‣ 4.3 Color and Size Information Extraction ‣ 4 Analysis and Results ‣ An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> contains summary statistics comparing YOLOv3 original with the traditional method. In the plot <em id="S4.SS3.p3.1.1" class="ltx_emph ltx_font_italic">Avg Size (mm)-All</em> is the predicted results from YOLOv3 original and <em id="S4.SS3.p3.1.2" class="ltx_emph ltx_font_italic">img_geno.AFW_(g)</em> is from the traditional method.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">In the bivariate plot, the two variables correlated quite well with very few outliers. This indicates that the proposed method and the traditional method agree with each other in the estimation of cherry size. The correlation value is 0.875, and p value is 0.001, which indicates that these
two variables are highly positive correlated. The linear fit shows the relationship between the predicted size and the weight from the traditional method. This linear relation explains 76.5% of the total variance in the data.</p>
</div>
<div id="S4.SS3.p5" class="ltx_para">
<p id="S4.SS3.p5.1" class="ltx_p">The reason why the two variables are not completely agreeing with each other was examined. The two variables calculated are not based on the same sample from the same population (tree). The proposed method uses 100 cherries in the image while the weight variables are calculated from two batches of around 100 cherries which includes the 100 cherries in the image. The larger value of the bounding box width and height is used to estimate the size of the cherry in the proposed method, while the traditional method uses average weight to estimate the size. Additionally, the fisheye effect is not taken into consideration. As discussed previously, there are also manual count or prediction errors.</p>
</div>
<figure id="S4.T7" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T7.2.1.1" class="ltx_text" style="font-size:90%;">Table 7</span>: </span><span id="S4.T7.3.2" class="ltx_text" style="font-size:90%;">Average colour prediction for YOLO-based models</span></figcaption>
<table id="S4.T7.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T7.4.1.1" class="ltx_tr">
<th id="S4.T7.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T7.4.1.1.1.1" class="ltx_text ltx_font_bold">Models</span></th>
<th id="S4.T7.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T7.4.1.1.2.1" class="ltx_text ltx_font_bold">Total Counts</span></th>
<th id="S4.T7.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T7.4.1.1.3.1" class="ltx_text ltx_font_bold">Correct Counts</span></th>
<th id="S4.T7.4.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T7.4.1.1.4.1" class="ltx_text ltx_font_bold">Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T7.4.2.1" class="ltx_tr">
<td id="S4.T7.4.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">YOLOv3_original</td>
<td id="S4.T7.4.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">364</td>
<td id="S4.T7.4.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">262</td>
<td id="S4.T7.4.2.1.4" class="ltx_td ltx_align_center ltx_border_t">72%</td>
</tr>
<tr id="S4.T7.4.3.2" class="ltx_tr">
<td id="S4.T7.4.3.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">YOLOv3_efficient_net</td>
<td id="S4.T7.4.3.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">364</td>
<td id="S4.T7.4.3.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">261</td>
<td id="S4.T7.4.3.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">72%</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS3.p6" class="ltx_para">
<p id="S4.SS3.p6.1" class="ltx_p">Table <a href="#S4.T7" title="Table 7 ‣ 4.3 Color and Size Information Extraction ‣ 4 Analysis and Results ‣ An Application of Deep Learning for Sweet Cherry Phenotyping using YOLO Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> presents the average colour prediction for each of the YOLO models. The analysis was based on whether the average predicted colour type falls in the range of the ones provided by the traditional method.</p>
</div>
<div id="S4.SS3.p7" class="ltx_para">
<p id="S4.SS3.p7.1" class="ltx_p">The results shows that the accuracy is 72%. Considering the sample difference and the variance in the human judgement of the colour, these results are acceptable based on input from subject matter experts. The possible reasons causing this discrepancy includes the colour scores given by technicians are subjective and are impacted by lighting conditions as well as an individual’s unique visual colour perception characteristics. Further, sample inconsistency, and image and colour card image white balance issues can further impact assessment.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion and Future Work</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">New cultivars generate enormous economic value for the horticulture industry. As a critical step in the creation of new cultivars, pheotyping requires a significant investment in terms of time and resources, especially when humans perform tedious and repetitive tasks such as data collection. A Python-driven application utilizing YOLO was developed to extract critical information from images including the number of cherries, the size and the colour of each cherry. This application helps to modernize the existing phenotyping procedures, permitting evaluation on a much larger scale and generation of accurate and reproducible data during research.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">The YOLO models obtained close to 99% accuracy in object detection and counting of cherries. They also scored over 90% on the IOU metric indicating the model is both accurate in detecting cherries and very precise for object localization, which is important when extracting size and colour information from the detected objects. The model surpassed human performance and even detected manual errors on previous years image data set, which was used for training and testing the model. The model offers significant performance improvements in time, and processing with the model takes 95% less time compared to manually counting. This research applies to many other fruit applications beyond cherries, and can be used by growers and environmental agencies.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Future work will examine further tuning of
parameters such as resizing images (regardless of whether using randomly resizing in the training process or not), confidence threshold, and non-max suppression threshold after training in order to improve the overall results.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgment</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The authors gratefully acknowledge the assistance of summer term students (Jakob Lavioe, Trista Algar) and technical staff (Chris Pagliocchini, Michael Wiess and Melanda Danenhower) of the Summerland Research and Development Centre of Agriculture and Agri-Food Canada (AAFC) in capturing and labelling the images used for training of models.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Agriculture and Agri-Food: Crop Profile for Cherry in Canada.

</span>
<span class="ltx_bibblock">[Online] Available:
<span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">http://publications.gc.ca/collections/collection˙2015/aac-aafc/A118-10-35-2013-eng.pdf</span>
(2013)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Aslam, A., Irtaza, A., Nida, N.: Object Detection and Localization in Natural
Scenes Through Single-Step and Two-Step Models.

</span>
<span class="ltx_bibblock">In: 2020 International Conference on Emerging Trends in Smart
Technologies (ICETST), pp. 1–7 (2020).

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1109/ICETST49965.2020.9080728" title="" class="ltx_ref">10.1109/ICETST49965.2020.9080728</a>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Bazame, H.C., Molin, J.P., Althoff, D., Martello, M.: Detection,
Classification, and Mapping of Coffee Fruits During Harvest with Computer
Vision.

</span>
<span class="ltx_bibblock">Computers and Electronics in Agriculture <span id="bib.bib3.1.1" class="ltx_text ltx_font_bold">183</span> (2021).

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/https://doi.org/10.1016/j.compag.2021.106066" title="" class="ltx_ref">https://doi.org/10.1016/j.compag.2021.106066</a>

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Duong, L.T., Nguyen, P.T., Di Sipio, C., Di Ruscio, D.: Automated fruit
recognition using EfficientNet and MixNet.

</span>
<span class="ltx_bibblock">Computers and Electronics in Agriculture <span id="bib.bib4.1.1" class="ltx_text ltx_font_bold">171</span> (2020).

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/https://doi.org/10.1016/j.compag.2020.105326" title="" class="ltx_ref">https://doi.org/10.1016/j.compag.2020.105326</a>

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Girshick, R.: Fast R-CNN.

</span>
<span class="ltx_bibblock">In: 2015 IEEE International Conference on Computer Vision (ICCV), pp.
1440–1448 (2015).

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1109/ICCV.2015.169" title="" class="ltx_ref">10.1109/ICCV.2015.169</a>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for
accurate object detection and semantic segmentation.

</span>
<span class="ltx_bibblock">In: 2014 IEEE Conference on Computer Vision and Pattern Recognition,
pp. 580–587 (2014).

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1109/CVPR.2014.81" title="" class="ltx_ref">10.1109/CVPR.2014.81</a>

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.: Deep Residual Learning for Image
Recognition.

</span>
<span class="ltx_bibblock">In: Proceedings of the IEEE conference on Computer Vision and Pattern
Recognition, pp. 770–778 (2016).

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1109/CVPR.2016.90" title="" class="ltx_ref">10.1109/CVPR.2016.90</a>

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C.:
SSD: Single Shot MultiBox Detector, pp. 21–37.

</span>
<span class="ltx_bibblock">Springer International Publishing, Cham (2016).

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/https://doi.org/10.1007/978-3-319-46448-0_2" title="" class="ltx_ref">https://doi.org/10.1007/978-3-319-46448-0_2</a>

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Mahmudul Hassan, S.K., Kumar Maji, A.: Identification of Plant Species Using
Deep Learning.

</span>
<span class="ltx_bibblock">In: Proceedings of International Conference on Frontiers in Computing
and Systems, pp. 115–125 (2021).

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/https://doi.org/10.1007/978-981-15-7834-2_11" title="" class="ltx_ref">https://doi.org/10.1007/978-981-15-7834-2_11</a>

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Neilsen, G.H., Neilsen, D., Kappel, F., Forge, T.: Interaction of Irrigation
and Soil Management on Sweet Cherry Productivity and Fruit Quality at
Different Crop Loads that Simulate Those Occurring by Environmental
Extremes.

</span>
<span class="ltx_bibblock">HortScience <span id="bib.bib10.1.1" class="ltx_text ltx_font_bold">49</span>(2), 215–220 (2014).

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/https://doi.org/10.21273/HORTSCI.49.2.215" title="" class="ltx_ref">https://doi.org/10.21273/HORTSCI.49.2.215</a>

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Nelson, L.: Expanding Cherry Production in British Columbia under Climate
Change.

</span>
<span class="ltx_bibblock">[Online] Available:
<span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://www.climateagriculturebc.ca/app/uploads/FI12-Expanding-Cherry-Production-BC-Climate-Change-2018-report.pdf</span>
(2018)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Rahman, M.A., Wang, Y.: Optimizing Intersection-Over-Union in Deep Neural
Networks for Image Segmentation.

</span>
<span class="ltx_bibblock">In: Advances in Visual Computing, pp. 234–244. Springer (2016).

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/https://doi.org/10.1007/978-3-319-50835-1_22" title="" class="ltx_ref">https://doi.org/10.1007/978-3-319-50835-1_22</a>

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Ramos, P., Prieto, F., Montoya, E., Oliveros, C.: Automatic Fruit Count on
Coffee Branches using Computer Vision.

</span>
<span class="ltx_bibblock">Computers and Electronics in Agriculture <span id="bib.bib13.1.1" class="ltx_text ltx_font_bold">137</span>, 9–22 (2017).

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/https://doi.org/10.1016/j.compag.2017.03.010" title="" class="ltx_ref">https://doi.org/10.1016/j.compag.2017.03.010</a>

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You Only Look Once:
Unified, Real-Time Object Detection.

</span>
<span class="ltx_bibblock">In: 2016 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 779–788. IEEE (2016).

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/https://doi.org/10.1109/CVPR.2016.91" title="" class="ltx_ref">https://doi.org/10.1109/CVPR.2016.91</a>

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Redmon, J., Farhadi, A.: YOLOv3: An Incremental Improvement.

</span>
<span class="ltx_bibblock">[Online] Available: <span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://arxiv.org/abs/1804.02767v1</span> (2018)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards Real-Time Object
Detection with Region Proposal Networks.

</span>
<span class="ltx_bibblock">IEEE Transactions on Pattern Analysis and Machine Intelligence
<span id="bib.bib16.1.1" class="ltx_text ltx_font_bold">39</span>(6), 1137–1149 (2017).

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/https://doi.ieeecomputersociety.org/10.1109/TPAMI.2016.2577031" title="" class="ltx_ref">https://doi.ieeecomputersociety.org/10.1109/TPAMI.2016.2577031</a>

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Svozil, D., Kvasnicka, V., Pospichal, J.: Introduction to Multi-Layer
Feed-Forward Neural Networks.

</span>
<span class="ltx_bibblock">Chemometrics and Intelligent Laboratory Systems <span id="bib.bib17.1.1" class="ltx_text ltx_font_bold">39</span>(1),
43–62 (1997).

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/https://doi.org/10.1016/S0169-7439(97)00061-0" title="" class="ltx_ref">https://doi.org/10.1016/S0169-7439(97)00061-0</a>

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Tan, M., Le, Q.V.: EfficientNet: Rethinking Model Scaling for Convolutional
Neural Networks.

</span>
<span class="ltx_bibblock">In: International Conference on Machine Learning, pp. 6105–6114.
PMLR (2019)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Tóth, B.P., Papp, D.: Deep Learning and SVM Classification for Plant
Recognition in Content-Based Large Scale Image Retrieval.

</span>
<span class="ltx_bibblock">In: CLEF 2016 - Conference and Labs of the Evaluation Forum (2016)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Villacrés, J., Auat Cheein, F.: Detection and Characterization of Cherries: A
Deep Learning Usability Case Study in Chile.

</span>
<span class="ltx_bibblock">Agronomy <span id="bib.bib20.1.1" class="ltx_text ltx_font_bold">10</span> (2020).

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/https://doi.org/10.3390/agronomy10060835" title="" class="ltx_ref">https://doi.org/10.3390/agronomy10060835</a>

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Viola, P., Jones, M.: Rapid Object Detection using a Boosted Cascade of Simple
Features.

</span>
<span class="ltx_bibblock">In: Proceedings of the 2001 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition. CVPR 2001, vol. 1, pp. I–I. IEEE
(2001).

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/https://doi.org/10.1109/CVPR.2001.990517" title="" class="ltx_ref">https://doi.org/10.1109/CVPR.2001.990517</a>

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2302.06697" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2302.06698" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2302.06698">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2302.06698" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2302.06699" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 02:10:16 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
