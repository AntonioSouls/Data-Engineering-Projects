<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Compute or Load KV Cache? Why not both?</title>
<!--Generated on Fri Oct  4 01:10:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.03065v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S1" title="In Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S2" title="In Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S2.SS1" title="In 2 Background ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span><span class="ltx_text">KV cache</span> in LLM Inference</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S2.SS2" title="In 2 Background ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span><span class="ltx_text">KV cache</span> Saving and Reusing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S2.SS3" title="In 2 Background ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Chunk Prefill</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S3" title="In Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Exploratory Experiments and Motivation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S4" title="In Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Design of <span class="ltx_text ltx_font_italic">Cake</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S4.SS1" title="In 4 Design of Cake ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Use Cases of <span class="ltx_text ltx_font_italic">Cake</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S4.SS2" title="In 4 Design of Cake ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Design Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S4.SS3" title="In 4 Design of Cake ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Benefits of <span class="ltx_text ltx_font_italic">Cake</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S5" title="In Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S5.SS1" title="In 5 Evaluation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Experiment Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S5.SS2" title="In 5 Evaluation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span><span class="ltx_text ltx_font_italic">Cake</span> Performance under Full GPU Resources Setting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S5.SS3" title="In 5 Evaluation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span><span class="ltx_text ltx_font_italic">Cake</span> Performance under Different GPU Usage</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S5.SS4" title="In 5 Evaluation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Incorporating <span class="ltx_text">KV cache</span> Compression with <span class="ltx_text ltx_font_italic">Cake</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S5.SS5" title="In 5 Evaluation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Overheads of <span class="ltx_text ltx_font_italic">Cake</span> in LLM Serving System</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S5.SS6" title="In 5 Evaluation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6 </span>Understand How <span class="ltx_text ltx_font_italic">Cake</span> Contributes to Computation Power</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S6" title="In Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S7" title="In Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S8" title="In Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Ethics Statement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#A1" title="In Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Details of <span class="ltx_text ltx_font_italic">Cake</span> Algorithm</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#A2" title="In Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Implementation</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#A2.SS1" title="In Appendix B Implementation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Enhancements to LMCache</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#A2.SS1.SSS0.Px1" title="In B.1 Enhancements to LMCache ‣ Appendix B Implementation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_title">Asynchronous Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#A2.SS1.SSS0.Px2" title="In B.1 Enhancements to LMCache ‣ Appendix B Implementation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_title">Buffer Preallocation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#A2.SS2" title="In Appendix B Implementation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>Integration with LLM Serving Systems</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#A3" title="In Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Performance of <span class="ltx_text ltx_font_italic">Cake</span> under different conditions</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document">Compute or Load KV Cache? Why not both?</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shuowei Jin
</span><span class="ltx_author_notes">Equal contribution.
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Xueshen Liu<span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Qingzhao Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Z. Morley Mao 
<br class="ltx_break"/>University of Michigan
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">{jinsw,liuxs,qzzhang,zmao@umich.edu}</span>
<br class="ltx_break"/>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1"><span class="ltx_text" id="id2.id1.1">Recent advancements in Large Language Models (LLMs) have significantly increased context window sizes, enabling sophisticated applications but also introducing substantial computational overheads, particularly computing key-value (KV) cache in the prefill stage. Prefix caching has emerged to save GPU power in this scenario, which saves KV cache at disks and reuse them across multiple queries. However, traditional prefix caching mechanisms often suffer from substantial latency because the speed of loading <span class="ltx_text" id="id2.id1.1.1">KV cache</span> from disks to GPU memory is bottlenecked by the throughput of I/O devices. To optimize the latency of long-context prefill, we propose <span class="ltx_text ltx_font_italic" id="id2.id1.1.2">Cake</span>, a novel <span class="ltx_text" id="id2.id1.1.3">KV cache</span> loader, which employs a bidirectional parallelized <span class="ltx_text" id="id2.id1.1.4">KV cache</span> generation strategy. Upon receiving a prefill task, <span class="ltx_text ltx_font_italic" id="id2.id1.1.5">Cake</span> simultaneously and dynamically loads saved <span class="ltx_text" id="id2.id1.1.6">KV cache</span> from prefix cache locations and computes <span class="ltx_text" id="id2.id1.1.7">KV cache</span> on local GPUs, maximizing the utilization of available computation and I/O bandwidth resources. Additionally, <span class="ltx_text ltx_font_italic" id="id2.id1.1.8">Cake</span> automatically adapts to diverse system statuses without manual parameter. tuning. In experiments on various prompt datasets, GPUs, and I/O devices, <span class="ltx_text ltx_font_italic" id="id2.id1.1.9">Cake</span> offers up to 68.1% Time To First Token (TTFT) reduction compare with compute-only method and 94.6% TTFT reduction compare with I/O-only method.</span></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large Language Models (LLMs) have been widely adopted in large-scale online services, making efficient online serving of these models a critical research and engineering challenge <cite class="ltx_cite ltx_citemacro_citep">(Kwon et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib22" title="">2023</a>; Agrawal et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib2" title="">2024</a>; Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib39" title="">2023</a>; Miao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib29" title="">2024</a>; Leviathan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib24" title="">2023</a>; Ning et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib30" title="">2023</a>; Jin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib19" title="">2024b</a>)</cite>. Recent advancements in LLM development have significantly expanded the models’ context windows, enabling sophisticated applications such as long document understanding <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib33" title="">2024</a>)</cite>, long-context Retrieval Augmented Generation (RAG) <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib17" title="">2024</a>)</cite>, and complex LLM agents <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib38" title="">2024</a>)</cite>. For instance, GPT-4 boasts a context window of 128k tokens <cite class="ltx_cite ltx_citemacro_citep">(openAI, <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib31" title="">2024</a>)</cite>, while Claude-3.5 Sounet extends this further to 200K tokens <cite class="ltx_cite ltx_citemacro_citep">(Anthropic, <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib4" title="">2024</a>)</cite>. However, processing these long-context prompts introduces substantial computational overhead, particularly in the prefill stage, where the key-value (KV) cache is calculated. <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_text" id="footnote1.1">KV cache</span> is an essential technique to reduce the computational overhead of LLM inference in the decoding stage for each request and widely adopted in the state-of-the-art inference system.</span></span></span> For example, generating <span class="ltx_text" id="S1.p1.1.1">KV cache</span> for a 200-page book like “The Great Gatsby” (approximately 72K tokens) requires about 180GB of memory. For a 70B parameter model, generating such a <span class="ltx_text" id="S1.p1.1.2">KV cache</span> on an A100 GPU takes approximately 30 seconds, significantly impacting user experience.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To mitigate this overhead, prefix caching, i.e., the cache of <span class="ltx_text" id="S1.p2.1.1">KV cache</span>, has emerged as a useful mechanism. This approach is particularly effective in applications where long contexts are frequently and repeatedly used across multiple requests. For example, in long document processing tasks, the KV cache of a large document can be reused for multiple queries about its content. Similarly, in coding assistance scenarios, a cached summary of the codebase can be reused across multiple code completion or Q&amp;A requests.
LLM service providers like Claude and Deepseek have begun implementing such prefix caching mechanism in their online services <cite class="ltx_cite ltx_citemacro_citep">(Anthropic, <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib3" title="">2023</a>; Deepseek, <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib10" title="">2024</a>)</cite>. Recent studies have proposed various system solutions to implement prefix caching <cite class="ltx_cite ltx_citemacro_citep">(Juravsky et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib20" title="">2024</a>; Jin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib18" title="">2024a</a>; Gim et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib13" title="">2024</a>)</cite>. LLM inference engines such as vLLM <cite class="ltx_cite ltx_citemacro_citep">(Kwon et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib22" title="">2023</a>)</cite> and SGLang <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib39" title="">2023</a>)</cite> cache KV in local CPU memory, but this approach is limited by CPU memory size. Given that disks offer much larger capacity and are more cost-efficient than CPU memory, storing KV caches in local or remote disks has become a more viable option <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib12" title="">2024</a>; AutoGen, <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib5" title="">2024</a>)</cite>. CacheGen <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib27" title="">2023</a>)</cite> proposes solutions to optimize KV streaming from local or remote disks to GPU memory, aiming for scalable prefix caching.</p>
</div>
<figure class="ltx_figure ltx_align_floatright" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="766" id="S1.F1.g1" src="x1.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Workflow of long-context LLM inference with prefix caching. <span class="ltx_text ltx_font_italic" id="S1.F1.2.1">Cake</span> operates in the KV loading phase (highlighted in blue). The configuration parameter is based on the specification of LambdaLab Server <cite class="ltx_cite ltx_citemacro_citep">(Lambda, <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib23" title="">2024</a>)</cite>.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In practice, long-context LLM inference workflow with prefix caching is depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">1</span></a>. Upon receiving an LLM request, the serving system first checks whether it has a prefix with available corresponding <span class="ltx_text" id="S1.p3.1.1">KV cache</span>. If so, the system directly loads <span class="ltx_text" id="S1.p3.1.2">KV cache</span> from the cache location to the GPU’s memory, saving the overhead of recomputation. Note that prefix caching may involve multiple levels, including CPU memory, local disks, and remote disks. After loading the cached prefix, the system continues the prefill of remaining tokens in the request and proceeds with subsequent steps.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">However, while prefix caching saves GPU computation, it is not optimized for latency, specifically Time-to-First-Token (TTFT), one of the key metrics in LLM serving. When the prefix cache is not found in local CPU memory (a common scenario due to CPU memory constraints), streaming <span class="ltx_text" id="S1.p4.1.1">KV cache</span> data from disk or network is often not faster than recomputing <span class="ltx_text" id="S1.p4.1.2">KV cache</span> if GPU power is available as we demonstrated in § <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S3" title="3 Exploratory Experiments and Motivation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">3</span></a>. Additionally, both the available computation and I/O resources vary based on the system workload.
Consequently, the two approaches of obtaining <span class="ltx_text" id="S1.p4.1.3">KV cache</span>- GPU computation and prefix cache loading - are only beneficial in terms of latency when there is sufficient GPU power or I/O bandwidth, respectively. Currently, there is a lack of dynamic <span class="ltx_text" id="S1.p4.1.4">KV cache</span> management systems that can optimize LLM inference latency across diverse system statuses.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To address this gap, we propose <span class="ltx_text ltx_font_italic" id="S1.p5.1.1">Cake</span> (<span class="ltx_text ltx_framed ltx_framed_underline" id="S1.p5.1.2">C</span>omputation and Network <span class="ltx_text ltx_framed ltx_framed_underline" id="S1.p5.1.3">A</span>ware <span class="ltx_text ltx_framed ltx_framed_underline" id="S1.p5.1.4">K</span>V Cach<span class="ltx_text ltx_framed ltx_framed_underline" id="S1.p5.1.5">E</span> loader), a system designed to achieve optimal latency for long-context LLM inference. <span class="ltx_text ltx_font_italic" id="S1.p5.1.6">Cake</span> simultaneously leverages local computation and data streaming from prefix cache locations to generate the <span class="ltx_text" id="S1.p5.1.7">KV cache</span>. It employs a bidirectional parallelized <span class="ltx_text" id="S1.p5.1.8">KV cache</span> generation strategy: upon receiving a prefill task, <span class="ltx_text ltx_font_italic" id="S1.p5.1.9">Cake</span> immediately utilizes available GPU power to compute <span class="ltx_text" id="S1.p5.1.10">KV cache</span> in normal order starting from the first token, while simultaneously loading the prefix cache in reverse order starting from the last token, until all tokens have corresponding <span class="ltx_text" id="S1.p5.1.11">KV cache</span> data.
This design is inspired by key observations about the characteristics of computing and streaming <span class="ltx_text" id="S1.p5.1.12">KV cache</span>: the computational cost of generating <span class="ltx_text" id="S1.p5.1.13">KV cache</span> increases with the token’s distance from the beginning of the sequence, while the data streaming cost remains constant regardless of token position. Importantly, <span class="ltx_text ltx_font_italic" id="S1.p5.1.14">Cake</span>’s design is both simple and elegant, adapting to diverse situations in terms of system architecture and available resources, without requiring manual parameter tuning.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The main contribution of this paper includes the design and evaluation of <span class="ltx_text ltx_font_italic" id="S1.p6.1.1">Cake</span>, which enables the following features improving the quality of LLM serving.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p7">
<p class="ltx_p" id="S1.p7.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.p7.1.m1.1"><semantics id="S1.p7.1.m1.1a"><mo id="S1.p7.1.m1.1.1" xref="S1.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.p7.1.m1.1b"><ci id="S1.p7.1.m1.1.1.cmml" xref="S1.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.p7.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S1.p7.1.1">Optimized latency:</span> <span class="ltx_text ltx_font_italic" id="S1.p7.1.2">Cake</span> optimizes the overall latency of <span class="ltx_text" id="S1.p7.1.3">KV cache loading</span> by achieving maximum utilization of computation and network streaming in parallel without any idle time.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p8">
<p class="ltx_p" id="S1.p8.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.p8.1.m1.1"><semantics id="S1.p8.1.m1.1a"><mo id="S1.p8.1.m1.1.1" xref="S1.p8.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.p8.1.m1.1b"><ci id="S1.p8.1.m1.1.1.cmml" xref="S1.p8.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.p8.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S1.p8.1.1">Automatic adaptation:</span> <span class="ltx_text ltx_font_italic" id="S1.p8.1.2">Cake</span> continuously adapts to the available network and computational resources, achieving low latency without downgrading system performance. Previous computation-only and network-only <span class="ltx_text" id="S1.p8.1.3">KV cache</span> prefill solutions are special cases of <span class="ltx_text ltx_font_italic" id="S1.p8.1.4">Cake</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p9">
<p class="ltx_p" id="S1.p9.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.p9.1.m1.1"><semantics id="S1.p9.1.m1.1a"><mo id="S1.p9.1.m1.1.1" xref="S1.p9.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.p9.1.m1.1b"><ci id="S1.p9.1.m1.1.1.cmml" xref="S1.p9.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p9.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.p9.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S1.p9.1.1">Negligible Overhead:</span> <span class="ltx_text ltx_font_italic" id="S1.p9.1.2">Cake</span> introduces minimal computational and memory overhead, ensuring it doesn’t negatively impact the performance of the underlying system.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p10">
<p class="ltx_p" id="S1.p10.1">Our experiments demonstrate that <span class="ltx_text ltx_font_italic" id="S1.p10.1.1">Cake</span> efficiently utilizes both computation and I/O fetching to significantly reduce the prefilling latency in long-context LLM inference. Evaluations on diverse datasets, including LongChat <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib25" title="">2023a</a>)</cite>, TriviaQA, and NarrativeQA <cite class="ltx_cite ltx_citemacro_citep">(Bai et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib6" title="">2023</a>)</cite>, show that <span class="ltx_text ltx_font_italic" id="S1.p10.1.2">Cake</span> achieves substantial improvements in Time-to-First-Token (TTFT). Compared to compute-only methods, <span class="ltx_text ltx_font_italic" id="S1.p10.1.3">Cake</span> achieves an average 36.7% reduction in TTFT. When compared to I/O-only methods, the improvement is even more pronounced, with an average 60.55% reduction in TTFT. Notably, <span class="ltx_text ltx_font_italic" id="S1.p10.1.4">Cake</span> accomplishes these performance gains while introducing minimal overhead to the system, making it a highly efficient and practical solution for optimizing long-context LLM inference across various scenarios and workloads.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span><span class="ltx_text" id="S2.SS1.1.1">KV cache</span> in LLM Inference</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Large language models (LLMs) have recently made significant impacts across numerous domains. The key to their success lies in the attention mechanism, which enables these models to scale up and effectively process long contexts <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib32" title="">2017</a>; Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib7" title="">2020</a>)</cite>. In the attention calculation process, the computation of Key (K) and Value (V) vectors for previously processed tokens becomes redundant during the decoding phase. Recognizing this, the concept of <span class="ltx_text" id="S2.SS1.p1.1.1">KV cache</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib37" title="">2023</a>)</cite> is introduced. This approach involves storing these computed values and reusing them to reduce computational overhead in subsequent decoding steps.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">State-of-the-art LLM inference engines <cite class="ltx_cite ltx_citemacro_citep">(Kwon et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib22" title="">2023</a>; Miao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib29" title="">2024</a>; Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib39" title="">2023</a>)</cite> typically divide the inference procedure into two distinct phases: prefill and decode. The prefill stage generates the initial <span class="ltx_text" id="S2.SS1.p2.1.1">KV cache</span> for the input prompt. In the subsequent decode stage, the model utilizes this cache to generate new tokens sequentially. As each new token is produced, its corresponding K and V vectors are computed and appended to the <span class="ltx_text" id="S2.SS1.p2.1.2">KV cache</span>. This caching mechanism significantly accelerates inference by converting the time complexity of token generation from quadratic to linear <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib35" title="">2024</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span><span class="ltx_text" id="S2.SS2.1.1">KV cache</span> Saving and Reusing</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The prefill stage is a highly resource-consuming procedure that often saturates GPU computation resources and causes high latency compared to the normal decoding procedure <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib2" title="">2024</a>)</cite>. This overhead becomes even more significant in long context scenarios <cite class="ltx_cite ltx_citemacro_citep">(Fu, <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib11" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Meanwhile, it is common for parts of prompts to be reused across multiple requests. For instance, system messages in chatbot services guiding the LLM model’s behavior are usually long and shared across multiple messages. In Retrieval-Augmented Generation scenarios, fetching long text blocks as the context to generate answers can improve the generation performance <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib17" title="">2024</a>)</cite>. Other use cases include coding assistants that need to maintain a summarized version of the codebase in the prompt <cite class="ltx_cite ltx_citemacro_citep">(Cursor, <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib9" title="">2024</a>)</cite>, and agentic search, tool use and multi-agent communication which require multiple rounds of API calls using the same set of historical data <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib34" title="">2023</a>; Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib26" title="">2023b</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">To address these challenges and optimize inference efficiency, various systems have been developed to save and reuse <span class="ltx_text" id="S2.SS2.p3.1.1">KV cache</span> via prefix caching mechanism <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib39" title="">2023</a>; Kwon et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib22" title="">2023</a>; Juravsky et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib20" title="">2024</a>; Jin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib18" title="">2024a</a>; Gim et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib13" title="">2024</a>; Gao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib12" title="">2024</a>)</cite>. A typical workflow of these systems are demonstrate in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">1</span></a>. These systems leverage different layers of the storage hierarchy, each with its own trade-offs. GPU memory offers the lowest latency but has the highest cost and smallest capacity, making it impractical for long-term <span class="ltx_text" id="S2.SS2.p3.1.2">KV cache</span> storage. CPU memory, used by some inference engines <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib39" title="">2023</a>; Kwon et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib22" title="">2023</a>)</cite>, is often insufficient for large-scale online serving systems handling millions of requests per second. Consequently, local or remote disk storage has emerged as a more viable option for large-scale operations, offering a balance between cost and capacity.
The trend towards disk-based <span class="ltx_text" id="S2.SS2.p3.1.3">KV cache</span> storage is evident in industry practices. For example, Deepseek, a major LLM API service provider, implements prefix caching on disk, potentially reducing users’ inference costs by up to 90% <cite class="ltx_cite ltx_citemacro_citep">(Deepseek, <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib10" title="">2024</a>)</cite>. Similarly, the multi-agent framework AutoGen employs disk-based prefix caching <cite class="ltx_cite ltx_citemacro_citep">(AutoGen, <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib5" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1"><span class="ltx_text ltx_font_italic" id="S2.SS2.p4.1.1">Cake</span> aligns with the prefix caching mechanism for long-context LLM inference, improving latency by simultaneously scheduling KV computation and loading.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Chunk Prefill</h3>
<div class="ltx_para ltx_noindent" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Chunk prefill is a technique used to optimize the prefill stage of LLM inference, particularly for long input sequences. Unlike the decoding procedure, which is memory-bound, prefill stage is a computation-intensive process that demands significant GPU resources for extended periods. For instance, prefilling 10,000 tokens on a 7B model on an A100 GPU takes approximately 1 seconds. Directly processing a long text prompt in its entirety for prefilling would monopolize the GPU, severely impacting the latency of other tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Originally proposed in Sarathi <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib1" title="">2023</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib2" title="">2024</a>)</cite>, this method divides the input sequence into smaller, near-equal sized chunks and processes them sequentially. By breaking down large prefill requests into manageable chunks, chunk prefill allows for improved throughput and reduced prefill operations’ latency impact in LLM serving systems. It enables the interleaving of prefill operations with decode operations, minimizing the blocking effect of long prompts on other tasks.This approach has been widely adopted in current mainstream LLM inference engines <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib39" title="">2023</a>; Kwon et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib22" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">In vLLM’s detailed implementation of chunk prefill <cite class="ltx_cite ltx_citemacro_citep">(Kwon et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib22" title="">2023</a>)</cite>, the inference engine forms a batch of requests for each inference step based on a predetermined token budget. The scheduler prioritizes decode requests, allocating one token from the budget to each. Any remaining tokens in the budget are then assigned to prefill requests. This dynamic allocation determines the chunk size for chunk prefill operations. By giving precedence to decode requests, this approach minimizes interference with ongoing decoding requests, ensuring lower inter-token latency (ITL) for decode while efficiently utilizing available resources for prefill tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1"><span class="ltx_text ltx_font_italic" id="S2.SS3.p4.1.1">Cake</span> adopts chunk prefill by default as it aims for the long-context inference scenarios.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Exploratory Experiments and Motivation</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Upon receiving a new prefill task, the <span class="ltx_text" id="S3.p1.1.1">KV cache</span> can be obtained through various sources, including the computation on local GPU, fetching from local disk, or data streaming from a remote location, as mentioned in § <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S2" title="2 Background ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">2</span></a>. This section evaluates the performance of different <span class="ltx_text" id="S3.p1.1.2">KV cache</span> loading or generation methods and identifies potential improvements, motivating the design of <span class="ltx_text ltx_font_italic" id="S3.p1.1.3">Cake</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p" id="S3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.p2.1.1">Finding 1: The latency of loading <span class="ltx_text" id="S3.p2.1.1.1">KV cache</span> from local or remote disks is linearly correlated with the data size</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p3">
<p class="ltx_p" id="S3.p3.6">The size of the <span class="ltx_text" id="S3.p3.6.1">KV cache</span> grows linearly with the number of tokens and can be calculated as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{\text{KV cache} size}=4\cdot N_{l}\cdot H\cdot L_{\text{max}}\cdot B%
\cdot P," class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2c.cmml"><mtext id="S3.E1.m1.1.1.1.1.2a" xref="S3.E1.m1.1.1.1.1.2c.cmml">KV cache</mtext><mtext id="S3.E1.m1.1.1.1.1.2b" xref="S3.E1.m1.1.1.1.1.2c.cmml"> size</mtext></mrow><mo id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><mn id="S3.E1.m1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.3.2.cmml">4</mn><mo id="S3.E1.m1.1.1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.E1.m1.1.1.1.1.3.1.cmml">⋅</mo><msub id="S3.E1.m1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.3.2" xref="S3.E1.m1.1.1.1.1.3.3.2.cmml">N</mi><mi id="S3.E1.m1.1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.1.3.3.3.cmml">l</mi></msub><mo id="S3.E1.m1.1.1.1.1.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.E1.m1.1.1.1.1.3.1.cmml">⋅</mo><mi id="S3.E1.m1.1.1.1.1.3.4" xref="S3.E1.m1.1.1.1.1.3.4.cmml">H</mi><mo id="S3.E1.m1.1.1.1.1.3.1b" lspace="0.222em" rspace="0.222em" xref="S3.E1.m1.1.1.1.1.3.1.cmml">⋅</mo><msub id="S3.E1.m1.1.1.1.1.3.5" xref="S3.E1.m1.1.1.1.1.3.5.cmml"><mi id="S3.E1.m1.1.1.1.1.3.5.2" xref="S3.E1.m1.1.1.1.1.3.5.2.cmml">L</mi><mtext id="S3.E1.m1.1.1.1.1.3.5.3" xref="S3.E1.m1.1.1.1.1.3.5.3a.cmml">max</mtext></msub><mo id="S3.E1.m1.1.1.1.1.3.1c" lspace="0.222em" rspace="0.222em" xref="S3.E1.m1.1.1.1.1.3.1.cmml">⋅</mo><mi id="S3.E1.m1.1.1.1.1.3.6" xref="S3.E1.m1.1.1.1.1.3.6.cmml">B</mi><mo id="S3.E1.m1.1.1.1.1.3.1d" lspace="0.222em" rspace="0.222em" xref="S3.E1.m1.1.1.1.1.3.1.cmml">⋅</mo><mi id="S3.E1.m1.1.1.1.1.3.7" xref="S3.E1.m1.1.1.1.1.3.7.cmml">P</mi></mrow></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"></eq><ci id="S3.E1.m1.1.1.1.1.2c.cmml" xref="S3.E1.m1.1.1.1.1.2"><mrow id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"><mtext id="S3.E1.m1.1.1.1.1.2a.cmml" xref="S3.E1.m1.1.1.1.1.2">KV cache</mtext><mtext id="S3.E1.m1.1.1.1.1.2b.cmml" xref="S3.E1.m1.1.1.1.1.2"> size</mtext></mrow></ci><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><ci id="S3.E1.m1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1">⋅</ci><cn id="S3.E1.m1.1.1.1.1.3.2.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.3.2">4</cn><apply id="S3.E1.m1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2">𝑁</ci><ci id="S3.E1.m1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3">𝑙</ci></apply><ci id="S3.E1.m1.1.1.1.1.3.4.cmml" xref="S3.E1.m1.1.1.1.1.3.4">𝐻</ci><apply id="S3.E1.m1.1.1.1.1.3.5.cmml" xref="S3.E1.m1.1.1.1.1.3.5"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.5.1.cmml" xref="S3.E1.m1.1.1.1.1.3.5">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.5.2.cmml" xref="S3.E1.m1.1.1.1.1.3.5.2">𝐿</ci><ci id="S3.E1.m1.1.1.1.1.3.5.3a.cmml" xref="S3.E1.m1.1.1.1.1.3.5.3"><mtext id="S3.E1.m1.1.1.1.1.3.5.3.cmml" mathsize="70%" xref="S3.E1.m1.1.1.1.1.3.5.3">max</mtext></ci></apply><ci id="S3.E1.m1.1.1.1.1.3.6.cmml" xref="S3.E1.m1.1.1.1.1.3.6">𝐵</ci><ci id="S3.E1.m1.1.1.1.1.3.7.cmml" xref="S3.E1.m1.1.1.1.1.3.7">𝑃</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\text{\text{KV cache} size}=4\cdot N_{l}\cdot H\cdot L_{\text{max}}\cdot B%
\cdot P,</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">KV cache size = 4 ⋅ italic_N start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ⋅ italic_H ⋅ italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT ⋅ italic_B ⋅ italic_P ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.p3.5">Where <math alttext="N_{l}" class="ltx_Math" display="inline" id="S3.p3.1.m1.1"><semantics id="S3.p3.1.m1.1a"><msub id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml"><mi id="S3.p3.1.m1.1.1.2" xref="S3.p3.1.m1.1.1.2.cmml">N</mi><mi id="S3.p3.1.m1.1.1.3" xref="S3.p3.1.m1.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><apply id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p3.1.m1.1.1.1.cmml" xref="S3.p3.1.m1.1.1">subscript</csymbol><ci id="S3.p3.1.m1.1.1.2.cmml" xref="S3.p3.1.m1.1.1.2">𝑁</ci><ci id="S3.p3.1.m1.1.1.3.cmml" xref="S3.p3.1.m1.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">N_{l}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.1.m1.1d">italic_N start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math> is the number of layers, <math alttext="H" class="ltx_Math" display="inline" id="S3.p3.2.m2.1"><semantics id="S3.p3.2.m2.1a"><mi id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><ci id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">H</annotation><annotation encoding="application/x-llamapun" id="S3.p3.2.m2.1d">italic_H</annotation></semantics></math> is the hidden size, <math alttext="L_{\text{max}}" class="ltx_Math" display="inline" id="S3.p3.3.m3.1"><semantics id="S3.p3.3.m3.1a"><msub id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml"><mi id="S3.p3.3.m3.1.1.2" xref="S3.p3.3.m3.1.1.2.cmml">L</mi><mtext id="S3.p3.3.m3.1.1.3" xref="S3.p3.3.m3.1.1.3a.cmml">max</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><apply id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p3.3.m3.1.1.1.cmml" xref="S3.p3.3.m3.1.1">subscript</csymbol><ci id="S3.p3.3.m3.1.1.2.cmml" xref="S3.p3.3.m3.1.1.2">𝐿</ci><ci id="S3.p3.3.m3.1.1.3a.cmml" xref="S3.p3.3.m3.1.1.3"><mtext id="S3.p3.3.m3.1.1.3.cmml" mathsize="70%" xref="S3.p3.3.m3.1.1.3">max</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">L_{\text{max}}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.3.m3.1d">italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT</annotation></semantics></math> is the maximum context length, <math alttext="B" class="ltx_Math" display="inline" id="S3.p3.4.m4.1"><semantics id="S3.p3.4.m4.1a"><mi id="S3.p3.4.m4.1.1" xref="S3.p3.4.m4.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.p3.4.m4.1b"><ci id="S3.p3.4.m4.1.1.cmml" xref="S3.p3.4.m4.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.m4.1c">B</annotation><annotation encoding="application/x-llamapun" id="S3.p3.4.m4.1d">italic_B</annotation></semantics></math> is the batch size, and <math alttext="P" class="ltx_Math" display="inline" id="S3.p3.5.m5.1"><semantics id="S3.p3.5.m5.1a"><mi id="S3.p3.5.m5.1.1" xref="S3.p3.5.m5.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.p3.5.m5.1b"><ci id="S3.p3.5.m5.1.1.cmml" xref="S3.p3.5.m5.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.5.m5.1c">P</annotation><annotation encoding="application/x-llamapun" id="S3.p3.5.m5.1d">italic_P</annotation></semantics></math> is the precision in bytes.
Using this formula, we can estimate that for the Llama3-70B model with FP16 precision, the <span class="ltx_text" id="S3.p3.5.1">KV cache</span> for a single token consumes approximately 2.5 MB. Consequently, a 15-page research paper contains around 10,000 tokens, would require a great amount of memory as large as 25 GB.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">Because of the size, the process of loading long-context <span class="ltx_text" id="S3.p4.1.1">KV cache</span> into GPU memory is not fast because of the constrained bottleneck throughput of the I/O devices. Local storage options vary in performance: SATA SSDs offer I/O bandwidths of around 600 MB/s, HDDs are limited to about 200 MB/s, and high-end NVMe SSDs can achieve read speeds of up to 3 GB/s at a premium cost. Even in the optimal case with a 3 GB/s read bandwidth, loading the <span class="ltx_text" id="S3.p4.1.2">KV cache</span> for a 10-page paper would take approximately 8 seconds. Such latency will be even longer when using remote storage, whose bottleneck becomes the network bandwidth. Typical network bandwidths rarely exceed 20 Gbps (i.e., 3GB/s) <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib27" title="">2023</a>; Jain et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib15" title="">2023</a>)</cite>, potentially leading to transfer times of up to 10 seconds to fetch the 25 GB <span class="ltx_text" id="S3.p4.1.3">KV cache</span> from a remote disk.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p5">
<p class="ltx_p" id="S3.p5.1"><span class="ltx_text ltx_font_bold" id="S3.p5.1.1">Finding 2: The latency and memory usage of computing <span class="ltx_text" id="S3.p5.1.1.1">KV cache</span> on GPUs increases quadratically with the length of the sequence.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p6">
<p class="ltx_p" id="S3.p6.1">During the prefill stage, the computation cost for generating <span class="ltx_text" id="S3.p6.1.1">KV cache</span> of later tokens (i.e., those with higher indices in the sequence) is expected to be higher than for earlier tokens
This is because of the inherent nature of attention mechanisms — computing <span class="ltx_text" id="S3.p6.1.2">KV cache</span> involves attention operations across the current and all preceding tokens.
To test this hypothesis, we conducted experiments on long-context prefill tasks with the chunk prefill mechanism described in § <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S2.SS3" title="2.3 Chunk Prefill ‣ 2 Background ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">2.3</span></a>. Results presented in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S3.F3" title="Figure 3 ‣ 3 Exploratory Experiments and Motivation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">3</span></a> demonstrate a clear pattern: the latency for each chunk linearly increases with its index; i.e., the latency for the whole sequence is quadratically correlated with the sequence length.
We can also observe the <span class="ltx_text" id="S3.p6.1.3">KV cache</span> memory usage linearly increases with the chunk index.
The observation is critical for <span class="ltx_text ltx_font_italic" id="S3.p6.1.4">Cake</span> to arrange computation tasks optimally.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p7">
<p class="ltx_p" id="S3.p7.1"><span class="ltx_text ltx_font_bold" id="S3.p7.1.1">Finding 3: Computing <span class="ltx_text" id="S3.p7.1.1.1">KV cache</span> could be even faster than loading cached <span class="ltx_text" id="S3.p7.1.1.2">KV cache</span></span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p8">
<p class="ltx_p" id="S3.p8.1">We compare the performance of loading and computing <span class="ltx_text" id="S3.p8.1.1">KV cache</span> at various system settings.
We use various GPUs to perform chunk prefill of a random context with 32k tokens through LongAlpaca-7B and -13B <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib8" title="">2023</a>)</cite> models, implemented on vLLM <cite class="ltx_cite ltx_citemacro_citep">(Kwon et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib22" title="">2023</a>)</cite>.
We use equavelant throughput as the evaluation metric, dividing the computed or loaded <span class="ltx_text" id="S3.p8.1.2">KV cache</span> file size by the time spent.
We evaluate the prefill using chunk sizes as 512, as suggested by vLLM.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p9">
<p class="ltx_p" id="S3.p9.1">We present the results in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S3.F3" title="Figure 3 ‣ 3 Exploratory Experiments and Motivation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">3</span></a>. Loading <span class="ltx_text" id="S3.p9.1.1">KV cache</span> from remote disk is faster than loading from a local HDD but is slower than loading from a local SSD. Throughput of computing <span class="ltx_text" id="S3.p9.1.2">KV cache</span> with 100% A100 GPU power is comparable to that of loading <span class="ltx_text" id="S3.p9.1.3">KV cache</span> from SSD disk or network.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p10">
<p class="ltx_p" id="S3.p10.1"><span class="ltx_text ltx_font_bold" id="S3.p10.1.1">Conclusion</span>. The above experiments highlighted the limitation of existing prefix caching – loading <span class="ltx_text" id="S3.p10.1.2">KV cache</span> via disk or network I/O introduces substantial latency. Recomputation of <span class="ltx_text" id="S3.p10.1.3">KV cache</span>, though consuming GPU power, is fast enough to potentially accelerate the prefill process. Our findings suggest that an optimal approach for prefix caching could simultaneously leverage available GPU power and I/O bandwidth. This strategy forms the core concept of <span class="ltx_text ltx_font_italic" id="S3.p10.1.4">Cake</span>.</p>
</div>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" id="S3.F3.1" style="width:199.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="564" id="S3.F3.1.g1" src="x2.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Equivalent <span class="ltx_text" id="S3.F3.1.2.1">KV cache</span> loading bandwidth.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" id="S3.F3.2" style="width:212.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="490" id="S3.F3.2.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Prefill latency(i.e., step time)/Total Memory Usage v.s. chunk index.</figcaption>
</figure>
</div>
</div>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Design of <span class="ltx_text ltx_font_italic" id="S4.1.1">Cake</span>
</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We propose <span class="ltx_text ltx_font_italic" id="S4.p1.1.1">Cake</span>, a system that adaptively utilizes both network and available computation resources to achieve low-latency <span class="ltx_text" id="S4.p1.1.2">KV cache</span> loading. In this section, we first discuss the scope of <span class="ltx_text ltx_font_italic" id="S4.p1.1.3">Cake</span> by specifying its use cases in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S4.SS1" title="4.1 Use Cases of Cake ‣ 4 Design of Cake ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">4.1</span></a>. We then elaborate design details in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S4.SS2" title="4.2 Design Details ‣ 4 Design of Cake ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">4.2</span></a> and analyze the benefit of <span class="ltx_text ltx_font_italic" id="S4.p1.1.4">Cake</span> in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S4.SS3" title="4.3 Benefits of Cake ‣ 4 Design of Cake ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">4.3</span></a>.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Use Cases of <span class="ltx_text ltx_font_italic" id="S4.SS1.1.1">Cake</span>
</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.1">Cake</span> works upon a LLM serving system with prefix caching as depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">1</span></a>.
<span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.2">Cake</span> is especially beneficial when the system needs to load saved <span class="ltx_text" id="S4.SS1.p1.1.3">KV cache</span> from disk storage to GPU Memory for inference. It’s worth noting that in cases where the required <span class="ltx_text" id="S4.SS1.p1.1.4">KV cache</span> data already resides in GPU or CPU memory, it is beyond the scope of <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.5">Cake</span>. This is because the bandwidth between CPU and GPU is dramatically higher than disk I/O or computation speeds, then comparing with I/O, computation can provide limited help. However, given the memory size constraints on CPU and GPU and the large size of long-context caches, such scenarios are relatively infrequent, making <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.6">Cake</span> a crucial component for most long-context LLM inference with prefix caching tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.1">Cake</span> is complementary to other optimization techniques. For instance, methods focused on reducing <span class="ltx_text" id="S4.SS1.p2.1.2">KV cache</span> size <cite class="ltx_cite ltx_citemacro_citep">(Hooper et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib14" title="">2024</a>; Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib16" title="">2023</a>; Kang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib21" title="">2024</a>)</cite> or optimizing <span class="ltx_text" id="S4.SS1.p2.1.3">KV cache</span> loading from local/remote disks <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib27" title="">2023</a>)</cite> can be used in conjunction with <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.4">Cake</span>, potentially yielding even greater performance improvements. Furthermore, <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.5">Cake</span> is designed to integrate seamlessly with state-of-the-art LLM serving systems <cite class="ltx_cite ltx_citemacro_citep">(Kwon et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib22" title="">2023</a>; Yao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib36" title="">2024</a>)</cite>, enhancing their capabilities in handling long-context scenarios efficiently.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Design Details</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We build <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">Cake</span> upon the chunk prefill design discussed in §<a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S2.SS3" title="2.3 Chunk Prefill ‣ 2 Background ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">2.3</span></a>, where a long sequence is split into chunks, and the prefill is performed chunk by chunk. Chunk prefill is widely adopted for long-context LLM inference systems to offer scalable management of concurrent multiple user requests. In <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.2">Cake</span>, chunks serve as the fundamental unit for scheduling prefill tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">The key idea of <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.1">Cake</span> is to simultaneously leverage both local computation and I/O for data streaming to optimize the latency of <span class="ltx_text" id="S4.SS2.p2.1.2">KV cache</span> prefill. For each long-context request, <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.3">Cake</span> determines an efficient schedule for using all available resources to accomplish <span class="ltx_text" id="S4.SS2.p2.1.4">KV cache</span> prefill, i.e., determining for each chunk whether to utilize local GPU computation or data streaming from local/remote disks, as well as the sequence of these operations.
As discussed in §<a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S3" title="3 Exploratory Experiments and Motivation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">3</span></a>, the computation of <span class="ltx_text" id="S4.SS2.p2.1.5">KV cache</span> on later chunks is more expensive than that on earlier chunks.
Therefore, we should prioritize computation operations on chunks near the beginning of the sequence while let cache loading operations to fill later chunks. Inspired by this intuition, we design <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.6">Cake</span> as a bidirectional parrallelized <span class="ltx_text" id="S4.SS2.p2.1.7">KV cache</span> loader as below.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="260" id="S4.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Diagram illustrating the workflow of <span class="ltx_text ltx_font_italic" id="S4.F4.2.1">Cake</span> </figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S4.F4" title="Figure 4 ‣ 4.2 Design Details ‣ 4 Design of Cake ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">4</span></a>, upon receiving a request, <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.1">Cake</span> splits the sequence into chunks and initiates two simultaneous processes:
(1) The local GPU computes <span class="ltx_text" id="S4.SS2.p3.1.2">KV cache</span> from the beginning chunk of the prompt, progressing towards the end.
(2) The data streaming process fetches <span class="ltx_text" id="S4.SS2.p3.1.3">KV cache</span> starting from the last chunk, moving in reverse direction.
This bidirectional approach continues until the two processes converge in the middle, signaling the completion of <span class="ltx_text" id="S4.SS2.p3.1.4">KV cache</span> generation for the entire prompt. We discuss the whole algorithms in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#A1" title="Appendix A Details of Cake Algorithm ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">A</span></a> and the system implementation details in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#A2" title="Appendix B Implementation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Benefits of <span class="ltx_text ltx_font_italic" id="S4.SS3.1.1">Cake</span>
</h3>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Our design offers several key benefits:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.1"><semantics id="S4.SS3.p2.1.m1.1a"><mo id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><ci id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S4.SS3.p2.1.1">Optimized latency:</span> <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.2">Cake</span> optimally reduces the overall latency of prefill with <span class="ltx_text" id="S4.SS3.p2.1.3">KV cache</span> loading by parallelizing computation and network streaming without any idle time.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S4.SS3.p3.1.m1.1"><semantics id="S4.SS3.p3.1.m1.1a"><mo id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><ci id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S4.SS3.p3.1.1">Automatic adaptation:</span> <span class="ltx_text ltx_font_italic" id="S4.SS3.p3.1.2">Cake</span> dynamically adapts to varying conditions without relying on manually defined parameters. It consistently offers latency improvements across different sequence lengths and diverse system configurations, tolerating fluctuations in network conditions and computational capabilities. This adaptability is evidenced by our comprehensive evaluation across various scenarios, as detailed in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S5" title="5 Evaluation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S4.SS3.p4.1.m1.1"><semantics id="S4.SS3.p4.1.m1.1a"><mo id="S4.SS3.p4.1.m1.1.1" xref="S4.SS3.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.1.m1.1b"><ci id="S4.SS3.p4.1.m1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p4.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S4.SS3.p4.1.1">Negligible overhead:</span> <span class="ltx_text ltx_font_italic" id="S4.SS3.p4.1.2">Cake</span> automatically balances the demand for computation and network resources according to dynamic situations without substantial additional overhead. Previous computation-only and network-only <span class="ltx_text" id="S4.SS3.p4.1.3">KV cache</span> loading solutions (e.g., <cite class="ltx_cite ltx_citemacro_cite">Kwon et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib22" title="">2023</a>); Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib27" title="">2023</a>)</cite>) become special cases of <span class="ltx_text ltx_font_italic" id="S4.SS3.p4.1.4">Cake</span> when either computation or network is unavailable. Additionally, <span class="ltx_text ltx_font_italic" id="S4.SS3.p4.1.5">Cake</span> operates without such computation-intensive profiling phase, further reducing its operational overhead.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Evaluation</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we first introduce the setup of our experiments and then we utilize thorough experiemnts to address the following questions:</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">1. How does <span class="ltx_text ltx_font_italic" id="S5.p2.1.1">Cake</span> perform under varying I/O bandwidth conditions when utilizing the full power of GPU? (§<a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S5.SS2" title="5.2 Cake Performance under Full GPU Resources Setting ‣ 5 Evaluation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">5.2</span></a>)</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">2. What is the impact of context length on <span class="ltx_text ltx_font_italic" id="S5.p3.1.1">Cake</span> performance? (§<a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S5.SS2" title="5.2 Cake Performance under Full GPU Resources Setting ‣ 5 Evaluation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">5.2</span></a>)</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">3. How does <span class="ltx_text ltx_font_italic" id="S5.p4.1.1">Cake</span> adapt to different levels of available GPU resources? (§<a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S5.SS3" title="5.3 Cake Performance under Different GPU Usage ‣ 5 Evaluation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">5.3</span></a>)</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">4. How effectively does <span class="ltx_text ltx_font_italic" id="S5.p5.1.1">Cake</span> integrate with state-of-the-art <span class="ltx_text" id="S5.p5.1.2">KV cache</span> compression techniques? (§<a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S5.SS4" title="5.4 Incorporating KV cache Compression with Cake ‣ 5 Evaluation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">5.4</span></a>)</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p6">
<p class="ltx_p" id="S5.p6.1">5. What overhead does <span class="ltx_text ltx_font_italic" id="S5.p6.1.1">Cake</span> introduce when integrated into vLLM’s inference procedure? (§<a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S5.SS5" title="5.5 Overheads of Cake in LLM Serving System ‣ 5 Evaluation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">5.5</span></a>)</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Experiment Setup</h3>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.1">Models.</span> We evaluate <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.2">Cake</span> on fine-tuned long context models LongAlpaca-7B and LongAlpaca-13B  <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib8" title="">2023</a>)</cite> based on LLama2. The per-token <span class="ltx_text" id="S5.SS1.p1.1.3">KV cache</span> size is 0.5MB and 0.78MB respectively, using 16-bit floating point as the data type.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">Evaluation Metrics.</span> We use time-to-first-token (TTFT) as our primary evaluation metric. TTFT is widely used in LLM inference, measuring the time between the arrival of a user query and the generation of the first token. In other words, it reflects either the time of loading stored <span class="ltx_text" id="S5.SS1.p2.1.2">KV cache</span> or computing new <span class="ltx_text" id="S5.SS1.p2.1.3">KV cache</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.1">Datasets.</span> We evaluate <span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.2">Cake</span> across a range of context lengths based on three datasets with different tasks: LongChat <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib25" title="">2023a</a>)</cite> for multi-turn conversation, and TriviaQA and NarrativeQA <cite class="ltx_cite ltx_citemacro_citep">(Bai et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib6" title="">2023</a>)</cite> for long document question-answering tasks. Based on the statistics analyzed in CacheGen <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib27" title="">2023</a>)</cite>, we found that most dataset queries fall between 5k to 16k tokens in length. Since the specific token values don’t affect our evaluation of <span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.3">Cake</span> performance (only the token length matters), we create synthetic prompts by uniformly sampling 20 data points between 5k to 16k tokens to evaluate the system’s performance.
To further stress test <span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.4">Cake</span> and evaluate its performance at the upper limit of the models’ capabilities, we also generate synthetic prompts with 32K tokens, which corresponds to the maximum context window supported by the LongAlpaca models.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p4.1.1">Baselines.</span> We compare <span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.2">Cake</span> to three types of <span class="ltx_text" id="S5.SS1.p4.1.3">KV cache</span> prefill/loading mechanisms:</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p5">
<p class="ltx_p" id="S5.SS1.p5.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S5.SS1.p5.1.m1.1"><semantics id="S5.SS1.p5.1.m1.1a"><mo id="S5.SS1.p5.1.m1.1.1" xref="S5.SS1.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.1.m1.1b"><ci id="S5.SS1.p5.1.m1.1.1.cmml" xref="S5.SS1.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p5.1.m1.1d">∙</annotation></semantics></math> Compute-only methods, which employ chunk prefill to compute all the <span class="ltx_text" id="S5.SS1.p5.1.1">KV cache</span>. As suggested by vLLM, the token budget size is set to 512 throughout the experiment.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p6">
<p class="ltx_p" id="S5.SS1.p6.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S5.SS1.p6.1.m1.1"><semantics id="S5.SS1.p6.1.m1.1a"><mo id="S5.SS1.p6.1.m1.1.1" xref="S5.SS1.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p6.1.m1.1b"><ci id="S5.SS1.p6.1.m1.1.1.cmml" xref="S5.SS1.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p6.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p6.1.m1.1d">∙</annotation></semantics></math> I/O Fetch-only, which loads saved <span class="ltx_text" id="S5.SS1.p6.1.1">KV cache</span> from local/remote disks through Disk/Network I/O.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p7">
<p class="ltx_p" id="S5.SS1.p7.1"><math alttext="\bullet" class="ltx_Math" display="inline" id="S5.SS1.p7.1.m1.1"><semantics id="S5.SS1.p7.1.m1.1a"><mo id="S5.SS1.p7.1.m1.1.1" xref="S5.SS1.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p7.1.m1.1b"><ci id="S5.SS1.p7.1.m1.1.1.cmml" xref="S5.SS1.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p7.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p7.1.m1.1d">∙</annotation></semantics></math> <span class="ltx_text" id="S5.SS1.p7.1.1">KV cache</span> Compression methods, which are orthogonal to our work. They can compress the <span class="ltx_text" id="S5.SS1.p7.1.2">KV cache</span> size to make them more efficiently transferable through I/O. In our evaluation, we combine the most common 8bit quantization and CacheGen compression technique <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib27" title="">2023</a>)</cite> with <span class="ltx_text ltx_font_italic" id="S5.SS1.p7.1.3">Cake</span> to further evaluate its performance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p8">
<p class="ltx_p" id="S5.SS1.p8.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p8.1.1">Hardware setting.</span> We run our evaluation on two server configurations:
1) An NVIDIA A100 80GB GPU server equipped with a 64-core AMD EPYC 7763 CPU and 2.0TB memory.
2) An NVIDIA H100 GPU server equipped with a 26-core vCPU and 200GB memory.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p9">
<p class="ltx_p" id="S5.SS1.p9.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p9.1.1">I/O Bandwidth Control.</span>
To precisely control I/O Bandwidth with different I/O Bandwidth traces, we calculate the delay time based on the size of the chunk and network bandwidth, and then instruct LLMCache to sleep for this calculated time before fitting the data into CPU memory.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p10">
<p class="ltx_p" id="S5.SS1.p10.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p10.1.1">GPU Resource Usage.</span> In online serving scenarios, it’s common for a machine to serve multiple users’ requests simultaneously. Thus, a user’s prefill operation may not always have access to the full GPU resources. To evaluate different available GPU resource conditions, we utilize vLLM’s token budget scheduling policy as discussed in §<a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S2.SS3" title="2.3 Chunk Prefill ‣ 2 Background ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">2.3</span></a>. We schedule a partial token budget to <span class="ltx_text ltx_font_italic" id="S5.SS1.p10.1.2">Cake</span>’ prefill request and use other synthetically generated requests to consume the rest of the token budget, simulating different levels of GPU resource availability.</p>
</div>
<figure class="ltx_figure" id="S5.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" id="S5.F5.1" style="width:216.8pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="585" id="S5.F5.1.g1" src="x5.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Prefill 32k context of Alpaca-7b.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" id="S5.F5.3" style="width:208.1pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="272" id="S5.F5.2.g1" src="x6.png" width="830"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="334" id="S5.F5.3.g2" src="x7.png" width="830"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Performance of <span class="ltx_text ltx_font_italic" id="S5.F5.3.2.1">Cake</span> on different context lengths.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Evaluation of <span class="ltx_text ltx_font_italic" id="S5.F5.5.1">Cake</span> with full GPU resources.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span><span class="ltx_text ltx_font_italic" id="S5.SS2.1.1">Cake</span> Performance under Full GPU Resources Setting</h3>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">In this section, we evaluate <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.1">Cake</span> with full GPU resources and present the results in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S5.F5" title="Figure 5 ‣ 5.1 Experiment Setup ‣ 5 Evaluation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">5</span></a>. We simulated three static I/O bandwidths (2000/5000/10000 mbps) to represent different I/O conditions (HDD, network, SSD) and recorded the TTFT for a 32k token context under various settings.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">Our observations show that compared to I/O fetch-only methods, utilizing full GPU power simultaneously reduces TTFT by 76.9-93.5% on A100 and 80.2-94.6% on H100. Moreover, compared to compute-only methods, <span class="ltx_text ltx_font_italic" id="S5.SS2.p2.1.1">Cake</span> saves up to 31.5% on A100 and 26.7% on H100. These results demonstrate that <span class="ltx_text ltx_font_italic" id="S5.SS2.p2.1.2">Cake</span> significantly reduces TTFT for long contexts.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">We further analyzed the workload distribution between computation and fetching in <span class="ltx_text ltx_font_italic" id="S5.SS2.p3.1.1">Cake</span> under different context lengths to understand context length effect on compute and I/O. In this setting, we fixed the compute power to use the full A100 GPU, the I/O bandwidth to 10000mbps. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S5.F5" title="Figure 5 ‣ 5.1 Experiment Setup ‣ 5 Evaluation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">5</span></a>. The top figure reveals that with 10000 mbps bandwidth, the percentage of <span class="ltx_text" id="S5.SS2.p3.1.2">KV cache</span> chunks processed by computation decreases by 10% as context length increases from 2k to 32k tokens. This aligns with our observation in §<a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S2.SS3" title="2.3 Chunk Prefill ‣ 2 Background ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">2.3</span></a>: as context length grows, computing <span class="ltx_text" id="S5.SS2.p3.1.3">KV cache</span> for later tokens becomes more time-consuming, while I/O time remains constant. Consequently, at the convergence point, a smaller percentage of tokens will be processed by computation as context length increases.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1">The bottom figure illustrates the ratio of <span class="ltx_text ltx_font_italic" id="S5.SS2.p4.1.1">Cake</span> TTFT to compute-only TTFT. We observe that as context length increases from 2k to 32k tokens, the relative TTFT is reduced by 15%. This demonstrates that <span class="ltx_text ltx_font_italic" id="S5.SS2.p4.1.2">Cake</span> becomes even more efficient compared to compute-only methods as context length increases.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p5">
<p class="ltx_p" id="S5.SS2.p5.1">Referring to Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S3.F3" title="Figure 3 ‣ 3 Exploratory Experiments and Motivation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">3</span></a>, we note that computing later chunks also requires more GPU memory. <span class="ltx_text" id="S5.SS2.p5.1.1">KV cache</span> memory consumption grows linearly with token size. With <span class="ltx_text ltx_font_italic" id="S5.SS2.p5.1.2">Cake</span> under 10000 mbps bandwidth, only 75% tokens need to do computation and store <span class="ltx_text" id="S5.SS2.p5.1.3">KV cache</span> in GPU memory, while the saved 25% space can be allocated for other short requests. This further highlights <span class="ltx_text ltx_font_italic" id="S5.SS2.p5.1.4">Cake</span>’s memory efficiency.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span><span class="ltx_text ltx_font_italic" id="S5.SS3.1.1">Cake</span> Performance under Different GPU Usage</h3>
<div class="ltx_para ltx_noindent" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">In this section, we evaluate <span class="ltx_text ltx_font_italic" id="S5.SS3.p1.1.1">Cake</span> under different GPU resource usage scenarios to assess its performance in real-world settings where GPU resources need to be shared among multiple users. The results are demonstrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S5.F6" title="Figure 6 ‣ 5.3 Cake Performance under Different GPU Usage ‣ 5 Evaluation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">6</span></a>. We control GPU usage by scheduling limited token budgets as described in §<a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S5.SS1" title="5.1 Experiment Setup ‣ 5 Evaluation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">5.1</span></a>. We present two representative results with a context length of <math alttext="14" class="ltx_Math" display="inline" id="S5.SS3.p1.1.m1.1"><semantics id="S5.SS3.p1.1.m1.1a"><mn id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml">14</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><cn id="S5.SS3.p1.1.m1.1.1.cmml" type="integer" xref="S5.SS3.p1.1.m1.1.1">14</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">14</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.1.m1.1d">14</annotation></semantics></math>k tokens, with additional results available in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#A3" title="Appendix C Performance of Cake under different conditions ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">Compared to the compute-only method, <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.1">Cake</span> leverages I/O prefetching to reduce latency by 5.3-68.1%. Higher I/O bandwidth allows <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.2">Cake</span> to provide greater benefits. When compared to I/O-only methods, <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.3">Cake</span> utilizes computation to reduce latency by 27.4-93.7%, with higher computation power yielding more significant improvements.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.2">Specifically, using the compute-only method, reducing computation power from 90% to 10% results in an <math alttext="8.22" class="ltx_Math" display="inline" id="S5.SS3.p3.1.m1.1"><semantics id="S5.SS3.p3.1.m1.1a"><mn id="S5.SS3.p3.1.m1.1.1" xref="S5.SS3.p3.1.m1.1.1.cmml">8.22</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.1.m1.1b"><cn id="S5.SS3.p3.1.m1.1.1.cmml" type="float" xref="S5.SS3.p3.1.m1.1.1">8.22</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.1.m1.1c">8.22</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.1.m1.1d">8.22</annotation></semantics></math>x increase in prefilling latency, leading to an additional <math alttext="14.34" class="ltx_Math" display="inline" id="S5.SS3.p3.2.m2.1"><semantics id="S5.SS3.p3.2.m2.1a"><mn id="S5.SS3.p3.2.m2.1.1" xref="S5.SS3.p3.2.m2.1.1.cmml">14.34</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.2.m2.1b"><cn id="S5.SS3.p3.2.m2.1.1.cmml" type="float" xref="S5.SS3.p3.2.m2.1.1">14.34</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.2.m2.1c">14.34</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.2.m2.1d">14.34</annotation></semantics></math>s delay. However, <span class="ltx_text ltx_font_italic" id="S5.SS3.p3.2.1">Cake</span> automatically fetches more chunks of <span class="ltx_text" id="S5.SS3.p3.2.2">KV cache</span> via I/O, reducing latency by up to 68.1% compared to the compute-only method and 27.4-65.1% compared to the I/O-only method under the same conditions. This observation generalizes across different context lengths, compute powers and model sizes, as shown in our results in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#A3" title="Appendix C Performance of Cake under different conditions ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">C</span></a>, demonstrating that <span class="ltx_text ltx_font_italic" id="S5.SS3.p3.2.3">Cake</span> consistently achieves the fastest prefilling speed compared to both computation-only and I/O-only methods.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1">In conclusion, <span class="ltx_text ltx_font_italic" id="S5.SS3.p4.1.1">Cake</span> significantly reduces prefilling latency when available computation power is insufficient by leveraging <span class="ltx_text" id="S5.SS3.p4.1.2">KV cache</span> fetching. This feature can be exploited to utilize fragmented computational resources for prefilling long context requests without adversely affecting other users’ experience.</p>
</div>
<figure class="ltx_figure" id="S5.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="S5.F6.1" style="width:182.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="525" id="S5.F6.1.g1" src="x8.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Computation power 90%</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="S5.F6.2" style="width:247.2pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="388" id="S5.F6.2.g1" src="x9.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Computation power 10%</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>TTFT of prefilling 14k context of Alpaca-7B on a single A100. <span class="ltx_text ltx_font_italic" id="S5.F6.5.1">Cake</span> with <span class="ltx_text ltx_font_italic" id="S5.F6.6.2">CacheGen</span> achieves the lowest latency comparing to compute-only and different I/O fetching methods.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Incorporating <span class="ltx_text" id="S5.SS4.1.1">KV cache</span> Compression with <span class="ltx_text ltx_font_italic" id="S5.SS4.2.2">Cake</span>
</h3>
<div class="ltx_para ltx_noindent" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.2">In this section, we show that <span class="ltx_text ltx_font_italic" id="S5.SS4.p1.2.1">Cake</span> can incorporate with state-of-the-art <span class="ltx_text" id="S5.SS4.p1.2.2">KV cache</span> compression technologies to further boost its performance. We use the same settings as in §<a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S5.SS3" title="5.3 Cake Performance under Different GPU Usage ‣ 5 Evaluation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">5.3</span></a> and apply the widely used 8bit-quantization and the latest CacheGen  <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib27" title="">2023</a>)</cite> to reduce the size of <span class="ltx_text" id="S5.SS4.p1.2.3">KV cache</span>. Theoretically, they will reduce the fetching time by <math alttext="2" class="ltx_Math" display="inline" id="S5.SS4.p1.1.m1.1"><semantics id="S5.SS4.p1.1.m1.1a"><mn id="S5.SS4.p1.1.m1.1.1" xref="S5.SS4.p1.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.1.m1.1b"><cn id="S5.SS4.p1.1.m1.1.1.cmml" type="integer" xref="S5.SS4.p1.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.1.m1.1c">2</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.1.m1.1d">2</annotation></semantics></math> and <math alttext="8.6" class="ltx_Math" display="inline" id="S5.SS4.p1.2.m2.1"><semantics id="S5.SS4.p1.2.m2.1a"><mn id="S5.SS4.p1.2.m2.1.1" xref="S5.SS4.p1.2.m2.1.1.cmml">8.6</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.2.m2.1b"><cn id="S5.SS4.p1.2.m2.1.1.cmml" type="float" xref="S5.SS4.p1.2.m2.1.1">8.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.2.m2.1c">8.6</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.2.m2.1d">8.6</annotation></semantics></math> respectively, thus lowering the bandwidth requirement.</p>
</div>
<figure class="ltx_figure ltx_align_floatright" id="S5.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="490" id="S5.F7.g1" src="x10.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Per-step inference time in vLLM before and after integration with <span class="ltx_text ltx_font_italic" id="S5.F7.4.1">Cake</span>. The solid line represents the step time without <span class="ltx_text ltx_font_italic" id="S5.F7.5.2">Cake</span>, while the ’x’ markers indicate step times with <span class="ltx_text ltx_font_italic" id="S5.F7.6.3">Cake</span>.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">Our results in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S5.F6" title="Figure 6 ‣ 5.3 Cake Performance under Different GPU Usage ‣ 5 Evaluation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">6</span></a> demonstrate that 8bit-quantization and CacheGen significantly enhance I/O bandwidth, achieving speedups of up to 1.95x and 7.97x respectively. Leveraging these improvements, <span class="ltx_text ltx_font_italic" id="S5.SS4.p2.1.1">Cake</span> is able to fetch more <span class="ltx_text" id="S5.SS4.p2.1.2">KV cache</span> chunks during computation, resulting in a remarkable 11.83x speedup compared to computation with only 10% GPU power. Furthermore, <span class="ltx_text ltx_font_italic" id="S5.SS4.p2.1.3">Cake</span> combined with CacheGen reduces time-to-first-token (TTFT) by 32.8-73.5% compared to <span class="ltx_text ltx_font_italic" id="S5.SS4.p2.1.4">Cake</span> with raw <span class="ltx_text" id="S5.SS4.p2.1.5">KV cache</span>, showcasing its ability to efficiently utilize all available resources to minimize latency.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS4.p3">
<p class="ltx_p" id="S5.SS4.p3.1">Conversely, when <span class="ltx_text ltx_font_italic" id="S5.SS4.p3.1.1">Cake</span> has access to 90% of GPU resources, it outperforms CacheGen-only fetching by reducing TTFT by 66.4%. This demonstrates <span class="ltx_text ltx_font_italic" id="S5.SS4.p3.1.2">Cake</span>’s adaptability across different resource availability scenarios. As detailed in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#A3" title="Appendix C Performance of Cake under different conditions ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">C</span></a>, <span class="ltx_text ltx_font_italic" id="S5.SS4.p3.1.3">Cake</span> consistently achieves superior performance compared to both computation-only and fetching-only methods across a wide range of scenarios, highlighting its versatility and effectiveness in optimizing prefilling latency.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Overheads of <span class="ltx_text ltx_font_italic" id="S5.SS5.1.1">Cake</span> in LLM Serving System</h3>
<div class="ltx_para ltx_noindent" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">To demonstrate <span class="ltx_text ltx_font_italic" id="S5.SS5.p1.1.1">Cake</span> has negaligible overheads, we compare the duration of each engine step on original vLLM and vLLM with <span class="ltx_text ltx_font_italic" id="S5.SS5.p1.1.2">Cake</span>. As is shown on Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S5.F7" title="Figure 7 ‣ 5.4 Incorporating KV cache Compression with Cake ‣ 5 Evaluation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">7</span></a>, we launch a chunk prefill job on A100 and H100 server, and the chunk prefill time of vLLM with <span class="ltx_text ltx_font_italic" id="S5.SS5.p1.1.3">Cake</span> basically follows the trace of original vLLM. This results prove that <span class="ltx_text ltx_font_italic" id="S5.SS5.p1.1.4">Cake</span> introduces negaligible overheads, as it only needs to check whether the next chunk is already fetched and doesn’t have to schedule anything at runtime.</p>
</div>
<figure class="ltx_figure" id="S5.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="S5.F8.1" style="width:208.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="479" id="S5.F8.1.g1" src="x11.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Automatic merging point adaptation of <span class="ltx_text ltx_font_italic" id="S5.F8.1.2.1">Cake</span> under dynamic I/O bandwidth. Scatter points represent KV cache computation initiated at the start, and dashed lines show I/O fetching from the end.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="S5.F8.2" style="width:208.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="470" id="S5.F8.2.g1" src="x12.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Time to prefill 14k context using <span class="ltx_text ltx_font_italic" id="S5.F8.2.2.1">Cake</span> across different bandwidth and GPU resources. This demonstrates Cake’s effective balance between computation and I/O bandwidth.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Case study of <span class="ltx_text ltx_font_italic" id="S5.F8.4.1">Cake</span> under various conditions using A100.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6 </span>Understand How <span class="ltx_text ltx_font_italic" id="S5.SS6.1.1">Cake</span> Contributes to Computation Power</h3>
<div class="ltx_para ltx_noindent" id="S5.SS6.p1">
<ol class="ltx_enumerate" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1">Auto Adaptation</span>: As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S5.F8" title="Figure 8 ‣ 5.5 Overheads of Cake in LLM Serving System ‣ 5 Evaluation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">8</span></a>, Cake dynamically utilizes both I/O and computational resources without chunk-level scheduling. Its bidirectional prefetching mechanism automatically determines the optimal merging point to minimize TTFT. Occasionally, the last chunk via I/O may experience delays in fetching, but <span class="ltx_text ltx_font_italic" id="S5.I1.i1.p1.1.2">Cake</span> begins decoding immediately once the <span class="ltx_text" id="S5.I1.i1.p1.1.3">KV cache</span> is complete, avoiding unnecessary waiting.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para ltx_noindent" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">Computation-bandwidth Translation</span>: Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S5.F8" title="Figure 8 ‣ 5.5 Overheads of Cake in LLM Serving System ‣ 5 Evaluation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">8</span></a> illustrates <span class="ltx_text ltx_font_italic" id="S5.I1.i2.p1.1.2">Cake</span>’s performance under various bandwidth conditions, from commodity networks (0.25-2GB/s) to high-speed NVMe SSDs (<math alttext="\approx" class="ltx_Math" display="inline" id="S5.I1.i2.p1.1.m1.1"><semantics id="S5.I1.i2.p1.1.m1.1a"><mo id="S5.I1.i2.p1.1.m1.1.1" xref="S5.I1.i2.p1.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S5.I1.i2.p1.1.m1.1b"><approx id="S5.I1.i2.p1.1.m1.1.1.cmml" xref="S5.I1.i2.p1.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i2.p1.1.m1.1c">\approx</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i2.p1.1.m1.1d">≈</annotation></semantics></math>4 GB/s). The plot shows TTFT across different levels of GPU resources. With fast I/O (4 GB/s), <span class="ltx_text ltx_font_italic" id="S5.I1.i2.p1.1.3">Cake</span> requires only 10% GPU power to outperform a compute-only method operating at 50% power.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this section, we discuss the benefits and potential overheads introduced by <span class="ltx_text ltx_font_italic" id="S6.p1.1.1">Cake</span>. Our system effectively utilizes both compute and I/O resources to reduce <span class="ltx_text" id="S6.p1.1.2">KV cache</span> loading latency, thereby improving the Time To First Token (TTFT).</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p2">
<p class="ltx_p" id="S6.p2.1"><span class="ltx_text ltx_font_bold" id="S6.p2.1.1">Comparison with Compute-Only Methods.</span> Unlike approaches that rely solely on computation, <span class="ltx_text ltx_font_italic" id="S6.p2.1.2">Cake</span> leverages I/O bandwidth to reduce the computational load, resulting in lower latency. This is particularly advantageous as I/O resources are often less costly and more readily available compared to high-performance compute resources.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p3">
<p class="ltx_p" id="S6.p3.1"><span class="ltx_text ltx_font_bold" id="S6.p3.1.1">Comparison with I/O-Only Methods.</span> While <span class="ltx_text ltx_font_italic" id="S6.p3.1.2">Cake</span> primarily focuses on efficient I/O utilization, it also employs compute resources to further reduce TTFT. This dual approach may introduce some additional computational overhead. However, as discussed in §<a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S5.SS3" title="5.3 Cake Performance under Different GPU Usage ‣ 5 Evaluation ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">5.3</span></a> and §<a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#S2.SS3" title="2.3 Chunk Prefill ‣ 2 Background ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">2.3</span></a>, we can strategically use only the unused token budget for chunk prefill computation. This approach minimizes the system’s computational overhead due to the batching effect and efficient GPU utilization. The added computation is efficiently processed alongside other tasks, leveraging GPU parallelism. Even if there are no unused token left, we can pause the compute procedure and let I/O contribute more to the procedure.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p4">
<p class="ltx_p" id="S6.p4.1"><span class="ltx_text ltx_font_bold" id="S6.p4.1.1">Cost-Benefit Analysis.</span> In many online services, users who pay more often receive higher priority and faster request processing. <span class="ltx_text ltx_font_italic" id="S6.p4.1.2">Cake</span> is particularly well-suited for these scenarios, offering significantly reduced TTFT for a marginal increase in cost.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this paper, we introduced <span class="ltx_text ltx_font_italic" id="S7.p1.1.1">Cake</span>, a novel approach that efficiently utilizes both I/O and compute resources to reduce Time To First Token (TTFT) for LLM serving systems with prefix caching. <span class="ltx_text ltx_font_italic" id="S7.p1.1.2">Cake</span> dynamically adapts to varying resource conditions, seamlessly integrating with existing <span class="ltx_text" id="S7.p1.1.3">KV cache</span> optimization techniques to achieve optimal latency with minimal overhead.
Our evaluation shows <span class="ltx_text ltx_font_italic" id="S7.p1.1.4">Cake</span> outperforms both compute-only and I/O-only methods, reducing TTFT by up to 95% compared to baselines. <span class="ltx_text ltx_font_italic" id="S7.p1.1.5">Cake</span> balances I/O and compute resources to maximize performance gains without significantly increasing costs. As a simple plug-in solution, <span class="ltx_text ltx_font_italic" id="S7.p1.1.6">Cake</span> is easily implementable in existing LLM serving systems with prefix caching, offering substantial performance improvements and straightforward integration to enhance the responsiveness and efficiency of LLM services.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Ethics Statement</h2>
<div class="ltx_para ltx_noindent" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">Our research on <span class="ltx_text ltx_font_italic" id="S8.p1.1.1">Cake</span>, a system for reducing Time To First Token (TTFT) in LLM serving systems, primarily focuses on improving computational efficiency without directly involving human subjects or sensitive personal data. However, we acknowledge broader ethical implications: potential disparities in service quality based on user access levels, the positive environmental impact through optimized resource utilization, our commitment to transparency and reproducibility, the risk of misuse in scaling harmful LLM applications, and the importance of adhering to ethical AI deployment guidelines. We declare no conflicts of interest and affirm our commitment to the ICLR Code of Ethics, having conducted this research with integrity and in compliance with established ethical standards in AI and computer science. While <span class="ltx_text ltx_font_italic" id="S8.p1.1.2">Cake</span> aims to enhance LLM services, we encourage implementers to consider fair allocation strategies, implement safeguards against misuse, and continue efforts to minimize the environmental footprint of AI systems.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. (2023)</span>
<span class="ltx_bibblock">
Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S Gulavani, and Ramachandran Ramjee.

</span>
<span class="ltx_bibblock">Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2308.16369</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. (2024)</span>
<span class="ltx_bibblock">
Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S Gulavani, Alexey Tumanov, and Ramachandran Ramjee.

</span>
<span class="ltx_bibblock">Taming throughput-latency tradeoff in llm inference with sarathi-serve.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2403.02310</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2023)</span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Prompt caching, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.anthropic.com/news/prompt-caching" title="">https://www.anthropic.com/news/prompt-caching</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2024)</span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Anthropic model comparison, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://docs.anthropic.com/en/docs/about-claude/models#model-comparison-table" title="">https://docs.anthropic.com/en/docs/about-claude/models#model-comparison-table</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AutoGen (2024)</span>
<span class="ltx_bibblock">
AutoGen.

</span>
<span class="ltx_bibblock">Autogen disk cache, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://microsoft.github.io/autogen/docs/reference/cache/disk_cache/" title="">https://microsoft.github.io/autogen/docs/reference/cache/disk_cache/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. (2023)</span>
<span class="ltx_bibblock">
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al.

</span>
<span class="ltx_bibblock">Longbench: A bilingual, multitask benchmark for long context understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2308.14508</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom B. Brown, Ben Mann, Nick Ryder, Deepak Subbiah, Jack Kaplan, Prafulla Dhariwal, and Dario Amodei.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Advances in Neural Information Processing Systems</em>, 33, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2005.14165" title="">https://arxiv.org/abs/2005.14165</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023)</span>
<span class="ltx_bibblock">
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia.

</span>
<span class="ltx_bibblock">Longlora: Efficient fine-tuning of long-context large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2309.12307</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cursor (2024)</span>
<span class="ltx_bibblock">
Cursor.

</span>
<span class="ltx_bibblock">Codebase indexing, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://docs.cursor.com/context/codebase-indexing" title="">https://docs.cursor.com/context/codebase-indexing</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deepseek (2024)</span>
<span class="ltx_bibblock">
Deepseek.

</span>
<span class="ltx_bibblock">Prompt caching api, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://platform.deepseek.com/api-docs/news/news0802/" title="">https://platform.deepseek.com/api-docs/news/news0802/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu (2024)</span>
<span class="ltx_bibblock">
Yao Fu.

</span>
<span class="ltx_bibblock">Challenges in deploying long-context transformers: A theoretical peak performance analysis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2405.08944</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2024)</span>
<span class="ltx_bibblock">
Bin Gao, Zhuomin He, Puru Sharma, Qingxuan Kang, Djordje Jevdjic, Junbo Deng, Xingkun Yang, Zhou Yu, and Pengfei Zuo.

</span>
<span class="ltx_bibblock">Attentionstore: Cost-effective attention reuse across multi-turn conversations in large language model serving.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2403.19708</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gim et al. (2024)</span>
<span class="ltx_bibblock">
In Gim, Guojun Chen, Seung-seob Lee, Nikhil Sarda, Anurag Khandelwal, and Lin Zhong.

</span>
<span class="ltx_bibblock">Prompt cache: Modular attention reuse for low-latency inference.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of Machine Learning and Systems</em>, 6:325–338, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hooper et al. (2024)</span>
<span class="ltx_bibblock">
Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami.

</span>
<span class="ltx_bibblock">Kvquant: Towards 10 million context length llm inference with kv cache quantization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2401.18079</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et al. (2023)</span>
<span class="ltx_bibblock">
Paras Jain, Sam Kumar, Sarah Wooders, Shishir G Patil, Joseph E Gonzalez, and Ion Stoica.

</span>
<span class="ltx_bibblock">Skyplane: Optimizing transfer cost and throughput using <math alttext="\{" class="ltx_Math" display="inline" id="bib.bib15.1.m1.1"><semantics id="bib.bib15.1.m1.1a"><mo id="bib.bib15.1.m1.1.1" stretchy="false" xref="bib.bib15.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib15.1.m1.1b"><ci id="bib.bib15.1.m1.1.1.cmml" xref="bib.bib15.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib15.1.m1.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib15.1.m1.1d">{</annotation></semantics></math>Cloud-Aware<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib15.2.m2.1"><semantics id="bib.bib15.2.m2.1a"><mo id="bib.bib15.2.m2.1.1" stretchy="false" xref="bib.bib15.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib15.2.m2.1b"><ci id="bib.bib15.2.m2.1.1.cmml" xref="bib.bib15.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib15.2.m2.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib15.2.m2.1d">}</annotation></semantics></math> overlays.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)</em>, pp.  1375–1389, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023)</span>
<span class="ltx_bibblock">
Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu.

</span>
<span class="ltx_bibblock">Llmlingua: Compressing prompts for accelerated inference of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2310.05736</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2024)</span>
<span class="ltx_bibblock">
Ziyan Jiang, Xueguang Ma, and Wenhu Chen.

</span>
<span class="ltx_bibblock">Longrag: Enhancing retrieval-augmented generation with long-context llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2406.15319</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. (2024a)</span>
<span class="ltx_bibblock">
Chao Jin, Zili Zhang, Xuanlin Jiang, Fangyue Liu, Xin Liu, Xuanzhe Liu, and Xin Jin.

</span>
<span class="ltx_bibblock">Ragcache: Efficient knowledge caching for retrieval-augmented generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:2404.12457</em>, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. (2024b)</span>
<span class="ltx_bibblock">
Shuowei Jin, Yongji Wu, Haizhong Zheng, Qingzhao Zhang, Matthew Lentz, Z Morley Mao, Atul Prakash, Feng Qian, and Danyang Zhuo.

</span>
<span class="ltx_bibblock">Adaptive skeleton graph decoding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2402.12280</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Juravsky et al. (2024)</span>
<span class="ltx_bibblock">
Jordan Juravsky, Bradley Brown, Ryan Ehrlich, Daniel Y Fu, Christopher Ré, and Azalia Mirhoseini.

</span>
<span class="ltx_bibblock">Hydragen: High-throughput llm inference with shared prefixes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2402.05099</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et al. (2024)</span>
<span class="ltx_bibblock">
Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, and Tuo Zhao.

</span>
<span class="ltx_bibblock">Gear: An efficient kv cache compression recipefor near-lossless generative inference of llm.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:2403.05527</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon et al. (2023)</span>
<span class="ltx_bibblock">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica.

</span>
<span class="ltx_bibblock">Efficient memory management for large language model serving with pagedattention.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 29th Symposium on Operating Systems Principles</em>, pp.  611–626, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lambda (2024)</span>
<span class="ltx_bibblock">
Lambda.

</span>
<span class="ltx_bibblock">Lambda lab gpu cloud specifications, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://lambdalabs.com/service/gpu-cloud" title="">https://lambdalabs.com/service/gpu-cloud</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leviathan et al. (2023)</span>
<span class="ltx_bibblock">
Yaniv Leviathan, Matan Kalman, and Yossi Matias.

</span>
<span class="ltx_bibblock">Fast inference from transformers via speculative decoding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">International Conference on Machine Learning</em>, pp.  19274–19286. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023a)</span>
<span class="ltx_bibblock">
Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang.

</span>
<span class="ltx_bibblock">How long can context length of open-source llms truly promise?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023b)</span>
<span class="ltx_bibblock">
Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.

</span>
<span class="ltx_bibblock">Camel: Communicative agents for” mind” exploration of large language model society.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Advances in Neural Information Processing Systems</em>, 36:51991–52008, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Yuhan Liu, Hanchen Li, Kuntai Du, Jiayi Yao, Yihua Cheng, Yuyang Huang, Shan Lu, Michael Maire, Henry Hoffmann, Ari Holtzman, et al.

</span>
<span class="ltx_bibblock">Cachegen: Fast context loading for language model applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2310.07240</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LMCache (2024)</span>
<span class="ltx_bibblock">
LMCache.

</span>
<span class="ltx_bibblock">Lmcache, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/LMCache/LMCache" title="">https://github.com/LMCache/LMCache</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miao et al. (2024)</span>
<span class="ltx_bibblock">
Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, et al.

</span>
<span class="ltx_bibblock">Specinfer: Accelerating large language model serving with tree-based speculative inference and verification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3</em>, pp.  932–949, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ning et al. (2023)</span>
<span class="ltx_bibblock">
Xuefei Ning, Zinan Lin, Zixuan Zhou, Zifu Wang, Huazhong Yang, and Yu Wang.

</span>
<span class="ltx_bibblock">Skeleton-of-thought: Large language models can do parallel decoding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings ENLSP-III</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">openAI (2024)</span>
<span class="ltx_bibblock">
openAI.

</span>
<span class="ltx_bibblock">Gpt-4o, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://platform.openai.com/docs/models/gpt-4o" title="">https://platform.openai.com/docs/models/gpt-4o</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Advances in Neural Information Processing Systems</em>, volume 30. Curran Associates, Inc., 2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024)</span>
<span class="ltx_bibblock">
Minzheng Wang, Longze Chen, Cheng Fu, Shengyi Liao, Xinghua Zhang, Bingli Wu, Haiyang Yu, Nan Xu, Lei Zhang, Run Luo, et al.

</span>
<span class="ltx_bibblock">Leave no document behind: Benchmarking long-context llms with extended multi-doc qa.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2406.17419</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2023)</span>
<span class="ltx_bibblock">
Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang.

</span>
<span class="ltx_bibblock">Autogen: Enabling next-gen llm applications via multi-agent conversation framework.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2308.08155</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2024)</span>
<span class="ltx_bibblock">
Jiawei Yang, Zhiqiang Huang, Chen Li, and Yifan Wang.

</span>
<span class="ltx_bibblock">Accelerating inference for transformers using mixed precision and efficient caching.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:2407.18003</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2407.18003" title="">https://arxiv.org/abs/2407.18003</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. (2024)</span>
<span class="ltx_bibblock">
Jiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu, and Junchen Jiang.

</span>
<span class="ltx_bibblock">Cacheblend: Fast large language model serving with cached knowledge fusion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2405.16444</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Yilong Zhang, Kun Sun, and Nicolas Papernot.

</span>
<span class="ltx_bibblock">Best of both worlds: Accelerator-aware caching for building efficient transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2303.06865</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2303.06865" title="">https://arxiv.org/abs/2303.06865</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024)</span>
<span class="ltx_bibblock">
Yusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister, Rui Zhang, and Sercan Ö Arik.

</span>
<span class="ltx_bibblock">Chain of agents: Large language models collaborating on long-context tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2406.02818</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2023)</span>
<span class="ltx_bibblock">
Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez, et al.

</span>
<span class="ltx_bibblock">Efficiently programming large language models using sglang.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:2312.07104</em>, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Details of <span class="ltx_text ltx_font_italic" id="A1.1.1">Cake</span> Algorithm</h2>
<div class="ltx_para ltx_noindent" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">The workflow of <span class="ltx_text ltx_font_italic" id="A1.p1.1.1">Cake</span> can be described in detail as follows:</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p2">
<p class="ltx_p" id="A1.p2.1">1. Upon receiving a request, <span class="ltx_text ltx_font_italic" id="A1.p2.1.1">Cake</span> first splits the input sequence into chunks of a predetermined size (e.g., 512 tokens as suggested by vLLM).</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p3">
<p class="ltx_p" id="A1.p3.3">2. Two pointers are initialized: <math alttext="compute\_ptr" class="ltx_Math" display="inline" id="A1.p3.1.m1.1"><semantics id="A1.p3.1.m1.1a"><mrow id="A1.p3.1.m1.1.1" xref="A1.p3.1.m1.1.1.cmml"><mi id="A1.p3.1.m1.1.1.2" xref="A1.p3.1.m1.1.1.2.cmml">c</mi><mo id="A1.p3.1.m1.1.1.1" xref="A1.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p3.1.m1.1.1.3" xref="A1.p3.1.m1.1.1.3.cmml">o</mi><mo id="A1.p3.1.m1.1.1.1a" xref="A1.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p3.1.m1.1.1.4" xref="A1.p3.1.m1.1.1.4.cmml">m</mi><mo id="A1.p3.1.m1.1.1.1b" xref="A1.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p3.1.m1.1.1.5" xref="A1.p3.1.m1.1.1.5.cmml">p</mi><mo id="A1.p3.1.m1.1.1.1c" xref="A1.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p3.1.m1.1.1.6" xref="A1.p3.1.m1.1.1.6.cmml">u</mi><mo id="A1.p3.1.m1.1.1.1d" xref="A1.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p3.1.m1.1.1.7" xref="A1.p3.1.m1.1.1.7.cmml">t</mi><mo id="A1.p3.1.m1.1.1.1e" xref="A1.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p3.1.m1.1.1.8" xref="A1.p3.1.m1.1.1.8.cmml">e</mi><mo id="A1.p3.1.m1.1.1.1f" xref="A1.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p3.1.m1.1.1.9" mathvariant="normal" xref="A1.p3.1.m1.1.1.9.cmml">_</mi><mo id="A1.p3.1.m1.1.1.1g" xref="A1.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p3.1.m1.1.1.10" xref="A1.p3.1.m1.1.1.10.cmml">p</mi><mo id="A1.p3.1.m1.1.1.1h" xref="A1.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p3.1.m1.1.1.11" xref="A1.p3.1.m1.1.1.11.cmml">t</mi><mo id="A1.p3.1.m1.1.1.1i" xref="A1.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p3.1.m1.1.1.12" xref="A1.p3.1.m1.1.1.12.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.p3.1.m1.1b"><apply id="A1.p3.1.m1.1.1.cmml" xref="A1.p3.1.m1.1.1"><times id="A1.p3.1.m1.1.1.1.cmml" xref="A1.p3.1.m1.1.1.1"></times><ci id="A1.p3.1.m1.1.1.2.cmml" xref="A1.p3.1.m1.1.1.2">𝑐</ci><ci id="A1.p3.1.m1.1.1.3.cmml" xref="A1.p3.1.m1.1.1.3">𝑜</ci><ci id="A1.p3.1.m1.1.1.4.cmml" xref="A1.p3.1.m1.1.1.4">𝑚</ci><ci id="A1.p3.1.m1.1.1.5.cmml" xref="A1.p3.1.m1.1.1.5">𝑝</ci><ci id="A1.p3.1.m1.1.1.6.cmml" xref="A1.p3.1.m1.1.1.6">𝑢</ci><ci id="A1.p3.1.m1.1.1.7.cmml" xref="A1.p3.1.m1.1.1.7">𝑡</ci><ci id="A1.p3.1.m1.1.1.8.cmml" xref="A1.p3.1.m1.1.1.8">𝑒</ci><ci id="A1.p3.1.m1.1.1.9.cmml" xref="A1.p3.1.m1.1.1.9">_</ci><ci id="A1.p3.1.m1.1.1.10.cmml" xref="A1.p3.1.m1.1.1.10">𝑝</ci><ci id="A1.p3.1.m1.1.1.11.cmml" xref="A1.p3.1.m1.1.1.11">𝑡</ci><ci id="A1.p3.1.m1.1.1.12.cmml" xref="A1.p3.1.m1.1.1.12">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p3.1.m1.1c">compute\_ptr</annotation><annotation encoding="application/x-llamapun" id="A1.p3.1.m1.1d">italic_c italic_o italic_m italic_p italic_u italic_t italic_e _ italic_p italic_t italic_r</annotation></semantics></math> starting at the beginning of the sequence (index 0), and <math alttext="io\_ptr" class="ltx_Math" display="inline" id="A1.p3.2.m2.1"><semantics id="A1.p3.2.m2.1a"><mrow id="A1.p3.2.m2.1.1" xref="A1.p3.2.m2.1.1.cmml"><mi id="A1.p3.2.m2.1.1.2" xref="A1.p3.2.m2.1.1.2.cmml">i</mi><mo id="A1.p3.2.m2.1.1.1" xref="A1.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="A1.p3.2.m2.1.1.3" xref="A1.p3.2.m2.1.1.3.cmml">o</mi><mo id="A1.p3.2.m2.1.1.1a" xref="A1.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="A1.p3.2.m2.1.1.4" mathvariant="normal" xref="A1.p3.2.m2.1.1.4.cmml">_</mi><mo id="A1.p3.2.m2.1.1.1b" xref="A1.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="A1.p3.2.m2.1.1.5" xref="A1.p3.2.m2.1.1.5.cmml">p</mi><mo id="A1.p3.2.m2.1.1.1c" xref="A1.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="A1.p3.2.m2.1.1.6" xref="A1.p3.2.m2.1.1.6.cmml">t</mi><mo id="A1.p3.2.m2.1.1.1d" xref="A1.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="A1.p3.2.m2.1.1.7" xref="A1.p3.2.m2.1.1.7.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.p3.2.m2.1b"><apply id="A1.p3.2.m2.1.1.cmml" xref="A1.p3.2.m2.1.1"><times id="A1.p3.2.m2.1.1.1.cmml" xref="A1.p3.2.m2.1.1.1"></times><ci id="A1.p3.2.m2.1.1.2.cmml" xref="A1.p3.2.m2.1.1.2">𝑖</ci><ci id="A1.p3.2.m2.1.1.3.cmml" xref="A1.p3.2.m2.1.1.3">𝑜</ci><ci id="A1.p3.2.m2.1.1.4.cmml" xref="A1.p3.2.m2.1.1.4">_</ci><ci id="A1.p3.2.m2.1.1.5.cmml" xref="A1.p3.2.m2.1.1.5">𝑝</ci><ci id="A1.p3.2.m2.1.1.6.cmml" xref="A1.p3.2.m2.1.1.6">𝑡</ci><ci id="A1.p3.2.m2.1.1.7.cmml" xref="A1.p3.2.m2.1.1.7">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p3.2.m2.1c">io\_ptr</annotation><annotation encoding="application/x-llamapun" id="A1.p3.2.m2.1d">italic_i italic_o _ italic_p italic_t italic_r</annotation></semantics></math> at the end of the sequence (index <math alttext="total\_tokens-1" class="ltx_Math" display="inline" id="A1.p3.3.m3.1"><semantics id="A1.p3.3.m3.1a"><mrow id="A1.p3.3.m3.1.1" xref="A1.p3.3.m3.1.1.cmml"><mrow id="A1.p3.3.m3.1.1.2" xref="A1.p3.3.m3.1.1.2.cmml"><mi id="A1.p3.3.m3.1.1.2.2" xref="A1.p3.3.m3.1.1.2.2.cmml">t</mi><mo id="A1.p3.3.m3.1.1.2.1" xref="A1.p3.3.m3.1.1.2.1.cmml">⁢</mo><mi id="A1.p3.3.m3.1.1.2.3" xref="A1.p3.3.m3.1.1.2.3.cmml">o</mi><mo id="A1.p3.3.m3.1.1.2.1a" xref="A1.p3.3.m3.1.1.2.1.cmml">⁢</mo><mi id="A1.p3.3.m3.1.1.2.4" xref="A1.p3.3.m3.1.1.2.4.cmml">t</mi><mo id="A1.p3.3.m3.1.1.2.1b" xref="A1.p3.3.m3.1.1.2.1.cmml">⁢</mo><mi id="A1.p3.3.m3.1.1.2.5" xref="A1.p3.3.m3.1.1.2.5.cmml">a</mi><mo id="A1.p3.3.m3.1.1.2.1c" xref="A1.p3.3.m3.1.1.2.1.cmml">⁢</mo><mi id="A1.p3.3.m3.1.1.2.6" xref="A1.p3.3.m3.1.1.2.6.cmml">l</mi><mo id="A1.p3.3.m3.1.1.2.1d" xref="A1.p3.3.m3.1.1.2.1.cmml">⁢</mo><mi id="A1.p3.3.m3.1.1.2.7" mathvariant="normal" xref="A1.p3.3.m3.1.1.2.7.cmml">_</mi><mo id="A1.p3.3.m3.1.1.2.1e" xref="A1.p3.3.m3.1.1.2.1.cmml">⁢</mo><mi id="A1.p3.3.m3.1.1.2.8" xref="A1.p3.3.m3.1.1.2.8.cmml">t</mi><mo id="A1.p3.3.m3.1.1.2.1f" xref="A1.p3.3.m3.1.1.2.1.cmml">⁢</mo><mi id="A1.p3.3.m3.1.1.2.9" xref="A1.p3.3.m3.1.1.2.9.cmml">o</mi><mo id="A1.p3.3.m3.1.1.2.1g" xref="A1.p3.3.m3.1.1.2.1.cmml">⁢</mo><mi id="A1.p3.3.m3.1.1.2.10" xref="A1.p3.3.m3.1.1.2.10.cmml">k</mi><mo id="A1.p3.3.m3.1.1.2.1h" xref="A1.p3.3.m3.1.1.2.1.cmml">⁢</mo><mi id="A1.p3.3.m3.1.1.2.11" xref="A1.p3.3.m3.1.1.2.11.cmml">e</mi><mo id="A1.p3.3.m3.1.1.2.1i" xref="A1.p3.3.m3.1.1.2.1.cmml">⁢</mo><mi id="A1.p3.3.m3.1.1.2.12" xref="A1.p3.3.m3.1.1.2.12.cmml">n</mi><mo id="A1.p3.3.m3.1.1.2.1j" xref="A1.p3.3.m3.1.1.2.1.cmml">⁢</mo><mi id="A1.p3.3.m3.1.1.2.13" xref="A1.p3.3.m3.1.1.2.13.cmml">s</mi></mrow><mo id="A1.p3.3.m3.1.1.1" xref="A1.p3.3.m3.1.1.1.cmml">−</mo><mn id="A1.p3.3.m3.1.1.3" xref="A1.p3.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p3.3.m3.1b"><apply id="A1.p3.3.m3.1.1.cmml" xref="A1.p3.3.m3.1.1"><minus id="A1.p3.3.m3.1.1.1.cmml" xref="A1.p3.3.m3.1.1.1"></minus><apply id="A1.p3.3.m3.1.1.2.cmml" xref="A1.p3.3.m3.1.1.2"><times id="A1.p3.3.m3.1.1.2.1.cmml" xref="A1.p3.3.m3.1.1.2.1"></times><ci id="A1.p3.3.m3.1.1.2.2.cmml" xref="A1.p3.3.m3.1.1.2.2">𝑡</ci><ci id="A1.p3.3.m3.1.1.2.3.cmml" xref="A1.p3.3.m3.1.1.2.3">𝑜</ci><ci id="A1.p3.3.m3.1.1.2.4.cmml" xref="A1.p3.3.m3.1.1.2.4">𝑡</ci><ci id="A1.p3.3.m3.1.1.2.5.cmml" xref="A1.p3.3.m3.1.1.2.5">𝑎</ci><ci id="A1.p3.3.m3.1.1.2.6.cmml" xref="A1.p3.3.m3.1.1.2.6">𝑙</ci><ci id="A1.p3.3.m3.1.1.2.7.cmml" xref="A1.p3.3.m3.1.1.2.7">_</ci><ci id="A1.p3.3.m3.1.1.2.8.cmml" xref="A1.p3.3.m3.1.1.2.8">𝑡</ci><ci id="A1.p3.3.m3.1.1.2.9.cmml" xref="A1.p3.3.m3.1.1.2.9">𝑜</ci><ci id="A1.p3.3.m3.1.1.2.10.cmml" xref="A1.p3.3.m3.1.1.2.10">𝑘</ci><ci id="A1.p3.3.m3.1.1.2.11.cmml" xref="A1.p3.3.m3.1.1.2.11">𝑒</ci><ci id="A1.p3.3.m3.1.1.2.12.cmml" xref="A1.p3.3.m3.1.1.2.12">𝑛</ci><ci id="A1.p3.3.m3.1.1.2.13.cmml" xref="A1.p3.3.m3.1.1.2.13">𝑠</ci></apply><cn id="A1.p3.3.m3.1.1.3.cmml" type="integer" xref="A1.p3.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p3.3.m3.1c">total\_tokens-1</annotation><annotation encoding="application/x-llamapun" id="A1.p3.3.m3.1d">italic_t italic_o italic_t italic_a italic_l _ italic_t italic_o italic_k italic_e italic_n italic_s - 1</annotation></semantics></math>).</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p4">
<p class="ltx_p" id="A1.p4.2">3. Two parallel processes are initiated:
a) A GPU computation thread that starts from <math alttext="compute\_ptr" class="ltx_Math" display="inline" id="A1.p4.1.m1.1"><semantics id="A1.p4.1.m1.1a"><mrow id="A1.p4.1.m1.1.1" xref="A1.p4.1.m1.1.1.cmml"><mi id="A1.p4.1.m1.1.1.2" xref="A1.p4.1.m1.1.1.2.cmml">c</mi><mo id="A1.p4.1.m1.1.1.1" xref="A1.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p4.1.m1.1.1.3" xref="A1.p4.1.m1.1.1.3.cmml">o</mi><mo id="A1.p4.1.m1.1.1.1a" xref="A1.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p4.1.m1.1.1.4" xref="A1.p4.1.m1.1.1.4.cmml">m</mi><mo id="A1.p4.1.m1.1.1.1b" xref="A1.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p4.1.m1.1.1.5" xref="A1.p4.1.m1.1.1.5.cmml">p</mi><mo id="A1.p4.1.m1.1.1.1c" xref="A1.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p4.1.m1.1.1.6" xref="A1.p4.1.m1.1.1.6.cmml">u</mi><mo id="A1.p4.1.m1.1.1.1d" xref="A1.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p4.1.m1.1.1.7" xref="A1.p4.1.m1.1.1.7.cmml">t</mi><mo id="A1.p4.1.m1.1.1.1e" xref="A1.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p4.1.m1.1.1.8" xref="A1.p4.1.m1.1.1.8.cmml">e</mi><mo id="A1.p4.1.m1.1.1.1f" xref="A1.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p4.1.m1.1.1.9" mathvariant="normal" xref="A1.p4.1.m1.1.1.9.cmml">_</mi><mo id="A1.p4.1.m1.1.1.1g" xref="A1.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p4.1.m1.1.1.10" xref="A1.p4.1.m1.1.1.10.cmml">p</mi><mo id="A1.p4.1.m1.1.1.1h" xref="A1.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p4.1.m1.1.1.11" xref="A1.p4.1.m1.1.1.11.cmml">t</mi><mo id="A1.p4.1.m1.1.1.1i" xref="A1.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p4.1.m1.1.1.12" xref="A1.p4.1.m1.1.1.12.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.p4.1.m1.1b"><apply id="A1.p4.1.m1.1.1.cmml" xref="A1.p4.1.m1.1.1"><times id="A1.p4.1.m1.1.1.1.cmml" xref="A1.p4.1.m1.1.1.1"></times><ci id="A1.p4.1.m1.1.1.2.cmml" xref="A1.p4.1.m1.1.1.2">𝑐</ci><ci id="A1.p4.1.m1.1.1.3.cmml" xref="A1.p4.1.m1.1.1.3">𝑜</ci><ci id="A1.p4.1.m1.1.1.4.cmml" xref="A1.p4.1.m1.1.1.4">𝑚</ci><ci id="A1.p4.1.m1.1.1.5.cmml" xref="A1.p4.1.m1.1.1.5">𝑝</ci><ci id="A1.p4.1.m1.1.1.6.cmml" xref="A1.p4.1.m1.1.1.6">𝑢</ci><ci id="A1.p4.1.m1.1.1.7.cmml" xref="A1.p4.1.m1.1.1.7">𝑡</ci><ci id="A1.p4.1.m1.1.1.8.cmml" xref="A1.p4.1.m1.1.1.8">𝑒</ci><ci id="A1.p4.1.m1.1.1.9.cmml" xref="A1.p4.1.m1.1.1.9">_</ci><ci id="A1.p4.1.m1.1.1.10.cmml" xref="A1.p4.1.m1.1.1.10">𝑝</ci><ci id="A1.p4.1.m1.1.1.11.cmml" xref="A1.p4.1.m1.1.1.11">𝑡</ci><ci id="A1.p4.1.m1.1.1.12.cmml" xref="A1.p4.1.m1.1.1.12">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p4.1.m1.1c">compute\_ptr</annotation><annotation encoding="application/x-llamapun" id="A1.p4.1.m1.1d">italic_c italic_o italic_m italic_p italic_u italic_t italic_e _ italic_p italic_t italic_r</annotation></semantics></math> and moves forward.
b) An I/O streaming thread that starts from <math alttext="io\_ptr" class="ltx_Math" display="inline" id="A1.p4.2.m2.1"><semantics id="A1.p4.2.m2.1a"><mrow id="A1.p4.2.m2.1.1" xref="A1.p4.2.m2.1.1.cmml"><mi id="A1.p4.2.m2.1.1.2" xref="A1.p4.2.m2.1.1.2.cmml">i</mi><mo id="A1.p4.2.m2.1.1.1" xref="A1.p4.2.m2.1.1.1.cmml">⁢</mo><mi id="A1.p4.2.m2.1.1.3" xref="A1.p4.2.m2.1.1.3.cmml">o</mi><mo id="A1.p4.2.m2.1.1.1a" xref="A1.p4.2.m2.1.1.1.cmml">⁢</mo><mi id="A1.p4.2.m2.1.1.4" mathvariant="normal" xref="A1.p4.2.m2.1.1.4.cmml">_</mi><mo id="A1.p4.2.m2.1.1.1b" xref="A1.p4.2.m2.1.1.1.cmml">⁢</mo><mi id="A1.p4.2.m2.1.1.5" xref="A1.p4.2.m2.1.1.5.cmml">p</mi><mo id="A1.p4.2.m2.1.1.1c" xref="A1.p4.2.m2.1.1.1.cmml">⁢</mo><mi id="A1.p4.2.m2.1.1.6" xref="A1.p4.2.m2.1.1.6.cmml">t</mi><mo id="A1.p4.2.m2.1.1.1d" xref="A1.p4.2.m2.1.1.1.cmml">⁢</mo><mi id="A1.p4.2.m2.1.1.7" xref="A1.p4.2.m2.1.1.7.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.p4.2.m2.1b"><apply id="A1.p4.2.m2.1.1.cmml" xref="A1.p4.2.m2.1.1"><times id="A1.p4.2.m2.1.1.1.cmml" xref="A1.p4.2.m2.1.1.1"></times><ci id="A1.p4.2.m2.1.1.2.cmml" xref="A1.p4.2.m2.1.1.2">𝑖</ci><ci id="A1.p4.2.m2.1.1.3.cmml" xref="A1.p4.2.m2.1.1.3">𝑜</ci><ci id="A1.p4.2.m2.1.1.4.cmml" xref="A1.p4.2.m2.1.1.4">_</ci><ci id="A1.p4.2.m2.1.1.5.cmml" xref="A1.p4.2.m2.1.1.5">𝑝</ci><ci id="A1.p4.2.m2.1.1.6.cmml" xref="A1.p4.2.m2.1.1.6">𝑡</ci><ci id="A1.p4.2.m2.1.1.7.cmml" xref="A1.p4.2.m2.1.1.7">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p4.2.m2.1c">io\_ptr</annotation><annotation encoding="application/x-llamapun" id="A1.p4.2.m2.1d">italic_i italic_o _ italic_p italic_t italic_r</annotation></semantics></math> and moves backward.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p5">
<p class="ltx_p" id="A1.p5.4">4. The GPU computation thread:
- Computes <span class="ltx_text" id="A1.p5.4.1">KV cache</span> for chunks starting from <math alttext="compute\_ptr" class="ltx_Math" display="inline" id="A1.p5.1.m1.1"><semantics id="A1.p5.1.m1.1a"><mrow id="A1.p5.1.m1.1.1" xref="A1.p5.1.m1.1.1.cmml"><mi id="A1.p5.1.m1.1.1.2" xref="A1.p5.1.m1.1.1.2.cmml">c</mi><mo id="A1.p5.1.m1.1.1.1" xref="A1.p5.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p5.1.m1.1.1.3" xref="A1.p5.1.m1.1.1.3.cmml">o</mi><mo id="A1.p5.1.m1.1.1.1a" xref="A1.p5.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p5.1.m1.1.1.4" xref="A1.p5.1.m1.1.1.4.cmml">m</mi><mo id="A1.p5.1.m1.1.1.1b" xref="A1.p5.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p5.1.m1.1.1.5" xref="A1.p5.1.m1.1.1.5.cmml">p</mi><mo id="A1.p5.1.m1.1.1.1c" xref="A1.p5.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p5.1.m1.1.1.6" xref="A1.p5.1.m1.1.1.6.cmml">u</mi><mo id="A1.p5.1.m1.1.1.1d" xref="A1.p5.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p5.1.m1.1.1.7" xref="A1.p5.1.m1.1.1.7.cmml">t</mi><mo id="A1.p5.1.m1.1.1.1e" xref="A1.p5.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p5.1.m1.1.1.8" xref="A1.p5.1.m1.1.1.8.cmml">e</mi><mo id="A1.p5.1.m1.1.1.1f" xref="A1.p5.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p5.1.m1.1.1.9" mathvariant="normal" xref="A1.p5.1.m1.1.1.9.cmml">_</mi><mo id="A1.p5.1.m1.1.1.1g" xref="A1.p5.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p5.1.m1.1.1.10" xref="A1.p5.1.m1.1.1.10.cmml">p</mi><mo id="A1.p5.1.m1.1.1.1h" xref="A1.p5.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p5.1.m1.1.1.11" xref="A1.p5.1.m1.1.1.11.cmml">t</mi><mo id="A1.p5.1.m1.1.1.1i" xref="A1.p5.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p5.1.m1.1.1.12" xref="A1.p5.1.m1.1.1.12.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.p5.1.m1.1b"><apply id="A1.p5.1.m1.1.1.cmml" xref="A1.p5.1.m1.1.1"><times id="A1.p5.1.m1.1.1.1.cmml" xref="A1.p5.1.m1.1.1.1"></times><ci id="A1.p5.1.m1.1.1.2.cmml" xref="A1.p5.1.m1.1.1.2">𝑐</ci><ci id="A1.p5.1.m1.1.1.3.cmml" xref="A1.p5.1.m1.1.1.3">𝑜</ci><ci id="A1.p5.1.m1.1.1.4.cmml" xref="A1.p5.1.m1.1.1.4">𝑚</ci><ci id="A1.p5.1.m1.1.1.5.cmml" xref="A1.p5.1.m1.1.1.5">𝑝</ci><ci id="A1.p5.1.m1.1.1.6.cmml" xref="A1.p5.1.m1.1.1.6">𝑢</ci><ci id="A1.p5.1.m1.1.1.7.cmml" xref="A1.p5.1.m1.1.1.7">𝑡</ci><ci id="A1.p5.1.m1.1.1.8.cmml" xref="A1.p5.1.m1.1.1.8">𝑒</ci><ci id="A1.p5.1.m1.1.1.9.cmml" xref="A1.p5.1.m1.1.1.9">_</ci><ci id="A1.p5.1.m1.1.1.10.cmml" xref="A1.p5.1.m1.1.1.10">𝑝</ci><ci id="A1.p5.1.m1.1.1.11.cmml" xref="A1.p5.1.m1.1.1.11">𝑡</ci><ci id="A1.p5.1.m1.1.1.12.cmml" xref="A1.p5.1.m1.1.1.12">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p5.1.m1.1c">compute\_ptr</annotation><annotation encoding="application/x-llamapun" id="A1.p5.1.m1.1d">italic_c italic_o italic_m italic_p italic_u italic_t italic_e _ italic_p italic_t italic_r</annotation></semantics></math>.
- After each chunk computation, it updates <math alttext="compute\_ptr" class="ltx_Math" display="inline" id="A1.p5.2.m2.1"><semantics id="A1.p5.2.m2.1a"><mrow id="A1.p5.2.m2.1.1" xref="A1.p5.2.m2.1.1.cmml"><mi id="A1.p5.2.m2.1.1.2" xref="A1.p5.2.m2.1.1.2.cmml">c</mi><mo id="A1.p5.2.m2.1.1.1" xref="A1.p5.2.m2.1.1.1.cmml">⁢</mo><mi id="A1.p5.2.m2.1.1.3" xref="A1.p5.2.m2.1.1.3.cmml">o</mi><mo id="A1.p5.2.m2.1.1.1a" xref="A1.p5.2.m2.1.1.1.cmml">⁢</mo><mi id="A1.p5.2.m2.1.1.4" xref="A1.p5.2.m2.1.1.4.cmml">m</mi><mo id="A1.p5.2.m2.1.1.1b" xref="A1.p5.2.m2.1.1.1.cmml">⁢</mo><mi id="A1.p5.2.m2.1.1.5" xref="A1.p5.2.m2.1.1.5.cmml">p</mi><mo id="A1.p5.2.m2.1.1.1c" xref="A1.p5.2.m2.1.1.1.cmml">⁢</mo><mi id="A1.p5.2.m2.1.1.6" xref="A1.p5.2.m2.1.1.6.cmml">u</mi><mo id="A1.p5.2.m2.1.1.1d" xref="A1.p5.2.m2.1.1.1.cmml">⁢</mo><mi id="A1.p5.2.m2.1.1.7" xref="A1.p5.2.m2.1.1.7.cmml">t</mi><mo id="A1.p5.2.m2.1.1.1e" xref="A1.p5.2.m2.1.1.1.cmml">⁢</mo><mi id="A1.p5.2.m2.1.1.8" xref="A1.p5.2.m2.1.1.8.cmml">e</mi><mo id="A1.p5.2.m2.1.1.1f" xref="A1.p5.2.m2.1.1.1.cmml">⁢</mo><mi id="A1.p5.2.m2.1.1.9" mathvariant="normal" xref="A1.p5.2.m2.1.1.9.cmml">_</mi><mo id="A1.p5.2.m2.1.1.1g" xref="A1.p5.2.m2.1.1.1.cmml">⁢</mo><mi id="A1.p5.2.m2.1.1.10" xref="A1.p5.2.m2.1.1.10.cmml">p</mi><mo id="A1.p5.2.m2.1.1.1h" xref="A1.p5.2.m2.1.1.1.cmml">⁢</mo><mi id="A1.p5.2.m2.1.1.11" xref="A1.p5.2.m2.1.1.11.cmml">t</mi><mo id="A1.p5.2.m2.1.1.1i" xref="A1.p5.2.m2.1.1.1.cmml">⁢</mo><mi id="A1.p5.2.m2.1.1.12" xref="A1.p5.2.m2.1.1.12.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.p5.2.m2.1b"><apply id="A1.p5.2.m2.1.1.cmml" xref="A1.p5.2.m2.1.1"><times id="A1.p5.2.m2.1.1.1.cmml" xref="A1.p5.2.m2.1.1.1"></times><ci id="A1.p5.2.m2.1.1.2.cmml" xref="A1.p5.2.m2.1.1.2">𝑐</ci><ci id="A1.p5.2.m2.1.1.3.cmml" xref="A1.p5.2.m2.1.1.3">𝑜</ci><ci id="A1.p5.2.m2.1.1.4.cmml" xref="A1.p5.2.m2.1.1.4">𝑚</ci><ci id="A1.p5.2.m2.1.1.5.cmml" xref="A1.p5.2.m2.1.1.5">𝑝</ci><ci id="A1.p5.2.m2.1.1.6.cmml" xref="A1.p5.2.m2.1.1.6">𝑢</ci><ci id="A1.p5.2.m2.1.1.7.cmml" xref="A1.p5.2.m2.1.1.7">𝑡</ci><ci id="A1.p5.2.m2.1.1.8.cmml" xref="A1.p5.2.m2.1.1.8">𝑒</ci><ci id="A1.p5.2.m2.1.1.9.cmml" xref="A1.p5.2.m2.1.1.9">_</ci><ci id="A1.p5.2.m2.1.1.10.cmml" xref="A1.p5.2.m2.1.1.10">𝑝</ci><ci id="A1.p5.2.m2.1.1.11.cmml" xref="A1.p5.2.m2.1.1.11">𝑡</ci><ci id="A1.p5.2.m2.1.1.12.cmml" xref="A1.p5.2.m2.1.1.12">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p5.2.m2.1c">compute\_ptr</annotation><annotation encoding="application/x-llamapun" id="A1.p5.2.m2.1d">italic_c italic_o italic_m italic_p italic_u italic_t italic_e _ italic_p italic_t italic_r</annotation></semantics></math> by adding the chunk size.
- Continues until <math alttext="compute\_ptr" class="ltx_Math" display="inline" id="A1.p5.3.m3.1"><semantics id="A1.p5.3.m3.1a"><mrow id="A1.p5.3.m3.1.1" xref="A1.p5.3.m3.1.1.cmml"><mi id="A1.p5.3.m3.1.1.2" xref="A1.p5.3.m3.1.1.2.cmml">c</mi><mo id="A1.p5.3.m3.1.1.1" xref="A1.p5.3.m3.1.1.1.cmml">⁢</mo><mi id="A1.p5.3.m3.1.1.3" xref="A1.p5.3.m3.1.1.3.cmml">o</mi><mo id="A1.p5.3.m3.1.1.1a" xref="A1.p5.3.m3.1.1.1.cmml">⁢</mo><mi id="A1.p5.3.m3.1.1.4" xref="A1.p5.3.m3.1.1.4.cmml">m</mi><mo id="A1.p5.3.m3.1.1.1b" xref="A1.p5.3.m3.1.1.1.cmml">⁢</mo><mi id="A1.p5.3.m3.1.1.5" xref="A1.p5.3.m3.1.1.5.cmml">p</mi><mo id="A1.p5.3.m3.1.1.1c" xref="A1.p5.3.m3.1.1.1.cmml">⁢</mo><mi id="A1.p5.3.m3.1.1.6" xref="A1.p5.3.m3.1.1.6.cmml">u</mi><mo id="A1.p5.3.m3.1.1.1d" xref="A1.p5.3.m3.1.1.1.cmml">⁢</mo><mi id="A1.p5.3.m3.1.1.7" xref="A1.p5.3.m3.1.1.7.cmml">t</mi><mo id="A1.p5.3.m3.1.1.1e" xref="A1.p5.3.m3.1.1.1.cmml">⁢</mo><mi id="A1.p5.3.m3.1.1.8" xref="A1.p5.3.m3.1.1.8.cmml">e</mi><mo id="A1.p5.3.m3.1.1.1f" xref="A1.p5.3.m3.1.1.1.cmml">⁢</mo><mi id="A1.p5.3.m3.1.1.9" mathvariant="normal" xref="A1.p5.3.m3.1.1.9.cmml">_</mi><mo id="A1.p5.3.m3.1.1.1g" xref="A1.p5.3.m3.1.1.1.cmml">⁢</mo><mi id="A1.p5.3.m3.1.1.10" xref="A1.p5.3.m3.1.1.10.cmml">p</mi><mo id="A1.p5.3.m3.1.1.1h" xref="A1.p5.3.m3.1.1.1.cmml">⁢</mo><mi id="A1.p5.3.m3.1.1.11" xref="A1.p5.3.m3.1.1.11.cmml">t</mi><mo id="A1.p5.3.m3.1.1.1i" xref="A1.p5.3.m3.1.1.1.cmml">⁢</mo><mi id="A1.p5.3.m3.1.1.12" xref="A1.p5.3.m3.1.1.12.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.p5.3.m3.1b"><apply id="A1.p5.3.m3.1.1.cmml" xref="A1.p5.3.m3.1.1"><times id="A1.p5.3.m3.1.1.1.cmml" xref="A1.p5.3.m3.1.1.1"></times><ci id="A1.p5.3.m3.1.1.2.cmml" xref="A1.p5.3.m3.1.1.2">𝑐</ci><ci id="A1.p5.3.m3.1.1.3.cmml" xref="A1.p5.3.m3.1.1.3">𝑜</ci><ci id="A1.p5.3.m3.1.1.4.cmml" xref="A1.p5.3.m3.1.1.4">𝑚</ci><ci id="A1.p5.3.m3.1.1.5.cmml" xref="A1.p5.3.m3.1.1.5">𝑝</ci><ci id="A1.p5.3.m3.1.1.6.cmml" xref="A1.p5.3.m3.1.1.6">𝑢</ci><ci id="A1.p5.3.m3.1.1.7.cmml" xref="A1.p5.3.m3.1.1.7">𝑡</ci><ci id="A1.p5.3.m3.1.1.8.cmml" xref="A1.p5.3.m3.1.1.8">𝑒</ci><ci id="A1.p5.3.m3.1.1.9.cmml" xref="A1.p5.3.m3.1.1.9">_</ci><ci id="A1.p5.3.m3.1.1.10.cmml" xref="A1.p5.3.m3.1.1.10">𝑝</ci><ci id="A1.p5.3.m3.1.1.11.cmml" xref="A1.p5.3.m3.1.1.11">𝑡</ci><ci id="A1.p5.3.m3.1.1.12.cmml" xref="A1.p5.3.m3.1.1.12">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p5.3.m3.1c">compute\_ptr</annotation><annotation encoding="application/x-llamapun" id="A1.p5.3.m3.1d">italic_c italic_o italic_m italic_p italic_u italic_t italic_e _ italic_p italic_t italic_r</annotation></semantics></math> reaches or surpasses <math alttext="io\_ptr" class="ltx_Math" display="inline" id="A1.p5.4.m4.1"><semantics id="A1.p5.4.m4.1a"><mrow id="A1.p5.4.m4.1.1" xref="A1.p5.4.m4.1.1.cmml"><mi id="A1.p5.4.m4.1.1.2" xref="A1.p5.4.m4.1.1.2.cmml">i</mi><mo id="A1.p5.4.m4.1.1.1" xref="A1.p5.4.m4.1.1.1.cmml">⁢</mo><mi id="A1.p5.4.m4.1.1.3" xref="A1.p5.4.m4.1.1.3.cmml">o</mi><mo id="A1.p5.4.m4.1.1.1a" xref="A1.p5.4.m4.1.1.1.cmml">⁢</mo><mi id="A1.p5.4.m4.1.1.4" mathvariant="normal" xref="A1.p5.4.m4.1.1.4.cmml">_</mi><mo id="A1.p5.4.m4.1.1.1b" xref="A1.p5.4.m4.1.1.1.cmml">⁢</mo><mi id="A1.p5.4.m4.1.1.5" xref="A1.p5.4.m4.1.1.5.cmml">p</mi><mo id="A1.p5.4.m4.1.1.1c" xref="A1.p5.4.m4.1.1.1.cmml">⁢</mo><mi id="A1.p5.4.m4.1.1.6" xref="A1.p5.4.m4.1.1.6.cmml">t</mi><mo id="A1.p5.4.m4.1.1.1d" xref="A1.p5.4.m4.1.1.1.cmml">⁢</mo><mi id="A1.p5.4.m4.1.1.7" xref="A1.p5.4.m4.1.1.7.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.p5.4.m4.1b"><apply id="A1.p5.4.m4.1.1.cmml" xref="A1.p5.4.m4.1.1"><times id="A1.p5.4.m4.1.1.1.cmml" xref="A1.p5.4.m4.1.1.1"></times><ci id="A1.p5.4.m4.1.1.2.cmml" xref="A1.p5.4.m4.1.1.2">𝑖</ci><ci id="A1.p5.4.m4.1.1.3.cmml" xref="A1.p5.4.m4.1.1.3">𝑜</ci><ci id="A1.p5.4.m4.1.1.4.cmml" xref="A1.p5.4.m4.1.1.4">_</ci><ci id="A1.p5.4.m4.1.1.5.cmml" xref="A1.p5.4.m4.1.1.5">𝑝</ci><ci id="A1.p5.4.m4.1.1.6.cmml" xref="A1.p5.4.m4.1.1.6">𝑡</ci><ci id="A1.p5.4.m4.1.1.7.cmml" xref="A1.p5.4.m4.1.1.7">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p5.4.m4.1c">io\_ptr</annotation><annotation encoding="application/x-llamapun" id="A1.p5.4.m4.1d">italic_i italic_o _ italic_p italic_t italic_r</annotation></semantics></math>, or until the required <span class="ltx_text" id="A1.p5.4.2">KV cache</span> is found in CPU memory.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p6">
<p class="ltx_p" id="A1.p6.4">5. The I/O streaming thread:
- Fetches pre-computed <span class="ltx_text" id="A1.p6.4.1">KV cache</span> for chunks ending at <math alttext="io\_ptr" class="ltx_Math" display="inline" id="A1.p6.1.m1.1"><semantics id="A1.p6.1.m1.1a"><mrow id="A1.p6.1.m1.1.1" xref="A1.p6.1.m1.1.1.cmml"><mi id="A1.p6.1.m1.1.1.2" xref="A1.p6.1.m1.1.1.2.cmml">i</mi><mo id="A1.p6.1.m1.1.1.1" xref="A1.p6.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p6.1.m1.1.1.3" xref="A1.p6.1.m1.1.1.3.cmml">o</mi><mo id="A1.p6.1.m1.1.1.1a" xref="A1.p6.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p6.1.m1.1.1.4" mathvariant="normal" xref="A1.p6.1.m1.1.1.4.cmml">_</mi><mo id="A1.p6.1.m1.1.1.1b" xref="A1.p6.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p6.1.m1.1.1.5" xref="A1.p6.1.m1.1.1.5.cmml">p</mi><mo id="A1.p6.1.m1.1.1.1c" xref="A1.p6.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p6.1.m1.1.1.6" xref="A1.p6.1.m1.1.1.6.cmml">t</mi><mo id="A1.p6.1.m1.1.1.1d" xref="A1.p6.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.p6.1.m1.1.1.7" xref="A1.p6.1.m1.1.1.7.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.p6.1.m1.1b"><apply id="A1.p6.1.m1.1.1.cmml" xref="A1.p6.1.m1.1.1"><times id="A1.p6.1.m1.1.1.1.cmml" xref="A1.p6.1.m1.1.1.1"></times><ci id="A1.p6.1.m1.1.1.2.cmml" xref="A1.p6.1.m1.1.1.2">𝑖</ci><ci id="A1.p6.1.m1.1.1.3.cmml" xref="A1.p6.1.m1.1.1.3">𝑜</ci><ci id="A1.p6.1.m1.1.1.4.cmml" xref="A1.p6.1.m1.1.1.4">_</ci><ci id="A1.p6.1.m1.1.1.5.cmml" xref="A1.p6.1.m1.1.1.5">𝑝</ci><ci id="A1.p6.1.m1.1.1.6.cmml" xref="A1.p6.1.m1.1.1.6">𝑡</ci><ci id="A1.p6.1.m1.1.1.7.cmml" xref="A1.p6.1.m1.1.1.7">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p6.1.m1.1c">io\_ptr</annotation><annotation encoding="application/x-llamapun" id="A1.p6.1.m1.1d">italic_i italic_o _ italic_p italic_t italic_r</annotation></semantics></math> from storage (local or remote) to CPU memory.
- After each chunk fetch, it updates <math alttext="io\_ptr" class="ltx_Math" display="inline" id="A1.p6.2.m2.1"><semantics id="A1.p6.2.m2.1a"><mrow id="A1.p6.2.m2.1.1" xref="A1.p6.2.m2.1.1.cmml"><mi id="A1.p6.2.m2.1.1.2" xref="A1.p6.2.m2.1.1.2.cmml">i</mi><mo id="A1.p6.2.m2.1.1.1" xref="A1.p6.2.m2.1.1.1.cmml">⁢</mo><mi id="A1.p6.2.m2.1.1.3" xref="A1.p6.2.m2.1.1.3.cmml">o</mi><mo id="A1.p6.2.m2.1.1.1a" xref="A1.p6.2.m2.1.1.1.cmml">⁢</mo><mi id="A1.p6.2.m2.1.1.4" mathvariant="normal" xref="A1.p6.2.m2.1.1.4.cmml">_</mi><mo id="A1.p6.2.m2.1.1.1b" xref="A1.p6.2.m2.1.1.1.cmml">⁢</mo><mi id="A1.p6.2.m2.1.1.5" xref="A1.p6.2.m2.1.1.5.cmml">p</mi><mo id="A1.p6.2.m2.1.1.1c" xref="A1.p6.2.m2.1.1.1.cmml">⁢</mo><mi id="A1.p6.2.m2.1.1.6" xref="A1.p6.2.m2.1.1.6.cmml">t</mi><mo id="A1.p6.2.m2.1.1.1d" xref="A1.p6.2.m2.1.1.1.cmml">⁢</mo><mi id="A1.p6.2.m2.1.1.7" xref="A1.p6.2.m2.1.1.7.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.p6.2.m2.1b"><apply id="A1.p6.2.m2.1.1.cmml" xref="A1.p6.2.m2.1.1"><times id="A1.p6.2.m2.1.1.1.cmml" xref="A1.p6.2.m2.1.1.1"></times><ci id="A1.p6.2.m2.1.1.2.cmml" xref="A1.p6.2.m2.1.1.2">𝑖</ci><ci id="A1.p6.2.m2.1.1.3.cmml" xref="A1.p6.2.m2.1.1.3">𝑜</ci><ci id="A1.p6.2.m2.1.1.4.cmml" xref="A1.p6.2.m2.1.1.4">_</ci><ci id="A1.p6.2.m2.1.1.5.cmml" xref="A1.p6.2.m2.1.1.5">𝑝</ci><ci id="A1.p6.2.m2.1.1.6.cmml" xref="A1.p6.2.m2.1.1.6">𝑡</ci><ci id="A1.p6.2.m2.1.1.7.cmml" xref="A1.p6.2.m2.1.1.7">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p6.2.m2.1c">io\_ptr</annotation><annotation encoding="application/x-llamapun" id="A1.p6.2.m2.1d">italic_i italic_o _ italic_p italic_t italic_r</annotation></semantics></math> by subtracting the chunk size.
- Continues until <math alttext="io\_ptr" class="ltx_Math" display="inline" id="A1.p6.3.m3.1"><semantics id="A1.p6.3.m3.1a"><mrow id="A1.p6.3.m3.1.1" xref="A1.p6.3.m3.1.1.cmml"><mi id="A1.p6.3.m3.1.1.2" xref="A1.p6.3.m3.1.1.2.cmml">i</mi><mo id="A1.p6.3.m3.1.1.1" xref="A1.p6.3.m3.1.1.1.cmml">⁢</mo><mi id="A1.p6.3.m3.1.1.3" xref="A1.p6.3.m3.1.1.3.cmml">o</mi><mo id="A1.p6.3.m3.1.1.1a" xref="A1.p6.3.m3.1.1.1.cmml">⁢</mo><mi id="A1.p6.3.m3.1.1.4" mathvariant="normal" xref="A1.p6.3.m3.1.1.4.cmml">_</mi><mo id="A1.p6.3.m3.1.1.1b" xref="A1.p6.3.m3.1.1.1.cmml">⁢</mo><mi id="A1.p6.3.m3.1.1.5" xref="A1.p6.3.m3.1.1.5.cmml">p</mi><mo id="A1.p6.3.m3.1.1.1c" xref="A1.p6.3.m3.1.1.1.cmml">⁢</mo><mi id="A1.p6.3.m3.1.1.6" xref="A1.p6.3.m3.1.1.6.cmml">t</mi><mo id="A1.p6.3.m3.1.1.1d" xref="A1.p6.3.m3.1.1.1.cmml">⁢</mo><mi id="A1.p6.3.m3.1.1.7" xref="A1.p6.3.m3.1.1.7.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.p6.3.m3.1b"><apply id="A1.p6.3.m3.1.1.cmml" xref="A1.p6.3.m3.1.1"><times id="A1.p6.3.m3.1.1.1.cmml" xref="A1.p6.3.m3.1.1.1"></times><ci id="A1.p6.3.m3.1.1.2.cmml" xref="A1.p6.3.m3.1.1.2">𝑖</ci><ci id="A1.p6.3.m3.1.1.3.cmml" xref="A1.p6.3.m3.1.1.3">𝑜</ci><ci id="A1.p6.3.m3.1.1.4.cmml" xref="A1.p6.3.m3.1.1.4">_</ci><ci id="A1.p6.3.m3.1.1.5.cmml" xref="A1.p6.3.m3.1.1.5">𝑝</ci><ci id="A1.p6.3.m3.1.1.6.cmml" xref="A1.p6.3.m3.1.1.6">𝑡</ci><ci id="A1.p6.3.m3.1.1.7.cmml" xref="A1.p6.3.m3.1.1.7">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p6.3.m3.1c">io\_ptr</annotation><annotation encoding="application/x-llamapun" id="A1.p6.3.m3.1d">italic_i italic_o _ italic_p italic_t italic_r</annotation></semantics></math> reaches or goes below <math alttext="compute\_ptr" class="ltx_Math" display="inline" id="A1.p6.4.m4.1"><semantics id="A1.p6.4.m4.1a"><mrow id="A1.p6.4.m4.1.1" xref="A1.p6.4.m4.1.1.cmml"><mi id="A1.p6.4.m4.1.1.2" xref="A1.p6.4.m4.1.1.2.cmml">c</mi><mo id="A1.p6.4.m4.1.1.1" xref="A1.p6.4.m4.1.1.1.cmml">⁢</mo><mi id="A1.p6.4.m4.1.1.3" xref="A1.p6.4.m4.1.1.3.cmml">o</mi><mo id="A1.p6.4.m4.1.1.1a" xref="A1.p6.4.m4.1.1.1.cmml">⁢</mo><mi id="A1.p6.4.m4.1.1.4" xref="A1.p6.4.m4.1.1.4.cmml">m</mi><mo id="A1.p6.4.m4.1.1.1b" xref="A1.p6.4.m4.1.1.1.cmml">⁢</mo><mi id="A1.p6.4.m4.1.1.5" xref="A1.p6.4.m4.1.1.5.cmml">p</mi><mo id="A1.p6.4.m4.1.1.1c" xref="A1.p6.4.m4.1.1.1.cmml">⁢</mo><mi id="A1.p6.4.m4.1.1.6" xref="A1.p6.4.m4.1.1.6.cmml">u</mi><mo id="A1.p6.4.m4.1.1.1d" xref="A1.p6.4.m4.1.1.1.cmml">⁢</mo><mi id="A1.p6.4.m4.1.1.7" xref="A1.p6.4.m4.1.1.7.cmml">t</mi><mo id="A1.p6.4.m4.1.1.1e" xref="A1.p6.4.m4.1.1.1.cmml">⁢</mo><mi id="A1.p6.4.m4.1.1.8" xref="A1.p6.4.m4.1.1.8.cmml">e</mi><mo id="A1.p6.4.m4.1.1.1f" xref="A1.p6.4.m4.1.1.1.cmml">⁢</mo><mi id="A1.p6.4.m4.1.1.9" mathvariant="normal" xref="A1.p6.4.m4.1.1.9.cmml">_</mi><mo id="A1.p6.4.m4.1.1.1g" xref="A1.p6.4.m4.1.1.1.cmml">⁢</mo><mi id="A1.p6.4.m4.1.1.10" xref="A1.p6.4.m4.1.1.10.cmml">p</mi><mo id="A1.p6.4.m4.1.1.1h" xref="A1.p6.4.m4.1.1.1.cmml">⁢</mo><mi id="A1.p6.4.m4.1.1.11" xref="A1.p6.4.m4.1.1.11.cmml">t</mi><mo id="A1.p6.4.m4.1.1.1i" xref="A1.p6.4.m4.1.1.1.cmml">⁢</mo><mi id="A1.p6.4.m4.1.1.12" xref="A1.p6.4.m4.1.1.12.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.p6.4.m4.1b"><apply id="A1.p6.4.m4.1.1.cmml" xref="A1.p6.4.m4.1.1"><times id="A1.p6.4.m4.1.1.1.cmml" xref="A1.p6.4.m4.1.1.1"></times><ci id="A1.p6.4.m4.1.1.2.cmml" xref="A1.p6.4.m4.1.1.2">𝑐</ci><ci id="A1.p6.4.m4.1.1.3.cmml" xref="A1.p6.4.m4.1.1.3">𝑜</ci><ci id="A1.p6.4.m4.1.1.4.cmml" xref="A1.p6.4.m4.1.1.4">𝑚</ci><ci id="A1.p6.4.m4.1.1.5.cmml" xref="A1.p6.4.m4.1.1.5">𝑝</ci><ci id="A1.p6.4.m4.1.1.6.cmml" xref="A1.p6.4.m4.1.1.6">𝑢</ci><ci id="A1.p6.4.m4.1.1.7.cmml" xref="A1.p6.4.m4.1.1.7">𝑡</ci><ci id="A1.p6.4.m4.1.1.8.cmml" xref="A1.p6.4.m4.1.1.8">𝑒</ci><ci id="A1.p6.4.m4.1.1.9.cmml" xref="A1.p6.4.m4.1.1.9">_</ci><ci id="A1.p6.4.m4.1.1.10.cmml" xref="A1.p6.4.m4.1.1.10">𝑝</ci><ci id="A1.p6.4.m4.1.1.11.cmml" xref="A1.p6.4.m4.1.1.11">𝑡</ci><ci id="A1.p6.4.m4.1.1.12.cmml" xref="A1.p6.4.m4.1.1.12">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p6.4.m4.1c">compute\_ptr</annotation><annotation encoding="application/x-llamapun" id="A1.p6.4.m4.1d">italic_c italic_o italic_m italic_p italic_u italic_t italic_e _ italic_p italic_t italic_r</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p7">
<p class="ltx_p" id="A1.p7.1">6. The process concludes when the two pointers meet or cross each other, indicating that <span class="ltx_text" id="A1.p7.1.1">KV cache</span> for the entire sequence has been either computed or loaded.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p8">
<p class="ltx_p" id="A1.p8.1">7. Finally, <span class="ltx_text ltx_font_italic" id="A1.p8.1.1">Cake</span> returns the complete <span class="ltx_text" id="A1.p8.1.2">KV cache</span> for the entire sequence, ready for use in the subsequent inference steps.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p9">
<p class="ltx_p" id="A1.p9.1">This bidirectional approach allows <span class="ltx_text ltx_font_italic" id="A1.p9.1.1">Cake</span> to efficiently utilize both computational and I/O resources simultaneously, minimizing idle time and optimizing the overall latency of <span class="ltx_text" id="A1.p9.1.2">KV cache</span> preparation for long-context LLM inference.</p>
</div>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="alg1.4.1.1">Algorithm 1</span> </span> <span class="ltx_text ltx_font_italic" id="alg1.5.2">Cake</span> Bidirectional <span class="ltx_text" id="alg1.6.3">KV cache</span> Loading Algorithm</figcaption>
<div class="ltx_listing ltx_listing" id="alg1.7">
<div class="ltx_listingline" id="alg1.l1">
<span class="ltx_tag ltx_tag_listingline">1:</span><span class="ltx_text ltx_font_bold" id="alg1.l1.1">procedure</span> <span class="ltx_text ltx_font_smallcaps" id="alg1.l1.2">ComputeKV</span>
</div>
<div class="ltx_listingline" id="alg1.l2">
<span class="ltx_tag ltx_tag_listingline">2:</span>     <span class="ltx_text ltx_font_bold" id="alg1.l2.1">while</span> <math alttext="compute\_ptr&lt;io\_ptr" class="ltx_Math" display="inline" id="alg1.l2.m1.1"><semantics id="alg1.l2.m1.1a"><mrow id="alg1.l2.m1.1.1" xref="alg1.l2.m1.1.1.cmml"><mrow id="alg1.l2.m1.1.1.2" xref="alg1.l2.m1.1.1.2.cmml"><mi id="alg1.l2.m1.1.1.2.2" xref="alg1.l2.m1.1.1.2.2.cmml">c</mi><mo id="alg1.l2.m1.1.1.2.1" xref="alg1.l2.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l2.m1.1.1.2.3" xref="alg1.l2.m1.1.1.2.3.cmml">o</mi><mo id="alg1.l2.m1.1.1.2.1a" xref="alg1.l2.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l2.m1.1.1.2.4" xref="alg1.l2.m1.1.1.2.4.cmml">m</mi><mo id="alg1.l2.m1.1.1.2.1b" xref="alg1.l2.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l2.m1.1.1.2.5" xref="alg1.l2.m1.1.1.2.5.cmml">p</mi><mo id="alg1.l2.m1.1.1.2.1c" xref="alg1.l2.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l2.m1.1.1.2.6" xref="alg1.l2.m1.1.1.2.6.cmml">u</mi><mo id="alg1.l2.m1.1.1.2.1d" xref="alg1.l2.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l2.m1.1.1.2.7" xref="alg1.l2.m1.1.1.2.7.cmml">t</mi><mo id="alg1.l2.m1.1.1.2.1e" xref="alg1.l2.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l2.m1.1.1.2.8" xref="alg1.l2.m1.1.1.2.8.cmml">e</mi><mo id="alg1.l2.m1.1.1.2.1f" xref="alg1.l2.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l2.m1.1.1.2.9" mathvariant="normal" xref="alg1.l2.m1.1.1.2.9.cmml">_</mi><mo id="alg1.l2.m1.1.1.2.1g" xref="alg1.l2.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l2.m1.1.1.2.10" xref="alg1.l2.m1.1.1.2.10.cmml">p</mi><mo id="alg1.l2.m1.1.1.2.1h" xref="alg1.l2.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l2.m1.1.1.2.11" xref="alg1.l2.m1.1.1.2.11.cmml">t</mi><mo id="alg1.l2.m1.1.1.2.1i" xref="alg1.l2.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l2.m1.1.1.2.12" xref="alg1.l2.m1.1.1.2.12.cmml">r</mi></mrow><mo id="alg1.l2.m1.1.1.1" xref="alg1.l2.m1.1.1.1.cmml">&lt;</mo><mrow id="alg1.l2.m1.1.1.3" xref="alg1.l2.m1.1.1.3.cmml"><mi id="alg1.l2.m1.1.1.3.2" xref="alg1.l2.m1.1.1.3.2.cmml">i</mi><mo id="alg1.l2.m1.1.1.3.1" xref="alg1.l2.m1.1.1.3.1.cmml">⁢</mo><mi id="alg1.l2.m1.1.1.3.3" xref="alg1.l2.m1.1.1.3.3.cmml">o</mi><mo id="alg1.l2.m1.1.1.3.1a" xref="alg1.l2.m1.1.1.3.1.cmml">⁢</mo><mi id="alg1.l2.m1.1.1.3.4" mathvariant="normal" xref="alg1.l2.m1.1.1.3.4.cmml">_</mi><mo id="alg1.l2.m1.1.1.3.1b" xref="alg1.l2.m1.1.1.3.1.cmml">⁢</mo><mi id="alg1.l2.m1.1.1.3.5" xref="alg1.l2.m1.1.1.3.5.cmml">p</mi><mo id="alg1.l2.m1.1.1.3.1c" xref="alg1.l2.m1.1.1.3.1.cmml">⁢</mo><mi id="alg1.l2.m1.1.1.3.6" xref="alg1.l2.m1.1.1.3.6.cmml">t</mi><mo id="alg1.l2.m1.1.1.3.1d" xref="alg1.l2.m1.1.1.3.1.cmml">⁢</mo><mi id="alg1.l2.m1.1.1.3.7" xref="alg1.l2.m1.1.1.3.7.cmml">r</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l2.m1.1b"><apply id="alg1.l2.m1.1.1.cmml" xref="alg1.l2.m1.1.1"><lt id="alg1.l2.m1.1.1.1.cmml" xref="alg1.l2.m1.1.1.1"></lt><apply id="alg1.l2.m1.1.1.2.cmml" xref="alg1.l2.m1.1.1.2"><times id="alg1.l2.m1.1.1.2.1.cmml" xref="alg1.l2.m1.1.1.2.1"></times><ci id="alg1.l2.m1.1.1.2.2.cmml" xref="alg1.l2.m1.1.1.2.2">𝑐</ci><ci id="alg1.l2.m1.1.1.2.3.cmml" xref="alg1.l2.m1.1.1.2.3">𝑜</ci><ci id="alg1.l2.m1.1.1.2.4.cmml" xref="alg1.l2.m1.1.1.2.4">𝑚</ci><ci id="alg1.l2.m1.1.1.2.5.cmml" xref="alg1.l2.m1.1.1.2.5">𝑝</ci><ci id="alg1.l2.m1.1.1.2.6.cmml" xref="alg1.l2.m1.1.1.2.6">𝑢</ci><ci id="alg1.l2.m1.1.1.2.7.cmml" xref="alg1.l2.m1.1.1.2.7">𝑡</ci><ci id="alg1.l2.m1.1.1.2.8.cmml" xref="alg1.l2.m1.1.1.2.8">𝑒</ci><ci id="alg1.l2.m1.1.1.2.9.cmml" xref="alg1.l2.m1.1.1.2.9">_</ci><ci id="alg1.l2.m1.1.1.2.10.cmml" xref="alg1.l2.m1.1.1.2.10">𝑝</ci><ci id="alg1.l2.m1.1.1.2.11.cmml" xref="alg1.l2.m1.1.1.2.11">𝑡</ci><ci id="alg1.l2.m1.1.1.2.12.cmml" xref="alg1.l2.m1.1.1.2.12">𝑟</ci></apply><apply id="alg1.l2.m1.1.1.3.cmml" xref="alg1.l2.m1.1.1.3"><times id="alg1.l2.m1.1.1.3.1.cmml" xref="alg1.l2.m1.1.1.3.1"></times><ci id="alg1.l2.m1.1.1.3.2.cmml" xref="alg1.l2.m1.1.1.3.2">𝑖</ci><ci id="alg1.l2.m1.1.1.3.3.cmml" xref="alg1.l2.m1.1.1.3.3">𝑜</ci><ci id="alg1.l2.m1.1.1.3.4.cmml" xref="alg1.l2.m1.1.1.3.4">_</ci><ci id="alg1.l2.m1.1.1.3.5.cmml" xref="alg1.l2.m1.1.1.3.5">𝑝</ci><ci id="alg1.l2.m1.1.1.3.6.cmml" xref="alg1.l2.m1.1.1.3.6">𝑡</ci><ci id="alg1.l2.m1.1.1.3.7.cmml" xref="alg1.l2.m1.1.1.3.7">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m1.1c">compute\_ptr&lt;io\_ptr</annotation><annotation encoding="application/x-llamapun" id="alg1.l2.m1.1d">italic_c italic_o italic_m italic_p italic_u italic_t italic_e _ italic_p italic_t italic_r &lt; italic_i italic_o _ italic_p italic_t italic_r</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="alg1.l2.2">do</span>
</div>
<div class="ltx_listingline" id="alg1.l3">
<span class="ltx_tag ltx_tag_listingline">3:</span>         <span class="ltx_text ltx_font_bold" id="alg1.l3.1">if</span> <span class="ltx_text ltx_font_smallcaps" id="alg1.l3.2">IsInCPUMemory</span>(<math alttext="compute\_ptr" class="ltx_Math" display="inline" id="alg1.l3.m1.1"><semantics id="alg1.l3.m1.1a"><mrow id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml"><mi id="alg1.l3.m1.1.1.2" xref="alg1.l3.m1.1.1.2.cmml">c</mi><mo id="alg1.l3.m1.1.1.1" xref="alg1.l3.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l3.m1.1.1.3" xref="alg1.l3.m1.1.1.3.cmml">o</mi><mo id="alg1.l3.m1.1.1.1a" xref="alg1.l3.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l3.m1.1.1.4" xref="alg1.l3.m1.1.1.4.cmml">m</mi><mo id="alg1.l3.m1.1.1.1b" xref="alg1.l3.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l3.m1.1.1.5" xref="alg1.l3.m1.1.1.5.cmml">p</mi><mo id="alg1.l3.m1.1.1.1c" xref="alg1.l3.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l3.m1.1.1.6" xref="alg1.l3.m1.1.1.6.cmml">u</mi><mo id="alg1.l3.m1.1.1.1d" xref="alg1.l3.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l3.m1.1.1.7" xref="alg1.l3.m1.1.1.7.cmml">t</mi><mo id="alg1.l3.m1.1.1.1e" xref="alg1.l3.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l3.m1.1.1.8" xref="alg1.l3.m1.1.1.8.cmml">e</mi><mo id="alg1.l3.m1.1.1.1f" xref="alg1.l3.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l3.m1.1.1.9" mathvariant="normal" xref="alg1.l3.m1.1.1.9.cmml">_</mi><mo id="alg1.l3.m1.1.1.1g" xref="alg1.l3.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l3.m1.1.1.10" xref="alg1.l3.m1.1.1.10.cmml">p</mi><mo id="alg1.l3.m1.1.1.1h" xref="alg1.l3.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l3.m1.1.1.11" xref="alg1.l3.m1.1.1.11.cmml">t</mi><mo id="alg1.l3.m1.1.1.1i" xref="alg1.l3.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l3.m1.1.1.12" xref="alg1.l3.m1.1.1.12.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><apply id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1"><times id="alg1.l3.m1.1.1.1.cmml" xref="alg1.l3.m1.1.1.1"></times><ci id="alg1.l3.m1.1.1.2.cmml" xref="alg1.l3.m1.1.1.2">𝑐</ci><ci id="alg1.l3.m1.1.1.3.cmml" xref="alg1.l3.m1.1.1.3">𝑜</ci><ci id="alg1.l3.m1.1.1.4.cmml" xref="alg1.l3.m1.1.1.4">𝑚</ci><ci id="alg1.l3.m1.1.1.5.cmml" xref="alg1.l3.m1.1.1.5">𝑝</ci><ci id="alg1.l3.m1.1.1.6.cmml" xref="alg1.l3.m1.1.1.6">𝑢</ci><ci id="alg1.l3.m1.1.1.7.cmml" xref="alg1.l3.m1.1.1.7">𝑡</ci><ci id="alg1.l3.m1.1.1.8.cmml" xref="alg1.l3.m1.1.1.8">𝑒</ci><ci id="alg1.l3.m1.1.1.9.cmml" xref="alg1.l3.m1.1.1.9">_</ci><ci id="alg1.l3.m1.1.1.10.cmml" xref="alg1.l3.m1.1.1.10">𝑝</ci><ci id="alg1.l3.m1.1.1.11.cmml" xref="alg1.l3.m1.1.1.11">𝑡</ci><ci id="alg1.l3.m1.1.1.12.cmml" xref="alg1.l3.m1.1.1.12">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">compute\_ptr</annotation><annotation encoding="application/x-llamapun" id="alg1.l3.m1.1d">italic_c italic_o italic_m italic_p italic_u italic_t italic_e _ italic_p italic_t italic_r</annotation></semantics></math>, <math alttext="CHUNK\_SIZE" class="ltx_Math" display="inline" id="alg1.l3.m2.1"><semantics id="alg1.l3.m2.1a"><mrow id="alg1.l3.m2.1.1" xref="alg1.l3.m2.1.1.cmml"><mi id="alg1.l3.m2.1.1.2" xref="alg1.l3.m2.1.1.2.cmml">C</mi><mo id="alg1.l3.m2.1.1.1" xref="alg1.l3.m2.1.1.1.cmml">⁢</mo><mi id="alg1.l3.m2.1.1.3" xref="alg1.l3.m2.1.1.3.cmml">H</mi><mo id="alg1.l3.m2.1.1.1a" xref="alg1.l3.m2.1.1.1.cmml">⁢</mo><mi id="alg1.l3.m2.1.1.4" xref="alg1.l3.m2.1.1.4.cmml">U</mi><mo id="alg1.l3.m2.1.1.1b" xref="alg1.l3.m2.1.1.1.cmml">⁢</mo><mi id="alg1.l3.m2.1.1.5" xref="alg1.l3.m2.1.1.5.cmml">N</mi><mo id="alg1.l3.m2.1.1.1c" xref="alg1.l3.m2.1.1.1.cmml">⁢</mo><mi id="alg1.l3.m2.1.1.6" xref="alg1.l3.m2.1.1.6.cmml">K</mi><mo id="alg1.l3.m2.1.1.1d" xref="alg1.l3.m2.1.1.1.cmml">⁢</mo><mi id="alg1.l3.m2.1.1.7" mathvariant="normal" xref="alg1.l3.m2.1.1.7.cmml">_</mi><mo id="alg1.l3.m2.1.1.1e" xref="alg1.l3.m2.1.1.1.cmml">⁢</mo><mi id="alg1.l3.m2.1.1.8" xref="alg1.l3.m2.1.1.8.cmml">S</mi><mo id="alg1.l3.m2.1.1.1f" xref="alg1.l3.m2.1.1.1.cmml">⁢</mo><mi id="alg1.l3.m2.1.1.9" xref="alg1.l3.m2.1.1.9.cmml">I</mi><mo id="alg1.l3.m2.1.1.1g" xref="alg1.l3.m2.1.1.1.cmml">⁢</mo><mi id="alg1.l3.m2.1.1.10" xref="alg1.l3.m2.1.1.10.cmml">Z</mi><mo id="alg1.l3.m2.1.1.1h" xref="alg1.l3.m2.1.1.1.cmml">⁢</mo><mi id="alg1.l3.m2.1.1.11" xref="alg1.l3.m2.1.1.11.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l3.m2.1b"><apply id="alg1.l3.m2.1.1.cmml" xref="alg1.l3.m2.1.1"><times id="alg1.l3.m2.1.1.1.cmml" xref="alg1.l3.m2.1.1.1"></times><ci id="alg1.l3.m2.1.1.2.cmml" xref="alg1.l3.m2.1.1.2">𝐶</ci><ci id="alg1.l3.m2.1.1.3.cmml" xref="alg1.l3.m2.1.1.3">𝐻</ci><ci id="alg1.l3.m2.1.1.4.cmml" xref="alg1.l3.m2.1.1.4">𝑈</ci><ci id="alg1.l3.m2.1.1.5.cmml" xref="alg1.l3.m2.1.1.5">𝑁</ci><ci id="alg1.l3.m2.1.1.6.cmml" xref="alg1.l3.m2.1.1.6">𝐾</ci><ci id="alg1.l3.m2.1.1.7.cmml" xref="alg1.l3.m2.1.1.7">_</ci><ci id="alg1.l3.m2.1.1.8.cmml" xref="alg1.l3.m2.1.1.8">𝑆</ci><ci id="alg1.l3.m2.1.1.9.cmml" xref="alg1.l3.m2.1.1.9">𝐼</ci><ci id="alg1.l3.m2.1.1.10.cmml" xref="alg1.l3.m2.1.1.10">𝑍</ci><ci id="alg1.l3.m2.1.1.11.cmml" xref="alg1.l3.m2.1.1.11">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m2.1c">CHUNK\_SIZE</annotation><annotation encoding="application/x-llamapun" id="alg1.l3.m2.1d">italic_C italic_H italic_U italic_N italic_K _ italic_S italic_I italic_Z italic_E</annotation></semantics></math>) <span class="ltx_text ltx_font_bold" id="alg1.l3.3">then</span>
</div>
<div class="ltx_listingline" id="alg1.l4">
<span class="ltx_tag ltx_tag_listingline">4:</span>              Signal I/O worker to stop

</div>
<div class="ltx_listingline" id="alg1.l5">
<span class="ltx_tag ltx_tag_listingline">5:</span>              <span class="ltx_text ltx_font_bold" id="alg1.l5.1">break</span>
         
</div>
<div class="ltx_listingline" id="alg1.l6">
<span class="ltx_tag ltx_tag_listingline">6:</span>         Compute <span class="ltx_text" id="alg1.l6.1">KV cache</span> for chunk starting at <math alttext="compute\_ptr" class="ltx_Math" display="inline" id="alg1.l6.m1.1"><semantics id="alg1.l6.m1.1a"><mrow id="alg1.l6.m1.1.1" xref="alg1.l6.m1.1.1.cmml"><mi id="alg1.l6.m1.1.1.2" xref="alg1.l6.m1.1.1.2.cmml">c</mi><mo id="alg1.l6.m1.1.1.1" xref="alg1.l6.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l6.m1.1.1.3" xref="alg1.l6.m1.1.1.3.cmml">o</mi><mo id="alg1.l6.m1.1.1.1a" xref="alg1.l6.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l6.m1.1.1.4" xref="alg1.l6.m1.1.1.4.cmml">m</mi><mo id="alg1.l6.m1.1.1.1b" xref="alg1.l6.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l6.m1.1.1.5" xref="alg1.l6.m1.1.1.5.cmml">p</mi><mo id="alg1.l6.m1.1.1.1c" xref="alg1.l6.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l6.m1.1.1.6" xref="alg1.l6.m1.1.1.6.cmml">u</mi><mo id="alg1.l6.m1.1.1.1d" xref="alg1.l6.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l6.m1.1.1.7" xref="alg1.l6.m1.1.1.7.cmml">t</mi><mo id="alg1.l6.m1.1.1.1e" xref="alg1.l6.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l6.m1.1.1.8" xref="alg1.l6.m1.1.1.8.cmml">e</mi><mo id="alg1.l6.m1.1.1.1f" xref="alg1.l6.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l6.m1.1.1.9" mathvariant="normal" xref="alg1.l6.m1.1.1.9.cmml">_</mi><mo id="alg1.l6.m1.1.1.1g" xref="alg1.l6.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l6.m1.1.1.10" xref="alg1.l6.m1.1.1.10.cmml">p</mi><mo id="alg1.l6.m1.1.1.1h" xref="alg1.l6.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l6.m1.1.1.11" xref="alg1.l6.m1.1.1.11.cmml">t</mi><mo id="alg1.l6.m1.1.1.1i" xref="alg1.l6.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l6.m1.1.1.12" xref="alg1.l6.m1.1.1.12.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l6.m1.1b"><apply id="alg1.l6.m1.1.1.cmml" xref="alg1.l6.m1.1.1"><times id="alg1.l6.m1.1.1.1.cmml" xref="alg1.l6.m1.1.1.1"></times><ci id="alg1.l6.m1.1.1.2.cmml" xref="alg1.l6.m1.1.1.2">𝑐</ci><ci id="alg1.l6.m1.1.1.3.cmml" xref="alg1.l6.m1.1.1.3">𝑜</ci><ci id="alg1.l6.m1.1.1.4.cmml" xref="alg1.l6.m1.1.1.4">𝑚</ci><ci id="alg1.l6.m1.1.1.5.cmml" xref="alg1.l6.m1.1.1.5">𝑝</ci><ci id="alg1.l6.m1.1.1.6.cmml" xref="alg1.l6.m1.1.1.6">𝑢</ci><ci id="alg1.l6.m1.1.1.7.cmml" xref="alg1.l6.m1.1.1.7">𝑡</ci><ci id="alg1.l6.m1.1.1.8.cmml" xref="alg1.l6.m1.1.1.8">𝑒</ci><ci id="alg1.l6.m1.1.1.9.cmml" xref="alg1.l6.m1.1.1.9">_</ci><ci id="alg1.l6.m1.1.1.10.cmml" xref="alg1.l6.m1.1.1.10">𝑝</ci><ci id="alg1.l6.m1.1.1.11.cmml" xref="alg1.l6.m1.1.1.11">𝑡</ci><ci id="alg1.l6.m1.1.1.12.cmml" xref="alg1.l6.m1.1.1.12">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m1.1c">compute\_ptr</annotation><annotation encoding="application/x-llamapun" id="alg1.l6.m1.1d">italic_c italic_o italic_m italic_p italic_u italic_t italic_e _ italic_p italic_t italic_r</annotation></semantics></math> using GPU

</div>
<div class="ltx_listingline" id="alg1.l7">
<span class="ltx_tag ltx_tag_listingline">7:</span>         <math alttext="compute\_ptr\leftarrow compute\_ptr+CHUNK\_SIZE" class="ltx_Math" display="inline" id="alg1.l7.m1.1"><semantics id="alg1.l7.m1.1a"><mrow id="alg1.l7.m1.1.1" xref="alg1.l7.m1.1.1.cmml"><mrow id="alg1.l7.m1.1.1.2" xref="alg1.l7.m1.1.1.2.cmml"><mi id="alg1.l7.m1.1.1.2.2" xref="alg1.l7.m1.1.1.2.2.cmml">c</mi><mo id="alg1.l7.m1.1.1.2.1" xref="alg1.l7.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.2.3" xref="alg1.l7.m1.1.1.2.3.cmml">o</mi><mo id="alg1.l7.m1.1.1.2.1a" xref="alg1.l7.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.2.4" xref="alg1.l7.m1.1.1.2.4.cmml">m</mi><mo id="alg1.l7.m1.1.1.2.1b" xref="alg1.l7.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.2.5" xref="alg1.l7.m1.1.1.2.5.cmml">p</mi><mo id="alg1.l7.m1.1.1.2.1c" xref="alg1.l7.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.2.6" xref="alg1.l7.m1.1.1.2.6.cmml">u</mi><mo id="alg1.l7.m1.1.1.2.1d" xref="alg1.l7.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.2.7" xref="alg1.l7.m1.1.1.2.7.cmml">t</mi><mo id="alg1.l7.m1.1.1.2.1e" xref="alg1.l7.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.2.8" xref="alg1.l7.m1.1.1.2.8.cmml">e</mi><mo id="alg1.l7.m1.1.1.2.1f" xref="alg1.l7.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.2.9" mathvariant="normal" xref="alg1.l7.m1.1.1.2.9.cmml">_</mi><mo id="alg1.l7.m1.1.1.2.1g" xref="alg1.l7.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.2.10" xref="alg1.l7.m1.1.1.2.10.cmml">p</mi><mo id="alg1.l7.m1.1.1.2.1h" xref="alg1.l7.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.2.11" xref="alg1.l7.m1.1.1.2.11.cmml">t</mi><mo id="alg1.l7.m1.1.1.2.1i" xref="alg1.l7.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.2.12" xref="alg1.l7.m1.1.1.2.12.cmml">r</mi></mrow><mo id="alg1.l7.m1.1.1.1" stretchy="false" xref="alg1.l7.m1.1.1.1.cmml">←</mo><mrow id="alg1.l7.m1.1.1.3" xref="alg1.l7.m1.1.1.3.cmml"><mrow id="alg1.l7.m1.1.1.3.2" xref="alg1.l7.m1.1.1.3.2.cmml"><mi id="alg1.l7.m1.1.1.3.2.2" xref="alg1.l7.m1.1.1.3.2.2.cmml">c</mi><mo id="alg1.l7.m1.1.1.3.2.1" xref="alg1.l7.m1.1.1.3.2.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.3.2.3" xref="alg1.l7.m1.1.1.3.2.3.cmml">o</mi><mo id="alg1.l7.m1.1.1.3.2.1a" xref="alg1.l7.m1.1.1.3.2.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.3.2.4" xref="alg1.l7.m1.1.1.3.2.4.cmml">m</mi><mo id="alg1.l7.m1.1.1.3.2.1b" xref="alg1.l7.m1.1.1.3.2.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.3.2.5" xref="alg1.l7.m1.1.1.3.2.5.cmml">p</mi><mo id="alg1.l7.m1.1.1.3.2.1c" xref="alg1.l7.m1.1.1.3.2.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.3.2.6" xref="alg1.l7.m1.1.1.3.2.6.cmml">u</mi><mo id="alg1.l7.m1.1.1.3.2.1d" xref="alg1.l7.m1.1.1.3.2.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.3.2.7" xref="alg1.l7.m1.1.1.3.2.7.cmml">t</mi><mo id="alg1.l7.m1.1.1.3.2.1e" xref="alg1.l7.m1.1.1.3.2.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.3.2.8" xref="alg1.l7.m1.1.1.3.2.8.cmml">e</mi><mo id="alg1.l7.m1.1.1.3.2.1f" xref="alg1.l7.m1.1.1.3.2.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.3.2.9" mathvariant="normal" xref="alg1.l7.m1.1.1.3.2.9.cmml">_</mi><mo id="alg1.l7.m1.1.1.3.2.1g" xref="alg1.l7.m1.1.1.3.2.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.3.2.10" xref="alg1.l7.m1.1.1.3.2.10.cmml">p</mi><mo id="alg1.l7.m1.1.1.3.2.1h" xref="alg1.l7.m1.1.1.3.2.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.3.2.11" xref="alg1.l7.m1.1.1.3.2.11.cmml">t</mi><mo id="alg1.l7.m1.1.1.3.2.1i" xref="alg1.l7.m1.1.1.3.2.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.3.2.12" xref="alg1.l7.m1.1.1.3.2.12.cmml">r</mi></mrow><mo id="alg1.l7.m1.1.1.3.1" xref="alg1.l7.m1.1.1.3.1.cmml">+</mo><mrow id="alg1.l7.m1.1.1.3.3" xref="alg1.l7.m1.1.1.3.3.cmml"><mi id="alg1.l7.m1.1.1.3.3.2" xref="alg1.l7.m1.1.1.3.3.2.cmml">C</mi><mo id="alg1.l7.m1.1.1.3.3.1" xref="alg1.l7.m1.1.1.3.3.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.3.3.3" xref="alg1.l7.m1.1.1.3.3.3.cmml">H</mi><mo id="alg1.l7.m1.1.1.3.3.1a" xref="alg1.l7.m1.1.1.3.3.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.3.3.4" xref="alg1.l7.m1.1.1.3.3.4.cmml">U</mi><mo id="alg1.l7.m1.1.1.3.3.1b" xref="alg1.l7.m1.1.1.3.3.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.3.3.5" xref="alg1.l7.m1.1.1.3.3.5.cmml">N</mi><mo id="alg1.l7.m1.1.1.3.3.1c" xref="alg1.l7.m1.1.1.3.3.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.3.3.6" xref="alg1.l7.m1.1.1.3.3.6.cmml">K</mi><mo id="alg1.l7.m1.1.1.3.3.1d" xref="alg1.l7.m1.1.1.3.3.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.3.3.7" mathvariant="normal" xref="alg1.l7.m1.1.1.3.3.7.cmml">_</mi><mo id="alg1.l7.m1.1.1.3.3.1e" xref="alg1.l7.m1.1.1.3.3.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.3.3.8" xref="alg1.l7.m1.1.1.3.3.8.cmml">S</mi><mo id="alg1.l7.m1.1.1.3.3.1f" xref="alg1.l7.m1.1.1.3.3.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.3.3.9" xref="alg1.l7.m1.1.1.3.3.9.cmml">I</mi><mo id="alg1.l7.m1.1.1.3.3.1g" xref="alg1.l7.m1.1.1.3.3.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.3.3.10" xref="alg1.l7.m1.1.1.3.3.10.cmml">Z</mi><mo id="alg1.l7.m1.1.1.3.3.1h" xref="alg1.l7.m1.1.1.3.3.1.cmml">⁢</mo><mi id="alg1.l7.m1.1.1.3.3.11" xref="alg1.l7.m1.1.1.3.3.11.cmml">E</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l7.m1.1b"><apply id="alg1.l7.m1.1.1.cmml" xref="alg1.l7.m1.1.1"><ci id="alg1.l7.m1.1.1.1.cmml" xref="alg1.l7.m1.1.1.1">←</ci><apply id="alg1.l7.m1.1.1.2.cmml" xref="alg1.l7.m1.1.1.2"><times id="alg1.l7.m1.1.1.2.1.cmml" xref="alg1.l7.m1.1.1.2.1"></times><ci id="alg1.l7.m1.1.1.2.2.cmml" xref="alg1.l7.m1.1.1.2.2">𝑐</ci><ci id="alg1.l7.m1.1.1.2.3.cmml" xref="alg1.l7.m1.1.1.2.3">𝑜</ci><ci id="alg1.l7.m1.1.1.2.4.cmml" xref="alg1.l7.m1.1.1.2.4">𝑚</ci><ci id="alg1.l7.m1.1.1.2.5.cmml" xref="alg1.l7.m1.1.1.2.5">𝑝</ci><ci id="alg1.l7.m1.1.1.2.6.cmml" xref="alg1.l7.m1.1.1.2.6">𝑢</ci><ci id="alg1.l7.m1.1.1.2.7.cmml" xref="alg1.l7.m1.1.1.2.7">𝑡</ci><ci id="alg1.l7.m1.1.1.2.8.cmml" xref="alg1.l7.m1.1.1.2.8">𝑒</ci><ci id="alg1.l7.m1.1.1.2.9.cmml" xref="alg1.l7.m1.1.1.2.9">_</ci><ci id="alg1.l7.m1.1.1.2.10.cmml" xref="alg1.l7.m1.1.1.2.10">𝑝</ci><ci id="alg1.l7.m1.1.1.2.11.cmml" xref="alg1.l7.m1.1.1.2.11">𝑡</ci><ci id="alg1.l7.m1.1.1.2.12.cmml" xref="alg1.l7.m1.1.1.2.12">𝑟</ci></apply><apply id="alg1.l7.m1.1.1.3.cmml" xref="alg1.l7.m1.1.1.3"><plus id="alg1.l7.m1.1.1.3.1.cmml" xref="alg1.l7.m1.1.1.3.1"></plus><apply id="alg1.l7.m1.1.1.3.2.cmml" xref="alg1.l7.m1.1.1.3.2"><times id="alg1.l7.m1.1.1.3.2.1.cmml" xref="alg1.l7.m1.1.1.3.2.1"></times><ci id="alg1.l7.m1.1.1.3.2.2.cmml" xref="alg1.l7.m1.1.1.3.2.2">𝑐</ci><ci id="alg1.l7.m1.1.1.3.2.3.cmml" xref="alg1.l7.m1.1.1.3.2.3">𝑜</ci><ci id="alg1.l7.m1.1.1.3.2.4.cmml" xref="alg1.l7.m1.1.1.3.2.4">𝑚</ci><ci id="alg1.l7.m1.1.1.3.2.5.cmml" xref="alg1.l7.m1.1.1.3.2.5">𝑝</ci><ci id="alg1.l7.m1.1.1.3.2.6.cmml" xref="alg1.l7.m1.1.1.3.2.6">𝑢</ci><ci id="alg1.l7.m1.1.1.3.2.7.cmml" xref="alg1.l7.m1.1.1.3.2.7">𝑡</ci><ci id="alg1.l7.m1.1.1.3.2.8.cmml" xref="alg1.l7.m1.1.1.3.2.8">𝑒</ci><ci id="alg1.l7.m1.1.1.3.2.9.cmml" xref="alg1.l7.m1.1.1.3.2.9">_</ci><ci id="alg1.l7.m1.1.1.3.2.10.cmml" xref="alg1.l7.m1.1.1.3.2.10">𝑝</ci><ci id="alg1.l7.m1.1.1.3.2.11.cmml" xref="alg1.l7.m1.1.1.3.2.11">𝑡</ci><ci id="alg1.l7.m1.1.1.3.2.12.cmml" xref="alg1.l7.m1.1.1.3.2.12">𝑟</ci></apply><apply id="alg1.l7.m1.1.1.3.3.cmml" xref="alg1.l7.m1.1.1.3.3"><times id="alg1.l7.m1.1.1.3.3.1.cmml" xref="alg1.l7.m1.1.1.3.3.1"></times><ci id="alg1.l7.m1.1.1.3.3.2.cmml" xref="alg1.l7.m1.1.1.3.3.2">𝐶</ci><ci id="alg1.l7.m1.1.1.3.3.3.cmml" xref="alg1.l7.m1.1.1.3.3.3">𝐻</ci><ci id="alg1.l7.m1.1.1.3.3.4.cmml" xref="alg1.l7.m1.1.1.3.3.4">𝑈</ci><ci id="alg1.l7.m1.1.1.3.3.5.cmml" xref="alg1.l7.m1.1.1.3.3.5">𝑁</ci><ci id="alg1.l7.m1.1.1.3.3.6.cmml" xref="alg1.l7.m1.1.1.3.3.6">𝐾</ci><ci id="alg1.l7.m1.1.1.3.3.7.cmml" xref="alg1.l7.m1.1.1.3.3.7">_</ci><ci id="alg1.l7.m1.1.1.3.3.8.cmml" xref="alg1.l7.m1.1.1.3.3.8">𝑆</ci><ci id="alg1.l7.m1.1.1.3.3.9.cmml" xref="alg1.l7.m1.1.1.3.3.9">𝐼</ci><ci id="alg1.l7.m1.1.1.3.3.10.cmml" xref="alg1.l7.m1.1.1.3.3.10">𝑍</ci><ci id="alg1.l7.m1.1.1.3.3.11.cmml" xref="alg1.l7.m1.1.1.3.3.11">𝐸</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m1.1c">compute\_ptr\leftarrow compute\_ptr+CHUNK\_SIZE</annotation><annotation encoding="application/x-llamapun" id="alg1.l7.m1.1d">italic_c italic_o italic_m italic_p italic_u italic_t italic_e _ italic_p italic_t italic_r ← italic_c italic_o italic_m italic_p italic_u italic_t italic_e _ italic_p italic_t italic_r + italic_C italic_H italic_U italic_N italic_K _ italic_S italic_I italic_Z italic_E</annotation></semantics></math>
     
</div>
<div class="ltx_listingline" id="alg1.l8">
<span class="ltx_tag ltx_tag_listingline">8:</span><span class="ltx_text ltx_font_bold" id="alg1.l8.1">procedure</span> <span class="ltx_text ltx_font_smallcaps" id="alg1.l8.2">FetchKV</span>
</div>
<div class="ltx_listingline" id="alg1.l9">
<span class="ltx_tag ltx_tag_listingline">9:</span>     <span class="ltx_text ltx_font_bold" id="alg1.l9.1">while</span> <math alttext="compute\_ptr&lt;io\_ptr" class="ltx_Math" display="inline" id="alg1.l9.m1.1"><semantics id="alg1.l9.m1.1a"><mrow id="alg1.l9.m1.1.1" xref="alg1.l9.m1.1.1.cmml"><mrow id="alg1.l9.m1.1.1.2" xref="alg1.l9.m1.1.1.2.cmml"><mi id="alg1.l9.m1.1.1.2.2" xref="alg1.l9.m1.1.1.2.2.cmml">c</mi><mo id="alg1.l9.m1.1.1.2.1" xref="alg1.l9.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l9.m1.1.1.2.3" xref="alg1.l9.m1.1.1.2.3.cmml">o</mi><mo id="alg1.l9.m1.1.1.2.1a" xref="alg1.l9.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l9.m1.1.1.2.4" xref="alg1.l9.m1.1.1.2.4.cmml">m</mi><mo id="alg1.l9.m1.1.1.2.1b" xref="alg1.l9.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l9.m1.1.1.2.5" xref="alg1.l9.m1.1.1.2.5.cmml">p</mi><mo id="alg1.l9.m1.1.1.2.1c" xref="alg1.l9.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l9.m1.1.1.2.6" xref="alg1.l9.m1.1.1.2.6.cmml">u</mi><mo id="alg1.l9.m1.1.1.2.1d" xref="alg1.l9.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l9.m1.1.1.2.7" xref="alg1.l9.m1.1.1.2.7.cmml">t</mi><mo id="alg1.l9.m1.1.1.2.1e" xref="alg1.l9.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l9.m1.1.1.2.8" xref="alg1.l9.m1.1.1.2.8.cmml">e</mi><mo id="alg1.l9.m1.1.1.2.1f" xref="alg1.l9.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l9.m1.1.1.2.9" mathvariant="normal" xref="alg1.l9.m1.1.1.2.9.cmml">_</mi><mo id="alg1.l9.m1.1.1.2.1g" xref="alg1.l9.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l9.m1.1.1.2.10" xref="alg1.l9.m1.1.1.2.10.cmml">p</mi><mo id="alg1.l9.m1.1.1.2.1h" xref="alg1.l9.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l9.m1.1.1.2.11" xref="alg1.l9.m1.1.1.2.11.cmml">t</mi><mo id="alg1.l9.m1.1.1.2.1i" xref="alg1.l9.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l9.m1.1.1.2.12" xref="alg1.l9.m1.1.1.2.12.cmml">r</mi></mrow><mo id="alg1.l9.m1.1.1.1" xref="alg1.l9.m1.1.1.1.cmml">&lt;</mo><mrow id="alg1.l9.m1.1.1.3" xref="alg1.l9.m1.1.1.3.cmml"><mi id="alg1.l9.m1.1.1.3.2" xref="alg1.l9.m1.1.1.3.2.cmml">i</mi><mo id="alg1.l9.m1.1.1.3.1" xref="alg1.l9.m1.1.1.3.1.cmml">⁢</mo><mi id="alg1.l9.m1.1.1.3.3" xref="alg1.l9.m1.1.1.3.3.cmml">o</mi><mo id="alg1.l9.m1.1.1.3.1a" xref="alg1.l9.m1.1.1.3.1.cmml">⁢</mo><mi id="alg1.l9.m1.1.1.3.4" mathvariant="normal" xref="alg1.l9.m1.1.1.3.4.cmml">_</mi><mo id="alg1.l9.m1.1.1.3.1b" xref="alg1.l9.m1.1.1.3.1.cmml">⁢</mo><mi id="alg1.l9.m1.1.1.3.5" xref="alg1.l9.m1.1.1.3.5.cmml">p</mi><mo id="alg1.l9.m1.1.1.3.1c" xref="alg1.l9.m1.1.1.3.1.cmml">⁢</mo><mi id="alg1.l9.m1.1.1.3.6" xref="alg1.l9.m1.1.1.3.6.cmml">t</mi><mo id="alg1.l9.m1.1.1.3.1d" xref="alg1.l9.m1.1.1.3.1.cmml">⁢</mo><mi id="alg1.l9.m1.1.1.3.7" xref="alg1.l9.m1.1.1.3.7.cmml">r</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l9.m1.1b"><apply id="alg1.l9.m1.1.1.cmml" xref="alg1.l9.m1.1.1"><lt id="alg1.l9.m1.1.1.1.cmml" xref="alg1.l9.m1.1.1.1"></lt><apply id="alg1.l9.m1.1.1.2.cmml" xref="alg1.l9.m1.1.1.2"><times id="alg1.l9.m1.1.1.2.1.cmml" xref="alg1.l9.m1.1.1.2.1"></times><ci id="alg1.l9.m1.1.1.2.2.cmml" xref="alg1.l9.m1.1.1.2.2">𝑐</ci><ci id="alg1.l9.m1.1.1.2.3.cmml" xref="alg1.l9.m1.1.1.2.3">𝑜</ci><ci id="alg1.l9.m1.1.1.2.4.cmml" xref="alg1.l9.m1.1.1.2.4">𝑚</ci><ci id="alg1.l9.m1.1.1.2.5.cmml" xref="alg1.l9.m1.1.1.2.5">𝑝</ci><ci id="alg1.l9.m1.1.1.2.6.cmml" xref="alg1.l9.m1.1.1.2.6">𝑢</ci><ci id="alg1.l9.m1.1.1.2.7.cmml" xref="alg1.l9.m1.1.1.2.7">𝑡</ci><ci id="alg1.l9.m1.1.1.2.8.cmml" xref="alg1.l9.m1.1.1.2.8">𝑒</ci><ci id="alg1.l9.m1.1.1.2.9.cmml" xref="alg1.l9.m1.1.1.2.9">_</ci><ci id="alg1.l9.m1.1.1.2.10.cmml" xref="alg1.l9.m1.1.1.2.10">𝑝</ci><ci id="alg1.l9.m1.1.1.2.11.cmml" xref="alg1.l9.m1.1.1.2.11">𝑡</ci><ci id="alg1.l9.m1.1.1.2.12.cmml" xref="alg1.l9.m1.1.1.2.12">𝑟</ci></apply><apply id="alg1.l9.m1.1.1.3.cmml" xref="alg1.l9.m1.1.1.3"><times id="alg1.l9.m1.1.1.3.1.cmml" xref="alg1.l9.m1.1.1.3.1"></times><ci id="alg1.l9.m1.1.1.3.2.cmml" xref="alg1.l9.m1.1.1.3.2">𝑖</ci><ci id="alg1.l9.m1.1.1.3.3.cmml" xref="alg1.l9.m1.1.1.3.3">𝑜</ci><ci id="alg1.l9.m1.1.1.3.4.cmml" xref="alg1.l9.m1.1.1.3.4">_</ci><ci id="alg1.l9.m1.1.1.3.5.cmml" xref="alg1.l9.m1.1.1.3.5">𝑝</ci><ci id="alg1.l9.m1.1.1.3.6.cmml" xref="alg1.l9.m1.1.1.3.6">𝑡</ci><ci id="alg1.l9.m1.1.1.3.7.cmml" xref="alg1.l9.m1.1.1.3.7">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m1.1c">compute\_ptr&lt;io\_ptr</annotation><annotation encoding="application/x-llamapun" id="alg1.l9.m1.1d">italic_c italic_o italic_m italic_p italic_u italic_t italic_e _ italic_p italic_t italic_r &lt; italic_i italic_o _ italic_p italic_t italic_r</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="alg1.l9.2">do</span>
</div>
<div class="ltx_listingline" id="alg1.l10">
<span class="ltx_tag ltx_tag_listingline">10:</span>         Fetch <span class="ltx_text" id="alg1.l10.1">KV cache</span> for chunk ending at <math alttext="io\_ptr" class="ltx_Math" display="inline" id="alg1.l10.m1.1"><semantics id="alg1.l10.m1.1a"><mrow id="alg1.l10.m1.1.1" xref="alg1.l10.m1.1.1.cmml"><mi id="alg1.l10.m1.1.1.2" xref="alg1.l10.m1.1.1.2.cmml">i</mi><mo id="alg1.l10.m1.1.1.1" xref="alg1.l10.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l10.m1.1.1.3" xref="alg1.l10.m1.1.1.3.cmml">o</mi><mo id="alg1.l10.m1.1.1.1a" xref="alg1.l10.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l10.m1.1.1.4" mathvariant="normal" xref="alg1.l10.m1.1.1.4.cmml">_</mi><mo id="alg1.l10.m1.1.1.1b" xref="alg1.l10.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l10.m1.1.1.5" xref="alg1.l10.m1.1.1.5.cmml">p</mi><mo id="alg1.l10.m1.1.1.1c" xref="alg1.l10.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l10.m1.1.1.6" xref="alg1.l10.m1.1.1.6.cmml">t</mi><mo id="alg1.l10.m1.1.1.1d" xref="alg1.l10.m1.1.1.1.cmml">⁢</mo><mi id="alg1.l10.m1.1.1.7" xref="alg1.l10.m1.1.1.7.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l10.m1.1b"><apply id="alg1.l10.m1.1.1.cmml" xref="alg1.l10.m1.1.1"><times id="alg1.l10.m1.1.1.1.cmml" xref="alg1.l10.m1.1.1.1"></times><ci id="alg1.l10.m1.1.1.2.cmml" xref="alg1.l10.m1.1.1.2">𝑖</ci><ci id="alg1.l10.m1.1.1.3.cmml" xref="alg1.l10.m1.1.1.3">𝑜</ci><ci id="alg1.l10.m1.1.1.4.cmml" xref="alg1.l10.m1.1.1.4">_</ci><ci id="alg1.l10.m1.1.1.5.cmml" xref="alg1.l10.m1.1.1.5">𝑝</ci><ci id="alg1.l10.m1.1.1.6.cmml" xref="alg1.l10.m1.1.1.6">𝑡</ci><ci id="alg1.l10.m1.1.1.7.cmml" xref="alg1.l10.m1.1.1.7">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.m1.1c">io\_ptr</annotation><annotation encoding="application/x-llamapun" id="alg1.l10.m1.1d">italic_i italic_o _ italic_p italic_t italic_r</annotation></semantics></math> from storage to CPU Memory

</div>
<div class="ltx_listingline" id="alg1.l11">
<span class="ltx_tag ltx_tag_listingline">11:</span>         <math alttext="io\_ptr\leftarrow io\_ptr-CHUNK\_SIZE" class="ltx_Math" display="inline" id="alg1.l11.m1.1"><semantics id="alg1.l11.m1.1a"><mrow id="alg1.l11.m1.1.1" xref="alg1.l11.m1.1.1.cmml"><mrow id="alg1.l11.m1.1.1.2" xref="alg1.l11.m1.1.1.2.cmml"><mi id="alg1.l11.m1.1.1.2.2" xref="alg1.l11.m1.1.1.2.2.cmml">i</mi><mo id="alg1.l11.m1.1.1.2.1" xref="alg1.l11.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l11.m1.1.1.2.3" xref="alg1.l11.m1.1.1.2.3.cmml">o</mi><mo id="alg1.l11.m1.1.1.2.1a" xref="alg1.l11.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l11.m1.1.1.2.4" mathvariant="normal" xref="alg1.l11.m1.1.1.2.4.cmml">_</mi><mo id="alg1.l11.m1.1.1.2.1b" xref="alg1.l11.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l11.m1.1.1.2.5" xref="alg1.l11.m1.1.1.2.5.cmml">p</mi><mo id="alg1.l11.m1.1.1.2.1c" xref="alg1.l11.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l11.m1.1.1.2.6" xref="alg1.l11.m1.1.1.2.6.cmml">t</mi><mo id="alg1.l11.m1.1.1.2.1d" xref="alg1.l11.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l11.m1.1.1.2.7" xref="alg1.l11.m1.1.1.2.7.cmml">r</mi></mrow><mo id="alg1.l11.m1.1.1.1" stretchy="false" xref="alg1.l11.m1.1.1.1.cmml">←</mo><mrow id="alg1.l11.m1.1.1.3" xref="alg1.l11.m1.1.1.3.cmml"><mrow id="alg1.l11.m1.1.1.3.2" xref="alg1.l11.m1.1.1.3.2.cmml"><mi id="alg1.l11.m1.1.1.3.2.2" xref="alg1.l11.m1.1.1.3.2.2.cmml">i</mi><mo id="alg1.l11.m1.1.1.3.2.1" xref="alg1.l11.m1.1.1.3.2.1.cmml">⁢</mo><mi id="alg1.l11.m1.1.1.3.2.3" xref="alg1.l11.m1.1.1.3.2.3.cmml">o</mi><mo id="alg1.l11.m1.1.1.3.2.1a" xref="alg1.l11.m1.1.1.3.2.1.cmml">⁢</mo><mi id="alg1.l11.m1.1.1.3.2.4" mathvariant="normal" xref="alg1.l11.m1.1.1.3.2.4.cmml">_</mi><mo id="alg1.l11.m1.1.1.3.2.1b" xref="alg1.l11.m1.1.1.3.2.1.cmml">⁢</mo><mi id="alg1.l11.m1.1.1.3.2.5" xref="alg1.l11.m1.1.1.3.2.5.cmml">p</mi><mo id="alg1.l11.m1.1.1.3.2.1c" xref="alg1.l11.m1.1.1.3.2.1.cmml">⁢</mo><mi id="alg1.l11.m1.1.1.3.2.6" xref="alg1.l11.m1.1.1.3.2.6.cmml">t</mi><mo id="alg1.l11.m1.1.1.3.2.1d" xref="alg1.l11.m1.1.1.3.2.1.cmml">⁢</mo><mi id="alg1.l11.m1.1.1.3.2.7" xref="alg1.l11.m1.1.1.3.2.7.cmml">r</mi></mrow><mo id="alg1.l11.m1.1.1.3.1" xref="alg1.l11.m1.1.1.3.1.cmml">−</mo><mrow id="alg1.l11.m1.1.1.3.3" xref="alg1.l11.m1.1.1.3.3.cmml"><mi id="alg1.l11.m1.1.1.3.3.2" xref="alg1.l11.m1.1.1.3.3.2.cmml">C</mi><mo id="alg1.l11.m1.1.1.3.3.1" xref="alg1.l11.m1.1.1.3.3.1.cmml">⁢</mo><mi id="alg1.l11.m1.1.1.3.3.3" xref="alg1.l11.m1.1.1.3.3.3.cmml">H</mi><mo id="alg1.l11.m1.1.1.3.3.1a" xref="alg1.l11.m1.1.1.3.3.1.cmml">⁢</mo><mi id="alg1.l11.m1.1.1.3.3.4" xref="alg1.l11.m1.1.1.3.3.4.cmml">U</mi><mo id="alg1.l11.m1.1.1.3.3.1b" xref="alg1.l11.m1.1.1.3.3.1.cmml">⁢</mo><mi id="alg1.l11.m1.1.1.3.3.5" xref="alg1.l11.m1.1.1.3.3.5.cmml">N</mi><mo id="alg1.l11.m1.1.1.3.3.1c" xref="alg1.l11.m1.1.1.3.3.1.cmml">⁢</mo><mi id="alg1.l11.m1.1.1.3.3.6" xref="alg1.l11.m1.1.1.3.3.6.cmml">K</mi><mo id="alg1.l11.m1.1.1.3.3.1d" xref="alg1.l11.m1.1.1.3.3.1.cmml">⁢</mo><mi id="alg1.l11.m1.1.1.3.3.7" mathvariant="normal" xref="alg1.l11.m1.1.1.3.3.7.cmml">_</mi><mo id="alg1.l11.m1.1.1.3.3.1e" xref="alg1.l11.m1.1.1.3.3.1.cmml">⁢</mo><mi id="alg1.l11.m1.1.1.3.3.8" xref="alg1.l11.m1.1.1.3.3.8.cmml">S</mi><mo id="alg1.l11.m1.1.1.3.3.1f" xref="alg1.l11.m1.1.1.3.3.1.cmml">⁢</mo><mi id="alg1.l11.m1.1.1.3.3.9" xref="alg1.l11.m1.1.1.3.3.9.cmml">I</mi><mo id="alg1.l11.m1.1.1.3.3.1g" xref="alg1.l11.m1.1.1.3.3.1.cmml">⁢</mo><mi id="alg1.l11.m1.1.1.3.3.10" xref="alg1.l11.m1.1.1.3.3.10.cmml">Z</mi><mo id="alg1.l11.m1.1.1.3.3.1h" xref="alg1.l11.m1.1.1.3.3.1.cmml">⁢</mo><mi id="alg1.l11.m1.1.1.3.3.11" xref="alg1.l11.m1.1.1.3.3.11.cmml">E</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l11.m1.1b"><apply id="alg1.l11.m1.1.1.cmml" xref="alg1.l11.m1.1.1"><ci id="alg1.l11.m1.1.1.1.cmml" xref="alg1.l11.m1.1.1.1">←</ci><apply id="alg1.l11.m1.1.1.2.cmml" xref="alg1.l11.m1.1.1.2"><times id="alg1.l11.m1.1.1.2.1.cmml" xref="alg1.l11.m1.1.1.2.1"></times><ci id="alg1.l11.m1.1.1.2.2.cmml" xref="alg1.l11.m1.1.1.2.2">𝑖</ci><ci id="alg1.l11.m1.1.1.2.3.cmml" xref="alg1.l11.m1.1.1.2.3">𝑜</ci><ci id="alg1.l11.m1.1.1.2.4.cmml" xref="alg1.l11.m1.1.1.2.4">_</ci><ci id="alg1.l11.m1.1.1.2.5.cmml" xref="alg1.l11.m1.1.1.2.5">𝑝</ci><ci id="alg1.l11.m1.1.1.2.6.cmml" xref="alg1.l11.m1.1.1.2.6">𝑡</ci><ci id="alg1.l11.m1.1.1.2.7.cmml" xref="alg1.l11.m1.1.1.2.7">𝑟</ci></apply><apply id="alg1.l11.m1.1.1.3.cmml" xref="alg1.l11.m1.1.1.3"><minus id="alg1.l11.m1.1.1.3.1.cmml" xref="alg1.l11.m1.1.1.3.1"></minus><apply id="alg1.l11.m1.1.1.3.2.cmml" xref="alg1.l11.m1.1.1.3.2"><times id="alg1.l11.m1.1.1.3.2.1.cmml" xref="alg1.l11.m1.1.1.3.2.1"></times><ci id="alg1.l11.m1.1.1.3.2.2.cmml" xref="alg1.l11.m1.1.1.3.2.2">𝑖</ci><ci id="alg1.l11.m1.1.1.3.2.3.cmml" xref="alg1.l11.m1.1.1.3.2.3">𝑜</ci><ci id="alg1.l11.m1.1.1.3.2.4.cmml" xref="alg1.l11.m1.1.1.3.2.4">_</ci><ci id="alg1.l11.m1.1.1.3.2.5.cmml" xref="alg1.l11.m1.1.1.3.2.5">𝑝</ci><ci id="alg1.l11.m1.1.1.3.2.6.cmml" xref="alg1.l11.m1.1.1.3.2.6">𝑡</ci><ci id="alg1.l11.m1.1.1.3.2.7.cmml" xref="alg1.l11.m1.1.1.3.2.7">𝑟</ci></apply><apply id="alg1.l11.m1.1.1.3.3.cmml" xref="alg1.l11.m1.1.1.3.3"><times id="alg1.l11.m1.1.1.3.3.1.cmml" xref="alg1.l11.m1.1.1.3.3.1"></times><ci id="alg1.l11.m1.1.1.3.3.2.cmml" xref="alg1.l11.m1.1.1.3.3.2">𝐶</ci><ci id="alg1.l11.m1.1.1.3.3.3.cmml" xref="alg1.l11.m1.1.1.3.3.3">𝐻</ci><ci id="alg1.l11.m1.1.1.3.3.4.cmml" xref="alg1.l11.m1.1.1.3.3.4">𝑈</ci><ci id="alg1.l11.m1.1.1.3.3.5.cmml" xref="alg1.l11.m1.1.1.3.3.5">𝑁</ci><ci id="alg1.l11.m1.1.1.3.3.6.cmml" xref="alg1.l11.m1.1.1.3.3.6">𝐾</ci><ci id="alg1.l11.m1.1.1.3.3.7.cmml" xref="alg1.l11.m1.1.1.3.3.7">_</ci><ci id="alg1.l11.m1.1.1.3.3.8.cmml" xref="alg1.l11.m1.1.1.3.3.8">𝑆</ci><ci id="alg1.l11.m1.1.1.3.3.9.cmml" xref="alg1.l11.m1.1.1.3.3.9">𝐼</ci><ci id="alg1.l11.m1.1.1.3.3.10.cmml" xref="alg1.l11.m1.1.1.3.3.10">𝑍</ci><ci id="alg1.l11.m1.1.1.3.3.11.cmml" xref="alg1.l11.m1.1.1.3.3.11">𝐸</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m1.1c">io\_ptr\leftarrow io\_ptr-CHUNK\_SIZE</annotation><annotation encoding="application/x-llamapun" id="alg1.l11.m1.1d">italic_i italic_o _ italic_p italic_t italic_r ← italic_i italic_o _ italic_p italic_t italic_r - italic_C italic_H italic_U italic_N italic_K _ italic_S italic_I italic_Z italic_E</annotation></semantics></math>
     
</div>
<div class="ltx_listingline" id="alg1.l12">
<span class="ltx_tag ltx_tag_listingline">12:</span>Initialize CPU Memory, <math alttext="compute\_ptr=0" class="ltx_Math" display="inline" id="alg1.l12.m1.1"><semantics id="alg1.l12.m1.1a"><mrow id="alg1.l12.m1.1.1" xref="alg1.l12.m1.1.1.cmml"><mrow id="alg1.l12.m1.1.1.2" xref="alg1.l12.m1.1.1.2.cmml"><mi id="alg1.l12.m1.1.1.2.2" xref="alg1.l12.m1.1.1.2.2.cmml">c</mi><mo id="alg1.l12.m1.1.1.2.1" xref="alg1.l12.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l12.m1.1.1.2.3" xref="alg1.l12.m1.1.1.2.3.cmml">o</mi><mo id="alg1.l12.m1.1.1.2.1a" xref="alg1.l12.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l12.m1.1.1.2.4" xref="alg1.l12.m1.1.1.2.4.cmml">m</mi><mo id="alg1.l12.m1.1.1.2.1b" xref="alg1.l12.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l12.m1.1.1.2.5" xref="alg1.l12.m1.1.1.2.5.cmml">p</mi><mo id="alg1.l12.m1.1.1.2.1c" xref="alg1.l12.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l12.m1.1.1.2.6" xref="alg1.l12.m1.1.1.2.6.cmml">u</mi><mo id="alg1.l12.m1.1.1.2.1d" xref="alg1.l12.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l12.m1.1.1.2.7" xref="alg1.l12.m1.1.1.2.7.cmml">t</mi><mo id="alg1.l12.m1.1.1.2.1e" xref="alg1.l12.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l12.m1.1.1.2.8" xref="alg1.l12.m1.1.1.2.8.cmml">e</mi><mo id="alg1.l12.m1.1.1.2.1f" xref="alg1.l12.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l12.m1.1.1.2.9" mathvariant="normal" xref="alg1.l12.m1.1.1.2.9.cmml">_</mi><mo id="alg1.l12.m1.1.1.2.1g" xref="alg1.l12.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l12.m1.1.1.2.10" xref="alg1.l12.m1.1.1.2.10.cmml">p</mi><mo id="alg1.l12.m1.1.1.2.1h" xref="alg1.l12.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l12.m1.1.1.2.11" xref="alg1.l12.m1.1.1.2.11.cmml">t</mi><mo id="alg1.l12.m1.1.1.2.1i" xref="alg1.l12.m1.1.1.2.1.cmml">⁢</mo><mi id="alg1.l12.m1.1.1.2.12" xref="alg1.l12.m1.1.1.2.12.cmml">r</mi></mrow><mo id="alg1.l12.m1.1.1.1" xref="alg1.l12.m1.1.1.1.cmml">=</mo><mn id="alg1.l12.m1.1.1.3" xref="alg1.l12.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="alg1.l12.m1.1b"><apply id="alg1.l12.m1.1.1.cmml" xref="alg1.l12.m1.1.1"><eq id="alg1.l12.m1.1.1.1.cmml" xref="alg1.l12.m1.1.1.1"></eq><apply id="alg1.l12.m1.1.1.2.cmml" xref="alg1.l12.m1.1.1.2"><times id="alg1.l12.m1.1.1.2.1.cmml" xref="alg1.l12.m1.1.1.2.1"></times><ci id="alg1.l12.m1.1.1.2.2.cmml" xref="alg1.l12.m1.1.1.2.2">𝑐</ci><ci id="alg1.l12.m1.1.1.2.3.cmml" xref="alg1.l12.m1.1.1.2.3">𝑜</ci><ci id="alg1.l12.m1.1.1.2.4.cmml" xref="alg1.l12.m1.1.1.2.4">𝑚</ci><ci id="alg1.l12.m1.1.1.2.5.cmml" xref="alg1.l12.m1.1.1.2.5">𝑝</ci><ci id="alg1.l12.m1.1.1.2.6.cmml" xref="alg1.l12.m1.1.1.2.6">𝑢</ci><ci id="alg1.l12.m1.1.1.2.7.cmml" xref="alg1.l12.m1.1.1.2.7">𝑡</ci><ci id="alg1.l12.m1.1.1.2.8.cmml" xref="alg1.l12.m1.1.1.2.8">𝑒</ci><ci id="alg1.l12.m1.1.1.2.9.cmml" xref="alg1.l12.m1.1.1.2.9">_</ci><ci id="alg1.l12.m1.1.1.2.10.cmml" xref="alg1.l12.m1.1.1.2.10">𝑝</ci><ci id="alg1.l12.m1.1.1.2.11.cmml" xref="alg1.l12.m1.1.1.2.11">𝑡</ci><ci id="alg1.l12.m1.1.1.2.12.cmml" xref="alg1.l12.m1.1.1.2.12">𝑟</ci></apply><cn id="alg1.l12.m1.1.1.3.cmml" type="integer" xref="alg1.l12.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l12.m1.1c">compute\_ptr=0</annotation><annotation encoding="application/x-llamapun" id="alg1.l12.m1.1d">italic_c italic_o italic_m italic_p italic_u italic_t italic_e _ italic_p italic_t italic_r = 0</annotation></semantics></math>, <math alttext="io\_ptr=total\_tokens-1" class="ltx_Math" display="inline" id="alg1.l12.m2.1"><semantics id="alg1.l12.m2.1a"><mrow id="alg1.l12.m2.1.1" xref="alg1.l12.m2.1.1.cmml"><mrow id="alg1.l12.m2.1.1.2" xref="alg1.l12.m2.1.1.2.cmml"><mi id="alg1.l12.m2.1.1.2.2" xref="alg1.l12.m2.1.1.2.2.cmml">i</mi><mo id="alg1.l12.m2.1.1.2.1" xref="alg1.l12.m2.1.1.2.1.cmml">⁢</mo><mi id="alg1.l12.m2.1.1.2.3" xref="alg1.l12.m2.1.1.2.3.cmml">o</mi><mo id="alg1.l12.m2.1.1.2.1a" xref="alg1.l12.m2.1.1.2.1.cmml">⁢</mo><mi id="alg1.l12.m2.1.1.2.4" mathvariant="normal" xref="alg1.l12.m2.1.1.2.4.cmml">_</mi><mo id="alg1.l12.m2.1.1.2.1b" xref="alg1.l12.m2.1.1.2.1.cmml">⁢</mo><mi id="alg1.l12.m2.1.1.2.5" xref="alg1.l12.m2.1.1.2.5.cmml">p</mi><mo id="alg1.l12.m2.1.1.2.1c" xref="alg1.l12.m2.1.1.2.1.cmml">⁢</mo><mi id="alg1.l12.m2.1.1.2.6" xref="alg1.l12.m2.1.1.2.6.cmml">t</mi><mo id="alg1.l12.m2.1.1.2.1d" xref="alg1.l12.m2.1.1.2.1.cmml">⁢</mo><mi id="alg1.l12.m2.1.1.2.7" xref="alg1.l12.m2.1.1.2.7.cmml">r</mi></mrow><mo id="alg1.l12.m2.1.1.1" xref="alg1.l12.m2.1.1.1.cmml">=</mo><mrow id="alg1.l12.m2.1.1.3" xref="alg1.l12.m2.1.1.3.cmml"><mrow id="alg1.l12.m2.1.1.3.2" xref="alg1.l12.m2.1.1.3.2.cmml"><mi id="alg1.l12.m2.1.1.3.2.2" xref="alg1.l12.m2.1.1.3.2.2.cmml">t</mi><mo id="alg1.l12.m2.1.1.3.2.1" xref="alg1.l12.m2.1.1.3.2.1.cmml">⁢</mo><mi id="alg1.l12.m2.1.1.3.2.3" xref="alg1.l12.m2.1.1.3.2.3.cmml">o</mi><mo id="alg1.l12.m2.1.1.3.2.1a" xref="alg1.l12.m2.1.1.3.2.1.cmml">⁢</mo><mi id="alg1.l12.m2.1.1.3.2.4" xref="alg1.l12.m2.1.1.3.2.4.cmml">t</mi><mo id="alg1.l12.m2.1.1.3.2.1b" xref="alg1.l12.m2.1.1.3.2.1.cmml">⁢</mo><mi id="alg1.l12.m2.1.1.3.2.5" xref="alg1.l12.m2.1.1.3.2.5.cmml">a</mi><mo id="alg1.l12.m2.1.1.3.2.1c" xref="alg1.l12.m2.1.1.3.2.1.cmml">⁢</mo><mi id="alg1.l12.m2.1.1.3.2.6" xref="alg1.l12.m2.1.1.3.2.6.cmml">l</mi><mo id="alg1.l12.m2.1.1.3.2.1d" xref="alg1.l12.m2.1.1.3.2.1.cmml">⁢</mo><mi id="alg1.l12.m2.1.1.3.2.7" mathvariant="normal" xref="alg1.l12.m2.1.1.3.2.7.cmml">_</mi><mo id="alg1.l12.m2.1.1.3.2.1e" xref="alg1.l12.m2.1.1.3.2.1.cmml">⁢</mo><mi id="alg1.l12.m2.1.1.3.2.8" xref="alg1.l12.m2.1.1.3.2.8.cmml">t</mi><mo id="alg1.l12.m2.1.1.3.2.1f" xref="alg1.l12.m2.1.1.3.2.1.cmml">⁢</mo><mi id="alg1.l12.m2.1.1.3.2.9" xref="alg1.l12.m2.1.1.3.2.9.cmml">o</mi><mo id="alg1.l12.m2.1.1.3.2.1g" xref="alg1.l12.m2.1.1.3.2.1.cmml">⁢</mo><mi id="alg1.l12.m2.1.1.3.2.10" xref="alg1.l12.m2.1.1.3.2.10.cmml">k</mi><mo id="alg1.l12.m2.1.1.3.2.1h" xref="alg1.l12.m2.1.1.3.2.1.cmml">⁢</mo><mi id="alg1.l12.m2.1.1.3.2.11" xref="alg1.l12.m2.1.1.3.2.11.cmml">e</mi><mo id="alg1.l12.m2.1.1.3.2.1i" xref="alg1.l12.m2.1.1.3.2.1.cmml">⁢</mo><mi id="alg1.l12.m2.1.1.3.2.12" xref="alg1.l12.m2.1.1.3.2.12.cmml">n</mi><mo id="alg1.l12.m2.1.1.3.2.1j" xref="alg1.l12.m2.1.1.3.2.1.cmml">⁢</mo><mi id="alg1.l12.m2.1.1.3.2.13" xref="alg1.l12.m2.1.1.3.2.13.cmml">s</mi></mrow><mo id="alg1.l12.m2.1.1.3.1" xref="alg1.l12.m2.1.1.3.1.cmml">−</mo><mn id="alg1.l12.m2.1.1.3.3" xref="alg1.l12.m2.1.1.3.3.cmml">1</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l12.m2.1b"><apply id="alg1.l12.m2.1.1.cmml" xref="alg1.l12.m2.1.1"><eq id="alg1.l12.m2.1.1.1.cmml" xref="alg1.l12.m2.1.1.1"></eq><apply id="alg1.l12.m2.1.1.2.cmml" xref="alg1.l12.m2.1.1.2"><times id="alg1.l12.m2.1.1.2.1.cmml" xref="alg1.l12.m2.1.1.2.1"></times><ci id="alg1.l12.m2.1.1.2.2.cmml" xref="alg1.l12.m2.1.1.2.2">𝑖</ci><ci id="alg1.l12.m2.1.1.2.3.cmml" xref="alg1.l12.m2.1.1.2.3">𝑜</ci><ci id="alg1.l12.m2.1.1.2.4.cmml" xref="alg1.l12.m2.1.1.2.4">_</ci><ci id="alg1.l12.m2.1.1.2.5.cmml" xref="alg1.l12.m2.1.1.2.5">𝑝</ci><ci id="alg1.l12.m2.1.1.2.6.cmml" xref="alg1.l12.m2.1.1.2.6">𝑡</ci><ci id="alg1.l12.m2.1.1.2.7.cmml" xref="alg1.l12.m2.1.1.2.7">𝑟</ci></apply><apply id="alg1.l12.m2.1.1.3.cmml" xref="alg1.l12.m2.1.1.3"><minus id="alg1.l12.m2.1.1.3.1.cmml" xref="alg1.l12.m2.1.1.3.1"></minus><apply id="alg1.l12.m2.1.1.3.2.cmml" xref="alg1.l12.m2.1.1.3.2"><times id="alg1.l12.m2.1.1.3.2.1.cmml" xref="alg1.l12.m2.1.1.3.2.1"></times><ci id="alg1.l12.m2.1.1.3.2.2.cmml" xref="alg1.l12.m2.1.1.3.2.2">𝑡</ci><ci id="alg1.l12.m2.1.1.3.2.3.cmml" xref="alg1.l12.m2.1.1.3.2.3">𝑜</ci><ci id="alg1.l12.m2.1.1.3.2.4.cmml" xref="alg1.l12.m2.1.1.3.2.4">𝑡</ci><ci id="alg1.l12.m2.1.1.3.2.5.cmml" xref="alg1.l12.m2.1.1.3.2.5">𝑎</ci><ci id="alg1.l12.m2.1.1.3.2.6.cmml" xref="alg1.l12.m2.1.1.3.2.6">𝑙</ci><ci id="alg1.l12.m2.1.1.3.2.7.cmml" xref="alg1.l12.m2.1.1.3.2.7">_</ci><ci id="alg1.l12.m2.1.1.3.2.8.cmml" xref="alg1.l12.m2.1.1.3.2.8">𝑡</ci><ci id="alg1.l12.m2.1.1.3.2.9.cmml" xref="alg1.l12.m2.1.1.3.2.9">𝑜</ci><ci id="alg1.l12.m2.1.1.3.2.10.cmml" xref="alg1.l12.m2.1.1.3.2.10">𝑘</ci><ci id="alg1.l12.m2.1.1.3.2.11.cmml" xref="alg1.l12.m2.1.1.3.2.11">𝑒</ci><ci id="alg1.l12.m2.1.1.3.2.12.cmml" xref="alg1.l12.m2.1.1.3.2.12">𝑛</ci><ci id="alg1.l12.m2.1.1.3.2.13.cmml" xref="alg1.l12.m2.1.1.3.2.13">𝑠</ci></apply><cn id="alg1.l12.m2.1.1.3.3.cmml" type="integer" xref="alg1.l12.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l12.m2.1c">io\_ptr=total\_tokens-1</annotation><annotation encoding="application/x-llamapun" id="alg1.l12.m2.1d">italic_i italic_o _ italic_p italic_t italic_r = italic_t italic_o italic_t italic_a italic_l _ italic_t italic_o italic_k italic_e italic_n italic_s - 1</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l13">
<span class="ltx_tag ltx_tag_listingline">13:</span>Start <span class="ltx_text ltx_font_smallcaps" id="alg1.l13.1">ComputeKV</span> in a new thread

</div>
<div class="ltx_listingline" id="alg1.l14">
<span class="ltx_tag ltx_tag_listingline">14:</span>Start <span class="ltx_text ltx_font_smallcaps" id="alg1.l14.1">FetchKV</span> in a new thread

</div>
<div class="ltx_listingline" id="alg1.l15">
<span class="ltx_tag ltx_tag_listingline">15:</span>Wait for both threads to complete

</div>
<div class="ltx_listingline" id="alg1.l16">
<span class="ltx_tag ltx_tag_listingline">16:</span><span class="ltx_text ltx_font_bold" id="alg1.l16.1">return</span> <span class="ltx_text" id="alg1.l16.2">KV cache</span> for the entire sequence

</div>
</div>
</figure>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Implementation</h2>
<div class="ltx_para ltx_noindent" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">We implement <span class="ltx_text ltx_font_italic" id="A2.p1.1.1">Cake</span> by extending LMCache <cite class="ltx_cite ltx_citemacro_citep">(LMCache, <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib28" title="">2024</a>)</cite> and integrating it with vLLM <cite class="ltx_cite ltx_citemacro_citep">(Kwon et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib22" title="">2023</a>)</cite>, adding approximately 1,000 lines of code.</p>
</div>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Enhancements to LMCache</h3>
<div class="ltx_para ltx_noindent" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1">LMCache, originally developed as the <span class="ltx_text" id="A2.SS1.p1.1.1">KV cache</span> management backend for CacheGen <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#bib.bib27" title="">2023</a>)</cite>, hashes token chunks into keys for efficient <span class="ltx_text" id="A2.SS1.p1.1.2">KV cache</span> retrieval. To enable <span class="ltx_text ltx_font_italic" id="A2.SS1.p1.1.3">Cake</span> to continuously receive <span class="ltx_text" id="A2.SS1.p1.1.4">KV cache</span> in the background, we introduce the following enhancements:</p>
</div>
<section class="ltx_paragraph" id="A2.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Asynchronous Retrieval</h4>
<div class="ltx_para ltx_noindent" id="A2.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="A2.SS1.SSS0.Px1.p1.1">We implement an asynchronous get operation to complement LMCache’s existing asynchronous put functionality. This involves creating a dedicated worker thread that continuously reads chunk keys from a task queue and retrieves the corresponding <span class="ltx_text" id="A2.SS1.SSS0.Px1.p1.1.1">KV cache</span> to memory. Upon successful retrieval, the chunk’s key is added to a resident dictionary for quick access.</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Buffer Preallocation</h4>
<div class="ltx_para ltx_noindent" id="A2.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="A2.SS1.SSS0.Px2.p1.1">We modify LMCache to preallocate chunk buffers as soon as a chunk key is pushed to the queue. This optimization allows the worker to immediately write received <span class="ltx_text" id="A2.SS1.SSS0.Px2.p1.1.1">KV cache</span> into memory and proceed to the next chunk without delay.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Integration with LLM Serving Systems</h3>
<div class="ltx_para ltx_noindent" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.1"><span class="ltx_text ltx_font_italic" id="A2.SS2.p1.1.1">Cake</span> operates concurrently with LLM serving systems like vLLM. The integration process works as follows:</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS2.p2">
<p class="ltx_p" id="A2.SS2.p2.1">1. Upon receiving a request, <span class="ltx_text ltx_font_italic" id="A2.SS2.p2.1.1">Cake</span> divides it into chunks based on the scheduled token budget.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS2.p3">
<p class="ltx_p" id="A2.SS2.p3.1">2. Hashed keys for these chunks are pushed to the task queue in reverse order using the <span class="ltx_text ltx_font_italic" id="A2.SS2.p3.1.1">push_seq</span> API.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS2.p4">
<p class="ltx_p" id="A2.SS2.p4.1">3. While the asynchronous get worker fetches <span class="ltx_text" id="A2.SS2.p4.1.1">KV cache</span> from the end of the sequence, vLLM begins chunk prefill from the start.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS2.p5">
<p class="ltx_p" id="A2.SS2.p5.1">4. After each vLLM engine step, <span class="ltx_text ltx_font_italic" id="A2.SS2.p5.1.1">Cake</span> checks if the next chunk of tokens is already in the resident dictionary using the <span class="ltx_text ltx_font_italic" id="A2.SS2.p5.1.2">is_resident</span> API.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS2.p6">
<p class="ltx_p" id="A2.SS2.p6.1">5. If the chunk is resident, <span class="ltx_text ltx_font_italic" id="A2.SS2.p6.1.1">Cake</span> interrupts the chunk prefill process and directs vLLM to begin token generation.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS2.p7">
<p class="ltx_p" id="A2.SS2.p7.1">6. If the chunk is not resident, chunk prefill continues until it encounters a chunk present in the dictionary.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS2.p8">
<p class="ltx_p" id="A2.SS2.p8.1">This bidirectional approach allows <span class="ltx_text ltx_font_italic" id="A2.SS2.p8.1.1">Cake</span> to efficiently utilize both I/O and computational resources, potentially reducing the Time To First Token (TTFT) for long-context LLM inference tasks.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Performance of <span class="ltx_text ltx_font_italic" id="A3.1.1">Cake</span> under different conditions</h2>
<div class="ltx_para ltx_noindent" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">As shown in Figures <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#A3.F9" title="Figure 9 ‣ Appendix C Performance of Cake under different conditions ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">9</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#A3.F10" title="Figure 10 ‣ Appendix C Performance of Cake under different conditions ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">10</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#A3.F11" title="Figure 11 ‣ Appendix C Performance of Cake under different conditions ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">11</span></a>, we test <span class="ltx_text ltx_font_italic" id="A3.p1.1.1">Cake</span> on Alpaca-13B model, which will generate larger <span class="ltx_text" id="A3.p1.1.2">KV cache</span> than Alpaca-7B. Selected diagrams includes 5/9/14k context lengths with 10/50/90% of GPU resources. Bandwidth we test ranges from 2000 mbps to 32000 mbps. The results show <span class="ltx_text ltx_font_italic" id="A3.p1.1.3">Cake</span> is still able to effectively leverage both computation and I/O and achieves the lowest TTFT.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.p2">
<p class="ltx_p" id="A3.p2.1">As shown in Figures <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#A3.F12" title="Figure 12 ‣ Appendix C Performance of Cake under different conditions ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">12</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#A3.F13" title="Figure 13 ‣ Appendix C Performance of Cake under different conditions ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">13</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.03065v1#A3.F14" title="Figure 14 ‣ Appendix C Performance of Cake under different conditions ‣ Compute or Load KV Cache? Why not both?"><span class="ltx_text ltx_ref_tag">14</span></a>, we compare the performance of <span class="ltx_text ltx_font_italic" id="A3.p2.1.1">Cake</span> with compute-only and I/O-only methods. We also incorporate 8bits-quantization and CacheGen into <span class="ltx_text ltx_font_italic" id="A3.p2.1.2">Cake</span>, which further boost our performance. Selected diagrams includes 5/9/14k context lengths with 10/50/90/100% of GPU resources. Bandwidth we test ranges from 2000 mbps to 10000 mbps. Across all the conditions, <span class="ltx_text ltx_font_italic" id="A3.p2.1.3">Cake</span> with CacheGen produces the best performance with the lowest TTFT.</p>
</div>
<figure class="ltx_figure" id="A3.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="A3.F9.1" style="width:125.7pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="704" id="A3.F9.1.g1" src="x13.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Computation power 90%</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="A3.F9.2" style="width:121.4pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="724" id="A3.F9.2.g1" src="x14.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Computation power 50%</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="A3.F9.3" style="width:169.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="526" id="A3.F9.3.g1" src="x15.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Computation power 10%</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>TTFT of prefilling 14k context of Alpaca-13B on a single A100.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F10">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="A3.F10.1" style="width:125.7pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="704" id="A3.F10.1.g1" src="x16.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Computation power 90%</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="A3.F10.2" style="width:121.4pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="724" id="A3.F10.2.g1" src="x17.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Computation power 50%</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="A3.F10.3" style="width:169.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="526" id="A3.F10.3.g1" src="x18.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Computation power 10%</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>TTFT of prefilling 9k context of Alpaca-13B on a single A100.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F11">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="A3.F11.1" style="width:125.7pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="704" id="A3.F11.1.g1" src="x19.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Computation power 90%</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="A3.F11.2" style="width:121.4pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="735" id="A3.F11.2.g1" src="x20.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Computation power 50%</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="A3.F11.3" style="width:169.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="526" id="A3.F11.3.g1" src="x21.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Computation power 10%</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>TTFT of prefilling 5k context of Alpaca-13B on a single A100.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F12">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="A3.F12.1" style="width:182.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="525" id="A3.F12.1.g1" src="x22.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Computation power 100%</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="A3.F12.2" style="width:247.2pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="388" id="A3.F12.2.g1" src="x23.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Computation power 90%</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="A3.F12.3" style="width:182.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="525" id="A3.F12.3.g1" src="x24.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Computation power 50%</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="A3.F12.4" style="width:247.2pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="388" id="A3.F12.4.g1" src="x25.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(d) </span>Computation power 10%</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>TTFT of prefilling 14k context of Alpaca-7B on a single A100.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F13">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="A3.F13.1" style="width:182.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="525" id="A3.F13.1.g1" src="x26.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Computation power 100%</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="A3.F13.2" style="width:247.2pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="388" id="A3.F13.2.g1" src="x27.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Computation power 90%</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="A3.F13.3" style="width:182.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="525" id="A3.F13.3.g1" src="x28.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Computation power 50%</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="A3.F13.4" style="width:247.2pt;"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="388" id="A3.F13.4.g1" src="x29.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(d) </span>Computation power 10%</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>TTFT of prefilling 9k context of Alpaca-7B on a single A100.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F14">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="A3.F14.1" style="width:182.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="525" id="A3.F14.1.g1" src="x30.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Computation power 100%</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="A3.F14.2" style="width:247.2pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="388" id="A3.F14.2.g1" src="x31.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Computation power 90%</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="A3.F14.3" style="width:182.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="525" id="A3.F14.3.g1" src="x32.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Computation power 50%</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="A3.F14.4" style="width:247.2pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="388" id="A3.F14.4.g1" src="x33.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(d) </span>Computation power 10%</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>TTFT of prefilling 5k context of Alpaca-7B on a single A100.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Oct  4 01:10:09 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
