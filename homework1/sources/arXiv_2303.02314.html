<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2303.02314] Virtual Sparse Convolution for Multimodal 3D Object Detection</title><meta property="og:description" content="Recently, virtual/pseudo-point-based 3D object detection that seamlessly fuses RGB images and LiDAR data by depth completion has gained great attention. However, virtual points generated from an image are very dense, i…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Virtual Sparse Convolution for Multimodal 3D Object Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Virtual Sparse Convolution for Multimodal 3D Object Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2303.02314">

<!--Generated on Thu Feb 29 21:49:41 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Virtual Sparse Convolution for Multimodal 3D Object Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hai Wu<sup id="id9.9.id1" class="ltx_sup">1</sup>  Chenglu Wen<sup id="id10.10.id2" class="ltx_sup"><span id="id10.10.id2.1" class="ltx_text ltx_font_italic">1</span></sup>  Shaoshuai Shi<sup id="id11.11.id3" class="ltx_sup">2</sup>  Xin Li<sup id="id12.12.id4" class="ltx_sup">3</sup>  Cheng Wang<sup id="id13.13.id5" class="ltx_sup">1</sup> 
<br class="ltx_break"><sup id="id14.14.id6" class="ltx_sup">1</sup>Xiamen University  <sup id="id15.15.id7" class="ltx_sup">2</sup>Max-Planck Institute  <sup id="id16.16.id8" class="ltx_sup">3</sup>Texas A&amp;M University
<br class="ltx_break">
</span><span class="ltx_author_notes">Corresponding author</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id17.id1" class="ltx_p">Recently, virtual/pseudo-point-based 3D object detection that seamlessly fuses RGB images and LiDAR data by depth completion has gained great attention. However, virtual points generated from an image are very dense, introducing a huge amount of redundant computation during detection. Meanwhile, noises brought by inaccurate depth completion significantly degrade detection precision. This paper proposes a fast yet effective backbone, termed <span id="id17.id1.1" class="ltx_text ltx_font_bold">VirConvNet</span>, based on a new operator <span id="id17.id1.2" class="ltx_text ltx_font_bold">VirConv</span> (Virtual Sparse Convolution), for virtual-point-based 3D object detection. VirConv consists of two key designs: (1) <span id="id17.id1.3" class="ltx_text ltx_font_bold">StVD</span> (Stochastic Voxel Discard) and (2) <span id="id17.id1.4" class="ltx_text ltx_font_bold">NRConv</span> (Noise-Resistant Submanifold Convolution). StVD alleviates the computation problem by discarding large amounts of nearby redundant voxels. NRConv tackles the noise problem by encoding voxel features in both 2D image and 3D LiDAR space. By integrating VirConv, we first develop an efficient pipeline <span id="id17.id1.5" class="ltx_text ltx_font_bold">VirConv-L</span> based on an early fusion design. Then, we build a high-precision pipeline <span id="id17.id1.6" class="ltx_text ltx_font_bold">VirConv-T</span> based on a transformed refinement scheme. Finally, we develop a semi-supervised pipeline <span id="id17.id1.7" class="ltx_text ltx_font_bold">VirConv-S</span> based on a pseudo-label framework. On the KITTI car 3D detection test leaderboard, our VirConv-L achieves <span id="id17.id1.8" class="ltx_text ltx_font_bold">85% AP</span> with a fast running speed of <span id="id17.id1.9" class="ltx_text ltx_font_bold">56ms</span>. Our VirConv-T and VirConv-S attains a high-precision of <span id="id17.id1.10" class="ltx_text ltx_font_bold">86.3%</span> and <span id="id17.id1.11" class="ltx_text ltx_font_bold">87.2% AP</span>, and currently <span id="id17.id1.12" class="ltx_text ltx_font_bold">rank 2nd and 1st<span id="footnotex1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span id="footnotex1.1.1.1" class="ltx_text ltx_font_medium">1</span></span><span id="footnotex1.9" class="ltx_text ltx_font_medium">On the date of CVPR deadline, </span><em id="footnotex1.10" class="ltx_emph ltx_font_medium ltx_font_italic">i.e</em><span id="footnotex1.11" class="ltx_text ltx_font_medium">.</span><span id="footnotex1.12" class="ltx_text"></span><span id="footnotex1.13" class="ltx_text ltx_font_medium">, Nov.11, 2022</span></span></span></span></span>, respectively. The code is available at <a target="_blank" href="https://github.com/hailanyi/VirConv" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/hailanyi/VirConv</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2303.02314/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="242" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Our VirConv-T achieves top average precision (AP) on both 3D and BEV moderate car detection in the KITTI benchmark (more details are in Table <a href="#S3.T1" title="Table 1 ‣ 3.3 Noise-Resistant Submanifold Convolution ‣ 3 VirConv for Multimodal 3D Detection ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Our VirConv-L runs fast at 56ms with competitive AP.</span></figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">3D object detection plays a critical role in autonomous driving <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. The LiDAR sensor measures the depth of scene <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> in the form of a point cloud and enables reliable localization of objects in various lighting environments. While LiDAR-based 3D object detection has made rapid progress in recent years <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, its performance drops significantly on distant objects, which inevitably have sparse sampling density in the scans.
Unlike LiDAR scans, color image sensors provide high-resolution sampling and rich context data of the scene.
The RGB image and LiDAR data can complement each other and usually boost 3D detection performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Early methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> extended the features of LiDAR points with image features, such as semantic mask and 2D CNN features. They did not increase the number of points; thus, the distant points still remain sparse. In contrast, the methods based on virtual/pseudo points (for simplicity, both denoted as <span id="S1.p2.1.1" class="ltx_text ltx_font_bold">virtual points</span> in the following) enrich the sparse points by creating additional points around the LiDAR points. For example, MVP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> creates the virtual points by completing the depth of 2D instance points from the nearest 3D points. SFD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> creates the virtual points based on depth completion networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. The virtual points complete the geometry of distant objects, showing the great potential for high-performance 3D detection.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.3" class="ltx_p">However, virtual points generated from an image are generally very dense. Taking the KITTI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> dataset as an example, an 1242<math id="S1.p3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.p3.1.m1.1a"><mo id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><times id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">\times</annotation></semantics></math>375 image generates 466k virtual points (<math id="S1.p3.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S1.p3.2.m2.1a"><mo id="S1.p3.2.m2.1.1" xref="S1.p3.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S1.p3.2.m2.1b"><csymbol cd="latexml" id="S1.p3.2.m2.1.1.cmml" xref="S1.p3.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.2.m2.1c">\sim</annotation></semantics></math><span id="S1.p3.3.1" class="ltx_text ltx_font_bold">27<math id="S1.p3.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.p3.3.1.m1.1a"><mo id="S1.p3.3.1.m1.1.1" xref="S1.p3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p3.3.1.m1.1b"><times id="S1.p3.3.1.m1.1.1.cmml" xref="S1.p3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.3.1.m1.1c">\times</annotation></semantics></math></span> more than the LiDAR scan points). This brings a huge computational burden and causes a severe efficiency issue (see Fig. <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (f)). Previous work addresses the density problem by using a larger voxel size <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> or by randomly down-sampling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> the points. However, applying such methods to virtual points will inevitably sacrifice useful shape cues from faraway points and result in decreased detection accuracy.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2303.02314/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="299" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.3.2" class="ltx_text" style="font-size:90%;">The noise problem and density problem of virtual points. (a) Virtual points in 3D space. (b) Virtual points in 2D space. (c) Noises (red) in 3D space. (d) Noises (red) distributed on 2D instance boundaries. (e) Virtual points number versus AP improvement along different distances by using Voxel-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> with late fusion (details see Sec. <a href="#S3.SS1" title="3.1 Virtual Points for Data Fusion ‣ 3 VirConv for Multimodal 3D Detection ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>). (f) Car 3D AP and inference time using Voxel-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> with LiDAR-only, virtual points-only, early fusion, and late fusion (details see Sec. <a href="#S3.SS1" title="3.1 Virtual Points for Data Fusion ‣ 3 VirConv for Multimodal 3D Detection ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>), respectively.</span></figcaption>
</figure>
<figure id="S1.F3" class="ltx_figure"><img src="/html/2303.02314/assets/x3.png" id="S1.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="187" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S1.F3.3.2" class="ltx_text" style="font-size:90%;">(a) VirConv block consists of a StVD layer, some NRConv layers and a 3D SpConv layer. (b) NRConv projects the voxels back to image space, and encodes virtual point features in both 2D and 3D space. (c) VirConv-L fuses the LiDAR points and the virtual points into a single point cloud, and encodes the multimodal features by our VirConvNet for 3D detection. </span></figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Another issue is that the depth completion can be inaccurate, and it brings a large amount of noise in the virtual points (see Fig. <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (c)).
Since it is very difficult to distinguish the noises from the background in 3D space, the localization precision of 3D detection is greatly degraded.
In addition, the noisy points are non-Gaussian distributed, and can not be filtered by conventional denoising algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
Although recent semantic segmentation network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> show promising results, they generally require extra annotations.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">To address these issues, this paper proposes a VirConvNet pipeline based on a new Virtual Sparse Convolution (VirConv) operator.
Our design builds on <span id="S1.p5.1.1" class="ltx_text ltx_font_bold">two main observations</span>.
(1) First, geometries of nearby objects are often relatively complete in LiDAR scans. Hence, most virtual points of nearby objects only bring marginal performance gain (see Fig. <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (e)(f)), but increase the computational cost significantly.
(2) Second, noisy points introduced by inaccurate depth completions are mostly distributed on the instance boundaries (see Fig. <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (d)).
They can be recognized in 2D images after being projected onto the image plane.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Based on these two observations, we design a <span id="S1.p6.1.1" class="ltx_text ltx_font_bold">StVD</span> (Stochastic Voxel Discard) scheme to retain those most important virtual points by a bin-based sampling, namely, discarding a huge number of nearby voxels while retaining faraway voxels. This can greatly speed up the network computation.
We also design a <span id="S1.p6.1.2" class="ltx_text ltx_font_bold">NRConv</span> (Noise-Resistant Submanifold Convolution) layer to encode geometry features of voxels in both 3D space and 2D image space.
The extended receptive field in 2D space allows our NRConv to distinguish the noise pattern on the instance boundaries in 2D image space. Consequently, the negative impact of noise can be suppressed.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">We develop three multimodal detectors to demonstrate the superiority of our VirConv: (1) a lightweight <span id="S1.p7.1.1" class="ltx_text ltx_font_bold">VirConv-L</span> constructed from Voxel-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>; (2) a high-precision <span id="S1.p7.1.2" class="ltx_text ltx_font_bold">VirConv-T</span> based on multi-stage 
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and multi-transformation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> design; (3) a semi-supervised <span id="S1.p7.1.3" class="ltx_text ltx_font_bold">VirConv-S</span> based on a pseudo-label <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> framework. The effectiveness of our design is verified by extensive experiments on the widely used KITTI dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and nuScenes dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
Our contributions are summarized as follows:</p>
</div>
<div id="S1.p8" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose a <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">VirConv</span> operator, which effectively encodes voxel features of virtual points by <span id="S1.I1.i1.p1.1.2" class="ltx_text ltx_font_bold">StVD</span> and <span id="S1.I1.i1.p1.1.3" class="ltx_text ltx_font_bold">NRConv</span>. The StVD discards a huge number of redundant voxels and substantially speeds up the 3D detection prominently. The NRConv extends the receptive field of 3D sparse convolution to the 2D image space and significantly reduces the impact of noisy points.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Built upon VirConv, we present three new multimodal detectors: a <span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">VirConv-L</span>, a <span id="S1.I1.i2.p1.1.2" class="ltx_text ltx_font_bold">VirConv-T</span>, and a semi-supervised <span id="S1.I1.i2.p1.1.3" class="ltx_text ltx_font_bold">VirConv-S</span> for efficient, high-precision, and semi-supervised 3D detection, respectively.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Extensive experiments demonstrated the effectiveness of our design (see Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). On the KITTI leaderboard, our VirConv-T and VirConv-S currently <span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">rank 2nd and 1st</span>, respectively. Our VirConv-L runs at <span id="S1.I1.i3.p1.1.2" class="ltx_text ltx_font_bold">56ms</span> with competitive precision.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">LiDAR-based 3D object detection.</span>
LiDAR-based 3D object detection has been widely studied in recent years. Early methods project the point clouds into a 2D Bird’s eye view (BEV) or range view images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> for 3D detection. Recently, voxel-based sparse convolution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and point-based set abstraction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> have become popular in designing effective detection frameworks.
However, the scanning resolution of LiDAR is generally very low for distant objects. The LiDAR-only detectors usually suffer from such sparsity. This paper addresses this problem by introducing RGB image data in a form of virtual points.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Multimodal 3D object detection.</span>
The RGB image and LiDAR data can complement each other and usually boost 3D detection performance. Early methods extend the features of LiDAR points with image features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. Some works encode the feature of two modalities independently and fuse the two features in the local Region of Interest (RoI) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> or BEV plane <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. We follow the recent work that fuses the two data via virtual points <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. The virtual points explicitly complete the geometry of distant objects by depth estimation, showing the great potential for high-performance 3D detection. But virtual points are extremely dense and often noisy. This paper addresses these problems through two new schemes, StVD and NRConv, respectively.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">3D object detection with re-sampled point clouds.</span>
The points captured by LiDAR are generally dense and unevenly distributed. Previous work speeds up the network by using a larger voxel size <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> or by randomly down-sampling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> the point clouds. However, applying these methods to the virtual points will significantly decrease the useful geometry cues, especially for the faraway objects. Different from that, our StVD retains all the useful faraway voxels and speeds up the network by discarding nearby redundant voxels.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Noise handling in 3D vision.</span>
Traditional methods handle the noises by filtering algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Recently, score-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> and semantic segmentation networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> are developed for point cloud noise removal. Different from the traditional noises that are randomly distributed in 3D space, the noises brought by inaccurate depth completion are mostly distributed on 2D instance boundaries. Although the noise can be roughly removed by some 2D edge detection method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, this will sacrifice the useful boundary
points of object. We design a new scheme, NRConv, that extends the receptive field of 3D sparse convolution to 2D image space, distinguishing the noise pattern without the loss of useful boundary points.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p"><span id="S2.p5.1.1" class="ltx_text ltx_font_bold">Semi-supervised 3D object detection.</span>
Recent semi-supervised methods boost 3D object detection by a large amount of unlabeled data. Inspired by the pseudo-label-based framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, we also constructed a VirConv-S pipeline to perform semi-supervised multimodal 3D object detection.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>VirConv for Multimodal 3D Detection</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.4" class="ltx_p">This paper proposes VirConvNet, based on a new VirConv operator, for virtual-point-based multimodal 3D object detection.
As shown in Fig. <a href="#S1.F3" title="Figure 3 ‣ 1 Introduction ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, VirConvNet first converts points into voxels, and gradually encodes voxels into feature volumes by a series of VirConv block with <math id="S3.p1.1.m1.1" class="ltx_math_unparsed" alttext="1\times" display="inline"><semantics id="S3.p1.1.m1.1a"><mrow id="S3.p1.1.m1.1b"><mn id="S3.p1.1.m1.1.1">1</mn><mo lspace="0.222em" id="S3.p1.1.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">1\times</annotation></semantics></math>, <math id="S3.p1.2.m2.1" class="ltx_math_unparsed" alttext="2\times" display="inline"><semantics id="S3.p1.2.m2.1a"><mrow id="S3.p1.2.m2.1b"><mn id="S3.p1.2.m2.1.1">2</mn><mo lspace="0.222em" id="S3.p1.2.m2.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">2\times</annotation></semantics></math>, <math id="S3.p1.3.m3.1" class="ltx_math_unparsed" alttext="4\times" display="inline"><semantics id="S3.p1.3.m3.1a"><mrow id="S3.p1.3.m3.1b"><mn id="S3.p1.3.m3.1.1">4</mn><mo lspace="0.222em" id="S3.p1.3.m3.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">4\times</annotation></semantics></math> and <math id="S3.p1.4.m4.1" class="ltx_math_unparsed" alttext="8\times" display="inline"><semantics id="S3.p1.4.m4.1a"><mrow id="S3.p1.4.m4.1b"><mn id="S3.p1.4.m4.1.1">8</mn><mo lspace="0.222em" id="S3.p1.4.m4.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">8\times</annotation></semantics></math> downsampling strides. The VirConv block consists of three parts (see Fig. <a href="#S1.F3" title="Figure 3 ‣ 1 Introduction ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (a)): (1) an StVD layer for speeding up the network and improving density robustness; (2) multiple NRConv layers for encoding features and decreasing the impact of noise; (3) a 3D SpConv layer for down-sampling the feature map. Based on the VirConv operator, we build three detectors for efficient, accurate, and semi-supervised multimodal 3D detection, respectively.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Virtual Points for Data Fusion</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.7" class="ltx_p">Many recent 3D detectors use virtual points <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> (pseudo points <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>) generated from an image by depth completion algorithms to fuse RGB and LiDAR data.
We denote the LiDAR points and virtual points as <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{P}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">𝐏</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝐏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\mathbf{P}</annotation></semantics></math> and <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{V}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">𝐕</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝐕</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\mathbf{V}</annotation></semantics></math>, respectively.
Recently, two popular fusion schemes have been applied for 3D object detection:
(1) early fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, which fuses <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{P}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">𝐏</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">𝐏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\mathbf{P}</annotation></semantics></math> and <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="\mathbf{V}" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">𝐕</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">𝐕</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">\mathbf{V}</annotation></semantics></math> into a single point cloud <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="\mathbf{P}^{*}" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><msup id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">𝐏</mi><mo id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">superscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">𝐏</ci><times id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">\mathbf{P}^{*}</annotation></semantics></math> and performs 3D object detection using existing detectors, and
(2) late fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, which encodes the features of <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="\mathbf{P}" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><mi id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml">𝐏</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">𝐏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">\mathbf{P}</annotation></semantics></math> and <math id="S3.SS1.p1.7.m7.1" class="ltx_Math" alttext="\mathbf{V}" display="inline"><semantics id="S3.SS1.p1.7.m7.1a"><mi id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml">𝐕</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><ci id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">𝐕</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">\mathbf{V}</annotation></semantics></math> by different backbone networks and fuses the two types of features in BEV plane or local RoI.
However, both fusion methods suffer from the dense and noisy nature of virtual points.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.2" class="ltx_text ltx_font_bold">(1) Density problem.</span>
As motioned in Section <a href="#S1" title="1 Introduction ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the virtual points are usually very dense. They introduce a huge computational burden, which significantly decreases the detection speed (e.g., more than <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">2<math id="S3.SS1.p2.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS1.p2.1.1.m1.1a"><mo id="S3.SS1.p2.1.1.m1.1.1" xref="S3.SS1.p2.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.1.m1.1b"><times id="S3.SS1.p2.1.1.m1.1.1.cmml" xref="S3.SS1.p2.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.1.m1.1c">\times</annotation></semantics></math></span> in Fig. <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (f)).
Existing work tackles the density issue by using a larger voxel size <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> or by randomly down-sampling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> the points. But these methods will inevitably sacrifice the shape cues from the virtual points, especially for the faraway object. Based on a pilot experiment on the KITTI dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> using the Voxel-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> with a late fusion, we observed that a huge number of virtual points introduced for nearby objects are redundant. Specifically, <span id="S3.SS1.p2.1.3" class="ltx_text ltx_font_bold">97%</span> of virtual points from the nearby objects bring only a <span id="S3.SS1.p2.1.4" class="ltx_text ltx_font_bold">0.18%</span> performance improvement, while <span id="S3.SS1.p2.1.5" class="ltx_text ltx_font_bold">3%</span> of virtual points for the faraway objects bring a <span id="S3.SS1.p2.1.6" class="ltx_text ltx_font_bold">2.2%</span> performance improvement.
The reason is that the geometry of nearby objects
is relatively complete for LiDAR points.
Such virtual points generally bring marginal performance gain but increase unnecessary computation. Motivated by this observation, we design an StVD (Stochastic Voxel Discard) scheme, which alleviates the computation problem by discarding nearby redundant voxels. In addition, the points of the distant object are much sparser than the nearby objects (see Fig. <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (e)). The StVD can simulate sparser training samples to improve detection robustness.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">(2) Noise problem.</span>
The virtual points generated by the depth completion network are usually noisy. An example is shown in Fig. <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (c). The noise is mostly introduced by the inaccurate depth completion, and is hardly distinguishable in 3D space. By using only virtual points, the detection performance drops <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mo id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><csymbol cd="latexml" id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">\sim</annotation></semantics></math>9% AP compared with the LiDAR-only detector (see Fig. <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (f)). In addition, the noisy points are non-Gaussian distributed, and cannot be filtered by traditional denoising algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
We observed that noise is mainly distributed on the instance boundaries (see Fig. <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (d)) and can be more easily recognized in 2D images. Although the edge detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> could be applied here to roughly remove the noise, this will sacrifice the useful boundary points which are beneficial to the object’s shape and position estimation. Our idea is to extend the receptive field of sparse convolution to the 2D image space, and distinguish the noise without the loss of shape cues.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Stochastic Voxel Discard</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To alleviate the computation problem and improve the density robustness for the virtual-point-based detector, we develop the StVD. It consists of two parts: (1) input StVD, which speeds up the network by discarding input voxels of virtual points during both the training and inference process; (2) layer StVD, which improves the density robustness by discarding voxels of virtual points at every VirConv block during only the training process.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.5" class="ltx_p"><span id="S3.SS2.p2.5.1" class="ltx_text ltx_font_bold">Input StVD.</span>
Two naive methods can keep less input voxels: (1) random sampling or (2) farthest point sampling (FPS).
However, the random sampling usually keeps unbalanced voxels at different distances and inevitably sacrifices some useful shape cues (in the red region at Fig. <a href="#S3.F4" title="Figure 4 ‣ 3.2 Stochastic Voxel Discard ‣ 3 VirConv for Multimodal 3D Detection ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (a) (b)). In addition, FPS needs huge extra computation when down-sampling the huge number of virtual points due to the high computational complexity (<math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="O(n^{2})" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">​</mo><mrow id="S3.SS2.p2.1.m1.1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p2.1.m1.1.1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.1.1.1.cmml">(</mo><msup id="S3.SS2.p2.1.m1.1.1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.1.1.1.2.cmml">n</mi><mn id="S3.SS2.p2.1.m1.1.1.1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.1.1.1.3.cmml">2</mn></msup><mo stretchy="false" id="S3.SS2.p2.1.m1.1.1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><times id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2"></times><ci id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">𝑂</ci><apply id="S3.SS2.p2.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1">superscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.1.2">𝑛</ci><cn type="integer" id="S3.SS2.p2.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">O(n^{2})</annotation></semantics></math>).
To tackle this problem, we introduce a bin-based sampling strategy to perform efficient and balanced sampling (see Fig. <a href="#S3.F4" title="Figure 4 ‣ 3.2 Stochastic Voxel Discard ‣ 3 VirConv for Multimodal 3D Detection ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (c)).
Specifically, We first divide the input voxels into <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="N^{b}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><msup id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">N</mi><mi id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">b</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">superscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">𝑁</ci><ci id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">N^{b}</annotation></semantics></math> bins (we adopt <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="N^{b}=10" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mrow id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><msup id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2.2" xref="S3.SS2.p2.3.m3.1.1.2.2.cmml">N</mi><mi id="S3.SS2.p2.3.m3.1.1.2.3" xref="S3.SS2.p2.3.m3.1.1.2.3.cmml">b</mi></msup><mo id="S3.SS2.p2.3.m3.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><eq id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1"></eq><apply id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.2.1.cmml" xref="S3.SS2.p2.3.m3.1.1.2">superscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.2.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2.2">𝑁</ci><ci id="S3.SS2.p2.3.m3.1.1.2.3.cmml" xref="S3.SS2.p2.3.m3.1.1.2.3">𝑏</ci></apply><cn type="integer" id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">N^{b}=10</annotation></semantics></math> in this paper) according to different distances. For the nearby bins (<math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="\leq" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mo id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><leq id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">\leq</annotation></semantics></math>30m based on the statistics in Fig. <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (e)), we randomly keep a fixed number (<math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mo id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><csymbol cd="latexml" id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">\sim</annotation></semantics></math> 1K) of voxels. For distant bins, we keep all of the inside voxels. After the bin-based sampling, we discard about <span id="S3.SS2.p2.5.2" class="ltx_text ltx_font_bold">90%</span> (which achieves the best precision-efficiency trade-off, see Fig. <a href="#S4.F6" title="Figure 6 ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>) of redundant voxels and it speeds up the network by about <span id="S3.SS2.p2.5.3" class="ltx_text ltx_font_bold">2 times</span>.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">Layer StVD.</span>
To improve the robustness of detection from sparse points, we also develop a layer StVD which is applied to the training process.
Specifically, we discard voxels at each VirConv block to simulate sparser training samples. We adopt a discarding rate of 15% in this paper (the layer StVD rate is discussed in Fig. <a href="#S4.F6" title="Figure 6 ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). The layer StVD serves as a data augmentation strategy to help enhance the 3D detector’s training.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2303.02314/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="173" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">(a)(b) show the voxel distributions after random sampling for all and nearby voxels, respectively. (c) shows the voxel distribution after bin-based sampling for all voxels.</span></figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Noise-Resistant Submanifold Convolution</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.6" class="ltx_p">As analyzed in Section <a href="#S3.SS1" title="3.1 Virtual Points for Data Fusion ‣ 3 VirConv for Multimodal 3D Detection ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, the noise introduced by the inaccurate depth completion can hardly be recognized from 3D space but can be easily recognized from 2D images. We develop an NRConv (see Fig. <a href="#S1.F3" title="Figure 3 ‣ 1 Introduction ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (b)) from the widely used submanifold sparse convolution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> to address the noise problem. Specifically, given <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">N</annotation></semantics></math> input voxels formulated by a 3D indices vector
<math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{H}\in\mathbb{R}^{N\times 3}" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mrow id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mi id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml">𝐇</mi><mo id="S3.SS3.p1.2.m2.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml"><mi id="S3.SS3.p1.2.m2.1.1.3.2" xref="S3.SS3.p1.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS3.p1.2.m2.1.1.3.3" xref="S3.SS3.p1.2.m2.1.1.3.3.cmml"><mi id="S3.SS3.p1.2.m2.1.1.3.3.2" xref="S3.SS3.p1.2.m2.1.1.3.3.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p1.2.m2.1.1.3.3.1" xref="S3.SS3.p1.2.m2.1.1.3.3.1.cmml">×</mo><mn id="S3.SS3.p1.2.m2.1.1.3.3.3" xref="S3.SS3.p1.2.m2.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><in id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1"></in><ci id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2">𝐇</ci><apply id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.3.1.cmml" xref="S3.SS3.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.3.2.cmml" xref="S3.SS3.p1.2.m2.1.1.3.2">ℝ</ci><apply id="S3.SS3.p1.2.m2.1.1.3.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3.3"><times id="S3.SS3.p1.2.m2.1.1.3.3.1.cmml" xref="S3.SS3.p1.2.m2.1.1.3.3.1"></times><ci id="S3.SS3.p1.2.m2.1.1.3.3.2.cmml" xref="S3.SS3.p1.2.m2.1.1.3.3.2">𝑁</ci><cn type="integer" id="S3.SS3.p1.2.m2.1.1.3.3.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">\mathbf{H}\in\mathbb{R}^{N\times 3}</annotation></semantics></math> and a features vector
<math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{X}\in\mathbb{R}^{N\times C^{in}}" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mrow id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mi id="S3.SS3.p1.3.m3.1.1.2" xref="S3.SS3.p1.3.m3.1.1.2.cmml">𝐗</mi><mo id="S3.SS3.p1.3.m3.1.1.1" xref="S3.SS3.p1.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3.cmml"><mi id="S3.SS3.p1.3.m3.1.1.3.2" xref="S3.SS3.p1.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS3.p1.3.m3.1.1.3.3" xref="S3.SS3.p1.3.m3.1.1.3.3.cmml"><mi id="S3.SS3.p1.3.m3.1.1.3.3.2" xref="S3.SS3.p1.3.m3.1.1.3.3.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p1.3.m3.1.1.3.3.1" xref="S3.SS3.p1.3.m3.1.1.3.3.1.cmml">×</mo><msup id="S3.SS3.p1.3.m3.1.1.3.3.3" xref="S3.SS3.p1.3.m3.1.1.3.3.3.cmml"><mi id="S3.SS3.p1.3.m3.1.1.3.3.3.2" xref="S3.SS3.p1.3.m3.1.1.3.3.3.2.cmml">C</mi><mrow id="S3.SS3.p1.3.m3.1.1.3.3.3.3" xref="S3.SS3.p1.3.m3.1.1.3.3.3.3.cmml"><mi id="S3.SS3.p1.3.m3.1.1.3.3.3.3.2" xref="S3.SS3.p1.3.m3.1.1.3.3.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.3.m3.1.1.3.3.3.3.1" xref="S3.SS3.p1.3.m3.1.1.3.3.3.3.1.cmml">​</mo><mi id="S3.SS3.p1.3.m3.1.1.3.3.3.3.3" xref="S3.SS3.p1.3.m3.1.1.3.3.3.3.3.cmml">n</mi></mrow></msup></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><in id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1.1"></in><ci id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2">𝐗</ci><apply id="S3.SS3.p1.3.m3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.3.1.cmml" xref="S3.SS3.p1.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS3.p1.3.m3.1.1.3.2.cmml" xref="S3.SS3.p1.3.m3.1.1.3.2">ℝ</ci><apply id="S3.SS3.p1.3.m3.1.1.3.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3.3"><times id="S3.SS3.p1.3.m3.1.1.3.3.1.cmml" xref="S3.SS3.p1.3.m3.1.1.3.3.1"></times><ci id="S3.SS3.p1.3.m3.1.1.3.3.2.cmml" xref="S3.SS3.p1.3.m3.1.1.3.3.2">𝑁</ci><apply id="S3.SS3.p1.3.m3.1.1.3.3.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.3.3.3.1.cmml" xref="S3.SS3.p1.3.m3.1.1.3.3.3">superscript</csymbol><ci id="S3.SS3.p1.3.m3.1.1.3.3.3.2.cmml" xref="S3.SS3.p1.3.m3.1.1.3.3.3.2">𝐶</ci><apply id="S3.SS3.p1.3.m3.1.1.3.3.3.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3.3.3.3"><times id="S3.SS3.p1.3.m3.1.1.3.3.3.3.1.cmml" xref="S3.SS3.p1.3.m3.1.1.3.3.3.3.1"></times><ci id="S3.SS3.p1.3.m3.1.1.3.3.3.3.2.cmml" xref="S3.SS3.p1.3.m3.1.1.3.3.3.3.2">𝑖</ci><ci id="S3.SS3.p1.3.m3.1.1.3.3.3.3.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3.3.3.3.3">𝑛</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">\mathbf{X}\in\mathbb{R}^{N\times C^{in}}</annotation></semantics></math>, we encode the noise-resistant geometry features <math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="\mathbf{Y}\in\mathbb{R}^{N\times C^{out}}" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><mrow id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml"><mi id="S3.SS3.p1.4.m4.1.1.2" xref="S3.SS3.p1.4.m4.1.1.2.cmml">𝐘</mi><mo id="S3.SS3.p1.4.m4.1.1.1" xref="S3.SS3.p1.4.m4.1.1.1.cmml">∈</mo><msup id="S3.SS3.p1.4.m4.1.1.3" xref="S3.SS3.p1.4.m4.1.1.3.cmml"><mi id="S3.SS3.p1.4.m4.1.1.3.2" xref="S3.SS3.p1.4.m4.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS3.p1.4.m4.1.1.3.3" xref="S3.SS3.p1.4.m4.1.1.3.3.cmml"><mi id="S3.SS3.p1.4.m4.1.1.3.3.2" xref="S3.SS3.p1.4.m4.1.1.3.3.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p1.4.m4.1.1.3.3.1" xref="S3.SS3.p1.4.m4.1.1.3.3.1.cmml">×</mo><msup id="S3.SS3.p1.4.m4.1.1.3.3.3" xref="S3.SS3.p1.4.m4.1.1.3.3.3.cmml"><mi id="S3.SS3.p1.4.m4.1.1.3.3.3.2" xref="S3.SS3.p1.4.m4.1.1.3.3.3.2.cmml">C</mi><mrow id="S3.SS3.p1.4.m4.1.1.3.3.3.3" xref="S3.SS3.p1.4.m4.1.1.3.3.3.3.cmml"><mi id="S3.SS3.p1.4.m4.1.1.3.3.3.3.2" xref="S3.SS3.p1.4.m4.1.1.3.3.3.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.4.m4.1.1.3.3.3.3.1" xref="S3.SS3.p1.4.m4.1.1.3.3.3.3.1.cmml">​</mo><mi id="S3.SS3.p1.4.m4.1.1.3.3.3.3.3" xref="S3.SS3.p1.4.m4.1.1.3.3.3.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.4.m4.1.1.3.3.3.3.1a" xref="S3.SS3.p1.4.m4.1.1.3.3.3.3.1.cmml">​</mo><mi id="S3.SS3.p1.4.m4.1.1.3.3.3.3.4" xref="S3.SS3.p1.4.m4.1.1.3.3.3.3.4.cmml">t</mi></mrow></msup></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><apply id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1"><in id="S3.SS3.p1.4.m4.1.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1.1"></in><ci id="S3.SS3.p1.4.m4.1.1.2.cmml" xref="S3.SS3.p1.4.m4.1.1.2">𝐘</ci><apply id="S3.SS3.p1.4.m4.1.1.3.cmml" xref="S3.SS3.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m4.1.1.3.1.cmml" xref="S3.SS3.p1.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS3.p1.4.m4.1.1.3.2.cmml" xref="S3.SS3.p1.4.m4.1.1.3.2">ℝ</ci><apply id="S3.SS3.p1.4.m4.1.1.3.3.cmml" xref="S3.SS3.p1.4.m4.1.1.3.3"><times id="S3.SS3.p1.4.m4.1.1.3.3.1.cmml" xref="S3.SS3.p1.4.m4.1.1.3.3.1"></times><ci id="S3.SS3.p1.4.m4.1.1.3.3.2.cmml" xref="S3.SS3.p1.4.m4.1.1.3.3.2">𝑁</ci><apply id="S3.SS3.p1.4.m4.1.1.3.3.3.cmml" xref="S3.SS3.p1.4.m4.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m4.1.1.3.3.3.1.cmml" xref="S3.SS3.p1.4.m4.1.1.3.3.3">superscript</csymbol><ci id="S3.SS3.p1.4.m4.1.1.3.3.3.2.cmml" xref="S3.SS3.p1.4.m4.1.1.3.3.3.2">𝐶</ci><apply id="S3.SS3.p1.4.m4.1.1.3.3.3.3.cmml" xref="S3.SS3.p1.4.m4.1.1.3.3.3.3"><times id="S3.SS3.p1.4.m4.1.1.3.3.3.3.1.cmml" xref="S3.SS3.p1.4.m4.1.1.3.3.3.3.1"></times><ci id="S3.SS3.p1.4.m4.1.1.3.3.3.3.2.cmml" xref="S3.SS3.p1.4.m4.1.1.3.3.3.3.2">𝑜</ci><ci id="S3.SS3.p1.4.m4.1.1.3.3.3.3.3.cmml" xref="S3.SS3.p1.4.m4.1.1.3.3.3.3.3">𝑢</ci><ci id="S3.SS3.p1.4.m4.1.1.3.3.3.3.4.cmml" xref="S3.SS3.p1.4.m4.1.1.3.3.3.3.4">𝑡</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">\mathbf{Y}\in\mathbb{R}^{N\times C^{out}}</annotation></semantics></math> in both 3D and 2D image space, where <math id="S3.SS3.p1.5.m5.1" class="ltx_Math" alttext="C^{in}" display="inline"><semantics id="S3.SS3.p1.5.m5.1a"><msup id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml"><mi id="S3.SS3.p1.5.m5.1.1.2" xref="S3.SS3.p1.5.m5.1.1.2.cmml">C</mi><mrow id="S3.SS3.p1.5.m5.1.1.3" xref="S3.SS3.p1.5.m5.1.1.3.cmml"><mi id="S3.SS3.p1.5.m5.1.1.3.2" xref="S3.SS3.p1.5.m5.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.5.m5.1.1.3.1" xref="S3.SS3.p1.5.m5.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p1.5.m5.1.1.3.3" xref="S3.SS3.p1.5.m5.1.1.3.3.cmml">n</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><apply id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.5.m5.1.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1">superscript</csymbol><ci id="S3.SS3.p1.5.m5.1.1.2.cmml" xref="S3.SS3.p1.5.m5.1.1.2">𝐶</ci><apply id="S3.SS3.p1.5.m5.1.1.3.cmml" xref="S3.SS3.p1.5.m5.1.1.3"><times id="S3.SS3.p1.5.m5.1.1.3.1.cmml" xref="S3.SS3.p1.5.m5.1.1.3.1"></times><ci id="S3.SS3.p1.5.m5.1.1.3.2.cmml" xref="S3.SS3.p1.5.m5.1.1.3.2">𝑖</ci><ci id="S3.SS3.p1.5.m5.1.1.3.3.cmml" xref="S3.SS3.p1.5.m5.1.1.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">C^{in}</annotation></semantics></math> and <math id="S3.SS3.p1.6.m6.1" class="ltx_Math" alttext="C^{out}" display="inline"><semantics id="S3.SS3.p1.6.m6.1a"><msup id="S3.SS3.p1.6.m6.1.1" xref="S3.SS3.p1.6.m6.1.1.cmml"><mi id="S3.SS3.p1.6.m6.1.1.2" xref="S3.SS3.p1.6.m6.1.1.2.cmml">C</mi><mrow id="S3.SS3.p1.6.m6.1.1.3" xref="S3.SS3.p1.6.m6.1.1.3.cmml"><mi id="S3.SS3.p1.6.m6.1.1.3.2" xref="S3.SS3.p1.6.m6.1.1.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.6.m6.1.1.3.1" xref="S3.SS3.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p1.6.m6.1.1.3.3" xref="S3.SS3.p1.6.m6.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.6.m6.1.1.3.1a" xref="S3.SS3.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p1.6.m6.1.1.3.4" xref="S3.SS3.p1.6.m6.1.1.3.4.cmml">t</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m6.1b"><apply id="S3.SS3.p1.6.m6.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.6.m6.1.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1">superscript</csymbol><ci id="S3.SS3.p1.6.m6.1.1.2.cmml" xref="S3.SS3.p1.6.m6.1.1.2">𝐶</ci><apply id="S3.SS3.p1.6.m6.1.1.3.cmml" xref="S3.SS3.p1.6.m6.1.1.3"><times id="S3.SS3.p1.6.m6.1.1.3.1.cmml" xref="S3.SS3.p1.6.m6.1.1.3.1"></times><ci id="S3.SS3.p1.6.m6.1.1.3.2.cmml" xref="S3.SS3.p1.6.m6.1.1.3.2">𝑜</ci><ci id="S3.SS3.p1.6.m6.1.1.3.3.cmml" xref="S3.SS3.p1.6.m6.1.1.3.3">𝑢</ci><ci id="S3.SS3.p1.6.m6.1.1.3.4.cmml" xref="S3.SS3.p1.6.m6.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m6.1c">C^{out}</annotation></semantics></math> denote the number of input and output feature channels respectively.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.5" class="ltx_p"><span id="S3.SS3.p2.5.1" class="ltx_text ltx_font_bold">Encoding geometry features in 3D space.</span>
For each voxel feature <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="X_{i}" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><msub id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">X</mi><mi id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">𝑋</ci><ci id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">X_{i}</annotation></semantics></math> in <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="\mathbf{X}" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mi id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">𝐗</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">𝐗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\mathbf{X}</annotation></semantics></math>, we first encode the geometry features by the 3D submanifold convolution kernel <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{K}^{3D}(\cdot)" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mrow id="S3.SS3.p2.3.m3.1.2" xref="S3.SS3.p2.3.m3.1.2.cmml"><msup id="S3.SS3.p2.3.m3.1.2.2" xref="S3.SS3.p2.3.m3.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.3.m3.1.2.2.2" xref="S3.SS3.p2.3.m3.1.2.2.2.cmml">𝒦</mi><mrow id="S3.SS3.p2.3.m3.1.2.2.3" xref="S3.SS3.p2.3.m3.1.2.2.3.cmml"><mn id="S3.SS3.p2.3.m3.1.2.2.3.2" xref="S3.SS3.p2.3.m3.1.2.2.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m3.1.2.2.3.1" xref="S3.SS3.p2.3.m3.1.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p2.3.m3.1.2.2.3.3" xref="S3.SS3.p2.3.m3.1.2.2.3.3.cmml">D</mi></mrow></msup><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m3.1.2.1" xref="S3.SS3.p2.3.m3.1.2.1.cmml">​</mo><mrow id="S3.SS3.p2.3.m3.1.2.3.2" xref="S3.SS3.p2.3.m3.1.2.cmml"><mo stretchy="false" id="S3.SS3.p2.3.m3.1.2.3.2.1" xref="S3.SS3.p2.3.m3.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS3.p2.3.m3.1.2.3.2.2" xref="S3.SS3.p2.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><apply id="S3.SS3.p2.3.m3.1.2.cmml" xref="S3.SS3.p2.3.m3.1.2"><times id="S3.SS3.p2.3.m3.1.2.1.cmml" xref="S3.SS3.p2.3.m3.1.2.1"></times><apply id="S3.SS3.p2.3.m3.1.2.2.cmml" xref="S3.SS3.p2.3.m3.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.2.2.1.cmml" xref="S3.SS3.p2.3.m3.1.2.2">superscript</csymbol><ci id="S3.SS3.p2.3.m3.1.2.2.2.cmml" xref="S3.SS3.p2.3.m3.1.2.2.2">𝒦</ci><apply id="S3.SS3.p2.3.m3.1.2.2.3.cmml" xref="S3.SS3.p2.3.m3.1.2.2.3"><times id="S3.SS3.p2.3.m3.1.2.2.3.1.cmml" xref="S3.SS3.p2.3.m3.1.2.2.3.1"></times><cn type="integer" id="S3.SS3.p2.3.m3.1.2.2.3.2.cmml" xref="S3.SS3.p2.3.m3.1.2.2.3.2">3</cn><ci id="S3.SS3.p2.3.m3.1.2.2.3.3.cmml" xref="S3.SS3.p2.3.m3.1.2.2.3.3">𝐷</ci></apply></apply><ci id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">\mathcal{K}^{3D}(\cdot)</annotation></semantics></math>. Specifically, the geometry features <math id="S3.SS3.p2.4.m4.1" class="ltx_Math" alttext="\hat{X}_{i}\in\mathbb{R}^{C^{out}/2}" display="inline"><semantics id="S3.SS3.p2.4.m4.1a"><mrow id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml"><msub id="S3.SS3.p2.4.m4.1.1.2" xref="S3.SS3.p2.4.m4.1.1.2.cmml"><mover accent="true" id="S3.SS3.p2.4.m4.1.1.2.2" xref="S3.SS3.p2.4.m4.1.1.2.2.cmml"><mi id="S3.SS3.p2.4.m4.1.1.2.2.2" xref="S3.SS3.p2.4.m4.1.1.2.2.2.cmml">X</mi><mo id="S3.SS3.p2.4.m4.1.1.2.2.1" xref="S3.SS3.p2.4.m4.1.1.2.2.1.cmml">^</mo></mover><mi id="S3.SS3.p2.4.m4.1.1.2.3" xref="S3.SS3.p2.4.m4.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS3.p2.4.m4.1.1.1" xref="S3.SS3.p2.4.m4.1.1.1.cmml">∈</mo><msup id="S3.SS3.p2.4.m4.1.1.3" xref="S3.SS3.p2.4.m4.1.1.3.cmml"><mi id="S3.SS3.p2.4.m4.1.1.3.2" xref="S3.SS3.p2.4.m4.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS3.p2.4.m4.1.1.3.3" xref="S3.SS3.p2.4.m4.1.1.3.3.cmml"><msup id="S3.SS3.p2.4.m4.1.1.3.3.2" xref="S3.SS3.p2.4.m4.1.1.3.3.2.cmml"><mi id="S3.SS3.p2.4.m4.1.1.3.3.2.2" xref="S3.SS3.p2.4.m4.1.1.3.3.2.2.cmml">C</mi><mrow id="S3.SS3.p2.4.m4.1.1.3.3.2.3" xref="S3.SS3.p2.4.m4.1.1.3.3.2.3.cmml"><mi id="S3.SS3.p2.4.m4.1.1.3.3.2.3.2" xref="S3.SS3.p2.4.m4.1.1.3.3.2.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.4.m4.1.1.3.3.2.3.1" xref="S3.SS3.p2.4.m4.1.1.3.3.2.3.1.cmml">​</mo><mi id="S3.SS3.p2.4.m4.1.1.3.3.2.3.3" xref="S3.SS3.p2.4.m4.1.1.3.3.2.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.4.m4.1.1.3.3.2.3.1a" xref="S3.SS3.p2.4.m4.1.1.3.3.2.3.1.cmml">​</mo><mi id="S3.SS3.p2.4.m4.1.1.3.3.2.3.4" xref="S3.SS3.p2.4.m4.1.1.3.3.2.3.4.cmml">t</mi></mrow></msup><mo id="S3.SS3.p2.4.m4.1.1.3.3.1" xref="S3.SS3.p2.4.m4.1.1.3.3.1.cmml">/</mo><mn id="S3.SS3.p2.4.m4.1.1.3.3.3" xref="S3.SS3.p2.4.m4.1.1.3.3.3.cmml">2</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><apply id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1"><in id="S3.SS3.p2.4.m4.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1.1"></in><apply id="S3.SS3.p2.4.m4.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.1.1.2.1.cmml" xref="S3.SS3.p2.4.m4.1.1.2">subscript</csymbol><apply id="S3.SS3.p2.4.m4.1.1.2.2.cmml" xref="S3.SS3.p2.4.m4.1.1.2.2"><ci id="S3.SS3.p2.4.m4.1.1.2.2.1.cmml" xref="S3.SS3.p2.4.m4.1.1.2.2.1">^</ci><ci id="S3.SS3.p2.4.m4.1.1.2.2.2.cmml" xref="S3.SS3.p2.4.m4.1.1.2.2.2">𝑋</ci></apply><ci id="S3.SS3.p2.4.m4.1.1.2.3.cmml" xref="S3.SS3.p2.4.m4.1.1.2.3">𝑖</ci></apply><apply id="S3.SS3.p2.4.m4.1.1.3.cmml" xref="S3.SS3.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.1.1.3.1.cmml" xref="S3.SS3.p2.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS3.p2.4.m4.1.1.3.2.cmml" xref="S3.SS3.p2.4.m4.1.1.3.2">ℝ</ci><apply id="S3.SS3.p2.4.m4.1.1.3.3.cmml" xref="S3.SS3.p2.4.m4.1.1.3.3"><divide id="S3.SS3.p2.4.m4.1.1.3.3.1.cmml" xref="S3.SS3.p2.4.m4.1.1.3.3.1"></divide><apply id="S3.SS3.p2.4.m4.1.1.3.3.2.cmml" xref="S3.SS3.p2.4.m4.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.1.1.3.3.2.1.cmml" xref="S3.SS3.p2.4.m4.1.1.3.3.2">superscript</csymbol><ci id="S3.SS3.p2.4.m4.1.1.3.3.2.2.cmml" xref="S3.SS3.p2.4.m4.1.1.3.3.2.2">𝐶</ci><apply id="S3.SS3.p2.4.m4.1.1.3.3.2.3.cmml" xref="S3.SS3.p2.4.m4.1.1.3.3.2.3"><times id="S3.SS3.p2.4.m4.1.1.3.3.2.3.1.cmml" xref="S3.SS3.p2.4.m4.1.1.3.3.2.3.1"></times><ci id="S3.SS3.p2.4.m4.1.1.3.3.2.3.2.cmml" xref="S3.SS3.p2.4.m4.1.1.3.3.2.3.2">𝑜</ci><ci id="S3.SS3.p2.4.m4.1.1.3.3.2.3.3.cmml" xref="S3.SS3.p2.4.m4.1.1.3.3.2.3.3">𝑢</ci><ci id="S3.SS3.p2.4.m4.1.1.3.3.2.3.4.cmml" xref="S3.SS3.p2.4.m4.1.1.3.3.2.3.4">𝑡</ci></apply></apply><cn type="integer" id="S3.SS3.p2.4.m4.1.1.3.3.3.cmml" xref="S3.SS3.p2.4.m4.1.1.3.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">\hat{X}_{i}\in\mathbb{R}^{C^{out}/2}</annotation></semantics></math> are calculated from the non-empty voxels within <math id="S3.SS3.p2.5.m5.1" class="ltx_Math" alttext="3\times 3\times 3" display="inline"><semantics id="S3.SS3.p2.5.m5.1a"><mrow id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml"><mn id="S3.SS3.p2.5.m5.1.1.2" xref="S3.SS3.p2.5.m5.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p2.5.m5.1.1.1" xref="S3.SS3.p2.5.m5.1.1.1.cmml">×</mo><mn id="S3.SS3.p2.5.m5.1.1.3" xref="S3.SS3.p2.5.m5.1.1.3.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p2.5.m5.1.1.1a" xref="S3.SS3.p2.5.m5.1.1.1.cmml">×</mo><mn id="S3.SS3.p2.5.m5.1.1.4" xref="S3.SS3.p2.5.m5.1.1.4.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><apply id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1"><times id="S3.SS3.p2.5.m5.1.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1.1"></times><cn type="integer" id="S3.SS3.p2.5.m5.1.1.2.cmml" xref="S3.SS3.p2.5.m5.1.1.2">3</cn><cn type="integer" id="S3.SS3.p2.5.m5.1.1.3.cmml" xref="S3.SS3.p2.5.m5.1.1.3">3</cn><cn type="integer" id="S3.SS3.p2.5.m5.1.1.4.cmml" xref="S3.SS3.p2.5.m5.1.1.4">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">3\times 3\times 3</annotation></semantics></math> neighborhood based on the corresponding 3D indices as</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<table id="S5.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.4" class="ltx_Math" alttext="\displaystyle\hat{X}_{i}=\mathcal{R}\left\{\mathcal{K}^{3D}\left(X_{i},X_{i}^{(f_{1})},...,X_{i}^{(f_{j})}\right)\right\}," display="inline"><semantics id="S3.E1.m1.4a"><mrow id="S3.E1.m1.4.4.1" xref="S3.E1.m1.4.4.1.1.cmml"><mrow id="S3.E1.m1.4.4.1.1" xref="S3.E1.m1.4.4.1.1.cmml"><msub id="S3.E1.m1.4.4.1.1.3" xref="S3.E1.m1.4.4.1.1.3.cmml"><mover accent="true" id="S3.E1.m1.4.4.1.1.3.2" xref="S3.E1.m1.4.4.1.1.3.2.cmml"><mi id="S3.E1.m1.4.4.1.1.3.2.2" xref="S3.E1.m1.4.4.1.1.3.2.2.cmml">X</mi><mo id="S3.E1.m1.4.4.1.1.3.2.1" xref="S3.E1.m1.4.4.1.1.3.2.1.cmml">^</mo></mover><mi id="S3.E1.m1.4.4.1.1.3.3" xref="S3.E1.m1.4.4.1.1.3.3.cmml">i</mi></msub><mo id="S3.E1.m1.4.4.1.1.2" xref="S3.E1.m1.4.4.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.4.4.1.1.1" xref="S3.E1.m1.4.4.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.4.4.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.3.cmml">ℛ</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.4.4.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.2.cmml"><mo id="S3.E1.m1.4.4.1.1.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.2.cmml">{</mo><mrow id="S3.E1.m1.4.4.1.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.1.cmml"><msup id="S3.E1.m1.4.4.1.1.1.1.1.1.5" xref="S3.E1.m1.4.4.1.1.1.1.1.1.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.4.4.1.1.1.1.1.1.5.2" xref="S3.E1.m1.4.4.1.1.1.1.1.1.5.2.cmml">𝒦</mi><mrow id="S3.E1.m1.4.4.1.1.1.1.1.1.5.3" xref="S3.E1.m1.4.4.1.1.1.1.1.1.5.3.cmml"><mn id="S3.E1.m1.4.4.1.1.1.1.1.1.5.3.2" xref="S3.E1.m1.4.4.1.1.1.1.1.1.5.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.1.1.1.1.1.1.5.3.1" xref="S3.E1.m1.4.4.1.1.1.1.1.1.5.3.1.cmml">​</mo><mi id="S3.E1.m1.4.4.1.1.1.1.1.1.5.3.3" xref="S3.E1.m1.4.4.1.1.1.1.1.1.5.3.3.cmml">D</mi></mrow></msup><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.1.1.1.1.1.1.4" xref="S3.E1.m1.4.4.1.1.1.1.1.1.4.cmml">​</mo><mrow id="S3.E1.m1.4.4.1.1.1.1.1.1.3.3" xref="S3.E1.m1.4.4.1.1.1.1.1.1.3.4.cmml"><mo id="S3.E1.m1.4.4.1.1.1.1.1.1.3.3.4" xref="S3.E1.m1.4.4.1.1.1.1.1.1.3.4.cmml">(</mo><msub id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml">X</mi><mi id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E1.m1.4.4.1.1.1.1.1.1.3.3.5" xref="S3.E1.m1.4.4.1.1.1.1.1.1.3.4.cmml">,</mo><msubsup id="S3.E1.m1.4.4.1.1.1.1.1.1.2.2.2" xref="S3.E1.m1.4.4.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E1.m1.4.4.1.1.1.1.1.1.2.2.2.2.2" xref="S3.E1.m1.4.4.1.1.1.1.1.1.2.2.2.2.2.cmml">X</mi><mi id="S3.E1.m1.4.4.1.1.1.1.1.1.2.2.2.2.3" xref="S3.E1.m1.4.4.1.1.1.1.1.1.2.2.2.2.3.cmml">i</mi><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.2.cmml">f</mi><mn id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.3.cmml">1</mn></msub><mo stretchy="false" id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.cmml">)</mo></mrow></msubsup><mo id="S3.E1.m1.4.4.1.1.1.1.1.1.3.3.6" xref="S3.E1.m1.4.4.1.1.1.1.1.1.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">…</mi><mo id="S3.E1.m1.4.4.1.1.1.1.1.1.3.3.7" xref="S3.E1.m1.4.4.1.1.1.1.1.1.3.4.cmml">,</mo><msubsup id="S3.E1.m1.4.4.1.1.1.1.1.1.3.3.3" xref="S3.E1.m1.4.4.1.1.1.1.1.1.3.3.3.cmml"><mi id="S3.E1.m1.4.4.1.1.1.1.1.1.3.3.3.2.2" xref="S3.E1.m1.4.4.1.1.1.1.1.1.3.3.3.2.2.cmml">X</mi><mi id="S3.E1.m1.4.4.1.1.1.1.1.1.3.3.3.2.3" xref="S3.E1.m1.4.4.1.1.1.1.1.1.3.3.3.2.3.cmml">i</mi><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.2" xref="S3.E1.m1.2.2.1.1.1.cmml">(</mo><msub id="S3.E1.m1.2.2.1.1.1" xref="S3.E1.m1.2.2.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.2.cmml">f</mi><mi id="S3.E1.m1.2.2.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S3.E1.m1.2.2.1.1.3" xref="S3.E1.m1.2.2.1.1.1.cmml">)</mo></mrow></msubsup><mo id="S3.E1.m1.4.4.1.1.1.1.1.1.3.3.8" xref="S3.E1.m1.4.4.1.1.1.1.1.1.3.4.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.4.4.1.1.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.1.2.cmml">}</mo></mrow></mrow></mrow><mo id="S3.E1.m1.4.4.1.2" xref="S3.E1.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.4b"><apply id="S3.E1.m1.4.4.1.1.cmml" xref="S3.E1.m1.4.4.1"><eq id="S3.E1.m1.4.4.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.2"></eq><apply id="S3.E1.m1.4.4.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.3.1.cmml" xref="S3.E1.m1.4.4.1.1.3">subscript</csymbol><apply id="S3.E1.m1.4.4.1.1.3.2.cmml" xref="S3.E1.m1.4.4.1.1.3.2"><ci id="S3.E1.m1.4.4.1.1.3.2.1.cmml" xref="S3.E1.m1.4.4.1.1.3.2.1">^</ci><ci id="S3.E1.m1.4.4.1.1.3.2.2.cmml" xref="S3.E1.m1.4.4.1.1.3.2.2">𝑋</ci></apply><ci id="S3.E1.m1.4.4.1.1.3.3.cmml" xref="S3.E1.m1.4.4.1.1.3.3">𝑖</ci></apply><apply id="S3.E1.m1.4.4.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1"><times id="S3.E1.m1.4.4.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.2"></times><ci id="S3.E1.m1.4.4.1.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.1.3">ℛ</ci><set id="S3.E1.m1.4.4.1.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1"><apply id="S3.E1.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1"><times id="S3.E1.m1.4.4.1.1.1.1.1.1.4.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.4"></times><apply id="S3.E1.m1.4.4.1.1.1.1.1.1.5.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.1.1.1.1.5.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.5">superscript</csymbol><ci id="S3.E1.m1.4.4.1.1.1.1.1.1.5.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.5.2">𝒦</ci><apply id="S3.E1.m1.4.4.1.1.1.1.1.1.5.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.5.3"><times id="S3.E1.m1.4.4.1.1.1.1.1.1.5.3.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.5.3.1"></times><cn type="integer" id="S3.E1.m1.4.4.1.1.1.1.1.1.5.3.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.5.3.2">3</cn><ci id="S3.E1.m1.4.4.1.1.1.1.1.1.5.3.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.5.3.3">𝐷</ci></apply></apply><vector id="S3.E1.m1.4.4.1.1.1.1.1.1.3.4.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.3.3"><apply id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.2">𝑋</ci><ci id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E1.m1.4.4.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.2.2.2">superscript</csymbol><apply id="S3.E1.m1.4.4.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E1.m1.4.4.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.2.2.2.2.2">𝑋</ci><ci id="S3.E1.m1.4.4.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.2.2.2.2.3">𝑖</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.2">𝑓</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.3">1</cn></apply></apply><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">…</ci><apply id="S3.E1.m1.4.4.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.1.1.1.1.3.3.3.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.3.3.3">superscript</csymbol><apply id="S3.E1.m1.4.4.1.1.1.1.1.1.3.3.3.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.1.1.1.1.3.3.3.2.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E1.m1.4.4.1.1.1.1.1.1.3.3.3.2.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.3.3.3.2.2">𝑋</ci><ci id="S3.E1.m1.4.4.1.1.1.1.1.1.3.3.3.2.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.3.3.3.2.3">𝑖</ci></apply><apply id="S3.E1.m1.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.2">𝑓</ci><ci id="S3.E1.m1.2.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.3">𝑗</ci></apply></apply></vector></apply></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.4c">\displaystyle\hat{X}_{i}=\mathcal{R}\left\{\mathcal{K}^{3D}\left(X_{i},X_{i}^{(f_{1})},...,X_{i}^{(f_{j})}\right)\right\},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS3.p4" class="ltx_para ltx_noindent">
<p id="S3.SS3.p4.3" class="ltx_p">where <math id="S3.SS3.p4.1.m1.5" class="ltx_Math" alttext="X_{i}^{(f_{1})},...,X_{i}^{(f_{j})}" display="inline"><semantics id="S3.SS3.p4.1.m1.5a"><mrow id="S3.SS3.p4.1.m1.5.5.2" xref="S3.SS3.p4.1.m1.5.5.3.cmml"><msubsup id="S3.SS3.p4.1.m1.4.4.1.1" xref="S3.SS3.p4.1.m1.4.4.1.1.cmml"><mi id="S3.SS3.p4.1.m1.4.4.1.1.2.2" xref="S3.SS3.p4.1.m1.4.4.1.1.2.2.cmml">X</mi><mi id="S3.SS3.p4.1.m1.4.4.1.1.2.3" xref="S3.SS3.p4.1.m1.4.4.1.1.2.3.cmml">i</mi><mrow id="S3.SS3.p4.1.m1.1.1.1.1" xref="S3.SS3.p4.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS3.p4.1.m1.1.1.1.1.2" xref="S3.SS3.p4.1.m1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS3.p4.1.m1.1.1.1.1.1" xref="S3.SS3.p4.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS3.p4.1.m1.1.1.1.1.1.2" xref="S3.SS3.p4.1.m1.1.1.1.1.1.2.cmml">f</mi><mn id="S3.SS3.p4.1.m1.1.1.1.1.1.3" xref="S3.SS3.p4.1.m1.1.1.1.1.1.3.cmml">1</mn></msub><mo stretchy="false" id="S3.SS3.p4.1.m1.1.1.1.1.3" xref="S3.SS3.p4.1.m1.1.1.1.1.1.cmml">)</mo></mrow></msubsup><mo id="S3.SS3.p4.1.m1.5.5.2.3" xref="S3.SS3.p4.1.m1.5.5.3.cmml">,</mo><mi mathvariant="normal" id="S3.SS3.p4.1.m1.3.3" xref="S3.SS3.p4.1.m1.3.3.cmml">…</mi><mo id="S3.SS3.p4.1.m1.5.5.2.4" xref="S3.SS3.p4.1.m1.5.5.3.cmml">,</mo><msubsup id="S3.SS3.p4.1.m1.5.5.2.2" xref="S3.SS3.p4.1.m1.5.5.2.2.cmml"><mi id="S3.SS3.p4.1.m1.5.5.2.2.2.2" xref="S3.SS3.p4.1.m1.5.5.2.2.2.2.cmml">X</mi><mi id="S3.SS3.p4.1.m1.5.5.2.2.2.3" xref="S3.SS3.p4.1.m1.5.5.2.2.2.3.cmml">i</mi><mrow id="S3.SS3.p4.1.m1.2.2.1.1" xref="S3.SS3.p4.1.m1.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.SS3.p4.1.m1.2.2.1.1.2" xref="S3.SS3.p4.1.m1.2.2.1.1.1.cmml">(</mo><msub id="S3.SS3.p4.1.m1.2.2.1.1.1" xref="S3.SS3.p4.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS3.p4.1.m1.2.2.1.1.1.2" xref="S3.SS3.p4.1.m1.2.2.1.1.1.2.cmml">f</mi><mi id="S3.SS3.p4.1.m1.2.2.1.1.1.3" xref="S3.SS3.p4.1.m1.2.2.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S3.SS3.p4.1.m1.2.2.1.1.3" xref="S3.SS3.p4.1.m1.2.2.1.1.1.cmml">)</mo></mrow></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.5b"><list id="S3.SS3.p4.1.m1.5.5.3.cmml" xref="S3.SS3.p4.1.m1.5.5.2"><apply id="S3.SS3.p4.1.m1.4.4.1.1.cmml" xref="S3.SS3.p4.1.m1.4.4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.1.m1.4.4.1.1.1.cmml" xref="S3.SS3.p4.1.m1.4.4.1.1">superscript</csymbol><apply id="S3.SS3.p4.1.m1.4.4.1.1.2.cmml" xref="S3.SS3.p4.1.m1.4.4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.1.m1.4.4.1.1.2.1.cmml" xref="S3.SS3.p4.1.m1.4.4.1.1">subscript</csymbol><ci id="S3.SS3.p4.1.m1.4.4.1.1.2.2.cmml" xref="S3.SS3.p4.1.m1.4.4.1.1.2.2">𝑋</ci><ci id="S3.SS3.p4.1.m1.4.4.1.1.2.3.cmml" xref="S3.SS3.p4.1.m1.4.4.1.1.2.3">𝑖</ci></apply><apply id="S3.SS3.p4.1.m1.1.1.1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p4.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS3.p4.1.m1.1.1.1.1.1.2">𝑓</ci><cn type="integer" id="S3.SS3.p4.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS3.p4.1.m1.1.1.1.1.1.3">1</cn></apply></apply><ci id="S3.SS3.p4.1.m1.3.3.cmml" xref="S3.SS3.p4.1.m1.3.3">…</ci><apply id="S3.SS3.p4.1.m1.5.5.2.2.cmml" xref="S3.SS3.p4.1.m1.5.5.2.2"><csymbol cd="ambiguous" id="S3.SS3.p4.1.m1.5.5.2.2.1.cmml" xref="S3.SS3.p4.1.m1.5.5.2.2">superscript</csymbol><apply id="S3.SS3.p4.1.m1.5.5.2.2.2.cmml" xref="S3.SS3.p4.1.m1.5.5.2.2"><csymbol cd="ambiguous" id="S3.SS3.p4.1.m1.5.5.2.2.2.1.cmml" xref="S3.SS3.p4.1.m1.5.5.2.2">subscript</csymbol><ci id="S3.SS3.p4.1.m1.5.5.2.2.2.2.cmml" xref="S3.SS3.p4.1.m1.5.5.2.2.2.2">𝑋</ci><ci id="S3.SS3.p4.1.m1.5.5.2.2.2.3.cmml" xref="S3.SS3.p4.1.m1.5.5.2.2.2.3">𝑖</ci></apply><apply id="S3.SS3.p4.1.m1.2.2.1.1.1.cmml" xref="S3.SS3.p4.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS3.p4.1.m1.2.2.1.1">subscript</csymbol><ci id="S3.SS3.p4.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS3.p4.1.m1.2.2.1.1.1.2">𝑓</ci><ci id="S3.SS3.p4.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS3.p4.1.m1.2.2.1.1.1.3">𝑗</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.5c">X_{i}^{(f_{1})},...,X_{i}^{(f_{j})}</annotation></semantics></math> denote neighbor features generated by <math id="S3.SS3.p4.2.m2.1" class="ltx_Math" alttext="\mathbf{H}" display="inline"><semantics id="S3.SS3.p4.2.m2.1a"><mi id="S3.SS3.p4.2.m2.1.1" xref="S3.SS3.p4.2.m2.1.1.cmml">𝐇</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.1b"><ci id="S3.SS3.p4.2.m2.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1">𝐇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.1c">\mathbf{H}</annotation></semantics></math>, and <math id="S3.SS3.p4.3.m3.1" class="ltx_Math" alttext="\mathcal{R}" display="inline"><semantics id="S3.SS3.p4.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p4.3.m3.1.1" xref="S3.SS3.p4.3.m3.1.1.cmml">ℛ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.3.m3.1b"><ci id="S3.SS3.p4.3.m3.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1">ℛ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.3.m3.1c">\mathcal{R}</annotation></semantics></math> denotes the nonlinear activation function.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.3" class="ltx_p"><span id="S3.SS3.p5.3.1" class="ltx_text ltx_font_bold">Encoding noise-aware features in 2D image space.</span>
The noise brought by the inaccurate depth completion significantly degrade the detection performance. Since the noise is mostly distributed on the 2D instance boundaries, we extend the convolution receptive field to the 2D image space and encode the noise-aware features using the 2D neighbor voxels. Specifically, we first convert the 3D indices to a set of grid points based on the voxelization parameters (the conversion denoted as <math id="S3.SS3.p5.1.m1.1" class="ltx_Math" alttext="\mathcal{G}(\cdot)" display="inline"><semantics id="S3.SS3.p5.1.m1.1a"><mrow id="S3.SS3.p5.1.m1.1.2" xref="S3.SS3.p5.1.m1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p5.1.m1.1.2.2" xref="S3.SS3.p5.1.m1.1.2.2.cmml">𝒢</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.1.m1.1.2.1" xref="S3.SS3.p5.1.m1.1.2.1.cmml">​</mo><mrow id="S3.SS3.p5.1.m1.1.2.3.2" xref="S3.SS3.p5.1.m1.1.2.cmml"><mo stretchy="false" id="S3.SS3.p5.1.m1.1.2.3.2.1" xref="S3.SS3.p5.1.m1.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS3.p5.1.m1.1.1" xref="S3.SS3.p5.1.m1.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS3.p5.1.m1.1.2.3.2.2" xref="S3.SS3.p5.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.1.m1.1b"><apply id="S3.SS3.p5.1.m1.1.2.cmml" xref="S3.SS3.p5.1.m1.1.2"><times id="S3.SS3.p5.1.m1.1.2.1.cmml" xref="S3.SS3.p5.1.m1.1.2.1"></times><ci id="S3.SS3.p5.1.m1.1.2.2.cmml" xref="S3.SS3.p5.1.m1.1.2.2">𝒢</ci><ci id="S3.SS3.p5.1.m1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.1.m1.1c">\mathcal{G}(\cdot)</annotation></semantics></math>). Since state-of-the-art detectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> also adopt the transformation augmentations (the augmentation denoted as <math id="S3.SS3.p5.2.m2.1" class="ltx_Math" alttext="\mathcal{T}(\cdot)" display="inline"><semantics id="S3.SS3.p5.2.m2.1a"><mrow id="S3.SS3.p5.2.m2.1.2" xref="S3.SS3.p5.2.m2.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p5.2.m2.1.2.2" xref="S3.SS3.p5.2.m2.1.2.2.cmml">𝒯</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.2.m2.1.2.1" xref="S3.SS3.p5.2.m2.1.2.1.cmml">​</mo><mrow id="S3.SS3.p5.2.m2.1.2.3.2" xref="S3.SS3.p5.2.m2.1.2.cmml"><mo stretchy="false" id="S3.SS3.p5.2.m2.1.2.3.2.1" xref="S3.SS3.p5.2.m2.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS3.p5.2.m2.1.1" xref="S3.SS3.p5.2.m2.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS3.p5.2.m2.1.2.3.2.2" xref="S3.SS3.p5.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.2.m2.1b"><apply id="S3.SS3.p5.2.m2.1.2.cmml" xref="S3.SS3.p5.2.m2.1.2"><times id="S3.SS3.p5.2.m2.1.2.1.cmml" xref="S3.SS3.p5.2.m2.1.2.1"></times><ci id="S3.SS3.p5.2.m2.1.2.2.cmml" xref="S3.SS3.p5.2.m2.1.2.2">𝒯</ci><ci id="S3.SS3.p5.2.m2.1.1.cmml" xref="S3.SS3.p5.2.m2.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.2.m2.1c">\mathcal{T}(\cdot)</annotation></semantics></math>) such as rotation and scaling, the grid points are generally misaligned with the corresponding image. Therefore, we transform the grid points backward into the original coordinate system based on the data augmentation parameters. Then we project the grid points into the 2D image plane based on the LiDAR-Camera calibration parameters (with the projection denoted as <math id="S3.SS3.p5.3.m3.1" class="ltx_Math" alttext="\mathcal{P}(\cdot)" display="inline"><semantics id="S3.SS3.p5.3.m3.1a"><mrow id="S3.SS3.p5.3.m3.1.2" xref="S3.SS3.p5.3.m3.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p5.3.m3.1.2.2" xref="S3.SS3.p5.3.m3.1.2.2.cmml">𝒫</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.3.m3.1.2.1" xref="S3.SS3.p5.3.m3.1.2.1.cmml">​</mo><mrow id="S3.SS3.p5.3.m3.1.2.3.2" xref="S3.SS3.p5.3.m3.1.2.cmml"><mo stretchy="false" id="S3.SS3.p5.3.m3.1.2.3.2.1" xref="S3.SS3.p5.3.m3.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS3.p5.3.m3.1.1" xref="S3.SS3.p5.3.m3.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS3.p5.3.m3.1.2.3.2.2" xref="S3.SS3.p5.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.3.m3.1b"><apply id="S3.SS3.p5.3.m3.1.2.cmml" xref="S3.SS3.p5.3.m3.1.2"><times id="S3.SS3.p5.3.m3.1.2.1.cmml" xref="S3.SS3.p5.3.m3.1.2.1"></times><ci id="S3.SS3.p5.3.m3.1.2.2.cmml" xref="S3.SS3.p5.3.m3.1.2.2">𝒫</ci><ci id="S3.SS3.p5.3.m3.1.1.cmml" xref="S3.SS3.p5.3.m3.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.3.m3.1c">\mathcal{P}(\cdot)</annotation></semantics></math>). The overall projection can be summarized by</p>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<table id="S5.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.2" class="ltx_Math" alttext="\displaystyle\mathbf{\hat{H}}=\mathcal{P}\left(\mathcal{T}^{-1}\left(\mathcal{G}\left(\mathbf{H}\right)\right)\right)," display="inline"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml"><mover accent="true" id="S3.E2.m1.2.2.1.1.3" xref="S3.E2.m1.2.2.1.1.3.cmml"><mi id="S3.E2.m1.2.2.1.1.3.2" xref="S3.E2.m1.2.2.1.1.3.2.cmml">𝐇</mi><mo id="S3.E2.m1.2.2.1.1.3.1" xref="S3.E2.m1.2.2.1.1.3.1.cmml">^</mo></mover><mo id="S3.E2.m1.2.2.1.1.2" xref="S3.E2.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.2.2.1.1.1" xref="S3.E2.m1.2.2.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.2.2.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.3.cmml">𝒫</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.2.2.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.cmml"><msup id="S3.E2.m1.2.2.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.2.2.1.1.1.1.1.1.3.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3.2.cmml">𝒯</mi><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.3.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3.3.cmml"><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.3.3a" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3.3.cmml">−</mo><mn id="S3.E2.m1.2.2.1.1.1.1.1.1.3.3.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3.3.2.cmml">1</mn></mrow></msup><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml">𝒢</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml">​</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">𝐇</mi><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.2.2.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.2.2.1.2" xref="S3.E2.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1"><eq id="S3.E2.m1.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.2"></eq><apply id="S3.E2.m1.2.2.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.3"><ci id="S3.E2.m1.2.2.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3.1">^</ci><ci id="S3.E2.m1.2.2.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.2">𝐇</ci></apply><apply id="S3.E2.m1.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1"><times id="S3.E2.m1.2.2.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.2"></times><ci id="S3.E2.m1.2.2.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.3">𝒫</ci><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1"><times id="S3.E2.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.2"></times><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3.2">𝒯</ci><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3.3"><minus id="S3.E2.m1.2.2.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3.3"></minus><cn type="integer" id="S3.E2.m1.2.2.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.3.3.2">1</cn></apply></apply><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1"><times id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1"></times><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2">𝒢</ci><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝐇</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">\displaystyle\mathbf{\hat{H}}=\mathcal{P}\left(\mathcal{T}^{-1}\left(\mathcal{G}\left(\mathbf{H}\right)\right)\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS3.p7" class="ltx_para ltx_noindent">
<p id="S3.SS3.p7.4" class="ltx_p">where <math id="S3.SS3.p7.1.m1.1" class="ltx_Math" alttext="\mathbf{\hat{H}}\in\mathbb{R}^{N\times 2}" display="inline"><semantics id="S3.SS3.p7.1.m1.1a"><mrow id="S3.SS3.p7.1.m1.1.1" xref="S3.SS3.p7.1.m1.1.1.cmml"><mover accent="true" id="S3.SS3.p7.1.m1.1.1.2" xref="S3.SS3.p7.1.m1.1.1.2.cmml"><mi id="S3.SS3.p7.1.m1.1.1.2.2" xref="S3.SS3.p7.1.m1.1.1.2.2.cmml">𝐇</mi><mo id="S3.SS3.p7.1.m1.1.1.2.1" xref="S3.SS3.p7.1.m1.1.1.2.1.cmml">^</mo></mover><mo id="S3.SS3.p7.1.m1.1.1.1" xref="S3.SS3.p7.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS3.p7.1.m1.1.1.3" xref="S3.SS3.p7.1.m1.1.1.3.cmml"><mi id="S3.SS3.p7.1.m1.1.1.3.2" xref="S3.SS3.p7.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS3.p7.1.m1.1.1.3.3" xref="S3.SS3.p7.1.m1.1.1.3.3.cmml"><mi id="S3.SS3.p7.1.m1.1.1.3.3.2" xref="S3.SS3.p7.1.m1.1.1.3.3.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p7.1.m1.1.1.3.3.1" xref="S3.SS3.p7.1.m1.1.1.3.3.1.cmml">×</mo><mn id="S3.SS3.p7.1.m1.1.1.3.3.3" xref="S3.SS3.p7.1.m1.1.1.3.3.3.cmml">2</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.1.m1.1b"><apply id="S3.SS3.p7.1.m1.1.1.cmml" xref="S3.SS3.p7.1.m1.1.1"><in id="S3.SS3.p7.1.m1.1.1.1.cmml" xref="S3.SS3.p7.1.m1.1.1.1"></in><apply id="S3.SS3.p7.1.m1.1.1.2.cmml" xref="S3.SS3.p7.1.m1.1.1.2"><ci id="S3.SS3.p7.1.m1.1.1.2.1.cmml" xref="S3.SS3.p7.1.m1.1.1.2.1">^</ci><ci id="S3.SS3.p7.1.m1.1.1.2.2.cmml" xref="S3.SS3.p7.1.m1.1.1.2.2">𝐇</ci></apply><apply id="S3.SS3.p7.1.m1.1.1.3.cmml" xref="S3.SS3.p7.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p7.1.m1.1.1.3.1.cmml" xref="S3.SS3.p7.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS3.p7.1.m1.1.1.3.2.cmml" xref="S3.SS3.p7.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS3.p7.1.m1.1.1.3.3.cmml" xref="S3.SS3.p7.1.m1.1.1.3.3"><times id="S3.SS3.p7.1.m1.1.1.3.3.1.cmml" xref="S3.SS3.p7.1.m1.1.1.3.3.1"></times><ci id="S3.SS3.p7.1.m1.1.1.3.3.2.cmml" xref="S3.SS3.p7.1.m1.1.1.3.3.2">𝑁</ci><cn type="integer" id="S3.SS3.p7.1.m1.1.1.3.3.3.cmml" xref="S3.SS3.p7.1.m1.1.1.3.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.1.m1.1c">\mathbf{\hat{H}}\in\mathbb{R}^{N\times 2}</annotation></semantics></math> denotes the 2D indices vector.
For each voxel feature <math id="S3.SS3.p7.2.m2.1" class="ltx_Math" alttext="X_{i}\in\mathbb{R}^{C^{in}}" display="inline"><semantics id="S3.SS3.p7.2.m2.1a"><mrow id="S3.SS3.p7.2.m2.1.1" xref="S3.SS3.p7.2.m2.1.1.cmml"><msub id="S3.SS3.p7.2.m2.1.1.2" xref="S3.SS3.p7.2.m2.1.1.2.cmml"><mi id="S3.SS3.p7.2.m2.1.1.2.2" xref="S3.SS3.p7.2.m2.1.1.2.2.cmml">X</mi><mi id="S3.SS3.p7.2.m2.1.1.2.3" xref="S3.SS3.p7.2.m2.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS3.p7.2.m2.1.1.1" xref="S3.SS3.p7.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS3.p7.2.m2.1.1.3" xref="S3.SS3.p7.2.m2.1.1.3.cmml"><mi id="S3.SS3.p7.2.m2.1.1.3.2" xref="S3.SS3.p7.2.m2.1.1.3.2.cmml">ℝ</mi><msup id="S3.SS3.p7.2.m2.1.1.3.3" xref="S3.SS3.p7.2.m2.1.1.3.3.cmml"><mi id="S3.SS3.p7.2.m2.1.1.3.3.2" xref="S3.SS3.p7.2.m2.1.1.3.3.2.cmml">C</mi><mrow id="S3.SS3.p7.2.m2.1.1.3.3.3" xref="S3.SS3.p7.2.m2.1.1.3.3.3.cmml"><mi id="S3.SS3.p7.2.m2.1.1.3.3.3.2" xref="S3.SS3.p7.2.m2.1.1.3.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.2.m2.1.1.3.3.3.1" xref="S3.SS3.p7.2.m2.1.1.3.3.3.1.cmml">​</mo><mi id="S3.SS3.p7.2.m2.1.1.3.3.3.3" xref="S3.SS3.p7.2.m2.1.1.3.3.3.3.cmml">n</mi></mrow></msup></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.2.m2.1b"><apply id="S3.SS3.p7.2.m2.1.1.cmml" xref="S3.SS3.p7.2.m2.1.1"><in id="S3.SS3.p7.2.m2.1.1.1.cmml" xref="S3.SS3.p7.2.m2.1.1.1"></in><apply id="S3.SS3.p7.2.m2.1.1.2.cmml" xref="S3.SS3.p7.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p7.2.m2.1.1.2.1.cmml" xref="S3.SS3.p7.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS3.p7.2.m2.1.1.2.2.cmml" xref="S3.SS3.p7.2.m2.1.1.2.2">𝑋</ci><ci id="S3.SS3.p7.2.m2.1.1.2.3.cmml" xref="S3.SS3.p7.2.m2.1.1.2.3">𝑖</ci></apply><apply id="S3.SS3.p7.2.m2.1.1.3.cmml" xref="S3.SS3.p7.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p7.2.m2.1.1.3.1.cmml" xref="S3.SS3.p7.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS3.p7.2.m2.1.1.3.2.cmml" xref="S3.SS3.p7.2.m2.1.1.3.2">ℝ</ci><apply id="S3.SS3.p7.2.m2.1.1.3.3.cmml" xref="S3.SS3.p7.2.m2.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS3.p7.2.m2.1.1.3.3.1.cmml" xref="S3.SS3.p7.2.m2.1.1.3.3">superscript</csymbol><ci id="S3.SS3.p7.2.m2.1.1.3.3.2.cmml" xref="S3.SS3.p7.2.m2.1.1.3.3.2">𝐶</ci><apply id="S3.SS3.p7.2.m2.1.1.3.3.3.cmml" xref="S3.SS3.p7.2.m2.1.1.3.3.3"><times id="S3.SS3.p7.2.m2.1.1.3.3.3.1.cmml" xref="S3.SS3.p7.2.m2.1.1.3.3.3.1"></times><ci id="S3.SS3.p7.2.m2.1.1.3.3.3.2.cmml" xref="S3.SS3.p7.2.m2.1.1.3.3.3.2">𝑖</ci><ci id="S3.SS3.p7.2.m2.1.1.3.3.3.3.cmml" xref="S3.SS3.p7.2.m2.1.1.3.3.3.3">𝑛</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.2.m2.1c">X_{i}\in\mathbb{R}^{C^{in}}</annotation></semantics></math>, we then calculate the noise-aware features <math id="S3.SS3.p7.3.m3.1" class="ltx_Math" alttext="\tilde{X}_{i}\in\mathbb{R}^{C^{out}/2}" display="inline"><semantics id="S3.SS3.p7.3.m3.1a"><mrow id="S3.SS3.p7.3.m3.1.1" xref="S3.SS3.p7.3.m3.1.1.cmml"><msub id="S3.SS3.p7.3.m3.1.1.2" xref="S3.SS3.p7.3.m3.1.1.2.cmml"><mover accent="true" id="S3.SS3.p7.3.m3.1.1.2.2" xref="S3.SS3.p7.3.m3.1.1.2.2.cmml"><mi id="S3.SS3.p7.3.m3.1.1.2.2.2" xref="S3.SS3.p7.3.m3.1.1.2.2.2.cmml">X</mi><mo id="S3.SS3.p7.3.m3.1.1.2.2.1" xref="S3.SS3.p7.3.m3.1.1.2.2.1.cmml">~</mo></mover><mi id="S3.SS3.p7.3.m3.1.1.2.3" xref="S3.SS3.p7.3.m3.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS3.p7.3.m3.1.1.1" xref="S3.SS3.p7.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS3.p7.3.m3.1.1.3" xref="S3.SS3.p7.3.m3.1.1.3.cmml"><mi id="S3.SS3.p7.3.m3.1.1.3.2" xref="S3.SS3.p7.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS3.p7.3.m3.1.1.3.3" xref="S3.SS3.p7.3.m3.1.1.3.3.cmml"><msup id="S3.SS3.p7.3.m3.1.1.3.3.2" xref="S3.SS3.p7.3.m3.1.1.3.3.2.cmml"><mi id="S3.SS3.p7.3.m3.1.1.3.3.2.2" xref="S3.SS3.p7.3.m3.1.1.3.3.2.2.cmml">C</mi><mrow id="S3.SS3.p7.3.m3.1.1.3.3.2.3" xref="S3.SS3.p7.3.m3.1.1.3.3.2.3.cmml"><mi id="S3.SS3.p7.3.m3.1.1.3.3.2.3.2" xref="S3.SS3.p7.3.m3.1.1.3.3.2.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.3.m3.1.1.3.3.2.3.1" xref="S3.SS3.p7.3.m3.1.1.3.3.2.3.1.cmml">​</mo><mi id="S3.SS3.p7.3.m3.1.1.3.3.2.3.3" xref="S3.SS3.p7.3.m3.1.1.3.3.2.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p7.3.m3.1.1.3.3.2.3.1a" xref="S3.SS3.p7.3.m3.1.1.3.3.2.3.1.cmml">​</mo><mi id="S3.SS3.p7.3.m3.1.1.3.3.2.3.4" xref="S3.SS3.p7.3.m3.1.1.3.3.2.3.4.cmml">t</mi></mrow></msup><mo id="S3.SS3.p7.3.m3.1.1.3.3.1" xref="S3.SS3.p7.3.m3.1.1.3.3.1.cmml">/</mo><mn id="S3.SS3.p7.3.m3.1.1.3.3.3" xref="S3.SS3.p7.3.m3.1.1.3.3.3.cmml">2</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.3.m3.1b"><apply id="S3.SS3.p7.3.m3.1.1.cmml" xref="S3.SS3.p7.3.m3.1.1"><in id="S3.SS3.p7.3.m3.1.1.1.cmml" xref="S3.SS3.p7.3.m3.1.1.1"></in><apply id="S3.SS3.p7.3.m3.1.1.2.cmml" xref="S3.SS3.p7.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p7.3.m3.1.1.2.1.cmml" xref="S3.SS3.p7.3.m3.1.1.2">subscript</csymbol><apply id="S3.SS3.p7.3.m3.1.1.2.2.cmml" xref="S3.SS3.p7.3.m3.1.1.2.2"><ci id="S3.SS3.p7.3.m3.1.1.2.2.1.cmml" xref="S3.SS3.p7.3.m3.1.1.2.2.1">~</ci><ci id="S3.SS3.p7.3.m3.1.1.2.2.2.cmml" xref="S3.SS3.p7.3.m3.1.1.2.2.2">𝑋</ci></apply><ci id="S3.SS3.p7.3.m3.1.1.2.3.cmml" xref="S3.SS3.p7.3.m3.1.1.2.3">𝑖</ci></apply><apply id="S3.SS3.p7.3.m3.1.1.3.cmml" xref="S3.SS3.p7.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p7.3.m3.1.1.3.1.cmml" xref="S3.SS3.p7.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS3.p7.3.m3.1.1.3.2.cmml" xref="S3.SS3.p7.3.m3.1.1.3.2">ℝ</ci><apply id="S3.SS3.p7.3.m3.1.1.3.3.cmml" xref="S3.SS3.p7.3.m3.1.1.3.3"><divide id="S3.SS3.p7.3.m3.1.1.3.3.1.cmml" xref="S3.SS3.p7.3.m3.1.1.3.3.1"></divide><apply id="S3.SS3.p7.3.m3.1.1.3.3.2.cmml" xref="S3.SS3.p7.3.m3.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS3.p7.3.m3.1.1.3.3.2.1.cmml" xref="S3.SS3.p7.3.m3.1.1.3.3.2">superscript</csymbol><ci id="S3.SS3.p7.3.m3.1.1.3.3.2.2.cmml" xref="S3.SS3.p7.3.m3.1.1.3.3.2.2">𝐶</ci><apply id="S3.SS3.p7.3.m3.1.1.3.3.2.3.cmml" xref="S3.SS3.p7.3.m3.1.1.3.3.2.3"><times id="S3.SS3.p7.3.m3.1.1.3.3.2.3.1.cmml" xref="S3.SS3.p7.3.m3.1.1.3.3.2.3.1"></times><ci id="S3.SS3.p7.3.m3.1.1.3.3.2.3.2.cmml" xref="S3.SS3.p7.3.m3.1.1.3.3.2.3.2">𝑜</ci><ci id="S3.SS3.p7.3.m3.1.1.3.3.2.3.3.cmml" xref="S3.SS3.p7.3.m3.1.1.3.3.2.3.3">𝑢</ci><ci id="S3.SS3.p7.3.m3.1.1.3.3.2.3.4.cmml" xref="S3.SS3.p7.3.m3.1.1.3.3.2.3.4">𝑡</ci></apply></apply><cn type="integer" id="S3.SS3.p7.3.m3.1.1.3.3.3.cmml" xref="S3.SS3.p7.3.m3.1.1.3.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.3.m3.1c">\tilde{X}_{i}\in\mathbb{R}^{C^{out}/2}</annotation></semantics></math> from the non-empty voxels within a <math id="S3.SS3.p7.4.m4.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S3.SS3.p7.4.m4.1a"><mrow id="S3.SS3.p7.4.m4.1.1" xref="S3.SS3.p7.4.m4.1.1.cmml"><mn id="S3.SS3.p7.4.m4.1.1.2" xref="S3.SS3.p7.4.m4.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p7.4.m4.1.1.1" xref="S3.SS3.p7.4.m4.1.1.1.cmml">×</mo><mn id="S3.SS3.p7.4.m4.1.1.3" xref="S3.SS3.p7.4.m4.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.4.m4.1b"><apply id="S3.SS3.p7.4.m4.1.1.cmml" xref="S3.SS3.p7.4.m4.1.1"><times id="S3.SS3.p7.4.m4.1.1.1.cmml" xref="S3.SS3.p7.4.m4.1.1.1"></times><cn type="integer" id="S3.SS3.p7.4.m4.1.1.2.cmml" xref="S3.SS3.p7.4.m4.1.1.2">3</cn><cn type="integer" id="S3.SS3.p7.4.m4.1.1.3.cmml" xref="S3.SS3.p7.4.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.4.m4.1c">3\times 3</annotation></semantics></math> neighborhood based on the corresponding 2D indices.</p>
</div>
<div id="S3.SS3.p8" class="ltx_para">
<table id="S5.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3.m1.4" class="ltx_Math" alttext="\displaystyle\tilde{X}_{i}=\mathcal{R}\left\{\mathcal{K}^{2D}\left(X_{i},\tilde{X}_{i}^{(f_{1})},...,\tilde{X}_{i}^{(f_{k})}\right)\right\}," display="inline"><semantics id="S3.E3.m1.4a"><mrow id="S3.E3.m1.4.4.1" xref="S3.E3.m1.4.4.1.1.cmml"><mrow id="S3.E3.m1.4.4.1.1" xref="S3.E3.m1.4.4.1.1.cmml"><msub id="S3.E3.m1.4.4.1.1.3" xref="S3.E3.m1.4.4.1.1.3.cmml"><mover accent="true" id="S3.E3.m1.4.4.1.1.3.2" xref="S3.E3.m1.4.4.1.1.3.2.cmml"><mi id="S3.E3.m1.4.4.1.1.3.2.2" xref="S3.E3.m1.4.4.1.1.3.2.2.cmml">X</mi><mo id="S3.E3.m1.4.4.1.1.3.2.1" xref="S3.E3.m1.4.4.1.1.3.2.1.cmml">~</mo></mover><mi id="S3.E3.m1.4.4.1.1.3.3" xref="S3.E3.m1.4.4.1.1.3.3.cmml">i</mi></msub><mo id="S3.E3.m1.4.4.1.1.2" xref="S3.E3.m1.4.4.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.4.4.1.1.1" xref="S3.E3.m1.4.4.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.4.4.1.1.1.3" xref="S3.E3.m1.4.4.1.1.1.3.cmml">ℛ</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.1.1.1.2" xref="S3.E3.m1.4.4.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.4.4.1.1.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.2.cmml"><mo id="S3.E3.m1.4.4.1.1.1.1.1.2" xref="S3.E3.m1.4.4.1.1.1.1.2.cmml">{</mo><mrow id="S3.E3.m1.4.4.1.1.1.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.1.1.cmml"><msup id="S3.E3.m1.4.4.1.1.1.1.1.1.5" xref="S3.E3.m1.4.4.1.1.1.1.1.1.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.4.4.1.1.1.1.1.1.5.2" xref="S3.E3.m1.4.4.1.1.1.1.1.1.5.2.cmml">𝒦</mi><mrow id="S3.E3.m1.4.4.1.1.1.1.1.1.5.3" xref="S3.E3.m1.4.4.1.1.1.1.1.1.5.3.cmml"><mn id="S3.E3.m1.4.4.1.1.1.1.1.1.5.3.2" xref="S3.E3.m1.4.4.1.1.1.1.1.1.5.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.1.1.1.1.1.1.5.3.1" xref="S3.E3.m1.4.4.1.1.1.1.1.1.5.3.1.cmml">​</mo><mi id="S3.E3.m1.4.4.1.1.1.1.1.1.5.3.3" xref="S3.E3.m1.4.4.1.1.1.1.1.1.5.3.3.cmml">D</mi></mrow></msup><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.1.1.1.1.1.1.4" xref="S3.E3.m1.4.4.1.1.1.1.1.1.4.cmml">​</mo><mrow id="S3.E3.m1.4.4.1.1.1.1.1.1.3.3" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3.4.cmml"><mo id="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.4" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3.4.cmml">(</mo><msub id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml">X</mi><mi id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.5" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3.4.cmml">,</mo><msubsup id="S3.E3.m1.4.4.1.1.1.1.1.1.2.2.2" xref="S3.E3.m1.4.4.1.1.1.1.1.1.2.2.2.cmml"><mover accent="true" id="S3.E3.m1.4.4.1.1.1.1.1.1.2.2.2.2.2" xref="S3.E3.m1.4.4.1.1.1.1.1.1.2.2.2.2.2.cmml"><mi id="S3.E3.m1.4.4.1.1.1.1.1.1.2.2.2.2.2.2" xref="S3.E3.m1.4.4.1.1.1.1.1.1.2.2.2.2.2.2.cmml">X</mi><mo id="S3.E3.m1.4.4.1.1.1.1.1.1.2.2.2.2.2.1" xref="S3.E3.m1.4.4.1.1.1.1.1.1.2.2.2.2.2.1.cmml">~</mo></mover><mi id="S3.E3.m1.4.4.1.1.1.1.1.1.2.2.2.2.3" xref="S3.E3.m1.4.4.1.1.1.1.1.1.2.2.2.2.3.cmml">i</mi><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.cmml">(</mo><msub id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.2.cmml">f</mi><mn id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.3.cmml">1</mn></msub><mo stretchy="false" id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.cmml">)</mo></mrow></msubsup><mo id="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.6" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.E3.m1.3.3" xref="S3.E3.m1.3.3.cmml">…</mi><mo id="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.7" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3.4.cmml">,</mo><msubsup id="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.3" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.3.cmml"><mover accent="true" id="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.3.2.2" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.3.2.2.cmml"><mi id="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.3.2.2.2" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.3.2.2.2.cmml">X</mi><mo id="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.3.2.2.1" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.3.2.2.1.cmml">~</mo></mover><mi id="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.3.2.3" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.3.2.3.cmml">i</mi><mrow id="S3.E3.m1.2.2.1.1" xref="S3.E3.m1.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.2.2.1.1.2" xref="S3.E3.m1.2.2.1.1.1.cmml">(</mo><msub id="S3.E3.m1.2.2.1.1.1" xref="S3.E3.m1.2.2.1.1.1.cmml"><mi id="S3.E3.m1.2.2.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.2.cmml">f</mi><mi id="S3.E3.m1.2.2.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.3.cmml">k</mi></msub><mo stretchy="false" id="S3.E3.m1.2.2.1.1.3" xref="S3.E3.m1.2.2.1.1.1.cmml">)</mo></mrow></msubsup><mo id="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.8" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3.4.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.4.4.1.1.1.1.1.3" xref="S3.E3.m1.4.4.1.1.1.1.2.cmml">}</mo></mrow></mrow></mrow><mo id="S3.E3.m1.4.4.1.2" xref="S3.E3.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.4b"><apply id="S3.E3.m1.4.4.1.1.cmml" xref="S3.E3.m1.4.4.1"><eq id="S3.E3.m1.4.4.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.2"></eq><apply id="S3.E3.m1.4.4.1.1.3.cmml" xref="S3.E3.m1.4.4.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.1.1.3.1.cmml" xref="S3.E3.m1.4.4.1.1.3">subscript</csymbol><apply id="S3.E3.m1.4.4.1.1.3.2.cmml" xref="S3.E3.m1.4.4.1.1.3.2"><ci id="S3.E3.m1.4.4.1.1.3.2.1.cmml" xref="S3.E3.m1.4.4.1.1.3.2.1">~</ci><ci id="S3.E3.m1.4.4.1.1.3.2.2.cmml" xref="S3.E3.m1.4.4.1.1.3.2.2">𝑋</ci></apply><ci id="S3.E3.m1.4.4.1.1.3.3.cmml" xref="S3.E3.m1.4.4.1.1.3.3">𝑖</ci></apply><apply id="S3.E3.m1.4.4.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1"><times id="S3.E3.m1.4.4.1.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.1.2"></times><ci id="S3.E3.m1.4.4.1.1.1.3.cmml" xref="S3.E3.m1.4.4.1.1.1.3">ℛ</ci><set id="S3.E3.m1.4.4.1.1.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1"><apply id="S3.E3.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1"><times id="S3.E3.m1.4.4.1.1.1.1.1.1.4.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.4"></times><apply id="S3.E3.m1.4.4.1.1.1.1.1.1.5.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.1.1.1.1.1.1.5.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.5">superscript</csymbol><ci id="S3.E3.m1.4.4.1.1.1.1.1.1.5.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.5.2">𝒦</ci><apply id="S3.E3.m1.4.4.1.1.1.1.1.1.5.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.5.3"><times id="S3.E3.m1.4.4.1.1.1.1.1.1.5.3.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.5.3.1"></times><cn type="integer" id="S3.E3.m1.4.4.1.1.1.1.1.1.5.3.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.5.3.2">2</cn><ci id="S3.E3.m1.4.4.1.1.1.1.1.1.5.3.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.5.3.3">𝐷</ci></apply></apply><vector id="S3.E3.m1.4.4.1.1.1.1.1.1.3.4.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3.3"><apply id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2">𝑋</ci><ci id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E3.m1.4.4.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.2.2.2">superscript</csymbol><apply id="S3.E3.m1.4.4.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.2.2.2">subscript</csymbol><apply id="S3.E3.m1.4.4.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.2.2.2.2.2"><ci id="S3.E3.m1.4.4.1.1.1.1.1.1.2.2.2.2.2.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.2.2.2.2.2.1">~</ci><ci id="S3.E3.m1.4.4.1.1.1.1.1.1.2.2.2.2.2.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.2.2.2.2.2.2">𝑋</ci></apply><ci id="S3.E3.m1.4.4.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.2.2.2.2.3">𝑖</ci></apply><apply id="S3.E3.m1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.2">𝑓</ci><cn type="integer" id="S3.E3.m1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.3">1</cn></apply></apply><ci id="S3.E3.m1.3.3.cmml" xref="S3.E3.m1.3.3">…</ci><apply id="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.3.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.3">superscript</csymbol><apply id="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.3.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.3.2.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.3">subscript</csymbol><apply id="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.3.2.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.3.2.2"><ci id="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.3.2.2.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.3.2.2.1">~</ci><ci id="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.3.2.2.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.3.2.2.2">𝑋</ci></apply><ci id="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.3.2.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.3.2.3">𝑖</ci></apply><apply id="S3.E3.m1.2.2.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.2">𝑓</ci><ci id="S3.E3.m1.2.2.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.3">𝑘</ci></apply></apply></vector></apply></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.4c">\displaystyle\tilde{X}_{i}=\mathcal{R}\left\{\mathcal{K}^{2D}\left(X_{i},\tilde{X}_{i}^{(f_{1})},...,\tilde{X}_{i}^{(f_{k})}\right)\right\},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS3.p9" class="ltx_para ltx_noindent">
<p id="S3.SS3.p9.3" class="ltx_p">where <math id="S3.SS3.p9.1.m1.5" class="ltx_Math" alttext="\tilde{X}_{i}^{(f_{1})},...,\tilde{X}_{i}^{(f_{k})}" display="inline"><semantics id="S3.SS3.p9.1.m1.5a"><mrow id="S3.SS3.p9.1.m1.5.5.2" xref="S3.SS3.p9.1.m1.5.5.3.cmml"><msubsup id="S3.SS3.p9.1.m1.4.4.1.1" xref="S3.SS3.p9.1.m1.4.4.1.1.cmml"><mover accent="true" id="S3.SS3.p9.1.m1.4.4.1.1.2.2" xref="S3.SS3.p9.1.m1.4.4.1.1.2.2.cmml"><mi id="S3.SS3.p9.1.m1.4.4.1.1.2.2.2" xref="S3.SS3.p9.1.m1.4.4.1.1.2.2.2.cmml">X</mi><mo id="S3.SS3.p9.1.m1.4.4.1.1.2.2.1" xref="S3.SS3.p9.1.m1.4.4.1.1.2.2.1.cmml">~</mo></mover><mi id="S3.SS3.p9.1.m1.4.4.1.1.2.3" xref="S3.SS3.p9.1.m1.4.4.1.1.2.3.cmml">i</mi><mrow id="S3.SS3.p9.1.m1.1.1.1.1" xref="S3.SS3.p9.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS3.p9.1.m1.1.1.1.1.2" xref="S3.SS3.p9.1.m1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS3.p9.1.m1.1.1.1.1.1" xref="S3.SS3.p9.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS3.p9.1.m1.1.1.1.1.1.2" xref="S3.SS3.p9.1.m1.1.1.1.1.1.2.cmml">f</mi><mn id="S3.SS3.p9.1.m1.1.1.1.1.1.3" xref="S3.SS3.p9.1.m1.1.1.1.1.1.3.cmml">1</mn></msub><mo stretchy="false" id="S3.SS3.p9.1.m1.1.1.1.1.3" xref="S3.SS3.p9.1.m1.1.1.1.1.1.cmml">)</mo></mrow></msubsup><mo id="S3.SS3.p9.1.m1.5.5.2.3" xref="S3.SS3.p9.1.m1.5.5.3.cmml">,</mo><mi mathvariant="normal" id="S3.SS3.p9.1.m1.3.3" xref="S3.SS3.p9.1.m1.3.3.cmml">…</mi><mo id="S3.SS3.p9.1.m1.5.5.2.4" xref="S3.SS3.p9.1.m1.5.5.3.cmml">,</mo><msubsup id="S3.SS3.p9.1.m1.5.5.2.2" xref="S3.SS3.p9.1.m1.5.5.2.2.cmml"><mover accent="true" id="S3.SS3.p9.1.m1.5.5.2.2.2.2" xref="S3.SS3.p9.1.m1.5.5.2.2.2.2.cmml"><mi id="S3.SS3.p9.1.m1.5.5.2.2.2.2.2" xref="S3.SS3.p9.1.m1.5.5.2.2.2.2.2.cmml">X</mi><mo id="S3.SS3.p9.1.m1.5.5.2.2.2.2.1" xref="S3.SS3.p9.1.m1.5.5.2.2.2.2.1.cmml">~</mo></mover><mi id="S3.SS3.p9.1.m1.5.5.2.2.2.3" xref="S3.SS3.p9.1.m1.5.5.2.2.2.3.cmml">i</mi><mrow id="S3.SS3.p9.1.m1.2.2.1.1" xref="S3.SS3.p9.1.m1.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.SS3.p9.1.m1.2.2.1.1.2" xref="S3.SS3.p9.1.m1.2.2.1.1.1.cmml">(</mo><msub id="S3.SS3.p9.1.m1.2.2.1.1.1" xref="S3.SS3.p9.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS3.p9.1.m1.2.2.1.1.1.2" xref="S3.SS3.p9.1.m1.2.2.1.1.1.2.cmml">f</mi><mi id="S3.SS3.p9.1.m1.2.2.1.1.1.3" xref="S3.SS3.p9.1.m1.2.2.1.1.1.3.cmml">k</mi></msub><mo stretchy="false" id="S3.SS3.p9.1.m1.2.2.1.1.3" xref="S3.SS3.p9.1.m1.2.2.1.1.1.cmml">)</mo></mrow></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p9.1.m1.5b"><list id="S3.SS3.p9.1.m1.5.5.3.cmml" xref="S3.SS3.p9.1.m1.5.5.2"><apply id="S3.SS3.p9.1.m1.4.4.1.1.cmml" xref="S3.SS3.p9.1.m1.4.4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p9.1.m1.4.4.1.1.1.cmml" xref="S3.SS3.p9.1.m1.4.4.1.1">superscript</csymbol><apply id="S3.SS3.p9.1.m1.4.4.1.1.2.cmml" xref="S3.SS3.p9.1.m1.4.4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p9.1.m1.4.4.1.1.2.1.cmml" xref="S3.SS3.p9.1.m1.4.4.1.1">subscript</csymbol><apply id="S3.SS3.p9.1.m1.4.4.1.1.2.2.cmml" xref="S3.SS3.p9.1.m1.4.4.1.1.2.2"><ci id="S3.SS3.p9.1.m1.4.4.1.1.2.2.1.cmml" xref="S3.SS3.p9.1.m1.4.4.1.1.2.2.1">~</ci><ci id="S3.SS3.p9.1.m1.4.4.1.1.2.2.2.cmml" xref="S3.SS3.p9.1.m1.4.4.1.1.2.2.2">𝑋</ci></apply><ci id="S3.SS3.p9.1.m1.4.4.1.1.2.3.cmml" xref="S3.SS3.p9.1.m1.4.4.1.1.2.3">𝑖</ci></apply><apply id="S3.SS3.p9.1.m1.1.1.1.1.1.cmml" xref="S3.SS3.p9.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p9.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS3.p9.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p9.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS3.p9.1.m1.1.1.1.1.1.2">𝑓</ci><cn type="integer" id="S3.SS3.p9.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS3.p9.1.m1.1.1.1.1.1.3">1</cn></apply></apply><ci id="S3.SS3.p9.1.m1.3.3.cmml" xref="S3.SS3.p9.1.m1.3.3">…</ci><apply id="S3.SS3.p9.1.m1.5.5.2.2.cmml" xref="S3.SS3.p9.1.m1.5.5.2.2"><csymbol cd="ambiguous" id="S3.SS3.p9.1.m1.5.5.2.2.1.cmml" xref="S3.SS3.p9.1.m1.5.5.2.2">superscript</csymbol><apply id="S3.SS3.p9.1.m1.5.5.2.2.2.cmml" xref="S3.SS3.p9.1.m1.5.5.2.2"><csymbol cd="ambiguous" id="S3.SS3.p9.1.m1.5.5.2.2.2.1.cmml" xref="S3.SS3.p9.1.m1.5.5.2.2">subscript</csymbol><apply id="S3.SS3.p9.1.m1.5.5.2.2.2.2.cmml" xref="S3.SS3.p9.1.m1.5.5.2.2.2.2"><ci id="S3.SS3.p9.1.m1.5.5.2.2.2.2.1.cmml" xref="S3.SS3.p9.1.m1.5.5.2.2.2.2.1">~</ci><ci id="S3.SS3.p9.1.m1.5.5.2.2.2.2.2.cmml" xref="S3.SS3.p9.1.m1.5.5.2.2.2.2.2">𝑋</ci></apply><ci id="S3.SS3.p9.1.m1.5.5.2.2.2.3.cmml" xref="S3.SS3.p9.1.m1.5.5.2.2.2.3">𝑖</ci></apply><apply id="S3.SS3.p9.1.m1.2.2.1.1.1.cmml" xref="S3.SS3.p9.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p9.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS3.p9.1.m1.2.2.1.1">subscript</csymbol><ci id="S3.SS3.p9.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS3.p9.1.m1.2.2.1.1.1.2">𝑓</ci><ci id="S3.SS3.p9.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS3.p9.1.m1.2.2.1.1.1.3">𝑘</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p9.1.m1.5c">\tilde{X}_{i}^{(f_{1})},...,\tilde{X}_{i}^{(f_{k})}</annotation></semantics></math> denote the neighbor voxel features generated by <math id="S3.SS3.p9.2.m2.1" class="ltx_Math" alttext="\mathbf{\hat{H}}" display="inline"><semantics id="S3.SS3.p9.2.m2.1a"><mover accent="true" id="S3.SS3.p9.2.m2.1.1" xref="S3.SS3.p9.2.m2.1.1.cmml"><mi id="S3.SS3.p9.2.m2.1.1.2" xref="S3.SS3.p9.2.m2.1.1.2.cmml">𝐇</mi><mo id="S3.SS3.p9.2.m2.1.1.1" xref="S3.SS3.p9.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS3.p9.2.m2.1b"><apply id="S3.SS3.p9.2.m2.1.1.cmml" xref="S3.SS3.p9.2.m2.1.1"><ci id="S3.SS3.p9.2.m2.1.1.1.cmml" xref="S3.SS3.p9.2.m2.1.1.1">^</ci><ci id="S3.SS3.p9.2.m2.1.1.2.cmml" xref="S3.SS3.p9.2.m2.1.1.2">𝐇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p9.2.m2.1c">\mathbf{\hat{H}}</annotation></semantics></math>, and <math id="S3.SS3.p9.3.m3.1" class="ltx_Math" alttext="\mathcal{K}^{2D}(\cdot)" display="inline"><semantics id="S3.SS3.p9.3.m3.1a"><mrow id="S3.SS3.p9.3.m3.1.2" xref="S3.SS3.p9.3.m3.1.2.cmml"><msup id="S3.SS3.p9.3.m3.1.2.2" xref="S3.SS3.p9.3.m3.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p9.3.m3.1.2.2.2" xref="S3.SS3.p9.3.m3.1.2.2.2.cmml">𝒦</mi><mrow id="S3.SS3.p9.3.m3.1.2.2.3" xref="S3.SS3.p9.3.m3.1.2.2.3.cmml"><mn id="S3.SS3.p9.3.m3.1.2.2.3.2" xref="S3.SS3.p9.3.m3.1.2.2.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS3.p9.3.m3.1.2.2.3.1" xref="S3.SS3.p9.3.m3.1.2.2.3.1.cmml">​</mo><mi id="S3.SS3.p9.3.m3.1.2.2.3.3" xref="S3.SS3.p9.3.m3.1.2.2.3.3.cmml">D</mi></mrow></msup><mo lspace="0em" rspace="0em" id="S3.SS3.p9.3.m3.1.2.1" xref="S3.SS3.p9.3.m3.1.2.1.cmml">​</mo><mrow id="S3.SS3.p9.3.m3.1.2.3.2" xref="S3.SS3.p9.3.m3.1.2.cmml"><mo stretchy="false" id="S3.SS3.p9.3.m3.1.2.3.2.1" xref="S3.SS3.p9.3.m3.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS3.p9.3.m3.1.1" xref="S3.SS3.p9.3.m3.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS3.p9.3.m3.1.2.3.2.2" xref="S3.SS3.p9.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p9.3.m3.1b"><apply id="S3.SS3.p9.3.m3.1.2.cmml" xref="S3.SS3.p9.3.m3.1.2"><times id="S3.SS3.p9.3.m3.1.2.1.cmml" xref="S3.SS3.p9.3.m3.1.2.1"></times><apply id="S3.SS3.p9.3.m3.1.2.2.cmml" xref="S3.SS3.p9.3.m3.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.p9.3.m3.1.2.2.1.cmml" xref="S3.SS3.p9.3.m3.1.2.2">superscript</csymbol><ci id="S3.SS3.p9.3.m3.1.2.2.2.cmml" xref="S3.SS3.p9.3.m3.1.2.2.2">𝒦</ci><apply id="S3.SS3.p9.3.m3.1.2.2.3.cmml" xref="S3.SS3.p9.3.m3.1.2.2.3"><times id="S3.SS3.p9.3.m3.1.2.2.3.1.cmml" xref="S3.SS3.p9.3.m3.1.2.2.3.1"></times><cn type="integer" id="S3.SS3.p9.3.m3.1.2.2.3.2.cmml" xref="S3.SS3.p9.3.m3.1.2.2.3.2">2</cn><ci id="S3.SS3.p9.3.m3.1.2.2.3.3.cmml" xref="S3.SS3.p9.3.m3.1.2.2.3.3">𝐷</ci></apply></apply><ci id="S3.SS3.p9.3.m3.1.1.cmml" xref="S3.SS3.p9.3.m3.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p9.3.m3.1c">\mathcal{K}^{2D}(\cdot)</annotation></semantics></math> denote the 2D submanifold convolution kernel. If there are multiple features in a single 2D neighbor voxel, we perform max-pooling and keep one feature in each voxel to perform the 2D convolution.</p>
</div>
<div id="S3.SS3.p10" class="ltx_para">
<p id="S3.SS3.p10.3" class="ltx_p">After the 3D and 2D features encoding, we adopt a simple concatenation to implicitly learn a noise-resistant feature.
Specifically, we finally concatenate <math id="S3.SS3.p10.1.m1.1" class="ltx_Math" alttext="\hat{X}_{i}" display="inline"><semantics id="S3.SS3.p10.1.m1.1a"><msub id="S3.SS3.p10.1.m1.1.1" xref="S3.SS3.p10.1.m1.1.1.cmml"><mover accent="true" id="S3.SS3.p10.1.m1.1.1.2" xref="S3.SS3.p10.1.m1.1.1.2.cmml"><mi id="S3.SS3.p10.1.m1.1.1.2.2" xref="S3.SS3.p10.1.m1.1.1.2.2.cmml">X</mi><mo id="S3.SS3.p10.1.m1.1.1.2.1" xref="S3.SS3.p10.1.m1.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS3.p10.1.m1.1.1.3" xref="S3.SS3.p10.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p10.1.m1.1b"><apply id="S3.SS3.p10.1.m1.1.1.cmml" xref="S3.SS3.p10.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p10.1.m1.1.1.1.cmml" xref="S3.SS3.p10.1.m1.1.1">subscript</csymbol><apply id="S3.SS3.p10.1.m1.1.1.2.cmml" xref="S3.SS3.p10.1.m1.1.1.2"><ci id="S3.SS3.p10.1.m1.1.1.2.1.cmml" xref="S3.SS3.p10.1.m1.1.1.2.1">^</ci><ci id="S3.SS3.p10.1.m1.1.1.2.2.cmml" xref="S3.SS3.p10.1.m1.1.1.2.2">𝑋</ci></apply><ci id="S3.SS3.p10.1.m1.1.1.3.cmml" xref="S3.SS3.p10.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p10.1.m1.1c">\hat{X}_{i}</annotation></semantics></math> and <math id="S3.SS3.p10.2.m2.1" class="ltx_Math" alttext="\tilde{X}_{i}" display="inline"><semantics id="S3.SS3.p10.2.m2.1a"><msub id="S3.SS3.p10.2.m2.1.1" xref="S3.SS3.p10.2.m2.1.1.cmml"><mover accent="true" id="S3.SS3.p10.2.m2.1.1.2" xref="S3.SS3.p10.2.m2.1.1.2.cmml"><mi id="S3.SS3.p10.2.m2.1.1.2.2" xref="S3.SS3.p10.2.m2.1.1.2.2.cmml">X</mi><mo id="S3.SS3.p10.2.m2.1.1.2.1" xref="S3.SS3.p10.2.m2.1.1.2.1.cmml">~</mo></mover><mi id="S3.SS3.p10.2.m2.1.1.3" xref="S3.SS3.p10.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p10.2.m2.1b"><apply id="S3.SS3.p10.2.m2.1.1.cmml" xref="S3.SS3.p10.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p10.2.m2.1.1.1.cmml" xref="S3.SS3.p10.2.m2.1.1">subscript</csymbol><apply id="S3.SS3.p10.2.m2.1.1.2.cmml" xref="S3.SS3.p10.2.m2.1.1.2"><ci id="S3.SS3.p10.2.m2.1.1.2.1.cmml" xref="S3.SS3.p10.2.m2.1.1.2.1">~</ci><ci id="S3.SS3.p10.2.m2.1.1.2.2.cmml" xref="S3.SS3.p10.2.m2.1.1.2.2">𝑋</ci></apply><ci id="S3.SS3.p10.2.m2.1.1.3.cmml" xref="S3.SS3.p10.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p10.2.m2.1c">\tilde{X}_{i}</annotation></semantics></math> to obtain the noise-resistant feature vector <math id="S3.SS3.p10.3.m3.1" class="ltx_Math" alttext="\mathbf{Y}\in\mathbb{R}^{N\times C^{out}}" display="inline"><semantics id="S3.SS3.p10.3.m3.1a"><mrow id="S3.SS3.p10.3.m3.1.1" xref="S3.SS3.p10.3.m3.1.1.cmml"><mi id="S3.SS3.p10.3.m3.1.1.2" xref="S3.SS3.p10.3.m3.1.1.2.cmml">𝐘</mi><mo id="S3.SS3.p10.3.m3.1.1.1" xref="S3.SS3.p10.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS3.p10.3.m3.1.1.3" xref="S3.SS3.p10.3.m3.1.1.3.cmml"><mi id="S3.SS3.p10.3.m3.1.1.3.2" xref="S3.SS3.p10.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS3.p10.3.m3.1.1.3.3" xref="S3.SS3.p10.3.m3.1.1.3.3.cmml"><mi id="S3.SS3.p10.3.m3.1.1.3.3.2" xref="S3.SS3.p10.3.m3.1.1.3.3.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p10.3.m3.1.1.3.3.1" xref="S3.SS3.p10.3.m3.1.1.3.3.1.cmml">×</mo><msup id="S3.SS3.p10.3.m3.1.1.3.3.3" xref="S3.SS3.p10.3.m3.1.1.3.3.3.cmml"><mi id="S3.SS3.p10.3.m3.1.1.3.3.3.2" xref="S3.SS3.p10.3.m3.1.1.3.3.3.2.cmml">C</mi><mrow id="S3.SS3.p10.3.m3.1.1.3.3.3.3" xref="S3.SS3.p10.3.m3.1.1.3.3.3.3.cmml"><mi id="S3.SS3.p10.3.m3.1.1.3.3.3.3.2" xref="S3.SS3.p10.3.m3.1.1.3.3.3.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p10.3.m3.1.1.3.3.3.3.1" xref="S3.SS3.p10.3.m3.1.1.3.3.3.3.1.cmml">​</mo><mi id="S3.SS3.p10.3.m3.1.1.3.3.3.3.3" xref="S3.SS3.p10.3.m3.1.1.3.3.3.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p10.3.m3.1.1.3.3.3.3.1a" xref="S3.SS3.p10.3.m3.1.1.3.3.3.3.1.cmml">​</mo><mi id="S3.SS3.p10.3.m3.1.1.3.3.3.3.4" xref="S3.SS3.p10.3.m3.1.1.3.3.3.3.4.cmml">t</mi></mrow></msup></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p10.3.m3.1b"><apply id="S3.SS3.p10.3.m3.1.1.cmml" xref="S3.SS3.p10.3.m3.1.1"><in id="S3.SS3.p10.3.m3.1.1.1.cmml" xref="S3.SS3.p10.3.m3.1.1.1"></in><ci id="S3.SS3.p10.3.m3.1.1.2.cmml" xref="S3.SS3.p10.3.m3.1.1.2">𝐘</ci><apply id="S3.SS3.p10.3.m3.1.1.3.cmml" xref="S3.SS3.p10.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p10.3.m3.1.1.3.1.cmml" xref="S3.SS3.p10.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS3.p10.3.m3.1.1.3.2.cmml" xref="S3.SS3.p10.3.m3.1.1.3.2">ℝ</ci><apply id="S3.SS3.p10.3.m3.1.1.3.3.cmml" xref="S3.SS3.p10.3.m3.1.1.3.3"><times id="S3.SS3.p10.3.m3.1.1.3.3.1.cmml" xref="S3.SS3.p10.3.m3.1.1.3.3.1"></times><ci id="S3.SS3.p10.3.m3.1.1.3.3.2.cmml" xref="S3.SS3.p10.3.m3.1.1.3.3.2">𝑁</ci><apply id="S3.SS3.p10.3.m3.1.1.3.3.3.cmml" xref="S3.SS3.p10.3.m3.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p10.3.m3.1.1.3.3.3.1.cmml" xref="S3.SS3.p10.3.m3.1.1.3.3.3">superscript</csymbol><ci id="S3.SS3.p10.3.m3.1.1.3.3.3.2.cmml" xref="S3.SS3.p10.3.m3.1.1.3.3.3.2">𝐶</ci><apply id="S3.SS3.p10.3.m3.1.1.3.3.3.3.cmml" xref="S3.SS3.p10.3.m3.1.1.3.3.3.3"><times id="S3.SS3.p10.3.m3.1.1.3.3.3.3.1.cmml" xref="S3.SS3.p10.3.m3.1.1.3.3.3.3.1"></times><ci id="S3.SS3.p10.3.m3.1.1.3.3.3.3.2.cmml" xref="S3.SS3.p10.3.m3.1.1.3.3.3.3.2">𝑜</ci><ci id="S3.SS3.p10.3.m3.1.1.3.3.3.3.3.cmml" xref="S3.SS3.p10.3.m3.1.1.3.3.3.3.3">𝑢</ci><ci id="S3.SS3.p10.3.m3.1.1.3.3.3.3.4.cmml" xref="S3.SS3.p10.3.m3.1.1.3.3.3.3.4">𝑡</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p10.3.m3.1c">\mathbf{Y}\in\mathbb{R}^{N\times C^{out}}</annotation></semantics></math> as</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.2" class="ltx_Math" alttext="\mathbf{Y}=\left[\left[\hat{X}_{i},\tilde{X}_{i}\right]^{T},...,\left[\hat{X}_{N},\tilde{X}_{N}\right]^{T}\right]^{T}." display="block"><semantics id="S3.E4.m1.2a"><mrow id="S3.E4.m1.2.2.1" xref="S3.E4.m1.2.2.1.1.cmml"><mrow id="S3.E4.m1.2.2.1.1" xref="S3.E4.m1.2.2.1.1.cmml"><mi id="S3.E4.m1.2.2.1.1.4" xref="S3.E4.m1.2.2.1.1.4.cmml">𝐘</mi><mo id="S3.E4.m1.2.2.1.1.3" xref="S3.E4.m1.2.2.1.1.3.cmml">=</mo><msup id="S3.E4.m1.2.2.1.1.2" xref="S3.E4.m1.2.2.1.1.2.cmml"><mrow id="S3.E4.m1.2.2.1.1.2.2.2" xref="S3.E4.m1.2.2.1.1.2.2.3.cmml"><mo id="S3.E4.m1.2.2.1.1.2.2.2.3" xref="S3.E4.m1.2.2.1.1.2.2.3.cmml">[</mo><msup id="S3.E4.m1.2.2.1.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.cmml"><mrow id="S3.E4.m1.2.2.1.1.1.1.1.1.2.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.2.3.cmml"><mo id="S3.E4.m1.2.2.1.1.1.1.1.1.2.2.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.2.3.cmml">[</mo><msub id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.cmml">X</mi><mo id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E4.m1.2.2.1.1.1.1.1.1.2.2.4" xref="S3.E4.m1.2.2.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S3.E4.m1.2.2.1.1.1.1.1.1.2.2.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.2.2.2.cmml"><mover accent="true" id="S3.E4.m1.2.2.1.1.1.1.1.1.2.2.2.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.2.2.2.2.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.2.2.2.2.2.cmml">X</mi><mo id="S3.E4.m1.2.2.1.1.1.1.1.1.2.2.2.2.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.2.2.2.2.1.cmml">~</mo></mover><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.2.2.2.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.2.2.2.3.cmml">i</mi></msub><mo id="S3.E4.m1.2.2.1.1.1.1.1.1.2.2.5" xref="S3.E4.m1.2.2.1.1.1.1.1.1.2.3.cmml">]</mo></mrow><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.4" xref="S3.E4.m1.2.2.1.1.1.1.1.1.4.cmml">T</mi></msup><mo id="S3.E4.m1.2.2.1.1.2.2.2.4" xref="S3.E4.m1.2.2.1.1.2.2.3.cmml">,</mo><mi mathvariant="normal" id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">…</mi><mo id="S3.E4.m1.2.2.1.1.2.2.2.5" xref="S3.E4.m1.2.2.1.1.2.2.3.cmml">,</mo><msup id="S3.E4.m1.2.2.1.1.2.2.2.2" xref="S3.E4.m1.2.2.1.1.2.2.2.2.cmml"><mrow id="S3.E4.m1.2.2.1.1.2.2.2.2.2.2" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.3.cmml"><mo id="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.3" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.3.cmml">[</mo><msub id="S3.E4.m1.2.2.1.1.2.2.2.2.1.1.1" xref="S3.E4.m1.2.2.1.1.2.2.2.2.1.1.1.cmml"><mover accent="true" id="S3.E4.m1.2.2.1.1.2.2.2.2.1.1.1.2" xref="S3.E4.m1.2.2.1.1.2.2.2.2.1.1.1.2.cmml"><mi id="S3.E4.m1.2.2.1.1.2.2.2.2.1.1.1.2.2" xref="S3.E4.m1.2.2.1.1.2.2.2.2.1.1.1.2.2.cmml">X</mi><mo id="S3.E4.m1.2.2.1.1.2.2.2.2.1.1.1.2.1" xref="S3.E4.m1.2.2.1.1.2.2.2.2.1.1.1.2.1.cmml">^</mo></mover><mi id="S3.E4.m1.2.2.1.1.2.2.2.2.1.1.1.3" xref="S3.E4.m1.2.2.1.1.2.2.2.2.1.1.1.3.cmml">N</mi></msub><mo id="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.4" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.3.cmml">,</mo><msub id="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.cmml"><mover accent="true" id="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.2" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.2.cmml"><mi id="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.2.2" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.2.2.cmml">X</mi><mo id="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.2.1" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.2.1.cmml">~</mo></mover><mi id="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.3" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.3.cmml">N</mi></msub><mo id="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.5" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.3.cmml">]</mo></mrow><mi id="S3.E4.m1.2.2.1.1.2.2.2.2.4" xref="S3.E4.m1.2.2.1.1.2.2.2.2.4.cmml">T</mi></msup><mo id="S3.E4.m1.2.2.1.1.2.2.2.6" xref="S3.E4.m1.2.2.1.1.2.2.3.cmml">]</mo></mrow><mi id="S3.E4.m1.2.2.1.1.2.4" xref="S3.E4.m1.2.2.1.1.2.4.cmml">T</mi></msup></mrow><mo lspace="0em" id="S3.E4.m1.2.2.1.2" xref="S3.E4.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.2b"><apply id="S3.E4.m1.2.2.1.1.cmml" xref="S3.E4.m1.2.2.1"><eq id="S3.E4.m1.2.2.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.3"></eq><ci id="S3.E4.m1.2.2.1.1.4.cmml" xref="S3.E4.m1.2.2.1.1.4">𝐘</ci><apply id="S3.E4.m1.2.2.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.2.3.cmml" xref="S3.E4.m1.2.2.1.1.2">superscript</csymbol><list id="S3.E4.m1.2.2.1.1.2.2.3.cmml" xref="S3.E4.m1.2.2.1.1.2.2.2"><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1">superscript</csymbol><interval closure="closed" id="S3.E4.m1.2.2.1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.2.2"><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2"><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2.1">^</ci><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2.2">𝑋</ci></apply><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.2.2.2">subscript</csymbol><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.2.2.2.2"><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.2.2.2.2.1">~</ci><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.2.2.2.2.2">𝑋</ci></apply><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.2.2.2.3">𝑖</ci></apply></interval><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.4.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.4">𝑇</ci></apply><ci id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1">…</ci><apply id="S3.E4.m1.2.2.1.1.2.2.2.2.cmml" xref="S3.E4.m1.2.2.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.2.2.2.2.3.cmml" xref="S3.E4.m1.2.2.1.1.2.2.2.2">superscript</csymbol><interval closure="closed" id="S3.E4.m1.2.2.1.1.2.2.2.2.2.3.cmml" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.2"><apply id="S3.E4.m1.2.2.1.1.2.2.2.2.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.2.2.2.2.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.2.2.2.2.1.1.1">subscript</csymbol><apply id="S3.E4.m1.2.2.1.1.2.2.2.2.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.2.2.2.2.1.1.1.2"><ci id="S3.E4.m1.2.2.1.1.2.2.2.2.1.1.1.2.1.cmml" xref="S3.E4.m1.2.2.1.1.2.2.2.2.1.1.1.2.1">^</ci><ci id="S3.E4.m1.2.2.1.1.2.2.2.2.1.1.1.2.2.cmml" xref="S3.E4.m1.2.2.1.1.2.2.2.2.1.1.1.2.2">𝑋</ci></apply><ci id="S3.E4.m1.2.2.1.1.2.2.2.2.1.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.2.2.2.2.1.1.1.3">𝑁</ci></apply><apply id="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.cmml" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.1.cmml" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2">subscript</csymbol><apply id="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.2.cmml" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.2"><ci id="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.2.1.cmml" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.2.1">~</ci><ci id="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.2.2.cmml" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.2.2">𝑋</ci></apply><ci id="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.3.cmml" xref="S3.E4.m1.2.2.1.1.2.2.2.2.2.2.2.3">𝑁</ci></apply></interval><ci id="S3.E4.m1.2.2.1.1.2.2.2.2.4.cmml" xref="S3.E4.m1.2.2.1.1.2.2.2.2.4">𝑇</ci></apply></list><ci id="S3.E4.m1.2.2.1.1.2.4.cmml" xref="S3.E4.m1.2.2.1.1.2.4">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.2c">\mathbf{Y}=\left[\left[\hat{X}_{i},\tilde{X}_{i}\right]^{T},...,\left[\hat{X}_{N},\tilde{X}_{N}\right]^{T}\right]^{T}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p10.4" class="ltx_p">Different from related noise segmentation and removal <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> methods, our NRConv implicitly distinguishes the noise pattern by extending the receptive field to 2D image space. Consequently, the impact of noise is suppressed without lose of shape cues.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2303.02314/assets/x5.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="194" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.3.2" class="ltx_text" style="font-size:90%;">Transformed refinement scheme. The inputs are first transformed with different rotations and reflections. Then, VoxelNet and VirConvNet encode the LiDAR and virtual points features, respectively. Next, RoIs are generated and refined by the backbone features under different transformations. At last, the refined RoIs from different stages are fused by boxes voting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</span></figcaption>
</figure>
<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:496.9pt;height:290.2pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-45.1pt,26.3pt) scale(0.846256441139462,0.846256441139462) ;">
<table id="S3.T1.4.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.4.1.1.1" class="ltx_tr">
<td id="S3.T1.4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S3.T1.4.1.1.1.1.1" class="ltx_text">Method</span></td>
<td id="S3.T1.4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S3.T1.4.1.1.1.2.1" class="ltx_text">Reference</span></td>
<td id="S3.T1.4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S3.T1.4.1.1.1.3.1" class="ltx_text">Modality</span></td>
<td id="S3.T1.4.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3">Car 3D AP (R40)</td>
<td id="S3.T1.4.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3">Car BEV AP (R40)</td>
<td id="S3.T1.4.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S3.T1.4.1.1.1.6.1" class="ltx_text">Time (ms)</span></td>
</tr>
<tr id="S3.T1.4.1.2.2" class="ltx_tr">
<td id="S3.T1.4.1.2.2.1" class="ltx_td ltx_align_center">Easy</td>
<td id="S3.T1.4.1.2.2.2" class="ltx_td ltx_align_center">Mod.</td>
<td id="S3.T1.4.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r">Hard</td>
<td id="S3.T1.4.1.2.2.4" class="ltx_td ltx_align_center">Easy</td>
<td id="S3.T1.4.1.2.2.5" class="ltx_td ltx_align_center">Mod.</td>
<td id="S3.T1.4.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r">Hard</td>
</tr>
<tr id="S3.T1.4.1.3.3" class="ltx_tr">
<td id="S3.T1.4.1.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">PV-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</td>
<td id="S3.T1.4.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">CVPR 2020</td>
<td id="S3.T1.4.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">LiDAR</td>
<td id="S3.T1.4.1.3.3.4" class="ltx_td ltx_align_center ltx_border_tt">90.25</td>
<td id="S3.T1.4.1.3.3.5" class="ltx_td ltx_align_center ltx_border_tt">81.43</td>
<td id="S3.T1.4.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">76.82</td>
<td id="S3.T1.4.1.3.3.7" class="ltx_td ltx_align_center ltx_border_tt">94.98</td>
<td id="S3.T1.4.1.3.3.8" class="ltx_td ltx_align_center ltx_border_tt">90.65</td>
<td id="S3.T1.4.1.3.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">86.14</td>
<td id="S3.T1.4.1.3.3.10" class="ltx_td ltx_align_center ltx_border_tt">80*</td>
</tr>
<tr id="S3.T1.4.1.4.4" class="ltx_tr">
<td id="S3.T1.4.1.4.4.1" class="ltx_td ltx_align_center ltx_border_r">Voxel-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</td>
<td id="S3.T1.4.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r">AAAI 2021</td>
<td id="S3.T1.4.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r">LiDAR</td>
<td id="S3.T1.4.1.4.4.4" class="ltx_td ltx_align_center">90.90</td>
<td id="S3.T1.4.1.4.4.5" class="ltx_td ltx_align_center">81.62</td>
<td id="S3.T1.4.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r">77.06</td>
<td id="S3.T1.4.1.4.4.7" class="ltx_td ltx_align_center">94.85</td>
<td id="S3.T1.4.1.4.4.8" class="ltx_td ltx_align_center">88.83</td>
<td id="S3.T1.4.1.4.4.9" class="ltx_td ltx_align_center ltx_border_r">86.13</td>
<td id="S3.T1.4.1.4.4.10" class="ltx_td ltx_align_center">40</td>
</tr>
<tr id="S3.T1.4.1.5.5" class="ltx_tr">
<td id="S3.T1.4.1.5.5.1" class="ltx_td ltx_align_center ltx_border_r">CT3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</td>
<td id="S3.T1.4.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r">ICCV 2021</td>
<td id="S3.T1.4.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r">LiDAR</td>
<td id="S3.T1.4.1.5.5.4" class="ltx_td ltx_align_center">87.83</td>
<td id="S3.T1.4.1.5.5.5" class="ltx_td ltx_align_center">81.77</td>
<td id="S3.T1.4.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r">77.16</td>
<td id="S3.T1.4.1.5.5.7" class="ltx_td ltx_align_center">92.36</td>
<td id="S3.T1.4.1.5.5.8" class="ltx_td ltx_align_center">88.83</td>
<td id="S3.T1.4.1.5.5.9" class="ltx_td ltx_align_center ltx_border_r">84.07</td>
<td id="S3.T1.4.1.5.5.10" class="ltx_td ltx_align_center">70*</td>
</tr>
<tr id="S3.T1.4.1.6.6" class="ltx_tr">
<td id="S3.T1.4.1.6.6.1" class="ltx_td ltx_align_center ltx_border_r">SE-SSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>
</td>
<td id="S3.T1.4.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r">CVPR 2021</td>
<td id="S3.T1.4.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r">LiDAR</td>
<td id="S3.T1.4.1.6.6.4" class="ltx_td ltx_align_center">91.49</td>
<td id="S3.T1.4.1.6.6.5" class="ltx_td ltx_align_center">82.54</td>
<td id="S3.T1.4.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r">77.15</td>
<td id="S3.T1.4.1.6.6.7" class="ltx_td ltx_align_center">95.68</td>
<td id="S3.T1.4.1.6.6.8" class="ltx_td ltx_align_center">91.84</td>
<td id="S3.T1.4.1.6.6.9" class="ltx_td ltx_align_center ltx_border_r">86.72</td>
<td id="S3.T1.4.1.6.6.10" class="ltx_td ltx_align_center"><span id="S3.T1.4.1.6.6.10.1" class="ltx_text ltx_font_bold">30</span></td>
</tr>
<tr id="S3.T1.4.1.7.7" class="ltx_tr">
<td id="S3.T1.4.1.7.7.1" class="ltx_td ltx_align_center ltx_border_r">BtcDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
</td>
<td id="S3.T1.4.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r">AAAI 2022</td>
<td id="S3.T1.4.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r">LiDAR</td>
<td id="S3.T1.4.1.7.7.4" class="ltx_td ltx_align_center">90.64</td>
<td id="S3.T1.4.1.7.7.5" class="ltx_td ltx_align_center">82.86</td>
<td id="S3.T1.4.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r">78.09</td>
<td id="S3.T1.4.1.7.7.7" class="ltx_td ltx_align_center">92.81</td>
<td id="S3.T1.4.1.7.7.8" class="ltx_td ltx_align_center">89.34</td>
<td id="S3.T1.4.1.7.7.9" class="ltx_td ltx_align_center ltx_border_r">84.55</td>
<td id="S3.T1.4.1.7.7.10" class="ltx_td ltx_align_center">90</td>
</tr>
<tr id="S3.T1.4.1.8.8" class="ltx_tr">
<td id="S3.T1.4.1.8.8.1" class="ltx_td ltx_align_center ltx_border_r">CasA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</td>
<td id="S3.T1.4.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r">TGRS 2022</td>
<td id="S3.T1.4.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r">LiDAR</td>
<td id="S3.T1.4.1.8.8.4" class="ltx_td ltx_align_center">91.58</td>
<td id="S3.T1.4.1.8.8.5" class="ltx_td ltx_align_center">83.06</td>
<td id="S3.T1.4.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r">80.08</td>
<td id="S3.T1.4.1.8.8.7" class="ltx_td ltx_align_center">95.19</td>
<td id="S3.T1.4.1.8.8.8" class="ltx_td ltx_align_center">91.54</td>
<td id="S3.T1.4.1.8.8.9" class="ltx_td ltx_align_center ltx_border_r">86.82</td>
<td id="S3.T1.4.1.8.8.10" class="ltx_td ltx_align_center">86</td>
</tr>
<tr id="S3.T1.4.1.9.9" class="ltx_tr">
<td id="S3.T1.4.1.9.9.1" class="ltx_td ltx_align_center ltx_border_r">Graph-Po <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>
</td>
<td id="S3.T1.4.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r">ECCV 2022</td>
<td id="S3.T1.4.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r">LiDAR</td>
<td id="S3.T1.4.1.9.9.4" class="ltx_td ltx_align_center">91.79</td>
<td id="S3.T1.4.1.9.9.5" class="ltx_td ltx_align_center">83.18</td>
<td id="S3.T1.4.1.9.9.6" class="ltx_td ltx_align_center ltx_border_r">77.98</td>
<td id="S3.T1.4.1.9.9.7" class="ltx_td ltx_align_center">95.79</td>
<td id="S3.T1.4.1.9.9.8" class="ltx_td ltx_align_center">92.12</td>
<td id="S3.T1.4.1.9.9.9" class="ltx_td ltx_align_center ltx_border_r">87.11</td>
<td id="S3.T1.4.1.9.9.10" class="ltx_td ltx_align_center">60</td>
</tr>
<tr id="S3.T1.4.1.10.10" class="ltx_tr">
<td id="S3.T1.4.1.10.10.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">F-PointNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>
</td>
<td id="S3.T1.4.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CVPR 2018</td>
<td id="S3.T1.4.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LiDAR+RGB</td>
<td id="S3.T1.4.1.10.10.4" class="ltx_td ltx_align_center ltx_border_t">82.19</td>
<td id="S3.T1.4.1.10.10.5" class="ltx_td ltx_align_center ltx_border_t">69.79</td>
<td id="S3.T1.4.1.10.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">60.59</td>
<td id="S3.T1.4.1.10.10.7" class="ltx_td ltx_align_center ltx_border_t">91.17</td>
<td id="S3.T1.4.1.10.10.8" class="ltx_td ltx_align_center ltx_border_t">84.67</td>
<td id="S3.T1.4.1.10.10.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">74.77</td>
<td id="S3.T1.4.1.10.10.10" class="ltx_td ltx_align_center ltx_border_t">170*</td>
</tr>
<tr id="S3.T1.4.1.11.11" class="ltx_tr">
<td id="S3.T1.4.1.11.11.1" class="ltx_td ltx_align_center ltx_border_r">UberATG-MMF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>
</td>
<td id="S3.T1.4.1.11.11.2" class="ltx_td ltx_align_center ltx_border_r">CVPR 2019</td>
<td id="S3.T1.4.1.11.11.3" class="ltx_td ltx_align_center ltx_border_r">LiDAR+RGB</td>
<td id="S3.T1.4.1.11.11.4" class="ltx_td ltx_align_center">88.40</td>
<td id="S3.T1.4.1.11.11.5" class="ltx_td ltx_align_center">77.43</td>
<td id="S3.T1.4.1.11.11.6" class="ltx_td ltx_align_center ltx_border_r">70.22</td>
<td id="S3.T1.4.1.11.11.7" class="ltx_td ltx_align_center">93.67</td>
<td id="S3.T1.4.1.11.11.8" class="ltx_td ltx_align_center">88.21</td>
<td id="S3.T1.4.1.11.11.9" class="ltx_td ltx_align_center ltx_border_r">81.99</td>
<td id="S3.T1.4.1.11.11.10" class="ltx_td ltx_align_center">80</td>
</tr>
<tr id="S3.T1.4.1.12.12" class="ltx_tr">
<td id="S3.T1.4.1.12.12.1" class="ltx_td ltx_align_center ltx_border_r">3D-CVF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</td>
<td id="S3.T1.4.1.12.12.2" class="ltx_td ltx_align_center ltx_border_r">ECCV 2020</td>
<td id="S3.T1.4.1.12.12.3" class="ltx_td ltx_align_center ltx_border_r">LiDAR+RGB</td>
<td id="S3.T1.4.1.12.12.4" class="ltx_td ltx_align_center">89.20</td>
<td id="S3.T1.4.1.12.12.5" class="ltx_td ltx_align_center">80.05</td>
<td id="S3.T1.4.1.12.12.6" class="ltx_td ltx_align_center ltx_border_r">73.11</td>
<td id="S3.T1.4.1.12.12.7" class="ltx_td ltx_align_center">93.52</td>
<td id="S3.T1.4.1.12.12.8" class="ltx_td ltx_align_center">89.56</td>
<td id="S3.T1.4.1.12.12.9" class="ltx_td ltx_align_center ltx_border_r">82.45</td>
<td id="S3.T1.4.1.12.12.10" class="ltx_td ltx_align_center">75</td>
</tr>
<tr id="S3.T1.4.1.13.13" class="ltx_tr">
<td id="S3.T1.4.1.13.13.1" class="ltx_td ltx_align_center ltx_border_r">Focals Conv <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</td>
<td id="S3.T1.4.1.13.13.2" class="ltx_td ltx_align_center ltx_border_r">CVPR 2022</td>
<td id="S3.T1.4.1.13.13.3" class="ltx_td ltx_align_center ltx_border_r">LiDAR+RGB</td>
<td id="S3.T1.4.1.13.13.4" class="ltx_td ltx_align_center">90.55</td>
<td id="S3.T1.4.1.13.13.5" class="ltx_td ltx_align_center">82.28</td>
<td id="S3.T1.4.1.13.13.6" class="ltx_td ltx_align_center ltx_border_r">77.59</td>
<td id="S3.T1.4.1.13.13.7" class="ltx_td ltx_align_center">92.67</td>
<td id="S3.T1.4.1.13.13.8" class="ltx_td ltx_align_center">89.00</td>
<td id="S3.T1.4.1.13.13.9" class="ltx_td ltx_align_center ltx_border_r">86.33</td>
<td id="S3.T1.4.1.13.13.10" class="ltx_td ltx_align_center">100*</td>
</tr>
<tr id="S3.T1.4.1.14.14" class="ltx_tr">
<td id="S3.T1.4.1.14.14.1" class="ltx_td ltx_align_center ltx_border_r">VPFNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>
</td>
<td id="S3.T1.4.1.14.14.2" class="ltx_td ltx_align_center ltx_border_r">TMM 2022</td>
<td id="S3.T1.4.1.14.14.3" class="ltx_td ltx_align_center ltx_border_r">LiDAR+RGB</td>
<td id="S3.T1.4.1.14.14.4" class="ltx_td ltx_align_center">91.02</td>
<td id="S3.T1.4.1.14.14.5" class="ltx_td ltx_align_center">83.21</td>
<td id="S3.T1.4.1.14.14.6" class="ltx_td ltx_align_center ltx_border_r">78.20</td>
<td id="S3.T1.4.1.14.14.7" class="ltx_td ltx_align_center">93.94</td>
<td id="S3.T1.4.1.14.14.8" class="ltx_td ltx_align_center">90.52</td>
<td id="S3.T1.4.1.14.14.9" class="ltx_td ltx_align_center ltx_border_r">86.25</td>
<td id="S3.T1.4.1.14.14.10" class="ltx_td ltx_align_center">62</td>
</tr>
<tr id="S3.T1.4.1.15.15" class="ltx_tr">
<td id="S3.T1.4.1.15.15.1" class="ltx_td ltx_align_center ltx_border_r">Graph-VoI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>
</td>
<td id="S3.T1.4.1.15.15.2" class="ltx_td ltx_align_center ltx_border_r">ECCV 2022</td>
<td id="S3.T1.4.1.15.15.3" class="ltx_td ltx_align_center ltx_border_r">LiDAR+RGB</td>
<td id="S3.T1.4.1.15.15.4" class="ltx_td ltx_align_center">91.89</td>
<td id="S3.T1.4.1.15.15.5" class="ltx_td ltx_align_center">83.27</td>
<td id="S3.T1.4.1.15.15.6" class="ltx_td ltx_align_center ltx_border_r">77.78</td>
<td id="S3.T1.4.1.15.15.7" class="ltx_td ltx_align_center">95.69</td>
<td id="S3.T1.4.1.15.15.8" class="ltx_td ltx_align_center">90.10</td>
<td id="S3.T1.4.1.15.15.9" class="ltx_td ltx_align_center ltx_border_r">86.85</td>
<td id="S3.T1.4.1.15.15.10" class="ltx_td ltx_align_center">76</td>
</tr>
<tr id="S3.T1.4.1.16.16" class="ltx_tr">
<td id="S3.T1.4.1.16.16.1" class="ltx_td ltx_align_center ltx_border_r">SFD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</td>
<td id="S3.T1.4.1.16.16.2" class="ltx_td ltx_align_center ltx_border_r">CVPR 2022</td>
<td id="S3.T1.4.1.16.16.3" class="ltx_td ltx_align_center ltx_border_r">LiDAR+RGB</td>
<td id="S3.T1.4.1.16.16.4" class="ltx_td ltx_align_center">91.73</td>
<td id="S3.T1.4.1.16.16.5" class="ltx_td ltx_align_center">84.76</td>
<td id="S3.T1.4.1.16.16.6" class="ltx_td ltx_align_center ltx_border_r">77.92</td>
<td id="S3.T1.4.1.16.16.7" class="ltx_td ltx_align_center">95.64</td>
<td id="S3.T1.4.1.16.16.8" class="ltx_td ltx_align_center">91.85</td>
<td id="S3.T1.4.1.16.16.9" class="ltx_td ltx_align_center ltx_border_r">86.83</td>
<td id="S3.T1.4.1.16.16.10" class="ltx_td ltx_align_center">98</td>
</tr>
<tr id="S3.T1.4.1.17.17" class="ltx_tr">
<td id="S3.T1.4.1.17.17.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">VirConv-L (Ours)</td>
<td id="S3.T1.4.1.17.17.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S3.T1.4.1.17.17.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LiDAR+RGB</td>
<td id="S3.T1.4.1.17.17.4" class="ltx_td ltx_align_center ltx_border_t">91.41</td>
<td id="S3.T1.4.1.17.17.5" class="ltx_td ltx_align_center ltx_border_t">85.05</td>
<td id="S3.T1.4.1.17.17.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">80.22</td>
<td id="S3.T1.4.1.17.17.7" class="ltx_td ltx_align_center ltx_border_t">95.53</td>
<td id="S3.T1.4.1.17.17.8" class="ltx_td ltx_align_center ltx_border_t">91.95</td>
<td id="S3.T1.4.1.17.17.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">87.07</td>
<td id="S3.T1.4.1.17.17.10" class="ltx_td ltx_align_center ltx_border_t">56</td>
</tr>
<tr id="S3.T1.4.1.18.18" class="ltx_tr">
<td id="S3.T1.4.1.18.18.1" class="ltx_td ltx_align_center ltx_border_r">VirConv-T (Ours)</td>
<td id="S3.T1.4.1.18.18.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T1.4.1.18.18.3" class="ltx_td ltx_align_center ltx_border_r">LiDAR+RGB</td>
<td id="S3.T1.4.1.18.18.4" class="ltx_td ltx_align_center"><span id="S3.T1.4.1.18.18.4.1" class="ltx_text ltx_font_bold">92.54</span></td>
<td id="S3.T1.4.1.18.18.5" class="ltx_td ltx_align_center"><span id="S3.T1.4.1.18.18.5.1" class="ltx_text ltx_font_bold">86.25</span></td>
<td id="S3.T1.4.1.18.18.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T1.4.1.18.18.6.1" class="ltx_text ltx_font_bold">81.24</span></td>
<td id="S3.T1.4.1.18.18.7" class="ltx_td ltx_align_center"><span id="S3.T1.4.1.18.18.7.1" class="ltx_text ltx_font_bold">96.11</span></td>
<td id="S3.T1.4.1.18.18.8" class="ltx_td ltx_align_center"><span id="S3.T1.4.1.18.18.8.1" class="ltx_text ltx_font_bold">92.65</span></td>
<td id="S3.T1.4.1.18.18.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T1.4.1.18.18.9.1" class="ltx_text ltx_font_bold">89.69</span></td>
<td id="S3.T1.4.1.18.18.10" class="ltx_td ltx_align_center">92</td>
</tr>
<tr id="S3.T1.4.1.19.19" class="ltx_tr">
<td id="S3.T1.4.1.19.19.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">VirConv-S (Our semi-supervised)</td>
<td id="S3.T1.4.1.19.19.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">-</td>
<td id="S3.T1.4.1.19.19.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">LiDAR+RGB</td>
<td id="S3.T1.4.1.19.19.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_tt">92.48</td>
<td id="S3.T1.4.1.19.19.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_tt">87.20</td>
<td id="S3.T1.4.1.19.19.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">82.45</td>
<td id="S3.T1.4.1.19.19.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_tt">95.99</td>
<td id="S3.T1.4.1.19.19.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_tt">93.52</td>
<td id="S3.T1.4.1.19.19.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">90.38</td>
<td id="S3.T1.4.1.19.19.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_tt">92</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.5.2.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.2.1" class="ltx_text" style="font-size:90%;">Car 3D detection results on the KITTI test set, where the best fully supervised methods are in bold and <math id="S3.T1.2.1.m1.1" class="ltx_Math" alttext="*" display="inline"><semantics id="S3.T1.2.1.m1.1b"><mo id="S3.T1.2.1.m1.1.1" xref="S3.T1.2.1.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.1.m1.1c"><times id="S3.T1.2.1.m1.1.1.cmml" xref="S3.T1.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.1.m1.1d">*</annotation></semantics></math> denotes that the result is from the KITTI leaderboard. Our VirConv-T outperforms all the other methods in both 3D AP and BEV AP metrics. Besides, our VirConv-L runs fast at 56ms with 85.05 AP, and our VirConv-S attains a high detection performance of 87.20 AP.</span></figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Detection Frameworks with VirConv</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">To demonstrate the superiority of our VirConv, we constructed VirConv-L, VirConv-T and VirConv-S from the widely used Voxel-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> for fast, accurate and semi-supervised 3D object detection, respectively.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.6" class="ltx_p"><span id="S3.SS4.p2.6.1" class="ltx_text ltx_font_bold">VirConv-L.</span>
We first construct the lightweight VirConv-L (Fig. <a href="#S1.F3" title="Figure 3 ‣ 1 Introduction ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (c)) for fast multimodal 3D detection. VirConv-L adopts an early fusion scheme and replaces the backbone of Voxel-RCNN with our VirConvNet. Specifically, we denote the LiDAR points as <math id="S3.SS4.p2.1.m1.7" class="ltx_Math" alttext="\mathbf{P}=\{p\},p=[x,y,z,\alpha]" display="inline"><semantics id="S3.SS4.p2.1.m1.7a"><mrow id="S3.SS4.p2.1.m1.7.7.2" xref="S3.SS4.p2.1.m1.7.7.3.cmml"><mrow id="S3.SS4.p2.1.m1.6.6.1.1" xref="S3.SS4.p2.1.m1.6.6.1.1.cmml"><mi id="S3.SS4.p2.1.m1.6.6.1.1.2" xref="S3.SS4.p2.1.m1.6.6.1.1.2.cmml">𝐏</mi><mo id="S3.SS4.p2.1.m1.6.6.1.1.1" xref="S3.SS4.p2.1.m1.6.6.1.1.1.cmml">=</mo><mrow id="S3.SS4.p2.1.m1.6.6.1.1.3.2" xref="S3.SS4.p2.1.m1.6.6.1.1.3.1.cmml"><mo stretchy="false" id="S3.SS4.p2.1.m1.6.6.1.1.3.2.1" xref="S3.SS4.p2.1.m1.6.6.1.1.3.1.cmml">{</mo><mi id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml">p</mi><mo stretchy="false" id="S3.SS4.p2.1.m1.6.6.1.1.3.2.2" xref="S3.SS4.p2.1.m1.6.6.1.1.3.1.cmml">}</mo></mrow></mrow><mo id="S3.SS4.p2.1.m1.7.7.2.3" xref="S3.SS4.p2.1.m1.7.7.3a.cmml">,</mo><mrow id="S3.SS4.p2.1.m1.7.7.2.2" xref="S3.SS4.p2.1.m1.7.7.2.2.cmml"><mi id="S3.SS4.p2.1.m1.7.7.2.2.2" xref="S3.SS4.p2.1.m1.7.7.2.2.2.cmml">p</mi><mo id="S3.SS4.p2.1.m1.7.7.2.2.1" xref="S3.SS4.p2.1.m1.7.7.2.2.1.cmml">=</mo><mrow id="S3.SS4.p2.1.m1.7.7.2.2.3.2" xref="S3.SS4.p2.1.m1.7.7.2.2.3.1.cmml"><mo stretchy="false" id="S3.SS4.p2.1.m1.7.7.2.2.3.2.1" xref="S3.SS4.p2.1.m1.7.7.2.2.3.1.cmml">[</mo><mi id="S3.SS4.p2.1.m1.2.2" xref="S3.SS4.p2.1.m1.2.2.cmml">x</mi><mo id="S3.SS4.p2.1.m1.7.7.2.2.3.2.2" xref="S3.SS4.p2.1.m1.7.7.2.2.3.1.cmml">,</mo><mi id="S3.SS4.p2.1.m1.3.3" xref="S3.SS4.p2.1.m1.3.3.cmml">y</mi><mo id="S3.SS4.p2.1.m1.7.7.2.2.3.2.3" xref="S3.SS4.p2.1.m1.7.7.2.2.3.1.cmml">,</mo><mi id="S3.SS4.p2.1.m1.4.4" xref="S3.SS4.p2.1.m1.4.4.cmml">z</mi><mo id="S3.SS4.p2.1.m1.7.7.2.2.3.2.4" xref="S3.SS4.p2.1.m1.7.7.2.2.3.1.cmml">,</mo><mi id="S3.SS4.p2.1.m1.5.5" xref="S3.SS4.p2.1.m1.5.5.cmml">α</mi><mo stretchy="false" id="S3.SS4.p2.1.m1.7.7.2.2.3.2.5" xref="S3.SS4.p2.1.m1.7.7.2.2.3.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.7b"><apply id="S3.SS4.p2.1.m1.7.7.3.cmml" xref="S3.SS4.p2.1.m1.7.7.2"><csymbol cd="ambiguous" id="S3.SS4.p2.1.m1.7.7.3a.cmml" xref="S3.SS4.p2.1.m1.7.7.2.3">formulae-sequence</csymbol><apply id="S3.SS4.p2.1.m1.6.6.1.1.cmml" xref="S3.SS4.p2.1.m1.6.6.1.1"><eq id="S3.SS4.p2.1.m1.6.6.1.1.1.cmml" xref="S3.SS4.p2.1.m1.6.6.1.1.1"></eq><ci id="S3.SS4.p2.1.m1.6.6.1.1.2.cmml" xref="S3.SS4.p2.1.m1.6.6.1.1.2">𝐏</ci><set id="S3.SS4.p2.1.m1.6.6.1.1.3.1.cmml" xref="S3.SS4.p2.1.m1.6.6.1.1.3.2"><ci id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">𝑝</ci></set></apply><apply id="S3.SS4.p2.1.m1.7.7.2.2.cmml" xref="S3.SS4.p2.1.m1.7.7.2.2"><eq id="S3.SS4.p2.1.m1.7.7.2.2.1.cmml" xref="S3.SS4.p2.1.m1.7.7.2.2.1"></eq><ci id="S3.SS4.p2.1.m1.7.7.2.2.2.cmml" xref="S3.SS4.p2.1.m1.7.7.2.2.2">𝑝</ci><list id="S3.SS4.p2.1.m1.7.7.2.2.3.1.cmml" xref="S3.SS4.p2.1.m1.7.7.2.2.3.2"><ci id="S3.SS4.p2.1.m1.2.2.cmml" xref="S3.SS4.p2.1.m1.2.2">𝑥</ci><ci id="S3.SS4.p2.1.m1.3.3.cmml" xref="S3.SS4.p2.1.m1.3.3">𝑦</ci><ci id="S3.SS4.p2.1.m1.4.4.cmml" xref="S3.SS4.p2.1.m1.4.4">𝑧</ci><ci id="S3.SS4.p2.1.m1.5.5.cmml" xref="S3.SS4.p2.1.m1.5.5">𝛼</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.7c">\mathbf{P}=\{p\},p=[x,y,z,\alpha]</annotation></semantics></math>, where <math id="S3.SS4.p2.2.m2.3" class="ltx_Math" alttext="x,y,z" display="inline"><semantics id="S3.SS4.p2.2.m2.3a"><mrow id="S3.SS4.p2.2.m2.3.4.2" xref="S3.SS4.p2.2.m2.3.4.1.cmml"><mi id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml">x</mi><mo id="S3.SS4.p2.2.m2.3.4.2.1" xref="S3.SS4.p2.2.m2.3.4.1.cmml">,</mo><mi id="S3.SS4.p2.2.m2.2.2" xref="S3.SS4.p2.2.m2.2.2.cmml">y</mi><mo id="S3.SS4.p2.2.m2.3.4.2.2" xref="S3.SS4.p2.2.m2.3.4.1.cmml">,</mo><mi id="S3.SS4.p2.2.m2.3.3" xref="S3.SS4.p2.2.m2.3.3.cmml">z</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.3b"><list id="S3.SS4.p2.2.m2.3.4.1.cmml" xref="S3.SS4.p2.2.m2.3.4.2"><ci id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1">𝑥</ci><ci id="S3.SS4.p2.2.m2.2.2.cmml" xref="S3.SS4.p2.2.m2.2.2">𝑦</ci><ci id="S3.SS4.p2.2.m2.3.3.cmml" xref="S3.SS4.p2.2.m2.3.3">𝑧</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.3c">x,y,z</annotation></semantics></math> denotes the coordinates and <math id="S3.SS4.p2.3.m3.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS4.p2.3.m3.1a"><mi id="S3.SS4.p2.3.m3.1.1" xref="S3.SS4.p2.3.m3.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.3.m3.1b"><ci id="S3.SS4.p2.3.m3.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.3.m3.1c">\alpha</annotation></semantics></math> refers intensity. We denote the virtual points as <math id="S3.SS4.p2.4.m4.6" class="ltx_Math" alttext="\mathbf{V}=\{v\},v=[x,y,z]" display="inline"><semantics id="S3.SS4.p2.4.m4.6a"><mrow id="S3.SS4.p2.4.m4.6.6.2" xref="S3.SS4.p2.4.m4.6.6.3.cmml"><mrow id="S3.SS4.p2.4.m4.5.5.1.1" xref="S3.SS4.p2.4.m4.5.5.1.1.cmml"><mi id="S3.SS4.p2.4.m4.5.5.1.1.2" xref="S3.SS4.p2.4.m4.5.5.1.1.2.cmml">𝐕</mi><mo id="S3.SS4.p2.4.m4.5.5.1.1.1" xref="S3.SS4.p2.4.m4.5.5.1.1.1.cmml">=</mo><mrow id="S3.SS4.p2.4.m4.5.5.1.1.3.2" xref="S3.SS4.p2.4.m4.5.5.1.1.3.1.cmml"><mo stretchy="false" id="S3.SS4.p2.4.m4.5.5.1.1.3.2.1" xref="S3.SS4.p2.4.m4.5.5.1.1.3.1.cmml">{</mo><mi id="S3.SS4.p2.4.m4.1.1" xref="S3.SS4.p2.4.m4.1.1.cmml">v</mi><mo stretchy="false" id="S3.SS4.p2.4.m4.5.5.1.1.3.2.2" xref="S3.SS4.p2.4.m4.5.5.1.1.3.1.cmml">}</mo></mrow></mrow><mo id="S3.SS4.p2.4.m4.6.6.2.3" xref="S3.SS4.p2.4.m4.6.6.3a.cmml">,</mo><mrow id="S3.SS4.p2.4.m4.6.6.2.2" xref="S3.SS4.p2.4.m4.6.6.2.2.cmml"><mi id="S3.SS4.p2.4.m4.6.6.2.2.2" xref="S3.SS4.p2.4.m4.6.6.2.2.2.cmml">v</mi><mo id="S3.SS4.p2.4.m4.6.6.2.2.1" xref="S3.SS4.p2.4.m4.6.6.2.2.1.cmml">=</mo><mrow id="S3.SS4.p2.4.m4.6.6.2.2.3.2" xref="S3.SS4.p2.4.m4.6.6.2.2.3.1.cmml"><mo stretchy="false" id="S3.SS4.p2.4.m4.6.6.2.2.3.2.1" xref="S3.SS4.p2.4.m4.6.6.2.2.3.1.cmml">[</mo><mi id="S3.SS4.p2.4.m4.2.2" xref="S3.SS4.p2.4.m4.2.2.cmml">x</mi><mo id="S3.SS4.p2.4.m4.6.6.2.2.3.2.2" xref="S3.SS4.p2.4.m4.6.6.2.2.3.1.cmml">,</mo><mi id="S3.SS4.p2.4.m4.3.3" xref="S3.SS4.p2.4.m4.3.3.cmml">y</mi><mo id="S3.SS4.p2.4.m4.6.6.2.2.3.2.3" xref="S3.SS4.p2.4.m4.6.6.2.2.3.1.cmml">,</mo><mi id="S3.SS4.p2.4.m4.4.4" xref="S3.SS4.p2.4.m4.4.4.cmml">z</mi><mo stretchy="false" id="S3.SS4.p2.4.m4.6.6.2.2.3.2.4" xref="S3.SS4.p2.4.m4.6.6.2.2.3.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.4.m4.6b"><apply id="S3.SS4.p2.4.m4.6.6.3.cmml" xref="S3.SS4.p2.4.m4.6.6.2"><csymbol cd="ambiguous" id="S3.SS4.p2.4.m4.6.6.3a.cmml" xref="S3.SS4.p2.4.m4.6.6.2.3">formulae-sequence</csymbol><apply id="S3.SS4.p2.4.m4.5.5.1.1.cmml" xref="S3.SS4.p2.4.m4.5.5.1.1"><eq id="S3.SS4.p2.4.m4.5.5.1.1.1.cmml" xref="S3.SS4.p2.4.m4.5.5.1.1.1"></eq><ci id="S3.SS4.p2.4.m4.5.5.1.1.2.cmml" xref="S3.SS4.p2.4.m4.5.5.1.1.2">𝐕</ci><set id="S3.SS4.p2.4.m4.5.5.1.1.3.1.cmml" xref="S3.SS4.p2.4.m4.5.5.1.1.3.2"><ci id="S3.SS4.p2.4.m4.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1">𝑣</ci></set></apply><apply id="S3.SS4.p2.4.m4.6.6.2.2.cmml" xref="S3.SS4.p2.4.m4.6.6.2.2"><eq id="S3.SS4.p2.4.m4.6.6.2.2.1.cmml" xref="S3.SS4.p2.4.m4.6.6.2.2.1"></eq><ci id="S3.SS4.p2.4.m4.6.6.2.2.2.cmml" xref="S3.SS4.p2.4.m4.6.6.2.2.2">𝑣</ci><list id="S3.SS4.p2.4.m4.6.6.2.2.3.1.cmml" xref="S3.SS4.p2.4.m4.6.6.2.2.3.2"><ci id="S3.SS4.p2.4.m4.2.2.cmml" xref="S3.SS4.p2.4.m4.2.2">𝑥</ci><ci id="S3.SS4.p2.4.m4.3.3.cmml" xref="S3.SS4.p2.4.m4.3.3">𝑦</ci><ci id="S3.SS4.p2.4.m4.4.4.cmml" xref="S3.SS4.p2.4.m4.4.4">𝑧</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.4.m4.6c">\mathbf{V}=\{v\},v=[x,y,z]</annotation></semantics></math>. We fuse them into a single point cloud <math id="S3.SS4.p2.5.m5.7" class="ltx_Math" alttext="\mathbf{P}^{*}=\{p^{*}\}_{k},p^{*}=[x,y,z,\alpha,\beta]" display="inline"><semantics id="S3.SS4.p2.5.m5.7a"><mrow id="S3.SS4.p2.5.m5.7.7.2" xref="S3.SS4.p2.5.m5.7.7.3.cmml"><mrow id="S3.SS4.p2.5.m5.6.6.1.1" xref="S3.SS4.p2.5.m5.6.6.1.1.cmml"><msup id="S3.SS4.p2.5.m5.6.6.1.1.3" xref="S3.SS4.p2.5.m5.6.6.1.1.3.cmml"><mi id="S3.SS4.p2.5.m5.6.6.1.1.3.2" xref="S3.SS4.p2.5.m5.6.6.1.1.3.2.cmml">𝐏</mi><mo id="S3.SS4.p2.5.m5.6.6.1.1.3.3" xref="S3.SS4.p2.5.m5.6.6.1.1.3.3.cmml">∗</mo></msup><mo id="S3.SS4.p2.5.m5.6.6.1.1.2" xref="S3.SS4.p2.5.m5.6.6.1.1.2.cmml">=</mo><msub id="S3.SS4.p2.5.m5.6.6.1.1.1" xref="S3.SS4.p2.5.m5.6.6.1.1.1.cmml"><mrow id="S3.SS4.p2.5.m5.6.6.1.1.1.1.1" xref="S3.SS4.p2.5.m5.6.6.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS4.p2.5.m5.6.6.1.1.1.1.1.2" xref="S3.SS4.p2.5.m5.6.6.1.1.1.1.2.cmml">{</mo><msup id="S3.SS4.p2.5.m5.6.6.1.1.1.1.1.1" xref="S3.SS4.p2.5.m5.6.6.1.1.1.1.1.1.cmml"><mi id="S3.SS4.p2.5.m5.6.6.1.1.1.1.1.1.2" xref="S3.SS4.p2.5.m5.6.6.1.1.1.1.1.1.2.cmml">p</mi><mo id="S3.SS4.p2.5.m5.6.6.1.1.1.1.1.1.3" xref="S3.SS4.p2.5.m5.6.6.1.1.1.1.1.1.3.cmml">∗</mo></msup><mo stretchy="false" id="S3.SS4.p2.5.m5.6.6.1.1.1.1.1.3" xref="S3.SS4.p2.5.m5.6.6.1.1.1.1.2.cmml">}</mo></mrow><mi id="S3.SS4.p2.5.m5.6.6.1.1.1.3" xref="S3.SS4.p2.5.m5.6.6.1.1.1.3.cmml">k</mi></msub></mrow><mo id="S3.SS4.p2.5.m5.7.7.2.3" xref="S3.SS4.p2.5.m5.7.7.3a.cmml">,</mo><mrow id="S3.SS4.p2.5.m5.7.7.2.2" xref="S3.SS4.p2.5.m5.7.7.2.2.cmml"><msup id="S3.SS4.p2.5.m5.7.7.2.2.2" xref="S3.SS4.p2.5.m5.7.7.2.2.2.cmml"><mi id="S3.SS4.p2.5.m5.7.7.2.2.2.2" xref="S3.SS4.p2.5.m5.7.7.2.2.2.2.cmml">p</mi><mo id="S3.SS4.p2.5.m5.7.7.2.2.2.3" xref="S3.SS4.p2.5.m5.7.7.2.2.2.3.cmml">∗</mo></msup><mo id="S3.SS4.p2.5.m5.7.7.2.2.1" xref="S3.SS4.p2.5.m5.7.7.2.2.1.cmml">=</mo><mrow id="S3.SS4.p2.5.m5.7.7.2.2.3.2" xref="S3.SS4.p2.5.m5.7.7.2.2.3.1.cmml"><mo stretchy="false" id="S3.SS4.p2.5.m5.7.7.2.2.3.2.1" xref="S3.SS4.p2.5.m5.7.7.2.2.3.1.cmml">[</mo><mi id="S3.SS4.p2.5.m5.1.1" xref="S3.SS4.p2.5.m5.1.1.cmml">x</mi><mo id="S3.SS4.p2.5.m5.7.7.2.2.3.2.2" xref="S3.SS4.p2.5.m5.7.7.2.2.3.1.cmml">,</mo><mi id="S3.SS4.p2.5.m5.2.2" xref="S3.SS4.p2.5.m5.2.2.cmml">y</mi><mo id="S3.SS4.p2.5.m5.7.7.2.2.3.2.3" xref="S3.SS4.p2.5.m5.7.7.2.2.3.1.cmml">,</mo><mi id="S3.SS4.p2.5.m5.3.3" xref="S3.SS4.p2.5.m5.3.3.cmml">z</mi><mo id="S3.SS4.p2.5.m5.7.7.2.2.3.2.4" xref="S3.SS4.p2.5.m5.7.7.2.2.3.1.cmml">,</mo><mi id="S3.SS4.p2.5.m5.4.4" xref="S3.SS4.p2.5.m5.4.4.cmml">α</mi><mo id="S3.SS4.p2.5.m5.7.7.2.2.3.2.5" xref="S3.SS4.p2.5.m5.7.7.2.2.3.1.cmml">,</mo><mi id="S3.SS4.p2.5.m5.5.5" xref="S3.SS4.p2.5.m5.5.5.cmml">β</mi><mo stretchy="false" id="S3.SS4.p2.5.m5.7.7.2.2.3.2.6" xref="S3.SS4.p2.5.m5.7.7.2.2.3.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.5.m5.7b"><apply id="S3.SS4.p2.5.m5.7.7.3.cmml" xref="S3.SS4.p2.5.m5.7.7.2"><csymbol cd="ambiguous" id="S3.SS4.p2.5.m5.7.7.3a.cmml" xref="S3.SS4.p2.5.m5.7.7.2.3">formulae-sequence</csymbol><apply id="S3.SS4.p2.5.m5.6.6.1.1.cmml" xref="S3.SS4.p2.5.m5.6.6.1.1"><eq id="S3.SS4.p2.5.m5.6.6.1.1.2.cmml" xref="S3.SS4.p2.5.m5.6.6.1.1.2"></eq><apply id="S3.SS4.p2.5.m5.6.6.1.1.3.cmml" xref="S3.SS4.p2.5.m5.6.6.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p2.5.m5.6.6.1.1.3.1.cmml" xref="S3.SS4.p2.5.m5.6.6.1.1.3">superscript</csymbol><ci id="S3.SS4.p2.5.m5.6.6.1.1.3.2.cmml" xref="S3.SS4.p2.5.m5.6.6.1.1.3.2">𝐏</ci><times id="S3.SS4.p2.5.m5.6.6.1.1.3.3.cmml" xref="S3.SS4.p2.5.m5.6.6.1.1.3.3"></times></apply><apply id="S3.SS4.p2.5.m5.6.6.1.1.1.cmml" xref="S3.SS4.p2.5.m5.6.6.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.5.m5.6.6.1.1.1.2.cmml" xref="S3.SS4.p2.5.m5.6.6.1.1.1">subscript</csymbol><set id="S3.SS4.p2.5.m5.6.6.1.1.1.1.2.cmml" xref="S3.SS4.p2.5.m5.6.6.1.1.1.1.1"><apply id="S3.SS4.p2.5.m5.6.6.1.1.1.1.1.1.cmml" xref="S3.SS4.p2.5.m5.6.6.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.5.m5.6.6.1.1.1.1.1.1.1.cmml" xref="S3.SS4.p2.5.m5.6.6.1.1.1.1.1.1">superscript</csymbol><ci id="S3.SS4.p2.5.m5.6.6.1.1.1.1.1.1.2.cmml" xref="S3.SS4.p2.5.m5.6.6.1.1.1.1.1.1.2">𝑝</ci><times id="S3.SS4.p2.5.m5.6.6.1.1.1.1.1.1.3.cmml" xref="S3.SS4.p2.5.m5.6.6.1.1.1.1.1.1.3"></times></apply></set><ci id="S3.SS4.p2.5.m5.6.6.1.1.1.3.cmml" xref="S3.SS4.p2.5.m5.6.6.1.1.1.3">𝑘</ci></apply></apply><apply id="S3.SS4.p2.5.m5.7.7.2.2.cmml" xref="S3.SS4.p2.5.m5.7.7.2.2"><eq id="S3.SS4.p2.5.m5.7.7.2.2.1.cmml" xref="S3.SS4.p2.5.m5.7.7.2.2.1"></eq><apply id="S3.SS4.p2.5.m5.7.7.2.2.2.cmml" xref="S3.SS4.p2.5.m5.7.7.2.2.2"><csymbol cd="ambiguous" id="S3.SS4.p2.5.m5.7.7.2.2.2.1.cmml" xref="S3.SS4.p2.5.m5.7.7.2.2.2">superscript</csymbol><ci id="S3.SS4.p2.5.m5.7.7.2.2.2.2.cmml" xref="S3.SS4.p2.5.m5.7.7.2.2.2.2">𝑝</ci><times id="S3.SS4.p2.5.m5.7.7.2.2.2.3.cmml" xref="S3.SS4.p2.5.m5.7.7.2.2.2.3"></times></apply><list id="S3.SS4.p2.5.m5.7.7.2.2.3.1.cmml" xref="S3.SS4.p2.5.m5.7.7.2.2.3.2"><ci id="S3.SS4.p2.5.m5.1.1.cmml" xref="S3.SS4.p2.5.m5.1.1">𝑥</ci><ci id="S3.SS4.p2.5.m5.2.2.cmml" xref="S3.SS4.p2.5.m5.2.2">𝑦</ci><ci id="S3.SS4.p2.5.m5.3.3.cmml" xref="S3.SS4.p2.5.m5.3.3">𝑧</ci><ci id="S3.SS4.p2.5.m5.4.4.cmml" xref="S3.SS4.p2.5.m5.4.4">𝛼</ci><ci id="S3.SS4.p2.5.m5.5.5.cmml" xref="S3.SS4.p2.5.m5.5.5">𝛽</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.5.m5.7c">\mathbf{P}^{*}=\{p^{*}\}_{k},p^{*}=[x,y,z,\alpha,\beta]</annotation></semantics></math>, where <math id="S3.SS4.p2.6.m6.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S3.SS4.p2.6.m6.1a"><mi id="S3.SS4.p2.6.m6.1.1" xref="S3.SS4.p2.6.m6.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.6.m6.1b"><ci id="S3.SS4.p2.6.m6.1.1.cmml" xref="S3.SS4.p2.6.m6.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.6.m6.1c">\beta</annotation></semantics></math> is an indicator denoting where the point came from. The intensity of virtual points is padded by zero. The fused points are encoded into feature volumes by our VirConvNet for 3D detection.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.6" class="ltx_p"><span id="S3.SS4.p3.6.1" class="ltx_text ltx_font_bold">VirConv-T.</span>
We then construct a high-precision VirConv-T based on a Transformed Refinement Scheme (TRS) and a late fusion scheme (see Fig. <a href="#S3.F5" title="Figure 5 ‣ 3.3 Noise-Resistant Submanifold Convolution ‣ 3 VirConv for Multimodal 3D Detection ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). CasA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and TED <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> achieve high detection performance based on three-stage refinement and multiple transformation design, respectively. However, both of them require heavy computations. We fuse the two high computation detectors into a single efficient pipeline. Specifically, we first transform <math id="S3.SS4.p3.1.m1.1" class="ltx_Math" alttext="\mathbf{P}" display="inline"><semantics id="S3.SS4.p3.1.m1.1a"><mi id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml">𝐏</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><ci id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">𝐏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">\mathbf{P}</annotation></semantics></math> and <math id="S3.SS4.p3.2.m2.1" class="ltx_Math" alttext="\mathbf{V}" display="inline"><semantics id="S3.SS4.p3.2.m2.1a"><mi id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml">𝐕</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><ci id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1">𝐕</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">\mathbf{V}</annotation></semantics></math> with different rotations and reflections. Then we adopt the VoxelNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and VirConvNet to encode the features of <math id="S3.SS4.p3.3.m3.1" class="ltx_Math" alttext="\mathbf{P}" display="inline"><semantics id="S3.SS4.p3.3.m3.1a"><mi id="S3.SS4.p3.3.m3.1.1" xref="S3.SS4.p3.3.m3.1.1.cmml">𝐏</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.3.m3.1b"><ci id="S3.SS4.p3.3.m3.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1">𝐏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.3.m3.1c">\mathbf{P}</annotation></semantics></math> and <math id="S3.SS4.p3.4.m4.1" class="ltx_Math" alttext="\mathbf{V}" display="inline"><semantics id="S3.SS4.p3.4.m4.1a"><mi id="S3.SS4.p3.4.m4.1.1" xref="S3.SS4.p3.4.m4.1.1.cmml">𝐕</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.4.m4.1b"><ci id="S3.SS4.p3.4.m4.1.1.cmml" xref="S3.SS4.p3.4.m4.1.1">𝐕</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.4.m4.1c">\mathbf{V}</annotation></semantics></math>, respectively. Similar to TED <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, the convolutional weights between different transformations are shared. After that, the RoIs are generated by a Region Proposal Network (RPN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and refined by the backbone features (the RoI features of <math id="S3.SS4.p3.5.m5.1" class="ltx_Math" alttext="\mathbf{P}" display="inline"><semantics id="S3.SS4.p3.5.m5.1a"><mi id="S3.SS4.p3.5.m5.1.1" xref="S3.SS4.p3.5.m5.1.1.cmml">𝐏</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.5.m5.1b"><ci id="S3.SS4.p3.5.m5.1.1.cmml" xref="S3.SS4.p3.5.m5.1.1">𝐏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.5.m5.1c">\mathbf{P}</annotation></semantics></math> and <math id="S3.SS4.p3.6.m6.1" class="ltx_Math" alttext="\mathbf{V}" display="inline"><semantics id="S3.SS4.p3.6.m6.1a"><mi id="S3.SS4.p3.6.m6.1.1" xref="S3.SS4.p3.6.m6.1.1.cmml">𝐕</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.6.m6.1b"><ci id="S3.SS4.p3.6.m6.1.1.cmml" xref="S3.SS4.p3.6.m6.1.1">𝐕</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.6.m6.1c">\mathbf{V}</annotation></semantics></math> fused by simple concatenation) under the first transformation. The refined RoIs are further refined by the backbone features under other transformations. Next, the refined RoIs from different refinement stages are fused by boxes voting, as is done by CasA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. We finally perform a non-maximum-suppression (NMS) on the fused RoIs to obtain detection results.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p"><span id="S3.SS4.p4.1.1" class="ltx_text ltx_font_bold">VirConv-S.</span>
We also design a semi-supervised pipeline, VirConv-S, using the widely used pseudo-label method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.
Specifically, first, a model is pre-trained using the labeled training data. Then, pseudo labels are generated on a larger-scale unannotated dataset using this pre-trained model.
A high-score threshold (empirically, 0.9) is adopted to filter out low-quality labels.
Finally, the VirConv-T model is trained using both real and pseudo labels.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>KITTI Datasets and Evaluation Metrics</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The KITTI 3D object detection dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> contains 7,481 and 7,518 LiDAR and image frames for training and testing, respectively. We divided the training data into a train split of 3712 frames and a validation split of 3769 frames following recent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. We also adopted the widely used evaluation metric: 3D Average Precision (AP) under 40 recall thresholds (R40). The IoU thresholds in this metric are 0.7, 0.5, and 0.5 for car, pedestrian, and cyclist, respectively.
We used the KITTI odometry dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> as the large-scale unlabeled dataset. The KITTI odometry dataset contains 43,552 LiDAR and image frames. We uniformly sampled 10,888 frames (denoted as the <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">semi</span> dataset) and used them to train our VirConv-S. There is no overlap found between the KITTI 3D detection dataset and the KITTI odometry dataset after checking the mapping files released by KITTI.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Setup Details </h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">Network details</span>.
Similar to SFD, our method uses the virtual points generated by PENet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.
VirConvNet adopts an architecture similar to the Voxel-RCNN backbone <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.
VirConvNet includes four levels of VirConv blocks with feature dimensions 16, 32, 64, and 64, respectively.
The input StVD rate and layer StVD rate are set to 90% and 15% by default.
On the KITTI dataset, all the detectors use the same detection range and voxel size as CasA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Losses and data augmentation</span>.
VirConv-L uses the same training loss as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.
VirConv-T and VirConv-S use the same training loss as CasA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.
In all these three pipelines, we adopted the widely used local and global data augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, including ground-truth sampling, local transformation (rotation and translation), and global transformation (rotation and flipping).</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">Training and inference details</span>.
All three detectors were trained on 8 Tesla V100 GPUs with the ADAM optimizer. We used a learning rate of 0.01 with a one-cycle learning rate strategy. We trained the VirConv-L and VirConv-T for 60 epochs. The weights of VirConv-S are initialized by the trained VirConv-T.
We further trained the VirConv-S on the labeled and unlabeled dataset for 10 epochs.
We used an NMS threshold of 0.8 to generate 160 object proposals with 1:1 positive and negative samples during training. During testing, we used an NMS threshold of 0.1 to remove redundant boxes after proposal refinement.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Main Results</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">Results on KITTI validation set.</span>
We report the car detection results on the KITTI validation set in Table <a href="#S4.T2" title="Table 2 ‣ 4.3 Main Results ‣ 4 Experiments ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Compared with the baseline detector Voxel-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, our VirConv-L, VirConv-T and VirConv-S show 3.42%, 5% and 5.68% 3D AP(R40) improvement in the moderate car class, respectively. We also reported the performance based on the 3D AP under 11 recall thresholds (R11). Our VirConv-L, VirConv-T and VirConv-S show 2.38%, 3.33% and 3.54% 3D AP(R11) improvement in the moderate car class, respectively. The performance gains are mostly derived from the VirConv design, which effectively addressed the density problem and noise problem brought by virtual points. Note that our VirConv-L also runs much faster than other multimodal detectors, thanks to our efficient StVD design.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.2.1.1" class="ltx_tr">
<th id="S4.T2.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T2.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Method</span></th>
<td id="S4.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span id="S4.T2.2.1.1.2.1" class="ltx_text" style="font-size:90%;">Car 3D AP (R40)</span></td>
<td id="S4.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.1.1.3.1" class="ltx_text" style="font-size:90%;">Mod.</span></td>
</tr>
<tr id="S4.T2.2.2.2" class="ltx_tr">
<td id="S4.T2.2.2.2.1" class="ltx_td ltx_align_center"><span id="S4.T2.2.2.2.1.1" class="ltx_text" style="font-size:90%;">Easy</span></td>
<td id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center"><span id="S4.T2.2.2.2.2.1" class="ltx_text" style="font-size:90%;">Mod.</span></td>
<td id="S4.T2.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.2.2.3.1" class="ltx_text" style="font-size:90%;">Hard</span></td>
<td id="S4.T2.2.2.2.4" class="ltx_td ltx_align_center"><span id="S4.T2.2.2.2.4.1" class="ltx_text" style="font-size:90%;">AP(R11)</span></td>
</tr>
<tr id="S4.T2.2.3.3" class="ltx_tr">
<th id="S4.T2.2.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S4.T2.2.3.3.1.1" class="ltx_text" style="font-size:90%;">Voxel-RCNN</span></th>
<td id="S4.T2.2.3.3.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.2.3.3.2.1" class="ltx_text" style="font-size:90%;">92.38</span></td>
<td id="S4.T2.2.3.3.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.2.3.3.3.1" class="ltx_text" style="font-size:90%;">85.29</span></td>
<td id="S4.T2.2.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.2.3.3.4.1" class="ltx_text" style="font-size:90%;">82.86</span></td>
<td id="S4.T2.2.3.3.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.2.3.3.5.1" class="ltx_text" style="font-size:90%;">84.52</span></td>
</tr>
<tr id="S4.T2.2.4.4" class="ltx_tr">
<th id="S4.T2.2.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T2.2.4.4.1.1" class="ltx_text" style="font-size:90%;">Voxel-RCNN(EF)</span></th>
<td id="S4.T2.2.4.4.2" class="ltx_td ltx_align_center"><span id="S4.T2.2.4.4.2.1" class="ltx_text" style="font-size:90%;">92.42</span></td>
<td id="S4.T2.2.4.4.3" class="ltx_td ltx_align_center"><span id="S4.T2.2.4.4.3.1" class="ltx_text" style="font-size:90%;">85.78</span></td>
<td id="S4.T2.2.4.4.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.4.4.4.1" class="ltx_text" style="font-size:90%;">83.10</span></td>
<td id="S4.T2.2.4.4.5" class="ltx_td ltx_align_center"><span id="S4.T2.2.4.4.5.1" class="ltx_text" style="font-size:90%;">84.94</span></td>
</tr>
<tr id="S4.T2.2.5.5" class="ltx_tr">
<th id="S4.T2.2.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T2.2.5.5.1.1" class="ltx_text" style="font-size:90%;">Voxel-RCNN(LF)</span></th>
<td id="S4.T2.2.5.5.2" class="ltx_td ltx_align_center"><span id="S4.T2.2.5.5.2.1" class="ltx_text" style="font-size:90%;">92.91</span></td>
<td id="S4.T2.2.5.5.3" class="ltx_td ltx_align_center"><span id="S4.T2.2.5.5.3.1" class="ltx_text" style="font-size:90%;">86.32</span></td>
<td id="S4.T2.2.5.5.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.5.5.4.1" class="ltx_text" style="font-size:90%;">83.97</span></td>
<td id="S4.T2.2.5.5.5" class="ltx_td ltx_align_center"><span id="S4.T2.2.5.5.5.1" class="ltx_text" style="font-size:90%;">85.23</span></td>
</tr>
<tr id="S4.T2.2.6.6" class="ltx_tr">
<th id="S4.T2.2.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T2.2.6.6.1.1" class="ltx_text" style="font-size:90%;">VirConv-L</span></th>
<td id="S4.T2.2.6.6.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.6.6.2.1" class="ltx_text" style="font-size:90%;">93.36</span></td>
<td id="S4.T2.2.6.6.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.6.6.3.1" class="ltx_text" style="font-size:90%;">88.71</span></td>
<td id="S4.T2.2.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.6.6.4.1" class="ltx_text" style="font-size:90%;">85.83</span></td>
<td id="S4.T2.2.6.6.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.6.6.5.1" class="ltx_text" style="font-size:90%;">86.70</span></td>
</tr>
<tr id="S4.T2.2.7.7" class="ltx_tr">
<th id="S4.T2.2.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T2.2.7.7.1.1" class="ltx_text" style="font-size:90%;">VirConv-T</span></th>
<td id="S4.T2.2.7.7.2" class="ltx_td ltx_align_center"><span id="S4.T2.2.7.7.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">95.81</span></td>
<td id="S4.T2.2.7.7.3" class="ltx_td ltx_align_center"><span id="S4.T2.2.7.7.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">90.29</span></td>
<td id="S4.T2.2.7.7.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.2.7.7.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">88.10</span></td>
<td id="S4.T2.2.7.7.5" class="ltx_td ltx_align_center"><span id="S4.T2.2.7.7.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">87.82</span></td>
</tr>
<tr id="S4.T2.2.8.8" class="ltx_tr">
<th id="S4.T2.2.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_tt"><span id="S4.T2.2.8.8.1.1" class="ltx_text" style="font-size:90%;">VirConv-S (semi)</span></th>
<td id="S4.T2.2.8.8.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_tt"><span id="S4.T2.2.8.8.2.1" class="ltx_text" style="font-size:90%;">95.76</span></td>
<td id="S4.T2.2.8.8.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_tt"><span id="S4.T2.2.8.8.3.1" class="ltx_text" style="font-size:90%;">90.97</span></td>
<td id="S4.T2.2.8.8.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt"><span id="S4.T2.2.8.8.4.1" class="ltx_text" style="font-size:90%;">89.14</span></td>
<td id="S4.T2.2.8.8.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_tt"><span id="S4.T2.2.8.8.5.1" class="ltx_text" style="font-size:90%;">88.06</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>3D car detection results on the KITTI validation set, where EF and LF denote early fusion and late fusion, respectively.</figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Results on KITTI test set.</span>
The experimental results on the KITTI test set are reported in Table <a href="#S3.T1" title="Table 1 ‣ 3.3 Noise-Resistant Submanifold Convolution ‣ 3 VirConv for Multimodal 3D Detection ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Our VirConv-L, VirConv-T, and VirConv-S outperform the baseline Voxel-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> by 3.43%, 4.63% and 5.58% 3D AP (R40) in the moderate car class, respectively.
The VirConv-L, VirConv-T, and VirConv-S also outperform the best previous 3D detector SFD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> by 0.29%, 1.49%, and 2.44%, respectively.
As of the date of the CVPR deadline (Nov.11, 2022), our VirConv-T and VirConv-S rank 2nd and 1st, respectively, on the KITTI 3D object detection leaderboard.
The results further demonstrate the effectiveness of our method.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation Study</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We conducted experiments on the KITTI validation set to examine the hyper-parameters and validate each component/design of the proposed method.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.3.4.1" class="ltx_tr">
<td id="S4.T3.3.4.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.4.1.1.1" class="ltx_text" style="font-size:90%;">Setting</span></td>
<td id="S4.T3.3.4.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.4.1.2.1" class="ltx_text" style="font-size:90%;">VirConv</span></td>
<td id="S4.T3.3.4.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.4.1.3.1" class="ltx_text" style="font-size:90%;">TRS</span></td>
<td id="S4.T3.3.4.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.4.1.4.1" class="ltx_text" style="font-size:90%;">3D AP</span></td>
<td id="S4.T3.3.4.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.4.1.5.1" class="ltx_text" style="font-size:90%;">Time (ms)</span></td>
</tr>
<tr id="S4.T3.3.5.2" class="ltx_tr">
<td id="S4.T3.3.5.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.5.2.1.1" class="ltx_text" style="font-size:90%;">LiDAR points</span></td>
<td id="S4.T3.3.5.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.5.2.2.1" class="ltx_text" style="font-size:90%;">No</span></td>
<td id="S4.T3.3.5.2.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T3.3.5.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.5.2.4.1" class="ltx_text" style="font-size:90%;">85.29</span></td>
<td id="S4.T3.3.5.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.5.2.5.1" class="ltx_text" style="font-size:90%;">38</span></td>
</tr>
<tr id="S4.T3.3.6.3" class="ltx_tr">
<td id="S4.T3.3.6.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="3"><span id="S4.T3.3.6.3.1.1" class="ltx_text" style="font-size:90%;">Virtual points</span></td>
<td id="S4.T3.3.6.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.6.3.2.1" class="ltx_text" style="font-size:90%;">No</span></td>
<td id="S4.T3.3.6.3.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T3.3.6.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.6.3.4.1" class="ltx_text" style="font-size:90%;">76.12</span></td>
<td id="S4.T3.3.6.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.6.3.5.1" class="ltx_text" style="font-size:90%;">84</span></td>
</tr>
<tr id="S4.T3.3.7.4" class="ltx_tr">
<td id="S4.T3.3.7.4.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.3.7.4.1.1" class="ltx_text" style="font-size:90%;">Yes</span></td>
<td id="S4.T3.3.7.4.2" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.3.7.4.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.3.7.4.3.1" class="ltx_text" style="font-size:90%;">79.55</span></td>
<td id="S4.T3.3.7.4.4" class="ltx_td ltx_align_center"><span id="S4.T3.3.7.4.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">52</span></td>
</tr>
<tr id="S4.T3.1.1" class="ltx_tr">
<td id="S4.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.1.1.2.1" class="ltx_text" style="font-size:90%;">Yes</span></td>
<td id="S4.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T3.1.1.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S4.T3.1.1.1.m1.1a"><mi mathsize="90%" mathvariant="normal" id="S4.T3.1.1.1.m1.1.1" xref="S4.T3.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S4.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">80.91</span></td>
<td id="S4.T3.1.1.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.4.1" class="ltx_text" style="font-size:90%;">71</span></td>
</tr>
<tr id="S4.T3.3.8.5" class="ltx_tr">
<td id="S4.T3.3.8.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="3"><span id="S4.T3.3.8.5.1.1" class="ltx_text" style="font-size:90%;">Early fusion</span></td>
<td id="S4.T3.3.8.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.8.5.2.1" class="ltx_text" style="font-size:90%;">No</span></td>
<td id="S4.T3.3.8.5.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T3.3.8.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.8.5.4.1" class="ltx_text" style="font-size:90%;">85.78</span></td>
<td id="S4.T3.3.8.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.8.5.5.1" class="ltx_text" style="font-size:90%;">88</span></td>
</tr>
<tr id="S4.T3.3.9.6" class="ltx_tr">
<td id="S4.T3.3.9.6.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.3.9.6.1.1" class="ltx_text" style="font-size:90%;">Yes</span></td>
<td id="S4.T3.3.9.6.2" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.3.9.6.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.3.9.6.3.1" class="ltx_text" style="font-size:90%;">88.71</span></td>
<td id="S4.T3.3.9.6.4" class="ltx_td ltx_align_center"><span id="S4.T3.3.9.6.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">56</span></td>
</tr>
<tr id="S4.T3.2.2" class="ltx_tr">
<td id="S4.T3.2.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.2.2.1" class="ltx_text" style="font-size:90%;">Yes</span></td>
<td id="S4.T3.2.2.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T3.2.2.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S4.T3.2.2.1.m1.1a"><mi mathsize="90%" mathvariant="normal" id="S4.T3.2.2.1.m1.1.1" xref="S4.T3.2.2.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.1.m1.1b"><ci id="S4.T3.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S4.T3.2.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.2.2.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">88.96</span></td>
<td id="S4.T3.2.2.4" class="ltx_td ltx_align_center"><span id="S4.T3.2.2.4.1" class="ltx_text" style="font-size:90%;">76</span></td>
</tr>
<tr id="S4.T3.3.10.7" class="ltx_tr">
<td id="S4.T3.3.10.7.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" rowspan="3"><span id="S4.T3.3.10.7.1.1" class="ltx_text" style="font-size:90%;">Late fusion</span></td>
<td id="S4.T3.3.10.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.10.7.2.1" class="ltx_text" style="font-size:90%;">No</span></td>
<td id="S4.T3.3.10.7.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T3.3.10.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.10.7.4.1" class="ltx_text" style="font-size:90%;">86.32</span></td>
<td id="S4.T3.3.10.7.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.10.7.5.1" class="ltx_text" style="font-size:90%;">120</span></td>
</tr>
<tr id="S4.T3.3.11.8" class="ltx_tr">
<td id="S4.T3.3.11.8.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.3.11.8.1.1" class="ltx_text" style="font-size:90%;">Yes</span></td>
<td id="S4.T3.3.11.8.2" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.3.11.8.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.3.11.8.3.1" class="ltx_text" style="font-size:90%;">88.97</span></td>
<td id="S4.T3.3.11.8.4" class="ltx_td ltx_align_center"><span id="S4.T3.3.11.8.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">74</span></td>
</tr>
<tr id="S4.T3.3.3" class="ltx_tr">
<td id="S4.T3.3.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T3.3.3.2.1" class="ltx_text" style="font-size:90%;">Yes</span></td>
<td id="S4.T3.3.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><math id="S4.T3.3.3.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S4.T3.3.3.1.m1.1a"><mi mathsize="90%" mathvariant="normal" id="S4.T3.3.3.1.m1.1.1" xref="S4.T3.3.3.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.1.m1.1b"><ci id="S4.T3.3.3.1.m1.1.1.cmml" xref="S4.T3.3.3.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S4.T3.3.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T3.3.3.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">90.29</span></td>
<td id="S4.T3.3.3.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.3.3.4.1" class="ltx_text" style="font-size:90%;">92</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Ablation results on the KITTI validation set by using different fusion scheme. </figcaption>
</figure>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.2.1.1" class="ltx_tr">
<th id="S4.T4.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.2.1.1.1.1" class="ltx_text" style="font-size:90%;">with</span></th>
<th id="S4.T4.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T4.2.1.1.2.1" class="ltx_text" style="font-size:90%;">with</span></th>
<th id="S4.T4.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3"><span id="S4.T4.2.1.1.3.1" class="ltx_text" style="font-size:90%;">3D AP</span></th>
<th id="S4.T4.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.2.1.1.4.1" class="ltx_text" style="font-size:90%;">Time</span></th>
</tr>
<tr id="S4.T4.2.2.2" class="ltx_tr">
<th id="S4.T4.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T4.2.2.2.1.1" class="ltx_text" style="font-size:90%;">StVD</span></th>
<th id="S4.T4.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S4.T4.2.2.2.2.1" class="ltx_text" style="font-size:90%;">NRConv</span></th>
<th id="S4.T4.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T4.2.2.2.3.1" class="ltx_text" style="font-size:90%;">Easy</span></th>
<th id="S4.T4.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T4.2.2.2.4.1" class="ltx_text" style="font-size:90%;">Mod.</span></th>
<th id="S4.T4.2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S4.T4.2.2.2.5.1" class="ltx_text" style="font-size:90%;">Hard</span></th>
<th id="S4.T4.2.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T4.2.2.2.6.1" class="ltx_text" style="font-size:90%;">(ms)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.2.3.1" class="ltx_tr">
<td id="S4.T4.2.3.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.3.1.1.1" class="ltx_text" style="font-size:90%;">No</span></td>
<td id="S4.T4.2.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.3.1.2.1" class="ltx_text" style="font-size:90%;">No</span></td>
<td id="S4.T4.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.3.1.3.1" class="ltx_text" style="font-size:90%;">94.26</span></td>
<td id="S4.T4.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.3.1.4.1" class="ltx_text" style="font-size:90%;">87.55</span></td>
<td id="S4.T4.2.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.3.1.5.1" class="ltx_text" style="font-size:90%;">85.49</span></td>
<td id="S4.T4.2.3.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.3.1.6.1" class="ltx_text" style="font-size:90%;">152</span></td>
</tr>
<tr id="S4.T4.2.4.2" class="ltx_tr">
<td id="S4.T4.2.4.2.1" class="ltx_td ltx_align_center"><span id="S4.T4.2.4.2.1.1" class="ltx_text" style="font-size:90%;">Yes</span></td>
<td id="S4.T4.2.4.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T4.2.4.2.2.1" class="ltx_text" style="font-size:90%;">No</span></td>
<td id="S4.T4.2.4.2.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.4.2.3.1" class="ltx_text" style="font-size:90%;">94.55</span></td>
<td id="S4.T4.2.4.2.4" class="ltx_td ltx_align_center"><span id="S4.T4.2.4.2.4.1" class="ltx_text" style="font-size:90%;">88.32</span></td>
<td id="S4.T4.2.4.2.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T4.2.4.2.5.1" class="ltx_text" style="font-size:90%;">85.95</span></td>
<td id="S4.T4.2.4.2.6" class="ltx_td ltx_align_center"><span id="S4.T4.2.4.2.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">87</span></td>
</tr>
<tr id="S4.T4.2.5.3" class="ltx_tr">
<td id="S4.T4.2.5.3.1" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.2.5.3.1.1" class="ltx_text" style="font-size:90%;">Yes</span></td>
<td id="S4.T4.2.5.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T4.2.5.3.2.1" class="ltx_text" style="font-size:90%;">Yes</span></td>
<td id="S4.T4.2.5.3.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.2.5.3.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">95.81</span></td>
<td id="S4.T4.2.5.3.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.2.5.3.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">90.29</span></td>
<td id="S4.T4.2.5.3.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T4.2.5.3.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">88.10</span></td>
<td id="S4.T4.2.5.3.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.2.5.3.6.1" class="ltx_text" style="font-size:90%;">92</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>Ablation results on the KITTI validation set by using different designed components. </figcaption>
</figure>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p"><span id="S4.SS4.p2.1.1" class="ltx_text ltx_font_bold">VirConv performance with different fusion schemes.</span>
Virtual points only, early fusion, and late fusion are three potential choices for virtual points-based 3D object detection. To investigate the performance of VirConv under these three settings, we first constructed three baselines: Voxel-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> with only virtual points, Voxel-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> with early fusion, and Voxel-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> with late fusion.
Then we replaced the backbone of Voxel-RCNN with our VirConvNet. The experimental results on the KITTI validation set are shown in Table <a href="#S4.T3" title="Table 3 ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
With our VirConv, the 3D AP has significantly improved by 3.43%, 2.93%, and 2.65%, under virtual points only, early fusion, and late fusion settings, respectively. Meanwhile, the efficiency significantly improves. This is because VirConv speeds up the network with the StVD design and decreases the noise impact with the NRConv design.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p"><span id="S4.SS4.p3.1.1" class="ltx_text ltx_font_bold">Effectiveness of StVD.</span>
We next investigated the effectiveness of StVD. The results are shown in Table <a href="#S4.T4" title="Table 4 ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. With StVD, VirConv-T not only performs more accurate 3D detection but also runs faster by about 2<math id="S4.SS4.p3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS4.p3.1.m1.1a"><mo id="S4.SS4.p3.1.m1.1.1" xref="S4.SS4.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.1b"><times id="S4.SS4.p3.1.m1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.1c">\times</annotation></semantics></math>. The reason lies in that StVD discards about 90% of redundant voxels to speed up the network, and it also improves the detection robustness by simulating more sparse training samples.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p"><span id="S4.SS4.p4.1.1" class="ltx_text ltx_font_bold">Influence of StVD rate.</span>
We then conducted experiments to select the best input and layer StVD rate. The results are shown in Fig. <a href="#S4.F6" title="Figure 6 ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. We observe that using a higher input StVD rate, the detection performance will decrease dramatically due to the geometry feature loss. On the contrary, using a lower input StVD rate, the efficiency is degraded with poor AP improvement. We found that by randomly discarding 90% of nearby voxels, we achieve the best accuracy-efficiency trad-off. Therefore, this paper adopts an input StVD rate of 90%. Similarly, by using a 15% layer StVD rate, we achieved the best detection accuracy.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2303.02314/assets/x6.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="204" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.3.2" class="ltx_text" style="font-size:90%;">Left: precision and speed trade-off by using different Input StVD rate. Right: detection performance by using different layer StVD rate.</span></figcaption>
</figure>
<div id="S4.SS4.p5" class="ltx_para">
<p id="S4.SS4.p5.1" class="ltx_p"><span id="S4.SS4.p5.1.1" class="ltx_text ltx_font_bold">Effectiveness of NRConv.</span>
We then investigated the effects of NRConv using VirConv-T. The results are shown in Table <a href="#S4.T4" title="Table 4 ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. With our NRConv, the car detection AP of VirConv-T improves from 88.32% to 90.29%.
Since the NRConv encodes the voxel features in both 3D and 2D image space, reducing the noise impact brought by the inaccurate depth completion, the detection performance is significantly improved.</p>
</div>
<div id="S4.SS4.p6" class="ltx_para">
<p id="S4.SS4.p6.1" class="ltx_p"><span id="S4.SS4.p6.1.1" class="ltx_text ltx_font_bold">Effectiveness of TRS.</span> We conducted experiments to examine the effect of TRS in VirConv-T. The results are shown in Table <a href="#S4.T3" title="Table 3 ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. With our TRS, detectors show 1.36%, 0.25%, and 1.32% performance improvement under virtual points only, early fusion, and late fusion, respectively. The performance gain is derived from the two-transform and two-stage refinement, which improves the transformation robustness and leads to better detection performance.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T5.2.1.1" class="ltx_tr">
<th id="S4.T5.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T5.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Class</span></th>
<th id="S4.T5.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T5.2.1.1.2.1" class="ltx_text" style="font-size:90%;">Method</span></th>
<td id="S4.T5.2.1.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="3"><span id="S4.T5.2.1.1.3.1" class="ltx_text" style="font-size:90%;">3D AP</span></td>
</tr>
<tr id="S4.T5.2.2.2" class="ltx_tr">
<td id="S4.T5.2.2.2.1" class="ltx_td ltx_align_center"><span id="S4.T5.2.2.2.1.1" class="ltx_text" style="font-size:90%;">Easy</span></td>
<td id="S4.T5.2.2.2.2" class="ltx_td ltx_align_center"><span id="S4.T5.2.2.2.2.1" class="ltx_text" style="font-size:90%;">Mod.</span></td>
<td id="S4.T5.2.2.2.3" class="ltx_td ltx_align_center"><span id="S4.T5.2.2.2.3.1" class="ltx_text" style="font-size:90%;">Hard</span></td>
</tr>
<tr id="S4.T5.2.3.3" class="ltx_tr">
<th id="S4.T5.2.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T5.2.3.3.1.1" class="ltx_text" style="font-size:90%;">Car</span></th>
<th id="S4.T5.2.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T5.2.3.3.2.1" class="ltx_text" style="font-size:90%;">Baseline</span></th>
<td id="S4.T5.2.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.2.3.3.3.1" class="ltx_text" style="font-size:90%;">89.39</span></td>
<td id="S4.T5.2.3.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.2.3.3.4.1" class="ltx_text" style="font-size:90%;">83.83</span></td>
<td id="S4.T5.2.3.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.2.3.3.5.1" class="ltx_text" style="font-size:90%;">87.73</span></td>
</tr>
<tr id="S4.T5.2.4.4" class="ltx_tr">
<th id="S4.T5.2.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T5.2.4.4.1.1" class="ltx_text" style="font-size:90%;">VirConv-T</span></th>
<td id="S4.T5.2.4.4.2" class="ltx_td ltx_align_center"><span id="S4.T5.2.4.4.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">94.98</span></td>
<td id="S4.T5.2.4.4.3" class="ltx_td ltx_align_center"><span id="S4.T5.2.4.4.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">89.96</span></td>
<td id="S4.T5.2.4.4.4" class="ltx_td ltx_align_center"><span id="S4.T5.2.4.4.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">88.13</span></td>
</tr>
<tr id="S4.T5.2.5.5" class="ltx_tr">
<th id="S4.T5.2.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T5.2.5.5.1.1" class="ltx_text" style="font-size:90%;">Pedestrian</span></th>
<th id="S4.T5.2.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T5.2.5.5.2.1" class="ltx_text" style="font-size:90%;">Baseline</span></th>
<td id="S4.T5.2.5.5.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.2.5.5.3.1" class="ltx_text" style="font-size:90%;">70.55</span></td>
<td id="S4.T5.2.5.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.2.5.5.4.1" class="ltx_text" style="font-size:90%;">62.92</span></td>
<td id="S4.T5.2.5.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.2.5.5.5.1" class="ltx_text" style="font-size:90%;">57.35</span></td>
</tr>
<tr id="S4.T5.2.6.6" class="ltx_tr">
<th id="S4.T5.2.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T5.2.6.6.1.1" class="ltx_text" style="font-size:90%;">VirConv-T</span></th>
<td id="S4.T5.2.6.6.2" class="ltx_td ltx_align_center"><span id="S4.T5.2.6.6.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">73.32</span></td>
<td id="S4.T5.2.6.6.3" class="ltx_td ltx_align_center"><span id="S4.T5.2.6.6.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">66.93</span></td>
<td id="S4.T5.2.6.6.4" class="ltx_td ltx_align_center"><span id="S4.T5.2.6.6.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">60.38</span></td>
</tr>
<tr id="S4.T5.2.7.7" class="ltx_tr">
<th id="S4.T5.2.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T5.2.7.7.1.1" class="ltx_text" style="font-size:90%;">Cyclist</span></th>
<th id="S4.T5.2.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T5.2.7.7.2.1" class="ltx_text" style="font-size:90%;">Baseline</span></th>
<td id="S4.T5.2.7.7.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.2.7.7.3.1" class="ltx_text" style="font-size:90%;">89.86</span></td>
<td id="S4.T5.2.7.7.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.2.7.7.4.1" class="ltx_text" style="font-size:90%;">71.13</span></td>
<td id="S4.T5.2.7.7.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.2.7.7.5.1" class="ltx_text" style="font-size:90%;">66.67</span></td>
</tr>
<tr id="S4.T5.2.8.8" class="ltx_tr">
<th id="S4.T5.2.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r"><span id="S4.T5.2.8.8.1.1" class="ltx_text" style="font-size:90%;">VirConv-T</span></th>
<td id="S4.T5.2.8.8.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.2.8.8.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">90.04</span></td>
<td id="S4.T5.2.8.8.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.2.8.8.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">73.90</span></td>
<td id="S4.T5.2.8.8.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.2.8.8.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">69.06</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 5: </span>3D Detection results (3D AP (R40)) of multi-class VirConv-T (KITTI validation set). </figcaption>
</figure>
<div id="S4.SS4.p7" class="ltx_para">
<p id="S4.SS4.p7.1" class="ltx_p"><span id="S4.SS4.p7.1.1" class="ltx_text ltx_font_bold">Multi-class performance.</span>
We also trained a multi-class VirConv-T to detect car, pedestrian and cyclist class instances using a single model.
We reported the multi-class 3D object detection performance in Table <a href="#S4.T5" title="Table 5 ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, where the baseline refers to the multi-class Voxel-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Compared with the baseline, the detection performance of VirConv-T in all classes has been significantly improved. The results demonstrate that our VirConv can be easily generalized to a multi-class model and boost the detection performance.</p>
</div>
<div id="S4.SS4.p8" class="ltx_para">
<p id="S4.SS4.p8.1" class="ltx_p"><span id="S4.SS4.p8.1.1" class="ltx_text ltx_font_bold">Performance breakdown.</span>
To investigate where our model improves the baseline most, we evaluate the detection performance based on the different distances. The results are shown in Fig. <a href="#S4.F7" title="Figure 7 ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. Our three detectors have significant improvements for faraway objects because our VirConv models better geometry features of distant sparse objects from the virtual points.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2303.02314/assets/x7.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="202" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.3.2" class="ltx_text" style="font-size:90%;">3D AP and performance improvement along different detection distance (KITTI validation set).</span></figcaption>
</figure>
<figure id="S4.T6" class="ltx_table">
<table id="S4.T6.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T6.2.1.1" class="ltx_tr">
<td id="S4.T6.2.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T6.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Method</span></td>
<td id="S4.T6.2.1.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.2.1.1.2.1" class="ltx_text" style="font-size:90%;">mAP</span></td>
<td id="S4.T6.2.1.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.2.1.1.3.1" class="ltx_text" style="font-size:90%;">NDS</span></td>
</tr>
<tr id="S4.T6.2.2.2" class="ltx_tr">
<td id="S4.T6.2.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S4.T6.2.2.2.1.1" class="ltx_text" style="font-size:90%;">CenterPoint + VP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T6.2.2.2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib45" title="" class="ltx_ref">45</a><span id="S4.T6.2.2.2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T6.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.2.2.2.2.1" class="ltx_text" style="font-size:90%;">66.4</span></td>
<td id="S4.T6.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.2.2.2.3.1" class="ltx_text" style="font-size:90%;">70.5</span></td>
</tr>
<tr id="S4.T6.2.3.3" class="ltx_tr">
<td id="S4.T6.2.3.3.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T6.2.3.3.1.1" class="ltx_text" style="font-size:90%;">CenterPoint + VP + VirConv</span></td>
<td id="S4.T6.2.3.3.2" class="ltx_td ltx_align_center"><span id="S4.T6.2.3.3.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">67.2</span></td>
<td id="S4.T6.2.3.3.3" class="ltx_td ltx_align_center"><span id="S4.T6.2.3.3.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">71.2</span></td>
</tr>
<tr id="S4.T6.2.4.4" class="ltx_tr">
<td id="S4.T6.2.4.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S4.T6.2.4.4.1.1" class="ltx_text" style="font-size:90%;">TransFusion </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T6.2.4.4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S4.T6.2.4.4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T6.2.4.4.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.2.4.4.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">68.9</span></td>
<td id="S4.T6.2.4.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.2.4.4.3.1" class="ltx_text" style="font-size:90%;">71.7</span></td>
</tr>
<tr id="S4.T6.2.5.5" class="ltx_tr">
<td id="S4.T6.2.5.5.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T6.2.5.5.1.1" class="ltx_text" style="font-size:90%;">TransFusion-L+VP</span></td>
<td id="S4.T6.2.5.5.2" class="ltx_td ltx_align_center"><span id="S4.T6.2.5.5.2.1" class="ltx_text" style="font-size:90%;">66.7</span></td>
<td id="S4.T6.2.5.5.3" class="ltx_td ltx_align_center"><span id="S4.T6.2.5.5.3.1" class="ltx_text" style="font-size:90%;">70.8</span></td>
</tr>
<tr id="S4.T6.2.6.6" class="ltx_tr">
<td id="S4.T6.2.6.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T6.2.6.6.1.1" class="ltx_text" style="font-size:90%;">TransFusion-L +VP + VirConv</span></td>
<td id="S4.T6.2.6.6.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T6.2.6.6.2.1" class="ltx_text" style="font-size:90%;">68.7</span></td>
<td id="S4.T6.2.6.6.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T6.2.6.6.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">72.3</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 6: </span>3D detection results on the nuScenes test set. </figcaption>
</figure>
<div id="S4.SS4.p9" class="ltx_para">
<p id="S4.SS4.p9.1" class="ltx_p"><span id="S4.SS4.p9.1.1" class="ltx_text ltx_font_bold">Evaluation on nuScenes test set.</span>
To demonstrate the universality of our method, we conducted an experiment on the nuScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> dataset. we compared our method with CenterPoint + VP (virtual point), TransFuison-L + VP and TransFusion. We adopted the same data augmentation strategy as TransFuison-L and trained the network for 30 epochs on 8 Tesla V100 GPUs. The results on the nuScenes test set are shown in Table <a href="#S4.T6" title="Table 6 ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Virtual Sparse Convolution for Multimodal 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. With VirConv, the detection performance of CenterPoint + VP and TransFuison-L + VP has been significantly improved. In addition, the TransFusion-L with VirConv even surpasses the TransFusion in terms of NDS, demonstrating that our model is able to boost the virtual point-based detector significantly.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This paper presented a new VirConv operator for virtual-point-based multimodal 3D object detection. VirConv addressed the density and noise problems of virtual points through the newly designed Stochastic Voxel Discard and Noise-Resistant Submanifold Convolution mechanisms. Built upon VirConv, we presented VirConv-L, VirConv-T, and VirConv-S for efficient, accurate, and semi-supervised 3D detection, respectively. Our VirConvNet holds the leading entry on both KITTI car 3D object detection and BEV detection leaderboards, demonstrating the effectiveness of our method.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Acknowledgements </span>
This work was supported in part by the National Natural Science Foundation of China (No.62171393), and the Fundamental Research Funds for the Central Universities (No.20720220064).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Xuyang Bai, Zeyu Hu, Xinge Zhu, Qingqiu Huang, Yilun Chen, Hongbo Fu, and
Chiew-Lan Tai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Transfusion: Robust lidar-camera fusion for 3d object detection with
transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Jorge Beltrán, Carlos Guindel, Francisco Miguel Moreno, Daniel Cruzado,
Fernando Garcia, and Arturo De La Escalera.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Birdnet: a 3d object detection framework from lidar information.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ITSC</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, pages 3517–3523. IEEE, 2018.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong,
Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">nuscenes: A multimodal dataset for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Multi-view 3d object detection network for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, pages 1907–1915, 2017.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Xuanyao Chen, Tianyuan Zhang, Yue Wang, Yilun Wang, and Hang Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Futr3d: A unified sensor fusion framework for 3d detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ArXiv</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Yukang Chen, Yanwei Li, X. Zhang, Jian Sun, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Focal sparse convolutional networks for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition CVPR)</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Jiajun Deng, Shaoshuai Shi, Peiwei Li, Wen gang Zhou, Yanyong Zhang, and
Houqiang Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Voxel r-cnn: Towards high performance voxel-based 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial
Intelligence</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Shachar Fleishman, Iddo Drori, and Daniel Cohen-Or.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Bilateral mesh denoising.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM SIGGRAPH 2003 Papers</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 2003.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Andreas Geiger, Philip Lenz, and Raquel Urtasun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Are we ready for autonomous driving? the kitti vision benchmark
suite.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages 3354–3361, 2012.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Benjamin Graham, Martin Engelcke, and Laurens van der Maaten.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">3d semantic segmentation with submanifold sparse convolutional
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 9224–9232, 2018.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Xian-Feng Han, Jesse S. Jin, Mingjie Wang, and Wei Jiang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Guided 3d point cloud filtering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Multimedia Tools and Applications</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, 77:17397–17411, 2017.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Xian-Feng Han, Jesse S. Jin, Mingjie Wang, Wei Jiang, Lei Gao, and Liping Xiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">A review of algorithms for filtering the 3d point cloud.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Signal Process. Image Commun.</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, 57:103–112, 2017.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Chenhang He, Hui Zeng, Jianqiang Huang, Xian-Sheng Hua, and Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Structure aware single-stage 3d object detection from point cloud.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages 11873–11882, 2020.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Jianzhong He, Shiliang Zhang, Ming Yang, Yanhu Shan, and Tiejun Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Bi-directional cascade network for perceptual edge detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">2019 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Robin Heinzler, Florian Piewak, Philipp Schindler, and Wilhelm Stork.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Cnn-based lidar point cloud de-noising in adverse weather.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Robotics and Automation Letters</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Mu Hu, Shuling Wang, Bin Li, Shiyu Ning, Li Fan, and Xiaojin Gong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Penet: Towards precise and efficient image guided depth completion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">In International Conference on Robotics and Automation
(ICRA)</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages 13656–13662, 2021.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang,
Agathoniki Trigoni, and A. Markham.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Randla-net: Efficient semantic segmentation of large-scale point
clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Jason Ku, Melissa Mozifian, Jungwook Lee, Ali Harakeh, and Steven L. Waslander.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Joint 3d proposal generation and object detection from view
aggregation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS)</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, pages 1–8, 2018.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar
Beijbom.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Pointpillars: Fast encoders for object detection from point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, pages 12697–12705, 2019.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Ming Liang, Binh Yang, Yun Chen, Rui Hu, and Raquel Urtasun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Multi-task multi-sensor fusion for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, pages 7337–7345, 2019.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang, Huizi Mao, Daniela Rus,
and Song Han.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Bevfusion: Multi-task multi-sensor fusion with unified bird’s-eye
view representation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ArXiv</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Shitong Luo and Wei Hu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Score-based point cloud denoising.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">2021 IEEE/CVF International Conference on Computer Vision
(ICCV)</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Jiageng Mao, Minzhe Niu, Haoyue Bai, Xiaodan Liang, Hang Xu, and Chunjing Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Pyramid r-cnn: Towards better performance and adaptability for 3d
object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision (ICCV)</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
C. Qi, W. Liu, Chenxia Wu, Hao Su, and Leonidas J. Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Frustum pointnets for 3d object detection from rgb-d data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, pages 918–927, 2018.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Hualian Sheng, Sijia Cai, Yuan Liu, Bing Deng, Jianqiang Huang, Xiansheng Hua,
and Min-Jian Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Improving 3d object detection with channel-wise transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision (ICCV)</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and
Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Pv-rcnn: Point-voxel feature set abstraction for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, pages 10526 – 10535, 2020.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Pointrcnn: 3d object proposal generation and detection from point
cloud.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, pages 770–779, 2019.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Shaoshuai Shi, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">From points to parts: 3d object detection from point cloud with
part-aware and part-aggregation network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence
(TPAMI)</span><span id="bib.bib28.4.2" class="ltx_text" style="font-size:90%;">, 43:2647–2664, 2021.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Vishwanath A. Sindagi, Yin Zhou, and Oncel Tuzel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Mvx-net: Multimodal voxelnet for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">In International Conference on Robotics and Automation
(ICRA)</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Sourabh Vora, Alex H. Lang, Bassam Helou, and Oscar Beijbom.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Pointpainting: Sequential fusion for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib30.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Chunwei Wang, Chao Ma, Ming Zhu, and Xiaokang Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Pointaugmenting: Cross-modal augmentation for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Dequan Wang, Coline Devin, Qi-Zhi Cai, Philipp Krähenbühl, and Trevor
Darrell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Monocular plan view networks for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS)</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
He Wang, Yezhen Cong, Or Litany, Yue Gao, and Leonidas J. Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">3dioumatch: Leveraging iou prediction for semi-supervised 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition CVPR)</span><span id="bib.bib33.4.2" class="ltx_text" style="font-size:90%;">, pages 14610–14619, 2021.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Hai Wu, Jinhao Deng, Chenglu Wen, Xin Li, and Cheng Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Casa: A cascade attention network for 3d object detection from lidar
point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Geoscience and Remote Sensing</span><span id="bib.bib34.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Hai Wu, Chenglu Wen, Wei Li, Ruigang Yang, and Cheng Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Learning transformation-equivariant features for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial
Intelligence</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Xiaopei Wu, Liang Peng, Honghui Yang, Liang Xie, Chenxi Huang, Chengqi Deng,
Haifeng Liu, and Deng Cai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Sparse fuse dense: Towards high quality 3d detection with depth
completion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Hongyi Xu, Feng Liu, Qianyu Zhou, Jinkun Hao, Zhijie Cao, Zhengyang Feng, and
Lizhuang Ma.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Semi-supervised 3d object detection via adaptive pseudo-labeling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">2021 IEEE International Conference on Image Processing (ICIP)</span><span id="bib.bib37.4.2" class="ltx_text" style="font-size:90%;">,
pages 3183–3187, 2021.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Qiangeng Xu, Yiqi Zhong, and Ulrich Neumann.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Behind the curtain: Learning occluded shapes for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial
Intelligence</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Yan Yan, Yuxing Mao, and Bo Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Second: Sparsely embedded convolutional detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Sensors</span><span id="bib.bib39.4.2" class="ltx_text" style="font-size:90%;">, 18, 2018.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Honghui Yang, Zili Liu, Xiaopei Wu, Wenxiao Wang, Wei Qian, Xiaofei He, and
Deng Cai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Graph r-cnn: Towards accurate 3d object detection with
semantic-decorated local graph.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV)</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Jihan Yang, Shaoshuai Shi, Zhe Wang, Hongsheng Li, and Xiaojuan Qi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">St3d: Self-training for unsupervised domain adaptation on 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Zetong Yang, Yanan Sun, Shu Liu, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">3dssd: Point-based 3d single stage object detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, pages 11040–11048, 2020.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Zetong Yang, Yanan Sun, Shu Liu, Xiaoyong Shen, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Std: Sparse-to-dense 3d object detector for point cloud.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision (ICCV)</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, pages 1951–1960, 2019.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Tianwei Yin, Xingyi Zhou, and Philipp Krähenbühl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Center-based 3d object detection and tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Tianwei Yin, Xingyi Zhou, and Philipp Krähenbühl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Multimodal virtual point 3d detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, volume abs/2111.06881, 2021.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Jin Hyeok Yoo, Yeocheol Kim, Ji Song Kim, and Jun Won Choi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">3d-cvf: Generating joint camera and lidar features using cross-view
spatial feature fusion for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib46.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV)</span><span id="bib.bib46.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Na Zhao, Tat-Seng Chua, and Gim Hee Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Sess: Self-ensembling semi-supervised 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Wu Zheng, Weiliang Tang, Li Jiang, and Chi-Wing Fu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Se-ssd: Self-ensembling single-stage object detector from point
cloud.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, pages 14494–14503, 2021.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Dingfu Zhou, Jin Fang, Xibin Song, Chenye Guan, Junbo Yin, Yuchao Dai, and
Ruigang Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Iou loss for 2d/3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">3DV</span><span id="bib.bib49.4.2" class="ltx_text" style="font-size:90%;">, pages 85–94, 2019.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Hanqi Zhu, Jiajun Deng, Yu Zhang, Jianmin Ji, Qi-Chao Mao, Houqiang Li, and
Yanyong Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Vpfnet: Improving 3d object detection with virtual point based lidar
and stereo data fusion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ArXiv</span><span id="bib.bib50.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2303.02313" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2303.02314" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2303.02314">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2303.02314" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2303.02315" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 21:49:41 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
