<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2008.09457] DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild</title><meta property="og:description" content="We introduce DOPE, the first method to detect and estimate whole-body 3D human poses, including bodies, hands and faces, in the wild.
Achieving this level of details is key for a number of applications that require undâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2008.09457">

<!--Generated on Sat Mar  2 09:43:51 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Human pose estimation,  human pose detection,  3D pose estimation,  2D pose estimation,  body pose estimation,  hand pose estimation,  face landmarks estimation">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>NAVER LABS Europe</span></span></span>
<h1 class="ltx_title ltx_title_document">DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Philippe Weinzaepfel â€ƒâ€ƒRomain BrÃ©gier â€ƒâ€ƒHadrien Combaluzier 
<br class="ltx_break">Vincent Leroy â€ƒâ€ƒGrÃ©gory Rogez
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">We introduce DOPE, the first method to detect and estimate whole-body 3D human poses, including bodies, hands and faces, in the wild.
Achieving this level of details is key for a number of applications that require understanding the interactions of the people with each other or with the environment. The main challenge is the lack of in-the-wild data with labeled whole-body 3D poses. In previous work, training data has been annotated or generated for simpler tasks focusing on bodies, hands or faces separately. In this work, we propose to take advantage of these datasets to train independent experts for each part, namely a body, a hand and a face expert, and distill their knowledge into a single deep network designed for whole-body 2D-3D pose detection. In practice, given a training image with partial or no annotation, each part expert detects
its subset of keypoints in 2D and 3D and the resulting estimations are combined to obtain whole-body pseudo ground-truth poses.
A distillation loss encourages the whole-body predictions to mimic the expertsâ€™ outputs.
Our results show that this approach significantly outperforms the same whole-body model trained without distillation while staying close to the performance of the experts. Importantly, DOPE is computationally less demanding than the ensemble of experts and can achieve real-time performance.
Test code and models are available at <a target="_blank" href="https://europe.naverlabs.com/research/computer-vision/dope" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://europe.naverlabs.com/research/computer-vision/dope</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Human pose estimation, human pose detection, 3D pose estimation, 2D pose estimation, body pose estimation, hand pose estimation, face landmarks estimation
</div>
<div id="id3" class="ltx_logical-block">
<div id="id3.p1" class="ltx_para">
<img src="/html/2008.09457/assets/x1.jpg" id="id1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="266" height="803" alt="[Uncaptioned image]"><img src="/html/2008.09457/assets/x2.jpg" id="id2.g2" class="ltx_graphics ltx_centering ltx_img_portrait" width="266" height="2102" alt="[Uncaptioned image]">
</div>
<figure id="S0.F1" class="ltx_figure ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span> Results of our DOPE approach for 2D-3D whole-body pose estimation.</figcaption>
</figure>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Understanding humans in real-world images and videos has numerous potential applications
ranging from avatar animation for augmented and virtual realityÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> to roboticsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.
To fully analyze the interactions of people with each other or with the environment, and to recognize their emotions or activities, a detailed pose of the whole human body would be beneficial.
This includes 3D body keypoints, <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p1.1.2" class="ltx_text"></span>, torsos, arms and legs, that give information on the global posture of the persons, but also detailed information about hands and faces to fully capture their expressiveness. The task of whole-body 3D human pose estimation has been mainly addressed part by part as indicated by the large literature on estimating 3D body poseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, 3D hand poseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> or 3D face landmarks and shapeÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> in the wild. These methods now reach outstanding performances on their specific tasks, and combining them in an efficient way is an open problem.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">More recently, a few approaches have been introduced that capture body, hands and face pose jointly.
Hidalgo <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S1.p2.1.2" class="ltx_text"></span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> extend OpenPoseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> to predict 2D whole-body poses in natural images.
To train their multi-task learning approach, they partly rely on datasets for which adding 2D pose annotations is possible, <em id="S1.p2.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p2.1.4" class="ltx_text"></span>, adding 2D hand pose annotationsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> to the MPII body pose datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Such annotation scheme is not possible when dealing with 3D poses. Importantly, they observe global failure cases when a significant part of the target person is occluded or outside of the image boundaries. Some other works have leveraged expressive parametric human models composed of body, hand and face components stitched together such as AdamÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> or SMPL-XÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>.
These optimization-based approaches remain sensitive to initialization and are usually slow to converge. Their performance highly depends on the intermediate estimation of the 3D orientations of body parts or 2D keypoint locations, and is therefore limited in cases of occlusions or truncations at the image boundary compared to more direct approaches.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we propose the first learning-based method that, given an image, detects the people present in the scene and directly predicts the 2D and 3D poses of their bodies, hands and faces, see examples in FigureÂ <a href="#S0.F1" title="Figure 1 â€£ DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
Inspired by LCR-Net++Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, a Faster R-CNN like architectureÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> tailored for in-the-wild 2D-3D body pose estimation, we design a classification-regression network where the object categories to detect are body, hand and face pose classes. In a second step, a class-specific regression is applied to refine body, hand and face pose estimates by deforming the average pose of each class both in 2D and 3D.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">There exists no in-the-wild dataset to directly train our network, <em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p4.1.2" class="ltx_text"></span>, images
with 3D pose annotations for body, hand and face poses. Such data could only be obtained in specific controlled environments, <em id="S1.p4.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p4.1.4" class="ltx_text"></span> in motion capture rooms or through computer-generation, which would not suit our purpose of whole-body pose estimation in unconstrained scenarios. However, multiple in-the-wild datasets are available for each independent task, <em id="S1.p4.1.5" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p4.1.6" class="ltx_text"></span>, for 3D body pose estimationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, 3D hand pose estimationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> or 3D facial landmark estimationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.
Task-specific methods trained on these datasets perform well in practice but our experiments show that training our single model for whole-body 3D pose estimation on the union of these datasets leads to poor performances.
Each dataset being annotated with partial pose information (<em id="S1.p4.1.7" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p4.1.8" class="ltx_text"></span>, its specific part), unannotated parts are mistakenly considered as negatives by our detection framework, burdening the performance of the network.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2008.09457/assets/x3.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="238" alt="Refer to caption">
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overview of our DOPE training scheme. Each training image is processed by the part experts to detect their specific parts and estimate their 2D and 3D poses. The resulting detections are combined to obtain the whole-body poses used as ground-truth for this image when training our network.
We show only 2D poses for the sake of clarity but we also distill the 3D poses.</figcaption>
</figure>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">To handle this problem, we propose to train independent experts for each part, namely body, hand and face experts, and distill their knowledge to our whole-body pose detection network designed to perform the three tasks jointly.
In practice, given a training image with partial or no annotation, each part expert detects and estimates its subset of keypoints, in 2D and 3D, and the resulting estimations are combined to obtain whole-body pseudo ground-truth poses for the whole-body network.
FigureÂ <a href="#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates this training scheme.
A distillation loss is applied on the networkâ€™s output to keep it close to the expertsâ€™ predictions.
We name our method <span id="S1.p5.1.1" class="ltx_text ltx_font_bold">DOPE</span> for <span id="S1.p5.1.2" class="ltx_text ltx_font_bold">D</span>istillation <span id="S1.p5.1.3" class="ltx_text ltx_font_bold">O</span>f <span id="S1.p5.1.4" class="ltx_text ltx_font_bold">P</span>art <span id="S1.p5.1.5" class="ltx_text ltx_font_bold">E</span>xperts.
Our unified DOPE model performs on par with the part experts when evaluating each of the three tasks on dedicated datasets, while being computationally less demanding than the ensemble of experts and achieving real-time performances. In summary, we propose (a) a new architecture that can detect and estimate the whole-body 2D-3D pose of multiple people in the wild in real-time and (b) a novel and effective training scheme based on distillation that leverages previous data collection efforts for the individual subtasks.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">This paper is organized as follows.
After reviewing the related work (SectionÂ <a href="#S2" title="2 Related work â€£ DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), we present our DOPE method in SectionÂ <a href="#S3" title="3 DOPE for 2D-3D whole-body pose estimation â€£ DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
Finally, experimental results for body, hand and face pose estimation are reported in SectionÂ <a href="#S4" title="4 Experiments â€£ DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The problem of 3D human whole-body pose estimation has been mainly tackled by breaking the body into parts and focusing on the pose inference of these parts separately. In the following, we briefly review the state of the art for each of these subtasks, before summarizing the few approaches that predict the 3D pose of the entire body, and finally discussing existing distillation methods.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">3D body pose estimation.</span>
Two basic categories of work can be found in the recent literature: (a) approaches that directly estimate the 3D body keypoints from an input imageÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> and (b) methods that leverage 2D human pose estimationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.
The latter ones rely on a previous localization of the body keypoints in the image, through an off-the-shelf 2D pose detectorÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, and lift them to 3D spaceÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> or, as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, use them to initialize the optimization procedure of a parametric model of the human body such as SMPLÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>. For our body expert, we employ LCR-Net++Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> that jointly estimates 2D and 3D body poses from the image and has demonstrated robustness to challenging in-the-wild scenarios, <em id="S2.p2.1.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S2.p2.1.3" class="ltx_text"></span>, showing multiple interacting persons, with cluttered backgrounds and/or captured under severe occlusions and truncations.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">3D hand pose estimation.</span>
3D hand pose estimation from depth data has been studied for many years and state-of-the-art results on this task are now impressive as shown in a recent surveyÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>.
RGB-based 3D hand pose estimation is more challenging, and has gained interest in recent years.
Regression-based techniques try to directly predict 3D location of hand keypointsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> or even the vertices of a meshÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> from an input image.
Some methods incorporate priors by regressing parameters of a deformable hand model such as MANOÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>,
and many techniques leverage intermediate representations such as 2D keypoints heatmaps to perform 3D predictionsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>.
However, pose estimation is often performed on an image cropped around a single hand, and hand detection is performed independently. For our hand expert, we therefore use the detector ofÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> (adapted to hands) that recently achieved outstanding performances in RGB-based 3D hand pose estimation under hand-object interactionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">3D face pose estimation.</span>
As with hands, the recovery of the pose of a face is typically performed from an image crop, by detecting particular 2D facial landmarksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>. To better perceive the 3D pose and shape of a face, some works propose to fit a 3D Morphable ModelÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite> or to regress dense 3D face representationsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. In this work, we also adoptÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> as face expert, resulting in an hybrid model-free approach that regresses 3D facial landmarks independently from their visibility, as in the approach introduced for the Menpo 3D benchmarkÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
<div id="S2.p5" class="ltx_para ltx_noindent">
<p id="S2.p5.1" class="ltx_p"><span id="S2.p5.1.1" class="ltx_text ltx_font_bold">3D Whole-body pose estimation.</span>
The few existing methodsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> that predict the 3D pose of the whole-body all rely on parametric models of the human body, namely AdamÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> and SMPL-XÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>. These models are obtained by combining body, hand and face parametric models. Adam stitches together three different models: a simpler version of SMPL for the body, an artist-created rig for the hands, and the FaceWarehouse modelÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> for the face. In the case of SMPL-X, the SMPL body model is augmented with the FLAME head modelÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> and MANOÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>. A more realistic model is obtained in the case of SMPL-X by learning the shape and pose-dependent blend shapes. Both methods are based on an optimization scheme guided by 2D joint locations or 3D part orientations. Monocular Total CaptureÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> remains limited to a single person while for SMPL-XÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, the optimization strategy is applied independently on each person detected by OpenPoseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Optimizing over the parameters of such models can be time-consuming and the performance often depends on a correct initialization. Our approach is the first one that predicts whole-body 3D pose without relying on the optimization of a parametric model and can make real-time predictions of multiple 3D whole-body poses in real-world scenes. In addition, our DOPE training scheme can leverage datasets that do not contain ground-truth for all the parts at once.</p>
</div>
<div id="S2.p6" class="ltx_para ltx_noindent">
<p id="S2.p6.1" class="ltx_p"><span id="S2.p6.1.1" class="ltx_text ltx_font_bold">Distillation.</span>
Our learning procedure is based on the concept of distillation which was proposed in the context of efficient neural network computation by using class probabilities of a higher-capacity model as soft targets of a smaller and faster modelÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>.
Distillation has been successfully employed for several problems in computer vision such as object detectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, video classificationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, action recognitionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, multi-task learningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> or lifelong learningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.
In addition to training a compact modelÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>,
several works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> have shown that distillation can be combined with privileged informationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>, also called generalized distillationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> in order to train a network while leveraging extra modalities available for training, <em id="S2.p6.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.p6.1.3" class="ltx_text"></span> training on RGB and depth data while only RGB is available at test time.
In this paper, we propose to use distillation in order to transfer the knowledge of several body-part experts into a unified network that outputs a more complete representation of the whole human body.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>DOPE for 2D-3D whole-body pose estimation</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">After introducing our architecture for multi-person whole-body pose estimation (Section Â <a href="#S3.SS1" title="3.1 Whole-body pose architecture â€£ 3 DOPE for 2D-3D whole-body pose estimation â€£ DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>), we detail our training procedure based on distillation (SectionÂ <a href="#S3.SS2" title="3.2 Distillation of part experts â€£ 3 DOPE for 2D-3D whole-body pose estimation â€£ DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>).</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2008.09457/assets/x4.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="201" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Overview of our whole-body pose estimation architecture. Given an input image, convolutional features are computed and fed into a Region Proposal Network (RPN) to produce a list of candidate boxes. For each box, after RoI-Align and a few additional layers, 6 final outputs are computed (2 for each part). The first one returns a classification score for each anchor-pose corresponding to this part (including a background class not represented for clarity) while the second one returns refined 2D-3D pose estimates obtained through class-specific regression from the fitted anchor pose.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Whole-body pose architecture</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.3" class="ltx_p">We propose a method that, given an image, detects the people present in the scene and directly predicts the 2D and 3D poses of their bodies, hands and faces.
Our network architecture takes inspiration fromÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, which extends a Faster R-CNN like architectureÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> to the problem of 2D-3D body pose estimation and has shown to be robust in the wild.
We thus design a Localization-Classification-Regression network where the objects to be detected are bodies, hands and faces with respectively <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="J_{B}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">J</mi><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">ğ½</ci><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">ğµ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">J_{B}</annotation></semantics></math>, <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="J_{H}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">J</mi><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">H</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">ğ½</ci><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">ğ»</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">J_{H}</annotation></semantics></math> and <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="J_{F}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><msub id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">J</mi><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">F</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">ğ½</ci><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">ğ¹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">J_{F}</annotation></semantics></math> keypoints to be estimated in 2D and 3D. FigureÂ <a href="#S3.F3" title="Figure 3 â€£ 3 DOPE for 2D-3D whole-body pose estimation â€£ DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows an overview of this architecture.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Localization.</span> Given an input image, convolutional features (ResNet50Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> up to block3 in practice) are computed and fed into a Region Proposal Network (RPN) Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> to produce a list of candidate boxes containing potential body, hand or face instances. Although they might belong to the same person, we specifically treat the parts as separate objects to be robust to cases where only a face, a hand or a body is visible in the image. Our network can also output whole-body poses of multiple people at once, when their different parts are visible.
The candidate boxes generated by the RPN are used to pool convolutional features using RoI Align, and after a few additional layers (block4 from ResNet50 in practice), they are fed to the classification and regression branches, 6 in total: one classification and one regression branch per part.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.8" class="ltx_p"><span id="S3.SS1.p3.8.1" class="ltx_text ltx_font_bold">Classification.</span> Classification is performed for the three sub-tasks: body, hand and face classification. As inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, pose classes are defined by clustering the 3D pose space. This clustering is applied independently in the 3 pose spaces, corresponding to the 3 parts, obtaining respectively a set of <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="K_{B}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><msub id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">K</mi><mi id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">ğ¾</ci><ci id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">ğµ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">K_{B}</annotation></semantics></math>, <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="K_{H}" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><msub id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mi id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml">K</mi><mi id="S3.SS1.p3.2.m2.1.1.3" xref="S3.SS1.p3.2.m2.1.1.3.cmml">H</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2">ğ¾</ci><ci id="S3.SS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3">ğ»</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">K_{H}</annotation></semantics></math> and <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="K_{F}" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><msub id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml"><mi id="S3.SS1.p3.3.m3.1.1.2" xref="S3.SS1.p3.3.m3.1.1.2.cmml">K</mi><mi id="S3.SS1.p3.3.m3.1.1.3" xref="S3.SS1.p3.3.m3.1.1.3.cmml">F</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><apply id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p3.3.m3.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2">ğ¾</ci><ci id="S3.SS1.p3.3.m3.1.1.3.cmml" xref="S3.SS1.p3.3.m3.1.1.3">ğ¹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">K_{F}</annotation></semantics></math> classes for bodies, hands and faces. Note that to handle left and right hands with the same detector, we actually consider <math id="S3.SS1.p3.4.m4.1" class="ltx_Math" alttext="2\times K_{H}" display="inline"><semantics id="S3.SS1.p3.4.m4.1a"><mrow id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml"><mn id="S3.SS1.p3.4.m4.1.1.2" xref="S3.SS1.p3.4.m4.1.1.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.4.m4.1.1.1" xref="S3.SS1.p3.4.m4.1.1.1.cmml">Ã—</mo><msub id="S3.SS1.p3.4.m4.1.1.3" xref="S3.SS1.p3.4.m4.1.1.3.cmml"><mi id="S3.SS1.p3.4.m4.1.1.3.2" xref="S3.SS1.p3.4.m4.1.1.3.2.cmml">K</mi><mi id="S3.SS1.p3.4.m4.1.1.3.3" xref="S3.SS1.p3.4.m4.1.1.3.3.cmml">H</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><apply id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1"><times id="S3.SS1.p3.4.m4.1.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1.1"></times><cn type="integer" id="S3.SS1.p3.4.m4.1.1.2.cmml" xref="S3.SS1.p3.4.m4.1.1.2">2</cn><apply id="S3.SS1.p3.4.m4.1.1.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.1.3.1.cmml" xref="S3.SS1.p3.4.m4.1.1.3">subscript</csymbol><ci id="S3.SS1.p3.4.m4.1.1.3.2.cmml" xref="S3.SS1.p3.4.m4.1.1.3.2">ğ¾</ci><ci id="S3.SS1.p3.4.m4.1.1.3.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3.3">ğ»</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">2\times K_{H}</annotation></semantics></math> hand classes, <math id="S3.SS1.p3.5.m5.1" class="ltx_Math" alttext="K_{H}" display="inline"><semantics id="S3.SS1.p3.5.m5.1a"><msub id="S3.SS1.p3.5.m5.1.1" xref="S3.SS1.p3.5.m5.1.1.cmml"><mi id="S3.SS1.p3.5.m5.1.1.2" xref="S3.SS1.p3.5.m5.1.1.2.cmml">K</mi><mi id="S3.SS1.p3.5.m5.1.1.3" xref="S3.SS1.p3.5.m5.1.1.3.cmml">H</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b"><apply id="S3.SS1.p3.5.m5.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m5.1.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p3.5.m5.1.1.2.cmml" xref="S3.SS1.p3.5.m5.1.1.2">ğ¾</ci><ci id="S3.SS1.p3.5.m5.1.1.3.cmml" xref="S3.SS1.p3.5.m5.1.1.3">ğ»</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">K_{H}</annotation></semantics></math> for each side. For each classification branch, we also consider an additional background class to use the classifier as a detector. Therefore, each candidate box is classified into <math id="S3.SS1.p3.6.m6.1" class="ltx_Math" alttext="K_{B}+1" display="inline"><semantics id="S3.SS1.p3.6.m6.1a"><mrow id="S3.SS1.p3.6.m6.1.1" xref="S3.SS1.p3.6.m6.1.1.cmml"><msub id="S3.SS1.p3.6.m6.1.1.2" xref="S3.SS1.p3.6.m6.1.1.2.cmml"><mi id="S3.SS1.p3.6.m6.1.1.2.2" xref="S3.SS1.p3.6.m6.1.1.2.2.cmml">K</mi><mi id="S3.SS1.p3.6.m6.1.1.2.3" xref="S3.SS1.p3.6.m6.1.1.2.3.cmml">B</mi></msub><mo id="S3.SS1.p3.6.m6.1.1.1" xref="S3.SS1.p3.6.m6.1.1.1.cmml">+</mo><mn id="S3.SS1.p3.6.m6.1.1.3" xref="S3.SS1.p3.6.m6.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.m6.1b"><apply id="S3.SS1.p3.6.m6.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1"><plus id="S3.SS1.p3.6.m6.1.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1.1"></plus><apply id="S3.SS1.p3.6.m6.1.1.2.cmml" xref="S3.SS1.p3.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.6.m6.1.1.2.1.cmml" xref="S3.SS1.p3.6.m6.1.1.2">subscript</csymbol><ci id="S3.SS1.p3.6.m6.1.1.2.2.cmml" xref="S3.SS1.p3.6.m6.1.1.2.2">ğ¾</ci><ci id="S3.SS1.p3.6.m6.1.1.2.3.cmml" xref="S3.SS1.p3.6.m6.1.1.2.3">ğµ</ci></apply><cn type="integer" id="S3.SS1.p3.6.m6.1.1.3.cmml" xref="S3.SS1.p3.6.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.6.m6.1c">K_{B}+1</annotation></semantics></math> labels for body classes, <math id="S3.SS1.p3.7.m7.1" class="ltx_Math" alttext="2K_{H}+1" display="inline"><semantics id="S3.SS1.p3.7.m7.1a"><mrow id="S3.SS1.p3.7.m7.1.1" xref="S3.SS1.p3.7.m7.1.1.cmml"><mrow id="S3.SS1.p3.7.m7.1.1.2" xref="S3.SS1.p3.7.m7.1.1.2.cmml"><mn id="S3.SS1.p3.7.m7.1.1.2.2" xref="S3.SS1.p3.7.m7.1.1.2.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS1.p3.7.m7.1.1.2.1" xref="S3.SS1.p3.7.m7.1.1.2.1.cmml">â€‹</mo><msub id="S3.SS1.p3.7.m7.1.1.2.3" xref="S3.SS1.p3.7.m7.1.1.2.3.cmml"><mi id="S3.SS1.p3.7.m7.1.1.2.3.2" xref="S3.SS1.p3.7.m7.1.1.2.3.2.cmml">K</mi><mi id="S3.SS1.p3.7.m7.1.1.2.3.3" xref="S3.SS1.p3.7.m7.1.1.2.3.3.cmml">H</mi></msub></mrow><mo id="S3.SS1.p3.7.m7.1.1.1" xref="S3.SS1.p3.7.m7.1.1.1.cmml">+</mo><mn id="S3.SS1.p3.7.m7.1.1.3" xref="S3.SS1.p3.7.m7.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.7.m7.1b"><apply id="S3.SS1.p3.7.m7.1.1.cmml" xref="S3.SS1.p3.7.m7.1.1"><plus id="S3.SS1.p3.7.m7.1.1.1.cmml" xref="S3.SS1.p3.7.m7.1.1.1"></plus><apply id="S3.SS1.p3.7.m7.1.1.2.cmml" xref="S3.SS1.p3.7.m7.1.1.2"><times id="S3.SS1.p3.7.m7.1.1.2.1.cmml" xref="S3.SS1.p3.7.m7.1.1.2.1"></times><cn type="integer" id="S3.SS1.p3.7.m7.1.1.2.2.cmml" xref="S3.SS1.p3.7.m7.1.1.2.2">2</cn><apply id="S3.SS1.p3.7.m7.1.1.2.3.cmml" xref="S3.SS1.p3.7.m7.1.1.2.3"><csymbol cd="ambiguous" id="S3.SS1.p3.7.m7.1.1.2.3.1.cmml" xref="S3.SS1.p3.7.m7.1.1.2.3">subscript</csymbol><ci id="S3.SS1.p3.7.m7.1.1.2.3.2.cmml" xref="S3.SS1.p3.7.m7.1.1.2.3.2">ğ¾</ci><ci id="S3.SS1.p3.7.m7.1.1.2.3.3.cmml" xref="S3.SS1.p3.7.m7.1.1.2.3.3">ğ»</ci></apply></apply><cn type="integer" id="S3.SS1.p3.7.m7.1.1.3.cmml" xref="S3.SS1.p3.7.m7.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.7.m7.1c">2K_{H}+1</annotation></semantics></math> for hands and <math id="S3.SS1.p3.8.m8.1" class="ltx_Math" alttext="K_{F}+1" display="inline"><semantics id="S3.SS1.p3.8.m8.1a"><mrow id="S3.SS1.p3.8.m8.1.1" xref="S3.SS1.p3.8.m8.1.1.cmml"><msub id="S3.SS1.p3.8.m8.1.1.2" xref="S3.SS1.p3.8.m8.1.1.2.cmml"><mi id="S3.SS1.p3.8.m8.1.1.2.2" xref="S3.SS1.p3.8.m8.1.1.2.2.cmml">K</mi><mi id="S3.SS1.p3.8.m8.1.1.2.3" xref="S3.SS1.p3.8.m8.1.1.2.3.cmml">F</mi></msub><mo id="S3.SS1.p3.8.m8.1.1.1" xref="S3.SS1.p3.8.m8.1.1.1.cmml">+</mo><mn id="S3.SS1.p3.8.m8.1.1.3" xref="S3.SS1.p3.8.m8.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.8.m8.1b"><apply id="S3.SS1.p3.8.m8.1.1.cmml" xref="S3.SS1.p3.8.m8.1.1"><plus id="S3.SS1.p3.8.m8.1.1.1.cmml" xref="S3.SS1.p3.8.m8.1.1.1"></plus><apply id="S3.SS1.p3.8.m8.1.1.2.cmml" xref="S3.SS1.p3.8.m8.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.8.m8.1.1.2.1.cmml" xref="S3.SS1.p3.8.m8.1.1.2">subscript</csymbol><ci id="S3.SS1.p3.8.m8.1.1.2.2.cmml" xref="S3.SS1.p3.8.m8.1.1.2.2">ğ¾</ci><ci id="S3.SS1.p3.8.m8.1.1.2.3.cmml" xref="S3.SS1.p3.8.m8.1.1.2.3">ğ¹</ci></apply><cn type="integer" id="S3.SS1.p3.8.m8.1.1.3.cmml" xref="S3.SS1.p3.8.m8.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.8.m8.1c">K_{F}+1</annotation></semantics></math> for faces.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.4" class="ltx_p"><span id="S3.SS1.p4.4.1" class="ltx_text ltx_font_bold">Regression.</span> In a third step, a class-specific regression is applied to estimate body, hand and face poses in 2D and 3D. First, for each class of each part, we define offline the â€˜anchor-posesâ€™, computed as the average 2D and 3D poses over all elements in the corresponding cluster. After fitting all the 2D anchor-poses into each of the candidate boxes, we perform class-specific regressions to deform these anchor-poses and match the actual 2D and 3D pose in each box. This operation is carried out for the 3 types of parts, obtaining <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="5\times J_{B}\times K_{B}" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><mrow id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml"><mn id="S3.SS1.p4.1.m1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p4.1.m1.1.1.1" xref="S3.SS1.p4.1.m1.1.1.1.cmml">Ã—</mo><msub id="S3.SS1.p4.1.m1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.3.cmml"><mi id="S3.SS1.p4.1.m1.1.1.3.2" xref="S3.SS1.p4.1.m1.1.1.3.2.cmml">J</mi><mi id="S3.SS1.p4.1.m1.1.1.3.3" xref="S3.SS1.p4.1.m1.1.1.3.3.cmml">B</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p4.1.m1.1.1.1a" xref="S3.SS1.p4.1.m1.1.1.1.cmml">Ã—</mo><msub id="S3.SS1.p4.1.m1.1.1.4" xref="S3.SS1.p4.1.m1.1.1.4.cmml"><mi id="S3.SS1.p4.1.m1.1.1.4.2" xref="S3.SS1.p4.1.m1.1.1.4.2.cmml">K</mi><mi id="S3.SS1.p4.1.m1.1.1.4.3" xref="S3.SS1.p4.1.m1.1.1.4.3.cmml">B</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><apply id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"><times id="S3.SS1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1.1"></times><cn type="integer" id="S3.SS1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.2">5</cn><apply id="S3.SS1.p4.1.m1.1.1.3.cmml" xref="S3.SS1.p4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.1.3.1.cmml" xref="S3.SS1.p4.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS1.p4.1.m1.1.1.3.2.cmml" xref="S3.SS1.p4.1.m1.1.1.3.2">ğ½</ci><ci id="S3.SS1.p4.1.m1.1.1.3.3.cmml" xref="S3.SS1.p4.1.m1.1.1.3.3">ğµ</ci></apply><apply id="S3.SS1.p4.1.m1.1.1.4.cmml" xref="S3.SS1.p4.1.m1.1.1.4"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.1.4.1.cmml" xref="S3.SS1.p4.1.m1.1.1.4">subscript</csymbol><ci id="S3.SS1.p4.1.m1.1.1.4.2.cmml" xref="S3.SS1.p4.1.m1.1.1.4.2">ğ¾</ci><ci id="S3.SS1.p4.1.m1.1.1.4.3.cmml" xref="S3.SS1.p4.1.m1.1.1.4.3">ğµ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">5\times J_{B}\times K_{B}</annotation></semantics></math> outputs for the body part, <math id="S3.SS1.p4.2.m2.1" class="ltx_Math" alttext="5\times 2\times J_{H}\times K_{H}" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><mrow id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml"><mn id="S3.SS1.p4.2.m2.1.1.2" xref="S3.SS1.p4.2.m2.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p4.2.m2.1.1.1" xref="S3.SS1.p4.2.m2.1.1.1.cmml">Ã—</mo><mn id="S3.SS1.p4.2.m2.1.1.3" xref="S3.SS1.p4.2.m2.1.1.3.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p4.2.m2.1.1.1a" xref="S3.SS1.p4.2.m2.1.1.1.cmml">Ã—</mo><msub id="S3.SS1.p4.2.m2.1.1.4" xref="S3.SS1.p4.2.m2.1.1.4.cmml"><mi id="S3.SS1.p4.2.m2.1.1.4.2" xref="S3.SS1.p4.2.m2.1.1.4.2.cmml">J</mi><mi id="S3.SS1.p4.2.m2.1.1.4.3" xref="S3.SS1.p4.2.m2.1.1.4.3.cmml">H</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p4.2.m2.1.1.1b" xref="S3.SS1.p4.2.m2.1.1.1.cmml">Ã—</mo><msub id="S3.SS1.p4.2.m2.1.1.5" xref="S3.SS1.p4.2.m2.1.1.5.cmml"><mi id="S3.SS1.p4.2.m2.1.1.5.2" xref="S3.SS1.p4.2.m2.1.1.5.2.cmml">K</mi><mi id="S3.SS1.p4.2.m2.1.1.5.3" xref="S3.SS1.p4.2.m2.1.1.5.3.cmml">H</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><apply id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1"><times id="S3.SS1.p4.2.m2.1.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1.1"></times><cn type="integer" id="S3.SS1.p4.2.m2.1.1.2.cmml" xref="S3.SS1.p4.2.m2.1.1.2">5</cn><cn type="integer" id="S3.SS1.p4.2.m2.1.1.3.cmml" xref="S3.SS1.p4.2.m2.1.1.3">2</cn><apply id="S3.SS1.p4.2.m2.1.1.4.cmml" xref="S3.SS1.p4.2.m2.1.1.4"><csymbol cd="ambiguous" id="S3.SS1.p4.2.m2.1.1.4.1.cmml" xref="S3.SS1.p4.2.m2.1.1.4">subscript</csymbol><ci id="S3.SS1.p4.2.m2.1.1.4.2.cmml" xref="S3.SS1.p4.2.m2.1.1.4.2">ğ½</ci><ci id="S3.SS1.p4.2.m2.1.1.4.3.cmml" xref="S3.SS1.p4.2.m2.1.1.4.3">ğ»</ci></apply><apply id="S3.SS1.p4.2.m2.1.1.5.cmml" xref="S3.SS1.p4.2.m2.1.1.5"><csymbol cd="ambiguous" id="S3.SS1.p4.2.m2.1.1.5.1.cmml" xref="S3.SS1.p4.2.m2.1.1.5">subscript</csymbol><ci id="S3.SS1.p4.2.m2.1.1.5.2.cmml" xref="S3.SS1.p4.2.m2.1.1.5.2">ğ¾</ci><ci id="S3.SS1.p4.2.m2.1.1.5.3.cmml" xref="S3.SS1.p4.2.m2.1.1.5.3">ğ»</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">5\times 2\times J_{H}\times K_{H}</annotation></semantics></math> for the hands and <math id="S3.SS1.p4.3.m3.1" class="ltx_Math" alttext="5\times J_{F}\times K_{F}" display="inline"><semantics id="S3.SS1.p4.3.m3.1a"><mrow id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml"><mn id="S3.SS1.p4.3.m3.1.1.2" xref="S3.SS1.p4.3.m3.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p4.3.m3.1.1.1" xref="S3.SS1.p4.3.m3.1.1.1.cmml">Ã—</mo><msub id="S3.SS1.p4.3.m3.1.1.3" xref="S3.SS1.p4.3.m3.1.1.3.cmml"><mi id="S3.SS1.p4.3.m3.1.1.3.2" xref="S3.SS1.p4.3.m3.1.1.3.2.cmml">J</mi><mi id="S3.SS1.p4.3.m3.1.1.3.3" xref="S3.SS1.p4.3.m3.1.1.3.3.cmml">F</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p4.3.m3.1.1.1a" xref="S3.SS1.p4.3.m3.1.1.1.cmml">Ã—</mo><msub id="S3.SS1.p4.3.m3.1.1.4" xref="S3.SS1.p4.3.m3.1.1.4.cmml"><mi id="S3.SS1.p4.3.m3.1.1.4.2" xref="S3.SS1.p4.3.m3.1.1.4.2.cmml">K</mi><mi id="S3.SS1.p4.3.m3.1.1.4.3" xref="S3.SS1.p4.3.m3.1.1.4.3.cmml">F</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><apply id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1"><times id="S3.SS1.p4.3.m3.1.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1.1"></times><cn type="integer" id="S3.SS1.p4.3.m3.1.1.2.cmml" xref="S3.SS1.p4.3.m3.1.1.2">5</cn><apply id="S3.SS1.p4.3.m3.1.1.3.cmml" xref="S3.SS1.p4.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p4.3.m3.1.1.3.1.cmml" xref="S3.SS1.p4.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS1.p4.3.m3.1.1.3.2.cmml" xref="S3.SS1.p4.3.m3.1.1.3.2">ğ½</ci><ci id="S3.SS1.p4.3.m3.1.1.3.3.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3">ğ¹</ci></apply><apply id="S3.SS1.p4.3.m3.1.1.4.cmml" xref="S3.SS1.p4.3.m3.1.1.4"><csymbol cd="ambiguous" id="S3.SS1.p4.3.m3.1.1.4.1.cmml" xref="S3.SS1.p4.3.m3.1.1.4">subscript</csymbol><ci id="S3.SS1.p4.3.m3.1.1.4.2.cmml" xref="S3.SS1.p4.3.m3.1.1.4.2">ğ¾</ci><ci id="S3.SS1.p4.3.m3.1.1.4.3.cmml" xref="S3.SS1.p4.3.m3.1.1.4.3">ğ¹</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">5\times J_{F}\times K_{F}</annotation></semantics></math> for the face. The number <math id="S3.SS1.p4.4.m4.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S3.SS1.p4.4.m4.1a"><mn id="S3.SS1.p4.4.m4.1.1" xref="S3.SS1.p4.4.m4.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m4.1b"><cn type="integer" id="S3.SS1.p4.4.m4.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m4.1c">5</annotation></semantics></math> corresponds to the number of dimensions, <em id="S3.SS1.p4.4.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS1.p4.4.3" class="ltx_text"></span>, 2D+3D.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para ltx_noindent">
<p id="S3.SS1.p5.1" class="ltx_p"><span id="S3.SS1.p5.1.1" class="ltx_text ltx_font_bold">Postprocessing.</span>
For each body, hand or face, multiple proposals can overlap and produce valid predictions. As inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, these pose candidates are combined, taking into account their 2D overlap, 3D similarity and classification scores.
To obtain whole-body poses from the independent part detections produced by our network, we simply attach a hand to a body if their respective wrist estimations are close enough in 2D, and similarly for the face with the head body keypoint.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Distillation of part experts</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.2" class="ltx_p">Even if in-the-wild datasets with 3D pose annotations have been produced for bodies, hands and faces separately, there exists no dataset covering the whole-body at once.
One possibility is to employ a union of these datasets to train our whole-body model. Since the datasets specifically designed for pose estimation of one part do not contain annotations for the others, <em id="S3.SS2.p1.2.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.SS2.p1.2.2" class="ltx_text"></span> body datasets do not have hand and face annotations and vice-versa, unannotated parts are therefore considered as negatives for their true classes in our detection architecture. In practice, this deteriorates the detectorâ€™s ability to detect these parts and leads to worse overall performances (<math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mo id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><csymbol cd="latexml" id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\sim</annotation></semantics></math>10% drop for hands and faces, and <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mo id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><csymbol cd="latexml" id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\sim</annotation></semantics></math>2% for bodies). To leverage the multiple part-specific datasets, we therefore propose to train independent experts for each part, namely body, hand and face experts, and distill their knowledge into our whole-body pose
network designed to perform the three tasks jointly.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.6" class="ltx_p"><span id="S3.SS2.p2.6.1" class="ltx_text ltx_font_bold">Part experts.</span> To ease the distillation of the knowledge,
we select our 3 experts to match the structure of the classification-regression branches of our whole-body pose estimation architecture and consider the same anchor poses as for the individual tasks.
We therefore selected the Localization-Classification-Regression network from LCR-Net++Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> as body expert and estimate <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="J_{B}=13" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><msub id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2.2" xref="S3.SS2.p2.1.m1.1.1.2.2.cmml">J</mi><mi id="S3.SS2.p2.1.m1.1.1.2.3" xref="S3.SS2.p2.1.m1.1.1.2.3.cmml">B</mi></msub><mo id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">13</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><eq id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></eq><apply id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.2.1.cmml" xref="S3.SS2.p2.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2.2">ğ½</ci><ci id="S3.SS2.p2.1.m1.1.1.2.3.cmml" xref="S3.SS2.p2.1.m1.1.1.2.3">ğµ</ci></apply><cn type="integer" id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">13</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">J_{B}=13</annotation></semantics></math> body joints with <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="K_{B}=10" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><msub id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2.2" xref="S3.SS2.p2.2.m2.1.1.2.2.cmml">K</mi><mi id="S3.SS2.p2.2.m2.1.1.2.3" xref="S3.SS2.p2.2.m2.1.1.2.3.cmml">B</mi></msub><mo id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><eq id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"></eq><apply id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.2.1.cmml" xref="S3.SS2.p2.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.2.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2.2">ğ¾</ci><ci id="S3.SS2.p2.2.m2.1.1.2.3.cmml" xref="S3.SS2.p2.2.m2.1.1.2.3">ğµ</ci></apply><cn type="integer" id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">K_{B}=10</annotation></semantics></math> classes. We also used the hand detection version of this architectureÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, replacing the <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="K_{B}" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><msub id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">K</mi><mi id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">ğ¾</ci><ci id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3">ğµ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">K_{B}</annotation></semantics></math> body pose classes by <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="K_{H}=5" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mrow id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><msub id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml"><mi id="S3.SS2.p2.4.m4.1.1.2.2" xref="S3.SS2.p2.4.m4.1.1.2.2.cmml">K</mi><mi id="S3.SS2.p2.4.m4.1.1.2.3" xref="S3.SS2.p2.4.m4.1.1.2.3.cmml">H</mi></msub><mo id="S3.SS2.p2.4.m4.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.cmml">=</mo><mn id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><eq id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1"></eq><apply id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.2.1.cmml" xref="S3.SS2.p2.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.2.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2.2">ğ¾</ci><ci id="S3.SS2.p2.4.m4.1.1.2.3.cmml" xref="S3.SS2.p2.4.m4.1.1.2.3">ğ»</ci></apply><cn type="integer" id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">K_{H}=5</annotation></semantics></math> hand anchor-poses for each side and using the standard number of <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="J_{H}=21" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mrow id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><msub id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml"><mi id="S3.SS2.p2.5.m5.1.1.2.2" xref="S3.SS2.p2.5.m5.1.1.2.2.cmml">J</mi><mi id="S3.SS2.p2.5.m5.1.1.2.3" xref="S3.SS2.p2.5.m5.1.1.2.3.cmml">H</mi></msub><mo id="S3.SS2.p2.5.m5.1.1.1" xref="S3.SS2.p2.5.m5.1.1.1.cmml">=</mo><mn id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml">21</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><eq id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1.1"></eq><apply id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.2.1.cmml" xref="S3.SS2.p2.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.2.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2.2">ğ½</ci><ci id="S3.SS2.p2.5.m5.1.1.2.3.cmml" xref="S3.SS2.p2.5.m5.1.1.2.3">ğ»</ci></apply><cn type="integer" id="S3.SS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3">21</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">J_{H}=21</annotation></semantics></math> hand joints: 1 keypoint for the wrist plus 4 for each finger. Finally, to obtain our face expert, we adapted the same architecture to detect 2D-3D facial landmarks. We used the 84 landmarks defined in the 3D Face Tracking Menpo benchmarkÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> that include eyes, eyebrows, nose, lips and facial contours. We defined <math id="S3.SS2.p2.6.m6.1" class="ltx_Math" alttext="K_{F}=10" display="inline"><semantics id="S3.SS2.p2.6.m6.1a"><mrow id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml"><msub id="S3.SS2.p2.6.m6.1.1.2" xref="S3.SS2.p2.6.m6.1.1.2.cmml"><mi id="S3.SS2.p2.6.m6.1.1.2.2" xref="S3.SS2.p2.6.m6.1.1.2.2.cmml">K</mi><mi id="S3.SS2.p2.6.m6.1.1.2.3" xref="S3.SS2.p2.6.m6.1.1.2.3.cmml">F</mi></msub><mo id="S3.SS2.p2.6.m6.1.1.1" xref="S3.SS2.p2.6.m6.1.1.1.cmml">=</mo><mn id="S3.SS2.p2.6.m6.1.1.3" xref="S3.SS2.p2.6.m6.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><apply id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1"><eq id="S3.SS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1.1"></eq><apply id="S3.SS2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m6.1.1.2.1.cmml" xref="S3.SS2.p2.6.m6.1.1.2">subscript</csymbol><ci id="S3.SS2.p2.6.m6.1.1.2.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2.2">ğ¾</ci><ci id="S3.SS2.p2.6.m6.1.1.2.3.cmml" xref="S3.SS2.p2.6.m6.1.1.2.3">ğ¹</ci></apply><cn type="integer" id="S3.SS2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">K_{F}=10</annotation></semantics></math> anchor-poses by applying K-means on all faces from the training set.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.9" class="ltx_p"><span id="S3.SS2.p3.9.1" class="ltx_text ltx_font_bold">Training via distillation.</span>
We propose to distill the knowledge of our three part experts to our whole-body pose detection model.
Let <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{B}" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">â„¬</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">â„¬</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\mathcal{B}</annotation></semantics></math>, <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="\mathcal{H}" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">â„‹</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><ci id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">â„‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">\mathcal{H}</annotation></semantics></math> and <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="\mathcal{F}" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml">â„±</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><ci id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">â„±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">\mathcal{F}</annotation></semantics></math> be the training datasets used for the three individuals tasks, <em id="S3.SS2.p3.9.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS2.p3.9.3" class="ltx_text"></span>, body, hand, and face pose detection, respectively.
They are associated with ground-truth (2D and 3D) pose annotations for bodies <math id="S3.SS2.p3.4.m4.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S3.SS2.p3.4.m4.1a"><mi id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><ci id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">b</annotation></semantics></math>, hands <math id="S3.SS2.p3.5.m5.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S3.SS2.p3.5.m5.1a"><mi id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><ci id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1">â„</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">h</annotation></semantics></math> and faces <math id="S3.SS2.p3.6.m6.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S3.SS2.p3.6.m6.1a"><mi id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b"><ci id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">f</annotation></semantics></math>, respectively.
In other words, the body expert is for instance trained on <math id="S3.SS2.p3.7.m7.2" class="ltx_Math" alttext="\mathcal{B}=\{I_{i},b_{i}\}_{i}" display="inline"><semantics id="S3.SS2.p3.7.m7.2a"><mrow id="S3.SS2.p3.7.m7.2.2" xref="S3.SS2.p3.7.m7.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.7.m7.2.2.4" xref="S3.SS2.p3.7.m7.2.2.4.cmml">â„¬</mi><mo id="S3.SS2.p3.7.m7.2.2.3" xref="S3.SS2.p3.7.m7.2.2.3.cmml">=</mo><msub id="S3.SS2.p3.7.m7.2.2.2" xref="S3.SS2.p3.7.m7.2.2.2.cmml"><mrow id="S3.SS2.p3.7.m7.2.2.2.2.2" xref="S3.SS2.p3.7.m7.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS2.p3.7.m7.2.2.2.2.2.3" xref="S3.SS2.p3.7.m7.2.2.2.2.3.cmml">{</mo><msub id="S3.SS2.p3.7.m7.1.1.1.1.1.1" xref="S3.SS2.p3.7.m7.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p3.7.m7.1.1.1.1.1.1.2" xref="S3.SS2.p3.7.m7.1.1.1.1.1.1.2.cmml">I</mi><mi id="S3.SS2.p3.7.m7.1.1.1.1.1.1.3" xref="S3.SS2.p3.7.m7.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.p3.7.m7.2.2.2.2.2.4" xref="S3.SS2.p3.7.m7.2.2.2.2.3.cmml">,</mo><msub id="S3.SS2.p3.7.m7.2.2.2.2.2.2" xref="S3.SS2.p3.7.m7.2.2.2.2.2.2.cmml"><mi id="S3.SS2.p3.7.m7.2.2.2.2.2.2.2" xref="S3.SS2.p3.7.m7.2.2.2.2.2.2.2.cmml">b</mi><mi id="S3.SS2.p3.7.m7.2.2.2.2.2.2.3" xref="S3.SS2.p3.7.m7.2.2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS2.p3.7.m7.2.2.2.2.2.5" xref="S3.SS2.p3.7.m7.2.2.2.2.3.cmml">}</mo></mrow><mi id="S3.SS2.p3.7.m7.2.2.2.4" xref="S3.SS2.p3.7.m7.2.2.2.4.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.7.m7.2b"><apply id="S3.SS2.p3.7.m7.2.2.cmml" xref="S3.SS2.p3.7.m7.2.2"><eq id="S3.SS2.p3.7.m7.2.2.3.cmml" xref="S3.SS2.p3.7.m7.2.2.3"></eq><ci id="S3.SS2.p3.7.m7.2.2.4.cmml" xref="S3.SS2.p3.7.m7.2.2.4">â„¬</ci><apply id="S3.SS2.p3.7.m7.2.2.2.cmml" xref="S3.SS2.p3.7.m7.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.2.2.2.3.cmml" xref="S3.SS2.p3.7.m7.2.2.2">subscript</csymbol><set id="S3.SS2.p3.7.m7.2.2.2.2.3.cmml" xref="S3.SS2.p3.7.m7.2.2.2.2.2"><apply id="S3.SS2.p3.7.m7.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.7.m7.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.1.1.2">ğ¼</ci><ci id="S3.SS2.p3.7.m7.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.SS2.p3.7.m7.2.2.2.2.2.2.cmml" xref="S3.SS2.p3.7.m7.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.2.2.2.2.2.2.1.cmml" xref="S3.SS2.p3.7.m7.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p3.7.m7.2.2.2.2.2.2.2.cmml" xref="S3.SS2.p3.7.m7.2.2.2.2.2.2.2">ğ‘</ci><ci id="S3.SS2.p3.7.m7.2.2.2.2.2.2.3.cmml" xref="S3.SS2.p3.7.m7.2.2.2.2.2.2.3">ğ‘–</ci></apply></set><ci id="S3.SS2.p3.7.m7.2.2.2.4.cmml" xref="S3.SS2.p3.7.m7.2.2.2.4">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.7.m7.2c">\mathcal{B}=\{I_{i},b_{i}\}_{i}</annotation></semantics></math>, <em id="S3.SS2.p3.9.4" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS2.p3.9.5" class="ltx_text"></span>, a set of images <math id="S3.SS2.p3.8.m8.1" class="ltx_Math" alttext="I_{i}" display="inline"><semantics id="S3.SS2.p3.8.m8.1a"><msub id="S3.SS2.p3.8.m8.1.1" xref="S3.SS2.p3.8.m8.1.1.cmml"><mi id="S3.SS2.p3.8.m8.1.1.2" xref="S3.SS2.p3.8.m8.1.1.2.cmml">I</mi><mi id="S3.SS2.p3.8.m8.1.1.3" xref="S3.SS2.p3.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.8.m8.1b"><apply id="S3.SS2.p3.8.m8.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.8.m8.1.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1">subscript</csymbol><ci id="S3.SS2.p3.8.m8.1.1.2.cmml" xref="S3.SS2.p3.8.m8.1.1.2">ğ¼</ci><ci id="S3.SS2.p3.8.m8.1.1.3.cmml" xref="S3.SS2.p3.8.m8.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.8.m8.1c">I_{i}</annotation></semantics></math> with body ground-truth annotations <math id="S3.SS2.p3.9.m9.1" class="ltx_Math" alttext="b_{i}" display="inline"><semantics id="S3.SS2.p3.9.m9.1a"><msub id="S3.SS2.p3.9.m9.1.1" xref="S3.SS2.p3.9.m9.1.1.cmml"><mi id="S3.SS2.p3.9.m9.1.1.2" xref="S3.SS2.p3.9.m9.1.1.2.cmml">b</mi><mi id="S3.SS2.p3.9.m9.1.1.3" xref="S3.SS2.p3.9.m9.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.9.m9.1b"><apply id="S3.SS2.p3.9.m9.1.1.cmml" xref="S3.SS2.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.9.m9.1.1.1.cmml" xref="S3.SS2.p3.9.m9.1.1">subscript</csymbol><ci id="S3.SS2.p3.9.m9.1.1.2.cmml" xref="S3.SS2.p3.9.m9.1.1.2">ğ‘</ci><ci id="S3.SS2.p3.9.m9.1.1.3.cmml" xref="S3.SS2.p3.9.m9.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.9.m9.1c">b_{i}</annotation></semantics></math>, and similarly for the other parts.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.5" class="ltx_p">To train our network, we need ground-truth annotations <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="w" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><mi id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><ci id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">ğ‘¤</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">w</annotation></semantics></math> for the whole body.
We propose to leverage the detections made by the experts in order to augment the annotations of the part-specific datasets.
We denote by <math id="S3.SS2.p4.2.m2.1" class="ltx_Math" alttext="\hat{b}_{i}" display="inline"><semantics id="S3.SS2.p4.2.m2.1a"><msub id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml"><mover accent="true" id="S3.SS2.p4.2.m2.1.1.2" xref="S3.SS2.p4.2.m2.1.1.2.cmml"><mi id="S3.SS2.p4.2.m2.1.1.2.2" xref="S3.SS2.p4.2.m2.1.1.2.2.cmml">b</mi><mo id="S3.SS2.p4.2.m2.1.1.2.1" xref="S3.SS2.p4.2.m2.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS2.p4.2.m2.1.1.3" xref="S3.SS2.p4.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><apply id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.2.m2.1.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">subscript</csymbol><apply id="S3.SS2.p4.2.m2.1.1.2.cmml" xref="S3.SS2.p4.2.m2.1.1.2"><ci id="S3.SS2.p4.2.m2.1.1.2.1.cmml" xref="S3.SS2.p4.2.m2.1.1.2.1">^</ci><ci id="S3.SS2.p4.2.m2.1.1.2.2.cmml" xref="S3.SS2.p4.2.m2.1.1.2.2">ğ‘</ci></apply><ci id="S3.SS2.p4.2.m2.1.1.3.cmml" xref="S3.SS2.p4.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">\hat{b}_{i}</annotation></semantics></math>, <math id="S3.SS2.p4.3.m3.1" class="ltx_Math" alttext="\hat{h}_{i}" display="inline"><semantics id="S3.SS2.p4.3.m3.1a"><msub id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml"><mover accent="true" id="S3.SS2.p4.3.m3.1.1.2" xref="S3.SS2.p4.3.m3.1.1.2.cmml"><mi id="S3.SS2.p4.3.m3.1.1.2.2" xref="S3.SS2.p4.3.m3.1.1.2.2.cmml">h</mi><mo id="S3.SS2.p4.3.m3.1.1.2.1" xref="S3.SS2.p4.3.m3.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS2.p4.3.m3.1.1.3" xref="S3.SS2.p4.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><apply id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.3.m3.1.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1">subscript</csymbol><apply id="S3.SS2.p4.3.m3.1.1.2.cmml" xref="S3.SS2.p4.3.m3.1.1.2"><ci id="S3.SS2.p4.3.m3.1.1.2.1.cmml" xref="S3.SS2.p4.3.m3.1.1.2.1">^</ci><ci id="S3.SS2.p4.3.m3.1.1.2.2.cmml" xref="S3.SS2.p4.3.m3.1.1.2.2">â„</ci></apply><ci id="S3.SS2.p4.3.m3.1.1.3.cmml" xref="S3.SS2.p4.3.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">\hat{h}_{i}</annotation></semantics></math> and <math id="S3.SS2.p4.4.m4.1" class="ltx_Math" alttext="\hat{f}_{i}" display="inline"><semantics id="S3.SS2.p4.4.m4.1a"><msub id="S3.SS2.p4.4.m4.1.1" xref="S3.SS2.p4.4.m4.1.1.cmml"><mover accent="true" id="S3.SS2.p4.4.m4.1.1.2" xref="S3.SS2.p4.4.m4.1.1.2.cmml"><mi id="S3.SS2.p4.4.m4.1.1.2.2" xref="S3.SS2.p4.4.m4.1.1.2.2.cmml">f</mi><mo id="S3.SS2.p4.4.m4.1.1.2.1" xref="S3.SS2.p4.4.m4.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS2.p4.4.m4.1.1.3" xref="S3.SS2.p4.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m4.1b"><apply id="S3.SS2.p4.4.m4.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.4.m4.1.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1">subscript</csymbol><apply id="S3.SS2.p4.4.m4.1.1.2.cmml" xref="S3.SS2.p4.4.m4.1.1.2"><ci id="S3.SS2.p4.4.m4.1.1.2.1.cmml" xref="S3.SS2.p4.4.m4.1.1.2.1">^</ci><ci id="S3.SS2.p4.4.m4.1.1.2.2.cmml" xref="S3.SS2.p4.4.m4.1.1.2.2">ğ‘“</ci></apply><ci id="S3.SS2.p4.4.m4.1.1.3.cmml" xref="S3.SS2.p4.4.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m4.1c">\hat{f}_{i}</annotation></semantics></math> the detections obtained when processing the images <math id="S3.SS2.p4.5.m5.1" class="ltx_Math" alttext="I_{i}" display="inline"><semantics id="S3.SS2.p4.5.m5.1a"><msub id="S3.SS2.p4.5.m5.1.1" xref="S3.SS2.p4.5.m5.1.1.cmml"><mi id="S3.SS2.p4.5.m5.1.1.2" xref="S3.SS2.p4.5.m5.1.1.2.cmml">I</mi><mi id="S3.SS2.p4.5.m5.1.1.3" xref="S3.SS2.p4.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.5.m5.1b"><apply id="S3.SS2.p4.5.m5.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.5.m5.1.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p4.5.m5.1.1.2.cmml" xref="S3.SS2.p4.5.m5.1.1.2">ğ¼</ci><ci id="S3.SS2.p4.5.m5.1.1.3.cmml" xref="S3.SS2.p4.5.m5.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.5.m5.1c">I_{i}</annotation></semantics></math> with our expert for body, hands and face respectively.
We train our DOPE network on:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.14" class="ltx_Math" alttext="\mathcal{W}_{DOPE}=\{I_{i},w_{i}\}_{i\in\mathcal{B}\cup\mathcal{H}\cup\mathcal{F}}\mbox{~{}~{}~{}~{}where~{}~{}}w_{i}=\left\{\begin{array}[]{ll}\{b_{i},\hat{h}_{i},\hat{f}_{i}\}&amp;\mbox{if }i\in\mathcal{B}~{},\\
\{\hat{b}_{i},h_{i},\hat{f}_{i}\}&amp;\mbox{if }i\in\mathcal{H}~{},\\
\{\hat{b}_{i},\hat{h}_{i},f_{i}\}&amp;\mbox{if }i\in\mathcal{F}~{}.\\
\end{array}\right." display="block"><semantics id="S3.E1.m1.14a"><mrow id="S3.E1.m1.14.14" xref="S3.E1.m1.14.14.cmml"><msub id="S3.E1.m1.14.14.4" xref="S3.E1.m1.14.14.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.14.14.4.2" xref="S3.E1.m1.14.14.4.2.cmml">ğ’²</mi><mrow id="S3.E1.m1.14.14.4.3" xref="S3.E1.m1.14.14.4.3.cmml"><mi id="S3.E1.m1.14.14.4.3.2" xref="S3.E1.m1.14.14.4.3.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.14.14.4.3.1" xref="S3.E1.m1.14.14.4.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.14.14.4.3.3" xref="S3.E1.m1.14.14.4.3.3.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.14.14.4.3.1a" xref="S3.E1.m1.14.14.4.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.14.14.4.3.4" xref="S3.E1.m1.14.14.4.3.4.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.14.14.4.3.1b" xref="S3.E1.m1.14.14.4.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.14.14.4.3.5" xref="S3.E1.m1.14.14.4.3.5.cmml">E</mi></mrow></msub><mo id="S3.E1.m1.14.14.5" xref="S3.E1.m1.14.14.5.cmml">=</mo><mrow id="S3.E1.m1.14.14.2" xref="S3.E1.m1.14.14.2.cmml"><msub id="S3.E1.m1.14.14.2.2" xref="S3.E1.m1.14.14.2.2.cmml"><mrow id="S3.E1.m1.14.14.2.2.2.2" xref="S3.E1.m1.14.14.2.2.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.14.14.2.2.2.2.3" xref="S3.E1.m1.14.14.2.2.2.3.cmml">{</mo><msub id="S3.E1.m1.13.13.1.1.1.1.1" xref="S3.E1.m1.13.13.1.1.1.1.1.cmml"><mi id="S3.E1.m1.13.13.1.1.1.1.1.2" xref="S3.E1.m1.13.13.1.1.1.1.1.2.cmml">I</mi><mi id="S3.E1.m1.13.13.1.1.1.1.1.3" xref="S3.E1.m1.13.13.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E1.m1.14.14.2.2.2.2.4" xref="S3.E1.m1.14.14.2.2.2.3.cmml">,</mo><msub id="S3.E1.m1.14.14.2.2.2.2.2" xref="S3.E1.m1.14.14.2.2.2.2.2.cmml"><mi id="S3.E1.m1.14.14.2.2.2.2.2.2" xref="S3.E1.m1.14.14.2.2.2.2.2.2.cmml">w</mi><mi id="S3.E1.m1.14.14.2.2.2.2.2.3" xref="S3.E1.m1.14.14.2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E1.m1.14.14.2.2.2.2.5" xref="S3.E1.m1.14.14.2.2.2.3.cmml">}</mo></mrow><mrow id="S3.E1.m1.14.14.2.2.4" xref="S3.E1.m1.14.14.2.2.4.cmml"><mi id="S3.E1.m1.14.14.2.2.4.2" xref="S3.E1.m1.14.14.2.2.4.2.cmml">i</mi><mo id="S3.E1.m1.14.14.2.2.4.1" xref="S3.E1.m1.14.14.2.2.4.1.cmml">âˆˆ</mo><mrow id="S3.E1.m1.14.14.2.2.4.3" xref="S3.E1.m1.14.14.2.2.4.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.14.14.2.2.4.3.2" xref="S3.E1.m1.14.14.2.2.4.3.2.cmml">â„¬</mi><mo id="S3.E1.m1.14.14.2.2.4.3.1" xref="S3.E1.m1.14.14.2.2.4.3.1.cmml">âˆª</mo><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.14.14.2.2.4.3.3" xref="S3.E1.m1.14.14.2.2.4.3.3.cmml">â„‹</mi><mo id="S3.E1.m1.14.14.2.2.4.3.1a" xref="S3.E1.m1.14.14.2.2.4.3.1.cmml">âˆª</mo><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.14.14.2.2.4.3.4" xref="S3.E1.m1.14.14.2.2.4.3.4.cmml">â„±</mi></mrow></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.14.14.2.3" xref="S3.E1.m1.14.14.2.3.cmml">â€‹</mo><mtext id="S3.E1.m1.14.14.2.4" xref="S3.E1.m1.14.14.2.4a.cmml">Â whereÂ </mtext><mo lspace="0em" rspace="0em" id="S3.E1.m1.14.14.2.3a" xref="S3.E1.m1.14.14.2.3.cmml">â€‹</mo><msub id="S3.E1.m1.14.14.2.5" xref="S3.E1.m1.14.14.2.5.cmml"><mi id="S3.E1.m1.14.14.2.5.2" xref="S3.E1.m1.14.14.2.5.2.cmml">w</mi><mi id="S3.E1.m1.14.14.2.5.3" xref="S3.E1.m1.14.14.2.5.3.cmml">i</mi></msub></mrow><mo id="S3.E1.m1.14.14.6" xref="S3.E1.m1.14.14.6.cmml">=</mo><mrow id="S3.E1.m1.14.14.7.2" xref="S3.E1.m1.14.14.7.1.cmml"><mo id="S3.E1.m1.14.14.7.2.1" xref="S3.E1.m1.14.14.7.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.E1.m1.12.12" xref="S3.E1.m1.12.12.cmml"><mtr id="S3.E1.m1.12.12a" xref="S3.E1.m1.12.12.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.12.12b" xref="S3.E1.m1.12.12.cmml"><mrow id="S3.E1.m1.3.3.3.3.3.3" xref="S3.E1.m1.3.3.3.3.3.4.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.3.3.3.3.4" xref="S3.E1.m1.3.3.3.3.3.4.cmml">{</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.cmml">b</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E1.m1.3.3.3.3.3.3.5" xref="S3.E1.m1.3.3.3.3.3.4.cmml">,</mo><msub id="S3.E1.m1.2.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.2.cmml"><mover accent="true" id="S3.E1.m1.2.2.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.2.2.cmml"><mi id="S3.E1.m1.2.2.2.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.2.2.2.cmml">h</mi><mo id="S3.E1.m1.2.2.2.2.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1.cmml">^</mo></mover><mi id="S3.E1.m1.2.2.2.2.2.2.2.3" xref="S3.E1.m1.2.2.2.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.E1.m1.3.3.3.3.3.3.6" xref="S3.E1.m1.3.3.3.3.3.4.cmml">,</mo><msub id="S3.E1.m1.3.3.3.3.3.3.3" xref="S3.E1.m1.3.3.3.3.3.3.3.cmml"><mover accent="true" id="S3.E1.m1.3.3.3.3.3.3.3.2" xref="S3.E1.m1.3.3.3.3.3.3.3.2.cmml"><mi id="S3.E1.m1.3.3.3.3.3.3.3.2.2" xref="S3.E1.m1.3.3.3.3.3.3.3.2.2.cmml">f</mi><mo id="S3.E1.m1.3.3.3.3.3.3.3.2.1" xref="S3.E1.m1.3.3.3.3.3.3.3.2.1.cmml">^</mo></mover><mi id="S3.E1.m1.3.3.3.3.3.3.3.3" xref="S3.E1.m1.3.3.3.3.3.3.3.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E1.m1.3.3.3.3.3.3.7" xref="S3.E1.m1.3.3.3.3.3.4.cmml">}</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.12.12c" xref="S3.E1.m1.12.12.cmml"><mrow id="S3.E1.m1.4.4.4.4.1.1" xref="S3.E1.m1.4.4.4.4.1.1.1.cmml"><mrow id="S3.E1.m1.4.4.4.4.1.1.1" xref="S3.E1.m1.4.4.4.4.1.1.1.cmml"><mrow id="S3.E1.m1.4.4.4.4.1.1.1.2" xref="S3.E1.m1.4.4.4.4.1.1.1.2.cmml"><mtext id="S3.E1.m1.4.4.4.4.1.1.1.2.2" xref="S3.E1.m1.4.4.4.4.1.1.1.2.2a.cmml">ifÂ </mtext><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.4.4.1.1.1.2.1" xref="S3.E1.m1.4.4.4.4.1.1.1.2.1.cmml">â€‹</mo><mi id="S3.E1.m1.4.4.4.4.1.1.1.2.3" xref="S3.E1.m1.4.4.4.4.1.1.1.2.3.cmml">i</mi></mrow><mo id="S3.E1.m1.4.4.4.4.1.1.1.1" xref="S3.E1.m1.4.4.4.4.1.1.1.1.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.4.4.4.4.1.1.1.3" xref="S3.E1.m1.4.4.4.4.1.1.1.3.cmml">â„¬</mi></mrow><mo lspace="0.330em" id="S3.E1.m1.4.4.4.4.1.1.2" xref="S3.E1.m1.4.4.4.4.1.1.1.cmml">,</mo></mrow></mtd></mtr><mtr id="S3.E1.m1.12.12d" xref="S3.E1.m1.12.12.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.12.12e" xref="S3.E1.m1.12.12.cmml"><mrow id="S3.E1.m1.7.7.7.3.3.3" xref="S3.E1.m1.7.7.7.3.3.4.cmml"><mo stretchy="false" id="S3.E1.m1.7.7.7.3.3.3.4" xref="S3.E1.m1.7.7.7.3.3.4.cmml">{</mo><msub id="S3.E1.m1.5.5.5.1.1.1.1" xref="S3.E1.m1.5.5.5.1.1.1.1.cmml"><mover accent="true" id="S3.E1.m1.5.5.5.1.1.1.1.2" xref="S3.E1.m1.5.5.5.1.1.1.1.2.cmml"><mi id="S3.E1.m1.5.5.5.1.1.1.1.2.2" xref="S3.E1.m1.5.5.5.1.1.1.1.2.2.cmml">b</mi><mo id="S3.E1.m1.5.5.5.1.1.1.1.2.1" xref="S3.E1.m1.5.5.5.1.1.1.1.2.1.cmml">^</mo></mover><mi id="S3.E1.m1.5.5.5.1.1.1.1.3" xref="S3.E1.m1.5.5.5.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E1.m1.7.7.7.3.3.3.5" xref="S3.E1.m1.7.7.7.3.3.4.cmml">,</mo><msub id="S3.E1.m1.6.6.6.2.2.2.2" xref="S3.E1.m1.6.6.6.2.2.2.2.cmml"><mi id="S3.E1.m1.6.6.6.2.2.2.2.2" xref="S3.E1.m1.6.6.6.2.2.2.2.2.cmml">h</mi><mi id="S3.E1.m1.6.6.6.2.2.2.2.3" xref="S3.E1.m1.6.6.6.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.E1.m1.7.7.7.3.3.3.6" xref="S3.E1.m1.7.7.7.3.3.4.cmml">,</mo><msub id="S3.E1.m1.7.7.7.3.3.3.3" xref="S3.E1.m1.7.7.7.3.3.3.3.cmml"><mover accent="true" id="S3.E1.m1.7.7.7.3.3.3.3.2" xref="S3.E1.m1.7.7.7.3.3.3.3.2.cmml"><mi id="S3.E1.m1.7.7.7.3.3.3.3.2.2" xref="S3.E1.m1.7.7.7.3.3.3.3.2.2.cmml">f</mi><mo id="S3.E1.m1.7.7.7.3.3.3.3.2.1" xref="S3.E1.m1.7.7.7.3.3.3.3.2.1.cmml">^</mo></mover><mi id="S3.E1.m1.7.7.7.3.3.3.3.3" xref="S3.E1.m1.7.7.7.3.3.3.3.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E1.m1.7.7.7.3.3.3.7" xref="S3.E1.m1.7.7.7.3.3.4.cmml">}</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.12.12f" xref="S3.E1.m1.12.12.cmml"><mrow id="S3.E1.m1.8.8.8.4.1.1" xref="S3.E1.m1.8.8.8.4.1.1.1.cmml"><mrow id="S3.E1.m1.8.8.8.4.1.1.1" xref="S3.E1.m1.8.8.8.4.1.1.1.cmml"><mrow id="S3.E1.m1.8.8.8.4.1.1.1.2" xref="S3.E1.m1.8.8.8.4.1.1.1.2.cmml"><mtext id="S3.E1.m1.8.8.8.4.1.1.1.2.2" xref="S3.E1.m1.8.8.8.4.1.1.1.2.2a.cmml">ifÂ </mtext><mo lspace="0em" rspace="0em" id="S3.E1.m1.8.8.8.4.1.1.1.2.1" xref="S3.E1.m1.8.8.8.4.1.1.1.2.1.cmml">â€‹</mo><mi id="S3.E1.m1.8.8.8.4.1.1.1.2.3" xref="S3.E1.m1.8.8.8.4.1.1.1.2.3.cmml">i</mi></mrow><mo id="S3.E1.m1.8.8.8.4.1.1.1.1" xref="S3.E1.m1.8.8.8.4.1.1.1.1.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.8.8.8.4.1.1.1.3" xref="S3.E1.m1.8.8.8.4.1.1.1.3.cmml">â„‹</mi></mrow><mo lspace="0.330em" id="S3.E1.m1.8.8.8.4.1.1.2" xref="S3.E1.m1.8.8.8.4.1.1.1.cmml">,</mo></mrow></mtd></mtr><mtr id="S3.E1.m1.12.12g" xref="S3.E1.m1.12.12.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.12.12h" xref="S3.E1.m1.12.12.cmml"><mrow id="S3.E1.m1.11.11.11.3.3.3" xref="S3.E1.m1.11.11.11.3.3.4.cmml"><mo stretchy="false" id="S3.E1.m1.11.11.11.3.3.3.4" xref="S3.E1.m1.11.11.11.3.3.4.cmml">{</mo><msub id="S3.E1.m1.9.9.9.1.1.1.1" xref="S3.E1.m1.9.9.9.1.1.1.1.cmml"><mover accent="true" id="S3.E1.m1.9.9.9.1.1.1.1.2" xref="S3.E1.m1.9.9.9.1.1.1.1.2.cmml"><mi id="S3.E1.m1.9.9.9.1.1.1.1.2.2" xref="S3.E1.m1.9.9.9.1.1.1.1.2.2.cmml">b</mi><mo id="S3.E1.m1.9.9.9.1.1.1.1.2.1" xref="S3.E1.m1.9.9.9.1.1.1.1.2.1.cmml">^</mo></mover><mi id="S3.E1.m1.9.9.9.1.1.1.1.3" xref="S3.E1.m1.9.9.9.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E1.m1.11.11.11.3.3.3.5" xref="S3.E1.m1.11.11.11.3.3.4.cmml">,</mo><msub id="S3.E1.m1.10.10.10.2.2.2.2" xref="S3.E1.m1.10.10.10.2.2.2.2.cmml"><mover accent="true" id="S3.E1.m1.10.10.10.2.2.2.2.2" xref="S3.E1.m1.10.10.10.2.2.2.2.2.cmml"><mi id="S3.E1.m1.10.10.10.2.2.2.2.2.2" xref="S3.E1.m1.10.10.10.2.2.2.2.2.2.cmml">h</mi><mo id="S3.E1.m1.10.10.10.2.2.2.2.2.1" xref="S3.E1.m1.10.10.10.2.2.2.2.2.1.cmml">^</mo></mover><mi id="S3.E1.m1.10.10.10.2.2.2.2.3" xref="S3.E1.m1.10.10.10.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.E1.m1.11.11.11.3.3.3.6" xref="S3.E1.m1.11.11.11.3.3.4.cmml">,</mo><msub id="S3.E1.m1.11.11.11.3.3.3.3" xref="S3.E1.m1.11.11.11.3.3.3.3.cmml"><mi id="S3.E1.m1.11.11.11.3.3.3.3.2" xref="S3.E1.m1.11.11.11.3.3.3.3.2.cmml">f</mi><mi id="S3.E1.m1.11.11.11.3.3.3.3.3" xref="S3.E1.m1.11.11.11.3.3.3.3.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E1.m1.11.11.11.3.3.3.7" xref="S3.E1.m1.11.11.11.3.3.4.cmml">}</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.12.12i" xref="S3.E1.m1.12.12.cmml"><mrow id="S3.E1.m1.12.12.12.4.1.1" xref="S3.E1.m1.12.12.12.4.1.1.1.cmml"><mrow id="S3.E1.m1.12.12.12.4.1.1.1" xref="S3.E1.m1.12.12.12.4.1.1.1.cmml"><mrow id="S3.E1.m1.12.12.12.4.1.1.1.2" xref="S3.E1.m1.12.12.12.4.1.1.1.2.cmml"><mtext id="S3.E1.m1.12.12.12.4.1.1.1.2.2" xref="S3.E1.m1.12.12.12.4.1.1.1.2.2a.cmml">ifÂ </mtext><mo lspace="0em" rspace="0em" id="S3.E1.m1.12.12.12.4.1.1.1.2.1" xref="S3.E1.m1.12.12.12.4.1.1.1.2.1.cmml">â€‹</mo><mi id="S3.E1.m1.12.12.12.4.1.1.1.2.3" xref="S3.E1.m1.12.12.12.4.1.1.1.2.3.cmml">i</mi></mrow><mo id="S3.E1.m1.12.12.12.4.1.1.1.1" xref="S3.E1.m1.12.12.12.4.1.1.1.1.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.12.12.12.4.1.1.1.3" xref="S3.E1.m1.12.12.12.4.1.1.1.3.cmml">â„±</mi></mrow><mo lspace="0.330em" id="S3.E1.m1.12.12.12.4.1.1.2" xref="S3.E1.m1.12.12.12.4.1.1.1.cmml">.</mo></mrow></mtd></mtr></mtable><mi id="S3.E1.m1.14.14.7.2.2" xref="S3.E1.m1.14.14.7.1.1.cmml"></mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.14b"><apply id="S3.E1.m1.14.14.cmml" xref="S3.E1.m1.14.14"><and id="S3.E1.m1.14.14a.cmml" xref="S3.E1.m1.14.14"></and><apply id="S3.E1.m1.14.14b.cmml" xref="S3.E1.m1.14.14"><eq id="S3.E1.m1.14.14.5.cmml" xref="S3.E1.m1.14.14.5"></eq><apply id="S3.E1.m1.14.14.4.cmml" xref="S3.E1.m1.14.14.4"><csymbol cd="ambiguous" id="S3.E1.m1.14.14.4.1.cmml" xref="S3.E1.m1.14.14.4">subscript</csymbol><ci id="S3.E1.m1.14.14.4.2.cmml" xref="S3.E1.m1.14.14.4.2">ğ’²</ci><apply id="S3.E1.m1.14.14.4.3.cmml" xref="S3.E1.m1.14.14.4.3"><times id="S3.E1.m1.14.14.4.3.1.cmml" xref="S3.E1.m1.14.14.4.3.1"></times><ci id="S3.E1.m1.14.14.4.3.2.cmml" xref="S3.E1.m1.14.14.4.3.2">ğ·</ci><ci id="S3.E1.m1.14.14.4.3.3.cmml" xref="S3.E1.m1.14.14.4.3.3">ğ‘‚</ci><ci id="S3.E1.m1.14.14.4.3.4.cmml" xref="S3.E1.m1.14.14.4.3.4">ğ‘ƒ</ci><ci id="S3.E1.m1.14.14.4.3.5.cmml" xref="S3.E1.m1.14.14.4.3.5">ğ¸</ci></apply></apply><apply id="S3.E1.m1.14.14.2.cmml" xref="S3.E1.m1.14.14.2"><times id="S3.E1.m1.14.14.2.3.cmml" xref="S3.E1.m1.14.14.2.3"></times><apply id="S3.E1.m1.14.14.2.2.cmml" xref="S3.E1.m1.14.14.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.14.14.2.2.3.cmml" xref="S3.E1.m1.14.14.2.2">subscript</csymbol><set id="S3.E1.m1.14.14.2.2.2.3.cmml" xref="S3.E1.m1.14.14.2.2.2.2"><apply id="S3.E1.m1.13.13.1.1.1.1.1.cmml" xref="S3.E1.m1.13.13.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.13.13.1.1.1.1.1.1.cmml" xref="S3.E1.m1.13.13.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.13.13.1.1.1.1.1.2.cmml" xref="S3.E1.m1.13.13.1.1.1.1.1.2">ğ¼</ci><ci id="S3.E1.m1.13.13.1.1.1.1.1.3.cmml" xref="S3.E1.m1.13.13.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.E1.m1.14.14.2.2.2.2.2.cmml" xref="S3.E1.m1.14.14.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.14.14.2.2.2.2.2.1.cmml" xref="S3.E1.m1.14.14.2.2.2.2.2">subscript</csymbol><ci id="S3.E1.m1.14.14.2.2.2.2.2.2.cmml" xref="S3.E1.m1.14.14.2.2.2.2.2.2">ğ‘¤</ci><ci id="S3.E1.m1.14.14.2.2.2.2.2.3.cmml" xref="S3.E1.m1.14.14.2.2.2.2.2.3">ğ‘–</ci></apply></set><apply id="S3.E1.m1.14.14.2.2.4.cmml" xref="S3.E1.m1.14.14.2.2.4"><in id="S3.E1.m1.14.14.2.2.4.1.cmml" xref="S3.E1.m1.14.14.2.2.4.1"></in><ci id="S3.E1.m1.14.14.2.2.4.2.cmml" xref="S3.E1.m1.14.14.2.2.4.2">ğ‘–</ci><apply id="S3.E1.m1.14.14.2.2.4.3.cmml" xref="S3.E1.m1.14.14.2.2.4.3"><union id="S3.E1.m1.14.14.2.2.4.3.1.cmml" xref="S3.E1.m1.14.14.2.2.4.3.1"></union><ci id="S3.E1.m1.14.14.2.2.4.3.2.cmml" xref="S3.E1.m1.14.14.2.2.4.3.2">â„¬</ci><ci id="S3.E1.m1.14.14.2.2.4.3.3.cmml" xref="S3.E1.m1.14.14.2.2.4.3.3">â„‹</ci><ci id="S3.E1.m1.14.14.2.2.4.3.4.cmml" xref="S3.E1.m1.14.14.2.2.4.3.4">â„±</ci></apply></apply></apply><ci id="S3.E1.m1.14.14.2.4a.cmml" xref="S3.E1.m1.14.14.2.4"><mtext id="S3.E1.m1.14.14.2.4.cmml" xref="S3.E1.m1.14.14.2.4">Â whereÂ </mtext></ci><apply id="S3.E1.m1.14.14.2.5.cmml" xref="S3.E1.m1.14.14.2.5"><csymbol cd="ambiguous" id="S3.E1.m1.14.14.2.5.1.cmml" xref="S3.E1.m1.14.14.2.5">subscript</csymbol><ci id="S3.E1.m1.14.14.2.5.2.cmml" xref="S3.E1.m1.14.14.2.5.2">ğ‘¤</ci><ci id="S3.E1.m1.14.14.2.5.3.cmml" xref="S3.E1.m1.14.14.2.5.3">ğ‘–</ci></apply></apply></apply><apply id="S3.E1.m1.14.14c.cmml" xref="S3.E1.m1.14.14"><eq id="S3.E1.m1.14.14.6.cmml" xref="S3.E1.m1.14.14.6"></eq><share href="#S3.E1.m1.14.14.2.cmml" id="S3.E1.m1.14.14d.cmml" xref="S3.E1.m1.14.14"></share><apply id="S3.E1.m1.14.14.7.1.cmml" xref="S3.E1.m1.14.14.7.2"><csymbol cd="latexml" id="S3.E1.m1.14.14.7.1.1.cmml" xref="S3.E1.m1.14.14.7.2.1">cases</csymbol><matrix id="S3.E1.m1.12.12.cmml" xref="S3.E1.m1.12.12"><matrixrow id="S3.E1.m1.12.12a.cmml" xref="S3.E1.m1.12.12"><set id="S3.E1.m1.3.3.3.3.3.4.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><apply id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2">ğ‘</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.E1.m1.2.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2">subscript</csymbol><apply id="S3.E1.m1.2.2.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2"><ci id="S3.E1.m1.2.2.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2.1">^</ci><ci id="S3.E1.m1.2.2.2.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2.2">â„</ci></apply><ci id="S3.E1.m1.2.2.2.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.3">ğ‘–</ci></apply><apply id="S3.E1.m1.3.3.3.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.3.3.3.3.3.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3.3">subscript</csymbol><apply id="S3.E1.m1.3.3.3.3.3.3.3.2.cmml" xref="S3.E1.m1.3.3.3.3.3.3.3.2"><ci id="S3.E1.m1.3.3.3.3.3.3.3.2.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3.3.2.1">^</ci><ci id="S3.E1.m1.3.3.3.3.3.3.3.2.2.cmml" xref="S3.E1.m1.3.3.3.3.3.3.3.2.2">ğ‘“</ci></apply><ci id="S3.E1.m1.3.3.3.3.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3.3.3.3.3">ğ‘–</ci></apply></set><apply id="S3.E1.m1.4.4.4.4.1.1.1.cmml" xref="S3.E1.m1.4.4.4.4.1.1"><in id="S3.E1.m1.4.4.4.4.1.1.1.1.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.1"></in><apply id="S3.E1.m1.4.4.4.4.1.1.1.2.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.2"><times id="S3.E1.m1.4.4.4.4.1.1.1.2.1.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.2.1"></times><ci id="S3.E1.m1.4.4.4.4.1.1.1.2.2a.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.2.2"><mtext id="S3.E1.m1.4.4.4.4.1.1.1.2.2.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.2.2">ifÂ </mtext></ci><ci id="S3.E1.m1.4.4.4.4.1.1.1.2.3.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.2.3">ğ‘–</ci></apply><ci id="S3.E1.m1.4.4.4.4.1.1.1.3.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.3">â„¬</ci></apply></matrixrow><matrixrow id="S3.E1.m1.12.12b.cmml" xref="S3.E1.m1.12.12"><set id="S3.E1.m1.7.7.7.3.3.4.cmml" xref="S3.E1.m1.7.7.7.3.3.3"><apply id="S3.E1.m1.5.5.5.1.1.1.1.cmml" xref="S3.E1.m1.5.5.5.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.5.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.5.1.1.1.1">subscript</csymbol><apply id="S3.E1.m1.5.5.5.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.5.1.1.1.1.2"><ci id="S3.E1.m1.5.5.5.1.1.1.1.2.1.cmml" xref="S3.E1.m1.5.5.5.1.1.1.1.2.1">^</ci><ci id="S3.E1.m1.5.5.5.1.1.1.1.2.2.cmml" xref="S3.E1.m1.5.5.5.1.1.1.1.2.2">ğ‘</ci></apply><ci id="S3.E1.m1.5.5.5.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.5.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.E1.m1.6.6.6.2.2.2.2.cmml" xref="S3.E1.m1.6.6.6.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.6.2.2.2.2.1.cmml" xref="S3.E1.m1.6.6.6.2.2.2.2">subscript</csymbol><ci id="S3.E1.m1.6.6.6.2.2.2.2.2.cmml" xref="S3.E1.m1.6.6.6.2.2.2.2.2">â„</ci><ci id="S3.E1.m1.6.6.6.2.2.2.2.3.cmml" xref="S3.E1.m1.6.6.6.2.2.2.2.3">ğ‘–</ci></apply><apply id="S3.E1.m1.7.7.7.3.3.3.3.cmml" xref="S3.E1.m1.7.7.7.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.7.3.3.3.3.1.cmml" xref="S3.E1.m1.7.7.7.3.3.3.3">subscript</csymbol><apply id="S3.E1.m1.7.7.7.3.3.3.3.2.cmml" xref="S3.E1.m1.7.7.7.3.3.3.3.2"><ci id="S3.E1.m1.7.7.7.3.3.3.3.2.1.cmml" xref="S3.E1.m1.7.7.7.3.3.3.3.2.1">^</ci><ci id="S3.E1.m1.7.7.7.3.3.3.3.2.2.cmml" xref="S3.E1.m1.7.7.7.3.3.3.3.2.2">ğ‘“</ci></apply><ci id="S3.E1.m1.7.7.7.3.3.3.3.3.cmml" xref="S3.E1.m1.7.7.7.3.3.3.3.3">ğ‘–</ci></apply></set><apply id="S3.E1.m1.8.8.8.4.1.1.1.cmml" xref="S3.E1.m1.8.8.8.4.1.1"><in id="S3.E1.m1.8.8.8.4.1.1.1.1.cmml" xref="S3.E1.m1.8.8.8.4.1.1.1.1"></in><apply id="S3.E1.m1.8.8.8.4.1.1.1.2.cmml" xref="S3.E1.m1.8.8.8.4.1.1.1.2"><times id="S3.E1.m1.8.8.8.4.1.1.1.2.1.cmml" xref="S3.E1.m1.8.8.8.4.1.1.1.2.1"></times><ci id="S3.E1.m1.8.8.8.4.1.1.1.2.2a.cmml" xref="S3.E1.m1.8.8.8.4.1.1.1.2.2"><mtext id="S3.E1.m1.8.8.8.4.1.1.1.2.2.cmml" xref="S3.E1.m1.8.8.8.4.1.1.1.2.2">ifÂ </mtext></ci><ci id="S3.E1.m1.8.8.8.4.1.1.1.2.3.cmml" xref="S3.E1.m1.8.8.8.4.1.1.1.2.3">ğ‘–</ci></apply><ci id="S3.E1.m1.8.8.8.4.1.1.1.3.cmml" xref="S3.E1.m1.8.8.8.4.1.1.1.3">â„‹</ci></apply></matrixrow><matrixrow id="S3.E1.m1.12.12c.cmml" xref="S3.E1.m1.12.12"><set id="S3.E1.m1.11.11.11.3.3.4.cmml" xref="S3.E1.m1.11.11.11.3.3.3"><apply id="S3.E1.m1.9.9.9.1.1.1.1.cmml" xref="S3.E1.m1.9.9.9.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.9.9.9.1.1.1.1.1.cmml" xref="S3.E1.m1.9.9.9.1.1.1.1">subscript</csymbol><apply id="S3.E1.m1.9.9.9.1.1.1.1.2.cmml" xref="S3.E1.m1.9.9.9.1.1.1.1.2"><ci id="S3.E1.m1.9.9.9.1.1.1.1.2.1.cmml" xref="S3.E1.m1.9.9.9.1.1.1.1.2.1">^</ci><ci id="S3.E1.m1.9.9.9.1.1.1.1.2.2.cmml" xref="S3.E1.m1.9.9.9.1.1.1.1.2.2">ğ‘</ci></apply><ci id="S3.E1.m1.9.9.9.1.1.1.1.3.cmml" xref="S3.E1.m1.9.9.9.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.E1.m1.10.10.10.2.2.2.2.cmml" xref="S3.E1.m1.10.10.10.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.10.10.10.2.2.2.2.1.cmml" xref="S3.E1.m1.10.10.10.2.2.2.2">subscript</csymbol><apply id="S3.E1.m1.10.10.10.2.2.2.2.2.cmml" xref="S3.E1.m1.10.10.10.2.2.2.2.2"><ci id="S3.E1.m1.10.10.10.2.2.2.2.2.1.cmml" xref="S3.E1.m1.10.10.10.2.2.2.2.2.1">^</ci><ci id="S3.E1.m1.10.10.10.2.2.2.2.2.2.cmml" xref="S3.E1.m1.10.10.10.2.2.2.2.2.2">â„</ci></apply><ci id="S3.E1.m1.10.10.10.2.2.2.2.3.cmml" xref="S3.E1.m1.10.10.10.2.2.2.2.3">ğ‘–</ci></apply><apply id="S3.E1.m1.11.11.11.3.3.3.3.cmml" xref="S3.E1.m1.11.11.11.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.11.11.11.3.3.3.3.1.cmml" xref="S3.E1.m1.11.11.11.3.3.3.3">subscript</csymbol><ci id="S3.E1.m1.11.11.11.3.3.3.3.2.cmml" xref="S3.E1.m1.11.11.11.3.3.3.3.2">ğ‘“</ci><ci id="S3.E1.m1.11.11.11.3.3.3.3.3.cmml" xref="S3.E1.m1.11.11.11.3.3.3.3.3">ğ‘–</ci></apply></set><apply id="S3.E1.m1.12.12.12.4.1.1.1.cmml" xref="S3.E1.m1.12.12.12.4.1.1"><in id="S3.E1.m1.12.12.12.4.1.1.1.1.cmml" xref="S3.E1.m1.12.12.12.4.1.1.1.1"></in><apply id="S3.E1.m1.12.12.12.4.1.1.1.2.cmml" xref="S3.E1.m1.12.12.12.4.1.1.1.2"><times id="S3.E1.m1.12.12.12.4.1.1.1.2.1.cmml" xref="S3.E1.m1.12.12.12.4.1.1.1.2.1"></times><ci id="S3.E1.m1.12.12.12.4.1.1.1.2.2a.cmml" xref="S3.E1.m1.12.12.12.4.1.1.1.2.2"><mtext id="S3.E1.m1.12.12.12.4.1.1.1.2.2.cmml" xref="S3.E1.m1.12.12.12.4.1.1.1.2.2">ifÂ </mtext></ci><ci id="S3.E1.m1.12.12.12.4.1.1.1.2.3.cmml" xref="S3.E1.m1.12.12.12.4.1.1.1.2.3">ğ‘–</ci></apply><ci id="S3.E1.m1.12.12.12.4.1.1.1.3.cmml" xref="S3.E1.m1.12.12.12.4.1.1.1.3">â„±</ci></apply></matrixrow></matrix></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.14c">\mathcal{W}_{DOPE}=\{I_{i},w_{i}\}_{i\in\mathcal{B}\cup\mathcal{H}\cup\mathcal{F}}\mbox{~{}~{}~{}~{}where~{}~{}}w_{i}=\left\{\begin{array}[]{ll}\{b_{i},\hat{h}_{i},\hat{f}_{i}\}&amp;\mbox{if }i\in\mathcal{B}~{},\\
\{\hat{b}_{i},h_{i},\hat{f}_{i}\}&amp;\mbox{if }i\in\mathcal{H}~{},\\
\{\hat{b}_{i},\hat{h}_{i},f_{i}\}&amp;\mbox{if }i\in\mathcal{F}~{}.\\
\end{array}\right.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p4.9" class="ltx_p">The detections <math id="S3.SS2.p4.6.m1.1" class="ltx_Math" alttext="\hat{b}_{i}" display="inline"><semantics id="S3.SS2.p4.6.m1.1a"><msub id="S3.SS2.p4.6.m1.1.1" xref="S3.SS2.p4.6.m1.1.1.cmml"><mover accent="true" id="S3.SS2.p4.6.m1.1.1.2" xref="S3.SS2.p4.6.m1.1.1.2.cmml"><mi id="S3.SS2.p4.6.m1.1.1.2.2" xref="S3.SS2.p4.6.m1.1.1.2.2.cmml">b</mi><mo id="S3.SS2.p4.6.m1.1.1.2.1" xref="S3.SS2.p4.6.m1.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS2.p4.6.m1.1.1.3" xref="S3.SS2.p4.6.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.6.m1.1b"><apply id="S3.SS2.p4.6.m1.1.1.cmml" xref="S3.SS2.p4.6.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.6.m1.1.1.1.cmml" xref="S3.SS2.p4.6.m1.1.1">subscript</csymbol><apply id="S3.SS2.p4.6.m1.1.1.2.cmml" xref="S3.SS2.p4.6.m1.1.1.2"><ci id="S3.SS2.p4.6.m1.1.1.2.1.cmml" xref="S3.SS2.p4.6.m1.1.1.2.1">^</ci><ci id="S3.SS2.p4.6.m1.1.1.2.2.cmml" xref="S3.SS2.p4.6.m1.1.1.2.2">ğ‘</ci></apply><ci id="S3.SS2.p4.6.m1.1.1.3.cmml" xref="S3.SS2.p4.6.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.6.m1.1c">\hat{b}_{i}</annotation></semantics></math>, <math id="S3.SS2.p4.7.m2.1" class="ltx_Math" alttext="\hat{h}_{i}" display="inline"><semantics id="S3.SS2.p4.7.m2.1a"><msub id="S3.SS2.p4.7.m2.1.1" xref="S3.SS2.p4.7.m2.1.1.cmml"><mover accent="true" id="S3.SS2.p4.7.m2.1.1.2" xref="S3.SS2.p4.7.m2.1.1.2.cmml"><mi id="S3.SS2.p4.7.m2.1.1.2.2" xref="S3.SS2.p4.7.m2.1.1.2.2.cmml">h</mi><mo id="S3.SS2.p4.7.m2.1.1.2.1" xref="S3.SS2.p4.7.m2.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS2.p4.7.m2.1.1.3" xref="S3.SS2.p4.7.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.7.m2.1b"><apply id="S3.SS2.p4.7.m2.1.1.cmml" xref="S3.SS2.p4.7.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.7.m2.1.1.1.cmml" xref="S3.SS2.p4.7.m2.1.1">subscript</csymbol><apply id="S3.SS2.p4.7.m2.1.1.2.cmml" xref="S3.SS2.p4.7.m2.1.1.2"><ci id="S3.SS2.p4.7.m2.1.1.2.1.cmml" xref="S3.SS2.p4.7.m2.1.1.2.1">^</ci><ci id="S3.SS2.p4.7.m2.1.1.2.2.cmml" xref="S3.SS2.p4.7.m2.1.1.2.2">â„</ci></apply><ci id="S3.SS2.p4.7.m2.1.1.3.cmml" xref="S3.SS2.p4.7.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.7.m2.1c">\hat{h}_{i}</annotation></semantics></math> and <math id="S3.SS2.p4.8.m3.1" class="ltx_Math" alttext="\hat{f}_{i}" display="inline"><semantics id="S3.SS2.p4.8.m3.1a"><msub id="S3.SS2.p4.8.m3.1.1" xref="S3.SS2.p4.8.m3.1.1.cmml"><mover accent="true" id="S3.SS2.p4.8.m3.1.1.2" xref="S3.SS2.p4.8.m3.1.1.2.cmml"><mi id="S3.SS2.p4.8.m3.1.1.2.2" xref="S3.SS2.p4.8.m3.1.1.2.2.cmml">f</mi><mo id="S3.SS2.p4.8.m3.1.1.2.1" xref="S3.SS2.p4.8.m3.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS2.p4.8.m3.1.1.3" xref="S3.SS2.p4.8.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.8.m3.1b"><apply id="S3.SS2.p4.8.m3.1.1.cmml" xref="S3.SS2.p4.8.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.8.m3.1.1.1.cmml" xref="S3.SS2.p4.8.m3.1.1">subscript</csymbol><apply id="S3.SS2.p4.8.m3.1.1.2.cmml" xref="S3.SS2.p4.8.m3.1.1.2"><ci id="S3.SS2.p4.8.m3.1.1.2.1.cmml" xref="S3.SS2.p4.8.m3.1.1.2.1">^</ci><ci id="S3.SS2.p4.8.m3.1.1.2.2.cmml" xref="S3.SS2.p4.8.m3.1.1.2.2">ğ‘“</ci></apply><ci id="S3.SS2.p4.8.m3.1.1.3.cmml" xref="S3.SS2.p4.8.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.8.m3.1c">\hat{f}_{i}</annotation></semantics></math> estimated by the experts are therefore considered as pseudo ground-truth for the missing keypoints in 2D and 3D.
In practice, ground-truth annotations are completed using these estimations, for example when some annotations have been incorrectly labeled or are simply missing.
Note that training images with no annotation at all could also be used to train our network, using only pseudo ground-truth annotationsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, <em id="S3.SS2.p4.9.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS2.p4.9.2" class="ltx_text"></span>, <math id="S3.SS2.p4.9.m4.3" class="ltx_Math" alttext="w_{i}=\{\hat{b}_{i},\hat{h}_{i},\hat{f}_{i}\}" display="inline"><semantics id="S3.SS2.p4.9.m4.3a"><mrow id="S3.SS2.p4.9.m4.3.3" xref="S3.SS2.p4.9.m4.3.3.cmml"><msub id="S3.SS2.p4.9.m4.3.3.5" xref="S3.SS2.p4.9.m4.3.3.5.cmml"><mi id="S3.SS2.p4.9.m4.3.3.5.2" xref="S3.SS2.p4.9.m4.3.3.5.2.cmml">w</mi><mi id="S3.SS2.p4.9.m4.3.3.5.3" xref="S3.SS2.p4.9.m4.3.3.5.3.cmml">i</mi></msub><mo id="S3.SS2.p4.9.m4.3.3.4" xref="S3.SS2.p4.9.m4.3.3.4.cmml">=</mo><mrow id="S3.SS2.p4.9.m4.3.3.3.3" xref="S3.SS2.p4.9.m4.3.3.3.4.cmml"><mo stretchy="false" id="S3.SS2.p4.9.m4.3.3.3.3.4" xref="S3.SS2.p4.9.m4.3.3.3.4.cmml">{</mo><msub id="S3.SS2.p4.9.m4.1.1.1.1.1" xref="S3.SS2.p4.9.m4.1.1.1.1.1.cmml"><mover accent="true" id="S3.SS2.p4.9.m4.1.1.1.1.1.2" xref="S3.SS2.p4.9.m4.1.1.1.1.1.2.cmml"><mi id="S3.SS2.p4.9.m4.1.1.1.1.1.2.2" xref="S3.SS2.p4.9.m4.1.1.1.1.1.2.2.cmml">b</mi><mo id="S3.SS2.p4.9.m4.1.1.1.1.1.2.1" xref="S3.SS2.p4.9.m4.1.1.1.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS2.p4.9.m4.1.1.1.1.1.3" xref="S3.SS2.p4.9.m4.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.p4.9.m4.3.3.3.3.5" xref="S3.SS2.p4.9.m4.3.3.3.4.cmml">,</mo><msub id="S3.SS2.p4.9.m4.2.2.2.2.2" xref="S3.SS2.p4.9.m4.2.2.2.2.2.cmml"><mover accent="true" id="S3.SS2.p4.9.m4.2.2.2.2.2.2" xref="S3.SS2.p4.9.m4.2.2.2.2.2.2.cmml"><mi id="S3.SS2.p4.9.m4.2.2.2.2.2.2.2" xref="S3.SS2.p4.9.m4.2.2.2.2.2.2.2.cmml">h</mi><mo id="S3.SS2.p4.9.m4.2.2.2.2.2.2.1" xref="S3.SS2.p4.9.m4.2.2.2.2.2.2.1.cmml">^</mo></mover><mi id="S3.SS2.p4.9.m4.2.2.2.2.2.3" xref="S3.SS2.p4.9.m4.2.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.SS2.p4.9.m4.3.3.3.3.6" xref="S3.SS2.p4.9.m4.3.3.3.4.cmml">,</mo><msub id="S3.SS2.p4.9.m4.3.3.3.3.3" xref="S3.SS2.p4.9.m4.3.3.3.3.3.cmml"><mover accent="true" id="S3.SS2.p4.9.m4.3.3.3.3.3.2" xref="S3.SS2.p4.9.m4.3.3.3.3.3.2.cmml"><mi id="S3.SS2.p4.9.m4.3.3.3.3.3.2.2" xref="S3.SS2.p4.9.m4.3.3.3.3.3.2.2.cmml">f</mi><mo id="S3.SS2.p4.9.m4.3.3.3.3.3.2.1" xref="S3.SS2.p4.9.m4.3.3.3.3.3.2.1.cmml">^</mo></mover><mi id="S3.SS2.p4.9.m4.3.3.3.3.3.3" xref="S3.SS2.p4.9.m4.3.3.3.3.3.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS2.p4.9.m4.3.3.3.3.7" xref="S3.SS2.p4.9.m4.3.3.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.9.m4.3b"><apply id="S3.SS2.p4.9.m4.3.3.cmml" xref="S3.SS2.p4.9.m4.3.3"><eq id="S3.SS2.p4.9.m4.3.3.4.cmml" xref="S3.SS2.p4.9.m4.3.3.4"></eq><apply id="S3.SS2.p4.9.m4.3.3.5.cmml" xref="S3.SS2.p4.9.m4.3.3.5"><csymbol cd="ambiguous" id="S3.SS2.p4.9.m4.3.3.5.1.cmml" xref="S3.SS2.p4.9.m4.3.3.5">subscript</csymbol><ci id="S3.SS2.p4.9.m4.3.3.5.2.cmml" xref="S3.SS2.p4.9.m4.3.3.5.2">ğ‘¤</ci><ci id="S3.SS2.p4.9.m4.3.3.5.3.cmml" xref="S3.SS2.p4.9.m4.3.3.5.3">ğ‘–</ci></apply><set id="S3.SS2.p4.9.m4.3.3.3.4.cmml" xref="S3.SS2.p4.9.m4.3.3.3.3"><apply id="S3.SS2.p4.9.m4.1.1.1.1.1.cmml" xref="S3.SS2.p4.9.m4.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.9.m4.1.1.1.1.1.1.cmml" xref="S3.SS2.p4.9.m4.1.1.1.1.1">subscript</csymbol><apply id="S3.SS2.p4.9.m4.1.1.1.1.1.2.cmml" xref="S3.SS2.p4.9.m4.1.1.1.1.1.2"><ci id="S3.SS2.p4.9.m4.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p4.9.m4.1.1.1.1.1.2.1">^</ci><ci id="S3.SS2.p4.9.m4.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p4.9.m4.1.1.1.1.1.2.2">ğ‘</ci></apply><ci id="S3.SS2.p4.9.m4.1.1.1.1.1.3.cmml" xref="S3.SS2.p4.9.m4.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.SS2.p4.9.m4.2.2.2.2.2.cmml" xref="S3.SS2.p4.9.m4.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p4.9.m4.2.2.2.2.2.1.cmml" xref="S3.SS2.p4.9.m4.2.2.2.2.2">subscript</csymbol><apply id="S3.SS2.p4.9.m4.2.2.2.2.2.2.cmml" xref="S3.SS2.p4.9.m4.2.2.2.2.2.2"><ci id="S3.SS2.p4.9.m4.2.2.2.2.2.2.1.cmml" xref="S3.SS2.p4.9.m4.2.2.2.2.2.2.1">^</ci><ci id="S3.SS2.p4.9.m4.2.2.2.2.2.2.2.cmml" xref="S3.SS2.p4.9.m4.2.2.2.2.2.2.2">â„</ci></apply><ci id="S3.SS2.p4.9.m4.2.2.2.2.2.3.cmml" xref="S3.SS2.p4.9.m4.2.2.2.2.2.3">ğ‘–</ci></apply><apply id="S3.SS2.p4.9.m4.3.3.3.3.3.cmml" xref="S3.SS2.p4.9.m4.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p4.9.m4.3.3.3.3.3.1.cmml" xref="S3.SS2.p4.9.m4.3.3.3.3.3">subscript</csymbol><apply id="S3.SS2.p4.9.m4.3.3.3.3.3.2.cmml" xref="S3.SS2.p4.9.m4.3.3.3.3.3.2"><ci id="S3.SS2.p4.9.m4.3.3.3.3.3.2.1.cmml" xref="S3.SS2.p4.9.m4.3.3.3.3.3.2.1">^</ci><ci id="S3.SS2.p4.9.m4.3.3.3.3.3.2.2.cmml" xref="S3.SS2.p4.9.m4.3.3.3.3.3.2.2">ğ‘“</ci></apply><ci id="S3.SS2.p4.9.m4.3.3.3.3.3.3.cmml" xref="S3.SS2.p4.9.m4.3.3.3.3.3.3">ğ‘–</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.9.m4.3c">w_{i}=\{\hat{b}_{i},\hat{h}_{i},\hat{f}_{i}\}</annotation></semantics></math>. The training scheme is illustrated in FigureÂ <a href="#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para ltx_noindent">
<p id="S3.SS2.p5.6" class="ltx_p"><span id="S3.SS2.p5.6.1" class="ltx_text ltx_font_bold">Loss.</span>
Our loss <math id="S3.SS2.p5.1.m1.1" class="ltx_Math" alttext="\mathcal{L}" display="inline"><semantics id="S3.SS2.p5.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml">â„’</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><ci id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1">â„’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">\mathcal{L}</annotation></semantics></math> to train the network combines the RPN loss <math id="S3.SS2.p5.2.m2.1" class="ltx_Math" alttext="\mathcal{L}_{RPN}" display="inline"><semantics id="S3.SS2.p5.2.m2.1a"><msub id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p5.2.m2.1.1.2" xref="S3.SS2.p5.2.m2.1.1.2.cmml">â„’</mi><mrow id="S3.SS2.p5.2.m2.1.1.3" xref="S3.SS2.p5.2.m2.1.1.3.cmml"><mi id="S3.SS2.p5.2.m2.1.1.3.2" xref="S3.SS2.p5.2.m2.1.1.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.2.m2.1.1.3.1" xref="S3.SS2.p5.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p5.2.m2.1.1.3.3" xref="S3.SS2.p5.2.m2.1.1.3.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.2.m2.1.1.3.1a" xref="S3.SS2.p5.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p5.2.m2.1.1.3.4" xref="S3.SS2.p5.2.m2.1.1.3.4.cmml">N</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><apply id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.2.m2.1.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p5.2.m2.1.1.2.cmml" xref="S3.SS2.p5.2.m2.1.1.2">â„’</ci><apply id="S3.SS2.p5.2.m2.1.1.3.cmml" xref="S3.SS2.p5.2.m2.1.1.3"><times id="S3.SS2.p5.2.m2.1.1.3.1.cmml" xref="S3.SS2.p5.2.m2.1.1.3.1"></times><ci id="S3.SS2.p5.2.m2.1.1.3.2.cmml" xref="S3.SS2.p5.2.m2.1.1.3.2">ğ‘…</ci><ci id="S3.SS2.p5.2.m2.1.1.3.3.cmml" xref="S3.SS2.p5.2.m2.1.1.3.3">ğ‘ƒ</ci><ci id="S3.SS2.p5.2.m2.1.1.3.4.cmml" xref="S3.SS2.p5.2.m2.1.1.3.4">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">\mathcal{L}_{RPN}</annotation></semantics></math> as well as the sum of three terms for each part <math id="S3.SS2.p5.3.m3.1" class="ltx_Math" alttext="p\in\{\text{body,hand,face}\}" display="inline"><semantics id="S3.SS2.p5.3.m3.1a"><mrow id="S3.SS2.p5.3.m3.1.2" xref="S3.SS2.p5.3.m3.1.2.cmml"><mi id="S3.SS2.p5.3.m3.1.2.2" xref="S3.SS2.p5.3.m3.1.2.2.cmml">p</mi><mo id="S3.SS2.p5.3.m3.1.2.1" xref="S3.SS2.p5.3.m3.1.2.1.cmml">âˆˆ</mo><mrow id="S3.SS2.p5.3.m3.1.2.3.2" xref="S3.SS2.p5.3.m3.1.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.p5.3.m3.1.2.3.2.1" xref="S3.SS2.p5.3.m3.1.2.3.1.cmml">{</mo><mtext id="S3.SS2.p5.3.m3.1.1" xref="S3.SS2.p5.3.m3.1.1a.cmml">body,hand,face</mtext><mo stretchy="false" id="S3.SS2.p5.3.m3.1.2.3.2.2" xref="S3.SS2.p5.3.m3.1.2.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.3.m3.1b"><apply id="S3.SS2.p5.3.m3.1.2.cmml" xref="S3.SS2.p5.3.m3.1.2"><in id="S3.SS2.p5.3.m3.1.2.1.cmml" xref="S3.SS2.p5.3.m3.1.2.1"></in><ci id="S3.SS2.p5.3.m3.1.2.2.cmml" xref="S3.SS2.p5.3.m3.1.2.2">ğ‘</ci><set id="S3.SS2.p5.3.m3.1.2.3.1.cmml" xref="S3.SS2.p5.3.m3.1.2.3.2"><ci id="S3.SS2.p5.3.m3.1.1a.cmml" xref="S3.SS2.p5.3.m3.1.1"><mtext id="S3.SS2.p5.3.m3.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1">body,hand,face</mtext></ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.3.m3.1c">p\in\{\text{body,hand,face}\}</annotation></semantics></math>:
(a) a classification loss <math id="S3.SS2.p5.4.m4.1" class="ltx_Math" alttext="\mathcal{L}^{p}_{cls}" display="inline"><semantics id="S3.SS2.p5.4.m4.1a"><msubsup id="S3.SS2.p5.4.m4.1.1" xref="S3.SS2.p5.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p5.4.m4.1.1.2.2" xref="S3.SS2.p5.4.m4.1.1.2.2.cmml">â„’</mi><mrow id="S3.SS2.p5.4.m4.1.1.3" xref="S3.SS2.p5.4.m4.1.1.3.cmml"><mi id="S3.SS2.p5.4.m4.1.1.3.2" xref="S3.SS2.p5.4.m4.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.4.m4.1.1.3.1" xref="S3.SS2.p5.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p5.4.m4.1.1.3.3" xref="S3.SS2.p5.4.m4.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.4.m4.1.1.3.1a" xref="S3.SS2.p5.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p5.4.m4.1.1.3.4" xref="S3.SS2.p5.4.m4.1.1.3.4.cmml">s</mi></mrow><mi id="S3.SS2.p5.4.m4.1.1.2.3" xref="S3.SS2.p5.4.m4.1.1.2.3.cmml">p</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.4.m4.1b"><apply id="S3.SS2.p5.4.m4.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.4.m4.1.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1">subscript</csymbol><apply id="S3.SS2.p5.4.m4.1.1.2.cmml" xref="S3.SS2.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.4.m4.1.1.2.1.cmml" xref="S3.SS2.p5.4.m4.1.1">superscript</csymbol><ci id="S3.SS2.p5.4.m4.1.1.2.2.cmml" xref="S3.SS2.p5.4.m4.1.1.2.2">â„’</ci><ci id="S3.SS2.p5.4.m4.1.1.2.3.cmml" xref="S3.SS2.p5.4.m4.1.1.2.3">ğ‘</ci></apply><apply id="S3.SS2.p5.4.m4.1.1.3.cmml" xref="S3.SS2.p5.4.m4.1.1.3"><times id="S3.SS2.p5.4.m4.1.1.3.1.cmml" xref="S3.SS2.p5.4.m4.1.1.3.1"></times><ci id="S3.SS2.p5.4.m4.1.1.3.2.cmml" xref="S3.SS2.p5.4.m4.1.1.3.2">ğ‘</ci><ci id="S3.SS2.p5.4.m4.1.1.3.3.cmml" xref="S3.SS2.p5.4.m4.1.1.3.3">ğ‘™</ci><ci id="S3.SS2.p5.4.m4.1.1.3.4.cmml" xref="S3.SS2.p5.4.m4.1.1.3.4">ğ‘ </ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.4.m4.1c">\mathcal{L}^{p}_{cls}</annotation></semantics></math>, (b) a regression loss <math id="S3.SS2.p5.5.m5.1" class="ltx_Math" alttext="\mathcal{L}^{p}_{reg}" display="inline"><semantics id="S3.SS2.p5.5.m5.1a"><msubsup id="S3.SS2.p5.5.m5.1.1" xref="S3.SS2.p5.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p5.5.m5.1.1.2.2" xref="S3.SS2.p5.5.m5.1.1.2.2.cmml">â„’</mi><mrow id="S3.SS2.p5.5.m5.1.1.3" xref="S3.SS2.p5.5.m5.1.1.3.cmml"><mi id="S3.SS2.p5.5.m5.1.1.3.2" xref="S3.SS2.p5.5.m5.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.5.m5.1.1.3.1" xref="S3.SS2.p5.5.m5.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p5.5.m5.1.1.3.3" xref="S3.SS2.p5.5.m5.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.5.m5.1.1.3.1a" xref="S3.SS2.p5.5.m5.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p5.5.m5.1.1.3.4" xref="S3.SS2.p5.5.m5.1.1.3.4.cmml">g</mi></mrow><mi id="S3.SS2.p5.5.m5.1.1.2.3" xref="S3.SS2.p5.5.m5.1.1.2.3.cmml">p</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.5.m5.1b"><apply id="S3.SS2.p5.5.m5.1.1.cmml" xref="S3.SS2.p5.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.5.m5.1.1.1.cmml" xref="S3.SS2.p5.5.m5.1.1">subscript</csymbol><apply id="S3.SS2.p5.5.m5.1.1.2.cmml" xref="S3.SS2.p5.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.5.m5.1.1.2.1.cmml" xref="S3.SS2.p5.5.m5.1.1">superscript</csymbol><ci id="S3.SS2.p5.5.m5.1.1.2.2.cmml" xref="S3.SS2.p5.5.m5.1.1.2.2">â„’</ci><ci id="S3.SS2.p5.5.m5.1.1.2.3.cmml" xref="S3.SS2.p5.5.m5.1.1.2.3">ğ‘</ci></apply><apply id="S3.SS2.p5.5.m5.1.1.3.cmml" xref="S3.SS2.p5.5.m5.1.1.3"><times id="S3.SS2.p5.5.m5.1.1.3.1.cmml" xref="S3.SS2.p5.5.m5.1.1.3.1"></times><ci id="S3.SS2.p5.5.m5.1.1.3.2.cmml" xref="S3.SS2.p5.5.m5.1.1.3.2">ğ‘Ÿ</ci><ci id="S3.SS2.p5.5.m5.1.1.3.3.cmml" xref="S3.SS2.p5.5.m5.1.1.3.3">ğ‘’</ci><ci id="S3.SS2.p5.5.m5.1.1.3.4.cmml" xref="S3.SS2.p5.5.m5.1.1.3.4">ğ‘”</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.5.m5.1c">\mathcal{L}^{p}_{reg}</annotation></semantics></math>, (c) a distillation loss <math id="S3.SS2.p5.6.m6.1" class="ltx_Math" alttext="\mathcal{L}^{p}_{dist}" display="inline"><semantics id="S3.SS2.p5.6.m6.1a"><msubsup id="S3.SS2.p5.6.m6.1.1" xref="S3.SS2.p5.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p5.6.m6.1.1.2.2" xref="S3.SS2.p5.6.m6.1.1.2.2.cmml">â„’</mi><mrow id="S3.SS2.p5.6.m6.1.1.3" xref="S3.SS2.p5.6.m6.1.1.3.cmml"><mi id="S3.SS2.p5.6.m6.1.1.3.2" xref="S3.SS2.p5.6.m6.1.1.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.6.m6.1.1.3.1" xref="S3.SS2.p5.6.m6.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p5.6.m6.1.1.3.3" xref="S3.SS2.p5.6.m6.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.6.m6.1.1.3.1a" xref="S3.SS2.p5.6.m6.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p5.6.m6.1.1.3.4" xref="S3.SS2.p5.6.m6.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.6.m6.1.1.3.1b" xref="S3.SS2.p5.6.m6.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p5.6.m6.1.1.3.5" xref="S3.SS2.p5.6.m6.1.1.3.5.cmml">t</mi></mrow><mi id="S3.SS2.p5.6.m6.1.1.2.3" xref="S3.SS2.p5.6.m6.1.1.2.3.cmml">p</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.6.m6.1b"><apply id="S3.SS2.p5.6.m6.1.1.cmml" xref="S3.SS2.p5.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.6.m6.1.1.1.cmml" xref="S3.SS2.p5.6.m6.1.1">subscript</csymbol><apply id="S3.SS2.p5.6.m6.1.1.2.cmml" xref="S3.SS2.p5.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.6.m6.1.1.2.1.cmml" xref="S3.SS2.p5.6.m6.1.1">superscript</csymbol><ci id="S3.SS2.p5.6.m6.1.1.2.2.cmml" xref="S3.SS2.p5.6.m6.1.1.2.2">â„’</ci><ci id="S3.SS2.p5.6.m6.1.1.2.3.cmml" xref="S3.SS2.p5.6.m6.1.1.2.3">ğ‘</ci></apply><apply id="S3.SS2.p5.6.m6.1.1.3.cmml" xref="S3.SS2.p5.6.m6.1.1.3"><times id="S3.SS2.p5.6.m6.1.1.3.1.cmml" xref="S3.SS2.p5.6.m6.1.1.3.1"></times><ci id="S3.SS2.p5.6.m6.1.1.3.2.cmml" xref="S3.SS2.p5.6.m6.1.1.3.2">ğ‘‘</ci><ci id="S3.SS2.p5.6.m6.1.1.3.3.cmml" xref="S3.SS2.p5.6.m6.1.1.3.3">ğ‘–</ci><ci id="S3.SS2.p5.6.m6.1.1.3.4.cmml" xref="S3.SS2.p5.6.m6.1.1.3.4">ğ‘ </ci><ci id="S3.SS2.p5.6.m6.1.1.3.5.cmml" xref="S3.SS2.p5.6.m6.1.1.3.5">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.6.m6.1c">\mathcal{L}^{p}_{dist}</annotation></semantics></math>:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.2" class="ltx_Math" alttext="\mathcal{L}=\mathcal{L}_{RPN}+\sum_{p\in\{\text{body,hand,face}\}}\mathcal{L}^{p}_{cls}+\mathcal{L}^{p}_{reg}+\mathcal{L}^{p}_{dist}~{}~{}," display="block"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.2.2.1.1.2" xref="S3.E2.m1.2.2.1.1.2.cmml">â„’</mi><mo id="S3.E2.m1.2.2.1.1.1" xref="S3.E2.m1.2.2.1.1.1.cmml">=</mo><mrow id="S3.E2.m1.2.2.1.1.3" xref="S3.E2.m1.2.2.1.1.3.cmml"><msub id="S3.E2.m1.2.2.1.1.3.2" xref="S3.E2.m1.2.2.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.2.2.1.1.3.2.2" xref="S3.E2.m1.2.2.1.1.3.2.2.cmml">â„’</mi><mrow id="S3.E2.m1.2.2.1.1.3.2.3" xref="S3.E2.m1.2.2.1.1.3.2.3.cmml"><mi id="S3.E2.m1.2.2.1.1.3.2.3.2" xref="S3.E2.m1.2.2.1.1.3.2.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.3.2.3.1" xref="S3.E2.m1.2.2.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.2.2.1.1.3.2.3.3" xref="S3.E2.m1.2.2.1.1.3.2.3.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.3.2.3.1a" xref="S3.E2.m1.2.2.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.2.2.1.1.3.2.3.4" xref="S3.E2.m1.2.2.1.1.3.2.3.4.cmml">N</mi></mrow></msub><mo rspace="0.055em" id="S3.E2.m1.2.2.1.1.3.1" xref="S3.E2.m1.2.2.1.1.3.1.cmml">+</mo><mrow id="S3.E2.m1.2.2.1.1.3.3" xref="S3.E2.m1.2.2.1.1.3.3.cmml"><munder id="S3.E2.m1.2.2.1.1.3.3.1" xref="S3.E2.m1.2.2.1.1.3.3.1.cmml"><mo movablelimits="false" id="S3.E2.m1.2.2.1.1.3.3.1.2" xref="S3.E2.m1.2.2.1.1.3.3.1.2.cmml">âˆ‘</mo><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.3.cmml">p</mi><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.2.cmml">âˆˆ</mo><mrow id="S3.E2.m1.1.1.1.4.2" xref="S3.E2.m1.1.1.1.4.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.4.2.1" xref="S3.E2.m1.1.1.1.4.1.cmml">{</mo><mtext id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1a.cmml">body,hand,face</mtext><mo stretchy="false" id="S3.E2.m1.1.1.1.4.2.2" xref="S3.E2.m1.1.1.1.4.1.cmml">}</mo></mrow></mrow></munder><msubsup id="S3.E2.m1.2.2.1.1.3.3.2" xref="S3.E2.m1.2.2.1.1.3.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.2.2.1.1.3.3.2.2.2" xref="S3.E2.m1.2.2.1.1.3.3.2.2.2.cmml">â„’</mi><mrow id="S3.E2.m1.2.2.1.1.3.3.2.3" xref="S3.E2.m1.2.2.1.1.3.3.2.3.cmml"><mi id="S3.E2.m1.2.2.1.1.3.3.2.3.2" xref="S3.E2.m1.2.2.1.1.3.3.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.3.3.2.3.1" xref="S3.E2.m1.2.2.1.1.3.3.2.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.2.2.1.1.3.3.2.3.3" xref="S3.E2.m1.2.2.1.1.3.3.2.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.3.3.2.3.1a" xref="S3.E2.m1.2.2.1.1.3.3.2.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.2.2.1.1.3.3.2.3.4" xref="S3.E2.m1.2.2.1.1.3.3.2.3.4.cmml">s</mi></mrow><mi id="S3.E2.m1.2.2.1.1.3.3.2.2.3" xref="S3.E2.m1.2.2.1.1.3.3.2.2.3.cmml">p</mi></msubsup></mrow><mo id="S3.E2.m1.2.2.1.1.3.1a" xref="S3.E2.m1.2.2.1.1.3.1.cmml">+</mo><msubsup id="S3.E2.m1.2.2.1.1.3.4" xref="S3.E2.m1.2.2.1.1.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.2.2.1.1.3.4.2.2" xref="S3.E2.m1.2.2.1.1.3.4.2.2.cmml">â„’</mi><mrow id="S3.E2.m1.2.2.1.1.3.4.3" xref="S3.E2.m1.2.2.1.1.3.4.3.cmml"><mi id="S3.E2.m1.2.2.1.1.3.4.3.2" xref="S3.E2.m1.2.2.1.1.3.4.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.3.4.3.1" xref="S3.E2.m1.2.2.1.1.3.4.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.2.2.1.1.3.4.3.3" xref="S3.E2.m1.2.2.1.1.3.4.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.3.4.3.1a" xref="S3.E2.m1.2.2.1.1.3.4.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.2.2.1.1.3.4.3.4" xref="S3.E2.m1.2.2.1.1.3.4.3.4.cmml">g</mi></mrow><mi id="S3.E2.m1.2.2.1.1.3.4.2.3" xref="S3.E2.m1.2.2.1.1.3.4.2.3.cmml">p</mi></msubsup><mo id="S3.E2.m1.2.2.1.1.3.1b" xref="S3.E2.m1.2.2.1.1.3.1.cmml">+</mo><msubsup id="S3.E2.m1.2.2.1.1.3.5" xref="S3.E2.m1.2.2.1.1.3.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.2.2.1.1.3.5.2.2" xref="S3.E2.m1.2.2.1.1.3.5.2.2.cmml">â„’</mi><mrow id="S3.E2.m1.2.2.1.1.3.5.3" xref="S3.E2.m1.2.2.1.1.3.5.3.cmml"><mi id="S3.E2.m1.2.2.1.1.3.5.3.2" xref="S3.E2.m1.2.2.1.1.3.5.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.3.5.3.1" xref="S3.E2.m1.2.2.1.1.3.5.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.2.2.1.1.3.5.3.3" xref="S3.E2.m1.2.2.1.1.3.5.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.3.5.3.1a" xref="S3.E2.m1.2.2.1.1.3.5.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.2.2.1.1.3.5.3.4" xref="S3.E2.m1.2.2.1.1.3.5.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.3.5.3.1b" xref="S3.E2.m1.2.2.1.1.3.5.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.2.2.1.1.3.5.3.5" xref="S3.E2.m1.2.2.1.1.3.5.3.5.cmml">t</mi></mrow><mi id="S3.E2.m1.2.2.1.1.3.5.2.3" xref="S3.E2.m1.2.2.1.1.3.5.2.3.cmml">p</mi></msubsup></mrow></mrow><mo id="S3.E2.m1.2.2.1.2" xref="S3.E2.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1"><eq id="S3.E2.m1.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1"></eq><ci id="S3.E2.m1.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.2">â„’</ci><apply id="S3.E2.m1.2.2.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.3"><plus id="S3.E2.m1.2.2.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3.1"></plus><apply id="S3.E2.m1.2.2.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.2.1.cmml" xref="S3.E2.m1.2.2.1.1.3.2">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.3.2.2.cmml" xref="S3.E2.m1.2.2.1.1.3.2.2">â„’</ci><apply id="S3.E2.m1.2.2.1.1.3.2.3.cmml" xref="S3.E2.m1.2.2.1.1.3.2.3"><times id="S3.E2.m1.2.2.1.1.3.2.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3.2.3.1"></times><ci id="S3.E2.m1.2.2.1.1.3.2.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.2.3.2">ğ‘…</ci><ci id="S3.E2.m1.2.2.1.1.3.2.3.3.cmml" xref="S3.E2.m1.2.2.1.1.3.2.3.3">ğ‘ƒ</ci><ci id="S3.E2.m1.2.2.1.1.3.2.3.4.cmml" xref="S3.E2.m1.2.2.1.1.3.2.3.4">ğ‘</ci></apply></apply><apply id="S3.E2.m1.2.2.1.1.3.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3"><apply id="S3.E2.m1.2.2.1.1.3.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.3.1.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.1">subscript</csymbol><sum id="S3.E2.m1.2.2.1.1.3.3.1.2.cmml" xref="S3.E2.m1.2.2.1.1.3.3.1.2"></sum><apply id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><in id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.2"></in><ci id="S3.E2.m1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.3">ğ‘</ci><set id="S3.E2.m1.1.1.1.4.1.cmml" xref="S3.E2.m1.1.1.1.4.2"><ci id="S3.E2.m1.1.1.1.1a.cmml" xref="S3.E2.m1.1.1.1.1"><mtext mathsize="70%" id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">body,hand,face</mtext></ci></set></apply></apply><apply id="S3.E2.m1.2.2.1.1.3.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.3.2.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2">subscript</csymbol><apply id="S3.E2.m1.2.2.1.1.3.3.2.2.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.3.2.2.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2">superscript</csymbol><ci id="S3.E2.m1.2.2.1.1.3.3.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.2.2">â„’</ci><ci id="S3.E2.m1.2.2.1.1.3.3.2.2.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.2.3">ğ‘</ci></apply><apply id="S3.E2.m1.2.2.1.1.3.3.2.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.3"><times id="S3.E2.m1.2.2.1.1.3.3.2.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.3.1"></times><ci id="S3.E2.m1.2.2.1.1.3.3.2.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.3.2">ğ‘</ci><ci id="S3.E2.m1.2.2.1.1.3.3.2.3.3.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.3.3">ğ‘™</ci><ci id="S3.E2.m1.2.2.1.1.3.3.2.3.4.cmml" xref="S3.E2.m1.2.2.1.1.3.3.2.3.4">ğ‘ </ci></apply></apply></apply><apply id="S3.E2.m1.2.2.1.1.3.4.cmml" xref="S3.E2.m1.2.2.1.1.3.4"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.4.1.cmml" xref="S3.E2.m1.2.2.1.1.3.4">subscript</csymbol><apply id="S3.E2.m1.2.2.1.1.3.4.2.cmml" xref="S3.E2.m1.2.2.1.1.3.4"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.4.2.1.cmml" xref="S3.E2.m1.2.2.1.1.3.4">superscript</csymbol><ci id="S3.E2.m1.2.2.1.1.3.4.2.2.cmml" xref="S3.E2.m1.2.2.1.1.3.4.2.2">â„’</ci><ci id="S3.E2.m1.2.2.1.1.3.4.2.3.cmml" xref="S3.E2.m1.2.2.1.1.3.4.2.3">ğ‘</ci></apply><apply id="S3.E2.m1.2.2.1.1.3.4.3.cmml" xref="S3.E2.m1.2.2.1.1.3.4.3"><times id="S3.E2.m1.2.2.1.1.3.4.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3.4.3.1"></times><ci id="S3.E2.m1.2.2.1.1.3.4.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.4.3.2">ğ‘Ÿ</ci><ci id="S3.E2.m1.2.2.1.1.3.4.3.3.cmml" xref="S3.E2.m1.2.2.1.1.3.4.3.3">ğ‘’</ci><ci id="S3.E2.m1.2.2.1.1.3.4.3.4.cmml" xref="S3.E2.m1.2.2.1.1.3.4.3.4">ğ‘”</ci></apply></apply><apply id="S3.E2.m1.2.2.1.1.3.5.cmml" xref="S3.E2.m1.2.2.1.1.3.5"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.5.1.cmml" xref="S3.E2.m1.2.2.1.1.3.5">subscript</csymbol><apply id="S3.E2.m1.2.2.1.1.3.5.2.cmml" xref="S3.E2.m1.2.2.1.1.3.5"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.5.2.1.cmml" xref="S3.E2.m1.2.2.1.1.3.5">superscript</csymbol><ci id="S3.E2.m1.2.2.1.1.3.5.2.2.cmml" xref="S3.E2.m1.2.2.1.1.3.5.2.2">â„’</ci><ci id="S3.E2.m1.2.2.1.1.3.5.2.3.cmml" xref="S3.E2.m1.2.2.1.1.3.5.2.3">ğ‘</ci></apply><apply id="S3.E2.m1.2.2.1.1.3.5.3.cmml" xref="S3.E2.m1.2.2.1.1.3.5.3"><times id="S3.E2.m1.2.2.1.1.3.5.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3.5.3.1"></times><ci id="S3.E2.m1.2.2.1.1.3.5.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.5.3.2">ğ‘‘</ci><ci id="S3.E2.m1.2.2.1.1.3.5.3.3.cmml" xref="S3.E2.m1.2.2.1.1.3.5.3.3">ğ‘–</ci><ci id="S3.E2.m1.2.2.1.1.3.5.3.4.cmml" xref="S3.E2.m1.2.2.1.1.3.5.3.4">ğ‘ </ci><ci id="S3.E2.m1.2.2.1.1.3.5.3.5.cmml" xref="S3.E2.m1.2.2.1.1.3.5.3.5">ğ‘¡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">\mathcal{L}=\mathcal{L}_{RPN}+\sum_{p\in\{\text{body,hand,face}\}}\mathcal{L}^{p}_{cls}+\mathcal{L}^{p}_{reg}+\mathcal{L}^{p}_{dist}~{}~{},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p5.9" class="ltx_p">where <math id="S3.SS2.p5.7.m1.1" class="ltx_Math" alttext="\mathcal{L}_{RPN}" display="inline"><semantics id="S3.SS2.p5.7.m1.1a"><msub id="S3.SS2.p5.7.m1.1.1" xref="S3.SS2.p5.7.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p5.7.m1.1.1.2" xref="S3.SS2.p5.7.m1.1.1.2.cmml">â„’</mi><mrow id="S3.SS2.p5.7.m1.1.1.3" xref="S3.SS2.p5.7.m1.1.1.3.cmml"><mi id="S3.SS2.p5.7.m1.1.1.3.2" xref="S3.SS2.p5.7.m1.1.1.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.7.m1.1.1.3.1" xref="S3.SS2.p5.7.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p5.7.m1.1.1.3.3" xref="S3.SS2.p5.7.m1.1.1.3.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.7.m1.1.1.3.1a" xref="S3.SS2.p5.7.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p5.7.m1.1.1.3.4" xref="S3.SS2.p5.7.m1.1.1.3.4.cmml">N</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.7.m1.1b"><apply id="S3.SS2.p5.7.m1.1.1.cmml" xref="S3.SS2.p5.7.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.7.m1.1.1.1.cmml" xref="S3.SS2.p5.7.m1.1.1">subscript</csymbol><ci id="S3.SS2.p5.7.m1.1.1.2.cmml" xref="S3.SS2.p5.7.m1.1.1.2">â„’</ci><apply id="S3.SS2.p5.7.m1.1.1.3.cmml" xref="S3.SS2.p5.7.m1.1.1.3"><times id="S3.SS2.p5.7.m1.1.1.3.1.cmml" xref="S3.SS2.p5.7.m1.1.1.3.1"></times><ci id="S3.SS2.p5.7.m1.1.1.3.2.cmml" xref="S3.SS2.p5.7.m1.1.1.3.2">ğ‘…</ci><ci id="S3.SS2.p5.7.m1.1.1.3.3.cmml" xref="S3.SS2.p5.7.m1.1.1.3.3">ğ‘ƒ</ci><ci id="S3.SS2.p5.7.m1.1.1.3.4.cmml" xref="S3.SS2.p5.7.m1.1.1.3.4">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.7.m1.1c">\mathcal{L}_{RPN}</annotation></semantics></math> is the RPN loss from Faster R-CNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>.
The classification loss <math id="S3.SS2.p5.8.m2.1" class="ltx_Math" alttext="\mathcal{L}^{p}_{cls}" display="inline"><semantics id="S3.SS2.p5.8.m2.1a"><msubsup id="S3.SS2.p5.8.m2.1.1" xref="S3.SS2.p5.8.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p5.8.m2.1.1.2.2" xref="S3.SS2.p5.8.m2.1.1.2.2.cmml">â„’</mi><mrow id="S3.SS2.p5.8.m2.1.1.3" xref="S3.SS2.p5.8.m2.1.1.3.cmml"><mi id="S3.SS2.p5.8.m2.1.1.3.2" xref="S3.SS2.p5.8.m2.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.8.m2.1.1.3.1" xref="S3.SS2.p5.8.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p5.8.m2.1.1.3.3" xref="S3.SS2.p5.8.m2.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.8.m2.1.1.3.1a" xref="S3.SS2.p5.8.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p5.8.m2.1.1.3.4" xref="S3.SS2.p5.8.m2.1.1.3.4.cmml">s</mi></mrow><mi id="S3.SS2.p5.8.m2.1.1.2.3" xref="S3.SS2.p5.8.m2.1.1.2.3.cmml">p</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.8.m2.1b"><apply id="S3.SS2.p5.8.m2.1.1.cmml" xref="S3.SS2.p5.8.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.8.m2.1.1.1.cmml" xref="S3.SS2.p5.8.m2.1.1">subscript</csymbol><apply id="S3.SS2.p5.8.m2.1.1.2.cmml" xref="S3.SS2.p5.8.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.8.m2.1.1.2.1.cmml" xref="S3.SS2.p5.8.m2.1.1">superscript</csymbol><ci id="S3.SS2.p5.8.m2.1.1.2.2.cmml" xref="S3.SS2.p5.8.m2.1.1.2.2">â„’</ci><ci id="S3.SS2.p5.8.m2.1.1.2.3.cmml" xref="S3.SS2.p5.8.m2.1.1.2.3">ğ‘</ci></apply><apply id="S3.SS2.p5.8.m2.1.1.3.cmml" xref="S3.SS2.p5.8.m2.1.1.3"><times id="S3.SS2.p5.8.m2.1.1.3.1.cmml" xref="S3.SS2.p5.8.m2.1.1.3.1"></times><ci id="S3.SS2.p5.8.m2.1.1.3.2.cmml" xref="S3.SS2.p5.8.m2.1.1.3.2">ğ‘</ci><ci id="S3.SS2.p5.8.m2.1.1.3.3.cmml" xref="S3.SS2.p5.8.m2.1.1.3.3">ğ‘™</ci><ci id="S3.SS2.p5.8.m2.1.1.3.4.cmml" xref="S3.SS2.p5.8.m2.1.1.3.4">ğ‘ </ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.8.m2.1c">\mathcal{L}^{p}_{cls}</annotation></semantics></math> for each part <math id="S3.SS2.p5.9.m3.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S3.SS2.p5.9.m3.1a"><mi id="S3.SS2.p5.9.m3.1.1" xref="S3.SS2.p5.9.m3.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.9.m3.1b"><ci id="S3.SS2.p5.9.m3.1.1.cmml" xref="S3.SS2.p5.9.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.9.m3.1c">p</annotation></semantics></math> is a standard softmax averaged over all boxes.
If a box sufficiently overlaps with a ground-truth box, its ground-truth label is obtained by finding the closest anchor-pose from the ground-truth pose. Otherwise it is assigned a background label, <em id="S3.SS2.p5.9.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS2.p5.9.2" class="ltx_text"></span>, 0.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p">The regression loss <math id="S3.SS2.p6.1.m1.1" class="ltx_Math" alttext="\mathcal{L}^{p}_{reg}" display="inline"><semantics id="S3.SS2.p6.1.m1.1a"><msubsup id="S3.SS2.p6.1.m1.1.1" xref="S3.SS2.p6.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p6.1.m1.1.1.2.2" xref="S3.SS2.p6.1.m1.1.1.2.2.cmml">â„’</mi><mrow id="S3.SS2.p6.1.m1.1.1.3" xref="S3.SS2.p6.1.m1.1.1.3.cmml"><mi id="S3.SS2.p6.1.m1.1.1.3.2" xref="S3.SS2.p6.1.m1.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.1.m1.1.1.3.1" xref="S3.SS2.p6.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p6.1.m1.1.1.3.3" xref="S3.SS2.p6.1.m1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.1.m1.1.1.3.1a" xref="S3.SS2.p6.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p6.1.m1.1.1.3.4" xref="S3.SS2.p6.1.m1.1.1.3.4.cmml">g</mi></mrow><mi id="S3.SS2.p6.1.m1.1.1.2.3" xref="S3.SS2.p6.1.m1.1.1.2.3.cmml">p</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.1.m1.1b"><apply id="S3.SS2.p6.1.m1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.1.m1.1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1">subscript</csymbol><apply id="S3.SS2.p6.1.m1.1.1.2.cmml" xref="S3.SS2.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.1.m1.1.1.2.1.cmml" xref="S3.SS2.p6.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.p6.1.m1.1.1.2.2.cmml" xref="S3.SS2.p6.1.m1.1.1.2.2">â„’</ci><ci id="S3.SS2.p6.1.m1.1.1.2.3.cmml" xref="S3.SS2.p6.1.m1.1.1.2.3">ğ‘</ci></apply><apply id="S3.SS2.p6.1.m1.1.1.3.cmml" xref="S3.SS2.p6.1.m1.1.1.3"><times id="S3.SS2.p6.1.m1.1.1.3.1.cmml" xref="S3.SS2.p6.1.m1.1.1.3.1"></times><ci id="S3.SS2.p6.1.m1.1.1.3.2.cmml" xref="S3.SS2.p6.1.m1.1.1.3.2">ğ‘Ÿ</ci><ci id="S3.SS2.p6.1.m1.1.1.3.3.cmml" xref="S3.SS2.p6.1.m1.1.1.3.3">ğ‘’</ci><ci id="S3.SS2.p6.1.m1.1.1.3.4.cmml" xref="S3.SS2.p6.1.m1.1.1.3.4">ğ‘”</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.1.m1.1c">\mathcal{L}^{p}_{reg}</annotation></semantics></math> is a standard L1 loss on the offset between ground-truth 2D-3D poses and their ground-truth anchor-poses, averaged over all boxes.
Note that the regression is class-specific, and the loss is only applied on the output of the regressor specific to the ground-truth class for each positive box.</p>
</div>
<div id="S3.SS2.p7" class="ltx_para">
<p id="S3.SS2.p7.3" class="ltx_p">The distillation loss <math id="S3.SS2.p7.1.m1.1" class="ltx_Math" alttext="\mathcal{L}^{p}_{dist}" display="inline"><semantics id="S3.SS2.p7.1.m1.1a"><msubsup id="S3.SS2.p7.1.m1.1.1" xref="S3.SS2.p7.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p7.1.m1.1.1.2.2" xref="S3.SS2.p7.1.m1.1.1.2.2.cmml">â„’</mi><mrow id="S3.SS2.p7.1.m1.1.1.3" xref="S3.SS2.p7.1.m1.1.1.3.cmml"><mi id="S3.SS2.p7.1.m1.1.1.3.2" xref="S3.SS2.p7.1.m1.1.1.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.1.m1.1.1.3.1" xref="S3.SS2.p7.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.1.m1.1.1.3.3" xref="S3.SS2.p7.1.m1.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.1.m1.1.1.3.1a" xref="S3.SS2.p7.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.1.m1.1.1.3.4" xref="S3.SS2.p7.1.m1.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.1.m1.1.1.3.1b" xref="S3.SS2.p7.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.1.m1.1.1.3.5" xref="S3.SS2.p7.1.m1.1.1.3.5.cmml">t</mi></mrow><mi id="S3.SS2.p7.1.m1.1.1.2.3" xref="S3.SS2.p7.1.m1.1.1.2.3.cmml">p</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.1.m1.1b"><apply id="S3.SS2.p7.1.m1.1.1.cmml" xref="S3.SS2.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.1.m1.1.1.1.cmml" xref="S3.SS2.p7.1.m1.1.1">subscript</csymbol><apply id="S3.SS2.p7.1.m1.1.1.2.cmml" xref="S3.SS2.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.1.m1.1.1.2.1.cmml" xref="S3.SS2.p7.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.p7.1.m1.1.1.2.2.cmml" xref="S3.SS2.p7.1.m1.1.1.2.2">â„’</ci><ci id="S3.SS2.p7.1.m1.1.1.2.3.cmml" xref="S3.SS2.p7.1.m1.1.1.2.3">ğ‘</ci></apply><apply id="S3.SS2.p7.1.m1.1.1.3.cmml" xref="S3.SS2.p7.1.m1.1.1.3"><times id="S3.SS2.p7.1.m1.1.1.3.1.cmml" xref="S3.SS2.p7.1.m1.1.1.3.1"></times><ci id="S3.SS2.p7.1.m1.1.1.3.2.cmml" xref="S3.SS2.p7.1.m1.1.1.3.2">ğ‘‘</ci><ci id="S3.SS2.p7.1.m1.1.1.3.3.cmml" xref="S3.SS2.p7.1.m1.1.1.3.3">ğ‘–</ci><ci id="S3.SS2.p7.1.m1.1.1.3.4.cmml" xref="S3.SS2.p7.1.m1.1.1.3.4">ğ‘ </ci><ci id="S3.SS2.p7.1.m1.1.1.3.5.cmml" xref="S3.SS2.p7.1.m1.1.1.3.5">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.1.m1.1c">\mathcal{L}^{p}_{dist}</annotation></semantics></math> is composed of two elements, one for the distillation of the classification scores <math id="S3.SS2.p7.2.m2.1" class="ltx_Math" alttext="\mathcal{L}^{p}_{dist\_cls}" display="inline"><semantics id="S3.SS2.p7.2.m2.1a"><msubsup id="S3.SS2.p7.2.m2.1.1" xref="S3.SS2.p7.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p7.2.m2.1.1.2.2" xref="S3.SS2.p7.2.m2.1.1.2.2.cmml">â„’</mi><mrow id="S3.SS2.p7.2.m2.1.1.3" xref="S3.SS2.p7.2.m2.1.1.3.cmml"><mi id="S3.SS2.p7.2.m2.1.1.3.2" xref="S3.SS2.p7.2.m2.1.1.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.2.m2.1.1.3.1" xref="S3.SS2.p7.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.2.m2.1.1.3.3" xref="S3.SS2.p7.2.m2.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.2.m2.1.1.3.1a" xref="S3.SS2.p7.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.2.m2.1.1.3.4" xref="S3.SS2.p7.2.m2.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.2.m2.1.1.3.1b" xref="S3.SS2.p7.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.2.m2.1.1.3.5" xref="S3.SS2.p7.2.m2.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.2.m2.1.1.3.1c" xref="S3.SS2.p7.2.m2.1.1.3.1.cmml">â€‹</mo><mi mathvariant="normal" id="S3.SS2.p7.2.m2.1.1.3.6" xref="S3.SS2.p7.2.m2.1.1.3.6.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.2.m2.1.1.3.1d" xref="S3.SS2.p7.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.2.m2.1.1.3.7" xref="S3.SS2.p7.2.m2.1.1.3.7.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.2.m2.1.1.3.1e" xref="S3.SS2.p7.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.2.m2.1.1.3.8" xref="S3.SS2.p7.2.m2.1.1.3.8.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.2.m2.1.1.3.1f" xref="S3.SS2.p7.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.2.m2.1.1.3.9" xref="S3.SS2.p7.2.m2.1.1.3.9.cmml">s</mi></mrow><mi id="S3.SS2.p7.2.m2.1.1.2.3" xref="S3.SS2.p7.2.m2.1.1.2.3.cmml">p</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.2.m2.1b"><apply id="S3.SS2.p7.2.m2.1.1.cmml" xref="S3.SS2.p7.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.2.m2.1.1.1.cmml" xref="S3.SS2.p7.2.m2.1.1">subscript</csymbol><apply id="S3.SS2.p7.2.m2.1.1.2.cmml" xref="S3.SS2.p7.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.2.m2.1.1.2.1.cmml" xref="S3.SS2.p7.2.m2.1.1">superscript</csymbol><ci id="S3.SS2.p7.2.m2.1.1.2.2.cmml" xref="S3.SS2.p7.2.m2.1.1.2.2">â„’</ci><ci id="S3.SS2.p7.2.m2.1.1.2.3.cmml" xref="S3.SS2.p7.2.m2.1.1.2.3">ğ‘</ci></apply><apply id="S3.SS2.p7.2.m2.1.1.3.cmml" xref="S3.SS2.p7.2.m2.1.1.3"><times id="S3.SS2.p7.2.m2.1.1.3.1.cmml" xref="S3.SS2.p7.2.m2.1.1.3.1"></times><ci id="S3.SS2.p7.2.m2.1.1.3.2.cmml" xref="S3.SS2.p7.2.m2.1.1.3.2">ğ‘‘</ci><ci id="S3.SS2.p7.2.m2.1.1.3.3.cmml" xref="S3.SS2.p7.2.m2.1.1.3.3">ğ‘–</ci><ci id="S3.SS2.p7.2.m2.1.1.3.4.cmml" xref="S3.SS2.p7.2.m2.1.1.3.4">ğ‘ </ci><ci id="S3.SS2.p7.2.m2.1.1.3.5.cmml" xref="S3.SS2.p7.2.m2.1.1.3.5">ğ‘¡</ci><ci id="S3.SS2.p7.2.m2.1.1.3.6.cmml" xref="S3.SS2.p7.2.m2.1.1.3.6">_</ci><ci id="S3.SS2.p7.2.m2.1.1.3.7.cmml" xref="S3.SS2.p7.2.m2.1.1.3.7">ğ‘</ci><ci id="S3.SS2.p7.2.m2.1.1.3.8.cmml" xref="S3.SS2.p7.2.m2.1.1.3.8">ğ‘™</ci><ci id="S3.SS2.p7.2.m2.1.1.3.9.cmml" xref="S3.SS2.p7.2.m2.1.1.3.9">ğ‘ </ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.2.m2.1c">\mathcal{L}^{p}_{dist\_cls}</annotation></semantics></math> and another one, <math id="S3.SS2.p7.3.m3.1" class="ltx_Math" alttext="\mathcal{L}^{p}_{dist\_reg}" display="inline"><semantics id="S3.SS2.p7.3.m3.1a"><msubsup id="S3.SS2.p7.3.m3.1.1" xref="S3.SS2.p7.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p7.3.m3.1.1.2.2" xref="S3.SS2.p7.3.m3.1.1.2.2.cmml">â„’</mi><mrow id="S3.SS2.p7.3.m3.1.1.3" xref="S3.SS2.p7.3.m3.1.1.3.cmml"><mi id="S3.SS2.p7.3.m3.1.1.3.2" xref="S3.SS2.p7.3.m3.1.1.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.3.m3.1.1.3.1" xref="S3.SS2.p7.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.3.m3.1.1.3.3" xref="S3.SS2.p7.3.m3.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.3.m3.1.1.3.1a" xref="S3.SS2.p7.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.3.m3.1.1.3.4" xref="S3.SS2.p7.3.m3.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.3.m3.1.1.3.1b" xref="S3.SS2.p7.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.3.m3.1.1.3.5" xref="S3.SS2.p7.3.m3.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.3.m3.1.1.3.1c" xref="S3.SS2.p7.3.m3.1.1.3.1.cmml">â€‹</mo><mi mathvariant="normal" id="S3.SS2.p7.3.m3.1.1.3.6" xref="S3.SS2.p7.3.m3.1.1.3.6.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.3.m3.1.1.3.1d" xref="S3.SS2.p7.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.3.m3.1.1.3.7" xref="S3.SS2.p7.3.m3.1.1.3.7.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.3.m3.1.1.3.1e" xref="S3.SS2.p7.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.3.m3.1.1.3.8" xref="S3.SS2.p7.3.m3.1.1.3.8.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.3.m3.1.1.3.1f" xref="S3.SS2.p7.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.3.m3.1.1.3.9" xref="S3.SS2.p7.3.m3.1.1.3.9.cmml">g</mi></mrow><mi id="S3.SS2.p7.3.m3.1.1.2.3" xref="S3.SS2.p7.3.m3.1.1.2.3.cmml">p</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.3.m3.1b"><apply id="S3.SS2.p7.3.m3.1.1.cmml" xref="S3.SS2.p7.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.3.m3.1.1.1.cmml" xref="S3.SS2.p7.3.m3.1.1">subscript</csymbol><apply id="S3.SS2.p7.3.m3.1.1.2.cmml" xref="S3.SS2.p7.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.3.m3.1.1.2.1.cmml" xref="S3.SS2.p7.3.m3.1.1">superscript</csymbol><ci id="S3.SS2.p7.3.m3.1.1.2.2.cmml" xref="S3.SS2.p7.3.m3.1.1.2.2">â„’</ci><ci id="S3.SS2.p7.3.m3.1.1.2.3.cmml" xref="S3.SS2.p7.3.m3.1.1.2.3">ğ‘</ci></apply><apply id="S3.SS2.p7.3.m3.1.1.3.cmml" xref="S3.SS2.p7.3.m3.1.1.3"><times id="S3.SS2.p7.3.m3.1.1.3.1.cmml" xref="S3.SS2.p7.3.m3.1.1.3.1"></times><ci id="S3.SS2.p7.3.m3.1.1.3.2.cmml" xref="S3.SS2.p7.3.m3.1.1.3.2">ğ‘‘</ci><ci id="S3.SS2.p7.3.m3.1.1.3.3.cmml" xref="S3.SS2.p7.3.m3.1.1.3.3">ğ‘–</ci><ci id="S3.SS2.p7.3.m3.1.1.3.4.cmml" xref="S3.SS2.p7.3.m3.1.1.3.4">ğ‘ </ci><ci id="S3.SS2.p7.3.m3.1.1.3.5.cmml" xref="S3.SS2.p7.3.m3.1.1.3.5">ğ‘¡</ci><ci id="S3.SS2.p7.3.m3.1.1.3.6.cmml" xref="S3.SS2.p7.3.m3.1.1.3.6">_</ci><ci id="S3.SS2.p7.3.m3.1.1.3.7.cmml" xref="S3.SS2.p7.3.m3.1.1.3.7">ğ‘Ÿ</ci><ci id="S3.SS2.p7.3.m3.1.1.3.8.cmml" xref="S3.SS2.p7.3.m3.1.1.3.8">ğ‘’</ci><ci id="S3.SS2.p7.3.m3.1.1.3.9.cmml" xref="S3.SS2.p7.3.m3.1.1.3.9">ğ‘”</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.3.m3.1c">\mathcal{L}^{p}_{dist\_reg}</annotation></semantics></math>, for the regression:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.1" class="ltx_Math" alttext="\mathcal{L}^{p}_{dist}=\mathcal{L}^{p}_{dist\_cls}+\mathcal{L}^{p}_{dist\_reg}~{}~{}." display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><msubsup id="S3.E3.m1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.1.1.1.1.2.2.2" xref="S3.E3.m1.1.1.1.1.2.2.2.cmml">â„’</mi><mrow id="S3.E3.m1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.2.3.cmml"><mi id="S3.E3.m1.1.1.1.1.2.3.2" xref="S3.E3.m1.1.1.1.1.2.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.2.3.1" xref="S3.E3.m1.1.1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.1.1.2.3.3" xref="S3.E3.m1.1.1.1.1.2.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.2.3.1a" xref="S3.E3.m1.1.1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.1.1.2.3.4" xref="S3.E3.m1.1.1.1.1.2.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.2.3.1b" xref="S3.E3.m1.1.1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.1.1.2.3.5" xref="S3.E3.m1.1.1.1.1.2.3.5.cmml">t</mi></mrow><mi id="S3.E3.m1.1.1.1.1.2.2.3" xref="S3.E3.m1.1.1.1.1.2.2.3.cmml">p</mi></msubsup><mo id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.3.cmml"><msubsup id="S3.E3.m1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.1.1.1.1.3.2.2.2" xref="S3.E3.m1.1.1.1.1.3.2.2.2.cmml">â„’</mi><mrow id="S3.E3.m1.1.1.1.1.3.2.3" xref="S3.E3.m1.1.1.1.1.3.2.3.cmml"><mi id="S3.E3.m1.1.1.1.1.3.2.3.2" xref="S3.E3.m1.1.1.1.1.3.2.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.3.2.3.1" xref="S3.E3.m1.1.1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.1.1.3.2.3.3" xref="S3.E3.m1.1.1.1.1.3.2.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.3.2.3.1a" xref="S3.E3.m1.1.1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.1.1.3.2.3.4" xref="S3.E3.m1.1.1.1.1.3.2.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.3.2.3.1b" xref="S3.E3.m1.1.1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.1.1.3.2.3.5" xref="S3.E3.m1.1.1.1.1.3.2.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.3.2.3.1c" xref="S3.E3.m1.1.1.1.1.3.2.3.1.cmml">â€‹</mo><mi mathvariant="normal" id="S3.E3.m1.1.1.1.1.3.2.3.6" xref="S3.E3.m1.1.1.1.1.3.2.3.6.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.3.2.3.1d" xref="S3.E3.m1.1.1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.1.1.3.2.3.7" xref="S3.E3.m1.1.1.1.1.3.2.3.7.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.3.2.3.1e" xref="S3.E3.m1.1.1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.1.1.3.2.3.8" xref="S3.E3.m1.1.1.1.1.3.2.3.8.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.3.2.3.1f" xref="S3.E3.m1.1.1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.1.1.3.2.3.9" xref="S3.E3.m1.1.1.1.1.3.2.3.9.cmml">s</mi></mrow><mi id="S3.E3.m1.1.1.1.1.3.2.2.3" xref="S3.E3.m1.1.1.1.1.3.2.2.3.cmml">p</mi></msubsup><mo id="S3.E3.m1.1.1.1.1.3.1" xref="S3.E3.m1.1.1.1.1.3.1.cmml">+</mo><msubsup id="S3.E3.m1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.1.1.1.1.3.3.2.2" xref="S3.E3.m1.1.1.1.1.3.3.2.2.cmml">â„’</mi><mrow id="S3.E3.m1.1.1.1.1.3.3.3" xref="S3.E3.m1.1.1.1.1.3.3.3.cmml"><mi id="S3.E3.m1.1.1.1.1.3.3.3.2" xref="S3.E3.m1.1.1.1.1.3.3.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.3.3.3.1" xref="S3.E3.m1.1.1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.1.1.3.3.3.3" xref="S3.E3.m1.1.1.1.1.3.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.3.3.3.1a" xref="S3.E3.m1.1.1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.1.1.3.3.3.4" xref="S3.E3.m1.1.1.1.1.3.3.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.3.3.3.1b" xref="S3.E3.m1.1.1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.1.1.3.3.3.5" xref="S3.E3.m1.1.1.1.1.3.3.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.3.3.3.1c" xref="S3.E3.m1.1.1.1.1.3.3.3.1.cmml">â€‹</mo><mi mathvariant="normal" id="S3.E3.m1.1.1.1.1.3.3.3.6" xref="S3.E3.m1.1.1.1.1.3.3.3.6.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.3.3.3.1d" xref="S3.E3.m1.1.1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.1.1.3.3.3.7" xref="S3.E3.m1.1.1.1.1.3.3.3.7.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.3.3.3.1e" xref="S3.E3.m1.1.1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.1.1.3.3.3.8" xref="S3.E3.m1.1.1.1.1.3.3.3.8.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.3.3.3.1f" xref="S3.E3.m1.1.1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.1.1.3.3.3.9" xref="S3.E3.m1.1.1.1.1.3.3.3.9.cmml">g</mi></mrow><mi id="S3.E3.m1.1.1.1.1.3.3.2.3" xref="S3.E3.m1.1.1.1.1.3.3.2.3.cmml">p</mi></msubsup></mrow></mrow><mo lspace="0em" id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><eq id="S3.E3.m1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"></eq><apply id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.2">subscript</csymbol><apply id="S3.E3.m1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.2">superscript</csymbol><ci id="S3.E3.m1.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2">â„’</ci><ci id="S3.E3.m1.1.1.1.1.2.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.2.3">ğ‘</ci></apply><apply id="S3.E3.m1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.3"><times id="S3.E3.m1.1.1.1.1.2.3.1.cmml" xref="S3.E3.m1.1.1.1.1.2.3.1"></times><ci id="S3.E3.m1.1.1.1.1.2.3.2.cmml" xref="S3.E3.m1.1.1.1.1.2.3.2">ğ‘‘</ci><ci id="S3.E3.m1.1.1.1.1.2.3.3.cmml" xref="S3.E3.m1.1.1.1.1.2.3.3">ğ‘–</ci><ci id="S3.E3.m1.1.1.1.1.2.3.4.cmml" xref="S3.E3.m1.1.1.1.1.2.3.4">ğ‘ </ci><ci id="S3.E3.m1.1.1.1.1.2.3.5.cmml" xref="S3.E3.m1.1.1.1.1.2.3.5">ğ‘¡</ci></apply></apply><apply id="S3.E3.m1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3"><plus id="S3.E3.m1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.3.1"></plus><apply id="S3.E3.m1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.1.1.3.2">subscript</csymbol><apply id="S3.E3.m1.1.1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.3.2">superscript</csymbol><ci id="S3.E3.m1.1.1.1.1.3.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.3.2.2.2">â„’</ci><ci id="S3.E3.m1.1.1.1.1.3.2.2.3.cmml" xref="S3.E3.m1.1.1.1.1.3.2.2.3">ğ‘</ci></apply><apply id="S3.E3.m1.1.1.1.1.3.2.3.cmml" xref="S3.E3.m1.1.1.1.1.3.2.3"><times id="S3.E3.m1.1.1.1.1.3.2.3.1.cmml" xref="S3.E3.m1.1.1.1.1.3.2.3.1"></times><ci id="S3.E3.m1.1.1.1.1.3.2.3.2.cmml" xref="S3.E3.m1.1.1.1.1.3.2.3.2">ğ‘‘</ci><ci id="S3.E3.m1.1.1.1.1.3.2.3.3.cmml" xref="S3.E3.m1.1.1.1.1.3.2.3.3">ğ‘–</ci><ci id="S3.E3.m1.1.1.1.1.3.2.3.4.cmml" xref="S3.E3.m1.1.1.1.1.3.2.3.4">ğ‘ </ci><ci id="S3.E3.m1.1.1.1.1.3.2.3.5.cmml" xref="S3.E3.m1.1.1.1.1.3.2.3.5">ğ‘¡</ci><ci id="S3.E3.m1.1.1.1.1.3.2.3.6.cmml" xref="S3.E3.m1.1.1.1.1.3.2.3.6">_</ci><ci id="S3.E3.m1.1.1.1.1.3.2.3.7.cmml" xref="S3.E3.m1.1.1.1.1.3.2.3.7">ğ‘</ci><ci id="S3.E3.m1.1.1.1.1.3.2.3.8.cmml" xref="S3.E3.m1.1.1.1.1.3.2.3.8">ğ‘™</ci><ci id="S3.E3.m1.1.1.1.1.3.2.3.9.cmml" xref="S3.E3.m1.1.1.1.1.3.2.3.9">ğ‘ </ci></apply></apply><apply id="S3.E3.m1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.1.1.3.3">subscript</csymbol><apply id="S3.E3.m1.1.1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.3.2.1.cmml" xref="S3.E3.m1.1.1.1.1.3.3">superscript</csymbol><ci id="S3.E3.m1.1.1.1.1.3.3.2.2.cmml" xref="S3.E3.m1.1.1.1.1.3.3.2.2">â„’</ci><ci id="S3.E3.m1.1.1.1.1.3.3.2.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3.2.3">ğ‘</ci></apply><apply id="S3.E3.m1.1.1.1.1.3.3.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3.3"><times id="S3.E3.m1.1.1.1.1.3.3.3.1.cmml" xref="S3.E3.m1.1.1.1.1.3.3.3.1"></times><ci id="S3.E3.m1.1.1.1.1.3.3.3.2.cmml" xref="S3.E3.m1.1.1.1.1.3.3.3.2">ğ‘‘</ci><ci id="S3.E3.m1.1.1.1.1.3.3.3.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3.3.3">ğ‘–</ci><ci id="S3.E3.m1.1.1.1.1.3.3.3.4.cmml" xref="S3.E3.m1.1.1.1.1.3.3.3.4">ğ‘ </ci><ci id="S3.E3.m1.1.1.1.1.3.3.3.5.cmml" xref="S3.E3.m1.1.1.1.1.3.3.3.5">ğ‘¡</ci><ci id="S3.E3.m1.1.1.1.1.3.3.3.6.cmml" xref="S3.E3.m1.1.1.1.1.3.3.3.6">_</ci><ci id="S3.E3.m1.1.1.1.1.3.3.3.7.cmml" xref="S3.E3.m1.1.1.1.1.3.3.3.7">ğ‘Ÿ</ci><ci id="S3.E3.m1.1.1.1.1.3.3.3.8.cmml" xref="S3.E3.m1.1.1.1.1.3.3.3.8">ğ‘’</ci><ci id="S3.E3.m1.1.1.1.1.3.3.3.9.cmml" xref="S3.E3.m1.1.1.1.1.3.3.3.9">ğ‘”</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\mathcal{L}^{p}_{dist}=\mathcal{L}^{p}_{dist\_cls}+\mathcal{L}^{p}_{dist\_reg}~{}~{}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p7.11" class="ltx_p">Given a box, the goal of the distillation loss is to make the output of the whole-body network as close as possible to the output of the part expert <math id="S3.SS2.p7.4.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S3.SS2.p7.4.m1.1a"><mi id="S3.SS2.p7.4.m1.1.1" xref="S3.SS2.p7.4.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.4.m1.1b"><ci id="S3.SS2.p7.4.m1.1.1.cmml" xref="S3.SS2.p7.4.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.4.m1.1c">p</annotation></semantics></math>.
The classification component <math id="S3.SS2.p7.5.m2.1" class="ltx_Math" alttext="\mathcal{L}^{p}_{dist\_cls}" display="inline"><semantics id="S3.SS2.p7.5.m2.1a"><msubsup id="S3.SS2.p7.5.m2.1.1" xref="S3.SS2.p7.5.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p7.5.m2.1.1.2.2" xref="S3.SS2.p7.5.m2.1.1.2.2.cmml">â„’</mi><mrow id="S3.SS2.p7.5.m2.1.1.3" xref="S3.SS2.p7.5.m2.1.1.3.cmml"><mi id="S3.SS2.p7.5.m2.1.1.3.2" xref="S3.SS2.p7.5.m2.1.1.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.5.m2.1.1.3.1" xref="S3.SS2.p7.5.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.5.m2.1.1.3.3" xref="S3.SS2.p7.5.m2.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.5.m2.1.1.3.1a" xref="S3.SS2.p7.5.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.5.m2.1.1.3.4" xref="S3.SS2.p7.5.m2.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.5.m2.1.1.3.1b" xref="S3.SS2.p7.5.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.5.m2.1.1.3.5" xref="S3.SS2.p7.5.m2.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.5.m2.1.1.3.1c" xref="S3.SS2.p7.5.m2.1.1.3.1.cmml">â€‹</mo><mi mathvariant="normal" id="S3.SS2.p7.5.m2.1.1.3.6" xref="S3.SS2.p7.5.m2.1.1.3.6.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.5.m2.1.1.3.1d" xref="S3.SS2.p7.5.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.5.m2.1.1.3.7" xref="S3.SS2.p7.5.m2.1.1.3.7.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.5.m2.1.1.3.1e" xref="S3.SS2.p7.5.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.5.m2.1.1.3.8" xref="S3.SS2.p7.5.m2.1.1.3.8.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.5.m2.1.1.3.1f" xref="S3.SS2.p7.5.m2.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.5.m2.1.1.3.9" xref="S3.SS2.p7.5.m2.1.1.3.9.cmml">s</mi></mrow><mi id="S3.SS2.p7.5.m2.1.1.2.3" xref="S3.SS2.p7.5.m2.1.1.2.3.cmml">p</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.5.m2.1b"><apply id="S3.SS2.p7.5.m2.1.1.cmml" xref="S3.SS2.p7.5.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.5.m2.1.1.1.cmml" xref="S3.SS2.p7.5.m2.1.1">subscript</csymbol><apply id="S3.SS2.p7.5.m2.1.1.2.cmml" xref="S3.SS2.p7.5.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.5.m2.1.1.2.1.cmml" xref="S3.SS2.p7.5.m2.1.1">superscript</csymbol><ci id="S3.SS2.p7.5.m2.1.1.2.2.cmml" xref="S3.SS2.p7.5.m2.1.1.2.2">â„’</ci><ci id="S3.SS2.p7.5.m2.1.1.2.3.cmml" xref="S3.SS2.p7.5.m2.1.1.2.3">ğ‘</ci></apply><apply id="S3.SS2.p7.5.m2.1.1.3.cmml" xref="S3.SS2.p7.5.m2.1.1.3"><times id="S3.SS2.p7.5.m2.1.1.3.1.cmml" xref="S3.SS2.p7.5.m2.1.1.3.1"></times><ci id="S3.SS2.p7.5.m2.1.1.3.2.cmml" xref="S3.SS2.p7.5.m2.1.1.3.2">ğ‘‘</ci><ci id="S3.SS2.p7.5.m2.1.1.3.3.cmml" xref="S3.SS2.p7.5.m2.1.1.3.3">ğ‘–</ci><ci id="S3.SS2.p7.5.m2.1.1.3.4.cmml" xref="S3.SS2.p7.5.m2.1.1.3.4">ğ‘ </ci><ci id="S3.SS2.p7.5.m2.1.1.3.5.cmml" xref="S3.SS2.p7.5.m2.1.1.3.5">ğ‘¡</ci><ci id="S3.SS2.p7.5.m2.1.1.3.6.cmml" xref="S3.SS2.p7.5.m2.1.1.3.6">_</ci><ci id="S3.SS2.p7.5.m2.1.1.3.7.cmml" xref="S3.SS2.p7.5.m2.1.1.3.7">ğ‘</ci><ci id="S3.SS2.p7.5.m2.1.1.3.8.cmml" xref="S3.SS2.p7.5.m2.1.1.3.8">ğ‘™</ci><ci id="S3.SS2.p7.5.m2.1.1.3.9.cmml" xref="S3.SS2.p7.5.m2.1.1.3.9">ğ‘ </ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.5.m2.1c">\mathcal{L}^{p}_{dist\_cls}</annotation></semantics></math> is a standard distillation loss between the predictions produced by the corresponding part expert and those estimated by the whole-body model for part <math id="S3.SS2.p7.6.m3.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S3.SS2.p7.6.m3.1a"><mi id="S3.SS2.p7.6.m3.1.1" xref="S3.SS2.p7.6.m3.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.6.m3.1b"><ci id="S3.SS2.p7.6.m3.1.1.cmml" xref="S3.SS2.p7.6.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.6.m3.1c">p</annotation></semantics></math>.
In other words, <math id="S3.SS2.p7.7.m4.1" class="ltx_Math" alttext="\mathcal{L}^{p}_{dist\_cls}" display="inline"><semantics id="S3.SS2.p7.7.m4.1a"><msubsup id="S3.SS2.p7.7.m4.1.1" xref="S3.SS2.p7.7.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p7.7.m4.1.1.2.2" xref="S3.SS2.p7.7.m4.1.1.2.2.cmml">â„’</mi><mrow id="S3.SS2.p7.7.m4.1.1.3" xref="S3.SS2.p7.7.m4.1.1.3.cmml"><mi id="S3.SS2.p7.7.m4.1.1.3.2" xref="S3.SS2.p7.7.m4.1.1.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.7.m4.1.1.3.1" xref="S3.SS2.p7.7.m4.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.7.m4.1.1.3.3" xref="S3.SS2.p7.7.m4.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.7.m4.1.1.3.1a" xref="S3.SS2.p7.7.m4.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.7.m4.1.1.3.4" xref="S3.SS2.p7.7.m4.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.7.m4.1.1.3.1b" xref="S3.SS2.p7.7.m4.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.7.m4.1.1.3.5" xref="S3.SS2.p7.7.m4.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.7.m4.1.1.3.1c" xref="S3.SS2.p7.7.m4.1.1.3.1.cmml">â€‹</mo><mi mathvariant="normal" id="S3.SS2.p7.7.m4.1.1.3.6" xref="S3.SS2.p7.7.m4.1.1.3.6.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.7.m4.1.1.3.1d" xref="S3.SS2.p7.7.m4.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.7.m4.1.1.3.7" xref="S3.SS2.p7.7.m4.1.1.3.7.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.7.m4.1.1.3.1e" xref="S3.SS2.p7.7.m4.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.7.m4.1.1.3.8" xref="S3.SS2.p7.7.m4.1.1.3.8.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.7.m4.1.1.3.1f" xref="S3.SS2.p7.7.m4.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.7.m4.1.1.3.9" xref="S3.SS2.p7.7.m4.1.1.3.9.cmml">s</mi></mrow><mi id="S3.SS2.p7.7.m4.1.1.2.3" xref="S3.SS2.p7.7.m4.1.1.2.3.cmml">p</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.7.m4.1b"><apply id="S3.SS2.p7.7.m4.1.1.cmml" xref="S3.SS2.p7.7.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.7.m4.1.1.1.cmml" xref="S3.SS2.p7.7.m4.1.1">subscript</csymbol><apply id="S3.SS2.p7.7.m4.1.1.2.cmml" xref="S3.SS2.p7.7.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.7.m4.1.1.2.1.cmml" xref="S3.SS2.p7.7.m4.1.1">superscript</csymbol><ci id="S3.SS2.p7.7.m4.1.1.2.2.cmml" xref="S3.SS2.p7.7.m4.1.1.2.2">â„’</ci><ci id="S3.SS2.p7.7.m4.1.1.2.3.cmml" xref="S3.SS2.p7.7.m4.1.1.2.3">ğ‘</ci></apply><apply id="S3.SS2.p7.7.m4.1.1.3.cmml" xref="S3.SS2.p7.7.m4.1.1.3"><times id="S3.SS2.p7.7.m4.1.1.3.1.cmml" xref="S3.SS2.p7.7.m4.1.1.3.1"></times><ci id="S3.SS2.p7.7.m4.1.1.3.2.cmml" xref="S3.SS2.p7.7.m4.1.1.3.2">ğ‘‘</ci><ci id="S3.SS2.p7.7.m4.1.1.3.3.cmml" xref="S3.SS2.p7.7.m4.1.1.3.3">ğ‘–</ci><ci id="S3.SS2.p7.7.m4.1.1.3.4.cmml" xref="S3.SS2.p7.7.m4.1.1.3.4">ğ‘ </ci><ci id="S3.SS2.p7.7.m4.1.1.3.5.cmml" xref="S3.SS2.p7.7.m4.1.1.3.5">ğ‘¡</ci><ci id="S3.SS2.p7.7.m4.1.1.3.6.cmml" xref="S3.SS2.p7.7.m4.1.1.3.6">_</ci><ci id="S3.SS2.p7.7.m4.1.1.3.7.cmml" xref="S3.SS2.p7.7.m4.1.1.3.7">ğ‘</ci><ci id="S3.SS2.p7.7.m4.1.1.3.8.cmml" xref="S3.SS2.p7.7.m4.1.1.3.8">ğ‘™</ci><ci id="S3.SS2.p7.7.m4.1.1.3.9.cmml" xref="S3.SS2.p7.7.m4.1.1.3.9">ğ‘ </ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.7.m4.1c">\mathcal{L}^{p}_{dist\_cls}</annotation></semantics></math> is the soft version of hard label loss <math id="S3.SS2.p7.8.m5.1" class="ltx_Math" alttext="\mathcal{L}^{p}_{cls}" display="inline"><semantics id="S3.SS2.p7.8.m5.1a"><msubsup id="S3.SS2.p7.8.m5.1.1" xref="S3.SS2.p7.8.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p7.8.m5.1.1.2.2" xref="S3.SS2.p7.8.m5.1.1.2.2.cmml">â„’</mi><mrow id="S3.SS2.p7.8.m5.1.1.3" xref="S3.SS2.p7.8.m5.1.1.3.cmml"><mi id="S3.SS2.p7.8.m5.1.1.3.2" xref="S3.SS2.p7.8.m5.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.8.m5.1.1.3.1" xref="S3.SS2.p7.8.m5.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.8.m5.1.1.3.3" xref="S3.SS2.p7.8.m5.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.8.m5.1.1.3.1a" xref="S3.SS2.p7.8.m5.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.8.m5.1.1.3.4" xref="S3.SS2.p7.8.m5.1.1.3.4.cmml">s</mi></mrow><mi id="S3.SS2.p7.8.m5.1.1.2.3" xref="S3.SS2.p7.8.m5.1.1.2.3.cmml">p</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.8.m5.1b"><apply id="S3.SS2.p7.8.m5.1.1.cmml" xref="S3.SS2.p7.8.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.8.m5.1.1.1.cmml" xref="S3.SS2.p7.8.m5.1.1">subscript</csymbol><apply id="S3.SS2.p7.8.m5.1.1.2.cmml" xref="S3.SS2.p7.8.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.8.m5.1.1.2.1.cmml" xref="S3.SS2.p7.8.m5.1.1">superscript</csymbol><ci id="S3.SS2.p7.8.m5.1.1.2.2.cmml" xref="S3.SS2.p7.8.m5.1.1.2.2">â„’</ci><ci id="S3.SS2.p7.8.m5.1.1.2.3.cmml" xref="S3.SS2.p7.8.m5.1.1.2.3">ğ‘</ci></apply><apply id="S3.SS2.p7.8.m5.1.1.3.cmml" xref="S3.SS2.p7.8.m5.1.1.3"><times id="S3.SS2.p7.8.m5.1.1.3.1.cmml" xref="S3.SS2.p7.8.m5.1.1.3.1"></times><ci id="S3.SS2.p7.8.m5.1.1.3.2.cmml" xref="S3.SS2.p7.8.m5.1.1.3.2">ğ‘</ci><ci id="S3.SS2.p7.8.m5.1.1.3.3.cmml" xref="S3.SS2.p7.8.m5.1.1.3.3">ğ‘™</ci><ci id="S3.SS2.p7.8.m5.1.1.3.4.cmml" xref="S3.SS2.p7.8.m5.1.1.3.4">ğ‘ </ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.8.m5.1c">\mathcal{L}^{p}_{cls}</annotation></semantics></math>.
The regression component <math id="S3.SS2.p7.9.m6.1" class="ltx_Math" alttext="\mathcal{L}^{p}_{dist\_reg}" display="inline"><semantics id="S3.SS2.p7.9.m6.1a"><msubsup id="S3.SS2.p7.9.m6.1.1" xref="S3.SS2.p7.9.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p7.9.m6.1.1.2.2" xref="S3.SS2.p7.9.m6.1.1.2.2.cmml">â„’</mi><mrow id="S3.SS2.p7.9.m6.1.1.3" xref="S3.SS2.p7.9.m6.1.1.3.cmml"><mi id="S3.SS2.p7.9.m6.1.1.3.2" xref="S3.SS2.p7.9.m6.1.1.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.9.m6.1.1.3.1" xref="S3.SS2.p7.9.m6.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.9.m6.1.1.3.3" xref="S3.SS2.p7.9.m6.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.9.m6.1.1.3.1a" xref="S3.SS2.p7.9.m6.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.9.m6.1.1.3.4" xref="S3.SS2.p7.9.m6.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.9.m6.1.1.3.1b" xref="S3.SS2.p7.9.m6.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.9.m6.1.1.3.5" xref="S3.SS2.p7.9.m6.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.9.m6.1.1.3.1c" xref="S3.SS2.p7.9.m6.1.1.3.1.cmml">â€‹</mo><mi mathvariant="normal" id="S3.SS2.p7.9.m6.1.1.3.6" xref="S3.SS2.p7.9.m6.1.1.3.6.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.9.m6.1.1.3.1d" xref="S3.SS2.p7.9.m6.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.9.m6.1.1.3.7" xref="S3.SS2.p7.9.m6.1.1.3.7.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.9.m6.1.1.3.1e" xref="S3.SS2.p7.9.m6.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.9.m6.1.1.3.8" xref="S3.SS2.p7.9.m6.1.1.3.8.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.9.m6.1.1.3.1f" xref="S3.SS2.p7.9.m6.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.9.m6.1.1.3.9" xref="S3.SS2.p7.9.m6.1.1.3.9.cmml">g</mi></mrow><mi id="S3.SS2.p7.9.m6.1.1.2.3" xref="S3.SS2.p7.9.m6.1.1.2.3.cmml">p</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.9.m6.1b"><apply id="S3.SS2.p7.9.m6.1.1.cmml" xref="S3.SS2.p7.9.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.9.m6.1.1.1.cmml" xref="S3.SS2.p7.9.m6.1.1">subscript</csymbol><apply id="S3.SS2.p7.9.m6.1.1.2.cmml" xref="S3.SS2.p7.9.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.9.m6.1.1.2.1.cmml" xref="S3.SS2.p7.9.m6.1.1">superscript</csymbol><ci id="S3.SS2.p7.9.m6.1.1.2.2.cmml" xref="S3.SS2.p7.9.m6.1.1.2.2">â„’</ci><ci id="S3.SS2.p7.9.m6.1.1.2.3.cmml" xref="S3.SS2.p7.9.m6.1.1.2.3">ğ‘</ci></apply><apply id="S3.SS2.p7.9.m6.1.1.3.cmml" xref="S3.SS2.p7.9.m6.1.1.3"><times id="S3.SS2.p7.9.m6.1.1.3.1.cmml" xref="S3.SS2.p7.9.m6.1.1.3.1"></times><ci id="S3.SS2.p7.9.m6.1.1.3.2.cmml" xref="S3.SS2.p7.9.m6.1.1.3.2">ğ‘‘</ci><ci id="S3.SS2.p7.9.m6.1.1.3.3.cmml" xref="S3.SS2.p7.9.m6.1.1.3.3">ğ‘–</ci><ci id="S3.SS2.p7.9.m6.1.1.3.4.cmml" xref="S3.SS2.p7.9.m6.1.1.3.4">ğ‘ </ci><ci id="S3.SS2.p7.9.m6.1.1.3.5.cmml" xref="S3.SS2.p7.9.m6.1.1.3.5">ğ‘¡</ci><ci id="S3.SS2.p7.9.m6.1.1.3.6.cmml" xref="S3.SS2.p7.9.m6.1.1.3.6">_</ci><ci id="S3.SS2.p7.9.m6.1.1.3.7.cmml" xref="S3.SS2.p7.9.m6.1.1.3.7">ğ‘Ÿ</ci><ci id="S3.SS2.p7.9.m6.1.1.3.8.cmml" xref="S3.SS2.p7.9.m6.1.1.3.8">ğ‘’</ci><ci id="S3.SS2.p7.9.m6.1.1.3.9.cmml" xref="S3.SS2.p7.9.m6.1.1.3.9">ğ‘”</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.9.m6.1c">\mathcal{L}^{p}_{dist\_reg}</annotation></semantics></math> is a L1 loss between the pose predicted by the part expert and the one estimated by the whole-body model for the ground-truth class.
Note that the pseudo ground-truth pose is obtained by averaging all overlapping estimates made by the part expert. While <math id="S3.SS2.p7.10.m7.1" class="ltx_Math" alttext="\mathcal{L}^{p}_{reg}" display="inline"><semantics id="S3.SS2.p7.10.m7.1a"><msubsup id="S3.SS2.p7.10.m7.1.1" xref="S3.SS2.p7.10.m7.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p7.10.m7.1.1.2.2" xref="S3.SS2.p7.10.m7.1.1.2.2.cmml">â„’</mi><mrow id="S3.SS2.p7.10.m7.1.1.3" xref="S3.SS2.p7.10.m7.1.1.3.cmml"><mi id="S3.SS2.p7.10.m7.1.1.3.2" xref="S3.SS2.p7.10.m7.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.10.m7.1.1.3.1" xref="S3.SS2.p7.10.m7.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.10.m7.1.1.3.3" xref="S3.SS2.p7.10.m7.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.10.m7.1.1.3.1a" xref="S3.SS2.p7.10.m7.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.10.m7.1.1.3.4" xref="S3.SS2.p7.10.m7.1.1.3.4.cmml">g</mi></mrow><mi id="S3.SS2.p7.10.m7.1.1.2.3" xref="S3.SS2.p7.10.m7.1.1.2.3.cmml">p</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.10.m7.1b"><apply id="S3.SS2.p7.10.m7.1.1.cmml" xref="S3.SS2.p7.10.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.10.m7.1.1.1.cmml" xref="S3.SS2.p7.10.m7.1.1">subscript</csymbol><apply id="S3.SS2.p7.10.m7.1.1.2.cmml" xref="S3.SS2.p7.10.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.10.m7.1.1.2.1.cmml" xref="S3.SS2.p7.10.m7.1.1">superscript</csymbol><ci id="S3.SS2.p7.10.m7.1.1.2.2.cmml" xref="S3.SS2.p7.10.m7.1.1.2.2">â„’</ci><ci id="S3.SS2.p7.10.m7.1.1.2.3.cmml" xref="S3.SS2.p7.10.m7.1.1.2.3">ğ‘</ci></apply><apply id="S3.SS2.p7.10.m7.1.1.3.cmml" xref="S3.SS2.p7.10.m7.1.1.3"><times id="S3.SS2.p7.10.m7.1.1.3.1.cmml" xref="S3.SS2.p7.10.m7.1.1.3.1"></times><ci id="S3.SS2.p7.10.m7.1.1.3.2.cmml" xref="S3.SS2.p7.10.m7.1.1.3.2">ğ‘Ÿ</ci><ci id="S3.SS2.p7.10.m7.1.1.3.3.cmml" xref="S3.SS2.p7.10.m7.1.1.3.3">ğ‘’</ci><ci id="S3.SS2.p7.10.m7.1.1.3.4.cmml" xref="S3.SS2.p7.10.m7.1.1.3.4">ğ‘”</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.10.m7.1c">\mathcal{L}^{p}_{reg}</annotation></semantics></math> is designed to enforce regression of this pseudo ground-truth pose, <math id="S3.SS2.p7.11.m8.1" class="ltx_Math" alttext="\mathcal{L}^{p}_{dist\_reg}" display="inline"><semantics id="S3.SS2.p7.11.m8.1a"><msubsup id="S3.SS2.p7.11.m8.1.1" xref="S3.SS2.p7.11.m8.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p7.11.m8.1.1.2.2" xref="S3.SS2.p7.11.m8.1.1.2.2.cmml">â„’</mi><mrow id="S3.SS2.p7.11.m8.1.1.3" xref="S3.SS2.p7.11.m8.1.1.3.cmml"><mi id="S3.SS2.p7.11.m8.1.1.3.2" xref="S3.SS2.p7.11.m8.1.1.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.11.m8.1.1.3.1" xref="S3.SS2.p7.11.m8.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.11.m8.1.1.3.3" xref="S3.SS2.p7.11.m8.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.11.m8.1.1.3.1a" xref="S3.SS2.p7.11.m8.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.11.m8.1.1.3.4" xref="S3.SS2.p7.11.m8.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.11.m8.1.1.3.1b" xref="S3.SS2.p7.11.m8.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.11.m8.1.1.3.5" xref="S3.SS2.p7.11.m8.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.11.m8.1.1.3.1c" xref="S3.SS2.p7.11.m8.1.1.3.1.cmml">â€‹</mo><mi mathvariant="normal" id="S3.SS2.p7.11.m8.1.1.3.6" xref="S3.SS2.p7.11.m8.1.1.3.6.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.11.m8.1.1.3.1d" xref="S3.SS2.p7.11.m8.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.11.m8.1.1.3.7" xref="S3.SS2.p7.11.m8.1.1.3.7.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.11.m8.1.1.3.1e" xref="S3.SS2.p7.11.m8.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.11.m8.1.1.3.8" xref="S3.SS2.p7.11.m8.1.1.3.8.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p7.11.m8.1.1.3.1f" xref="S3.SS2.p7.11.m8.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p7.11.m8.1.1.3.9" xref="S3.SS2.p7.11.m8.1.1.3.9.cmml">g</mi></mrow><mi id="S3.SS2.p7.11.m8.1.1.2.3" xref="S3.SS2.p7.11.m8.1.1.2.3.cmml">p</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.11.m8.1b"><apply id="S3.SS2.p7.11.m8.1.1.cmml" xref="S3.SS2.p7.11.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.11.m8.1.1.1.cmml" xref="S3.SS2.p7.11.m8.1.1">subscript</csymbol><apply id="S3.SS2.p7.11.m8.1.1.2.cmml" xref="S3.SS2.p7.11.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.11.m8.1.1.2.1.cmml" xref="S3.SS2.p7.11.m8.1.1">superscript</csymbol><ci id="S3.SS2.p7.11.m8.1.1.2.2.cmml" xref="S3.SS2.p7.11.m8.1.1.2.2">â„’</ci><ci id="S3.SS2.p7.11.m8.1.1.2.3.cmml" xref="S3.SS2.p7.11.m8.1.1.2.3">ğ‘</ci></apply><apply id="S3.SS2.p7.11.m8.1.1.3.cmml" xref="S3.SS2.p7.11.m8.1.1.3"><times id="S3.SS2.p7.11.m8.1.1.3.1.cmml" xref="S3.SS2.p7.11.m8.1.1.3.1"></times><ci id="S3.SS2.p7.11.m8.1.1.3.2.cmml" xref="S3.SS2.p7.11.m8.1.1.3.2">ğ‘‘</ci><ci id="S3.SS2.p7.11.m8.1.1.3.3.cmml" xref="S3.SS2.p7.11.m8.1.1.3.3">ğ‘–</ci><ci id="S3.SS2.p7.11.m8.1.1.3.4.cmml" xref="S3.SS2.p7.11.m8.1.1.3.4">ğ‘ </ci><ci id="S3.SS2.p7.11.m8.1.1.3.5.cmml" xref="S3.SS2.p7.11.m8.1.1.3.5">ğ‘¡</ci><ci id="S3.SS2.p7.11.m8.1.1.3.6.cmml" xref="S3.SS2.p7.11.m8.1.1.3.6">_</ci><ci id="S3.SS2.p7.11.m8.1.1.3.7.cmml" xref="S3.SS2.p7.11.m8.1.1.3.7">ğ‘Ÿ</ci><ci id="S3.SS2.p7.11.m8.1.1.3.8.cmml" xref="S3.SS2.p7.11.m8.1.1.3.8">ğ‘’</ci><ci id="S3.SS2.p7.11.m8.1.1.3.9.cmml" xref="S3.SS2.p7.11.m8.1.1.3.9">ğ‘”</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.11.m8.1c">\mathcal{L}^{p}_{dist\_reg}</annotation></semantics></math> favors regression of the exact same pose predicted by the part expert for a given box.</p>
</div>
<div id="S3.SS2.p8" class="ltx_para">
<p id="S3.SS2.p8.1" class="ltx_p">In practice, proposals generated by the RPNs of part experts and whole-body model are different
but computing distillation losses requires some proposals to coincide.
At training, we thus augment the proposals of the whole-body model with positive boxes from the part experts to compute these losses.
In summary, given a training image, we:
(a) run each part expert, keeping the positive boxes with classification scores and regression outputs,
(b) run the whole-body model, adding the positive boxes from the experts to the list of proposals.
Losses based on pseudo ground-truths are then averaged over all boxes while distillation losses are averaged only over positive boxes from the part experts.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Training details</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p"><span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_bold">Data.</span> We train our body expert on the same combination of the MPIIÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, COCOÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, LSPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, LSPEÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, Human3.6MÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> and SurrealÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> datasets augmented with pseudo 3D ground-truth annotations as inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>. We applied random horizontal flips while training for 50 epochs. We train our hand expert on the RenderedHand (RH) datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> for 100 epochs, with color jittering, random horizontal flipping and perspective transforms. <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="K_{H}=5" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><msub id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2.2" xref="S3.SS3.p1.1.m1.1.1.2.2.cmml">K</mi><mi id="S3.SS3.p1.1.m1.1.1.2.3" xref="S3.SS3.p1.1.m1.1.1.2.3.cmml">H</mi></msub><mo id="S3.SS3.p1.1.m1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><eq id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1"></eq><apply id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.2.1.cmml" xref="S3.SS3.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2.2">ğ¾</ci><ci id="S3.SS3.p1.1.m1.1.1.2.3.cmml" xref="S3.SS3.p1.1.m1.1.1.2.3">ğ»</ci></apply><cn type="integer" id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">K_{H}=5</annotation></semantics></math> anchor poses are obtained by clustering the 3D poses of right and flipped left hands from the training set. Finally, we train the face expert for 50 epochs on the Menpo datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> with random horizontal flips and color jittering during training.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">Implementation.</span>
We implement DOPE in PytorchÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, following the Faster R-CNN implementation from Torchvision. We consider a ResNet50 backboneÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.
We train it for 50 epochs, using the union of the datasets of each part expert, simply doubling the RH dataset used for hands as the number of images is significantly lower than for the other parts. The same data augmentation strategy used for training each part expert is employed for the whole-body network. We use Stochastic Gradient Descent (SGD) with a momentum of 0.9, a weight decay of <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="0.0001" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mn id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">0.0001</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><cn type="float" id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">0.0001</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">0.0001</annotation></semantics></math> and an initial learning rate of 0.02, which is divided by 10 after 30 and 45 epochs.
All images are resized such that the smallest image dimension is 800 pixels during training and testing and 1000 proposals are kept at test time.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.1" class="ltx_p"><span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_bold">Runtime.</span>
DOPE runs at 100ms on a single NVIDIA T4 GPU.
When reducing the smallest image size to 400px and the number of box proposals to 50, and using half precision, it runs at 28 ms per image, <em id="S3.SS3.p3.1.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS3.p3.1.3" class="ltx_text"></span>, in real-time at 35 fps, with a 2-3% decrease of performance.
For comparison, each of our experts runs at a similar framerate as our whole-body model since only the last layers change. Optimization-based 3D whole-body estimation methodsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> take
up to a minute to process each person.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Given that there is no dataset to evaluate whole-body 3D pose estimation in the wild, we evaluate our method on each task separately.
After presenting datasets and metrics (SectionÂ <a href="#S4.SS1" title="4.1 Evaluation datasets and metrics â€£ 4 Experiments â€£ DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>), we compare the performance of our whole-body model to the experts (SectionÂ <a href="#S4.SS2" title="4.2 Comparison to the experts â€£ 4 Experiments â€£ DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>) and to the state of the art (SectionÂ <a href="#S4.SS3" title="4.3 Comparison to the state of the art â€£ 4 Experiments â€£ DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>).</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Evaluation datasets and metrics</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">MPII for 2D body pose estimation.</span>
As inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, we remove 1000 images from the MPIIÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> training set and use them to evaluate our 2D body pose estimation results.
We follow the standard evaluation protocol and report the PCKh@0.5 which is the percentage of correct keypoints with a keypoint being considered as correctly predicted if the error is smaller than half the size of the head.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">MuPoTs for 3D body pose estimation.</span>
MuPoTs-3DÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> (Multi-person Pose estimation Test Set in 3D) is composed of more than 8,000 frames from 20 real-world scenes with up to three subjects. The ground-truth 3D poses, obtained using a multi-view MoCap system, have a slightly different format than the one estimated by our body expert and whole-body model. To better fit their 14-joint skeleton model, we modified the regression layer of our networks to output 14 keypoints instead of 13 while freezing the rest of the network.
We finetuned this last layer only on the MuCo-3DHP datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, the standard training set when testing on MuPoTs.
We report the 3D-PCK, <em id="S4.SS1.p2.1.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS1.p2.1.3" class="ltx_text"></span>, the percentage of joint predictions with less than 15cm error, per sequence, and averaged over the subjects for which ground truth is available.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">RenderedHand for 3D hand pose estimation.</span>
RenderedHand (RH) test setÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> consists of 2,728 images showing the hands of a single person.
We report the standard AUC (Area Under the Curve) metric when plotting the 3D-PCK after normalizing the scale and relative translation between the ground-truth and the prediction. Note that while state-of-the-art methods evaluate hand pose estimation given ground-truth crops around the hands, we instead perform an automatic detection but miss around 2% of the hands.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.p4.3" class="ltx_p"><span id="S4.SS1.p4.3.1" class="ltx_text ltx_font_bold">Menpo for facial landmark estimation.</span>
We report results for facial landmark evaluation using the standard 3D-aware 2D metricÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> on the 30 videos from the test set of the ICCVâ€™17 challengeÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>.
Given a ground truth-matrix <math id="S4.SS1.p4.1.m1.3" class="ltx_Math" alttext="{\bm{s}}\in\mathcal{M}_{N,2}(\mathbb{R})" display="inline"><semantics id="S4.SS1.p4.1.m1.3a"><mrow id="S4.SS1.p4.1.m1.3.4" xref="S4.SS1.p4.1.m1.3.4.cmml"><mi id="S4.SS1.p4.1.m1.3.4.2" xref="S4.SS1.p4.1.m1.3.4.2.cmml">ğ’”</mi><mo id="S4.SS1.p4.1.m1.3.4.1" xref="S4.SS1.p4.1.m1.3.4.1.cmml">âˆˆ</mo><mrow id="S4.SS1.p4.1.m1.3.4.3" xref="S4.SS1.p4.1.m1.3.4.3.cmml"><msub id="S4.SS1.p4.1.m1.3.4.3.2" xref="S4.SS1.p4.1.m1.3.4.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p4.1.m1.3.4.3.2.2" xref="S4.SS1.p4.1.m1.3.4.3.2.2.cmml">â„³</mi><mrow id="S4.SS1.p4.1.m1.2.2.2.4" xref="S4.SS1.p4.1.m1.2.2.2.3.cmml"><mi id="S4.SS1.p4.1.m1.1.1.1.1" xref="S4.SS1.p4.1.m1.1.1.1.1.cmml">N</mi><mo id="S4.SS1.p4.1.m1.2.2.2.4.1" xref="S4.SS1.p4.1.m1.2.2.2.3.cmml">,</mo><mn id="S4.SS1.p4.1.m1.2.2.2.2" xref="S4.SS1.p4.1.m1.2.2.2.2.cmml">2</mn></mrow></msub><mo lspace="0em" rspace="0em" id="S4.SS1.p4.1.m1.3.4.3.1" xref="S4.SS1.p4.1.m1.3.4.3.1.cmml">â€‹</mo><mrow id="S4.SS1.p4.1.m1.3.4.3.3.2" xref="S4.SS1.p4.1.m1.3.4.3.cmml"><mo stretchy="false" id="S4.SS1.p4.1.m1.3.4.3.3.2.1" xref="S4.SS1.p4.1.m1.3.4.3.cmml">(</mo><mi id="S4.SS1.p4.1.m1.3.3" xref="S4.SS1.p4.1.m1.3.3.cmml">â„</mi><mo stretchy="false" id="S4.SS1.p4.1.m1.3.4.3.3.2.2" xref="S4.SS1.p4.1.m1.3.4.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.3b"><apply id="S4.SS1.p4.1.m1.3.4.cmml" xref="S4.SS1.p4.1.m1.3.4"><in id="S4.SS1.p4.1.m1.3.4.1.cmml" xref="S4.SS1.p4.1.m1.3.4.1"></in><ci id="S4.SS1.p4.1.m1.3.4.2.cmml" xref="S4.SS1.p4.1.m1.3.4.2">ğ’”</ci><apply id="S4.SS1.p4.1.m1.3.4.3.cmml" xref="S4.SS1.p4.1.m1.3.4.3"><times id="S4.SS1.p4.1.m1.3.4.3.1.cmml" xref="S4.SS1.p4.1.m1.3.4.3.1"></times><apply id="S4.SS1.p4.1.m1.3.4.3.2.cmml" xref="S4.SS1.p4.1.m1.3.4.3.2"><csymbol cd="ambiguous" id="S4.SS1.p4.1.m1.3.4.3.2.1.cmml" xref="S4.SS1.p4.1.m1.3.4.3.2">subscript</csymbol><ci id="S4.SS1.p4.1.m1.3.4.3.2.2.cmml" xref="S4.SS1.p4.1.m1.3.4.3.2.2">â„³</ci><list id="S4.SS1.p4.1.m1.2.2.2.3.cmml" xref="S4.SS1.p4.1.m1.2.2.2.4"><ci id="S4.SS1.p4.1.m1.1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1.1.1">ğ‘</ci><cn type="integer" id="S4.SS1.p4.1.m1.2.2.2.2.cmml" xref="S4.SS1.p4.1.m1.2.2.2.2">2</cn></list></apply><ci id="S4.SS1.p4.1.m1.3.3.cmml" xref="S4.SS1.p4.1.m1.3.3">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.3c">{\bm{s}}\in\mathcal{M}_{N,2}(\mathbb{R})</annotation></semantics></math> representing the 2D coordinates in the image of the <math id="S4.SS1.p4.2.m2.1" class="ltx_Math" alttext="N=84" display="inline"><semantics id="S4.SS1.p4.2.m2.1a"><mrow id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml"><mi id="S4.SS1.p4.2.m2.1.1.2" xref="S4.SS1.p4.2.m2.1.1.2.cmml">N</mi><mo id="S4.SS1.p4.2.m2.1.1.1" xref="S4.SS1.p4.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS1.p4.2.m2.1.1.3" xref="S4.SS1.p4.2.m2.1.1.3.cmml">84</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.1b"><apply id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1"><eq id="S4.SS1.p4.2.m2.1.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1.1"></eq><ci id="S4.SS1.p4.2.m2.1.1.2.cmml" xref="S4.SS1.p4.2.m2.1.1.2">ğ‘</ci><cn type="integer" id="S4.SS1.p4.2.m2.1.1.3.cmml" xref="S4.SS1.p4.2.m2.1.1.3">84</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.1c">N=84</annotation></semantics></math> landmarks of a face, and a facial landmark prediction <math id="S4.SS1.p4.3.m3.3" class="ltx_Math" alttext="\hat{\bm{s}}\in\mathcal{M}_{N,2}(\mathbb{R})" display="inline"><semantics id="S4.SS1.p4.3.m3.3a"><mrow id="S4.SS1.p4.3.m3.3.4" xref="S4.SS1.p4.3.m3.3.4.cmml"><mover accent="true" id="S4.SS1.p4.3.m3.3.4.2" xref="S4.SS1.p4.3.m3.3.4.2.cmml"><mi id="S4.SS1.p4.3.m3.3.4.2.2" xref="S4.SS1.p4.3.m3.3.4.2.2.cmml">ğ’”</mi><mo id="S4.SS1.p4.3.m3.3.4.2.1" xref="S4.SS1.p4.3.m3.3.4.2.1.cmml">^</mo></mover><mo id="S4.SS1.p4.3.m3.3.4.1" xref="S4.SS1.p4.3.m3.3.4.1.cmml">âˆˆ</mo><mrow id="S4.SS1.p4.3.m3.3.4.3" xref="S4.SS1.p4.3.m3.3.4.3.cmml"><msub id="S4.SS1.p4.3.m3.3.4.3.2" xref="S4.SS1.p4.3.m3.3.4.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p4.3.m3.3.4.3.2.2" xref="S4.SS1.p4.3.m3.3.4.3.2.2.cmml">â„³</mi><mrow id="S4.SS1.p4.3.m3.2.2.2.4" xref="S4.SS1.p4.3.m3.2.2.2.3.cmml"><mi id="S4.SS1.p4.3.m3.1.1.1.1" xref="S4.SS1.p4.3.m3.1.1.1.1.cmml">N</mi><mo id="S4.SS1.p4.3.m3.2.2.2.4.1" xref="S4.SS1.p4.3.m3.2.2.2.3.cmml">,</mo><mn id="S4.SS1.p4.3.m3.2.2.2.2" xref="S4.SS1.p4.3.m3.2.2.2.2.cmml">2</mn></mrow></msub><mo lspace="0em" rspace="0em" id="S4.SS1.p4.3.m3.3.4.3.1" xref="S4.SS1.p4.3.m3.3.4.3.1.cmml">â€‹</mo><mrow id="S4.SS1.p4.3.m3.3.4.3.3.2" xref="S4.SS1.p4.3.m3.3.4.3.cmml"><mo stretchy="false" id="S4.SS1.p4.3.m3.3.4.3.3.2.1" xref="S4.SS1.p4.3.m3.3.4.3.cmml">(</mo><mi id="S4.SS1.p4.3.m3.3.3" xref="S4.SS1.p4.3.m3.3.3.cmml">â„</mi><mo stretchy="false" id="S4.SS1.p4.3.m3.3.4.3.3.2.2" xref="S4.SS1.p4.3.m3.3.4.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.3.m3.3b"><apply id="S4.SS1.p4.3.m3.3.4.cmml" xref="S4.SS1.p4.3.m3.3.4"><in id="S4.SS1.p4.3.m3.3.4.1.cmml" xref="S4.SS1.p4.3.m3.3.4.1"></in><apply id="S4.SS1.p4.3.m3.3.4.2.cmml" xref="S4.SS1.p4.3.m3.3.4.2"><ci id="S4.SS1.p4.3.m3.3.4.2.1.cmml" xref="S4.SS1.p4.3.m3.3.4.2.1">^</ci><ci id="S4.SS1.p4.3.m3.3.4.2.2.cmml" xref="S4.SS1.p4.3.m3.3.4.2.2">ğ’”</ci></apply><apply id="S4.SS1.p4.3.m3.3.4.3.cmml" xref="S4.SS1.p4.3.m3.3.4.3"><times id="S4.SS1.p4.3.m3.3.4.3.1.cmml" xref="S4.SS1.p4.3.m3.3.4.3.1"></times><apply id="S4.SS1.p4.3.m3.3.4.3.2.cmml" xref="S4.SS1.p4.3.m3.3.4.3.2"><csymbol cd="ambiguous" id="S4.SS1.p4.3.m3.3.4.3.2.1.cmml" xref="S4.SS1.p4.3.m3.3.4.3.2">subscript</csymbol><ci id="S4.SS1.p4.3.m3.3.4.3.2.2.cmml" xref="S4.SS1.p4.3.m3.3.4.3.2.2">â„³</ci><list id="S4.SS1.p4.3.m3.2.2.2.3.cmml" xref="S4.SS1.p4.3.m3.2.2.2.4"><ci id="S4.SS1.p4.3.m3.1.1.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1.1.1">ğ‘</ci><cn type="integer" id="S4.SS1.p4.3.m3.2.2.2.2.cmml" xref="S4.SS1.p4.3.m3.2.2.2.2">2</cn></list></apply><ci id="S4.SS1.p4.3.m3.3.3.cmml" xref="S4.SS1.p4.3.m3.3.3">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.3.m3.3c">\hat{\bm{s}}\in\mathcal{M}_{N,2}(\mathbb{R})</annotation></semantics></math>, this 2D normalized point-to-point RMS error is defined as:</p>
<table id="S4.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E4.m1.4" class="ltx_Math" alttext="\epsilon({\bm{s}},\hat{\bm{s}})=\cfrac{\|{\bm{s}}-\hat{\bm{s}}\|_{2}}{\sqrt{N}d_{scale}}~{}~{}," display="block"><semantics id="S4.E4.m1.4a"><mrow id="S4.E4.m1.4.4.1" xref="S4.E4.m1.4.4.1.1.cmml"><mrow id="S4.E4.m1.4.4.1.1" xref="S4.E4.m1.4.4.1.1.cmml"><mrow id="S4.E4.m1.4.4.1.1.2" xref="S4.E4.m1.4.4.1.1.2.cmml"><mi id="S4.E4.m1.4.4.1.1.2.2" xref="S4.E4.m1.4.4.1.1.2.2.cmml">Ïµ</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.4.4.1.1.2.1" xref="S4.E4.m1.4.4.1.1.2.1.cmml">â€‹</mo><mrow id="S4.E4.m1.4.4.1.1.2.3.2" xref="S4.E4.m1.4.4.1.1.2.3.1.cmml"><mo stretchy="false" id="S4.E4.m1.4.4.1.1.2.3.2.1" xref="S4.E4.m1.4.4.1.1.2.3.1.cmml">(</mo><mi id="S4.E4.m1.2.2" xref="S4.E4.m1.2.2.cmml">ğ’”</mi><mo id="S4.E4.m1.4.4.1.1.2.3.2.2" xref="S4.E4.m1.4.4.1.1.2.3.1.cmml">,</mo><mover accent="true" id="S4.E4.m1.3.3" xref="S4.E4.m1.3.3.cmml"><mi id="S4.E4.m1.3.3.2" xref="S4.E4.m1.3.3.2.cmml">ğ’”</mi><mo id="S4.E4.m1.3.3.1" xref="S4.E4.m1.3.3.1.cmml">^</mo></mover><mo stretchy="false" id="S4.E4.m1.4.4.1.1.2.3.2.3" xref="S4.E4.m1.4.4.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S4.E4.m1.4.4.1.1.1" xref="S4.E4.m1.4.4.1.1.1.cmml">=</mo><mfrac id="S4.E4.m1.1.1" xref="S4.E4.m1.1.1.cmml"><msub id="S4.E4.m1.1.1.1" xref="S4.E4.m1.1.1.1.cmml"><mrow id="S4.E4.m1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.E4.m1.1.1.1.1.1.2" xref="S4.E4.m1.1.1.1.1.2.1.cmml">â€–</mo><mrow id="S4.E4.m1.1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.1.1.cmml"><mi id="S4.E4.m1.1.1.1.1.1.1.2" xref="S4.E4.m1.1.1.1.1.1.1.2.cmml">ğ’”</mi><mo id="S4.E4.m1.1.1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.1.1.1.cmml">âˆ’</mo><mover accent="true" id="S4.E4.m1.1.1.1.1.1.1.3" xref="S4.E4.m1.1.1.1.1.1.1.3.cmml"><mi id="S4.E4.m1.1.1.1.1.1.1.3.2" xref="S4.E4.m1.1.1.1.1.1.1.3.2.cmml">ğ’”</mi><mo id="S4.E4.m1.1.1.1.1.1.1.3.1" xref="S4.E4.m1.1.1.1.1.1.1.3.1.cmml">^</mo></mover></mrow><mo stretchy="false" id="S4.E4.m1.1.1.1.1.1.3" xref="S4.E4.m1.1.1.1.1.2.1.cmml">â€–</mo></mrow><mn id="S4.E4.m1.1.1.1.3" xref="S4.E4.m1.1.1.1.3.cmml">2</mn></msub><mrow id="S4.E4.m1.1.1.3" xref="S4.E4.m1.1.1.3.cmml"><msqrt id="S4.E4.m1.1.1.3.2" xref="S4.E4.m1.1.1.3.2.cmml"><mi id="S4.E4.m1.1.1.3.2.2" xref="S4.E4.m1.1.1.3.2.2.cmml">N</mi></msqrt><mo lspace="0em" rspace="0em" id="S4.E4.m1.1.1.3.1" xref="S4.E4.m1.1.1.3.1.cmml">â€‹</mo><msub id="S4.E4.m1.1.1.3.3" xref="S4.E4.m1.1.1.3.3.cmml"><mi id="S4.E4.m1.1.1.3.3.2" xref="S4.E4.m1.1.1.3.3.2.cmml">d</mi><mrow id="S4.E4.m1.1.1.3.3.3" xref="S4.E4.m1.1.1.3.3.3.cmml"><mi id="S4.E4.m1.1.1.3.3.3.2" xref="S4.E4.m1.1.1.3.3.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.1.1.3.3.3.1" xref="S4.E4.m1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="S4.E4.m1.1.1.3.3.3.3" xref="S4.E4.m1.1.1.3.3.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.1.1.3.3.3.1a" xref="S4.E4.m1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="S4.E4.m1.1.1.3.3.3.4" xref="S4.E4.m1.1.1.3.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.1.1.3.3.3.1b" xref="S4.E4.m1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="S4.E4.m1.1.1.3.3.3.5" xref="S4.E4.m1.1.1.3.3.3.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.1.1.3.3.3.1c" xref="S4.E4.m1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="S4.E4.m1.1.1.3.3.3.6" xref="S4.E4.m1.1.1.3.3.3.6.cmml">e</mi></mrow></msub></mrow></mfrac></mrow><mo lspace="0.660em" id="S4.E4.m1.4.4.1.2" xref="S4.E4.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m1.4b"><apply id="S4.E4.m1.4.4.1.1.cmml" xref="S4.E4.m1.4.4.1"><eq id="S4.E4.m1.4.4.1.1.1.cmml" xref="S4.E4.m1.4.4.1.1.1"></eq><apply id="S4.E4.m1.4.4.1.1.2.cmml" xref="S4.E4.m1.4.4.1.1.2"><times id="S4.E4.m1.4.4.1.1.2.1.cmml" xref="S4.E4.m1.4.4.1.1.2.1"></times><ci id="S4.E4.m1.4.4.1.1.2.2.cmml" xref="S4.E4.m1.4.4.1.1.2.2">italic-Ïµ</ci><interval closure="open" id="S4.E4.m1.4.4.1.1.2.3.1.cmml" xref="S4.E4.m1.4.4.1.1.2.3.2"><ci id="S4.E4.m1.2.2.cmml" xref="S4.E4.m1.2.2">ğ’”</ci><apply id="S4.E4.m1.3.3.cmml" xref="S4.E4.m1.3.3"><ci id="S4.E4.m1.3.3.1.cmml" xref="S4.E4.m1.3.3.1">^</ci><ci id="S4.E4.m1.3.3.2.cmml" xref="S4.E4.m1.3.3.2">ğ’”</ci></apply></interval></apply><apply id="S4.E4.m1.1.1.cmml" xref="S4.E4.m1.1.1"><csymbol cd="latexml" id="S4.E4.m1.1.1.2.cmml" xref="S4.E4.m1.1.1">continued-fraction</csymbol><apply id="S4.E4.m1.1.1.1.cmml" xref="S4.E4.m1.1.1.1"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.1.2.cmml" xref="S4.E4.m1.1.1.1">subscript</csymbol><apply id="S4.E4.m1.1.1.1.1.2.cmml" xref="S4.E4.m1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E4.m1.1.1.1.1.2.1.cmml" xref="S4.E4.m1.1.1.1.1.1.2">norm</csymbol><apply id="S4.E4.m1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1"><minus id="S4.E4.m1.1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1"></minus><ci id="S4.E4.m1.1.1.1.1.1.1.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.2">ğ’”</ci><apply id="S4.E4.m1.1.1.1.1.1.1.3.cmml" xref="S4.E4.m1.1.1.1.1.1.1.3"><ci id="S4.E4.m1.1.1.1.1.1.1.3.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.3.1">^</ci><ci id="S4.E4.m1.1.1.1.1.1.1.3.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.3.2">ğ’”</ci></apply></apply></apply><cn type="integer" id="S4.E4.m1.1.1.1.3.cmml" xref="S4.E4.m1.1.1.1.3">2</cn></apply><apply id="S4.E4.m1.1.1.3.cmml" xref="S4.E4.m1.1.1.3"><times id="S4.E4.m1.1.1.3.1.cmml" xref="S4.E4.m1.1.1.3.1"></times><apply id="S4.E4.m1.1.1.3.2.cmml" xref="S4.E4.m1.1.1.3.2"><root id="S4.E4.m1.1.1.3.2a.cmml" xref="S4.E4.m1.1.1.3.2"></root><ci id="S4.E4.m1.1.1.3.2.2.cmml" xref="S4.E4.m1.1.1.3.2.2">ğ‘</ci></apply><apply id="S4.E4.m1.1.1.3.3.cmml" xref="S4.E4.m1.1.1.3.3"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.3.3.1.cmml" xref="S4.E4.m1.1.1.3.3">subscript</csymbol><ci id="S4.E4.m1.1.1.3.3.2.cmml" xref="S4.E4.m1.1.1.3.3.2">ğ‘‘</ci><apply id="S4.E4.m1.1.1.3.3.3.cmml" xref="S4.E4.m1.1.1.3.3.3"><times id="S4.E4.m1.1.1.3.3.3.1.cmml" xref="S4.E4.m1.1.1.3.3.3.1"></times><ci id="S4.E4.m1.1.1.3.3.3.2.cmml" xref="S4.E4.m1.1.1.3.3.3.2">ğ‘ </ci><ci id="S4.E4.m1.1.1.3.3.3.3.cmml" xref="S4.E4.m1.1.1.3.3.3.3">ğ‘</ci><ci id="S4.E4.m1.1.1.3.3.3.4.cmml" xref="S4.E4.m1.1.1.3.3.3.4">ğ‘</ci><ci id="S4.E4.m1.1.1.3.3.3.5.cmml" xref="S4.E4.m1.1.1.3.3.3.5">ğ‘™</ci><ci id="S4.E4.m1.1.1.3.3.3.6.cmml" xref="S4.E4.m1.1.1.3.3.3.6">ğ‘’</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m1.4c">\epsilon({\bm{s}},\hat{\bm{s}})=\cfrac{\|{\bm{s}}-\hat{\bm{s}}\|_{2}}{\sqrt{N}d_{scale}}~{}~{},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S4.SS1.p4.5" class="ltx_p">where <math id="S4.SS1.p4.4.m1.1" class="ltx_Math" alttext="d_{scale}" display="inline"><semantics id="S4.SS1.p4.4.m1.1a"><msub id="S4.SS1.p4.4.m1.1.1" xref="S4.SS1.p4.4.m1.1.1.cmml"><mi id="S4.SS1.p4.4.m1.1.1.2" xref="S4.SS1.p4.4.m1.1.1.2.cmml">d</mi><mrow id="S4.SS1.p4.4.m1.1.1.3" xref="S4.SS1.p4.4.m1.1.1.3.cmml"><mi id="S4.SS1.p4.4.m1.1.1.3.2" xref="S4.SS1.p4.4.m1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.4.m1.1.1.3.1" xref="S4.SS1.p4.4.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS1.p4.4.m1.1.1.3.3" xref="S4.SS1.p4.4.m1.1.1.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.4.m1.1.1.3.1a" xref="S4.SS1.p4.4.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS1.p4.4.m1.1.1.3.4" xref="S4.SS1.p4.4.m1.1.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.4.m1.1.1.3.1b" xref="S4.SS1.p4.4.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS1.p4.4.m1.1.1.3.5" xref="S4.SS1.p4.4.m1.1.1.3.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.4.m1.1.1.3.1c" xref="S4.SS1.p4.4.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS1.p4.4.m1.1.1.3.6" xref="S4.SS1.p4.4.m1.1.1.3.6.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.4.m1.1b"><apply id="S4.SS1.p4.4.m1.1.1.cmml" xref="S4.SS1.p4.4.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.4.m1.1.1.1.cmml" xref="S4.SS1.p4.4.m1.1.1">subscript</csymbol><ci id="S4.SS1.p4.4.m1.1.1.2.cmml" xref="S4.SS1.p4.4.m1.1.1.2">ğ‘‘</ci><apply id="S4.SS1.p4.4.m1.1.1.3.cmml" xref="S4.SS1.p4.4.m1.1.1.3"><times id="S4.SS1.p4.4.m1.1.1.3.1.cmml" xref="S4.SS1.p4.4.m1.1.1.3.1"></times><ci id="S4.SS1.p4.4.m1.1.1.3.2.cmml" xref="S4.SS1.p4.4.m1.1.1.3.2">ğ‘ </ci><ci id="S4.SS1.p4.4.m1.1.1.3.3.cmml" xref="S4.SS1.p4.4.m1.1.1.3.3">ğ‘</ci><ci id="S4.SS1.p4.4.m1.1.1.3.4.cmml" xref="S4.SS1.p4.4.m1.1.1.3.4">ğ‘</ci><ci id="S4.SS1.p4.4.m1.1.1.3.5.cmml" xref="S4.SS1.p4.4.m1.1.1.3.5">ğ‘™</ci><ci id="S4.SS1.p4.4.m1.1.1.3.6.cmml" xref="S4.SS1.p4.4.m1.1.1.3.6">ğ‘’</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.4.m1.1c">d_{scale}</annotation></semantics></math> is the length of the diagonal of the minimal 2D bounding box of <math id="S4.SS1.p4.5.m2.1" class="ltx_Math" alttext="{\bm{s}}" display="inline"><semantics id="S4.SS1.p4.5.m2.1a"><mi id="S4.SS1.p4.5.m2.1.1" xref="S4.SS1.p4.5.m2.1.1.cmml">ğ’”</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.5.m2.1b"><ci id="S4.SS1.p4.5.m2.1.1.cmml" xref="S4.SS1.p4.5.m2.1.1">ğ’”</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.5.m2.1c">{\bm{s}}</annotation></semantics></math>.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison between our part experts and our whole-body model</figcaption>
<table id="S4.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.2.3.1" class="ltx_tr">
<th id="S4.T1.2.3.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt" rowspan="2"></th>
<td id="S4.T1.2.3.1.2" class="ltx_td ltx_align_center ltx_border_tt">MPII</td>
<td id="S4.T1.2.3.1.3" class="ltx_td ltx_align_center ltx_border_tt">MuPoTs</td>
<td id="S4.T1.2.3.1.4" class="ltx_td ltx_align_center ltx_border_tt">RH test</td>
<td id="S4.T1.2.3.1.5" class="ltx_td ltx_align_center ltx_border_tt">Menpo</td>
</tr>
<tr id="S4.T1.2.4.2" class="ltx_tr">
<td id="S4.T1.2.4.2.1" class="ltx_td ltx_align_center">(PCKh@0.5)</td>
<td id="S4.T1.2.4.2.2" class="ltx_td ltx_align_center">(PCK3D)</td>
<td id="S4.T1.2.4.2.3" class="ltx_td ltx_align_center">(AUC)</td>
<td id="S4.T1.2.4.2.4" class="ltx_td ltx_align_center">(AUC)</td>
</tr>
<tr id="S4.T1.2.5.3" class="ltx_tr">
<th id="S4.T1.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">body expert</th>
<td id="S4.T1.2.5.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.2.5.3.2.1" class="ltx_text ltx_font_bold">89.6</span></td>
<td id="S4.T1.2.5.3.3" class="ltx_td ltx_align_center ltx_border_t">66.8</td>
<td id="S4.T1.2.5.3.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T1.2.5.3.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T1.2.6.4" class="ltx_tr">
<th id="S4.T1.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">hand expert</th>
<td id="S4.T1.2.6.4.2" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.2.6.4.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.2.6.4.4" class="ltx_td ltx_align_center"><span id="S4.T1.2.6.4.4.1" class="ltx_text ltx_font_bold">87.1</span></td>
<td id="S4.T1.2.6.4.5" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T1.2.7.5" class="ltx_tr">
<th id="S4.T1.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">face expert</th>
<td id="S4.T1.2.7.5.2" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.2.7.5.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.2.7.5.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.2.7.5.5" class="ltx_td ltx_align_center">73.9</td>
</tr>
<tr id="S4.T1.2.8.6" class="ltx_tr">
<th id="S4.T1.2.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">whole-body trained on gt</th>
<td id="S4.T1.2.8.6.2" class="ltx_td ltx_align_center ltx_border_t">88.3</td>
<td id="S4.T1.2.8.6.3" class="ltx_td ltx_align_center ltx_border_t">66.6</td>
<td id="S4.T1.2.8.6.4" class="ltx_td ltx_align_center ltx_border_t">81.1</td>
<td id="S4.T1.2.8.6.5" class="ltx_td ltx_align_center ltx_border_t">61.7</td>
</tr>
<tr id="S4.T1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">DOPE without <math id="S4.T1.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{L}^{p}_{dist}" display="inline"><semantics id="S4.T1.1.1.1.m1.1a"><msubsup id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T1.1.1.1.m1.1.1.2.2" xref="S4.T1.1.1.1.m1.1.1.2.2.cmml">â„’</mi><mrow id="S4.T1.1.1.1.m1.1.1.3" xref="S4.T1.1.1.1.m1.1.1.3.cmml"><mi id="S4.T1.1.1.1.m1.1.1.3.2" xref="S4.T1.1.1.1.m1.1.1.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.T1.1.1.1.m1.1.1.3.1" xref="S4.T1.1.1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T1.1.1.1.m1.1.1.3.3" xref="S4.T1.1.1.1.m1.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.T1.1.1.1.m1.1.1.3.1a" xref="S4.T1.1.1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T1.1.1.1.m1.1.1.3.4" xref="S4.T1.1.1.1.m1.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.T1.1.1.1.m1.1.1.3.1b" xref="S4.T1.1.1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T1.1.1.1.m1.1.1.3.5" xref="S4.T1.1.1.1.m1.1.1.3.5.cmml">t</mi></mrow><mi id="S4.T1.1.1.1.m1.1.1.2.3" xref="S4.T1.1.1.1.m1.1.1.2.3.cmml">p</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><apply id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.1.1.1.m1.1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">subscript</csymbol><apply id="S4.T1.1.1.1.m1.1.1.2.cmml" xref="S4.T1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.1.1.1.m1.1.1.2.1.cmml" xref="S4.T1.1.1.1.m1.1.1">superscript</csymbol><ci id="S4.T1.1.1.1.m1.1.1.2.2.cmml" xref="S4.T1.1.1.1.m1.1.1.2.2">â„’</ci><ci id="S4.T1.1.1.1.m1.1.1.2.3.cmml" xref="S4.T1.1.1.1.m1.1.1.2.3">ğ‘</ci></apply><apply id="S4.T1.1.1.1.m1.1.1.3.cmml" xref="S4.T1.1.1.1.m1.1.1.3"><times id="S4.T1.1.1.1.m1.1.1.3.1.cmml" xref="S4.T1.1.1.1.m1.1.1.3.1"></times><ci id="S4.T1.1.1.1.m1.1.1.3.2.cmml" xref="S4.T1.1.1.1.m1.1.1.3.2">ğ‘‘</ci><ci id="S4.T1.1.1.1.m1.1.1.3.3.cmml" xref="S4.T1.1.1.1.m1.1.1.3.3">ğ‘–</ci><ci id="S4.T1.1.1.1.m1.1.1.3.4.cmml" xref="S4.T1.1.1.1.m1.1.1.3.4">ğ‘ </ci><ci id="S4.T1.1.1.1.m1.1.1.3.5.cmml" xref="S4.T1.1.1.1.m1.1.1.3.5">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">\mathcal{L}^{p}_{dist}</annotation></semantics></math>
</th>
<td id="S4.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_t">88.3</td>
<td id="S4.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_t">66.4</td>
<td id="S4.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">83.5</td>
<td id="S4.T1.1.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.5.1" class="ltx_text ltx_font_bold">75.2</span></td>
</tr>
<tr id="S4.T1.2.2" class="ltx_tr">
<th id="S4.T1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">DOPE with <math id="S4.T1.2.2.1.m1.1" class="ltx_Math" alttext="\mathcal{L}^{p}_{dist}" display="inline"><semantics id="S4.T1.2.2.1.m1.1a"><msubsup id="S4.T1.2.2.1.m1.1.1" xref="S4.T1.2.2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.T1.2.2.1.m1.1.1.2.2" xref="S4.T1.2.2.1.m1.1.1.2.2.cmml">â„’</mi><mrow id="S4.T1.2.2.1.m1.1.1.3" xref="S4.T1.2.2.1.m1.1.1.3.cmml"><mi id="S4.T1.2.2.1.m1.1.1.3.2" xref="S4.T1.2.2.1.m1.1.1.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.T1.2.2.1.m1.1.1.3.1" xref="S4.T1.2.2.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T1.2.2.1.m1.1.1.3.3" xref="S4.T1.2.2.1.m1.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.T1.2.2.1.m1.1.1.3.1a" xref="S4.T1.2.2.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T1.2.2.1.m1.1.1.3.4" xref="S4.T1.2.2.1.m1.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.T1.2.2.1.m1.1.1.3.1b" xref="S4.T1.2.2.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.T1.2.2.1.m1.1.1.3.5" xref="S4.T1.2.2.1.m1.1.1.3.5.cmml">t</mi></mrow><mi id="S4.T1.2.2.1.m1.1.1.2.3" xref="S4.T1.2.2.1.m1.1.1.2.3.cmml">p</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.1.m1.1b"><apply id="S4.T1.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.2.2.1.m1.1.1.1.cmml" xref="S4.T1.2.2.1.m1.1.1">subscript</csymbol><apply id="S4.T1.2.2.1.m1.1.1.2.cmml" xref="S4.T1.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.2.2.1.m1.1.1.2.1.cmml" xref="S4.T1.2.2.1.m1.1.1">superscript</csymbol><ci id="S4.T1.2.2.1.m1.1.1.2.2.cmml" xref="S4.T1.2.2.1.m1.1.1.2.2">â„’</ci><ci id="S4.T1.2.2.1.m1.1.1.2.3.cmml" xref="S4.T1.2.2.1.m1.1.1.2.3">ğ‘</ci></apply><apply id="S4.T1.2.2.1.m1.1.1.3.cmml" xref="S4.T1.2.2.1.m1.1.1.3"><times id="S4.T1.2.2.1.m1.1.1.3.1.cmml" xref="S4.T1.2.2.1.m1.1.1.3.1"></times><ci id="S4.T1.2.2.1.m1.1.1.3.2.cmml" xref="S4.T1.2.2.1.m1.1.1.3.2">ğ‘‘</ci><ci id="S4.T1.2.2.1.m1.1.1.3.3.cmml" xref="S4.T1.2.2.1.m1.1.1.3.3">ğ‘–</ci><ci id="S4.T1.2.2.1.m1.1.1.3.4.cmml" xref="S4.T1.2.2.1.m1.1.1.3.4">ğ‘ </ci><ci id="S4.T1.2.2.1.m1.1.1.3.5.cmml" xref="S4.T1.2.2.1.m1.1.1.3.5">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.1.m1.1c">\mathcal{L}^{p}_{dist}</annotation></semantics></math>
</th>
<td id="S4.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_bb">88.8</td>
<td id="S4.T1.2.2.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.2.2.3.1" class="ltx_text ltx_font_bold">67.2</span></td>
<td id="S4.T1.2.2.4" class="ltx_td ltx_align_center ltx_border_bb">84.9</td>
<td id="S4.T1.2.2.5" class="ltx_td ltx_align_center ltx_border_bb">75.0</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Comparison to the experts</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">TableÂ <a href="#S4.T1" title="Table 1 â€£ 4.1 Evaluation datasets and metrics â€£ 4 Experiments â€£ DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents a comparison of the performances obtained by the part experts and our DOPE model, for body, hand and face pose estimation tasks.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">We first compare the part experts to a baseline where our whole-body network is trained on the partial ground-truth available for each dataset, <em id="S4.SS2.p2.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.SS2.p2.1.2" class="ltx_text"></span> only body annotations are available on images from body datasets, <em id="S4.SS2.p2.1.3" class="ltx_emph ltx_font_italic">etc</em>.<span id="S4.SS2.p2.1.4" class="ltx_text"></span> The performance degrades quite significantly compared to those of the hand and face experts (-6% AUC for hand on the RH dataset and -12% for face landmarks).
This is explained by a lower detection rate of the detector due to the fact that, for instance, unannotated faces present in the body datasets are considered as negatives during training.
The performance of this model on body pose estimation is quite similar to the one of the body expert: as bodies are not observed too much in hand and face datasets, there are almost no missing body annotations.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">We then compare the experts to a first version of our DOPE model without the distillation loss <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{L}^{p}_{dist}" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><msubsup id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p3.1.m1.1.1.2.2" xref="S4.SS2.p3.1.m1.1.1.2.2.cmml">â„’</mi><mrow id="S4.SS2.p3.1.m1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.3.cmml"><mi id="S4.SS2.p3.1.m1.1.1.3.2" xref="S4.SS2.p3.1.m1.1.1.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p3.1.m1.1.1.3.1" xref="S4.SS2.p3.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS2.p3.1.m1.1.1.3.3" xref="S4.SS2.p3.1.m1.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p3.1.m1.1.1.3.1a" xref="S4.SS2.p3.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS2.p3.1.m1.1.1.3.4" xref="S4.SS2.p3.1.m1.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p3.1.m1.1.1.3.1b" xref="S4.SS2.p3.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS2.p3.1.m1.1.1.3.5" xref="S4.SS2.p3.1.m1.1.1.3.5.cmml">t</mi></mrow><mi id="S4.SS2.p3.1.m1.1.1.2.3" xref="S4.SS2.p3.1.m1.1.1.2.3.cmml">p</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">subscript</csymbol><apply id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.1.m1.1.1.2.1.cmml" xref="S4.SS2.p3.1.m1.1.1">superscript</csymbol><ci id="S4.SS2.p3.1.m1.1.1.2.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2.2">â„’</ci><ci id="S4.SS2.p3.1.m1.1.1.2.3.cmml" xref="S4.SS2.p3.1.m1.1.1.2.3">ğ‘</ci></apply><apply id="S4.SS2.p3.1.m1.1.1.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3"><times id="S4.SS2.p3.1.m1.1.1.3.1.cmml" xref="S4.SS2.p3.1.m1.1.1.3.1"></times><ci id="S4.SS2.p3.1.m1.1.1.3.2.cmml" xref="S4.SS2.p3.1.m1.1.1.3.2">ğ‘‘</ci><ci id="S4.SS2.p3.1.m1.1.1.3.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3.3">ğ‘–</ci><ci id="S4.SS2.p3.1.m1.1.1.3.4.cmml" xref="S4.SS2.p3.1.m1.1.1.3.4">ğ‘ </ci><ci id="S4.SS2.p3.1.m1.1.1.3.5.cmml" xref="S4.SS2.p3.1.m1.1.1.3.5">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">\mathcal{L}^{p}_{dist}</annotation></semantics></math>.
The performance on body pose estimation remains similar but, for hands and faces, a significant gain is obtained, in particular for faces,
where the whole-body network performs even better than the expert.
This might be explained by the fact that the whole-body network is trained on a larger variety of data, including images from body and hands datasets with many additional faces.
In contrast, the hand component performs slightly lower than the expert. One hypothesis is that many hands in the body datasets are too small to be accurately estimated, leading to noisy pseudo ground-truth poses. However, the performance remains close to that of the hand expert.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">With the addition of the distillation loss, the accuracy increases for hand pose estimation (+1.4%) and slightly for body pose estimation (+0.5% on MPII, +0.8% on MuPoTs), bringing the performance of the whole-body network even closer to the expertsâ€™ results.
Sometimes, DOPE even outperforms the part expert as observed on MuPoTs for multi-person 3D pose estimation or on Menpo for facial landmark detection.
FigureÂ <a href="#S4.F4" title="Figure 4 â€£ 4.2 Comparison to the experts â€£ 4 Experiments â€£ DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents some qualitative results for the part experts and our proposed DOPE model trained with distillation loss. Two additional examples of our modelâ€™s results can be found in FigureÂ <a href="#S0.F1" title="Figure 1 â€£ DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. DOPE produces high-quality whole-body detections that include bodies, hands and faces.
In the example on the left in FigureÂ <a href="#S4.F4" title="Figure 4 â€£ 4.2 Comparison to the experts â€£ 4 Experiments â€£ DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, our whole-body network correctly detects and estimates the pose of three hands, misdetecting only the ladyâ€™s right hand. By contrast, the hand expert only finds one hand in this image.
Note that our method is holistic for each part: if a part is sufficiently visible in the image, a prediction is made for every keypoint of the part despite partial occlusions or truncations, as shown for the bodies in this same example. However, if a part is not visible, no prediction is made. This is the case for the occluded hands in the middle and right examples in FigureÂ <a href="#S4.F4" title="Figure 4 â€£ 4.2 Comparison to the experts â€£ 4 Experiments â€£ DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
Overall, these examples illustrate that our method can be applied in the wild, including scenes with multiple interacting people, varied background, severe occlusions or truncations.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<div id="S4.F4.1" class="ltx_block ltx_figure_panel">
<div id="S4.F4.1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:8.9pt;height:45.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:45.3pt;transform:translate(-18.18pt,-17.21pt) rotate(-90deg) ;">
<p id="S4.F4.1.1.1" class="ltx_p">body</p>
</span></div>
<img src="/html/2008.09457/assets/fig/examples_MPIIvalid/008539769_body.jpg" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="189" height="106" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2008.09457/assets/fig/examples_MPIIvalid/015994080_body.jpg" id="S4.F4.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="189" height="106" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2008.09457/assets/fig/examples_MPIIvalid/018346425_body.jpg" id="S4.F4.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="189" height="106" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<div id="S4.F4.2" class="ltx_block ltx_figure_panel">
<div id="S4.F4.2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:6.9pt;height:45.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:45.6pt;transform:translate(-19.32pt,-19.32pt) rotate(-90deg) ;">
<p id="S4.F4.2.1.1" class="ltx_p">hands</p>
</span></div>
<img src="/html/2008.09457/assets/fig/examples_MPIIvalid/008539769_hand.jpg" id="S4.F4.g4" class="ltx_graphics ltx_centering ltx_img_landscape" width="189" height="106" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2008.09457/assets/fig/examples_MPIIvalid/015994080_hand.jpg" id="S4.F4.g5" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="189" height="106" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2008.09457/assets/fig/examples_MPIIvalid/018346425_hand.jpg" id="S4.F4.g6" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="189" height="106" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<div id="S4.F4.3" class="ltx_block ltx_figure_panel">
<div id="S4.F4.3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:6.9pt;height:40.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:40.3pt;transform:translate(-16.65pt,-16.65pt) rotate(-90deg) ;">
<p id="S4.F4.3.1.1" class="ltx_p">face</p>
</span></div>
<img src="/html/2008.09457/assets/fig/examples_MPIIvalid/008539769_face.jpg" id="S4.F4.g7" class="ltx_graphics ltx_centering ltx_img_landscape" width="189" height="106" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2008.09457/assets/fig/examples_MPIIvalid/015994080_face.jpg" id="S4.F4.g8" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="189" height="106" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2008.09457/assets/fig/examples_MPIIvalid/018346425_face.jpg" id="S4.F4.g9" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="189" height="106" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<div id="S4.F4.4" class="ltx_block ltx_figure_panel">
<div id="S4.F4.4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:10.0pt;height:56.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:56.1pt;transform:translate(-23.05pt,-21.8pt) rotate(-90deg) ;">
<p id="S4.F4.4.1.1" class="ltx_p"><span id="S4.F4.4.1.1.1" class="ltx_text ltx_font_bold">DOPE (2D)</span></p>
</span></div>
<img src="/html/2008.09457/assets/fig/examples_MPIIvalid/008539769_dope.jpg" id="S4.F4.g10" class="ltx_graphics ltx_centering ltx_img_landscape" width="189" height="106" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2008.09457/assets/fig/examples_MPIIvalid/015994080_dope.jpg" id="S4.F4.g11" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="189" height="106" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2008.09457/assets/fig/examples_MPIIvalid/018346425_dope.jpg" id="S4.F4.g12" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="189" height="106" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<div id="S4.F4.5" class="ltx_block ltx_figure_panel">
<div id="S4.F4.5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:10.0pt;height:79.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:79.4pt;transform:translate(-34.71pt,-33.46pt) rotate(-90deg) ;">
<p id="S4.F4.5.1.1" class="ltx_p"><span id="S4.F4.5.1.1.1" class="ltx_text ltx_font_bold">DOPE (3D)</span></p>
</span></div>
<img src="/html/2008.09457/assets/x5.jpg" id="S4.F4.g13" class="ltx_graphics ltx_centering ltx_img_portrait" width="209" height="600" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2008.09457/assets/x6.jpg" id="S4.F4.g14" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="209" height="1381" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2008.09457/assets/x7.jpg" id="S4.F4.g15" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="209" height="1381" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Each column shows an example with the results of the 3 experts on the first three rows (we show only the 2D for clarity). The last two rows show the results obtained by our proposed DOPE approach in 2D and in 3D respectively.</figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure">
<table id="S4.F5.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.F5.2.2" class="ltx_tr">
<th id="S4.F5.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-bottom:-5.69046pt;">Â Â 

<span id="S4.F5.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:195.1pt;"><img src="/html/2008.09457/assets/x8.png" id="S4.F5.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="346" alt="Refer to caption">
</span>
</th>
<td id="S4.F5.2.2.2" class="ltx_td ltx_align_center" style="padding-bottom:-5.69046pt;">
<span id="S4.F5.2.2.2.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:195.1pt;"><img src="/html/2008.09457/assets/x9.png" id="S4.F5.2.2.2.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="346" alt="Refer to caption">
</span>
</td>
</tr>
<tr id="S4.F5.2.3.1" class="ltx_tr">
<th id="S4.F5.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-bottom:5.69046pt;">(a)</th>
<td id="S4.F5.2.3.1.2" class="ltx_td ltx_align_center" style="padding-bottom:5.69046pt;">(b)</td>
</tr>
<tr id="S4.F5.2.4.2" class="ltx_tr">
<th id="S4.F5.2.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">
<div id="S4.F5.2.4.2.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle ltx_transformed_outer" style="width:108.4pt;height:282.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(147.3pt,-87.2pt) scale(2.61564981981025,2.61564981981025) ;">
<table id="S4.F5.2.4.2.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.F5.2.4.2.1.1.1.1" class="ltx_tr">
<td id="S4.F5.2.4.2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">method</td>
<td id="S4.F5.2.4.2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">PCK3D</td>
</tr>
<tr id="S4.F5.2.4.2.1.1.1.2" class="ltx_tr">
<td id="S4.F5.2.4.2.1.1.1.2.1" class="ltx_td ltx_align_center ltx_border_t">VNectÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>
</td>
<td id="S4.F5.2.4.2.1.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">65.0</td>
</tr>
<tr id="S4.F5.2.4.2.1.1.1.3" class="ltx_tr">
<td id="S4.F5.2.4.2.1.1.1.3.1" class="ltx_td ltx_align_center">XNectÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</td>
<td id="S4.F5.2.4.2.1.1.1.3.2" class="ltx_td ltx_align_center">70.4</td>
</tr>
<tr id="S4.F5.2.4.2.1.1.1.4" class="ltx_tr">
<td id="S4.F5.2.4.2.1.1.1.4.1" class="ltx_td ltx_align_center">LCRNet++Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>
</td>
<td id="S4.F5.2.4.2.1.1.1.4.2" class="ltx_td ltx_align_center"><span id="S4.F5.2.4.2.1.1.1.4.2.1" class="ltx_text ltx_font_bold">70.6</span></td>
</tr>
<tr id="S4.F5.2.4.2.1.1.1.5" class="ltx_tr">
<td id="S4.F5.2.4.2.1.1.1.5.1" class="ltx_td ltx_align_center ltx_border_t">body expert</td>
<td id="S4.F5.2.4.2.1.1.1.5.2" class="ltx_td ltx_align_center ltx_border_t">66.8</td>
</tr>
<tr id="S4.F5.2.4.2.1.1.1.6" class="ltx_tr">
<td id="S4.F5.2.4.2.1.1.1.6.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.F5.2.4.2.1.1.1.6.1.1" class="ltx_text ltx_font_bold">DOPE</span></td>
<td id="S4.F5.2.4.2.1.1.1.6.2" class="ltx_td ltx_align_center ltx_border_bb">67.2</td>
</tr>
</table>
</span></div>
</th>
<td id="S4.F5.2.4.2.2" class="ltx_td ltx_align_center">
<div id="S4.F5.2.4.2.2.1" class="ltx_inline-block ltx_minipage ltx_align_middle ltx_transformed_outer" style="width:173.4pt;height:165.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(108.5pt,-37.6pt) scale(1.83445336342354,1.83445336342354) ;">
<table id="S4.F5.2.4.2.2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.F5.2.4.2.2.1.1.1" class="ltx_tr">
<td id="S4.F5.2.4.2.2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">method</td>
<td id="S4.F5.2.4.2.2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">Â Â RH (PCK2D)</td>
<td id="S4.F5.2.4.2.2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Â Â Menpo (PCK2D)</td>
</tr>
<tr id="S4.F5.2.4.2.2.1.1.2" class="ltx_tr">
<td id="S4.F5.2.4.2.2.1.1.2.1" class="ltx_td ltx_align_center ltx_border_t">Experts</td>
<td id="S4.F5.2.4.2.2.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.F5.2.4.2.2.1.1.2.2.1" class="ltx_text ltx_font_bold">78.1</span></td>
<td id="S4.F5.2.4.2.2.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">94.5</td>
</tr>
<tr id="S4.F5.2.4.2.2.1.1.3" class="ltx_tr">
<td id="S4.F5.2.4.2.2.1.1.3.1" class="ltx_td ltx_align_center"><span id="S4.F5.2.4.2.2.1.1.3.1.1" class="ltx_text ltx_font_bold">DOPE</span></td>
<td id="S4.F5.2.4.2.2.1.1.3.2" class="ltx_td ltx_align_center">71.0</td>
<td id="S4.F5.2.4.2.2.1.1.3.3" class="ltx_td ltx_align_center"><span id="S4.F5.2.4.2.2.1.1.3.3.1" class="ltx_text ltx_font_bold">94.9</span></td>
</tr>
<tr id="S4.F5.2.4.2.2.1.1.4" class="ltx_tr">
<td id="S4.F5.2.4.2.2.1.1.4.1" class="ltx_td ltx_align_center ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite></td>
<td id="S4.F5.2.4.2.2.1.1.4.2" class="ltx_td ltx_align_center ltx_border_t">36.1</td>
<td id="S4.F5.2.4.2.2.1.1.4.3" class="ltx_td ltx_align_center ltx_border_t">88.0</td>
</tr>
<tr id="S4.F5.2.4.2.2.1.1.5" class="ltx_tr">
<td id="S4.F5.2.4.2.2.1.1.5.1" class="ltx_td ltx_align_center ltx_border_bb"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite></td>
<td id="S4.F5.2.4.2.2.1.1.5.2" class="ltx_td ltx_align_center ltx_border_bb">23.5</td>
<td id="S4.F5.2.4.2.2.1.1.5.3" class="ltx_td ltx_align_center ltx_border_bb">71.5</td>
</tr>
</table>
</span></div>
</td>
</tr>
<tr id="S4.F5.2.5.3" class="ltx_tr">
<th id="S4.F5.2.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">(c)</th>
<td id="S4.F5.2.5.3.2" class="ltx_td ltx_align_center">(d)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Comparison to the state of the art: (a) PCK3D on RH for varying error threshold (hand). (b) Percentage of images with correct face detections for varying 3DA-2D thresholds on Menpo (face). (c) PCK3D on MuPoTs (body). (d) 2D PCK at a threshold of 10% of the tight bounding boxâ€™s largest size on RH (hand) and 5% on Menpo (face). The higher the better.</figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<div id="S4.F6.1" class="ltx_block ltx_figure_panel">
<div id="S4.F6.1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:8.6pt;height:45.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:45.5pt;transform:translate(-18.46pt,-17.48pt) rotate(-90deg) ;">
<p id="S4.F6.1.1.1" class="ltx_p">image</p>
</span></div>
<img src="/html/2008.09457/assets/fig/mpii_vs_smplx/0856.jpg" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="189" height="106" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2008.09457/assets/fig/mpii_vs_smplx/0072.jpg" id="S4.F6.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="189" height="106" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2008.09457/assets/fig/mpii_vs_smplx/0048.jpg" id="S4.F6.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="189" height="106" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<div id="S4.F6.2" class="ltx_block ltx_figure_panel">
<div id="S4.F6.2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:10.0pt;height:114.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:114.7pt;transform:translate(-52.37pt,-51.12pt) rotate(-90deg) ;">
<p id="S4.F6.2.1.1" class="ltx_p">MTCÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite></p>
</span></div>
<img src="/html/2008.09457/assets/fig/mpii_vs_smplx/mtc_0856_fullbody.jpg" id="S4.F6.g4" class="ltx_graphics ltx_centering ltx_img_landscape" width="189" height="106" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2008.09457/assets/fig/mpii_vs_smplx/mtc_0072_fullbody_composed.jpg" id="S4.F6.g5" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="189" height="106" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2008.09457/assets/fig/mpii_vs_smplx/mtc_0048_fullbody.jpg" id="S4.F6.g6" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="189" height="106" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<div id="S4.F6.3" class="ltx_block ltx_figure_panel">
<div id="S4.F6.3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:10.0pt;height:127.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:127.8pt;transform:translate(-58.9pt,-57.65pt) rotate(-90deg) ;">
<p id="S4.F6.3.1.1" class="ltx_p">video-MTCÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite></p>
</span></div>
<img src="/html/2008.09457/assets/fig/mpii_vs_smplx/mtc_0856_fullbody_sequence.jpg" id="S4.F6.g7" class="ltx_graphics ltx_centering ltx_img_landscape" width="189" height="106" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2008.09457/assets/fig/mpii_vs_smplx/mtc_0072_fullbody_sequence_composed.jpg" id="S4.F6.g8" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="189" height="106" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2008.09457/assets/fig/mpii_vs_smplx/mtc_0048_fullbody_sequence.jpg" id="S4.F6.g9" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="189" height="106" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<div id="S4.F6.4" class="ltx_block ltx_figure_panel">
<div id="S4.F6.4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:10.0pt;height:100.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:100.1pt;transform:translate(-45.03pt,-43.78pt) rotate(-90deg) ;">
<p id="S4.F6.4.1.1" class="ltx_p">SMPLify-XÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite></p>
</span></div>
<img src="/html/2008.09457/assets/fig/mpii_vs_smplx/smplx_0856.jpg" id="S4.F6.g10" class="ltx_graphics ltx_centering ltx_img_landscape" width="189" height="106" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2008.09457/assets/fig/mpii_vs_smplx/smplx_0072.jpg" id="S4.F6.g11" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="189" height="106" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2008.09457/assets/fig/mpii_vs_smplx/smplx_0048.jpg" id="S4.F6.g12" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="189" height="106" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<div id="S4.F6.5" class="ltx_block ltx_figure_panel">
<div id="S4.F6.5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:6.8pt;height:59pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:59.0pt;transform:translate(-26.08pt,-26.08pt) rotate(-90deg) ;">
<p id="S4.F6.5.1.1" class="ltx_p"><span id="S4.F6.5.1.1.1" class="ltx_text ltx_font_bold">DOPE</span></p>
</span></div>
<img src="/html/2008.09457/assets/x10.jpg" id="S4.F6.g13" class="ltx_graphics ltx_centering ltx_img_portrait" width="209" height="600" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2008.09457/assets/x11.jpg" id="S4.F6.g14" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="209" height="600" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2008.09457/assets/x12.jpg" id="S4.F6.g15" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="209" height="600" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Three examples with from top to bottom the original image, the results from MTCÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> from static image or the video, SMPLify-XÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> and ours.</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Comparison to the state of the art</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">Comparison on individual tasks.</span> In FigureÂ <a href="#S4.F5" title="Figure 5 â€£ 4.2 Comparison to the experts â€£ 4 Experiments â€£ DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we compare our DOPE approach to the state of the art for each individual task.
Note that our main goal is not to outperform the state of the art on each of these tasks but rather to unify 3 individual models into a single one while still achieving a competitive performance.
DOPE is among the top performing methods for all three tasks, <em id="S4.SS3.p1.1.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS3.p1.1.3" class="ltx_text"></span>, hand (a), face (b) and body (c) 3D pose estimation, while being the first and only method to report on these three tasks together. Additionally, our detection network tackles a more difficult task than most of our competitors who assume that a bounding box around the ground-truth handsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite> or facesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> is given at test time. We also compare with existing whole-body 2D pose estimation methods (d).</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Qualitative comparison toÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>.</span>
Since there is no dataset to numerically compare the performances of our learning-based approach in the wild against the optimization-based pipelines such asÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>,
we show some qualitative examples in FigureÂ <a href="#S4.F6" title="Figure 6 â€£ 4.2 Comparison to the experts â€£ 4 Experiments â€£ DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.
We find that Monocular Total CaptureÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> performs quite poorly on static images (second row), in particular due to occlusions.
It greatly benefits from temporal information when processing the sequences from which the images are extracted (third row).
However, there are still some errors, especially in case of occlusions (<em id="S4.SS3.p2.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.SS3.p2.1.3" class="ltx_text"></span> legs in the left column image).
ForÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> (fourth row), in the first example, OpenPoseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> does not estimate the 2D location of the feet that fall out of the field of view, impacting the optimization of the modelâ€™s legs. In our case, the pose of the legs is correctly estimated. The same phenomenon happens in the second example where a little girl is kneeling and the self-occlusions prevent her feet from being detected. Finally, in the third example, the optimization gets stuck in a local minimum while our estimation is more robust.
In addition of its robustness in the wild, our learning-based approach is also about 1000 times faster thanÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> which takes about a minute per person in an image.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We have proposed DOPE, the first learning-based method to detect and estimate whole-body 3D human poses in the wild, including body, hand and face 2D-3D keypoints.
We tackled the lack of training data for this task by leveraging distillation from part experts to our whole-body network.
Our experiments validated this approach showing performances close to the part expertsâ€™ results.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Our method allows training a network on a more diverse set of in-the-wild images, potentially without any pose annotations. In future work, we will investigate if our model could benefit from additional unlabeled training data.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Andriluka, M., Pishchulin, L., Gehler, P., Schiele, B.: 2D human pose
estimation: New benchmark and state of the art analysis. In: CVPR (2014)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Armagan, A., Garcia-Hernando, G., Baek, S., Hampali, S., Rad, M., Zhang, Z.,
Xie, S., Chen, M., Zhang, B., Xiong, F., Xiao, Y., Cao, Z., Yuan, J., Ren,
P., Huang, W., Sun, H., HrÃºz, M., Kanis, J., Krnoul, Z., Wan, Q., Li,
S., Yang, L., Lee, D., Yao, A., Zhou, W., Mei, S., Liu, Y., Spurr, A., Iqbal,
U., Molchanov, P., Weinzaepfel, P., BrÃ©gier, R., Rogez, G., Lepetit,
V., Kim, T.: Measuring generalisation to unseen viewpoints, articulations,
shapes and objects for 3D hand pose estimation under hand-object
interaction. In: ECCV (2020)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Arnab, A., Doersch, C., Zisserman, A.: Exploiting temporal context for 3D
human pose estimation in the wild. In: CVPR (2019)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Bhardwaj, S., Srinivasan, M., Khapra, M.M.: Efficient video classification
using fewer frames. In: CVPR (2019)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Blanz, V., Vetter, T.: A morphable model for the synthesis of 3D faces. In:
SIGGRAPH (1999)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Bogo, F., Kanazawa, A., Lassner, C., Gehler, P., Romero, J., Black, M.J.: Keep
it SMPL: Automatic estimation of 3D human pose and shape from a single
image. In: ECCV (2016)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Booth, J., Roussos, A., Zafeiriou, S., Ponniahy, A., Dunaway, D.: A 3D
morphable model learnt from 10,000 faces. In: CVPR (2016)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Boukhayma, A., deÂ Bem, R., Torr, P.H.S.: 3D hand shape and pose from images
in the wild. In: CVPR (2019)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Bulat, A., Tzimiropoulos, G.: How far are we from solving the 2D &amp; 3D face
alignment problem? (and a dataset of 230,000 3D facial landmarks). In: ICCV
(2017)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Cai, Y., Ge, L., Cai, J., Yuan, J.: Weakly-supervised 3D hand pose estimation
from monocular RGB images. In: ECCV (2018)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Cao, C., Weng, Y., Zhou, S., Tong, Y., Zhou, K.: FaceWarehouse: a 3D facial
expression database for visual computing. IEEE Transactions on Visualization
and Computer Graphics (2013)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Cao, Z., Hidalgo, G., Simon, T., Wei, S.E., Sheikh, Y.: OpenPose: realtime
multi-person 2D pose estimation using Part Affinity Fields. In: arXiv
preprint arXiv:1812.08008 (2018)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Chen, C.H., Ramanan, D.: 3D human pose estimation = 2D pose estimation +
matching. In: CVPR (2017)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Chen, G., Choi, W., Yu, X., Han, T., Chandraker, M.: Learning efficient object
detection models with knowledge distillation. In: NeurIPS (2017)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Cimen, G., Maurhofer, C., Sumner, B., Guay, M.: Ar poser: Automatically
augmenting mobile pictures with digital avatars imitating poses. In: CGVCVIP
(2018)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Crasto, N., Weinzaepfel, P., Alahari, K., Schmid, C.: MARS: Motion-augmented
RGB stream for action recognition. In: CVPR (2019)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Crispell, D., Bazik, M.: Pix2Face: Direct 3D face model estimation. In:
ICCV Workshop (2017)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Deng, J., Roussos, A., Chrysos, G., Ververas, E., Kotsia, I., Shen, J.,
Zafeiriou, S.: The Menpo benchmark for multi-pose 2D and 3D facial
landmark localisation and tracking. IJCV (2019)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Fang, H.S., Xie, S., Tai, Y.W., Lu, C.: RMPE: Regional multi-person pose
estimation. In: ICCV (2017)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Feng, Y., Wu, F., Shao, X., Wang, Y., Zhou, X.: Joint 3D face reconstruction
and dense alignment with position map regression network. In: ECCV (2020)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Garcia-Salguero, M., Gonzalez-Jimenez, J., Moreno, F.A.: Human 3D pose
estimation with a tilting camera for social mobile robot interaction. Sensors
(2019)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Ge, L., Ren, Z., Li, Y., Xue, Z., Wang, Y., Cai, J., Yuan, J.: 3D hand shape
and pose estimation from a single RGB image. In: CVPR (2019)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Gui, L.Y., Zhang, K., Wang, Y.X., Liang, X., Moura, J.M., Veloso, M.: Teaching
robots to predict human motion. In: IROS (2018)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Habibie, I., Xu, W., Mehta, D., Pons-Moll, G., Theobalt, C.: In the wild human
pose estimation using explicit 2D features and intermediate 3D
representations. In: CVPR (2019)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Hampali, S., Rad, M., Oberweger, M., Lepetit, V.: Honnotate: A method for 3D
annotation of hand and objects poses. In: CVPR (2020)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Hasson, Y., Varol, G., Tzionas, D., Kalevatykh, I., Black, M.J., Laptev, I.,
Schmid, C.: Learning joint reconstruction of hands and manipulated objects.
In: CVPR (2019)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
He, K., Gkioxari, G., DollÃ¡r, P., Girshick, R.: Mask R-CNN. In: ICCV
(2017)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
recognition. In: CVPR (2016)

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Hidalgo, G., Raaj, Y., Idrees, H., Xiang, D., Joo, H., Simon, T., Sheikh, Y.:
Single-network whole-body pose estimation. In: ICCV (2019)

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural
network. In: NIPS workshop (2014)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Hoffman, J., Gupta, S., Darrell, T.: Learning with side information through
modality hallucination. In: CVPR (2016)

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Hou, S., Pan, X., ChangeÂ Loy, C., Wang, Z., Lin, D.: Lifelong learning via
progressive distillation and retrospection. In: ECCV (2018)

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Ionescu, C., Papava, D., Olaru, V., Sminchisescu, C.: Human3.6M: Large scale
datasets and predictive methods for 3D human sensing in natural
environments. IEEE trans. PAMI (2013)

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Jackson, A.S., Bulat, A., Argyriou, V., Tzimiropoulos, G.: Large pose 3D face
reconstruction from a single image via direct volumetric CNN regression.
In: ICCV (2017)

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Johnson, S., Everingham, M.: Clustered pose and nonlinear appearance models for
human pose estimation. In: BMVC (2010)

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Johnson, S., Everingham, M.: Learning effective human pose estimation from
inaccurate annotation. In: CVPR (2011)

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Joo, H., Simon, T., Sheikh, Y.: Total capture: A 3D deformation model for
tracking faces, hands, and bodies. In: CVPR (2018)

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Lassner, C., Romero, J., Kiefel, M., Bogo, F., Black, M.J., Gehler, P.V.: Unite
the people: Closing the loop between 3D and 2D human representations. In:
CVPR (2017)

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Lee, D.H.: Pseudo-label: The simple and efficient semi-supervised learning
method for deep neural networks. In: ICML Workshop (2013)

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Li, T., Bolkart, T., Black, M.J., Li, H., Romero, J.: Learning a model of
facial shape and expression from 4D scans. ACM Transactions on Graphics
(ToG) (2017)

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,
DollÃ¡r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In:
ECCV (2014)

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Liu, X., He, P., Chen, W., Gao, J.: Improving multi-task deep neural networks
via knowledge distillation for natural language understanding. arXiv preprint
arXiv:1904.09482 (2019)

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: SMPL: A
skinned multi-person linear model. ACM transactions on Graphics (2015)

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Lopez-Paz, D., Bottou, L., SchÃ¶lkopf, B., Vapnik, V.: Unifying distillation
and privileged information. In: ICLR (2016)

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Martinez, J., Hossain, R., Romero, J., Little, J.J.: A simple yet effective
baseline for 3D human pose estimation. In: ICCV (2017)

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Mehta, D., Sotnychenko, O., Mueller, F., Xu, W., Sridhar, S., Pons-Moll, G.,
Theobalt, C.: Single-shot multi-person 3D pose estimation from monocular
RGB. In: 3DV (2018)

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Mehta, D., Sridhar, S., Sotnychenko, O., Rhodin, H., Shafiei, M., Seidel, H.P.,
Xu, W., Casas, D., Theobalt, C.: VNect: Real-time 3D human pose
estimation with a single RGB camera. ACM Transactions on Graphics (2017)

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Moon, G., Chang, J.Y., Lee, K.M.: Camera distance-aware top-down approach for
3D multi-person pose estimation from a single RGB image. In: ICCV (2019)

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Mueller, F., Bernard, F., Sotnychenko, O., Mehta, D., Sridhar, S., Casas, D.,
Theobalt, C.: GANerated hands for real-time 3D hand tracking from
monocular RGB. In: CVPR (2018)

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,
DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
Bai, J., Chintala, S.: Pytorch: An imperative style, high-performance deep
learning library. In: NeurIPS (2019)

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Pavlakos, G., Choutas, V., Ghorbani, N., Bolkart, T., Osman, A.A., Tzionas, D.,
Black, M.J.: Expressive body capture: 3D hands, face, and body from a
single image. In: CVPR (2019)

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Pavlakos, G., Zhou, X., Derpanis, K.G., Daniilidis, K.: Coarse-to-fine
volumetric prediction for single-image 3D human pose. In: CVPR (2017)

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time
object detection with region proposal networks. In: NIPS (2015)

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Rogez, G., Schmid, C.: Mocap-guided data augmentation for 3D pose estimation
in the wild. In: NIPS (2016)

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Rogez, G., Weinzaepfel, P., Schmid, C.: LCR-Net++: Multi-person 2D and 3D
pose detection in natural images. IEEE trans. PAMI (2019)

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Romero, J., Tzionas, D., Black, M.J.: Embodied hands: modeling and capturing
hands and bodies together. ACM Transactions on Graphics (2017)

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Sanyal, S., Bolkart, T., Feng, H., Black, M.J.: Learning to regress 3D face
shape and expression from an image without 3D supervision. In: CVPR
(2019)

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Simon, T., Joo, H., Matthews, I., Sheikh, Y.: Hand keypoint detection in single
images using multiview bootstrapping. In: CVPR (2017)

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Spurr, A., Song, J., Park, S., Hilliges, O.: Cross-modal deep variational hand
pose estimation. In: CVPR (2018)

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
SupanÄiÄ, J.S., Rogez, G., Yang, Y., Shotton, J., Ramanan, D.: Depth-based
hand pose estimation: Methods, data, and challenges. IJCV (2018)

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Vapnik, V., Izmailov, R.: Learning using privileged information: Similarity
control and knowledge transfer. JMLR (2015)

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Varol, G., Romero, J., Martin, X., Mahmood, N., Black, M.J., Laptev, I.,
Schmid, C.: Learning from synthetic humans. In: CVPR (2017)

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Wu, Y., Ji, Q.: Facial landmark detection: a literature survey. IJCV (2019)

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Xiang, D., Joo, H., Sheikh, Y.: Monocular total capture: Posing face, body, and
hands in the wild. In: CVPR (2019)

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Xiong, P., Li, G., Sun, Y.: Combining local and global features for 3D face
tracking. In: ICCV Workshops (2017)

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Yang, L., Li, S., Lee, D., Yao, A.: Aligning latent spaces for 3D hand pose
estimation. In: ICCV (2019)

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Yuan, S., Stenger, B., Kim, T.K.: RGB-based 3D hand pose estimation via
privileged learning with depth images. arXiv preprint arXiv:1811.07376
(2018)

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Zadeh, A., Baltrusaitis, T., Morency, L.P.: Convolutional experts constrained
local model for facial landmark detection. In: CVPR Workshop (2017)

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Zafeiriou, S., Chrysos, G., Roussos, A., Ververas, E., Deng, J., Trigeorgis,
G.: The 3D menpo facial landmark tracking challenge. In: ICCV Workshops
(2017)

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Zhang, J., Jiao, J., Chen, M., Qu, L., Xu, X., Yang, Q.: A hand pose tracking
benchmark from stereo matching. In: ICIP (2017)

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Zhang, X., Li, Q., Mo, H., Zhang, W., Zheng, W.: End-to-end hand mesh recovery
from a monocular RGB image. In: ICCV (2019)

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Zhu, X., Liu, X., Lei, Z., Li, S.Z.: Face alignment in full pose range: A 3D
total solution. IEEE trans. PAMI (2017)

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Zimmermann, C., Brox, T.: Learning to estimate 3D hand pose from single RGB
images. In: ICCV (2017)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2008.09456" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2008.09457" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2008.09457">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2008.09457" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2008.09458" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 09:43:51 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
