<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2305.09948] HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models</title><meta property="og:description" content="Human-Object Interaction (HOI) detection is a task to localize humans and objects in an image and predict the interactions in human-object pairs.
In real-world scenarios, HOI detection models are required systematic ge…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2305.09948">

<!--Generated on Thu Feb 29 07:13:04 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id1.1.id1" class="ltx_text ltx_font_bold">Kentaro Takemoto  </span><span id="id2.2.id2" class="ltx_text ltx_font_italic" style="font-size:90%;">k.takemoto@fujitsu.com 
<br class="ltx_break">Artificial Intelligence Laboratory
<br class="ltx_break">Fujitsu Limited
</span><span id="id3.3.id3" class="ltx_text ltx_font_bold">Moyuru Yamada  </span><span id="id4.4.id4" class="ltx_text ltx_font_italic" style="font-size:90%;">yamada.moyuru@fujitsu.com 
<br class="ltx_break">Artificial Intelligence Laboratory
<br class="ltx_break">Fujitsu Limited
</span><span id="id5.5.id5" class="ltx_text ltx_font_bold">Tomotake Sasaki  </span><span id="id6.6.id6" class="ltx_text ltx_font_italic" style="font-size:90%;">tomotake.sasaki@fujitsu.com 
<br class="ltx_break">Artificial Intelligence Laboratory
<br class="ltx_break">Fujitsu Limited
</span><span id="id7.7.id7" class="ltx_text ltx_font_bold">Hisanao Akima  </span><span id="id8.8.id8" class="ltx_text ltx_font_italic" style="font-size:90%;">akima.hisanao@fujitsu.com 
<br class="ltx_break">Artificial Intelligence Laboratory
<br class="ltx_break">Fujitsu Limited
</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id9.id1" class="ltx_p">Human-Object Interaction (HOI) detection is a task to localize humans and objects in an image and predict the interactions in human-object pairs.
In real-world scenarios, HOI detection models are required systematic generalization, i.e., generalization to novel combinations of objects and interactions, because the train data are expected to cover a limited portion of all possible combinations.
However, to our knowledge, no open benchmarks or previous work exist for evaluating the systematic generalization performance of HOI detection models.
To address this issue, we created two new sets of HOI detection data splits named HICO-DET-SG and V-COCO-SG based on the HICO-DET and V-COCO datasets, respectively.
When evaluated on the new data splits, the representative HOI detection models performed much more poorly than when evaluated on the original splits.
This reveals that systematic generalization is a challenging goal in HOI detection.
By analyzing the evaluation results, we also gain insights for improving the systematic generalization performance and identify four possible future research directions.
We hope that our new data splits and presented analysis will encourage further research on systematic generalization in HOI detection.
</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Human-Object Interaction (HOI) detection is a task to localize humans and objects in an image and predict the interactions in human-object pairs.
HOI detection has been attracting large interest in computer vision, as it is useful for various applications such as self-driving cars, anomaly detection, analysis of surveillance video, and so on.
The outputs of this task are typically represented as &lt;human, <span id="S1.p1.1.1" class="ltx_text ltx_font_bold">interaction</span>, <span id="S1.p1.1.2" class="ltx_text ltx_framed ltx_framed_underline">object</span>&gt; triplets.
The publication of HOI detection datasets <cite class="ltx_cite ltx_citemacro_citep">(Koppula et al., <a href="#bib.bib32" title="" class="ltx_ref">2013</a>; Everingham et al., <a href="#bib.bib13" title="" class="ltx_ref">2014</a>; Gupta &amp; Malik, <a href="#bib.bib19" title="" class="ltx_ref">2015</a>; Chao et al., <a href="#bib.bib8" title="" class="ltx_ref">2018</a>; Gu et al., <a href="#bib.bib18" title="" class="ltx_ref">2018</a>; Chiou et al., <a href="#bib.bib11" title="" class="ltx_ref">2021</a>)</cite> has triggered a large number of studies on this task <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a href="#bib.bib15" title="" class="ltx_ref">2018</a>; Gkioxari et al., <a href="#bib.bib17" title="" class="ltx_ref">2018</a>; Li et al., <a href="#bib.bib36" title="" class="ltx_ref">2019</a>; Gao et al., <a href="#bib.bib16" title="" class="ltx_ref">2020</a>; Li et al., <a href="#bib.bib37" title="" class="ltx_ref">2020</a>; Liao et al., <a href="#bib.bib38" title="" class="ltx_ref">2020</a>; Kim et al., <a href="#bib.bib30" title="" class="ltx_ref">2021</a>; Chen &amp; Yanai, <a href="#bib.bib9" title="" class="ltx_ref">2021</a>; Tamura et al., <a href="#bib.bib52" title="" class="ltx_ref">2021</a>; Zhang et al., <a href="#bib.bib61" title="" class="ltx_ref">2021</a>; Chen et al., <a href="#bib.bib10" title="" class="ltx_ref">2021</a>; Zou et al., <a href="#bib.bib64" title="" class="ltx_ref">2021</a>; Zhang et al., <a href="#bib.bib62" title="" class="ltx_ref">2022</a>; Liao et al., <a href="#bib.bib39" title="" class="ltx_ref">2022</a>; Ma et al., <a href="#bib.bib45" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">HOI detection is
an advanced computer vision task as it requires a model not only to localize humans and objects but also to predict interactions between them.
Moreover, humans can have different interactions with the same object (e.g., <span id="S1.p2.1.1" class="ltx_text ltx_font_bold">wash</span> a horse and <span id="S1.p2.1.2" class="ltx_text ltx_font_bold">walk</span> a horse) and the same interaction with different objects (e.g., wash a <span id="S1.p2.1.3" class="ltx_text ltx_framed ltx_framed_underline">horse</span> and wash a <span id="S1.p2.1.4" class="ltx_text ltx_framed ltx_framed_underline">car</span>). In real-world scenarios, the train data will likely cover a limited portion of all possible combinations of objects and interactions of interest.
Thus, HOI detection models must be generalizable to novel combinations of known objects and interactions.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">Such generalization to novel combinations of known concepts, called systematic generalization,
is a highly desired property for machine learning models.
The systematic generalization performance is evaluated in various tasks such as sequence-to-sequence parsing <cite class="ltx_cite ltx_citemacro_citep">(Lake &amp; Baroni, <a href="#bib.bib33" title="" class="ltx_ref">2018</a>)</cite>, language understanding <cite class="ltx_cite ltx_citemacro_citep">(Ruis et al., <a href="#bib.bib49" title="" class="ltx_ref">2020</a>; Bergen et al., <a href="#bib.bib5" title="" class="ltx_ref">2021</a>)</cite>, visual properties extraction <cite class="ltx_cite ltx_citemacro_citep">(Ullman et al., <a href="#bib.bib55" title="" class="ltx_ref">2021</a>)</cite>, and visual question answering <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al., <a href="#bib.bib25" title="" class="ltx_ref">2017</a>; Bahdanau et al., <a href="#bib.bib1" title="" class="ltx_ref">2019</a>; <a href="#bib.bib2" title="" class="ltx_ref">2020</a>; D’Amario et al., <a href="#bib.bib12" title="" class="ltx_ref">2021</a>; Hsu et al., <a href="#bib.bib22" title="" class="ltx_ref">2022</a>; Yamada et al., <a href="#bib.bib59" title="" class="ltx_ref">2023</a>; Kamata et al., <a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite>.
However, to our knowledge, no open benchmarks or evaluation studies have been published for the systematic generalization in HOI detection.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">The existing HOI detection datasets cannot be used for evaluating the systematic generalization performance for novel combinations of known objects and interactions as they are, because their train and test data provide the same object-interaction combinations.
On such datasets, a model might predict an interaction class based solely on the paired object class or an object class based solely on the paired interaction class rather than by capturing the visual cues such as human posture and positional relations.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">In this paper, we introduce two new sets of HOI detection data splits named HICO-DET-SG and V-COCO-SG, which we created based on the HICO-DET <cite class="ltx_cite ltx_citemacro_citep">(Chao et al., <a href="#bib.bib8" title="" class="ltx_ref">2018</a>)</cite> and V-COCO <cite class="ltx_cite ltx_citemacro_citep">(Gupta &amp; Malik, <a href="#bib.bib19" title="" class="ltx_ref">2015</a>)</cite> datasets, respectively, for evaluating the systematic generalization (SG) capabilities of HOI detection models.
An illustration of such data split is shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2305.09948/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="216" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.14.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.15.2" class="ltx_text" style="font-size:90%;">Illustration of a data split for evaluating the systematic generalization performance of Human-Object Interaction (HOI) detection models. All images and annotations are selected from HICO-DET-SG split3. The train data consists of combinations such as &lt;human, <span id="S1.F1.15.2.1" class="ltx_text ltx_font_bold">wash</span>, <span id="S1.F1.15.2.2" class="ltx_text ltx_framed ltx_framed_underline">car</span>&gt;, &lt;human, <span id="S1.F1.15.2.3" class="ltx_text ltx_font_bold">wash</span>, <span id="S1.F1.15.2.4" class="ltx_text ltx_framed ltx_framed_underline">elephant</span>&gt;, &lt;human, <span id="S1.F1.15.2.5" class="ltx_text ltx_font_bold">walk</span>, <span id="S1.F1.15.2.6" class="ltx_text ltx_framed ltx_framed_underline">horse</span>&gt;, and &lt;human, <span id="S1.F1.15.2.7" class="ltx_text ltx_font_bold">straddle</span>, <span id="S1.F1.15.2.8" class="ltx_text ltx_framed ltx_framed_underline">horse</span>&gt;. After trained on such data, an HOI detection model is tested whether it can generalize to novel combinations in the test data such as &lt;human, <span id="S1.F1.15.2.9" class="ltx_text ltx_font_bold">wash</span>, <span id="S1.F1.15.2.10" class="ltx_text ltx_framed ltx_framed_underline">horse</span>&gt;. To systematically generalize to such novel combinations, the model must learn the visual cues of the object (in this case, <span id="S1.F1.15.2.11" class="ltx_text ltx_framed ltx_framed_underline">horse</span>) and the interaction (in this case, <span id="S1.F1.15.2.12" class="ltx_text ltx_font_bold">wash</span>) independently of the specifically paired interaction/object classes in the train data.</span></figcaption>
</figure>
<div id="S1.p6" class="ltx_para ltx_noindent">
<p id="S1.p6.1" class="ltx_p">To ensure that the test performance is not an artifact of a specific selection of combinations in the train and test data,
we prepared three distinct train-test splits of HICO-DET-SG and V-COCO-SG, respectively.
We evaluated recent representative HOI detection models on our data splits and found a large degradation from the performances on the original data splits. We also analyzed the results and gained insights to improve systematic generalization performance of HOI detection models.</p>
</div>
<div id="S1.p7" class="ltx_para ltx_noindent">
<p id="S1.p7.1" class="ltx_p">Our contributions are summarized below:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We created two new sets of HOI detection data splits with no overlapping object-interaction combinations in train and test data, which serve for studying systematic generalization in HOI detection.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We evaluated the systematic generalization performances of representative HOI detection models on our new data splits and found large decreases in the test performance from those on the original splits; this reveals that the systematic generalization is a challenging goal in HOI detection.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.i3.p1.1" class="ltx_p">We derived four possible future directions to improve systematic generalization performance in HOI detection based on the analysis and considerations on our experimental results and related work: 1) increasing the diversity of the train data, 2) introducing two-stage or other modular structures into a model, 3) utilizing pretraining, and 4) integrating commonsense knowledge from external natural language resources.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p8" class="ltx_para ltx_noindent">
<p id="S1.p8.1" class="ltx_p">The JSON files determining HICO-DET-SG and V-COCO-SG and the source code that created the files are publicly available at <a target="_blank" href="https://github.com/FujitsuResearch/hoi_sg" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/FujitsuResearch/hoi_sg</a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">In this section, we first briefly review the HOI detection task. Then we explain the related work to the new research topic we deal with in this study, namely, systematic generalization in HOI detection.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Overview of Human-Object Interaction (HOI) detection</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">As explained in the Introduction, HOI detection is a task to localize humans and objects in an image and predict the interactions between them.
The HICO-DET <cite class="ltx_cite ltx_citemacro_citep">(Chao et al., <a href="#bib.bib8" title="" class="ltx_ref">2018</a>)</cite> and V-COCO <cite class="ltx_cite ltx_citemacro_citep">(Gupta &amp; Malik, <a href="#bib.bib19" title="" class="ltx_ref">2015</a>)</cite> are the two most popular datasets for HOI detection.
Preceding to the HICO-DET dataset, the HICO dataset <cite class="ltx_cite ltx_citemacro_citep">(Chao et al., <a href="#bib.bib7" title="" class="ltx_ref">2015</a>)</cite> was created for HOI recognition task, which classifies an object and a human’s interaction with that object in an image without bounding-boxes. Subsequently, HICO-DET dataset was created based on HICO by adding bounding-boxes around the humans and objects in the images. Moreover, one image in the dataset is associated with multiple human, object, and interaction annotations.
The V-COCO dataset was created based on the Microsoft COCO object detection dataset <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib41" title="" class="ltx_ref">2014</a>)</cite> by adding annotations of interactions (verbs).
Statistics of the HICO-DET and V-COCO datasets are given in Table <a href="#S3.T1" title="Table 1 ‣ 3.2 Statistics of the HICO-DET-SG and V-COCO-SG ‣ 3 HICO-DET-SG and V-COCO-SG ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.1" class="ltx_p">By definition, HOI detection consists of two tasks: localizing the human and object instances in a given image and predicting the interactions between them.
Accordingly, there are two types of model architectures to solve the HOI detection task: two-stage model and one-stage model.
Two-stage models <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a href="#bib.bib15" title="" class="ltx_ref">2018</a>; Li et al., <a href="#bib.bib36" title="" class="ltx_ref">2019</a>; Gao et al., <a href="#bib.bib16" title="" class="ltx_ref">2020</a>; Li et al., <a href="#bib.bib37" title="" class="ltx_ref">2020</a>; Liao et al., <a href="#bib.bib39" title="" class="ltx_ref">2022</a>; Zhang et al., <a href="#bib.bib62" title="" class="ltx_ref">2022</a>)</cite> detect humans and objects at the first stage and then classify the interactions in all human-object pairs at the second stage. Aiming to improve both the instance detection and interaction classification via multi-task learning and to reduce inference time, one-stage models <cite class="ltx_cite ltx_citemacro_citep">(Gkioxari et al., <a href="#bib.bib17" title="" class="ltx_ref">2018</a>; Liao et al., <a href="#bib.bib38" title="" class="ltx_ref">2020</a>)</cite> have been proposed recently and gained popularity over two-stage models.
Some recent one-stage models <cite class="ltx_cite ltx_citemacro_citep">(Kim et al., <a href="#bib.bib30" title="" class="ltx_ref">2021</a>; Tamura et al., <a href="#bib.bib52" title="" class="ltx_ref">2021</a>; Chen &amp; Yanai, <a href="#bib.bib9" title="" class="ltx_ref">2021</a>; Zhang et al., <a href="#bib.bib61" title="" class="ltx_ref">2021</a>; Chen et al., <a href="#bib.bib10" title="" class="ltx_ref">2021</a>; Zou et al., <a href="#bib.bib64" title="" class="ltx_ref">2021</a>; Ma et al., <a href="#bib.bib45" title="" class="ltx_ref">2023</a>)</cite> are based on the Transformer architecture <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a href="#bib.bib58" title="" class="ltx_ref">2017</a>)</cite>, which is designed to capture long-range relationships in an input and has been successful in natural language processing <cite class="ltx_cite ltx_citemacro_citep">(Kalyan et al., <a href="#bib.bib26" title="" class="ltx_ref">2021</a>; Lin et al., <a href="#bib.bib40" title="" class="ltx_ref">2022</a>)</cite> and computer vision <cite class="ltx_cite ltx_citemacro_citep">(Khan et al., <a href="#bib.bib29" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.p3.1" class="ltx_p">HOI detection is closely related to Scene Graph Generation (SGG) <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al., <a href="#bib.bib24" title="" class="ltx_ref">2015</a>)</cite>, which is a task to generate a visually-grounded scene graph that most accurately correlates with an image. A scene graph consists of nodes corresponding to object bounding boxes with their object categories, and edges indicating the pairwise relations between objects.
While HOI detection is closely related to SGG, HOI detection differs from SGG in two main ways.
First, the subjects in SGG can be of any type (humans, cars, etc.), whereas in HOI detection they are only humans.
Second, the relations in SGG can be both positional relations (e.g., next to) and actions (e.g., play with), whereas in HOI detection they only consist of the latter.
Therefore, HOI detection can be regarded as a subset of SGG in a sense.
On the other hand, HOI detection can also be regarded as a task focusing on measuring a model’s ability for complex scene understanding, because action recognition requires additional information, not merely the locations of humans and objects.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Studies related to systematic generalization in HOI detection</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">Systematic generalization <cite class="ltx_cite ltx_citemacro_citep">(Lake &amp; Baroni, <a href="#bib.bib33" title="" class="ltx_ref">2018</a>; Bahdanau et al., <a href="#bib.bib1" title="" class="ltx_ref">2019</a>; Ruis et al., <a href="#bib.bib49" title="" class="ltx_ref">2020</a>; Bahdanau et al., <a href="#bib.bib2" title="" class="ltx_ref">2020</a>; Bergen et al., <a href="#bib.bib5" title="" class="ltx_ref">2021</a>; D’Amario et al., <a href="#bib.bib12" title="" class="ltx_ref">2021</a>; Yamada et al., <a href="#bib.bib59" title="" class="ltx_ref">2023</a>; Kamata et al., <a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite>, also referred to as compositional generalization <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al., <a href="#bib.bib25" title="" class="ltx_ref">2017</a>; Kim &amp; Linzen, <a href="#bib.bib31" title="" class="ltx_ref">2020</a>; Hsu et al., <a href="#bib.bib22" title="" class="ltx_ref">2022</a>)</cite> or combinatorial generalization <cite class="ltx_cite ltx_citemacro_citep">(Vankov &amp; Bowers, <a href="#bib.bib57" title="" class="ltx_ref">2020</a>; Ullman et al., <a href="#bib.bib55" title="" class="ltx_ref">2021</a>)</cite>, is a special case of Out-of-Distribution (OoD) generalization, i.e., generalization to data distributions that differ from the train data <cite class="ltx_cite ltx_citemacro_citep">(Teney et al., <a href="#bib.bib54" title="" class="ltx_ref">2020</a>; Hendrycks et al., <a href="#bib.bib21" title="" class="ltx_ref">2021</a>; Shen et al., <a href="#bib.bib50" title="" class="ltx_ref">2021</a>; Ye et al., <a href="#bib.bib60" title="" class="ltx_ref">2022</a>)</cite>.
Among many types of OoD generalization, systematic generalization has been particularly considered as a hallmark of human intelligence and contrasted to the properties of artificial neural networks in each era <cite class="ltx_cite ltx_citemacro_citep">(Fodor &amp; Pylyshyn, <a href="#bib.bib14" title="" class="ltx_ref">1988</a>; Marcus, <a href="#bib.bib47" title="" class="ltx_ref">2001</a>; van der Velde et al., <a href="#bib.bib56" title="" class="ltx_ref">2004</a>; Lake et al., <a href="#bib.bib34" title="" class="ltx_ref">2017</a>; <a href="#bib.bib35" title="" class="ltx_ref">2019</a>; Baroni, <a href="#bib.bib4" title="" class="ltx_ref">2020</a>; Smolensky et al., <a href="#bib.bib51" title="" class="ltx_ref">2022</a>)</cite>.
Recently, the systematic generalization capability of deep neural networks, including Transformer-based models, has been actively studied in various tasks, as explained in the Introduction.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.1" class="ltx_p">In HOI detection, HICO-DET dataset provides rare-triplets evaluation to measure the few-shot generalization ability of the models (rare-triplets are defined as those which appear less than 10 times in the train data). Generalization to rare-triplets is a type of OoD generalization and some existing work <cite class="ltx_cite ltx_citemacro_citep">(Baldassarre et al., <a href="#bib.bib3" title="" class="ltx_ref">2020</a>; Ji et al., <a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite> attempted to improve this performance.
However, to our knowledge, no benchmarks or previous work have tackled systematic generalization, i.e., zero-shot generalization, in HOI detection. We present the first data splits for evaluating the systematic generalization performance and benchmark results of the representative HOI models in this paper.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.p3.1" class="ltx_p">In SGG, there are some previous work to evaluate <cite class="ltx_cite ltx_citemacro_citep">(Tang et al., <a href="#bib.bib53" title="" class="ltx_ref">2020</a>)</cite> and improve <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib44" title="" class="ltx_ref">2016</a>; Kan et al., <a href="#bib.bib28" title="" class="ltx_ref">2021</a>)</cite> systematic generalization for new combinations of subject, relation and object classes under the name of zero-shot generalization.
All studies revealed large performance degradations in the systematic generalization compared to the in-distribution generalization (generalization within the same combinations as the train data) unless some techniques are intentionally used.
As explained in the previous subsection, HOI detection can be regarded as a subset of SGG focusing on measuring a model’s capability for complex scene understanding.
Thus, we regard the improvement of systematic generalization performance in HOI detection as an early step toward better models for SGG and other visual understanding tasks.
</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>HICO-DET-SG and V-COCO-SG</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">In this section, we first explain the creation process of HICO-DET-SG and V-COCO-SG, the novel data splits for evaluating systematic generalization performance in HOI detection task.
We then present the statistics of HICO-DET-SG and V-COCO-SG.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>The creation process of the systematic generalization (SG) splits</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">When designing a pair of train data and test data of a systematic generalization (SG) split, we disallowed overlapping object–interaction combination classes between the train and test data
so that HOI detection models are required to generalize to novel combinations.
For example, in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the train data consists of combinations such as &lt;human, <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">wash</span>, <span id="S3.SS1.p1.1.2" class="ltx_text ltx_framed ltx_framed_underline">car</span>&gt;, &lt;human, <span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_bold">wash</span>, <span id="S3.SS1.p1.1.4" class="ltx_text ltx_framed ltx_framed_underline">elephant</span>&gt;, &lt;human, <span id="S3.SS1.p1.1.5" class="ltx_text ltx_font_bold">walk</span>, <span id="S3.SS1.p1.1.6" class="ltx_text ltx_framed ltx_framed_underline">horse</span>&gt;, and &lt;human, <span id="S3.SS1.p1.1.7" class="ltx_text ltx_font_bold">straddle</span>, <span id="S3.SS1.p1.1.8" class="ltx_text ltx_framed ltx_framed_underline">horse</span>&gt;. An HOI detection model is tested whether it can generalize to novel combinations in the test data, such as &lt;human, <span id="S3.SS1.p1.1.9" class="ltx_text ltx_font_bold">wash</span>, <span id="S3.SS1.p1.1.10" class="ltx_text ltx_framed ltx_framed_underline">horse</span>&gt;.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p">In the train data, we ensure that every object class is paired with multiple interaction classes, and that every interaction class is paired with multiple object classes.
This split design makes it possible for a model to learn the concepts of object/interaction themselves independently from the specific interaction/object classes paired in the train data.
To ensure that the test performance is not an artifact of a specific selection of combinations in the train and the test data, we prepared three distinct train-test splits of the HICO-DET-SG and V-COCO-SG, respectively.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p">When designing a test data, we ensure that there is no image containing the same object-interaction combination classes as the train data. Consequently, the SG splits contain fewer images and HOIs in total than the original splits.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.1" class="ltx_p">The creation process of the SG splits is further detailed in Appendix <a href="#A1" title="Appendix A Further details of the creation process of systematic generalization (SG) splits ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Statistics of the HICO-DET-SG and V-COCO-SG</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">The new HICO-DET-SG data splits were created based on the HICO-DET dataset <cite class="ltx_cite ltx_citemacro_citep">(Chao et al., <a href="#bib.bib8" title="" class="ltx_ref">2018</a>)</cite> as explained above.
The statistics of the original HICO-DET dataset and the HICO-DET-SG data splits are given in Table <a href="#S3.T1" title="Table 1 ‣ 3.2 Statistics of the HICO-DET-SG and V-COCO-SG ‣ 3 HICO-DET-SG and V-COCO-SG ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (upper half).
The original HICO-DET dataset contains 80 classes of objects, 117 classes of interactions, and 600 classes of object-interaction combinations.
In the HICO-DET-SG splits, 540 out of 600 object-interaction combination classes are assigned to the train data; the remaining 60 classes are assigned to the test data.
</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p">The new V-COCO-SG data splits were created by the same process based on the V-COCO dataset <cite class="ltx_cite ltx_citemacro_citep">(Gupta &amp; Malik, <a href="#bib.bib19" title="" class="ltx_ref">2015</a>)</cite>.
The statistics of the original V-COCO dataset and the V-COCO-SG splits are given in Table <a href="#S3.T1" title="Table 1 ‣ 3.2 Statistics of the HICO-DET-SG and V-COCO-SG ‣ 3 HICO-DET-SG and V-COCO-SG ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (lower half).
The original V-COCO dataset contains 80 classes of objects, 29 classes of interactions, and 228 classes of object-interaction combinations.
In the V-COCO-SG splits, 160 out of 228 object-interaction combination classes are assigned to the train data; the remaining 68 classes are assigned to the test data.
</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.3.2" class="ltx_text" style="font-size:90%;">Statistics of the HICO-DET-SG and V-COCO-SG as well as the original HICO-DET and V-COCO. The train and test data of the systematic generalization (SG) splits are composed of non-overlapping object-interaction combination classes. The test data of the SG splits contain fewer images and HOI triplets than the original test data because when designing the test data we ensure that there is no image containing the same object-interaction combination classes as the train data.</span></figcaption>
<table id="S3.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.4.1.1" class="ltx_tr">
<th id="S3.T1.4.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S3.T1.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"># of images</th>
<th id="S3.T1.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"># of HOI triplets</th>
<th id="S3.T1.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4"># of object-interaction comb. classes</th>
</tr>
<tr id="S3.T1.4.2.2" class="ltx_tr">
<th id="S3.T1.4.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r">Data splits</th>
<th id="S3.T1.4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">train</th>
<th id="S3.T1.4.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">test</th>
<th id="S3.T1.4.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">train</th>
<th id="S3.T1.4.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">test</th>
<th id="S3.T1.4.2.2.6" class="ltx_td ltx_th ltx_th_column"></th>
<th id="S3.T1.4.2.2.7" class="ltx_td ltx_th ltx_th_column"></th>
<th id="S3.T1.4.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column">    train</th>
<th id="S3.T1.4.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column">test</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.4.3.1" class="ltx_tr">
<th id="S3.T1.4.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Original HICO-DET</th>
<td id="S3.T1.4.3.1.2" class="ltx_td ltx_align_center ltx_border_t">38,118</td>
<td id="S3.T1.4.3.1.3" class="ltx_td ltx_align_center ltx_border_t">9,061</td>
<td id="S3.T1.4.3.1.4" class="ltx_td ltx_align_center ltx_border_t">117,871</td>
<td id="S3.T1.4.3.1.5" class="ltx_td ltx_align_center ltx_border_t">33,405</td>
<td id="S3.T1.4.3.1.6" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.4.3.1.7" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.4.3.1.8" class="ltx_td ltx_align_center ltx_border_t">    600</td>
<td id="S3.T1.4.3.1.9" class="ltx_td ltx_align_center ltx_border_t">600</td>
</tr>
<tr id="S3.T1.4.4.2" class="ltx_tr">
<th id="S3.T1.4.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">HICO-DET-SG split1</th>
<td id="S3.T1.4.4.2.2" class="ltx_td ltx_align_center">38,312</td>
<td id="S3.T1.4.4.2.3" class="ltx_td ltx_align_center">8,515</td>
<td id="S3.T1.4.4.2.4" class="ltx_td ltx_align_center">119,331</td>
<td id="S3.T1.4.4.2.5" class="ltx_td ltx_align_center">14,475</td>
<td id="S3.T1.4.4.2.6" class="ltx_td"></td>
<td id="S3.T1.4.4.2.7" class="ltx_td"></td>
<td id="S3.T1.4.4.2.8" class="ltx_td ltx_align_center">    540</td>
<td id="S3.T1.4.4.2.9" class="ltx_td ltx_align_center">60</td>
</tr>
<tr id="S3.T1.4.5.3" class="ltx_tr">
<th id="S3.T1.4.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">HICO-DET-SG split2</th>
<td id="S3.T1.4.5.3.2" class="ltx_td ltx_align_center">39,213</td>
<td id="S3.T1.4.5.3.3" class="ltx_td ltx_align_center">7,656</td>
<td id="S3.T1.4.5.3.4" class="ltx_td ltx_align_center">122,299</td>
<td id="S3.T1.4.5.3.5" class="ltx_td ltx_align_center">14,811</td>
<td id="S3.T1.4.5.3.6" class="ltx_td"></td>
<td id="S3.T1.4.5.3.7" class="ltx_td"></td>
<td id="S3.T1.4.5.3.8" class="ltx_td ltx_align_center">    540</td>
<td id="S3.T1.4.5.3.9" class="ltx_td ltx_align_center">60</td>
</tr>
<tr id="S3.T1.4.6.4" class="ltx_tr">
<th id="S3.T1.4.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">HICO-DET-SG split3</th>
<td id="S3.T1.4.6.4.2" class="ltx_td ltx_align_center">40,672</td>
<td id="S3.T1.4.6.4.3" class="ltx_td ltx_align_center">6,229</td>
<td id="S3.T1.4.6.4.4" class="ltx_td ltx_align_center">120,096</td>
<td id="S3.T1.4.6.4.5" class="ltx_td ltx_align_center">8,994</td>
<td id="S3.T1.4.6.4.6" class="ltx_td"></td>
<td id="S3.T1.4.6.4.7" class="ltx_td"></td>
<td id="S3.T1.4.6.4.8" class="ltx_td ltx_align_center">    540</td>
<td id="S3.T1.4.6.4.9" class="ltx_td ltx_align_center">60</td>
</tr>
<tr id="S3.T1.4.7.5" class="ltx_tr">
<th id="S3.T1.4.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Original V-COCO</th>
<td id="S3.T1.4.7.5.2" class="ltx_td ltx_align_center ltx_border_t">5,400</td>
<td id="S3.T1.4.7.5.3" class="ltx_td ltx_align_center ltx_border_t">4,946</td>
<td id="S3.T1.4.7.5.4" class="ltx_td ltx_align_center ltx_border_t">14,153</td>
<td id="S3.T1.4.7.5.5" class="ltx_td ltx_align_center ltx_border_t">12,649</td>
<td id="S3.T1.4.7.5.6" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.4.7.5.7" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.4.7.5.8" class="ltx_td ltx_align_center ltx_border_t">    228</td>
<td id="S3.T1.4.7.5.9" class="ltx_td ltx_align_center ltx_border_t">228</td>
</tr>
<tr id="S3.T1.4.8.6" class="ltx_tr">
<th id="S3.T1.4.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">V-COCO-SG split1</th>
<td id="S3.T1.4.8.6.2" class="ltx_td ltx_align_center">7,297</td>
<td id="S3.T1.4.8.6.3" class="ltx_td ltx_align_center">2,850</td>
<td id="S3.T1.4.8.6.4" class="ltx_td ltx_align_center">18,214</td>
<td id="S3.T1.4.8.6.5" class="ltx_td ltx_align_center">3,872</td>
<td id="S3.T1.4.8.6.6" class="ltx_td"></td>
<td id="S3.T1.4.8.6.7" class="ltx_td"></td>
<td id="S3.T1.4.8.6.8" class="ltx_td ltx_align_center">    160</td>
<td id="S3.T1.4.8.6.9" class="ltx_td ltx_align_center">68</td>
</tr>
<tr id="S3.T1.4.9.7" class="ltx_tr">
<th id="S3.T1.4.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">V-COCO-SG split2</th>
<td id="S3.T1.4.9.7.2" class="ltx_td ltx_align_center">7,057</td>
<td id="S3.T1.4.9.7.3" class="ltx_td ltx_align_center">3,066</td>
<td id="S3.T1.4.9.7.4" class="ltx_td ltx_align_center">15,644</td>
<td id="S3.T1.4.9.7.5" class="ltx_td ltx_align_center">4,322</td>
<td id="S3.T1.4.9.7.6" class="ltx_td"></td>
<td id="S3.T1.4.9.7.7" class="ltx_td"></td>
<td id="S3.T1.4.9.7.8" class="ltx_td ltx_align_center">    160</td>
<td id="S3.T1.4.9.7.9" class="ltx_td ltx_align_center">68</td>
</tr>
<tr id="S3.T1.4.10.8" class="ltx_tr">
<th id="S3.T1.4.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">V-COCO-SG split3</th>
<td id="S3.T1.4.10.8.2" class="ltx_td ltx_align_center ltx_border_bb">6,210</td>
<td id="S3.T1.4.10.8.3" class="ltx_td ltx_align_center ltx_border_bb">3,888</td>
<td id="S3.T1.4.10.8.4" class="ltx_td ltx_align_center ltx_border_bb">10,951</td>
<td id="S3.T1.4.10.8.5" class="ltx_td ltx_align_center ltx_border_bb">6,244</td>
<td id="S3.T1.4.10.8.6" class="ltx_td ltx_border_bb"></td>
<td id="S3.T1.4.10.8.7" class="ltx_td ltx_border_bb"></td>
<td id="S3.T1.4.10.8.8" class="ltx_td ltx_align_center ltx_border_bb">    160</td>
<td id="S3.T1.4.10.8.9" class="ltx_td ltx_align_center ltx_border_bb">68</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental setups for evaluating representative HOI detection models</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">This section describes the experimental setups for evaluating the systematic generalization ability of recent representative HOI detection models. Subsection <a href="#S4.SS1" title="4.1 HOI detection models ‣ 4 Experimental setups for evaluating representative HOI detection models ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a> explains the evaluated models and the reasons for their selection. Subsection <a href="#S4.SS2" title="4.2 Pretraining, hyperparameters, and other conditions ‣ 4 Experimental setups for evaluating representative HOI detection models ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> explains the experimental conditions in detail.</p>
</div>
<figure id="S4.T2" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.4.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.5.2" class="ltx_text" style="font-size:90%;">Characteristics of the four HOI detection models: HOTR, QPIC, FGAHOI, and STIP. The upper half of the table describes the architecture types, feature extractors, and base models of the four models. The lower half gives the mean average precision (mAP, where higher values are desired) on the original HICO-DET and V-COCO reported in the original papers of each model.
</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.T2.1.1" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.2.1" class="ltx_tr">
<th id="S4.T2.1.1.2.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="S4.T2.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.2.1.2.1" class="ltx_text" style="font-size:90%;">HOTR</span></td>
<td id="S4.T2.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.2.1.3.1" class="ltx_text" style="font-size:90%;">QPIC</span></td>
<td id="S4.T2.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.2.1.4.1" class="ltx_text" style="font-size:90%;">FGAHOI</span></td>
<td id="S4.T2.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.2.1.5.1" class="ltx_text" style="font-size:90%;">STIP</span></td>
</tr>
<tr id="S4.T2.1.1.3.2" class="ltx_tr">
<th id="S4.T2.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T2.1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">Architecture type</span></th>
<td id="S4.T2.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.3.2.2.1" class="ltx_text" style="font-size:90%;">One-stage</span></td>
<td id="S4.T2.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.3.2.3.1" class="ltx_text" style="font-size:90%;">One-stage</span></td>
<td id="S4.T2.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.3.2.4.1" class="ltx_text" style="font-size:90%;">One-stage</span></td>
<td id="S4.T2.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.3.2.5.1" class="ltx_text" style="font-size:90%;">Two-stage</span></td>
</tr>
<tr id="S4.T2.1.1.4.3" class="ltx_tr">
<th id="S4.T2.1.1.4.3.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S4.T2.1.1.4.3.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.4.3.2.1" class="ltx_text" style="font-size:90%;">parallel</span></td>
<td id="S4.T2.1.1.4.3.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.4.3.3.1" class="ltx_text" style="font-size:90%;">end-to-end</span></td>
<td id="S4.T2.1.1.4.3.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.4.3.4.1" class="ltx_text" style="font-size:90%;">end-to-end</span></td>
<td id="S4.T2.1.1.4.3.5" class="ltx_td"></td>
</tr>
<tr id="S4.T2.1.1.5.4" class="ltx_tr">
<th id="S4.T2.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T2.1.1.5.4.1.1" class="ltx_text" style="font-size:90%;">Feature extractor</span></th>
<td id="S4.T2.1.1.5.4.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.5.4.2.1" class="ltx_text" style="font-size:90%;">CNN</span></td>
<td id="S4.T2.1.1.5.4.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.5.4.3.1" class="ltx_text" style="font-size:90%;">CNN</span></td>
<td id="S4.T2.1.1.5.4.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.5.4.4.1" class="ltx_text" style="font-size:90%;">Multi-scale Transformer</span></td>
<td id="S4.T2.1.1.5.4.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.5.4.5.1" class="ltx_text" style="font-size:90%;">CNN</span></td>
</tr>
<tr id="S4.T2.1.1.6.5" class="ltx_tr">
<th id="S4.T2.1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T2.1.1.6.5.1.1" class="ltx_text" style="font-size:90%;">Base model</span></th>
<td id="S4.T2.1.1.6.5.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.6.5.2.1" class="ltx_text" style="font-size:90%;">DETR</span></td>
<td id="S4.T2.1.1.6.5.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.6.5.3.1" class="ltx_text" style="font-size:90%;">DETR</span></td>
<td id="S4.T2.1.1.6.5.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.6.5.4.1" class="ltx_text" style="font-size:90%;">Deformable DETR</span></td>
<td id="S4.T2.1.1.6.5.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.6.5.5.1" class="ltx_text" style="font-size:90%;">DETR</span></td>
</tr>
<tr id="S4.T2.1.1.7.6" class="ltx_tr">
<th id="S4.T2.1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T2.1.1.7.6.1.1" class="ltx_text" style="font-size:90%;">mAP (%) on original HICO-DET</span></th>
<td id="S4.T2.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.7.6.2.1" class="ltx_text" style="font-size:90%;">25.73</span></td>
<td id="S4.T2.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.7.6.3.1" class="ltx_text" style="font-size:90%;">29.90</span></td>
<td id="S4.T2.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.7.6.4.1" class="ltx_text" style="font-size:90%;">37.18</span></td>
<td id="S4.T2.1.1.7.6.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.7.6.5.1" class="ltx_text" style="font-size:90%;">32.22</span></td>
</tr>
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S4.T2.1.1.1.1.1" class="ltx_text" style="font-size:90%;">mAP (%) on original HICO-DET (rare)</span><sup id="S4.T2.1.1.1.1.2" class="ltx_sup"><span id="S4.T2.1.1.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">∗</span></sup>
</th>
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.1.2.1" class="ltx_text" style="font-size:90%;">17.34</span></td>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.1.3.1" class="ltx_text" style="font-size:90%;">23.92</span></td>
<td id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.1.4.1" class="ltx_text" style="font-size:90%;">30.71</span></td>
<td id="S4.T2.1.1.1.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.1.5.1" class="ltx_text" style="font-size:90%;">28.15</span></td>
</tr>
<tr id="S4.T2.1.1.8.7" class="ltx_tr">
<th id="S4.T2.1.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r"><span id="S4.T2.1.1.8.7.1.1" class="ltx_text" style="font-size:90%;">mAP (%) on original V-COCO</span></th>
<td id="S4.T2.1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.1.1.8.7.2.1" class="ltx_text" style="font-size:90%;">63.8</span></td>
<td id="S4.T2.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.1.1.8.7.3.1" class="ltx_text" style="font-size:90%;">61.0</span></td>
<td id="S4.T2.1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.1.1.8.7.4.1" class="ltx_text" style="font-size:90%;">61.2</span></td>
<td id="S4.T2.1.1.8.7.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.1.1.8.7.5.1" class="ltx_text" style="font-size:90%;">70.65</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.T2.2.2" class="ltx_p ltx_align_left ltx_figure_panel ltx_align_center"><span id="S4.T2.2.2.1" class="ltx_text" style="font-size:90%;"> <sup id="S4.T2.2.2.1.1" class="ltx_sup"><span id="S4.T2.2.2.1.1.1" class="ltx_text ltx_font_italic" style="font-size:89%;">∗</span></sup><span id="S4.T2.2.2.1.2" class="ltx_text" style="font-size:89%;"> The few-shot generalization performance evaluated on the rare-triplets (see Subsection <a href="#S2.SS2" title="2.2 Studies related to systematic generalization in HOI detection ‣ 2 Related work ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>).</span></span></p>
</div>
</div>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>HOI detection models</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">We evaluated the systematic generalization performance of four representative HOI detection models: HOTR <cite class="ltx_cite ltx_citemacro_citep">(Kim et al., <a href="#bib.bib30" title="" class="ltx_ref">2021</a>)</cite>, QPIC <cite class="ltx_cite ltx_citemacro_citep">(Tamura et al., <a href="#bib.bib52" title="" class="ltx_ref">2021</a>)</cite>,
FGAHOI <cite class="ltx_cite ltx_citemacro_citep">(Ma et al., <a href="#bib.bib45" title="" class="ltx_ref">2023</a>)</cite>, and STIP <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib62" title="" class="ltx_ref">2022</a>)</cite>. The characteristics of the four models are shown in Table <a href="#S4.T2" title="Table 2 ‣ 4 Experimental setups for evaluating representative HOI detection models ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. All models except STIP adopt the recently popularized one-stage architecture.
The backbone (feature extraction) network of each model was pretrained on object detection task using the Microsoft COCO dataset <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib41" title="" class="ltx_ref">2014</a>)</cite>.
The encoders and decoders of HOTR, QPIC, and STIP were also pretrained on object detection task using the COCO dataset.
However, the encoder and decoder of FGAHOI cannot be pretrained on object detection task because there are some modifications compared to its base object-detection model, namely, Deformable DETR <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a href="#bib.bib63" title="" class="ltx_ref">2021</a>)</cite>.
Therefore, we report the results of HOTR, QPIC, and STIP both with and without pretraining the encoder and decoder on the object detection task
in
Figure <a href="#S5.F2" title="Figure 2 ‣ 5 Evaluation results ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> of Section <a href="#S5" title="5 Evaluation results ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and Tables <a href="#A2.T3" title="Table 3 ‣ Appendix B Further details of the evaluation results ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#A2.T4" title="Table 4 ‣ Appendix B Further details of the evaluation results ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> of Appendix <a href="#A2" title="Appendix B Further details of the evaluation results ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a> for the sake of a fair comparison.
The details of each model are given below.</p>
</div>
<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">HOTR.</h5>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">HOTR (Human-Object interaction detection TRansformer) <cite class="ltx_cite ltx_citemacro_citep">(Kim et al., <a href="#bib.bib30" title="" class="ltx_ref">2021</a>)</cite> is among the first Transformer-based models for HOI detection. This model adopts a one-stage parallel architecture and consists of a CNN-based backbone (feature extractor), a shared encoder, an instance (human + object) decoder, and an interaction decoder.
To get the matching between instance and interaction decoder outputs, three independent feed-forward networks, named HO Pointers, are trained to predict the correct &lt;human, <span id="S4.SS1.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_bold">interaction</span>, <span id="S4.SS1.SSS0.Px1.p1.1.2" class="ltx_text ltx_framed ltx_framed_underline">object</span>&gt; combination matching.
Most part of the network (except HO Pointers) is based on DETR (DEtection TRansformer) <cite class="ltx_cite ltx_citemacro_citep">(Carion et al., <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>, a Transformer-based object detector. Therefore, we can pretrain the backbone, shared encoder, instance decoder, and interaction decoder with DETR weights trained on the object detection task.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">QPIC.</h5>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">QPIC (Query-based Pairwise human-object interaction detection with Image-wide Contextual information) <cite class="ltx_cite ltx_citemacro_citep">(Tamura et al., <a href="#bib.bib52" title="" class="ltx_ref">2021</a>)</cite> is another Transformer-based HOI detection model proposed around the same time as HOTR.
Like HOTR, it is based mainly on DETR but unlike HOTR, it adopts a one-stage end-to-end architecture with a single decoder and no HO Pointers.
The backbone, encoder, and decoder can be pretrained using the weights of DETR trained on the object detection task.
</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">FGAHOI.</h5>

<div id="S4.SS1.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS0.Px3.p1.1" class="ltx_p">FGAHOI (Fine-Grained Anchors for Human-Object Interaction detection) <cite class="ltx_cite ltx_citemacro_citep">(Ma et al., <a href="#bib.bib45" title="" class="ltx_ref">2023</a>)</cite> had exhibited the best performance on the HICO-DET dataset on the Papers with Code leaderboard <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://paperswithcode.com/sota/human-object-interaction-detection-on-hico" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://paperswithcode.com/sota/human-object-interaction-detection-on-hico</a></span></span></span> at the time of our experiments.
The encoder of FGAHOI is trained to generate query-based anchors representing the points with high objectness scores in an image. To improve the computational efficiency, the decoder of FGAHOI predicts objects and interactions on the anchors alone and uses Deformable DETR <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a href="#bib.bib63" title="" class="ltx_ref">2021</a>)</cite>, a modified version of DETR that computes self-attention from a limited range of feature maps. Therefore, the model can extract multi-scale feature maps from an image.
Although the basic components described above are based on those of QAHOI <cite class="ltx_cite ltx_citemacro_citep">(Chen &amp; Yanai, <a href="#bib.bib9" title="" class="ltx_ref">2021</a>)</cite>, FGAHOI can generate more fine-grained anchors than its predecessor due to the combination of three novel components: a multi-scale sampling mechanism, a hierarchical spatial-aware merging mechanism, and a task-aware merging mechanism.
A novel training strategy (stage-wise training) is also designed to reduce the training pressure caused by overly complex tasks done by FGAHOI.
The backbone of the network can be pretrained because it is based on Swin Transformer <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib42" title="" class="ltx_ref">2021</a>)</cite>, but the encoder and the decoder of FGAHOI cannot be pretrained because there are some modifications on the Deformable DETR (object detection model). For this reason, the performances of FGAHOI are reported only for the non-pretrained encorder and decorder cases in
Figure <a href="#S5.F2" title="Figure 2 ‣ 5 Evaluation results ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> of Section <a href="#S5" title="5 Evaluation results ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and Tables <a href="#A2.T3" title="Table 3 ‣ Appendix B Further details of the evaluation results ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#A2.T4" title="Table 4 ‣ Appendix B Further details of the evaluation results ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> of Appendix <a href="#A2" title="Appendix B Further details of the evaluation results ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">STIP.</h5>

<div id="S4.SS1.SSS0.Px4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS0.Px4.p1.1" class="ltx_p">STIP (Structure-aware Transformer over Interaction Proposals) <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib62" title="" class="ltx_ref">2022</a>)</cite> had exhibited the best performance on the V-COCO dataset on the Papers with Code leaderboard<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://paperswithcode.com/sota/human-object-interaction-detection-on-v-coco" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://paperswithcode.com/sota/human-object-interaction-detection-on-v-coco</a></span></span></span> at the time of our experiments.
This model has a two-stage architecture to perform HOI set prediction from non-parametric interaction queries detected by an independent instance detector.
Therefore, the model can explore inter- and intra-interaction structures during early training epochs by fixing the correspondence between the interaction query and each target HOI.
The backbone and object detector can both be pretrained using the weights of DETR pretrained on an object detection task because the first stage of the network is the same as DETR.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Pretraining, hyperparameters, and other conditions</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">We used the official source code of the four models taken from the publicly-available repositories (URLs are listed in the References).</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p">The backbone (feature extraction) networks of all models were pretrained on the object detection task using the Microsoft COCO dataset <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib41" title="" class="ltx_ref">2014</a>)</cite> under the settings reported in the respective original papers.
The feasibility of pretraining the encoder and decoder parts depends on the model structure.
The FGAHOI results were obtained without pretraining the encoder and decoder, because (as explained previously), the encoder and decoder of FGAHOI cannot be pretrained.
For a fair comparison, we report the results of HOTR, QPIC, and STIP both with and without pretraining the encoder and decoder on the object detection task using the COCO dataset, although the original papers encouraged the use of pretrained weights to optimize the performance of the original HOI detection task.
</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p">Adopting the hyperparameters reported in the original papers and the official code repositories, we trained each model as described below.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para ltx_noindent">
<p id="S4.SS2.p4.3" class="ltx_p">For HOTR, ResNet-50 <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite> was used as the backbone, the number of both encoder and decoder layers was set to 6, the number of attention heads was set to 8, the number of query embeddings was set to 300, the hidden dimension of embeddings in the Transformer was set to 256, and the dropout rate was set to 0.1. The model was trained for 100 epochs using the AdamW optimizer <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov &amp; Hutter, <a href="#bib.bib43" title="" class="ltx_ref">2019</a>)</cite> with a batch size of 2, an initial learning rate of <math id="S4.SS2.p4.1.m1.1" class="ltx_Math" alttext="10^{-5}" display="inline"><semantics id="S4.SS2.p4.1.m1.1a"><msup id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml"><mn id="S4.SS2.p4.1.m1.1.1.2" xref="S4.SS2.p4.1.m1.1.1.2.cmml">10</mn><mrow id="S4.SS2.p4.1.m1.1.1.3" xref="S4.SS2.p4.1.m1.1.1.3.cmml"><mo id="S4.SS2.p4.1.m1.1.1.3a" xref="S4.SS2.p4.1.m1.1.1.3.cmml">−</mo><mn id="S4.SS2.p4.1.m1.1.1.3.2" xref="S4.SS2.p4.1.m1.1.1.3.2.cmml">5</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><apply id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p4.1.m1.1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.SS2.p4.1.m1.1.1.2.cmml" xref="S4.SS2.p4.1.m1.1.1.2">10</cn><apply id="S4.SS2.p4.1.m1.1.1.3.cmml" xref="S4.SS2.p4.1.m1.1.1.3"><minus id="S4.SS2.p4.1.m1.1.1.3.1.cmml" xref="S4.SS2.p4.1.m1.1.1.3"></minus><cn type="integer" id="S4.SS2.p4.1.m1.1.1.3.2.cmml" xref="S4.SS2.p4.1.m1.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">10^{-5}</annotation></semantics></math> for the backbone network and <math id="S4.SS2.p4.2.m2.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="S4.SS2.p4.2.m2.1a"><msup id="S4.SS2.p4.2.m2.1.1" xref="S4.SS2.p4.2.m2.1.1.cmml"><mn id="S4.SS2.p4.2.m2.1.1.2" xref="S4.SS2.p4.2.m2.1.1.2.cmml">10</mn><mrow id="S4.SS2.p4.2.m2.1.1.3" xref="S4.SS2.p4.2.m2.1.1.3.cmml"><mo id="S4.SS2.p4.2.m2.1.1.3a" xref="S4.SS2.p4.2.m2.1.1.3.cmml">−</mo><mn id="S4.SS2.p4.2.m2.1.1.3.2" xref="S4.SS2.p4.2.m2.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.2.m2.1b"><apply id="S4.SS2.p4.2.m2.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p4.2.m2.1.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1">superscript</csymbol><cn type="integer" id="S4.SS2.p4.2.m2.1.1.2.cmml" xref="S4.SS2.p4.2.m2.1.1.2">10</cn><apply id="S4.SS2.p4.2.m2.1.1.3.cmml" xref="S4.SS2.p4.2.m2.1.1.3"><minus id="S4.SS2.p4.2.m2.1.1.3.1.cmml" xref="S4.SS2.p4.2.m2.1.1.3"></minus><cn type="integer" id="S4.SS2.p4.2.m2.1.1.3.2.cmml" xref="S4.SS2.p4.2.m2.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.2.m2.1c">10^{-4}</annotation></semantics></math> for the other networks, and a weight decay of <math id="S4.SS2.p4.3.m3.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="S4.SS2.p4.3.m3.1a"><msup id="S4.SS2.p4.3.m3.1.1" xref="S4.SS2.p4.3.m3.1.1.cmml"><mn id="S4.SS2.p4.3.m3.1.1.2" xref="S4.SS2.p4.3.m3.1.1.2.cmml">10</mn><mrow id="S4.SS2.p4.3.m3.1.1.3" xref="S4.SS2.p4.3.m3.1.1.3.cmml"><mo id="S4.SS2.p4.3.m3.1.1.3a" xref="S4.SS2.p4.3.m3.1.1.3.cmml">−</mo><mn id="S4.SS2.p4.3.m3.1.1.3.2" xref="S4.SS2.p4.3.m3.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.3.m3.1b"><apply id="S4.SS2.p4.3.m3.1.1.cmml" xref="S4.SS2.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p4.3.m3.1.1.1.cmml" xref="S4.SS2.p4.3.m3.1.1">superscript</csymbol><cn type="integer" id="S4.SS2.p4.3.m3.1.1.2.cmml" xref="S4.SS2.p4.3.m3.1.1.2">10</cn><apply id="S4.SS2.p4.3.m3.1.1.3.cmml" xref="S4.SS2.p4.3.m3.1.1.3"><minus id="S4.SS2.p4.3.m3.1.1.3.1.cmml" xref="S4.SS2.p4.3.m3.1.1.3"></minus><cn type="integer" id="S4.SS2.p4.3.m3.1.1.3.2.cmml" xref="S4.SS2.p4.3.m3.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.3.m3.1c">10^{-4}</annotation></semantics></math>. Both learning rates decayed after 80 epochs.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para ltx_noindent">
<p id="S4.SS2.p5.3" class="ltx_p">For QPIC, ResNet-101 <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite> was used as the backbone, the number of both encoder and decoder layers was set to 6, the number of attention heads was set to 8, the number of query embeddings was set to 100, the hidden dimension of embeddings in the Transformer was set to 256, and the dropout rate was set to 0.1. The model was trained for 150 epochs using the AdamW optimizer with a batch size of 16, an initial learning rate of <math id="S4.SS2.p5.1.m1.1" class="ltx_Math" alttext="10^{-5}" display="inline"><semantics id="S4.SS2.p5.1.m1.1a"><msup id="S4.SS2.p5.1.m1.1.1" xref="S4.SS2.p5.1.m1.1.1.cmml"><mn id="S4.SS2.p5.1.m1.1.1.2" xref="S4.SS2.p5.1.m1.1.1.2.cmml">10</mn><mrow id="S4.SS2.p5.1.m1.1.1.3" xref="S4.SS2.p5.1.m1.1.1.3.cmml"><mo id="S4.SS2.p5.1.m1.1.1.3a" xref="S4.SS2.p5.1.m1.1.1.3.cmml">−</mo><mn id="S4.SS2.p5.1.m1.1.1.3.2" xref="S4.SS2.p5.1.m1.1.1.3.2.cmml">5</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.1.m1.1b"><apply id="S4.SS2.p5.1.m1.1.1.cmml" xref="S4.SS2.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p5.1.m1.1.1.1.cmml" xref="S4.SS2.p5.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.SS2.p5.1.m1.1.1.2.cmml" xref="S4.SS2.p5.1.m1.1.1.2">10</cn><apply id="S4.SS2.p5.1.m1.1.1.3.cmml" xref="S4.SS2.p5.1.m1.1.1.3"><minus id="S4.SS2.p5.1.m1.1.1.3.1.cmml" xref="S4.SS2.p5.1.m1.1.1.3"></minus><cn type="integer" id="S4.SS2.p5.1.m1.1.1.3.2.cmml" xref="S4.SS2.p5.1.m1.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.1.m1.1c">10^{-5}</annotation></semantics></math> for the backbone network and <math id="S4.SS2.p5.2.m2.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="S4.SS2.p5.2.m2.1a"><msup id="S4.SS2.p5.2.m2.1.1" xref="S4.SS2.p5.2.m2.1.1.cmml"><mn id="S4.SS2.p5.2.m2.1.1.2" xref="S4.SS2.p5.2.m2.1.1.2.cmml">10</mn><mrow id="S4.SS2.p5.2.m2.1.1.3" xref="S4.SS2.p5.2.m2.1.1.3.cmml"><mo id="S4.SS2.p5.2.m2.1.1.3a" xref="S4.SS2.p5.2.m2.1.1.3.cmml">−</mo><mn id="S4.SS2.p5.2.m2.1.1.3.2" xref="S4.SS2.p5.2.m2.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.2.m2.1b"><apply id="S4.SS2.p5.2.m2.1.1.cmml" xref="S4.SS2.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p5.2.m2.1.1.1.cmml" xref="S4.SS2.p5.2.m2.1.1">superscript</csymbol><cn type="integer" id="S4.SS2.p5.2.m2.1.1.2.cmml" xref="S4.SS2.p5.2.m2.1.1.2">10</cn><apply id="S4.SS2.p5.2.m2.1.1.3.cmml" xref="S4.SS2.p5.2.m2.1.1.3"><minus id="S4.SS2.p5.2.m2.1.1.3.1.cmml" xref="S4.SS2.p5.2.m2.1.1.3"></minus><cn type="integer" id="S4.SS2.p5.2.m2.1.1.3.2.cmml" xref="S4.SS2.p5.2.m2.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.2.m2.1c">10^{-4}</annotation></semantics></math> for the other networks, and a weight decay of <math id="S4.SS2.p5.3.m3.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="S4.SS2.p5.3.m3.1a"><msup id="S4.SS2.p5.3.m3.1.1" xref="S4.SS2.p5.3.m3.1.1.cmml"><mn id="S4.SS2.p5.3.m3.1.1.2" xref="S4.SS2.p5.3.m3.1.1.2.cmml">10</mn><mrow id="S4.SS2.p5.3.m3.1.1.3" xref="S4.SS2.p5.3.m3.1.1.3.cmml"><mo id="S4.SS2.p5.3.m3.1.1.3a" xref="S4.SS2.p5.3.m3.1.1.3.cmml">−</mo><mn id="S4.SS2.p5.3.m3.1.1.3.2" xref="S4.SS2.p5.3.m3.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.3.m3.1b"><apply id="S4.SS2.p5.3.m3.1.1.cmml" xref="S4.SS2.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p5.3.m3.1.1.1.cmml" xref="S4.SS2.p5.3.m3.1.1">superscript</csymbol><cn type="integer" id="S4.SS2.p5.3.m3.1.1.2.cmml" xref="S4.SS2.p5.3.m3.1.1.2">10</cn><apply id="S4.SS2.p5.3.m3.1.1.3.cmml" xref="S4.SS2.p5.3.m3.1.1.3"><minus id="S4.SS2.p5.3.m3.1.1.3.1.cmml" xref="S4.SS2.p5.3.m3.1.1.3"></minus><cn type="integer" id="S4.SS2.p5.3.m3.1.1.3.2.cmml" xref="S4.SS2.p5.3.m3.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.3.m3.1c">10^{-4}</annotation></semantics></math>. Both learning rates decayed after 100 epochs. The hyperparameters of the Hungarian costs and loss weights related to the bounding box were 2.5 times larger than those unrelated to the bounding box.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para ltx_noindent">
<p id="S4.SS2.p6.4" class="ltx_p">For FGAHOI, Swin-Large<math id="S4.SS2.p6.1.m1.1" class="ltx_Math" alttext="{}^{*}_{+}" display="inline"><semantics id="S4.SS2.p6.1.m1.1a"><mmultiscripts id="S4.SS2.p6.1.m1.1.1" xref="S4.SS2.p6.1.m1.1.1.cmml"><mi id="S4.SS2.p6.1.m1.1.1.2.2" xref="S4.SS2.p6.1.m1.1.1.2.2.cmml"></mi><mprescripts id="S4.SS2.p6.1.m1.1.1a" xref="S4.SS2.p6.1.m1.1.1.cmml"></mprescripts><mo id="S4.SS2.p6.1.m1.1.1.3" xref="S4.SS2.p6.1.m1.1.1.3.cmml">+</mo><mrow id="S4.SS2.p6.1.m1.1.1b" xref="S4.SS2.p6.1.m1.1.1.cmml"></mrow><mrow id="S4.SS2.p6.1.m1.1.1c" xref="S4.SS2.p6.1.m1.1.1.cmml"></mrow><mo id="S4.SS2.p6.1.m1.1.1.2.3" xref="S4.SS2.p6.1.m1.1.1.2.3.cmml">∗</mo></mmultiscripts><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.1.m1.1b"><apply id="S4.SS2.p6.1.m1.1.1.cmml" xref="S4.SS2.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p6.1.m1.1.1.1.cmml" xref="S4.SS2.p6.1.m1.1.1">subscript</csymbol><apply id="S4.SS2.p6.1.m1.1.1.2.cmml" xref="S4.SS2.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p6.1.m1.1.1.2.1.cmml" xref="S4.SS2.p6.1.m1.1.1">superscript</csymbol><csymbol cd="latexml" id="S4.SS2.p6.1.m1.1.1.2.2.cmml" xref="S4.SS2.p6.1.m1.1.1.2.2">absent</csymbol><times id="S4.SS2.p6.1.m1.1.1.2.3.cmml" xref="S4.SS2.p6.1.m1.1.1.2.3"></times></apply><plus id="S4.SS2.p6.1.m1.1.1.3.cmml" xref="S4.SS2.p6.1.m1.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.1.m1.1c">{}^{*}_{+}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib42" title="" class="ltx_ref">2021</a>)</cite> was used as the backbone, the number of both encoder and decoder layers was set to 6, the number of attention heads was set to 8, the number of query embeddings was set to 300, the hidden dimension of embeddings in the Transformer was set to 256, and the dropout was not applied. The model was trained using the AdamW optimizer with a batch size of 16, an initial learning rate of <math id="S4.SS2.p6.2.m2.1" class="ltx_Math" alttext="10^{-5}" display="inline"><semantics id="S4.SS2.p6.2.m2.1a"><msup id="S4.SS2.p6.2.m2.1.1" xref="S4.SS2.p6.2.m2.1.1.cmml"><mn id="S4.SS2.p6.2.m2.1.1.2" xref="S4.SS2.p6.2.m2.1.1.2.cmml">10</mn><mrow id="S4.SS2.p6.2.m2.1.1.3" xref="S4.SS2.p6.2.m2.1.1.3.cmml"><mo id="S4.SS2.p6.2.m2.1.1.3a" xref="S4.SS2.p6.2.m2.1.1.3.cmml">−</mo><mn id="S4.SS2.p6.2.m2.1.1.3.2" xref="S4.SS2.p6.2.m2.1.1.3.2.cmml">5</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.2.m2.1b"><apply id="S4.SS2.p6.2.m2.1.1.cmml" xref="S4.SS2.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p6.2.m2.1.1.1.cmml" xref="S4.SS2.p6.2.m2.1.1">superscript</csymbol><cn type="integer" id="S4.SS2.p6.2.m2.1.1.2.cmml" xref="S4.SS2.p6.2.m2.1.1.2">10</cn><apply id="S4.SS2.p6.2.m2.1.1.3.cmml" xref="S4.SS2.p6.2.m2.1.1.3"><minus id="S4.SS2.p6.2.m2.1.1.3.1.cmml" xref="S4.SS2.p6.2.m2.1.1.3"></minus><cn type="integer" id="S4.SS2.p6.2.m2.1.1.3.2.cmml" xref="S4.SS2.p6.2.m2.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.2.m2.1c">10^{-5}</annotation></semantics></math> for the backbone network and <math id="S4.SS2.p6.3.m3.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="S4.SS2.p6.3.m3.1a"><msup id="S4.SS2.p6.3.m3.1.1" xref="S4.SS2.p6.3.m3.1.1.cmml"><mn id="S4.SS2.p6.3.m3.1.1.2" xref="S4.SS2.p6.3.m3.1.1.2.cmml">10</mn><mrow id="S4.SS2.p6.3.m3.1.1.3" xref="S4.SS2.p6.3.m3.1.1.3.cmml"><mo id="S4.SS2.p6.3.m3.1.1.3a" xref="S4.SS2.p6.3.m3.1.1.3.cmml">−</mo><mn id="S4.SS2.p6.3.m3.1.1.3.2" xref="S4.SS2.p6.3.m3.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.3.m3.1b"><apply id="S4.SS2.p6.3.m3.1.1.cmml" xref="S4.SS2.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p6.3.m3.1.1.1.cmml" xref="S4.SS2.p6.3.m3.1.1">superscript</csymbol><cn type="integer" id="S4.SS2.p6.3.m3.1.1.2.cmml" xref="S4.SS2.p6.3.m3.1.1.2">10</cn><apply id="S4.SS2.p6.3.m3.1.1.3.cmml" xref="S4.SS2.p6.3.m3.1.1.3"><minus id="S4.SS2.p6.3.m3.1.1.3.1.cmml" xref="S4.SS2.p6.3.m3.1.1.3"></minus><cn type="integer" id="S4.SS2.p6.3.m3.1.1.3.2.cmml" xref="S4.SS2.p6.3.m3.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.3.m3.1c">10^{-4}</annotation></semantics></math> for the other networks, and a weight decay of <math id="S4.SS2.p6.4.m4.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="S4.SS2.p6.4.m4.1a"><msup id="S4.SS2.p6.4.m4.1.1" xref="S4.SS2.p6.4.m4.1.1.cmml"><mn id="S4.SS2.p6.4.m4.1.1.2" xref="S4.SS2.p6.4.m4.1.1.2.cmml">10</mn><mrow id="S4.SS2.p6.4.m4.1.1.3" xref="S4.SS2.p6.4.m4.1.1.3.cmml"><mo id="S4.SS2.p6.4.m4.1.1.3a" xref="S4.SS2.p6.4.m4.1.1.3.cmml">−</mo><mn id="S4.SS2.p6.4.m4.1.1.3.2" xref="S4.SS2.p6.4.m4.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.4.m4.1b"><apply id="S4.SS2.p6.4.m4.1.1.cmml" xref="S4.SS2.p6.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS2.p6.4.m4.1.1.1.cmml" xref="S4.SS2.p6.4.m4.1.1">superscript</csymbol><cn type="integer" id="S4.SS2.p6.4.m4.1.1.2.cmml" xref="S4.SS2.p6.4.m4.1.1.2">10</cn><apply id="S4.SS2.p6.4.m4.1.1.3.cmml" xref="S4.SS2.p6.4.m4.1.1.3"><minus id="S4.SS2.p6.4.m4.1.1.3.1.cmml" xref="S4.SS2.p6.4.m4.1.1.3"></minus><cn type="integer" id="S4.SS2.p6.4.m4.1.1.3.2.cmml" xref="S4.SS2.p6.4.m4.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.4.m4.1c">10^{-4}</annotation></semantics></math>. On HICO-DET and HICO-DET-SG, the base network was trained for 150 epochs and the learning rate was dropped from the 120th epoch during the first stage of training. Subsequent training was performed over 40 epochs with a learning rate drop at the 15th epoch. On V-COCO and V-COCO-SG, the base network was trained for 90 epochs and the learning rate was dropped from the 60th epoch during the first stage of training. Subsequent training was performed over 30 epochs with a learning rate drop at the 10th epoch.</p>
</div>
<div id="S4.SS2.p7" class="ltx_para ltx_noindent">
<p id="S4.SS2.p7.1" class="ltx_p">For STIP, ResNet-50 <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite> was used as the backbone, the number of both encoder and decoder layers was set to 6, the number of attention heads was set to 8, the number of query embeddings for object detector was set to 100, the number of queries for interaction decoder was set to 32, the hidden dimension of embeddings in the Transformer was set to 256, and the dropout rate was set to 0.1. The model was trained for 30 epochs using the AdamW optimizer with a batch size of 8 and a learning rate of <math id="S4.SS2.p7.1.m1.1" class="ltx_Math" alttext="5\times 10^{-5}" display="inline"><semantics id="S4.SS2.p7.1.m1.1a"><mrow id="S4.SS2.p7.1.m1.1.1" xref="S4.SS2.p7.1.m1.1.1.cmml"><mn id="S4.SS2.p7.1.m1.1.1.2" xref="S4.SS2.p7.1.m1.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p7.1.m1.1.1.1" xref="S4.SS2.p7.1.m1.1.1.1.cmml">×</mo><msup id="S4.SS2.p7.1.m1.1.1.3" xref="S4.SS2.p7.1.m1.1.1.3.cmml"><mn id="S4.SS2.p7.1.m1.1.1.3.2" xref="S4.SS2.p7.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.SS2.p7.1.m1.1.1.3.3" xref="S4.SS2.p7.1.m1.1.1.3.3.cmml"><mo id="S4.SS2.p7.1.m1.1.1.3.3a" xref="S4.SS2.p7.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.SS2.p7.1.m1.1.1.3.3.2" xref="S4.SS2.p7.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p7.1.m1.1b"><apply id="S4.SS2.p7.1.m1.1.1.cmml" xref="S4.SS2.p7.1.m1.1.1"><times id="S4.SS2.p7.1.m1.1.1.1.cmml" xref="S4.SS2.p7.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.p7.1.m1.1.1.2.cmml" xref="S4.SS2.p7.1.m1.1.1.2">5</cn><apply id="S4.SS2.p7.1.m1.1.1.3.cmml" xref="S4.SS2.p7.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p7.1.m1.1.1.3.1.cmml" xref="S4.SS2.p7.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS2.p7.1.m1.1.1.3.2.cmml" xref="S4.SS2.p7.1.m1.1.1.3.2">10</cn><apply id="S4.SS2.p7.1.m1.1.1.3.3.cmml" xref="S4.SS2.p7.1.m1.1.1.3.3"><minus id="S4.SS2.p7.1.m1.1.1.3.3.1.cmml" xref="S4.SS2.p7.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.SS2.p7.1.m1.1.1.3.3.2.cmml" xref="S4.SS2.p7.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p7.1.m1.1c">5\times 10^{-5}</annotation></semantics></math>.</p>
</div>
<div id="S4.SS2.p8" class="ltx_para ltx_noindent">
<p id="S4.SS2.p8.1" class="ltx_p">We trained and tested the following seven types of models: HOTR, QPIC, FGAHOI, and STIP without pretraining the encoder and decoder, and HOTR, QPIC, and STIP with pretraining the encoder and decoder. Training and testing were performed once on each split. One training required approximately 1-2 days using 4 NVIDIA V100 GPUs.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Evaluation results</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">This section reports the systematic generalization performances of the four representative HOI detection models (HOTR <cite class="ltx_cite ltx_citemacro_citep">(Kim et al., <a href="#bib.bib30" title="" class="ltx_ref">2021</a>)</cite>, QPIC <cite class="ltx_cite ltx_citemacro_citep">(Tamura et al., <a href="#bib.bib52" title="" class="ltx_ref">2021</a>)</cite>,
FGAHOI <cite class="ltx_cite ltx_citemacro_citep">(Ma et al., <a href="#bib.bib45" title="" class="ltx_ref">2023</a>)</cite>, and STIP <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib62" title="" class="ltx_ref">2022</a>)</cite>) evaluated on the HICO-DET-SG and V-COCO-SG.
A qualitative inspection of failure cases is also presented.</p>
</div>
<figure id="S5.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F2.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:219.0pt;"><img src="/html/2305.09948/assets/x2.png" id="S5.F2.1.g1" class="ltx_graphics ltx_img_square" width="461" height="414" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F2.1.1.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F2.1.2.2" class="ltx_text" style="font-size:90%;">Results on HICO-DET-SG</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F2.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:210.3pt;"><img src="/html/2305.09948/assets/x3.png" id="S5.F2.2.g1" class="ltx_graphics ltx_img_square" width="461" height="430" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F2.2.1.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F2.2.2.2" class="ltx_text" style="font-size:90%;">Results on V-COCO-SG</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F2.4.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S5.F2.5.2" class="ltx_text" style="font-size:90%;">Evaluation results of the systematic generalization performances on (a) HICO-DET-SG and (b) V-COCO-SG data splits, and the in-distribution generalization performances on the original splits. The mAPs (%) for the test data, which are the higher the better, of all models are considerably lower on both HICO-DET-SG and V-COCO-SG (dark blue) than on the original splits (pale blue) that evaluate the in-distribution generalization ability. The dark blue bars and the error bars represent the averages and the standard deviations, respectively, across the three distinct SG splits.
“Not pretrained” denotes that the encoders and decoders of the models were trained from scratch, wherreas “Pretrained” denotes that the initial encoder and decoder weights were copied from DETR trained on object detection task using the Microsoft COCO dataset.
The results of FGAHOI are reported only for “Not pretrained” cases because the encoder and decoder of FGAHOI cannot be pretrained on object detection task as explained in Section <a href="#S4" title="4 Experimental setups for evaluating representative HOI detection models ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
Further details of the evaluation results are given in Tables <a href="#A2.T3" title="Table 3 ‣ Appendix B Further details of the evaluation results ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#A2.T4" title="Table 4 ‣ Appendix B Further details of the evaluation results ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> in Appendix <a href="#A2" title="Appendix B Further details of the evaluation results ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>.</span></figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Degradation in the systematic generalization performance</h3>

<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.p1.1" class="ltx_p">Figure <a href="#S5.F2" title="Figure 2 ‣ 5 Evaluation results ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> compares the evaluation results on HICO-DET-SG and V-COCO-SG with those on the original splits.
The evaluation metric is the mean average precision (mAP), which was adopted in the original papers of the four models. A high mAP indicates that a model is well-performing.
Regarding the HICO-DET-SG and V-COCO-SG, the averages and standard deviations calculated over the three distinct splits are presented in each subfigure.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.1" class="ltx_p">The mAPs of all models are considerably lower on both HICO-DET-SG and V-COCO-SG (dark blue) than on the original splits (pale blue) that evaluate the in-distribution generalization ability.
The differences among the test mAPs on the three SG splits are less than 3 percentage points, regardless of the model and base dataset.
This means that the performances of all models largely degraded for any selection of object-interaction combinations in the train and test data.
These results highlights the difficulty of systematic generalization in HOI detection task, i.e., recognizing novel combinations of known objects and interactions.
</p>
</div>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.1" class="ltx_p">Further details of the evaluation results are given in Appendix <a href="#A2" title="Appendix B Further details of the evaluation results ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
<figure id="S5.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<div id="S5.F3.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:138.8pt;">
<img src="/html/2305.09948/assets/x4.png" id="S5.F3.1.g1" class="ltx_graphics ltx_img_portrait" width="369" height="511" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<div id="S5.F3.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:145.3pt;">
<img src="/html/2305.09948/assets/x5.png" id="S5.F3.2.g1" class="ltx_graphics ltx_img_landscape" width="462" height="294" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<div id="S5.F3.3" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:134.4pt;">
<img src="/html/2305.09948/assets/x6.png" id="S5.F3.3.g1" class="ltx_graphics ltx_img_landscape" width="461" height="316" alt="Refer to caption">
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F3.13.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S5.F3.14.2" class="ltx_text" style="font-size:90%;">Three failure cases of STIP with the pretrained encoder and decoder after training and testing on HICO-DET-SG split3. (a) An example of predicting the wrong interaction class. The model predicted the interaction as <span id="S5.F3.14.2.1" class="ltx_text ltx_font_bold">straddle</span>, although the correct class is <span id="S5.F3.14.2.2" class="ltx_text ltx_font_bold">wash</span>. (b) An example of detecting the wrong object. The model predicted an irrelevant region as a wrong class <span id="S5.F3.14.2.3" class="ltx_text ltx_framed ltx_framed_underline">bench</span>, although it should detect a <span id="S5.F3.14.2.4" class="ltx_text ltx_framed ltx_framed_underline">bed</span> under the person. (c) An example of wrong class prediction of both object and interaction. The model predicted &lt;human, <span id="S5.F3.14.2.5" class="ltx_text ltx_font_bold">hit</span>, <span id="S5.F3.14.2.6" class="ltx_text ltx_framed ltx_framed_underline">baseball bat</span>&gt; triplet although the correct answer is &lt;human, <span id="S5.F3.14.2.7" class="ltx_text ltx_font_bold">swing</span>, <span id="S5.F3.14.2.8" class="ltx_text ltx_framed ltx_framed_underline">tennis racket</span>&gt;.</span></figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Qualitative inspection of failure cases</h3>

<div id="S5.SS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.p1.1" class="ltx_p">To further reveal the difficulty of systematic generalization in HOI detection, we inspected the failure cases. Figure <a href="#S5.F3" title="Figure 3 ‣ 5.1 Degradation in the systematic generalization performance ‣ 5 Evaluation results ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> show the outputs of STIP with pretrained encoder and decoder trained and tested on HICO-DET-SG split3, which achieved the highest mAP among all models on all SG splits.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.p2.1" class="ltx_p">Figure <a href="#S5.F3" title="Figure 3 ‣ 5.1 Degradation in the systematic generalization performance ‣ 5 Evaluation results ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (a) shows an example of predicting the wrong interaction class, the most frequently observed error type. In this example, the model predicts the interaction as <span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_bold">straddle</span>, although the correct class is <span id="S5.SS2.p2.1.2" class="ltx_text ltx_font_bold">wash</span>.
The &lt;human, <span id="S5.SS2.p2.1.3" class="ltx_text ltx_font_bold">straddle</span>, <span id="S5.SS2.p2.1.4" class="ltx_text ltx_framed ltx_framed_underline">horse</span>&gt; triplet appears in the train data but the &lt;human, <span id="S5.SS2.p2.1.5" class="ltx_text ltx_font_bold">wash</span>, <span id="S5.SS2.p2.1.6" class="ltx_text ltx_framed ltx_framed_underline">horse</span>&gt; triplet appears only in the test data (the <span id="S5.SS2.p2.1.7" class="ltx_text ltx_font_bold">wash</span> interaction appears with other objects in the train data).
The model appears to predict the interaction from the object class (<span id="S5.SS2.p2.1.8" class="ltx_text ltx_framed ltx_framed_underline">horse</span>) alone and cannot generalize to the novel combination of &lt;human, <span id="S5.SS2.p2.1.9" class="ltx_text ltx_font_bold">wash</span>, <span id="S5.SS2.p2.1.10" class="ltx_text ltx_framed ltx_framed_underline">horse</span>&gt;.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.p3.1" class="ltx_p">Figure <a href="#S5.F3" title="Figure 3 ‣ 5.1 Degradation in the systematic generalization performance ‣ 5 Evaluation results ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (b) shows an example of detecting the wrong object. The model is supposed to detect a <span id="S5.SS2.p3.1.1" class="ltx_text ltx_framed ltx_framed_underline">bed</span> under the person in the image but instead predicts an irrelevant region of the wrong class, <span id="S5.SS2.p3.1.2" class="ltx_text ltx_framed ltx_framed_underline">bench</span>.
The &lt;human, <span id="S5.SS2.p3.1.3" class="ltx_text ltx_font_bold">lie on</span>, <span id="S5.SS2.p3.1.4" class="ltx_text ltx_framed ltx_framed_underline">bench</span>&gt; triplet appears in the train data but the &lt;human, <span id="S5.SS2.p3.1.5" class="ltx_text ltx_font_bold">lie on</span>, <span id="S5.SS2.p3.1.6" class="ltx_text ltx_framed ltx_framed_underline">bed</span>&gt; triplet appears only in the test data (<span id="S5.SS2.p3.1.7" class="ltx_text ltx_framed ltx_framed_underline">bed</span> appears with other interactions in the train data).
Besides being unable to generalize to the novel combination,
the model appears to predict the interaction (through its interaction decoder) mainly from visual cues of the human posture rather than from visual cues of the object or the positional relationships.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para ltx_noindent">
<p id="S5.SS2.p4.1" class="ltx_p">Figure <a href="#S5.F3" title="Figure 3 ‣ 5.1 Degradation in the systematic generalization performance ‣ 5 Evaluation results ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (c) shows an example of wrong class prediction of both object and interaction. The model predicted the <span id="S5.SS2.p4.1.1" class="ltx_text ltx_framed ltx_framed_underline">tennis racket</span> as a <span id="S5.SS2.p4.1.2" class="ltx_text ltx_framed ltx_framed_underline">baseball bat</span> and <span id="S5.SS2.p4.1.3" class="ltx_text ltx_font_bold">swing</span> as <span id="S5.SS2.p4.1.4" class="ltx_text ltx_font_bold">hit</span>.
The &lt;human, <span id="S5.SS2.p4.1.5" class="ltx_text ltx_font_bold">hit</span>, <span id="S5.SS2.p4.1.6" class="ltx_text ltx_framed ltx_framed_underline">baseball bat</span>&gt; triplet appears in the train data but the &lt;human, <span id="S5.SS2.p4.1.7" class="ltx_text ltx_font_bold">swing</span>, <span id="S5.SS2.p4.1.8" class="ltx_text ltx_framed ltx_framed_underline">tennis racket</span>&gt; triplet appears only in the test data; moreover, the train data include the &lt;human, <span id="S5.SS2.p4.1.9" class="ltx_text ltx_font_bold">swing</span>, <span id="S5.SS2.p4.1.10" class="ltx_text ltx_framed ltx_framed_underline">baseball bat</span>&gt; triplet but not the &lt;human, <span id="S5.SS2.p4.1.11" class="ltx_text ltx_font_bold">hit</span>, <span id="S5.SS2.p4.1.12" class="ltx_text ltx_framed ltx_framed_underline">tennis racket</span>&gt; triplet.
The model detected the object as a <span id="S5.SS2.p4.1.13" class="ltx_text ltx_framed ltx_framed_underline">baseball bat</span> at the first stage. Based on the detection result, the interaction was predicted as <span id="S5.SS2.p4.1.14" class="ltx_text ltx_font_bold">hit</span> at the second stage, most-likely because the <span id="S5.SS2.p4.1.15" class="ltx_text ltx_framed ltx_framed_underline">baseball bat</span> frequently appeared with the <span id="S5.SS2.p4.1.16" class="ltx_text ltx_font_bold">hit</span> interaction in the train data.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>

<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Comparison of the results on HICO-DET-SG and V-COCO-SG</h3>

<div id="S6.SS1.p1" class="ltx_para ltx_noindent">
<p id="S6.SS1.p1.1" class="ltx_p">Comparing the results on the HICO-DET-SG and V-COCO-SG, we find that the performance gap is larger between HICO-DET (the original split) and HICO-DET-SG (the systematic generalization split) than between V-COCO and V-COCO-SG.
This difference might reflect differences in the number of images and HOIs between the datasets. The HICO-DET-SG train data contains approximately 5.6 times as many images and 12.6 times as many HOIs as the train data of V-COCO-SG (Table <a href="#S3.T1" title="Table 1 ‣ 3.2 Statistics of the HICO-DET-SG and V-COCO-SG ‣ 3 HICO-DET-SG and V-COCO-SG ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
In more detail, the variety of object-interaction combination classes is 3.4 times higher in the HICO-DET-SG train data than in the V-COCO-SG train data, and more examples for one object-interaction combination exists in the HICO-DET-SG train data.
In other computer vision tasks, increasing the diversity of train data is known to improve the systematic generalization performance <cite class="ltx_cite ltx_citemacro_citep">(Lake &amp; Baroni, <a href="#bib.bib33" title="" class="ltx_ref">2018</a>; Bahdanau et al., <a href="#bib.bib1" title="" class="ltx_ref">2019</a>; D’Amario et al., <a href="#bib.bib12" title="" class="ltx_ref">2021</a>; Madan et al., <a href="#bib.bib46" title="" class="ltx_ref">2022</a>)</cite>.
The same trend might be expected in HOI detection.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Comparison across models</h3>

<div id="S6.SS2.p1" class="ltx_para ltx_noindent">
<p id="S6.SS2.p1.1" class="ltx_p">The models achieved different performances on the SG splits.
Without encoder and decoder pretraining, HOTR completely failed to generalize, as evidenced by the nearly 0% mAP.
Even with the encoder and decoder pretrained on object detection task, the mAP of HOTR only improved to less than 3%.
FGAHOI also underperformed on SG splits, with mAPs of approximately 6% or less.
In contrast, QPIC showed some generalizability to novel combinations especially when using pretrained DETR weights; they achieved approximately 20% mAPs on the HICO-DET-SG splits.
STIP with pretraining outperformed all other models on both the HICO-DET-SG and V-COCO-SG data splits. This superior performance might be attributed to the two-stage architecture of STIP, in which the instance and interaction detectors are independent and less affected by each other than in one-stage architectures.
Supporting this inference, modular structures improved systematic generalization ability of deep neural networks in other computer vision tasks <cite class="ltx_cite ltx_citemacro_citep">(Purushwalkam et al., <a href="#bib.bib48" title="" class="ltx_ref">2019</a>; Bahdanau et al., <a href="#bib.bib1" title="" class="ltx_ref">2019</a>; <a href="#bib.bib2" title="" class="ltx_ref">2020</a>; D’Amario et al., <a href="#bib.bib12" title="" class="ltx_ref">2021</a>; Madan et al., <a href="#bib.bib46" title="" class="ltx_ref">2022</a>; Yamada et al., <a href="#bib.bib59" title="" class="ltx_ref">2023</a>; Kamata et al., <a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Importance of pretraining the encoder and decoder</h3>

<div id="S6.SS3.p1" class="ltx_para ltx_noindent">
<p id="S6.SS3.p1.1" class="ltx_p">Pretraining the encoder and decoder on object detection task using the Microsoft COCO dataset
improved the systematic generalization performances of HOTR, QPIC, and STIP.
With pretraining, the mAP on HICO-DET improved from 0.4% to 2.7% for HOTR, from 12.1% to 20.8% for QPIC, and from 0.0% to 23.2% for STIP.
Also the mAP on V-COCO changed from around 0.2% to around 2.1% for HOTR, from around 0.6% to around 3.8% for QPIC, and from around 0.0% to 6.3% for STIP.
Note that the pretraining using the COCO dataset improved the systematic generalization performance not only on V-COCO-SG (that is based on COCO dataset) but also on HICO-DET-SG.
</p>
</div>
<div id="S6.SS3.p2" class="ltx_para ltx_noindent">
<p id="S6.SS3.p2.1" class="ltx_p">In general, vision Transformers require a large amount of training to achieve high performance when trained from scratch <cite class="ltx_cite ltx_citemacro_citep">(Khan et al., <a href="#bib.bib29" title="" class="ltx_ref">2021</a>)</cite>. Therefore, without pretraining, it is natural that the Transformer-based HOI detection models perform poorly on in-distribution generalization and even more poorly on systematic generalization.
The performance of STIP was particularly degraded without pretraining, possibly because the number of training epochs (30) was much smaller for STIP than for the other models (100).
FGAHOI could not be evaluated with its encoder and decoder pretranined, because the network is designed only for HOI detection task and it is hard to obtain pretrained weights using other tasks such as object detection. If we modify the final part of
FGAHOI to enable training on other tasks,
the systematic generalization performance of FGAHOI would presumably improve with the pretrained weights.
</p>
</div>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Toward the improvement of systematic generalization performance in HOI detection</h3>

<div id="S6.SS4.p1" class="ltx_para ltx_noindent">
<p id="S6.SS4.p1.1" class="ltx_p">Based on the above experimental results, we propose the following suggestions for improving the systematic generalization performance of future HOI detection models.
</p>
</div>
<div id="S6.SS4.p2" class="ltx_para ltx_noindent">
<p id="S6.SS4.p2.1" class="ltx_p">First, increasing the diversity of the train data (variety of the object-interaction combination classes) likely improve systematic generalization ability of HOI detection models.
This possible solution is supported by the differences between HICO-DET-SG and V-COCO-SG and the results in other computer vision tasks <cite class="ltx_cite ltx_citemacro_citep">(Lake &amp; Baroni, <a href="#bib.bib33" title="" class="ltx_ref">2018</a>; Bahdanau et al., <a href="#bib.bib1" title="" class="ltx_ref">2019</a>; D’Amario et al., <a href="#bib.bib12" title="" class="ltx_ref">2021</a>; Madan et al., <a href="#bib.bib46" title="" class="ltx_ref">2022</a>)</cite> (see Subsection <a href="#S6.SS1" title="6.1 Comparison of the results on HICO-DET-SG and V-COCO-SG ‣ 6 Discussion ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1</span></a>).</p>
</div>
<div id="S6.SS4.p3" class="ltx_para ltx_noindent">
<p id="S6.SS4.p3.1" class="ltx_p">Second, introducing two-stage or other modular structures might improve the systematic generalization performance of HOI detection models.
This suggestion is supported by the higher performance of the pretrained STIP than of the other models and by the results in other computer vision tasks <cite class="ltx_cite ltx_citemacro_citep">(Purushwalkam et al., <a href="#bib.bib48" title="" class="ltx_ref">2019</a>; Bahdanau et al., <a href="#bib.bib1" title="" class="ltx_ref">2019</a>; <a href="#bib.bib2" title="" class="ltx_ref">2020</a>; D’Amario et al., <a href="#bib.bib12" title="" class="ltx_ref">2021</a>; Madan et al., <a href="#bib.bib46" title="" class="ltx_ref">2022</a>; Yamada et al., <a href="#bib.bib59" title="" class="ltx_ref">2023</a>; Kamata et al., <a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite> (see Subsection <a href="#S6.SS2" title="6.2 Comparison across models ‣ 6 Discussion ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.2</span></a>).
Two-stage model has also shown its effectiveness for systematic generalization in Scene Graph Generation <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib44" title="" class="ltx_ref">2016</a>)</cite>, a task closely related to HOI detection (see Section <a href="#S2" title="2 Related work ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<div id="S6.SS4.p4" class="ltx_para ltx_noindent">
<p id="S6.SS4.p4.1" class="ltx_p">Third, our results also confirmed that pretraining the encoder and decoder improves the systematic generalization performance of HOI detection models (see Subsection <a href="#S6.SS3" title="6.3 Importance of pretraining the encoder and decoder ‣ 6 Discussion ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.3</span></a>).
In this study, the encoder and decoder were initialized with the weights of DETR <cite class="ltx_cite ltx_citemacro_citep">(Carion et al., <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite> trained on object detection task using the Microsoft COCO dataset.
Pretraining on other related tasks such as Scene Graph Generation might also improve the systematic generalization performance of HOI detection models.</p>
</div>
<div id="S6.SS4.p5" class="ltx_para ltx_noindent">
<p id="S6.SS4.p5.1" class="ltx_p">In addition, previous work <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib44" title="" class="ltx_ref">2016</a>; Kan et al., <a href="#bib.bib28" title="" class="ltx_ref">2021</a>)</cite> have shown that integrating commonsense knowledge from external natural language resources is effective for improving the systematic generalization performance in Scene Graph Generation.
This is the fourth direction we think worth pursuing in HOI detection as well.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para ltx_noindent">
<p id="S7.p1.1" class="ltx_p">We created new splits of two HOI detection datasets, HICO-DET-SG and V-COCO-SG, whose train and test data consist of separate combinations of object-interaction classes for evaluating the systematic generalization performance of HOI detection models.
The test performances of representative HOI detection models were considerably worse on our SG splits than on the original splits, indicating that systematic generalization is a challenging goal in HOI detection.
We also analyzed the results and presented four possible research directions for improving the systematic generalization performance.
We hope that our new data splits and presented analysis will encourage further research on systematic generalization in HOI detection.

</p>
</div>
<section id="S7.SS0.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Reproducibility statement</h4>

<div id="S7.SS0.SSSx1.p1" class="ltx_para ltx_noindent">
<p id="S7.SS0.SSSx1.p1.1" class="ltx_p">The JSON files determining HICO-DET-SG and V-COCO-SG and the source code that created the files are publicly available at <a target="_blank" href="https://github.com/FujitsuResearch/hoi_sg" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/FujitsuResearch/hoi_sg</a>.
The URLs of other existing assets (datasets and source code) used in this study are provided in the References.
The experimental setups for performance evaluation of representative HOI detection models are described in Section <a href="#S4" title="4 Experimental setups for evaluating representative HOI detection models ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
<section id="S7.SS0.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Acknowledgments</h4>

<div id="S7.SS0.SSSx2.p1" class="ltx_para ltx_noindent">
<p id="S7.SS0.SSSx2.p1.1" class="ltx_p">The authors thank the anonymous reviewers at DistShift 2022 (NeurIPS Workshop) for providing valuable feedback.
Computational resource of AI Bridging Cloud Infrastructure (ABCI) provided by National Institute of Advanced Industrial Science and Technology (AIST) was used for the experiments in this study.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bahdanau et al. (2019)</span>
<span class="ltx_bibblock">
Dzmitry Bahdanau, Shikhar Murty, Michael Noukhovitch, Thien Huu Nguyen, Harm de
Vries, and Aaron Courville.

</span>
<span class="ltx_bibblock">Systematic generalization: What is required and can it be learned?

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 7th International Conference on Learning
Representations (ICLR)</em>, 2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=HkezXnA9YX" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=HkezXnA9YX</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bahdanau et al. (2020)</span>
<span class="ltx_bibblock">
Dzmitry Bahdanau, Harm de Vries, Timothy J. O’Donnell, Shikhar Murty, Philippe
Beaudoin, Yoshua Bengio, and Aaron C. Courville.

</span>
<span class="ltx_bibblock">CLOSURE: Assessing systematic generalization of CLEVR models.

</span>
<span class="ltx_bibblock">arXiv preprint, arXiv:1912.05783v2, 2020.

</span>
<span class="ltx_bibblock">DOI <a target="_blank" href="https://doi.org/10.48550/arXiv.1912.05783" title="" class="ltx_ref ltx_href">10.48550/arXiv.1912.05783</a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baldassarre et al. (2020)</span>
<span class="ltx_bibblock">
Federico Baldassarre, Kevin Smith, Josephine Sullivan, and Hossein Azizpour.

</span>
<span class="ltx_bibblock">Explanation-based weakly-supervised learning of visual relations with
graph networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 16th European Conference on Computer
Vision (ECCV)</em>, pp.  612–630, 2020.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1007/978-3-030-58604-1_37" title="" class="ltx_ref ltx_href">10.1007/978-3-030-58604-1_37</a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baroni (2020)</span>
<span class="ltx_bibblock">
Marco Baroni.

</span>
<span class="ltx_bibblock">Linguistic generalization and compositionality in modern artificial
neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Philosophical Transactions of the Royal Society B: Biological
Sciences</em>, 375(1791):20190307, 2020.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1098/rstb.2019.0307" title="" class="ltx_ref ltx_href">10.1098/rstb.2019.0307</a>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bergen et al. (2021)</span>
<span class="ltx_bibblock">
Leon Bergen, Timothy J. O’Donnell, and Dzmitry Bahdanau.

</span>
<span class="ltx_bibblock">Systematic generalization with edge transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 34
(NeurIPS)</em>, pp.  1390–1402, 2021.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://proceedings.neurips.cc/paper/2021/hash/0a4dc6dae338c9cb08947c07581f77a2-Abstract.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2021/hash/0a4dc6dae338c9cb08947c07581f77a2-Abstract.html</a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carion et al. (2020)</span>
<span class="ltx_bibblock">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
Kirillov, and Sergey Zagoruyko.

</span>
<span class="ltx_bibblock">End-to-end object detection with transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 16th European Conference on Computer
Vision (ECCV)</em>, pp.  213–229, 2020.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1007/978-3-030-58452-8_13" title="" class="ltx_ref ltx_href">10.1007/978-3-030-58452-8_13</a>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chao et al. (2015)</span>
<span class="ltx_bibblock">
Yu-Wei Chao, Zhan Wang, Yugeng He, Jiaxuan Wang, and Jia Deng.

</span>
<span class="ltx_bibblock">HICO: A benchmark for recognizing human-object interactions in
images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV)</em>, pp.  1017–1025, 2015.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1109/ICCV.2015.122" title="" class="ltx_ref ltx_href">10.1109/ICCV.2015.122</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chao et al. (2018)</span>
<span class="ltx_bibblock">
Yu-Wei Chao, Yunfan Liu, Xieyang Liu, Huayi Zeng, and Jia Deng.

</span>
<span class="ltx_bibblock">Learning to detect human-object interactions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Winter Conference on
Applications of Computer Vision (WACV)</em>, pp.  381–389, 2018.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1109/WACV.2018.00048" title="" class="ltx_ref ltx_href">10.1109/WACV.2018.00048</a>

<br class="ltx_break">The HICO-DET dataset is publicly available at
<a target="_blank" href="http://www-personal.umich.edu/~ywchao/hico/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www-personal.umich.edu/~ywchao/hico/</a> (Accessed on September
29th, 2022).

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen &amp; Yanai (2021)</span>
<span class="ltx_bibblock">
Junwen Chen and Keiji Yanai.

</span>
<span class="ltx_bibblock">QAHOI: Query-based anchors for human-object interaction detection.

</span>
<span class="ltx_bibblock">arXiv preprint, arXiv:2112.08647, 2021.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.48550/arXiv.2112.08647" title="" class="ltx_ref ltx_href">10.48550/arXiv.2112.08647</a>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2021)</span>
<span class="ltx_bibblock">
Mingfei Chen, Yue Liao, Si Liu, Zhiyuan Chen, Fei Wang, and Chen Qian.

</span>
<span class="ltx_bibblock">Reformulating HOI detection as adaptive set prediction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, pp.  9000–9009, 2021.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1109/CVPR46437.2021.00889" title="" class="ltx_ref ltx_href">10.1109/CVPR46437.2021.00889</a>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiou et al. (2021)</span>
<span class="ltx_bibblock">
Meng-Jiun Chiou, Chun-Yu Liao, Li-Wei Wang, Roger Zimmermann, and Jiashi Feng.

</span>
<span class="ltx_bibblock">ST-HOI: A spatial-temporal baseline for human-object interaction
detection in videos.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Workshop on Intelligent Cross-Data
Analysis and Retrieval</em>, pp.  9–17, 2021.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1145/3463944.3469097" title="" class="ltx_ref ltx_href">10.1145/3463944.3469097</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">D’Amario et al. (2021)</span>
<span class="ltx_bibblock">
Vanessa D’Amario, Tomotake Sasaki, and Xavier Boix.

</span>
<span class="ltx_bibblock">How modular should neural module networks be for systematic
generalization?

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 34
(NeurIPS)</em>, pp.  23374–23385, 2021.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2021/hash/c467978aaae44a0e8054e174bc0da4bb-Abstract.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper_files/paper/2021/hash/c467978aaae44a0e8054e174bc0da4bb-Abstract.html</a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Everingham et al. (2014)</span>
<span class="ltx_bibblock">
Mark Everingham, S. M. Ali Eslami, Luc Van Gool, Christopher K. I. Williams,
John M. Winn, and Andrew Zisserman.

</span>
<span class="ltx_bibblock">The Pascal visual object classes challenge: A retrospective.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, 111:98–136, 2014.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1007/s11263-014-0733-5" title="" class="ltx_ref ltx_href">10.1007/s11263-014-0733-5</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fodor &amp; Pylyshyn (1988)</span>
<span class="ltx_bibblock">
Jerry A. Fodor and Zenon W. Pylyshyn.

</span>
<span class="ltx_bibblock">Connectionism and cognitive architecture: A critical analysis.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Cognition</em>, 28(1-2):3–71, 1988.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1016/0010-0277(88)90031-5" title="" class="ltx_ref ltx_href">10.1016/0010-0277(88)90031-5</a>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2018)</span>
<span class="ltx_bibblock">
Chen Gao, Yuliang Zou, and Jia-Bin Huang.

</span>
<span class="ltx_bibblock">iCAN: Instance-centric attention network for human-object
interaction detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th British Machine Vision Conference
(BMVC)</em>, 2018.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.48550/arXiv.1808.10437" title="" class="ltx_ref ltx_href">10.48550/arXiv.1808.10437</a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2020)</span>
<span class="ltx_bibblock">
Chen Gao, Jiarui Xu, Yuliang Zou, and Jia-Bin Huang.

</span>
<span class="ltx_bibblock">DRG: Dual relation graph for human-object interaction detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 16th European Conference on Computer
Vision (ECCV)</em>, 2020.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1007/978-3-030-58610-2_41" title="" class="ltx_ref ltx_href">10.1007/978-3-030-58610-2_41</a>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gkioxari et al. (2018)</span>
<span class="ltx_bibblock">
Georgia Gkioxari, Ross Girshick, Piotr Dollár, and Kaiming He.

</span>
<span class="ltx_bibblock">Detecting and recognizing human-object interactions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, pp.  8359–8367, 2018.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1109/CVPR.2018.00872" title="" class="ltx_ref ltx_href">10.1109/CVPR.2018.00872</a>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al. (2018)</span>
<span class="ltx_bibblock">
Chunhui Gu, Chen Sun, David A. Ross, Carl Vondrick, Caroline Pantofaru, Yeqing
Li, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul
Sukthankar, Cordelia Schmid, and Jitendra Malik.

</span>
<span class="ltx_bibblock">AVA: A video dataset of spatio-temporally localized atomic visual
actions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, pp.  6047–6056, 2018.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1109/CVPR.2018.00633" title="" class="ltx_ref ltx_href">10.1109/CVPR.2018.00633</a>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta &amp; Malik (2015)</span>
<span class="ltx_bibblock">
Saurabh Gupta and Jitendra Malik.

</span>
<span class="ltx_bibblock">Visual semantic role labeling.

</span>
<span class="ltx_bibblock">arXiv preprint, arXiv:1505.04474, 2015.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://doi.org/10.48550/arXiv.1505.04474" title="" class="ltx_ref ltx_href">10.48550/arXiv.1505.04474</a>

<br class="ltx_break">The V-COCO dataset is publicly available at
<a target="_blank" href="https://github.com/s-gupta/v-coco" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/s-gupta/v-coco</a> (Accessed on September 29th, 2022).

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2016)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, pp.  770–778, 2016.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1109/CVPR.2016.90" title="" class="ltx_ref ltx_href">10.1109/CVPR.2016.90</a>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et al. (2021)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan
Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob
Steinhardt, and Justin Gilmer.

</span>
<span class="ltx_bibblock">The many faces of robustness: A critical analysis of
out-of-distribution generalization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV)</em>, pp.  8320–8329, 2021.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1109/ICCV48922.2021.00823" title="" class="ltx_ref ltx_href">10.1109/ICCV48922.2021.00823</a>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et al. (2022)</span>
<span class="ltx_bibblock">
Joy Hsu, Jiayuan Mao, and Jiaju Wu.

</span>
<span class="ltx_bibblock">DisCo: Improving compositional generalization in visual reasoning
through distribution coverage.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Transactions on Machine Learning Research</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=EgHnKOLaKW" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=EgHnKOLaKW</a>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al. (2021)</span>
<span class="ltx_bibblock">
Zhong Ji, Xiyao Liu, Yanwei Pang, Wangli Ouyang, and Xuelong Li.

</span>
<span class="ltx_bibblock">Few-shot human-object interaction recognition with semantic-guided
attentive prototypes network.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Image Processing</em>, 30:1648–1661, 2021.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1109/TIP.2020.3046861" title="" class="ltx_ref ltx_href">10.1109/TIP.2020.3046861</a>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2015)</span>
<span class="ltx_bibblock">
Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David Shamma, Michael
Bernstein, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Image retrieval using scene graphs.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, pp.  3668–3678, 2015.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1109/CVPR.2015.7298990" title="" class="ltx_ref ltx_href">10.1109/CVPR.2015.7298990</a>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2017)</span>
<span class="ltx_bibblock">
Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei,
C. Lawrence Zitnick, and Ross Girshick.

</span>
<span class="ltx_bibblock">CLEVR: A diagnostic dataset for compositional language and
elementary visual reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, pp.  1988–1997, 2017.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1109/CVPR.2017.215" title="" class="ltx_ref ltx_href">10.1109/CVPR.2017.215</a>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kalyan et al. (2021)</span>
<span class="ltx_bibblock">
Katikapalli Subramanyam Kalyan, Ajit Rajasekharan, and Sivanesan Sangeetha.

</span>
<span class="ltx_bibblock">AMMUS : A survey of transformer-based pretrained models in natural
language processing.

</span>
<span class="ltx_bibblock">arXiv preprint, arXiv:2108.05542, 2021.

</span>
<span class="ltx_bibblock">DOI <a target="_blank" href="https://doi.org/10.48550/arXiv.2108.05542" title="" class="ltx_ref ltx_href">10.48550/arXiv.2108.05542</a>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamata et al. (2023)</span>
<span class="ltx_bibblock">
Yuichi Kamata, Moyuru Yamada, and Takayuki Okatani.

</span>
<span class="ltx_bibblock">Self-Modularized Transformer: Learn to modularize networks for
systematic generalization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 18th International Joint Conference on
Computer Vision, Imaging and Computer Graphics Theory and Applications -
Volume 5: VISAPP</em>, pp.  599–606, 2023.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.5220/0011682100003417" title="" class="ltx_ref ltx_href">10.5220/0011682100003417</a>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kan et al. (2021)</span>
<span class="ltx_bibblock">
Xuan Kan, Hejie Cui, and Carl Yang.

</span>
<span class="ltx_bibblock">Zero-shot scene graph relation prediction through commonsense
knowledge integration.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Machine Learning
and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)</em>,
pp.  466–482, 2021.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1007/978-3-030-86520-7_29" title="" class="ltx_ref ltx_href">10.1007/978-3-030-86520-7_29</a>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khan et al. (2021)</span>
<span class="ltx_bibblock">
Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz
Khan, and Mubarak Shah.

</span>
<span class="ltx_bibblock">Transformers in vision: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">ACM Computing Survey</em>, 54(10s):1–41, 2021.

</span>
<span class="ltx_bibblock">DOI <a target="_blank" href="https://www.doi.org/10.1145/3505244" title="" class="ltx_ref ltx_href">10.1145/3505244</a>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2021)</span>
<span class="ltx_bibblock">
Bumsoo Kim, Junhyun Lee, Jaewoo Kang, Eun-Sol Kim, and Hyunwoo J. Kim.

</span>
<span class="ltx_bibblock">HOTR: End-to-end human-object interaction detection with
transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, pp.  74–83, 2021.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1109/CVPR46437.2021.00014" title="" class="ltx_ref ltx_href">10.1109/CVPR46437.2021.00014</a>

<br class="ltx_break">The official source code of HOTR is publicly available at
<a target="_blank" href="https://github.com/kakaobrain/HOTR" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/kakaobrain/HOTR</a> (Accessed on September 29th, 2022).

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim &amp; Linzen (2020)</span>
<span class="ltx_bibblock">
Najoung Kim and Tal Linzen.

</span>
<span class="ltx_bibblock">COGS: A compositional generalization challenge based on semantic
interpretation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pp.  9087–9105, 2020.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.18653/v1/2020.emnlp-main.731" title="" class="ltx_ref ltx_href">10.18653/v1/2020.emnlp-main.731</a>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koppula et al. (2013)</span>
<span class="ltx_bibblock">
Hema Swetha Koppula, Rudhir Gupta, and Ashutosh Saxena.

</span>
<span class="ltx_bibblock">Learning human activities and object affordances from RGB-D videos.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">International Journal of Robotics Research</em>, 32(8):951–970, 2013.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1177/0278364913478446" title="" class="ltx_ref ltx_href">10.1177/0278364913478446</a>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lake &amp; Baroni (2018)</span>
<span class="ltx_bibblock">
Brenden M. Lake and Marco Baroni.

</span>
<span class="ltx_bibblock">Generalization without systematicity: On the compositional skills of
sequence-to-sequence recurrent networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 35th International Conference on Machine
Learning (ICML)</em>, pp.  2873–2882, 2018.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://proceedings.mlr.press/v80/lake18a.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://proceedings.mlr.press/v80/lake18a.html</a>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lake et al. (2017)</span>
<span class="ltx_bibblock">
Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gershman.

</span>
<span class="ltx_bibblock">Building machines that learn and think like people.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Behavioral and Brain Sciences</em>, 40:e253, 2017.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1017/S0140525X16001837" title="" class="ltx_ref ltx_href">10.1017/S0140525X16001837</a>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lake et al. (2019)</span>
<span class="ltx_bibblock">
Brenden M. Lake, Tal Linzen, and Marco Baroni.

</span>
<span class="ltx_bibblock">Human few-shot learning of compositional instructions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 41st Annual Conference of the Cognitive
Science Society (CogSci)</em>, pp.  611–617, 2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://dblp1.uni-trier.de/rec/conf/cogsci/LakeLB19.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://dblp1.uni-trier.de/rec/conf/cogsci/LakeLB19.html</a>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2019)</span>
<span class="ltx_bibblock">
Yong-Lu Li, Siyuan Zhou, Xijie Huang, Liang Xu, Ze Ma, Hao-Shu Fang, Yanfeng
Wang, and Cewu Lu.

</span>
<span class="ltx_bibblock">Transferable interactiveness knowledge for human-object interaction
detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR)</em>, pp.  3580–3589, 2019.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1109/CVPR.2019.00370" title="" class="ltx_ref ltx_href">10.1109/CVPR.2019.00370</a>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020)</span>
<span class="ltx_bibblock">
Yong-Lu Li, Xinpeng Liu, Xiaoqian Wu, Yizhuo Li, and Cewu Lu.

</span>
<span class="ltx_bibblock">HOI analysis: Integrating and decomposing human-object interaction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 33
(NeurIPS)</em>, pp.  5011–5022, 2020.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://proceedings.neurips.cc/paper/2020/hash/3493894fa4ea036cfc6433c3e2ee63b0-Abstract.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2020/hash/3493894fa4ea036cfc6433c3e2ee63b0-Abstract.html</a>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liao et al. (2020)</span>
<span class="ltx_bibblock">
Yue Liao, Si Liu, Fei Wang, Yanjie Chen, Chen Qian, and Jiashi Feng.

</span>
<span class="ltx_bibblock">PPDM: Parallel point detection and matching for real-time
human-object interaction detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, pp.  482–490, 2020.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1109/CVPR42600.2020.00056" title="" class="ltx_ref ltx_href">10.1109/CVPR42600.2020.00056</a>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liao et al. (2022)</span>
<span class="ltx_bibblock">
Yue Liao, Aixi Zhang, Miao Lu, Yongliang Wang, Xiaobo Li, and Si Liu.

</span>
<span class="ltx_bibblock">GEN-VLKT: Simplify association and enhance interaction
understanding for HOI detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, pp.  20123–20132, 2022.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1109/CVPR52688.2022.01949" title="" class="ltx_ref ltx_href">10.1109/CVPR52688.2022.01949</a>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2022)</span>
<span class="ltx_bibblock">
Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu.

</span>
<span class="ltx_bibblock">A survey of transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">AI Open</em>, 3:111–132, 2022.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1016/j.aiopen.2022.10.001" title="" class="ltx_ref ltx_href">10.1016/j.aiopen.2022.10.001</a>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft COCO: Common objects in context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 13th European Conference on Computer
Vision (ECCV)</em>, pp.  740–755, 2014.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1007/978-3-319-10602-1_48" title="" class="ltx_ref ltx_href">10.1007/978-3-319-10602-1_48</a>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021)</span>
<span class="ltx_bibblock">
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
Baining Guo.

</span>
<span class="ltx_bibblock">Swin transformer: Hierarchical vision transformer using shifted
windows.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV)</em>, pp.  9992–10002, 2021.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1109/ICCV48922.2021.00986" title="" class="ltx_ref ltx_href">10.1109/ICCV48922.2021.00986</a>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov &amp; Hutter (2019)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 7th International Conference on Learning
Representations (ICLR)</em>, 2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=Bkg6RiCqY7" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=Bkg6RiCqY7</a>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2016)</span>
<span class="ltx_bibblock">
Cewu Lu, Ranjay Krishna, Michael Bernstein, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Visual relationship detection with language priors.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 14th European Conference on Computer
Vision (ECCV)</em>, pp.  852–869, 2016.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1007/978-3-319-46448-0_51" title="" class="ltx_ref ltx_href">10.1007/978-3-319-46448-0_51</a>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2023)</span>
<span class="ltx_bibblock">
Shuailei Ma, Yuefeng Wang, Shanze Wang, and Ying Wei.

</span>
<span class="ltx_bibblock">FGAHOI: Fine-grained anchors for human-object interaction
detection.

</span>
<span class="ltx_bibblock">arXiv preprint, arXiv:2301.04019, 2023.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://doi.org/10.48550/arXiv.2301.04019" title="" class="ltx_ref ltx_href">10.48550/arXiv.2301.04019</a>

<br class="ltx_break">The official source code of FGAHOI is publicly available at
<a target="_blank" href="https://github.com/xiaomabufei/FGAHOI" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/xiaomabufei/FGAHOI</a> (Accessed on March 23th, 2023).

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madan et al. (2022)</span>
<span class="ltx_bibblock">
Spandan Madan, Timothy Henry, Jamell Dozier, Helen Ho, Nishchal Bhandari,
Tomotake Sasaki, Frédo Durand, Hanspeter Pfister, and Xavier Boix.

</span>
<span class="ltx_bibblock">When and how convolutional neural networks generalize to
out-of-distribution category–viewpoint combinations.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Nature Machine Intelligence</em>, 4(2):146–153, 2022.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1038/s42256-021-00437-5" title="" class="ltx_ref ltx_href">10.1038/s42256-021-00437-5</a>.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marcus (2001)</span>
<span class="ltx_bibblock">
Gary F. Marcus.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">The algebraic mind: Integrating connectionism and cognitive
science</em>.

</span>
<span class="ltx_bibblock">MIT press, 2001.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Purushwalkam et al. (2019)</span>
<span class="ltx_bibblock">
Senthil Purushwalkam, Maximillian Nickel, Abhinav Gupta, and Marc’Aurelio
Ranzato.

</span>
<span class="ltx_bibblock">Task-driven modular networks for zero-shot compositional learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV)</em>, pp.  3592–3601, 2019.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1109/ICCV.2019.00369" title="" class="ltx_ref ltx_href">10.1109/ICCV.2019.00369</a>.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ruis et al. (2020)</span>
<span class="ltx_bibblock">
Laura Ruis, Jacob Andreas, Marco Baroni, Diane Bouchacourt, and Brenden M.
Lake.

</span>
<span class="ltx_bibblock">A benchmark for systematic generalization in grounded language
understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 33
(NeurIPS)</em>, 2020.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://proceedings.neurips.cc/paper/2020/hash/e5a90182cc81e12ab5e72d66e0b46fe3-Abstract.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2020/hash/e5a90182cc81e12ab5e72d66e0b46fe3-Abstract.html</a>.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. (2021)</span>
<span class="ltx_bibblock">
Zheyan Shen, Jiashuo Liu, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng
Cui.

</span>
<span class="ltx_bibblock">Towards out-of-distribution generalization: A survey.

</span>
<span class="ltx_bibblock">arXiv preprint, arxiv.2108.13624, 2021.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.48550/arXiv:2108.13624" title="" class="ltx_ref ltx_href">10.48550/arXiv:2108.13624</a>.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smolensky et al. (2022)</span>
<span class="ltx_bibblock">
Paul Smolensky, Richard Thomas McCoy, Roland Fernandez, Matthew Goldrick, and
Jianfeng Gao.

</span>
<span class="ltx_bibblock">Neurocompositional computing: From the central paradox of cognition
to a new generation of AI systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">AI Magazine</em>, 43(3):308–322, 2022.

</span>
<span class="ltx_bibblock">DOI <a target="_blank" href="https://doi.org/10.1002/aaai.12065" title="" class="ltx_ref ltx_href">10.1002/aaai.12065</a>.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tamura et al. (2021)</span>
<span class="ltx_bibblock">
Masato Tamura, Hiroki Ohashi, and Tomoaki Yoshinaga.

</span>
<span class="ltx_bibblock">QPIC: Query-based pairwise human-object interaction detection with
image-wide contextual information.

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, pp.  10405–10414, 2021.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1109/CVPR46437.2021.01027" title="" class="ltx_ref ltx_href">10.1109/CVPR46437.2021.01027</a>

<br class="ltx_break">The official source code of QPIC is publicly available at
<a target="_blank" href="https://github.com/hitachi-rd-cv/qpic" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/hitachi-rd-cv/qpic</a> (Accessed on September 29th,
2022).

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2020)</span>
<span class="ltx_bibblock">
Kaihua Tang, Yulei Niu, Jianqiang Huang, Jiaxin Shi, and Hanwang Zhang.

</span>
<span class="ltx_bibblock">Unbiased scene graph generation from biased training.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, pp.  3713–3722, 2020.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1109/CVPR42600.2020.00377" title="" class="ltx_ref ltx_href">10.1109/CVPR42600.2020.00377</a>.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teney et al. (2020)</span>
<span class="ltx_bibblock">
Damien Teney, Ehsan Abbasnejad, Kushal Kafle, Robik Shrestha, Christopher
Kanan, and Anton van den Hengel.

</span>
<span class="ltx_bibblock">On the value of out-of-distribution testing: An example of
Goodhart’s law.

</span>
<span class="ltx_bibblock">In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 33
(NeurIPS)</em>, pp.  407–417, 2020.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://proceedings.neurips.cc/paper/2020/hash/045117b0e0a11a242b9765e79cbf113f-Abstract.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2020/hash/045117b0e0a11a242b9765e79cbf113f-Abstract.html</a>.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ullman et al. (2021)</span>
<span class="ltx_bibblock">
Shimon Ullman, Liav Assif, Alona Strugatski, Ben-Zion Vatashsky, Hila Levi,
Aviv Netanyahu, and Adam Uri Yaari.

</span>
<span class="ltx_bibblock">Image interpretation by iterative bottom-up top-down processing.

</span>
<span class="ltx_bibblock">Technical Report CBMM Memo No. 120, Center for Brains, Minds and
Machines, 2021.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://cbmm.mit.edu/publications/image-interpretation-iterative-bottom-top-down-processing" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://cbmm.mit.edu/publications/image-interpretation-iterative-bottom-top-down-processing</a>.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van der Velde et al. (2004)</span>
<span class="ltx_bibblock">
Frank van der Velde, Gwendid T. van der Voort van der Kleij, and Marc de Kamps.

</span>
<span class="ltx_bibblock">Lack of combinatorial productivity in language processing with simple
recurrent networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Connection Science</em>, 16(1):21–46, 2004.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://doi.org/10.1080/09540090310001656597" title="" class="ltx_ref ltx_href">10.1080/09540090310001656597</a>.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vankov &amp; Bowers (2020)</span>
<span class="ltx_bibblock">
Ivan I. Vankov and Jeffrey S. Bowers.

</span>
<span class="ltx_bibblock">Training neural networks to encode symbols enables combinatorial
generalization.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">Philosophical Transactions of the Royal Society B: Biological
Sciences</em>, 375(1791):20190309, 2020.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1098/rstb.2019.0309" title="" class="ltx_ref ltx_href">10.1098/rstb.2019.0309</a>.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 30
(NeurIPS)</em>, 2017.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html</a>.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yamada et al. (2023)</span>
<span class="ltx_bibblock">
Moyuru Yamada, Vanessa D’Amario, Kentaro Takemoto, Xavier Boix, and Tomotake
Sasaki.

</span>
<span class="ltx_bibblock">Transformer Module Networks for systematic generalization in visual
question answering.

</span>
<span class="ltx_bibblock">Technical Report CBMM Memo No. 121, Ver.2, Center for Brains, Minds
and Machines, 2023.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://cbmm.mit.edu/publications/transformer-module-networks-systematic-generalization-visual-question-answering" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://cbmm.mit.edu/publications/transformer-module-networks-systematic-generalization-visual-question-answering</a>.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. (2022)</span>
<span class="ltx_bibblock">
Nanyang Ye, Kaican Li, Haoyue Bai, Runpeng Yu, Lanqing Hong, Fengwei Zhou,
Zhenguo Li, and Jun Zhu.

</span>
<span class="ltx_bibblock">OoD-Bench: Quantifying and understanding two dimensions of
out-of-distribution generalization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, pp.  7937–7948, 2022.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1109/CVPR52688.2022.00779" title="" class="ltx_ref ltx_href">10.1109/CVPR52688.2022.00779</a>.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2021)</span>
<span class="ltx_bibblock">
Aixi Zhang, Yue Liao, Si Liu, Miao Lu, Yongliang Wang, Chen Gao, and Xiaobo Li.

</span>
<span class="ltx_bibblock">Mining the benefits of two-stage and one-stage HOI detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 34
(NeurIPS)</em>, 2021.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://papers.nips.cc/paper_files/paper/2021/hash/8f1d43620bc6bb580df6e80b0dc05c48-Abstract.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://papers.nips.cc/paper_files/paper/2021/hash/8f1d43620bc6bb580df6e80b0dc05c48-Abstract.html</a>.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2022)</span>
<span class="ltx_bibblock">
Yong Zhang, Yingwei Pan, Ting Yao, Rui Huang, Tao Mei, and Chang-Wen Chen.

</span>
<span class="ltx_bibblock">Exploring structure-aware transformer over interaction proposals for
human-object interaction detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, pp.  19526–19535, 2022.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1109/CVPR52688.2022.01894" title="" class="ltx_ref ltx_href">10.1109/CVPR52688.2022.01894</a>

<br class="ltx_break">The official source code of STIP is publicly available at
<a target="_blank" href="https://github.com/zyong812/STIP" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/zyong812/STIP</a> (Accessed on September 29th, 2022).

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2021)</span>
<span class="ltx_bibblock">
Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai.

</span>
<span class="ltx_bibblock">Deformable DETR: Deformable transformers for end-to-end object
detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 9th International Conference on Learning
Representations (ICLR)</em>, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=gZ9hCDWe6ke" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=gZ9hCDWe6ke</a>.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zou et al. (2021)</span>
<span class="ltx_bibblock">
Cheng Zou, Bohan Wang, Yue Hu, Junqi Liu, Qian Wu, Yu Zhao, Boxun Li, Chenguang
Zhang, Chi Zhang, Yichen Wei, and Jian Sun.

</span>
<span class="ltx_bibblock">End-to-end human object interaction detection with HOI transformer.

</span>
<span class="ltx_bibblock">In <em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, pp.  11820–11829, 2021.

</span>
<span class="ltx_bibblock">DOI
<a target="_blank" href="https://www.doi.org/10.1109/CVPR46437.2021.01165" title="" class="ltx_ref ltx_href">10.1109/CVPR46437.2021.01165</a>.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Further details of the creation process of systematic generalization (SG) splits</h2>

<div id="A1.p1" class="ltx_para ltx_noindent">
<p id="A1.p1.1" class="ltx_p">This Appendix explains the process of splitting the HICO-DET and V-COCO datasets for evaluating systematic generalization performance.</p>
</div>
<div id="A1.p2" class="ltx_para ltx_noindent">
<p id="A1.p2.1" class="ltx_p">First, we decided the numbers of object-interaction combination classes in the train and test data. Initially we attempted to match their ratio to the ratio of HOI triplets in the original train and test data, but eventually included more combination classes in the train data (540 in HICO-DET-SG and 160 in V-COCO-SG) to ensure that every object class is paired with multiple interaction classes and that every interaction class is paired with multiple object classes. This makes it possible for a model to learn the concepts of object/interaction themselves independently of the specific interaction/object paired in the train data.</p>
</div>
<div id="A1.p3" class="ltx_para ltx_noindent">
<p id="A1.p3.1" class="ltx_p">We then created the SG splits as described in Algorithm <a href="#alg1" title="Algorithm 1 ‣ Appendix A Further details of the creation process of systematic generalization (SG) splits ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="A1.p4" class="ltx_para ltx_noindent">
<p id="A1.p4.1" class="ltx_p">We used a random selection method to determine which of the train data or the test data each object-interaction combination class should be
assigned to (line <a href="#alg1.l3" title="In Algorithm 1 ‣ Appendix A Further details of the creation process of systematic generalization (SG) splits ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<div id="A1.p5" class="ltx_para ltx_noindent">
<p id="A1.p5.1" class="ltx_p">Subsequently, we checked HOI triplets in each image and determined whether the image should be allocated to the train data, test data, or neither of them (lines <a href="#alg1.l6" title="In Algorithm 1 ‣ Appendix A Further details of the creation process of systematic generalization (SG) splits ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>-<a href="#alg1.l18" title="In Algorithm 1 ‣ Appendix A Further details of the creation process of systematic generalization (SG) splits ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18</span></a>);
an image was allocated to the train data or the test data when all object-interaction combinations in the image had been assigned to the train data or the test data, respectively.
When an image contained both object-interaction combinations assigned to the train data and ones assigned to the test data, the image was not allocated to either the train data or the test data.
This process resulted in a decrease in the total numbers of images and triplets of the SG splits compared to those of the original datasets (Table <a href="#S3.T1" title="Table 1 ‣ 3.2 Statistics of the HICO-DET-SG and V-COCO-SG ‣ 3 HICO-DET-SG and V-COCO-SG ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div id="A1.p6" class="ltx_para ltx_noindent">
<p id="A1.p6.1" class="ltx_p">After allocating all the images, we verified whether all object and interaction classes alone were contained in the train data (line <a href="#alg1.l19" title="In Algorithm 1 ‣ Appendix A Further details of the creation process of systematic generalization (SG) splits ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">19</span></a>).
If the condition was not met, the process was repeated with a different random seed (line <a href="#alg1.l22" title="In Algorithm 1 ‣ Appendix A Further details of the creation process of systematic generalization (SG) splits ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">22</span></a> and then went back to line <a href="#alg1.l3" title="In Algorithm 1 ‣ Appendix A Further details of the creation process of systematic generalization (SG) splits ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<div id="A1.p7" class="ltx_para ltx_noindent">
<p id="A1.p7.1" class="ltx_p">To ensure that the test performance is not an artifact of a specific selection of object-interaction combination classes in the train and test data, we prepared three distinct train-test splits for each of the HICO-DET-SG and V-COCO-SG.</p>
</div>
<div id="A1.p8" class="ltx_para ltx_noindent">
<p id="A1.p8.1" class="ltx_p">The actual source code that created the SG splits is publicly available at the following repository: <a target="_blank" href="https://github.com/FujitsuResearch/hoi_sg" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/FujitsuResearch/hoi_sg</a>.</p>
</div>
<figure id="A1.fig1" class="ltx_figure">
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.2.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Creation of the systematic generalization (SG) splits.</figcaption>
<div id="alg1.3" class="ltx_listing ltx_listing">
<div id="alg1.l1" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l1.1.1.1" class="ltx_text" style="font-size:80%;">1:</span></span>seed = 0

</div>
<div id="alg1.l2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l2.1.1.1" class="ltx_text" style="font-size:80%;">2:</span></span><span id="alg1.l2.2" class="ltx_text ltx_font_bold">while</span> flag <span id="alg1.l2.3" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l3.1.1.1" class="ltx_text" style="font-size:80%;">3:</span></span>     test_combinations = SELECT(all_combinations, seed) 

</div>
<div id="alg1.l4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l4.1.1.1" class="ltx_text" style="font-size:80%;">4:</span></span>     train_data = []

</div>
<div id="alg1.l5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l5.1.1.1" class="ltx_text" style="font-size:80%;">5:</span></span>     test_data = []

</div>
<div id="alg1.l6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l6.1.1.1" class="ltx_text" style="font-size:80%;">6:</span></span>     <span id="alg1.l6.2" class="ltx_text ltx_font_bold">for</span> scene in dataset <span id="alg1.l6.3" class="ltx_text ltx_font_bold">do</span> 

</div>
<div id="alg1.l7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l7.1.1.1" class="ltx_text" style="font-size:80%;">7:</span></span>         sum = 0

</div>
<div id="alg1.l8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l8.1.1.1" class="ltx_text" style="font-size:80%;">8:</span></span>         test_hois = []

</div>
<div id="alg1.l9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l9.1.1.1" class="ltx_text" style="font-size:80%;">9:</span></span>         <span id="alg1.l9.2" class="ltx_text ltx_font_bold">for</span> hoi in scene.hois <span id="alg1.l9.3" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l10.1.1.1" class="ltx_text" style="font-size:80%;">10:</span></span>              match = MATCH(test_combinations, [hoi.object_class, hoi.interaction_class])

</div>
<div id="alg1.l11" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l11.1.1.1" class="ltx_text" style="font-size:80%;">11:</span></span>              sum = sum + match

</div>
<div id="alg1.l12" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l12.1.1.1" class="ltx_text" style="font-size:80%;">12:</span></span>         <span id="alg1.l12.2" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l12.3" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l13" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l13.1.1.1" class="ltx_text" style="font-size:80%;">13:</span></span>         <span id="alg1.l13.2" class="ltx_text ltx_font_bold">if</span> sum == 0 <span id="alg1.l13.3" class="ltx_text ltx_font_bold">then</span>

</div>
<div id="alg1.l14" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l14.1.1.1" class="ltx_text" style="font-size:80%;">14:</span></span>              train_data.append(scene)

</div>
<div id="alg1.l15" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l15.1.1.1" class="ltx_text" style="font-size:80%;">15:</span></span>         <span id="alg1.l15.2" class="ltx_text ltx_font_bold">else</span> <span id="alg1.l15.3" class="ltx_text ltx_font_bold">if</span> sum == length(scene.hois) <span id="alg1.l15.4" class="ltx_text ltx_font_bold">then</span>

</div>
<div id="alg1.l16" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l16.1.1.1" class="ltx_text" style="font-size:80%;">16:</span></span>              test_data.append(scene)

</div>
<div id="alg1.l17" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l17.1.1.1" class="ltx_text" style="font-size:80%;">17:</span></span>         <span id="alg1.l17.2" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l17.3" class="ltx_text ltx_font_bold">if</span>
</div>
<div id="alg1.l18" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l18.1.1.1" class="ltx_text" style="font-size:80%;">18:</span></span>     <span id="alg1.l18.2" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l18.3" class="ltx_text ltx_font_bold">for</span>

</div>
<div id="alg1.l19" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l19.1.1.1" class="ltx_text" style="font-size:80%;">19:</span></span>     <span id="alg1.l19.2" class="ltx_text ltx_font_bold">if</span> VERIFY(train_data) <span id="alg1.l19.3" class="ltx_text ltx_font_bold">then</span> 

</div>
<div id="alg1.l20" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l20.1.1.1" class="ltx_text" style="font-size:80%;">20:</span></span>         flag = False

</div>
<div id="alg1.l21" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l21.1.1.1" class="ltx_text" style="font-size:80%;">21:</span></span>     <span id="alg1.l21.2" class="ltx_text ltx_font_bold">else</span>
</div>
<div id="alg1.l22" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l22.1.1.1" class="ltx_text" style="font-size:80%;">22:</span></span>         seed += 1 

</div>
<div id="alg1.l23" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l23.1.1.1" class="ltx_text" style="font-size:80%;">23:</span></span>     <span id="alg1.l23.2" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l23.3" class="ltx_text ltx_font_bold">if</span>
</div>
<div id="alg1.l24" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l24.1.1.1" class="ltx_text" style="font-size:80%;">24:</span></span><span id="alg1.l24.2" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l24.3" class="ltx_text ltx_font_bold">while</span>
</div>
</div>
</figure>
<figcaption class="ltx_caption">Algorithm <span class="ltx_ref ltx_nolink ltx_ref_self"><span class="ltx_text ltx_ref_tag">1</span></span> - Remark: “test_combinations” is a list of object-interaction combination classes to be contained only in the test data, “all_combinations” is a list of all combinations of object-interaction classes in the original dataset, “SELECT” is the function that randomly selects “test_combinations” from “all_combinations” with a specified seed, “dataset” represents the set of images in the original dataset (whole the train and test data) and their annotations, and “scene” represents a set of one image and HOI triplets in the image. The “MATCH” function returns 1 if the first argument is equal to the second argument, and 0 otherwise. The “VERIFY” function verifies that all object and interaction classes alone are contained in the train data, that every object class is paired with multiple interaction classes, and that every interaction class is paired with multiple object classes. When creating another distinct split, the initial seed is set larger than the seed used for creating the existing split.</figcaption>
</figure>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Further details of the evaluation results</h2>

<div id="A2.p1" class="ltx_para ltx_noindent">
<p id="A2.p1.1" class="ltx_p">To ensure that the models are well-trained, we evaluated the performances of the models on the train data before the evaluation on the test data.
The left halves of Table <a href="#A2.T3" title="Table 3 ‣ Appendix B Further details of the evaluation results ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and Table <a href="#A2.T4" title="Table 4 ‣ Appendix B Further details of the evaluation results ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> display the mAPs on the train data of HICO-DET-SG and V-COCO-SG, respectively.
For both datasets and for all the models, the mAPs on the train data of the SG splits are coequal to or slightly higher than the original splits.
This is probably attributed to the lower variety of triplets in the train data of the SG splits: HICO-DET-SG and V-COCO-SG contain 540 and 160 object-interaction combinations, respectively, out of 600 and 228 combinations contained in the original datasets, respectively.
The right halves of these tables represent the raw data used for constructing Figure <a href="#S5.F2" title="Figure 2 ‣ 5 Evaluation results ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="A2.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="A2.T3.2.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="A2.T3.3.2" class="ltx_text" style="font-size:90%;">Further details of the systematic generalization performances evaluated on HICO-DET-SG data splits. The mAPs (%) for both train and test data are given in this table. The right half of the table represents the raw data used for constructing Figure <a href="#S5.F2" title="Figure 2 ‣ 5 Evaluation results ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (a). The values in brackets are those reported in the original papers. The term “pretraining” means that the initial weights of the model’s encoder and decoder were copied from DETR <cite class="ltx_cite ltx_citemacro_citep">(Carion et al., <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite> trained on object detection using the Microsoft COCO dataset <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib41" title="" class="ltx_ref">2014</a>)</cite>. The results of FGAHOI are reported only for the non-pretrained cases due to the reasons given in Section <a href="#S4" title="4 Experimental setups for evaluating representative HOI detection models ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The test performances of all models are considerably lower on the HICO-DET-SG compared to original HICO-DET.</span></figcaption>
<table id="A2.T3.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A2.T3.4.1.1" class="ltx_tr">
<th id="A2.T3.4.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt"></th>
<td id="A2.T3.4.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="7">Evaluation on train data (reference)</td>
<td id="A2.T3.4.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="7"><span id="A2.T3.4.1.1.3.1" class="ltx_text ltx_font_bold">Evaluation on test data (main)</span></td>
</tr>
<tr id="A2.T3.4.2.2" class="ltx_tr">
<th id="A2.T3.4.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="A2.T3.4.2.2.2" class="ltx_td ltx_align_center" colspan="2">HOTR</td>
<td id="A2.T3.4.2.2.3" class="ltx_td ltx_align_center" colspan="2">QPIC</td>
<td id="A2.T3.4.2.2.4" class="ltx_td ltx_align_center">FGAHOI</td>
<td id="A2.T3.4.2.2.5" class="ltx_td ltx_align_center ltx_border_r" colspan="2">STIP</td>
<td id="A2.T3.4.2.2.6" class="ltx_td ltx_align_center" colspan="2">HOTR</td>
<td id="A2.T3.4.2.2.7" class="ltx_td ltx_align_center" colspan="2">QPIC</td>
<td id="A2.T3.4.2.2.8" class="ltx_td ltx_align_center">FGAHOI</td>
<td id="A2.T3.4.2.2.9" class="ltx_td ltx_align_center" colspan="2">STIP</td>
</tr>
<tr id="A2.T3.4.3.3" class="ltx_tr">
<th id="A2.T3.4.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">pretraining</th>
<td id="A2.T3.4.3.3.2" class="ltx_td ltx_align_center">✗</td>
<td id="A2.T3.4.3.3.3" class="ltx_td ltx_align_center">✓</td>
<td id="A2.T3.4.3.3.4" class="ltx_td ltx_align_center">✗</td>
<td id="A2.T3.4.3.3.5" class="ltx_td ltx_align_center">✓</td>
<td id="A2.T3.4.3.3.6" class="ltx_td ltx_align_center">✗</td>
<td id="A2.T3.4.3.3.7" class="ltx_td ltx_align_center">✗</td>
<td id="A2.T3.4.3.3.8" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="A2.T3.4.3.3.9" class="ltx_td ltx_align_center">✗</td>
<td id="A2.T3.4.3.3.10" class="ltx_td ltx_align_center">✓</td>
<td id="A2.T3.4.3.3.11" class="ltx_td ltx_align_center">✗</td>
<td id="A2.T3.4.3.3.12" class="ltx_td ltx_align_center">✓</td>
<td id="A2.T3.4.3.3.13" class="ltx_td ltx_align_center">✗</td>
<td id="A2.T3.4.3.3.14" class="ltx_td ltx_align_center">✗</td>
<td id="A2.T3.4.3.3.15" class="ltx_td ltx_align_center">✓</td>
</tr>
<tr id="A2.T3.4.4.4" class="ltx_tr">
<th id="A2.T3.4.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Original HICO-DET</th>
<td id="A2.T3.4.4.4.2" class="ltx_td ltx_align_center ltx_border_t">30.54</td>
<td id="A2.T3.4.4.4.3" class="ltx_td ltx_align_center ltx_border_t">44.80</td>
<td id="A2.T3.4.4.4.4" class="ltx_td ltx_align_center ltx_border_t">33.29</td>
<td id="A2.T3.4.4.4.5" class="ltx_td ltx_align_center ltx_border_t">46.28</td>
<td id="A2.T3.4.4.4.6" class="ltx_td ltx_align_center ltx_border_t">55.82</td>
<td id="A2.T3.4.4.4.7" class="ltx_td ltx_align_center ltx_border_t">13.47</td>
<td id="A2.T3.4.4.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">32.23</td>
<td id="A2.T3.4.4.4.9" class="ltx_td ltx_align_center ltx_border_t">17.63</td>
<td id="A2.T3.4.4.4.10" class="ltx_td ltx_align_center ltx_border_t">26.30</td>
<td id="A2.T3.4.4.4.11" class="ltx_td ltx_align_center ltx_border_t">21.70</td>
<td id="A2.T3.4.4.4.12" class="ltx_td ltx_align_center ltx_border_t">29.59</td>
<td id="A2.T3.4.4.4.13" class="ltx_td ltx_align_center ltx_border_t">36.91</td>
<td id="A2.T3.4.4.4.14" class="ltx_td ltx_align_center ltx_border_t">13.21</td>
<td id="A2.T3.4.4.4.15" class="ltx_td ltx_align_center ltx_border_t">31.57</td>
</tr>
<tr id="A2.T3.4.5.5" class="ltx_tr">
<th id="A2.T3.4.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">(mAP in literature)</th>
<td id="A2.T3.4.5.5.2" class="ltx_td"></td>
<td id="A2.T3.4.5.5.3" class="ltx_td"></td>
<td id="A2.T3.4.5.5.4" class="ltx_td"></td>
<td id="A2.T3.4.5.5.5" class="ltx_td"></td>
<td id="A2.T3.4.5.5.6" class="ltx_td"></td>
<td id="A2.T3.4.5.5.7" class="ltx_td"></td>
<td id="A2.T3.4.5.5.8" class="ltx_td ltx_border_r"></td>
<td id="A2.T3.4.5.5.9" class="ltx_td"></td>
<td id="A2.T3.4.5.5.10" class="ltx_td ltx_align_center">(25.73)</td>
<td id="A2.T3.4.5.5.11" class="ltx_td"></td>
<td id="A2.T3.4.5.5.12" class="ltx_td ltx_align_center">(29.90)</td>
<td id="A2.T3.4.5.5.13" class="ltx_td ltx_align_center">(37.18)</td>
<td id="A2.T3.4.5.5.14" class="ltx_td"></td>
<td id="A2.T3.4.5.5.15" class="ltx_td ltx_align_center">(32.22)</td>
</tr>
<tr id="A2.T3.4.6.6" class="ltx_tr">
<th id="A2.T3.4.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">SG split1</th>
<td id="A2.T3.4.6.6.2" class="ltx_td ltx_align_center ltx_border_t">33.92</td>
<td id="A2.T3.4.6.6.3" class="ltx_td ltx_align_center ltx_border_t">46.03</td>
<td id="A2.T3.4.6.6.4" class="ltx_td ltx_align_center ltx_border_t">34.05</td>
<td id="A2.T3.4.6.6.5" class="ltx_td ltx_align_center ltx_border_t">49.70</td>
<td id="A2.T3.4.6.6.6" class="ltx_td ltx_align_center ltx_border_t">58.97</td>
<td id="A2.T3.4.6.6.7" class="ltx_td ltx_align_center ltx_border_t">13.69</td>
<td id="A2.T3.4.6.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">33.17</td>
<td id="A2.T3.4.6.6.9" class="ltx_td ltx_align_center ltx_border_t">0.33</td>
<td id="A2.T3.4.6.6.10" class="ltx_td ltx_align_center ltx_border_t">2.81</td>
<td id="A2.T3.4.6.6.11" class="ltx_td ltx_align_center ltx_border_t">11.23</td>
<td id="A2.T3.4.6.6.12" class="ltx_td ltx_align_center ltx_border_t">21.94</td>
<td id="A2.T3.4.6.6.13" class="ltx_td ltx_align_center ltx_border_t">6.14</td>
<td id="A2.T3.4.6.6.14" class="ltx_td ltx_align_center ltx_border_t">0.00</td>
<td id="A2.T3.4.6.6.15" class="ltx_td ltx_align_center ltx_border_t">24.53</td>
</tr>
<tr id="A2.T3.4.7.7" class="ltx_tr">
<th id="A2.T3.4.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">SG split2</th>
<td id="A2.T3.4.7.7.2" class="ltx_td ltx_align_center">31.48</td>
<td id="A2.T3.4.7.7.3" class="ltx_td ltx_align_center">42.04</td>
<td id="A2.T3.4.7.7.4" class="ltx_td ltx_align_center">30.23</td>
<td id="A2.T3.4.7.7.5" class="ltx_td ltx_align_center">48.28</td>
<td id="A2.T3.4.7.7.6" class="ltx_td ltx_align_center">55.95</td>
<td id="A2.T3.4.7.7.7" class="ltx_td ltx_align_center">15.13</td>
<td id="A2.T3.4.7.7.8" class="ltx_td ltx_align_center ltx_border_r">34.44</td>
<td id="A2.T3.4.7.7.9" class="ltx_td ltx_align_center">0.40</td>
<td id="A2.T3.4.7.7.10" class="ltx_td ltx_align_center">2.97</td>
<td id="A2.T3.4.7.7.11" class="ltx_td ltx_align_center">12.87</td>
<td id="A2.T3.4.7.7.12" class="ltx_td ltx_align_center">19.98</td>
<td id="A2.T3.4.7.7.13" class="ltx_td ltx_align_center">5.93</td>
<td id="A2.T3.4.7.7.14" class="ltx_td ltx_align_center">0.00</td>
<td id="A2.T3.4.7.7.15" class="ltx_td ltx_align_center">24.14</td>
</tr>
<tr id="A2.T3.4.8.8" class="ltx_tr">
<th id="A2.T3.4.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">SG split3</th>
<td id="A2.T3.4.8.8.2" class="ltx_td ltx_align_center">32.05</td>
<td id="A2.T3.4.8.8.3" class="ltx_td ltx_align_center">44.91</td>
<td id="A2.T3.4.8.8.4" class="ltx_td ltx_align_center">35.54</td>
<td id="A2.T3.4.8.8.5" class="ltx_td ltx_align_center">47.90</td>
<td id="A2.T3.4.8.8.6" class="ltx_td ltx_align_center">54.72</td>
<td id="A2.T3.4.8.8.7" class="ltx_td ltx_align_center">13.25</td>
<td id="A2.T3.4.8.8.8" class="ltx_td ltx_align_center ltx_border_r">30.61</td>
<td id="A2.T3.4.8.8.9" class="ltx_td ltx_align_center">0.62</td>
<td id="A2.T3.4.8.8.10" class="ltx_td ltx_align_center">3.01</td>
<td id="A2.T3.4.8.8.11" class="ltx_td ltx_align_center">15.43</td>
<td id="A2.T3.4.8.8.12" class="ltx_td ltx_align_center">20.58</td>
<td id="A2.T3.4.8.8.13" class="ltx_td ltx_align_center">6.83</td>
<td id="A2.T3.4.8.8.14" class="ltx_td ltx_align_center">0.00</td>
<td id="A2.T3.4.8.8.15" class="ltx_td ltx_align_center">26.19</td>
</tr>
<tr id="A2.T3.4.9.9" class="ltx_tr">
<th id="A2.T3.4.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">Average</th>
<td id="A2.T3.4.9.9.2" class="ltx_td ltx_align_center ltx_border_bb">32.48</td>
<td id="A2.T3.4.9.9.3" class="ltx_td ltx_align_center ltx_border_bb">44.33</td>
<td id="A2.T3.4.9.9.4" class="ltx_td ltx_align_center ltx_border_bb">33.30</td>
<td id="A2.T3.4.9.9.5" class="ltx_td ltx_align_center ltx_border_bb">48.63</td>
<td id="A2.T3.4.9.9.6" class="ltx_td ltx_align_center ltx_border_bb">56.55</td>
<td id="A2.T3.4.9.9.7" class="ltx_td ltx_align_center ltx_border_bb">14.02</td>
<td id="A2.T3.4.9.9.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">32.74</td>
<td id="A2.T3.4.9.9.9" class="ltx_td ltx_align_center ltx_border_bb">0.45</td>
<td id="A2.T3.4.9.9.10" class="ltx_td ltx_align_center ltx_border_bb">2.93</td>
<td id="A2.T3.4.9.9.11" class="ltx_td ltx_align_center ltx_border_bb">13.18</td>
<td id="A2.T3.4.9.9.12" class="ltx_td ltx_align_center ltx_border_bb">20.83</td>
<td id="A2.T3.4.9.9.13" class="ltx_td ltx_align_center ltx_border_bb">6.30</td>
<td id="A2.T3.4.9.9.14" class="ltx_td ltx_align_center ltx_border_bb">0.00</td>
<td id="A2.T3.4.9.9.15" class="ltx_td ltx_align_center ltx_border_bb">24.95</td>
</tr>
</tbody>
</table>
</figure>
<figure id="A2.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="A2.T4.2.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="A2.T4.3.2" class="ltx_text" style="font-size:90%;">Further details of the systematic generalization performances evaluated on V-COCO-SG data splits. The mAPs (%) for both train and test data are given in this table. The right half of the table represents the raw data used for constructing Figure <a href="#S5.F2" title="Figure 2 ‣ 5 Evaluation results ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (b). The values in brackets are those reported in the original papers. The term “pretraining” means that the initial weights of the model’s encoder and decoder were copied from DETR <cite class="ltx_cite ltx_citemacro_citep">(Carion et al., <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite> trained on object detection using the Microsoft COCO dataset <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib41" title="" class="ltx_ref">2014</a>)</cite>. The results of FGAHOI are reported only for the non-pretrained cases due to the reasons given in Section <a href="#S4" title="4 Experimental setups for evaluating representative HOI detection models ‣ HICO-DET-SG and V-COCO-SG: New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The test performances of all models are considerably lower on the V-COCO-SG compared to original V-COCO for all the models.</span></figcaption>
<table id="A2.T4.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A2.T4.4.1.1" class="ltx_tr">
<th id="A2.T4.4.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt"></th>
<td id="A2.T4.4.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="7">Evaluation on train data (reference)</td>
<td id="A2.T4.4.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="7"><span id="A2.T4.4.1.1.3.1" class="ltx_text ltx_font_bold">Evaluation on test data (main)</span></td>
</tr>
<tr id="A2.T4.4.2.2" class="ltx_tr">
<th id="A2.T4.4.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="A2.T4.4.2.2.2" class="ltx_td ltx_align_center" colspan="2">HOTR</td>
<td id="A2.T4.4.2.2.3" class="ltx_td ltx_align_center" colspan="2">QPIC</td>
<td id="A2.T4.4.2.2.4" class="ltx_td ltx_align_center">FGAHOI</td>
<td id="A2.T4.4.2.2.5" class="ltx_td ltx_align_center ltx_border_r" colspan="2">STIP</td>
<td id="A2.T4.4.2.2.6" class="ltx_td ltx_align_center" colspan="2">HOTR</td>
<td id="A2.T4.4.2.2.7" class="ltx_td ltx_align_center" colspan="2">QPIC</td>
<td id="A2.T4.4.2.2.8" class="ltx_td ltx_align_center">FGAHOI</td>
<td id="A2.T4.4.2.2.9" class="ltx_td ltx_align_center" colspan="2">STIP</td>
</tr>
<tr id="A2.T4.4.3.3" class="ltx_tr">
<th id="A2.T4.4.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">pretraining</th>
<td id="A2.T4.4.3.3.2" class="ltx_td ltx_align_center">✗</td>
<td id="A2.T4.4.3.3.3" class="ltx_td ltx_align_center">✓</td>
<td id="A2.T4.4.3.3.4" class="ltx_td ltx_align_center">✗</td>
<td id="A2.T4.4.3.3.5" class="ltx_td ltx_align_center">✓</td>
<td id="A2.T4.4.3.3.6" class="ltx_td ltx_align_center">✗</td>
<td id="A2.T4.4.3.3.7" class="ltx_td ltx_align_center">✗</td>
<td id="A2.T4.4.3.3.8" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="A2.T4.4.3.3.9" class="ltx_td ltx_align_center">✗</td>
<td id="A2.T4.4.3.3.10" class="ltx_td ltx_align_center">✓</td>
<td id="A2.T4.4.3.3.11" class="ltx_td ltx_align_center">✗</td>
<td id="A2.T4.4.3.3.12" class="ltx_td ltx_align_center">✓</td>
<td id="A2.T4.4.3.3.13" class="ltx_td ltx_align_center">✗</td>
<td id="A2.T4.4.3.3.14" class="ltx_td ltx_align_center">✗</td>
<td id="A2.T4.4.3.3.15" class="ltx_td ltx_align_center">✓</td>
</tr>
<tr id="A2.T4.4.4.4" class="ltx_tr">
<th id="A2.T4.4.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Original V-COCO</th>
<td id="A2.T4.4.4.4.2" class="ltx_td ltx_align_center ltx_border_t">28.23</td>
<td id="A2.T4.4.4.4.3" class="ltx_td ltx_align_center ltx_border_t">64.72</td>
<td id="A2.T4.4.4.4.4" class="ltx_td ltx_align_center ltx_border_t">30.61</td>
<td id="A2.T4.4.4.4.5" class="ltx_td ltx_align_center ltx_border_t">65.63</td>
<td id="A2.T4.4.4.4.6" class="ltx_td ltx_align_center ltx_border_t">64.27</td>
<td id="A2.T4.4.4.4.7" class="ltx_td ltx_align_center ltx_border_t">19.10</td>
<td id="A2.T4.4.4.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">72.89</td>
<td id="A2.T4.4.4.4.9" class="ltx_td ltx_align_center ltx_border_t">24.26</td>
<td id="A2.T4.4.4.4.10" class="ltx_td ltx_align_center ltx_border_t">62.54</td>
<td id="A2.T4.4.4.4.11" class="ltx_td ltx_align_center ltx_border_t">27.64</td>
<td id="A2.T4.4.4.4.12" class="ltx_td ltx_align_center ltx_border_t">63.41</td>
<td id="A2.T4.4.4.4.13" class="ltx_td ltx_align_center ltx_border_t">61.57</td>
<td id="A2.T4.4.4.4.14" class="ltx_td ltx_align_center ltx_border_t">18.43</td>
<td id="A2.T4.4.4.4.15" class="ltx_td ltx_align_center ltx_border_t">70.43</td>
</tr>
<tr id="A2.T4.4.5.5" class="ltx_tr">
<th id="A2.T4.4.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">(mAP in literature)</th>
<td id="A2.T4.4.5.5.2" class="ltx_td"></td>
<td id="A2.T4.4.5.5.3" class="ltx_td"></td>
<td id="A2.T4.4.5.5.4" class="ltx_td"></td>
<td id="A2.T4.4.5.5.5" class="ltx_td"></td>
<td id="A2.T4.4.5.5.6" class="ltx_td"></td>
<td id="A2.T4.4.5.5.7" class="ltx_td"></td>
<td id="A2.T4.4.5.5.8" class="ltx_td ltx_border_r"></td>
<td id="A2.T4.4.5.5.9" class="ltx_td"></td>
<td id="A2.T4.4.5.5.10" class="ltx_td ltx_align_center">(63.8)</td>
<td id="A2.T4.4.5.5.11" class="ltx_td"></td>
<td id="A2.T4.4.5.5.12" class="ltx_td ltx_align_center">(61.0)</td>
<td id="A2.T4.4.5.5.13" class="ltx_td ltx_align_center">(61.2)</td>
<td id="A2.T4.4.5.5.14" class="ltx_td"></td>
<td id="A2.T4.4.5.5.15" class="ltx_td ltx_align_center">(70.7)</td>
</tr>
<tr id="A2.T4.4.6.6" class="ltx_tr">
<th id="A2.T4.4.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">V-COCO-SG split1</th>
<td id="A2.T4.4.6.6.2" class="ltx_td ltx_align_center ltx_border_t">30.57</td>
<td id="A2.T4.4.6.6.3" class="ltx_td ltx_align_center ltx_border_t">65.79</td>
<td id="A2.T4.4.6.6.4" class="ltx_td ltx_align_center ltx_border_t">31.24</td>
<td id="A2.T4.4.6.6.5" class="ltx_td ltx_align_center ltx_border_t">67.25</td>
<td id="A2.T4.4.6.6.6" class="ltx_td ltx_align_center ltx_border_t">67.49</td>
<td id="A2.T4.4.6.6.7" class="ltx_td ltx_align_center ltx_border_t">23.51</td>
<td id="A2.T4.4.6.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">71.91</td>
<td id="A2.T4.4.6.6.9" class="ltx_td ltx_align_center ltx_border_t">0.31</td>
<td id="A2.T4.4.6.6.10" class="ltx_td ltx_align_center ltx_border_t">2.23</td>
<td id="A2.T4.4.6.6.11" class="ltx_td ltx_align_center ltx_border_t">1.12</td>
<td id="A2.T4.4.6.6.12" class="ltx_td ltx_align_center ltx_border_t">4.22</td>
<td id="A2.T4.4.6.6.13" class="ltx_td ltx_align_center ltx_border_t">4.23</td>
<td id="A2.T4.4.6.6.14" class="ltx_td ltx_align_center ltx_border_t">0.17</td>
<td id="A2.T4.4.6.6.15" class="ltx_td ltx_align_center ltx_border_t">6.87</td>
</tr>
<tr id="A2.T4.4.7.7" class="ltx_tr">
<th id="A2.T4.4.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">V-COCO-SG split2</th>
<td id="A2.T4.4.7.7.2" class="ltx_td ltx_align_center">31.53</td>
<td id="A2.T4.4.7.7.3" class="ltx_td ltx_align_center">67.28</td>
<td id="A2.T4.4.7.7.4" class="ltx_td ltx_align_center">32.53</td>
<td id="A2.T4.4.7.7.5" class="ltx_td ltx_align_center">68.43</td>
<td id="A2.T4.4.7.7.6" class="ltx_td ltx_align_center">69.28</td>
<td id="A2.T4.4.7.7.7" class="ltx_td ltx_align_center">20.04</td>
<td id="A2.T4.4.7.7.8" class="ltx_td ltx_align_center ltx_border_r">74.38</td>
<td id="A2.T4.4.7.7.9" class="ltx_td ltx_align_center">0.29</td>
<td id="A2.T4.4.7.7.10" class="ltx_td ltx_align_center">3.02</td>
<td id="A2.T4.4.7.7.11" class="ltx_td ltx_align_center">0.68</td>
<td id="A2.T4.4.7.7.12" class="ltx_td ltx_align_center">4.85</td>
<td id="A2.T4.4.7.7.13" class="ltx_td ltx_align_center">4.84</td>
<td id="A2.T4.4.7.7.14" class="ltx_td ltx_align_center">0.00</td>
<td id="A2.T4.4.7.7.15" class="ltx_td ltx_align_center">6.27</td>
</tr>
<tr id="A2.T4.4.8.8" class="ltx_tr">
<th id="A2.T4.4.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">V-COCO-SG split3</th>
<td id="A2.T4.4.8.8.2" class="ltx_td ltx_align_center">28.21</td>
<td id="A2.T4.4.8.8.3" class="ltx_td ltx_align_center">61.07</td>
<td id="A2.T4.4.8.8.4" class="ltx_td ltx_align_center">29.83</td>
<td id="A2.T4.4.8.8.5" class="ltx_td ltx_align_center">60.47</td>
<td id="A2.T4.4.8.8.6" class="ltx_td ltx_align_center">63.24</td>
<td id="A2.T4.4.8.8.7" class="ltx_td ltx_align_center">22.41</td>
<td id="A2.T4.4.8.8.8" class="ltx_td ltx_align_center ltx_border_r">73.43</td>
<td id="A2.T4.4.8.8.9" class="ltx_td ltx_align_center">0.38</td>
<td id="A2.T4.4.8.8.10" class="ltx_td ltx_align_center">2.13</td>
<td id="A2.T4.4.8.8.11" class="ltx_td ltx_align_center">0.39</td>
<td id="A2.T4.4.8.8.12" class="ltx_td ltx_align_center">2.96</td>
<td id="A2.T4.4.8.8.13" class="ltx_td ltx_align_center">3.99</td>
<td id="A2.T4.4.8.8.14" class="ltx_td ltx_align_center">0.00</td>
<td id="A2.T4.4.8.8.15" class="ltx_td ltx_align_center">6.91</td>
</tr>
<tr id="A2.T4.4.9.9" class="ltx_tr">
<th id="A2.T4.4.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">Average over SG splits</th>
<td id="A2.T4.4.9.9.2" class="ltx_td ltx_align_center ltx_border_bb">30.10</td>
<td id="A2.T4.4.9.9.3" class="ltx_td ltx_align_center ltx_border_bb">64.71</td>
<td id="A2.T4.4.9.9.4" class="ltx_td ltx_align_center ltx_border_bb">31.20</td>
<td id="A2.T4.4.9.9.5" class="ltx_td ltx_align_center ltx_border_bb">65.38</td>
<td id="A2.T4.4.9.9.6" class="ltx_td ltx_align_center ltx_border_bb">66.67</td>
<td id="A2.T4.4.9.9.7" class="ltx_td ltx_align_center ltx_border_bb">21.99</td>
<td id="A2.T4.4.9.9.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">73.24</td>
<td id="A2.T4.4.9.9.9" class="ltx_td ltx_align_center ltx_border_bb">0.33</td>
<td id="A2.T4.4.9.9.10" class="ltx_td ltx_align_center ltx_border_bb">2.46</td>
<td id="A2.T4.4.9.9.11" class="ltx_td ltx_align_center ltx_border_bb">0.73</td>
<td id="A2.T4.4.9.9.12" class="ltx_td ltx_align_center ltx_border_bb">4.08</td>
<td id="A2.T4.4.9.9.13" class="ltx_td ltx_align_center ltx_border_bb">4.35</td>
<td id="A2.T4.4.9.9.14" class="ltx_td ltx_align_center ltx_border_bb">0.06</td>
<td id="A2.T4.4.9.9.15" class="ltx_td ltx_align_center ltx_border_bb">6.68</td>
</tr>
</tbody>
</table>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2305.09947" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2305.09948" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2305.09948">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2305.09948" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2305.09949" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 07:13:04 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
