<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2008.10060] Multi-Person Full Body Pose Estimation</title><meta property="og:description" content="Multi-person pose estimation plays an important role in many fields. Although previous works have researched a lot on different parts of human pose estimation, full body pose estimation for multi-person still needs furâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Multi-Person Full Body Pose Estimation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Multi-Person Full Body Pose Estimation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2008.10060">

<!--Generated on Fri Mar  1 16:47:07 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Multi-Person Full Body Pose Estimation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Haoyi Zhu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Shanghai Jiao Tong University, China 
<br class="ltx_break">{zhuhaoyi, jiec_tech, jiangshaofei}@sjtu.edu.cn
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Cheng Jie
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Shanghai Jiao Tong University, China 
<br class="ltx_break">{zhuhaoyi, jiec_tech, jiangshaofei}@sjtu.edu.cn
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Shaofei Jiang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Shanghai Jiao Tong University, China 
<br class="ltx_break">{zhuhaoyi, jiec_tech, jiangshaofei}@sjtu.edu.cn
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Multi-person pose estimation plays an important role in many fields. Although previous works have researched a lot on different parts of human pose estimation, full body pose estimation for multi-person still needs further research. Our work has developed an integrated model through knowledge distillation which can estimate full body poses. Trained based on the AlphaPose system and MSCOCO2017 dataset, our model achieves 51.5 mAP on the validation dataset annotated manually by ourselves. Related resources are available at <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://esflfei.github.io/esflfei.gethub.io/website.html</span>.</p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold">Keywords:</span> Multi-Person Pose Estimation, Full Body Pose, Knowledge Distillation</p>
</div>
<section id="S1" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Multi-person pose estimation has become increasingly popular in computer vision field in recent years. It has many applications such as human-computer interaction, augmented reality, and sports analytics. It can also improve the performance of re-targeting, tracking, and action recognition.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Previous works on this topic mainly focus on pose estimation of human body or different parts of human, such as head pose estimation and hand pose estimation. However, little research has been conducted on full body pose estimation. OpenPose<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> is currently the only system that can estimate multi-person full body pose, which is bottom-up and have to use mutiple networks, and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> develops a single network on full body pose estimation based on it, which applies PAF network architecture and multi-task learning to get body part candidates and uses bipartite graph matching to reach the final full body pose.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Our work develops an integrated model to directly estimate multi-person full body pose through a single network based on AlphaPose<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, the state-of-the-art multi-person body pose estimation system. The insight of this paper is to train a multi-person full body pose estimation model through knowledge distillation. Our inspiration is from the teacher-student model. The body keypoints groundtruth can be obtained from the annotation of MSCOCO2017 dataset<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Based on them, we can get the predicted keypoints of face, hand and foot. We treat the predicted keypoints as pseudo labels and put all of them together with the body keypoints to get the full body pose label. Thus, we can use it to train a model that can estimate multi-person full body pose.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We train our model on the AlphaPose system and the MSCOCO2017 train dataset. We then annotate full body keypoints on MSCOCO2017 validation dataset by ourselves, where our model reaches a result of 51.5 mAP, 10.0 higher than the latest OpenPose model. Our model performs pretty well on foot and body. When faces or hands are too small or ocludded, the detection accuracy of them will decrease.</p>
</div>
</section>
<section id="S2" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">There are four parts of research related to our work, including hand pose estimation, face keypoint detection, foot keypoint detection, and body pose estimation.</p>
</div>
<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph ltx_indentfirst">
<h4 class="ltx_title ltx_title_paragraph">Hand Pose Estimation</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">Due to high cost and challenges in manual annotation of hand keypoint, there does not exist any large hand keypoint dataset. To overcome this problem, Simon et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> generated a labeled hand keypoint dataset by developing multiview bootstrapping and trained a single view hand keypoint detector.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph ltx_indentfirst">
<h4 class="ltx_title ltx_title_paragraph">Face Keypoint Detection</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">There are mainly two kinds of approaches to achieve face keypoint detection: regression-based methods and template fitting. Regression Methods rely on Convolutional Neural Networks and often use convolutional heatmap regression, while template fitting usually employ a series of regression functions to fit the original image by creating face templates.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph ltx_indentfirst">
<h4 class="ltx_title ltx_title_paragraph">Foot Keypoint Detection</h4>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">Cao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> developed the first foot dataset based on the COCO dataset. The first detector combining body and foot keypoint was also trained.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px4" class="ltx_paragraph ltx_indentfirst">
<h4 class="ltx_title ltx_title_paragraph">Body Pose Estimation</h4>

<div id="S2.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px4.p1.1" class="ltx_p">The early way to accomplish body pose estimation is to infer from both local observations and spatial dependencies of body parts. It is divided into two categories: tree-structured graphical-based models and non-tree models. With the development of CNN, the accuracy on body pose estimation grew rapidly and multi-person estimation became possible, mainly containing two ways: top-down and bottom-up.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Approach</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our aim is to acquire a pseudo label of 133 full body pose keypoints through knowledge distillation, including 17 body keypoints, 6 foot keypoints, 68 face keypoints and 42 hand keypoints (21 per hand), and use them as groundtruth when training. In this paper, we get the pseudo label based on the MSCOCO2017 train dataset. Since the 17 body keypoints have already been labeled in the dataset, we actually only have to obtain the rest and merge all of them together.</p>
</div>
<section id="S3.SS1" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data Annotation</h3>

<figure id="S3.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F1.1" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S3.F1.1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:138.8pt;">
<img src="/html/2008.10060/assets/footkp.jpg" id="S3.F1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="667" height="445" alt="Refer to caption">
</div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F1.2" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S3.F1.2.1" class="ltx_block ltx_minipage ltx_align_top" style="width:138.8pt;">
<img src="/html/2008.10060/assets/facekp.jpg" id="S3.F1.2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="667" height="143" alt="Refer to caption">
</div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F1.3" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S3.F1.3.1" class="ltx_block ltx_minipage ltx_align_top" style="width:138.8pt;">
<img src="/html/2008.10060/assets/handkp.jpg" id="S3.F1.3.1.g1" class="ltx_graphics ltx_img_landscape" width="667" height="143" alt="Refer to caption">
</div>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The visualization results of our data annotation. (a)(b)(c) are examples of the body and foot keypoints, face keypoints and hand keypoints respectively. </figcaption>
</figure>
<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph ltx_indentfirst">
<h4 class="ltx_title ltx_title_paragraph">Foot Keypoints</h4>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">Using the existing model based on AlphaPose which detects 17 body keypoints and 6 extended foot keypoints, we can directly obtain the foot keypoints and the visualized connection.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph ltx_indentfirst">
<h4 class="ltx_title ltx_title_paragraph">Face Keypoints</h4>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">Based on the regular architecture of PRNet, we first predict the position of face bounding box using the annotated body parts of nose, eyes and ears. The face bounding box is then cropped and fed into PRNet where the input 2D image is mapped to a corresponding colored UV texture map so that we can predict UV parameters by CNN to do face reconstruction<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Finally, we get the 68 detected face keypoints.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px3" class="ltx_paragraph ltx_indentfirst">
<h4 class="ltx_title ltx_title_paragraph">Hand Keypoints</h4>

<div id="S3.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px3.p1.1" class="ltx_p">In this section, we extract the hand detection block in OpenPose independently for the output of 21 keypoints per hand. We use annotated body parts to predict the hand box proposals. The proposals then go through a hand estimation model based on Multiview Bootstrapped Training<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, which generates geometrically corresponding hand keypoints annotations under an external supervision source of multiple views and uses these annotations to further improve the detector. Finally, the detected hand keypoints are added to the end of keypoints list.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S3.F2.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:232.2pt;">
<img src="/html/2008.10060/assets/full.jpg" id="S3.F2.1.g1" class="ltx_graphics ltx_img_landscape" width="667" height="442" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S3.F2.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:232.2pt;">
<img src="/html/2008.10060/assets/full2.jpg" id="S3.F2.2.g1" class="ltx_graphics ltx_img_landscape" width="667" height="445" alt="Refer to caption">
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The final result of full body pose annotation.</figcaption>
</figure>
<div id="S3.SS1.SSS0.Px3.p2" class="ltx_para">
<p id="S3.SS1.SSS0.Px3.p2.1" class="ltx_p">Now a multi-person full body keypoints annotation containing body, face, hand and foot keypoints is constructed. The merged json can then be utilized as the ground truth for the subsequent training. It can be directly thrown into a single mature pose estimation network (AlphaPose in this paper) to train a full body pose estimation model.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>System Framework</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Our work is based on the AlphaPose system which follows the RMPE framework<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. It is SPPE-based and follows the two-step framework which contains human detection and pose estimation. The whole framework mainly consists of three blocks: Symmetric Spatial Transformer Network (SSTN), Parametric Pose Non-Maximum-Suppression (NMS) and Pose-Guided Proposals Generator (PGPG). The SSTN attached to the SPPE is used for obtaining an accurate single person region from a rough bounding box. After detecting human proposals, SSTN transforms the proposals to make them more suitable for SPPE, and de-transforms them after SPPE. To improve this step, in the training process, there is an added parallel SPPE branch which back-propagates the center-located pose errors. A parametric pose NMS is then introduced to eliminate redundant pose estimations, which defines a novel pose distance metric with a data-driven optimizer. Finally, the PGPG is employed for data augmentation by learning the output distribution of human detection results.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Training</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Our model is trained using seven Nvidia GeForce RTX 2080 Ti graphics cards. Our batch size is set to 8 and the Adam optimizer is used. The input resolution is 256Ã—192 while the output heatmap resolution is set to 64Ã—48. The initial learning rate is set to 0.001 with a learning factor of 0.1. Our model is trained and iterated through 328 epochs to improve performance.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We have manually annotated the full body keypoints on validation dataset of MSCOCO2017<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, which includes 5000 images, to evaluate our model. We choose YOLO as our human detector, due to its high efficiency and accuracy. We apply AlphaPose to train our model. Figure 3 shows the visualization result of our model. We then run the OpenPose system, using both the model_25 (the common model of OpenPose<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>) plus face and hand flag and model_135 (the model trained by [5]), and compare its results with ours, which is shown in <span id="S4.p1.1.1" class="ltx_text ltx_font_bold">table 1</span>. We can see that our mAP reaches 51.5, 10.0 higher than <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and 13.3 higher than <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Among the three methods, our AP and AR under any condition are all the best.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.8" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.8.8" class="ltx_tr">
<th id="S4.T1.8.8.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Methods</th>
<th id="S4.T1.8.8.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.8.8.10.1" class="ltx_text ltx_font_bold">mAP</span></th>
<th id="S4.T1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP<sup id="S4.T1.1.1.1.1" class="ltx_sup"><span id="S4.T1.1.1.1.1.1" class="ltx_text ltx_font_italic">50</span></sup>
</th>
<th id="S4.T1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP<sup id="S4.T1.2.2.2.1" class="ltx_sup"><span id="S4.T1.2.2.2.1.1" class="ltx_text ltx_font_italic">75</span></sup>
</th>
<th id="S4.T1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP<sup id="S4.T1.3.3.3.1" class="ltx_sup"><span id="S4.T1.3.3.3.1.1" class="ltx_text ltx_font_italic">M</span></sup>
</th>
<th id="S4.T1.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">AP<sup id="S4.T1.4.4.4.1" class="ltx_sup"><span id="S4.T1.4.4.4.1.1" class="ltx_text ltx_font_italic">L</span></sup>
</th>
<th id="S4.T1.8.8.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.8.8.11.1" class="ltx_text ltx_font_bold">mAR</span></th>
<th id="S4.T1.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AR<sup id="S4.T1.5.5.5.1" class="ltx_sup"><span id="S4.T1.5.5.5.1.1" class="ltx_text ltx_font_italic">50</span></sup>
</th>
<th id="S4.T1.6.6.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AR<sup id="S4.T1.6.6.6.1" class="ltx_sup"><span id="S4.T1.6.6.6.1.1" class="ltx_text ltx_font_italic">75</span></sup>
</th>
<th id="S4.T1.7.7.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AR<sup id="S4.T1.7.7.7.1" class="ltx_sup"><span id="S4.T1.7.7.7.1.1" class="ltx_text ltx_font_italic">M</span></sup>
</th>
<th id="S4.T1.8.8.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AR<sup id="S4.T1.8.8.8.1" class="ltx_sup"><span id="S4.T1.8.8.8.1.1" class="ltx_text ltx_font_italic">L</span></sup>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.8.9.1" class="ltx_tr">
<th id="S4.T1.8.9.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">OpenPose<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</th>
<th id="S4.T1.8.9.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">38.2</th>
<td id="S4.T1.8.9.1.3" class="ltx_td ltx_align_center ltx_border_t">51.3</td>
<td id="S4.T1.8.9.1.4" class="ltx_td ltx_align_center ltx_border_t">32.5</td>
<td id="S4.T1.8.9.1.5" class="ltx_td ltx_align_center ltx_border_t">28.8</td>
<td id="S4.T1.8.9.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">46.6</td>
<td id="S4.T1.8.9.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">44.8</td>
<td id="S4.T1.8.9.1.8" class="ltx_td ltx_align_center ltx_border_t">58.7</td>
<td id="S4.T1.8.9.1.9" class="ltx_td ltx_align_center ltx_border_t">41.2</td>
<td id="S4.T1.8.9.1.10" class="ltx_td ltx_align_center ltx_border_t">29.0</td>
<td id="S4.T1.8.9.1.11" class="ltx_td ltx_align_center ltx_border_t">55.6</td>
</tr>
<tr id="S4.T1.8.10.2" class="ltx_tr">
<th id="S4.T1.8.10.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">single<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</th>
<th id="S4.T1.8.10.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">41.5</th>
<td id="S4.T1.8.10.2.3" class="ltx_td ltx_align_center">69.4</td>
<td id="S4.T1.8.10.2.4" class="ltx_td ltx_align_center">29.1</td>
<td id="S4.T1.8.10.2.5" class="ltx_td ltx_align_center">38.8</td>
<td id="S4.T1.8.10.2.6" class="ltx_td ltx_align_center ltx_border_r">36.4</td>
<td id="S4.T1.8.10.2.7" class="ltx_td ltx_align_center ltx_border_r">49.4</td>
<td id="S4.T1.8.10.2.8" class="ltx_td ltx_align_center">73.1</td>
<td id="S4.T1.8.10.2.9" class="ltx_td ltx_align_center">42.2</td>
<td id="S4.T1.8.10.2.10" class="ltx_td ltx_align_center">40.1</td>
<td id="S4.T1.8.10.2.11" class="ltx_td ltx_align_center">48.6</td>
</tr>
<tr id="S4.T1.8.11.3" class="ltx_tr">
<th id="S4.T1.8.11.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">ours</th>
<th id="S4.T1.8.11.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.8.11.3.2.1" class="ltx_text ltx_font_bold">51.5</span></th>
<td id="S4.T1.8.11.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.8.11.3.3.1" class="ltx_text ltx_font_bold">74.0</span></td>
<td id="S4.T1.8.11.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.8.11.3.4.1" class="ltx_text ltx_font_bold">46.5</span></td>
<td id="S4.T1.8.11.3.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.8.11.3.5.1" class="ltx_text ltx_font_bold">45.8</span></td>
<td id="S4.T1.8.11.3.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.8.11.3.6.1" class="ltx_text ltx_font_bold">46.9</span></td>
<td id="S4.T1.8.11.3.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.8.11.3.7.1" class="ltx_text ltx_font_bold">59.7</span></td>
<td id="S4.T1.8.11.3.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.8.11.3.8.1" class="ltx_text ltx_font_bold">77.1</span></td>
<td id="S4.T1.8.11.3.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.8.11.3.9.1" class="ltx_text ltx_font_bold">57.4</span></td>
<td id="S4.T1.8.11.3.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.8.11.3.10.1" class="ltx_text ltx_font_bold">53.2</span></td>
<td id="S4.T1.8.11.3.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.8.11.3.11.1" class="ltx_text ltx_font_bold">56.7</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Results of OpenPose and our method on the validation dataset.</figcaption>
</figure>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">We have found that the foot and body part perform the best. Even if the human instances are small or crowded, they still works well. The face part performs well most of time, but when faces are too small or partly hidden, the detection accuracy will decrease. The hand detection, however, performs the worst and is easy to fail or make mistakes when hands are too small or occluded.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S4.F3.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:232.2pt;">
<img src="/html/2008.10060/assets/res1.jpg" id="S4.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="667" height="445" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S4.F3.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:232.2pt;">
<img src="/html/2008.10060/assets/res2.jpg" id="S4.F3.2.g1" class="ltx_graphics ltx_img_landscape" width="667" height="454" alt="Refer to caption">
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Two of the visulization results of our work.</figcaption>
</figure>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">The failures mainly result from the pseudo label, because the PRNet itself perform not well on small and occluded faces and the accuracy of hand detection in Openpose is limited. In other words, there originally exists errors in the ground truth we used for training, which is certain to influence the final result. But anyway, our method has been proved to be correct and effective. In general, our contribution is that we have proposed a novel, better method of full body pose estimation, obtained the full body pseudo labels and trained a full body pose estimation model better than previous ones.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh.

</span>
<span class="ltx_bibblock">Openpose: realtime multi-person 2d pose estimation using part
affinity fields.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1812.08008</span>, 2018.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu.

</span>
<span class="ltx_bibblock">Rmpe: Regional multi-person pose estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision</span>, pages 2334â€“2343, 2017.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Yao Feng, Fan Wu, Xiaohu Shao, Yanfeng Wang, and XiÂ Zhou.

</span>
<span class="ltx_bibblock">Joint 3d face reconstruction and dense alignment with position map
regression network.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, pages 534â€“551, 2018.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr DollÃ¡r, and CÂ Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">European conference on computer vision</span>, pages 740â€“755.
Springer, 2014.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
GinesÂ Hidalgo Martinez, Yaadhav Raaj, Haroon Idrees, Donglai Xiang, Hanbyul
Joo, Tomas Simon, and Yaser Sheikh.

</span>
<span class="ltx_bibblock">Single-network whole-body pose estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">2019 IEEE/CVF International Conference on Computer Vision
(ICCV)</span>, pages 6981â€“6990. IEEE, 2019.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Tomas Simon, Hanbyul Joo, Iain Matthews, and Yaser Sheikh.

</span>
<span class="ltx_bibblock">Hand keypoint detection in single images using multiview
bootstrapping.

</span>
<span class="ltx_bibblock">In <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition</span>, pages 1145â€“1153, 2017.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2008.10058" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2008.10060" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2008.10060">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2008.10060" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2008.10061" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 16:47:07 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
