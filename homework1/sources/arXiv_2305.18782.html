<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2305.18782] VVC Extension Scheme for Object Detection Using Contrast Reduction</title><meta property="og:description" content="In recent years, video analysis using Artificial Intelligence (AI) has been widely used, due to the remarkable development of image recognition technology using deep learning.
In 2019, the Moving Picture Experts Group …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="VVC Extension Scheme for Object Detection Using Contrast Reduction">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="VVC Extension Scheme for Object Detection Using Contrast Reduction">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2305.18782">

<!--Generated on Thu Feb 29 03:45:39 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
video coding,  VVC,  object detection,  VCM
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">VVC Extension Scheme for Object Detection Using Contrast Reduction</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Takahiro Shindo, Taiju Watanabe, Kein Yamada and Hiroshi Watanabe
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_font_italic">Graduate School of Fundamental Science and Engineering, Waseda University,
<br class="ltx_break"></span>Tokyo, Japan
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">In recent years, video analysis using Artificial Intelligence (AI) has been widely used, due to the remarkable development of image recognition technology using deep learning.
In 2019, the Moving Picture Experts Group (MPEG) has started standardization of Video Coding for Machines (VCM) as a video coding technology for image recognition.
In the framework of VCM, both higher image recognition accuracy and video compression performance are required.
In this paper, we propose an extention scheme of video coding for object detection using Versatile Video Coding (VVC).
Unlike video for human vision, video used for object detection does not require a large image size or high contrast.
Since downsampling of the image can reduce the amount of information to be transmitted.
Due to the decrease in image contrast, entropy of the image becomes smaller.
Therefore, in our proposed scheme, the original image is reduced in size and contrast, then coded with VVC encoder to achieve high compression performance.
Then, the output image from the VVC decoder is restored to its original image size using the bicubic method.
Experimental results show that the proposed video coding scheme achieves better coding performance than regular VVC in terms of object detection accuracy.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>
video coding, VVC, object detection, VCM

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">With the spread of video analysis using AI, video compression methods are essential for these purposes.
The amount of video information required for image recognition is considered to be less than that of video for human vision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
Therefore, coding schemes for image recognition that exceed the coding performance of VVC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> are being considered.
In 2019, MPEG has positioned video coding for image recognition as VCM and started standardization.
In its standardization activities, most of the proposed coding schemes use VVC, aiming to improve coding performance by combining VVC with video pre-processing and post-processing.
In video pre-processing, some methods have been proposed to reduce the amount of information in videos.
For example, there are pre-processing methods that convert the image size, reduce the number of frames, and use Region of Interest.
Post-processing of video includes methods such as restoring image size and converting to frames that are useful for image recognition.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, coding scheme using image contrast reduction have not been considered, even though a lower bitrate can be achieved.
Therefore, this paper investigates the effect of reducing the image contrast on object detection accuracy.
For the object detection model, YOLO-v7 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> is used, which has both high object detection accuracy and fast object detection speed.
Experimental results show that VVC extention scheme using contrast reduction reduces the bitrate required for video transmission and improves coding performance in object detection accuracy.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Works</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">VVC is the latest video coding technology standardized in July 2020.
This video coding method was created to encode video for human vision.
However, due to its high coding performance, it has also attracted attention in VCM standardization activities.
In fact, most of the proposed video coding methods in the standardization activities partially use VVC to achieve high coding performance.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">VVC reduces the amount of video information by performing intra-frame and inter-frame predictions.
According to these prediction results, only the prediction error need to be transmitted.
After these errors are block segmented and converted into frequency components, the information mainly in the high-frequency components is removed.
This is because the reduction of high-frequency components does not have a significant impact on image quality.
The resulting video information from these processes is compressed to a minimal amount by entropy coding for transmission.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Proposed Method</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We propose a method to lower the amount of bits required for video transmission by reducing the contrast of the image.
Videos for object detection models need not be as vividly colored as videos for human vision.
By lowering the contrast of video frames, the entropy of them can be suppressed.
In other words, the bitrate required for video transmission can be reduced.
Furthermore, to decrease the amount of information in the video, the image size is halved, and the image size is restored after the video is transmitted.
This image resizing method is described in the document published by MPEG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, and is utilized to create anchors of VCM.
The bicubic method is used as the image resizing method, and VVC is used as the video coding method.
The proposed video processing method is shown in Fig. 1-(a), and the algorithm used for the contrast reduction process is described below.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2305.18782/assets/proc.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="287" height="255" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Processing flow of videos. (a) proposed coding scheme using contrast reduction; (b) coding scheme for comparison.</figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.7" class="ltx_Math" alttext="I^{\prime}(x,y)=(1-\alpha){*}I(x,y)+\frac{\alpha}{3{*}X{*}Y}\sum_{x=1}^{X}\!\sum_{y=1}^{Y}I(x,y)," display="block"><semantics id="S3.E1.m1.7a"><mrow id="S3.E1.m1.7.7.1" xref="S3.E1.m1.7.7.1.1.cmml"><mrow id="S3.E1.m1.7.7.1.1" xref="S3.E1.m1.7.7.1.1.cmml"><mrow id="S3.E1.m1.7.7.1.1.3" xref="S3.E1.m1.7.7.1.1.3.cmml"><msup id="S3.E1.m1.7.7.1.1.3.2" xref="S3.E1.m1.7.7.1.1.3.2.cmml"><mi id="S3.E1.m1.7.7.1.1.3.2.2" xref="S3.E1.m1.7.7.1.1.3.2.2.cmml">I</mi><mo id="S3.E1.m1.7.7.1.1.3.2.3" xref="S3.E1.m1.7.7.1.1.3.2.3.cmml">′</mo></msup><mo lspace="0em" rspace="0em" id="S3.E1.m1.7.7.1.1.3.1" xref="S3.E1.m1.7.7.1.1.3.1.cmml">​</mo><mrow id="S3.E1.m1.7.7.1.1.3.3.2" xref="S3.E1.m1.7.7.1.1.3.3.1.cmml"><mo stretchy="false" id="S3.E1.m1.7.7.1.1.3.3.2.1" xref="S3.E1.m1.7.7.1.1.3.3.1.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">x</mi><mo id="S3.E1.m1.7.7.1.1.3.3.2.2" xref="S3.E1.m1.7.7.1.1.3.3.1.cmml">,</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">y</mi><mo stretchy="false" id="S3.E1.m1.7.7.1.1.3.3.2.3" xref="S3.E1.m1.7.7.1.1.3.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.7.7.1.1.2" xref="S3.E1.m1.7.7.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.7.7.1.1.1" xref="S3.E1.m1.7.7.1.1.1.cmml"><mrow id="S3.E1.m1.7.7.1.1.1.1" xref="S3.E1.m1.7.7.1.1.1.1.cmml"><mrow id="S3.E1.m1.7.7.1.1.1.1.1" xref="S3.E1.m1.7.7.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.7.7.1.1.1.1.1.1.1" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.7.7.1.1.1.1.1.1.1.2" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml"><mn id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.cmml">−</mo><mi id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.3.cmml">α</mi></mrow><mo rspace="0.055em" stretchy="false" id="S3.E1.m1.7.7.1.1.1.1.1.1.1.3" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.E1.m1.7.7.1.1.1.1.1.2" xref="S3.E1.m1.7.7.1.1.1.1.1.2.cmml">∗</mo><mi id="S3.E1.m1.7.7.1.1.1.1.1.3" xref="S3.E1.m1.7.7.1.1.1.1.1.3.cmml">I</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.7.7.1.1.1.1.2" xref="S3.E1.m1.7.7.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.7.7.1.1.1.1.3.2" xref="S3.E1.m1.7.7.1.1.1.1.3.1.cmml"><mo stretchy="false" id="S3.E1.m1.7.7.1.1.1.1.3.2.1" xref="S3.E1.m1.7.7.1.1.1.1.3.1.cmml">(</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">x</mi><mo id="S3.E1.m1.7.7.1.1.1.1.3.2.2" xref="S3.E1.m1.7.7.1.1.1.1.3.1.cmml">,</mo><mi id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">y</mi><mo stretchy="false" id="S3.E1.m1.7.7.1.1.1.1.3.2.3" xref="S3.E1.m1.7.7.1.1.1.1.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.7.7.1.1.1.2" xref="S3.E1.m1.7.7.1.1.1.2.cmml">+</mo><mrow id="S3.E1.m1.7.7.1.1.1.3" xref="S3.E1.m1.7.7.1.1.1.3.cmml"><mfrac id="S3.E1.m1.7.7.1.1.1.3.2" xref="S3.E1.m1.7.7.1.1.1.3.2.cmml"><mi id="S3.E1.m1.7.7.1.1.1.3.2.2" xref="S3.E1.m1.7.7.1.1.1.3.2.2.cmml">α</mi><mrow id="S3.E1.m1.7.7.1.1.1.3.2.3" xref="S3.E1.m1.7.7.1.1.1.3.2.3.cmml"><mn id="S3.E1.m1.7.7.1.1.1.3.2.3.2" xref="S3.E1.m1.7.7.1.1.1.3.2.3.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.7.7.1.1.1.3.2.3.1" xref="S3.E1.m1.7.7.1.1.1.3.2.3.1.cmml">∗</mo><mi id="S3.E1.m1.7.7.1.1.1.3.2.3.3" xref="S3.E1.m1.7.7.1.1.1.3.2.3.3.cmml">X</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.7.7.1.1.1.3.2.3.1a" xref="S3.E1.m1.7.7.1.1.1.3.2.3.1.cmml">∗</mo><mi id="S3.E1.m1.7.7.1.1.1.3.2.3.4" xref="S3.E1.m1.7.7.1.1.1.3.2.3.4.cmml">Y</mi></mrow></mfrac><mo lspace="0em" rspace="0em" id="S3.E1.m1.7.7.1.1.1.3.1" xref="S3.E1.m1.7.7.1.1.1.3.1.cmml">​</mo><mrow id="S3.E1.m1.7.7.1.1.1.3.3" xref="S3.E1.m1.7.7.1.1.1.3.3.cmml"><munderover id="S3.E1.m1.7.7.1.1.1.3.3.1" xref="S3.E1.m1.7.7.1.1.1.3.3.1.cmml"><mo movablelimits="false" rspace="0em" id="S3.E1.m1.7.7.1.1.1.3.3.1.2.2" xref="S3.E1.m1.7.7.1.1.1.3.3.1.2.2.cmml">∑</mo><mrow id="S3.E1.m1.7.7.1.1.1.3.3.1.2.3" xref="S3.E1.m1.7.7.1.1.1.3.3.1.2.3.cmml"><mi id="S3.E1.m1.7.7.1.1.1.3.3.1.2.3.2" xref="S3.E1.m1.7.7.1.1.1.3.3.1.2.3.2.cmml">x</mi><mo id="S3.E1.m1.7.7.1.1.1.3.3.1.2.3.1" xref="S3.E1.m1.7.7.1.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="S3.E1.m1.7.7.1.1.1.3.3.1.2.3.3" xref="S3.E1.m1.7.7.1.1.1.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.7.7.1.1.1.3.3.1.3" xref="S3.E1.m1.7.7.1.1.1.3.3.1.3.cmml">X</mi></munderover><mrow id="S3.E1.m1.7.7.1.1.1.3.3.2" xref="S3.E1.m1.7.7.1.1.1.3.3.2.cmml"><munderover id="S3.E1.m1.7.7.1.1.1.3.3.2.1" xref="S3.E1.m1.7.7.1.1.1.3.3.2.1.cmml"><mo movablelimits="false" id="S3.E1.m1.7.7.1.1.1.3.3.2.1.2.2" xref="S3.E1.m1.7.7.1.1.1.3.3.2.1.2.2.cmml">∑</mo><mrow id="S3.E1.m1.7.7.1.1.1.3.3.2.1.2.3" xref="S3.E1.m1.7.7.1.1.1.3.3.2.1.2.3.cmml"><mi id="S3.E1.m1.7.7.1.1.1.3.3.2.1.2.3.2" xref="S3.E1.m1.7.7.1.1.1.3.3.2.1.2.3.2.cmml">y</mi><mo id="S3.E1.m1.7.7.1.1.1.3.3.2.1.2.3.1" xref="S3.E1.m1.7.7.1.1.1.3.3.2.1.2.3.1.cmml">=</mo><mn id="S3.E1.m1.7.7.1.1.1.3.3.2.1.2.3.3" xref="S3.E1.m1.7.7.1.1.1.3.3.2.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.7.7.1.1.1.3.3.2.1.3" xref="S3.E1.m1.7.7.1.1.1.3.3.2.1.3.cmml">Y</mi></munderover><mrow id="S3.E1.m1.7.7.1.1.1.3.3.2.2" xref="S3.E1.m1.7.7.1.1.1.3.3.2.2.cmml"><mi id="S3.E1.m1.7.7.1.1.1.3.3.2.2.2" xref="S3.E1.m1.7.7.1.1.1.3.3.2.2.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.7.7.1.1.1.3.3.2.2.1" xref="S3.E1.m1.7.7.1.1.1.3.3.2.2.1.cmml">​</mo><mrow id="S3.E1.m1.7.7.1.1.1.3.3.2.2.3.2" xref="S3.E1.m1.7.7.1.1.1.3.3.2.2.3.1.cmml"><mo stretchy="false" id="S3.E1.m1.7.7.1.1.1.3.3.2.2.3.2.1" xref="S3.E1.m1.7.7.1.1.1.3.3.2.2.3.1.cmml">(</mo><mi id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml">x</mi><mo id="S3.E1.m1.7.7.1.1.1.3.3.2.2.3.2.2" xref="S3.E1.m1.7.7.1.1.1.3.3.2.2.3.1.cmml">,</mo><mi id="S3.E1.m1.6.6" xref="S3.E1.m1.6.6.cmml">y</mi><mo stretchy="false" id="S3.E1.m1.7.7.1.1.1.3.3.2.2.3.2.3" xref="S3.E1.m1.7.7.1.1.1.3.3.2.2.3.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.7.7.1.2" xref="S3.E1.m1.7.7.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.7b"><apply id="S3.E1.m1.7.7.1.1.cmml" xref="S3.E1.m1.7.7.1"><eq id="S3.E1.m1.7.7.1.1.2.cmml" xref="S3.E1.m1.7.7.1.1.2"></eq><apply id="S3.E1.m1.7.7.1.1.3.cmml" xref="S3.E1.m1.7.7.1.1.3"><times id="S3.E1.m1.7.7.1.1.3.1.cmml" xref="S3.E1.m1.7.7.1.1.3.1"></times><apply id="S3.E1.m1.7.7.1.1.3.2.cmml" xref="S3.E1.m1.7.7.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.3.2.1.cmml" xref="S3.E1.m1.7.7.1.1.3.2">superscript</csymbol><ci id="S3.E1.m1.7.7.1.1.3.2.2.cmml" xref="S3.E1.m1.7.7.1.1.3.2.2">𝐼</ci><ci id="S3.E1.m1.7.7.1.1.3.2.3.cmml" xref="S3.E1.m1.7.7.1.1.3.2.3">′</ci></apply><interval closure="open" id="S3.E1.m1.7.7.1.1.3.3.1.cmml" xref="S3.E1.m1.7.7.1.1.3.3.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑥</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝑦</ci></interval></apply><apply id="S3.E1.m1.7.7.1.1.1.cmml" xref="S3.E1.m1.7.7.1.1.1"><plus id="S3.E1.m1.7.7.1.1.1.2.cmml" xref="S3.E1.m1.7.7.1.1.1.2"></plus><apply id="S3.E1.m1.7.7.1.1.1.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1"><times id="S3.E1.m1.7.7.1.1.1.1.2.cmml" xref="S3.E1.m1.7.7.1.1.1.1.2"></times><apply id="S3.E1.m1.7.7.1.1.1.1.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1"><times id="S3.E1.m1.7.7.1.1.1.1.1.2.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.2"></times><apply id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1"><minus id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.2">1</cn><ci id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.3">𝛼</ci></apply><ci id="S3.E1.m1.7.7.1.1.1.1.1.3.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.3">𝐼</ci></apply><interval closure="open" id="S3.E1.m1.7.7.1.1.1.1.3.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.3.2"><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">𝑥</ci><ci id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">𝑦</ci></interval></apply><apply id="S3.E1.m1.7.7.1.1.1.3.cmml" xref="S3.E1.m1.7.7.1.1.1.3"><times id="S3.E1.m1.7.7.1.1.1.3.1.cmml" xref="S3.E1.m1.7.7.1.1.1.3.1"></times><apply id="S3.E1.m1.7.7.1.1.1.3.2.cmml" xref="S3.E1.m1.7.7.1.1.1.3.2"><divide id="S3.E1.m1.7.7.1.1.1.3.2.1.cmml" xref="S3.E1.m1.7.7.1.1.1.3.2"></divide><ci id="S3.E1.m1.7.7.1.1.1.3.2.2.cmml" xref="S3.E1.m1.7.7.1.1.1.3.2.2">𝛼</ci><apply id="S3.E1.m1.7.7.1.1.1.3.2.3.cmml" xref="S3.E1.m1.7.7.1.1.1.3.2.3"><times id="S3.E1.m1.7.7.1.1.1.3.2.3.1.cmml" xref="S3.E1.m1.7.7.1.1.1.3.2.3.1"></times><cn type="integer" id="S3.E1.m1.7.7.1.1.1.3.2.3.2.cmml" xref="S3.E1.m1.7.7.1.1.1.3.2.3.2">3</cn><ci id="S3.E1.m1.7.7.1.1.1.3.2.3.3.cmml" xref="S3.E1.m1.7.7.1.1.1.3.2.3.3">𝑋</ci><ci id="S3.E1.m1.7.7.1.1.1.3.2.3.4.cmml" xref="S3.E1.m1.7.7.1.1.1.3.2.3.4">𝑌</ci></apply></apply><apply id="S3.E1.m1.7.7.1.1.1.3.3.cmml" xref="S3.E1.m1.7.7.1.1.1.3.3"><apply id="S3.E1.m1.7.7.1.1.1.3.3.1.cmml" xref="S3.E1.m1.7.7.1.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.1.3.3.1.1.cmml" xref="S3.E1.m1.7.7.1.1.1.3.3.1">superscript</csymbol><apply id="S3.E1.m1.7.7.1.1.1.3.3.1.2.cmml" xref="S3.E1.m1.7.7.1.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.1.3.3.1.2.1.cmml" xref="S3.E1.m1.7.7.1.1.1.3.3.1">subscript</csymbol><sum id="S3.E1.m1.7.7.1.1.1.3.3.1.2.2.cmml" xref="S3.E1.m1.7.7.1.1.1.3.3.1.2.2"></sum><apply id="S3.E1.m1.7.7.1.1.1.3.3.1.2.3.cmml" xref="S3.E1.m1.7.7.1.1.1.3.3.1.2.3"><eq id="S3.E1.m1.7.7.1.1.1.3.3.1.2.3.1.cmml" xref="S3.E1.m1.7.7.1.1.1.3.3.1.2.3.1"></eq><ci id="S3.E1.m1.7.7.1.1.1.3.3.1.2.3.2.cmml" xref="S3.E1.m1.7.7.1.1.1.3.3.1.2.3.2">𝑥</ci><cn type="integer" id="S3.E1.m1.7.7.1.1.1.3.3.1.2.3.3.cmml" xref="S3.E1.m1.7.7.1.1.1.3.3.1.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.7.7.1.1.1.3.3.1.3.cmml" xref="S3.E1.m1.7.7.1.1.1.3.3.1.3">𝑋</ci></apply><apply id="S3.E1.m1.7.7.1.1.1.3.3.2.cmml" xref="S3.E1.m1.7.7.1.1.1.3.3.2"><apply id="S3.E1.m1.7.7.1.1.1.3.3.2.1.cmml" xref="S3.E1.m1.7.7.1.1.1.3.3.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.1.3.3.2.1.1.cmml" xref="S3.E1.m1.7.7.1.1.1.3.3.2.1">superscript</csymbol><apply id="S3.E1.m1.7.7.1.1.1.3.3.2.1.2.cmml" xref="S3.E1.m1.7.7.1.1.1.3.3.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.1.3.3.2.1.2.1.cmml" xref="S3.E1.m1.7.7.1.1.1.3.3.2.1">subscript</csymbol><sum id="S3.E1.m1.7.7.1.1.1.3.3.2.1.2.2.cmml" xref="S3.E1.m1.7.7.1.1.1.3.3.2.1.2.2"></sum><apply id="S3.E1.m1.7.7.1.1.1.3.3.2.1.2.3.cmml" xref="S3.E1.m1.7.7.1.1.1.3.3.2.1.2.3"><eq id="S3.E1.m1.7.7.1.1.1.3.3.2.1.2.3.1.cmml" xref="S3.E1.m1.7.7.1.1.1.3.3.2.1.2.3.1"></eq><ci id="S3.E1.m1.7.7.1.1.1.3.3.2.1.2.3.2.cmml" xref="S3.E1.m1.7.7.1.1.1.3.3.2.1.2.3.2">𝑦</ci><cn type="integer" id="S3.E1.m1.7.7.1.1.1.3.3.2.1.2.3.3.cmml" xref="S3.E1.m1.7.7.1.1.1.3.3.2.1.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.7.7.1.1.1.3.3.2.1.3.cmml" xref="S3.E1.m1.7.7.1.1.1.3.3.2.1.3">𝑌</ci></apply><apply id="S3.E1.m1.7.7.1.1.1.3.3.2.2.cmml" xref="S3.E1.m1.7.7.1.1.1.3.3.2.2"><times id="S3.E1.m1.7.7.1.1.1.3.3.2.2.1.cmml" xref="S3.E1.m1.7.7.1.1.1.3.3.2.2.1"></times><ci id="S3.E1.m1.7.7.1.1.1.3.3.2.2.2.cmml" xref="S3.E1.m1.7.7.1.1.1.3.3.2.2.2">𝐼</ci><interval closure="open" id="S3.E1.m1.7.7.1.1.1.3.3.2.2.3.1.cmml" xref="S3.E1.m1.7.7.1.1.1.3.3.2.2.3.2"><ci id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.5.5">𝑥</ci><ci id="S3.E1.m1.6.6.cmml" xref="S3.E1.m1.6.6">𝑦</ci></interval></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.7c">I^{\prime}(x,y)=(1-\alpha){*}I(x,y)+\frac{\alpha}{3{*}X{*}Y}\sum_{x=1}^{X}\!\sum_{y=1}^{Y}I(x,y),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.p2.5" class="ltx_p">where <math id="S3.p2.1.m1.2" class="ltx_Math" alttext="I(x,y)" display="inline"><semantics id="S3.p2.1.m1.2a"><mrow id="S3.p2.1.m1.2.3" xref="S3.p2.1.m1.2.3.cmml"><mi id="S3.p2.1.m1.2.3.2" xref="S3.p2.1.m1.2.3.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.p2.1.m1.2.3.1" xref="S3.p2.1.m1.2.3.1.cmml">​</mo><mrow id="S3.p2.1.m1.2.3.3.2" xref="S3.p2.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.p2.1.m1.2.3.3.2.1" xref="S3.p2.1.m1.2.3.3.1.cmml">(</mo><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">x</mi><mo id="S3.p2.1.m1.2.3.3.2.2" xref="S3.p2.1.m1.2.3.3.1.cmml">,</mo><mi id="S3.p2.1.m1.2.2" xref="S3.p2.1.m1.2.2.cmml">y</mi><mo stretchy="false" id="S3.p2.1.m1.2.3.3.2.3" xref="S3.p2.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.2b"><apply id="S3.p2.1.m1.2.3.cmml" xref="S3.p2.1.m1.2.3"><times id="S3.p2.1.m1.2.3.1.cmml" xref="S3.p2.1.m1.2.3.1"></times><ci id="S3.p2.1.m1.2.3.2.cmml" xref="S3.p2.1.m1.2.3.2">𝐼</ci><interval closure="open" id="S3.p2.1.m1.2.3.3.1.cmml" xref="S3.p2.1.m1.2.3.3.2"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">𝑥</ci><ci id="S3.p2.1.m1.2.2.cmml" xref="S3.p2.1.m1.2.2">𝑦</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.2c">I(x,y)</annotation></semantics></math> and <math id="S3.p2.2.m2.2" class="ltx_Math" alttext="I^{\prime}(x,y)" display="inline"><semantics id="S3.p2.2.m2.2a"><mrow id="S3.p2.2.m2.2.3" xref="S3.p2.2.m2.2.3.cmml"><msup id="S3.p2.2.m2.2.3.2" xref="S3.p2.2.m2.2.3.2.cmml"><mi id="S3.p2.2.m2.2.3.2.2" xref="S3.p2.2.m2.2.3.2.2.cmml">I</mi><mo id="S3.p2.2.m2.2.3.2.3" xref="S3.p2.2.m2.2.3.2.3.cmml">′</mo></msup><mo lspace="0em" rspace="0em" id="S3.p2.2.m2.2.3.1" xref="S3.p2.2.m2.2.3.1.cmml">​</mo><mrow id="S3.p2.2.m2.2.3.3.2" xref="S3.p2.2.m2.2.3.3.1.cmml"><mo stretchy="false" id="S3.p2.2.m2.2.3.3.2.1" xref="S3.p2.2.m2.2.3.3.1.cmml">(</mo><mi id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">x</mi><mo id="S3.p2.2.m2.2.3.3.2.2" xref="S3.p2.2.m2.2.3.3.1.cmml">,</mo><mi id="S3.p2.2.m2.2.2" xref="S3.p2.2.m2.2.2.cmml">y</mi><mo stretchy="false" id="S3.p2.2.m2.2.3.3.2.3" xref="S3.p2.2.m2.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.2b"><apply id="S3.p2.2.m2.2.3.cmml" xref="S3.p2.2.m2.2.3"><times id="S3.p2.2.m2.2.3.1.cmml" xref="S3.p2.2.m2.2.3.1"></times><apply id="S3.p2.2.m2.2.3.2.cmml" xref="S3.p2.2.m2.2.3.2"><csymbol cd="ambiguous" id="S3.p2.2.m2.2.3.2.1.cmml" xref="S3.p2.2.m2.2.3.2">superscript</csymbol><ci id="S3.p2.2.m2.2.3.2.2.cmml" xref="S3.p2.2.m2.2.3.2.2">𝐼</ci><ci id="S3.p2.2.m2.2.3.2.3.cmml" xref="S3.p2.2.m2.2.3.2.3">′</ci></apply><interval closure="open" id="S3.p2.2.m2.2.3.3.1.cmml" xref="S3.p2.2.m2.2.3.3.2"><ci id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">𝑥</ci><ci id="S3.p2.2.m2.2.2.cmml" xref="S3.p2.2.m2.2.2">𝑦</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.2c">I^{\prime}(x,y)</annotation></semantics></math> indicate the input and output image.
<math id="S3.p2.3.m3.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.p2.3.m3.1a"><mi id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><ci id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">X</annotation></semantics></math> and <math id="S3.p2.4.m4.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S3.p2.4.m4.1a"><mi id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><ci id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">Y</annotation></semantics></math> are the width and height of the image.
<math id="S3.p2.5.m5.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.p2.5.m5.1a"><mi id="S3.p2.5.m5.1.1" xref="S3.p2.5.m5.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.p2.5.m5.1b"><ci id="S3.p2.5.m5.1.1.cmml" xref="S3.p2.5.m5.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m5.1c">\alpha</annotation></semantics></math> represents the ratio of the number of tones to be reduced.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiment</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Evaluation method</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To measure coding performance in object detection accuracy, we use the SFU-HW-v1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> dataset.
This dataset consists of raw videos and the corresponding annotations for object detection, and is designated as Common Test Condition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> in VCM standardization activities.
The dataset contains 18 raw videos, which are classified into classes A to E according to its image size and characteristics.
In this experiment, a total of 8 sequences from classes B and C are used.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">We reduce the contrast of these videos using Eq. 1. In this experiment, the value of <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mi id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><ci id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">\alpha</annotation></semantics></math> is set to 0.25.
To demonstrate the effectiveness of the proposed video coding scheme, we compare its performance with that of regular VVC.
The processing of comparison method is shown in Fig. 1-(b).
In order to accurately compare the encoding performance, the image size is changed as referred to in the proposed method.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">VTM17.2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> is used as the video coding method and “lowdelay_P” is used as the frame reference structure.
The quantization parameters in the proposed coding method are 14 integers from 32 to 45.
In the coding method for comparison, the parameters are 13 integers from 35 to 47.
VVC coded video is input to YOLO-v7 to perform object detection.
The confidence threshold of the detection model is set to 0.25.
Average Precision (AP) is used for detection accuracy, and IoU threshold is always set to 0.5 when calculating AP.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Results</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The relationship between bitrate and mAP is shown in Fig. 2, and the relationship between bitrate and AP, which represents the detection accuracy of “person”, is shown in Fig. 3.
In both figures, the blue curve represents the coding performance of the proposed method and the red curve represents that of the comparison method.
By reducing the contrast of the image, it is possible to encode the image with smaller quantization parameters.
This leads to decoding images with less contrast but less block noise, which improves the coding performance in terms of object detection accuracy.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2305.18782/assets/g0.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="124" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The relationship between bitrate and mAP.</figcaption>
</figure>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2305.18782/assets/g1.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="124" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The relationship between bitrate and AP (person).</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we proposed a coding scheme to reduce bitrate without lowering object detection accuracy by reducing image contrast before transmitting the video.
Experimental results showed that the proposed method outperforms a general VVC in terms of coding performance of object detection accuracy.
For future research, it is necessary to measure the coding performance in terms of object detection accuracy when object detection models are trained using low contrast images.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">These research results were obtained from the commissioned research (No.05101) by National Institute of Information and Communications Technology (NICT), Japan.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"> H. Choi <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Scalable Image Coding for Humans and Machines,” IEEE Transaction on Image Processing, vol. 31, 2022, pp. 2739-2754.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"> Versatile Video Coding, Standard ISO/IEC 23090-3, ISO/IEC JTC 1, Jul. 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"> ISO/IEC JTC1/SC29/WG2, “CfP response report for Video Coding for Machine,” N248, Oct. 2022.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"> Chien-Yao Wang <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">et al.</span>, “YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for realtime object detectors,” arXiv preprint arXiv:2207.02696, 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"> ISO/IEC JTC1/SC29/WG04, “Common test conditions for video coding for machines,” N278, Nov. 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"> H. Choi <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">et al.</span>, “A dataset of labelled objects on raw video sequences,” Data in Brief, 34:106701, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"> VTM software repository, version VTM-17.2. Available online: https://vcgit.hhi.fraunhofer.de/jvet/VVCSoftware_VTM.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2305.18781" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2305.18782" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2305.18782">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2305.18782" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2305.18783" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 03:45:39 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
