<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2212.04613] Contrastive View Design Strategies to Enhance Robustness to Domain Shifts in Downstream Object Detection</title><meta property="og:description" content="Contrastive learning has emerged as a competitive pretraining method for object detection. Despite this progress, there has been minimal investigation into the robustness of contrastively pretrained detectors when face…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Contrastive View Design Strategies to Enhance Robustness to Domain Shifts in Downstream Object Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Contrastive View Design Strategies to Enhance Robustness to Domain Shifts in Downstream Object Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2212.04613">

<!--Generated on Fri Mar  1 11:54:19 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Contrastive View Design Strategies to Enhance Robustness to Domain Shifts in Downstream Object Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Kyle Buettner<sup id="id1.1.id1" class="ltx_sup">1</sup>,
Adriana Kovashka<sup id="id2.2.id2" class="ltx_sup">2</sup>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">Contrastive learning has emerged as a competitive pretraining method for object detection. Despite this progress, there has been minimal investigation into the robustness of contrastively pretrained detectors when faced with domain shifts. To address this gap, we conduct an empirical study of contrastive learning and out-of-domain object detection, studying how contrastive view design affects robustness. In particular, we perform a case study of the detection-focused pretext task Instance Localization (InsLoc) and propose strategies to augment views and enhance robustness in appearance-shifted and context-shifted scenarios. Amongst these strategies, we propose changes to cropping such as altering the percentage used, adding IoU constraints, and integrating saliency-based object priors. We also explore the addition of shortcut-reducing augmentations such as Poisson blending, texture flattening, and elastic deformation. We benchmark these strategies on abstract, weather, and context domain shifts and illustrate robust ways to combine them, in both pretraining on single-object and multi-object image datasets. Overall, our results and insights show how to ensure robustness through the choice of views in contrastive learning.</p>
</div>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Introduction</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">Self-supervised learning has been rising in popularity in computer vision, with many top methods using contrastive learning <cite class="ltx_cite ltx_citemacro_citep">(Hadsell, Chopra, and LeCun <a href="#bib.bib14" title="" class="ltx_ref">2006</a>)</cite>, a form of learning that optimizes feature representations for positive samples to be close together and for negative samples to be far apart. Self-supervised contrastive models such as SimCLR <cite class="ltx_cite ltx_citemacro_citep">(Chen et al. <a href="#bib.bib4" title="" class="ltx_ref">2020a</a>)</cite> and MoCo <cite class="ltx_cite ltx_citemacro_citep">(He et al. <a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite> have been shown to approach or surpass the performance of supervised models when representations are transferred to downstream image classification, object detection, and semantic segmentation tasks <cite class="ltx_cite ltx_citemacro_citep">(Ericsson, Gouk, and Hospedales <a href="#bib.bib8" title="" class="ltx_ref">2021</a>)</cite>. This success is in part due to strategic data augmentation pipelines that these models use to create effective positive and negative <span id="Sx1.p1.1.1" class="ltx_text ltx_font_italic">views</span> (samples) for learning.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p">Despite this progress, the out-of-distribution robustness of contrastive representations has been minimally studied, especially with regards to object detection. We hypothesize that existing data augmentation pipelines in contrastive learning may result in representations that <span id="Sx1.p2.1.1" class="ltx_text ltx_font_italic">lack</span> robustness in such domain-shifted detection scenarios. For example, as shown in Fig. <a href="#Sx1.F1" title="Figure 1 ‣ Introduction ‣ Contrastive View Design Strategies to Enhance Robustness to Domain Shifts in Downstream Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, state-of-the-art pipelines (<span id="Sx1.p2.1.2" class="ltx_text ltx_font_italic">e</span>.<span id="Sx1.p2.1.3" class="ltx_text ltx_font_italic">g</span>. SimCLR, MoCo) create positive views with aggressive random cropping of a single image. Such use of this augmentation can lead to features for object regions being made similar to those for the background or co-occurring objects, potentially causing contextual bias and hurting in out-of-context scenarios. Random cropping may also result in texture-biased rather than shape-biased features, since the shapes of object parts may not be consistent across crops. A lack of shape in representations can lead to degraded performance when texture is shifted (<span id="Sx1.p2.1.4" class="ltx_text ltx_font_italic">e</span>.<span id="Sx1.p2.1.5" class="ltx_text ltx_font_italic">g</span>. appearance changes due to weather).</p>
</div>
<figure id="Sx1.F1" class="ltx_figure"><img src="/html/2212.04613/assets/x1.png" id="Sx1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="201" height="35" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="Sx1.F1.14.1" class="ltx_text ltx_font_bold">What properties of contrastive views enable robust downstream object detection?</span> Existing contrastive view design methods may cause detectors to lack robustness in domain-shifted settings (<span id="Sx1.F1.15.2" class="ltx_text ltx_font_italic">e</span>.<span id="Sx1.F1.16.3" class="ltx_text ltx_font_italic">g</span>. appearance, context). For instance, state-of-the-art pipelines use random cropping, which may cause contextual bias as objects can be aligned to common backgrounds (<span id="Sx1.F1.17.4" class="ltx_text ltx_font_italic">e</span>.<span id="Sx1.F1.18.5" class="ltx_text ltx_font_italic">g</span>. boat in <span id="Sx1.F1.19.6" class="ltx_text ltx_font_bold">a</span>, horses in <span id="Sx1.F1.20.7" class="ltx_text ltx_font_bold">b</span>) or co-occurring objects (<span id="Sx1.F1.21.8" class="ltx_text ltx_font_italic">e</span>.<span id="Sx1.F1.22.9" class="ltx_text ltx_font_italic">g</span>. player, glove, ball, and bat in <span id="Sx1.F1.23.10" class="ltx_text ltx_font_bold">c</span>). Due to a lack of spatial consistency between views, its use may also discourage the learning of object shape (<span id="Sx1.F1.24.11" class="ltx_text ltx_font_italic">e</span>.<span id="Sx1.F1.25.12" class="ltx_text ltx_font_italic">g</span>. cat in <span id="Sx1.F1.26.13" class="ltx_text ltx_font_bold">d</span>).</figcaption>
</figure>
<div id="Sx1.p3" class="ltx_para">
<p id="Sx1.p3.1" class="ltx_p">In this work, we explore strategies to improve contrastive view design for enhanced robustness in domain-shifted detection. In particular, we conduct an empirical study of the state-of-the-art detection pretext task InsLoc <cite class="ltx_cite ltx_citemacro_citep">(Yang et al. <a href="#bib.bib47" title="" class="ltx_ref">2021</a>)</cite> and strategically alter the views created from InsLoc’s data augmentation pipeline by adjusting cropping, adding shortcut-reducing appearance augmentations, and integrating saliency-based object priors. These strategies are evaluated in-domain and out-of-domain in appearance-shifted and context-shifted detection scenarios. Experiments are also conducted following pretraining on both single-object and multi-object image datasets. From these experiments, we present these insights into contrastive view design:</p>
</div>
<div id="Sx1.p4" class="ltx_para">
<ul id="Sx1.I1" class="ltx_itemize">
<li id="Sx1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i1.p1" class="ltx_para">
<p id="Sx1.I1.i1.p1.1" class="ltx_p">Increasing the minimum % of an image used for crops or adding an IoU constraint between views causes the model to learn from spatially consistent views with larger object parts. These strategies encourage the learning of shape and improve robustness to appearance shifts.</p>
</div>
</li>
<li id="Sx1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i2.p1" class="ltx_para">
<p id="Sx1.I1.i2.p1.1" class="ltx_p">Shortcut-reducing augmentations enhance the effectiveness of non-aggressive cropping, exemplified by improvements over InsLoc both <span id="Sx1.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">in-domain</span> (up to +2.73 AP) and <span id="Sx1.I1.i2.p1.1.2" class="ltx_text ltx_font_italic">out-of-domain</span> (up to +3.07 AP).</p>
</div>
</li>
<li id="Sx1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i3.p1" class="ltx_para">
<p id="Sx1.I1.i3.p1.1" class="ltx_p">The use of saliency priors in views is effective for out-of-context robustness. Their use is best in a mechanism that removes background and tightens crops to object regions.</p>
</div>
</li>
<li id="Sx1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i4.p1" class="ltx_para">
<p id="Sx1.I1.i4.p1.1" class="ltx_p">Applying shortcut-reducing augmentation to the non-salient regions in views, in combination with crop tightening and shape strategies, is effective for enhancing robustness to both appearance and context shifts.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Background and Related Work</h2>

<section id="Sx2.SS0.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Self-supervised and contrastive learning</h4>

<div id="Sx2.SS0.SSSx1.p1" class="ltx_para">
<p id="Sx2.SS0.SSSx1.p1.1" class="ltx_p">Many top self-supervised methods in vision use contrastive learning and the instance discrimination pretext task <cite class="ltx_cite ltx_citemacro_citep">(Wu et al. <a href="#bib.bib43" title="" class="ltx_ref">2018</a>)</cite>, where each image is its own class, and the goal is to discern that two positive samples (or <span id="Sx2.SS0.SSSx1.p1.1.1" class="ltx_text ltx_font_italic">views</span>) are from the same image when considered versus a set of negatives. Typically, positives are generated through aggressive data augmentation of a single image, and they are compared to a large number of negatives from other images. While the representations for negatives have been stored in large memory banks <cite class="ltx_cite ltx_citemacro_citep">(Wu et al. <a href="#bib.bib43" title="" class="ltx_ref">2018</a>)</cite>, recent methods optimize the learning pipeline to rather use large batch sizes <cite class="ltx_cite ltx_citemacro_citep">(Chen et al. <a href="#bib.bib4" title="" class="ltx_ref">2020a</a>)</cite> or a dynamic dictionary <cite class="ltx_cite ltx_citemacro_citep">(He et al. <a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite>. Alternatively, the online clustering approach of <cite class="ltx_cite ltx_citemacro_citep">(Caron et al. <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite> avoids the need for pairwise comparisons entirely, and the iterative approach of BYOL <cite class="ltx_cite ltx_citemacro_citep">(Grill et al. <a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite> avoids using negatives. In general, contrastive methods are evaluated by transferring representations to downstream tasks such as object detection, and <span id="Sx2.SS0.SSSx1.p1.1.2" class="ltx_text ltx_font_italic">domain shifts are not usually considered with detection</span>. General detectors have been shown to lack robustness to shifts, retaining only 30-60% of performance when tested on natural corruptions <cite class="ltx_cite ltx_citemacro_citep">(Michaelis et al. <a href="#bib.bib24" title="" class="ltx_ref">2019</a>)</cite>, and contrastive detectors may similarly lack robustness. In this work, we fill in the need to more broadly characterize and improve the generalizability of contrastive representations through our study of view design strategies in domain-shifted detection.</p>
</div>
</section>
<section id="Sx2.SS0.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Data augmentations and views in contrastive learning</h4>

<div id="Sx2.SS0.SSSx2.p1" class="ltx_para">
<p id="Sx2.SS0.SSSx2.p1.1" class="ltx_p">Recent works have investigated how to construct positive and negative views for contrastive learning. In particular, topics explored include how to learn views <cite class="ltx_cite ltx_citemacro_citep">(Tian et al. <a href="#bib.bib38" title="" class="ltx_ref">2020</a>)</cite>, how to mine hard negatives <cite class="ltx_cite ltx_citemacro_citep">(Kalantidis et al. <a href="#bib.bib20" title="" class="ltx_ref">2020</a>; Robinson et al. <a href="#bib.bib32" title="" class="ltx_ref">2021b</a>)</cite>, how to handle false negatives <cite class="ltx_cite ltx_citemacro_citep">(Chuang et al. <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>, and how to modify sample features to avoid shortcuts <cite class="ltx_cite ltx_citemacro_citep">(Robinson et al. <a href="#bib.bib31" title="" class="ltx_ref">2021a</a>)</cite>. Positives have also been studied with regards to intra-image augmentations and instance discrimination. For instance, SimCLR <cite class="ltx_cite ltx_citemacro_citep">(Chen et al. <a href="#bib.bib4" title="" class="ltx_ref">2020a</a>)</cite> finds that creating positives with random cropping, color distortion, and Gaussian blur is effective for ImageNet classification. In this work, we also empirically explore augmentations for positives, but consider those specifically targeting domain robustness in detection, <span id="Sx2.SS0.SSSx2.p1.1.1" class="ltx_text ltx_font_italic">which include some previously unexplored in contrastive learning: Poisson blending, texture flattening, and elastic deformation</span>. We also show that modifying cropping to be less aggressive or use IoU constraints can improve out-of-domain robustness.</p>
</div>
</section>
<section id="Sx2.SS0.SSSx3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Robustness in contrastive learning</h4>

<div id="Sx2.SS0.SSSx3.p1" class="ltx_para">
<p id="Sx2.SS0.SSSx3.p1.1" class="ltx_p">Neural network representations have been shown to not generalize well to various domain shifts (<span id="Sx2.SS0.SSSx3.p1.1.1" class="ltx_text ltx_font_italic">e</span>.<span id="Sx2.SS0.SSSx3.p1.1.2" class="ltx_text ltx_font_italic">g</span>. pose <cite class="ltx_cite ltx_citemacro_citep">(Alcorn et al. <a href="#bib.bib1" title="" class="ltx_ref">2019</a>)</cite>, corruptions <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks and Dietterich <a href="#bib.bib17" title="" class="ltx_ref">2019</a>)</cite>) and to suffer from biases (<span id="Sx2.SS0.SSSx3.p1.1.3" class="ltx_text ltx_font_italic">e</span>.<span id="Sx2.SS0.SSSx3.p1.1.4" class="ltx_text ltx_font_italic">g</span>. texture <cite class="ltx_cite ltx_citemacro_citep">(Geirhos et al. <a href="#bib.bib12" title="" class="ltx_ref">2019</a>)</cite>, context <cite class="ltx_cite ltx_citemacro_citep">(Singh et al. <a href="#bib.bib37" title="" class="ltx_ref">2020</a>)</cite>, background <cite class="ltx_cite ltx_citemacro_citep">(Xiao et al. <a href="#bib.bib44" title="" class="ltx_ref">2021</a>)</cite>). Contrastive representations face similar issues, for instance versus viewpoint shifts <cite class="ltx_cite ltx_citemacro_citep">(Purushwalkam and Gupta <a href="#bib.bib29" title="" class="ltx_ref">2020</a>)</cite> and texture-shape conflicts <cite class="ltx_cite ltx_citemacro_citep">(Geirhos et al. <a href="#bib.bib11" title="" class="ltx_ref">2020</a>)</cite>. <span id="Sx2.SS0.SSSx3.p1.1.5" class="ltx_text ltx_font_italic">Most works that strive to explicitly improve contrastive robustness either focus on image recognition <cite class="ltx_cite ltx_citemacro_citep">(Ge et al. <a href="#bib.bib10" title="" class="ltx_ref">2021</a>; Khosla et al. <a href="#bib.bib21" title="" class="ltx_ref">2020</a>)</cite>, proxy recognition tasks like the Background Challenge <cite class="ltx_cite ltx_citemacro_citep">(Xiao et al. <a href="#bib.bib44" title="" class="ltx_ref">2021</a>)</cite>, or evaluate only when transferring representations to object detection, but not on domain shifts in detection.</span> We alternatively consider contrastive robustness with respect to object detection and relevant domain shifts. One shift we consider is <span id="Sx2.SS0.SSSx3.p1.1.6" class="ltx_text ltx_font_italic">in context</span>, as research has identified contextual bias as an issue in contrastive pretraining on multi-object image datasets. For example, <cite class="ltx_cite ltx_citemacro_citep">(Selvaraju et al. <a href="#bib.bib35" title="" class="ltx_ref">2021</a>)</cite> addresses contextual bias in COCO pretraining by constraining crops to overlap with saliency maps and using a Grad-CAM attention loss, leading to improvements over MoCo on COCO and VOC detection. Similarly, <cite class="ltx_cite ltx_citemacro_citep">(Mo et al. <a href="#bib.bib25" title="" class="ltx_ref">2021</a>)</cite> proposes two augmentations, object-aware cropping (OA-Crop) and background mixup (BG-Mixup), to reduce contextual bias in MoCo-v2 and BYOL. Notably, these cropping strategies have minimally been tested with detection (just in-domain), so it is unclear how such strategies perform in out-of-domain detection. We evaluate these strategies out-of-domain and show that they do not always result in improvements. We thus propose a hybrid strategy of the methods and show that it substantially improves out-of-context robustness.</p>
</div>
</section>
<section id="Sx2.SS0.SSSx4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Pretext tasks for object detection</h4>

<div id="Sx2.SS0.SSSx4.p1" class="ltx_para">
<p id="Sx2.SS0.SSSx4.p1.1" class="ltx_p">Our research also fits with recent works that tailor pretraining to downstream tasks besides image classification, such as object detection. In particular, we explore InsLoc, a pretext task in which detection-focused representations are built in contrastive learning through integration of bounding box information <cite class="ltx_cite ltx_citemacro_citep">(Yang et al. <a href="#bib.bib47" title="" class="ltx_ref">2021</a>)</cite>. Other notable approaches exist which leverage selective search proposals <cite class="ltx_cite ltx_citemacro_citep">(Wei et al. <a href="#bib.bib41" title="" class="ltx_ref">2021</a>)</cite>, global and local views <cite class="ltx_cite ltx_citemacro_citep">(Xie et al. <a href="#bib.bib45" title="" class="ltx_ref">2021a</a>)</cite>, spatially consistent representation learning <cite class="ltx_cite ltx_citemacro_citep">(Roh et al. <a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite>, weak supervision <cite class="ltx_cite ltx_citemacro_citep">(Zhong et al. <a href="#bib.bib49" title="" class="ltx_ref">2021</a>)</cite>, pixel-level pretext tasks <cite class="ltx_cite ltx_citemacro_citep">(Wang et al. <a href="#bib.bib40" title="" class="ltx_ref">2021</a>; Xie et al. <a href="#bib.bib46" title="" class="ltx_ref">2021b</a>)</cite>, and transformers <cite class="ltx_cite ltx_citemacro_citep">(Dai et al. <a href="#bib.bib7" title="" class="ltx_ref">2021</a>)</cite>. <span id="Sx2.SS0.SSSx4.p1.1.1" class="ltx_text ltx_font_italic">Our work is orthogonal to such works as we provide contrastive view design insights that can guide future detection-focused pretext tasks to have greater out-of-domain robustness.</span></p>
</div>
</section>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Experimental Approach</h2>

<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.1" class="ltx_p">In this study, our goal is to analyze how strategic changes to contrastive views impact downstream object detection robustness.
In particular, we consider two families of domain shifts that cause drops in object detection performance: appearance and context. <em id="Sx3.p1.1.1" class="ltx_emph ltx_font_italic">Appearance shift</em> in our study is defined as change in the visual characteristics of objects (such as color brightness and texture). <em id="Sx3.p1.1.2" class="ltx_emph ltx_font_italic">Context shift</em> in our study is defined as when an object appears with different objects or in different backgrounds during train and test time.</p>
</div>
<div id="Sx3.p2" class="ltx_para">
<p id="Sx3.p2.14" class="ltx_p">Formally, we view contrastive learning from a knowledge transfer perspective. There is a source (pretext) task <math id="Sx3.p2.1.m1.1" class="ltx_Math" alttext="s" display="inline"><semantics id="Sx3.p2.1.m1.1a"><mi id="Sx3.p2.1.m1.1.1" xref="Sx3.p2.1.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="Sx3.p2.1.m1.1b"><ci id="Sx3.p2.1.m1.1.1.cmml" xref="Sx3.p2.1.m1.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p2.1.m1.1c">s</annotation></semantics></math> and a downstream object detection task <math id="Sx3.p2.2.m2.1" class="ltx_Math" alttext="d" display="inline"><semantics id="Sx3.p2.2.m2.1a"><mi id="Sx3.p2.2.m2.1.1" xref="Sx3.p2.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="Sx3.p2.2.m2.1b"><ci id="Sx3.p2.2.m2.1.1.cmml" xref="Sx3.p2.2.m2.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p2.2.m2.1c">d</annotation></semantics></math>. Unsupervised, contrastive pretraining is performed in <math id="Sx3.p2.3.m3.1" class="ltx_Math" alttext="s" display="inline"><semantics id="Sx3.p2.3.m3.1a"><mi id="Sx3.p2.3.m3.1.1" xref="Sx3.p2.3.m3.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="Sx3.p2.3.m3.1b"><ci id="Sx3.p2.3.m3.1.1.cmml" xref="Sx3.p2.3.m3.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p2.3.m3.1c">s</annotation></semantics></math> on dataset <math id="Sx3.p2.4.m4.1" class="ltx_Math" alttext="\mathbf{s}" display="inline"><semantics id="Sx3.p2.4.m4.1a"><mi id="Sx3.p2.4.m4.1.1" xref="Sx3.p2.4.m4.1.1.cmml">𝐬</mi><annotation-xml encoding="MathML-Content" id="Sx3.p2.4.m4.1b"><ci id="Sx3.p2.4.m4.1.1.cmml" xref="Sx3.p2.4.m4.1.1">𝐬</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p2.4.m4.1c">\mathbf{s}</annotation></semantics></math>, then representations are transferred to <math id="Sx3.p2.5.m5.1" class="ltx_Math" alttext="d" display="inline"><semantics id="Sx3.p2.5.m5.1a"><mi id="Sx3.p2.5.m5.1.1" xref="Sx3.p2.5.m5.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="Sx3.p2.5.m5.1b"><ci id="Sx3.p2.5.m5.1.1.cmml" xref="Sx3.p2.5.m5.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p2.5.m5.1c">d</annotation></semantics></math> for supervised finetuning on dataset <math id="Sx3.p2.6.m6.1" class="ltx_Math" alttext="\mathbf{d}" display="inline"><semantics id="Sx3.p2.6.m6.1a"><mi id="Sx3.p2.6.m6.1.1" xref="Sx3.p2.6.m6.1.1.cmml">𝐝</mi><annotation-xml encoding="MathML-Content" id="Sx3.p2.6.m6.1b"><ci id="Sx3.p2.6.m6.1.1.cmml" xref="Sx3.p2.6.m6.1.1">𝐝</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p2.6.m6.1c">\mathbf{d}</annotation></semantics></math>. We explore strategies to adapt <math id="Sx3.p2.7.m7.1" class="ltx_Math" alttext="s" display="inline"><semantics id="Sx3.p2.7.m7.1a"><mi id="Sx3.p2.7.m7.1.1" xref="Sx3.p2.7.m7.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="Sx3.p2.7.m7.1b"><ci id="Sx3.p2.7.m7.1.1.cmml" xref="Sx3.p2.7.m7.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p2.7.m7.1c">s</annotation></semantics></math> to <math id="Sx3.p2.8.m8.1" class="ltx_Math" alttext="s^{\prime}" display="inline"><semantics id="Sx3.p2.8.m8.1a"><msup id="Sx3.p2.8.m8.1.1" xref="Sx3.p2.8.m8.1.1.cmml"><mi id="Sx3.p2.8.m8.1.1.2" xref="Sx3.p2.8.m8.1.1.2.cmml">s</mi><mo id="Sx3.p2.8.m8.1.1.3" xref="Sx3.p2.8.m8.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="Sx3.p2.8.m8.1b"><apply id="Sx3.p2.8.m8.1.1.cmml" xref="Sx3.p2.8.m8.1.1"><csymbol cd="ambiguous" id="Sx3.p2.8.m8.1.1.1.cmml" xref="Sx3.p2.8.m8.1.1">superscript</csymbol><ci id="Sx3.p2.8.m8.1.1.2.cmml" xref="Sx3.p2.8.m8.1.1.2">𝑠</ci><ci id="Sx3.p2.8.m8.1.1.3.cmml" xref="Sx3.p2.8.m8.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p2.8.m8.1c">s^{\prime}</annotation></semantics></math>, which represents a task with views augmented to be robust to families of domain shifts (<span id="Sx3.p2.14.1" class="ltx_text ltx_font_italic">e</span>.<span id="Sx3.p2.14.2" class="ltx_text ltx_font_italic">g</span>. appearance, context). To evaluate the effectiveness of <math id="Sx3.p2.9.m9.1" class="ltx_Math" alttext="s^{\prime}" display="inline"><semantics id="Sx3.p2.9.m9.1a"><msup id="Sx3.p2.9.m9.1.1" xref="Sx3.p2.9.m9.1.1.cmml"><mi id="Sx3.p2.9.m9.1.1.2" xref="Sx3.p2.9.m9.1.1.2.cmml">s</mi><mo id="Sx3.p2.9.m9.1.1.3" xref="Sx3.p2.9.m9.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="Sx3.p2.9.m9.1b"><apply id="Sx3.p2.9.m9.1.1.cmml" xref="Sx3.p2.9.m9.1.1"><csymbol cd="ambiguous" id="Sx3.p2.9.m9.1.1.1.cmml" xref="Sx3.p2.9.m9.1.1">superscript</csymbol><ci id="Sx3.p2.9.m9.1.1.2.cmml" xref="Sx3.p2.9.m9.1.1.2">𝑠</ci><ci id="Sx3.p2.9.m9.1.1.3.cmml" xref="Sx3.p2.9.m9.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p2.9.m9.1c">s^{\prime}</annotation></semantics></math>, representations from <math id="Sx3.p2.10.m10.1" class="ltx_Math" alttext="s^{\prime}" display="inline"><semantics id="Sx3.p2.10.m10.1a"><msup id="Sx3.p2.10.m10.1.1" xref="Sx3.p2.10.m10.1.1.cmml"><mi id="Sx3.p2.10.m10.1.1.2" xref="Sx3.p2.10.m10.1.1.2.cmml">s</mi><mo id="Sx3.p2.10.m10.1.1.3" xref="Sx3.p2.10.m10.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="Sx3.p2.10.m10.1b"><apply id="Sx3.p2.10.m10.1.1.cmml" xref="Sx3.p2.10.m10.1.1"><csymbol cd="ambiguous" id="Sx3.p2.10.m10.1.1.1.cmml" xref="Sx3.p2.10.m10.1.1">superscript</csymbol><ci id="Sx3.p2.10.m10.1.1.2.cmml" xref="Sx3.p2.10.m10.1.1.2">𝑠</ci><ci id="Sx3.p2.10.m10.1.1.3.cmml" xref="Sx3.p2.10.m10.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p2.10.m10.1c">s^{\prime}</annotation></semantics></math> (after finetuning on <math id="Sx3.p2.11.m11.1" class="ltx_Math" alttext="\mathbf{d}" display="inline"><semantics id="Sx3.p2.11.m11.1a"><mi id="Sx3.p2.11.m11.1.1" xref="Sx3.p2.11.m11.1.1.cmml">𝐝</mi><annotation-xml encoding="MathML-Content" id="Sx3.p2.11.m11.1b"><ci id="Sx3.p2.11.m11.1.1.cmml" xref="Sx3.p2.11.m11.1.1">𝐝</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p2.11.m11.1c">\mathbf{d}</annotation></semantics></math>) are evaluated on <math id="Sx3.p2.12.m12.1" class="ltx_Math" alttext="\mathbf{d}" display="inline"><semantics id="Sx3.p2.12.m12.1a"><mi id="Sx3.p2.12.m12.1.1" xref="Sx3.p2.12.m12.1.1.cmml">𝐝</mi><annotation-xml encoding="MathML-Content" id="Sx3.p2.12.m12.1b"><ci id="Sx3.p2.12.m12.1.1.cmml" xref="Sx3.p2.12.m12.1.1">𝐝</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p2.12.m12.1c">\mathbf{d}</annotation></semantics></math> and various <math id="Sx3.p2.13.m13.1" class="ltx_Math" alttext="\mathbf{t_{i}}" display="inline"><semantics id="Sx3.p2.13.m13.1a"><msub id="Sx3.p2.13.m13.1.1" xref="Sx3.p2.13.m13.1.1.cmml"><mi id="Sx3.p2.13.m13.1.1.2" xref="Sx3.p2.13.m13.1.1.2.cmml">𝐭</mi><mi id="Sx3.p2.13.m13.1.1.3" xref="Sx3.p2.13.m13.1.1.3.cmml">𝐢</mi></msub><annotation-xml encoding="MathML-Content" id="Sx3.p2.13.m13.1b"><apply id="Sx3.p2.13.m13.1.1.cmml" xref="Sx3.p2.13.m13.1.1"><csymbol cd="ambiguous" id="Sx3.p2.13.m13.1.1.1.cmml" xref="Sx3.p2.13.m13.1.1">subscript</csymbol><ci id="Sx3.p2.13.m13.1.1.2.cmml" xref="Sx3.p2.13.m13.1.1.2">𝐭</ci><ci id="Sx3.p2.13.m13.1.1.3.cmml" xref="Sx3.p2.13.m13.1.1.3">𝐢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p2.13.m13.1c">\mathbf{t_{i}}</annotation></semantics></math>, which represent target datasets domain-shifted from <math id="Sx3.p2.14.m14.1" class="ltx_Math" alttext="\mathbf{d}" display="inline"><semantics id="Sx3.p2.14.m14.1a"><mi id="Sx3.p2.14.m14.1.1" xref="Sx3.p2.14.m14.1.1.cmml">𝐝</mi><annotation-xml encoding="MathML-Content" id="Sx3.p2.14.m14.1b"><ci id="Sx3.p2.14.m14.1.1.cmml" xref="Sx3.p2.14.m14.1.1">𝐝</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p2.14.m14.1c">\mathbf{d}</annotation></semantics></math>.</p>
</div>
<section id="Sx3.SS0.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Base pretext task</h4>

<div id="Sx3.SS0.SSSx1.p1" class="ltx_para">
<p id="Sx3.SS0.SSSx1.p1.1" class="ltx_p">We select <span id="Sx3.SS0.SSSx1.p1.1.1" class="ltx_text ltx_font_italic">s</span> to be Instance Localization (InsLoc) <cite class="ltx_cite ltx_citemacro_citep">(Yang et al. <a href="#bib.bib47" title="" class="ltx_ref">2021</a>)</cite>, a state-of-the-art detection-focused, contrastive pretext task. In particular, InsLoc is designed as an improvement over MoCo-v2 <cite class="ltx_cite ltx_citemacro_citep">(Chen et al. <a href="#bib.bib5" title="" class="ltx_ref">2020b</a>)</cite> and uses the Faster R-CNN detector <cite class="ltx_cite ltx_citemacro_citep">(Ren et al. <a href="#bib.bib30" title="" class="ltx_ref">2015</a>)</cite>. Positive views are created by pasting random crops from an image at various aspect ratios and scales onto random locations of two random background images. Instance discrimination is then performed between foreground features obtained with RoI-Align <cite class="ltx_cite ltx_citemacro_citep">(He et al. <a href="#bib.bib16" title="" class="ltx_ref">2017</a>)</cite> from the two composited images (positive <span id="Sx3.SS0.SSSx1.p1.1.2" class="ltx_text ltx_font_italic">query</span> and <span id="Sx3.SS0.SSSx1.p1.1.3" class="ltx_text ltx_font_italic">key</span> views) and negatives maintained in a dictionary. This task is optimized with the InfoNCE loss <cite class="ltx_cite ltx_citemacro_citep">(Van den Oord, Li, and Vinyals <a href="#bib.bib39" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<div id="Sx3.SS0.SSSx1.p2" class="ltx_para">
<p id="Sx3.SS0.SSSx1.p2.1" class="ltx_p">Outlining the specifics of InsLoc’s augmentation pipeline further, the crops used for views are uniformly sampled to be between 20-100% of an image. Crops are then resized to random aspect ratios between 0.5 and 2 and width and height scales between 128 and 256 pixels. Composited images have size 256<math id="Sx3.SS0.SSSx1.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="Sx3.SS0.SSSx1.p2.1.m1.1a"><mo id="Sx3.SS0.SSSx1.p2.1.m1.1.1" xref="Sx3.SS0.SSSx1.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="Sx3.SS0.SSSx1.p2.1.m1.1b"><times id="Sx3.SS0.SSSx1.p2.1.m1.1.1.cmml" xref="Sx3.SS0.SSSx1.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SS0.SSSx1.p2.1.m1.1c">\times</annotation></semantics></math>256 pixels. Other augmentations used include random applications of Gaussian blurring, horizontal flipping, color jittering, and grayscale conversion. Notably, InsLoc’s appearance augmentations and cropping are characteristic of state-of-the-art contrastive methods <cite class="ltx_cite ltx_citemacro_citep">(Chen et al. <a href="#bib.bib4" title="" class="ltx_ref">2020a</a>; He et al. <a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite>, <span id="Sx3.SS0.SSSx1.p2.1.1" class="ltx_text ltx_font_italic">making InsLoc a fitting contrastive case study</span>.</p>
</div>
</section>
<section id="Sx3.SS0.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Strategies to enhance InsLoc</h4>

<div id="Sx3.SS0.SSSx2.p1" class="ltx_para">
<p id="Sx3.SS0.SSSx2.p1.1" class="ltx_p">We propose multiple strategies to augment the InsLoc view pipeline (<span id="Sx3.SS0.SSSx2.p1.1.1" class="ltx_text ltx_font_italic">s’</span>) for enhanced robustness in appearance-shifted and out-of-context detection scenarios. First, we consider cropping since InsLoc, like other contrastive methods <cite class="ltx_cite ltx_citemacro_citep">(Chen et al. <a href="#bib.bib4" title="" class="ltx_ref">2020a</a>; He et al. <a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite>, uses aggressive random cropping to create positive views (see Fig. <a href="#Sx1.F1" title="Figure 1 ‣ Introduction ‣ Contrastive View Design Strategies to Enhance Robustness to Domain Shifts in Downstream Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). As random cropping has been shown to bias a model towards texture <cite class="ltx_cite ltx_citemacro_citep">(Hermann, Chen, and Kornblith <a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite>, we reason that InsLoc may struggle when texture shifts in detection such as when it is raining or snowing. Models that learn shape on the other hand can be effective in such situations <cite class="ltx_cite ltx_citemacro_citep">(Geirhos et al. <a href="#bib.bib12" title="" class="ltx_ref">2019</a>)</cite>. We thus explore simple strategies to encourage InsLoc to learn shape. In particular, we experiment with <span id="Sx3.SS0.SSSx2.p1.1.2" class="ltx_text ltx_font_italic">geometric changes to crops</span>, specifically increasing the minimum % of an image used to crop <span id="Sx3.SS0.SSSx2.p1.1.3" class="ltx_text ltx_font_italic">m</span> and enforcing an IoU constraint <span id="Sx3.SS0.SSSx2.p1.1.4" class="ltx_text ltx_font_italic">t</span> between views. We expect such changes to increase the spatial consistency between crops and encourage the model to learn object parts and shapes. In turn, the model can become more robust to texture shifts.</p>
</div>
<figure id="Sx3.F2" class="ltx_figure"><img src="/html/2212.04613/assets/x2.png" id="Sx3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="216" height="66" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Our shortcut-reducing augmentations of study, shown within InsLoc. We perform each augmentation on the crop in the query view (left), but not in the key view (right).</figcaption>
</figure>
<div id="Sx3.SS0.SSSx2.p2" class="ltx_para">
<p id="Sx3.SS0.SSSx2.p2.1" class="ltx_p">Furthermore, we consider adding <span id="Sx3.SS0.SSSx2.p2.1.1" class="ltx_text ltx_font_italic">shortcut-reducing appearance augmentations</span>, as we find that InsLoc may not adequately discourage the model from attending to shortcuts that are non-robust in appearance-shifted scenarios, such as high-frequency noise and texture or color histograms. One strategy we explore is <span id="Sx3.SS0.SSSx2.p2.1.2" class="ltx_text ltx_font_italic">Poisson blending</span> <cite class="ltx_cite ltx_citemacro_citep">(Pérez, Gangnet, and Blake <a href="#bib.bib27" title="" class="ltx_ref">2003</a>)</cite>, which is a method to seamlessly blend crops into a background image. We use Poisson blending instead of simple copy-pasting in InsLoc to reduce contrast between foreground and background regions, effectively making the pretext task harder as the model cannot use contrast as a shortcut to solve the task. It is also found that Poisson blending can introduce random illumination effects from the background, which may be desirable to learn invariance towards for appearance shifts. Second, we explore the <span id="Sx3.SS0.SSSx2.p2.1.3" class="ltx_text ltx_font_italic">texture flattening</span> application of solving the Poisson equation, as it washes out texture and changes brightness while only preserving gradients at edge locations. We reason that this augmentation can be effective to teach the model to not overfit to high-frequency texture shortcuts. Last, we investigate <span id="Sx3.SS0.SSSx2.p2.1.4" class="ltx_text ltx_font_italic">elastic deformation</span> <cite class="ltx_cite ltx_citemacro_citep">(Simard et al. <a href="#bib.bib36" title="" class="ltx_ref">2003</a>)</cite>, an augmentation that alters images by moving pixels with displacement fields. This augmentation can help make features more invariant to local changes in edges and noise shortcuts. We illustrate our proposed use of these strategies in Fig. <a href="#Sx3.F2" title="Figure 2 ‣ Strategies to enhance InsLoc ‣ Experimental Approach ‣ Contrastive View Design Strategies to Enhance Robustness to Domain Shifts in Downstream Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Augmentations are applied 100% of the time, unless otherwise noted. We use Poisson blending and texture flattening as provided in the OpenCV library <cite class="ltx_cite ltx_citemacro_citep">(Bradski <a href="#bib.bib2" title="" class="ltx_ref">2000</a>)</cite> and the algorithm of <cite class="ltx_cite ltx_citemacro_citep">(Simard et al. <a href="#bib.bib36" title="" class="ltx_ref">2003</a>)</cite> for elastic deformation.</p>
</div>
<div id="Sx3.SS0.SSSx2.p3" class="ltx_para">
<p id="Sx3.SS0.SSSx2.p3.1" class="ltx_p">Lastly, we note that random cropping may result in the aligning of context (background and objects or objects and objects), which can lead to representations that are contextually biased and not robust in out-of-context detection. To address this problem, we experiment with <span id="Sx3.SS0.SSSx2.p3.1.1" class="ltx_text ltx_font_italic">strategies that use saliency-based object priors for crops</span>, as they can enable crops to refer to specific object regions rather than to background or co-occurring objects. In particular, we investigate two state-of-the-art approaches <cite class="ltx_cite ltx_citemacro_citep">(Mo et al. <a href="#bib.bib25" title="" class="ltx_ref">2021</a>; Selvaraju et al. <a href="#bib.bib35" title="" class="ltx_ref">2021</a>)</cite>, as well as a hybrid of such approaches. We compare each strategy out-of-domain and also consider combining saliency strategies with shape and appearance strategies.</p>
</div>
</section>
<section id="Sx3.SS0.SSSx3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Pretraining and finetuning datasets</h4>

<div id="Sx3.SS0.SSSx3.p1" class="ltx_para">
<p id="Sx3.SS0.SSSx3.p1.1" class="ltx_p">We identify two pretraining scenarios <span id="Sx3.SS0.SSSx3.p1.1.1" class="ltx_text ltx_font_bold">s</span> to evaluate the robustness of contrastive view design strategies. First is ImageNet pretraining <cite class="ltx_cite ltx_citemacro_citep">(Krizhevsky, Sutskever, and Hinton <a href="#bib.bib22" title="" class="ltx_ref">2012</a>)</cite>, a standard scenario for contrastive approaches <cite class="ltx_cite ltx_citemacro_citep">(Chen et al. <a href="#bib.bib4" title="" class="ltx_ref">2020a</a>; He et al. <a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite>. With the heavy computational nature of contrastive pretraining, along with our goal to conduct multiple experiments, we sample ImageNet from over 1 million to 50,000 images and call this set <span id="Sx3.SS0.SSSx3.p1.1.2" class="ltx_text ltx_font_italic">ImageNet-Subset</span>. Notably, most ImageNet images are <span id="Sx3.SS0.SSSx3.p1.1.3" class="ltx_text ltx_font_italic">iconic</span>, containing a single, large, centered object. For a dataset with different properties, we also consider pretraining on COCO <cite class="ltx_cite ltx_citemacro_citep">(Lin et al. <a href="#bib.bib23" title="" class="ltx_ref">2014</a>)</cite>, which contains more <span id="Sx3.SS0.SSSx3.p1.1.4" class="ltx_text ltx_font_italic">scene</span> imagery, having multiple, potentially small objects. With the goal of self-supervised learning to learn robust representations on large, uncurated datasets, which are likely to contain scene imagery, COCO is a practical dataset to study. Also its multi-object nature makes it an apt test case for benchmarking contextual bias downstream. We pretrain specifically with COCO2017train and do not sample since its size is relatively small (118,287 images).</p>
</div>
<div id="Sx3.SS0.SSSx3.p2" class="ltx_para">
<p id="Sx3.SS0.SSSx3.p2.1" class="ltx_p">We explore two finetuning datasets <span id="Sx3.SS0.SSSx3.p2.1.1" class="ltx_text ltx_font_bold">d</span>: VOC <cite class="ltx_cite ltx_citemacro_citep">(Everingham et al. <a href="#bib.bib9" title="" class="ltx_ref">2010</a>)</cite> when <span id="Sx3.SS0.SSSx3.p2.1.2" class="ltx_text ltx_font_bold">s</span> is ImageNet-Subset and both VOC and COCO2017train when <span id="Sx3.SS0.SSSx3.p2.1.3" class="ltx_text ltx_font_bold">s</span> is COCO. For VOC, we specifically use VOC0712train+val (16,551 images). We evaluate on COCO2017val (5,000 images) and VOC07test (4,952 images). In our sampling of ImageNet, we ensure semantic overlap with VOC by choosing 132 images for each of 379 classes from 13 synset classes that are related to VOC’s classes: aircraft, vehicle, bird, boat, container, cat, furniture, ungulate, dog, person, plant, train, and electronics.</p>
</div>
<figure id="Sx3.F3" class="ltx_figure"><img src="/html/2212.04613/assets/x3.png" id="Sx3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="206" height="74" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Our domain-shifted test sets. Weather and Abstract are for appearance shifts, and UnRel is for context shifts.</figcaption>
</figure>
</section>
<section id="Sx3.SS0.SSSx4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Domain shift datasets</h4>

<div id="Sx3.SS0.SSSx4.p1" class="ltx_para">
<p id="Sx3.SS0.SSSx4.p1.1" class="ltx_p">We select various datasets <math id="Sx3.SS0.SSSx4.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{t_{i}}" display="inline"><semantics id="Sx3.SS0.SSSx4.p1.1.m1.1a"><msub id="Sx3.SS0.SSSx4.p1.1.m1.1.1" xref="Sx3.SS0.SSSx4.p1.1.m1.1.1.cmml"><mi id="Sx3.SS0.SSSx4.p1.1.m1.1.1.2" xref="Sx3.SS0.SSSx4.p1.1.m1.1.1.2.cmml">𝐭</mi><mi id="Sx3.SS0.SSSx4.p1.1.m1.1.1.3" xref="Sx3.SS0.SSSx4.p1.1.m1.1.1.3.cmml">𝐢</mi></msub><annotation-xml encoding="MathML-Content" id="Sx3.SS0.SSSx4.p1.1.m1.1b"><apply id="Sx3.SS0.SSSx4.p1.1.m1.1.1.cmml" xref="Sx3.SS0.SSSx4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="Sx3.SS0.SSSx4.p1.1.m1.1.1.1.cmml" xref="Sx3.SS0.SSSx4.p1.1.m1.1.1">subscript</csymbol><ci id="Sx3.SS0.SSSx4.p1.1.m1.1.1.2.cmml" xref="Sx3.SS0.SSSx4.p1.1.m1.1.1.2">𝐭</ci><ci id="Sx3.SS0.SSSx4.p1.1.m1.1.1.3.cmml" xref="Sx3.SS0.SSSx4.p1.1.m1.1.1.3">𝐢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SS0.SSSx4.p1.1.m1.1c">\mathbf{t_{i}}</annotation></semantics></math> for out-of-domain evaluation. First, when <span id="Sx3.SS0.SSSx4.p1.1.1" class="ltx_text ltx_font_bold">d</span> is VOC, we test on the challenging, abstract Clipart, Watercolor, and Comic object detection datasets <cite class="ltx_cite ltx_citemacro_citep">(Inoue et al. <a href="#bib.bib19" title="" class="ltx_ref">2018</a>)</cite>, as they represent significant domain shifts in appearance. Clipart has the same 20 classes as VOC and 1,000 samples, while Watercolor and Comic share 6 classes with VOC and have 2,000 samples each. We take the average performance across the three sets and describe the overall set as <span id="Sx3.SS0.SSSx4.p1.1.2" class="ltx_text ltx_font_italic">Abstract</span>. When <span id="Sx3.SS0.SSSx4.p1.1.3" class="ltx_text ltx_font_bold">d</span> is COCO, we test on the out-of-context UnRel dataset <cite class="ltx_cite ltx_citemacro_citep">(Peyre et al. <a href="#bib.bib28" title="" class="ltx_ref">2017</a>)</cite>. This set captures relations that are “unusual” between objects (<span id="Sx3.SS0.SSSx4.p1.1.4" class="ltx_text ltx_font_italic">e</span>.<span id="Sx3.SS0.SSSx4.p1.1.5" class="ltx_text ltx_font_italic">g</span>. car under elephant), making this set useful for evaluating out-of-context robustness. We evaluate on 29 classes which overlap with COCO thing classes (1,049 images). Lastly, for both VOC and COCO, we consider Pascal-C and COCO-C <cite class="ltx_cite ltx_citemacro_citep">(Michaelis et al. <a href="#bib.bib24" title="" class="ltx_ref">2019</a>)</cite>, a collection of sets that are synthetically domain-shifted on natural corruption types. In particular, we explore the appearance-based Weather split at severity level 5, which consists of brightness, fog, frost, and snow shifts. We refer to the overall sets for VOC and COCO as VOC-Weather and COCO-Weather, respectively. Examples for the test sets are shown in Fig. <a href="#Sx3.F3" title="Figure 3 ‣ Pretraining and finetuning datasets ‣ Experimental Approach ‣ Contrastive View Design Strategies to Enhance Robustness to Domain Shifts in Downstream Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
<section id="Sx3.SS0.SSSx5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Training setup</h4>

<div id="Sx3.SS0.SSSx5.p1" class="ltx_para">
<p id="Sx3.SS0.SSSx5.p1.1" class="ltx_p">Pretraining is performed with the provided InsLoc implementation <cite class="ltx_cite ltx_citemacro_citep">(Yang et al. <a href="#bib.bib47" title="" class="ltx_ref">2021</a>)</cite>. Faster R-CNN <cite class="ltx_cite ltx_citemacro_citep">(Ren et al. <a href="#bib.bib30" title="" class="ltx_ref">2015</a>)</cite>, with a ResNet-50 backbone and FPN, serves as the trained detector. With high computational costs for contrastive pretraining, these experiments consider a fixed pretraining budget of 200 epochs. For COCO, pretraining is performed with per-GPU batch size 64 and learning rate 0.03 on 4 NVIDIA Quadro RTX 5000 GPUs with memory 16 GB. For ImageNet-Subset, pretraining is performed with per-GPU batch size 32 and learning rate 0.015 on 2 NVIDIA GeForce GTX 1080 Ti GPUs with memory 11 GB. All pretraining uses a dictionary size of <span id="Sx3.SS0.SSSx5.p1.1.1" class="ltx_text ltx_font_italic">K</span>=8,192. Full finetuning of all layers is performed within the Detectron2 <cite class="ltx_cite ltx_citemacro_citep">(Wu et al. <a href="#bib.bib42" title="" class="ltx_ref">2019</a>)</cite> framework with a 24k iteration schedule, a learning rate of 0.02, and a batch size of 4 on 2 NVIDIA GeForce GTX 1080 Ti GPUs, unless otherwise noted.</p>
</div>
</section>
</section>
<section id="Sx4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Experiments and Analysis</h2>

<div id="Sx4.p1" class="ltx_para">
<p id="Sx4.p1.1" class="ltx_p">In this section, we outline various strategies for contrastive view design and evaluate their effectiveness in the InsLoc pretext task, considering both pretraining on ImageNet-Subset and COCO. Evaluation metrics are AP and AP<sub id="Sx4.p1.1.1" class="ltx_sub"><span id="Sx4.p1.1.1.1" class="ltx_text ltx_font_italic">50</span></sub>.</p>
</div>
<section id="Sx4.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Pretraining on ImageNet-Subset</h3>

<section id="Sx4.SSx1.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">How can we encourage contrastive learning to capture object shape and become more robust to appearance domain shifts?</h4>

<figure id="Sx4.F4" class="ltx_figure"><img src="/html/2212.04613/assets/x4.png" id="Sx4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="218" height="72" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>In-domain (VOC) vs. out-of-domain (Abstract, VOC-Weather) AP for models pretrained in InsLoc with various values of IoU constraint <span id="Sx4.F4.3.1" class="ltx_text ltx_font_italic">t</span> and minimum % crop <span id="Sx4.F4.4.2" class="ltx_text ltx_font_italic">m</span>, averaged over three trials. Note that the top-performing settings for domain robustness (0.70 IoU constraint, 45-100% crop) are different from the settings of the InsLoc baseline (20-100% crop and no IoU constraint). </figcaption>
</figure>
<div id="Sx4.SSx1.SSSx1.p1" class="ltx_para">
<p id="Sx4.SSx1.SSSx1.p1.1" class="ltx_p">First, we consider appearance domain shifts in detection, where object shapes are preserved and texture is distorted. We wish to encourage InsLoc to capture object shapes for such scenarios and thus propose two simple strategies: (1) increasing the minimum % of an image sampled as a crop <span id="Sx4.SSx1.SSSx1.p1.1.1" class="ltx_text ltx_font_italic">m</span> and (2) adding an IoU contraint <span id="Sx4.SSx1.SSSx1.p1.1.2" class="ltx_text ltx_font_italic">t</span>, such that query and key crops must have at least such IoU. To evaluate these strategies, we pretrain InsLoc on ImageNet-Subset using two different values of <span id="Sx4.SSx1.SSSx1.p1.1.3" class="ltx_text ltx_font_italic">m</span>, 45% and 70%, in addition to InsLoc’s default value of 20%, while keeping the maximum crop bound as 100%. InsLoc is additionally pretrained with two IoU constraint values of <span id="Sx4.SSx1.SSSx1.p1.1.4" class="ltx_text ltx_font_italic">t</span>, 45% and 70%. Then for each experiment, we perform finetuning on VOC and evaluate in-domain on VOC and out-of-domain on Abstract and VOC-Weather (two sets with distorted texture).</p>
</div>
<div id="Sx4.SSx1.SSSx1.p2" class="ltx_para">
<p id="Sx4.SSx1.SSSx1.p2.2" class="ltx_p">Results over three trials are shown in Fig. <a href="#Sx4.F4" title="Figure 4 ‣ How can we encourage contrastive learning to capture object shape and become more robust to appearance domain shifts? ‣ Pretraining on ImageNet-Subset ‣ Experiments and Analysis ‣ Contrastive View Design Strategies to Enhance Robustness to Domain Shifts in Downstream Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Notably, we find that the default InsLoc settings (20-100% crops, no IoU constraint) result in the best in-domain AP, but <span id="Sx4.SSx1.SSSx1.p2.2.3" class="ltx_text ltx_font_italic">not the top</span> out-of-domain AP. In particular, <span id="Sx4.SSx1.SSSx1.p2.1.1" class="ltx_text ltx_font_italic">m<math id="Sx4.SSx1.SSSx1.p2.1.1.m1.1" class="ltx_Math" alttext="=" display="inline"><semantics id="Sx4.SSx1.SSSx1.p2.1.1.m1.1a"><mo id="Sx4.SSx1.SSSx1.p2.1.1.m1.1.1" xref="Sx4.SSx1.SSSx1.p2.1.1.m1.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="Sx4.SSx1.SSSx1.p2.1.1.m1.1b"><eq id="Sx4.SSx1.SSSx1.p2.1.1.m1.1.1.cmml" xref="Sx4.SSx1.SSSx1.p2.1.1.m1.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx1.SSSx1.p2.1.1.m1.1c">=</annotation></semantics></math></span>45 and <span id="Sx4.SSx1.SSSx1.p2.2.2" class="ltx_text ltx_font_italic">t<math id="Sx4.SSx1.SSSx1.p2.2.2.m1.1" class="ltx_Math" alttext="=" display="inline"><semantics id="Sx4.SSx1.SSSx1.p2.2.2.m1.1a"><mo id="Sx4.SSx1.SSSx1.p2.2.2.m1.1.1" xref="Sx4.SSx1.SSSx1.p2.2.2.m1.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="Sx4.SSx1.SSSx1.p2.2.2.m1.1b"><eq id="Sx4.SSx1.SSSx1.p2.2.2.m1.1.1.cmml" xref="Sx4.SSx1.SSSx1.p2.2.2.m1.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx1.SSSx1.p2.2.2.m1.1c">=</annotation></semantics></math></span>70 have the highest out-of-domain AP for their respective value comparisons (up to +1.19 AP on Abstract, +0.79 AP on VOC-Weather). In general, these results show out-of-domain benefits with having substantial overlap and higher minimum crop %. <span id="Sx4.SSx1.SSSx1.p2.2.4" class="ltx_text ltx_font_italic">Overall, these results highlight that including larger object regions in crops and encouraging spatial consistency between views are effective strategies to ensure greater robustness to appearance shifts.</span> We also note that the robustness sweet spot for <span id="Sx4.SSx1.SSSx1.p2.2.5" class="ltx_text ltx_font_italic">m</span> may be related to an observed tradeoff with in-domain AP, which drops as <span id="Sx4.SSx1.SSSx1.p2.2.6" class="ltx_text ltx_font_italic">m</span> or <span id="Sx4.SSx1.SSSx1.p2.2.7" class="ltx_text ltx_font_italic">t</span> increases. We reason that while we are encouraging the model to learn shape features, we are also increasing the probability that the model can attend to natural, high-frequency shortcuts in images since crops that have more area and overlap more might share more of these signals. We next consider augmentations to remove shortcuts and improve these strategies.</p>
</div>
</section>
<section id="Sx4.SSx1.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Can shortcut-reducing augmentations make shape strategies more effective?</h4>

<div id="Sx4.SSx1.SSSx2.p1" class="ltx_para">
<p id="Sx4.SSx1.SSSx2.p1.1" class="ltx_p">Though contrastive view pipelines typically have significant appearance augmentations like Gaussian blur, grayscale conversion, and color jitter <cite class="ltx_cite ltx_citemacro_citep">(Chen et al. <a href="#bib.bib5" title="" class="ltx_ref">2020b</a>)</cite>, we reason that even more aggressive augmentations may be beneficial with our % crop and IoU strategies to further limit shortcuts and better learn shape. SimCLR <cite class="ltx_cite ltx_citemacro_citep">(Chen et al. <a href="#bib.bib4" title="" class="ltx_ref">2020a</a>)</cite> serves as motivation, as the authors explore augmentations to avoid color histogram shortcuts. We explore Poisson blending, texture flattening, and elastic deformation as stronger augmentations to similarly reduce shortcuts and enable InsLoc to learn robust features.</p>
</div>
<figure id="Sx4.T1" class="ltx_table">
<table id="Sx4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Sx4.T1.1.1.1" class="ltx_tr">
<th id="Sx4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="Sx4.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<td id="Sx4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx4.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">VOC</span></td>
<td id="Sx4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx4.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Abstract</span></td>
<td id="Sx4.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx4.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">Weather</span></td>
</tr>
<tr id="Sx4.T1.1.2.2" class="ltx_tr">
<th id="Sx4.T1.1.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<td id="Sx4.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r">AP</td>
<td id="Sx4.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r">AP</td>
<td id="Sx4.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r">AP</td>
</tr>
<tr id="Sx4.T1.1.3.3" class="ltx_tr">
<th id="Sx4.T1.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">InsLoc, <span id="Sx4.T1.1.3.3.1.1" class="ltx_text ltx_font_italic">m</span>=20 (Baseline)</th>
<td id="Sx4.T1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="background-color:#F2F2F2;"><span id="Sx4.T1.1.3.3.2.1" class="ltx_text" style="background-color:#F2F2F2;">38.91</span></td>
<td id="Sx4.T1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">10.84</td>
<td id="Sx4.T1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">16.66</td>
</tr>
<tr id="Sx4.T1.1.4.4" class="ltx_tr">
<th id="Sx4.T1.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">+Poisson Blending, <span id="Sx4.T1.1.4.4.1.1" class="ltx_text ltx_font_italic">m</span>=20</th>
<td id="Sx4.T1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#F2F2F2;"><span id="Sx4.T1.1.4.4.2.1" class="ltx_text" style="background-color:#F2F2F2;">38.49</span></td>
<td id="Sx4.T1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.95</td>
<td id="Sx4.T1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16.98</td>
</tr>
<tr id="Sx4.T1.1.5.5" class="ltx_tr">
<th id="Sx4.T1.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">InsLoc, <span id="Sx4.T1.1.5.5.1.1" class="ltx_text ltx_font_italic">m</span>=45</th>
<td id="Sx4.T1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="background-color:#F2F2F2;"><span id="Sx4.T1.1.5.5.2.1" class="ltx_text" style="background-color:#F2F2F2;">38.86</span></td>
<td id="Sx4.T1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">11.41</td>
<td id="Sx4.T1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">17.45</td>
</tr>
<tr id="Sx4.T1.1.6.6" class="ltx_tr">
<th id="Sx4.T1.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">+Poisson Blending, <span id="Sx4.T1.1.6.6.1.1" class="ltx_text ltx_font_italic">m</span>=45</th>
<td id="Sx4.T1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#F2F2F2;"><span id="Sx4.T1.1.6.6.2.1" class="ltx_text" style="background-color:#F2F2F2;">40.22</span></td>
<td id="Sx4.T1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12.96</td>
<td id="Sx4.T1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">19.23</td>
</tr>
<tr id="Sx4.T1.1.7.7" class="ltx_tr">
<th id="Sx4.T1.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">InsLoc, <span id="Sx4.T1.1.7.7.1.1" class="ltx_text ltx_font_italic">m</span>=70</th>
<td id="Sx4.T1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="background-color:#F2F2F2;"><span id="Sx4.T1.1.7.7.2.1" class="ltx_text" style="background-color:#F2F2F2;">38.01</span></td>
<td id="Sx4.T1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">11.19</td>
<td id="Sx4.T1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">16.96</td>
</tr>
<tr id="Sx4.T1.1.8.8" class="ltx_tr">
<th id="Sx4.T1.1.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">+Poisson Blending, <span id="Sx4.T1.1.8.8.1.1" class="ltx_text ltx_font_italic">m</span>=70</th>
<td id="Sx4.T1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#F2F2F2;"><span id="Sx4.T1.1.8.8.2.1" class="ltx_text ltx_font_bold" style="background-color:#F2F2F2;">40.54</span></td>
<td id="Sx4.T1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx4.T1.1.8.8.3.1" class="ltx_text ltx_font_bold">13.00</span></td>
<td id="Sx4.T1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx4.T1.1.8.8.4.1" class="ltx_text ltx_font_bold">19.73</span></td>
</tr>
<tr id="Sx4.T1.1.9.9" class="ltx_tr">
<th id="Sx4.T1.1.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">InsLoc, <span id="Sx4.T1.1.9.9.1.1" class="ltx_text ltx_font_italic">t</span>=70</th>
<td id="Sx4.T1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="background-color:#F2F2F2;"><span id="Sx4.T1.1.9.9.2.1" class="ltx_text" style="background-color:#F2F2F2;">38.13</span></td>
<td id="Sx4.T1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">12.03</td>
<td id="Sx4.T1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">17.17</td>
</tr>
<tr id="Sx4.T1.1.10.10" class="ltx_tr">
<th id="Sx4.T1.1.10.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">+Poisson Blending, <span id="Sx4.T1.1.10.10.1.1" class="ltx_text ltx_font_italic">t</span>=70</th>
<td id="Sx4.T1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="background-color:#F2F2F2;"><span id="Sx4.T1.1.10.10.2.1" class="ltx_text" style="background-color:#F2F2F2;">39.31</span></td>
<td id="Sx4.T1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">11.97</td>
<td id="Sx4.T1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">18.36</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>In-domain (VOC) vs. out-of-domain (Abstract, VOC-Weather) AP following InsLoc pretraining with and without Poisson blending InsLoc’s query crop, for various values of min % crop <span id="Sx4.T1.5.1" class="ltx_text ltx_font_italic">m</span> and IoU threshold <span id="Sx4.T1.6.2" class="ltx_text ltx_font_italic">t</span>. Note that the use of our Poisson blending strategy results in substantial in-domain and out-of-domain gains, especially at <span id="Sx4.T1.7.3" class="ltx_text ltx_font_italic">m</span>=70.</figcaption>
</figure>
<figure id="Sx4.T2" class="ltx_table">
<table id="Sx4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Sx4.T2.1.1.1" class="ltx_tr">
<th id="Sx4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="Sx4.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<td id="Sx4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx4.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">VOC</span></td>
<td id="Sx4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx4.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">Abstract</span></td>
<td id="Sx4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx4.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">Weather</span></td>
</tr>
<tr id="Sx4.T2.1.2.2" class="ltx_tr">
<th id="Sx4.T2.1.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<td id="Sx4.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r">AP</td>
<td id="Sx4.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r">AP</td>
<td id="Sx4.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r">AP</td>
</tr>
<tr id="Sx4.T2.1.3.3" class="ltx_tr">
<th id="Sx4.T2.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">InsLoc, <span id="Sx4.T2.1.3.3.1.1" class="ltx_text ltx_font_italic">m</span>=20 (Baseline)</th>
<td id="Sx4.T2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="background-color:#F2F2F2;"><span id="Sx4.T2.1.3.3.2.1" class="ltx_text" style="background-color:#F2F2F2;">38.91</span></td>
<td id="Sx4.T2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">10.84</td>
<td id="Sx4.T2.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">16.66</td>
</tr>
<tr id="Sx4.T2.1.4.4" class="ltx_tr">
<th id="Sx4.T2.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">InsLoc, <span id="Sx4.T2.1.4.4.1.1" class="ltx_text ltx_font_italic">m</span>=70</th>
<td id="Sx4.T2.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#F2F2F2;"><span id="Sx4.T2.1.4.4.2.1" class="ltx_text" style="background-color:#F2F2F2;">38.01</span></td>
<td id="Sx4.T2.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.19</td>
<td id="Sx4.T2.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16.96</td>
</tr>
<tr id="Sx4.T2.1.5.5" class="ltx_tr">
<th id="Sx4.T2.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">+Poisson Blending, <span id="Sx4.T2.1.5.5.1.1" class="ltx_text ltx_font_italic">m</span>=70</th>
<td id="Sx4.T2.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="background-color:#F2F2F2;"><span id="Sx4.T2.1.5.5.2.1" class="ltx_text" style="background-color:#F2F2F2;">40.54</span></td>
<td id="Sx4.T2.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">13.00</td>
<td id="Sx4.T2.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="Sx4.T2.1.5.5.4.1" class="ltx_text ltx_font_bold">19.73</span></td>
</tr>
<tr id="Sx4.T2.1.6.6" class="ltx_tr">
<th id="Sx4.T2.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">+Elastic Deformation, <span id="Sx4.T2.1.6.6.1.1" class="ltx_text ltx_font_italic">m</span>=70</th>
<td id="Sx4.T2.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#F2F2F2;"><span id="Sx4.T2.1.6.6.2.1" class="ltx_text" style="background-color:#F2F2F2;">40.94</span></td>
<td id="Sx4.T2.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13.26</td>
<td id="Sx4.T2.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">18.70</td>
</tr>
<tr id="Sx4.T2.1.7.7" class="ltx_tr">
<th id="Sx4.T2.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">+Texture Flattening, <span id="Sx4.T2.1.7.7.1.1" class="ltx_text ltx_font_italic">m</span>=70</th>
<td id="Sx4.T2.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#F2F2F2;"><span id="Sx4.T2.1.7.7.2.1" class="ltx_text" style="background-color:#F2F2F2;">40.45</span></td>
<td id="Sx4.T2.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx4.T2.1.7.7.3.1" class="ltx_text ltx_font_bold">13.57</span></td>
<td id="Sx4.T2.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">19.58</td>
</tr>
<tr id="Sx4.T2.1.8.8" class="ltx_tr">
<th id="Sx4.T2.1.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_tt">+Apply 25% of Time, <span id="Sx4.T2.1.8.8.1.1" class="ltx_text ltx_font_italic">m</span>=70</th>
<td id="Sx4.T2.1.8.8.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt" style="background-color:#F2F2F2;"><span id="Sx4.T2.1.8.8.2.1" class="ltx_text ltx_font_bold" style="background-color:#F2F2F2;">41.64</span></td>
<td id="Sx4.T2.1.8.8.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">12.53</td>
<td id="Sx4.T2.1.8.8.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">19.70</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>In-domain (VOC) vs. out-of-domain (Abstract, VOC-Weather) AP when pretraining InsLoc with shortcut-reducing augmentations at min % crop <span id="Sx4.T2.3.1" class="ltx_text ltx_font_italic">m</span>=70. “Apply 25% of Time” means that we either apply Poisson blending, apply elastic deformation, apply texture flattening, or use the baseline setting, each with probability 25%.</figcaption>
</figure>
<div id="Sx4.SSx1.SSSx2.p2" class="ltx_para">
<p id="Sx4.SSx1.SSSx2.p2.1" class="ltx_p">To first test how augmentations interact with the % crop and IoU strategies, we perform Poisson blending with various values of <span id="Sx4.SSx1.SSSx2.p2.1.1" class="ltx_text ltx_font_italic">m</span> and <span id="Sx4.SSx1.SSSx2.p2.1.2" class="ltx_text ltx_font_italic">t</span>, shown in Table <a href="#Sx4.T1" title="Table 1 ‣ Can shortcut-reducing augmentations make shape strategies more effective? ‣ Pretraining on ImageNet-Subset ‣ Experiments and Analysis ‣ Contrastive View Design Strategies to Enhance Robustness to Domain Shifts in Downstream Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Different from in Fig. <a href="#Sx4.F4" title="Figure 4 ‣ How can we encourage contrastive learning to capture object shape and become more robust to appearance domain shifts? ‣ Pretraining on ImageNet-Subset ‣ Experiments and Analysis ‣ Contrastive View Design Strategies to Enhance Robustness to Domain Shifts in Downstream Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we find that the top domain robustness setting is <span id="Sx4.SSx1.SSSx2.p2.1.3" class="ltx_text ltx_font_italic">m</span>=70 (rather than <span id="Sx4.SSx1.SSSx2.p2.1.4" class="ltx_text ltx_font_italic">t</span>=70) and that <span id="Sx4.SSx1.SSSx2.p2.1.5" class="ltx_text ltx_font_italic">significant</span> out-of-domain gains over the InsLoc baseline are achieved in such setting (+2.16 AP on Abstract, +3.07 AP on VOC-Weather). Moreover, we find that <span id="Sx4.SSx1.SSSx2.p2.1.6" class="ltx_text ltx_font_italic">in-domain</span> AP also increases in this setting (+1.63 AP), indicating that <span id="Sx4.SSx1.SSSx2.p2.1.7" class="ltx_text ltx_font_italic">shortcut augmentations can enable the learning of shape without tradeoffs in-domain</span>. Note also that Poisson blending at <span id="Sx4.SSx1.SSSx2.p2.1.8" class="ltx_text ltx_font_italic">m</span>=20 is not effective in-domain and is less effective out-of-domain. <span id="Sx4.SSx1.SSSx2.p2.1.9" class="ltx_text ltx_font_italic">These results indicate that shortcut-reducing augmentations may not be effective unless the model is encouraged to capture robust object features like shape</span>, which can be done at higher crop %.</p>
</div>
<div id="Sx4.SSx1.SSSx2.p3" class="ltx_para">
<p id="Sx4.SSx1.SSSx2.p3.1" class="ltx_p">We further use the top setting of <span id="Sx4.SSx1.SSSx2.p3.1.1" class="ltx_text ltx_font_italic">m</span>=70 to test each of Poisson blending, elastic deformation, and texture flattening, shown over three trials in Table <a href="#Sx4.T2" title="Table 2 ‣ Can shortcut-reducing augmentations make shape strategies more effective? ‣ Pretraining on ImageNet-Subset ‣ Experiments and Analysis ‣ Contrastive View Design Strategies to Enhance Robustness to Domain Shifts in Downstream Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We find that all augmentations help both in-domain and out-of-domain. In particular, Poisson blending is the top for VOC-Weather (+3.07 AP) and texture flattening is for Abstract (+2.73 AP). We reason that texture flattening simulates the flattened texture of digital Abstract imagery well, while Poisson blending’s random illumination effects are helpful for the texture changes seen with weather. Also shown in Table <a href="#Sx4.T2" title="Table 2 ‣ Can shortcut-reducing augmentations make shape strategies more effective? ‣ Pretraining on ImageNet-Subset ‣ Experiments and Analysis ‣ Contrastive View Design Strategies to Enhance Robustness to Domain Shifts in Downstream Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> is a scenario where we either apply one of the three augmentations or just use the default InsLoc setting, each with probability 25%. We find even more substantial gains in-domain (+2.73 AP) in such scenario. <span id="Sx4.SSx1.SSSx2.p3.1.2" class="ltx_text ltx_font_italic">These results demonstrate that the benefits of creating views with shape-encouraging and shortcut-reducing strategies are not limited to out-of-domain robustness, and these strategies can lead to more robust object features overall.</span> To our knowledge, we are the first to demonstrate such effectiveness of Poisson blending, texture flattening, and elastic deformation as augmentations for contrastive views.</p>
</div>
</section>
</section>
<section id="Sx4.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Pretraining on COCO</h3>

<section id="Sx4.SSx2.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">How do saliency-based view strategies compare on out-of-context and appearance domain shifts?</h4>

<div id="Sx4.SSx2.SSSx1.p1" class="ltx_para">
<p id="Sx4.SSx2.SSSx1.p1.1" class="ltx_p">Intra-image cropping in contrastive learning has been noted to be potentially harmful when pretraining on multi-object images (<span id="Sx4.SSx2.SSSx1.p1.1.1" class="ltx_text ltx_font_italic">e</span>.<span id="Sx4.SSx2.SSSx1.p1.1.2" class="ltx_text ltx_font_italic">g</span>. COCO) <cite class="ltx_cite ltx_citemacro_citep">(Purushwalkam and Gupta <a href="#bib.bib29" title="" class="ltx_ref">2020</a>)</cite>. Approaches have aimed to reduce the impact of contextual bias in such case through using saliency-based object priors, with OA-Crop <cite class="ltx_cite ltx_citemacro_citep">(Mo et al. <a href="#bib.bib25" title="" class="ltx_ref">2021</a>)</cite> and CAST <cite class="ltx_cite ltx_citemacro_citep">(Selvaraju et al. <a href="#bib.bib35" title="" class="ltx_ref">2021</a>)</cite> representing two more robust cropping methods. OA-Crop uses an initial pretraining of MoCo-v2 to gather Contrastive Class Activation Maps and creates a number of object crops for an image from bounding boxes around salient regions from these maps. During training, one randomly selected object crop, rather than the entire image, is used as the source from which to crop views. CAST alternatively ensures that crops overlap with saliency maps gathered with DeepUSPS <cite class="ltx_cite ltx_citemacro_citep">(Nguyen et al. <a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite>, an “unsupervised” saliency detector (though it still uses ImageNet-supervised weights).</p>
</div>
<figure id="Sx4.F5" class="ltx_figure"><img src="/html/2212.04613/assets/x5.png" id="Sx4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="180" height="33" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>CAST <cite class="ltx_cite ltx_citemacro_citep">(Selvaraju et al. <a href="#bib.bib35" title="" class="ltx_ref">2021</a>)</cite> vs. DeepUSPS-Tightened crops. The blue box shows a randomly chosen object crop that is the source for query and key views. Green and orange boxes show example query and key crops.</figcaption>
</figure>
<figure id="Sx4.T3" class="ltx_table">
<table id="Sx4.T3.3.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Sx4.T3.3.3.4.1" class="ltx_tr">
<td id="Sx4.T3.3.3.4.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="Sx4.T3.3.3.4.1.1.1" class="ltx_text ltx_font_bold">Cropping Method</span></td>
<td id="Sx4.T3.3.3.4.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx4.T3.3.3.4.1.2.1" class="ltx_text ltx_font_bold">COCO</span></td>
<td id="Sx4.T3.3.3.4.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx4.T3.3.3.4.1.3.1" class="ltx_text ltx_font_bold">UnRel</span></td>
<td id="Sx4.T3.3.3.4.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx4.T3.3.3.4.1.4.1" class="ltx_text ltx_font_bold">Weather</span></td>
</tr>
<tr id="Sx4.T3.3.3.3" class="ltx_tr">
<td id="Sx4.T3.3.3.3.4" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="Sx4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r">AP<sub id="Sx4.T3.1.1.1.1.1" class="ltx_sub"><span id="Sx4.T3.1.1.1.1.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</td>
<td id="Sx4.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r">AP<sub id="Sx4.T3.2.2.2.2.1" class="ltx_sub"><span id="Sx4.T3.2.2.2.2.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</td>
<td id="Sx4.T3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r">AP<sub id="Sx4.T3.3.3.3.3.1" class="ltx_sub"><span id="Sx4.T3.3.3.3.3.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</td>
</tr>
<tr id="Sx4.T3.3.3.5.2" class="ltx_tr">
<td id="Sx4.T3.3.3.5.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt">InsLoc</td>
<td id="Sx4.T3.3.3.5.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="background-color:#F2F2F2;"><span id="Sx4.T3.3.3.5.2.2.1" class="ltx_text" style="background-color:#F2F2F2;">26.16</span></td>
<td id="Sx4.T3.3.3.5.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">22.60</td>
<td id="Sx4.T3.3.3.5.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">10.53</td>
</tr>
<tr id="Sx4.T3.3.3.6.3" class="ltx_tr">
<td id="Sx4.T3.3.3.6.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt">+OACrop</td>
<td id="Sx4.T3.3.3.6.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="background-color:#F2F2F2;"><span id="Sx4.T3.3.3.6.3.2.1" class="ltx_text" style="background-color:#F2F2F2;">25.55</span></td>
<td id="Sx4.T3.3.3.6.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">23.10</td>
<td id="Sx4.T3.3.3.6.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">9.82</td>
</tr>
<tr id="Sx4.T3.3.3.7.4" class="ltx_tr">
<td id="Sx4.T3.3.3.7.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">+CAST</td>
<td id="Sx4.T3.3.3.7.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#F2F2F2;"><span id="Sx4.T3.3.3.7.4.2.1" class="ltx_text" style="background-color:#F2F2F2;">26.70</span></td>
<td id="Sx4.T3.3.3.7.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">22.36</td>
<td id="Sx4.T3.3.3.7.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.58</td>
</tr>
<tr id="Sx4.T3.3.3.8.5" class="ltx_tr">
<td id="Sx4.T3.3.3.8.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">+DeepUSPS-Tight, <span id="Sx4.T3.3.3.8.5.1.1" class="ltx_text ltx_font_italic">m</span>=8</td>
<td id="Sx4.T3.3.3.8.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#F2F2F2;"><span id="Sx4.T3.3.3.8.5.2.1" class="ltx_text" style="background-color:#F2F2F2;">27.76</span></td>
<td id="Sx4.T3.3.3.8.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">24.59</td>
<td id="Sx4.T3.3.3.8.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12.20</td>
</tr>
<tr id="Sx4.T3.3.3.9.6" class="ltx_tr">
<td id="Sx4.T3.3.3.9.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">+DeepUSPS-Tight, <span id="Sx4.T3.3.3.9.6.1.1" class="ltx_text ltx_font_italic">m</span>=20</td>
<td id="Sx4.T3.3.3.9.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="background-color:#F2F2F2;"><span id="Sx4.T3.3.3.9.6.2.1" class="ltx_text ltx_font_bold" style="background-color:#F2F2F2;">28.39</span></td>
<td id="Sx4.T3.3.3.9.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="Sx4.T3.3.3.9.6.3.1" class="ltx_text ltx_font_bold">26.11</span></td>
<td id="Sx4.T3.3.3.9.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="Sx4.T3.3.3.9.6.4.1" class="ltx_text ltx_font_bold">12.33</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>In-domain (COCO) vs. out-of-domain (UnRel, COCO-Weather) AP<sub id="Sx4.T3.9.1" class="ltx_sub"><span id="Sx4.T3.9.1.1" class="ltx_text ltx_font_italic">50</span></sub> of saliency strategies within InsLoc. For InsLoc, OACrop, and CAST, results are with the default or optimal cropping values if reported (<span id="Sx4.T3.10.2" class="ltx_text ltx_font_italic">m</span>=20, 8, and 20 respectively). DeepUSPS-Tightened is tested at <span id="Sx4.T3.11.3" class="ltx_text ltx_font_italic">m</span>=8 and 20.</figcaption>
</figure>
<div id="Sx4.SSx2.SSSx1.p2" class="ltx_para">
<p id="Sx4.SSx2.SSSx1.p2.1" class="ltx_p">Notably, these methods have not been evaluated in out-of-domain detection, so we fill in this gap by comparing them within InsLoc. We also consider a hybrid approach called <span id="Sx4.SSx2.SSSx1.p2.1.1" class="ltx_text ltx_font_italic">DeepUSPS-Tightened</span> crops, where DeepUSPS saliency maps, rather than ContraCAMs, are used to create object crops like OA-Crop, as we observe DeepUSPS’s maps are higher quality. We emphasize that the difference between between CAST and DeepUSPS-Tightened crops is that maps are used with CAST to ensure that crops <span id="Sx4.SSx2.SSSx1.p2.1.2" class="ltx_text ltx_font_italic">overlap</span> with objects, rather than to <span id="Sx4.SSx2.SSSx1.p2.1.3" class="ltx_text ltx_font_italic">reduce</span> background area and <span id="Sx4.SSx2.SSSx1.p2.1.4" class="ltx_text ltx_font_italic">tighten</span> crops to objects, which is the goal of the hybrid that we propose. We exemplify these differences in Fig. <a href="#Sx4.F5" title="Figure 5 ‣ How do saliency-based view strategies compare on out-of-context and appearance domain shifts? ‣ Pretraining on COCO ‣ Experiments and Analysis ‣ Contrastive View Design Strategies to Enhance Robustness to Domain Shifts in Downstream Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div id="Sx4.SSx2.SSSx1.p3" class="ltx_para">
<p id="Sx4.SSx2.SSSx1.p3.7" class="ltx_p">In Table <a href="#Sx4.T3" title="Table 3 ‣ How do saliency-based view strategies compare on out-of-context and appearance domain shifts? ‣ Pretraining on COCO ‣ Experiments and Analysis ‣ Contrastive View Design Strategies to Enhance Robustness to Domain Shifts in Downstream Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we compare each strategy following COCO pretraining and finetuning in terms of AP<sub id="Sx4.SSx2.SSSx1.p3.7.1" class="ltx_sub"><span id="Sx4.SSx2.SSSx1.p3.7.1.1" class="ltx_text ltx_font_italic">50</span></sub> on COCO, the out-of-context UnRel, and the appearance-shifted COCO-Weather. We use the saliency maps provided by <cite class="ltx_cite ltx_citemacro_citep">(Mo et al. <a href="#bib.bib25" title="" class="ltx_ref">2021</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citep">(Selvaraju et al. <a href="#bib.bib35" title="" class="ltx_ref">2021</a>)</cite>, as well as their top reported min % crop (or default if not reported). We also test DeepUSPS-Tightened at <span id="Sx4.SSx2.SSSx1.p3.7.2" class="ltx_text ltx_font_italic">m</span>=8 and <span id="Sx4.SSx2.SSSx1.p3.7.3" class="ltx_text ltx_font_italic">m</span>=20. A first observation is that there is a <span id="Sx4.SSx2.SSSx1.p3.7.4" class="ltx_text ltx_font_italic">significant</span> domain gap (-3.56 AP<sub id="Sx4.SSx2.SSSx1.p3.7.5" class="ltx_sub"><span id="Sx4.SSx2.SSSx1.p3.7.5.1" class="ltx_text ltx_font_italic">50</span></sub>) between COCO and UnRel without incorporating any saliency strategy, <span id="Sx4.SSx2.SSSx1.p3.7.6" class="ltx_text ltx_font_italic">indicating contextual bias in downstream object detection</span>. The OACrop strategy improves AP<sub id="Sx4.SSx2.SSSx1.p3.7.7" class="ltx_sub"><span id="Sx4.SSx2.SSSx1.p3.7.7.1" class="ltx_text ltx_font_italic">50</span></sub> on UnRel, while CAST does not. Alternatively, CAST improves on COCO and COCO-Weather (slightly) while OA-Crop does not. Notably, <em id="Sx4.SSx2.SSSx1.p3.7.8" class="ltx_emph ltx_font_italic">our hybrid DeepUSPS-Tightened leads to the top gains</em> vs. InsLoc (+2.23 AP<sub id="Sx4.SSx2.SSSx1.p3.7.9" class="ltx_sub"><span id="Sx4.SSx2.SSSx1.p3.7.9.1" class="ltx_text ltx_font_italic">50</span></sub> on COCO, +3.51 AP<sub id="Sx4.SSx2.SSSx1.p3.7.10" class="ltx_sub"><span id="Sx4.SSx2.SSSx1.p3.7.10.1" class="ltx_text ltx_font_italic">50</span></sub> on UnRel, +1.80 AP<sub id="Sx4.SSx2.SSSx1.p3.7.11" class="ltx_sub"><span id="Sx4.SSx2.SSSx1.p3.7.11.1" class="ltx_text ltx_font_italic">50</span></sub> on COCO-Weather). The domain gap is much smaller than InsLoc as well (-2.28 vs. -3.56 AP<sub id="Sx4.SSx2.SSSx1.p3.7.12" class="ltx_sub"><span id="Sx4.SSx2.SSSx1.p3.7.12.1" class="ltx_text ltx_font_italic">50</span></sub>).</p>
</div>
<figure id="Sx4.F6" class="ltx_figure"><img src="/html/2212.04613/assets/x6.png" id="Sx4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="193" height="48" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Our proposed texture flattening of non-salient regions (TFNS) augmentation vs. full image texture flattening.</figcaption>
</figure>
<figure id="Sx4.T4" class="ltx_table">
<table id="Sx4.T4.3.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Sx4.T4.3.3.4.1" class="ltx_tr">
<td id="Sx4.T4.3.3.4.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="Sx4.T4.3.3.4.1.1.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="Sx4.T4.3.3.4.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx4.T4.3.3.4.1.2.1" class="ltx_text ltx_font_bold">COCO</span></td>
<td id="Sx4.T4.3.3.4.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx4.T4.3.3.4.1.3.1" class="ltx_text ltx_font_bold">UnRel</span></td>
<td id="Sx4.T4.3.3.4.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx4.T4.3.3.4.1.4.1" class="ltx_text ltx_font_bold">Weather</span></td>
</tr>
<tr id="Sx4.T4.3.3.3" class="ltx_tr">
<td id="Sx4.T4.3.3.3.4" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="Sx4.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r">AP<sub id="Sx4.T4.1.1.1.1.1" class="ltx_sub"><span id="Sx4.T4.1.1.1.1.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</td>
<td id="Sx4.T4.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r">AP<sub id="Sx4.T4.2.2.2.2.1" class="ltx_sub"><span id="Sx4.T4.2.2.2.2.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</td>
<td id="Sx4.T4.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r">AP<sub id="Sx4.T4.3.3.3.3.1" class="ltx_sub"><span id="Sx4.T4.3.3.3.3.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</td>
</tr>
<tr id="Sx4.T4.3.3.5.2" class="ltx_tr">
<td id="Sx4.T4.3.3.5.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt">InsLoc</td>
<td id="Sx4.T4.3.3.5.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="background-color:#F2F2F2;"><span id="Sx4.T4.3.3.5.2.2.1" class="ltx_text" style="background-color:#F2F2F2;">26.16</span></td>
<td id="Sx4.T4.3.3.5.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">22.60</td>
<td id="Sx4.T4.3.3.5.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">10.53</td>
</tr>
<tr id="Sx4.T4.3.3.6.3" class="ltx_tr">
<td id="Sx4.T4.3.3.6.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt">+DeepUSPS-Tight/TF</td>
<td id="Sx4.T4.3.3.6.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="background-color:#F2F2F2;"><span id="Sx4.T4.3.3.6.3.2.1" class="ltx_text" style="background-color:#F2F2F2;">27.73</span></td>
<td id="Sx4.T4.3.3.6.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">25.27</td>
<td id="Sx4.T4.3.3.6.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">11.94</td>
</tr>
<tr id="Sx4.T4.3.3.7.4" class="ltx_tr">
<td id="Sx4.T4.3.3.7.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">+DeepUSPS-Tight/TFNS</td>
<td id="Sx4.T4.3.3.7.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="background-color:#F2F2F2;"><span id="Sx4.T4.3.3.7.4.2.1" class="ltx_text ltx_font_bold" style="background-color:#F2F2F2;">28.74</span></td>
<td id="Sx4.T4.3.3.7.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="Sx4.T4.3.3.7.4.3.1" class="ltx_text ltx_font_bold">25.71</span></td>
<td id="Sx4.T4.3.3.7.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="Sx4.T4.3.3.7.4.4.1" class="ltx_text ltx_font_bold">12.44</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>In-domain (COCO) vs. out-of-domain (UnRel, COCO-Weather) AP<sub id="Sx4.T4.9.1" class="ltx_sub"><span id="Sx4.T4.9.1.1" class="ltx_text ltx_font_italic">50</span></sub> for full image (TF) vs. non-salient region texture flattening (TFNS) of InsLoc query crops in combination with DeepUSPS-Tightened strategy (at <span id="Sx4.T4.10.2" class="ltx_text ltx_font_italic">m</span>=70), with respect to InsLoc baseline (at <span id="Sx4.T4.11.3" class="ltx_text ltx_font_italic">m</span>=20). </figcaption>
</figure>
<figure id="Sx4.T5" class="ltx_table">
<table id="Sx4.T5.3.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Sx4.T5.3.3.4.1" class="ltx_tr">
<td id="Sx4.T5.3.3.4.1.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t"></td>
<td id="Sx4.T5.3.3.4.1.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="Sx4.T5.3.3.4.1.3" class="ltx_td ltx_border_rr ltx_border_t"></td>
<td id="Sx4.T5.3.3.4.1.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" colspan="3"><span id="Sx4.T5.3.3.4.1.4.1" class="ltx_text ltx_font_italic">VOC Finetuning</span></td>
<td id="Sx4.T5.3.3.4.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span id="Sx4.T5.3.3.4.1.5.1" class="ltx_text ltx_font_italic">COCO Finetuning</span></td>
</tr>
<tr id="Sx4.T5.3.3.5.2" class="ltx_tr">
<td id="Sx4.T5.3.3.5.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span id="Sx4.T5.3.3.5.2.1.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="Sx4.T5.3.3.5.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="Sx4.T5.3.3.5.2.2.1" class="ltx_text ltx_font_bold">Crop %</span></td>
<td id="Sx4.T5.3.3.5.2.3" class="ltx_td ltx_align_center ltx_border_rr"><span id="Sx4.T5.3.3.5.2.3.1" class="ltx_text ltx_font_bold">Augmentations</span></td>
<td id="Sx4.T5.3.3.5.2.4" class="ltx_td ltx_align_center"><span id="Sx4.T5.3.3.5.2.4.1" class="ltx_text ltx_font_bold">VOC</span></td>
<td id="Sx4.T5.3.3.5.2.5" class="ltx_td ltx_align_center"><span id="Sx4.T5.3.3.5.2.5.1" class="ltx_text ltx_font_bold">Abstract</span></td>
<td id="Sx4.T5.3.3.5.2.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="Sx4.T5.3.3.5.2.6.1" class="ltx_text ltx_font_bold">Weather</span></td>
<td id="Sx4.T5.3.3.5.2.7" class="ltx_td ltx_align_center"><span id="Sx4.T5.3.3.5.2.7.1" class="ltx_text ltx_font_bold">COCO</span></td>
<td id="Sx4.T5.3.3.5.2.8" class="ltx_td ltx_align_center"><span id="Sx4.T5.3.3.5.2.8.1" class="ltx_text ltx_font_bold">UnRel</span></td>
<td id="Sx4.T5.3.3.5.2.9" class="ltx_td ltx_align_center ltx_border_r"><span id="Sx4.T5.3.3.5.2.9.1" class="ltx_text ltx_font_bold">Weather</span></td>
</tr>
<tr id="Sx4.T5.3.3.3" class="ltx_tr">
<td id="Sx4.T5.3.3.3.4" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="Sx4.T5.3.3.3.5" class="ltx_td ltx_border_r"></td>
<td id="Sx4.T5.3.3.3.6" class="ltx_td ltx_border_rr"></td>
<td id="Sx4.T5.3.3.3.7" class="ltx_td ltx_align_center">AP</td>
<td id="Sx4.T5.3.3.3.8" class="ltx_td ltx_align_center">AP</td>
<td id="Sx4.T5.3.3.3.9" class="ltx_td ltx_align_center ltx_border_rr">AP</td>
<td id="Sx4.T5.1.1.1.1" class="ltx_td ltx_align_center">AP<sub id="Sx4.T5.1.1.1.1.1" class="ltx_sub"><span id="Sx4.T5.1.1.1.1.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</td>
<td id="Sx4.T5.2.2.2.2" class="ltx_td ltx_align_center">AP<sub id="Sx4.T5.2.2.2.2.1" class="ltx_sub"><span id="Sx4.T5.2.2.2.2.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</td>
<td id="Sx4.T5.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r">AP<sub id="Sx4.T5.3.3.3.3.1" class="ltx_sub"><span id="Sx4.T5.3.3.3.3.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</td>
</tr>
<tr id="Sx4.T5.3.3.6.3" class="ltx_tr">
<td id="Sx4.T5.3.3.6.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt">InsLoc</td>
<td id="Sx4.T5.3.3.6.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">20-100</td>
<td id="Sx4.T5.3.3.6.3.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">Default</td>
<td id="Sx4.T5.3.3.6.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="background-color:#F2F2F2;"><span id="Sx4.T5.3.3.6.3.4.1" class="ltx_text" style="background-color:#F2F2F2;">39.79</span></td>
<td id="Sx4.T5.3.3.6.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">11.31</td>
<td id="Sx4.T5.3.3.6.3.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">18.47</td>
<td id="Sx4.T5.3.3.6.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="background-color:#F2F2F2;"><span id="Sx4.T5.3.3.6.3.7.1" class="ltx_text" style="background-color:#F2F2F2;">26.16</span></td>
<td id="Sx4.T5.3.3.6.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">22.60</td>
<td id="Sx4.T5.3.3.6.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">10.53</td>
</tr>
<tr id="Sx4.T5.3.3.7.4" class="ltx_tr">
<td id="Sx4.T5.3.3.7.4.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="Sx4.T5.3.3.7.4.2" class="ltx_td ltx_align_center ltx_border_r">70-100</td>
<td id="Sx4.T5.3.3.7.4.3" class="ltx_td ltx_align_center ltx_border_rr">Default</td>
<td id="Sx4.T5.3.3.7.4.4" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#F2F2F2;"><span id="Sx4.T5.3.3.7.4.4.1" class="ltx_text" style="background-color:#F2F2F2;">38.45</span></td>
<td id="Sx4.T5.3.3.7.4.5" class="ltx_td ltx_align_center ltx_border_r">12.19</td>
<td id="Sx4.T5.3.3.7.4.6" class="ltx_td ltx_align_center ltx_border_rr">17.37</td>
<td id="Sx4.T5.3.3.7.4.7" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#F2F2F2;"><span id="Sx4.T5.3.3.7.4.7.1" class="ltx_text" style="background-color:#F2F2F2;">24.70</span></td>
<td id="Sx4.T5.3.3.7.4.8" class="ltx_td ltx_align_center ltx_border_r">20.87</td>
<td id="Sx4.T5.3.3.7.4.9" class="ltx_td ltx_align_center ltx_border_r">9.56</td>
</tr>
<tr id="Sx4.T5.3.3.8.5" class="ltx_tr">
<td id="Sx4.T5.3.3.8.5.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="Sx4.T5.3.3.8.5.2" class="ltx_td ltx_align_center ltx_border_r">20-100</td>
<td id="Sx4.T5.3.3.8.5.3" class="ltx_td ltx_align_center ltx_border_rr">+TFNS</td>
<td id="Sx4.T5.3.3.8.5.4" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#F2F2F2;"><span id="Sx4.T5.3.3.8.5.4.1" class="ltx_text ltx_framed ltx_framed_underline" style="background-color:#F2F2F2;">40.43</span></td>
<td id="Sx4.T5.3.3.8.5.5" class="ltx_td ltx_align_center ltx_border_r">12.45</td>
<td id="Sx4.T5.3.3.8.5.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="Sx4.T5.3.3.8.5.6.1" class="ltx_text ltx_framed ltx_framed_underline">19.42</span></td>
<td id="Sx4.T5.3.3.8.5.7" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#F2F2F2;"><span id="Sx4.T5.3.3.8.5.7.1" class="ltx_text" style="background-color:#F2F2F2;">26.94</span></td>
<td id="Sx4.T5.3.3.8.5.8" class="ltx_td ltx_align_center ltx_border_r">23.00</td>
<td id="Sx4.T5.3.3.8.5.9" class="ltx_td ltx_align_center ltx_border_r"><span id="Sx4.T5.3.3.8.5.9.1" class="ltx_text ltx_framed ltx_framed_underline">11.37</span></td>
</tr>
<tr id="Sx4.T5.3.3.9.6" class="ltx_tr">
<td id="Sx4.T5.3.3.9.6.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="Sx4.T5.3.3.9.6.2" class="ltx_td ltx_align_center ltx_border_r">70-100</td>
<td id="Sx4.T5.3.3.9.6.3" class="ltx_td ltx_align_center ltx_border_rr">+TFNS</td>
<td id="Sx4.T5.3.3.9.6.4" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#F2F2F2;"><span id="Sx4.T5.3.3.9.6.4.1" class="ltx_text" style="background-color:#F2F2F2;">40.09</span></td>
<td id="Sx4.T5.3.3.9.6.5" class="ltx_td ltx_align_center ltx_border_r"><span id="Sx4.T5.3.3.9.6.5.1" class="ltx_text ltx_framed ltx_framed_underline">12.56</span></td>
<td id="Sx4.T5.3.3.9.6.6" class="ltx_td ltx_align_center ltx_border_rr">19.05</td>
<td id="Sx4.T5.3.3.9.6.7" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#F2F2F2;"><span id="Sx4.T5.3.3.9.6.7.1" class="ltx_text ltx_framed ltx_framed_underline" style="background-color:#F2F2F2;">27.12</span></td>
<td id="Sx4.T5.3.3.9.6.8" class="ltx_td ltx_align_center ltx_border_r"><span id="Sx4.T5.3.3.9.6.8.1" class="ltx_text ltx_framed ltx_framed_underline">23.03</span></td>
<td id="Sx4.T5.3.3.9.6.9" class="ltx_td ltx_align_center ltx_border_r">11.25</td>
</tr>
<tr id="Sx4.T5.3.3.10.7" class="ltx_tr">
<td id="Sx4.T5.3.3.10.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">InsLoc</td>
<td id="Sx4.T5.3.3.10.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8-100</td>
<td id="Sx4.T5.3.3.10.7.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">Default</td>
<td id="Sx4.T5.3.3.10.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#F2F2F2;"><span id="Sx4.T5.3.3.10.7.4.1" class="ltx_text" style="background-color:#F2F2F2;">40.87</span></td>
<td id="Sx4.T5.3.3.10.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.03</td>
<td id="Sx4.T5.3.3.10.7.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">19.85</td>
<td id="Sx4.T5.3.3.10.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#F2F2F2;"><span id="Sx4.T5.3.3.10.7.7.1" class="ltx_text" style="background-color:#F2F2F2;">27.76</span></td>
<td id="Sx4.T5.3.3.10.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">24.59</td>
<td id="Sx4.T5.3.3.10.7.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12.20</td>
</tr>
<tr id="Sx4.T5.3.3.11.8" class="ltx_tr">
<td id="Sx4.T5.3.3.11.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">+DeepUSPS-Tightened</td>
<td id="Sx4.T5.3.3.11.8.2" class="ltx_td ltx_align_center ltx_border_r">20-100</td>
<td id="Sx4.T5.3.3.11.8.3" class="ltx_td ltx_align_center ltx_border_rr">Default</td>
<td id="Sx4.T5.3.3.11.8.4" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#F2F2F2;"><span id="Sx4.T5.3.3.11.8.4.1" class="ltx_text" style="background-color:#F2F2F2;">41.56</span></td>
<td id="Sx4.T5.3.3.11.8.5" class="ltx_td ltx_align_center ltx_border_r">11.78</td>
<td id="Sx4.T5.3.3.11.8.6" class="ltx_td ltx_align_center ltx_border_rr">19.86</td>
<td id="Sx4.T5.3.3.11.8.7" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#F2F2F2;"><span id="Sx4.T5.3.3.11.8.7.1" class="ltx_text" style="background-color:#F2F2F2;">28.39</span></td>
<td id="Sx4.T5.3.3.11.8.8" class="ltx_td ltx_align_center ltx_border_r">26.11</td>
<td id="Sx4.T5.3.3.11.8.9" class="ltx_td ltx_align_center ltx_border_r">12.33</td>
</tr>
<tr id="Sx4.T5.3.3.12.9" class="ltx_tr">
<td id="Sx4.T5.3.3.12.9.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="Sx4.T5.3.3.12.9.2" class="ltx_td ltx_align_center ltx_border_r">70-100</td>
<td id="Sx4.T5.3.3.12.9.3" class="ltx_td ltx_align_center ltx_border_rr">Default</td>
<td id="Sx4.T5.3.3.12.9.4" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#F2F2F2;"><span id="Sx4.T5.3.3.12.9.4.1" class="ltx_text" style="background-color:#F2F2F2;">39.30</span></td>
<td id="Sx4.T5.3.3.12.9.5" class="ltx_td ltx_align_center ltx_border_r">12.11</td>
<td id="Sx4.T5.3.3.12.9.6" class="ltx_td ltx_align_center ltx_border_rr">17.69</td>
<td id="Sx4.T5.3.3.12.9.7" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#F2F2F2;"><span id="Sx4.T5.3.3.12.9.7.1" class="ltx_text" style="background-color:#F2F2F2;">25.46</span></td>
<td id="Sx4.T5.3.3.12.9.8" class="ltx_td ltx_align_center ltx_border_r">22.66</td>
<td id="Sx4.T5.3.3.12.9.9" class="ltx_td ltx_align_center ltx_border_r">10.67</td>
</tr>
<tr id="Sx4.T5.3.3.13.10" class="ltx_tr">
<td id="Sx4.T5.3.3.13.10.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="Sx4.T5.3.3.13.10.2" class="ltx_td ltx_align_center ltx_border_r">8-100</td>
<td id="Sx4.T5.3.3.13.10.3" class="ltx_td ltx_align_center ltx_border_rr">+TFNS</td>
<td id="Sx4.T5.3.3.13.10.4" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#F2F2F2;"><span id="Sx4.T5.3.3.13.10.4.1" class="ltx_text" style="background-color:#F2F2F2;">41.08</span></td>
<td id="Sx4.T5.3.3.13.10.5" class="ltx_td ltx_align_center ltx_border_r">12.48</td>
<td id="Sx4.T5.3.3.13.10.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="Sx4.T5.3.3.13.10.6.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">20.64</span></td>
<td id="Sx4.T5.3.3.13.10.7" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#F2F2F2;"><span id="Sx4.T5.3.3.13.10.7.1" class="ltx_text" style="background-color:#F2F2F2;">28.44</span></td>
<td id="Sx4.T5.3.3.13.10.8" class="ltx_td ltx_align_center ltx_border_r"><span id="Sx4.T5.3.3.13.10.8.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">27.31</span></td>
<td id="Sx4.T5.3.3.13.10.9" class="ltx_td ltx_align_center ltx_border_r">12.27</td>
</tr>
<tr id="Sx4.T5.3.3.14.11" class="ltx_tr">
<td id="Sx4.T5.3.3.14.11.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="Sx4.T5.3.3.14.11.2" class="ltx_td ltx_align_center ltx_border_r">20-100</td>
<td id="Sx4.T5.3.3.14.11.3" class="ltx_td ltx_align_center ltx_border_rr">+TFNS</td>
<td id="Sx4.T5.3.3.14.11.4" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#F2F2F2;"><span id="Sx4.T5.3.3.14.11.4.1" class="ltx_text" style="background-color:#F2F2F2;">41.70</span></td>
<td id="Sx4.T5.3.3.14.11.5" class="ltx_td ltx_align_center ltx_border_r">12.50</td>
<td id="Sx4.T5.3.3.14.11.6" class="ltx_td ltx_align_center ltx_border_rr">19.54</td>
<td id="Sx4.T5.3.3.14.11.7" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#F2F2F2;"><span id="Sx4.T5.3.3.14.11.7.1" class="ltx_text" style="background-color:#F2F2F2;">28.66</span></td>
<td id="Sx4.T5.3.3.14.11.8" class="ltx_td ltx_align_center ltx_border_r">24.93</td>
<td id="Sx4.T5.3.3.14.11.9" class="ltx_td ltx_align_center ltx_border_r">12.12</td>
</tr>
<tr id="Sx4.T5.3.3.15.12" class="ltx_tr">
<td id="Sx4.T5.3.3.15.12.1" class="ltx_td ltx_border_b ltx_border_l ltx_border_r"></td>
<td id="Sx4.T5.3.3.15.12.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">70-100</td>
<td id="Sx4.T5.3.3.15.12.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr">+TFNS</td>
<td id="Sx4.T5.3.3.15.12.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="background-color:#F2F2F2;"><span id="Sx4.T5.3.3.15.12.4.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="background-color:#F2F2F2;">41.80</span></td>
<td id="Sx4.T5.3.3.15.12.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="Sx4.T5.3.3.15.12.5.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">13.18</span></td>
<td id="Sx4.T5.3.3.15.12.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr">20.46</td>
<td id="Sx4.T5.3.3.15.12.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="background-color:#F2F2F2;"><span id="Sx4.T5.3.3.15.12.7.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="background-color:#F2F2F2;">28.74</span></td>
<td id="Sx4.T5.3.3.15.12.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">25.71</td>
<td id="Sx4.T5.3.3.15.12.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="Sx4.T5.3.3.15.12.9.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">12.44</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>InsLoc+DeepUSPS-Tightened vs. InsLoc pretrained at various crop % and with/without texture flattening of non-salient regions (TFNS), using 24k schedule for both VOC and COCO finetuning. Underlined=top per method, bold=best overall. </figcaption>
</figure>
<div id="Sx4.SSx2.SSSx1.p4" class="ltx_para">
<p id="Sx4.SSx2.SSSx1.p4.1" class="ltx_p">We infer that the high quality of DeepUSPS maps along with the <span id="Sx4.SSx2.SSSx1.p4.1.1" class="ltx_text ltx_font_italic">removal</span> of background through cropping are potential reasons for DeepUSPS-Tightened to have the top AP<sub id="Sx4.SSx2.SSSx1.p4.1.2" class="ltx_sub"><span id="Sx4.SSx2.SSSx1.p4.1.2.1" class="ltx_text ltx_font_italic">50</span></sub>. In detection pretraining, we reason that it may not be important to include background in crops, and that <span id="Sx4.SSx2.SSSx1.p4.1.3" class="ltx_text ltx_font_italic">strategic use of a quality saliency detector can enable detectors to be less biased and more robust out-of-context.</span></p>
</div>
</section>
<section id="Sx4.SSx2.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Can shape and shortcut-reducing strategies further help robustness with saliency strategies?</h4>

<div id="Sx4.SSx2.SSSx2.p1" class="ltx_para">
<p id="Sx4.SSx2.SSSx2.p1.1" class="ltx_p">While tightened crops remove many non-salient pixels, some remain inside crops. The pretext task can thus be solved by matching positives based on the background, rather than objects, which can hurt out-of-context robustness. Moreover, aggressive cropping of even object-based crops can still lead to representations that do not capture shape well, hurting appearance shift robustness. Inspired by the ImageNet-Subset experiments, we also explore shape and appearance strategies with COCO pretraining. In particular, we consider <span id="Sx4.SSx2.SSSx2.p1.1.1" class="ltx_text ltx_font_italic">m</span>=70 as a shape strategy and texture flattening as a shortcut-reducing strategy. Since we have saliency maps, we also propose another strategy: texture flattening of non-salient regions only (TFNS). We specifically propose to distort the background (non-salient regions marked by DeepUSPS) of one InsLoc view to encourage background invariance between views during pretraining. We reason that TFNS may be effective for out-of-context robustness as shortcuts may come more significantly from the background rather than salient object regions. We illustrate this proposed augmentation in Fig. <a href="#Sx4.F6" title="Figure 6 ‣ How do saliency-based view strategies compare on out-of-context and appearance domain shifts? ‣ Pretraining on COCO ‣ Experiments and Analysis ‣ Contrastive View Design Strategies to Enhance Robustness to Domain Shifts in Downstream Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div id="Sx4.SSx2.SSSx2.p2" class="ltx_para">
<p id="Sx4.SSx2.SSSx2.p2.1" class="ltx_p">In Table <a href="#Sx4.T4" title="Table 4 ‣ How do saliency-based view strategies compare on out-of-context and appearance domain shifts? ‣ Pretraining on COCO ‣ Experiments and Analysis ‣ Contrastive View Design Strategies to Enhance Robustness to Domain Shifts in Downstream Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we show results with <span id="Sx4.SSx2.SSSx2.p2.1.1" class="ltx_text ltx_font_italic">m</span>=70, DeepUSPS-Tightened crops, and texture flattening (full and just non-salient regions). We observe that both strategies are effective, and TFNS leads to larger gains across all sets. We reason that learning some level of texture, even with shape, is important, and TFNS preserves important texture (of objects) while removing unimportant texture (high-frequency shortcuts which come from the background).</p>
</div>
<div id="Sx4.SSx2.SSSx2.p3" class="ltx_para">
<p id="Sx4.SSx2.SSSx2.p3.2" class="ltx_p">For a more thorough evaluation of TFNS and DeepUSPS-Tightened, in Table <a href="#Sx4.T5" title="Table 5 ‣ How do saliency-based view strategies compare on out-of-context and appearance domain shifts? ‣ Pretraining on COCO ‣ Experiments and Analysis ‣ Contrastive View Design Strategies to Enhance Robustness to Domain Shifts in Downstream Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we present results of InsLoc with these strategies and various values of <span id="Sx4.SSx2.SSSx2.p3.2.1" class="ltx_text ltx_font_italic">m</span>, in finetuning on both VOC and COCO. We find that the combination of DeepUSPS-Tightened, <span id="Sx4.SSx2.SSSx2.p3.2.2" class="ltx_text ltx_font_italic">m</span>=70, and TFNS results in the top AP on VOC and Abstract and the top AP<sub id="Sx4.SSx2.SSSx2.p3.2.3" class="ltx_sub"><span id="Sx4.SSx2.SSSx2.p3.2.3.1" class="ltx_text ltx_font_italic">50</span></sub> on COCO and COCO-Weather. <span id="Sx4.SSx2.SSSx2.p3.2.4" class="ltx_text ltx_font_italic">These results illustrate the high overall effectiveness of combining shape, shortcut-reducing, and saliency strategies.</span> We also observe that the top performance on UnRel (27.31 AP<sub id="Sx4.SSx2.SSSx2.p3.2.5" class="ltx_sub"><span id="Sx4.SSx2.SSSx2.p3.2.5.1" class="ltx_text ltx_font_italic">50</span></sub>) is achieved at <span id="Sx4.SSx2.SSSx2.p3.2.6" class="ltx_text ltx_font_italic">m</span>=8, along with TFNS and DeepUSPS-Tightened. We reason that since context is a “natural” domain shift, where object texture is preserved, shape may less useful, and aggressive cropping at <span id="Sx4.SSx2.SSSx2.p3.2.7" class="ltx_text ltx_font_italic">m</span>=8 of <span id="Sx4.SSx2.SSSx2.p3.2.8" class="ltx_text ltx_font_italic">object crops with mostly salient pixels</span> can result in effective texture features. High performance is even achieved on VOC-Weather in this setting, demonstrating these features to be robust to some texture shift. A last note is that we find TFNS gains to be highest at <span id="Sx4.SSx2.SSSx2.p3.2.9" class="ltx_text ltx_font_italic">m</span>=70, which makes sense as many non-salient pixels exist in such views.</p>
</div>
</section>
<section id="Sx4.SSx2.SSSx3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">How does texture flattening of non-salient regions compare to another strategy for context debiasing?</h4>

<div id="Sx4.SSx2.SSSx3.p1" class="ltx_para">
<p id="Sx4.SSx2.SSSx3.p1.1" class="ltx_p">We gain further understanding of the effectiveness of TFNS through evaluating it versus replacing the query crop’s non-salient pixels with a random grayscale value, a top background debiasing strategy <cite class="ltx_cite ltx_citemacro_citep">(Ryali, Schwab, and Morcos <a href="#bib.bib34" title="" class="ltx_ref">2021</a>; Zhao et al. <a href="#bib.bib48" title="" class="ltx_ref">2021</a>)</cite>. Results are shown for COCO pretraining and VOC finetuning in Table <a href="#Sx4.T6" title="Table 6 ‣ How does texture flattening of non-salient regions compare to another strategy for context debiasing? ‣ Pretraining on COCO ‣ Experiments and Analysis ‣ Contrastive View Design Strategies to Enhance Robustness to Domain Shifts in Downstream Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. We find that TFNS outperforms the grayscale strategy on all sets. We reason that TFNS is more beneficial for background debiasing as in distorting the background, it maintains continuity between an image’s salient and non-salient pixels, making images seen in pretraining more natural and closer to those seen at test time.</p>
</div>
<figure id="Sx4.T6" class="ltx_table">
<table id="Sx4.T6.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx4.T6.1.1.1" class="ltx_tr">
<th id="Sx4.T6.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="Sx4.T6.1.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<th id="Sx4.T6.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx4.T6.1.1.1.2.1" class="ltx_text ltx_font_bold">VOC</span></th>
<th id="Sx4.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx4.T6.1.1.1.3.1" class="ltx_text ltx_font_bold">Abstract</span></th>
<th id="Sx4.T6.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx4.T6.1.1.1.4.1" class="ltx_text ltx_font_bold">Weather</span></th>
</tr>
<tr id="Sx4.T6.1.2.2" class="ltx_tr">
<th id="Sx4.T6.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r"></th>
<th id="Sx4.T6.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">AP</th>
<th id="Sx4.T6.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">AP</th>
<th id="Sx4.T6.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">AP</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx4.T6.1.3.1" class="ltx_tr">
<td id="Sx4.T6.1.3.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt">InsLoc+RandGrayBG</td>
<td id="Sx4.T6.1.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="background-color:#F2F2F2;"><span id="Sx4.T6.1.3.1.2.1" class="ltx_text" style="background-color:#F2F2F2;">40.99</span></td>
<td id="Sx4.T6.1.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">12.97</td>
<td id="Sx4.T6.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">18.36</td>
</tr>
<tr id="Sx4.T6.1.4.2" class="ltx_tr">
<td id="Sx4.T6.1.4.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">InsLoc+TFNS</td>
<td id="Sx4.T6.1.4.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="background-color:#F2F2F2;"><span id="Sx4.T6.1.4.2.2.1" class="ltx_text ltx_font_bold" style="background-color:#F2F2F2;">41.80</span></td>
<td id="Sx4.T6.1.4.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="Sx4.T6.1.4.2.3.1" class="ltx_text ltx_font_bold">13.18</span></td>
<td id="Sx4.T6.1.4.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="Sx4.T6.1.4.2.4.1" class="ltx_text ltx_font_bold">20.46</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>InsLoc+DeepUSPS-Tightened (<span id="Sx4.T6.3.1" class="ltx_text ltx_font_italic">m</span>=70), using query crops with random grayscale backgrounds (RandGrayBG) vs. crops with non-salient region texture flattening (TFNS).</figcaption>
</figure>
<figure id="Sx4.T7" class="ltx_table">
<table id="Sx4.T7.3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx4.T7.3.3.4.1" class="ltx_tr">
<th id="Sx4.T7.3.3.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="Sx4.T7.3.3.4.1.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<th id="Sx4.T7.3.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2"><span id="Sx4.T7.3.3.4.1.2.1" class="ltx_text ltx_font_bold">COCO</span></th>
<th id="Sx4.T7.3.3.4.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2"><span id="Sx4.T7.3.3.4.1.3.1" class="ltx_text ltx_font_bold">UnRel</span></th>
<th id="Sx4.T7.3.3.4.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2"><span id="Sx4.T7.3.3.4.1.4.1" class="ltx_text ltx_font_bold">Weather</span></th>
</tr>
<tr id="Sx4.T7.3.3.3" class="ltx_tr">
<th id="Sx4.T7.3.3.3.4" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="Sx4.T7.3.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">AP</th>
<th id="Sx4.T7.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">AP<sub id="Sx4.T7.1.1.1.1.1" class="ltx_sub"><span id="Sx4.T7.1.1.1.1.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</th>
<th id="Sx4.T7.3.3.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column">AP</th>
<th id="Sx4.T7.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">AP<sub id="Sx4.T7.2.2.2.2.1" class="ltx_sub"><span id="Sx4.T7.2.2.2.2.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</th>
<th id="Sx4.T7.3.3.3.7" class="ltx_td ltx_align_center ltx_th ltx_th_column">AP</th>
<th id="Sx4.T7.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">AP<sub id="Sx4.T7.3.3.3.3.1" class="ltx_sub"><span id="Sx4.T7.3.3.3.3.1.1" class="ltx_text ltx_font_italic">50</span></sub>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx4.T7.3.3.5.1" class="ltx_tr">
<th id="Sx4.T7.3.3.5.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">InsLoc</th>
<td id="Sx4.T7.3.3.5.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="background-color:#F2F2F2;"><span id="Sx4.T7.3.3.5.1.2.1" class="ltx_text" style="background-color:#F2F2F2;">29.63</span></td>
<td id="Sx4.T7.3.3.5.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="background-color:#F2F2F2;"><span id="Sx4.T7.3.3.5.1.3.1" class="ltx_text" style="background-color:#F2F2F2;">46.50</span></td>
<td id="Sx4.T7.3.3.5.1.4" class="ltx_td ltx_align_center ltx_border_tt">25.88</td>
<td id="Sx4.T7.3.3.5.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">41.68</td>
<td id="Sx4.T7.3.3.5.1.6" class="ltx_td ltx_align_center ltx_border_tt">14.25</td>
<td id="Sx4.T7.3.3.5.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">23.32</td>
</tr>
<tr id="Sx4.T7.3.3.6.2" class="ltx_tr">
<th id="Sx4.T7.3.3.6.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Ours</th>
<td id="Sx4.T7.3.3.6.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="background-color:#F2F2F2;"><span id="Sx4.T7.3.3.6.2.2.1" class="ltx_text ltx_font_bold" style="background-color:#F2F2F2;">30.08</span></td>
<td id="Sx4.T7.3.3.6.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="background-color:#F2F2F2;"><span id="Sx4.T7.3.3.6.2.3.1" class="ltx_text ltx_font_bold" style="background-color:#F2F2F2;">47.30</span></td>
<td id="Sx4.T7.3.3.6.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="Sx4.T7.3.3.6.2.4.1" class="ltx_text ltx_font_bold">27.46</span></td>
<td id="Sx4.T7.3.3.6.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="Sx4.T7.3.3.6.2.5.1" class="ltx_text ltx_font_bold">42.93</span></td>
<td id="Sx4.T7.3.3.6.2.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="Sx4.T7.3.3.6.2.6.1" class="ltx_text ltx_font_bold">14.52</span></td>
<td id="Sx4.T7.3.3.6.2.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="Sx4.T7.3.3.6.2.7.1" class="ltx_text ltx_font_bold">23.81</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>InsLoc with our top strategies (DeepUSPS-Tightened, <span id="Sx4.T7.7.1" class="ltx_text ltx_font_italic">m</span>=70, and texture flattening of non-salient regions) vs. InsLoc baseline. Results are compared following COCO pretraining and finetuning (using a 2<math id="Sx4.T7.5.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="Sx4.T7.5.m1.1b"><mo id="Sx4.T7.5.m1.1.1" xref="Sx4.T7.5.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="Sx4.T7.5.m1.1c"><times id="Sx4.T7.5.m1.1.1.cmml" xref="Sx4.T7.5.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Sx4.T7.5.m1.1d">\times</annotation></semantics></math> schedule).</figcaption>
</figure>
</section>
<section id="Sx4.SSx2.SSSx4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">How are results at a longer training schedule?</h4>

<div id="Sx4.SSx2.SSSx4.p1" class="ltx_para">
<p id="Sx4.SSx2.SSSx4.p1.1" class="ltx_p">In Table <a href="#Sx4.T7" title="Table 7 ‣ How does texture flattening of non-salient regions compare to another strategy for context debiasing? ‣ Pretraining on COCO ‣ Experiments and Analysis ‣ Contrastive View Design Strategies to Enhance Robustness to Domain Shifts in Downstream Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, we lastly show that our strategies maintain effectiveness on COCO even when using a longer finetuning schedule (the 2<math id="Sx4.SSx2.SSSx4.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="Sx4.SSx2.SSSx4.p1.1.m1.1a"><mo id="Sx4.SSx2.SSSx4.p1.1.m1.1.1" xref="Sx4.SSx2.SSSx4.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="Sx4.SSx2.SSSx4.p1.1.m1.1b"><times id="Sx4.SSx2.SSSx4.p1.1.m1.1.1.cmml" xref="Sx4.SSx2.SSSx4.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx2.SSSx4.p1.1.m1.1c">\times</annotation></semantics></math>, 180k iteration schedule from <cite class="ltx_cite ltx_citemacro_citep">(Yang et al. <a href="#bib.bib47" title="" class="ltx_ref">2021</a>)</cite>). Gains can notably be observed on <span id="Sx4.SSx2.SSSx4.p1.1.1" class="ltx_text ltx_font_italic">both</span> out-of-domain test sets: +1.58 AP on UnRel and +0.27 AP on COCO-Weather.</p>
</div>
</section>
</section>
</section>
<section id="Sx5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Conclusion</h2>

<div id="Sx5.p1" class="ltx_para">
<p id="Sx5.p1.1" class="ltx_p">In this work, we present contrastive view design strategies to improve robustness to domain shifts in object detection. We show that we can make the contrastive augmentation pipeline more robust to domain shifts in appearance through encouraging the learning of shape (with higher minimum crop % and IoU constraints). Furthermore, combining these shape strategies with shortcut-reducing appearance augmentations is shown to lead to more robust object features overall, demonstrated by both in-domain and out-of-domain performance improvements. Finally, when pretraining on multi-object image datasets with saliency map priors, we find that tightening crops to salient regions, along with texture flattening the remaining non-salient pixels in a view, is an effective strategy to achieve out-of-context detection robustness. Overall, these strategies can serve to guide view design in future detection-focused, contrastive pretraining methods.</p>
</div>
<div id="Sx5.p2" class="ltx_para ltx_noindent">
<p id="Sx5.p2.1" class="ltx_p"><span id="Sx5.p2.1.1" class="ltx_text ltx_font_bold">Acknowledgements:</span> This work was supported by the National Science Foundation under Grant No. 2006885.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alcorn et al. (2019)</span>
<span class="ltx_bibblock">
Alcorn, M. A.; Li, Q.; Gong, Z.; Wang, C.; Mai, L.; Ku, W.-S.; and Nguyen, A.
2019.

</span>
<span class="ltx_bibblock">Strike (with) a pose: Neural networks are easily fooled by strange
poses of familiar objects.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, 4845–4854.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bradski (2000)</span>
<span class="ltx_bibblock">
Bradski, G. 2000.

</span>
<span class="ltx_bibblock">The OpenCV Library.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Dr. Dobb’s Journal of Software Tools</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caron et al. (2020)</span>
<span class="ltx_bibblock">
Caron, M.; Misra, I.; Mairal, J.; Goyal, P.; Bojanowski, P.; and Joulin, A.
2020.

</span>
<span class="ltx_bibblock">Unsupervised learning of visual features by contrasting cluster
assignments.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 33:
9912–9924.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2020a)</span>
<span class="ltx_bibblock">
Chen, T.; Kornblith, S.; Norouzi, M.; and Hinton, G. 2020a.

</span>
<span class="ltx_bibblock">A simple framework for contrastive learning of visual
representations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, 1597–1607.
PMLR.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2020b)</span>
<span class="ltx_bibblock">
Chen, X.; Fan, H.; Girshick, R.; and He, K. 2020b.

</span>
<span class="ltx_bibblock">Improved baselines with momentum contrastive learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.04297</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chuang et al. (2020)</span>
<span class="ltx_bibblock">
Chuang, C.-Y.; Robinson, J.; Lin, Y.-C.; Torralba, A.; and Jegelka, S. 2020.

</span>
<span class="ltx_bibblock">Debiased contrastive learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 33:
8765–8775.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. (2021)</span>
<span class="ltx_bibblock">
Dai, Z.; Cai, B.; Lin, Y.; and Chen, J. 2021.

</span>
<span class="ltx_bibblock">UP-DETR: Unsupervised pre-training for object detection with
transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, 1601–1610.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ericsson, Gouk, and Hospedales (2021)</span>
<span class="ltx_bibblock">
Ericsson, L.; Gouk, H.; and Hospedales, T. M. 2021.

</span>
<span class="ltx_bibblock">How Well Do Self-Supervised Models Transfer?

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, 5414–5423.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Everingham et al. (2010)</span>
<span class="ltx_bibblock">
Everingham, M.; Van Gool, L.; Williams, C. K.; Winn, J.; and Zisserman, A.
2010.

</span>
<span class="ltx_bibblock">The Pascal visual object classes (VOC) challenge.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, 88(2): 303–338.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ge et al. (2021)</span>
<span class="ltx_bibblock">
Ge, S.; Mishra, S.; Li, C.-L.; Wang, H.; and Jacobs, D. 2021.

</span>
<span class="ltx_bibblock">Robust Contrastive Learning Using Negative Samples with Diminished
Semantics.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 34.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geirhos et al. (2020)</span>
<span class="ltx_bibblock">
Geirhos, R.; Narayanappa, K.; Mitzkus, B.; Bethge, M.; Wichmann, F. A.; and
Brendel, W. 2020.

</span>
<span class="ltx_bibblock">On the surprising similarities between supervised and self-supervised
models.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.08377</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geirhos et al. (2019)</span>
<span class="ltx_bibblock">
Geirhos, R.; Rubisch, P.; Michaelis, C.; Bethge, M.; Wichmann, F. A.; and
Brendel, W. 2019.

</span>
<span class="ltx_bibblock">ImageNet-trained CNNs are biased towards texture; increasing
shape bias improves accuracy and robustness.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">7th International Conference on Learning Representations,
ICLR</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grill et al. (2020)</span>
<span class="ltx_bibblock">
Grill, J.-B.; Strub, F.; Altché, F.; Tallec, C.; Richemond, P.;
Buchatskaya, E.; Doersch, C.; Avila Pires, B.; Guo, Z.; Gheshlaghi Azar, M.;
et al. 2020.

</span>
<span class="ltx_bibblock">Bootstrap your own latent - A new approach to self-supervised
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 33:
21271–21284.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hadsell, Chopra, and LeCun (2006)</span>
<span class="ltx_bibblock">
Hadsell, R.; Chopra, S.; and LeCun, Y. 2006.

</span>
<span class="ltx_bibblock">Dimensionality reduction by learning an invariant mapping.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">2006 IEEE Computer Society Conference on Computer Vision and
Pattern Recognition (CVPR’06)</em>. IEEE.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2020)</span>
<span class="ltx_bibblock">
He, K.; Fan, H.; Wu, Y.; Xie, S.; and Girshick, R. 2020.

</span>
<span class="ltx_bibblock">Momentum contrast for unsupervised visual representation learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, 9729–9738.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2017)</span>
<span class="ltx_bibblock">
He, K.; Gkioxari, G.; Dollar, P.; and Girshick, R. 2017.

</span>
<span class="ltx_bibblock">Mask R-CNN.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision (ICCV)</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks and Dietterich (2019)</span>
<span class="ltx_bibblock">
Hendrycks, D.; and Dietterich, T. 2019.

</span>
<span class="ltx_bibblock">Benchmarking neural network robustness to common corruptions and
perturbations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">7th International Conference on Learning Representations,
ICLR</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hermann, Chen, and Kornblith (2020)</span>
<span class="ltx_bibblock">
Hermann, K.; Chen, T.; and Kornblith, S. 2020.

</span>
<span class="ltx_bibblock">The origins and prevalence of texture bias in convolutional neural
networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 33:
19000–19015.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Inoue et al. (2018)</span>
<span class="ltx_bibblock">
Inoue, N.; Furuta, R.; Yamasaki, T.; and Aizawa, K. 2018.

</span>
<span class="ltx_bibblock">Cross-domain weakly-supervised object detection through progressive
domain adaptation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, 5001–5009.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kalantidis et al. (2020)</span>
<span class="ltx_bibblock">
Kalantidis, Y.; Sariyildiz, M. B.; Pion, N.; Weinzaepfel, P.; and Larlus, D.
2020.

</span>
<span class="ltx_bibblock">Hard negative mixing for contrastive learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 33:
21798–21809.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khosla et al. (2020)</span>
<span class="ltx_bibblock">
Khosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y.; Isola, P.; Maschinot,
A.; Liu, C.; and Krishnan, D. 2020.

</span>
<span class="ltx_bibblock">Supervised contrastive learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 33:
18661–18673.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky, Sutskever, and Hinton (2012)</span>
<span class="ltx_bibblock">
Krizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012.

</span>
<span class="ltx_bibblock">ImageNet classification with deep convolutional neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 25.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ramanan, D.;
Dollár, P.; and Zitnick, C. L. 2014.

</span>
<span class="ltx_bibblock">Microsoft COCO: Common objects in context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, 740–755. Springer.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Michaelis et al. (2019)</span>
<span class="ltx_bibblock">
Michaelis, C.; Mitzkus, B.; Geirhos, R.; Rusak, E.; Bringmann, O.; Ecker,
A. S.; Bethge, M.; and Brendel, W. 2019.

</span>
<span class="ltx_bibblock">Benchmarking Robustness in Object Detection: Autonomous Driving when
Winter is Coming.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1907.07484.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mo et al. (2021)</span>
<span class="ltx_bibblock">
Mo, S.; Kang, H.; Sohn, K.; Li, C.-L.; and Shin, J. 2021.

</span>
<span class="ltx_bibblock">Object-aware contrastive learning for debiased scene representation.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 34.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. (2019)</span>
<span class="ltx_bibblock">
Nguyen, T.; Dax, M.; Mummadi, C. K.; Ngo, N.; Nguyen, T. H. P.; Lou, Z.; and
Brox, T. 2019.

</span>
<span class="ltx_bibblock">DeepUSPS: Deep robust unsupervised saliency prediction via
self-supervision.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 32.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pérez, Gangnet, and Blake (2003)</span>
<span class="ltx_bibblock">
Pérez, P.; Gangnet, M.; and Blake, A. 2003.

</span>
<span class="ltx_bibblock">Poisson image editing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">ACM SIGGRAPH 2003 Papers</em>, 313–318.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peyre et al. (2017)</span>
<span class="ltx_bibblock">
Peyre, J.; Sivic, J.; Laptev, I.; and Schmid, C. 2017.

</span>
<span class="ltx_bibblock">Weakly-supervised learning of visual relations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision</em>, 5179–5188.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Purushwalkam and Gupta (2020)</span>
<span class="ltx_bibblock">
Purushwalkam, S.; and Gupta, A. 2020.

</span>
<span class="ltx_bibblock">Demystifying contrastive self-supervised learning: Invariances,
augmentations and dataset biases.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 33:
3407–3418.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2015)</span>
<span class="ltx_bibblock">
Ren, S.; He, K.; Girshick, R.; and Sun, J. 2015.

</span>
<span class="ltx_bibblock">Faster R-CNN: Towards real-time object detection with region
proposal networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 28.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robinson et al. (2021a)</span>
<span class="ltx_bibblock">
Robinson, J.; Sun, L.; Yu, K.; Batmanghelich, K.; Jegelka, S.; and Sra, S.
2021a.

</span>
<span class="ltx_bibblock">Can contrastive learning avoid shortcut solutions?

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 34:
4974–4986.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robinson et al. (2021b)</span>
<span class="ltx_bibblock">
Robinson, J. D.; Chuang, C.; Sra, S.; and Jegelka, S. 2021b.

</span>
<span class="ltx_bibblock">Contrastive Learning with Hard Negative Samples.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">9th International Conference on Learning Representations,
ICLR</em>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roh et al. (2021)</span>
<span class="ltx_bibblock">
Roh, B.; Shin, W.; Kim, I.; and Kim, S. 2021.

</span>
<span class="ltx_bibblock">Spatially consistent representation learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, 1144–1153.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ryali, Schwab, and Morcos (2021)</span>
<span class="ltx_bibblock">
Ryali, C.; Schwab, D. J.; and Morcos, A. S. 2021.

</span>
<span class="ltx_bibblock">Learning Background Invariance Improves Generalization and Robustness
in Self-Supervised Learning on ImageNet and Beyond.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Workshop on ImageNet: Past, Present, and Future, held in
conjunction with NeurIPS</em>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Selvaraju et al. (2021)</span>
<span class="ltx_bibblock">
Selvaraju, R. R.; Desai, K.; Johnson, J.; and Naik, N. 2021.

</span>
<span class="ltx_bibblock">CASTing your model: Learning to localize improves self-supervised
representations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, 11058–11067.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simard et al. (2003)</span>
<span class="ltx_bibblock">
Simard, P. Y.; Steinkraus, D.; Platt, J. C.; et al. 2003.

</span>
<span class="ltx_bibblock">Best practices for convolutional neural networks applied to visual
document analysis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">ICDAR</em>, volume 3.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al. (2020)</span>
<span class="ltx_bibblock">
Singh, K. K.; Mahajan, D.; Grauman, K.; Lee, Y. J.; Feiszli, M.; and
Ghadiyaram, D. 2020.

</span>
<span class="ltx_bibblock">Don’t judge an object by its context: Learning to overcome contextual
bias.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, 11070–11078.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et al. (2020)</span>
<span class="ltx_bibblock">
Tian, Y.; Sun, C.; Poole, B.; Krishnan, D.; Schmid, C.; and Isola, P. 2020.

</span>
<span class="ltx_bibblock">What makes for good views for contrastive learning?

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 33:
6827–6839.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van den Oord, Li, and Vinyals (2018)</span>
<span class="ltx_bibblock">
Van den Oord, A.; Li, Y.; and Vinyals, O. 2018.

</span>
<span class="ltx_bibblock">Representation learning with contrastive predictive coding.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em>, arXiv–1807.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021)</span>
<span class="ltx_bibblock">
Wang, X.; Zhang, R.; Shen, C.; Kong, T.; and Li, L. 2021.

</span>
<span class="ltx_bibblock">Dense contrastive learning for self-supervised visual pre-training.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, 3024–3033.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2021)</span>
<span class="ltx_bibblock">
Wei, F.; Gao, Y.; Wu, Z.; Hu, H.; and Lin, S. 2021.

</span>
<span class="ltx_bibblock">Aligning pretraining for detection via object-level contrastive
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 34.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2019)</span>
<span class="ltx_bibblock">
Wu, Y.; Kirillov, A.; Massa, F.; Lo, W.-Y.; and Girshick, R. 2019.

</span>
<span class="ltx_bibblock">Detectron2.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://github.com/facebookresearch/detectron2</span>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2018)</span>
<span class="ltx_bibblock">
Wu, Z.; Xiong, Y.; Yu, S. X.; and Lin, D. 2018.

</span>
<span class="ltx_bibblock">Unsupervised feature learning via non-parametric instance
discrimination.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, 3733–3742.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al. (2021)</span>
<span class="ltx_bibblock">
Xiao, K.; Engstrom, L.; Ilyas, A.; and Madry, A. 2021.

</span>
<span class="ltx_bibblock">Noise or signal: The role of image backgrounds in object recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">9th International Conference on Learning Representations,
ICLR</em>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. (2021a)</span>
<span class="ltx_bibblock">
Xie, E.; Ding, J.; Wang, W.; Zhan, X.; Xu, H.; Sun, P.; Li, Z.; and Luo, P.
2021a.

</span>
<span class="ltx_bibblock">DetCo: Unsupervised Contrastive Learning for Object Detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV)</em>, 8392–8401.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. (2021b)</span>
<span class="ltx_bibblock">
Xie, Z.; Lin, Y.; Zhang, Z.; Cao, Y.; Lin, S.; and Hu, H. 2021b.

</span>
<span class="ltx_bibblock">Propagate yourself: Exploring pixel-level consistency for
unsupervised visual representation learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, 16684–16693.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2021)</span>
<span class="ltx_bibblock">
Yang, C.; Wu, Z.; Zhou, B.; and Lin, S. 2021.

</span>
<span class="ltx_bibblock">Instance localization for self-supervised detection pretraining.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, 3987–3996.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2021)</span>
<span class="ltx_bibblock">
Zhao, N.; Wu, Z.; Lau, R. W.; and Lin, S. 2021.

</span>
<span class="ltx_bibblock">Distilling localization for self-supervised representation learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, volume 35, 10990–10998.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et al. (2021)</span>
<span class="ltx_bibblock">
Zhong, Y.; Wang, J.; Wang, L.; Peng, J.; Wang, Y.-X.; and Zhang, L. 2021.

</span>
<span class="ltx_bibblock">DAP: Detection-Aware Pre-Training With Weak Supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, 4537–4546.

</span>
</li>
</ul>
</section>
<div id="p1" class="ltx_para">
<br class="ltx_break">
</div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2212.04612" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2212.04613" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2212.04613">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2212.04613" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2212.04614" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 11:54:19 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
