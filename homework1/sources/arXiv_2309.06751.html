<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2309.06751] Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances</title><meta property="og:description" content="Remote sensing object detection (RSOD), one of the most fundamental and challenging tasks in the remote sensing field, has received longstanding attention. In recent years, deep learning techniques have demonstrated ro…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2309.06751">

<!--Generated on Wed Feb 28 05:57:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Object detection, 
Remote sensing images, 
Deep learning, 
Technical evolution
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Xiangrong Zhang, ,
Tianyang Zhang,
Guanchun Wang,
Peng Zhu,
Xu Tang, ,
Xiuping Jia, ,
and Licheng Jiao
</span><span class="ltx_author_notes">Xiangrong Zhang, Tianyang Zhang, Guanchun Wang, Peng Zhu, Xu Tang, and Licheng Jiao are with the School of Artificial Intelligence, Xidian University, Xi’an 710071, China (e-mail: xrzhang@mail.xidian.edu.cn).
Xiuping Jia is with the School of Engineering and Information Technology, University of New South Wales, Canberra, ACT 2612, Australia.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Remote sensing object detection (RSOD), one of the most fundamental and challenging tasks in the remote sensing field, has received longstanding attention. In recent years, deep learning techniques have demonstrated robust feature representation capabilities and led to a big leap in the development of RSOD techniques. In this era of rapid technical evolution, this review aims to present a comprehensive review of the recent achievements in deep learning based RSOD methods. More than 300 papers are covered in this review. We identify five main challenges in RSOD, including multi-scale object detection, rotated object detection, weak object detection, tiny object detection, and object detection with limited supervision, and systematically review the corresponding methods developed in a hierarchical division manner. We also review the widely used benchmark datasets and evaluation metrics within the field of RSOD, as well as the application scenarios for RSOD.
Future research directions are provided for further promoting the research in RSOD.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Object detection,
Remote sensing images,
Deep learning,
Technical evolution

</div>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2309.06751/assets/x1.png" id="S0.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="452" height="296" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A typical example of remote sensing images.
(a) The complex context and massive background noises lead to weak feature responses of objects.
(b) Huge scale variations exist in both inter-category and intra-category objects.
(c) Objects are distributed with arbitrary orientations.
(d) Tiny objects tend to exhibit extremely small scales.</figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">With the rapid advances in earth observation technology, remote sensing satellites (e.g., Google Earth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, WordWide-3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, and Gaofen series satellites <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>) have made significant improvements in spatial, temporal, and spectral resolutions and a massive number of remote sensing images (RSIs) are now accessible. Benefiting from the dramatic increase in available RSIs, human beings have entered an era of remote sensing big data, and the automatic interpretation of RSIs has become an active yield challenging topic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">RSOD aims to determine whether or not objects of interest exist in a given RSI and to return the category and position of each predicted object. The term ’object’ in this survey refers to man-made or highly structured objects (such as airplanes, vehicles, and ships) rather than unstructured scene objects (e.g., land, sky, and grass). As the cornerstone in the automatic interpretation of RSIs, RSOD has received significant attention.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In general, RSIs are taken at an overhead viewpoint with different ground sampling distances (GSDs) and cover widespread regions of the Earth’s surface. As a result, the geospatial objects exhibit more significant diversity in scale, angle, and appearance. Based on the characteristics of geospatial objects in RSIs, we summarize the major challenges of RSOD in the following five aspects:</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">(1) <span id="S1.p4.1.1" class="ltx_text ltx_font_bold">Huge Scale Variations.</span> On the one hand, there are generally massive scale variations across different categories of objects, as illustrated in Fig. <a href="#S0.F1" title="Figure 1 ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(b): a vehicle may be as small as 10 pixel area, while an airplane can be 20 times larger than the vehicle. On the other hand, the intra-category objects also show a wide range of scales. Therefore, the detection models require to handle both large-scale and small-scale objects.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">(2) <span id="S1.p5.1.1" class="ltx_text ltx_font_bold">Arbitrary Orientations.</span> The unique overhead viewpoint leads to the geospatial objects often distributed with arbitrary orientations, as shown in Fig. <a href="#S0.F1" title="Figure 1 ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(c). This rotated object detection task exacerbates the challenge of RSOD, making it important for the detector to be perceptive of orientation.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">(3) <span id="S1.p6.1.1" class="ltx_text ltx_font_bold">Weak Feature Responses.</span> Generally, RSIs contain complex context and massive background noises. As depicted in Fig. <a href="#S0.F1" title="Figure 1 ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(a), some vehicles are obscured by shadows, and the surrounding background noises tend to have a similar appearance to vehicles. This intricate interference may overwhelm the objects of interest and deteriorate their feature representation, which results in the objects of interest being presented as weak feature responses <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">(4) <span id="S1.p7.1.1" class="ltx_text ltx_font_bold">Tiny Objects.</span> As shown in Fig. <a href="#S0.F1" title="Figure 1 ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(d), tiny objects tend to exhibit extremely small scales and limited appearance information, resulting in a poor-quality feature representation. In addition, the current prevailing detection paradigms inevitably weaken or even discard the representation of tiny objects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. These problems in tiny object detection bring new difficulties to existing detection methods.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">(5) <span id="S1.p8.1.1" class="ltx_text ltx_font_bold">Expensive Annotation.</span> The complex characteristics of geospatial objects in terms of scale and angle, as well as the expert knowledge required for fine-grained annotations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, make the accurate box-level annotations of RSIs a time-consuming and labor-intensive task. However, the current deep learning based detectors rely heavily on abundant well-labeled data to reach performance saturation. Therefore, the efficient RSOD methods in a lack of sufficient supervised information scenario remain challenging.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">To tackle these challenges, numerous RSOD methods have emerged in the past two decades. At the early stage, researchers adopted template matching <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and prior knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> for object detection in remote sensing scenes. These early methods rely more on hand-crafted templates or prior knowledge, leading to unstable results. Later, machine learning approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> have become mainstream in RSOD, which view object detection as a classification task. Concretely, the machine learning model first searches a set of object proposals from the input image and extracts the texture, context, and other features of these object proposals. Then, it employs an independent classifier to identify the object categories in these object proposals. However, shallow learning based features from the machine learning approaches significantly restrict the representations of objects, especially in more challenging scenarios. Besides, the machine learning based object detection methods cannot be trained in an end-to-end manner, which is no longer applicable in the era of remote sensing big data.</p>
</div>
<div id="S1.p10" class="ltx_para">
<p id="S1.p10.1" class="ltx_p">Recently, deep learning techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> have demonstrated powerful feature representation capabilities from massive amounts of data, and the state-of-the-art detectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> in computer vision achieve object detection ability that rivals that of humans <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. Drawing on the advanced progress of deep learning techniques, various deep learning based methods have dominated the RSOD and led to remarkable breakthroughs in detection performance. Compared to the traditional methods, deep neural network architecture can extract high-level semantic features and obtain much more robust feature representations of objects. In addition, the efficient end-to-end training manner and automated feature extraction fashion make the deep learning based object detection methods more suitable for RSOD in the remote sensing big data era.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2309.06751/assets/x2.png" id="S1.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="452" height="398" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Structured taxonomy of the deep learning based RSOD methods in this review. A hierarchical division is adopted to detailed describe each sub-category.</figcaption>
</figure>
<div id="S1.p11" class="ltx_para">
<p id="S1.p11.1" class="ltx_p">Along with the prevalence of RSOD, a number of geospatial object detection surveys <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> have been published in recent years. For example, Cheng <span id="S1.p11.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> reviewed the early development of RSOD. Han <span id="S1.p11.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> focused on small and weak object detection in RSIs. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, the authors reviewed airplane detection methods. Li <span id="S1.p11.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> conducted a thorough survey on deep learning based detectors in the remote sensing community according to various improvement strategies. Besides, some work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> mainly focused on publishing novel benchmark datasets for RSOD and briefly reviewed object detection methods in the field of remote sensing. Compared with previous works, this survey provides a comprehensive analysis of the major challenges in RSOD based on the characteristics of geospatial objects and systematically categorizes and summarizes the deep learning based remote sensing object detectors according to these challenges. Moreover, more than 300 papers on RSOD are reviewed in this work, leading to a more comprehensive and systematic survey.</p>
</div>
<div id="S1.p12" class="ltx_para">
<p id="S1.p12.1" class="ltx_p">Fig. <a href="#S1.F2" title="Figure 2 ‣ I Introduction ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the taxonomy of object detection methods in this review. According to the major challenges in RSOD, we divide the current deep learning based RSOD methods into five main categories: multi-scale object detection, rotated object detection, weak object detection, tiny object detection, and object detection with limited supervision. In each category, we further summarize the sub-categories based on the improvement strategies or learning paradigms designed for the category-specific challenges. For multi-scale object detection, we mainly review the three widely used methods: data augmentation strategy, multi-scale feature representation, and high-quality multi-scale anchor generation. With regard to rotated object detection, we mainly focus on the rotated bounding box representation and rotation-insensitive feature learning. For weak object detection, we divide it into two classes: background noise suppressing and related context mining. As for tiny object detection, we detail it into three streams: discriminative feature extraction, super-resolution reconstruction, and improved detection metrics. According to the learning paradigms, we divide object detection with limited supervision into weakly-supervised object detection, semi-supervised object detection, and few-shot object detection. Notably, there are still detailed divisions in each sub-category, as shown in the rounded rectangles in Fig. <a href="#S1.F2" title="Figure 2 ‣ I Introduction ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. This hierarchical division provides a systematic review and summarization of existing methods. It helps researchers understand RSOD more comprehensively and facilitate further progress, which is the main purpose of this review.</p>
</div>
<div id="S1.p13" class="ltx_para">
<p id="S1.p13.1" class="ltx_p">In summary, the main contributions of this review are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We comprehensively analyze the major challenges in RSOD based on the characteristics of geospatial objects, including huge scale variations, arbitrary orientations, weak feature responses, tiny objects, and expensive annotations.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We systematically summarize the deep learning based object detectors in the remote sensing community and categorize them in a hierarchical manner according to their motivation.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We present a forward-looking discussion of future research directions for RSOD to motivate the further progress of RSOD.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Multi-scale Object Detection</span>
</h2>

<figure id="S2.F3" class="ltx_figure"><img src="/html/2309.06751/assets/x3.png" id="S2.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="221" height="92" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Scale variations for each category (the short names for categories can be referred to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>) in the DOTAv2.0 dataset. Huge scale variations exist in both inter-categories and intra-categories.</figcaption>
</figure>
<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Due to the different spatial resolutions between RSIs, the huge scale variation is a notoriously challenging problem in RSOD and seriously degrades the detection performance. As depicted in Fig. <a href="#S2.F3" title="Figure 3 ‣ II Multi-scale Object Detection ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we present the distribution of object pixel areas for each category in the DOTAv2.0 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. Obviously, the scales vary greatly between categories, in which a small vehicle may only contain less than 10 pixel area while an airport exceeds 10<sup id="S2.p1.1.1" class="ltx_sup"><span id="S2.p1.1.1.1" class="ltx_text ltx_font_italic">5</span></sup> pixel area. Worse still, the huge intra-category scale variations further exacerbate the difficulties of multi-scale object detection. To tackle the huge scale variation problem, current studies are mainly divided into data augmentation, multi-scale feature representation, and multi-scale anchor generation. Fig. <a href="#S2.F4" title="Figure 4 ‣ II-A Data Augmentation ‣ II Multi-scale Object Detection ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> gives a brief summary of multi-scale object detection methods.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Data Augmentation</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Data augmentation is a simple yet widely applied approach for increasing dataset diversity. As for the scale variation problem in multi-scale object detection, image scaling is a straightforward and effective augmentation method. Zhao <span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> fed multi-scale image pyramids into multiple networks and fused the output features from these networks to generate multi-scale feature representations. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, Azimi <span id="S2.SS1.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> proposed a combined image cascade and feature pyramid network to extract object features on various scales. Although image pyramids can effectively increase the detection performance for multi-scale objects, the inference time and computational complexity are severely increased. To tackle this problem, Shamsolmoali <span id="S2.SS1.p1.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> designed a lightweight image pyramid module (LIPM). The proposed LIPM receives multiple down-sampling images to generate multi-scale feature maps and fuses the output multi-scale feature maps with the corresponding scale feature maps from the backbone. Moreover, some modern data augmentation methods (e.g., Moscia and Stitcher <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>) also show remarkable effectiveness in multi-scale object detection, especially for small objects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>.</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2309.06751/assets/x4.png" id="S2.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="216" height="176" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>A brief summary of multi-scale object detection methods.</figcaption>
</figure>
<figure id="S2.F5" class="ltx_figure"><img src="/html/2309.06751/assets/x5.png" id="S2.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="452" height="138" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Single-scale feature representation and six paradigms for multi-scale feature representation. (a) Single-scale feature representation. (b) Multi-scale feature integration. (c) Pyramidal feature hierarchy. (d) Feature pyramid networks. (e) Top-down and Bottom-up. (f) Cross-scale feature balance.</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Multi-scale Feature Representation</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Early studies in RSOD usually utilize the last single feature map of the backbone to detect objects, as illustrated in Fig.<a href="#S2.F5" title="Figure 5 ‣ II-A Data Augmentation ‣ II Multi-scale Object Detection ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(a). However, such a single-scale feature map prediction limits the detector to handle the object with a wide range of scale <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. Consequently, multi-scale feature representation methods have been proposed and have become an effective solution to the huge object scale variation problem in RSOD. The current multi-scale feature representation methods are mainly divided into three streams: multi-scale feature integration, pyramidal feature hierarchy, and feature pyramid networks.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS1.4.1.1" class="ltx_text">II-B</span>1 </span>Multi-scale Feature Integration</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">Convolutional neural networks (CNN) usually adopt a deep hierarchical structure where different level features have different characteristics. The shallow-level features usually contain fine-grained features (e.g., points, edges, and textures of objects) and provide detailed spatial location information, which is more suitable for object localization. In contrast, features from higher-level layers show stronger semantic information and present discriminative information for object classification. To combine the information from different layers and generate the multi-scale representation, some researchers introduced multi-layer feature integration methods that integrate features from multiple layers into a single feature map and perform the detection on this rebuilt feature map <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>. Fig. <a href="#S2.F5" title="Figure 5 ‣ II-A Data Augmentation ‣ II Multi-scale Object Detection ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(b) depicts the structure of multi-layer feature integration methods.</p>
</div>
<div id="S2.SS2.SSS1.p2" class="ltx_para">
<p id="S2.SS2.SSS1.p2.1" class="ltx_p">Zhang <span id="S2.SS2.SSS1.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> designed a hierarchical robust CNN to extract hierarchical spatial semantic information by fusing multi-scale convolutional features from three different layers and introduced multiple fully connected layers to enhance the rotation and scaling robustness of the network. Considering the different norms between multi-layer features, Lin <span id="S2.SS2.SSS1.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> applied an L2 normalization for each feature before integration to maintain stability in the network training stage. Unlike previous multi-scale feature integration at the level of the convolutional layer, Zheng <span id="S2.SS2.SSS1.p2.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> designed the HyBlock to build multi-scale feature representation at the intra-layer level. The HyBlock employs the atrous separable convolution with pyramidal receptive fields to learn the hyper-scale features, alleviating the scale-variation issue in RSOD.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS2.4.1.1" class="ltx_text">II-B</span>2 </span>Pyramidal Feature Hierarchy</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">The key insight behind the pyramidal feature hierarchy is that the features in different layers can encode object information from different scales. For instance, small objects are more likely to appear in shallow layers, while large objects tend to exist in deep layers. Therefore, the pyramidal feature hierarchy employs multiple-layer features for independent prediction to detect objects with a wide scale range, as depicted in Fig. <a href="#S2.F5" title="Figure 5 ‣ II-A Data Augmentation ‣ II Multi-scale Object Detection ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(c). SSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> is a typical representative of the pyramidal feature hierarchy, which has a wide range of extended applications in both natural scenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> and remote sensing scenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>.</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para">
<p id="S2.SS2.SSS2.p2.3" class="ltx_p">To improve the detection performance for small vehicles, Liang <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> <span id="S2.SS2.SSS2.p2.3.1" class="ltx_text ltx_font_italic">et al.</span> added an extra scaling branch to the SSD, which consists of a deconvolution module and an average pooling layer. Referring to hierarchical regression layers in SSD, Wang <span id="S2.SS2.SSS2.p2.3.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> introduced the scale-invariant regression layers (SIRLs), where three isolated regression layers are employed to capture the information of full-scale objects. Based on the SIRLs, a novel specific scale joint loss is introduced to accelerate network convergence. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>, Li <span id="S2.SS2.SSS2.p2.3.3" class="ltx_text ltx_font_italic">et al.</span> proposed the HSF-Net that introduces the hierarchical selective filtering layer in both RPN and detection sub-network. Specifically, the hierarchical selective filtering layer employs three convolutional layers with different kernel sizes (e.g., <math id="S2.SS2.SSS2.p2.1.m1.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S2.SS2.SSS2.p2.1.m1.1a"><mrow id="S2.SS2.SSS2.p2.1.m1.1.1" xref="S2.SS2.SSS2.p2.1.m1.1.1.cmml"><mn id="S2.SS2.SSS2.p2.1.m1.1.1.2" xref="S2.SS2.SSS2.p2.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.SSS2.p2.1.m1.1.1.1" xref="S2.SS2.SSS2.p2.1.m1.1.1.1.cmml">×</mo><mn id="S2.SS2.SSS2.p2.1.m1.1.1.3" xref="S2.SS2.SSS2.p2.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p2.1.m1.1b"><apply id="S2.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S2.SS2.SSS2.p2.1.m1.1.1"><times id="S2.SS2.SSS2.p2.1.m1.1.1.1.cmml" xref="S2.SS2.SSS2.p2.1.m1.1.1.1"></times><cn type="integer" id="S2.SS2.SSS2.p2.1.m1.1.1.2.cmml" xref="S2.SS2.SSS2.p2.1.m1.1.1.2">1</cn><cn type="integer" id="S2.SS2.SSS2.p2.1.m1.1.1.3.cmml" xref="S2.SS2.SSS2.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p2.1.m1.1c">1\times 1</annotation></semantics></math>, <math id="S2.SS2.SSS2.p2.2.m2.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S2.SS2.SSS2.p2.2.m2.1a"><mrow id="S2.SS2.SSS2.p2.2.m2.1.1" xref="S2.SS2.SSS2.p2.2.m2.1.1.cmml"><mn id="S2.SS2.SSS2.p2.2.m2.1.1.2" xref="S2.SS2.SSS2.p2.2.m2.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.SSS2.p2.2.m2.1.1.1" xref="S2.SS2.SSS2.p2.2.m2.1.1.1.cmml">×</mo><mn id="S2.SS2.SSS2.p2.2.m2.1.1.3" xref="S2.SS2.SSS2.p2.2.m2.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p2.2.m2.1b"><apply id="S2.SS2.SSS2.p2.2.m2.1.1.cmml" xref="S2.SS2.SSS2.p2.2.m2.1.1"><times id="S2.SS2.SSS2.p2.2.m2.1.1.1.cmml" xref="S2.SS2.SSS2.p2.2.m2.1.1.1"></times><cn type="integer" id="S2.SS2.SSS2.p2.2.m2.1.1.2.cmml" xref="S2.SS2.SSS2.p2.2.m2.1.1.2">3</cn><cn type="integer" id="S2.SS2.SSS2.p2.2.m2.1.1.3.cmml" xref="S2.SS2.SSS2.p2.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p2.2.m2.1c">3\times 3</annotation></semantics></math>, and <math id="S2.SS2.SSS2.p2.3.m3.1" class="ltx_Math" alttext="5\times 5" display="inline"><semantics id="S2.SS2.SSS2.p2.3.m3.1a"><mrow id="S2.SS2.SSS2.p2.3.m3.1.1" xref="S2.SS2.SSS2.p2.3.m3.1.1.cmml"><mn id="S2.SS2.SSS2.p2.3.m3.1.1.2" xref="S2.SS2.SSS2.p2.3.m3.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.SSS2.p2.3.m3.1.1.1" xref="S2.SS2.SSS2.p2.3.m3.1.1.1.cmml">×</mo><mn id="S2.SS2.SSS2.p2.3.m3.1.1.3" xref="S2.SS2.SSS2.p2.3.m3.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p2.3.m3.1b"><apply id="S2.SS2.SSS2.p2.3.m3.1.1.cmml" xref="S2.SS2.SSS2.p2.3.m3.1.1"><times id="S2.SS2.SSS2.p2.3.m3.1.1.1.cmml" xref="S2.SS2.SSS2.p2.3.m3.1.1.1"></times><cn type="integer" id="S2.SS2.SSS2.p2.3.m3.1.1.2.cmml" xref="S2.SS2.SSS2.p2.3.m3.1.1.2">5</cn><cn type="integer" id="S2.SS2.SSS2.p2.3.m3.1.1.3.cmml" xref="S2.SS2.SSS2.p2.3.m3.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p2.3.m3.1c">5\times 5</annotation></semantics></math>) to obtain multiple receptive field features, which benefits multi-scale ship detection.</p>
</div>
</section>
<section id="S2.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS3.4.1.1" class="ltx_text">II-B</span>3 </span>Feature Pyramid Networks</h4>

<div id="S2.SS2.SSS3.p1" class="ltx_para">
<p id="S2.SS2.SSS3.p1.1" class="ltx_p">Pyramidal feature hierarchy methods use independent multi-level features for detection and ignore the complementary information between features at different levels, resulting in weak semantic information for low-level features. To tackle this problem, Lin <span id="S2.SS2.SSS3.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> proposed the feature pyramid network (FPN). As shown in Fig. <a href="#S2.F5" title="Figure 5 ‣ II-A Data Augmentation ‣ II Multi-scale Object Detection ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(d), the FPN introduces a top-down pathway to transfer the rich semantic information from high-level features to shallow-level features, leading to rich semantic features at all levels (please refer to the detailed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>). Thanks to the significant improvement of FPN for multi-scale object detection, FPN and its extensions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> play a dominant role in multi-scale feature representation.</p>
</div>
<div id="S2.SS2.SSS3.p2" class="ltx_para">
<p id="S2.SS2.SSS3.p2.1" class="ltx_p">Considering the extreme aspect ratios of geospatial objects (e.g., bridges, harbors, and airports), Hou <span id="S2.SS2.SSS3.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> proposed an asymmetric feature pyramid network (AFPN). The AFPN adopts the asymmetric convolution block to enhance the feature representation regarding the cross-shaped skeleton and improve the performance of large aspect ratio objects. Zhang <span id="S2.SS2.SSS3.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> designed a Laplacian feature pyramid network (LFPN) to inject high-frequency information into the multi-scale pyramidal feature representation, which is useful for accurate object detection but has been ignored by previous work. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>, Zhang <span id="S2.SS2.SSS3.p2.1.3" class="ltx_text ltx_font_italic">et al.</span> introduced the high-resolution feature pyramid network (HRFPN) to fully leverage the high-resolution feature representations, leading to precise and robust SAR ship detection. In addition, some researchers integrated the novel feature fusion module <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>, <a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>, attention machine <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>, <a href="#bib.bib75" title="" class="ltx_ref">75</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>, <a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>, or dilation convolution layer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>, <a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> into the FPN to further obtain a more discriminative multi-scale feature representation.</p>
</div>
<div id="S2.SS2.SSS3.p3" class="ltx_para">
<p id="S2.SS2.SSS3.p3.1" class="ltx_p">The FPN introduces a top-down pathway to transfer the high-level semantic information into the shallow layers, while the low-level spatial information is still lost in the top layers after the long-distance propagation in the backbone. Drawing on this problem, Fu <span id="S2.SS2.SSS3.p3.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> proposed a feature-fusion architecture (FFA) that integrates an auxiliary bottom-up pathway into the FPN structure to transfer the low-level spatial information to the top layers features via a short path, as depicted in Fig. <a href="#S2.F5" title="Figure 5 ‣ II-A Data Augmentation ‣ II Multi-scale Object Detection ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(e). The FFA ensures the detector extracts multi-scale feature pyramids with rich semantic and detailed spatial information. Similarly, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>, <a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>, the authors introduced a bidirectional FPN that learns the importance of different level features through learnable parameters and fuses the multi-level features through iteratively top-down and bottom-up pathways.</p>
</div>
<div id="S2.SS2.SSS3.p4" class="ltx_para">
<p id="S2.SS2.SSS3.p4.1" class="ltx_p">Different from the above sequential enhancement pathway <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>, some studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>, <a href="#bib.bib84" title="" class="ltx_ref">84</a>, <a href="#bib.bib85" title="" class="ltx_ref">85</a>, <a href="#bib.bib86" title="" class="ltx_ref">86</a>, <a href="#bib.bib87" title="" class="ltx_ref">87</a>, <a href="#bib.bib88" title="" class="ltx_ref">88</a>, <a href="#bib.bib89" title="" class="ltx_ref">89</a>, <a href="#bib.bib90" title="" class="ltx_ref">90</a>, <a href="#bib.bib91" title="" class="ltx_ref">91</a>, <a href="#bib.bib92" title="" class="ltx_ref">92</a>, <a href="#bib.bib93" title="" class="ltx_ref">93</a>, <a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite> adopt a cross-level feature fusion manner. As shown in Fig. <a href="#S2.F5" title="Figure 5 ‣ II-A Data Augmentation ‣ II Multi-scale Object Detection ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(f), the cross-level feature fusion methods fully collect the features at all levels to adaptively obtain balanced feature maps. Cheng <span id="S2.SS2.SSS3.p4.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite> utilized feature concatenation operation to achieve cross-scale feature fusion. Considering that features from different levels should have different contributions to the feature fusion, Fu <span id="S2.SS2.SSS3.p4.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite> proposed level-based attention to learn the unique contribution of features from each level. Thanks to the powerful global information extraction ability of the transformer structure, some work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>, <a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite> introduced the transformer structures to integrate and refine multi-level features. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>, Chen <span id="S2.SS2.SSS3.p4.1.3" class="ltx_text ltx_font_italic">et al.</span> presented a cascading attention network where position supervision is introduced to enhance the semantic information of multi-level features.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Multi-scale Anchor Generation</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Apart from the data augmentation and multi-scale feature representation methods, multi-scale anchor generation can also tackle the huge object scale variation problem in RSOD. Due to the difference in the scale range of objects in natural and remote sensing scenes, some studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>, <a href="#bib.bib96" title="" class="ltx_ref">96</a>, <a href="#bib.bib97" title="" class="ltx_ref">97</a>, <a href="#bib.bib98" title="" class="ltx_ref">98</a>, <a href="#bib.bib99" title="" class="ltx_ref">99</a>, <a href="#bib.bib100" title="" class="ltx_ref">100</a>, <a href="#bib.bib101" title="" class="ltx_ref">101</a>, <a href="#bib.bib102" title="" class="ltx_ref">102</a>, <a href="#bib.bib103" title="" class="ltx_ref">103</a>, <a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite> modify the anchor settings in common object detection to better cover the scales of geospatial objects.</p>
</div>
<figure id="S2.F6" class="ltx_figure"><img src="/html/2309.06751/assets/x6.png" id="S2.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="216" height="174" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>A brief summary of rotated object detection methods.</figcaption>
</figure>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Guo <span id="S2.SS3.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite> injected extra anchors with more scales and aspect ratios into the detector for multi-scale object detection. Dong <span id="S2.SS3.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite> designed more suitable anchor scales based on the statistics of object scales in the training set. Qiu <span id="S2.SS3.p2.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite> extended the original square RoI features into vertical, square, and horizontal RoI features and fused these RoI features to represent objects with different aspect ratios in a more flexible way. The above methods follow a fixed anchor setting, while current studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>, <a href="#bib.bib101" title="" class="ltx_ref">101</a>, <a href="#bib.bib102" title="" class="ltx_ref">102</a>, <a href="#bib.bib103" title="" class="ltx_ref">103</a>, <a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite> have attempted to dynamically learn the anchor during the training phase. Considering the aspect ratio variations between different categories, Hou <span id="S2.SS3.p2.1.4" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite> devised a novel self-adaptive aspect ratio anchor (SARA) to adaptively learn an appropriate aspect ratio for each category. The SARA embeds the learnable category-wise aspect ratio values into the regression branch to adaptively update the aspect ratio of each category with the gradient of the location regression loss. Inspired by GA-RPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>, some researchers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>, <a href="#bib.bib103" title="" class="ltx_ref">103</a>, <a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite> introduced a lightweight sub-network into the detector to adaptively learn the location and shape information of anchors.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Rotated Object Detection</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Arbitrary orientation of objects is another major challenge in RSOD. Since the objects in RSIs are acquired from a bird’s eye view, they exhibit the property of arbitrary orientations, so the widely used horizontal bounding box (HBB) representation in generic object detection is insufficient to locate rotated objects accurately. Therefore, numerous researchers have focused on the arbitrary orientation property of geospatial objects, which can be summarized into rotated object representation and rotation-invariant feature learning.
A brief summary of rotated object detection methods is depicted in Fig. <a href="#S2.F6" title="Figure 6 ‣ II-C Multi-scale Anchor Generation ‣ II Multi-scale Object Detection ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2309.06751/assets/x7.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="221" height="132" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Visualization of the five parameters representation and eight parameters representation methods for rotated objects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite>.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Rotated Object Representation</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Rotated object representation is essential for RSOD to avoid redundant backgrounds and obtain precise detection results. Recent rotated object representation methods can be mainly summarized into several categories: five parameters representation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>, <a href="#bib.bib108" title="" class="ltx_ref">108</a>, <a href="#bib.bib109" title="" class="ltx_ref">109</a>, <a href="#bib.bib110" title="" class="ltx_ref">110</a>, <a href="#bib.bib111" title="" class="ltx_ref">111</a>, <a href="#bib.bib112" title="" class="ltx_ref">112</a>, <a href="#bib.bib113" title="" class="ltx_ref">113</a>, <a href="#bib.bib114" title="" class="ltx_ref">114</a>, <a href="#bib.bib115" title="" class="ltx_ref">115</a>, <a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite>, eight parameters representation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib117" title="" class="ltx_ref">117</a>, <a href="#bib.bib118" title="" class="ltx_ref">118</a>, <a href="#bib.bib119" title="" class="ltx_ref">119</a>, <a href="#bib.bib120" title="" class="ltx_ref">120</a>, <a href="#bib.bib121" title="" class="ltx_ref">121</a>, <a href="#bib.bib122" title="" class="ltx_ref">122</a>, <a href="#bib.bib123" title="" class="ltx_ref">123</a>, <a href="#bib.bib124" title="" class="ltx_ref">124</a>, <a href="#bib.bib125" title="" class="ltx_ref">125</a>, <a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite>, angle classification representation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib127" title="" class="ltx_ref">127</a>, <a href="#bib.bib128" title="" class="ltx_ref">128</a>, <a href="#bib.bib106" title="" class="ltx_ref">106</a>, <a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite>, gaussian distribution representation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib130" title="" class="ltx_ref">130</a>, <a href="#bib.bib131" title="" class="ltx_ref">131</a>, <a href="#bib.bib132" title="" class="ltx_ref">132</a>, <a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite>, and others <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib134" title="" class="ltx_ref">134</a>, <a href="#bib.bib135" title="" class="ltx_ref">135</a>, <a href="#bib.bib136" title="" class="ltx_ref">136</a>, <a href="#bib.bib137" title="" class="ltx_ref">137</a>, <a href="#bib.bib138" title="" class="ltx_ref">138</a>, <a href="#bib.bib139" title="" class="ltx_ref">139</a>, <a href="#bib.bib140" title="" class="ltx_ref">140</a>, <a href="#bib.bib141" title="" class="ltx_ref">141</a>, <a href="#bib.bib142" title="" class="ltx_ref">142</a>, <a href="#bib.bib143" title="" class="ltx_ref">143</a>, <a href="#bib.bib144" title="" class="ltx_ref">144</a>]</cite>.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS1.4.1.1" class="ltx_text">III-A</span>1 </span>Five Parameters</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.6" class="ltx_p">The most popular solution is representing objects with a five-parameter method <math id="S3.SS1.SSS1.p1.1.m1.5" class="ltx_Math" alttext="(x,y,w,h,\theta)" display="inline"><semantics id="S3.SS1.SSS1.p1.1.m1.5a"><mrow id="S3.SS1.SSS1.p1.1.m1.5.6.2" xref="S3.SS1.SSS1.p1.1.m1.5.6.1.cmml"><mo stretchy="false" id="S3.SS1.SSS1.p1.1.m1.5.6.2.1" xref="S3.SS1.SSS1.p1.1.m1.5.6.1.cmml">(</mo><mi id="S3.SS1.SSS1.p1.1.m1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.cmml">x</mi><mo id="S3.SS1.SSS1.p1.1.m1.5.6.2.2" xref="S3.SS1.SSS1.p1.1.m1.5.6.1.cmml">,</mo><mi id="S3.SS1.SSS1.p1.1.m1.2.2" xref="S3.SS1.SSS1.p1.1.m1.2.2.cmml">y</mi><mo id="S3.SS1.SSS1.p1.1.m1.5.6.2.3" xref="S3.SS1.SSS1.p1.1.m1.5.6.1.cmml">,</mo><mi id="S3.SS1.SSS1.p1.1.m1.3.3" xref="S3.SS1.SSS1.p1.1.m1.3.3.cmml">w</mi><mo id="S3.SS1.SSS1.p1.1.m1.5.6.2.4" xref="S3.SS1.SSS1.p1.1.m1.5.6.1.cmml">,</mo><mi id="S3.SS1.SSS1.p1.1.m1.4.4" xref="S3.SS1.SSS1.p1.1.m1.4.4.cmml">h</mi><mo id="S3.SS1.SSS1.p1.1.m1.5.6.2.5" xref="S3.SS1.SSS1.p1.1.m1.5.6.1.cmml">,</mo><mi id="S3.SS1.SSS1.p1.1.m1.5.5" xref="S3.SS1.SSS1.p1.1.m1.5.5.cmml">θ</mi><mo stretchy="false" id="S3.SS1.SSS1.p1.1.m1.5.6.2.6" xref="S3.SS1.SSS1.p1.1.m1.5.6.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.1.m1.5b"><vector id="S3.SS1.SSS1.p1.1.m1.5.6.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.5.6.2"><ci id="S3.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1">𝑥</ci><ci id="S3.SS1.SSS1.p1.1.m1.2.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.2.2">𝑦</ci><ci id="S3.SS1.SSS1.p1.1.m1.3.3.cmml" xref="S3.SS1.SSS1.p1.1.m1.3.3">𝑤</ci><ci id="S3.SS1.SSS1.p1.1.m1.4.4.cmml" xref="S3.SS1.SSS1.p1.1.m1.4.4">ℎ</ci><ci id="S3.SS1.SSS1.p1.1.m1.5.5.cmml" xref="S3.SS1.SSS1.p1.1.m1.5.5">𝜃</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.1.m1.5c">(x,y,w,h,\theta)</annotation></semantics></math>, which simply adds an extra rotation angle parameter <math id="S3.SS1.SSS1.p1.2.m2.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS1.SSS1.p1.2.m2.1a"><mi id="S3.SS1.SSS1.p1.2.m2.1.1" xref="S3.SS1.SSS1.p1.2.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.2.m2.1b"><ci id="S3.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.2.m2.1c">\theta</annotation></semantics></math> on HBB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>, <a href="#bib.bib108" title="" class="ltx_ref">108</a>, <a href="#bib.bib109" title="" class="ltx_ref">109</a>, <a href="#bib.bib110" title="" class="ltx_ref">110</a>, <a href="#bib.bib111" title="" class="ltx_ref">111</a>, <a href="#bib.bib112" title="" class="ltx_ref">112</a>, <a href="#bib.bib113" title="" class="ltx_ref">113</a>, <a href="#bib.bib114" title="" class="ltx_ref">114</a>, <a href="#bib.bib115" title="" class="ltx_ref">115</a>]</cite>. The definition of the angular range plays a crucial role in such methods, where two kinds of definitions are derived. Some studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>, <a href="#bib.bib108" title="" class="ltx_ref">108</a>, <a href="#bib.bib109" title="" class="ltx_ref">109</a>, <a href="#bib.bib110" title="" class="ltx_ref">110</a>, <a href="#bib.bib111" title="" class="ltx_ref">111</a>, <a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite> define <math id="S3.SS1.SSS1.p1.3.m3.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS1.SSS1.p1.3.m3.1a"><mi id="S3.SS1.SSS1.p1.3.m3.1.1" xref="S3.SS1.SSS1.p1.3.m3.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.3.m3.1b"><ci id="S3.SS1.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.3.m3.1c">\theta</annotation></semantics></math> as the acute angle to the x-axis and restrict the angular range to 90<sup id="S3.SS1.SSS1.p1.6.1" class="ltx_sup"><span id="S3.SS1.SSS1.p1.6.1.1" class="ltx_text ltx_font_italic">∘</span></sup>, as shown in Fig. <a href="#S3.F7" title="Figure 7 ‣ III Rotated Object Detection ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>(a). As the most representative work, Yang <span id="S3.SS1.SSS1.p1.6.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite> followed the five parameters method to detect rotated objects and designed an IoU-aware loss function to tackle the boundary discontinuity problem of rotation angles. Another group <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>, <a href="#bib.bib114" title="" class="ltx_ref">114</a>, <a href="#bib.bib115" title="" class="ltx_ref">115</a>, <a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite> refers to <math id="S3.SS1.SSS1.p1.5.m5.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS1.SSS1.p1.5.m5.1a"><mi id="S3.SS1.SSS1.p1.5.m5.1.1" xref="S3.SS1.SSS1.p1.5.m5.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.5.m5.1b"><ci id="S3.SS1.SSS1.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS1.p1.5.m5.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.5.m5.1c">\theta</annotation></semantics></math> as the angle between the x-axis and the long side, whose range is 180<sup id="S3.SS1.SSS1.p1.6.3" class="ltx_sup"><span id="S3.SS1.SSS1.p1.6.3.1" class="ltx_text ltx_font_italic">∘</span></sup>, as depicted in Fig. <a href="#S3.F7" title="Figure 7 ‣ III Rotated Object Detection ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>(b). Ding <span id="S3.SS1.SSS1.p1.6.4" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite> regressed rotation angles by five-parameter methods and transformed the features of horizontal regions into rotated ones to facilitate rotated object detection.</p>
</div>
<figure id="S3.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F8.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S3.F8.sf1.1" class="ltx_block ltx_minipage ltx_align_bottom" style="width:99.7pt;">
<img src="" id="S3.F8.sf1.1.g1" class="ltx_graphics ltx_missing ltx_missing_image" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F8.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S3.F8.sf2.1" class="ltx_block ltx_minipage ltx_align_bottom" style="width:99.7pt;">
<img src="/html/2309.06751/assets/x9.png" id="S3.F8.sf2.1.g1" class="ltx_graphics ltx_img_square" width="415" height="438" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Boundary discontinuity challenge of five parameters method and eight parameters method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>, <a href="#bib.bib121" title="" class="ltx_ref">121</a>]</cite>. </figcaption>
</figure>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS2.4.1.1" class="ltx_text">III-A</span>2 </span>Eight Parameters</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">Different from five-parameter methods, eight-parameter methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib117" title="" class="ltx_ref">117</a>, <a href="#bib.bib118" title="" class="ltx_ref">118</a>, <a href="#bib.bib119" title="" class="ltx_ref">119</a>, <a href="#bib.bib120" title="" class="ltx_ref">120</a>, <a href="#bib.bib121" title="" class="ltx_ref">121</a>, <a href="#bib.bib122" title="" class="ltx_ref">122</a>, <a href="#bib.bib123" title="" class="ltx_ref">123</a>, <a href="#bib.bib124" title="" class="ltx_ref">124</a>, <a href="#bib.bib125" title="" class="ltx_ref">125</a>, <a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite> solve the issue of rotated object representation by directly regressing four vertices <math id="S3.SS1.SSS2.p1.1.m1.4" class="ltx_Math" alttext="\{(a_{x},a_{y}),(b_{x},b_{y}),(c_{x},c_{y}),(d_{x},d_{y})\}" display="inline"><semantics id="S3.SS1.SSS2.p1.1.m1.4a"><mrow id="S3.SS1.SSS2.p1.1.m1.4.4.4" xref="S3.SS1.SSS2.p1.1.m1.4.4.5.cmml"><mo stretchy="false" id="S3.SS1.SSS2.p1.1.m1.4.4.4.5" xref="S3.SS1.SSS2.p1.1.m1.4.4.5.cmml">{</mo><mrow id="S3.SS1.SSS2.p1.1.m1.1.1.1.1.2" xref="S3.SS1.SSS2.p1.1.m1.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.SS1.SSS2.p1.1.m1.1.1.1.1.2.3" xref="S3.SS1.SSS2.p1.1.m1.1.1.1.1.3.cmml">(</mo><msub id="S3.SS1.SSS2.p1.1.m1.1.1.1.1.1.1" xref="S3.SS1.SSS2.p1.1.m1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS2.p1.1.m1.1.1.1.1.1.1.2" xref="S3.SS1.SSS2.p1.1.m1.1.1.1.1.1.1.2.cmml">a</mi><mi id="S3.SS1.SSS2.p1.1.m1.1.1.1.1.1.1.3" xref="S3.SS1.SSS2.p1.1.m1.1.1.1.1.1.1.3.cmml">x</mi></msub><mo id="S3.SS1.SSS2.p1.1.m1.1.1.1.1.2.4" xref="S3.SS1.SSS2.p1.1.m1.1.1.1.1.3.cmml">,</mo><msub id="S3.SS1.SSS2.p1.1.m1.1.1.1.1.2.2" xref="S3.SS1.SSS2.p1.1.m1.1.1.1.1.2.2.cmml"><mi id="S3.SS1.SSS2.p1.1.m1.1.1.1.1.2.2.2" xref="S3.SS1.SSS2.p1.1.m1.1.1.1.1.2.2.2.cmml">a</mi><mi id="S3.SS1.SSS2.p1.1.m1.1.1.1.1.2.2.3" xref="S3.SS1.SSS2.p1.1.m1.1.1.1.1.2.2.3.cmml">y</mi></msub><mo stretchy="false" id="S3.SS1.SSS2.p1.1.m1.1.1.1.1.2.5" xref="S3.SS1.SSS2.p1.1.m1.1.1.1.1.3.cmml">)</mo></mrow><mo id="S3.SS1.SSS2.p1.1.m1.4.4.4.6" xref="S3.SS1.SSS2.p1.1.m1.4.4.5.cmml">,</mo><mrow id="S3.SS1.SSS2.p1.1.m1.2.2.2.2.2" xref="S3.SS1.SSS2.p1.1.m1.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.SSS2.p1.1.m1.2.2.2.2.2.3" xref="S3.SS1.SSS2.p1.1.m1.2.2.2.2.3.cmml">(</mo><msub id="S3.SS1.SSS2.p1.1.m1.2.2.2.2.1.1" xref="S3.SS1.SSS2.p1.1.m1.2.2.2.2.1.1.cmml"><mi id="S3.SS1.SSS2.p1.1.m1.2.2.2.2.1.1.2" xref="S3.SS1.SSS2.p1.1.m1.2.2.2.2.1.1.2.cmml">b</mi><mi id="S3.SS1.SSS2.p1.1.m1.2.2.2.2.1.1.3" xref="S3.SS1.SSS2.p1.1.m1.2.2.2.2.1.1.3.cmml">x</mi></msub><mo id="S3.SS1.SSS2.p1.1.m1.2.2.2.2.2.4" xref="S3.SS1.SSS2.p1.1.m1.2.2.2.2.3.cmml">,</mo><msub id="S3.SS1.SSS2.p1.1.m1.2.2.2.2.2.2" xref="S3.SS1.SSS2.p1.1.m1.2.2.2.2.2.2.cmml"><mi id="S3.SS1.SSS2.p1.1.m1.2.2.2.2.2.2.2" xref="S3.SS1.SSS2.p1.1.m1.2.2.2.2.2.2.2.cmml">b</mi><mi id="S3.SS1.SSS2.p1.1.m1.2.2.2.2.2.2.3" xref="S3.SS1.SSS2.p1.1.m1.2.2.2.2.2.2.3.cmml">y</mi></msub><mo stretchy="false" id="S3.SS1.SSS2.p1.1.m1.2.2.2.2.2.5" xref="S3.SS1.SSS2.p1.1.m1.2.2.2.2.3.cmml">)</mo></mrow><mo id="S3.SS1.SSS2.p1.1.m1.4.4.4.7" xref="S3.SS1.SSS2.p1.1.m1.4.4.5.cmml">,</mo><mrow id="S3.SS1.SSS2.p1.1.m1.3.3.3.3.2" xref="S3.SS1.SSS2.p1.1.m1.3.3.3.3.3.cmml"><mo stretchy="false" id="S3.SS1.SSS2.p1.1.m1.3.3.3.3.2.3" xref="S3.SS1.SSS2.p1.1.m1.3.3.3.3.3.cmml">(</mo><msub id="S3.SS1.SSS2.p1.1.m1.3.3.3.3.1.1" xref="S3.SS1.SSS2.p1.1.m1.3.3.3.3.1.1.cmml"><mi id="S3.SS1.SSS2.p1.1.m1.3.3.3.3.1.1.2" xref="S3.SS1.SSS2.p1.1.m1.3.3.3.3.1.1.2.cmml">c</mi><mi id="S3.SS1.SSS2.p1.1.m1.3.3.3.3.1.1.3" xref="S3.SS1.SSS2.p1.1.m1.3.3.3.3.1.1.3.cmml">x</mi></msub><mo id="S3.SS1.SSS2.p1.1.m1.3.3.3.3.2.4" xref="S3.SS1.SSS2.p1.1.m1.3.3.3.3.3.cmml">,</mo><msub id="S3.SS1.SSS2.p1.1.m1.3.3.3.3.2.2" xref="S3.SS1.SSS2.p1.1.m1.3.3.3.3.2.2.cmml"><mi id="S3.SS1.SSS2.p1.1.m1.3.3.3.3.2.2.2" xref="S3.SS1.SSS2.p1.1.m1.3.3.3.3.2.2.2.cmml">c</mi><mi id="S3.SS1.SSS2.p1.1.m1.3.3.3.3.2.2.3" xref="S3.SS1.SSS2.p1.1.m1.3.3.3.3.2.2.3.cmml">y</mi></msub><mo stretchy="false" id="S3.SS1.SSS2.p1.1.m1.3.3.3.3.2.5" xref="S3.SS1.SSS2.p1.1.m1.3.3.3.3.3.cmml">)</mo></mrow><mo id="S3.SS1.SSS2.p1.1.m1.4.4.4.8" xref="S3.SS1.SSS2.p1.1.m1.4.4.5.cmml">,</mo><mrow id="S3.SS1.SSS2.p1.1.m1.4.4.4.4.2" xref="S3.SS1.SSS2.p1.1.m1.4.4.4.4.3.cmml"><mo stretchy="false" id="S3.SS1.SSS2.p1.1.m1.4.4.4.4.2.3" xref="S3.SS1.SSS2.p1.1.m1.4.4.4.4.3.cmml">(</mo><msub id="S3.SS1.SSS2.p1.1.m1.4.4.4.4.1.1" xref="S3.SS1.SSS2.p1.1.m1.4.4.4.4.1.1.cmml"><mi id="S3.SS1.SSS2.p1.1.m1.4.4.4.4.1.1.2" xref="S3.SS1.SSS2.p1.1.m1.4.4.4.4.1.1.2.cmml">d</mi><mi id="S3.SS1.SSS2.p1.1.m1.4.4.4.4.1.1.3" xref="S3.SS1.SSS2.p1.1.m1.4.4.4.4.1.1.3.cmml">x</mi></msub><mo id="S3.SS1.SSS2.p1.1.m1.4.4.4.4.2.4" xref="S3.SS1.SSS2.p1.1.m1.4.4.4.4.3.cmml">,</mo><msub id="S3.SS1.SSS2.p1.1.m1.4.4.4.4.2.2" xref="S3.SS1.SSS2.p1.1.m1.4.4.4.4.2.2.cmml"><mi id="S3.SS1.SSS2.p1.1.m1.4.4.4.4.2.2.2" xref="S3.SS1.SSS2.p1.1.m1.4.4.4.4.2.2.2.cmml">d</mi><mi id="S3.SS1.SSS2.p1.1.m1.4.4.4.4.2.2.3" xref="S3.SS1.SSS2.p1.1.m1.4.4.4.4.2.2.3.cmml">y</mi></msub><mo stretchy="false" id="S3.SS1.SSS2.p1.1.m1.4.4.4.4.2.5" xref="S3.SS1.SSS2.p1.1.m1.4.4.4.4.3.cmml">)</mo></mrow><mo stretchy="false" id="S3.SS1.SSS2.p1.1.m1.4.4.4.9" xref="S3.SS1.SSS2.p1.1.m1.4.4.5.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.1.m1.4b"><set id="S3.SS1.SSS2.p1.1.m1.4.4.5.cmml" xref="S3.SS1.SSS2.p1.1.m1.4.4.4"><interval closure="open" id="S3.SS1.SSS2.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.1.1.2"><apply id="S3.SS1.SSS2.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.1.1.1.1.2">𝑎</ci><ci id="S3.SS1.SSS2.p1.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.1.1.1.1.3">𝑥</ci></apply><apply id="S3.SS1.SSS2.p1.1.m1.1.1.1.1.2.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.1.m1.1.1.1.1.2.2.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.SS1.SSS2.p1.1.m1.1.1.1.1.2.2.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.1.1.2.2.2">𝑎</ci><ci id="S3.SS1.SSS2.p1.1.m1.1.1.1.1.2.2.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.1.1.2.2.3">𝑦</ci></apply></interval><interval closure="open" id="S3.SS1.SSS2.p1.1.m1.2.2.2.2.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.2.2.2.2.2"><apply id="S3.SS1.SSS2.p1.1.m1.2.2.2.2.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.1.m1.2.2.2.2.1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.2.2.2.2.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.1.m1.2.2.2.2.1.1.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.2.2.2.2.1.1.2">𝑏</ci><ci id="S3.SS1.SSS2.p1.1.m1.2.2.2.2.1.1.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.2.2.2.2.1.1.3">𝑥</ci></apply><apply id="S3.SS1.SSS2.p1.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.1.m1.2.2.2.2.2.2.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.SSS2.p1.1.m1.2.2.2.2.2.2.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.2.2.2.2.2.2.2">𝑏</ci><ci id="S3.SS1.SSS2.p1.1.m1.2.2.2.2.2.2.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.2.2.2.2.2.2.3">𝑦</ci></apply></interval><interval closure="open" id="S3.SS1.SSS2.p1.1.m1.3.3.3.3.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.3.3.3.3.2"><apply id="S3.SS1.SSS2.p1.1.m1.3.3.3.3.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.3.3.3.3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.1.m1.3.3.3.3.1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.3.3.3.3.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.1.m1.3.3.3.3.1.1.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.3.3.3.3.1.1.2">𝑐</ci><ci id="S3.SS1.SSS2.p1.1.m1.3.3.3.3.1.1.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.3.3.3.3.1.1.3">𝑥</ci></apply><apply id="S3.SS1.SSS2.p1.1.m1.3.3.3.3.2.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.3.3.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.1.m1.3.3.3.3.2.2.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.3.3.3.3.2.2">subscript</csymbol><ci id="S3.SS1.SSS2.p1.1.m1.3.3.3.3.2.2.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.3.3.3.3.2.2.2">𝑐</ci><ci id="S3.SS1.SSS2.p1.1.m1.3.3.3.3.2.2.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.3.3.3.3.2.2.3">𝑦</ci></apply></interval><interval closure="open" id="S3.SS1.SSS2.p1.1.m1.4.4.4.4.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.4.4.4.4.2"><apply id="S3.SS1.SSS2.p1.1.m1.4.4.4.4.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.4.4.4.4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.1.m1.4.4.4.4.1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.4.4.4.4.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.1.m1.4.4.4.4.1.1.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.4.4.4.4.1.1.2">𝑑</ci><ci id="S3.SS1.SSS2.p1.1.m1.4.4.4.4.1.1.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.4.4.4.4.1.1.3">𝑥</ci></apply><apply id="S3.SS1.SSS2.p1.1.m1.4.4.4.4.2.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.4.4.4.4.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.1.m1.4.4.4.4.2.2.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.4.4.4.4.2.2">subscript</csymbol><ci id="S3.SS1.SSS2.p1.1.m1.4.4.4.4.2.2.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.4.4.4.4.2.2.2">𝑑</ci><ci id="S3.SS1.SSS2.p1.1.m1.4.4.4.4.2.2.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.4.4.4.4.2.2.3">𝑦</ci></apply></interval></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.1.m1.4c">\{(a_{x},a_{y}),(b_{x},b_{y}),(c_{x},c_{y}),(d_{x},d_{y})\}</annotation></semantics></math>, as shown in Fig. <a href="#S3.F7" title="Figure 7 ‣ III Rotated Object Detection ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>(c). Xia <span id="S3.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib117" title="" class="ltx_ref">117</a>]</cite> first adopted the eight-parameter method for rotated object representation, which directly supervises the detection model by minimizing the difference between each vertex and ground truth coordinates during training. However, the sequence order of these vertices is essential for the eight-parameter method to avoid unstable training. As shown in Fig. <a href="#S3.F8" title="Figure 8 ‣ III-A1 Five Parameters ‣ III-A Rotated Object Representation ‣ III Rotated Object Detection ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, it is intuitive that regressing targets from the red dotted arrow is an easier route, but the actual process follows the red solid arrows, which causes the difficulty of model training. To this end, Qian <span id="S3.SS1.SSS2.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>, <a href="#bib.bib121" title="" class="ltx_ref">121</a>]</cite> proposed a modulated loss function that calculates the losses under different sorted orders and selects the minimum case to learn, efficiently improving the detection performance.</p>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS3.4.1.1" class="ltx_text">III-A</span>3 </span>Angle Classification</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">To address the issue described in Fig. <a href="#S3.F8" title="Figure 8 ‣ III-A1 Five Parameters ‣ III-A Rotated Object Representation ‣ III Rotated Object Detection ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> from the source, many researchers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>, <a href="#bib.bib129" title="" class="ltx_ref">129</a>, <a href="#bib.bib127" title="" class="ltx_ref">127</a>, <a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite> take a detour from the boundary challenge of regression by transforming the angle prediction problem into an angle classification task. Yang <span id="S3.SS1.SSS3.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite> proposed the first angle classification method for rotated object detection, which converts the continuous angle into a discrete kind and trains the model with novel circular smooth labels. However, the angle classification head <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite> introduces additional parameters and degrades the detector’s efficiency. To overcome this, Yang <span id="S3.SS1.SSS3.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite> improved the <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite> with a densely coded label that ensures both the accuracy and efficiency of the model.</p>
</div>
</section>
<section id="S3.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS4.4.1.1" class="ltx_text">III-A</span>4 </span>Gaussian Distribution</h4>

<div id="S3.SS1.SSS4.p1" class="ltx_para">
<p id="S3.SS1.SSS4.p1.1" class="ltx_p">Although the above methods achieve promising progress, they do not consider the misalignment between the actual detection performance and optimization metric. Most recently, a series of works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib130" title="" class="ltx_ref">130</a>, <a href="#bib.bib131" title="" class="ltx_ref">131</a>, <a href="#bib.bib132" title="" class="ltx_ref">132</a>, <a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite> aims to handle this challenge by representing rotated objects with Gaussian distribution, as shown in Fig. <a href="#S3.F9" title="Figure 9 ‣ III-A5 Others ‣ III-A Rotated Object Representation ‣ III Rotated Object Detection ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. Specifically, these methods convert rotated objects into a 2D Gaussian distribution <math id="S3.SS1.SSS4.p1.1.m1.2" class="ltx_Math" alttext="\mathcal{N}\left({\mu},{\Sigma}\right)" display="inline"><semantics id="S3.SS1.SSS4.p1.1.m1.2a"><mrow id="S3.SS1.SSS4.p1.1.m1.2.3" xref="S3.SS1.SSS4.p1.1.m1.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS4.p1.1.m1.2.3.2" xref="S3.SS1.SSS4.p1.1.m1.2.3.2.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.1.m1.2.3.1" xref="S3.SS1.SSS4.p1.1.m1.2.3.1.cmml">​</mo><mrow id="S3.SS1.SSS4.p1.1.m1.2.3.3.2" xref="S3.SS1.SSS4.p1.1.m1.2.3.3.1.cmml"><mo id="S3.SS1.SSS4.p1.1.m1.2.3.3.2.1" xref="S3.SS1.SSS4.p1.1.m1.2.3.3.1.cmml">(</mo><mi id="S3.SS1.SSS4.p1.1.m1.1.1" xref="S3.SS1.SSS4.p1.1.m1.1.1.cmml">μ</mi><mo id="S3.SS1.SSS4.p1.1.m1.2.3.3.2.2" xref="S3.SS1.SSS4.p1.1.m1.2.3.3.1.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.SSS4.p1.1.m1.2.2" xref="S3.SS1.SSS4.p1.1.m1.2.2.cmml">Σ</mi><mo id="S3.SS1.SSS4.p1.1.m1.2.3.3.2.3" xref="S3.SS1.SSS4.p1.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS4.p1.1.m1.2b"><apply id="S3.SS1.SSS4.p1.1.m1.2.3.cmml" xref="S3.SS1.SSS4.p1.1.m1.2.3"><times id="S3.SS1.SSS4.p1.1.m1.2.3.1.cmml" xref="S3.SS1.SSS4.p1.1.m1.2.3.1"></times><ci id="S3.SS1.SSS4.p1.1.m1.2.3.2.cmml" xref="S3.SS1.SSS4.p1.1.m1.2.3.2">𝒩</ci><interval closure="open" id="S3.SS1.SSS4.p1.1.m1.2.3.3.1.cmml" xref="S3.SS1.SSS4.p1.1.m1.2.3.3.2"><ci id="S3.SS1.SSS4.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS4.p1.1.m1.1.1">𝜇</ci><ci id="S3.SS1.SSS4.p1.1.m1.2.2.cmml" xref="S3.SS1.SSS4.p1.1.m1.2.2">Σ</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS4.p1.1.m1.2c">\mathcal{N}\left({\mu},{\Sigma}\right)</annotation></semantics></math> as follows:</p>
<table id="S3.E1" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E1X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1X.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle{\Sigma}^{1/2}" display="inline"><semantics id="S3.E1X.2.1.1.m1.1a"><msup id="S3.E1X.2.1.1.m1.1.1" xref="S3.E1X.2.1.1.m1.1.1.cmml"><mi mathvariant="normal" id="S3.E1X.2.1.1.m1.1.1.2" xref="S3.E1X.2.1.1.m1.1.1.2.cmml">Σ</mi><mrow id="S3.E1X.2.1.1.m1.1.1.3" xref="S3.E1X.2.1.1.m1.1.1.3.cmml"><mn id="S3.E1X.2.1.1.m1.1.1.3.2" xref="S3.E1X.2.1.1.m1.1.1.3.2.cmml">1</mn><mo id="S3.E1X.2.1.1.m1.1.1.3.1" xref="S3.E1X.2.1.1.m1.1.1.3.1.cmml">/</mo><mn id="S3.E1X.2.1.1.m1.1.1.3.3" xref="S3.E1X.2.1.1.m1.1.1.3.3.cmml">2</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.E1X.2.1.1.m1.1b"><apply id="S3.E1X.2.1.1.m1.1.1.cmml" xref="S3.E1X.2.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.E1X.2.1.1.m1.1.1.1.cmml" xref="S3.E1X.2.1.1.m1.1.1">superscript</csymbol><ci id="S3.E1X.2.1.1.m1.1.1.2.cmml" xref="S3.E1X.2.1.1.m1.1.1.2">Σ</ci><apply id="S3.E1X.2.1.1.m1.1.1.3.cmml" xref="S3.E1X.2.1.1.m1.1.1.3"><divide id="S3.E1X.2.1.1.m1.1.1.3.1.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.1"></divide><cn type="integer" id="S3.E1X.2.1.1.m1.1.1.3.2.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.2">1</cn><cn type="integer" id="S3.E1X.2.1.1.m1.1.1.3.3.cmml" xref="S3.E1X.2.1.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1X.2.1.1.m1.1c">\displaystyle{\Sigma}^{1/2}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1X.3.2.2.m1.1" class="ltx_Math" alttext="\displaystyle=\mathbf{R}\mathbf{\Lambda}\mathbf{R}^{\top}" display="inline"><semantics id="S3.E1X.3.2.2.m1.1a"><mrow id="S3.E1X.3.2.2.m1.1.1" xref="S3.E1X.3.2.2.m1.1.1.cmml"><mi id="S3.E1X.3.2.2.m1.1.1.2" xref="S3.E1X.3.2.2.m1.1.1.2.cmml"></mi><mo id="S3.E1X.3.2.2.m1.1.1.1" xref="S3.E1X.3.2.2.m1.1.1.1.cmml">=</mo><mrow id="S3.E1X.3.2.2.m1.1.1.3" xref="S3.E1X.3.2.2.m1.1.1.3.cmml"><mi id="S3.E1X.3.2.2.m1.1.1.3.2" xref="S3.E1X.3.2.2.m1.1.1.3.2.cmml">𝐑</mi><mo lspace="0em" rspace="0em" id="S3.E1X.3.2.2.m1.1.1.3.1" xref="S3.E1X.3.2.2.m1.1.1.3.1.cmml">​</mo><mi id="S3.E1X.3.2.2.m1.1.1.3.3" xref="S3.E1X.3.2.2.m1.1.1.3.3.cmml">𝚲</mi><mo lspace="0em" rspace="0em" id="S3.E1X.3.2.2.m1.1.1.3.1a" xref="S3.E1X.3.2.2.m1.1.1.3.1.cmml">​</mo><msup id="S3.E1X.3.2.2.m1.1.1.3.4" xref="S3.E1X.3.2.2.m1.1.1.3.4.cmml"><mi id="S3.E1X.3.2.2.m1.1.1.3.4.2" xref="S3.E1X.3.2.2.m1.1.1.3.4.2.cmml">𝐑</mi><mo id="S3.E1X.3.2.2.m1.1.1.3.4.3" xref="S3.E1X.3.2.2.m1.1.1.3.4.3.cmml">⊤</mo></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1X.3.2.2.m1.1b"><apply id="S3.E1X.3.2.2.m1.1.1.cmml" xref="S3.E1X.3.2.2.m1.1.1"><eq id="S3.E1X.3.2.2.m1.1.1.1.cmml" xref="S3.E1X.3.2.2.m1.1.1.1"></eq><csymbol cd="latexml" id="S3.E1X.3.2.2.m1.1.1.2.cmml" xref="S3.E1X.3.2.2.m1.1.1.2">absent</csymbol><apply id="S3.E1X.3.2.2.m1.1.1.3.cmml" xref="S3.E1X.3.2.2.m1.1.1.3"><times id="S3.E1X.3.2.2.m1.1.1.3.1.cmml" xref="S3.E1X.3.2.2.m1.1.1.3.1"></times><ci id="S3.E1X.3.2.2.m1.1.1.3.2.cmml" xref="S3.E1X.3.2.2.m1.1.1.3.2">𝐑</ci><ci id="S3.E1X.3.2.2.m1.1.1.3.3.cmml" xref="S3.E1X.3.2.2.m1.1.1.3.3">𝚲</ci><apply id="S3.E1X.3.2.2.m1.1.1.3.4.cmml" xref="S3.E1X.3.2.2.m1.1.1.3.4"><csymbol cd="ambiguous" id="S3.E1X.3.2.2.m1.1.1.3.4.1.cmml" xref="S3.E1X.3.2.2.m1.1.1.3.4">superscript</csymbol><ci id="S3.E1X.3.2.2.m1.1.1.3.4.2.cmml" xref="S3.E1X.3.2.2.m1.1.1.3.4.2">𝐑</ci><csymbol cd="latexml" id="S3.E1X.3.2.2.m1.1.1.3.4.3.cmml" xref="S3.E1X.3.2.2.m1.1.1.3.4.3">top</csymbol></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1X.3.2.2.m1.1c">\displaystyle=\mathbf{R}\mathbf{\Lambda}\mathbf{R}^{\top}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="4" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(1)</span></td>
</tr>
<tr id="S3.E1Xa" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1Xa.2.1.1.m1.3" class="ltx_Math" alttext="\displaystyle=\left(\begin{array}[]{cc}\cos\theta&amp;-\sin\theta\\
\sin\theta&amp;\cos\theta\end{array}\right)\left(\begin{array}[]{cc}\frac{w}{2}&amp;0\\
0&amp;\frac{h}{2}\end{array}\right)\left(\begin{array}[]{cc}\cos\theta&amp;\sin\theta\\
-\sin\theta&amp;\cos\theta\end{array}\right)" display="inline"><semantics id="S3.E1Xa.2.1.1.m1.3a"><mrow id="S3.E1Xa.2.1.1.m1.3.4" xref="S3.E1Xa.2.1.1.m1.3.4.cmml"><mi id="S3.E1Xa.2.1.1.m1.3.4.2" xref="S3.E1Xa.2.1.1.m1.3.4.2.cmml"></mi><mo id="S3.E1Xa.2.1.1.m1.3.4.1" xref="S3.E1Xa.2.1.1.m1.3.4.1.cmml">=</mo><mrow id="S3.E1Xa.2.1.1.m1.3.4.3" xref="S3.E1Xa.2.1.1.m1.3.4.3.cmml"><mrow id="S3.E1Xa.2.1.1.m1.3.4.3.2.2" xref="S3.E1Xa.2.1.1.m1.1.1.cmml"><mo id="S3.E1Xa.2.1.1.m1.3.4.3.2.2.1" xref="S3.E1Xa.2.1.1.m1.1.1.cmml">(</mo><mtable columnspacing="5pt" rowspacing="0pt" id="S3.E1Xa.2.1.1.m1.1.1" xref="S3.E1Xa.2.1.1.m1.1.1.cmml"><mtr id="S3.E1Xa.2.1.1.m1.1.1a" xref="S3.E1Xa.2.1.1.m1.1.1.cmml"><mtd id="S3.E1Xa.2.1.1.m1.1.1b" xref="S3.E1Xa.2.1.1.m1.1.1.cmml"><mrow id="S3.E1Xa.2.1.1.m1.1.1.1.1.1" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.cmml"><mi id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.1" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.1.cmml">cos</mi><mo lspace="0.167em" id="S3.E1Xa.2.1.1.m1.1.1.1.1.1a" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.cmml">⁡</mo><mi id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.2" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.2.cmml">θ</mi></mrow></mtd><mtd id="S3.E1Xa.2.1.1.m1.1.1c" xref="S3.E1Xa.2.1.1.m1.1.1.cmml"><mrow id="S3.E1Xa.2.1.1.m1.1.1.1.2.1" xref="S3.E1Xa.2.1.1.m1.1.1.1.2.1.cmml"><mo rspace="0.167em" id="S3.E1Xa.2.1.1.m1.1.1.1.2.1a" xref="S3.E1Xa.2.1.1.m1.1.1.1.2.1.cmml">−</mo><mrow id="S3.E1Xa.2.1.1.m1.1.1.1.2.1.2" xref="S3.E1Xa.2.1.1.m1.1.1.1.2.1.2.cmml"><mi id="S3.E1Xa.2.1.1.m1.1.1.1.2.1.2.1" xref="S3.E1Xa.2.1.1.m1.1.1.1.2.1.2.1.cmml">sin</mi><mo lspace="0.167em" id="S3.E1Xa.2.1.1.m1.1.1.1.2.1.2a" xref="S3.E1Xa.2.1.1.m1.1.1.1.2.1.2.cmml">⁡</mo><mi id="S3.E1Xa.2.1.1.m1.1.1.1.2.1.2.2" xref="S3.E1Xa.2.1.1.m1.1.1.1.2.1.2.2.cmml">θ</mi></mrow></mrow></mtd></mtr><mtr id="S3.E1Xa.2.1.1.m1.1.1d" xref="S3.E1Xa.2.1.1.m1.1.1.cmml"><mtd id="S3.E1Xa.2.1.1.m1.1.1e" xref="S3.E1Xa.2.1.1.m1.1.1.cmml"><mrow id="S3.E1Xa.2.1.1.m1.1.1.2.1.1" xref="S3.E1Xa.2.1.1.m1.1.1.2.1.1.cmml"><mi id="S3.E1Xa.2.1.1.m1.1.1.2.1.1.1" xref="S3.E1Xa.2.1.1.m1.1.1.2.1.1.1.cmml">sin</mi><mo lspace="0.167em" id="S3.E1Xa.2.1.1.m1.1.1.2.1.1a" xref="S3.E1Xa.2.1.1.m1.1.1.2.1.1.cmml">⁡</mo><mi id="S3.E1Xa.2.1.1.m1.1.1.2.1.1.2" xref="S3.E1Xa.2.1.1.m1.1.1.2.1.1.2.cmml">θ</mi></mrow></mtd><mtd id="S3.E1Xa.2.1.1.m1.1.1f" xref="S3.E1Xa.2.1.1.m1.1.1.cmml"><mrow id="S3.E1Xa.2.1.1.m1.1.1.2.2.1" xref="S3.E1Xa.2.1.1.m1.1.1.2.2.1.cmml"><mi id="S3.E1Xa.2.1.1.m1.1.1.2.2.1.1" xref="S3.E1Xa.2.1.1.m1.1.1.2.2.1.1.cmml">cos</mi><mo lspace="0.167em" id="S3.E1Xa.2.1.1.m1.1.1.2.2.1a" xref="S3.E1Xa.2.1.1.m1.1.1.2.2.1.cmml">⁡</mo><mi id="S3.E1Xa.2.1.1.m1.1.1.2.2.1.2" xref="S3.E1Xa.2.1.1.m1.1.1.2.2.1.2.cmml">θ</mi></mrow></mtd></mtr></mtable><mo id="S3.E1Xa.2.1.1.m1.3.4.3.2.2.2" xref="S3.E1Xa.2.1.1.m1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E1Xa.2.1.1.m1.3.4.3.1" xref="S3.E1Xa.2.1.1.m1.3.4.3.1.cmml">​</mo><mrow id="S3.E1Xa.2.1.1.m1.3.4.3.3.2" xref="S3.E1Xa.2.1.1.m1.2.2.cmml"><mo id="S3.E1Xa.2.1.1.m1.3.4.3.3.2.1" xref="S3.E1Xa.2.1.1.m1.2.2.cmml">(</mo><mtable columnspacing="5pt" rowspacing="0pt" id="S3.E1Xa.2.1.1.m1.2.2" xref="S3.E1Xa.2.1.1.m1.2.2.cmml"><mtr id="S3.E1Xa.2.1.1.m1.2.2a" xref="S3.E1Xa.2.1.1.m1.2.2.cmml"><mtd id="S3.E1Xa.2.1.1.m1.2.2b" xref="S3.E1Xa.2.1.1.m1.2.2.cmml"><mfrac id="S3.E1Xa.2.1.1.m1.2.2.1.1.1" xref="S3.E1Xa.2.1.1.m1.2.2.1.1.1.cmml"><mi id="S3.E1Xa.2.1.1.m1.2.2.1.1.1.2" xref="S3.E1Xa.2.1.1.m1.2.2.1.1.1.2.cmml">w</mi><mn id="S3.E1Xa.2.1.1.m1.2.2.1.1.1.3" xref="S3.E1Xa.2.1.1.m1.2.2.1.1.1.3.cmml">2</mn></mfrac></mtd><mtd id="S3.E1Xa.2.1.1.m1.2.2c" xref="S3.E1Xa.2.1.1.m1.2.2.cmml"><mn id="S3.E1Xa.2.1.1.m1.2.2.1.2.1" xref="S3.E1Xa.2.1.1.m1.2.2.1.2.1.cmml">0</mn></mtd></mtr><mtr id="S3.E1Xa.2.1.1.m1.2.2d" xref="S3.E1Xa.2.1.1.m1.2.2.cmml"><mtd id="S3.E1Xa.2.1.1.m1.2.2e" xref="S3.E1Xa.2.1.1.m1.2.2.cmml"><mn id="S3.E1Xa.2.1.1.m1.2.2.2.1.1" xref="S3.E1Xa.2.1.1.m1.2.2.2.1.1.cmml">0</mn></mtd><mtd id="S3.E1Xa.2.1.1.m1.2.2f" xref="S3.E1Xa.2.1.1.m1.2.2.cmml"><mfrac id="S3.E1Xa.2.1.1.m1.2.2.2.2.1" xref="S3.E1Xa.2.1.1.m1.2.2.2.2.1.cmml"><mi id="S3.E1Xa.2.1.1.m1.2.2.2.2.1.2" xref="S3.E1Xa.2.1.1.m1.2.2.2.2.1.2.cmml">h</mi><mn id="S3.E1Xa.2.1.1.m1.2.2.2.2.1.3" xref="S3.E1Xa.2.1.1.m1.2.2.2.2.1.3.cmml">2</mn></mfrac></mtd></mtr></mtable><mo id="S3.E1Xa.2.1.1.m1.3.4.3.3.2.2" xref="S3.E1Xa.2.1.1.m1.2.2.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E1Xa.2.1.1.m1.3.4.3.1a" xref="S3.E1Xa.2.1.1.m1.3.4.3.1.cmml">​</mo><mrow id="S3.E1Xa.2.1.1.m1.3.4.3.4.2" xref="S3.E1Xa.2.1.1.m1.3.3.cmml"><mo id="S3.E1Xa.2.1.1.m1.3.4.3.4.2.1" xref="S3.E1Xa.2.1.1.m1.3.3.cmml">(</mo><mtable columnspacing="5pt" rowspacing="0pt" id="S3.E1Xa.2.1.1.m1.3.3" xref="S3.E1Xa.2.1.1.m1.3.3.cmml"><mtr id="S3.E1Xa.2.1.1.m1.3.3a" xref="S3.E1Xa.2.1.1.m1.3.3.cmml"><mtd id="S3.E1Xa.2.1.1.m1.3.3b" xref="S3.E1Xa.2.1.1.m1.3.3.cmml"><mrow id="S3.E1Xa.2.1.1.m1.3.3.1.1.1" xref="S3.E1Xa.2.1.1.m1.3.3.1.1.1.cmml"><mi id="S3.E1Xa.2.1.1.m1.3.3.1.1.1.1" xref="S3.E1Xa.2.1.1.m1.3.3.1.1.1.1.cmml">cos</mi><mo lspace="0.167em" id="S3.E1Xa.2.1.1.m1.3.3.1.1.1a" xref="S3.E1Xa.2.1.1.m1.3.3.1.1.1.cmml">⁡</mo><mi id="S3.E1Xa.2.1.1.m1.3.3.1.1.1.2" xref="S3.E1Xa.2.1.1.m1.3.3.1.1.1.2.cmml">θ</mi></mrow></mtd><mtd id="S3.E1Xa.2.1.1.m1.3.3c" xref="S3.E1Xa.2.1.1.m1.3.3.cmml"><mrow id="S3.E1Xa.2.1.1.m1.3.3.1.2.1" xref="S3.E1Xa.2.1.1.m1.3.3.1.2.1.cmml"><mi id="S3.E1Xa.2.1.1.m1.3.3.1.2.1.1" xref="S3.E1Xa.2.1.1.m1.3.3.1.2.1.1.cmml">sin</mi><mo lspace="0.167em" id="S3.E1Xa.2.1.1.m1.3.3.1.2.1a" xref="S3.E1Xa.2.1.1.m1.3.3.1.2.1.cmml">⁡</mo><mi id="S3.E1Xa.2.1.1.m1.3.3.1.2.1.2" xref="S3.E1Xa.2.1.1.m1.3.3.1.2.1.2.cmml">θ</mi></mrow></mtd></mtr><mtr id="S3.E1Xa.2.1.1.m1.3.3d" xref="S3.E1Xa.2.1.1.m1.3.3.cmml"><mtd id="S3.E1Xa.2.1.1.m1.3.3e" xref="S3.E1Xa.2.1.1.m1.3.3.cmml"><mrow id="S3.E1Xa.2.1.1.m1.3.3.2.1.1" xref="S3.E1Xa.2.1.1.m1.3.3.2.1.1.cmml"><mo rspace="0.167em" id="S3.E1Xa.2.1.1.m1.3.3.2.1.1a" xref="S3.E1Xa.2.1.1.m1.3.3.2.1.1.cmml">−</mo><mrow id="S3.E1Xa.2.1.1.m1.3.3.2.1.1.2" xref="S3.E1Xa.2.1.1.m1.3.3.2.1.1.2.cmml"><mi id="S3.E1Xa.2.1.1.m1.3.3.2.1.1.2.1" xref="S3.E1Xa.2.1.1.m1.3.3.2.1.1.2.1.cmml">sin</mi><mo lspace="0.167em" id="S3.E1Xa.2.1.1.m1.3.3.2.1.1.2a" xref="S3.E1Xa.2.1.1.m1.3.3.2.1.1.2.cmml">⁡</mo><mi id="S3.E1Xa.2.1.1.m1.3.3.2.1.1.2.2" xref="S3.E1Xa.2.1.1.m1.3.3.2.1.1.2.2.cmml">θ</mi></mrow></mrow></mtd><mtd id="S3.E1Xa.2.1.1.m1.3.3f" xref="S3.E1Xa.2.1.1.m1.3.3.cmml"><mrow id="S3.E1Xa.2.1.1.m1.3.3.2.2.1" xref="S3.E1Xa.2.1.1.m1.3.3.2.2.1.cmml"><mi id="S3.E1Xa.2.1.1.m1.3.3.2.2.1.1" xref="S3.E1Xa.2.1.1.m1.3.3.2.2.1.1.cmml">cos</mi><mo lspace="0.167em" id="S3.E1Xa.2.1.1.m1.3.3.2.2.1a" xref="S3.E1Xa.2.1.1.m1.3.3.2.2.1.cmml">⁡</mo><mi id="S3.E1Xa.2.1.1.m1.3.3.2.2.1.2" xref="S3.E1Xa.2.1.1.m1.3.3.2.2.1.2.cmml">θ</mi></mrow></mtd></mtr></mtable><mo id="S3.E1Xa.2.1.1.m1.3.4.3.4.2.2" xref="S3.E1Xa.2.1.1.m1.3.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1Xa.2.1.1.m1.3b"><apply id="S3.E1Xa.2.1.1.m1.3.4.cmml" xref="S3.E1Xa.2.1.1.m1.3.4"><eq id="S3.E1Xa.2.1.1.m1.3.4.1.cmml" xref="S3.E1Xa.2.1.1.m1.3.4.1"></eq><csymbol cd="latexml" id="S3.E1Xa.2.1.1.m1.3.4.2.cmml" xref="S3.E1Xa.2.1.1.m1.3.4.2">absent</csymbol><apply id="S3.E1Xa.2.1.1.m1.3.4.3.cmml" xref="S3.E1Xa.2.1.1.m1.3.4.3"><times id="S3.E1Xa.2.1.1.m1.3.4.3.1.cmml" xref="S3.E1Xa.2.1.1.m1.3.4.3.1"></times><matrix id="S3.E1Xa.2.1.1.m1.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.3.4.3.2.2"><matrixrow id="S3.E1Xa.2.1.1.m1.1.1a.cmml" xref="S3.E1Xa.2.1.1.m1.3.4.3.2.2"><apply id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1"><cos id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.1"></cos><ci id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.2.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.2">𝜃</ci></apply><apply id="S3.E1Xa.2.1.1.m1.1.1.1.2.1.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.1.2.1"><minus id="S3.E1Xa.2.1.1.m1.1.1.1.2.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.1.2.1"></minus><apply id="S3.E1Xa.2.1.1.m1.1.1.1.2.1.2.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.1.2.1.2"><sin id="S3.E1Xa.2.1.1.m1.1.1.1.2.1.2.1.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.1.2.1.2.1"></sin><ci id="S3.E1Xa.2.1.1.m1.1.1.1.2.1.2.2.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.1.2.1.2.2">𝜃</ci></apply></apply></matrixrow><matrixrow id="S3.E1Xa.2.1.1.m1.1.1b.cmml" xref="S3.E1Xa.2.1.1.m1.3.4.3.2.2"><apply id="S3.E1Xa.2.1.1.m1.1.1.2.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.2.1.1"><sin id="S3.E1Xa.2.1.1.m1.1.1.2.1.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.2.1.1.1"></sin><ci id="S3.E1Xa.2.1.1.m1.1.1.2.1.1.2.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.2.1.1.2">𝜃</ci></apply><apply id="S3.E1Xa.2.1.1.m1.1.1.2.2.1.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.2.2.1"><cos id="S3.E1Xa.2.1.1.m1.1.1.2.2.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.2.2.1.1"></cos><ci id="S3.E1Xa.2.1.1.m1.1.1.2.2.1.2.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.2.2.1.2">𝜃</ci></apply></matrixrow></matrix><matrix id="S3.E1Xa.2.1.1.m1.2.2.cmml" xref="S3.E1Xa.2.1.1.m1.3.4.3.3.2"><matrixrow id="S3.E1Xa.2.1.1.m1.2.2a.cmml" xref="S3.E1Xa.2.1.1.m1.3.4.3.3.2"><apply id="S3.E1Xa.2.1.1.m1.2.2.1.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.1.1.1"><divide id="S3.E1Xa.2.1.1.m1.2.2.1.1.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.1.1.1"></divide><ci id="S3.E1Xa.2.1.1.m1.2.2.1.1.1.2.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.1.1.1.2">𝑤</ci><cn type="integer" id="S3.E1Xa.2.1.1.m1.2.2.1.1.1.3.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.1.1.1.3">2</cn></apply><cn type="integer" id="S3.E1Xa.2.1.1.m1.2.2.1.2.1.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.1.2.1">0</cn></matrixrow><matrixrow id="S3.E1Xa.2.1.1.m1.2.2b.cmml" xref="S3.E1Xa.2.1.1.m1.3.4.3.3.2"><cn type="integer" id="S3.E1Xa.2.1.1.m1.2.2.2.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.2.1.1">0</cn><apply id="S3.E1Xa.2.1.1.m1.2.2.2.2.1.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.2.2.1"><divide id="S3.E1Xa.2.1.1.m1.2.2.2.2.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.2.2.1"></divide><ci id="S3.E1Xa.2.1.1.m1.2.2.2.2.1.2.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.2.2.1.2">ℎ</ci><cn type="integer" id="S3.E1Xa.2.1.1.m1.2.2.2.2.1.3.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.2.2.1.3">2</cn></apply></matrixrow></matrix><matrix id="S3.E1Xa.2.1.1.m1.3.3.cmml" xref="S3.E1Xa.2.1.1.m1.3.4.3.4.2"><matrixrow id="S3.E1Xa.2.1.1.m1.3.3a.cmml" xref="S3.E1Xa.2.1.1.m1.3.4.3.4.2"><apply id="S3.E1Xa.2.1.1.m1.3.3.1.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.3.3.1.1.1"><cos id="S3.E1Xa.2.1.1.m1.3.3.1.1.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.3.3.1.1.1.1"></cos><ci id="S3.E1Xa.2.1.1.m1.3.3.1.1.1.2.cmml" xref="S3.E1Xa.2.1.1.m1.3.3.1.1.1.2">𝜃</ci></apply><apply id="S3.E1Xa.2.1.1.m1.3.3.1.2.1.cmml" xref="S3.E1Xa.2.1.1.m1.3.3.1.2.1"><sin id="S3.E1Xa.2.1.1.m1.3.3.1.2.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.3.3.1.2.1.1"></sin><ci id="S3.E1Xa.2.1.1.m1.3.3.1.2.1.2.cmml" xref="S3.E1Xa.2.1.1.m1.3.3.1.2.1.2">𝜃</ci></apply></matrixrow><matrixrow id="S3.E1Xa.2.1.1.m1.3.3b.cmml" xref="S3.E1Xa.2.1.1.m1.3.4.3.4.2"><apply id="S3.E1Xa.2.1.1.m1.3.3.2.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.3.3.2.1.1"><minus id="S3.E1Xa.2.1.1.m1.3.3.2.1.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.3.3.2.1.1"></minus><apply id="S3.E1Xa.2.1.1.m1.3.3.2.1.1.2.cmml" xref="S3.E1Xa.2.1.1.m1.3.3.2.1.1.2"><sin id="S3.E1Xa.2.1.1.m1.3.3.2.1.1.2.1.cmml" xref="S3.E1Xa.2.1.1.m1.3.3.2.1.1.2.1"></sin><ci id="S3.E1Xa.2.1.1.m1.3.3.2.1.1.2.2.cmml" xref="S3.E1Xa.2.1.1.m1.3.3.2.1.1.2.2">𝜃</ci></apply></apply><apply id="S3.E1Xa.2.1.1.m1.3.3.2.2.1.cmml" xref="S3.E1Xa.2.1.1.m1.3.3.2.2.1"><cos id="S3.E1Xa.2.1.1.m1.3.3.2.2.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.3.3.2.2.1.1"></cos><ci id="S3.E1Xa.2.1.1.m1.3.3.2.2.1.2.cmml" xref="S3.E1Xa.2.1.1.m1.3.3.2.2.1.2">𝜃</ci></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1Xa.2.1.1.m1.3c">\displaystyle=\left(\begin{array}[]{cc}\cos\theta&amp;-\sin\theta\\
\sin\theta&amp;\cos\theta\end{array}\right)\left(\begin{array}[]{cc}\frac{w}{2}&amp;0\\
0&amp;\frac{h}{2}\end{array}\right)\left(\begin{array}[]{cc}\cos\theta&amp;\sin\theta\\
-\sin\theta&amp;\cos\theta\end{array}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr id="S3.E1Xb" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1Xb.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle=\left(\begin{array}[]{cc}\frac{w}{2}\cos^{2}\theta+\frac{h}{2}\sin^{2}\theta&amp;\frac{w-h}{2}\cos\theta\sin\theta\\
\frac{w-h}{2}\cos\theta\sin\theta&amp;\frac{w}{2}\sin^{2}\theta+\frac{h}{2}\cos^{2}\theta\end{array}\right)" display="inline"><semantics id="S3.E1Xb.2.1.1.m1.1a"><mrow id="S3.E1Xb.2.1.1.m1.1.2" xref="S3.E1Xb.2.1.1.m1.1.2.cmml"><mi id="S3.E1Xb.2.1.1.m1.1.2.2" xref="S3.E1Xb.2.1.1.m1.1.2.2.cmml"></mi><mo id="S3.E1Xb.2.1.1.m1.1.2.1" xref="S3.E1Xb.2.1.1.m1.1.2.1.cmml">=</mo><mrow id="S3.E1Xb.2.1.1.m1.1.2.3.2" xref="S3.E1Xb.2.1.1.m1.1.1.cmml"><mo id="S3.E1Xb.2.1.1.m1.1.2.3.2.1" xref="S3.E1Xb.2.1.1.m1.1.1.cmml">(</mo><mtable columnspacing="5pt" rowspacing="0pt" id="S3.E1Xb.2.1.1.m1.1.1" xref="S3.E1Xb.2.1.1.m1.1.1.cmml"><mtr id="S3.E1Xb.2.1.1.m1.1.1a" xref="S3.E1Xb.2.1.1.m1.1.1.cmml"><mtd id="S3.E1Xb.2.1.1.m1.1.1b" xref="S3.E1Xb.2.1.1.m1.1.1.cmml"><mrow id="S3.E1Xb.2.1.1.m1.1.1.1.1.1" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.cmml"><mrow id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.cmml"><mfrac id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.2" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.2.cmml"><mi id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.2.2" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.2.2.cmml">w</mi><mn id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.2.3" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.2.3.cmml">2</mn></mfrac><mo lspace="0.167em" rspace="0em" id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.1" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.1.cmml">​</mo><mrow id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.3" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.3.cmml"><msup id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.3.1" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.3.1.cmml"><mi id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.3.1.2" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.3.1.2.cmml">cos</mi><mn id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.3.1.3" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.3.1.3.cmml">2</mn></msup><mo lspace="0.167em" id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.3a" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.3.cmml">⁡</mo><mi id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.3.2" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.3.2.cmml">θ</mi></mrow></mrow><mo id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.cmml"><mfrac id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.2" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.2.cmml"><mi id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.2.2" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.2.2.cmml">h</mi><mn id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.2.3" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.2.3.cmml">2</mn></mfrac><mo lspace="0.167em" rspace="0em" id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.1" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.1.cmml">​</mo><mrow id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.3" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.3.cmml"><msup id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.3.1" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.3.1.cmml"><mi id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.3.1.2" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.3.1.2.cmml">sin</mi><mn id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.3.1.3" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.3.1.3.cmml">2</mn></msup><mo lspace="0.167em" id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.3a" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.3.cmml">⁡</mo><mi id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.3.2" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.3.2.cmml">θ</mi></mrow></mrow></mrow></mtd><mtd id="S3.E1Xb.2.1.1.m1.1.1c" xref="S3.E1Xb.2.1.1.m1.1.1.cmml"><mrow id="S3.E1Xb.2.1.1.m1.1.1.1.2.1" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.cmml"><mfrac id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.2" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.2.cmml"><mrow id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.2.2" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.2.2.cmml"><mi id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.2.2.2" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.2.2.2.cmml">w</mi><mo id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.2.2.1" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.2.2.1.cmml">−</mo><mi id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.2.2.3" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.2.2.3.cmml">h</mi></mrow><mn id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.2.3" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.2.3.cmml">2</mn></mfrac><mo lspace="0.167em" rspace="0em" id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.1" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.1.cmml">​</mo><mrow id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.3" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.3.cmml"><mi id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.3.1" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.3.1.cmml">cos</mi><mo lspace="0.167em" id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.3a" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.3.cmml">⁡</mo><mi id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.3.2" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.3.2.cmml">θ</mi></mrow><mo lspace="0.167em" rspace="0em" id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.1a" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.1.cmml">​</mo><mrow id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.4" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.4.cmml"><mi id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.4.1" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.4.1.cmml">sin</mi><mo lspace="0.167em" id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.4a" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.4.cmml">⁡</mo><mi id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.4.2" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.4.2.cmml">θ</mi></mrow></mrow></mtd></mtr><mtr id="S3.E1Xb.2.1.1.m1.1.1d" xref="S3.E1Xb.2.1.1.m1.1.1.cmml"><mtd id="S3.E1Xb.2.1.1.m1.1.1e" xref="S3.E1Xb.2.1.1.m1.1.1.cmml"><mrow id="S3.E1Xb.2.1.1.m1.1.1.2.1.1" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.cmml"><mfrac id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.2" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.2.cmml"><mrow id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.2.2" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.2.2.cmml"><mi id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.2.2.2" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.2.2.2.cmml">w</mi><mo id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.2.2.1" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.2.2.1.cmml">−</mo><mi id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.2.2.3" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.2.2.3.cmml">h</mi></mrow><mn id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.2.3" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.2.3.cmml">2</mn></mfrac><mo lspace="0.167em" rspace="0em" id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.1" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.1.cmml">​</mo><mrow id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.3" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.3.cmml"><mi id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.3.1" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.3.1.cmml">cos</mi><mo lspace="0.167em" id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.3a" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.3.cmml">⁡</mo><mi id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.3.2" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.3.2.cmml">θ</mi></mrow><mo lspace="0.167em" rspace="0em" id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.1a" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.1.cmml">​</mo><mrow id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.4" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.4.cmml"><mi id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.4.1" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.4.1.cmml">sin</mi><mo lspace="0.167em" id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.4a" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.4.cmml">⁡</mo><mi id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.4.2" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.4.2.cmml">θ</mi></mrow></mrow></mtd><mtd id="S3.E1Xb.2.1.1.m1.1.1f" xref="S3.E1Xb.2.1.1.m1.1.1.cmml"><mrow id="S3.E1Xb.2.1.1.m1.1.1.2.2.1" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.cmml"><mrow id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.cmml"><mfrac id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.2" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.2.cmml"><mi id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.2.2" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.2.2.cmml">w</mi><mn id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.2.3" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.2.3.cmml">2</mn></mfrac><mo lspace="0.167em" rspace="0em" id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.1" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.1.cmml">​</mo><mrow id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.3" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.3.cmml"><msup id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.3.1" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.3.1.cmml"><mi id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.3.1.2" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.3.1.2.cmml">sin</mi><mn id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.3.1.3" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.3.1.3.cmml">2</mn></msup><mo lspace="0.167em" id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.3a" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.3.cmml">⁡</mo><mi id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.3.2" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.3.2.cmml">θ</mi></mrow></mrow><mo id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.1" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.1.cmml">+</mo><mrow id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.cmml"><mfrac id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.2" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.2.cmml"><mi id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.2.2" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.2.2.cmml">h</mi><mn id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.2.3" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.2.3.cmml">2</mn></mfrac><mo lspace="0.167em" rspace="0em" id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.1" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.1.cmml">​</mo><mrow id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.3" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.3.cmml"><msup id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.3.1" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.3.1.cmml"><mi id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.3.1.2" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.3.1.2.cmml">cos</mi><mn id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.3.1.3" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.3.1.3.cmml">2</mn></msup><mo lspace="0.167em" id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.3a" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.3.cmml">⁡</mo><mi id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.3.2" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.3.2.cmml">θ</mi></mrow></mrow></mrow></mtd></mtr></mtable><mo id="S3.E1Xb.2.1.1.m1.1.2.3.2.2" xref="S3.E1Xb.2.1.1.m1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1Xb.2.1.1.m1.1b"><apply id="S3.E1Xb.2.1.1.m1.1.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.2"><eq id="S3.E1Xb.2.1.1.m1.1.2.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.2.1"></eq><csymbol cd="latexml" id="S3.E1Xb.2.1.1.m1.1.2.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.2.2">absent</csymbol><matrix id="S3.E1Xb.2.1.1.m1.1.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.2.3.2"><matrixrow id="S3.E1Xb.2.1.1.m1.1.1a.cmml" xref="S3.E1Xb.2.1.1.m1.1.2.3.2"><apply id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1"><plus id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1"></plus><apply id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2"><times id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.1"></times><apply id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.2"><divide id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.2.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.2"></divide><ci id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.2.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.2.2">𝑤</ci><cn type="integer" id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.2.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.2.3">2</cn></apply><apply id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.3"><apply id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.3.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.3.1"><csymbol cd="ambiguous" id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.3.1.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.3.1">superscript</csymbol><cos id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.3.1.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.3.1.2"></cos><cn type="integer" id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.3.1.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.3.1.3">2</cn></apply><ci id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.3.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.2.3.2">𝜃</ci></apply></apply><apply id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3"><times id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.1"></times><apply id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.2"><divide id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.2.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.2"></divide><ci id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.2.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.2.2">ℎ</ci><cn type="integer" id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.2.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.2.3">2</cn></apply><apply id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.3"><apply id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.3.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.3.1.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.3.1">superscript</csymbol><sin id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.3.1.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.3.1.2"></sin><cn type="integer" id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.3.1.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.3.1.3">2</cn></apply><ci id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.3.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.3.3.2">𝜃</ci></apply></apply></apply><apply id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1"><times id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.1"></times><apply id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.2"><divide id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.2.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.2"></divide><apply id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.2.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.2.2"><minus id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.2.2.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.2.2.1"></minus><ci id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.2.2.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.2.2.2">𝑤</ci><ci id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.2.2.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.2.2.3">ℎ</ci></apply><cn type="integer" id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.2.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.2.3">2</cn></apply><apply id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.3"><cos id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.3.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.3.1"></cos><ci id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.3.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.3.2">𝜃</ci></apply><apply id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.4.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.4"><sin id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.4.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.4.1"></sin><ci id="S3.E1Xb.2.1.1.m1.1.1.1.2.1.4.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.2.1.4.2">𝜃</ci></apply></apply></matrixrow><matrixrow id="S3.E1Xb.2.1.1.m1.1.1b.cmml" xref="S3.E1Xb.2.1.1.m1.1.2.3.2"><apply id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1"><times id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.1"></times><apply id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.2"><divide id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.2.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.2"></divide><apply id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.2.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.2.2"><minus id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.2.2.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.2.2.1"></minus><ci id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.2.2.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.2.2.2">𝑤</ci><ci id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.2.2.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.2.2.3">ℎ</ci></apply><cn type="integer" id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.2.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.2.3">2</cn></apply><apply id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.3"><cos id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.3.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.3.1"></cos><ci id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.3.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.3.2">𝜃</ci></apply><apply id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.4.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.4"><sin id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.4.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.4.1"></sin><ci id="S3.E1Xb.2.1.1.m1.1.1.2.1.1.4.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.1.1.4.2">𝜃</ci></apply></apply><apply id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1"><plus id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.1"></plus><apply id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2"><times id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.1"></times><apply id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.2"><divide id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.2.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.2"></divide><ci id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.2.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.2.2">𝑤</ci><cn type="integer" id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.2.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.2.3">2</cn></apply><apply id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.3"><apply id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.3.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.3.1"><csymbol cd="ambiguous" id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.3.1.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.3.1">superscript</csymbol><sin id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.3.1.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.3.1.2"></sin><cn type="integer" id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.3.1.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.3.1.3">2</cn></apply><ci id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.3.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.2.3.2">𝜃</ci></apply></apply><apply id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3"><times id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.1"></times><apply id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.2"><divide id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.2.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.2"></divide><ci id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.2.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.2.2">ℎ</ci><cn type="integer" id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.2.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.2.3">2</cn></apply><apply id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.3"><apply id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.3.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.3.1"><csymbol cd="ambiguous" id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.3.1.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.3.1">superscript</csymbol><cos id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.3.1.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.3.1.2"></cos><cn type="integer" id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.3.1.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.3.1.3">2</cn></apply><ci id="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.3.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.2.2.1.3.3.2">𝜃</ci></apply></apply></apply></matrixrow></matrix></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1Xb.2.1.1.m1.1c">\displaystyle=\left(\begin{array}[]{cc}\frac{w}{2}\cos^{2}\theta+\frac{h}{2}\sin^{2}\theta&amp;\frac{w-h}{2}\cos\theta\sin\theta\\
\frac{w-h}{2}\cos\theta\sin\theta&amp;\frac{w}{2}\sin^{2}\theta+\frac{h}{2}\cos^{2}\theta\end{array}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr id="S3.E1Xc" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1Xc.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle{\mu}" display="inline"><semantics id="S3.E1Xc.2.1.1.m1.1a"><mi id="S3.E1Xc.2.1.1.m1.1.1" xref="S3.E1Xc.2.1.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S3.E1Xc.2.1.1.m1.1b"><ci id="S3.E1Xc.2.1.1.m1.1.1.cmml" xref="S3.E1Xc.2.1.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.E1Xc.2.1.1.m1.1c">\displaystyle{\mu}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1Xc.3.2.2.m1.2" class="ltx_Math" alttext="\displaystyle=(x,y)^{\top}" display="inline"><semantics id="S3.E1Xc.3.2.2.m1.2a"><mrow id="S3.E1Xc.3.2.2.m1.2.3" xref="S3.E1Xc.3.2.2.m1.2.3.cmml"><mi id="S3.E1Xc.3.2.2.m1.2.3.2" xref="S3.E1Xc.3.2.2.m1.2.3.2.cmml"></mi><mo id="S3.E1Xc.3.2.2.m1.2.3.1" xref="S3.E1Xc.3.2.2.m1.2.3.1.cmml">=</mo><msup id="S3.E1Xc.3.2.2.m1.2.3.3" xref="S3.E1Xc.3.2.2.m1.2.3.3.cmml"><mrow id="S3.E1Xc.3.2.2.m1.2.3.3.2.2" xref="S3.E1Xc.3.2.2.m1.2.3.3.2.1.cmml"><mo stretchy="false" id="S3.E1Xc.3.2.2.m1.2.3.3.2.2.1" xref="S3.E1Xc.3.2.2.m1.2.3.3.2.1.cmml">(</mo><mi id="S3.E1Xc.3.2.2.m1.1.1" xref="S3.E1Xc.3.2.2.m1.1.1.cmml">x</mi><mo id="S3.E1Xc.3.2.2.m1.2.3.3.2.2.2" xref="S3.E1Xc.3.2.2.m1.2.3.3.2.1.cmml">,</mo><mi id="S3.E1Xc.3.2.2.m1.2.2" xref="S3.E1Xc.3.2.2.m1.2.2.cmml">y</mi><mo stretchy="false" id="S3.E1Xc.3.2.2.m1.2.3.3.2.2.3" xref="S3.E1Xc.3.2.2.m1.2.3.3.2.1.cmml">)</mo></mrow><mo id="S3.E1Xc.3.2.2.m1.2.3.3.3" xref="S3.E1Xc.3.2.2.m1.2.3.3.3.cmml">⊤</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.E1Xc.3.2.2.m1.2b"><apply id="S3.E1Xc.3.2.2.m1.2.3.cmml" xref="S3.E1Xc.3.2.2.m1.2.3"><eq id="S3.E1Xc.3.2.2.m1.2.3.1.cmml" xref="S3.E1Xc.3.2.2.m1.2.3.1"></eq><csymbol cd="latexml" id="S3.E1Xc.3.2.2.m1.2.3.2.cmml" xref="S3.E1Xc.3.2.2.m1.2.3.2">absent</csymbol><apply id="S3.E1Xc.3.2.2.m1.2.3.3.cmml" xref="S3.E1Xc.3.2.2.m1.2.3.3"><csymbol cd="ambiguous" id="S3.E1Xc.3.2.2.m1.2.3.3.1.cmml" xref="S3.E1Xc.3.2.2.m1.2.3.3">superscript</csymbol><interval closure="open" id="S3.E1Xc.3.2.2.m1.2.3.3.2.1.cmml" xref="S3.E1Xc.3.2.2.m1.2.3.3.2.2"><ci id="S3.E1Xc.3.2.2.m1.1.1.cmml" xref="S3.E1Xc.3.2.2.m1.1.1">𝑥</ci><ci id="S3.E1Xc.3.2.2.m1.2.2.cmml" xref="S3.E1Xc.3.2.2.m1.2.2">𝑦</ci></interval><csymbol cd="latexml" id="S3.E1Xc.3.2.2.m1.2.3.3.3.cmml" xref="S3.E1Xc.3.2.2.m1.2.3.3.3">top</csymbol></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1Xc.3.2.2.m1.2c">\displaystyle=(x,y)^{\top}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p id="S3.SS1.SSS4.p1.3" class="ltx_p">where <math id="S3.SS1.SSS4.p1.2.m1.1" class="ltx_Math" alttext="\mathbf{R}" display="inline"><semantics id="S3.SS1.SSS4.p1.2.m1.1a"><mi id="S3.SS1.SSS4.p1.2.m1.1.1" xref="S3.SS1.SSS4.p1.2.m1.1.1.cmml">𝐑</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS4.p1.2.m1.1b"><ci id="S3.SS1.SSS4.p1.2.m1.1.1.cmml" xref="S3.SS1.SSS4.p1.2.m1.1.1">𝐑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS4.p1.2.m1.1c">\mathbf{R}</annotation></semantics></math> represents the rotation matrix, and <math id="S3.SS1.SSS4.p1.3.m2.1" class="ltx_Math" alttext="\mathbf{\Lambda}" display="inline"><semantics id="S3.SS1.SSS4.p1.3.m2.1a"><mi id="S3.SS1.SSS4.p1.3.m2.1.1" xref="S3.SS1.SSS4.p1.3.m2.1.1.cmml">𝚲</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS4.p1.3.m2.1b"><ci id="S3.SS1.SSS4.p1.3.m2.1.1.cmml" xref="S3.SS1.SSS4.p1.3.m2.1.1">𝚲</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS4.p1.3.m2.1c">\mathbf{\Lambda}</annotation></semantics></math> represents the diagonal matrix of eigenvalues.
With the Gaussian distribution representation in Eq. <a href="#S3.E1" title="In III-A4 Gaussian Distribution ‣ III-A Rotated Object Representation ‣ III Rotated Object Detection ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the IoU between two rotated objects can be simplified as a distance estimation between two distributions. Besides, the Gaussian distribution representation discards the definition of angular boundary and effectively solves the angular boundary problem. Yang <span id="S3.SS1.SSS4.p1.3.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib130" title="" class="ltx_ref">130</a>]</cite> proposed a novel metric with Gaussian Wasserstein distance (GWD) for measuring the distance between distributions, which achieves remarkable performance by efficiently approximating the rotation IoU. Based on this, Yang <span id="S3.SS1.SSS4.p1.3.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib131" title="" class="ltx_ref">131</a>]</cite> introduced a Kullback-Leibler divergence (KLD) metric to enhance its scale invariance.</p>
</div>
</section>
<section id="S3.SS1.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS5.4.1.1" class="ltx_text">III-A</span>5 </span>Others</h4>

<div id="S3.SS1.SSS5.p1" class="ltx_para">
<p id="S3.SS1.SSS5.p1.1" class="ltx_p">Some researchers solve the rotated object representation by other approaches, such as segmentation-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib134" title="" class="ltx_ref">134</a>, <a href="#bib.bib135" title="" class="ltx_ref">135</a>, <a href="#bib.bib136" title="" class="ltx_ref">136</a>]</cite> and keypoint-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib137" title="" class="ltx_ref">137</a>, <a href="#bib.bib138" title="" class="ltx_ref">138</a>, <a href="#bib.bib139" title="" class="ltx_ref">139</a>, <a href="#bib.bib140" title="" class="ltx_ref">140</a>, <a href="#bib.bib141" title="" class="ltx_ref">141</a>, <a href="#bib.bib142" title="" class="ltx_ref">142</a>, <a href="#bib.bib143" title="" class="ltx_ref">143</a>, <a href="#bib.bib144" title="" class="ltx_ref">144</a>]</cite>. The representative one in segmentation-based methods is Mask OBB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib134" title="" class="ltx_ref">134</a>]</cite>, which deploys the segmentation method on each horizontal proposal to obtain the pixel-level object region and produce the minimum external rectangle as a rotated bounding box. On the other side, Wei <span id="S3.SS1.SSS5.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib142" title="" class="ltx_ref">142</a>]</cite> adopted a keypoint-based representation for rotated objects, which locates the object center and leverages a pair of middle lines to represent the whole object.
In addition, Yang <span id="S3.SS1.SSS5.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib145" title="" class="ltx_ref">145</a>]</cite> proposed the first rotated object detector supervised by horizontal box annotations, which adopts the self-supervised learning of two different views to predict the angles of rotated objects.</p>
</div>
<figure id="S3.F9" class="ltx_figure"><img src="/html/2309.06751/assets/x10.png" id="S3.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="212" height="165" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Visualization of the Gaussian distribution representation methods for rotated objects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib130" title="" class="ltx_ref">130</a>]</cite>.</figcaption>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Rotation-invariant Feature Learning</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Rotation-invariant features indicate the features remain consistent under any rotation transformations. Thus, rotation-invariant feature learning of objects is a crucial research field to tackle the arbitrary orientation problem in rotated object detection.
To this purpose, many researchers proposed a series of methods for learning the rotational invariance of objects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib146" title="" class="ltx_ref">146</a>, <a href="#bib.bib147" title="" class="ltx_ref">147</a>, <a href="#bib.bib148" title="" class="ltx_ref">148</a>, <a href="#bib.bib149" title="" class="ltx_ref">149</a>, <a href="#bib.bib150" title="" class="ltx_ref">150</a>, <a href="#bib.bib151" title="" class="ltx_ref">151</a>, <a href="#bib.bib152" title="" class="ltx_ref">152</a>, <a href="#bib.bib153" title="" class="ltx_ref">153</a>, <a href="#bib.bib154" title="" class="ltx_ref">154</a>, <a href="#bib.bib155" title="" class="ltx_ref">155</a>, <a href="#bib.bib156" title="" class="ltx_ref">156</a>, <a href="#bib.bib157" title="" class="ltx_ref">157</a>]</cite>, which significantly improves rotated object detection in RSIs.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Cheng <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib146" title="" class="ltx_ref">146</a>]</cite> proposed the first rotation-invariant object detector to precisely recognize objects by using rotation-insensitive features, which enforces the features of objects to be consistent at different rotation angles. Later, Cheng <span id="S3.SS2.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib148" title="" class="ltx_ref">148</a>, <a href="#bib.bib149" title="" class="ltx_ref">149</a>]</cite> employed the rotation-invariant and fisher discrimination regularizers to encourage the detector to learn both rotation-invariant and discriminative features. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib150" title="" class="ltx_ref">150</a>, <a href="#bib.bib151" title="" class="ltx_ref">151</a>]</cite>, Wu <span id="S3.SS2.p2.1.3" class="ltx_text ltx_font_italic">et al.</span> analyzed object rotation invariance under polar coordinates in the Fourier domain and designed a spatial-frequency channel feature extraction module to obtain the rotation-invariant features. Considering misalignment between axis-aligned convolutional features and rotated objects, Han <span id="S3.SS2.p2.1.4" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib156" title="" class="ltx_ref">156</a>]</cite> proposed an oriented detection module that adopts a novel alignment convolution operation to learn the orientation information. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib155" title="" class="ltx_ref">155</a>]</cite>, Han <span id="S3.SS2.p2.1.5" class="ltx_text ltx_font_italic">et al.</span> further devised a rotation-equivariant detector to explicitly encode rotation equivariance and rotation invariance. Besides, some researchers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib157" title="" class="ltx_ref">157</a>, <a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> extended the RPN with a series of predefined rotated anchors to cope with the arbitrary orientation characteristics of geospatial objects.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Detection performance of rotated object detection methods on the DOTAv1.0 dataset with rotated annotations.</figcaption>
<table id="S3.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.4.5.1" class="ltx_tr">
<th id="S3.T1.4.5.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" style="padding:1.4pt 4.0pt;"><span id="S3.T1.4.5.1.1.1" class="ltx_text" style="font-size:70%;">Models</span></th>
<th id="S3.T1.4.5.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:1.4pt 4.0pt;"><span id="S3.T1.4.5.1.2.1" class="ltx_text" style="font-size:70%;">Backbone</span></th>
<th id="S3.T1.4.5.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:1.4pt 4.0pt;"><span id="S3.T1.4.5.1.3.1" class="ltx_text" style="font-size:70%;">Methods</span></th>
<th id="S3.T1.4.5.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:1.4pt 4.0pt;"><span id="S3.T1.4.5.1.4.1" class="ltx_text" style="font-size:70%;">mAP(%)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.4.6.1" class="ltx_tr">
<th id="S3.T1.4.6.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding:1.4pt 4.0pt;">
<span id="S3.T1.4.6.1.1.1" class="ltx_text" style="font-size:70%;">SCRDet</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T1.4.6.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib107" title="" class="ltx_ref">107</a><span id="S3.T1.4.6.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S3.T1.4.6.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.4pt 4.0pt;"><span id="S3.T1.4.6.1.2.1" class="ltx_text" style="font-size:70%;">R-101-FPN</span></td>
<td id="S3.T1.4.6.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.4pt 4.0pt;"><span id="S3.T1.4.6.1.3.1" class="ltx_text" style="font-size:70%;">Five parameters</span></td>
<td id="S3.T1.4.6.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.4pt 4.0pt;"><span id="S3.T1.4.6.1.4.1" class="ltx_text" style="font-size:70%;">72.61</span></td>
</tr>
<tr id="S3.T1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding:1.4pt 4.0pt;">
<span id="S3.T1.1.1.1.1" class="ltx_text" style="font-size:70%;">O</span><sup id="S3.T1.1.1.1.2" class="ltx_sup"><span id="S3.T1.1.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:70%;">2</span></sup><span id="S3.T1.1.1.1.3" class="ltx_text" style="font-size:70%;">Det</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T1.1.1.1.4.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib142" title="" class="ltx_ref">142</a><span id="S3.T1.1.1.1.5.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S3.T1.1.1.2" class="ltx_td ltx_align_center" style="padding:1.4pt 4.0pt;"><span id="S3.T1.1.1.2.1" class="ltx_text" style="font-size:70%;">H-104</span></td>
<td id="S3.T1.1.1.3" class="ltx_td ltx_align_center" style="padding:1.4pt 4.0pt;"><span id="S3.T1.1.1.3.1" class="ltx_text" style="font-size:70%;">Keypoint-based</span></td>
<td id="S3.T1.1.1.4" class="ltx_td ltx_align_center" style="padding:1.4pt 4.0pt;"><span id="S3.T1.1.1.4.1" class="ltx_text" style="font-size:70%;">72.8</span></td>
</tr>
<tr id="S3.T1.2.2" class="ltx_tr">
<th id="S3.T1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding:1.4pt 4.0pt;">
<span id="S3.T1.2.2.1.1" class="ltx_text" style="font-size:70%;">R</span><sup id="S3.T1.2.2.1.2" class="ltx_sup"><span id="S3.T1.2.2.1.2.1" class="ltx_text ltx_font_italic" style="font-size:70%;">3</span></sup><span id="S3.T1.2.2.1.3" class="ltx_text" style="font-size:70%;">Det</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T1.2.2.1.4.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib108" title="" class="ltx_ref">108</a><span id="S3.T1.2.2.1.5.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S3.T1.2.2.2" class="ltx_td ltx_align_center" style="padding:1.4pt 4.0pt;"><span id="S3.T1.2.2.2.1" class="ltx_text" style="font-size:70%;">R-101-FPN</span></td>
<td id="S3.T1.2.2.3" class="ltx_td ltx_align_center" style="padding:1.4pt 4.0pt;"><span id="S3.T1.2.2.3.1" class="ltx_text" style="font-size:70%;">Five parameters</span></td>
<td id="S3.T1.2.2.4" class="ltx_td ltx_align_center" style="padding:1.4pt 4.0pt;"><span id="S3.T1.2.2.4.1" class="ltx_text" style="font-size:70%;">73.79</span></td>
</tr>
<tr id="S3.T1.3.3" class="ltx_tr">
<th id="S3.T1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding:1.4pt 4.0pt;">
<span id="S3.T1.3.3.1.1" class="ltx_text" style="font-size:70%;">S</span><sup id="S3.T1.3.3.1.2" class="ltx_sup"><span id="S3.T1.3.3.1.2.1" class="ltx_text ltx_font_italic" style="font-size:70%;">2</span></sup><span id="S3.T1.3.3.1.3" class="ltx_text" style="font-size:70%;">ANet</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T1.3.3.1.4.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib156" title="" class="ltx_ref">156</a><span id="S3.T1.3.3.1.5.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S3.T1.3.3.2" class="ltx_td ltx_align_center" style="padding:1.4pt 4.0pt;"><span id="S3.T1.3.3.2.1" class="ltx_text" style="font-size:70%;">R-50-FPN</span></td>
<td id="S3.T1.3.3.3" class="ltx_td ltx_align_center" style="padding:1.4pt 4.0pt;"><span id="S3.T1.3.3.3.1" class="ltx_text" style="font-size:70%;">Rotation-invariant feature</span></td>
<td id="S3.T1.3.3.4" class="ltx_td ltx_align_center" style="padding:1.4pt 4.0pt;"><span id="S3.T1.3.3.4.1" class="ltx_text" style="font-size:70%;">74.12</span></td>
</tr>
<tr id="S3.T1.4.7.2" class="ltx_tr">
<th id="S3.T1.4.7.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding:1.4pt 4.0pt;">
<span id="S3.T1.4.7.2.1.1" class="ltx_text" style="font-size:70%;">RoI Transformer </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T1.4.7.2.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib114" title="" class="ltx_ref">114</a><span id="S3.T1.4.7.2.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S3.T1.4.7.2.2" class="ltx_td ltx_align_center" style="padding:1.4pt 4.0pt;"><span id="S3.T1.4.7.2.2.1" class="ltx_text" style="font-size:70%;">R-50-FPN</span></td>
<td id="S3.T1.4.7.2.3" class="ltx_td ltx_align_center" style="padding:1.4pt 4.0pt;"><span id="S3.T1.4.7.2.3.1" class="ltx_text" style="font-size:70%;">Five parameters</span></td>
<td id="S3.T1.4.7.2.4" class="ltx_td ltx_align_center" style="padding:1.4pt 4.0pt;"><span id="S3.T1.4.7.2.4.1" class="ltx_text" style="font-size:70%;">74.61</span></td>
</tr>
<tr id="S3.T1.4.8.3" class="ltx_tr">
<th id="S3.T1.4.8.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding:1.4pt 4.0pt;">
<span id="S3.T1.4.8.3.1.1" class="ltx_text" style="font-size:70%;">Mask OBB</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T1.4.8.3.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib134" title="" class="ltx_ref">134</a><span id="S3.T1.4.8.3.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S3.T1.4.8.3.2" class="ltx_td ltx_align_center" style="padding:1.4pt 4.0pt;"><span id="S3.T1.4.8.3.2.1" class="ltx_text" style="font-size:70%;">R-50-FPN</span></td>
<td id="S3.T1.4.8.3.3" class="ltx_td ltx_align_center" style="padding:1.4pt 4.0pt;"><span id="S3.T1.4.8.3.3.1" class="ltx_text" style="font-size:70%;">Segmentation-based</span></td>
<td id="S3.T1.4.8.3.4" class="ltx_td ltx_align_center" style="padding:1.4pt 4.0pt;"><span id="S3.T1.4.8.3.4.1" class="ltx_text" style="font-size:70%;">74.86</span></td>
</tr>
<tr id="S3.T1.4.9.4" class="ltx_tr">
<th id="S3.T1.4.9.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding:1.4pt 4.0pt;">
<span id="S3.T1.4.9.4.1.1" class="ltx_text" style="font-size:70%;">Gliding Vertex</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T1.4.9.4.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib120" title="" class="ltx_ref">120</a><span id="S3.T1.4.9.4.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S3.T1.4.9.4.2" class="ltx_td ltx_align_center" style="padding:1.4pt 4.0pt;"><span id="S3.T1.4.9.4.2.1" class="ltx_text" style="font-size:70%;">R-101-FPN</span></td>
<td id="S3.T1.4.9.4.3" class="ltx_td ltx_align_center" style="padding:1.4pt 4.0pt;"><span id="S3.T1.4.9.4.3.1" class="ltx_text" style="font-size:70%;">Four vertices</span></td>
<td id="S3.T1.4.9.4.4" class="ltx_td ltx_align_center" style="padding:1.4pt 4.0pt;"><span id="S3.T1.4.9.4.4.1" class="ltx_text" style="font-size:70%;">75.02</span></td>
</tr>
<tr id="S3.T1.4.10.5" class="ltx_tr">
<th id="S3.T1.4.10.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding:1.4pt 4.0pt;">
<span id="S3.T1.4.10.5.1.1" class="ltx_text" style="font-size:70%;">DCL</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T1.4.10.5.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib128" title="" class="ltx_ref">128</a><span id="S3.T1.4.10.5.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S3.T1.4.10.5.2" class="ltx_td ltx_align_center" style="padding:1.4pt 4.0pt;"><span id="S3.T1.4.10.5.2.1" class="ltx_text" style="font-size:70%;">R-152-FPN</span></td>
<td id="S3.T1.4.10.5.3" class="ltx_td ltx_align_center" style="padding:1.4pt 4.0pt;"><span id="S3.T1.4.10.5.3.1" class="ltx_text" style="font-size:70%;">Angle classification</span></td>
<td id="S3.T1.4.10.5.4" class="ltx_td ltx_align_center" style="padding:1.4pt 4.0pt;"><span id="S3.T1.4.10.5.4.1" class="ltx_text" style="font-size:70%;">75.54</span></td>
</tr>
<tr id="S3.T1.4.11.6" class="ltx_tr">
<th id="S3.T1.4.11.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding:1.4pt 4.0pt;">
<span id="S3.T1.4.11.6.1.1" class="ltx_text" style="font-size:70%;">ReDet</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T1.4.11.6.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib155" title="" class="ltx_ref">155</a><span id="S3.T1.4.11.6.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S3.T1.4.11.6.2" class="ltx_td ltx_align_center" style="padding:1.4pt 4.0pt;"><span id="S3.T1.4.11.6.2.1" class="ltx_text" style="font-size:70%;">ReR50-ReFPN</span></td>
<td id="S3.T1.4.11.6.3" class="ltx_td ltx_align_center" style="padding:1.4pt 4.0pt;"><span id="S3.T1.4.11.6.3.1" class="ltx_text" style="font-size:70%;">Rotation-invariant feature</span></td>
<td id="S3.T1.4.11.6.4" class="ltx_td ltx_align_center" style="padding:1.4pt 4.0pt;"><span id="S3.T1.4.11.6.4.1" class="ltx_text" style="font-size:70%;">76.25</span></td>
</tr>
<tr id="S3.T1.4.12.7" class="ltx_tr">
<th id="S3.T1.4.12.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding:1.4pt 4.0pt;">
<span id="S3.T1.4.12.7.1.1" class="ltx_text" style="font-size:70%;">Oriented R-CNN</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T1.4.12.7.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib124" title="" class="ltx_ref">124</a><span id="S3.T1.4.12.7.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S3.T1.4.12.7.2" class="ltx_td ltx_align_center" style="padding:1.4pt 4.0pt;"><span id="S3.T1.4.12.7.2.1" class="ltx_text" style="font-size:70%;">R-101-FPN</span></td>
<td id="S3.T1.4.12.7.3" class="ltx_td ltx_align_center" style="padding:1.4pt 4.0pt;"><span id="S3.T1.4.12.7.3.1" class="ltx_text" style="font-size:70%;">Four vertices</span></td>
<td id="S3.T1.4.12.7.4" class="ltx_td ltx_align_center" style="padding:1.4pt 4.0pt;"><span id="S3.T1.4.12.7.4.1" class="ltx_text" style="font-size:70%;">76.28</span></td>
</tr>
<tr id="S3.T1.4.4" class="ltx_tr">
<th id="S3.T1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" style="padding:1.4pt 4.0pt;">
<span id="S3.T1.4.4.1.1" class="ltx_text" style="font-size:70%;">R</span><sup id="S3.T1.4.4.1.2" class="ltx_sup"><span id="S3.T1.4.4.1.2.1" class="ltx_text ltx_font_italic" style="font-size:70%;">3</span></sup><span id="S3.T1.4.4.1.3" class="ltx_text" style="font-size:70%;">Det-KLD</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T1.4.4.1.4.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib131" title="" class="ltx_ref">131</a><span id="S3.T1.4.4.1.5.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S3.T1.4.4.2" class="ltx_td ltx_align_center ltx_border_b" style="padding:1.4pt 4.0pt;"><span id="S3.T1.4.4.2.1" class="ltx_text" style="font-size:70%;">R-50-FPN</span></td>
<td id="S3.T1.4.4.3" class="ltx_td ltx_align_center ltx_border_b" style="padding:1.4pt 4.0pt;"><span id="S3.T1.4.4.3.1" class="ltx_text" style="font-size:70%;">Gaussian distribution</span></td>
<td id="S3.T1.4.4.4" class="ltx_td ltx_align_center ltx_border_b" style="padding:1.4pt 4.0pt;"><span id="S3.T1.4.4.4.1" class="ltx_text" style="font-size:70%;">77.36</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">We summarize the detection performance of milestone rotated object detection methods in Table <a href="#S3.T1" title="TABLE I ‣ III-B Rotation-invariant Feature Learning ‣ III Rotated Object Detection ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Weak Object Detection</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Objects of interest in RSIs are typically embedded in complex scenes with intricate object spatial patterns and massive background noise. The complex context and background noise severely harm the feature representation of objects of interest, resulting in weak feature responses to objects of interest. Thus, many existing works have concentrated on improving the feature representation of the objects of interest, which can be grouped into two streams: suppressing background noise and mining related context information.
A brief summary of weak object detection methods is given in Fig. <a href="#S4.F10" title="Figure 10 ‣ IV Weak Object Detection ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>.</p>
</div>
<figure id="S4.F10" class="ltx_figure"><img src="/html/2309.06751/assets/x11.png" id="S4.F10.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="216" height="170" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>A brief summary of weak object detection methods.</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Suppressing background noise</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">This type of method aims to strengthen the weak response of the object region in the feature map by weakening the response of background regions. It can be mainly divided into two categories: implicit learning and explicit supervision.</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS1.4.1.1" class="ltx_text">IV-A</span>1 </span>Implicit Learning</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">Implicit learning methods employ carefully designed modules into the detector to adaptively learn important features and suppress redundant features during the training phase, thereby reducing the background noise interference.</p>
</div>
<div id="S4.SS1.SSS1.p2" class="ltx_para">
<p id="S4.SS1.SSS1.p2.1" class="ltx_p">In machine learning, dimensionality reduction can effectively learn compact feature representation and suppress irrelevant features. Drawing on the above property, Ye <span id="S4.SS1.SSS1.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib158" title="" class="ltx_ref">158</a>]</cite> proposed a feature filtration module that captures the low-dimensional feature maps by consecutive bottleneck layers to filter background noise interference. Inspired by the selective focus of human visual perception, the attention mechanism has been proposed and heavily researched<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib159" title="" class="ltx_ref">159</a>, <a href="#bib.bib160" title="" class="ltx_ref">160</a>, <a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite>. The attention mechanism redistributes the feature importance during the network learning phase to enhance important features and suppress redundant information. Consequently, the attention mechanism has also been widely introduced in RSOD to tackle the background noise interference problem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib162" title="" class="ltx_ref">162</a>, <a href="#bib.bib163" title="" class="ltx_ref">163</a>, <a href="#bib.bib164" title="" class="ltx_ref">164</a>, <a href="#bib.bib165" title="" class="ltx_ref">165</a>, <a href="#bib.bib166" title="" class="ltx_ref">166</a>, <a href="#bib.bib167" title="" class="ltx_ref">167</a>, <a href="#bib.bib168" title="" class="ltx_ref">168</a>, <a href="#bib.bib169" title="" class="ltx_ref">169</a>, <a href="#bib.bib170" title="" class="ltx_ref">170</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib162" title="" class="ltx_ref">162</a>]</cite>, Huang <span id="S4.SS1.SSS1.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> emphasized the importance of patch-patch dependencies for RSOD and designed a novel non-local perceptual pyramidal attention (NP-Attention). The NP-Attention learns spatial multi-scale non-local dependencies and channel dependencies to enable the detector to concentrate on the object region rather than the background. Considering the strong scattering interference of the land area in SAR images, Sun <span id="S4.SS1.SSS1.p2.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite> presented a ship attention module to highlight the feature representation of ships and reduce the false alarm from the land area. Moreover, a series of attention mechanisms devised for RSOD (e.g., spatial shuffle-group enhance attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib165" title="" class="ltx_ref">165</a>]</cite>, multi-scale spatial and channel-wise attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib166" title="" class="ltx_ref">166</a>]</cite>, discrete wavelet multi-scale attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib167" title="" class="ltx_ref">167</a>]</cite>, etc.) have demonstrated their effectiveness in suppressing background noise.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS2.4.1.1" class="ltx_text">IV-A</span>2 </span>Explicit Supervision</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">Unlike implicit learning methods, the explicit supervision approach employs auxiliary saliency supervision information to explicitly guide the detector to highlight the foreground regions and weaken the background.</p>
</div>
<div id="S4.SS1.SSS2.p2" class="ltx_para">
<p id="S4.SS1.SSS2.p2.1" class="ltx_p">Li <span id="S4.SS1.SSS2.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib171" title="" class="ltx_ref">171</a>]</cite> employed the region contrast method to obtain the saliency map and construct the saliency feature pyramid by fusing the multi-scale feature maps with the saliency map. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib172" title="" class="ltx_ref">172</a>]</cite>, Lei <span id="S4.SS1.SSS2.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> extracted the saliency map with the saliency detection method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib173" title="" class="ltx_ref">173</a>]</cite> and proposed a saliency reconstruction network. The saliency reconstruction network utilizes the saliency map as pixel-level supervision to guide the training of the detector to strengthen saliency regions in feature maps. The above saliency detection methods are typically unsupervised, and the generated saliency map may contain non-object regions, as shown in Fig. <a href="#S4.F11" title="Figure 11 ‣ IV-A2 Explicit Supervision ‣ IV-A Suppressing background noise ‣ IV Weak Object Detection ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>(b), providing inaccurate guidance to the detector. Therefore, later works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>, <a href="#bib.bib134" title="" class="ltx_ref">134</a>, <a href="#bib.bib174" title="" class="ltx_ref">174</a>, <a href="#bib.bib175" title="" class="ltx_ref">175</a>, <a href="#bib.bib176" title="" class="ltx_ref">176</a>, <a href="#bib.bib177" title="" class="ltx_ref">177</a>, <a href="#bib.bib178" title="" class="ltx_ref">178</a>, <a href="#bib.bib179" title="" class="ltx_ref">179</a>, <a href="#bib.bib180" title="" class="ltx_ref">180</a>]</cite> transformed the box-level annotation into object-level saliency guidance information (as shown in Fig. <a href="#S4.F11" title="Figure 11 ‣ IV-A2 Explicit Supervision ‣ IV-A Suppressing background noise ‣ IV Weak Object Detection ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>(c)) to generate more accurate saliency supervision. Yang <span id="S4.SS1.SSS2.p2.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite> designed a pixel attention network that employs object-level saliency supervision to enhance the object cues and weaken the background information. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib175" title="" class="ltx_ref">175</a>]</cite>, Zhang <span id="S4.SS1.SSS2.p2.1.4" class="ltx_text ltx_font_italic">et al.</span> proposed the FoRDet to exploit object-level saliency supervision in a more concise way. Concretely, the proposed FoRDet leverages the prediction of foreground regions in the coarse stage (supervised under box-level annotation) to enhance the feature representation of the foreground regions in the refined stage.</p>
</div>
<figure id="S4.F11" class="ltx_figure"><img src="/html/2309.06751/assets/x12.png" id="S4.F11.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="221" height="68" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>(a) Input image. (b) Saliency map generated by the saliency detection method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib173" title="" class="ltx_ref">173</a>]</cite>. (c) Object-level saliency map.</figcaption>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Mining related context information </span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Context information typically refers to the spatial and semantic relations between the object and the surrounding environment or scene. This context information can provide auxiliary feature representations to the object that could not be clearly distinguished. Thus, mining context information can effectively solve the weak feature responses problem in RSOD. According to the category of context information, existing methods are mainly classified into local and global context information mining.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS1.4.1.1" class="ltx_text">IV-B</span>1 </span>Local Context Information Mining</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">Local context information refers to the correlation between the object and its surrounding environment in visual information and spatial distribution<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib181" title="" class="ltx_ref">181</a>, <a href="#bib.bib182" title="" class="ltx_ref">182</a>, <a href="#bib.bib147" title="" class="ltx_ref">147</a>, <a href="#bib.bib183" title="" class="ltx_ref">183</a>, <a href="#bib.bib184" title="" class="ltx_ref">184</a>, <a href="#bib.bib185" title="" class="ltx_ref">185</a>, <a href="#bib.bib186" title="" class="ltx_ref">186</a>, <a href="#bib.bib187" title="" class="ltx_ref">187</a>]</cite>. Zhang <span id="S4.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib181" title="" class="ltx_ref">181</a>]</cite> generated multiple local context regions by scaling the original region proposal into three different sizes and proposed a contextual bidirectional enhancement module to fuse the local context features with object features. The context-aware convolutional neural network (CA-CNN)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib182" title="" class="ltx_ref">182</a>]</cite> employed a Context-RoI mining layer to extract context information surrounding objects. The Context-RoI for an object is first generated by merging a series of filtered proposals around the object and then fused with the object RoI as the final object feature representation for classification and regression. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib183" title="" class="ltx_ref">183</a>]</cite>, Ma <span id="S4.SS2.SSS1.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> exploit the gated recurrent units (GRUs) to fuse object features with the local context information, leading to a more discriminative feature representation for the object. Graph Convolutional Networks (GCN) have recently shown better performance in object-object relationship reasoning. Hence, Tian <span id="S4.SS2.SSS1.p1.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib184" title="" class="ltx_ref">184</a>, <a href="#bib.bib185" title="" class="ltx_ref">185</a>]</cite> constructed spatial and semantic graphs to model and learn the contextual relationships between objects.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS2.4.1.1" class="ltx_text">IV-B</span>2 </span>Global Context Information Mining</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">Global context information exploits the association between the object and the scene<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib188" title="" class="ltx_ref">188</a>, <a href="#bib.bib189" title="" class="ltx_ref">189</a>, <a href="#bib.bib190" title="" class="ltx_ref">190</a>, <a href="#bib.bib191" title="" class="ltx_ref">191</a>, <a href="#bib.bib192" title="" class="ltx_ref">192</a>, <a href="#bib.bib193" title="" class="ltx_ref">193</a>, <a href="#bib.bib194" title="" class="ltx_ref">194</a>, <a href="#bib.bib195" title="" class="ltx_ref">195</a>]</cite>, e.g., vehicles generally locate on roads and ships typically appear at sea. Chen <span id="S4.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib188" title="" class="ltx_ref">188</a>]</cite> extracted the scene context information from the global image feature with the RoI-Align operation and fused it with the object-level RoI features to strengthen the relationship between the object and the scene. Liu <span id="S4.SS2.SSS2.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib192" title="" class="ltx_ref">192</a>]</cite> designed a scene auxiliary detection head that exploits the scene context information under scene-level supervision. The scene auxiliary detection head embeds the predicted scene vector into the classification branch to fuse the object-level features with scene-level context information. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib193" title="" class="ltx_ref">193</a>]</cite>, Tao <span id="S4.SS2.SSS2.p1.1.3" class="ltx_text ltx_font_italic">et al.</span> presented a scene context-driven vehicle detection approach. Specifically, a pre-trained scene classifier is introduced to classify each image patch into three scene categories, then the scene-specific vehicle detectors are employed to achieve preliminary detection results, and finally the detection results are further optimized with the scene contextual information.</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p">Considering the complementarity of local and global context information, Zhang <span id="S4.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib196" title="" class="ltx_ref">196</a>]</cite> proposed a CAD-Net to mine both local and global context information. CAD-Net employed a pyramid local context network to learn the object-level local context information and designed a global context network to extract scene-level global context information. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite>, Teng <span id="S4.SS2.SSS2.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> proposed a GLNet to collect context information from global to local so as to achieve a robust and accurate detector for RSIs. Besides, some studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib197" title="" class="ltx_ref">197</a>, <a href="#bib.bib198" title="" class="ltx_ref">198</a>, <a href="#bib.bib199" title="" class="ltx_ref">199</a>]</cite> also introduced the ASPP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib200" title="" class="ltx_ref">200</a>]</cite> or RFB module <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> to leverage both local and global context information.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Tiny Object Detection </span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.4" class="ltx_p">The typical ground sampling distance (GSD) for RSIs is 1-3 meters, which means that even large objects (e.g., airplanes, ships, and storage tanks) can only occupy less than <math id="S5.p1.1.m1.1" class="ltx_Math" alttext="16\times 16" display="inline"><semantics id="S5.p1.1.m1.1a"><mrow id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml"><mn id="S5.p1.1.m1.1.1.2" xref="S5.p1.1.m1.1.1.2.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S5.p1.1.m1.1.1.1" xref="S5.p1.1.m1.1.1.1.cmml">×</mo><mn id="S5.p1.1.m1.1.1.3" xref="S5.p1.1.m1.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><apply id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"><times id="S5.p1.1.m1.1.1.1.cmml" xref="S5.p1.1.m1.1.1.1"></times><cn type="integer" id="S5.p1.1.m1.1.1.2.cmml" xref="S5.p1.1.m1.1.1.2">16</cn><cn type="integer" id="S5.p1.1.m1.1.1.3.cmml" xref="S5.p1.1.m1.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">16\times 16</annotation></semantics></math> pixels. Besides, even in high-resolution RSIs with a GSD of 0.25m, a vehicle with a dimension of <math id="S5.p1.2.m2.1" class="ltx_Math" alttext="3\times 1.5" display="inline"><semantics id="S5.p1.2.m2.1a"><mrow id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml"><mn id="S5.p1.2.m2.1.1.2" xref="S5.p1.2.m2.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S5.p1.2.m2.1.1.1" xref="S5.p1.2.m2.1.1.1.cmml">×</mo><mn id="S5.p1.2.m2.1.1.3" xref="S5.p1.2.m2.1.1.3.cmml">1.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><apply id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1"><times id="S5.p1.2.m2.1.1.1.cmml" xref="S5.p1.2.m2.1.1.1"></times><cn type="integer" id="S5.p1.2.m2.1.1.2.cmml" xref="S5.p1.2.m2.1.1.2">3</cn><cn type="float" id="S5.p1.2.m2.1.1.3.cmml" xref="S5.p1.2.m2.1.1.3">1.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">3\times 1.5</annotation></semantics></math>m<sup id="S5.p1.4.1" class="ltx_sup"><span id="S5.p1.4.1.1" class="ltx_text ltx_font_italic">2</span></sup> only covers 72 pixels (<math id="S5.p1.4.m4.1" class="ltx_Math" alttext="12\times 6" display="inline"><semantics id="S5.p1.4.m4.1a"><mrow id="S5.p1.4.m4.1.1" xref="S5.p1.4.m4.1.1.cmml"><mn id="S5.p1.4.m4.1.1.2" xref="S5.p1.4.m4.1.1.2.cmml">12</mn><mo lspace="0.222em" rspace="0.222em" id="S5.p1.4.m4.1.1.1" xref="S5.p1.4.m4.1.1.1.cmml">×</mo><mn id="S5.p1.4.m4.1.1.3" xref="S5.p1.4.m4.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.4.m4.1b"><apply id="S5.p1.4.m4.1.1.cmml" xref="S5.p1.4.m4.1.1"><times id="S5.p1.4.m4.1.1.1.cmml" xref="S5.p1.4.m4.1.1.1"></times><cn type="integer" id="S5.p1.4.m4.1.1.2.cmml" xref="S5.p1.4.m4.1.1.2">12</cn><cn type="integer" id="S5.p1.4.m4.1.1.3.cmml" xref="S5.p1.4.m4.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.4.m4.1c">12\times 6</annotation></semantics></math>). This prevalence of tiny objects in RSIs further increases the difficulty of RSOD. Current studies on tiny object detection are mainly grouped into discriminative feature learning, super-resolution based methods, and improved detection metrics.
The tiny object detection methods are briefly summarized in Fig. <a href="#S5.F12" title="Figure 12 ‣ V Tiny Object Detection ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>.</p>
</div>
<figure id="S5.F12" class="ltx_figure"><img src="/html/2309.06751/assets/x13.png" id="S5.F12.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="216" height="159" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>A brief summary of tiny object detection methods.</figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">Discriminative Feature Learning</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">The extremely small scales (less than <math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="16\times 16" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mrow id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml"><mn id="S5.SS1.p1.1.m1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.2.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS1.p1.1.m1.1.1.1" xref="S5.SS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S5.SS1.p1.1.m1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><apply id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"><times id="S5.SS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S5.SS1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.p1.1.m1.1.1.2">16</cn><cn type="integer" id="S5.SS1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.p1.1.m1.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">16\times 16</annotation></semantics></math> pixels) of tiny objects make them exhibit limited appearance information, which poses serious challenges for detectors to learn the features of tiny objects. To tackle the above problem, many researchers focus on improving the discriminative feature learning ability for tiny objects<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib201" title="" class="ltx_ref">201</a>, <a href="#bib.bib202" title="" class="ltx_ref">202</a>, <a href="#bib.bib203" title="" class="ltx_ref">203</a>, <a href="#bib.bib204" title="" class="ltx_ref">204</a>, <a href="#bib.bib205" title="" class="ltx_ref">205</a>, <a href="#bib.bib206" title="" class="ltx_ref">206</a>, <a href="#bib.bib207" title="" class="ltx_ref">207</a>, <a href="#bib.bib208" title="" class="ltx_ref">208</a>]</cite>.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Since tiny object mainly exists in shallow features and lacks high-level semantic information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>, some literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib201" title="" class="ltx_ref">201</a>, <a href="#bib.bib202" title="" class="ltx_ref">202</a>, <a href="#bib.bib203" title="" class="ltx_ref">203</a>]</cite> introduces top-down structures to fuse high-level semantic information into shallow features to strengthen the semantic information for tiny objects. Considering the limited appearance information of the tiny objects, some studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib204" title="" class="ltx_ref">204</a>, <a href="#bib.bib205" title="" class="ltx_ref">205</a>, <a href="#bib.bib206" title="" class="ltx_ref">206</a>, <a href="#bib.bib207" title="" class="ltx_ref">207</a>, <a href="#bib.bib208" title="" class="ltx_ref">208</a>]</cite> establish the connection between the tiny object and the surrounding contextual information through the self-attention mechanism or dilated convolution to enhance the feature discriminative of tiny objects. Notably, some previously mentioned studies on multi-scale feature learning and context information mining also demonstrate remarkable effectiveness in tiny object detection.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">Super-resolution based Method</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">The extremely small scale is a crucial issue for tiny object detection, so increasing the resolution of images is an intuitive solution to promote the detection performance of tiny objects. Some methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib209" title="" class="ltx_ref">209</a>, <a href="#bib.bib210" title="" class="ltx_ref">210</a>, <a href="#bib.bib211" title="" class="ltx_ref">211</a>, <a href="#bib.bib212" title="" class="ltx_ref">212</a>]</cite> employ the super-resolution strategies as a pre-processing step into the detection pipeline to enlarge the resolution of input images. For example, Rabbi <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib211" title="" class="ltx_ref">211</a>]</cite> emphasized the importance of edge information for tiny object detection and proposed an edge-enhanced super-resolution generative adversarial network (GAN) to generate visually pleasing high-resolution RSIs with detailed edge information. Wu <span id="S5.SS2.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib212" title="" class="ltx_ref">212</a>]</cite> developed a Point-to-Region detection framework for tiny objects. The Point-to-Region framework first obtains the proposal regions with key-point prediction and then employs a multi-task GAN to perform the super-resolution on the proposal regions and detect the tiny objects in these proposal regions. However, the high-resolution image generated by super-resolution brings extra computational complexity to the detection pipeline. Drawing on this problem, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib213" title="" class="ltx_ref">213</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib214" title="" class="ltx_ref">214</a>]</cite> employ the super-resolution strategy at the feature level to acquire discriminative feature representation for tiny objects and effectively save computational resources.</p>
</div>
<figure id="S5.F13" class="ltx_figure"><img src="/html/2309.06751/assets/x14.png" id="S5.F13.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="221" height="97" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>A comparison between (a) the IoU-Deviation Curve and (b) the NWD Deviation Curve <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Please refer to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> for detail.</figcaption>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.4.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.5.2" class="ltx_text ltx_font_italic">Improved Detection Metrics for Tiny Object</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Unlike the first two types of methods, recent advanced works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib215" title="" class="ltx_ref">215</a>, <a href="#bib.bib216" title="" class="ltx_ref">216</a>, <a href="#bib.bib217" title="" class="ltx_ref">217</a>, <a href="#bib.bib218" title="" class="ltx_ref">218</a>, <a href="#bib.bib219" title="" class="ltx_ref">219</a>, <a href="#bib.bib220" title="" class="ltx_ref">220</a>, <a href="#bib.bib221" title="" class="ltx_ref">221</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib222" title="" class="ltx_ref">222</a>]</cite> assert that the current prevailing detection paradigms are unsuitable for tiny object detection and inevitably hinder tiny object detection performance. Pang <span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib215" title="" class="ltx_ref">215</a>]</cite> argued that the excessive down-sampling operations in modern detectors leads to the loss of tiny objects on the feature map and proposed a zoom-out and zoom-in structure to enlarge the feature map.
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib218" title="" class="ltx_ref">218</a>]</cite>, Yan <span id="S5.SS3.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> adjusted the IoU threshold in the label assignment to increase the positive assigned anchors for tiny objects, facilitating the learning of tiny objects. Dong <span id="S5.SS3.p1.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib219" title="" class="ltx_ref">219</a>]</cite> devised the Sig-NMS to reduce the suppression of tiny objects by large and medium objects in traditional non-maximum suppression (NMS).</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, Xu <span id="S5.SS3.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> pointed out that the IoU metric is unsuitable for tiny object detection. As shown in Fig. <a href="#S5.F13" title="Figure 13 ‣ V-B Super-resolution based Method ‣ V Tiny Object Detection ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>, the IoU metric is sensitive to slight location offsets.
Besides, the IoU-based label assignment suffers from a severe scale imbalance problem, where tiny objects tend to be assigned with insufficient positive samples.
To solve these problems, Xu <span id="S5.SS3.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> designed a normalized Wasserstein distance (NWD) to replace the IoU metric. The NWD models the tiny objects as 2D Gaussian distributions and utilizes the normalized Wasserstein distance between Gaussian distributions to represent the location relationship between tiny objects, detailed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
Compared with the IoU metric, the proposed NWD metric is smooth to location deviations and has the characteristic of scale balance, as depicted in Fig. <a href="#S5.F13" title="Figure 13 ‣ V-B Super-resolution based Method ‣ V Tiny Object Detection ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>(b). In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib222" title="" class="ltx_ref">222</a>]</cite>, Xu <span id="S5.SS3.p2.1.3" class="ltx_text ltx_font_italic">et al.</span> further proposed the receptive field distance (RFLA) for tiny object detection and achieved state-of-the-art performance.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Object Detection with Limited Supervision</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In recent years, the widely used deep learning based detectors in RSIs heavily rely on large-scale datasets with high-quality annotations to achieve state-of-the-art performance. However, collecting volumes of well-labeled data is considerably expensive and time-consuming (e.g., a bounding box annotation would cost about 10 seconds), which leads to a data-limited or annotation-limited scenario in RSOD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. This lack of sufficient supervised information seriously degrades the detection performance. To tackle this problem, researchers have explored various tasks in RSOD with limited supervision. We summarize the previous research into three main types: weakly-supervised object detection, semi-supervised object detection, and few-shot object detection. Fig. <a href="#S6.F14" title="Figure 14 ‣ VI Object Detection with Limited Supervision ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> provides a brief summary of object detection methods with limited supervision.</p>
</div>
<figure id="S6.F14" class="ltx_figure"><img src="/html/2309.06751/assets/x15.png" id="S6.F14.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="216" height="106" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>A brief summary of object detection methods with limited supervision.</figcaption>
</figure>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS1.4.1.1" class="ltx_text">VI-A</span> </span><span id="S6.SS1.5.2" class="ltx_text ltx_font_italic">Weakly-supervised Object detection</span>
</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.5" class="ltx_p">Compared to fully supervised object detection, weakly-supervised object detection (WSOD) only contains weakly supervised information. Formally, WSOD consists of a training data set <math id="S6.SS1.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{D}_{\text{train}}=\left\{\left(X_{i},y_{i}\right)\right\}_{i=1}^{I}" display="inline"><semantics id="S6.SS1.p1.1.m1.1a"><mrow id="S6.SS1.p1.1.m1.1.1" xref="S6.SS1.p1.1.m1.1.1.cmml"><msub id="S6.SS1.p1.1.m1.1.1.3" xref="S6.SS1.p1.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.SS1.p1.1.m1.1.1.3.2" xref="S6.SS1.p1.1.m1.1.1.3.2.cmml">𝒟</mi><mtext id="S6.SS1.p1.1.m1.1.1.3.3" xref="S6.SS1.p1.1.m1.1.1.3.3a.cmml">train</mtext></msub><mo id="S6.SS1.p1.1.m1.1.1.2" xref="S6.SS1.p1.1.m1.1.1.2.cmml">=</mo><msubsup id="S6.SS1.p1.1.m1.1.1.1" xref="S6.SS1.p1.1.m1.1.1.1.cmml"><mrow id="S6.SS1.p1.1.m1.1.1.1.1.1.1" xref="S6.SS1.p1.1.m1.1.1.1.1.1.2.cmml"><mo id="S6.SS1.p1.1.m1.1.1.1.1.1.1.2" xref="S6.SS1.p1.1.m1.1.1.1.1.1.2.cmml">{</mo><mrow id="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.2" xref="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.3.cmml"><mo id="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.2.3" xref="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.3.cmml">(</mo><msub id="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.1.1" xref="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.1.1.2" xref="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.1.1.2.cmml">X</mi><mi id="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.1.1.3" xref="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.2.4" xref="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.3.cmml">,</mo><msub id="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.2.2" xref="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.2.2.2" xref="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.2.2.2.cmml">y</mi><mi id="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.2.2.3" xref="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.2.2.3.cmml">i</mi></msub><mo id="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.2.5" xref="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.3.cmml">)</mo></mrow><mo id="S6.SS1.p1.1.m1.1.1.1.1.1.1.3" xref="S6.SS1.p1.1.m1.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S6.SS1.p1.1.m1.1.1.1.1.3" xref="S6.SS1.p1.1.m1.1.1.1.1.3.cmml"><mi id="S6.SS1.p1.1.m1.1.1.1.1.3.2" xref="S6.SS1.p1.1.m1.1.1.1.1.3.2.cmml">i</mi><mo id="S6.SS1.p1.1.m1.1.1.1.1.3.1" xref="S6.SS1.p1.1.m1.1.1.1.1.3.1.cmml">=</mo><mn id="S6.SS1.p1.1.m1.1.1.1.1.3.3" xref="S6.SS1.p1.1.m1.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S6.SS1.p1.1.m1.1.1.1.3" xref="S6.SS1.p1.1.m1.1.1.1.3.cmml">I</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.1.m1.1b"><apply id="S6.SS1.p1.1.m1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1"><eq id="S6.SS1.p1.1.m1.1.1.2.cmml" xref="S6.SS1.p1.1.m1.1.1.2"></eq><apply id="S6.SS1.p1.1.m1.1.1.3.cmml" xref="S6.SS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S6.SS1.p1.1.m1.1.1.3.1.cmml" xref="S6.SS1.p1.1.m1.1.1.3">subscript</csymbol><ci id="S6.SS1.p1.1.m1.1.1.3.2.cmml" xref="S6.SS1.p1.1.m1.1.1.3.2">𝒟</ci><ci id="S6.SS1.p1.1.m1.1.1.3.3a.cmml" xref="S6.SS1.p1.1.m1.1.1.3.3"><mtext mathsize="70%" id="S6.SS1.p1.1.m1.1.1.3.3.cmml" xref="S6.SS1.p1.1.m1.1.1.3.3">train</mtext></ci></apply><apply id="S6.SS1.p1.1.m1.1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1.1"><csymbol cd="ambiguous" id="S6.SS1.p1.1.m1.1.1.1.2.cmml" xref="S6.SS1.p1.1.m1.1.1.1">superscript</csymbol><apply id="S6.SS1.p1.1.m1.1.1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1.1"><csymbol cd="ambiguous" id="S6.SS1.p1.1.m1.1.1.1.1.2.cmml" xref="S6.SS1.p1.1.m1.1.1.1">subscript</csymbol><set id="S6.SS1.p1.1.m1.1.1.1.1.1.2.cmml" xref="S6.SS1.p1.1.m1.1.1.1.1.1.1"><interval closure="open" id="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.2"><apply id="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.1.1.2">𝑋</ci><ci id="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.2.2.2">𝑦</ci><ci id="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S6.SS1.p1.1.m1.1.1.1.1.1.1.1.2.2.3">𝑖</ci></apply></interval></set><apply id="S6.SS1.p1.1.m1.1.1.1.1.3.cmml" xref="S6.SS1.p1.1.m1.1.1.1.1.3"><eq id="S6.SS1.p1.1.m1.1.1.1.1.3.1.cmml" xref="S6.SS1.p1.1.m1.1.1.1.1.3.1"></eq><ci id="S6.SS1.p1.1.m1.1.1.1.1.3.2.cmml" xref="S6.SS1.p1.1.m1.1.1.1.1.3.2">𝑖</ci><cn type="integer" id="S6.SS1.p1.1.m1.1.1.1.1.3.3.cmml" xref="S6.SS1.p1.1.m1.1.1.1.1.3.3">1</cn></apply></apply><ci id="S6.SS1.p1.1.m1.1.1.1.3.cmml" xref="S6.SS1.p1.1.m1.1.1.1.3">𝐼</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.1.m1.1c">\mathcal{D}_{\text{train}}=\left\{\left(X_{i},y_{i}\right)\right\}_{i=1}^{I}</annotation></semantics></math> , where <math id="S6.SS1.p1.2.m2.3" class="ltx_Math" alttext="X_{i}=\left\{x_{1},...,x_{m_{i}}\right\}" display="inline"><semantics id="S6.SS1.p1.2.m2.3a"><mrow id="S6.SS1.p1.2.m2.3.3" xref="S6.SS1.p1.2.m2.3.3.cmml"><msub id="S6.SS1.p1.2.m2.3.3.4" xref="S6.SS1.p1.2.m2.3.3.4.cmml"><mi id="S6.SS1.p1.2.m2.3.3.4.2" xref="S6.SS1.p1.2.m2.3.3.4.2.cmml">X</mi><mi id="S6.SS1.p1.2.m2.3.3.4.3" xref="S6.SS1.p1.2.m2.3.3.4.3.cmml">i</mi></msub><mo id="S6.SS1.p1.2.m2.3.3.3" xref="S6.SS1.p1.2.m2.3.3.3.cmml">=</mo><mrow id="S6.SS1.p1.2.m2.3.3.2.2" xref="S6.SS1.p1.2.m2.3.3.2.3.cmml"><mo id="S6.SS1.p1.2.m2.3.3.2.2.3" xref="S6.SS1.p1.2.m2.3.3.2.3.cmml">{</mo><msub id="S6.SS1.p1.2.m2.2.2.1.1.1" xref="S6.SS1.p1.2.m2.2.2.1.1.1.cmml"><mi id="S6.SS1.p1.2.m2.2.2.1.1.1.2" xref="S6.SS1.p1.2.m2.2.2.1.1.1.2.cmml">x</mi><mn id="S6.SS1.p1.2.m2.2.2.1.1.1.3" xref="S6.SS1.p1.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S6.SS1.p1.2.m2.3.3.2.2.4" xref="S6.SS1.p1.2.m2.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="S6.SS1.p1.2.m2.1.1" xref="S6.SS1.p1.2.m2.1.1.cmml">…</mi><mo id="S6.SS1.p1.2.m2.3.3.2.2.5" xref="S6.SS1.p1.2.m2.3.3.2.3.cmml">,</mo><msub id="S6.SS1.p1.2.m2.3.3.2.2.2" xref="S6.SS1.p1.2.m2.3.3.2.2.2.cmml"><mi id="S6.SS1.p1.2.m2.3.3.2.2.2.2" xref="S6.SS1.p1.2.m2.3.3.2.2.2.2.cmml">x</mi><msub id="S6.SS1.p1.2.m2.3.3.2.2.2.3" xref="S6.SS1.p1.2.m2.3.3.2.2.2.3.cmml"><mi id="S6.SS1.p1.2.m2.3.3.2.2.2.3.2" xref="S6.SS1.p1.2.m2.3.3.2.2.2.3.2.cmml">m</mi><mi id="S6.SS1.p1.2.m2.3.3.2.2.2.3.3" xref="S6.SS1.p1.2.m2.3.3.2.2.2.3.3.cmml">i</mi></msub></msub><mo id="S6.SS1.p1.2.m2.3.3.2.2.6" xref="S6.SS1.p1.2.m2.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.2.m2.3b"><apply id="S6.SS1.p1.2.m2.3.3.cmml" xref="S6.SS1.p1.2.m2.3.3"><eq id="S6.SS1.p1.2.m2.3.3.3.cmml" xref="S6.SS1.p1.2.m2.3.3.3"></eq><apply id="S6.SS1.p1.2.m2.3.3.4.cmml" xref="S6.SS1.p1.2.m2.3.3.4"><csymbol cd="ambiguous" id="S6.SS1.p1.2.m2.3.3.4.1.cmml" xref="S6.SS1.p1.2.m2.3.3.4">subscript</csymbol><ci id="S6.SS1.p1.2.m2.3.3.4.2.cmml" xref="S6.SS1.p1.2.m2.3.3.4.2">𝑋</ci><ci id="S6.SS1.p1.2.m2.3.3.4.3.cmml" xref="S6.SS1.p1.2.m2.3.3.4.3">𝑖</ci></apply><set id="S6.SS1.p1.2.m2.3.3.2.3.cmml" xref="S6.SS1.p1.2.m2.3.3.2.2"><apply id="S6.SS1.p1.2.m2.2.2.1.1.1.cmml" xref="S6.SS1.p1.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S6.SS1.p1.2.m2.2.2.1.1.1.1.cmml" xref="S6.SS1.p1.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S6.SS1.p1.2.m2.2.2.1.1.1.2.cmml" xref="S6.SS1.p1.2.m2.2.2.1.1.1.2">𝑥</ci><cn type="integer" id="S6.SS1.p1.2.m2.2.2.1.1.1.3.cmml" xref="S6.SS1.p1.2.m2.2.2.1.1.1.3">1</cn></apply><ci id="S6.SS1.p1.2.m2.1.1.cmml" xref="S6.SS1.p1.2.m2.1.1">…</ci><apply id="S6.SS1.p1.2.m2.3.3.2.2.2.cmml" xref="S6.SS1.p1.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S6.SS1.p1.2.m2.3.3.2.2.2.1.cmml" xref="S6.SS1.p1.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S6.SS1.p1.2.m2.3.3.2.2.2.2.cmml" xref="S6.SS1.p1.2.m2.3.3.2.2.2.2">𝑥</ci><apply id="S6.SS1.p1.2.m2.3.3.2.2.2.3.cmml" xref="S6.SS1.p1.2.m2.3.3.2.2.2.3"><csymbol cd="ambiguous" id="S6.SS1.p1.2.m2.3.3.2.2.2.3.1.cmml" xref="S6.SS1.p1.2.m2.3.3.2.2.2.3">subscript</csymbol><ci id="S6.SS1.p1.2.m2.3.3.2.2.2.3.2.cmml" xref="S6.SS1.p1.2.m2.3.3.2.2.2.3.2">𝑚</ci><ci id="S6.SS1.p1.2.m2.3.3.2.2.2.3.3.cmml" xref="S6.SS1.p1.2.m2.3.3.2.2.2.3.3">𝑖</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.2.m2.3c">X_{i}=\left\{x_{1},...,x_{m_{i}}\right\}</annotation></semantics></math> is a collection of training samples, termed as bag, <math id="S6.SS1.p1.3.m3.1" class="ltx_Math" alttext="m_{i}" display="inline"><semantics id="S6.SS1.p1.3.m3.1a"><msub id="S6.SS1.p1.3.m3.1.1" xref="S6.SS1.p1.3.m3.1.1.cmml"><mi id="S6.SS1.p1.3.m3.1.1.2" xref="S6.SS1.p1.3.m3.1.1.2.cmml">m</mi><mi id="S6.SS1.p1.3.m3.1.1.3" xref="S6.SS1.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.3.m3.1b"><apply id="S6.SS1.p1.3.m3.1.1.cmml" xref="S6.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S6.SS1.p1.3.m3.1.1.1.cmml" xref="S6.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S6.SS1.p1.3.m3.1.1.2.cmml" xref="S6.SS1.p1.3.m3.1.1.2">𝑚</ci><ci id="S6.SS1.p1.3.m3.1.1.3.cmml" xref="S6.SS1.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.3.m3.1c">m_{i}</annotation></semantics></math> is the total number of training samples in the bag, and <math id="S6.SS1.p1.4.m4.1" class="ltx_Math" alttext="y_{i}" display="inline"><semantics id="S6.SS1.p1.4.m4.1a"><msub id="S6.SS1.p1.4.m4.1.1" xref="S6.SS1.p1.4.m4.1.1.cmml"><mi id="S6.SS1.p1.4.m4.1.1.2" xref="S6.SS1.p1.4.m4.1.1.2.cmml">y</mi><mi id="S6.SS1.p1.4.m4.1.1.3" xref="S6.SS1.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.4.m4.1b"><apply id="S6.SS1.p1.4.m4.1.1.cmml" xref="S6.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S6.SS1.p1.4.m4.1.1.1.cmml" xref="S6.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S6.SS1.p1.4.m4.1.1.2.cmml" xref="S6.SS1.p1.4.m4.1.1.2">𝑦</ci><ci id="S6.SS1.p1.4.m4.1.1.3.cmml" xref="S6.SS1.p1.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.4.m4.1c">y_{i}</annotation></semantics></math> is the weakly supervised information (e.g., image-level labels<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib223" title="" class="ltx_ref">223</a>]</cite> or point-level labels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib224" title="" class="ltx_ref">224</a>]</cite>) of <math id="S6.SS1.p1.5.m5.1" class="ltx_Math" alttext="X_{i}" display="inline"><semantics id="S6.SS1.p1.5.m5.1a"><msub id="S6.SS1.p1.5.m5.1.1" xref="S6.SS1.p1.5.m5.1.1.cmml"><mi id="S6.SS1.p1.5.m5.1.1.2" xref="S6.SS1.p1.5.m5.1.1.2.cmml">X</mi><mi id="S6.SS1.p1.5.m5.1.1.3" xref="S6.SS1.p1.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.5.m5.1b"><apply id="S6.SS1.p1.5.m5.1.1.cmml" xref="S6.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S6.SS1.p1.5.m5.1.1.1.cmml" xref="S6.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S6.SS1.p1.5.m5.1.1.2.cmml" xref="S6.SS1.p1.5.m5.1.1.2">𝑋</ci><ci id="S6.SS1.p1.5.m5.1.1.3.cmml" xref="S6.SS1.p1.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.5.m5.1c">X_{i}</annotation></semantics></math>. Effectively transferring image-level supervision to object-level labels is the key challenge in WSOD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib225" title="" class="ltx_ref">225</a>]</cite>.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">Han <span id="S6.SS1.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib226" title="" class="ltx_ref">226</a>]</cite> introduced a deep Boltzmann machine to learn the high-level features of objects and proposed a weakly-supervised learning framework based on Bayesian principles for remote sensing WSOD. Li <span id="S6.SS1.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib227" title="" class="ltx_ref">227</a>]</cite> exploited the mutual information between scene pairs to learn the discriminative convolutional weights and employed a multi-scale category activation map to locate geospatial objects.</p>
</div>
<div id="S6.SS1.p3" class="ltx_para">
<p id="S6.SS1.p3.1" class="ltx_p">Motivated by the remarkable performance of WSDDN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib228" title="" class="ltx_ref">228</a>]</cite>, a series of remote sensing WSOD methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib229" title="" class="ltx_ref">229</a>, <a href="#bib.bib230" title="" class="ltx_ref">230</a>, <a href="#bib.bib231" title="" class="ltx_ref">231</a>, <a href="#bib.bib232" title="" class="ltx_ref">232</a>, <a href="#bib.bib233" title="" class="ltx_ref">233</a>, <a href="#bib.bib234" title="" class="ltx_ref">234</a>, <a href="#bib.bib235" title="" class="ltx_ref">235</a>, <a href="#bib.bib236" title="" class="ltx_ref">236</a>, <a href="#bib.bib237" title="" class="ltx_ref">237</a>, <a href="#bib.bib238" title="" class="ltx_ref">238</a>, <a href="#bib.bib239" title="" class="ltx_ref">239</a>, <a href="#bib.bib240" title="" class="ltx_ref">240</a>, <a href="#bib.bib241" title="" class="ltx_ref">241</a>]</cite> are proposed. As shown in Fig. <a href="#S6.F15" title="Figure 15 ‣ VI-A Weakly-supervised Object detection ‣ VI Object Detection with Limited Supervision ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>, the paradigm of the current WSOD methods usually consists of two steps, which first constructs a multiple instance learning model (MIL) to find contributing proposals to the image classification task as pseudo-labels and then utilizes them to train the detector. Yao <span id="S6.SS1.p3.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib229" title="" class="ltx_ref">229</a>]</cite> introduced a dynamic curriculum learning strategy where the detector progressively improves detection performance through an easy-to-hard training process. Feng <span id="S6.SS1.p3.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib231" title="" class="ltx_ref">231</a>]</cite> designed a progressive contextual instance refinement method that suppresses low-quality object parts and highlights the whole object by leveraging surrounding context information. Wang <span id="S6.SS1.p3.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib233" title="" class="ltx_ref">233</a>]</cite> introduced the spatial and appearance relation graph into WSOD, which propagates high-quality label information to mine more possible objects. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib240" title="" class="ltx_ref">240</a>]</cite>, Feng <span id="S6.SS1.p3.1.4" class="ltx_text ltx_font_italic">et al.</span> argued that existing remote sensing WSOD methods ignored the arbitrary orientations of geospatial objects, resulting in rotation-sensitive object detectors. To address this problem, Feng <span id="S6.SS1.p3.1.5" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib240" title="" class="ltx_ref">240</a>]</cite> proposed a RINet, which brings rotation-invariant yet diverse feature learning for WSOD by employing rotation-invariant learning and multi-instance mining.</p>
</div>
<div id="S6.SS1.p4" class="ltx_para">
<p id="S6.SS1.p4.1" class="ltx_p">We summarize the performance of milestone WSOD methods in Table <a href="#S6.T2" title="TABLE II ‣ VI-A Weakly-supervised Object detection ‣ VI Object Detection with Limited Supervision ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, where the correct localization metric (CorLoc)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib242" title="" class="ltx_ref">242</a>]</cite> is adopted to evaluate the localization performance.</p>
</div>
<figure id="S6.F15" class="ltx_figure"><img src="/html/2309.06751/assets/x16.png" id="S6.F15.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="212" height="93" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>The two-step paradigm of recent WSOD methods<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib229" title="" class="ltx_ref">229</a>, <a href="#bib.bib230" title="" class="ltx_ref">230</a>, <a href="#bib.bib231" title="" class="ltx_ref">231</a>, <a href="#bib.bib232" title="" class="ltx_ref">232</a>, <a href="#bib.bib233" title="" class="ltx_ref">233</a>, <a href="#bib.bib234" title="" class="ltx_ref">234</a>, <a href="#bib.bib235" title="" class="ltx_ref">235</a>, <a href="#bib.bib236" title="" class="ltx_ref">236</a>, <a href="#bib.bib237" title="" class="ltx_ref">237</a>, <a href="#bib.bib238" title="" class="ltx_ref">238</a>, <a href="#bib.bib239" title="" class="ltx_ref">239</a>, <a href="#bib.bib240" title="" class="ltx_ref">240</a>, <a href="#bib.bib241" title="" class="ltx_ref">241</a>]</cite>.</figcaption>
</figure>
<figure id="S6.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Performance of weakly supervised object detection methods on the NWPU VHR-10.v2 and DIOR datasets.</figcaption>
<table id="S6.T2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T2.3.1.1" class="ltx_tr">
<th id="S6.T2.3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:1.6pt 4.0pt;" rowspan="2"><span id="S6.T2.3.1.1.1.1" class="ltx_text" style="font-size:80%;">Models</span></th>
<th id="S6.T2.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:1.6pt 4.0pt;" colspan="2"><span id="S6.T2.3.1.1.2.1" class="ltx_text" style="font-size:80%;">NWPU VHR-10</span></th>
<th id="S6.T2.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:1.6pt 4.0pt;" colspan="2"><span id="S6.T2.3.1.1.3.1" class="ltx_text" style="font-size:80%;">DIOR</span></th>
</tr>
<tr id="S6.T2.3.2.2" class="ltx_tr">
<th id="S6.T2.3.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.2.2.1.1" class="ltx_text" style="font-size:80%;">CorLoc(%)</span></th>
<th id="S6.T2.3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.2.2.2.1" class="ltx_text" style="font-size:80%;">mAP(%)</span></th>
<th id="S6.T2.3.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.2.2.3.1" class="ltx_text" style="font-size:80%;">CorLoc(%)</span></th>
<th id="S6.T2.3.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.2.2.4.1" class="ltx_text" style="font-size:80%;">mAP(%)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T2.3.3.1" class="ltx_tr">
<td id="S6.T2.3.3.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.6pt 4.0pt;">
<span id="S6.T2.3.3.1.1.1" class="ltx_text" style="font-size:80%;">WSDDN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T2.3.3.1.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib228" title="" class="ltx_ref">228</a><span id="S6.T2.3.3.1.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S6.T2.3.3.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.3.1.2.1" class="ltx_text" style="font-size:80%;">35.24</span></td>
<td id="S6.T2.3.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.3.1.3.1" class="ltx_text" style="font-size:80%;">35.12</span></td>
<td id="S6.T2.3.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.3.1.4.1" class="ltx_text" style="font-size:80%;">32.44</span></td>
<td id="S6.T2.3.3.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.3.1.5.1" class="ltx_text" style="font-size:80%;">13.26</span></td>
</tr>
<tr id="S6.T2.3.4.2" class="ltx_tr">
<td id="S6.T2.3.4.2.1" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;">
<span id="S6.T2.3.4.2.1.1" class="ltx_text" style="font-size:80%;">DCL </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T2.3.4.2.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib229" title="" class="ltx_ref">229</a><span id="S6.T2.3.4.2.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S6.T2.3.4.2.2" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.4.2.2.1" class="ltx_text" style="font-size:80%;">69.65</span></td>
<td id="S6.T2.3.4.2.3" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.4.2.3.1" class="ltx_text" style="font-size:80%;">52.11</span></td>
<td id="S6.T2.3.4.2.4" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.4.2.4.1" class="ltx_text" style="font-size:80%;">42.23</span></td>
<td id="S6.T2.3.4.2.5" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.4.2.5.1" class="ltx_text" style="font-size:80%;">20.19</span></td>
</tr>
<tr id="S6.T2.3.5.3" class="ltx_tr">
<td id="S6.T2.3.5.3.1" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;">
<span id="S6.T2.3.5.3.1.1" class="ltx_text" style="font-size:80%;">DPLG </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T2.3.5.3.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib230" title="" class="ltx_ref">230</a><span id="S6.T2.3.5.3.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S6.T2.3.5.3.2" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.5.3.2.1" class="ltx_text" style="font-size:80%;">61.50</span></td>
<td id="S6.T2.3.5.3.3" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.5.3.3.1" class="ltx_text" style="font-size:80%;">53.60</span></td>
<td id="S6.T2.3.5.3.4" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.5.3.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S6.T2.3.5.3.5" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.5.3.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S6.T2.3.6.4" class="ltx_tr">
<td id="S6.T2.3.6.4.1" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;">
<span id="S6.T2.3.6.4.1.1" class="ltx_text" style="font-size:80%;">PCIR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T2.3.6.4.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib231" title="" class="ltx_ref">231</a><span id="S6.T2.3.6.4.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S6.T2.3.6.4.2" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.6.4.2.1" class="ltx_text" style="font-size:80%;">71.87</span></td>
<td id="S6.T2.3.6.4.3" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.6.4.3.1" class="ltx_text" style="font-size:80%;">54.97</span></td>
<td id="S6.T2.3.6.4.4" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.6.4.4.1" class="ltx_text" style="font-size:80%;">46.12</span></td>
<td id="S6.T2.3.6.4.5" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.6.4.5.1" class="ltx_text" style="font-size:80%;">24.92</span></td>
</tr>
<tr id="S6.T2.3.7.5" class="ltx_tr">
<td id="S6.T2.3.7.5.1" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;">
<span id="S6.T2.3.7.5.1.1" class="ltx_text" style="font-size:80%;">MIGL </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T2.3.7.5.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib233" title="" class="ltx_ref">233</a><span id="S6.T2.3.7.5.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S6.T2.3.7.5.2" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.7.5.2.1" class="ltx_text" style="font-size:80%;">70.16</span></td>
<td id="S6.T2.3.7.5.3" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.7.5.3.1" class="ltx_text" style="font-size:80%;">55.95</span></td>
<td id="S6.T2.3.7.5.4" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.7.5.4.1" class="ltx_text" style="font-size:80%;">46.80</span></td>
<td id="S6.T2.3.7.5.5" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.7.5.5.1" class="ltx_text" style="font-size:80%;">25.11</span></td>
</tr>
<tr id="S6.T2.3.8.6" class="ltx_tr">
<td id="S6.T2.3.8.6.1" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;">
<span id="S6.T2.3.8.6.1.1" class="ltx_text" style="font-size:80%;">TCANet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T2.3.8.6.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib234" title="" class="ltx_ref">234</a><span id="S6.T2.3.8.6.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S6.T2.3.8.6.2" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.8.6.2.1" class="ltx_text" style="font-size:80%;">72.76</span></td>
<td id="S6.T2.3.8.6.3" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.8.6.3.1" class="ltx_text" style="font-size:80%;">58.82</span></td>
<td id="S6.T2.3.8.6.4" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.8.6.4.1" class="ltx_text" style="font-size:80%;">48.41</span></td>
<td id="S6.T2.3.8.6.5" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.8.6.5.1" class="ltx_text" style="font-size:80%;">25.82</span></td>
</tr>
<tr id="S6.T2.3.9.7" class="ltx_tr">
<td id="S6.T2.3.9.7.1" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;">
<span id="S6.T2.3.9.7.1.1" class="ltx_text" style="font-size:80%;">SAENet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T2.3.9.7.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib235" title="" class="ltx_ref">235</a><span id="S6.T2.3.9.7.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S6.T2.3.9.7.2" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.9.7.2.1" class="ltx_text" style="font-size:80%;">73.46</span></td>
<td id="S6.T2.3.9.7.3" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.9.7.3.1" class="ltx_text" style="font-size:80%;">60.72</span></td>
<td id="S6.T2.3.9.7.4" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.9.7.4.1" class="ltx_text" style="font-size:80%;">49.42</span></td>
<td id="S6.T2.3.9.7.5" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.9.7.5.1" class="ltx_text" style="font-size:80%;">27.10</span></td>
</tr>
<tr id="S6.T2.3.10.8" class="ltx_tr">
<td id="S6.T2.3.10.8.1" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;">
<span id="S6.T2.3.10.8.1.1" class="ltx_text" style="font-size:80%;">OS-DES </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T2.3.10.8.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib236" title="" class="ltx_ref">236</a><span id="S6.T2.3.10.8.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S6.T2.3.10.8.2" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.10.8.2.1" class="ltx_text" style="font-size:80%;">73.68</span></td>
<td id="S6.T2.3.10.8.3" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.10.8.3.1" class="ltx_text" style="font-size:80%;">61.49</span></td>
<td id="S6.T2.3.10.8.4" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.10.8.4.1" class="ltx_text" style="font-size:80%;">49.92</span></td>
<td id="S6.T2.3.10.8.5" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.10.8.5.1" class="ltx_text" style="font-size:80%;">27.52</span></td>
</tr>
<tr id="S6.T2.3.11.9" class="ltx_tr">
<td id="S6.T2.3.11.9.1" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;">
<span id="S6.T2.3.11.9.1.1" class="ltx_text" style="font-size:80%;">SPG+MELM </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T2.3.11.9.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib239" title="" class="ltx_ref">239</a><span id="S6.T2.3.11.9.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S6.T2.3.11.9.2" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.11.9.2.1" class="ltx_text" style="font-size:80%;">73.41</span></td>
<td id="S6.T2.3.11.9.3" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.11.9.3.1" class="ltx_text" style="font-size:80%;">62.80</span></td>
<td id="S6.T2.3.11.9.4" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.11.9.4.1" class="ltx_text" style="font-size:80%;">48.30</span></td>
<td id="S6.T2.3.11.9.5" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.11.9.5.1" class="ltx_text" style="font-size:80%;">25.77</span></td>
</tr>
<tr id="S6.T2.3.12.10" class="ltx_tr">
<td id="S6.T2.3.12.10.1" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;">
<span id="S6.T2.3.12.10.1.1" class="ltx_text" style="font-size:80%;">RINet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T2.3.12.10.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib240" title="" class="ltx_ref">240</a><span id="S6.T2.3.12.10.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S6.T2.3.12.10.2" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.12.10.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S6.T2.3.12.10.3" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.12.10.3.1" class="ltx_text" style="font-size:80%;">70.4</span></td>
<td id="S6.T2.3.12.10.4" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.12.10.4.1" class="ltx_text" style="font-size:80%;">52.8</span></td>
<td id="S6.T2.3.12.10.5" class="ltx_td ltx_align_center" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.12.10.5.1" class="ltx_text" style="font-size:80%;">28.3</span></td>
</tr>
<tr id="S6.T2.3.13.11" class="ltx_tr">
<td id="S6.T2.3.13.11.1" class="ltx_td ltx_align_center ltx_border_b" style="padding:1.6pt 4.0pt;">
<span id="S6.T2.3.13.11.1.1" class="ltx_text" style="font-size:80%;">MOL </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T2.3.13.11.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib241" title="" class="ltx_ref">241</a><span id="S6.T2.3.13.11.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S6.T2.3.13.11.2" class="ltx_td ltx_align_center ltx_border_b" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.13.11.2.1" class="ltx_text" style="font-size:80%;">75.96</span></td>
<td id="S6.T2.3.13.11.3" class="ltx_td ltx_align_center ltx_border_b" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.13.11.3.1" class="ltx_text" style="font-size:80%;">75.46</span></td>
<td id="S6.T2.3.13.11.4" class="ltx_td ltx_align_center ltx_border_b" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.13.11.4.1" class="ltx_text" style="font-size:80%;">50.66</span></td>
<td id="S6.T2.3.13.11.5" class="ltx_td ltx_align_center ltx_border_b" style="padding:1.6pt 4.0pt;"><span id="S6.T2.3.13.11.5.1" class="ltx_text" style="font-size:80%;">29.21</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS2.4.1.1" class="ltx_text">VI-B</span> </span><span id="S6.SS2.5.2" class="ltx_text ltx_font_italic">Semi-supervised Object detection</span>
</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.2" class="ltx_p">Semi-supervised Object detection (SSOD) typically contains only a small part (no more than 50%) of well-labeled samples <math id="S6.SS2.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{D}_{\text{labeled}}=\left\{\left(x_{i},y_{i}\right)\right\}_{i=1}^{I_{labeled}}" display="inline"><semantics id="S6.SS2.p1.1.m1.1a"><mrow id="S6.SS2.p1.1.m1.1.1" xref="S6.SS2.p1.1.m1.1.1.cmml"><msub id="S6.SS2.p1.1.m1.1.1.3" xref="S6.SS2.p1.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.SS2.p1.1.m1.1.1.3.2" xref="S6.SS2.p1.1.m1.1.1.3.2.cmml">𝒟</mi><mtext id="S6.SS2.p1.1.m1.1.1.3.3" xref="S6.SS2.p1.1.m1.1.1.3.3a.cmml">labeled</mtext></msub><mo id="S6.SS2.p1.1.m1.1.1.2" xref="S6.SS2.p1.1.m1.1.1.2.cmml">=</mo><msubsup id="S6.SS2.p1.1.m1.1.1.1" xref="S6.SS2.p1.1.m1.1.1.1.cmml"><mrow id="S6.SS2.p1.1.m1.1.1.1.1.1.1" xref="S6.SS2.p1.1.m1.1.1.1.1.1.2.cmml"><mo id="S6.SS2.p1.1.m1.1.1.1.1.1.1.2" xref="S6.SS2.p1.1.m1.1.1.1.1.1.2.cmml">{</mo><mrow id="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.2" xref="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.3.cmml"><mo id="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.2.3" xref="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.3.cmml">(</mo><msub id="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.1.1" xref="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.1.1.2" xref="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.1.1.3" xref="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.2.4" xref="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.3.cmml">,</mo><msub id="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.2.2" xref="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.2.2.2" xref="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.2.2.2.cmml">y</mi><mi id="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.2.2.3" xref="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.2.2.3.cmml">i</mi></msub><mo id="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.2.5" xref="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.3.cmml">)</mo></mrow><mo id="S6.SS2.p1.1.m1.1.1.1.1.1.1.3" xref="S6.SS2.p1.1.m1.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S6.SS2.p1.1.m1.1.1.1.1.3" xref="S6.SS2.p1.1.m1.1.1.1.1.3.cmml"><mi id="S6.SS2.p1.1.m1.1.1.1.1.3.2" xref="S6.SS2.p1.1.m1.1.1.1.1.3.2.cmml">i</mi><mo id="S6.SS2.p1.1.m1.1.1.1.1.3.1" xref="S6.SS2.p1.1.m1.1.1.1.1.3.1.cmml">=</mo><mn id="S6.SS2.p1.1.m1.1.1.1.1.3.3" xref="S6.SS2.p1.1.m1.1.1.1.1.3.3.cmml">1</mn></mrow><msub id="S6.SS2.p1.1.m1.1.1.1.3" xref="S6.SS2.p1.1.m1.1.1.1.3.cmml"><mi id="S6.SS2.p1.1.m1.1.1.1.3.2" xref="S6.SS2.p1.1.m1.1.1.1.3.2.cmml">I</mi><mrow id="S6.SS2.p1.1.m1.1.1.1.3.3" xref="S6.SS2.p1.1.m1.1.1.1.3.3.cmml"><mi id="S6.SS2.p1.1.m1.1.1.1.3.3.2" xref="S6.SS2.p1.1.m1.1.1.1.3.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S6.SS2.p1.1.m1.1.1.1.3.3.1" xref="S6.SS2.p1.1.m1.1.1.1.3.3.1.cmml">​</mo><mi id="S6.SS2.p1.1.m1.1.1.1.3.3.3" xref="S6.SS2.p1.1.m1.1.1.1.3.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S6.SS2.p1.1.m1.1.1.1.3.3.1a" xref="S6.SS2.p1.1.m1.1.1.1.3.3.1.cmml">​</mo><mi id="S6.SS2.p1.1.m1.1.1.1.3.3.4" xref="S6.SS2.p1.1.m1.1.1.1.3.3.4.cmml">b</mi><mo lspace="0em" rspace="0em" id="S6.SS2.p1.1.m1.1.1.1.3.3.1b" xref="S6.SS2.p1.1.m1.1.1.1.3.3.1.cmml">​</mo><mi id="S6.SS2.p1.1.m1.1.1.1.3.3.5" xref="S6.SS2.p1.1.m1.1.1.1.3.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.SS2.p1.1.m1.1.1.1.3.3.1c" xref="S6.SS2.p1.1.m1.1.1.1.3.3.1.cmml">​</mo><mi id="S6.SS2.p1.1.m1.1.1.1.3.3.6" xref="S6.SS2.p1.1.m1.1.1.1.3.3.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S6.SS2.p1.1.m1.1.1.1.3.3.1d" xref="S6.SS2.p1.1.m1.1.1.1.3.3.1.cmml">​</mo><mi id="S6.SS2.p1.1.m1.1.1.1.3.3.7" xref="S6.SS2.p1.1.m1.1.1.1.3.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.SS2.p1.1.m1.1.1.1.3.3.1e" xref="S6.SS2.p1.1.m1.1.1.1.3.3.1.cmml">​</mo><mi id="S6.SS2.p1.1.m1.1.1.1.3.3.8" xref="S6.SS2.p1.1.m1.1.1.1.3.3.8.cmml">d</mi></mrow></msub></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.1.m1.1b"><apply id="S6.SS2.p1.1.m1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1"><eq id="S6.SS2.p1.1.m1.1.1.2.cmml" xref="S6.SS2.p1.1.m1.1.1.2"></eq><apply id="S6.SS2.p1.1.m1.1.1.3.cmml" xref="S6.SS2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S6.SS2.p1.1.m1.1.1.3.1.cmml" xref="S6.SS2.p1.1.m1.1.1.3">subscript</csymbol><ci id="S6.SS2.p1.1.m1.1.1.3.2.cmml" xref="S6.SS2.p1.1.m1.1.1.3.2">𝒟</ci><ci id="S6.SS2.p1.1.m1.1.1.3.3a.cmml" xref="S6.SS2.p1.1.m1.1.1.3.3"><mtext mathsize="70%" id="S6.SS2.p1.1.m1.1.1.3.3.cmml" xref="S6.SS2.p1.1.m1.1.1.3.3">labeled</mtext></ci></apply><apply id="S6.SS2.p1.1.m1.1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1.1"><csymbol cd="ambiguous" id="S6.SS2.p1.1.m1.1.1.1.2.cmml" xref="S6.SS2.p1.1.m1.1.1.1">superscript</csymbol><apply id="S6.SS2.p1.1.m1.1.1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1.1"><csymbol cd="ambiguous" id="S6.SS2.p1.1.m1.1.1.1.1.2.cmml" xref="S6.SS2.p1.1.m1.1.1.1">subscript</csymbol><set id="S6.SS2.p1.1.m1.1.1.1.1.1.2.cmml" xref="S6.SS2.p1.1.m1.1.1.1.1.1.1"><interval closure="open" id="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.2"><apply id="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.2.2.2">𝑦</ci><ci id="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S6.SS2.p1.1.m1.1.1.1.1.1.1.1.2.2.3">𝑖</ci></apply></interval></set><apply id="S6.SS2.p1.1.m1.1.1.1.1.3.cmml" xref="S6.SS2.p1.1.m1.1.1.1.1.3"><eq id="S6.SS2.p1.1.m1.1.1.1.1.3.1.cmml" xref="S6.SS2.p1.1.m1.1.1.1.1.3.1"></eq><ci id="S6.SS2.p1.1.m1.1.1.1.1.3.2.cmml" xref="S6.SS2.p1.1.m1.1.1.1.1.3.2">𝑖</ci><cn type="integer" id="S6.SS2.p1.1.m1.1.1.1.1.3.3.cmml" xref="S6.SS2.p1.1.m1.1.1.1.1.3.3">1</cn></apply></apply><apply id="S6.SS2.p1.1.m1.1.1.1.3.cmml" xref="S6.SS2.p1.1.m1.1.1.1.3"><csymbol cd="ambiguous" id="S6.SS2.p1.1.m1.1.1.1.3.1.cmml" xref="S6.SS2.p1.1.m1.1.1.1.3">subscript</csymbol><ci id="S6.SS2.p1.1.m1.1.1.1.3.2.cmml" xref="S6.SS2.p1.1.m1.1.1.1.3.2">𝐼</ci><apply id="S6.SS2.p1.1.m1.1.1.1.3.3.cmml" xref="S6.SS2.p1.1.m1.1.1.1.3.3"><times id="S6.SS2.p1.1.m1.1.1.1.3.3.1.cmml" xref="S6.SS2.p1.1.m1.1.1.1.3.3.1"></times><ci id="S6.SS2.p1.1.m1.1.1.1.3.3.2.cmml" xref="S6.SS2.p1.1.m1.1.1.1.3.3.2">𝑙</ci><ci id="S6.SS2.p1.1.m1.1.1.1.3.3.3.cmml" xref="S6.SS2.p1.1.m1.1.1.1.3.3.3">𝑎</ci><ci id="S6.SS2.p1.1.m1.1.1.1.3.3.4.cmml" xref="S6.SS2.p1.1.m1.1.1.1.3.3.4">𝑏</ci><ci id="S6.SS2.p1.1.m1.1.1.1.3.3.5.cmml" xref="S6.SS2.p1.1.m1.1.1.1.3.3.5">𝑒</ci><ci id="S6.SS2.p1.1.m1.1.1.1.3.3.6.cmml" xref="S6.SS2.p1.1.m1.1.1.1.3.3.6">𝑙</ci><ci id="S6.SS2.p1.1.m1.1.1.1.3.3.7.cmml" xref="S6.SS2.p1.1.m1.1.1.1.3.3.7">𝑒</ci><ci id="S6.SS2.p1.1.m1.1.1.1.3.3.8.cmml" xref="S6.SS2.p1.1.m1.1.1.1.3.3.8">𝑑</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.1.m1.1c">\mathcal{D}_{\text{labeled}}=\left\{\left(x_{i},y_{i}\right)\right\}_{i=1}^{I_{labeled}}</annotation></semantics></math>, difficult to construct a reliable supervised detector, and has a large number of unlabeled samples <math id="S6.SS2.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{D}_{\text{unlabeled}}=\left\{\left(x_{j}\right)\right\}_{j=1}^{I_{unlabeled}}" display="inline"><semantics id="S6.SS2.p1.2.m2.1a"><mrow id="S6.SS2.p1.2.m2.1.1" xref="S6.SS2.p1.2.m2.1.1.cmml"><msub id="S6.SS2.p1.2.m2.1.1.3" xref="S6.SS2.p1.2.m2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.SS2.p1.2.m2.1.1.3.2" xref="S6.SS2.p1.2.m2.1.1.3.2.cmml">𝒟</mi><mtext id="S6.SS2.p1.2.m2.1.1.3.3" xref="S6.SS2.p1.2.m2.1.1.3.3a.cmml">unlabeled</mtext></msub><mo id="S6.SS2.p1.2.m2.1.1.2" xref="S6.SS2.p1.2.m2.1.1.2.cmml">=</mo><msubsup id="S6.SS2.p1.2.m2.1.1.1" xref="S6.SS2.p1.2.m2.1.1.1.cmml"><mrow id="S6.SS2.p1.2.m2.1.1.1.1.1.1" xref="S6.SS2.p1.2.m2.1.1.1.1.1.2.cmml"><mo id="S6.SS2.p1.2.m2.1.1.1.1.1.1.2" xref="S6.SS2.p1.2.m2.1.1.1.1.1.2.cmml">{</mo><mrow id="S6.SS2.p1.2.m2.1.1.1.1.1.1.1.1" xref="S6.SS2.p1.2.m2.1.1.1.1.1.1.1.1.1.cmml"><mo id="S6.SS2.p1.2.m2.1.1.1.1.1.1.1.1.2" xref="S6.SS2.p1.2.m2.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S6.SS2.p1.2.m2.1.1.1.1.1.1.1.1.1" xref="S6.SS2.p1.2.m2.1.1.1.1.1.1.1.1.1.cmml"><mi id="S6.SS2.p1.2.m2.1.1.1.1.1.1.1.1.1.2" xref="S6.SS2.p1.2.m2.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S6.SS2.p1.2.m2.1.1.1.1.1.1.1.1.1.3" xref="S6.SS2.p1.2.m2.1.1.1.1.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S6.SS2.p1.2.m2.1.1.1.1.1.1.1.1.3" xref="S6.SS2.p1.2.m2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S6.SS2.p1.2.m2.1.1.1.1.1.1.3" xref="S6.SS2.p1.2.m2.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S6.SS2.p1.2.m2.1.1.1.1.3" xref="S6.SS2.p1.2.m2.1.1.1.1.3.cmml"><mi id="S6.SS2.p1.2.m2.1.1.1.1.3.2" xref="S6.SS2.p1.2.m2.1.1.1.1.3.2.cmml">j</mi><mo id="S6.SS2.p1.2.m2.1.1.1.1.3.1" xref="S6.SS2.p1.2.m2.1.1.1.1.3.1.cmml">=</mo><mn id="S6.SS2.p1.2.m2.1.1.1.1.3.3" xref="S6.SS2.p1.2.m2.1.1.1.1.3.3.cmml">1</mn></mrow><msub id="S6.SS2.p1.2.m2.1.1.1.3" xref="S6.SS2.p1.2.m2.1.1.1.3.cmml"><mi id="S6.SS2.p1.2.m2.1.1.1.3.2" xref="S6.SS2.p1.2.m2.1.1.1.3.2.cmml">I</mi><mrow id="S6.SS2.p1.2.m2.1.1.1.3.3" xref="S6.SS2.p1.2.m2.1.1.1.3.3.cmml"><mi id="S6.SS2.p1.2.m2.1.1.1.3.3.2" xref="S6.SS2.p1.2.m2.1.1.1.3.3.2.cmml">u</mi><mo lspace="0em" rspace="0em" id="S6.SS2.p1.2.m2.1.1.1.3.3.1" xref="S6.SS2.p1.2.m2.1.1.1.3.3.1.cmml">​</mo><mi id="S6.SS2.p1.2.m2.1.1.1.3.3.3" xref="S6.SS2.p1.2.m2.1.1.1.3.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S6.SS2.p1.2.m2.1.1.1.3.3.1a" xref="S6.SS2.p1.2.m2.1.1.1.3.3.1.cmml">​</mo><mi id="S6.SS2.p1.2.m2.1.1.1.3.3.4" xref="S6.SS2.p1.2.m2.1.1.1.3.3.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S6.SS2.p1.2.m2.1.1.1.3.3.1b" xref="S6.SS2.p1.2.m2.1.1.1.3.3.1.cmml">​</mo><mi id="S6.SS2.p1.2.m2.1.1.1.3.3.5" xref="S6.SS2.p1.2.m2.1.1.1.3.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S6.SS2.p1.2.m2.1.1.1.3.3.1c" xref="S6.SS2.p1.2.m2.1.1.1.3.3.1.cmml">​</mo><mi id="S6.SS2.p1.2.m2.1.1.1.3.3.6" xref="S6.SS2.p1.2.m2.1.1.1.3.3.6.cmml">b</mi><mo lspace="0em" rspace="0em" id="S6.SS2.p1.2.m2.1.1.1.3.3.1d" xref="S6.SS2.p1.2.m2.1.1.1.3.3.1.cmml">​</mo><mi id="S6.SS2.p1.2.m2.1.1.1.3.3.7" xref="S6.SS2.p1.2.m2.1.1.1.3.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.SS2.p1.2.m2.1.1.1.3.3.1e" xref="S6.SS2.p1.2.m2.1.1.1.3.3.1.cmml">​</mo><mi id="S6.SS2.p1.2.m2.1.1.1.3.3.8" xref="S6.SS2.p1.2.m2.1.1.1.3.3.8.cmml">l</mi><mo lspace="0em" rspace="0em" id="S6.SS2.p1.2.m2.1.1.1.3.3.1f" xref="S6.SS2.p1.2.m2.1.1.1.3.3.1.cmml">​</mo><mi id="S6.SS2.p1.2.m2.1.1.1.3.3.9" xref="S6.SS2.p1.2.m2.1.1.1.3.3.9.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.SS2.p1.2.m2.1.1.1.3.3.1g" xref="S6.SS2.p1.2.m2.1.1.1.3.3.1.cmml">​</mo><mi id="S6.SS2.p1.2.m2.1.1.1.3.3.10" xref="S6.SS2.p1.2.m2.1.1.1.3.3.10.cmml">d</mi></mrow></msub></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.2.m2.1b"><apply id="S6.SS2.p1.2.m2.1.1.cmml" xref="S6.SS2.p1.2.m2.1.1"><eq id="S6.SS2.p1.2.m2.1.1.2.cmml" xref="S6.SS2.p1.2.m2.1.1.2"></eq><apply id="S6.SS2.p1.2.m2.1.1.3.cmml" xref="S6.SS2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S6.SS2.p1.2.m2.1.1.3.1.cmml" xref="S6.SS2.p1.2.m2.1.1.3">subscript</csymbol><ci id="S6.SS2.p1.2.m2.1.1.3.2.cmml" xref="S6.SS2.p1.2.m2.1.1.3.2">𝒟</ci><ci id="S6.SS2.p1.2.m2.1.1.3.3a.cmml" xref="S6.SS2.p1.2.m2.1.1.3.3"><mtext mathsize="70%" id="S6.SS2.p1.2.m2.1.1.3.3.cmml" xref="S6.SS2.p1.2.m2.1.1.3.3">unlabeled</mtext></ci></apply><apply id="S6.SS2.p1.2.m2.1.1.1.cmml" xref="S6.SS2.p1.2.m2.1.1.1"><csymbol cd="ambiguous" id="S6.SS2.p1.2.m2.1.1.1.2.cmml" xref="S6.SS2.p1.2.m2.1.1.1">superscript</csymbol><apply id="S6.SS2.p1.2.m2.1.1.1.1.cmml" xref="S6.SS2.p1.2.m2.1.1.1"><csymbol cd="ambiguous" id="S6.SS2.p1.2.m2.1.1.1.1.2.cmml" xref="S6.SS2.p1.2.m2.1.1.1">subscript</csymbol><set id="S6.SS2.p1.2.m2.1.1.1.1.1.2.cmml" xref="S6.SS2.p1.2.m2.1.1.1.1.1.1"><apply id="S6.SS2.p1.2.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S6.SS2.p1.2.m2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S6.SS2.p1.2.m2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S6.SS2.p1.2.m2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S6.SS2.p1.2.m2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S6.SS2.p1.2.m2.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S6.SS2.p1.2.m2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S6.SS2.p1.2.m2.1.1.1.1.1.1.1.1.1.3">𝑗</ci></apply></set><apply id="S6.SS2.p1.2.m2.1.1.1.1.3.cmml" xref="S6.SS2.p1.2.m2.1.1.1.1.3"><eq id="S6.SS2.p1.2.m2.1.1.1.1.3.1.cmml" xref="S6.SS2.p1.2.m2.1.1.1.1.3.1"></eq><ci id="S6.SS2.p1.2.m2.1.1.1.1.3.2.cmml" xref="S6.SS2.p1.2.m2.1.1.1.1.3.2">𝑗</ci><cn type="integer" id="S6.SS2.p1.2.m2.1.1.1.1.3.3.cmml" xref="S6.SS2.p1.2.m2.1.1.1.1.3.3">1</cn></apply></apply><apply id="S6.SS2.p1.2.m2.1.1.1.3.cmml" xref="S6.SS2.p1.2.m2.1.1.1.3"><csymbol cd="ambiguous" id="S6.SS2.p1.2.m2.1.1.1.3.1.cmml" xref="S6.SS2.p1.2.m2.1.1.1.3">subscript</csymbol><ci id="S6.SS2.p1.2.m2.1.1.1.3.2.cmml" xref="S6.SS2.p1.2.m2.1.1.1.3.2">𝐼</ci><apply id="S6.SS2.p1.2.m2.1.1.1.3.3.cmml" xref="S6.SS2.p1.2.m2.1.1.1.3.3"><times id="S6.SS2.p1.2.m2.1.1.1.3.3.1.cmml" xref="S6.SS2.p1.2.m2.1.1.1.3.3.1"></times><ci id="S6.SS2.p1.2.m2.1.1.1.3.3.2.cmml" xref="S6.SS2.p1.2.m2.1.1.1.3.3.2">𝑢</ci><ci id="S6.SS2.p1.2.m2.1.1.1.3.3.3.cmml" xref="S6.SS2.p1.2.m2.1.1.1.3.3.3">𝑛</ci><ci id="S6.SS2.p1.2.m2.1.1.1.3.3.4.cmml" xref="S6.SS2.p1.2.m2.1.1.1.3.3.4">𝑙</ci><ci id="S6.SS2.p1.2.m2.1.1.1.3.3.5.cmml" xref="S6.SS2.p1.2.m2.1.1.1.3.3.5">𝑎</ci><ci id="S6.SS2.p1.2.m2.1.1.1.3.3.6.cmml" xref="S6.SS2.p1.2.m2.1.1.1.3.3.6">𝑏</ci><ci id="S6.SS2.p1.2.m2.1.1.1.3.3.7.cmml" xref="S6.SS2.p1.2.m2.1.1.1.3.3.7">𝑒</ci><ci id="S6.SS2.p1.2.m2.1.1.1.3.3.8.cmml" xref="S6.SS2.p1.2.m2.1.1.1.3.3.8">𝑙</ci><ci id="S6.SS2.p1.2.m2.1.1.1.3.3.9.cmml" xref="S6.SS2.p1.2.m2.1.1.1.3.3.9">𝑒</ci><ci id="S6.SS2.p1.2.m2.1.1.1.3.3.10.cmml" xref="S6.SS2.p1.2.m2.1.1.1.3.3.10">𝑑</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.2.m2.1c">\mathcal{D}_{\text{unlabeled}}=\left\{\left(x_{j}\right)\right\}_{j=1}^{I_{unlabeled}}</annotation></semantics></math>. SSOD aims to improve detection performance under scarce supervised information by learning the latent information from volume unlabeled samples.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">Hou <span id="S6.SS2.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib243" title="" class="ltx_ref">243</a>]</cite> proposed a SCLANet for semi-supervised SAR ship detection. The SCLANet employs adversarial learning between labeled and unlabeled samples to exploit the unlabeled sample information and adopts consistency learning for unlabeled samples to enhance the robustness of the network. The pseudo-label generation mechanism is also a widely used approach for semi-supervised object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib244" title="" class="ltx_ref">244</a>, <a href="#bib.bib245" title="" class="ltx_ref">245</a>, <a href="#bib.bib246" title="" class="ltx_ref">246</a>, <a href="#bib.bib247" title="" class="ltx_ref">247</a>, <a href="#bib.bib248" title="" class="ltx_ref">248</a>]</cite>, and the typical paradigm is shown in Fig. <a href="#S6.F16" title="Figure 16 ‣ VI-B Semi-supervised Object detection ‣ VI Object Detection with Limited Supervision ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a>. First, a pre-trained detector learned from scare labeled samples are used to predict unlabeled samples, then the pseudo labels with higher confidence scores are selected as the trusted part, and finally, the model is retrained with the labeled and pseudo-labeled samples. Wu <span id="S6.SS2.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib246" title="" class="ltx_ref">246</a>]</cite> proposed a self-paced curriculum learning that follows an “easy to hard” scheme to select more reliable pseudo labels. Zhong <span id="S6.SS2.p2.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib245" title="" class="ltx_ref">245</a>]</cite> adopt an active learning strategy in which high-scored predictions are manually adjusted by experts to obtain refined pseudo labels. Chen <span id="S6.SS2.p2.1.4" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib247" title="" class="ltx_ref">247</a>]</cite> employed teacher-student mutual learning to fully leverage unlabeled samples and iteratively generate higher-quality pseudo-labels.</p>
</div>
<div id="S6.SS2.p3" class="ltx_para">
<p id="S6.SS2.p3.1" class="ltx_p">In addition, some studies<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib249" title="" class="ltx_ref">249</a>, <a href="#bib.bib250" title="" class="ltx_ref">250</a>, <a href="#bib.bib251" title="" class="ltx_ref">251</a>, <a href="#bib.bib252" title="" class="ltx_ref">252</a>, <a href="#bib.bib253" title="" class="ltx_ref">253</a>]</cite> have worked on weakly semi-supervised object detection, in which the unlabeled samples are replaced with weakly annotated samples. Du <span id="S6.SS2.p3.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib251" title="" class="ltx_ref">251</a>, <a href="#bib.bib252" title="" class="ltx_ref">252</a>]</cite> employed a large number of image-level labeled samples to improve SAR vehicle detection performance under scarce box-level labeled samples. Chen <span id="S6.SS2.p3.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib253" title="" class="ltx_ref">253</a>]</cite> adopted a small portion of pixel-level labeled samples and a dominant amount of box-level labeled samples to boost the performance in label-scarce instance segmentation.</p>
</div>
<figure id="S6.F16" class="ltx_figure"><img src="/html/2309.06751/assets/x17.png" id="S6.F16.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="212" height="78" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>The pipeline of pseudo-label generation mechanism in SSOD.</figcaption>
</figure>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS3.4.1.1" class="ltx_text">VI-C</span> </span><span id="S6.SS3.5.2" class="ltx_text ltx_font_italic">Few-shot Object Detection</span>
</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.5" class="ltx_p">Few-shot object detection (FSOD) refers to detecting novel classes with only a limited number (no more than 30) of samples. Generally, FSOD contains a base class dataset with abundant samples <math id="S6.SS3.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{D}_{\text{base}}=\left\{\left(x_{i},y_{i}\right),y_{i}\in C_{base}\right\}_{i=1}^{I_{base}}" display="inline"><semantics id="S6.SS3.p1.1.m1.1a"><mrow id="S6.SS3.p1.1.m1.1.1" xref="S6.SS3.p1.1.m1.1.1.cmml"><msub id="S6.SS3.p1.1.m1.1.1.3" xref="S6.SS3.p1.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.SS3.p1.1.m1.1.1.3.2" xref="S6.SS3.p1.1.m1.1.1.3.2.cmml">𝒟</mi><mtext id="S6.SS3.p1.1.m1.1.1.3.3" xref="S6.SS3.p1.1.m1.1.1.3.3a.cmml">base</mtext></msub><mo id="S6.SS3.p1.1.m1.1.1.2" xref="S6.SS3.p1.1.m1.1.1.2.cmml">=</mo><msubsup id="S6.SS3.p1.1.m1.1.1.1" xref="S6.SS3.p1.1.m1.1.1.1.cmml"><mrow id="S6.SS3.p1.1.m1.1.1.1.1.1.1" xref="S6.SS3.p1.1.m1.1.1.1.1.1.2.cmml"><mo id="S6.SS3.p1.1.m1.1.1.1.1.1.1.2" xref="S6.SS3.p1.1.m1.1.1.1.1.1.2.cmml">{</mo><mrow id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.cmml"><mrow id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.2.2" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.2.3.cmml"><mrow id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mo id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">(</mo><msub id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.2.4" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">,</mo><msub id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml">y</mi><mi id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml">i</mi></msub><mo id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.2.5" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">)</mo></mrow><mo id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.2.2.3" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.2.2.2" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.2.2.2.2" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.2.2.2.2.cmml">y</mi><mi id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.2.2.2.3" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.2.2.2.3.cmml">i</mi></msub></mrow><mo id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.3" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.3.cmml">∈</mo><msub id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.cmml"><mi id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.2" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.2.cmml">C</mi><mrow id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.3" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.3.cmml"><mi id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.3.2" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.3.1" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.3.1.cmml">​</mo><mi id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.3.3" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.3.1a" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.3.1.cmml">​</mo><mi id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.3.4" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.3.1b" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.3.1.cmml">​</mo><mi id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.3.5" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.3.5.cmml">e</mi></mrow></msub></mrow><mo id="S6.SS3.p1.1.m1.1.1.1.1.1.1.3" xref="S6.SS3.p1.1.m1.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S6.SS3.p1.1.m1.1.1.1.1.3" xref="S6.SS3.p1.1.m1.1.1.1.1.3.cmml"><mi id="S6.SS3.p1.1.m1.1.1.1.1.3.2" xref="S6.SS3.p1.1.m1.1.1.1.1.3.2.cmml">i</mi><mo id="S6.SS3.p1.1.m1.1.1.1.1.3.1" xref="S6.SS3.p1.1.m1.1.1.1.1.3.1.cmml">=</mo><mn id="S6.SS3.p1.1.m1.1.1.1.1.3.3" xref="S6.SS3.p1.1.m1.1.1.1.1.3.3.cmml">1</mn></mrow><msub id="S6.SS3.p1.1.m1.1.1.1.3" xref="S6.SS3.p1.1.m1.1.1.1.3.cmml"><mi id="S6.SS3.p1.1.m1.1.1.1.3.2" xref="S6.SS3.p1.1.m1.1.1.1.3.2.cmml">I</mi><mrow id="S6.SS3.p1.1.m1.1.1.1.3.3" xref="S6.SS3.p1.1.m1.1.1.1.3.3.cmml"><mi id="S6.SS3.p1.1.m1.1.1.1.3.3.2" xref="S6.SS3.p1.1.m1.1.1.1.3.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p1.1.m1.1.1.1.3.3.1" xref="S6.SS3.p1.1.m1.1.1.1.3.3.1.cmml">​</mo><mi id="S6.SS3.p1.1.m1.1.1.1.3.3.3" xref="S6.SS3.p1.1.m1.1.1.1.3.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p1.1.m1.1.1.1.3.3.1a" xref="S6.SS3.p1.1.m1.1.1.1.3.3.1.cmml">​</mo><mi id="S6.SS3.p1.1.m1.1.1.1.3.3.4" xref="S6.SS3.p1.1.m1.1.1.1.3.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p1.1.m1.1.1.1.3.3.1b" xref="S6.SS3.p1.1.m1.1.1.1.3.3.1.cmml">​</mo><mi id="S6.SS3.p1.1.m1.1.1.1.3.3.5" xref="S6.SS3.p1.1.m1.1.1.1.3.3.5.cmml">e</mi></mrow></msub></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.1.m1.1b"><apply id="S6.SS3.p1.1.m1.1.1.cmml" xref="S6.SS3.p1.1.m1.1.1"><eq id="S6.SS3.p1.1.m1.1.1.2.cmml" xref="S6.SS3.p1.1.m1.1.1.2"></eq><apply id="S6.SS3.p1.1.m1.1.1.3.cmml" xref="S6.SS3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S6.SS3.p1.1.m1.1.1.3.1.cmml" xref="S6.SS3.p1.1.m1.1.1.3">subscript</csymbol><ci id="S6.SS3.p1.1.m1.1.1.3.2.cmml" xref="S6.SS3.p1.1.m1.1.1.3.2">𝒟</ci><ci id="S6.SS3.p1.1.m1.1.1.3.3a.cmml" xref="S6.SS3.p1.1.m1.1.1.3.3"><mtext mathsize="70%" id="S6.SS3.p1.1.m1.1.1.3.3.cmml" xref="S6.SS3.p1.1.m1.1.1.3.3">base</mtext></ci></apply><apply id="S6.SS3.p1.1.m1.1.1.1.cmml" xref="S6.SS3.p1.1.m1.1.1.1"><csymbol cd="ambiguous" id="S6.SS3.p1.1.m1.1.1.1.2.cmml" xref="S6.SS3.p1.1.m1.1.1.1">superscript</csymbol><apply id="S6.SS3.p1.1.m1.1.1.1.1.cmml" xref="S6.SS3.p1.1.m1.1.1.1"><csymbol cd="ambiguous" id="S6.SS3.p1.1.m1.1.1.1.1.2.cmml" xref="S6.SS3.p1.1.m1.1.1.1">subscript</csymbol><set id="S6.SS3.p1.1.m1.1.1.1.1.1.2.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1"><apply id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1"><in id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.3"></in><list id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.2.2"><interval closure="open" id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.2"><apply id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.2.2.2">𝑦</ci><ci id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.1.1.1.2.2.3">𝑖</ci></apply></interval><apply id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.2.2.2.2">𝑦</ci><ci id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.2.2.2.3">𝑖</ci></apply></list><apply id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.1.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.2.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.2">𝐶</ci><apply id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.3.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.3"><times id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.3.1.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.3.1"></times><ci id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.3.2.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.3.2">𝑏</ci><ci id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.3.3.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.3.3">𝑎</ci><ci id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.3.4.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.3.4">𝑠</ci><ci id="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.3.5.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.1.1.1.4.3.5">𝑒</ci></apply></apply></apply></set><apply id="S6.SS3.p1.1.m1.1.1.1.1.3.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.3"><eq id="S6.SS3.p1.1.m1.1.1.1.1.3.1.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.3.1"></eq><ci id="S6.SS3.p1.1.m1.1.1.1.1.3.2.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.3.2">𝑖</ci><cn type="integer" id="S6.SS3.p1.1.m1.1.1.1.1.3.3.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.3.3">1</cn></apply></apply><apply id="S6.SS3.p1.1.m1.1.1.1.3.cmml" xref="S6.SS3.p1.1.m1.1.1.1.3"><csymbol cd="ambiguous" id="S6.SS3.p1.1.m1.1.1.1.3.1.cmml" xref="S6.SS3.p1.1.m1.1.1.1.3">subscript</csymbol><ci id="S6.SS3.p1.1.m1.1.1.1.3.2.cmml" xref="S6.SS3.p1.1.m1.1.1.1.3.2">𝐼</ci><apply id="S6.SS3.p1.1.m1.1.1.1.3.3.cmml" xref="S6.SS3.p1.1.m1.1.1.1.3.3"><times id="S6.SS3.p1.1.m1.1.1.1.3.3.1.cmml" xref="S6.SS3.p1.1.m1.1.1.1.3.3.1"></times><ci id="S6.SS3.p1.1.m1.1.1.1.3.3.2.cmml" xref="S6.SS3.p1.1.m1.1.1.1.3.3.2">𝑏</ci><ci id="S6.SS3.p1.1.m1.1.1.1.3.3.3.cmml" xref="S6.SS3.p1.1.m1.1.1.1.3.3.3">𝑎</ci><ci id="S6.SS3.p1.1.m1.1.1.1.3.3.4.cmml" xref="S6.SS3.p1.1.m1.1.1.1.3.3.4">𝑠</ci><ci id="S6.SS3.p1.1.m1.1.1.1.3.3.5.cmml" xref="S6.SS3.p1.1.m1.1.1.1.3.3.5">𝑒</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.1.m1.1c">\mathcal{D}_{\text{base}}=\left\{\left(x_{i},y_{i}\right),y_{i}\in C_{base}\right\}_{i=1}^{I_{base}}</annotation></semantics></math> and a novel class dataset with only <math id="S6.SS3.p1.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S6.SS3.p1.2.m2.1a"><mi id="S6.SS3.p1.2.m2.1.1" xref="S6.SS3.p1.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.2.m2.1b"><ci id="S6.SS3.p1.2.m2.1.1.cmml" xref="S6.SS3.p1.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.2.m2.1c">K</annotation></semantics></math>-shot samples <math id="S6.SS3.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{D}_{\text{novel}}=\left\{\left(x_{j},y_{j}\right),y_{j}\in C_{novel}\right\}_{j=1}^{C_{novel}*K}" display="inline"><semantics id="S6.SS3.p1.3.m3.1a"><mrow id="S6.SS3.p1.3.m3.1.1" xref="S6.SS3.p1.3.m3.1.1.cmml"><msub id="S6.SS3.p1.3.m3.1.1.3" xref="S6.SS3.p1.3.m3.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.SS3.p1.3.m3.1.1.3.2" xref="S6.SS3.p1.3.m3.1.1.3.2.cmml">𝒟</mi><mtext id="S6.SS3.p1.3.m3.1.1.3.3" xref="S6.SS3.p1.3.m3.1.1.3.3a.cmml">novel</mtext></msub><mo id="S6.SS3.p1.3.m3.1.1.2" xref="S6.SS3.p1.3.m3.1.1.2.cmml">=</mo><msubsup id="S6.SS3.p1.3.m3.1.1.1" xref="S6.SS3.p1.3.m3.1.1.1.cmml"><mrow id="S6.SS3.p1.3.m3.1.1.1.1.1.1" xref="S6.SS3.p1.3.m3.1.1.1.1.1.2.cmml"><mo id="S6.SS3.p1.3.m3.1.1.1.1.1.1.2" xref="S6.SS3.p1.3.m3.1.1.1.1.1.2.cmml">{</mo><mrow id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.cmml"><mrow id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.2.2" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.2.3.cmml"><mrow id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.2" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.3.cmml"><mo id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.2.3" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.3.cmml">(</mo><msub id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.1" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.2.4" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.3.cmml">,</mo><msub id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.2.2" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml">y</mi><mi id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml">j</mi></msub><mo id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.2.5" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.3.cmml">)</mo></mrow><mo id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.2.2.3" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.2.2.2" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.2.2.2.2" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.2.2.2.2.cmml">y</mi><mi id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.2.2.2.3" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.2.2.2.3.cmml">j</mi></msub></mrow><mo id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.3" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.3.cmml">∈</mo><msub id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.cmml"><mi id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.2" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.2.cmml">C</mi><mrow id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.cmml"><mi id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.2" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.1" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.1.cmml">​</mo><mi id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.3" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.1a" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.1.cmml">​</mo><mi id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.4" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.4.cmml">v</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.1b" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.1.cmml">​</mo><mi id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.5" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.1c" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.1.cmml">​</mo><mi id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.6" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.6.cmml">l</mi></mrow></msub></mrow><mo id="S6.SS3.p1.3.m3.1.1.1.1.1.1.3" xref="S6.SS3.p1.3.m3.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S6.SS3.p1.3.m3.1.1.1.1.3" xref="S6.SS3.p1.3.m3.1.1.1.1.3.cmml"><mi id="S6.SS3.p1.3.m3.1.1.1.1.3.2" xref="S6.SS3.p1.3.m3.1.1.1.1.3.2.cmml">j</mi><mo id="S6.SS3.p1.3.m3.1.1.1.1.3.1" xref="S6.SS3.p1.3.m3.1.1.1.1.3.1.cmml">=</mo><mn id="S6.SS3.p1.3.m3.1.1.1.1.3.3" xref="S6.SS3.p1.3.m3.1.1.1.1.3.3.cmml">1</mn></mrow><mrow id="S6.SS3.p1.3.m3.1.1.1.3" xref="S6.SS3.p1.3.m3.1.1.1.3.cmml"><msub id="S6.SS3.p1.3.m3.1.1.1.3.2" xref="S6.SS3.p1.3.m3.1.1.1.3.2.cmml"><mi id="S6.SS3.p1.3.m3.1.1.1.3.2.2" xref="S6.SS3.p1.3.m3.1.1.1.3.2.2.cmml">C</mi><mrow id="S6.SS3.p1.3.m3.1.1.1.3.2.3" xref="S6.SS3.p1.3.m3.1.1.1.3.2.3.cmml"><mi id="S6.SS3.p1.3.m3.1.1.1.3.2.3.2" xref="S6.SS3.p1.3.m3.1.1.1.3.2.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p1.3.m3.1.1.1.3.2.3.1" xref="S6.SS3.p1.3.m3.1.1.1.3.2.3.1.cmml">​</mo><mi id="S6.SS3.p1.3.m3.1.1.1.3.2.3.3" xref="S6.SS3.p1.3.m3.1.1.1.3.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p1.3.m3.1.1.1.3.2.3.1a" xref="S6.SS3.p1.3.m3.1.1.1.3.2.3.1.cmml">​</mo><mi id="S6.SS3.p1.3.m3.1.1.1.3.2.3.4" xref="S6.SS3.p1.3.m3.1.1.1.3.2.3.4.cmml">v</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p1.3.m3.1.1.1.3.2.3.1b" xref="S6.SS3.p1.3.m3.1.1.1.3.2.3.1.cmml">​</mo><mi id="S6.SS3.p1.3.m3.1.1.1.3.2.3.5" xref="S6.SS3.p1.3.m3.1.1.1.3.2.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p1.3.m3.1.1.1.3.2.3.1c" xref="S6.SS3.p1.3.m3.1.1.1.3.2.3.1.cmml">​</mo><mi id="S6.SS3.p1.3.m3.1.1.1.3.2.3.6" xref="S6.SS3.p1.3.m3.1.1.1.3.2.3.6.cmml">l</mi></mrow></msub><mo lspace="0.222em" rspace="0.222em" id="S6.SS3.p1.3.m3.1.1.1.3.1" xref="S6.SS3.p1.3.m3.1.1.1.3.1.cmml">∗</mo><mi id="S6.SS3.p1.3.m3.1.1.1.3.3" xref="S6.SS3.p1.3.m3.1.1.1.3.3.cmml">K</mi></mrow></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.3.m3.1b"><apply id="S6.SS3.p1.3.m3.1.1.cmml" xref="S6.SS3.p1.3.m3.1.1"><eq id="S6.SS3.p1.3.m3.1.1.2.cmml" xref="S6.SS3.p1.3.m3.1.1.2"></eq><apply id="S6.SS3.p1.3.m3.1.1.3.cmml" xref="S6.SS3.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S6.SS3.p1.3.m3.1.1.3.1.cmml" xref="S6.SS3.p1.3.m3.1.1.3">subscript</csymbol><ci id="S6.SS3.p1.3.m3.1.1.3.2.cmml" xref="S6.SS3.p1.3.m3.1.1.3.2">𝒟</ci><ci id="S6.SS3.p1.3.m3.1.1.3.3a.cmml" xref="S6.SS3.p1.3.m3.1.1.3.3"><mtext mathsize="70%" id="S6.SS3.p1.3.m3.1.1.3.3.cmml" xref="S6.SS3.p1.3.m3.1.1.3.3">novel</mtext></ci></apply><apply id="S6.SS3.p1.3.m3.1.1.1.cmml" xref="S6.SS3.p1.3.m3.1.1.1"><csymbol cd="ambiguous" id="S6.SS3.p1.3.m3.1.1.1.2.cmml" xref="S6.SS3.p1.3.m3.1.1.1">superscript</csymbol><apply id="S6.SS3.p1.3.m3.1.1.1.1.cmml" xref="S6.SS3.p1.3.m3.1.1.1"><csymbol cd="ambiguous" id="S6.SS3.p1.3.m3.1.1.1.1.2.cmml" xref="S6.SS3.p1.3.m3.1.1.1">subscript</csymbol><set id="S6.SS3.p1.3.m3.1.1.1.1.1.2.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1"><apply id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1"><in id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.3.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.3"></in><list id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.2.3.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.2.2"><interval closure="open" id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.2"><apply id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑗</ci></apply><apply id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.2.2.2">𝑦</ci><ci id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.1.1.1.2.2.3">𝑗</ci></apply></interval><apply id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.2.2.2.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.2.2.2.2">𝑦</ci><ci id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.2.2.2.3">𝑗</ci></apply></list><apply id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.1.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.2.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.2">𝐶</ci><apply id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3"><times id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.1.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.1"></times><ci id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.2.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.2">𝑛</ci><ci id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.3.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.3">𝑜</ci><ci id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.4.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.4">𝑣</ci><ci id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.5.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.5">𝑒</ci><ci id="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.6.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.1.1.1.4.3.6">𝑙</ci></apply></apply></apply></set><apply id="S6.SS3.p1.3.m3.1.1.1.1.3.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.3"><eq id="S6.SS3.p1.3.m3.1.1.1.1.3.1.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.3.1"></eq><ci id="S6.SS3.p1.3.m3.1.1.1.1.3.2.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.3.2">𝑗</ci><cn type="integer" id="S6.SS3.p1.3.m3.1.1.1.1.3.3.cmml" xref="S6.SS3.p1.3.m3.1.1.1.1.3.3">1</cn></apply></apply><apply id="S6.SS3.p1.3.m3.1.1.1.3.cmml" xref="S6.SS3.p1.3.m3.1.1.1.3"><times id="S6.SS3.p1.3.m3.1.1.1.3.1.cmml" xref="S6.SS3.p1.3.m3.1.1.1.3.1"></times><apply id="S6.SS3.p1.3.m3.1.1.1.3.2.cmml" xref="S6.SS3.p1.3.m3.1.1.1.3.2"><csymbol cd="ambiguous" id="S6.SS3.p1.3.m3.1.1.1.3.2.1.cmml" xref="S6.SS3.p1.3.m3.1.1.1.3.2">subscript</csymbol><ci id="S6.SS3.p1.3.m3.1.1.1.3.2.2.cmml" xref="S6.SS3.p1.3.m3.1.1.1.3.2.2">𝐶</ci><apply id="S6.SS3.p1.3.m3.1.1.1.3.2.3.cmml" xref="S6.SS3.p1.3.m3.1.1.1.3.2.3"><times id="S6.SS3.p1.3.m3.1.1.1.3.2.3.1.cmml" xref="S6.SS3.p1.3.m3.1.1.1.3.2.3.1"></times><ci id="S6.SS3.p1.3.m3.1.1.1.3.2.3.2.cmml" xref="S6.SS3.p1.3.m3.1.1.1.3.2.3.2">𝑛</ci><ci id="S6.SS3.p1.3.m3.1.1.1.3.2.3.3.cmml" xref="S6.SS3.p1.3.m3.1.1.1.3.2.3.3">𝑜</ci><ci id="S6.SS3.p1.3.m3.1.1.1.3.2.3.4.cmml" xref="S6.SS3.p1.3.m3.1.1.1.3.2.3.4">𝑣</ci><ci id="S6.SS3.p1.3.m3.1.1.1.3.2.3.5.cmml" xref="S6.SS3.p1.3.m3.1.1.1.3.2.3.5">𝑒</ci><ci id="S6.SS3.p1.3.m3.1.1.1.3.2.3.6.cmml" xref="S6.SS3.p1.3.m3.1.1.1.3.2.3.6">𝑙</ci></apply></apply><ci id="S6.SS3.p1.3.m3.1.1.1.3.3.cmml" xref="S6.SS3.p1.3.m3.1.1.1.3.3">𝐾</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.3.m3.1c">\mathcal{D}_{\text{novel}}=\left\{\left(x_{j},y_{j}\right),y_{j}\in C_{novel}\right\}_{j=1}^{C_{novel}*K}</annotation></semantics></math>. Note that <math id="S6.SS3.p1.4.m4.1" class="ltx_Math" alttext="C_{base}" display="inline"><semantics id="S6.SS3.p1.4.m4.1a"><msub id="S6.SS3.p1.4.m4.1.1" xref="S6.SS3.p1.4.m4.1.1.cmml"><mi id="S6.SS3.p1.4.m4.1.1.2" xref="S6.SS3.p1.4.m4.1.1.2.cmml">C</mi><mrow id="S6.SS3.p1.4.m4.1.1.3" xref="S6.SS3.p1.4.m4.1.1.3.cmml"><mi id="S6.SS3.p1.4.m4.1.1.3.2" xref="S6.SS3.p1.4.m4.1.1.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p1.4.m4.1.1.3.1" xref="S6.SS3.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S6.SS3.p1.4.m4.1.1.3.3" xref="S6.SS3.p1.4.m4.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p1.4.m4.1.1.3.1a" xref="S6.SS3.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S6.SS3.p1.4.m4.1.1.3.4" xref="S6.SS3.p1.4.m4.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p1.4.m4.1.1.3.1b" xref="S6.SS3.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S6.SS3.p1.4.m4.1.1.3.5" xref="S6.SS3.p1.4.m4.1.1.3.5.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.4.m4.1b"><apply id="S6.SS3.p1.4.m4.1.1.cmml" xref="S6.SS3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S6.SS3.p1.4.m4.1.1.1.cmml" xref="S6.SS3.p1.4.m4.1.1">subscript</csymbol><ci id="S6.SS3.p1.4.m4.1.1.2.cmml" xref="S6.SS3.p1.4.m4.1.1.2">𝐶</ci><apply id="S6.SS3.p1.4.m4.1.1.3.cmml" xref="S6.SS3.p1.4.m4.1.1.3"><times id="S6.SS3.p1.4.m4.1.1.3.1.cmml" xref="S6.SS3.p1.4.m4.1.1.3.1"></times><ci id="S6.SS3.p1.4.m4.1.1.3.2.cmml" xref="S6.SS3.p1.4.m4.1.1.3.2">𝑏</ci><ci id="S6.SS3.p1.4.m4.1.1.3.3.cmml" xref="S6.SS3.p1.4.m4.1.1.3.3">𝑎</ci><ci id="S6.SS3.p1.4.m4.1.1.3.4.cmml" xref="S6.SS3.p1.4.m4.1.1.3.4">𝑠</ci><ci id="S6.SS3.p1.4.m4.1.1.3.5.cmml" xref="S6.SS3.p1.4.m4.1.1.3.5">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.4.m4.1c">C_{base}</annotation></semantics></math> and <math id="S6.SS3.p1.5.m5.1" class="ltx_Math" alttext="C_{novel}" display="inline"><semantics id="S6.SS3.p1.5.m5.1a"><msub id="S6.SS3.p1.5.m5.1.1" xref="S6.SS3.p1.5.m5.1.1.cmml"><mi id="S6.SS3.p1.5.m5.1.1.2" xref="S6.SS3.p1.5.m5.1.1.2.cmml">C</mi><mrow id="S6.SS3.p1.5.m5.1.1.3" xref="S6.SS3.p1.5.m5.1.1.3.cmml"><mi id="S6.SS3.p1.5.m5.1.1.3.2" xref="S6.SS3.p1.5.m5.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p1.5.m5.1.1.3.1" xref="S6.SS3.p1.5.m5.1.1.3.1.cmml">​</mo><mi id="S6.SS3.p1.5.m5.1.1.3.3" xref="S6.SS3.p1.5.m5.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p1.5.m5.1.1.3.1a" xref="S6.SS3.p1.5.m5.1.1.3.1.cmml">​</mo><mi id="S6.SS3.p1.5.m5.1.1.3.4" xref="S6.SS3.p1.5.m5.1.1.3.4.cmml">v</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p1.5.m5.1.1.3.1b" xref="S6.SS3.p1.5.m5.1.1.3.1.cmml">​</mo><mi id="S6.SS3.p1.5.m5.1.1.3.5" xref="S6.SS3.p1.5.m5.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.SS3.p1.5.m5.1.1.3.1c" xref="S6.SS3.p1.5.m5.1.1.3.1.cmml">​</mo><mi id="S6.SS3.p1.5.m5.1.1.3.6" xref="S6.SS3.p1.5.m5.1.1.3.6.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.5.m5.1b"><apply id="S6.SS3.p1.5.m5.1.1.cmml" xref="S6.SS3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S6.SS3.p1.5.m5.1.1.1.cmml" xref="S6.SS3.p1.5.m5.1.1">subscript</csymbol><ci id="S6.SS3.p1.5.m5.1.1.2.cmml" xref="S6.SS3.p1.5.m5.1.1.2">𝐶</ci><apply id="S6.SS3.p1.5.m5.1.1.3.cmml" xref="S6.SS3.p1.5.m5.1.1.3"><times id="S6.SS3.p1.5.m5.1.1.3.1.cmml" xref="S6.SS3.p1.5.m5.1.1.3.1"></times><ci id="S6.SS3.p1.5.m5.1.1.3.2.cmml" xref="S6.SS3.p1.5.m5.1.1.3.2">𝑛</ci><ci id="S6.SS3.p1.5.m5.1.1.3.3.cmml" xref="S6.SS3.p1.5.m5.1.1.3.3">𝑜</ci><ci id="S6.SS3.p1.5.m5.1.1.3.4.cmml" xref="S6.SS3.p1.5.m5.1.1.3.4">𝑣</ci><ci id="S6.SS3.p1.5.m5.1.1.3.5.cmml" xref="S6.SS3.p1.5.m5.1.1.3.5">𝑒</ci><ci id="S6.SS3.p1.5.m5.1.1.3.6.cmml" xref="S6.SS3.p1.5.m5.1.1.3.6">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.5.m5.1c">C_{novel}</annotation></semantics></math> are disjointed. As depicted in Fig. <a href="#S6.F17" title="Figure 17 ‣ VI-C Few-shot Object Detection ‣ VI Object Detection with Limited Supervision ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a>, a typical FSOD paradigm consists of a two-stage training pipeline where the base training stage establishes prior knowledge with abundant base class samples, and the few-shot fine-tuning stage leverages the prior knowledge to facilitate the learning of few-shot novel concepts. The research on remote sensing FSOD mainly focuses on meta-learning methods<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib254" title="" class="ltx_ref">254</a>, <a href="#bib.bib255" title="" class="ltx_ref">255</a>, <a href="#bib.bib256" title="" class="ltx_ref">256</a>, <a href="#bib.bib257" title="" class="ltx_ref">257</a>, <a href="#bib.bib258" title="" class="ltx_ref">258</a>, <a href="#bib.bib259" title="" class="ltx_ref">259</a>]</cite> and transfer-learning methods<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib260" title="" class="ltx_ref">260</a>, <a href="#bib.bib261" title="" class="ltx_ref">261</a>, <a href="#bib.bib262" title="" class="ltx_ref">262</a>, <a href="#bib.bib263" title="" class="ltx_ref">263</a>, <a href="#bib.bib264" title="" class="ltx_ref">264</a>, <a href="#bib.bib265" title="" class="ltx_ref">265</a>, <a href="#bib.bib266" title="" class="ltx_ref">266</a>, <a href="#bib.bib267" title="" class="ltx_ref">267</a>, <a href="#bib.bib268" title="" class="ltx_ref">268</a>, <a href="#bib.bib269" title="" class="ltx_ref">269</a>]</cite>.</p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p id="S6.SS3.p2.1" class="ltx_p">The meta-learning based methods acquire task-level knowledge by simulating a series of few-shot learning tasks and generalize this knowledge to tackle few-shot learning of novel classes. Li <span id="S6.SS3.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib255" title="" class="ltx_ref">255</a>]</cite> first employed meta-learning for remote sensing FSOD and achieved satisfactory detection performance with only 1 to 10 labeled samples. Later, a series of meta-learning based few-shot detectors have been developed in the remote sensing community<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib254" title="" class="ltx_ref">254</a>, <a href="#bib.bib255" title="" class="ltx_ref">255</a>, <a href="#bib.bib256" title="" class="ltx_ref">256</a>, <a href="#bib.bib257" title="" class="ltx_ref">257</a>, <a href="#bib.bib258" title="" class="ltx_ref">258</a>, <a href="#bib.bib259" title="" class="ltx_ref">259</a>]</cite>. For example, Cheng <span id="S6.SS3.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib254" title="" class="ltx_ref">254</a>]</cite> proposed a Prototype-CNN to generate better foreground proposals and class-aware RoI features for remote sensing FSOD by learning class-specific prototypes. Wang <span id="S6.SS3.p2.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib258" title="" class="ltx_ref">258</a>]</cite> presented a meta-metric training paradigm to enable the few-shot learner with flexible scalability for fast adaptation to the few-shot novel tasks.</p>
</div>
<div id="S6.SS3.p3" class="ltx_para">
<p id="S6.SS3.p3.1" class="ltx_p">Transfer-learning based methods aim at fine-tuning the common knowledge learned from the abundant annotated data to the few-shot novel data and typically consist of a base training stage and a few-shot fine-tuning stage. Huang <span id="S6.SS3.p3.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib266" title="" class="ltx_ref">266</a>]</cite> proposed a balanced fine-tuning strategy to alleviate the number imbalance problem between novel class samples and base class samples. Zhou <span id="S6.SS3.p3.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib265" title="" class="ltx_ref">265</a>]</cite> introduced proposal-level contrast learning in the fine-tuning phase to learn more robust feature representations in few-shot scenarios. Compared with the meta-learning based methods, the transfer-learning based method has a simpler and memory-efficient training paradigm.</p>
</div>
<figure id="S6.F17" class="ltx_figure"><img src="/html/2309.06751/assets/x18.png" id="S6.F17.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="212" height="80" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>The two-stage training pipeline of FSOD.</figcaption>
</figure>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Datasets and Evaluation Metrics</span>
</h2>

<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S7.SS1.4.1.1" class="ltx_text">VII-A</span> </span><span id="S7.SS1.5.2" class="ltx_text ltx_font_italic">Datasets Introduction and Selection</span>
</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">Datasets have played an indispensable role throughout the development of object detection in RSIs. On the one hand, datasets serve as a common ground for the performance evaluation and comparison of detectors. On the other hand, datasets push researchers to address increasingly challenging problems in the RSOD field. In the past decade, several datasets with different attributes have been released to facilitate the development of RSOD, as shown in Table <a href="#S7.T3" title="TABLE III ‣ VII-A Datasets Introduction and Selection ‣ VII Datasets and Evaluation Metrics ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. In this section, we mainly introduce 10 widely used datasets with specific characteristics.</p>
</div>
<figure id="S7.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Comparisons of widely used datasets in the field of RSOD. HBB and OBB refer to <span id="S7.T3.26.1" class="ltx_text ltx_font_italic">horizontal bounding box</span> and <span id="S7.T3.27.2" class="ltx_text ltx_font_italic">oriented bounding box</span>, respectively. <sup id="S7.T3.28.3" class="ltx_sup"><span id="S7.T3.28.3.1" class="ltx_text ltx_font_italic">∗</span></sup> stands for <span id="S7.T3.29.4" class="ltx_text ltx_font_italic">the average image width</span>.</figcaption>
<div id="S7.T3.15" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:262.3pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-141.1pt,85.2pt) scale(0.605726036779984,0.605726036779984) ;">
<table id="S7.T3.15.13" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S7.T3.15.13.14.1" class="ltx_tr">
<th id="S7.T3.15.13.14.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.14.1.1.1" class="ltx_text" style="font-size:90%;">Dataset</span></th>
<th id="S7.T3.15.13.14.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.14.1.2.1" class="ltx_text" style="font-size:90%;">Source</span></th>
<th id="S7.T3.15.13.14.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.14.1.3.1" class="ltx_text" style="font-size:90%;">Annotation</span></th>
<th id="S7.T3.15.13.14.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.14.1.4.1" class="ltx_text" style="font-size:90%;">Categories</span></th>
<th id="S7.T3.15.13.14.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.14.1.5.1" class="ltx_text" style="font-size:90%;">Instances</span></th>
<th id="S7.T3.15.13.14.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.14.1.6.1" class="ltx_text" style="font-size:90%;">Images</span></th>
<th id="S7.T3.15.13.14.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.14.1.7.1" class="ltx_text" style="font-size:90%;">Image width</span></th>
<th id="S7.T3.15.13.14.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.14.1.8.1" class="ltx_text" style="font-size:90%;">Resolution</span></th>
<th id="S7.T3.15.13.14.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.14.1.9.1" class="ltx_text" style="font-size:90%;">Year</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S7.T3.15.13.15.1" class="ltx_tr">
<td id="S7.T3.15.13.15.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S7.T3.15.13.15.1.1.1" class="ltx_text" style="font-size:90%;">TAS </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T3.15.13.15.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib270" title="" class="ltx_ref">270</a><span id="S7.T3.15.13.15.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S7.T3.15.13.15.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.15.1.2.1" class="ltx_text" style="font-size:90%;">Google Earth</span></td>
<td id="S7.T3.15.13.15.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.15.1.3.1" class="ltx_text" style="font-size:90%;">HBB</span></td>
<td id="S7.T3.15.13.15.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.15.1.4.1" class="ltx_text" style="font-size:90%;">1</span></td>
<td id="S7.T3.15.13.15.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.15.1.5.1" class="ltx_text" style="font-size:90%;">1,319</span></td>
<td id="S7.T3.15.13.15.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.15.1.6.1" class="ltx_text" style="font-size:90%;">30</span></td>
<td id="S7.T3.15.13.15.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.15.1.7.1" class="ltx_text" style="font-size:90%;">792</span></td>
<td id="S7.T3.15.13.15.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.15.1.8.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S7.T3.15.13.15.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.15.1.9.1" class="ltx_text" style="font-size:90%;">2008</span></td>
</tr>
<tr id="S7.T3.3.1.1" class="ltx_tr">
<td id="S7.T3.3.1.1.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S7.T3.3.1.1.2.1" class="ltx_text" style="font-size:90%;">SZTAKI-INRIA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T3.3.1.1.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib271" title="" class="ltx_ref">271</a><span id="S7.T3.3.1.1.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S7.T3.3.1.1.3" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.3.1.1.3.1" class="ltx_text" style="font-size:90%;">Quick Bird, IKONOS and Google Earth</span></td>
<td id="S7.T3.3.1.1.4" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.3.1.1.4.1" class="ltx_text" style="font-size:90%;">OBB</span></td>
<td id="S7.T3.3.1.1.5" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.3.1.1.5.1" class="ltx_text" style="font-size:90%;">1</span></td>
<td id="S7.T3.3.1.1.6" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.3.1.1.6.1" class="ltx_text" style="font-size:90%;">665</span></td>
<td id="S7.T3.3.1.1.7" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.3.1.1.7.1" class="ltx_text" style="font-size:90%;">9</span></td>
<td id="S7.T3.3.1.1.1" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<math id="S7.T3.3.1.1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S7.T3.3.1.1.1.m1.1a"><mo mathsize="90%" id="S7.T3.3.1.1.1.m1.1.1" xref="S7.T3.3.1.1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S7.T3.3.1.1.1.m1.1b"><csymbol cd="latexml" id="S7.T3.3.1.1.1.m1.1.1.cmml" xref="S7.T3.3.1.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.T3.3.1.1.1.m1.1c">\sim</annotation></semantics></math><span id="S7.T3.3.1.1.1.1" class="ltx_text" style="font-size:90%;">800</span>
</td>
<td id="S7.T3.3.1.1.8" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.3.1.1.8.1" class="ltx_text" style="font-size:90%;">0.5-1m</span></td>
<td id="S7.T3.3.1.1.9" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.3.1.1.9.1" class="ltx_text" style="font-size:90%;">2012</span></td>
</tr>
<tr id="S7.T3.4.2.2" class="ltx_tr">
<td id="S7.T3.4.2.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S7.T3.4.2.2.2.1" class="ltx_text" style="font-size:90%;">NWPU VHR-10 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T3.4.2.2.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S7.T3.4.2.2.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S7.T3.4.2.2.3" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.4.2.2.3.1" class="ltx_text" style="font-size:90%;">Google Earth</span></td>
<td id="S7.T3.4.2.2.4" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.4.2.2.4.1" class="ltx_text" style="font-size:90%;">HBB</span></td>
<td id="S7.T3.4.2.2.5" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.4.2.2.5.1" class="ltx_text" style="font-size:90%;">10</span></td>
<td id="S7.T3.4.2.2.6" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.4.2.2.6.1" class="ltx_text" style="font-size:90%;">3,651</span></td>
<td id="S7.T3.4.2.2.7" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.4.2.2.7.1" class="ltx_text" style="font-size:90%;">800</span></td>
<td id="S7.T3.4.2.2.1" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<math id="S7.T3.4.2.2.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S7.T3.4.2.2.1.m1.1a"><mo mathsize="90%" id="S7.T3.4.2.2.1.m1.1.1" xref="S7.T3.4.2.2.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S7.T3.4.2.2.1.m1.1b"><csymbol cd="latexml" id="S7.T3.4.2.2.1.m1.1.1.cmml" xref="S7.T3.4.2.2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.T3.4.2.2.1.m1.1c">\sim</annotation></semantics></math><span id="S7.T3.4.2.2.1.1" class="ltx_text" style="font-size:90%;">1,000</span>
</td>
<td id="S7.T3.4.2.2.8" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.4.2.2.8.1" class="ltx_text" style="font-size:90%;">0.3-2m</span></td>
<td id="S7.T3.4.2.2.9" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.4.2.2.9.1" class="ltx_text" style="font-size:90%;">2014</span></td>
</tr>
<tr id="S7.T3.15.13.16.2" class="ltx_tr">
<td id="S7.T3.15.13.16.2.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S7.T3.15.13.16.2.1.1" class="ltx_text" style="font-size:90%;">VEDAI </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T3.15.13.16.2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib272" title="" class="ltx_ref">272</a><span id="S7.T3.15.13.16.2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S7.T3.15.13.16.2.2" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.16.2.2.1" class="ltx_text" style="font-size:90%;">Utah AGRC</span></td>
<td id="S7.T3.15.13.16.2.3" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.16.2.3.1" class="ltx_text" style="font-size:90%;">OBB</span></td>
<td id="S7.T3.15.13.16.2.4" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.16.2.4.1" class="ltx_text" style="font-size:90%;">9</span></td>
<td id="S7.T3.15.13.16.2.5" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.16.2.5.1" class="ltx_text" style="font-size:90%;">2,950</span></td>
<td id="S7.T3.15.13.16.2.6" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.16.2.6.1" class="ltx_text" style="font-size:90%;">1,268</span></td>
<td id="S7.T3.15.13.16.2.7" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.16.2.7.1" class="ltx_text" style="font-size:90%;">1,024</span></td>
<td id="S7.T3.15.13.16.2.8" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.16.2.8.1" class="ltx_text" style="font-size:90%;">0.125m</span></td>
<td id="S7.T3.15.13.16.2.9" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.16.2.9.1" class="ltx_text" style="font-size:90%;">2015</span></td>
</tr>
<tr id="S7.T3.15.13.17.3" class="ltx_tr">
<td id="S7.T3.15.13.17.3.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S7.T3.15.13.17.3.1.1" class="ltx_text" style="font-size:90%;">DLR 3k </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T3.15.13.17.3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib273" title="" class="ltx_ref">273</a><span id="S7.T3.15.13.17.3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S7.T3.15.13.17.3.2" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.17.3.2.1" class="ltx_text" style="font-size:90%;">DLR 3K camera system</span></td>
<td id="S7.T3.15.13.17.3.3" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.17.3.3.1" class="ltx_text" style="font-size:90%;">OBB</span></td>
<td id="S7.T3.15.13.17.3.4" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.17.3.4.1" class="ltx_text" style="font-size:90%;">8</span></td>
<td id="S7.T3.15.13.17.3.5" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.17.3.5.1" class="ltx_text" style="font-size:90%;">14,235</span></td>
<td id="S7.T3.15.13.17.3.6" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.17.3.6.1" class="ltx_text" style="font-size:90%;">20</span></td>
<td id="S7.T3.15.13.17.3.7" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.17.3.7.1" class="ltx_text" style="font-size:90%;">5,616</span></td>
<td id="S7.T3.15.13.17.3.8" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.17.3.8.1" class="ltx_text" style="font-size:90%;">0.13m</span></td>
<td id="S7.T3.15.13.17.3.9" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.17.3.9.1" class="ltx_text" style="font-size:90%;">2015</span></td>
</tr>
<tr id="S7.T3.5.3.3" class="ltx_tr">
<td id="S7.T3.5.3.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S7.T3.5.3.3.2.1" class="ltx_text" style="font-size:90%;">UCAS-AOD </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T3.5.3.3.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib274" title="" class="ltx_ref">274</a><span id="S7.T3.5.3.3.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S7.T3.5.3.3.3" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.5.3.3.3.1" class="ltx_text" style="font-size:90%;">Google Earth</span></td>
<td id="S7.T3.5.3.3.4" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.5.3.3.4.1" class="ltx_text" style="font-size:90%;">OBB</span></td>
<td id="S7.T3.5.3.3.5" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.5.3.3.5.1" class="ltx_text" style="font-size:90%;">2</span></td>
<td id="S7.T3.5.3.3.6" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.5.3.3.6.1" class="ltx_text" style="font-size:90%;">6,029</span></td>
<td id="S7.T3.5.3.3.7" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.5.3.3.7.1" class="ltx_text" style="font-size:90%;">910</span></td>
<td id="S7.T3.5.3.3.1" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<math id="S7.T3.5.3.3.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S7.T3.5.3.3.1.m1.1a"><mo mathsize="90%" id="S7.T3.5.3.3.1.m1.1.1" xref="S7.T3.5.3.3.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S7.T3.5.3.3.1.m1.1b"><csymbol cd="latexml" id="S7.T3.5.3.3.1.m1.1.1.cmml" xref="S7.T3.5.3.3.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.T3.5.3.3.1.m1.1c">\sim</annotation></semantics></math><span id="S7.T3.5.3.3.1.1" class="ltx_text" style="font-size:90%;">1,000</span>
</td>
<td id="S7.T3.5.3.3.8" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.5.3.3.8.1" class="ltx_text" style="font-size:90%;">0.3-2m</span></td>
<td id="S7.T3.5.3.3.9" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.5.3.3.9.1" class="ltx_text" style="font-size:90%;">2015</span></td>
</tr>
<tr id="S7.T3.6.4.4" class="ltx_tr">
<td id="S7.T3.6.4.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S7.T3.6.4.4.2.1" class="ltx_text" style="font-size:90%;">COWC </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T3.6.4.4.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib275" title="" class="ltx_ref">275</a><span id="S7.T3.6.4.4.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S7.T3.6.4.4.3" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.6.4.4.3.1" class="ltx_text" style="font-size:90%;">Multiple Sources</span></td>
<td id="S7.T3.6.4.4.4" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.6.4.4.4.1" class="ltx_text" style="font-size:90%;">Point</span></td>
<td id="S7.T3.6.4.4.5" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.6.4.4.5.1" class="ltx_text" style="font-size:90%;">1</span></td>
<td id="S7.T3.6.4.4.6" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.6.4.4.6.1" class="ltx_text" style="font-size:90%;">32,716</span></td>
<td id="S7.T3.6.4.4.7" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.6.4.4.7.1" class="ltx_text" style="font-size:90%;">53</span></td>
<td id="S7.T3.6.4.4.1" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S7.T3.6.4.4.1.1" class="ltx_text" style="font-size:90%;">2,000</span><math id="S7.T3.6.4.4.1.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S7.T3.6.4.4.1.m1.1a"><mo mathsize="90%" id="S7.T3.6.4.4.1.m1.1.1" xref="S7.T3.6.4.4.1.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S7.T3.6.4.4.1.m1.1b"><minus id="S7.T3.6.4.4.1.m1.1.1.cmml" xref="S7.T3.6.4.4.1.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S7.T3.6.4.4.1.m1.1c">-</annotation></semantics></math><span id="S7.T3.6.4.4.1.2" class="ltx_text" style="font-size:90%;">19,000</span>
</td>
<td id="S7.T3.6.4.4.8" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.6.4.4.8.1" class="ltx_text" style="font-size:90%;">0.15m</span></td>
<td id="S7.T3.6.4.4.9" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.6.4.4.9.1" class="ltx_text" style="font-size:90%;">2016</span></td>
</tr>
<tr id="S7.T3.7.5.5" class="ltx_tr">
<td id="S7.T3.7.5.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S7.T3.7.5.5.2.1" class="ltx_text" style="font-size:90%;">HRSC </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T3.7.5.5.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib276" title="" class="ltx_ref">276</a><span id="S7.T3.7.5.5.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S7.T3.7.5.5.3" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.7.5.5.3.1" class="ltx_text" style="font-size:90%;">Google Earth</span></td>
<td id="S7.T3.7.5.5.4" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.7.5.5.4.1" class="ltx_text" style="font-size:90%;">OBB</span></td>
<td id="S7.T3.7.5.5.5" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.7.5.5.5.1" class="ltx_text" style="font-size:90%;">26</span></td>
<td id="S7.T3.7.5.5.6" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.7.5.5.6.1" class="ltx_text" style="font-size:90%;">2,976</span></td>
<td id="S7.T3.7.5.5.7" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.7.5.5.7.1" class="ltx_text" style="font-size:90%;">1,061</span></td>
<td id="S7.T3.7.5.5.1" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<math id="S7.T3.7.5.5.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S7.T3.7.5.5.1.m1.1a"><mo mathsize="90%" id="S7.T3.7.5.5.1.m1.1.1" xref="S7.T3.7.5.5.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S7.T3.7.5.5.1.m1.1b"><csymbol cd="latexml" id="S7.T3.7.5.5.1.m1.1.1.cmml" xref="S7.T3.7.5.5.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.T3.7.5.5.1.m1.1c">\sim</annotation></semantics></math><span id="S7.T3.7.5.5.1.1" class="ltx_text" style="font-size:90%;">1,100</span>
</td>
<td id="S7.T3.7.5.5.8" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.7.5.5.8.1" class="ltx_text" style="font-size:90%;">0.4-2m</span></td>
<td id="S7.T3.7.5.5.9" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.7.5.5.9.1" class="ltx_text" style="font-size:90%;">2016</span></td>
</tr>
<tr id="S7.T3.8.6.6" class="ltx_tr">
<td id="S7.T3.8.6.6.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S7.T3.8.6.6.2.1" class="ltx_text" style="font-size:90%;">RSOD </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T3.8.6.6.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib43" title="" class="ltx_ref">43</a><span id="S7.T3.8.6.6.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S7.T3.8.6.6.3" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.8.6.6.3.1" class="ltx_text" style="font-size:90%;">Google Earth and Tianditu</span></td>
<td id="S7.T3.8.6.6.4" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.8.6.6.4.1" class="ltx_text" style="font-size:90%;">HBB</span></td>
<td id="S7.T3.8.6.6.5" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.8.6.6.5.1" class="ltx_text" style="font-size:90%;">4</span></td>
<td id="S7.T3.8.6.6.6" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.8.6.6.6.1" class="ltx_text" style="font-size:90%;">6,950</span></td>
<td id="S7.T3.8.6.6.7" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.8.6.6.7.1" class="ltx_text" style="font-size:90%;">976</span></td>
<td id="S7.T3.8.6.6.1" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<math id="S7.T3.8.6.6.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S7.T3.8.6.6.1.m1.1a"><mo mathsize="90%" id="S7.T3.8.6.6.1.m1.1.1" xref="S7.T3.8.6.6.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S7.T3.8.6.6.1.m1.1b"><csymbol cd="latexml" id="S7.T3.8.6.6.1.m1.1.1.cmml" xref="S7.T3.8.6.6.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.T3.8.6.6.1.m1.1c">\sim</annotation></semantics></math><span id="S7.T3.8.6.6.1.1" class="ltx_text" style="font-size:90%;">1,000</span>
</td>
<td id="S7.T3.8.6.6.8" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.8.6.6.8.1" class="ltx_text" style="font-size:90%;">0.3-3m</span></td>
<td id="S7.T3.8.6.6.9" class="ltx_td ltx_align_left" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.8.6.6.9.1" class="ltx_text" style="font-size:90%;">2017</span></td>
</tr>
<tr id="S7.T3.15.13.18.4" class="ltx_tr">
<td id="S7.T3.15.13.18.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S7.T3.15.13.18.4.1.1" class="ltx_text" style="font-size:90%;">SSDD </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T3.15.13.18.4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib277" title="" class="ltx_ref">277</a><span id="S7.T3.15.13.18.4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S7.T3.15.13.18.4.2" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.18.4.2.1" class="ltx_text" style="font-size:90%;">RadarSat-2, TerraSARX and Sentinel-1</span></td>
<td id="S7.T3.15.13.18.4.3" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.18.4.3.1" class="ltx_text" style="font-size:90%;">HBB</span></td>
<td id="S7.T3.15.13.18.4.4" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.18.4.4.1" class="ltx_text" style="font-size:90%;">1</span></td>
<td id="S7.T3.15.13.18.4.5" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.18.4.5.1" class="ltx_text" style="font-size:90%;">2,456</span></td>
<td id="S7.T3.15.13.18.4.6" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.18.4.6.1" class="ltx_text" style="font-size:90%;">1,160</span></td>
<td id="S7.T3.15.13.18.4.7" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.18.4.7.1" class="ltx_text" style="font-size:90%;">500</span></td>
<td id="S7.T3.15.13.18.4.8" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.18.4.8.1" class="ltx_text" style="font-size:90%;">1-15m</span></td>
<td id="S7.T3.15.13.18.4.9" class="ltx_td ltx_align_left" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.18.4.9.1" class="ltx_text" style="font-size:90%;">2017</span></td>
</tr>
<tr id="S7.T3.9.7.7" class="ltx_tr">
<td id="S7.T3.9.7.7.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S7.T3.9.7.7.2.1" class="ltx_text" style="font-size:90%;">LEVIR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T3.9.7.7.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib278" title="" class="ltx_ref">278</a><span id="S7.T3.9.7.7.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S7.T3.9.7.7.3" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.9.7.7.3.1" class="ltx_text" style="font-size:90%;">Google Earth</span></td>
<td id="S7.T3.9.7.7.4" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.9.7.7.4.1" class="ltx_text" style="font-size:90%;">HBB</span></td>
<td id="S7.T3.9.7.7.5" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.9.7.7.5.1" class="ltx_text" style="font-size:90%;">3</span></td>
<td id="S7.T3.9.7.7.6" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.9.7.7.6.1" class="ltx_text" style="font-size:90%;">11,000</span></td>
<td id="S7.T3.9.7.7.7" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.9.7.7.7.1" class="ltx_text" style="font-size:90%;">22,000</span></td>
<td id="S7.T3.9.7.7.1" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S7.T3.9.7.7.1.1" class="ltx_text" style="font-size:90%;">800</span><math id="S7.T3.9.7.7.1.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S7.T3.9.7.7.1.m1.1a"><mo mathsize="90%" id="S7.T3.9.7.7.1.m1.1.1" xref="S7.T3.9.7.7.1.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S7.T3.9.7.7.1.m1.1b"><minus id="S7.T3.9.7.7.1.m1.1.1.cmml" xref="S7.T3.9.7.7.1.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S7.T3.9.7.7.1.m1.1c">-</annotation></semantics></math><span id="S7.T3.9.7.7.1.2" class="ltx_text" style="font-size:90%;">600</span>
</td>
<td id="S7.T3.9.7.7.8" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.9.7.7.8.1" class="ltx_text" style="font-size:90%;">0.2-1m</span></td>
<td id="S7.T3.9.7.7.9" class="ltx_td ltx_align_left" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.9.7.7.9.1" class="ltx_text" style="font-size:90%;">2018</span></td>
</tr>
<tr id="S7.T3.10.8.8" class="ltx_tr">
<td id="S7.T3.10.8.8.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S7.T3.10.8.8.2.1" class="ltx_text" style="font-size:90%;">xView </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T3.10.8.8.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib2" title="" class="ltx_ref">2</a><span id="S7.T3.10.8.8.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S7.T3.10.8.8.3" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.10.8.8.3.1" class="ltx_text" style="font-size:90%;">Worldview-3</span></td>
<td id="S7.T3.10.8.8.4" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.10.8.8.4.1" class="ltx_text" style="font-size:90%;">HBB</span></td>
<td id="S7.T3.10.8.8.5" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.10.8.8.5.1" class="ltx_text" style="font-size:90%;">60</span></td>
<td id="S7.T3.10.8.8.6" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.10.8.8.6.1" class="ltx_text" style="font-size:90%;">1,000,000</span></td>
<td id="S7.T3.10.8.8.7" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.10.8.8.7.1" class="ltx_text" style="font-size:90%;">1,413</span></td>
<td id="S7.T3.10.8.8.1" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<math id="S7.T3.10.8.8.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S7.T3.10.8.8.1.m1.1a"><mo mathsize="90%" id="S7.T3.10.8.8.1.m1.1.1" xref="S7.T3.10.8.8.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S7.T3.10.8.8.1.m1.1b"><csymbol cd="latexml" id="S7.T3.10.8.8.1.m1.1.1.cmml" xref="S7.T3.10.8.8.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.T3.10.8.8.1.m1.1c">\sim</annotation></semantics></math><span id="S7.T3.10.8.8.1.1" class="ltx_text" style="font-size:90%;">3,000</span>
</td>
<td id="S7.T3.10.8.8.8" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.10.8.8.8.1" class="ltx_text" style="font-size:90%;">0.3m</span></td>
<td id="S7.T3.10.8.8.9" class="ltx_td ltx_align_left" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.10.8.8.9.1" class="ltx_text" style="font-size:90%;">2018</span></td>
</tr>
<tr id="S7.T3.11.9.9" class="ltx_tr">
<td id="S7.T3.11.9.9.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S7.T3.11.9.9.2.1" class="ltx_text" style="font-size:90%;">DOTA-v1.0 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T3.11.9.9.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib117" title="" class="ltx_ref">117</a><span id="S7.T3.11.9.9.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S7.T3.11.9.9.3" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.11.9.9.3.1" class="ltx_text" style="font-size:90%;">Google Earth, JL-1, and GF-2</span></td>
<td id="S7.T3.11.9.9.4" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.11.9.9.4.1" class="ltx_text" style="font-size:90%;">HBB and OBB</span></td>
<td id="S7.T3.11.9.9.5" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.11.9.9.5.1" class="ltx_text" style="font-size:90%;">15</span></td>
<td id="S7.T3.11.9.9.6" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.11.9.9.6.1" class="ltx_text" style="font-size:90%;">188,282</span></td>
<td id="S7.T3.11.9.9.7" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.11.9.9.7.1" class="ltx_text" style="font-size:90%;">2,806</span></td>
<td id="S7.T3.11.9.9.1" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S7.T3.11.9.9.1.1" class="ltx_text" style="font-size:90%;">800</span><math id="S7.T3.11.9.9.1.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S7.T3.11.9.9.1.m1.1a"><mo mathsize="90%" id="S7.T3.11.9.9.1.m1.1.1" xref="S7.T3.11.9.9.1.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S7.T3.11.9.9.1.m1.1b"><minus id="S7.T3.11.9.9.1.m1.1.1.cmml" xref="S7.T3.11.9.9.1.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S7.T3.11.9.9.1.m1.1c">-</annotation></semantics></math><span id="S7.T3.11.9.9.1.2" class="ltx_text" style="font-size:90%;">13,000</span>
</td>
<td id="S7.T3.11.9.9.8" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.11.9.9.8.1" class="ltx_text" style="font-size:90%;">0.1-1m</span></td>
<td id="S7.T3.11.9.9.9" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.11.9.9.9.1" class="ltx_text" style="font-size:90%;">2018</span></td>
</tr>
<tr id="S7.T3.12.10.10" class="ltx_tr">
<td id="S7.T3.12.10.10.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S7.T3.12.10.10.2.1" class="ltx_text" style="font-size:90%;">HRRSD </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T3.12.10.10.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib48" title="" class="ltx_ref">48</a><span id="S7.T3.12.10.10.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S7.T3.12.10.10.3" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.12.10.10.3.1" class="ltx_text" style="font-size:90%;">Google Earth and Baidu Map</span></td>
<td id="S7.T3.12.10.10.4" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.12.10.10.4.1" class="ltx_text" style="font-size:90%;">HBB</span></td>
<td id="S7.T3.12.10.10.5" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.12.10.10.5.1" class="ltx_text" style="font-size:90%;">13</span></td>
<td id="S7.T3.12.10.10.6" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.12.10.10.6.1" class="ltx_text" style="font-size:90%;">55,740</span></td>
<td id="S7.T3.12.10.10.7" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.12.10.10.7.1" class="ltx_text" style="font-size:90%;">21,761</span></td>
<td id="S7.T3.12.10.10.1" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S7.T3.12.10.10.1.1" class="ltx_text" style="font-size:90%;">152</span><math id="S7.T3.12.10.10.1.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S7.T3.12.10.10.1.m1.1a"><mo mathsize="90%" id="S7.T3.12.10.10.1.m1.1.1" xref="S7.T3.12.10.10.1.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S7.T3.12.10.10.1.m1.1b"><minus id="S7.T3.12.10.10.1.m1.1.1.cmml" xref="S7.T3.12.10.10.1.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S7.T3.12.10.10.1.m1.1c">-</annotation></semantics></math><span id="S7.T3.12.10.10.1.2" class="ltx_text" style="font-size:90%;">10,569</span>
</td>
<td id="S7.T3.12.10.10.8" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.12.10.10.8.1" class="ltx_text" style="font-size:90%;">0.15-1.2m</span></td>
<td id="S7.T3.12.10.10.9" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.12.10.10.9.1" class="ltx_text" style="font-size:90%;">2019</span></td>
</tr>
<tr id="S7.T3.15.13.19.5" class="ltx_tr">
<td id="S7.T3.15.13.19.5.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S7.T3.15.13.19.5.1.1" class="ltx_text" style="font-size:90%;">DIOR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T3.15.13.19.5.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib28" title="" class="ltx_ref">28</a><span id="S7.T3.15.13.19.5.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S7.T3.15.13.19.5.2" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.19.5.2.1" class="ltx_text" style="font-size:90%;">Google Earth</span></td>
<td id="S7.T3.15.13.19.5.3" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.19.5.3.1" class="ltx_text" style="font-size:90%;">HBB</span></td>
<td id="S7.T3.15.13.19.5.4" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.19.5.4.1" class="ltx_text" style="font-size:90%;">20</span></td>
<td id="S7.T3.15.13.19.5.5" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.19.5.5.1" class="ltx_text" style="font-size:90%;">190,288</span></td>
<td id="S7.T3.15.13.19.5.6" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.19.5.6.1" class="ltx_text" style="font-size:90%;">23,463</span></td>
<td id="S7.T3.15.13.19.5.7" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.19.5.7.1" class="ltx_text" style="font-size:90%;">800</span></td>
<td id="S7.T3.15.13.19.5.8" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.19.5.8.1" class="ltx_text" style="font-size:90%;">0.5-30m</span></td>
<td id="S7.T3.15.13.19.5.9" class="ltx_td ltx_align_left" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.19.5.9.1" class="ltx_text" style="font-size:90%;">2019</span></td>
</tr>
<tr id="S7.T3.15.13.20.6" class="ltx_tr">
<td id="S7.T3.15.13.20.6.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S7.T3.15.13.20.6.1.1" class="ltx_text" style="font-size:90%;">AIR-SARShip-1.0 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T3.15.13.20.6.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib279" title="" class="ltx_ref">279</a><span id="S7.T3.15.13.20.6.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S7.T3.15.13.20.6.2" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.20.6.2.1" class="ltx_text" style="font-size:90%;">Gaofen-3</span></td>
<td id="S7.T3.15.13.20.6.3" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.20.6.3.1" class="ltx_text" style="font-size:90%;">HBB</span></td>
<td id="S7.T3.15.13.20.6.4" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.20.6.4.1" class="ltx_text" style="font-size:90%;">1</span></td>
<td id="S7.T3.15.13.20.6.5" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.20.6.5.1" class="ltx_text" style="font-size:90%;">3,000</span></td>
<td id="S7.T3.15.13.20.6.6" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.20.6.6.1" class="ltx_text" style="font-size:90%;">31</span></td>
<td id="S7.T3.15.13.20.6.7" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.20.6.7.1" class="ltx_text" style="font-size:90%;">3,000</span></td>
<td id="S7.T3.15.13.20.6.8" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.20.6.8.1" class="ltx_text" style="font-size:90%;">1m and 3m</span></td>
<td id="S7.T3.15.13.20.6.9" class="ltx_td ltx_align_left" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.20.6.9.1" class="ltx_text" style="font-size:90%;">2019</span></td>
</tr>
<tr id="S7.T3.13.11.11" class="ltx_tr">
<td id="S7.T3.13.11.11.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S7.T3.13.11.11.2.1" class="ltx_text" style="font-size:90%;">MAR20 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T3.13.11.11.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib280" title="" class="ltx_ref">280</a><span id="S7.T3.13.11.11.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S7.T3.13.11.11.3" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.13.11.11.3.1" class="ltx_text" style="font-size:90%;">Google Earth</span></td>
<td id="S7.T3.13.11.11.4" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.13.11.11.4.1" class="ltx_text" style="font-size:90%;">HBB and OBB</span></td>
<td id="S7.T3.13.11.11.5" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.13.11.11.5.1" class="ltx_text" style="font-size:90%;">20</span></td>
<td id="S7.T3.13.11.11.6" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.13.11.11.6.1" class="ltx_text" style="font-size:90%;">22,341</span></td>
<td id="S7.T3.13.11.11.7" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.13.11.11.7.1" class="ltx_text" style="font-size:90%;">3,824</span></td>
<td id="S7.T3.13.11.11.1" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<math id="S7.T3.13.11.11.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S7.T3.13.11.11.1.m1.1a"><mo mathsize="90%" id="S7.T3.13.11.11.1.m1.1.1" xref="S7.T3.13.11.11.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S7.T3.13.11.11.1.m1.1b"><csymbol cd="latexml" id="S7.T3.13.11.11.1.m1.1.1.cmml" xref="S7.T3.13.11.11.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.T3.13.11.11.1.m1.1c">\sim</annotation></semantics></math><span id="S7.T3.13.11.11.1.1" class="ltx_text" style="font-size:90%;">800</span>
</td>
<td id="S7.T3.13.11.11.8" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.13.11.11.8.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S7.T3.13.11.11.9" class="ltx_td ltx_align_left" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.13.11.11.9.1" class="ltx_text" style="font-size:90%;">2020</span></td>
</tr>
<tr id="S7.T3.15.13.21.7" class="ltx_tr">
<td id="S7.T3.15.13.21.7.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S7.T3.15.13.21.7.1.1" class="ltx_text" style="font-size:90%;">FGSD </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T3.15.13.21.7.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib281" title="" class="ltx_ref">281</a><span id="S7.T3.15.13.21.7.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S7.T3.15.13.21.7.2" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.21.7.2.1" class="ltx_text" style="font-size:90%;">Google Earth</span></td>
<td id="S7.T3.15.13.21.7.3" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.21.7.3.1" class="ltx_text" style="font-size:90%;">OBB</span></td>
<td id="S7.T3.15.13.21.7.4" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.21.7.4.1" class="ltx_text" style="font-size:90%;">43</span></td>
<td id="S7.T3.15.13.21.7.5" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.21.7.5.1" class="ltx_text" style="font-size:90%;">5,634</span></td>
<td id="S7.T3.15.13.21.7.6" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.21.7.6.1" class="ltx_text" style="font-size:90%;">2,612</span></td>
<td id="S7.T3.15.13.21.7.7" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.21.7.7.1" class="ltx_text" style="font-size:90%;">930</span></td>
<td id="S7.T3.15.13.21.7.8" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.21.7.8.1" class="ltx_text" style="font-size:90%;">0.12-1.93m</span></td>
<td id="S7.T3.15.13.21.7.9" class="ltx_td ltx_align_left" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.21.7.9.1" class="ltx_text" style="font-size:90%;">2020</span></td>
</tr>
<tr id="S7.T3.15.13.22.8" class="ltx_tr">
<td id="S7.T3.15.13.22.8.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S7.T3.15.13.22.8.1.1" class="ltx_text" style="font-size:90%;">DOSR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T3.15.13.22.8.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib282" title="" class="ltx_ref">282</a><span id="S7.T3.15.13.22.8.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S7.T3.15.13.22.8.2" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.22.8.2.1" class="ltx_text" style="font-size:90%;">Google Earth</span></td>
<td id="S7.T3.15.13.22.8.3" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.22.8.3.1" class="ltx_text" style="font-size:90%;">OBB</span></td>
<td id="S7.T3.15.13.22.8.4" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.22.8.4.1" class="ltx_text" style="font-size:90%;">20</span></td>
<td id="S7.T3.15.13.22.8.5" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.22.8.5.1" class="ltx_text" style="font-size:90%;">6,172</span></td>
<td id="S7.T3.15.13.22.8.6" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.22.8.6.1" class="ltx_text" style="font-size:90%;">1,066</span></td>
<td id="S7.T3.15.13.22.8.7" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.22.8.7.1" class="ltx_text" style="font-size:90%;">600-1,300</span></td>
<td id="S7.T3.15.13.22.8.8" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.22.8.8.1" class="ltx_text" style="font-size:90%;">0.5-2.5m</span></td>
<td id="S7.T3.15.13.22.8.9" class="ltx_td ltx_align_left" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.22.8.9.1" class="ltx_text" style="font-size:90%;">2021</span></td>
</tr>
<tr id="S7.T3.15.13.23.9" class="ltx_tr">
<td id="S7.T3.15.13.23.9.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S7.T3.15.13.23.9.1.1" class="ltx_text" style="font-size:90%;">AI-TOD </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T3.15.13.23.9.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib283" title="" class="ltx_ref">283</a><span id="S7.T3.15.13.23.9.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S7.T3.15.13.23.9.2" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.23.9.2.1" class="ltx_text" style="font-size:90%;">Multiple Sources</span></td>
<td id="S7.T3.15.13.23.9.3" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.23.9.3.1" class="ltx_text" style="font-size:90%;">HBB</span></td>
<td id="S7.T3.15.13.23.9.4" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.23.9.4.1" class="ltx_text" style="font-size:90%;">8</span></td>
<td id="S7.T3.15.13.23.9.5" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.23.9.5.1" class="ltx_text" style="font-size:90%;">700,621</span></td>
<td id="S7.T3.15.13.23.9.6" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.23.9.6.1" class="ltx_text" style="font-size:90%;">28,036</span></td>
<td id="S7.T3.15.13.23.9.7" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.23.9.7.1" class="ltx_text" style="font-size:90%;">800</span></td>
<td id="S7.T3.15.13.23.9.8" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.23.9.8.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S7.T3.15.13.23.9.9" class="ltx_td ltx_align_left" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.23.9.9.1" class="ltx_text" style="font-size:90%;">2021</span></td>
</tr>
<tr id="S7.T3.15.13.24.10" class="ltx_tr">
<td id="S7.T3.15.13.24.10.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S7.T3.15.13.24.10.1.1" class="ltx_text" style="font-size:90%;">FAIR1M </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T3.15.13.24.10.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib34" title="" class="ltx_ref">34</a><span id="S7.T3.15.13.24.10.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S7.T3.15.13.24.10.2" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.24.10.2.1" class="ltx_text" style="font-size:90%;">Gaofen satellites and Google Earth</span></td>
<td id="S7.T3.15.13.24.10.3" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.24.10.3.1" class="ltx_text" style="font-size:90%;">OBB</span></td>
<td id="S7.T3.15.13.24.10.4" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.24.10.4.1" class="ltx_text" style="font-size:90%;">37</span></td>
<td id="S7.T3.15.13.24.10.5" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.24.10.5.1" class="ltx_text" style="font-size:90%;">1,020,579</span></td>
<td id="S7.T3.15.13.24.10.6" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.24.10.6.1" class="ltx_text" style="font-size:90%;">42,796</span></td>
<td id="S7.T3.15.13.24.10.7" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.24.10.7.1" class="ltx_text" style="font-size:90%;">600-10,000</span></td>
<td id="S7.T3.15.13.24.10.8" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.24.10.8.1" class="ltx_text" style="font-size:90%;">0.3-0.8m</span></td>
<td id="S7.T3.15.13.24.10.9" class="ltx_td ltx_align_left" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.24.10.9.1" class="ltx_text" style="font-size:90%;">2021</span></td>
</tr>
<tr id="S7.T3.15.13.25.11" class="ltx_tr">
<td id="S7.T3.15.13.25.11.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S7.T3.15.13.25.11.1.1" class="ltx_text" style="font-size:90%;">DOTA-v2.0 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T3.15.13.25.11.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib33" title="" class="ltx_ref">33</a><span id="S7.T3.15.13.25.11.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S7.T3.15.13.25.11.2" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.25.11.2.1" class="ltx_text" style="font-size:90%;">Google Earth, JL-1, GF-2 and airborne images</span></td>
<td id="S7.T3.15.13.25.11.3" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.25.11.3.1" class="ltx_text" style="font-size:90%;">HBB and OBB</span></td>
<td id="S7.T3.15.13.25.11.4" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.25.11.4.1" class="ltx_text" style="font-size:90%;">18</span></td>
<td id="S7.T3.15.13.25.11.5" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.25.11.5.1" class="ltx_text" style="font-size:90%;">1,793,658</span></td>
<td id="S7.T3.15.13.25.11.6" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.25.11.6.1" class="ltx_text" style="font-size:90%;">11,268</span></td>
<td id="S7.T3.15.13.25.11.7" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.25.11.7.1" class="ltx_text" style="font-size:90%;">800-20,000</span></td>
<td id="S7.T3.15.13.25.11.8" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.25.11.8.1" class="ltx_text" style="font-size:90%;">0.1-4.5m</span></td>
<td id="S7.T3.15.13.25.11.9" class="ltx_td ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.25.11.9.1" class="ltx_text" style="font-size:90%;">2021</span></td>
</tr>
<tr id="S7.T3.15.13.13" class="ltx_tr">
<td id="S7.T3.15.13.13.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S7.T3.15.13.13.3.1" class="ltx_text" style="font-size:90%;">SODA-A </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T3.15.13.13.3.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib284" title="" class="ltx_ref">284</a><span id="S7.T3.15.13.13.3.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S7.T3.15.13.13.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.13.4.1" class="ltx_text" style="font-size:90%;">Google Earth</span></td>
<td id="S7.T3.15.13.13.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.13.5.1" class="ltx_text" style="font-size:90%;">OBB</span></td>
<td id="S7.T3.15.13.13.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.13.6.1" class="ltx_text" style="font-size:90%;">9</span></td>
<td id="S7.T3.15.13.13.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.13.7.1" class="ltx_text" style="font-size:90%;">800,203</span></td>
<td id="S7.T3.15.13.13.8" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.13.8.1" class="ltx_text" style="font-size:90%;">2,510</span></td>
<td id="S7.T3.15.13.13.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S7.T3.15.13.13.2.1" class="ltx_text" style="font-size:90%;">4,761</span><math id="S7.T3.14.12.12.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S7.T3.14.12.12.1.m1.1a"><mo mathsize="90%" id="S7.T3.14.12.12.1.m1.1.1" xref="S7.T3.14.12.12.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S7.T3.14.12.12.1.m1.1b"><times id="S7.T3.14.12.12.1.m1.1.1.cmml" xref="S7.T3.14.12.12.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S7.T3.14.12.12.1.m1.1c">\times</annotation></semantics></math><span id="S7.T3.15.13.13.2.2" class="ltx_text" style="font-size:90%;">2,777</span><sup id="S7.T3.15.13.13.2.3" class="ltx_sup"><span id="S7.T3.15.13.13.2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">∗</span></sup>
</td>
<td id="S7.T3.15.13.13.9" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.13.9.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S7.T3.15.13.13.10" class="ltx_td ltx_align_left ltx_border_b" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S7.T3.15.13.13.10.1" class="ltx_text" style="font-size:90%;">2022</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S7.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Dataset selection guidelines in RSOD for different challenges and scenarios.</figcaption>
<table id="S7.T4.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S7.T4.3.1.1" class="ltx_tr">
<th id="S7.T4.3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:1.4pt 3.8pt;"><span id="S7.T4.3.1.1.1.1" class="ltx_text" style="font-size:70%;">Scenarios</span></th>
<th id="S7.T4.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:1.4pt 3.8pt;"><span id="S7.T4.3.1.1.2.1" class="ltx_text" style="font-size:70%;">Datasets</span></th>
<th id="S7.T4.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:1.4pt 3.8pt;"><span id="S7.T4.3.1.1.3.1" class="ltx_text" style="font-size:70%;">Methods</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S7.T4.3.2.1" class="ltx_tr">
<td id="S7.T4.3.2.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.4pt 3.8pt;"><span id="S7.T4.3.2.1.1.1" class="ltx_text" style="font-size:70%;">Multi-scale Objects</span></td>
<td id="S7.T4.3.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.4pt 3.8pt;"><span id="S7.T4.3.2.1.2.1" class="ltx_text" style="font-size:70%;">DOTA, DIOR, FAIR1M</span></td>
<td id="S7.T4.3.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.4pt 3.8pt;">
<span id="S7.T4.3.2.1.3.1" class="ltx_text" style="font-size:70%;">HyNet</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T4.3.2.1.3.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib51" title="" class="ltx_ref">51</a><span id="S7.T4.3.2.1.3.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S7.T4.3.2.1.3.4" class="ltx_text" style="font-size:70%;">, FFA</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T4.3.2.1.3.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib80" title="" class="ltx_ref">80</a><span id="S7.T4.3.2.1.3.6.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
</tr>
<tr id="S7.T4.3.3.2" class="ltx_tr">
<td id="S7.T4.3.3.2.1" class="ltx_td ltx_align_center" style="padding:1.4pt 3.8pt;"><span id="S7.T4.3.3.2.1.1" class="ltx_text" style="font-size:70%;">Rotated Objects</span></td>
<td id="S7.T4.3.3.2.2" class="ltx_td ltx_align_center" style="padding:1.4pt 3.8pt;"><span id="S7.T4.3.3.2.2.1" class="ltx_text" style="font-size:70%;">DOTA, HRSC</span></td>
<td id="S7.T4.3.3.2.3" class="ltx_td ltx_align_center" style="padding:1.4pt 3.8pt;">
<span id="S7.T4.3.3.2.3.1" class="ltx_text" style="font-size:70%;">KLD</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T4.3.3.2.3.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib131" title="" class="ltx_ref">131</a><span id="S7.T4.3.3.2.3.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S7.T4.3.3.2.3.4" class="ltx_text" style="font-size:70%;">, ReDet</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T4.3.3.2.3.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib155" title="" class="ltx_ref">155</a><span id="S7.T4.3.3.2.3.6.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
</tr>
<tr id="S7.T4.3.4.3" class="ltx_tr">
<td id="S7.T4.3.4.3.1" class="ltx_td ltx_align_center" style="padding:1.4pt 3.8pt;"><span id="S7.T4.3.4.3.1.1" class="ltx_text" style="font-size:70%;">Weak Objects</span></td>
<td id="S7.T4.3.4.3.2" class="ltx_td ltx_align_center" style="padding:1.4pt 3.8pt;"><span id="S7.T4.3.4.3.2.1" class="ltx_text" style="font-size:70%;">DOTA, DIOR, FAIR1M</span></td>
<td id="S7.T4.3.4.3.3" class="ltx_td ltx_align_center" style="padding:1.4pt 3.8pt;">
<span id="S7.T4.3.4.3.3.1" class="ltx_text" style="font-size:70%;">RECNN</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T4.3.4.3.3.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib172" title="" class="ltx_ref">172</a><span id="S7.T4.3.4.3.3.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S7.T4.3.4.3.3.4" class="ltx_text" style="font-size:70%;">, CADNet</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T4.3.4.3.3.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib196" title="" class="ltx_ref">196</a><span id="S7.T4.3.4.3.3.6.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
</tr>
<tr id="S7.T4.3.5.4" class="ltx_tr">
<td id="S7.T4.3.5.4.1" class="ltx_td ltx_align_center" style="padding:1.4pt 3.8pt;"><span id="S7.T4.3.5.4.1.1" class="ltx_text" style="font-size:70%;">Tiny Objects</span></td>
<td id="S7.T4.3.5.4.2" class="ltx_td ltx_align_center" style="padding:1.4pt 3.8pt;"><span id="S7.T4.3.5.4.2.1" class="ltx_text" style="font-size:70%;">SODA-A, AI-TOD</span></td>
<td id="S7.T4.3.5.4.3" class="ltx_td ltx_align_center" style="padding:1.4pt 3.8pt;">
<span id="S7.T4.3.5.4.3.1" class="ltx_text" style="font-size:70%;">NWD</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T4.3.5.4.3.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S7.T4.3.5.4.3.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S7.T4.3.5.4.3.4" class="ltx_text" style="font-size:70%;">, FSANet</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T4.3.5.4.3.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib216" title="" class="ltx_ref">216</a><span id="S7.T4.3.5.4.3.6.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
</tr>
<tr id="S7.T4.3.6.5" class="ltx_tr">
<td id="S7.T4.3.6.5.1" class="ltx_td ltx_align_center" style="padding:1.4pt 3.8pt;"><span id="S7.T4.3.6.5.1.1" class="ltx_text" style="font-size:70%;">Weakly Supervision</span></td>
<td id="S7.T4.3.6.5.2" class="ltx_td ltx_align_center" style="padding:1.4pt 3.8pt;"><span id="S7.T4.3.6.5.2.1" class="ltx_text" style="font-size:70%;">NWPU VHR-10, DIOR</span></td>
<td id="S7.T4.3.6.5.3" class="ltx_td ltx_align_center" style="padding:1.4pt 3.8pt;">
<span id="S7.T4.3.6.5.3.1" class="ltx_text" style="font-size:70%;">RINet</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T4.3.6.5.3.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib240" title="" class="ltx_ref">240</a><span id="S7.T4.3.6.5.3.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S7.T4.3.6.5.3.4" class="ltx_text" style="font-size:70%;">, MOL</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T4.3.6.5.3.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib241" title="" class="ltx_ref">241</a><span id="S7.T4.3.6.5.3.6.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
</tr>
<tr id="S7.T4.3.7.6" class="ltx_tr">
<td id="S7.T4.3.7.6.1" class="ltx_td ltx_align_center" style="padding:1.4pt 3.8pt;"><span id="S7.T4.3.7.6.1.1" class="ltx_text" style="font-size:70%;">Few-shot Supervision</span></td>
<td id="S7.T4.3.7.6.2" class="ltx_td ltx_align_center" style="padding:1.4pt 3.8pt;"><span id="S7.T4.3.7.6.2.1" class="ltx_text" style="font-size:70%;">NWPU VHR-10, DIOR</span></td>
<td id="S7.T4.3.7.6.3" class="ltx_td ltx_align_center" style="padding:1.4pt 3.8pt;">
<span id="S7.T4.3.7.6.3.1" class="ltx_text" style="font-size:70%;">P-CNN</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T4.3.7.6.3.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib254" title="" class="ltx_ref">254</a><span id="S7.T4.3.7.6.3.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S7.T4.3.7.6.3.4" class="ltx_text" style="font-size:70%;">, G-FSDet</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T4.3.7.6.3.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib269" title="" class="ltx_ref">269</a><span id="S7.T4.3.7.6.3.6.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
</tr>
<tr id="S7.T4.3.8.7" class="ltx_tr">
<td id="S7.T4.3.8.7.1" class="ltx_td ltx_align_center" style="padding:1.4pt 3.8pt;"><span id="S7.T4.3.8.7.1.1" class="ltx_text" style="font-size:70%;">Fine-grained Objects</span></td>
<td id="S7.T4.3.8.7.2" class="ltx_td ltx_align_center" style="padding:1.4pt 3.8pt;"><span id="S7.T4.3.8.7.2.1" class="ltx_text" style="font-size:70%;">DOSR, FAIR1M</span></td>
<td id="S7.T4.3.8.7.3" class="ltx_td ltx_align_center" style="padding:1.4pt 3.8pt;">
<span id="S7.T4.3.8.7.3.1" class="ltx_text" style="font-size:70%;">RBFPN</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T4.3.8.7.3.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib94" title="" class="ltx_ref">94</a><span id="S7.T4.3.8.7.3.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S7.T4.3.8.7.3.4" class="ltx_text" style="font-size:70%;">, EIRNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T4.3.8.7.3.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib282" title="" class="ltx_ref">282</a><span id="S7.T4.3.8.7.3.6.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
</tr>
<tr id="S7.T4.3.9.8" class="ltx_tr">
<td id="S7.T4.3.9.8.1" class="ltx_td ltx_align_center" style="padding:1.4pt 3.8pt;"><span id="S7.T4.3.9.8.1.1" class="ltx_text" style="font-size:70%;">SAR image Objects</span></td>
<td id="S7.T4.3.9.8.2" class="ltx_td ltx_align_center" style="padding:1.4pt 3.8pt;"><span id="S7.T4.3.9.8.2.1" class="ltx_text" style="font-size:70%;">SSDD, AIR-SARShip</span></td>
<td id="S7.T4.3.9.8.3" class="ltx_td ltx_align_center" style="padding:1.4pt 3.8pt;">
<span id="S7.T4.3.9.8.3.1" class="ltx_text" style="font-size:70%;">SSPNet</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T4.3.9.8.3.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib163" title="" class="ltx_ref">163</a><span id="S7.T4.3.9.8.3.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S7.T4.3.9.8.3.4" class="ltx_text" style="font-size:70%;">,
HyperLiNet</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T4.3.9.8.3.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib285" title="" class="ltx_ref">285</a><span id="S7.T4.3.9.8.3.6.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
</tr>
<tr id="S7.T4.3.10.9" class="ltx_tr">
<td id="S7.T4.3.10.9.1" class="ltx_td ltx_align_center ltx_border_b" style="padding:1.4pt 3.8pt;"><span id="S7.T4.3.10.9.1.1" class="ltx_text" style="font-size:70%;">Specific Objects</span></td>
<td id="S7.T4.3.10.9.2" class="ltx_td ltx_align_center ltx_border_b" style="padding:1.4pt 3.8pt;"><span id="S7.T4.3.10.9.2.1" class="ltx_text" style="font-size:70%;">HRSC, MAR20</span></td>
<td id="S7.T4.3.10.9.3" class="ltx_td ltx_align_center ltx_border_b" style="padding:1.4pt 3.8pt;">
<span id="S7.T4.3.10.9.3.1" class="ltx_text" style="font-size:70%;">GRS-Det</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T4.3.10.9.3.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib135" title="" class="ltx_ref">135</a><span id="S7.T4.3.10.9.3.3.2" class="ltx_text" style="font-size:70%;">]</span></cite><span id="S7.T4.3.10.9.3.4" class="ltx_text" style="font-size:70%;">, COLOR</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.T4.3.10.9.3.5.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib245" title="" class="ltx_ref">245</a><span id="S7.T4.3.10.9.3.6.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S7.F18" class="ltx_figure"><img src="/html/2309.06751/assets/x19.png" id="S7.F18.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="442" height="483" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 18: </span>Visualization of different RSOD datasets. Diverse resolutions, massive instances, multi-sensor images, and fine-grained categories are typical characteristics of RSOD datasets.</figcaption>
</figure>
<figure id="S7.F19" class="ltx_figure"><img src="/html/2309.06751/assets/x20.png" id="S7.F19.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="442" height="239" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 19: </span>The widespread applications of RSOD make substantial contributions to implementing of SDGs and improving society.
(a) Collapsed buildings detection in Post-Earthquake for disaster assessment.
(b) Corn plant detection for precision agriculture.
(c-d) Building and vehicle detection for sustainable cities and communities.
(e) Solar photovoltaic detection for climate change mitigation.
(f) Litter detection along the shore for ocean conservation.
(g) African mammals detection for wildlife surveillance.
(h) Single tree detection for forest ecosystem protection.</figcaption>
</figure>
<div id="S7.SS1.p2" class="ltx_para">
<p id="S7.SS1.p2.1" class="ltx_p"><span id="S7.SS1.p2.1.1" class="ltx_text ltx_font_bold">NWPU VHR-10<cite class="ltx_cite ltx_citemacro_cite"><span id="S7.SS1.p2.1.1.1.1" class="ltx_text ltx_font_medium">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S7.SS1.p2.1.1.2.2" class="ltx_text ltx_font_medium">]</span></cite></span>. This dataset is a multi-class geospatial object detection dataset. It contains 3,775 HBB annotated instances in ten categories: airplane, ship, storage tank, baseball diamond, tennis court, basketball court, ground track field, harbor, bridge, and vehicle. There are 800 very high-resolution RSIs, consisting of 715 color images from Google Earth and 85 pan-sharpened color infrared images from Vaihingen data. The image resolutions range from 0.5 to 2 m.</p>
</div>
<div id="S7.SS1.p3" class="ltx_para">
<p id="S7.SS1.p3.1" class="ltx_p"><span id="S7.SS1.p3.1.1" class="ltx_text ltx_font_bold">VEDAI<cite class="ltx_cite ltx_citemacro_cite"><span id="S7.SS1.p3.1.1.1.1" class="ltx_text ltx_font_medium">[</span><a href="#bib.bib272" title="" class="ltx_ref">272</a><span id="S7.SS1.p3.1.1.2.2" class="ltx_text ltx_font_medium">]</span></cite></span>. VEDAI is a fine-grained vehicle detection dataset that contains five fine-grained vehicle categories: camping car, car, pick-up, tractor, truck, and van. There are 1,210 images and 3,700 instances in the VEDAI dataset, and the size of each image is <math id="S7.SS1.p3.1.m1.3" class="ltx_Math" alttext="1,024\times 1,024" display="inline"><semantics id="S7.SS1.p3.1.m1.3a"><mrow id="S7.SS1.p3.1.m1.3.3.1" xref="S7.SS1.p3.1.m1.3.3.2.cmml"><mn id="S7.SS1.p3.1.m1.1.1" xref="S7.SS1.p3.1.m1.1.1.cmml">1</mn><mo id="S7.SS1.p3.1.m1.3.3.1.2" xref="S7.SS1.p3.1.m1.3.3.2.cmml">,</mo><mrow id="S7.SS1.p3.1.m1.3.3.1.1" xref="S7.SS1.p3.1.m1.3.3.1.1.cmml"><mn id="S7.SS1.p3.1.m1.3.3.1.1.2" xref="S7.SS1.p3.1.m1.3.3.1.1.2.cmml">024</mn><mo lspace="0.222em" rspace="0.222em" id="S7.SS1.p3.1.m1.3.3.1.1.1" xref="S7.SS1.p3.1.m1.3.3.1.1.1.cmml">×</mo><mn id="S7.SS1.p3.1.m1.3.3.1.1.3" xref="S7.SS1.p3.1.m1.3.3.1.1.3.cmml">1</mn></mrow><mo id="S7.SS1.p3.1.m1.3.3.1.3" xref="S7.SS1.p3.1.m1.3.3.2.cmml">,</mo><mn id="S7.SS1.p3.1.m1.2.2" xref="S7.SS1.p3.1.m1.2.2.cmml">024</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.1.m1.3b"><list id="S7.SS1.p3.1.m1.3.3.2.cmml" xref="S7.SS1.p3.1.m1.3.3.1"><cn type="integer" id="S7.SS1.p3.1.m1.1.1.cmml" xref="S7.SS1.p3.1.m1.1.1">1</cn><apply id="S7.SS1.p3.1.m1.3.3.1.1.cmml" xref="S7.SS1.p3.1.m1.3.3.1.1"><times id="S7.SS1.p3.1.m1.3.3.1.1.1.cmml" xref="S7.SS1.p3.1.m1.3.3.1.1.1"></times><cn type="integer" id="S7.SS1.p3.1.m1.3.3.1.1.2.cmml" xref="S7.SS1.p3.1.m1.3.3.1.1.2">024</cn><cn type="integer" id="S7.SS1.p3.1.m1.3.3.1.1.3.cmml" xref="S7.SS1.p3.1.m1.3.3.1.1.3">1</cn></apply><cn type="integer" id="S7.SS1.p3.1.m1.2.2.cmml" xref="S7.SS1.p3.1.m1.2.2">024</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.1.m1.3c">1,024\times 1,024</annotation></semantics></math>. The small area and the arbitrary orientation of vehicles are the main challenges in the VEDAI dataset.</p>
</div>
<div id="S7.SS1.p4" class="ltx_para">
<p id="S7.SS1.p4.1" class="ltx_p"><span id="S7.SS1.p4.1.1" class="ltx_text ltx_font_bold">UCAS-AOD<cite class="ltx_cite ltx_citemacro_cite"><span id="S7.SS1.p4.1.1.1.1" class="ltx_text ltx_font_medium">[</span><a href="#bib.bib274" title="" class="ltx_ref">274</a><span id="S7.SS1.p4.1.1.2.2" class="ltx_text ltx_font_medium">]</span></cite></span>. The UCAS-AOD dataset includes 910 images and 6,029 objects, where 3,210 aircraft are contained in 600 images and 2,819 vehicles are contained in 310 images. All images are acquired from Google Earth with an image size of approximately <math id="S7.SS1.p4.1.m1.3" class="ltx_Math" alttext="1,000\times 1,000" display="inline"><semantics id="S7.SS1.p4.1.m1.3a"><mrow id="S7.SS1.p4.1.m1.3.3.1" xref="S7.SS1.p4.1.m1.3.3.2.cmml"><mn id="S7.SS1.p4.1.m1.1.1" xref="S7.SS1.p4.1.m1.1.1.cmml">1</mn><mo id="S7.SS1.p4.1.m1.3.3.1.2" xref="S7.SS1.p4.1.m1.3.3.2.cmml">,</mo><mrow id="S7.SS1.p4.1.m1.3.3.1.1" xref="S7.SS1.p4.1.m1.3.3.1.1.cmml"><mn id="S7.SS1.p4.1.m1.3.3.1.1.2" xref="S7.SS1.p4.1.m1.3.3.1.1.2.cmml">000</mn><mo lspace="0.222em" rspace="0.222em" id="S7.SS1.p4.1.m1.3.3.1.1.1" xref="S7.SS1.p4.1.m1.3.3.1.1.1.cmml">×</mo><mn id="S7.SS1.p4.1.m1.3.3.1.1.3" xref="S7.SS1.p4.1.m1.3.3.1.1.3.cmml">1</mn></mrow><mo id="S7.SS1.p4.1.m1.3.3.1.3" xref="S7.SS1.p4.1.m1.3.3.2.cmml">,</mo><mn id="S7.SS1.p4.1.m1.2.2" xref="S7.SS1.p4.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p4.1.m1.3b"><list id="S7.SS1.p4.1.m1.3.3.2.cmml" xref="S7.SS1.p4.1.m1.3.3.1"><cn type="integer" id="S7.SS1.p4.1.m1.1.1.cmml" xref="S7.SS1.p4.1.m1.1.1">1</cn><apply id="S7.SS1.p4.1.m1.3.3.1.1.cmml" xref="S7.SS1.p4.1.m1.3.3.1.1"><times id="S7.SS1.p4.1.m1.3.3.1.1.1.cmml" xref="S7.SS1.p4.1.m1.3.3.1.1.1"></times><cn type="integer" id="S7.SS1.p4.1.m1.3.3.1.1.2.cmml" xref="S7.SS1.p4.1.m1.3.3.1.1.2">000</cn><cn type="integer" id="S7.SS1.p4.1.m1.3.3.1.1.3.cmml" xref="S7.SS1.p4.1.m1.3.3.1.1.3">1</cn></apply><cn type="integer" id="S7.SS1.p4.1.m1.2.2.cmml" xref="S7.SS1.p4.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p4.1.m1.3c">1,000\times 1,000</annotation></semantics></math>.</p>
</div>
<div id="S7.SS1.p5" class="ltx_para">
<p id="S7.SS1.p5.2" class="ltx_p"><span id="S7.SS1.p5.2.1" class="ltx_text ltx_font_bold">HRSC<cite class="ltx_cite ltx_citemacro_cite"><span id="S7.SS1.p5.2.1.1.1" class="ltx_text ltx_font_medium">[</span><a href="#bib.bib276" title="" class="ltx_ref">276</a><span id="S7.SS1.p5.2.1.2.2" class="ltx_text ltx_font_medium">]</span></cite></span>. The HRSC dataset is widely used for arbitrary orientation ship detection and consists of 1,070 images and 2,976 instances with OBB annotation. The images are captured from Google Earth, containing offshore and inshore scenes. The image sizes vary from <math id="S7.SS1.p5.1.m1.1" class="ltx_Math" alttext="300\times 300" display="inline"><semantics id="S7.SS1.p5.1.m1.1a"><mrow id="S7.SS1.p5.1.m1.1.1" xref="S7.SS1.p5.1.m1.1.1.cmml"><mn id="S7.SS1.p5.1.m1.1.1.2" xref="S7.SS1.p5.1.m1.1.1.2.cmml">300</mn><mo lspace="0.222em" rspace="0.222em" id="S7.SS1.p5.1.m1.1.1.1" xref="S7.SS1.p5.1.m1.1.1.1.cmml">×</mo><mn id="S7.SS1.p5.1.m1.1.1.3" xref="S7.SS1.p5.1.m1.1.1.3.cmml">300</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p5.1.m1.1b"><apply id="S7.SS1.p5.1.m1.1.1.cmml" xref="S7.SS1.p5.1.m1.1.1"><times id="S7.SS1.p5.1.m1.1.1.1.cmml" xref="S7.SS1.p5.1.m1.1.1.1"></times><cn type="integer" id="S7.SS1.p5.1.m1.1.1.2.cmml" xref="S7.SS1.p5.1.m1.1.1.2">300</cn><cn type="integer" id="S7.SS1.p5.1.m1.1.1.3.cmml" xref="S7.SS1.p5.1.m1.1.1.3">300</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p5.1.m1.1c">300\times 300</annotation></semantics></math> and <math id="S7.SS1.p5.2.m2.2" class="ltx_Math" alttext="1,500\times 900" display="inline"><semantics id="S7.SS1.p5.2.m2.2a"><mrow id="S7.SS1.p5.2.m2.2.2.1" xref="S7.SS1.p5.2.m2.2.2.2.cmml"><mn id="S7.SS1.p5.2.m2.1.1" xref="S7.SS1.p5.2.m2.1.1.cmml">1</mn><mo id="S7.SS1.p5.2.m2.2.2.1.2" xref="S7.SS1.p5.2.m2.2.2.2.cmml">,</mo><mrow id="S7.SS1.p5.2.m2.2.2.1.1" xref="S7.SS1.p5.2.m2.2.2.1.1.cmml"><mn id="S7.SS1.p5.2.m2.2.2.1.1.2" xref="S7.SS1.p5.2.m2.2.2.1.1.2.cmml">500</mn><mo lspace="0.222em" rspace="0.222em" id="S7.SS1.p5.2.m2.2.2.1.1.1" xref="S7.SS1.p5.2.m2.2.2.1.1.1.cmml">×</mo><mn id="S7.SS1.p5.2.m2.2.2.1.1.3" xref="S7.SS1.p5.2.m2.2.2.1.1.3.cmml">900</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p5.2.m2.2b"><list id="S7.SS1.p5.2.m2.2.2.2.cmml" xref="S7.SS1.p5.2.m2.2.2.1"><cn type="integer" id="S7.SS1.p5.2.m2.1.1.cmml" xref="S7.SS1.p5.2.m2.1.1">1</cn><apply id="S7.SS1.p5.2.m2.2.2.1.1.cmml" xref="S7.SS1.p5.2.m2.2.2.1.1"><times id="S7.SS1.p5.2.m2.2.2.1.1.1.cmml" xref="S7.SS1.p5.2.m2.2.2.1.1.1"></times><cn type="integer" id="S7.SS1.p5.2.m2.2.2.1.1.2.cmml" xref="S7.SS1.p5.2.m2.2.2.1.1.2">500</cn><cn type="integer" id="S7.SS1.p5.2.m2.2.2.1.1.3.cmml" xref="S7.SS1.p5.2.m2.2.2.1.1.3">900</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p5.2.m2.2c">1,500\times 900</annotation></semantics></math>, and the image resolutions range from 2 to 0.4 m.</p>
</div>
<div id="S7.SS1.p6" class="ltx_para">
<p id="S7.SS1.p6.1" class="ltx_p"><span id="S7.SS1.p6.1.1" class="ltx_text ltx_font_bold">SSDD<cite class="ltx_cite ltx_citemacro_cite"><span id="S7.SS1.p6.1.1.1.1" class="ltx_text ltx_font_medium">[</span><a href="#bib.bib277" title="" class="ltx_ref">277</a><span id="S7.SS1.p6.1.1.2.2" class="ltx_text ltx_font_medium">]</span></cite></span>. SSDD is the first open dataset for SAR image ship detection and contains 1,160 SAR images and 2,456 ships. The SAR images in the SSDD dataset are collected from different sensors with resolutions from 1m to 15 m and have different polarizations (HH, VV, VH, and HV). Subsequently, the author further refines and enriches the SSDD dataset into three different types to satisfy the current research of SAR ship detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib286" title="" class="ltx_ref">286</a>]</cite>.</p>
</div>
<div id="S7.SS1.p7" class="ltx_para">
<p id="S7.SS1.p7.1" class="ltx_p"><span id="S7.SS1.p7.1.1" class="ltx_text ltx_font_bold">xView<cite class="ltx_cite ltx_citemacro_cite"><span id="S7.SS1.p7.1.1.1.1" class="ltx_text ltx_font_medium">[</span><a href="#bib.bib2" title="" class="ltx_ref">2</a><span id="S7.SS1.p7.1.1.2.2" class="ltx_text ltx_font_medium">]</span></cite></span>. The xView dataset is one of the largest publicly available datasets in ROSD, with approximately 1 million labeled objects across 60 fine-grained classes. Compared to other RSOD datasets, the images in xView dataset are collected from WorldView-3 at 0.3m ground sample distance, providing higher resolution images. Moreover, the xView dataset cover over 1,400 km<sup id="S7.SS1.p7.1.2" class="ltx_sup">2</sup> of the earth’s surface, which leads to higher diversity.</p>
</div>
<div id="S7.SS1.p8" class="ltx_para">
<p id="S7.SS1.p8.1" class="ltx_p"><span id="S7.SS1.p8.1.1" class="ltx_text ltx_font_bold">DOTA<cite class="ltx_cite ltx_citemacro_cite"><span id="S7.SS1.p8.1.1.1.1" class="ltx_text ltx_font_medium">[</span><a href="#bib.bib117" title="" class="ltx_ref">117</a><span id="S7.SS1.p8.1.1.2.2" class="ltx_text ltx_font_medium">]</span></cite></span>. DOTA is a large-scale dataset consisting of 188,282 objects annotated with both HBB and OBB. All objects are divided into 15 categories: plane, ship, storage tank, baseball diamond, tennis court, swimming pool, ground track field, harbor, bridge, large vehicle, small vehicle, helicopter, roundabout, soccer ball field, and basketball court. The images in this dataset are collected from Google Earth, JL-1 satellite, and GF-2 satellite with a spatial resolution of 0.1 to 1 m. Recently, the latest DOTAv2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> has been publicly available, which contains over 1.7 million objects of 18 categories.</p>
</div>
<div id="S7.SS1.p9" class="ltx_para">
<p id="S7.SS1.p9.1" class="ltx_p"><span id="S7.SS1.p9.1.1" class="ltx_text ltx_font_bold">DIOR<cite class="ltx_cite ltx_citemacro_cite"><span id="S7.SS1.p9.1.1.1.1" class="ltx_text ltx_font_medium">[</span><a href="#bib.bib28" title="" class="ltx_ref">28</a><span id="S7.SS1.p9.1.1.2.2" class="ltx_text ltx_font_medium">]</span></cite></span>. DIOR is an object detection dataset for optical RSIs. There are 23,463 optical images in this dataset with a spatial resolution of 0.5 to 30m. The total number of objects in the dataset is 192,472, and all objects are labeled with HBB. The categories of objects are as follows: airplane, airport, baseball field, basketball court, bridge, chimney, dam, expressway service area, expressway toll station, harbor, golf course, ground track field, overpass, ship, stadium, storage tank, tennis court, train station, vehicle, and windmill.</p>
</div>
<div id="S7.SS1.p10" class="ltx_para">
<p id="S7.SS1.p10.1" class="ltx_p"><span id="S7.SS1.p10.1.1" class="ltx_text ltx_font_bold">FAIR1M<cite class="ltx_cite ltx_citemacro_cite"><span id="S7.SS1.p10.1.1.1.1" class="ltx_text ltx_font_medium">[</span><a href="#bib.bib34" title="" class="ltx_ref">34</a><span id="S7.SS1.p10.1.1.2.2" class="ltx_text ltx_font_medium">]</span></cite></span>. FAIR1M is a more challenging dataset for fine-grained object detection in RSIs, including 5 categories and 37 subcategories. There are more than 40,000 images and more than 1 million objects annotated by oriented bounding boxes. The images are acquired from multiple platforms with a resolution of 0.3 m to 0.8 m and are spread across different countries and regions. The fine-grained categories, massive numbers of objects, large ranges of sizes and orientations, and diverse scenes make the FAIR1M more challenging.</p>
</div>
<div id="S7.SS1.p11" class="ltx_para">
<p id="S7.SS1.p11.1" class="ltx_p"><span id="S7.SS1.p11.1.1" class="ltx_text ltx_font_bold">SODA-A</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib284" title="" class="ltx_ref">284</a>]</cite>. SODA-A is a recently released dataset designed for tiny object detection in RSIs. This dataset consists of 2,510 images with an average image size of <math id="S7.SS1.p11.1.m1.3" class="ltx_Math" alttext="4,761\times 2,777" display="inline"><semantics id="S7.SS1.p11.1.m1.3a"><mrow id="S7.SS1.p11.1.m1.3.3.1" xref="S7.SS1.p11.1.m1.3.3.2.cmml"><mn id="S7.SS1.p11.1.m1.1.1" xref="S7.SS1.p11.1.m1.1.1.cmml">4</mn><mo id="S7.SS1.p11.1.m1.3.3.1.2" xref="S7.SS1.p11.1.m1.3.3.2.cmml">,</mo><mrow id="S7.SS1.p11.1.m1.3.3.1.1" xref="S7.SS1.p11.1.m1.3.3.1.1.cmml"><mn id="S7.SS1.p11.1.m1.3.3.1.1.2" xref="S7.SS1.p11.1.m1.3.3.1.1.2.cmml">761</mn><mo lspace="0.222em" rspace="0.222em" id="S7.SS1.p11.1.m1.3.3.1.1.1" xref="S7.SS1.p11.1.m1.3.3.1.1.1.cmml">×</mo><mn id="S7.SS1.p11.1.m1.3.3.1.1.3" xref="S7.SS1.p11.1.m1.3.3.1.1.3.cmml">2</mn></mrow><mo id="S7.SS1.p11.1.m1.3.3.1.3" xref="S7.SS1.p11.1.m1.3.3.2.cmml">,</mo><mn id="S7.SS1.p11.1.m1.2.2" xref="S7.SS1.p11.1.m1.2.2.cmml">777</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p11.1.m1.3b"><list id="S7.SS1.p11.1.m1.3.3.2.cmml" xref="S7.SS1.p11.1.m1.3.3.1"><cn type="integer" id="S7.SS1.p11.1.m1.1.1.cmml" xref="S7.SS1.p11.1.m1.1.1">4</cn><apply id="S7.SS1.p11.1.m1.3.3.1.1.cmml" xref="S7.SS1.p11.1.m1.3.3.1.1"><times id="S7.SS1.p11.1.m1.3.3.1.1.1.cmml" xref="S7.SS1.p11.1.m1.3.3.1.1.1"></times><cn type="integer" id="S7.SS1.p11.1.m1.3.3.1.1.2.cmml" xref="S7.SS1.p11.1.m1.3.3.1.1.2">761</cn><cn type="integer" id="S7.SS1.p11.1.m1.3.3.1.1.3.cmml" xref="S7.SS1.p11.1.m1.3.3.1.1.3">2</cn></apply><cn type="integer" id="S7.SS1.p11.1.m1.2.2.cmml" xref="S7.SS1.p11.1.m1.2.2">777</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p11.1.m1.3c">4,761\times 2,777</annotation></semantics></math>, and 800,203 objects with OBB annotation. All objects are divided into four subsets (i.e., extremely small, relatively small, generally small, and normal) based on their area ranges. There are nine categories in this dataset, including airplane, helicopter, small-vehicle, large-vehicle, ship, container, storage-tank, swimming-pool, and windmill.</p>
</div>
<div id="S7.SS1.p12" class="ltx_para">
<p id="S7.SS1.p12.1" class="ltx_p">The above review shows that the early published datasets generally have limited samples. For example, NWPU VHR-10<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> only contains 10 categories and 3,651 instances, and UCAC-AOD<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib274" title="" class="ltx_ref">274</a>]</cite> consists of 2 categories with 6,029 instances. In recent years, researchers have not only introduced massive amounts of data and fine-grained level objects but also collected data from multi-sensor, various resolutions, and diverse scenes (e.g., DOTA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib117" title="" class="ltx_ref">117</a>]</cite>, DIOR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, FAIR1M<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>) to satisfy the practical applications in RSOD. Fig. <a href="#S7.F18" title="Figure 18 ‣ VII-A Datasets Introduction and Selection ‣ VII Datasets and Evaluation Metrics ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18</span></a> depicts the typical samples of different RSOD datasets.</p>
</div>
<div id="S7.SS1.p13" class="ltx_para">
<p id="S7.SS1.p13.3" class="ltx_p">We also provide the dataset selection guidelines in Table <a href="#S7.T4" title="TABLE IV ‣ VII-A Datasets Introduction and Selection ‣ VII Datasets and Evaluation Metrics ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> to help researchers select proper datasets and methods for different challenges and scenarios. Notably, only the image-level annotations of the datasets are available for the weakly supervision scenario. As for the few-shot supervision scenario, there are only <math id="S7.SS1.p13.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S7.SS1.p13.1.m1.1a"><mi id="S7.SS1.p13.1.m1.1.1" xref="S7.SS1.p13.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.p13.1.m1.1b"><ci id="S7.SS1.p13.1.m1.1.1.cmml" xref="S7.SS1.p13.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p13.1.m1.1c">K</annotation></semantics></math>-shot box-level annotated samples for each novel class, where <math id="S7.SS1.p13.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S7.SS1.p13.2.m2.1a"><mi id="S7.SS1.p13.2.m2.1.1" xref="S7.SS1.p13.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.p13.2.m2.1b"><ci id="S7.SS1.p13.2.m2.1.1.cmml" xref="S7.SS1.p13.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p13.2.m2.1c">K</annotation></semantics></math> is set to <math id="S7.SS1.p13.3.m3.5" class="ltx_Math" alttext="\left\{3,5,10,20,30\right\}" display="inline"><semantics id="S7.SS1.p13.3.m3.5a"><mrow id="S7.SS1.p13.3.m3.5.6.2" xref="S7.SS1.p13.3.m3.5.6.1.cmml"><mo id="S7.SS1.p13.3.m3.5.6.2.1" xref="S7.SS1.p13.3.m3.5.6.1.cmml">{</mo><mn id="S7.SS1.p13.3.m3.1.1" xref="S7.SS1.p13.3.m3.1.1.cmml">3</mn><mo id="S7.SS1.p13.3.m3.5.6.2.2" xref="S7.SS1.p13.3.m3.5.6.1.cmml">,</mo><mn id="S7.SS1.p13.3.m3.2.2" xref="S7.SS1.p13.3.m3.2.2.cmml">5</mn><mo id="S7.SS1.p13.3.m3.5.6.2.3" xref="S7.SS1.p13.3.m3.5.6.1.cmml">,</mo><mn id="S7.SS1.p13.3.m3.3.3" xref="S7.SS1.p13.3.m3.3.3.cmml">10</mn><mo id="S7.SS1.p13.3.m3.5.6.2.4" xref="S7.SS1.p13.3.m3.5.6.1.cmml">,</mo><mn id="S7.SS1.p13.3.m3.4.4" xref="S7.SS1.p13.3.m3.4.4.cmml">20</mn><mo id="S7.SS1.p13.3.m3.5.6.2.5" xref="S7.SS1.p13.3.m3.5.6.1.cmml">,</mo><mn id="S7.SS1.p13.3.m3.5.5" xref="S7.SS1.p13.3.m3.5.5.cmml">30</mn><mo id="S7.SS1.p13.3.m3.5.6.2.6" xref="S7.SS1.p13.3.m3.5.6.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p13.3.m3.5b"><set id="S7.SS1.p13.3.m3.5.6.1.cmml" xref="S7.SS1.p13.3.m3.5.6.2"><cn type="integer" id="S7.SS1.p13.3.m3.1.1.cmml" xref="S7.SS1.p13.3.m3.1.1">3</cn><cn type="integer" id="S7.SS1.p13.3.m3.2.2.cmml" xref="S7.SS1.p13.3.m3.2.2">5</cn><cn type="integer" id="S7.SS1.p13.3.m3.3.3.cmml" xref="S7.SS1.p13.3.m3.3.3">10</cn><cn type="integer" id="S7.SS1.p13.3.m3.4.4.cmml" xref="S7.SS1.p13.3.m3.4.4">20</cn><cn type="integer" id="S7.SS1.p13.3.m3.5.5.cmml" xref="S7.SS1.p13.3.m3.5.5">30</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p13.3.m3.5c">\left\{3,5,10,20,30\right\}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S7.SS2.4.1.1" class="ltx_text">VII-B</span> </span><span id="S7.SS2.5.2" class="ltx_text ltx_font_italic">Evaluation Metrics</span>
</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.1" class="ltx_p">In addition to the dataset, the evaluation metrics are equally important. Generally, the inference speed and the detection accuracy are the two commonly adopted metrics for evaluating the performance of detectors.</p>
</div>
<div id="S7.SS2.p2" class="ltx_para">
<p id="S7.SS2.p2.1" class="ltx_p"><span id="S7.SS2.p2.1.1" class="ltx_text ltx_font_bold">Frames Per Second</span> (FPS) is a standard metric for inference speed evaluation that indicates the number of images that the detector can detect per second. Notably, both the image size and hardware devices can influence the inference speed.</p>
</div>
<div id="S7.SS2.p3" class="ltx_para">
<p id="S7.SS2.p3.11" class="ltx_p"><span id="S7.SS2.p3.11.1" class="ltx_text ltx_font_bold">Average Precision</span> (AP) is the most commonly used metric for detection accuracy. Given a test image <math id="S7.SS2.p3.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S7.SS2.p3.1.m1.1a"><mi id="S7.SS2.p3.1.m1.1.1" xref="S7.SS2.p3.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.1.m1.1b"><ci id="S7.SS2.p3.1.m1.1.1.cmml" xref="S7.SS2.p3.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.1.m1.1c">I</annotation></semantics></math>, let <math id="S7.SS2.p3.2.m2.1" class="ltx_Math" alttext="\left\{\left(b_{i},c_{i},p_{i}\right)\right\}_{i=1}^{N}" display="inline"><semantics id="S7.SS2.p3.2.m2.1a"><msubsup id="S7.SS2.p3.2.m2.1.1" xref="S7.SS2.p3.2.m2.1.1.cmml"><mrow id="S7.SS2.p3.2.m2.1.1.1.1.1" xref="S7.SS2.p3.2.m2.1.1.1.1.2.cmml"><mo id="S7.SS2.p3.2.m2.1.1.1.1.1.2" xref="S7.SS2.p3.2.m2.1.1.1.1.2.cmml">{</mo><mrow id="S7.SS2.p3.2.m2.1.1.1.1.1.1.3" xref="S7.SS2.p3.2.m2.1.1.1.1.1.1.4.cmml"><mo id="S7.SS2.p3.2.m2.1.1.1.1.1.1.3.4" xref="S7.SS2.p3.2.m2.1.1.1.1.1.1.4.cmml">(</mo><msub id="S7.SS2.p3.2.m2.1.1.1.1.1.1.1.1" xref="S7.SS2.p3.2.m2.1.1.1.1.1.1.1.1.cmml"><mi id="S7.SS2.p3.2.m2.1.1.1.1.1.1.1.1.2" xref="S7.SS2.p3.2.m2.1.1.1.1.1.1.1.1.2.cmml">b</mi><mi id="S7.SS2.p3.2.m2.1.1.1.1.1.1.1.1.3" xref="S7.SS2.p3.2.m2.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S7.SS2.p3.2.m2.1.1.1.1.1.1.3.5" xref="S7.SS2.p3.2.m2.1.1.1.1.1.1.4.cmml">,</mo><msub id="S7.SS2.p3.2.m2.1.1.1.1.1.1.2.2" xref="S7.SS2.p3.2.m2.1.1.1.1.1.1.2.2.cmml"><mi id="S7.SS2.p3.2.m2.1.1.1.1.1.1.2.2.2" xref="S7.SS2.p3.2.m2.1.1.1.1.1.1.2.2.2.cmml">c</mi><mi id="S7.SS2.p3.2.m2.1.1.1.1.1.1.2.2.3" xref="S7.SS2.p3.2.m2.1.1.1.1.1.1.2.2.3.cmml">i</mi></msub><mo id="S7.SS2.p3.2.m2.1.1.1.1.1.1.3.6" xref="S7.SS2.p3.2.m2.1.1.1.1.1.1.4.cmml">,</mo><msub id="S7.SS2.p3.2.m2.1.1.1.1.1.1.3.3" xref="S7.SS2.p3.2.m2.1.1.1.1.1.1.3.3.cmml"><mi id="S7.SS2.p3.2.m2.1.1.1.1.1.1.3.3.2" xref="S7.SS2.p3.2.m2.1.1.1.1.1.1.3.3.2.cmml">p</mi><mi id="S7.SS2.p3.2.m2.1.1.1.1.1.1.3.3.3" xref="S7.SS2.p3.2.m2.1.1.1.1.1.1.3.3.3.cmml">i</mi></msub><mo id="S7.SS2.p3.2.m2.1.1.1.1.1.1.3.7" xref="S7.SS2.p3.2.m2.1.1.1.1.1.1.4.cmml">)</mo></mrow><mo id="S7.SS2.p3.2.m2.1.1.1.1.1.3" xref="S7.SS2.p3.2.m2.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S7.SS2.p3.2.m2.1.1.1.3" xref="S7.SS2.p3.2.m2.1.1.1.3.cmml"><mi id="S7.SS2.p3.2.m2.1.1.1.3.2" xref="S7.SS2.p3.2.m2.1.1.1.3.2.cmml">i</mi><mo id="S7.SS2.p3.2.m2.1.1.1.3.1" xref="S7.SS2.p3.2.m2.1.1.1.3.1.cmml">=</mo><mn id="S7.SS2.p3.2.m2.1.1.1.3.3" xref="S7.SS2.p3.2.m2.1.1.1.3.3.cmml">1</mn></mrow><mi id="S7.SS2.p3.2.m2.1.1.3" xref="S7.SS2.p3.2.m2.1.1.3.cmml">N</mi></msubsup><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.2.m2.1b"><apply id="S7.SS2.p3.2.m2.1.1.cmml" xref="S7.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S7.SS2.p3.2.m2.1.1.2.cmml" xref="S7.SS2.p3.2.m2.1.1">superscript</csymbol><apply id="S7.SS2.p3.2.m2.1.1.1.cmml" xref="S7.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S7.SS2.p3.2.m2.1.1.1.2.cmml" xref="S7.SS2.p3.2.m2.1.1">subscript</csymbol><set id="S7.SS2.p3.2.m2.1.1.1.1.2.cmml" xref="S7.SS2.p3.2.m2.1.1.1.1.1"><vector id="S7.SS2.p3.2.m2.1.1.1.1.1.1.4.cmml" xref="S7.SS2.p3.2.m2.1.1.1.1.1.1.3"><apply id="S7.SS2.p3.2.m2.1.1.1.1.1.1.1.1.cmml" xref="S7.SS2.p3.2.m2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S7.SS2.p3.2.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S7.SS2.p3.2.m2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S7.SS2.p3.2.m2.1.1.1.1.1.1.1.1.2.cmml" xref="S7.SS2.p3.2.m2.1.1.1.1.1.1.1.1.2">𝑏</ci><ci id="S7.SS2.p3.2.m2.1.1.1.1.1.1.1.1.3.cmml" xref="S7.SS2.p3.2.m2.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S7.SS2.p3.2.m2.1.1.1.1.1.1.2.2.cmml" xref="S7.SS2.p3.2.m2.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S7.SS2.p3.2.m2.1.1.1.1.1.1.2.2.1.cmml" xref="S7.SS2.p3.2.m2.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S7.SS2.p3.2.m2.1.1.1.1.1.1.2.2.2.cmml" xref="S7.SS2.p3.2.m2.1.1.1.1.1.1.2.2.2">𝑐</ci><ci id="S7.SS2.p3.2.m2.1.1.1.1.1.1.2.2.3.cmml" xref="S7.SS2.p3.2.m2.1.1.1.1.1.1.2.2.3">𝑖</ci></apply><apply id="S7.SS2.p3.2.m2.1.1.1.1.1.1.3.3.cmml" xref="S7.SS2.p3.2.m2.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S7.SS2.p3.2.m2.1.1.1.1.1.1.3.3.1.cmml" xref="S7.SS2.p3.2.m2.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S7.SS2.p3.2.m2.1.1.1.1.1.1.3.3.2.cmml" xref="S7.SS2.p3.2.m2.1.1.1.1.1.1.3.3.2">𝑝</ci><ci id="S7.SS2.p3.2.m2.1.1.1.1.1.1.3.3.3.cmml" xref="S7.SS2.p3.2.m2.1.1.1.1.1.1.3.3.3">𝑖</ci></apply></vector></set><apply id="S7.SS2.p3.2.m2.1.1.1.3.cmml" xref="S7.SS2.p3.2.m2.1.1.1.3"><eq id="S7.SS2.p3.2.m2.1.1.1.3.1.cmml" xref="S7.SS2.p3.2.m2.1.1.1.3.1"></eq><ci id="S7.SS2.p3.2.m2.1.1.1.3.2.cmml" xref="S7.SS2.p3.2.m2.1.1.1.3.2">𝑖</ci><cn type="integer" id="S7.SS2.p3.2.m2.1.1.1.3.3.cmml" xref="S7.SS2.p3.2.m2.1.1.1.3.3">1</cn></apply></apply><ci id="S7.SS2.p3.2.m2.1.1.3.cmml" xref="S7.SS2.p3.2.m2.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.2.m2.1c">\left\{\left(b_{i},c_{i},p_{i}\right)\right\}_{i=1}^{N}</annotation></semantics></math> denotes the prediction detections, where <math id="S7.SS2.p3.3.m3.1" class="ltx_Math" alttext="b_{i}" display="inline"><semantics id="S7.SS2.p3.3.m3.1a"><msub id="S7.SS2.p3.3.m3.1.1" xref="S7.SS2.p3.3.m3.1.1.cmml"><mi id="S7.SS2.p3.3.m3.1.1.2" xref="S7.SS2.p3.3.m3.1.1.2.cmml">b</mi><mi id="S7.SS2.p3.3.m3.1.1.3" xref="S7.SS2.p3.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.3.m3.1b"><apply id="S7.SS2.p3.3.m3.1.1.cmml" xref="S7.SS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S7.SS2.p3.3.m3.1.1.1.cmml" xref="S7.SS2.p3.3.m3.1.1">subscript</csymbol><ci id="S7.SS2.p3.3.m3.1.1.2.cmml" xref="S7.SS2.p3.3.m3.1.1.2">𝑏</ci><ci id="S7.SS2.p3.3.m3.1.1.3.cmml" xref="S7.SS2.p3.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.3.m3.1c">b_{i}</annotation></semantics></math> is the predicted box, <math id="S7.SS2.p3.4.m4.1" class="ltx_Math" alttext="c_{i}" display="inline"><semantics id="S7.SS2.p3.4.m4.1a"><msub id="S7.SS2.p3.4.m4.1.1" xref="S7.SS2.p3.4.m4.1.1.cmml"><mi id="S7.SS2.p3.4.m4.1.1.2" xref="S7.SS2.p3.4.m4.1.1.2.cmml">c</mi><mi id="S7.SS2.p3.4.m4.1.1.3" xref="S7.SS2.p3.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.4.m4.1b"><apply id="S7.SS2.p3.4.m4.1.1.cmml" xref="S7.SS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S7.SS2.p3.4.m4.1.1.1.cmml" xref="S7.SS2.p3.4.m4.1.1">subscript</csymbol><ci id="S7.SS2.p3.4.m4.1.1.2.cmml" xref="S7.SS2.p3.4.m4.1.1.2">𝑐</ci><ci id="S7.SS2.p3.4.m4.1.1.3.cmml" xref="S7.SS2.p3.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.4.m4.1c">c_{i}</annotation></semantics></math> is the predicted label, and <math id="S7.SS2.p3.5.m5.1" class="ltx_Math" alttext="p_{i}" display="inline"><semantics id="S7.SS2.p3.5.m5.1a"><msub id="S7.SS2.p3.5.m5.1.1" xref="S7.SS2.p3.5.m5.1.1.cmml"><mi id="S7.SS2.p3.5.m5.1.1.2" xref="S7.SS2.p3.5.m5.1.1.2.cmml">p</mi><mi id="S7.SS2.p3.5.m5.1.1.3" xref="S7.SS2.p3.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.5.m5.1b"><apply id="S7.SS2.p3.5.m5.1.1.cmml" xref="S7.SS2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S7.SS2.p3.5.m5.1.1.1.cmml" xref="S7.SS2.p3.5.m5.1.1">subscript</csymbol><ci id="S7.SS2.p3.5.m5.1.1.2.cmml" xref="S7.SS2.p3.5.m5.1.1.2">𝑝</ci><ci id="S7.SS2.p3.5.m5.1.1.3.cmml" xref="S7.SS2.p3.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.5.m5.1c">p_{i}</annotation></semantics></math> is the confidence score. Let <math id="S7.SS2.p3.6.m6.1" class="ltx_Math" alttext="\left\{\left(b^{gt}_{j},c^{gt}_{j}\right)\right\}_{j=1}^{M}" display="inline"><semantics id="S7.SS2.p3.6.m6.1a"><msubsup id="S7.SS2.p3.6.m6.1.1" xref="S7.SS2.p3.6.m6.1.1.cmml"><mrow id="S7.SS2.p3.6.m6.1.1.1.1.1" xref="S7.SS2.p3.6.m6.1.1.1.1.2.cmml"><mo id="S7.SS2.p3.6.m6.1.1.1.1.1.2" xref="S7.SS2.p3.6.m6.1.1.1.1.2.cmml">{</mo><mrow id="S7.SS2.p3.6.m6.1.1.1.1.1.1.2" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.3.cmml"><mo id="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.3" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.3.cmml">(</mo><msubsup id="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.cmml"><mi id="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.2.2" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.2.2.cmml">b</mi><mi id="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.3" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.3.cmml">j</mi><mrow id="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.2.3" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.2.3.2" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.2.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.2.3.1" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.2.3.3" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.2.3.3.cmml">t</mi></mrow></msubsup><mo id="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.4" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.3.cmml">,</mo><msubsup id="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.cmml"><mi id="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.2.2" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.2.2.cmml">c</mi><mi id="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.3" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.3.cmml">j</mi><mrow id="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.2.3" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.2.3.cmml"><mi id="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.2.3.2" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.2.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.2.3.1" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.2.3.1.cmml">​</mo><mi id="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.2.3.3" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.2.3.3.cmml">t</mi></mrow></msubsup><mo id="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.5" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.3.cmml">)</mo></mrow><mo id="S7.SS2.p3.6.m6.1.1.1.1.1.3" xref="S7.SS2.p3.6.m6.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S7.SS2.p3.6.m6.1.1.1.3" xref="S7.SS2.p3.6.m6.1.1.1.3.cmml"><mi id="S7.SS2.p3.6.m6.1.1.1.3.2" xref="S7.SS2.p3.6.m6.1.1.1.3.2.cmml">j</mi><mo id="S7.SS2.p3.6.m6.1.1.1.3.1" xref="S7.SS2.p3.6.m6.1.1.1.3.1.cmml">=</mo><mn id="S7.SS2.p3.6.m6.1.1.1.3.3" xref="S7.SS2.p3.6.m6.1.1.1.3.3.cmml">1</mn></mrow><mi id="S7.SS2.p3.6.m6.1.1.3" xref="S7.SS2.p3.6.m6.1.1.3.cmml">M</mi></msubsup><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.6.m6.1b"><apply id="S7.SS2.p3.6.m6.1.1.cmml" xref="S7.SS2.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S7.SS2.p3.6.m6.1.1.2.cmml" xref="S7.SS2.p3.6.m6.1.1">superscript</csymbol><apply id="S7.SS2.p3.6.m6.1.1.1.cmml" xref="S7.SS2.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S7.SS2.p3.6.m6.1.1.1.2.cmml" xref="S7.SS2.p3.6.m6.1.1">subscript</csymbol><set id="S7.SS2.p3.6.m6.1.1.1.1.2.cmml" xref="S7.SS2.p3.6.m6.1.1.1.1.1"><interval closure="open" id="S7.SS2.p3.6.m6.1.1.1.1.1.1.3.cmml" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.2"><apply id="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.cmml" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.1.cmml" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.2.cmml" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.2.1.cmml" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.2.2.cmml" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.2.2">𝑏</ci><apply id="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.2.3.cmml" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.2.3"><times id="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.2.3.1"></times><ci id="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.2.3.2">𝑔</ci><ci id="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.2.3.3">𝑡</ci></apply></apply><ci id="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.3.cmml" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.1.1.3">𝑗</ci></apply><apply id="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.cmml" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.1.cmml" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2">subscript</csymbol><apply id="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.2.cmml" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.2.1.cmml" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2">superscript</csymbol><ci id="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.2.2.cmml" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.2.2">𝑐</ci><apply id="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.2.3.cmml" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.2.3"><times id="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.2.3.1.cmml" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.2.3.1"></times><ci id="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.2.3.2.cmml" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.2.3.2">𝑔</ci><ci id="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.2.3.3.cmml" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.2.3.3">𝑡</ci></apply></apply><ci id="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.3.cmml" xref="S7.SS2.p3.6.m6.1.1.1.1.1.1.2.2.3">𝑗</ci></apply></interval></set><apply id="S7.SS2.p3.6.m6.1.1.1.3.cmml" xref="S7.SS2.p3.6.m6.1.1.1.3"><eq id="S7.SS2.p3.6.m6.1.1.1.3.1.cmml" xref="S7.SS2.p3.6.m6.1.1.1.3.1"></eq><ci id="S7.SS2.p3.6.m6.1.1.1.3.2.cmml" xref="S7.SS2.p3.6.m6.1.1.1.3.2">𝑗</ci><cn type="integer" id="S7.SS2.p3.6.m6.1.1.1.3.3.cmml" xref="S7.SS2.p3.6.m6.1.1.1.3.3">1</cn></apply></apply><ci id="S7.SS2.p3.6.m6.1.1.3.cmml" xref="S7.SS2.p3.6.m6.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.6.m6.1c">\left\{\left(b^{gt}_{j},c^{gt}_{j}\right)\right\}_{j=1}^{M}</annotation></semantics></math> refers to the ground truth annotations on the test image <math id="S7.SS2.p3.7.m7.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S7.SS2.p3.7.m7.1a"><mi id="S7.SS2.p3.7.m7.1.1" xref="S7.SS2.p3.7.m7.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.7.m7.1b"><ci id="S7.SS2.p3.7.m7.1.1.cmml" xref="S7.SS2.p3.7.m7.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.7.m7.1c">I</annotation></semantics></math>, where <math id="S7.SS2.p3.8.m8.1" class="ltx_Math" alttext="b^{gt}_{j}" display="inline"><semantics id="S7.SS2.p3.8.m8.1a"><msubsup id="S7.SS2.p3.8.m8.1.1" xref="S7.SS2.p3.8.m8.1.1.cmml"><mi id="S7.SS2.p3.8.m8.1.1.2.2" xref="S7.SS2.p3.8.m8.1.1.2.2.cmml">b</mi><mi id="S7.SS2.p3.8.m8.1.1.3" xref="S7.SS2.p3.8.m8.1.1.3.cmml">j</mi><mrow id="S7.SS2.p3.8.m8.1.1.2.3" xref="S7.SS2.p3.8.m8.1.1.2.3.cmml"><mi id="S7.SS2.p3.8.m8.1.1.2.3.2" xref="S7.SS2.p3.8.m8.1.1.2.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p3.8.m8.1.1.2.3.1" xref="S7.SS2.p3.8.m8.1.1.2.3.1.cmml">​</mo><mi id="S7.SS2.p3.8.m8.1.1.2.3.3" xref="S7.SS2.p3.8.m8.1.1.2.3.3.cmml">t</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.8.m8.1b"><apply id="S7.SS2.p3.8.m8.1.1.cmml" xref="S7.SS2.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S7.SS2.p3.8.m8.1.1.1.cmml" xref="S7.SS2.p3.8.m8.1.1">subscript</csymbol><apply id="S7.SS2.p3.8.m8.1.1.2.cmml" xref="S7.SS2.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S7.SS2.p3.8.m8.1.1.2.1.cmml" xref="S7.SS2.p3.8.m8.1.1">superscript</csymbol><ci id="S7.SS2.p3.8.m8.1.1.2.2.cmml" xref="S7.SS2.p3.8.m8.1.1.2.2">𝑏</ci><apply id="S7.SS2.p3.8.m8.1.1.2.3.cmml" xref="S7.SS2.p3.8.m8.1.1.2.3"><times id="S7.SS2.p3.8.m8.1.1.2.3.1.cmml" xref="S7.SS2.p3.8.m8.1.1.2.3.1"></times><ci id="S7.SS2.p3.8.m8.1.1.2.3.2.cmml" xref="S7.SS2.p3.8.m8.1.1.2.3.2">𝑔</ci><ci id="S7.SS2.p3.8.m8.1.1.2.3.3.cmml" xref="S7.SS2.p3.8.m8.1.1.2.3.3">𝑡</ci></apply></apply><ci id="S7.SS2.p3.8.m8.1.1.3.cmml" xref="S7.SS2.p3.8.m8.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.8.m8.1c">b^{gt}_{j}</annotation></semantics></math> is the ground-truth box, and <math id="S7.SS2.p3.9.m9.1" class="ltx_Math" alttext="c^{gt}_{j}" display="inline"><semantics id="S7.SS2.p3.9.m9.1a"><msubsup id="S7.SS2.p3.9.m9.1.1" xref="S7.SS2.p3.9.m9.1.1.cmml"><mi id="S7.SS2.p3.9.m9.1.1.2.2" xref="S7.SS2.p3.9.m9.1.1.2.2.cmml">c</mi><mi id="S7.SS2.p3.9.m9.1.1.3" xref="S7.SS2.p3.9.m9.1.1.3.cmml">j</mi><mrow id="S7.SS2.p3.9.m9.1.1.2.3" xref="S7.SS2.p3.9.m9.1.1.2.3.cmml"><mi id="S7.SS2.p3.9.m9.1.1.2.3.2" xref="S7.SS2.p3.9.m9.1.1.2.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p3.9.m9.1.1.2.3.1" xref="S7.SS2.p3.9.m9.1.1.2.3.1.cmml">​</mo><mi id="S7.SS2.p3.9.m9.1.1.2.3.3" xref="S7.SS2.p3.9.m9.1.1.2.3.3.cmml">t</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.9.m9.1b"><apply id="S7.SS2.p3.9.m9.1.1.cmml" xref="S7.SS2.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S7.SS2.p3.9.m9.1.1.1.cmml" xref="S7.SS2.p3.9.m9.1.1">subscript</csymbol><apply id="S7.SS2.p3.9.m9.1.1.2.cmml" xref="S7.SS2.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S7.SS2.p3.9.m9.1.1.2.1.cmml" xref="S7.SS2.p3.9.m9.1.1">superscript</csymbol><ci id="S7.SS2.p3.9.m9.1.1.2.2.cmml" xref="S7.SS2.p3.9.m9.1.1.2.2">𝑐</ci><apply id="S7.SS2.p3.9.m9.1.1.2.3.cmml" xref="S7.SS2.p3.9.m9.1.1.2.3"><times id="S7.SS2.p3.9.m9.1.1.2.3.1.cmml" xref="S7.SS2.p3.9.m9.1.1.2.3.1"></times><ci id="S7.SS2.p3.9.m9.1.1.2.3.2.cmml" xref="S7.SS2.p3.9.m9.1.1.2.3.2">𝑔</ci><ci id="S7.SS2.p3.9.m9.1.1.2.3.3.cmml" xref="S7.SS2.p3.9.m9.1.1.2.3.3">𝑡</ci></apply></apply><ci id="S7.SS2.p3.9.m9.1.1.3.cmml" xref="S7.SS2.p3.9.m9.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.9.m9.1c">c^{gt}_{j}</annotation></semantics></math> is the ground truth category. A prediction detection <math id="S7.SS2.p3.10.m10.3" class="ltx_Math" alttext="\left(b_{i},c_{i},p_{i}\right)" display="inline"><semantics id="S7.SS2.p3.10.m10.3a"><mrow id="S7.SS2.p3.10.m10.3.3.3" xref="S7.SS2.p3.10.m10.3.3.4.cmml"><mo id="S7.SS2.p3.10.m10.3.3.3.4" xref="S7.SS2.p3.10.m10.3.3.4.cmml">(</mo><msub id="S7.SS2.p3.10.m10.1.1.1.1" xref="S7.SS2.p3.10.m10.1.1.1.1.cmml"><mi id="S7.SS2.p3.10.m10.1.1.1.1.2" xref="S7.SS2.p3.10.m10.1.1.1.1.2.cmml">b</mi><mi id="S7.SS2.p3.10.m10.1.1.1.1.3" xref="S7.SS2.p3.10.m10.1.1.1.1.3.cmml">i</mi></msub><mo id="S7.SS2.p3.10.m10.3.3.3.5" xref="S7.SS2.p3.10.m10.3.3.4.cmml">,</mo><msub id="S7.SS2.p3.10.m10.2.2.2.2" xref="S7.SS2.p3.10.m10.2.2.2.2.cmml"><mi id="S7.SS2.p3.10.m10.2.2.2.2.2" xref="S7.SS2.p3.10.m10.2.2.2.2.2.cmml">c</mi><mi id="S7.SS2.p3.10.m10.2.2.2.2.3" xref="S7.SS2.p3.10.m10.2.2.2.2.3.cmml">i</mi></msub><mo id="S7.SS2.p3.10.m10.3.3.3.6" xref="S7.SS2.p3.10.m10.3.3.4.cmml">,</mo><msub id="S7.SS2.p3.10.m10.3.3.3.3" xref="S7.SS2.p3.10.m10.3.3.3.3.cmml"><mi id="S7.SS2.p3.10.m10.3.3.3.3.2" xref="S7.SS2.p3.10.m10.3.3.3.3.2.cmml">p</mi><mi id="S7.SS2.p3.10.m10.3.3.3.3.3" xref="S7.SS2.p3.10.m10.3.3.3.3.3.cmml">i</mi></msub><mo id="S7.SS2.p3.10.m10.3.3.3.7" xref="S7.SS2.p3.10.m10.3.3.4.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.10.m10.3b"><vector id="S7.SS2.p3.10.m10.3.3.4.cmml" xref="S7.SS2.p3.10.m10.3.3.3"><apply id="S7.SS2.p3.10.m10.1.1.1.1.cmml" xref="S7.SS2.p3.10.m10.1.1.1.1"><csymbol cd="ambiguous" id="S7.SS2.p3.10.m10.1.1.1.1.1.cmml" xref="S7.SS2.p3.10.m10.1.1.1.1">subscript</csymbol><ci id="S7.SS2.p3.10.m10.1.1.1.1.2.cmml" xref="S7.SS2.p3.10.m10.1.1.1.1.2">𝑏</ci><ci id="S7.SS2.p3.10.m10.1.1.1.1.3.cmml" xref="S7.SS2.p3.10.m10.1.1.1.1.3">𝑖</ci></apply><apply id="S7.SS2.p3.10.m10.2.2.2.2.cmml" xref="S7.SS2.p3.10.m10.2.2.2.2"><csymbol cd="ambiguous" id="S7.SS2.p3.10.m10.2.2.2.2.1.cmml" xref="S7.SS2.p3.10.m10.2.2.2.2">subscript</csymbol><ci id="S7.SS2.p3.10.m10.2.2.2.2.2.cmml" xref="S7.SS2.p3.10.m10.2.2.2.2.2">𝑐</ci><ci id="S7.SS2.p3.10.m10.2.2.2.2.3.cmml" xref="S7.SS2.p3.10.m10.2.2.2.2.3">𝑖</ci></apply><apply id="S7.SS2.p3.10.m10.3.3.3.3.cmml" xref="S7.SS2.p3.10.m10.3.3.3.3"><csymbol cd="ambiguous" id="S7.SS2.p3.10.m10.3.3.3.3.1.cmml" xref="S7.SS2.p3.10.m10.3.3.3.3">subscript</csymbol><ci id="S7.SS2.p3.10.m10.3.3.3.3.2.cmml" xref="S7.SS2.p3.10.m10.3.3.3.3.2">𝑝</ci><ci id="S7.SS2.p3.10.m10.3.3.3.3.3.cmml" xref="S7.SS2.p3.10.m10.3.3.3.3.3">𝑖</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.10.m10.3c">\left(b_{i},c_{i},p_{i}\right)</annotation></semantics></math> is assigned as a True Positive (TP) for ground truth annotation <math id="S7.SS2.p3.11.m11.2" class="ltx_Math" alttext="\left(b^{gt}_{j},c^{gt}_{j}\right)" display="inline"><semantics id="S7.SS2.p3.11.m11.2a"><mrow id="S7.SS2.p3.11.m11.2.2.2" xref="S7.SS2.p3.11.m11.2.2.3.cmml"><mo id="S7.SS2.p3.11.m11.2.2.2.3" xref="S7.SS2.p3.11.m11.2.2.3.cmml">(</mo><msubsup id="S7.SS2.p3.11.m11.1.1.1.1" xref="S7.SS2.p3.11.m11.1.1.1.1.cmml"><mi id="S7.SS2.p3.11.m11.1.1.1.1.2.2" xref="S7.SS2.p3.11.m11.1.1.1.1.2.2.cmml">b</mi><mi id="S7.SS2.p3.11.m11.1.1.1.1.3" xref="S7.SS2.p3.11.m11.1.1.1.1.3.cmml">j</mi><mrow id="S7.SS2.p3.11.m11.1.1.1.1.2.3" xref="S7.SS2.p3.11.m11.1.1.1.1.2.3.cmml"><mi id="S7.SS2.p3.11.m11.1.1.1.1.2.3.2" xref="S7.SS2.p3.11.m11.1.1.1.1.2.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p3.11.m11.1.1.1.1.2.3.1" xref="S7.SS2.p3.11.m11.1.1.1.1.2.3.1.cmml">​</mo><mi id="S7.SS2.p3.11.m11.1.1.1.1.2.3.3" xref="S7.SS2.p3.11.m11.1.1.1.1.2.3.3.cmml">t</mi></mrow></msubsup><mo id="S7.SS2.p3.11.m11.2.2.2.4" xref="S7.SS2.p3.11.m11.2.2.3.cmml">,</mo><msubsup id="S7.SS2.p3.11.m11.2.2.2.2" xref="S7.SS2.p3.11.m11.2.2.2.2.cmml"><mi id="S7.SS2.p3.11.m11.2.2.2.2.2.2" xref="S7.SS2.p3.11.m11.2.2.2.2.2.2.cmml">c</mi><mi id="S7.SS2.p3.11.m11.2.2.2.2.3" xref="S7.SS2.p3.11.m11.2.2.2.2.3.cmml">j</mi><mrow id="S7.SS2.p3.11.m11.2.2.2.2.2.3" xref="S7.SS2.p3.11.m11.2.2.2.2.2.3.cmml"><mi id="S7.SS2.p3.11.m11.2.2.2.2.2.3.2" xref="S7.SS2.p3.11.m11.2.2.2.2.2.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p3.11.m11.2.2.2.2.2.3.1" xref="S7.SS2.p3.11.m11.2.2.2.2.2.3.1.cmml">​</mo><mi id="S7.SS2.p3.11.m11.2.2.2.2.2.3.3" xref="S7.SS2.p3.11.m11.2.2.2.2.2.3.3.cmml">t</mi></mrow></msubsup><mo id="S7.SS2.p3.11.m11.2.2.2.5" xref="S7.SS2.p3.11.m11.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.11.m11.2b"><interval closure="open" id="S7.SS2.p3.11.m11.2.2.3.cmml" xref="S7.SS2.p3.11.m11.2.2.2"><apply id="S7.SS2.p3.11.m11.1.1.1.1.cmml" xref="S7.SS2.p3.11.m11.1.1.1.1"><csymbol cd="ambiguous" id="S7.SS2.p3.11.m11.1.1.1.1.1.cmml" xref="S7.SS2.p3.11.m11.1.1.1.1">subscript</csymbol><apply id="S7.SS2.p3.11.m11.1.1.1.1.2.cmml" xref="S7.SS2.p3.11.m11.1.1.1.1"><csymbol cd="ambiguous" id="S7.SS2.p3.11.m11.1.1.1.1.2.1.cmml" xref="S7.SS2.p3.11.m11.1.1.1.1">superscript</csymbol><ci id="S7.SS2.p3.11.m11.1.1.1.1.2.2.cmml" xref="S7.SS2.p3.11.m11.1.1.1.1.2.2">𝑏</ci><apply id="S7.SS2.p3.11.m11.1.1.1.1.2.3.cmml" xref="S7.SS2.p3.11.m11.1.1.1.1.2.3"><times id="S7.SS2.p3.11.m11.1.1.1.1.2.3.1.cmml" xref="S7.SS2.p3.11.m11.1.1.1.1.2.3.1"></times><ci id="S7.SS2.p3.11.m11.1.1.1.1.2.3.2.cmml" xref="S7.SS2.p3.11.m11.1.1.1.1.2.3.2">𝑔</ci><ci id="S7.SS2.p3.11.m11.1.1.1.1.2.3.3.cmml" xref="S7.SS2.p3.11.m11.1.1.1.1.2.3.3">𝑡</ci></apply></apply><ci id="S7.SS2.p3.11.m11.1.1.1.1.3.cmml" xref="S7.SS2.p3.11.m11.1.1.1.1.3">𝑗</ci></apply><apply id="S7.SS2.p3.11.m11.2.2.2.2.cmml" xref="S7.SS2.p3.11.m11.2.2.2.2"><csymbol cd="ambiguous" id="S7.SS2.p3.11.m11.2.2.2.2.1.cmml" xref="S7.SS2.p3.11.m11.2.2.2.2">subscript</csymbol><apply id="S7.SS2.p3.11.m11.2.2.2.2.2.cmml" xref="S7.SS2.p3.11.m11.2.2.2.2"><csymbol cd="ambiguous" id="S7.SS2.p3.11.m11.2.2.2.2.2.1.cmml" xref="S7.SS2.p3.11.m11.2.2.2.2">superscript</csymbol><ci id="S7.SS2.p3.11.m11.2.2.2.2.2.2.cmml" xref="S7.SS2.p3.11.m11.2.2.2.2.2.2">𝑐</ci><apply id="S7.SS2.p3.11.m11.2.2.2.2.2.3.cmml" xref="S7.SS2.p3.11.m11.2.2.2.2.2.3"><times id="S7.SS2.p3.11.m11.2.2.2.2.2.3.1.cmml" xref="S7.SS2.p3.11.m11.2.2.2.2.2.3.1"></times><ci id="S7.SS2.p3.11.m11.2.2.2.2.2.3.2.cmml" xref="S7.SS2.p3.11.m11.2.2.2.2.2.3.2">𝑔</ci><ci id="S7.SS2.p3.11.m11.2.2.2.2.2.3.3.cmml" xref="S7.SS2.p3.11.m11.2.2.2.2.2.3.3">𝑡</ci></apply></apply><ci id="S7.SS2.p3.11.m11.2.2.2.2.3.cmml" xref="S7.SS2.p3.11.m11.2.2.2.2.3">𝑗</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.11.m11.2c">\left(b^{gt}_{j},c^{gt}_{j}\right)</annotation></semantics></math>, if it meets both of the following criteria:</p>
<ul id="S7.I1" class="ltx_itemize">
<li id="S7.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i1.p1" class="ltx_para">
<p id="S7.I1.i1.p1.3" class="ltx_p">The confidence score <math id="S7.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="p_{i}" display="inline"><semantics id="S7.I1.i1.p1.1.m1.1a"><msub id="S7.I1.i1.p1.1.m1.1.1" xref="S7.I1.i1.p1.1.m1.1.1.cmml"><mi id="S7.I1.i1.p1.1.m1.1.1.2" xref="S7.I1.i1.p1.1.m1.1.1.2.cmml">p</mi><mi id="S7.I1.i1.p1.1.m1.1.1.3" xref="S7.I1.i1.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S7.I1.i1.p1.1.m1.1b"><apply id="S7.I1.i1.p1.1.m1.1.1.cmml" xref="S7.I1.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S7.I1.i1.p1.1.m1.1.1.1.cmml" xref="S7.I1.i1.p1.1.m1.1.1">subscript</csymbol><ci id="S7.I1.i1.p1.1.m1.1.1.2.cmml" xref="S7.I1.i1.p1.1.m1.1.1.2">𝑝</ci><ci id="S7.I1.i1.p1.1.m1.1.1.3.cmml" xref="S7.I1.i1.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.I1.i1.p1.1.m1.1c">p_{i}</annotation></semantics></math> is greater than the confidence threshold <math id="S7.I1.i1.p1.2.m2.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S7.I1.i1.p1.2.m2.1a"><mi id="S7.I1.i1.p1.2.m2.1.1" xref="S7.I1.i1.p1.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S7.I1.i1.p1.2.m2.1b"><ci id="S7.I1.i1.p1.2.m2.1.1.cmml" xref="S7.I1.i1.p1.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.I1.i1.p1.2.m2.1c">t</annotation></semantics></math>, and the predicted label is same as the ground truth label <math id="S7.I1.i1.p1.3.m3.1" class="ltx_Math" alttext="c^{gt}_{j}" display="inline"><semantics id="S7.I1.i1.p1.3.m3.1a"><msubsup id="S7.I1.i1.p1.3.m3.1.1" xref="S7.I1.i1.p1.3.m3.1.1.cmml"><mi id="S7.I1.i1.p1.3.m3.1.1.2.2" xref="S7.I1.i1.p1.3.m3.1.1.2.2.cmml">c</mi><mi id="S7.I1.i1.p1.3.m3.1.1.3" xref="S7.I1.i1.p1.3.m3.1.1.3.cmml">j</mi><mrow id="S7.I1.i1.p1.3.m3.1.1.2.3" xref="S7.I1.i1.p1.3.m3.1.1.2.3.cmml"><mi id="S7.I1.i1.p1.3.m3.1.1.2.3.2" xref="S7.I1.i1.p1.3.m3.1.1.2.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S7.I1.i1.p1.3.m3.1.1.2.3.1" xref="S7.I1.i1.p1.3.m3.1.1.2.3.1.cmml">​</mo><mi id="S7.I1.i1.p1.3.m3.1.1.2.3.3" xref="S7.I1.i1.p1.3.m3.1.1.2.3.3.cmml">t</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S7.I1.i1.p1.3.m3.1b"><apply id="S7.I1.i1.p1.3.m3.1.1.cmml" xref="S7.I1.i1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S7.I1.i1.p1.3.m3.1.1.1.cmml" xref="S7.I1.i1.p1.3.m3.1.1">subscript</csymbol><apply id="S7.I1.i1.p1.3.m3.1.1.2.cmml" xref="S7.I1.i1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S7.I1.i1.p1.3.m3.1.1.2.1.cmml" xref="S7.I1.i1.p1.3.m3.1.1">superscript</csymbol><ci id="S7.I1.i1.p1.3.m3.1.1.2.2.cmml" xref="S7.I1.i1.p1.3.m3.1.1.2.2">𝑐</ci><apply id="S7.I1.i1.p1.3.m3.1.1.2.3.cmml" xref="S7.I1.i1.p1.3.m3.1.1.2.3"><times id="S7.I1.i1.p1.3.m3.1.1.2.3.1.cmml" xref="S7.I1.i1.p1.3.m3.1.1.2.3.1"></times><ci id="S7.I1.i1.p1.3.m3.1.1.2.3.2.cmml" xref="S7.I1.i1.p1.3.m3.1.1.2.3.2">𝑔</ci><ci id="S7.I1.i1.p1.3.m3.1.1.2.3.3.cmml" xref="S7.I1.i1.p1.3.m3.1.1.2.3.3">𝑡</ci></apply></apply><ci id="S7.I1.i1.p1.3.m3.1.1.3.cmml" xref="S7.I1.i1.p1.3.m3.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.I1.i1.p1.3.m3.1c">c^{gt}_{j}</annotation></semantics></math>.</p>
</div>
</li>
<li id="S7.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i2.p1" class="ltx_para">
<p id="S7.I1.i2.p1.3" class="ltx_p">The IoU between the predicted box <math id="S7.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="b_{i}" display="inline"><semantics id="S7.I1.i2.p1.1.m1.1a"><msub id="S7.I1.i2.p1.1.m1.1.1" xref="S7.I1.i2.p1.1.m1.1.1.cmml"><mi id="S7.I1.i2.p1.1.m1.1.1.2" xref="S7.I1.i2.p1.1.m1.1.1.2.cmml">b</mi><mi id="S7.I1.i2.p1.1.m1.1.1.3" xref="S7.I1.i2.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S7.I1.i2.p1.1.m1.1b"><apply id="S7.I1.i2.p1.1.m1.1.1.cmml" xref="S7.I1.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S7.I1.i2.p1.1.m1.1.1.1.cmml" xref="S7.I1.i2.p1.1.m1.1.1">subscript</csymbol><ci id="S7.I1.i2.p1.1.m1.1.1.2.cmml" xref="S7.I1.i2.p1.1.m1.1.1.2">𝑏</ci><ci id="S7.I1.i2.p1.1.m1.1.1.3.cmml" xref="S7.I1.i2.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.I1.i2.p1.1.m1.1c">b_{i}</annotation></semantics></math> and the ground truth box <math id="S7.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="b^{gt}_{j}" display="inline"><semantics id="S7.I1.i2.p1.2.m2.1a"><msubsup id="S7.I1.i2.p1.2.m2.1.1" xref="S7.I1.i2.p1.2.m2.1.1.cmml"><mi id="S7.I1.i2.p1.2.m2.1.1.2.2" xref="S7.I1.i2.p1.2.m2.1.1.2.2.cmml">b</mi><mi id="S7.I1.i2.p1.2.m2.1.1.3" xref="S7.I1.i2.p1.2.m2.1.1.3.cmml">j</mi><mrow id="S7.I1.i2.p1.2.m2.1.1.2.3" xref="S7.I1.i2.p1.2.m2.1.1.2.3.cmml"><mi id="S7.I1.i2.p1.2.m2.1.1.2.3.2" xref="S7.I1.i2.p1.2.m2.1.1.2.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S7.I1.i2.p1.2.m2.1.1.2.3.1" xref="S7.I1.i2.p1.2.m2.1.1.2.3.1.cmml">​</mo><mi id="S7.I1.i2.p1.2.m2.1.1.2.3.3" xref="S7.I1.i2.p1.2.m2.1.1.2.3.3.cmml">t</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S7.I1.i2.p1.2.m2.1b"><apply id="S7.I1.i2.p1.2.m2.1.1.cmml" xref="S7.I1.i2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S7.I1.i2.p1.2.m2.1.1.1.cmml" xref="S7.I1.i2.p1.2.m2.1.1">subscript</csymbol><apply id="S7.I1.i2.p1.2.m2.1.1.2.cmml" xref="S7.I1.i2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S7.I1.i2.p1.2.m2.1.1.2.1.cmml" xref="S7.I1.i2.p1.2.m2.1.1">superscript</csymbol><ci id="S7.I1.i2.p1.2.m2.1.1.2.2.cmml" xref="S7.I1.i2.p1.2.m2.1.1.2.2">𝑏</ci><apply id="S7.I1.i2.p1.2.m2.1.1.2.3.cmml" xref="S7.I1.i2.p1.2.m2.1.1.2.3"><times id="S7.I1.i2.p1.2.m2.1.1.2.3.1.cmml" xref="S7.I1.i2.p1.2.m2.1.1.2.3.1"></times><ci id="S7.I1.i2.p1.2.m2.1.1.2.3.2.cmml" xref="S7.I1.i2.p1.2.m2.1.1.2.3.2">𝑔</ci><ci id="S7.I1.i2.p1.2.m2.1.1.2.3.3.cmml" xref="S7.I1.i2.p1.2.m2.1.1.2.3.3">𝑡</ci></apply></apply><ci id="S7.I1.i2.p1.2.m2.1.1.3.cmml" xref="S7.I1.i2.p1.2.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.I1.i2.p1.2.m2.1c">b^{gt}_{j}</annotation></semantics></math> is larger than the IoU threshold <math id="S7.I1.i2.p1.3.m3.1" class="ltx_Math" alttext="\varepsilon" display="inline"><semantics id="S7.I1.i2.p1.3.m3.1a"><mi id="S7.I1.i2.p1.3.m3.1.1" xref="S7.I1.i2.p1.3.m3.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S7.I1.i2.p1.3.m3.1b"><ci id="S7.I1.i2.p1.3.m3.1.1.cmml" xref="S7.I1.i2.p1.3.m3.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.I1.i2.p1.3.m3.1c">\varepsilon</annotation></semantics></math>. The IoU is calculated as follows:</p>
</div>
<div id="S7.I1.i2.p2" class="ltx_para">
<table id="S7.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S7.E2.m1.6" class="ltx_Math" alttext="\mathrm{IoU}\left(b,b^{g}\right)=\frac{\operatorname{area}\left(b\cap b^{g}\right)}{\operatorname{area}\left(b\cup b^{g}\right)}" display="block"><semantics id="S7.E2.m1.6a"><mrow id="S7.E2.m1.6.6" xref="S7.E2.m1.6.6.cmml"><mrow id="S7.E2.m1.6.6.1" xref="S7.E2.m1.6.6.1.cmml"><mi id="S7.E2.m1.6.6.1.3" xref="S7.E2.m1.6.6.1.3.cmml">IoU</mi><mo lspace="0em" rspace="0em" id="S7.E2.m1.6.6.1.2" xref="S7.E2.m1.6.6.1.2.cmml">​</mo><mrow id="S7.E2.m1.6.6.1.1.1" xref="S7.E2.m1.6.6.1.1.2.cmml"><mo id="S7.E2.m1.6.6.1.1.1.2" xref="S7.E2.m1.6.6.1.1.2.cmml">(</mo><mi id="S7.E2.m1.5.5" xref="S7.E2.m1.5.5.cmml">b</mi><mo id="S7.E2.m1.6.6.1.1.1.3" xref="S7.E2.m1.6.6.1.1.2.cmml">,</mo><msup id="S7.E2.m1.6.6.1.1.1.1" xref="S7.E2.m1.6.6.1.1.1.1.cmml"><mi id="S7.E2.m1.6.6.1.1.1.1.2" xref="S7.E2.m1.6.6.1.1.1.1.2.cmml">b</mi><mi id="S7.E2.m1.6.6.1.1.1.1.3" xref="S7.E2.m1.6.6.1.1.1.1.3.cmml">g</mi></msup><mo id="S7.E2.m1.6.6.1.1.1.4" xref="S7.E2.m1.6.6.1.1.2.cmml">)</mo></mrow></mrow><mo id="S7.E2.m1.6.6.2" xref="S7.E2.m1.6.6.2.cmml">=</mo><mfrac id="S7.E2.m1.4.4" xref="S7.E2.m1.4.4.cmml"><mrow id="S7.E2.m1.2.2.2.2" xref="S7.E2.m1.2.2.2.3.cmml"><mi id="S7.E2.m1.1.1.1.1" xref="S7.E2.m1.1.1.1.1.cmml">area</mi><mo id="S7.E2.m1.2.2.2.2a" xref="S7.E2.m1.2.2.2.3.cmml">⁡</mo><mrow id="S7.E2.m1.2.2.2.2.1" xref="S7.E2.m1.2.2.2.3.cmml"><mo id="S7.E2.m1.2.2.2.2.1.2" xref="S7.E2.m1.2.2.2.3.cmml">(</mo><mrow id="S7.E2.m1.2.2.2.2.1.1" xref="S7.E2.m1.2.2.2.2.1.1.cmml"><mi id="S7.E2.m1.2.2.2.2.1.1.2" xref="S7.E2.m1.2.2.2.2.1.1.2.cmml">b</mi><mo id="S7.E2.m1.2.2.2.2.1.1.1" xref="S7.E2.m1.2.2.2.2.1.1.1.cmml">∩</mo><msup id="S7.E2.m1.2.2.2.2.1.1.3" xref="S7.E2.m1.2.2.2.2.1.1.3.cmml"><mi id="S7.E2.m1.2.2.2.2.1.1.3.2" xref="S7.E2.m1.2.2.2.2.1.1.3.2.cmml">b</mi><mi id="S7.E2.m1.2.2.2.2.1.1.3.3" xref="S7.E2.m1.2.2.2.2.1.1.3.3.cmml">g</mi></msup></mrow><mo id="S7.E2.m1.2.2.2.2.1.3" xref="S7.E2.m1.2.2.2.3.cmml">)</mo></mrow></mrow><mrow id="S7.E2.m1.4.4.4.2" xref="S7.E2.m1.4.4.4.3.cmml"><mi id="S7.E2.m1.3.3.3.1" xref="S7.E2.m1.3.3.3.1.cmml">area</mi><mo id="S7.E2.m1.4.4.4.2a" xref="S7.E2.m1.4.4.4.3.cmml">⁡</mo><mrow id="S7.E2.m1.4.4.4.2.1" xref="S7.E2.m1.4.4.4.3.cmml"><mo id="S7.E2.m1.4.4.4.2.1.2" xref="S7.E2.m1.4.4.4.3.cmml">(</mo><mrow id="S7.E2.m1.4.4.4.2.1.1" xref="S7.E2.m1.4.4.4.2.1.1.cmml"><mi id="S7.E2.m1.4.4.4.2.1.1.2" xref="S7.E2.m1.4.4.4.2.1.1.2.cmml">b</mi><mo id="S7.E2.m1.4.4.4.2.1.1.1" xref="S7.E2.m1.4.4.4.2.1.1.1.cmml">∪</mo><msup id="S7.E2.m1.4.4.4.2.1.1.3" xref="S7.E2.m1.4.4.4.2.1.1.3.cmml"><mi id="S7.E2.m1.4.4.4.2.1.1.3.2" xref="S7.E2.m1.4.4.4.2.1.1.3.2.cmml">b</mi><mi id="S7.E2.m1.4.4.4.2.1.1.3.3" xref="S7.E2.m1.4.4.4.2.1.1.3.3.cmml">g</mi></msup></mrow><mo id="S7.E2.m1.4.4.4.2.1.3" xref="S7.E2.m1.4.4.4.3.cmml">)</mo></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S7.E2.m1.6b"><apply id="S7.E2.m1.6.6.cmml" xref="S7.E2.m1.6.6"><eq id="S7.E2.m1.6.6.2.cmml" xref="S7.E2.m1.6.6.2"></eq><apply id="S7.E2.m1.6.6.1.cmml" xref="S7.E2.m1.6.6.1"><times id="S7.E2.m1.6.6.1.2.cmml" xref="S7.E2.m1.6.6.1.2"></times><ci id="S7.E2.m1.6.6.1.3.cmml" xref="S7.E2.m1.6.6.1.3">IoU</ci><interval closure="open" id="S7.E2.m1.6.6.1.1.2.cmml" xref="S7.E2.m1.6.6.1.1.1"><ci id="S7.E2.m1.5.5.cmml" xref="S7.E2.m1.5.5">𝑏</ci><apply id="S7.E2.m1.6.6.1.1.1.1.cmml" xref="S7.E2.m1.6.6.1.1.1.1"><csymbol cd="ambiguous" id="S7.E2.m1.6.6.1.1.1.1.1.cmml" xref="S7.E2.m1.6.6.1.1.1.1">superscript</csymbol><ci id="S7.E2.m1.6.6.1.1.1.1.2.cmml" xref="S7.E2.m1.6.6.1.1.1.1.2">𝑏</ci><ci id="S7.E2.m1.6.6.1.1.1.1.3.cmml" xref="S7.E2.m1.6.6.1.1.1.1.3">𝑔</ci></apply></interval></apply><apply id="S7.E2.m1.4.4.cmml" xref="S7.E2.m1.4.4"><divide id="S7.E2.m1.4.4.5.cmml" xref="S7.E2.m1.4.4"></divide><apply id="S7.E2.m1.2.2.2.3.cmml" xref="S7.E2.m1.2.2.2.2"><ci id="S7.E2.m1.1.1.1.1.cmml" xref="S7.E2.m1.1.1.1.1">area</ci><apply id="S7.E2.m1.2.2.2.2.1.1.cmml" xref="S7.E2.m1.2.2.2.2.1.1"><intersect id="S7.E2.m1.2.2.2.2.1.1.1.cmml" xref="S7.E2.m1.2.2.2.2.1.1.1"></intersect><ci id="S7.E2.m1.2.2.2.2.1.1.2.cmml" xref="S7.E2.m1.2.2.2.2.1.1.2">𝑏</ci><apply id="S7.E2.m1.2.2.2.2.1.1.3.cmml" xref="S7.E2.m1.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S7.E2.m1.2.2.2.2.1.1.3.1.cmml" xref="S7.E2.m1.2.2.2.2.1.1.3">superscript</csymbol><ci id="S7.E2.m1.2.2.2.2.1.1.3.2.cmml" xref="S7.E2.m1.2.2.2.2.1.1.3.2">𝑏</ci><ci id="S7.E2.m1.2.2.2.2.1.1.3.3.cmml" xref="S7.E2.m1.2.2.2.2.1.1.3.3">𝑔</ci></apply></apply></apply><apply id="S7.E2.m1.4.4.4.3.cmml" xref="S7.E2.m1.4.4.4.2"><ci id="S7.E2.m1.3.3.3.1.cmml" xref="S7.E2.m1.3.3.3.1">area</ci><apply id="S7.E2.m1.4.4.4.2.1.1.cmml" xref="S7.E2.m1.4.4.4.2.1.1"><union id="S7.E2.m1.4.4.4.2.1.1.1.cmml" xref="S7.E2.m1.4.4.4.2.1.1.1"></union><ci id="S7.E2.m1.4.4.4.2.1.1.2.cmml" xref="S7.E2.m1.4.4.4.2.1.1.2">𝑏</ci><apply id="S7.E2.m1.4.4.4.2.1.1.3.cmml" xref="S7.E2.m1.4.4.4.2.1.1.3"><csymbol cd="ambiguous" id="S7.E2.m1.4.4.4.2.1.1.3.1.cmml" xref="S7.E2.m1.4.4.4.2.1.1.3">superscript</csymbol><ci id="S7.E2.m1.4.4.4.2.1.1.3.2.cmml" xref="S7.E2.m1.4.4.4.2.1.1.3.2">𝑏</ci><ci id="S7.E2.m1.4.4.4.2.1.1.3.3.cmml" xref="S7.E2.m1.4.4.4.2.1.1.3.3">𝑔</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.E2.m1.6c">\mathrm{IoU}\left(b,b^{g}\right)=\frac{\operatorname{area}\left(b\cap b^{g}\right)}{\operatorname{area}\left(b\cup b^{g}\right)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S7.I1.i2.p3" class="ltx_para">
<p id="S7.I1.i2.p3.2" class="ltx_p">where <math id="S7.I1.i2.p3.1.m1.1" class="ltx_Math" alttext="area(b_{i}\cap b^{gt}_{j})" display="inline"><semantics id="S7.I1.i2.p3.1.m1.1a"><mrow id="S7.I1.i2.p3.1.m1.1.1" xref="S7.I1.i2.p3.1.m1.1.1.cmml"><mi id="S7.I1.i2.p3.1.m1.1.1.3" xref="S7.I1.i2.p3.1.m1.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S7.I1.i2.p3.1.m1.1.1.2" xref="S7.I1.i2.p3.1.m1.1.1.2.cmml">​</mo><mi id="S7.I1.i2.p3.1.m1.1.1.4" xref="S7.I1.i2.p3.1.m1.1.1.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S7.I1.i2.p3.1.m1.1.1.2a" xref="S7.I1.i2.p3.1.m1.1.1.2.cmml">​</mo><mi id="S7.I1.i2.p3.1.m1.1.1.5" xref="S7.I1.i2.p3.1.m1.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S7.I1.i2.p3.1.m1.1.1.2b" xref="S7.I1.i2.p3.1.m1.1.1.2.cmml">​</mo><mi id="S7.I1.i2.p3.1.m1.1.1.6" xref="S7.I1.i2.p3.1.m1.1.1.6.cmml">a</mi><mo lspace="0em" rspace="0em" id="S7.I1.i2.p3.1.m1.1.1.2c" xref="S7.I1.i2.p3.1.m1.1.1.2.cmml">​</mo><mrow id="S7.I1.i2.p3.1.m1.1.1.1.1" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S7.I1.i2.p3.1.m1.1.1.1.1.2" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S7.I1.i2.p3.1.m1.1.1.1.1.1" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.cmml"><msub id="S7.I1.i2.p3.1.m1.1.1.1.1.1.2" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.2.cmml"><mi id="S7.I1.i2.p3.1.m1.1.1.1.1.1.2.2" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.2.2.cmml">b</mi><mi id="S7.I1.i2.p3.1.m1.1.1.1.1.1.2.3" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S7.I1.i2.p3.1.m1.1.1.1.1.1.1" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.1.cmml">∩</mo><msubsup id="S7.I1.i2.p3.1.m1.1.1.1.1.1.3" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.cmml"><mi id="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.2.2" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.2.2.cmml">b</mi><mi id="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.3" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.3.cmml">j</mi><mrow id="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.2.3" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.2.3.cmml"><mi id="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.2.3.2" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.2.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.2.3.1" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.2.3.1.cmml">​</mo><mi id="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.2.3.3" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.2.3.3.cmml">t</mi></mrow></msubsup></mrow><mo stretchy="false" id="S7.I1.i2.p3.1.m1.1.1.1.1.3" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S7.I1.i2.p3.1.m1.1b"><apply id="S7.I1.i2.p3.1.m1.1.1.cmml" xref="S7.I1.i2.p3.1.m1.1.1"><times id="S7.I1.i2.p3.1.m1.1.1.2.cmml" xref="S7.I1.i2.p3.1.m1.1.1.2"></times><ci id="S7.I1.i2.p3.1.m1.1.1.3.cmml" xref="S7.I1.i2.p3.1.m1.1.1.3">𝑎</ci><ci id="S7.I1.i2.p3.1.m1.1.1.4.cmml" xref="S7.I1.i2.p3.1.m1.1.1.4">𝑟</ci><ci id="S7.I1.i2.p3.1.m1.1.1.5.cmml" xref="S7.I1.i2.p3.1.m1.1.1.5">𝑒</ci><ci id="S7.I1.i2.p3.1.m1.1.1.6.cmml" xref="S7.I1.i2.p3.1.m1.1.1.6">𝑎</ci><apply id="S7.I1.i2.p3.1.m1.1.1.1.1.1.cmml" xref="S7.I1.i2.p3.1.m1.1.1.1.1"><intersect id="S7.I1.i2.p3.1.m1.1.1.1.1.1.1.cmml" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.1"></intersect><apply id="S7.I1.i2.p3.1.m1.1.1.1.1.1.2.cmml" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S7.I1.i2.p3.1.m1.1.1.1.1.1.2.1.cmml" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.2">subscript</csymbol><ci id="S7.I1.i2.p3.1.m1.1.1.1.1.1.2.2.cmml" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.2.2">𝑏</ci><ci id="S7.I1.i2.p3.1.m1.1.1.1.1.1.2.3.cmml" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.cmml" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.1.cmml" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.3">subscript</csymbol><apply id="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.2.cmml" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.2.1.cmml" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.3">superscript</csymbol><ci id="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.2.2.cmml" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.2.2">𝑏</ci><apply id="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.2.3.cmml" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.2.3"><times id="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.2.3.1.cmml" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.2.3.1"></times><ci id="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.2.3.2.cmml" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.2.3.2">𝑔</ci><ci id="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.2.3.3.cmml" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.2.3.3">𝑡</ci></apply></apply><ci id="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.3.cmml" xref="S7.I1.i2.p3.1.m1.1.1.1.1.1.3.3">𝑗</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.I1.i2.p3.1.m1.1c">area(b_{i}\cap b^{gt}_{j})</annotation></semantics></math> and <math id="S7.I1.i2.p3.2.m2.1" class="ltx_Math" alttext="area(b_{i}\cup b^{gt}_{j})" display="inline"><semantics id="S7.I1.i2.p3.2.m2.1a"><mrow id="S7.I1.i2.p3.2.m2.1.1" xref="S7.I1.i2.p3.2.m2.1.1.cmml"><mi id="S7.I1.i2.p3.2.m2.1.1.3" xref="S7.I1.i2.p3.2.m2.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S7.I1.i2.p3.2.m2.1.1.2" xref="S7.I1.i2.p3.2.m2.1.1.2.cmml">​</mo><mi id="S7.I1.i2.p3.2.m2.1.1.4" xref="S7.I1.i2.p3.2.m2.1.1.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S7.I1.i2.p3.2.m2.1.1.2a" xref="S7.I1.i2.p3.2.m2.1.1.2.cmml">​</mo><mi id="S7.I1.i2.p3.2.m2.1.1.5" xref="S7.I1.i2.p3.2.m2.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S7.I1.i2.p3.2.m2.1.1.2b" xref="S7.I1.i2.p3.2.m2.1.1.2.cmml">​</mo><mi id="S7.I1.i2.p3.2.m2.1.1.6" xref="S7.I1.i2.p3.2.m2.1.1.6.cmml">a</mi><mo lspace="0em" rspace="0em" id="S7.I1.i2.p3.2.m2.1.1.2c" xref="S7.I1.i2.p3.2.m2.1.1.2.cmml">​</mo><mrow id="S7.I1.i2.p3.2.m2.1.1.1.1" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S7.I1.i2.p3.2.m2.1.1.1.1.2" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S7.I1.i2.p3.2.m2.1.1.1.1.1" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.cmml"><msub id="S7.I1.i2.p3.2.m2.1.1.1.1.1.2" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.2.cmml"><mi id="S7.I1.i2.p3.2.m2.1.1.1.1.1.2.2" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.2.2.cmml">b</mi><mi id="S7.I1.i2.p3.2.m2.1.1.1.1.1.2.3" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S7.I1.i2.p3.2.m2.1.1.1.1.1.1" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.1.cmml">∪</mo><msubsup id="S7.I1.i2.p3.2.m2.1.1.1.1.1.3" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.cmml"><mi id="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.2.2" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.2.2.cmml">b</mi><mi id="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.3" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.3.cmml">j</mi><mrow id="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.2.3" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.2.3.cmml"><mi id="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.2.3.2" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.2.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.2.3.1" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.2.3.1.cmml">​</mo><mi id="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.2.3.3" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.2.3.3.cmml">t</mi></mrow></msubsup></mrow><mo stretchy="false" id="S7.I1.i2.p3.2.m2.1.1.1.1.3" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S7.I1.i2.p3.2.m2.1b"><apply id="S7.I1.i2.p3.2.m2.1.1.cmml" xref="S7.I1.i2.p3.2.m2.1.1"><times id="S7.I1.i2.p3.2.m2.1.1.2.cmml" xref="S7.I1.i2.p3.2.m2.1.1.2"></times><ci id="S7.I1.i2.p3.2.m2.1.1.3.cmml" xref="S7.I1.i2.p3.2.m2.1.1.3">𝑎</ci><ci id="S7.I1.i2.p3.2.m2.1.1.4.cmml" xref="S7.I1.i2.p3.2.m2.1.1.4">𝑟</ci><ci id="S7.I1.i2.p3.2.m2.1.1.5.cmml" xref="S7.I1.i2.p3.2.m2.1.1.5">𝑒</ci><ci id="S7.I1.i2.p3.2.m2.1.1.6.cmml" xref="S7.I1.i2.p3.2.m2.1.1.6">𝑎</ci><apply id="S7.I1.i2.p3.2.m2.1.1.1.1.1.cmml" xref="S7.I1.i2.p3.2.m2.1.1.1.1"><union id="S7.I1.i2.p3.2.m2.1.1.1.1.1.1.cmml" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.1"></union><apply id="S7.I1.i2.p3.2.m2.1.1.1.1.1.2.cmml" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S7.I1.i2.p3.2.m2.1.1.1.1.1.2.1.cmml" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.2">subscript</csymbol><ci id="S7.I1.i2.p3.2.m2.1.1.1.1.1.2.2.cmml" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.2.2">𝑏</ci><ci id="S7.I1.i2.p3.2.m2.1.1.1.1.1.2.3.cmml" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.cmml" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.1.cmml" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.3">subscript</csymbol><apply id="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.2.cmml" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.2.1.cmml" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.3">superscript</csymbol><ci id="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.2.2.cmml" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.2.2">𝑏</ci><apply id="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.2.3.cmml" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.2.3"><times id="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.2.3.1.cmml" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.2.3.1"></times><ci id="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.2.3.2.cmml" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.2.3.2">𝑔</ci><ci id="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.2.3.3.cmml" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.2.3.3">𝑡</ci></apply></apply><ci id="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.3.cmml" xref="S7.I1.i2.p3.2.m2.1.1.1.1.1.3.3">𝑗</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.I1.i2.p3.2.m2.1c">area(b_{i}\cup b^{gt}_{j})</annotation></semantics></math> stand for the intersection and union area of the predicted box and ground truth box.</p>
</div>
</li>
</ul>
</div>
<div id="S7.SS2.p4" class="ltx_para">
<p id="S7.SS2.p4.1" class="ltx_p">Otherwise, it is considered to be a False Positive (FP). It is worth noting that multiple prediction detections may match the same ground truth annotation according to the above criteria, but only the prediction detection with the highest confidence score is assigned as a TP, and the rest are FPs<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib287" title="" class="ltx_ref">287</a>]</cite>.</p>
</div>
<div id="S7.SS2.p5" class="ltx_para">
<p id="S7.SS2.p5.1" class="ltx_p">Based on TP and FP detections, the Precision (P) and Recall (R) can be computed as Eq. <a href="#S7.E3" title="In VII-B Evaluation Metrics ‣ VII Datasets and Evaluation Metrics ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and Eq. <a href="#S7.E4" title="In VII-B Evaluation Metrics ‣ VII Datasets and Evaluation Metrics ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
<table id="S7.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S7.E3.m1.1" class="ltx_Math" alttext="P=\frac{TP}{TP+FP}" display="block"><semantics id="S7.E3.m1.1a"><mrow id="S7.E3.m1.1.1" xref="S7.E3.m1.1.1.cmml"><mi id="S7.E3.m1.1.1.2" xref="S7.E3.m1.1.1.2.cmml">P</mi><mo id="S7.E3.m1.1.1.1" xref="S7.E3.m1.1.1.1.cmml">=</mo><mfrac id="S7.E3.m1.1.1.3" xref="S7.E3.m1.1.1.3.cmml"><mrow id="S7.E3.m1.1.1.3.2" xref="S7.E3.m1.1.1.3.2.cmml"><mi id="S7.E3.m1.1.1.3.2.2" xref="S7.E3.m1.1.1.3.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S7.E3.m1.1.1.3.2.1" xref="S7.E3.m1.1.1.3.2.1.cmml">​</mo><mi id="S7.E3.m1.1.1.3.2.3" xref="S7.E3.m1.1.1.3.2.3.cmml">P</mi></mrow><mrow id="S7.E3.m1.1.1.3.3" xref="S7.E3.m1.1.1.3.3.cmml"><mrow id="S7.E3.m1.1.1.3.3.2" xref="S7.E3.m1.1.1.3.3.2.cmml"><mi id="S7.E3.m1.1.1.3.3.2.2" xref="S7.E3.m1.1.1.3.3.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S7.E3.m1.1.1.3.3.2.1" xref="S7.E3.m1.1.1.3.3.2.1.cmml">​</mo><mi id="S7.E3.m1.1.1.3.3.2.3" xref="S7.E3.m1.1.1.3.3.2.3.cmml">P</mi></mrow><mo id="S7.E3.m1.1.1.3.3.1" xref="S7.E3.m1.1.1.3.3.1.cmml">+</mo><mrow id="S7.E3.m1.1.1.3.3.3" xref="S7.E3.m1.1.1.3.3.3.cmml"><mi id="S7.E3.m1.1.1.3.3.3.2" xref="S7.E3.m1.1.1.3.3.3.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S7.E3.m1.1.1.3.3.3.1" xref="S7.E3.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S7.E3.m1.1.1.3.3.3.3" xref="S7.E3.m1.1.1.3.3.3.3.cmml">P</mi></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S7.E3.m1.1b"><apply id="S7.E3.m1.1.1.cmml" xref="S7.E3.m1.1.1"><eq id="S7.E3.m1.1.1.1.cmml" xref="S7.E3.m1.1.1.1"></eq><ci id="S7.E3.m1.1.1.2.cmml" xref="S7.E3.m1.1.1.2">𝑃</ci><apply id="S7.E3.m1.1.1.3.cmml" xref="S7.E3.m1.1.1.3"><divide id="S7.E3.m1.1.1.3.1.cmml" xref="S7.E3.m1.1.1.3"></divide><apply id="S7.E3.m1.1.1.3.2.cmml" xref="S7.E3.m1.1.1.3.2"><times id="S7.E3.m1.1.1.3.2.1.cmml" xref="S7.E3.m1.1.1.3.2.1"></times><ci id="S7.E3.m1.1.1.3.2.2.cmml" xref="S7.E3.m1.1.1.3.2.2">𝑇</ci><ci id="S7.E3.m1.1.1.3.2.3.cmml" xref="S7.E3.m1.1.1.3.2.3">𝑃</ci></apply><apply id="S7.E3.m1.1.1.3.3.cmml" xref="S7.E3.m1.1.1.3.3"><plus id="S7.E3.m1.1.1.3.3.1.cmml" xref="S7.E3.m1.1.1.3.3.1"></plus><apply id="S7.E3.m1.1.1.3.3.2.cmml" xref="S7.E3.m1.1.1.3.3.2"><times id="S7.E3.m1.1.1.3.3.2.1.cmml" xref="S7.E3.m1.1.1.3.3.2.1"></times><ci id="S7.E3.m1.1.1.3.3.2.2.cmml" xref="S7.E3.m1.1.1.3.3.2.2">𝑇</ci><ci id="S7.E3.m1.1.1.3.3.2.3.cmml" xref="S7.E3.m1.1.1.3.3.2.3">𝑃</ci></apply><apply id="S7.E3.m1.1.1.3.3.3.cmml" xref="S7.E3.m1.1.1.3.3.3"><times id="S7.E3.m1.1.1.3.3.3.1.cmml" xref="S7.E3.m1.1.1.3.3.3.1"></times><ci id="S7.E3.m1.1.1.3.3.3.2.cmml" xref="S7.E3.m1.1.1.3.3.3.2">𝐹</ci><ci id="S7.E3.m1.1.1.3.3.3.3.cmml" xref="S7.E3.m1.1.1.3.3.3.3">𝑃</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.E3.m1.1c">P=\frac{TP}{TP+FP}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<table id="S7.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S7.E4.m1.1" class="ltx_Math" alttext="R=\frac{TP}{TP+FN}" display="block"><semantics id="S7.E4.m1.1a"><mrow id="S7.E4.m1.1.1" xref="S7.E4.m1.1.1.cmml"><mi id="S7.E4.m1.1.1.2" xref="S7.E4.m1.1.1.2.cmml">R</mi><mo id="S7.E4.m1.1.1.1" xref="S7.E4.m1.1.1.1.cmml">=</mo><mfrac id="S7.E4.m1.1.1.3" xref="S7.E4.m1.1.1.3.cmml"><mrow id="S7.E4.m1.1.1.3.2" xref="S7.E4.m1.1.1.3.2.cmml"><mi id="S7.E4.m1.1.1.3.2.2" xref="S7.E4.m1.1.1.3.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S7.E4.m1.1.1.3.2.1" xref="S7.E4.m1.1.1.3.2.1.cmml">​</mo><mi id="S7.E4.m1.1.1.3.2.3" xref="S7.E4.m1.1.1.3.2.3.cmml">P</mi></mrow><mrow id="S7.E4.m1.1.1.3.3" xref="S7.E4.m1.1.1.3.3.cmml"><mrow id="S7.E4.m1.1.1.3.3.2" xref="S7.E4.m1.1.1.3.3.2.cmml"><mi id="S7.E4.m1.1.1.3.3.2.2" xref="S7.E4.m1.1.1.3.3.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S7.E4.m1.1.1.3.3.2.1" xref="S7.E4.m1.1.1.3.3.2.1.cmml">​</mo><mi id="S7.E4.m1.1.1.3.3.2.3" xref="S7.E4.m1.1.1.3.3.2.3.cmml">P</mi></mrow><mo id="S7.E4.m1.1.1.3.3.1" xref="S7.E4.m1.1.1.3.3.1.cmml">+</mo><mrow id="S7.E4.m1.1.1.3.3.3" xref="S7.E4.m1.1.1.3.3.3.cmml"><mi id="S7.E4.m1.1.1.3.3.3.2" xref="S7.E4.m1.1.1.3.3.3.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S7.E4.m1.1.1.3.3.3.1" xref="S7.E4.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S7.E4.m1.1.1.3.3.3.3" xref="S7.E4.m1.1.1.3.3.3.3.cmml">N</mi></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S7.E4.m1.1b"><apply id="S7.E4.m1.1.1.cmml" xref="S7.E4.m1.1.1"><eq id="S7.E4.m1.1.1.1.cmml" xref="S7.E4.m1.1.1.1"></eq><ci id="S7.E4.m1.1.1.2.cmml" xref="S7.E4.m1.1.1.2">𝑅</ci><apply id="S7.E4.m1.1.1.3.cmml" xref="S7.E4.m1.1.1.3"><divide id="S7.E4.m1.1.1.3.1.cmml" xref="S7.E4.m1.1.1.3"></divide><apply id="S7.E4.m1.1.1.3.2.cmml" xref="S7.E4.m1.1.1.3.2"><times id="S7.E4.m1.1.1.3.2.1.cmml" xref="S7.E4.m1.1.1.3.2.1"></times><ci id="S7.E4.m1.1.1.3.2.2.cmml" xref="S7.E4.m1.1.1.3.2.2">𝑇</ci><ci id="S7.E4.m1.1.1.3.2.3.cmml" xref="S7.E4.m1.1.1.3.2.3">𝑃</ci></apply><apply id="S7.E4.m1.1.1.3.3.cmml" xref="S7.E4.m1.1.1.3.3"><plus id="S7.E4.m1.1.1.3.3.1.cmml" xref="S7.E4.m1.1.1.3.3.1"></plus><apply id="S7.E4.m1.1.1.3.3.2.cmml" xref="S7.E4.m1.1.1.3.3.2"><times id="S7.E4.m1.1.1.3.3.2.1.cmml" xref="S7.E4.m1.1.1.3.3.2.1"></times><ci id="S7.E4.m1.1.1.3.3.2.2.cmml" xref="S7.E4.m1.1.1.3.3.2.2">𝑇</ci><ci id="S7.E4.m1.1.1.3.3.2.3.cmml" xref="S7.E4.m1.1.1.3.3.2.3">𝑃</ci></apply><apply id="S7.E4.m1.1.1.3.3.3.cmml" xref="S7.E4.m1.1.1.3.3.3"><times id="S7.E4.m1.1.1.3.3.3.1.cmml" xref="S7.E4.m1.1.1.3.3.3.1"></times><ci id="S7.E4.m1.1.1.3.3.3.2.cmml" xref="S7.E4.m1.1.1.3.3.3.2">𝐹</ci><ci id="S7.E4.m1.1.1.3.3.3.3.cmml" xref="S7.E4.m1.1.1.3.3.3.3">𝑁</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.E4.m1.1c">R=\frac{TP}{TP+FN}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S7.SS2.p5.2" class="ltx_p">where <span id="S7.SS2.p5.2.1" class="ltx_text ltx_font_italic">FN</span> denotes the number of false negatives. The precision measures the fraction of true positives of the prediction detections and the recall measures the fraction of positives that are correctly detected. However, the above two evaluation metrics only reflect the single aspect of detection performance.</p>
</div>
<div id="S7.SS2.p6" class="ltx_para">
<p id="S7.SS2.p6.1" class="ltx_p">Taking into account both precision and recall, AP provides a comprehensive evaluation of detection performance and is calculated individually for each class. For a given class, the Precision/Recall Curve (PRC) is drawn according to the detection of maximum Precision at each Recall, and the AP summarises the shape of the PRC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib287" title="" class="ltx_ref">287</a>]</cite>. For multi-class object detection, the mean of the AP values for all classes, termed <math id="S7.SS2.p6.1.m1.1" class="ltx_Math" alttext="mAP" display="inline"><semantics id="S7.SS2.p6.1.m1.1a"><mrow id="S7.SS2.p6.1.m1.1.1" xref="S7.SS2.p6.1.m1.1.1.cmml"><mi id="S7.SS2.p6.1.m1.1.1.2" xref="S7.SS2.p6.1.m1.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p6.1.m1.1.1.1" xref="S7.SS2.p6.1.m1.1.1.1.cmml">​</mo><mi id="S7.SS2.p6.1.m1.1.1.3" xref="S7.SS2.p6.1.m1.1.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p6.1.m1.1.1.1a" xref="S7.SS2.p6.1.m1.1.1.1.cmml">​</mo><mi id="S7.SS2.p6.1.m1.1.1.4" xref="S7.SS2.p6.1.m1.1.1.4.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p6.1.m1.1b"><apply id="S7.SS2.p6.1.m1.1.1.cmml" xref="S7.SS2.p6.1.m1.1.1"><times id="S7.SS2.p6.1.m1.1.1.1.cmml" xref="S7.SS2.p6.1.m1.1.1.1"></times><ci id="S7.SS2.p6.1.m1.1.1.2.cmml" xref="S7.SS2.p6.1.m1.1.1.2">𝑚</ci><ci id="S7.SS2.p6.1.m1.1.1.3.cmml" xref="S7.SS2.p6.1.m1.1.1.3">𝐴</ci><ci id="S7.SS2.p6.1.m1.1.1.4.cmml" xref="S7.SS2.p6.1.m1.1.1.4">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p6.1.m1.1c">mAP</annotation></semantics></math>, is adopted to evaluate the overall detection accuracy.</p>
</div>
<div id="S7.SS2.p7" class="ltx_para">
<p id="S7.SS2.p7.4" class="ltx_p">The early studies mainly employ a fixed IoU based AP metric (i.e., <math id="S7.SS2.p7.1.m1.1" class="ltx_Math" alttext="AP_{50}" display="inline"><semantics id="S7.SS2.p7.1.m1.1a"><mrow id="S7.SS2.p7.1.m1.1.1" xref="S7.SS2.p7.1.m1.1.1.cmml"><mi id="S7.SS2.p7.1.m1.1.1.2" xref="S7.SS2.p7.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p7.1.m1.1.1.1" xref="S7.SS2.p7.1.m1.1.1.1.cmml">​</mo><msub id="S7.SS2.p7.1.m1.1.1.3" xref="S7.SS2.p7.1.m1.1.1.3.cmml"><mi id="S7.SS2.p7.1.m1.1.1.3.2" xref="S7.SS2.p7.1.m1.1.1.3.2.cmml">P</mi><mn id="S7.SS2.p7.1.m1.1.1.3.3" xref="S7.SS2.p7.1.m1.1.1.3.3.cmml">50</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p7.1.m1.1b"><apply id="S7.SS2.p7.1.m1.1.1.cmml" xref="S7.SS2.p7.1.m1.1.1"><times id="S7.SS2.p7.1.m1.1.1.1.cmml" xref="S7.SS2.p7.1.m1.1.1.1"></times><ci id="S7.SS2.p7.1.m1.1.1.2.cmml" xref="S7.SS2.p7.1.m1.1.1.2">𝐴</ci><apply id="S7.SS2.p7.1.m1.1.1.3.cmml" xref="S7.SS2.p7.1.m1.1.1.3"><csymbol cd="ambiguous" id="S7.SS2.p7.1.m1.1.1.3.1.cmml" xref="S7.SS2.p7.1.m1.1.1.3">subscript</csymbol><ci id="S7.SS2.p7.1.m1.1.1.3.2.cmml" xref="S7.SS2.p7.1.m1.1.1.3.2">𝑃</ci><cn type="integer" id="S7.SS2.p7.1.m1.1.1.3.3.cmml" xref="S7.SS2.p7.1.m1.1.1.3.3">50</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p7.1.m1.1c">AP_{50}</annotation></semantics></math>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib117" title="" class="ltx_ref">117</a>]</cite>, where the IoU threshold <math id="S7.SS2.p7.2.m2.1" class="ltx_Math" alttext="\varepsilon" display="inline"><semantics id="S7.SS2.p7.2.m2.1a"><mi id="S7.SS2.p7.2.m2.1.1" xref="S7.SS2.p7.2.m2.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S7.SS2.p7.2.m2.1b"><ci id="S7.SS2.p7.2.m2.1.1.cmml" xref="S7.SS2.p7.2.m2.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p7.2.m2.1c">\varepsilon</annotation></semantics></math> is given as 0.5. This low IoU threshold exhibits a high tolerance for bounding box deviations and fails to satisfy the high localization accuracy requirements. Later, some works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib130" title="" class="ltx_ref">130</a>, <a href="#bib.bib131" title="" class="ltx_ref">131</a>, <a href="#bib.bib284" title="" class="ltx_ref">284</a>]</cite> introduce a novel evaluation metric, named <math id="S7.SS2.p7.3.m3.1" class="ltx_Math" alttext="AP_{50:95}" display="inline"><semantics id="S7.SS2.p7.3.m3.1a"><mrow id="S7.SS2.p7.3.m3.1.1" xref="S7.SS2.p7.3.m3.1.1.cmml"><mi id="S7.SS2.p7.3.m3.1.1.2" xref="S7.SS2.p7.3.m3.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p7.3.m3.1.1.1" xref="S7.SS2.p7.3.m3.1.1.1.cmml">​</mo><msub id="S7.SS2.p7.3.m3.1.1.3" xref="S7.SS2.p7.3.m3.1.1.3.cmml"><mi id="S7.SS2.p7.3.m3.1.1.3.2" xref="S7.SS2.p7.3.m3.1.1.3.2.cmml">P</mi><mrow id="S7.SS2.p7.3.m3.1.1.3.3" xref="S7.SS2.p7.3.m3.1.1.3.3.cmml"><mn id="S7.SS2.p7.3.m3.1.1.3.3.2" xref="S7.SS2.p7.3.m3.1.1.3.3.2.cmml">50</mn><mo lspace="0.278em" rspace="0.278em" id="S7.SS2.p7.3.m3.1.1.3.3.1" xref="S7.SS2.p7.3.m3.1.1.3.3.1.cmml">:</mo><mn id="S7.SS2.p7.3.m3.1.1.3.3.3" xref="S7.SS2.p7.3.m3.1.1.3.3.3.cmml">95</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p7.3.m3.1b"><apply id="S7.SS2.p7.3.m3.1.1.cmml" xref="S7.SS2.p7.3.m3.1.1"><times id="S7.SS2.p7.3.m3.1.1.1.cmml" xref="S7.SS2.p7.3.m3.1.1.1"></times><ci id="S7.SS2.p7.3.m3.1.1.2.cmml" xref="S7.SS2.p7.3.m3.1.1.2">𝐴</ci><apply id="S7.SS2.p7.3.m3.1.1.3.cmml" xref="S7.SS2.p7.3.m3.1.1.3"><csymbol cd="ambiguous" id="S7.SS2.p7.3.m3.1.1.3.1.cmml" xref="S7.SS2.p7.3.m3.1.1.3">subscript</csymbol><ci id="S7.SS2.p7.3.m3.1.1.3.2.cmml" xref="S7.SS2.p7.3.m3.1.1.3.2">𝑃</ci><apply id="S7.SS2.p7.3.m3.1.1.3.3.cmml" xref="S7.SS2.p7.3.m3.1.1.3.3"><ci id="S7.SS2.p7.3.m3.1.1.3.3.1.cmml" xref="S7.SS2.p7.3.m3.1.1.3.3.1">:</ci><cn type="integer" id="S7.SS2.p7.3.m3.1.1.3.3.2.cmml" xref="S7.SS2.p7.3.m3.1.1.3.3.2">50</cn><cn type="integer" id="S7.SS2.p7.3.m3.1.1.3.3.3.cmml" xref="S7.SS2.p7.3.m3.1.1.3.3.3">95</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p7.3.m3.1c">AP_{50:95}</annotation></semantics></math>, which averages the AP over 10 IoU thresholds from 0.5 to 0.95 with an interval of 0.05. The <math id="S7.SS2.p7.4.m4.1" class="ltx_Math" alttext="AP_{50:95}" display="inline"><semantics id="S7.SS2.p7.4.m4.1a"><mrow id="S7.SS2.p7.4.m4.1.1" xref="S7.SS2.p7.4.m4.1.1.cmml"><mi id="S7.SS2.p7.4.m4.1.1.2" xref="S7.SS2.p7.4.m4.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p7.4.m4.1.1.1" xref="S7.SS2.p7.4.m4.1.1.1.cmml">​</mo><msub id="S7.SS2.p7.4.m4.1.1.3" xref="S7.SS2.p7.4.m4.1.1.3.cmml"><mi id="S7.SS2.p7.4.m4.1.1.3.2" xref="S7.SS2.p7.4.m4.1.1.3.2.cmml">P</mi><mrow id="S7.SS2.p7.4.m4.1.1.3.3" xref="S7.SS2.p7.4.m4.1.1.3.3.cmml"><mn id="S7.SS2.p7.4.m4.1.1.3.3.2" xref="S7.SS2.p7.4.m4.1.1.3.3.2.cmml">50</mn><mo lspace="0.278em" rspace="0.278em" id="S7.SS2.p7.4.m4.1.1.3.3.1" xref="S7.SS2.p7.4.m4.1.1.3.3.1.cmml">:</mo><mn id="S7.SS2.p7.4.m4.1.1.3.3.3" xref="S7.SS2.p7.4.m4.1.1.3.3.3.cmml">95</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p7.4.m4.1b"><apply id="S7.SS2.p7.4.m4.1.1.cmml" xref="S7.SS2.p7.4.m4.1.1"><times id="S7.SS2.p7.4.m4.1.1.1.cmml" xref="S7.SS2.p7.4.m4.1.1.1"></times><ci id="S7.SS2.p7.4.m4.1.1.2.cmml" xref="S7.SS2.p7.4.m4.1.1.2">𝐴</ci><apply id="S7.SS2.p7.4.m4.1.1.3.cmml" xref="S7.SS2.p7.4.m4.1.1.3"><csymbol cd="ambiguous" id="S7.SS2.p7.4.m4.1.1.3.1.cmml" xref="S7.SS2.p7.4.m4.1.1.3">subscript</csymbol><ci id="S7.SS2.p7.4.m4.1.1.3.2.cmml" xref="S7.SS2.p7.4.m4.1.1.3.2">𝑃</ci><apply id="S7.SS2.p7.4.m4.1.1.3.3.cmml" xref="S7.SS2.p7.4.m4.1.1.3.3"><ci id="S7.SS2.p7.4.m4.1.1.3.3.1.cmml" xref="S7.SS2.p7.4.m4.1.1.3.3.1">:</ci><cn type="integer" id="S7.SS2.p7.4.m4.1.1.3.3.2.cmml" xref="S7.SS2.p7.4.m4.1.1.3.3.2">50</cn><cn type="integer" id="S7.SS2.p7.4.m4.1.1.3.3.3.cmml" xref="S7.SS2.p7.4.m4.1.1.3.3.3">95</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p7.4.m4.1c">AP_{50:95}</annotation></semantics></math> considers higher IoU thresholds and encourages more accurate localization.</p>
</div>
<div id="S7.SS2.p8" class="ltx_para">
<p id="S7.SS2.p8.6" class="ltx_p">As the cornerstone of evaluation metrics in RSOD, AP has various extensions for different specific tasks. In the few-shot learning scenario, <math id="S7.SS2.p8.1.m1.1" class="ltx_Math" alttext="AP_{novel}" display="inline"><semantics id="S7.SS2.p8.1.m1.1a"><mrow id="S7.SS2.p8.1.m1.1.1" xref="S7.SS2.p8.1.m1.1.1.cmml"><mi id="S7.SS2.p8.1.m1.1.1.2" xref="S7.SS2.p8.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p8.1.m1.1.1.1" xref="S7.SS2.p8.1.m1.1.1.1.cmml">​</mo><msub id="S7.SS2.p8.1.m1.1.1.3" xref="S7.SS2.p8.1.m1.1.1.3.cmml"><mi id="S7.SS2.p8.1.m1.1.1.3.2" xref="S7.SS2.p8.1.m1.1.1.3.2.cmml">P</mi><mrow id="S7.SS2.p8.1.m1.1.1.3.3" xref="S7.SS2.p8.1.m1.1.1.3.3.cmml"><mi id="S7.SS2.p8.1.m1.1.1.3.3.2" xref="S7.SS2.p8.1.m1.1.1.3.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p8.1.m1.1.1.3.3.1" xref="S7.SS2.p8.1.m1.1.1.3.3.1.cmml">​</mo><mi id="S7.SS2.p8.1.m1.1.1.3.3.3" xref="S7.SS2.p8.1.m1.1.1.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p8.1.m1.1.1.3.3.1a" xref="S7.SS2.p8.1.m1.1.1.3.3.1.cmml">​</mo><mi id="S7.SS2.p8.1.m1.1.1.3.3.4" xref="S7.SS2.p8.1.m1.1.1.3.3.4.cmml">v</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p8.1.m1.1.1.3.3.1b" xref="S7.SS2.p8.1.m1.1.1.3.3.1.cmml">​</mo><mi id="S7.SS2.p8.1.m1.1.1.3.3.5" xref="S7.SS2.p8.1.m1.1.1.3.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p8.1.m1.1.1.3.3.1c" xref="S7.SS2.p8.1.m1.1.1.3.3.1.cmml">​</mo><mi id="S7.SS2.p8.1.m1.1.1.3.3.6" xref="S7.SS2.p8.1.m1.1.1.3.3.6.cmml">l</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p8.1.m1.1b"><apply id="S7.SS2.p8.1.m1.1.1.cmml" xref="S7.SS2.p8.1.m1.1.1"><times id="S7.SS2.p8.1.m1.1.1.1.cmml" xref="S7.SS2.p8.1.m1.1.1.1"></times><ci id="S7.SS2.p8.1.m1.1.1.2.cmml" xref="S7.SS2.p8.1.m1.1.1.2">𝐴</ci><apply id="S7.SS2.p8.1.m1.1.1.3.cmml" xref="S7.SS2.p8.1.m1.1.1.3"><csymbol cd="ambiguous" id="S7.SS2.p8.1.m1.1.1.3.1.cmml" xref="S7.SS2.p8.1.m1.1.1.3">subscript</csymbol><ci id="S7.SS2.p8.1.m1.1.1.3.2.cmml" xref="S7.SS2.p8.1.m1.1.1.3.2">𝑃</ci><apply id="S7.SS2.p8.1.m1.1.1.3.3.cmml" xref="S7.SS2.p8.1.m1.1.1.3.3"><times id="S7.SS2.p8.1.m1.1.1.3.3.1.cmml" xref="S7.SS2.p8.1.m1.1.1.3.3.1"></times><ci id="S7.SS2.p8.1.m1.1.1.3.3.2.cmml" xref="S7.SS2.p8.1.m1.1.1.3.3.2">𝑛</ci><ci id="S7.SS2.p8.1.m1.1.1.3.3.3.cmml" xref="S7.SS2.p8.1.m1.1.1.3.3.3">𝑜</ci><ci id="S7.SS2.p8.1.m1.1.1.3.3.4.cmml" xref="S7.SS2.p8.1.m1.1.1.3.3.4">𝑣</ci><ci id="S7.SS2.p8.1.m1.1.1.3.3.5.cmml" xref="S7.SS2.p8.1.m1.1.1.3.3.5">𝑒</ci><ci id="S7.SS2.p8.1.m1.1.1.3.3.6.cmml" xref="S7.SS2.p8.1.m1.1.1.3.3.6">𝑙</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p8.1.m1.1c">AP_{novel}</annotation></semantics></math> and <math id="S7.SS2.p8.2.m2.1" class="ltx_Math" alttext="AP_{base}" display="inline"><semantics id="S7.SS2.p8.2.m2.1a"><mrow id="S7.SS2.p8.2.m2.1.1" xref="S7.SS2.p8.2.m2.1.1.cmml"><mi id="S7.SS2.p8.2.m2.1.1.2" xref="S7.SS2.p8.2.m2.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p8.2.m2.1.1.1" xref="S7.SS2.p8.2.m2.1.1.1.cmml">​</mo><msub id="S7.SS2.p8.2.m2.1.1.3" xref="S7.SS2.p8.2.m2.1.1.3.cmml"><mi id="S7.SS2.p8.2.m2.1.1.3.2" xref="S7.SS2.p8.2.m2.1.1.3.2.cmml">P</mi><mrow id="S7.SS2.p8.2.m2.1.1.3.3" xref="S7.SS2.p8.2.m2.1.1.3.3.cmml"><mi id="S7.SS2.p8.2.m2.1.1.3.3.2" xref="S7.SS2.p8.2.m2.1.1.3.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p8.2.m2.1.1.3.3.1" xref="S7.SS2.p8.2.m2.1.1.3.3.1.cmml">​</mo><mi id="S7.SS2.p8.2.m2.1.1.3.3.3" xref="S7.SS2.p8.2.m2.1.1.3.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p8.2.m2.1.1.3.3.1a" xref="S7.SS2.p8.2.m2.1.1.3.3.1.cmml">​</mo><mi id="S7.SS2.p8.2.m2.1.1.3.3.4" xref="S7.SS2.p8.2.m2.1.1.3.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p8.2.m2.1.1.3.3.1b" xref="S7.SS2.p8.2.m2.1.1.3.3.1.cmml">​</mo><mi id="S7.SS2.p8.2.m2.1.1.3.3.5" xref="S7.SS2.p8.2.m2.1.1.3.3.5.cmml">e</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p8.2.m2.1b"><apply id="S7.SS2.p8.2.m2.1.1.cmml" xref="S7.SS2.p8.2.m2.1.1"><times id="S7.SS2.p8.2.m2.1.1.1.cmml" xref="S7.SS2.p8.2.m2.1.1.1"></times><ci id="S7.SS2.p8.2.m2.1.1.2.cmml" xref="S7.SS2.p8.2.m2.1.1.2">𝐴</ci><apply id="S7.SS2.p8.2.m2.1.1.3.cmml" xref="S7.SS2.p8.2.m2.1.1.3"><csymbol cd="ambiguous" id="S7.SS2.p8.2.m2.1.1.3.1.cmml" xref="S7.SS2.p8.2.m2.1.1.3">subscript</csymbol><ci id="S7.SS2.p8.2.m2.1.1.3.2.cmml" xref="S7.SS2.p8.2.m2.1.1.3.2">𝑃</ci><apply id="S7.SS2.p8.2.m2.1.1.3.3.cmml" xref="S7.SS2.p8.2.m2.1.1.3.3"><times id="S7.SS2.p8.2.m2.1.1.3.3.1.cmml" xref="S7.SS2.p8.2.m2.1.1.3.3.1"></times><ci id="S7.SS2.p8.2.m2.1.1.3.3.2.cmml" xref="S7.SS2.p8.2.m2.1.1.3.3.2">𝑏</ci><ci id="S7.SS2.p8.2.m2.1.1.3.3.3.cmml" xref="S7.SS2.p8.2.m2.1.1.3.3.3">𝑎</ci><ci id="S7.SS2.p8.2.m2.1.1.3.3.4.cmml" xref="S7.SS2.p8.2.m2.1.1.3.3.4">𝑠</ci><ci id="S7.SS2.p8.2.m2.1.1.3.3.5.cmml" xref="S7.SS2.p8.2.m2.1.1.3.3.5">𝑒</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p8.2.m2.1c">AP_{base}</annotation></semantics></math> are two critical metrics to evaluate the performance of few-shot detectors, where <math id="S7.SS2.p8.3.m3.1" class="ltx_Math" alttext="AP_{novel}" display="inline"><semantics id="S7.SS2.p8.3.m3.1a"><mrow id="S7.SS2.p8.3.m3.1.1" xref="S7.SS2.p8.3.m3.1.1.cmml"><mi id="S7.SS2.p8.3.m3.1.1.2" xref="S7.SS2.p8.3.m3.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p8.3.m3.1.1.1" xref="S7.SS2.p8.3.m3.1.1.1.cmml">​</mo><msub id="S7.SS2.p8.3.m3.1.1.3" xref="S7.SS2.p8.3.m3.1.1.3.cmml"><mi id="S7.SS2.p8.3.m3.1.1.3.2" xref="S7.SS2.p8.3.m3.1.1.3.2.cmml">P</mi><mrow id="S7.SS2.p8.3.m3.1.1.3.3" xref="S7.SS2.p8.3.m3.1.1.3.3.cmml"><mi id="S7.SS2.p8.3.m3.1.1.3.3.2" xref="S7.SS2.p8.3.m3.1.1.3.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p8.3.m3.1.1.3.3.1" xref="S7.SS2.p8.3.m3.1.1.3.3.1.cmml">​</mo><mi id="S7.SS2.p8.3.m3.1.1.3.3.3" xref="S7.SS2.p8.3.m3.1.1.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p8.3.m3.1.1.3.3.1a" xref="S7.SS2.p8.3.m3.1.1.3.3.1.cmml">​</mo><mi id="S7.SS2.p8.3.m3.1.1.3.3.4" xref="S7.SS2.p8.3.m3.1.1.3.3.4.cmml">v</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p8.3.m3.1.1.3.3.1b" xref="S7.SS2.p8.3.m3.1.1.3.3.1.cmml">​</mo><mi id="S7.SS2.p8.3.m3.1.1.3.3.5" xref="S7.SS2.p8.3.m3.1.1.3.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p8.3.m3.1.1.3.3.1c" xref="S7.SS2.p8.3.m3.1.1.3.3.1.cmml">​</mo><mi id="S7.SS2.p8.3.m3.1.1.3.3.6" xref="S7.SS2.p8.3.m3.1.1.3.3.6.cmml">l</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p8.3.m3.1b"><apply id="S7.SS2.p8.3.m3.1.1.cmml" xref="S7.SS2.p8.3.m3.1.1"><times id="S7.SS2.p8.3.m3.1.1.1.cmml" xref="S7.SS2.p8.3.m3.1.1.1"></times><ci id="S7.SS2.p8.3.m3.1.1.2.cmml" xref="S7.SS2.p8.3.m3.1.1.2">𝐴</ci><apply id="S7.SS2.p8.3.m3.1.1.3.cmml" xref="S7.SS2.p8.3.m3.1.1.3"><csymbol cd="ambiguous" id="S7.SS2.p8.3.m3.1.1.3.1.cmml" xref="S7.SS2.p8.3.m3.1.1.3">subscript</csymbol><ci id="S7.SS2.p8.3.m3.1.1.3.2.cmml" xref="S7.SS2.p8.3.m3.1.1.3.2">𝑃</ci><apply id="S7.SS2.p8.3.m3.1.1.3.3.cmml" xref="S7.SS2.p8.3.m3.1.1.3.3"><times id="S7.SS2.p8.3.m3.1.1.3.3.1.cmml" xref="S7.SS2.p8.3.m3.1.1.3.3.1"></times><ci id="S7.SS2.p8.3.m3.1.1.3.3.2.cmml" xref="S7.SS2.p8.3.m3.1.1.3.3.2">𝑛</ci><ci id="S7.SS2.p8.3.m3.1.1.3.3.3.cmml" xref="S7.SS2.p8.3.m3.1.1.3.3.3">𝑜</ci><ci id="S7.SS2.p8.3.m3.1.1.3.3.4.cmml" xref="S7.SS2.p8.3.m3.1.1.3.3.4">𝑣</ci><ci id="S7.SS2.p8.3.m3.1.1.3.3.5.cmml" xref="S7.SS2.p8.3.m3.1.1.3.3.5">𝑒</ci><ci id="S7.SS2.p8.3.m3.1.1.3.3.6.cmml" xref="S7.SS2.p8.3.m3.1.1.3.3.6">𝑙</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p8.3.m3.1c">AP_{novel}</annotation></semantics></math> and <math id="S7.SS2.p8.4.m4.1" class="ltx_Math" alttext="AP_{base}" display="inline"><semantics id="S7.SS2.p8.4.m4.1a"><mrow id="S7.SS2.p8.4.m4.1.1" xref="S7.SS2.p8.4.m4.1.1.cmml"><mi id="S7.SS2.p8.4.m4.1.1.2" xref="S7.SS2.p8.4.m4.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p8.4.m4.1.1.1" xref="S7.SS2.p8.4.m4.1.1.1.cmml">​</mo><msub id="S7.SS2.p8.4.m4.1.1.3" xref="S7.SS2.p8.4.m4.1.1.3.cmml"><mi id="S7.SS2.p8.4.m4.1.1.3.2" xref="S7.SS2.p8.4.m4.1.1.3.2.cmml">P</mi><mrow id="S7.SS2.p8.4.m4.1.1.3.3" xref="S7.SS2.p8.4.m4.1.1.3.3.cmml"><mi id="S7.SS2.p8.4.m4.1.1.3.3.2" xref="S7.SS2.p8.4.m4.1.1.3.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p8.4.m4.1.1.3.3.1" xref="S7.SS2.p8.4.m4.1.1.3.3.1.cmml">​</mo><mi id="S7.SS2.p8.4.m4.1.1.3.3.3" xref="S7.SS2.p8.4.m4.1.1.3.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p8.4.m4.1.1.3.3.1a" xref="S7.SS2.p8.4.m4.1.1.3.3.1.cmml">​</mo><mi id="S7.SS2.p8.4.m4.1.1.3.3.4" xref="S7.SS2.p8.4.m4.1.1.3.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p8.4.m4.1.1.3.3.1b" xref="S7.SS2.p8.4.m4.1.1.3.3.1.cmml">​</mo><mi id="S7.SS2.p8.4.m4.1.1.3.3.5" xref="S7.SS2.p8.4.m4.1.1.3.3.5.cmml">e</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p8.4.m4.1b"><apply id="S7.SS2.p8.4.m4.1.1.cmml" xref="S7.SS2.p8.4.m4.1.1"><times id="S7.SS2.p8.4.m4.1.1.1.cmml" xref="S7.SS2.p8.4.m4.1.1.1"></times><ci id="S7.SS2.p8.4.m4.1.1.2.cmml" xref="S7.SS2.p8.4.m4.1.1.2">𝐴</ci><apply id="S7.SS2.p8.4.m4.1.1.3.cmml" xref="S7.SS2.p8.4.m4.1.1.3"><csymbol cd="ambiguous" id="S7.SS2.p8.4.m4.1.1.3.1.cmml" xref="S7.SS2.p8.4.m4.1.1.3">subscript</csymbol><ci id="S7.SS2.p8.4.m4.1.1.3.2.cmml" xref="S7.SS2.p8.4.m4.1.1.3.2">𝑃</ci><apply id="S7.SS2.p8.4.m4.1.1.3.3.cmml" xref="S7.SS2.p8.4.m4.1.1.3.3"><times id="S7.SS2.p8.4.m4.1.1.3.3.1.cmml" xref="S7.SS2.p8.4.m4.1.1.3.3.1"></times><ci id="S7.SS2.p8.4.m4.1.1.3.3.2.cmml" xref="S7.SS2.p8.4.m4.1.1.3.3.2">𝑏</ci><ci id="S7.SS2.p8.4.m4.1.1.3.3.3.cmml" xref="S7.SS2.p8.4.m4.1.1.3.3.3">𝑎</ci><ci id="S7.SS2.p8.4.m4.1.1.3.3.4.cmml" xref="S7.SS2.p8.4.m4.1.1.3.3.4">𝑠</ci><ci id="S7.SS2.p8.4.m4.1.1.3.3.5.cmml" xref="S7.SS2.p8.4.m4.1.1.3.3.5">𝑒</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p8.4.m4.1c">AP_{base}</annotation></semantics></math> represent detection performance on the novel class and base class, respectively. An excellent few-shot detector should achieve satisfactory performance in the novel class and avoid performance degradation in the base class <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib269" title="" class="ltx_ref">269</a>]</cite>. In the incremental detection of remote sensing objects, <math id="S7.SS2.p8.5.m5.1" class="ltx_Math" alttext="AP_{old}" display="inline"><semantics id="S7.SS2.p8.5.m5.1a"><mrow id="S7.SS2.p8.5.m5.1.1" xref="S7.SS2.p8.5.m5.1.1.cmml"><mi id="S7.SS2.p8.5.m5.1.1.2" xref="S7.SS2.p8.5.m5.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p8.5.m5.1.1.1" xref="S7.SS2.p8.5.m5.1.1.1.cmml">​</mo><msub id="S7.SS2.p8.5.m5.1.1.3" xref="S7.SS2.p8.5.m5.1.1.3.cmml"><mi id="S7.SS2.p8.5.m5.1.1.3.2" xref="S7.SS2.p8.5.m5.1.1.3.2.cmml">P</mi><mrow id="S7.SS2.p8.5.m5.1.1.3.3" xref="S7.SS2.p8.5.m5.1.1.3.3.cmml"><mi id="S7.SS2.p8.5.m5.1.1.3.3.2" xref="S7.SS2.p8.5.m5.1.1.3.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p8.5.m5.1.1.3.3.1" xref="S7.SS2.p8.5.m5.1.1.3.3.1.cmml">​</mo><mi id="S7.SS2.p8.5.m5.1.1.3.3.3" xref="S7.SS2.p8.5.m5.1.1.3.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p8.5.m5.1.1.3.3.1a" xref="S7.SS2.p8.5.m5.1.1.3.3.1.cmml">​</mo><mi id="S7.SS2.p8.5.m5.1.1.3.3.4" xref="S7.SS2.p8.5.m5.1.1.3.3.4.cmml">d</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p8.5.m5.1b"><apply id="S7.SS2.p8.5.m5.1.1.cmml" xref="S7.SS2.p8.5.m5.1.1"><times id="S7.SS2.p8.5.m5.1.1.1.cmml" xref="S7.SS2.p8.5.m5.1.1.1"></times><ci id="S7.SS2.p8.5.m5.1.1.2.cmml" xref="S7.SS2.p8.5.m5.1.1.2">𝐴</ci><apply id="S7.SS2.p8.5.m5.1.1.3.cmml" xref="S7.SS2.p8.5.m5.1.1.3"><csymbol cd="ambiguous" id="S7.SS2.p8.5.m5.1.1.3.1.cmml" xref="S7.SS2.p8.5.m5.1.1.3">subscript</csymbol><ci id="S7.SS2.p8.5.m5.1.1.3.2.cmml" xref="S7.SS2.p8.5.m5.1.1.3.2">𝑃</ci><apply id="S7.SS2.p8.5.m5.1.1.3.3.cmml" xref="S7.SS2.p8.5.m5.1.1.3.3"><times id="S7.SS2.p8.5.m5.1.1.3.3.1.cmml" xref="S7.SS2.p8.5.m5.1.1.3.3.1"></times><ci id="S7.SS2.p8.5.m5.1.1.3.3.2.cmml" xref="S7.SS2.p8.5.m5.1.1.3.3.2">𝑜</ci><ci id="S7.SS2.p8.5.m5.1.1.3.3.3.cmml" xref="S7.SS2.p8.5.m5.1.1.3.3.3">𝑙</ci><ci id="S7.SS2.p8.5.m5.1.1.3.3.4.cmml" xref="S7.SS2.p8.5.m5.1.1.3.3.4">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p8.5.m5.1c">AP_{old}</annotation></semantics></math> and <math id="S7.SS2.p8.6.m6.1" class="ltx_Math" alttext="AP_{inc}" display="inline"><semantics id="S7.SS2.p8.6.m6.1a"><mrow id="S7.SS2.p8.6.m6.1.1" xref="S7.SS2.p8.6.m6.1.1.cmml"><mi id="S7.SS2.p8.6.m6.1.1.2" xref="S7.SS2.p8.6.m6.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p8.6.m6.1.1.1" xref="S7.SS2.p8.6.m6.1.1.1.cmml">​</mo><msub id="S7.SS2.p8.6.m6.1.1.3" xref="S7.SS2.p8.6.m6.1.1.3.cmml"><mi id="S7.SS2.p8.6.m6.1.1.3.2" xref="S7.SS2.p8.6.m6.1.1.3.2.cmml">P</mi><mrow id="S7.SS2.p8.6.m6.1.1.3.3" xref="S7.SS2.p8.6.m6.1.1.3.3.cmml"><mi id="S7.SS2.p8.6.m6.1.1.3.3.2" xref="S7.SS2.p8.6.m6.1.1.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p8.6.m6.1.1.3.3.1" xref="S7.SS2.p8.6.m6.1.1.3.3.1.cmml">​</mo><mi id="S7.SS2.p8.6.m6.1.1.3.3.3" xref="S7.SS2.p8.6.m6.1.1.3.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S7.SS2.p8.6.m6.1.1.3.3.1a" xref="S7.SS2.p8.6.m6.1.1.3.3.1.cmml">​</mo><mi id="S7.SS2.p8.6.m6.1.1.3.3.4" xref="S7.SS2.p8.6.m6.1.1.3.3.4.cmml">c</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p8.6.m6.1b"><apply id="S7.SS2.p8.6.m6.1.1.cmml" xref="S7.SS2.p8.6.m6.1.1"><times id="S7.SS2.p8.6.m6.1.1.1.cmml" xref="S7.SS2.p8.6.m6.1.1.1"></times><ci id="S7.SS2.p8.6.m6.1.1.2.cmml" xref="S7.SS2.p8.6.m6.1.1.2">𝐴</ci><apply id="S7.SS2.p8.6.m6.1.1.3.cmml" xref="S7.SS2.p8.6.m6.1.1.3"><csymbol cd="ambiguous" id="S7.SS2.p8.6.m6.1.1.3.1.cmml" xref="S7.SS2.p8.6.m6.1.1.3">subscript</csymbol><ci id="S7.SS2.p8.6.m6.1.1.3.2.cmml" xref="S7.SS2.p8.6.m6.1.1.3.2">𝑃</ci><apply id="S7.SS2.p8.6.m6.1.1.3.3.cmml" xref="S7.SS2.p8.6.m6.1.1.3.3"><times id="S7.SS2.p8.6.m6.1.1.3.3.1.cmml" xref="S7.SS2.p8.6.m6.1.1.3.3.1"></times><ci id="S7.SS2.p8.6.m6.1.1.3.3.2.cmml" xref="S7.SS2.p8.6.m6.1.1.3.3.2">𝑖</ci><ci id="S7.SS2.p8.6.m6.1.1.3.3.3.cmml" xref="S7.SS2.p8.6.m6.1.1.3.3.3">𝑛</ci><ci id="S7.SS2.p8.6.m6.1.1.3.3.4.cmml" xref="S7.SS2.p8.6.m6.1.1.3.3.4">𝑐</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p8.6.m6.1c">AP_{inc}</annotation></semantics></math> are employed to evaluate the performance of the old and incremental classes on different incremental tasks. In addition, the harmonic mean is also a vital evaluation metric for incremental object detection<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib288" title="" class="ltx_ref">288</a>]</cite>, which provides a comprehensive performance evaluation of both old and incremental classes, as described by Eq. <a href="#S7.E5" title="In VII-B Evaluation Metrics ‣ VII Datasets and Evaluation Metrics ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>:</p>
<table id="S7.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S7.E5.m1.1" class="ltx_Math" alttext="HM=\frac{2AP_{old}AP_{inc}}{AP_{old}+AP_{inc}}" display="block"><semantics id="S7.E5.m1.1a"><mrow id="S7.E5.m1.1.1" xref="S7.E5.m1.1.1.cmml"><mrow id="S7.E5.m1.1.1.2" xref="S7.E5.m1.1.1.2.cmml"><mi id="S7.E5.m1.1.1.2.2" xref="S7.E5.m1.1.1.2.2.cmml">H</mi><mo lspace="0em" rspace="0em" id="S7.E5.m1.1.1.2.1" xref="S7.E5.m1.1.1.2.1.cmml">​</mo><mi id="S7.E5.m1.1.1.2.3" xref="S7.E5.m1.1.1.2.3.cmml">M</mi></mrow><mo id="S7.E5.m1.1.1.1" xref="S7.E5.m1.1.1.1.cmml">=</mo><mfrac id="S7.E5.m1.1.1.3" xref="S7.E5.m1.1.1.3.cmml"><mrow id="S7.E5.m1.1.1.3.2" xref="S7.E5.m1.1.1.3.2.cmml"><mn id="S7.E5.m1.1.1.3.2.2" xref="S7.E5.m1.1.1.3.2.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S7.E5.m1.1.1.3.2.1" xref="S7.E5.m1.1.1.3.2.1.cmml">​</mo><mi id="S7.E5.m1.1.1.3.2.3" xref="S7.E5.m1.1.1.3.2.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S7.E5.m1.1.1.3.2.1a" xref="S7.E5.m1.1.1.3.2.1.cmml">​</mo><msub id="S7.E5.m1.1.1.3.2.4" xref="S7.E5.m1.1.1.3.2.4.cmml"><mi id="S7.E5.m1.1.1.3.2.4.2" xref="S7.E5.m1.1.1.3.2.4.2.cmml">P</mi><mrow id="S7.E5.m1.1.1.3.2.4.3" xref="S7.E5.m1.1.1.3.2.4.3.cmml"><mi id="S7.E5.m1.1.1.3.2.4.3.2" xref="S7.E5.m1.1.1.3.2.4.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S7.E5.m1.1.1.3.2.4.3.1" xref="S7.E5.m1.1.1.3.2.4.3.1.cmml">​</mo><mi id="S7.E5.m1.1.1.3.2.4.3.3" xref="S7.E5.m1.1.1.3.2.4.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S7.E5.m1.1.1.3.2.4.3.1a" xref="S7.E5.m1.1.1.3.2.4.3.1.cmml">​</mo><mi id="S7.E5.m1.1.1.3.2.4.3.4" xref="S7.E5.m1.1.1.3.2.4.3.4.cmml">d</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S7.E5.m1.1.1.3.2.1b" xref="S7.E5.m1.1.1.3.2.1.cmml">​</mo><mi id="S7.E5.m1.1.1.3.2.5" xref="S7.E5.m1.1.1.3.2.5.cmml">A</mi><mo lspace="0em" rspace="0em" id="S7.E5.m1.1.1.3.2.1c" xref="S7.E5.m1.1.1.3.2.1.cmml">​</mo><msub id="S7.E5.m1.1.1.3.2.6" xref="S7.E5.m1.1.1.3.2.6.cmml"><mi id="S7.E5.m1.1.1.3.2.6.2" xref="S7.E5.m1.1.1.3.2.6.2.cmml">P</mi><mrow id="S7.E5.m1.1.1.3.2.6.3" xref="S7.E5.m1.1.1.3.2.6.3.cmml"><mi id="S7.E5.m1.1.1.3.2.6.3.2" xref="S7.E5.m1.1.1.3.2.6.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S7.E5.m1.1.1.3.2.6.3.1" xref="S7.E5.m1.1.1.3.2.6.3.1.cmml">​</mo><mi id="S7.E5.m1.1.1.3.2.6.3.3" xref="S7.E5.m1.1.1.3.2.6.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S7.E5.m1.1.1.3.2.6.3.1a" xref="S7.E5.m1.1.1.3.2.6.3.1.cmml">​</mo><mi id="S7.E5.m1.1.1.3.2.6.3.4" xref="S7.E5.m1.1.1.3.2.6.3.4.cmml">c</mi></mrow></msub></mrow><mrow id="S7.E5.m1.1.1.3.3" xref="S7.E5.m1.1.1.3.3.cmml"><mrow id="S7.E5.m1.1.1.3.3.2" xref="S7.E5.m1.1.1.3.3.2.cmml"><mi id="S7.E5.m1.1.1.3.3.2.2" xref="S7.E5.m1.1.1.3.3.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S7.E5.m1.1.1.3.3.2.1" xref="S7.E5.m1.1.1.3.3.2.1.cmml">​</mo><msub id="S7.E5.m1.1.1.3.3.2.3" xref="S7.E5.m1.1.1.3.3.2.3.cmml"><mi id="S7.E5.m1.1.1.3.3.2.3.2" xref="S7.E5.m1.1.1.3.3.2.3.2.cmml">P</mi><mrow id="S7.E5.m1.1.1.3.3.2.3.3" xref="S7.E5.m1.1.1.3.3.2.3.3.cmml"><mi id="S7.E5.m1.1.1.3.3.2.3.3.2" xref="S7.E5.m1.1.1.3.3.2.3.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S7.E5.m1.1.1.3.3.2.3.3.1" xref="S7.E5.m1.1.1.3.3.2.3.3.1.cmml">​</mo><mi id="S7.E5.m1.1.1.3.3.2.3.3.3" xref="S7.E5.m1.1.1.3.3.2.3.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S7.E5.m1.1.1.3.3.2.3.3.1a" xref="S7.E5.m1.1.1.3.3.2.3.3.1.cmml">​</mo><mi id="S7.E5.m1.1.1.3.3.2.3.3.4" xref="S7.E5.m1.1.1.3.3.2.3.3.4.cmml">d</mi></mrow></msub></mrow><mo id="S7.E5.m1.1.1.3.3.1" xref="S7.E5.m1.1.1.3.3.1.cmml">+</mo><mrow id="S7.E5.m1.1.1.3.3.3" xref="S7.E5.m1.1.1.3.3.3.cmml"><mi id="S7.E5.m1.1.1.3.3.3.2" xref="S7.E5.m1.1.1.3.3.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S7.E5.m1.1.1.3.3.3.1" xref="S7.E5.m1.1.1.3.3.3.1.cmml">​</mo><msub id="S7.E5.m1.1.1.3.3.3.3" xref="S7.E5.m1.1.1.3.3.3.3.cmml"><mi id="S7.E5.m1.1.1.3.3.3.3.2" xref="S7.E5.m1.1.1.3.3.3.3.2.cmml">P</mi><mrow id="S7.E5.m1.1.1.3.3.3.3.3" xref="S7.E5.m1.1.1.3.3.3.3.3.cmml"><mi id="S7.E5.m1.1.1.3.3.3.3.3.2" xref="S7.E5.m1.1.1.3.3.3.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S7.E5.m1.1.1.3.3.3.3.3.1" xref="S7.E5.m1.1.1.3.3.3.3.3.1.cmml">​</mo><mi id="S7.E5.m1.1.1.3.3.3.3.3.3" xref="S7.E5.m1.1.1.3.3.3.3.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S7.E5.m1.1.1.3.3.3.3.3.1a" xref="S7.E5.m1.1.1.3.3.3.3.3.1.cmml">​</mo><mi id="S7.E5.m1.1.1.3.3.3.3.3.4" xref="S7.E5.m1.1.1.3.3.3.3.3.4.cmml">c</mi></mrow></msub></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S7.E5.m1.1b"><apply id="S7.E5.m1.1.1.cmml" xref="S7.E5.m1.1.1"><eq id="S7.E5.m1.1.1.1.cmml" xref="S7.E5.m1.1.1.1"></eq><apply id="S7.E5.m1.1.1.2.cmml" xref="S7.E5.m1.1.1.2"><times id="S7.E5.m1.1.1.2.1.cmml" xref="S7.E5.m1.1.1.2.1"></times><ci id="S7.E5.m1.1.1.2.2.cmml" xref="S7.E5.m1.1.1.2.2">𝐻</ci><ci id="S7.E5.m1.1.1.2.3.cmml" xref="S7.E5.m1.1.1.2.3">𝑀</ci></apply><apply id="S7.E5.m1.1.1.3.cmml" xref="S7.E5.m1.1.1.3"><divide id="S7.E5.m1.1.1.3.1.cmml" xref="S7.E5.m1.1.1.3"></divide><apply id="S7.E5.m1.1.1.3.2.cmml" xref="S7.E5.m1.1.1.3.2"><times id="S7.E5.m1.1.1.3.2.1.cmml" xref="S7.E5.m1.1.1.3.2.1"></times><cn type="integer" id="S7.E5.m1.1.1.3.2.2.cmml" xref="S7.E5.m1.1.1.3.2.2">2</cn><ci id="S7.E5.m1.1.1.3.2.3.cmml" xref="S7.E5.m1.1.1.3.2.3">𝐴</ci><apply id="S7.E5.m1.1.1.3.2.4.cmml" xref="S7.E5.m1.1.1.3.2.4"><csymbol cd="ambiguous" id="S7.E5.m1.1.1.3.2.4.1.cmml" xref="S7.E5.m1.1.1.3.2.4">subscript</csymbol><ci id="S7.E5.m1.1.1.3.2.4.2.cmml" xref="S7.E5.m1.1.1.3.2.4.2">𝑃</ci><apply id="S7.E5.m1.1.1.3.2.4.3.cmml" xref="S7.E5.m1.1.1.3.2.4.3"><times id="S7.E5.m1.1.1.3.2.4.3.1.cmml" xref="S7.E5.m1.1.1.3.2.4.3.1"></times><ci id="S7.E5.m1.1.1.3.2.4.3.2.cmml" xref="S7.E5.m1.1.1.3.2.4.3.2">𝑜</ci><ci id="S7.E5.m1.1.1.3.2.4.3.3.cmml" xref="S7.E5.m1.1.1.3.2.4.3.3">𝑙</ci><ci id="S7.E5.m1.1.1.3.2.4.3.4.cmml" xref="S7.E5.m1.1.1.3.2.4.3.4">𝑑</ci></apply></apply><ci id="S7.E5.m1.1.1.3.2.5.cmml" xref="S7.E5.m1.1.1.3.2.5">𝐴</ci><apply id="S7.E5.m1.1.1.3.2.6.cmml" xref="S7.E5.m1.1.1.3.2.6"><csymbol cd="ambiguous" id="S7.E5.m1.1.1.3.2.6.1.cmml" xref="S7.E5.m1.1.1.3.2.6">subscript</csymbol><ci id="S7.E5.m1.1.1.3.2.6.2.cmml" xref="S7.E5.m1.1.1.3.2.6.2">𝑃</ci><apply id="S7.E5.m1.1.1.3.2.6.3.cmml" xref="S7.E5.m1.1.1.3.2.6.3"><times id="S7.E5.m1.1.1.3.2.6.3.1.cmml" xref="S7.E5.m1.1.1.3.2.6.3.1"></times><ci id="S7.E5.m1.1.1.3.2.6.3.2.cmml" xref="S7.E5.m1.1.1.3.2.6.3.2">𝑖</ci><ci id="S7.E5.m1.1.1.3.2.6.3.3.cmml" xref="S7.E5.m1.1.1.3.2.6.3.3">𝑛</ci><ci id="S7.E5.m1.1.1.3.2.6.3.4.cmml" xref="S7.E5.m1.1.1.3.2.6.3.4">𝑐</ci></apply></apply></apply><apply id="S7.E5.m1.1.1.3.3.cmml" xref="S7.E5.m1.1.1.3.3"><plus id="S7.E5.m1.1.1.3.3.1.cmml" xref="S7.E5.m1.1.1.3.3.1"></plus><apply id="S7.E5.m1.1.1.3.3.2.cmml" xref="S7.E5.m1.1.1.3.3.2"><times id="S7.E5.m1.1.1.3.3.2.1.cmml" xref="S7.E5.m1.1.1.3.3.2.1"></times><ci id="S7.E5.m1.1.1.3.3.2.2.cmml" xref="S7.E5.m1.1.1.3.3.2.2">𝐴</ci><apply id="S7.E5.m1.1.1.3.3.2.3.cmml" xref="S7.E5.m1.1.1.3.3.2.3"><csymbol cd="ambiguous" id="S7.E5.m1.1.1.3.3.2.3.1.cmml" xref="S7.E5.m1.1.1.3.3.2.3">subscript</csymbol><ci id="S7.E5.m1.1.1.3.3.2.3.2.cmml" xref="S7.E5.m1.1.1.3.3.2.3.2">𝑃</ci><apply id="S7.E5.m1.1.1.3.3.2.3.3.cmml" xref="S7.E5.m1.1.1.3.3.2.3.3"><times id="S7.E5.m1.1.1.3.3.2.3.3.1.cmml" xref="S7.E5.m1.1.1.3.3.2.3.3.1"></times><ci id="S7.E5.m1.1.1.3.3.2.3.3.2.cmml" xref="S7.E5.m1.1.1.3.3.2.3.3.2">𝑜</ci><ci id="S7.E5.m1.1.1.3.3.2.3.3.3.cmml" xref="S7.E5.m1.1.1.3.3.2.3.3.3">𝑙</ci><ci id="S7.E5.m1.1.1.3.3.2.3.3.4.cmml" xref="S7.E5.m1.1.1.3.3.2.3.3.4">𝑑</ci></apply></apply></apply><apply id="S7.E5.m1.1.1.3.3.3.cmml" xref="S7.E5.m1.1.1.3.3.3"><times id="S7.E5.m1.1.1.3.3.3.1.cmml" xref="S7.E5.m1.1.1.3.3.3.1"></times><ci id="S7.E5.m1.1.1.3.3.3.2.cmml" xref="S7.E5.m1.1.1.3.3.3.2">𝐴</ci><apply id="S7.E5.m1.1.1.3.3.3.3.cmml" xref="S7.E5.m1.1.1.3.3.3.3"><csymbol cd="ambiguous" id="S7.E5.m1.1.1.3.3.3.3.1.cmml" xref="S7.E5.m1.1.1.3.3.3.3">subscript</csymbol><ci id="S7.E5.m1.1.1.3.3.3.3.2.cmml" xref="S7.E5.m1.1.1.3.3.3.3.2">𝑃</ci><apply id="S7.E5.m1.1.1.3.3.3.3.3.cmml" xref="S7.E5.m1.1.1.3.3.3.3.3"><times id="S7.E5.m1.1.1.3.3.3.3.3.1.cmml" xref="S7.E5.m1.1.1.3.3.3.3.3.1"></times><ci id="S7.E5.m1.1.1.3.3.3.3.3.2.cmml" xref="S7.E5.m1.1.1.3.3.3.3.3.2">𝑖</ci><ci id="S7.E5.m1.1.1.3.3.3.3.3.3.cmml" xref="S7.E5.m1.1.1.3.3.3.3.3.3">𝑛</ci><ci id="S7.E5.m1.1.1.3.3.3.3.3.4.cmml" xref="S7.E5.m1.1.1.3.3.3.3.3.4">𝑐</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.E5.m1.1c">HM=\frac{2AP_{old}AP_{inc}}{AP_{old}+AP_{inc}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span id="S8.1.1" class="ltx_text ltx_font_smallcaps">Applications</span>
</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">Deep learning techniques have injected significant innovations into RSOD, leading to an effective way to automatically identify objects of interest from voluminous RSIs.
Therefore, RSOD methods have been applied in a rich diversity of practice scenarios that significantly support the implementation of Sustainable Development Goals (SDGs) and the improvement of society<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib289" title="" class="ltx_ref">289</a>, <a href="#bib.bib290" title="" class="ltx_ref">290</a>, <a href="#bib.bib291" title="" class="ltx_ref">291</a>]</cite>, as depicted in Fig. <a href="#S7.F19" title="Figure 19 ‣ VII-A Datasets Introduction and Selection ‣ VII Datasets and Evaluation Metrics ‣ Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances" class="ltx_ref"><span class="ltx_text ltx_ref_tag">19</span></a>.</p>
</div>
<section id="S8.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S8.SS1.4.1.1" class="ltx_text">VIII-A</span> </span><span id="S8.SS1.5.2" class="ltx_text ltx_font_italic">Disaster Management</span>
</h3>

<div id="S8.SS1.p1" class="ltx_para">
<p id="S8.SS1.p1.1" class="ltx_p">Natural disasters pose a serious threat to the safety of human life and property. A quick and precise understanding of disaster impact and extent of damage is critical to disaster management.
RSOD methods can accurately identify ground objects from a bird’s-eye view of the disaster-affected area, providing a novel potential for disaster management
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib292" title="" class="ltx_ref">292</a>, <a href="#bib.bib293" title="" class="ltx_ref">293</a>, <a href="#bib.bib294" title="" class="ltx_ref">294</a>, <a href="#bib.bib295" title="" class="ltx_ref">295</a>, <a href="#bib.bib296" title="" class="ltx_ref">296</a>]</cite>. Guan <span id="S8.SS1.p1.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib293" title="" class="ltx_ref">293</a>]</cite> proposed a novel instance segmentation model to accurately detect fire in a complex environment, which can be applied to the forest fire disaster response. Ma <span id="S8.SS1.p1.1.2" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib295" title="" class="ltx_ref">295</a>]</cite> designed a real-time detection method for collapsed building assessment in Post-Earthquake.</p>
</div>
</section>
<section id="S8.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S8.SS2.4.1.1" class="ltx_text">VIII-B</span> </span><span id="S8.SS2.5.2" class="ltx_text ltx_font_italic">Precision Agriculture</span>
</h3>

<div id="S8.SS2.p1" class="ltx_para">
<p id="S8.SS2.p1.1" class="ltx_p">With the unprecedented and still-expanding population, ensuring agricultural production is a fundamental obstacle to feeding the growing population. RSOD has the ability to monitor crop growth and estimate food production, promoting further progress for precision agriculture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib297" title="" class="ltx_ref">297</a>, <a href="#bib.bib298" title="" class="ltx_ref">298</a>, <a href="#bib.bib299" title="" class="ltx_ref">299</a>, <a href="#bib.bib300" title="" class="ltx_ref">300</a>, <a href="#bib.bib301" title="" class="ltx_ref">301</a>, <a href="#bib.bib302" title="" class="ltx_ref">302</a>]</cite>. Pang <span id="S8.SS2.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib298" title="" class="ltx_ref">298</a>]</cite> used RSIs for early-season maize detection and achieved an accurate estimation of emergence rates. Chen <span id="S8.SS2.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib302" title="" class="ltx_ref">302</a>]</cite> designed an automatic strawberry flower detection system to monitor the growth cycle of strawberry fields.</p>
</div>
</section>
<section id="S8.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S8.SS3.4.1.1" class="ltx_text">VIII-C</span> </span><span id="S8.SS3.5.2" class="ltx_text ltx_font_italic">Sustainable Cities and Communities</span>
</h3>

<div id="S8.SS3.p1" class="ltx_para">
<p id="S8.SS3.p1.1" class="ltx_p">Half of the global population now lives in cities, and this population will keep growing in the coming decades. Sustainable cities and communities are the goals of modern city development, in which RSOD can make a significant impact. For instance, building and vehicle detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib303" title="" class="ltx_ref">303</a>, <a href="#bib.bib304" title="" class="ltx_ref">304</a>, <a href="#bib.bib305" title="" class="ltx_ref">305</a>, <a href="#bib.bib306" title="" class="ltx_ref">306</a>]</cite> can help estimate population density distribution and transport traffic statistics, providing suggestions for city development planning. Infrastructure distribution detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib307" title="" class="ltx_ref">307</a>]</cite> can assist in disaster assessment and early warning in the city environment.</p>
</div>
</section>
<section id="S8.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S8.SS4.4.1.1" class="ltx_text">VIII-D</span> </span><span id="S8.SS4.5.2" class="ltx_text ltx_font_italic">Climate Action</span>
</h3>

<div id="S8.SS4.p1" class="ltx_para">
<p id="S8.SS4.p1.1" class="ltx_p">The ongoing climate change forces humans to face the daunting challenge of the climate crisis.
Some researchers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib308" title="" class="ltx_ref">308</a>, <a href="#bib.bib309" title="" class="ltx_ref">309</a>, <a href="#bib.bib310" title="" class="ltx_ref">310</a>]</cite> employed object detection methods for automatically mapping tundra ice-wedge polygon to document and analyze the effects of climate warming on the Arctic region. Besides, RSOD can produce statistics on the number and spatial distribution of solar panels and wind turbines <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib311" title="" class="ltx_ref">311</a>, <a href="#bib.bib312" title="" class="ltx_ref">312</a>, <a href="#bib.bib313" title="" class="ltx_ref">313</a>, <a href="#bib.bib314" title="" class="ltx_ref">314</a>]</cite>, facilitating the mitigation of greenhouse gas emissions.</p>
</div>
</section>
<section id="S8.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S8.SS5.4.1.1" class="ltx_text">VIII-E</span> </span><span id="S8.SS5.5.2" class="ltx_text ltx_font_italic">Ocean Conservation</span>
</h3>

<div id="S8.SS5.p1" class="ltx_para">
<p id="S8.SS5.p1.1" class="ltx_p">The oceans cover nearly three-quarters of the Earth’s surface, and more than 3 billion people depend on the diverse life of the oceans and coasts. The ocean is gradually deteriorating due to pollution, and the RSOD can provide powerful support for ocean conservation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib315" title="" class="ltx_ref">315</a>]</cite>.
Several works applied detection methods for litter detection along shores<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib316" title="" class="ltx_ref">316</a>]</cite>, floating plastic detection at sea <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib317" title="" class="ltx_ref">317</a>]</cite>, deep-sea debris detection<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib318" title="" class="ltx_ref">318</a>]</cite>, etc. Another important application is ship detection<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib135" title="" class="ltx_ref">135</a>, <a href="#bib.bib136" title="" class="ltx_ref">136</a>]</cite>, which can help monitor illegal fishing activities.</p>
</div>
</section>
<section id="S8.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S8.SS6.4.1.1" class="ltx_text">VIII-F</span> </span><span id="S8.SS6.5.2" class="ltx_text ltx_font_italic">Wildlife Surveillance</span>
</h3>

<div id="S8.SS6.p1" class="ltx_para">
<p id="S8.SS6.p1.1" class="ltx_p">A global loss of biodiversity is observed at all levels, and object detection in combination with RSIs provides a novel perspective for wildlife conservation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib319" title="" class="ltx_ref">319</a>, <a href="#bib.bib320" title="" class="ltx_ref">320</a>, <a href="#bib.bib321" title="" class="ltx_ref">321</a>, <a href="#bib.bib322" title="" class="ltx_ref">322</a>, <a href="#bib.bib323" title="" class="ltx_ref">323</a>]</cite>. Delplanque <span id="S8.SS6.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib322" title="" class="ltx_ref">322</a>]</cite> adopted the deep learning based detector for multi-species detection and identification of African mammals. Kellenberger <span id="S8.SS6.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib323" title="" class="ltx_ref">323</a>]</cite> designed a weakly supervised wildlife detection framework that only requires image-level labels to identify wildlife.</p>
</div>
</section>
<section id="S8.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S8.SS7.4.1.1" class="ltx_text">VIII-G</span> </span><span id="S8.SS7.5.2" class="ltx_text ltx_font_italic">Forest Ecosystem Protection</span>
</h3>

<div id="S8.SS7.p1" class="ltx_para">
<p id="S8.SS7.p1.1" class="ltx_p">The forest ecosystem plays an important role in ecological protection, climate regulation, and carbon cycling. Understanding the condition of trees is essential for forest ecosystem protection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib324" title="" class="ltx_ref">324</a>, <a href="#bib.bib325" title="" class="ltx_ref">325</a>, <a href="#bib.bib326" title="" class="ltx_ref">326</a>, <a href="#bib.bib327" title="" class="ltx_ref">327</a>, <a href="#bib.bib328" title="" class="ltx_ref">328</a>]</cite>. Safonova <span id="S8.SS7.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib326" title="" class="ltx_ref">326</a>]</cite> analyzed the shape, texture, and color of the detected trees’ crowns to determine their damage stage, providing a more efficient way to assess forest health. Sani-Mohammed <span id="S8.SS7.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib328" title="" class="ltx_ref">328</a>]</cite> utilized an instance segmentation approach to map the standing dead trees, which is imperative for forest ecosystem management and protection.</p>
</div>
</section>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IX </span><span id="S9.1.1" class="ltx_text ltx_font_smallcaps">Future Directions</span>
</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p">Apart from the five RSOD research topics mentioned in this survey, there is still much work to be done in this field. Therefore, we present a forward-looking discussion of future directions to further improve and enhance the detectors in remote sensing scenes.</p>
</div>
<section id="S9.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S9.SS1.4.1.1" class="ltx_text">IX-A</span> </span><span id="S9.SS1.5.2" class="ltx_text ltx_font_italic">Unified detection framework for large-scale remote sensing images</span>
</h3>

<div id="S9.SS1.p1" class="ltx_para">
<p id="S9.SS1.p1.1" class="ltx_p">Benefiting from the development of remote sensing technology, high-resolution large-scale RSIs (e.g., over <math id="S9.SS1.p1.1.m1.3" class="ltx_Math" alttext="10,000\times 10,000" display="inline"><semantics id="S9.SS1.p1.1.m1.3a"><mrow id="S9.SS1.p1.1.m1.3.3.1" xref="S9.SS1.p1.1.m1.3.3.2.cmml"><mn id="S9.SS1.p1.1.m1.1.1" xref="S9.SS1.p1.1.m1.1.1.cmml">10</mn><mo id="S9.SS1.p1.1.m1.3.3.1.2" xref="S9.SS1.p1.1.m1.3.3.2.cmml">,</mo><mrow id="S9.SS1.p1.1.m1.3.3.1.1" xref="S9.SS1.p1.1.m1.3.3.1.1.cmml"><mn id="S9.SS1.p1.1.m1.3.3.1.1.2" xref="S9.SS1.p1.1.m1.3.3.1.1.2.cmml">000</mn><mo lspace="0.222em" rspace="0.222em" id="S9.SS1.p1.1.m1.3.3.1.1.1" xref="S9.SS1.p1.1.m1.3.3.1.1.1.cmml">×</mo><mn id="S9.SS1.p1.1.m1.3.3.1.1.3" xref="S9.SS1.p1.1.m1.3.3.1.1.3.cmml">10</mn></mrow><mo id="S9.SS1.p1.1.m1.3.3.1.3" xref="S9.SS1.p1.1.m1.3.3.2.cmml">,</mo><mn id="S9.SS1.p1.1.m1.2.2" xref="S9.SS1.p1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S9.SS1.p1.1.m1.3b"><list id="S9.SS1.p1.1.m1.3.3.2.cmml" xref="S9.SS1.p1.1.m1.3.3.1"><cn type="integer" id="S9.SS1.p1.1.m1.1.1.cmml" xref="S9.SS1.p1.1.m1.1.1">10</cn><apply id="S9.SS1.p1.1.m1.3.3.1.1.cmml" xref="S9.SS1.p1.1.m1.3.3.1.1"><times id="S9.SS1.p1.1.m1.3.3.1.1.1.cmml" xref="S9.SS1.p1.1.m1.3.3.1.1.1"></times><cn type="integer" id="S9.SS1.p1.1.m1.3.3.1.1.2.cmml" xref="S9.SS1.p1.1.m1.3.3.1.1.2">000</cn><cn type="integer" id="S9.SS1.p1.1.m1.3.3.1.1.3.cmml" xref="S9.SS1.p1.1.m1.3.3.1.1.3">10</cn></apply><cn type="integer" id="S9.SS1.p1.1.m1.2.2.cmml" xref="S9.SS1.p1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S9.SS1.p1.1.m1.3c">10,000\times 10,000</annotation></semantics></math> pixels) can be easily obtained. However, limited by the GPU memory, the current mainstream RSOD methods fail to directly perform object detection in large-scale RSIs but adopt a sliding window strategy, mainly including sliding window cropping, patch prediction, and results merging. On the one hand, this sliding window framework requires complex data pre-processing and post-processing, compared with the unified detection framework. On the other hand, the objects usually occupy a small area of the RSIs, and the invalid calculation of the massive backgrounds leads to increasing computation time and memory consumption. Some studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib215" title="" class="ltx_ref">215</a>, <a href="#bib.bib329" title="" class="ltx_ref">329</a>, <a href="#bib.bib330" title="" class="ltx_ref">330</a>]</cite> proposed a coarse-to-fine detection framework for object detection in large-scale RSIs.
This framework first locates the regions of interest by filtering out meaningless regions and then achieves accurate detection from these filtered regions.</p>
</div>
</section>
<section id="S9.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S9.SS2.4.1.1" class="ltx_text">IX-B</span> </span><span id="S9.SS2.5.2" class="ltx_text ltx_font_italic">Detection with Multi-modal remote sensing images</span>
</h3>

<div id="S9.SS2.p1" class="ltx_para">
<p id="S9.SS2.p1.1" class="ltx_p">Restricted by the sensor imaging mechanism, the detectors based on the single-modal RSIs often have detection performance deviations, which are difficult to meet in practical applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib331" title="" class="ltx_ref">331</a>]</cite>. In contrast, the multi-modal RSIs from different sensors have their characteristics. For instance, hyperspectral images contain high spectral resolution and fine-grained spectral features, SAR images provide abundant texture information, and optical images exhibit high spatial resolution with rich detailed information. The integrated processing of multi-modal RSIs can improve the interpretation ability of the scene and obtain a more objective and comprehensive understanding of the geospatial objects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib332" title="" class="ltx_ref">332</a>, <a href="#bib.bib333" title="" class="ltx_ref">333</a>, <a href="#bib.bib334" title="" class="ltx_ref">334</a>]</cite>, providing the possibility to further improve the detection performance of RSOD.</p>
</div>
</section>
<section id="S9.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S9.SS3.4.1.1" class="ltx_text">IX-C</span> </span><span id="S9.SS3.5.2" class="ltx_text ltx_font_italic">Domain adaptation object detection in remote sensing images</span>
</h3>

<div id="S9.SS3.p1" class="ltx_para">
<p id="S9.SS3.p1.1" class="ltx_p">Due to the diversity of remote sensing satellite sensors, resolutions, and bands, as well as the influence of weather conditions, seasons, and geospatial regions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, RSIs collected from different satellites are generally drawn from similar but not identical distributions. Such distribution differences (also called the domain gap) severely restrict the generalization performance of the detector. Recent studies on domain adaptation object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib335" title="" class="ltx_ref">335</a>, <a href="#bib.bib336" title="" class="ltx_ref">336</a>, <a href="#bib.bib337" title="" class="ltx_ref">337</a>, <a href="#bib.bib338" title="" class="ltx_ref">338</a>]</cite> have proposed to tackle the domain gap problem. However, these studies only focus on the domain adaptation detectors in the single-modal, while the cross-modal domain adaptation object detection (e.g., from optical images to SAR images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib339" title="" class="ltx_ref">339</a>, <a href="#bib.bib340" title="" class="ltx_ref">340</a>]</cite>) is a more challenging and worthwhile topic to investigate.</p>
</div>
</section>
<section id="S9.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S9.SS4.4.1.1" class="ltx_text">IX-D</span> </span><span id="S9.SS4.5.2" class="ltx_text ltx_font_italic">Incremental detection of remote sensing objects</span>
</h3>

<div id="S9.SS4.p1" class="ltx_para">
<p id="S9.SS4.p1.1" class="ltx_p">The real-world environment is dynamic and open, where the number of categories evolved over time. However, mainstream detectors require both old and new data to retrain the model when meeting new categories, resulting in high computational costs. Recently, incremental learning has been considered the most promising way to solve this problem, which can learn new knowledge without forgetting old knowledge with only new data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib341" title="" class="ltx_ref">341</a>]</cite>. Incremental learning has been preliminarily explored in the remote sensing community <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib342" title="" class="ltx_ref">342</a>, <a href="#bib.bib343" title="" class="ltx_ref">343</a>, <a href="#bib.bib344" title="" class="ltx_ref">344</a>, <a href="#bib.bib345" title="" class="ltx_ref">345</a>]</cite>. For example, Chen <span id="S9.SS4.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib342" title="" class="ltx_ref">342</a>]</cite> integrated knowledge distillation into FPN and detection heads to learn new concepts while maintaining the old ones. More thorough research is still needed in incremental RSOD to meet the dynamic learning task in practical application.</p>
</div>
</section>
<section id="S9.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S9.SS5.4.1.1" class="ltx_text">IX-E</span> </span><span id="S9.SS5.5.2" class="ltx_text ltx_font_italic">Self-supervised pre-trained models for remote sensing scenes</span>
</h3>

<div id="S9.SS5.p1" class="ltx_para">
<p id="S9.SS5.p1.1" class="ltx_p">Current RSOD methods are always initialized with the ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib346" title="" class="ltx_ref">346</a>]</cite> pre-trained weights. However, there is an inevitable domain gap between the natural and remote sensing scenes, probably limiting the performance of RSOD. Recently, the self-supervised pre-training approaches have received extensive attention and shown excellent performance in the classification and downstream tasks in the nature scenes. Benefiting from the rapid advances in remote sensing technology, the abundant remote sensing data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib347" title="" class="ltx_ref">347</a>, <a href="#bib.bib348" title="" class="ltx_ref">348</a>]</cite> also provide sufficient data support for self-supervised pre-training. Some researchers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib349" title="" class="ltx_ref">349</a>, <a href="#bib.bib350" title="" class="ltx_ref">350</a>, <a href="#bib.bib351" title="" class="ltx_ref">351</a>, <a href="#bib.bib352" title="" class="ltx_ref">352</a>, <a href="#bib.bib353" title="" class="ltx_ref">353</a>]</cite> have initially demonstrated the effectiveness of remote sensing pre-training on representative downstream tasks. Therefore, exploring the self-supervised pre-training models based on multi-source remote sensing data deserves further research.</p>
</div>
</section>
<section id="S9.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S9.SS6.4.1.1" class="ltx_text">IX-F</span> </span><span id="S9.SS6.5.2" class="ltx_text ltx_font_italic">Compact and efficient object detection architectures</span>
</h3>

<div id="S9.SS6.p1" class="ltx_para">
<p id="S9.SS6.p1.1" class="ltx_p">Most existing airborne and satellite-borne satellites require sending back the remote sensing data for interpretation, leading to additional resource overheads. Thus, it is essential to investigate compact and efficient detectors for airborne and satellite-borne platforms to reduce resource consumption in data transmission. Drawing on this demand, some researchers have proposed lightweight detectors through model design <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib285" title="" class="ltx_ref">285</a>, <a href="#bib.bib354" title="" class="ltx_ref">354</a>, <a href="#bib.bib355" title="" class="ltx_ref">355</a>]</cite>, network pruning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib356" title="" class="ltx_ref">356</a>, <a href="#bib.bib357" title="" class="ltx_ref">357</a>]</cite>, and knowledge distillation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib358" title="" class="ltx_ref">358</a>, <a href="#bib.bib359" title="" class="ltx_ref">359</a>, <a href="#bib.bib360" title="" class="ltx_ref">360</a>]</cite>. However, these detectors still rely heavily on high-performance GPUs and cannot be deployed on airborne and satellite-borne satellites. Therefore, designing compact and efficient object detection architectures for limited resources scenarios remains challenging.</p>
</div>
</section>
</section>
<section id="S10" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">X </span><span id="S10.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S10.p1" class="ltx_para">
<p id="S10.p1.1" class="ltx_p">Object detection has been a fundamental but challenging research topic in the remote sensing community. Thanks to the rapid development of deep learning techniques, RSOD has received considerable attention and gained remarkable achievements in the past decade. In this review, we present a systematic review and summarization of existing deep learning based methods in RSOD. Firstly, we summarized the five main challenges in RSOD according to the characteristics of geospatial objects and categorized the methods into five streams: multi-scale object detection, rotated object detection, weak object detection, tiny object detection, and object detection with limited supervision. Then, we adopted a systematic hierarchical division to review and summarize the methods in each category. Next, we introduced the typical benchmark datasets, evaluation metrics, and practical applications in the RSOD field.
Finally, considering the limitations of existing RSOD methods, we discussed some promising directions for further research.</p>
</div>
<div id="S10.p2" class="ltx_para">
<p id="S10.p2.1" class="ltx_p">Given this time of high-speed technical evolution in RSOD, we believe this survey can help researchers to achieve a more comprehensive understanding of the main topics in this field and to find potential directions for future research.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgement</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work was supported in part by the National Natural Science Foundation of China, under Grant 62276197, Grant 62006178, Grant 62171332; in part by the Key Research and Development Program in the Shaanxi Province of China under Grant 2019ZDLGY03-08.
Xiangrong Zhang is the corresponding author.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
N. Gorelick, M. Hancher, M. Dixon, S. Ilyushchenko, D. Thau, and R. Moore,
“Google earth engine: Planetary-scale geospatial analysis for everyone,”
<em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Remote Sens. Environ.</em>, vol. 202, pp. 18–27, 2017.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
D. Lam, R. Kuzma, K. McGee, S. Dooley, M. Laielli, M. Klaric, Y. Bulatov, and
B. McCord, “xview: Objects in context in overhead imagery,” 2018. [Online].
Available: http://arxiv.org/abs/1802.07856

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Z. Li, H. Shen, H. Li, G. Xia, P. Gamba, and L. Zhang, “Multi-feature combined
cloud and cloud shadow detection in gaofen-1 wide field of view imagery,”
<em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Remote Sens. Environ.</em>, vol. 191, pp. 342–358, 2017.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
S. Zhang, R. Wu, K. Xu, J. Wang, and W. Sun, “R-cnn-based ship detection from
high resolution remote sensing imagery,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 11,
no. 6, p. 631, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Y. Wang, C. Wang, H. Zhang, Y. Dong, and S. Wei, “Automatic ship detection
based on retinanet using multi-resolution gaofen-3 imagery,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Remote
Sens.</em>, vol. 11, no. 5, p. 531, 2019.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
X. X. Zhu, D. Tuia, L. Mou, G.-S. Xia, L. Zhang, F. Xu, and F. Fraundorfer,
“Deep learning in remote sensing: A comprehensive review and list of
resources,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote Sens. Mag.</em>, vol. 5, no. 4, pp. 8–36,
2017.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
L. Zhang, L. Zhang, and B. Du, “Deep learning for remote sensing data: A
technical tutorial on the state of the art,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote Sens.
Mag.</em>, vol. 4, no. 2, pp. 22–40, 2016.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
L. Zhang and L. Zhang, “Artificial intelligence for remote sensing data
analysis: A review of challenges and opportunities,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci.
Remote Sens. Mag.</em>, vol. 10, no. 2, pp. 270–294, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
W. Han, J. Chen, L. Wang, R. Feng, F. Li, L. Wu, T. Tian, and J. Yan, “Methods
for small, weak object detection in optical high-resolution remote sensing
images: A survey of advances and challenges,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote
Sens. Mag.</em>, vol. 9, no. 4, pp. 8–34, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
C. Xu, J. Wang, W. Yang, H. Yu, L. Yu, and G.-S. Xia, “Detecting tiny objects
in aerial images: A normalized wasserstein distance and a new benchmark,”
<em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">ISPRS J. Photogrammetry Remote Sens.</em>, vol. 190, pp. 79–93, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
J. Yue, L. Fang, P. Ghamisi, W. Xie, J. Li, J. Chanussot, and A. Plaza,
“Optical remote sensing image understanding with weak supervision: Concepts,
methods, and perspectives,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote Sens. Mag.</em>, vol. 10,
no. 2, pp. 250–269, 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
C. Xu and H. Duan, “Artificial bee colony (abc) optimized edge potential
function (epf) approach to target recognition for low-altitude aircraft,”
<em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Pattern Recognit. Lett.</em>, vol. 31, no. 13, pp. 1759–1772, 2010.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
X. Sun, H. Wang, and K. Fu, “Automatic detection of geospatial objects using
taxonomic semantics,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote Sens. Lett.</em>, vol. 7,
no. 1, pp. 23–27, 2010.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Y. Lin, H. He, Z. Yin, and F. Chen, “Rotation-invariant object detection in
remote sensing images based on radial-gradient angle,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci.
Remote Sens. Lett.</em>, vol. 12, no. 4, pp. 746–750, 2015.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
H. Moon, R. Chellappa, and A. Rosenfeld, “Performance analysis of a simple
vehicle detection algorithm,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Image Vis. Comput.</em>, vol. 20, no. 1, pp.
1–13, 2002.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
S. Leninisha and K. Vani, “Water flow based geometric active deformable model
for road network,” <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">ISPRS J. Photogrammetry Remote Sens.</em>, vol. 102,
pp. 140–147, 2015.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
D. Chaudhuri and A. Samal, “An automatic bridge detection technique for
multispectral images,” <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 46,
no. 9, pp. 2720–2727, 2008.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
G. Cheng, J. Han, P. Zhou, and L. Guo, “Multi-class geospatial object
detection and geographic image classification based on collection of part
detectors,” <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">ISPRS J. Photogrammetry Remote Sens.</em>, vol. 98, pp.
119–132, 2014.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
L. Zhang, L. Zhang, D. Tao, and X. Huang, “Sparse transfer manifold embedding
for hyperspectral target detection,” <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote
Sens.</em>, vol. 52, no. 2, pp. 1030–1043, 2013.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
J. Han, P. Zhou, D. Zhang, G. Cheng, L. Guo, Z. Liu, S. Bu, and J. Wu,
“Efficient, simultaneous detection of multi-class geospatial targets based
on visual saliency modeling and discriminative learning of sparse coding,”
<em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">ISPRS J. Photogrammetry Remote Sens.</em>, vol. 89, pp. 37–48, 2014.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
H. Sun, X. Sun, H. Wang, Y. Li, and X. Li, “Automatic target detection in
high-resolution remote sensing images using spatial sparse coding
bag-of-words model,” <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote Sens. Lett.</em>, vol. 9,
no. 1, pp. 109–113, 2011.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">nature</em>, vol. 521,
no. 7553, pp. 436–444, 2015.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
S. Ren, K. He, R. B. Girshick, and J. Sun, “Faster R-CNN: towards real-time
object detection with region proposal networks,” in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proc. Annu. Conf.
Neural Inf. Process. Syst</em>, 2015, pp. 91–99.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
J. Redmon and A. Farhadi, “YOLO9000: better, faster, stronger,” in
<em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</em>, 2017, pp.
6517–6525.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
T. Lin, P. Goyal, R. B. Girshick, K. He, and P. Dollár, “Focal loss for
dense object detection,” in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Int. Conf. Comput. Vis.
(ICCV)</em>, 2017, pp. 2999–3007.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Z. Tian, C. Shen, H. Chen, and T. He, “FCOS: fully convolutional one-stage
object detection,” in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</em>,
2019, pp. 9626–9635.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
L. Liu, W. Ouyang, X. Wang, P. W. Fieguth, J. Chen, X. Liu, and
M. Pietikäinen, “Deep learning for generic object detection: A
survey,” <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Int. J. Comput. Vis.</em>, vol. 128, no. 2, pp. 261–318, 2020.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
K. Li, G. Wan, G. Cheng, L. Meng, and J. Han, “Object detection in optical
remote sensing images: A survey and a new benchmark,” <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">ISPRS J.
Photogrammetry Remote Sens.</em>, vol. 159, pp. 296–307, 2020.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
G. Cheng and J. Han, “A survey on object detection in optical remote sensing
images,” <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">ISPRS J. Photogrammetry Remote Sens.</em>, vol. 117, pp. 11–28,
2016.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
U. Alganci, M. Soydas, and E. Sertel, “Comparative research on deep learning
approaches for airplane detection from very high-resolution satellite
images,” <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 12, no. 3, p. 458, 2020.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Z. Li, Y. Wang, N. Zhang, Y. Zhang, Z. Zhao, D. Xu, G. Ben, and Y. Gao, “Deep
learning-based object detection techniques for remote sensing images: A
survey,” <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 14, no. 10, p. 2385, 2022.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
J. Kang, S. Tariq, H. Oh, and S. S. Woo, “A survey of deep learning-based
object detection methods and datasets for overhead imagery,” <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">IEEE
Access</em>, vol. 10, pp. 20 118–20 134, 2022.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
J. Ding, N. Xue, G. Xia, X. Bai, W. Yang, M. Y. Yang, S. J. Belongie, J. Luo,
M. Datcu, M. Pelillo, and L. Zhang, “Object detection in aerial images: A
large-scale benchmark and challenges,” <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Pattern Anal.
Mach. Intell.</em>, vol. 44, no. 11, pp. 7778–7796, 2022.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
X. Sun, P. Wang, Z. Yan, F. Xu, R. Wang, W. Diao, J. Chen, J. Li, Y. Feng,
T. Xu, M. Weinmann, S. Hinz, C. Wang, and K. Fu, “Fair1m: A benchmark
dataset for fine-grained object recognition in high-resolution remote sensing
imagery,” <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">ISPRS J. Photogrammetry Remote Sens.</em>, vol. 184, pp.
116–130, 2022.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
W. Zhao, W. Ma, L. Jiao, P. Chen, S. Yang, and B. Hou, “Multi-scale image
block-level F-CNN for remote sensing images object detection,”
<em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 7, pp. 43 607–43 621, 2019.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
S. M. Azimi, E. Vig, R. Bahmanyar, M. Körner, and P. Reinartz, “Towards
multi-class object detection in unconstrained remote sensing imagery,” in
<em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Asian Conference on Computer Vision</em>, vol. 11363, 2018, pp. 150–165.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
P. Shamsolmoali, M. Zareapoor, J. Chanussot, H. Zhou, and J. Yang, “Rotation
equivariant feature image pyramid network for object detection in optical
remote sensing imagery,” <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 60,
pp. 1–14, 2022. [Online]. Available:
https://doi.org/10.1109/TGRS.2021.3112481

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Y. Chen, P. Zhang, Z. Li, Y. Li, X. Zhang, G. Meng, S. Xiang, J. Sun, and
J. Jia, “Stitcher: Feedback-driven data provider for object detection,”
2020. [Online]. Available: https://arxiv.org/abs/2004.12432

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
X. Xu, X. Zhang, and T. Zhang, “Lite-yolov5: A lightweight deep learning
detector for on-board ship detection in large-scene sentinel-1 SAR
images,” <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 14, no. 4, p. 1018, 2022. [Online].
Available: https://doi.org/10.3390/rs14041018

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
N. Su, Z. Huang, Y. Yan, C. Zhao, and S. Zhou, “Detect larger at once:
Large-area remote-sensing image arbitrary-oriented ship detection,”
<em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote Sens. Lett.</em>, vol. 19, pp. 1–5, 2022. [Online].
Available: https://doi.org/10.1109/LGRS.2022.3144485

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
B. Zhao, Y. Wu, X. Guan, L. Gao, and B. Zhang, “An improved aggregated-mosaic
method for the sparse object detection of remote sensing imagery,”
<em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 13, no. 13, p. 2602, 2021.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
X. Han, Y. Zhong, and L. Zhang, “An efficient and robust integrated geospatial
object detection framework for high spatial resolution remote sensing
imagery,” <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 9, no. 7, p. 666, 2017.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Y. Long, Y. Gong, Z. Xiao, and Q. Liu, “Accurate object localization in remote
sensing images based on convolutional neural networks,” <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">IEEE Trans.
Geosci. Remote Sens.</em>, vol. 55, no. 5, pp. 2486–2498, 2017.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Y. Zhong, X. Han, and L. Zhang, “Multi-class geospatial object detection based
on a position-sensitive balancing framework for high spatial resolution
remote sensing imagery,” <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">ISPRS J. Photogrammetry Remote Sens.</em>, vol.
138, pp. 281–294, 2018.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
P. Ding, Y. Zhang, W.-J. Deng, P. Jia, and A. Kuijper, “A light and faster
regional convolutional neural network for object detection in optical remote
sensing images,” <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">ISPRS J. Photogrammetry Remote Sens.</em>, vol. 141, pp.
208–218, 2018.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
W. Liu, L. Ma, and H. Chen, “Arbitrary-oriented ship detection framework in
optical remote-sensing images,” <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote Sens. Lett.</em>,
vol. 15, no. 6, pp. 937–941, 2018.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
W. Liu, L. Ma, J. Wang, and H. Chen, “Detection of multiclass objects in
optical remote sensing images,” <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote Sens. Lett.</em>,
vol. 16, no. 5, pp. 791–795, 2019.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Y. Zhang, Y. Yuan, Y. Feng, and X. Lu, “Hierarchical and robust convolutional
neural network for very high-resolution remote sensing object detection,”
<em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 57, no. 8, pp. 5535–5548,
2019.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Z. Lin, K. Ji, X. Leng, and G. Kuang, “Squeeze and excitation rank faster
R-CNN for ship detection in SAR images,” <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote
Sens. Lett.</em>, vol. 16, no. 5, pp. 751–755, 2019.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Z. Deng, H. Sun, S. Zhou, J. Zhao, L. Lei, and H. Zou, “Multi-scale object
detection in remote sensing imagery with convolutional neural networks,”
<em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">ISPRS J. Photogrammetry Remote Sens.</em>, vol. 145, pp. 3–22, 2018.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Z. Zheng, Y. Zhong, A. Ma, X. Han, J. Zhao, Y. Liu, and L. Zhang, “Hynet:
Hyper-scale object detection network framework for multiple spatial
resolution remote sensing imagery,” <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">ISPRS J. Photogrammetry Remote
Sens.</em>, vol. 166, pp. 1–14, 2020.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Y. Ren, C. Zhu, and S. Xiao, “Deformable faster r-cnn with aggregating
multi-layer features for partially occluded object detection in optical
remote sensing images,” <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 10, no. 9, p. 1470, 2018.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg,
“Ssd: Single shot multibox detector,” in <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">in Proc. Euro. Conf. Comput.
Vis.</em>   Springer, 2016, pp. 21–37.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
S. Liu, D. Huang, and Y. Wang, “Receptive field block net for accurate and
fast object detection,” in <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">in Proc. Euro. Conf. Comput. Vis.</em>, 2018,
pp. 385–400.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Z. Shen, Z. Liu, J. Li, Y.-G. Jiang, Y. Chen, and X. Xue, “Dsod: Learning
deeply supervised object detectors from scratch,” in <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Int.
Conf. Comput. Vis. (ICCV)</em>, 2017, pp. 1919–1927.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Z. Zhang, S. Qiao, C. Xie, W. Shen, B. Wang, and A. L. Yuille, “Single-shot
object detection with enriched semantics,” in <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Conf. Comput.
Vis. Pattern Recognit. (CVPR)</em>, 2018, pp. 5813–5821.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
X. Lu, J. Ji, Z. Xing, and Q. Miao, “Attention and feature fusion SSD for
remote sensing object detection,” <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Instrum. Meas.</em>,
vol. 70, pp. 1–9, 2021.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
G. Wang, Y. Zhuang, H. Chen, X. Liu, T. Zhang, L. Li, S. Dong, and Q. Sang,
“Fsod-net: Full-scale object detection from optical remote sensing
imagery,” <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 60, pp. 1–18,
2022.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
B. Hou, Z. Ren, W. Zhao, Q. Wu, and L. Jiao, “Object detection in
high-resolution panchromatic images using deep models and spatial template
matching,” <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 58, no. 2, pp.
956–970, 2020.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
X. Liang, J. Zhang, L. Zhuo, Y. Li, and Q. Tian, “Small object detection in
unmanned aerial vehicle images using feature fusion and scaling-based single
shot detector with spatial context analysis,” <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Circuits
Syst. Video Technol.</em>, vol. 30, no. 6, pp. 1758–1770, 2020.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Z. Wang, L. Du, J. Mao, B. Liu, and D. Yang, “Sar target detection based on
ssd with data augmentation and transfer learning,” <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci.
Remote Sens. Lett.</em>, vol. 16, no. 1, pp. 150–154, 2018.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
S. Bao, X. Zhong, R. Zhu, X. Zhang, Z. Li, and M. Li, “Single shot anchor
refinement network for oriented object detection in optical remote sensing
imagery,” <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 7, pp. 87 150–87 161, 2019.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
T. Xu, X. Sun, W. Diao, L. Zhao, K. Fu, and H. Wang, “ASSD: feature aligned
single-shot detection for multiscale objects in aerial imagery,”
<em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 60, pp. 1–17, 2022.
[Online]. Available: https://doi.org/10.1109/TGRS.2021.3089170

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Q. Li, L. Mou, Q. Liu, Y. Wang, and X. X. Zhu, “Hsf-net: Multiscale deep
feature embedding for ship detection in optical remote sensing imagery,”
<em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 56, no. 12, pp. 7147–7161,
2018.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie,
“Feature pyramid networks for object detection,” in <em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Conf.
Comput. Vis. Pattern Recognit. (CVPR)</em>, 2017, pp. 2117–2125.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
S. Liu, L. Qi, H. Qin, J. Shi, and J. Jia, “Path aggregation network for
instance segmentation,” in <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Int. Conf. Comput. Vis. Pattern
Recognit. (CVPR)</em>, Jun. 2018, pp. 8759–8768.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
J. Pang, K. Chen, J. Shi, H. Feng, W. Ouyang, and D. Lin, “Libra R-CNN:
towards balanced learning for object detection,” in <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Int.
Conf. Comput. Vis. Pattern Recognit. (CVPR)</em>, Jun. 2019, pp. 821–830.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
M. Tan, R. Pang, and Q. V. Le, “Efficientdet: Scalable and efficient object
detection,” in <em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.
(CVPR)</em>, 2020, pp. 10 781–10 790.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
L. Hou, K. Lu, and J. Xue, “Refined one-stage oriented object detection method
for remote sensing images,” <em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Image Process.</em>, vol. 31,
pp. 1545–1558, 2022.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
W. Zhang, L. Jiao, Y. Li, Z. Huang, and H. Wang, “Laplacian feature pyramid
network for object detection in VHR optical remote sensing images,”
<em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 60, pp. 1–14, 2022.
[Online]. Available: https://doi.org/10.1109/TGRS.2021.3072488

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
S. Wei, H. Su, J. Ming, C. Wang, M. Yan, D. Kumar, J. Shi, and X. Zhang,
“Precise and robust ship detection for high-resolution SAR imagery based
on hr-sdnet,” <em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 12, no. 1, p. 167, 2020.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
G. Cheng, M. He, H. Hong, X. Yao, X. Qian, and L. Guo, “Guiding clean features
for object detection in remote sensing images,” <em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote
Sens. Lett.</em>, vol. 19, pp. 1–5, 2022.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
J. Jiao, Y. Zhang, H. Sun, X. Yang, X. Gao, W. Hong, K. Fu, and X. Sun, “A
densely connected end-to-end neural network for multiscale and multiscene
SAR ship detection,” <em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 6, pp. 20 881–20 892,
2018.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Q. Guo, H. Wang, and F. Xu, “Scattering enhanced attention pyramid network for
aircraft detection in SAR images,” <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote
Sens.</em>, vol. 59, no. 9, pp. 7570–7587, 2021.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Y. Li, Q. Huang, X. Pei, L. Jiao, and R. Shang, “Radet: Refine feature pyramid
network and multi-layer attention network for arbitrary-oriented object
detection of remote sensing images,” <em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 12, no. 3, p.
389, 2020.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
L. Shi, L. Kuang, X. Xu, B. Pan, and Z. Shi, “Canet: Centerness-aware network
for object detection in remote sensing images,” <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci.
Remote Sens.</em>, vol. 60, pp. 1–13, 2022.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
R. Yang, Z. Pan, X. Jia, L. Zhang, and Y. Deng, “A novel cnn-based detector
for ship detection based on rotatable bounding box in SAR images,”
<em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</em>, vol. 14, pp.
1938–1958, 2021.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Y. Zhao, L. Zhao, B. Xiong, and G. Kuang, “Attention receptive pyramid network
for ship detection in SAR images,” <em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">IEEE J. Sel. Top. Appl. Earth
Obs. Remote Sens.</em>, vol. 13, pp. 2738–2756, 2020.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
X. Yang, X. Zhang, N. Wang, and X. Gao, “A robust one-stage detector for
multiscale ship detection with complex background in massive SAR images,”
<em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 60, pp. 1–12, 2022.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
K. Fu, Z. Chang, Y. Zhang, G. Xu, K. Zhang, and X. Sun, “Rotation-aware and
multi-scale convolutional neural network for object detection in remote
sensing images,” <em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">ISPRS J. Photogrammetry Remote Sens.</em>, vol. 161, pp.
294–308, 2020.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
W. Huang, G. Li, B. Jin, Q. Chen, J. Yin, and L. Huang, “Scenario
context-aware-based bidirectional feature pyramid network for remote sensing
target detection,” <em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote Sens. Lett.</em>, vol. 19, pp.
1–5, 2022. [Online]. Available:
https://doi.org/10.1109/LGRS.2021.3135935

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
V. Chalavadi, J. Prudviraj, R. Datla, C. S. Babu, and K. M. C, “msodanet: A
network for multi-scale object detection in aerial images using hierarchical
dilated convolutions,” <em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">Pattern Recognit.</em>, vol. 126, p. 108548, 2022.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
G. Cheng, Y. Si, H. Hong, X. Yao, and L. Guo, “Cross-scale feature fusion for
object detection in optical remote sensing images,” <em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci.
Remote Sens. Lett.</em>, vol. 18, no. 3, pp. 431–435, 2021.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
J. Fu, X. Sun, Z. Wang, and K. Fu, “An anchor-free method based on feature
balancing and refinement network for multiscale ship detection in SAR
images,” <em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 59, no. 2, pp.
1331–1344, 2021.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Y. Liu, Q. Li, Y. Yuan, Q. Du, and Q. Wang, “Abnet: Adaptive balanced network
for multiscale object detection in remote sensing imagery,” <em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">IEEE
Trans. Geosci. Remote Sens.</em>, vol. 60, pp. 1–14, 2022. [Online]. Available:
https://doi.org/10.1109/TGRS.2021.3133956

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
H. Guo, X. Yang, N. Wang, B. Song, and X. Gao, “A rotational libra R-CNN
method for ship detection,” <em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>,
vol. 58, no. 8, pp. 5772–5781, 2020.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
T. Zhang, Y. Zhuang, G. Wang, S. Dong, H. Chen, and L. Li, “Multiscale
semantic fusion-guided fractal convolutional object detection network for
optical remote sensing imagery,” <em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>,
vol. 60, pp. 1–20, 2022. [Online]. Available:
https://doi.org/10.1109/TGRS.2021.3108476

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Y. Zheng, P. Sun, Z. Zhou, W. Xu, and Q. Ren, “Adt-det: Adaptive dynamic
refined single-stage transformer detector for arbitrary-oriented object
detection in satellite optical imagery,” <em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 13,
no. 13, p. 2623, 2021.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Z. Wei, D. Liang, D. Zhang, L. Zhang, Q. Geng, M. Wei, and H. Zhou, “Learning
calibrated-guidance for object detection in aerial images,” <em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">IEEE J.
Sel. Top. Appl. Earth Obs. Remote Sens.</em>, vol. 15, pp. 2721–2733, 2022.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
L. Chen, C. Liu, F. Chang, S. Li, and Z. Nie, “Adaptive multi-level feature
fusion and attention-based network for arbitrary-oriented object detection in
remote sensing imagery,” <em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">Neurocomputing</em>, vol. 451, pp. 67–80, 2021.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
X. Sun, P. Wang, C. Wang, Y. Liu, and K. Fu, “Pbnet: Part-based convolutional
neural network for complex composite object detection in remote sensing
imagery,” <em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">ISPRS J. Photogrammetry Remote Sens.</em>, vol. 173, pp. 50–65,
2021.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
T. Zhang, X. Zhang, C. Liu, J. Shi, S. Wei, I. Ahmad, X. Zhan, Y. Zhou, D. Pan,
J. Li, and H. Su, “Balance learning for ship detection from synthetic
aperture radar remote sensing imagery,” <em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">ISPRS J. Photogrammetry Remote
Sens.</em>, vol. 182, pp. 190–207, 2021.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
T. Zhang, X. Zhang, and X. Ke, “Quad-fpn: A novel quad feature pyramid network
for sar ship detection,” <em id="bib.bib93.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 13, no. 14, p. 2771,
2021.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
J. Song, L. Miao, Q. Ming, Z. Zhou, and Y. Dong, “Fine-grained object
detection in remote sensing images via adaptive label assignment and
refined-balanced feature pyramid network,” <em id="bib.bib94.1.1" class="ltx_emph ltx_font_italic">IEEE J. Sel. Top. Appl.
Earth Obs. Remote. Sens.</em>, vol. 16, pp. 71–82, 2023.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
W. Guo, W. Yang, H. Zhang, and G. Hua, “Geospatial object detection in high
resolution satellite images based on multi-scale convolutional neural
network,” <em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 10, no. 1, p. 131, 2018.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
S. Zhang, G. He, H. Chen, N. Jing, and Q. Wang, “Scale adaptive proposal
network for object detection in remote sensing images,” <em id="bib.bib96.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci.
Remote Sens. Lett.</em>, vol. 16, no. 6, pp. 864–868, 2019.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
C. Li, C. Xu, Z. Cui, D. Wang, T. Zhang, and J. Yang, “Feature-attentioned
object detection in remote sensing imagery,” in <em id="bib.bib97.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Int. Conf.
Image Process. Conf. (ICIP)</em>.   IEEE,
2019, pp. 3886–3890.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
Z. Dong, M. Wang, Y. Wang, Y. Zhu, and Z. Zhang, “Object detection in high
resolution remote sensing imagery based on convolutional neural networks with
suitable object scale features,” <em id="bib.bib98.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>,
vol. 58, no. 3, pp. 2104–2114, 2020.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
H. Qiu, H. Li, Q. Wu, F. Meng, K. N. Ngan, and H. Shi,
“A<math id="bib.bib99.1.m1.1" class="ltx_Math" alttext="{}^{\mbox{2}}" display="inline"><semantics id="bib.bib99.1.m1.1a"><msup id="bib.bib99.1.m1.1.1" xref="bib.bib99.1.m1.1.1.cmml"><mi id="bib.bib99.1.m1.1.1a" xref="bib.bib99.1.m1.1.1.cmml"></mi><mtext id="bib.bib99.1.m1.1.1.1" xref="bib.bib99.1.m1.1.1.1a.cmml">2</mtext></msup><annotation-xml encoding="MathML-Content" id="bib.bib99.1.m1.1b"><apply id="bib.bib99.1.m1.1.1.cmml" xref="bib.bib99.1.m1.1.1"><ci id="bib.bib99.1.m1.1.1.1a.cmml" xref="bib.bib99.1.m1.1.1.1"><mtext mathsize="70%" id="bib.bib99.1.m1.1.1.1.cmml" xref="bib.bib99.1.m1.1.1.1">2</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib99.1.m1.1c">{}^{\mbox{2}}</annotation></semantics></math>rmnet: Adaptively aspect ratio multi-scale network for
object detection in remote sensing images,” <em id="bib.bib99.2.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 11,
no. 13, p. 1594, 2019.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
J. Hou, X. Zhu, and X. Yin, “Self-adaptive aspect ratio anchor for oriented
object detection in remote sensing images,” <em id="bib.bib100.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 13,
no. 7, p. 1318, 2021.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
N. Mo, L. Yan, R. Zhu, and H. Xie, “Class-specific anchor based and
context-guided multi-class object detection in high resolution remote sensing
imagery with a convolutional neural network,” <em id="bib.bib101.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 11,
no. 3, p. 272, 2019.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
Z. Tian, R. Zhan, J. Hu, W. Wang, Z. He, and Z. Zhuang, “Generating anchor
boxes based on attention mechanism for object detection in remote sensing
images,” <em id="bib.bib102.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 12, no. 15, p. 2416, 2020.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
Z. Teng, Y. Duan, Y. Liu, B. Zhang, and J. Fan, “Global to local:
Clip-lstm-based object detection from remote sensing images,” <em id="bib.bib103.1.1" class="ltx_emph ltx_font_italic">IEEE
Trans. Geosci. Remote Sens.</em>, vol. 60, pp. 1–13, 2022. [Online]. Available:
https://doi.org/10.1109/TGRS.2021.3064840

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
Y. Yu, H. Guan, D. Li, T. Gu, E. Tang, and A. Li, “Orientation guided
anchoring for geospatial object detection from remote sensing imagery,”
<em id="bib.bib104.1.1" class="ltx_emph ltx_font_italic">ISPRS J. Photogrammetry Remote Sens.</em>, vol. 160, pp. 67–82, 2020.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
J. Wang, K. Chen, S. Yang, C. C. Loy, and D. Lin, “Region proposal by guided
anchoring,” in <em id="bib.bib105.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.
(CVPR)</em>, 2019, pp. 2965–2974.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
X. Yang and J. Yan, “On the arbitrary-oriented object detection:
Classification based approaches revisited,” <em id="bib.bib106.1.1" class="ltx_emph ltx_font_italic">Int. J. Comput. Vis.</em>,
vol. 130, no. 5, pp. 1340–1365, 2022.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
X. Yang, J. Yang, J. Yan, Y. Zhang, T. Zhang, Z. Guo, X. Sun, and K. Fu,
“Scrdet: Towards more robust detection for small, cluttered and rotated
objects,” in <em id="bib.bib107.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</em>, 2019, pp.
8232–8241.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
X. Yang, J. Yan, Z. Feng, and T. He, “R3det: Refined single-stage detector
with feature refinement for rotating object,” in <em id="bib.bib108.1.1" class="ltx_emph ltx_font_italic">Pro. AAAI Conf.
Artific. Intell.</em>, vol. 35, no. 4, 2021, pp. 3163–3171.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
X. Yang, H. Sun, K. Fu, J. Yang, X. Sun, M. Yan, and Z. Guo, “Automatic ship
detection in remote sensing images from google earth of complex scenes based
on multiscale rotation dense feature pyramid networks,” <em id="bib.bib109.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>,
vol. 10, no. 1, p. 132, 2018.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
X. Yang, H. Sun, X. Sun, M. Yan, Z. Guo, and K. Fu, “Position detection and
direction prediction for arbitrary-oriented ships via multitask rotation
region convolutional neural network,” <em id="bib.bib110.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 6, pp.
50 839–50 849, 2018.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
Q. Ming, L. Miao, Z. Zhou, and Y. Dong, “Cfc-net: A critical feature
capturing network for arbitrary-oriented object detection in remote-sensing
images,” <em id="bib.bib111.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 60, pp. 1–14,
2022. [Online]. Available: https://doi.org/10.1109/TGRS.2021.3095186

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
Q. Ming, Z. Zhou, L. Miao, H. Zhang, and L. Li, “Dynamic anchor learning for
arbitrary-oriented object detection,” in <em id="bib.bib112.1.1" class="ltx_emph ltx_font_italic">Pro. AAAI Conf. Artific.
Intell.</em>, 2021, pp. 2355–2363.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
Y. Zhu, J. Du, and X. Wu, “Adaptive period embedding for representing oriented
objects in aerial images,” <em id="bib.bib113.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>,
vol. 58, no. 10, pp. 7247–7257, 2020.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
J. Ding, N. Xue, Y. Long, G.-S. Xia, and Q. Lu, “Learning roi transformer for
oriented object detection in aerial images,” in <em id="bib.bib114.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Conf.
Comput. Vis. Pattern Recognit. (CVPR)</em>, 2019, pp. 2849–2858.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
Q. An, Z. Pan, L. Liu, and H. You, “Drbox-v2: An improved detector with
rotatable boxes for target detection in SAR images,” <em id="bib.bib115.1.1" class="ltx_emph ltx_font_italic">IEEE Trans.
Geosci. Remote Sens.</em>, vol. 57, no. 11, pp. 8333–8349, 2019.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
Q. Li, L. Mou, Q. Xu, Y. Zhang, and X. X. Zhu, “R<math id="bib.bib116.1.m1.1" class="ltx_Math" alttext="{}^{\mbox{3}}" display="inline"><semantics id="bib.bib116.1.m1.1a"><msup id="bib.bib116.1.m1.1.1" xref="bib.bib116.1.m1.1.1.cmml"><mi id="bib.bib116.1.m1.1.1a" xref="bib.bib116.1.m1.1.1.cmml"></mi><mtext id="bib.bib116.1.m1.1.1.1" xref="bib.bib116.1.m1.1.1.1a.cmml">3</mtext></msup><annotation-xml encoding="MathML-Content" id="bib.bib116.1.m1.1b"><apply id="bib.bib116.1.m1.1.1.cmml" xref="bib.bib116.1.m1.1.1"><ci id="bib.bib116.1.m1.1.1.1a.cmml" xref="bib.bib116.1.m1.1.1.1"><mtext mathsize="70%" id="bib.bib116.1.m1.1.1.1.cmml" xref="bib.bib116.1.m1.1.1.1">3</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib116.1.m1.1c">{}^{\mbox{3}}</annotation></semantics></math>-net: A
deep network for multi-oriented vehicle detection in aerial images and
videos,” 2018. [Online]. Available: http://arxiv.org/abs/1808.05560

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
G. Xia, X. Bai, J. Ding, Z. Zhu, S. J. Belongie, J. Luo, M. Datcu, M. Pelillo,
and L. Zhang, “DOTA: A large-scale dataset for object detection in
aerial images,” in <em id="bib.bib117.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Int. Conf. Comput. Vis. Pattern
Recognit. (CVPR)</em>, 2018, pp. 3974–3983.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
Y. Liu, S. Zhang, L. Jin, L. Xie, Y. Wu, and Z. Wang, “Omnidirectional scene
text detection with sequential-free box discretization,” <em id="bib.bib118.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1906.02371</em>, 2019.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
W. Qian, X. Yang, S. Peng, J. Yan, and Y. Guo, “Learning modulated loss for
rotated object detection,” in <em id="bib.bib119.1.1" class="ltx_emph ltx_font_italic">in Proc. AAAI Conf. Artific. Intell.</em>,
vol. 35, no. 3, 2021, pp. 2458–2466.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
Y. Xu, M. Fu, Q. Wang, Y. Wang, K. Chen, G.-S. Xia, and X. Bai, “Gliding
vertex on the horizontal bounding box for multi-oriented object detection,”
<em id="bib.bib120.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Pattern Anal. Mach. Intell.</em>, vol. 43, no. 4, pp.
1452–1459, 2021.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
W. Qian, X. Yang, S. Peng, X. Zhang, and J. Yan, “Rsdet++: Point-based
modulated loss for more accurate rotated object detection,” <em id="bib.bib121.1.1" class="ltx_emph ltx_font_italic">IEEE
Trans. Circuits Syst. Video Technol.</em>, vol. 32, no. 11, pp. 7869–7879, 2022.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
J. Luo, Y. Hu, and J. Li, “Surround-net: A multi-branch arbitrary-oriented
detector for remote sensing,” <em id="bib.bib122.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 14, no. 7, p. 1751,
2022.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
Q. Song, F. Yang, L. Yang, C. Liu, M. Hu, and L. Xia, “Learning point-guided
localization for detection in remote sensing images,” <em id="bib.bib123.1.1" class="ltx_emph ltx_font_italic">IEEE J. Sel.
Top. Appl. Earth Obs. Remote Sens.</em>, vol. 14, pp. 1084–1094, 2021.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
X. Xie, G. Cheng, J. Wang, X. Yao, and J. Han, “Oriented r-cnn for object
detection,” in <em id="bib.bib124.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</em>, 2021, pp.
3520–3529.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
Y. Yao, G. Cheng, G. Wang, S. Li, P. Zhou, X. Xie, and J. Han, “On improving
bounding box representations for oriented object detection,” <em id="bib.bib125.1.1" class="ltx_emph ltx_font_italic">IEEE
Trans. Geosci. Remote Sens.</em>, vol. 61, pp. 1–11, 2023. [Online]. Available:
https://doi.org/10.1109/TGRS.2022.3231340

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
Q. Ming, L. Miao, Z. Zhou, X. Yang, and Y. Dong, “Optimization for
arbitrary-oriented object detection via representation invariance loss,”
<em id="bib.bib126.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote Sens. Lett.</em>, vol. 19, pp. 1–5, 2022. [Online].
Available: https://doi.org/10.1109/LGRS.2021.3115110

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
X. Yang and J. Yan, “Arbitrary-oriented object detection with circular smooth
label,” in <em id="bib.bib127.1.1" class="ltx_emph ltx_font_italic">Proc. Euro. Conf. Comput. Vis.</em>   Springer, 2020, pp. 677–694.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
X. Yang, L. Hou, Y. Zhou, W. Wang, and J. Yan, “Dense label encoding for
boundary discontinuity free rotation detection,” in <em id="bib.bib128.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Conf.
Comput. Vis. Pattern Recognit. (CVPR)</em>, 2021, pp. 15 819–15 829.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
J. Wang, F. Li, and H. Bi, “Gaussian focal loss: Learning distribution
polarized angle prediction for rotated object detection in aerial images,”
<em id="bib.bib129.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, 2022.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
X. Yang, J. Yan, Q. Ming, W. Wang, X. Zhang, and Q. Tian, “Rethinking rotated
object detection with gaussian wasserstein distance loss,” in <em id="bib.bib130.1.1" class="ltx_emph ltx_font_italic">Proc.
Int. Conf. Machine Learn</em>, 2021, pp. 11 830–11 841.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
X. Yang, X. Yang, J. Yang, Q. Ming, W. Wang, Q. Tian, and J. Yan, “Learning
high-precision bounding box for rotated object detection via kullback-leibler
divergence,” vol. 34, pp. 18 381–18 394, 2021.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
X. Yang, Y. Zhou, G. Zhang, J. Yang, W. Wang, J. Yan, X. Zhang, and Q. Tian,
“The kfiou loss for rotated object detection,” <em id="bib.bib132.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2201.12558</em>, 2022.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
X. Yang, G. Zhang, X. Yang, Y. Zhou, W. Wang, J. Tang, T. He, and J. Yan,
“Detecting rotated objects as gaussian distributions and its 3-d
generalization,” <em id="bib.bib133.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Pattern Anal. Mach. Intell.</em>, 2022.
[Online]. Available: https://doi.org/10.1109/TPAMI.2022.3197152.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
J. Wang, J. Ding, H. Guo, W. Cheng, T. Pan, and W. Yang, “Mask obb: A semantic
attention-based mask oriented bounding box representation for multi-category
object detection in aerial images,” <em id="bib.bib134.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 11, no. 24, p.
2930, 2019.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
X. Zhang, G. Wang, P. Zhu, T. Zhang, C. Li, and L. Jiao, “Grs-det: An
anchor-free rotation ship detector based on gaussian-mask in remote sensing
images,” <em id="bib.bib135.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 59, no. 4, pp.
3518–3531, 2020.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
Y. Yang, X. Tang, Y. Cheung, X. Zhang, F. Liu, J. Ma, and L. Jiao,
“Ar<math id="bib.bib136.1.m1.1" class="ltx_Math" alttext="{}^{\mbox{2}}" display="inline"><semantics id="bib.bib136.1.m1.1a"><msup id="bib.bib136.1.m1.1.1" xref="bib.bib136.1.m1.1.1.cmml"><mi id="bib.bib136.1.m1.1.1a" xref="bib.bib136.1.m1.1.1.cmml"></mi><mtext id="bib.bib136.1.m1.1.1.1" xref="bib.bib136.1.m1.1.1.1a.cmml">2</mtext></msup><annotation-xml encoding="MathML-Content" id="bib.bib136.1.m1.1b"><apply id="bib.bib136.1.m1.1.1.cmml" xref="bib.bib136.1.m1.1.1"><ci id="bib.bib136.1.m1.1.1.1a.cmml" xref="bib.bib136.1.m1.1.1.1"><mtext mathsize="70%" id="bib.bib136.1.m1.1.1.1.cmml" xref="bib.bib136.1.m1.1.1.1">2</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib136.1.m1.1c">{}^{\mbox{2}}</annotation></semantics></math>det: An accurate and real-time rotational one-stage ship
detector in remote sensing images,” <em id="bib.bib136.2.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote
Sens.</em>, vol. 60, pp. 1–14, 2022. [Online]. Available:
https://doi.org/10.1109/TGRS.2021.3092433

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
F. Zhang, X. Wang, S. Zhou, Y. Wang, and Y. Hou, “Arbitrary-oriented ship
detection through center-head point extraction,” <em id="bib.bib137.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci.
Remote Sens.</em>, vol. 60, pp. 1–14, 2021.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
J. Yi, P. Wu, B. Liu, Q. Huang, H. Qu, and D. Metaxas, “Oriented object
detection in aerial images with box boundary-aware vectors,” in <em id="bib.bib138.1.1" class="ltx_emph ltx_font_italic">Proc.
IEEE Winter Conf. Appl. Comput. Vis. (WACV)</em>, 2021, pp. 2150–2159.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
Z. Xiao, L. Qian, W. Shao, X. Tan, and K. Wang, “Axis learning for orientated
objects detection in aerial images,” <em id="bib.bib139.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 12, no. 6, p.
908, 2020.

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
X. He, S. Ma, L. He, L. Ru, and C. Wang, “Learning rotated inscribed ellipse
for oriented object detection in remote sensing images,” <em id="bib.bib140.1.1" class="ltx_emph ltx_font_italic">Remote
Sens.</em>, vol. 13, no. 18, p. 3622, 2021.

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock">
K. Fu, Z. Chang, Y. Zhang, and X. Sun, “Point-based estimator for
arbitrary-oriented object detection in aerial images,” <em id="bib.bib141.1.1" class="ltx_emph ltx_font_italic">IEEE Trans.
Geosci. Remote Sens.</em>, vol. 59, no. 5, pp. 4370–4387, 2020.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock">
H. Wei, Y. Zhang, Z. Chang, H. Li, H. Wang, and X. Sun, “Oriented objects as
pairs of middle lines,” <em id="bib.bib142.1.1" class="ltx_emph ltx_font_italic">ISPRS J. Photogrammetry Remote Sens.</em>, vol.
169, pp. 268–279, 2020.

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock">
L. Zhou, H. Wei, H. Li, W. Zhao, Y. Zhang, and Y. Zhang, “Arbitrary-oriented
object detection in remote sensing images based on polar coordinates,”
<em id="bib.bib143.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 8, pp. 223 373–223 384, 2020.

</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock">
X. Zheng, W. Zhang, L. Huan, J. Gong, and H. Zhang, “Apronet: Detecting
objects with precise orientation from aerial images,” <em id="bib.bib144.1.1" class="ltx_emph ltx_font_italic">ISPRS J.
Photogrammetry Remote Sens.</em>, vol. 181, pp. 99–112, 2021.

</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock">
X. Yang, G. Zhang, W. Li, X. Wang, Y. Zhou, and J. Yan, “H2rbox: Horizontal
box annotation is all you need for oriented object detection,” 2022.
[Online]. Available: https://doi.org/10.48550/arXiv.2210.06742

</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock">
G. Cheng, P. Zhou, and J. Han, “Learning rotation-invariant convolutional
neural networks for object detection in vhr optical remote sensing images,”
<em id="bib.bib146.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 54, no. 12, pp. 7405–7415,
2016.

</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock">
K. Li, G. Cheng, S. Bu, and X. You, “Rotation-insensitive and
context-augmented object detection in remote sensing images,” <em id="bib.bib147.1.1" class="ltx_emph ltx_font_italic">IEEE
Trans. Geosci. Remote Sens.</em>, vol. 56, no. 4, pp. 2337–2348, 2017.

</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock">
G. Cheng, P. Zhou, and J. Han, “Rifd-cnn: Rotation-invariant and fisher
discriminative convolutional neural networks for object detection,” in
<em id="bib.bib148.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</em>, 2016, pp.
2884–2893.

</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
G. Cheng, J. Han, P. Zhou, and D. Xu, “Learning rotation-invariant and fisher
discriminative convolutional neural networks for object detection,”
<em id="bib.bib149.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Image Process.</em>, vol. 28, no. 1, pp. 265–278, 2019.

</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock">
X. Wu, D. Hong, J. Tian, J. Chanussot, W. Li, and R. Tao, “Orsim detector: A
novel object detection framework in optical remote sensing imagery using
spatial-frequency channel features,” <em id="bib.bib150.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote
Sens.</em>, vol. 57, no. 7, pp. 5146–5158, 2019.

</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock">
X. Wu, D. Hong, J. Chanussot, Y. Xu, R. Tao, and Y. Wang, “Fourier-based
rotation-invariant feature boosting: An efficient framework for geospatial
object detection,” <em id="bib.bib151.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote Sens. Lett.</em>, vol. 17, no. 2,
pp. 302–306, 2019.

</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock">
G. Wang, X. Wang, B. Fan, and C. Pan, “Feature extraction by
rotation-invariant matrix representation for object detection in aerial
image,” <em id="bib.bib152.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote Sens. Lett.</em>, vol. 14, no. 6, pp.
851–855, 2017.

</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock">
X. Wu, D. Hong, P. Ghamisi, W. Li, and R. Tao, “Msri-ccf: Multi-scale and
rotation-insensitive convolutional channel features for geospatial object
detection,” <em id="bib.bib153.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 10, no. 12, p. 1990, 2018.

</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[154]</span>
<span class="ltx_bibblock">
M. Zand, A. Etemad, and M. Greenspan, “Oriented bounding boxes for small and
freely rotated objects,” <em id="bib.bib154.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 60,
pp. 1–15, 2021.

</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[155]</span>
<span class="ltx_bibblock">
J. Han, J. Ding, N. Xue, and G.-S. Xia, “Redet: A rotation-equivariant
detector for aerial object detection,” in <em id="bib.bib155.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Conf. Comput.
Vis. Pattern Recognit. (CVPR)</em>, 2021, pp. 2786–2795.

</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[156]</span>
<span class="ltx_bibblock">
J. Han, J. Ding, J. Li, and G. Xia, “Align deep features for oriented object
detection,” <em id="bib.bib156.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 60, pp. 1–11,
2022. [Online]. Available: https://doi.org/10.1109/TGRS.2021.3062048

</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[157]</span>
<span class="ltx_bibblock">
X. Yao, H. Shen, X. Feng, G. Cheng, and J. Han, “R2ipoints: Pursuing
rotation-insensitive point representation for aerial object detection,”
<em id="bib.bib157.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, 2022.

</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[158]</span>
<span class="ltx_bibblock">
X. Ye, F. Xiong, J. Lu, J. Zhou, and Y. Qian, “<math id="bib.bib158.1.m1.1" class="ltx_Math" alttext="\mathcal{R}^{3}" display="inline"><semantics id="bib.bib158.1.m1.1a"><msup id="bib.bib158.1.m1.1.1" xref="bib.bib158.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="bib.bib158.1.m1.1.1.2" xref="bib.bib158.1.m1.1.1.2.cmml">ℛ</mi><mn id="bib.bib158.1.m1.1.1.3" xref="bib.bib158.1.m1.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="bib.bib158.1.m1.1b"><apply id="bib.bib158.1.m1.1.1.cmml" xref="bib.bib158.1.m1.1.1"><csymbol cd="ambiguous" id="bib.bib158.1.m1.1.1.1.cmml" xref="bib.bib158.1.m1.1.1">superscript</csymbol><ci id="bib.bib158.1.m1.1.1.2.cmml" xref="bib.bib158.1.m1.1.1.2">ℛ</ci><cn type="integer" id="bib.bib158.1.m1.1.1.3.cmml" xref="bib.bib158.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib158.1.m1.1c">\mathcal{R}^{3}</annotation></semantics></math>-net: Feature
fusion and filtration network for object detection in optical remote sensing
images,” <em id="bib.bib158.2.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 12, no. 24, p. 4027, 2020.

</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[159]</span>
<span class="ltx_bibblock">
J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in <em id="bib.bib159.1.1" class="ltx_emph ltx_font_italic">Proc.
IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</em>, 2018, pp. 7132–7141.

</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[160]</span>
<span class="ltx_bibblock">
X. Li, W. Wang, X. Hu, and J. Yang, “Selective kernel networks,” in
<em id="bib.bib160.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</em>, 2019, pp.
510–519.

</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[161]</span>
<span class="ltx_bibblock">
J. Fu, J. Liu, H. Tian, Y. Li, Y. Bao, Z. Fang, and H. Lu, “Dual attention
network for scene segmentation,” in <em id="bib.bib161.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Conf. Comput. Vis.
Pattern Recognit. (CVPR)</em>, Jun. 2019, pp. 3146–3154.

</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[162]</span>
<span class="ltx_bibblock">
Z. Huang, W. Li, X. Xia, X. Wu, Z. Cai, and R. Tao, “A novel nonlocal-aware
pyramid and multiscale multitask refinement detector for object detection in
remote sensing images,” <em id="bib.bib162.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 60,
pp. 1–20, 2022. [Online]. Available:
https://doi.org/10.1109/TGRS.2021.3059450

</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[163]</span>
<span class="ltx_bibblock">
Y. Sun, X. Sun, Z. Wang, and K. Fu, “Oriented ship detection based on strong
scattering points network in large-scale SAR images,” <em id="bib.bib163.1.1" class="ltx_emph ltx_font_italic">IEEE Trans.
Geosci. Remote Sens.</em>, vol. 60, pp. 1–18, 2022. [Online]. Available:
https://doi.org/10.1109/TGRS.2021.3130117

</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[164]</span>
<span class="ltx_bibblock">
W. Ma, N. Li, H. Zhu, L. Jiao, X. Tang, Y. Guo, and B. Hou, “Feature
split-merge-enhancement network for remote sensing object detection,”
<em id="bib.bib164.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 60, pp. 1–17, 2022.

</span>
</li>
<li id="bib.bib165" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[165]</span>
<span class="ltx_bibblock">
Z. Cui, X. Wang, N. Liu, Z. Cao, and J. Yang, “Ship detection in large-scale
SAR images via spatial shuffle-group enhance attention,” <em id="bib.bib165.1.1" class="ltx_emph ltx_font_italic">IEEE
Trans. Geosci. Remote Sens.</em>, vol. 59, no. 1, pp. 379–391, 2021.

</span>
</li>
<li id="bib.bib166" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[166]</span>
<span class="ltx_bibblock">
J. Chen, L. Wan, J. Zhu, G. Xu, and M. Deng, “Multi-scale spatial and
channel-wise attention for improving object detection in remote sensing
imagery,” <em id="bib.bib166.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote Sens. Lett.</em>, vol. 17, no. 4, pp.
681–685, 2020.

</span>
</li>
<li id="bib.bib167" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[167]</span>
<span class="ltx_bibblock">
J. Bai, J. Ren, Y. Yang, Z. Xiao, W. Yu, V. Havyarimana, and L. Jiao, “Object
detection in large-scale remote-sensing images based on time-frequency
analysis and feature optimization,” <em id="bib.bib167.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote
Sens.</em>, vol. 60, pp. 1–16, 2022. [Online]. Available:
https://doi.org/10.1109/TGRS.2021.3119344

</span>
</li>
<li id="bib.bib168" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[168]</span>
<span class="ltx_bibblock">
J. Hu, X. Zhi, S. Jiang, H. Tang, W. Zhang, and L. Bruzzone, “Supervised
multi-scale attention-guided ship detection in optical remote sensing
images,” <em id="bib.bib168.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 60, pp. 1–14,
2022. [Online]. Available: https://doi.org/10.1109/TGRS.2022.3206306

</span>
</li>
<li id="bib.bib169" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[169]</span>
<span class="ltx_bibblock">
Y. Guo, X. Tong, X. Xu, S. Liu, Y. Feng, and H. Xie, “An anchor-free network
with density map and attention mechanism for multiscale object detection in
aerial images,” <em id="bib.bib169.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote Sens. Lett.</em>, vol. 19, pp. 1–5,
2022. [Online]. Available: https://doi.org/10.1109/LGRS.2022.3207178

</span>
</li>
<li id="bib.bib170" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[170]</span>
<span class="ltx_bibblock">
D. Yu and S. Ji, “A new spatial-oriented object detection framework for remote
sensing images,” <em id="bib.bib170.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 60, pp.
1–16, 2022. [Online]. Available:
https://doi.org/10.1109/TGRS.2021.3127232

</span>
</li>
<li id="bib.bib171" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[171]</span>
<span class="ltx_bibblock">
C. Li, B. Luo, H. Hong, X. Su, Y. Wang, J. Liu, C. Wang, J. Zhang, and L. Wei,
“Object detection based on global-local saliency constraint in aerial
images,” <em id="bib.bib171.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 12, no. 9, p. 1435, 2020.

</span>
</li>
<li id="bib.bib172" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[172]</span>
<span class="ltx_bibblock">
J. Lei, X. Luo, L. Fang, M. Wang, and Y. Gu, “Region-enhanced convolutional
neural network for object detection in remote sensing images,” <em id="bib.bib172.1.1" class="ltx_emph ltx_font_italic">IEEE
Trans. Geosci. Remote Sens.</em>, vol. 58, no. 8, pp. 5693–5702, 2020.

</span>
</li>
<li id="bib.bib173" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[173]</span>
<span class="ltx_bibblock">
Y. Yuan, C. Li, J. Kim, W. Cai, and D. D. Feng, “Reversion correction and
regularized random walk ranking for saliency detection,” <em id="bib.bib173.1.1" class="ltx_emph ltx_font_italic">IEEE Trans.
Image Process.</em>, vol. 27, no. 3, pp. 1311–1322, 2018.

</span>
</li>
<li id="bib.bib174" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[174]</span>
<span class="ltx_bibblock">
C. Xu, C. Li, Z. Cui, T. Zhang, and J. Yang, “Hierarchical semantic
propagation for object detection in remote sensing imagery,” <em id="bib.bib174.1.1" class="ltx_emph ltx_font_italic">IEEE
Trans. Geosci. Remote Sens.</em>, vol. 58, no. 6, pp. 4353–4364, 2020.

</span>
</li>
<li id="bib.bib175" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[175]</span>
<span class="ltx_bibblock">
T. Zhang, X. Zhang, P. Zhu, P. Chen, X. Tang, C. Li, and L. Jiao, “Foreground
refinement network for rotated object detection in remote sensing images,”
<em id="bib.bib175.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 60, pp. 1–13, 2021.

</span>
</li>
<li id="bib.bib176" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[176]</span>
<span class="ltx_bibblock">
J. Wang, W. Yang, H. Li, H. Zhang, and G. Xia, “Learning center probability
map for detecting objects in aerial images,” <em id="bib.bib176.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci.
Remote Sens.</em>, vol. 59, no. 5, pp. 4307–4323, 2021.

</span>
</li>
<li id="bib.bib177" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[177]</span>
<span class="ltx_bibblock">
Z. Fang, J. Ren, H. Sun, S. Marshall, J. Han, and H. Zhao, “Safdet: A
semi-anchor-free detector for effective detection of oriented objects in
aerial images,” <em id="bib.bib177.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 12, no. 19, p. 3225, 2020.

</span>
</li>
<li id="bib.bib178" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[178]</span>
<span class="ltx_bibblock">
Z. Ren, Y. Tang, Z. He, L. Tian, Y. Yang, and W. Zhang, “Ship detection in
high-resolution optical remote sensing images aided by saliency
information,” <em id="bib.bib178.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 60, pp. 1–16,
2022. [Online]. Available: https://doi.org/10.1109/TGRS.2022.3173610

</span>
</li>
<li id="bib.bib179" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[179]</span>
<span class="ltx_bibblock">
H. Qu, L. Shen, W. Guo, and J. Wang, “Ships detection in SAR images based on
anchor-free model with mask guidance features,” <em id="bib.bib179.1.1" class="ltx_emph ltx_font_italic">IEEE J. Sel. Top.
Appl. Earth Obs. Remote Sens.</em>, vol. 15, pp. 666–675, 2022.

</span>
</li>
<li id="bib.bib180" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[180]</span>
<span class="ltx_bibblock">
S. Liu, L. Zhang, H. Lu, and Y. He, “Center-boundary dual attention for
oriented object detection in remote sensing images,” <em id="bib.bib180.1.1" class="ltx_emph ltx_font_italic">IEEE Trans.
Geosci. Remote Sens.</em>, vol. 60, pp. 1–14, 2022. [Online]. Available:
https://doi.org/10.1109/TGRS.2021.3069056

</span>
</li>
<li id="bib.bib181" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[181]</span>
<span class="ltx_bibblock">
J. Zhang, C. Xie, X. Xu, Z. Shi, and B. Pan, “A contextual bidirectional
enhancement method for remote sensing image object detection,” <em id="bib.bib181.1.1" class="ltx_emph ltx_font_italic">IEEE
J. Sel. Top. Appl. Earth Obs. Remote Sens.</em>, vol. 13, pp. 4518–4531, 2020.

</span>
</li>
<li id="bib.bib182" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[182]</span>
<span class="ltx_bibblock">
Y. Gong, Z. Xiao, X. Tan, H. Sui, C. Xu, H. Duan, and D. Li, “Context-aware
convolutional neural network for object detection in VHR remote sensing
imagery,” <em id="bib.bib182.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 58, no. 1, pp.
34–44, 2020.

</span>
</li>
<li id="bib.bib183" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[183]</span>
<span class="ltx_bibblock">
W. Ma, Q. Guo, Y. Wu, W. Zhao, X. Zhang, and L. Jiao, “A novel multi-model
decision fusion network for object detection in remote sensing images,”
<em id="bib.bib183.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 11, no. 7, p. 737, 2019.

</span>
</li>
<li id="bib.bib184" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[184]</span>
<span class="ltx_bibblock">
S. Tian, L. Kang, X. Xing, Z. Li, L. Zhao, C. Fan, and Y. Zhang, “Siamese
graph embedding network for object detection in remote sensing images,”
<em id="bib.bib184.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote Sens. Lett.</em>, vol. 18, no. 4, pp. 602–606, 2021.

</span>
</li>
<li id="bib.bib185" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[185]</span>
<span class="ltx_bibblock">
S. Tian, L. Kang, X. Xing, J. Tian, C. Fan, and Y. Zhang, “A
relation-augmented embedded graph attention network for remote sensing object
detection,” <em id="bib.bib185.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 60, pp. 1–18,
2022. [Online]. Available: https://doi.org/10.1109/TGRS.2021.3073269

</span>
</li>
<li id="bib.bib186" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[186]</span>
<span class="ltx_bibblock">
Y. Wu, K. Zhang, J. Wang, Y. Wang, Q. Wang, and Q. Li, “Cdd-net: A
context-driven detection network for multiclass object detection,”
<em id="bib.bib186.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote Sens. Lett.</em>, vol. 19, pp. 1–5, 2022. [Online].
Available: https://doi.org/10.1109/LGRS.2020.3042465

</span>
</li>
<li id="bib.bib187" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[187]</span>
<span class="ltx_bibblock">
Y. Han, J. Liao, T. Lu, T. Pu, and Z. Peng, “Kcpnet: Knowledge-driven context
perception networks for ship detection in infrared imagery,” <em id="bib.bib187.1.1" class="ltx_emph ltx_font_italic">IEEE
Trans. Geosci. Remote Sens.</em>, vol. 61, pp. 1–19, 2023. [Online]. Available:
https://doi.org/10.1109/TGRS.2022.3233401

</span>
</li>
<li id="bib.bib188" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[188]</span>
<span class="ltx_bibblock">
C. Chen, W. Gong, Y. Chen, and W. Li, “Object detection in remote sensing
images based on a scene-contextual feature pyramid network,” <em id="bib.bib188.1.1" class="ltx_emph ltx_font_italic">Remote
Sens.</em>, vol. 11, no. 3, p. 339, 2019.

</span>
</li>
<li id="bib.bib189" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[189]</span>
<span class="ltx_bibblock">
Z. Wu, B. Hou, B. Ren, Z. Ren, S. Wang, and L. Jiao, “A deep detection network
based on interaction of instance segmentation and object detection for SAR
images,” <em id="bib.bib189.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 13, no. 13, p. 2582, 2021.

</span>
</li>
<li id="bib.bib190" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[190]</span>
<span class="ltx_bibblock">
Y. Wu, K. Zhang, J. Wang, Y. Wang, Q. Wang, and X. Li, “Gcwnet: A global
context-weaving network for object detection in remote sensing images,”
<em id="bib.bib190.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 60, pp. 1–12, 2022.
[Online]. Available: https://doi.org/10.1109/TGRS.2022.3155899

</span>
</li>
<li id="bib.bib191" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[191]</span>
<span class="ltx_bibblock">
G. Shi, J. Zhang, J. Liu, C. Zhang, C. Zhou, and S. Yang, “Global
context-augmented objection detection in VHR optical remote sensing
images,” <em id="bib.bib191.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 59, no. 12, pp.
10 604–10 617, 2021.

</span>
</li>
<li id="bib.bib192" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[192]</span>
<span class="ltx_bibblock">
J. Liu, S. Li, C. Zhou, X. Cao, Y. Gao, and B. Wang, “Sraf-net: A
scene-relevant anchor-free object detection network in remote sensing
images,” <em id="bib.bib192.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 60, pp. 1–14,
2022. [Online]. Available: https://doi.org/10.1109/TGRS.2021.3124959

</span>
</li>
<li id="bib.bib193" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[193]</span>
<span class="ltx_bibblock">
C. Tao, L. Mi, Y. Li, J. Qi, Y. Xiao, and J. Zhang, “Scene context-driven
vehicle detection in high-resolution aerial images,” <em id="bib.bib193.1.1" class="ltx_emph ltx_font_italic">IEEE Trans.
Geosci. Remote Sens.</em>, vol. 57, no. 10, pp. 7339–7351, 2019.

</span>
</li>
<li id="bib.bib194" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[194]</span>
<span class="ltx_bibblock">
K. Zhang, Y. Wu, J. Wang, Y. Wang, and Q. Wang, “Semantic context-aware
network for multiscale object detection in remote sensing images,”
<em id="bib.bib194.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote Sens. Lett.</em>, vol. 19, pp. 1–5, 2022. [Online].
Available: https://doi.org/10.1109/LGRS.2021.3067313

</span>
</li>
<li id="bib.bib195" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[195]</span>
<span class="ltx_bibblock">
M. Wang, Q. Li, Y. Gu, L. Fang, and X. X. Zhu, “Scaf-net: Scene context
attention-based fusion network for vehicle detection in aerial imagery,”
<em id="bib.bib195.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote Sens. Lett.</em>, vol. 19, pp. 1–5, 2022. [Online].
Available: https://doi.org/10.1109/LGRS.2021.3107281

</span>
</li>
<li id="bib.bib196" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[196]</span>
<span class="ltx_bibblock">
G. Zhang, S. Lu, and W. Zhang, “Cad-net: A context-aware detection network
for objects in remote sensing imagery,” <em id="bib.bib196.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote
Sens.</em>, vol. 57, no. 12, pp. 10 015–10 024, 2019.

</span>
</li>
<li id="bib.bib197" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[197]</span>
<span class="ltx_bibblock">
E. Liu, Y. Zheng, B. Pan, X. Xu, and Z. Shi, “Dcl-net: Augmenting the
capability of classification and localization for remote sensing object
detection,” <em id="bib.bib197.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 59, no. 9, pp.
7933–7944, 2021.

</span>
</li>
<li id="bib.bib198" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[198]</span>
<span class="ltx_bibblock">
Y. Feng, W. Diao, X. Sun, M. Yan, and X. Gao, “Towards automated ship
detection and category recognition from high-resolution aerial images,”
<em id="bib.bib198.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 11, no. 16, p. 1901, 2019.

</span>
</li>
<li id="bib.bib199" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[199]</span>
<span class="ltx_bibblock">
P. Wang, X. Sun, W. Diao, and K. Fu, “FMSSD: feature-merged single-shot
detection for multiscale objects in large-scale remote sensing imagery,”
<em id="bib.bib199.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 58, no. 5, pp. 3377–3390,
2020.

</span>
</li>
<li id="bib.bib200" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[200]</span>
<span class="ltx_bibblock">
L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, “Deeplab:
Semantic image segmentation with deep convolutional nets, atrous convolution,
and fully connected crfs,” <em id="bib.bib200.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Pattern Anal. Mach. Intell.</em>,
vol. 40, no. 4, pp. 834–848, 2018.

</span>
</li>
<li id="bib.bib201" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[201]</span>
<span class="ltx_bibblock">
Y. Bai, R. Li, S. Gou, C. Zhang, Y. Chen, and Z. Zheng, “Cross-connected
bidirectional pyramid network for infrared small-dim target detection,”
<em id="bib.bib201.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote Sens. Lett.</em>, vol. 19, pp. 1–5, 2022. [Online].
Available: https://doi.org/10.1109/LGRS.2022.3145577

</span>
</li>
<li id="bib.bib202" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[202]</span>
<span class="ltx_bibblock">
Y. Li, Q. Huang, X. Pei, Y. Chen, L. Jiao, and R. Shang, “Cross-layer
attention network for small object detection in remote sensing imagery,”
<em id="bib.bib202.1.1" class="ltx_emph ltx_font_italic">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</em>, vol. 14, pp.
2148–2161, 2021.

</span>
</li>
<li id="bib.bib203" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[203]</span>
<span class="ltx_bibblock">
H. Gong, T. Mu, Q. Li, H. Dai, C. Li, Z. He, W. Wang, F. Han, A. Tuniyazi,
H. Li, X. Lang, Z. Li, and B. Wang, “Swin-transformer-enabled yolov5 with
attention mechanism for small object detection on satellite images,”
<em id="bib.bib203.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 14, no. 12, p. 2861, 2022.

</span>
</li>
<li id="bib.bib204" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[204]</span>
<span class="ltx_bibblock">
J. Qu, C. Su, Z. Zhang, and A. Razi, “Dilated convolution and feature fusion
SSD network for small object detection in remote sensing images,”
<em id="bib.bib204.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 8, pp. 82 832–82 843, 2020.

</span>
</li>
<li id="bib.bib205" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[205]</span>
<span class="ltx_bibblock">
T. Ma, Z. Yang, J. Wang, S. Sun, X. Ren, and U. Ahmad, “Infrared small target
detection network with generate label and feature mapping,” <em id="bib.bib205.1.1" class="ltx_emph ltx_font_italic">IEEE
Geosci. Remote Sens. Lett.</em>, vol. 19, pp. 1–5, 2022. [Online]. Available:
https://doi.org/10.1109/LGRS.2022.3140432

</span>
</li>
<li id="bib.bib206" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[206]</span>
<span class="ltx_bibblock">
W. Han, A. Kuerban, Y. Yang, Z. Huang, B. Liu, and J. Gao, “Multi-vision
network for accurate and real-time small object detection in optical remote
sensing images,” <em id="bib.bib206.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote Sens. Lett.</em>, vol. 19, pp.
1–5, 2022. [Online]. Available:
https://doi.org/10.1109/LGRS.2020.3044422

</span>
</li>
<li id="bib.bib207" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[207]</span>
<span class="ltx_bibblock">
Q. Hou, Z. Wang, F. Tan, Y. Zhao, H. Zheng, and W. Zhang, “Ristdnet: Robust
infrared small target detection network,” <em id="bib.bib207.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote Sens.
Lett.</em>, vol. 19, pp. 1–5, 2022. [Online]. Available:
https://doi.org/10.1109/LGRS.2021.3050828

</span>
</li>
<li id="bib.bib208" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[208]</span>
<span class="ltx_bibblock">
X. Lu, Y. Zhang, Y. Yuan, and Y. Feng, “Gated and axis-concentrated
localization network for remote sensing object detection,” <em id="bib.bib208.1.1" class="ltx_emph ltx_font_italic">IEEE
Trans. Geosci. Remote Sens.</em>, vol. 58, no. 1, pp. 179–192, 2020.

</span>
</li>
<li id="bib.bib209" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[209]</span>
<span class="ltx_bibblock">
L. Courtrai, M. Pham, and S. Lefèvre, “Small object detection in remote
sensing images based on super-resolution with auxiliary generative
adversarial networks,” <em id="bib.bib209.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 12, no. 19, p. 3152, 2020.

</span>
</li>
<li id="bib.bib210" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[210]</span>
<span class="ltx_bibblock">
S. M. A. Bashir and Y. Wang, “Small object detection in remote sensing images
with residual feature aggregation-based super-resolution and object detector
network,” <em id="bib.bib210.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 13, no. 9, p. 1854, 2021.

</span>
</li>
<li id="bib.bib211" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[211]</span>
<span class="ltx_bibblock">
J. Rabbi, N. Ray, M. Schubert, S. Chowdhury, and D. Chao, “Small-object
detection in remote sensing images with end-to-end edge-enhanced GAN and
object detector network,” <em id="bib.bib211.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 12, no. 9, p. 1432,
2020.

</span>
</li>
<li id="bib.bib212" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[212]</span>
<span class="ltx_bibblock">
J. Wu and S. Xu, “From point to region: Accurate and efficient hierarchical
small object detection in low-resolution remote sensing images,”
<em id="bib.bib212.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 13, no. 13, p. 2620, 2021.

</span>
</li>
<li id="bib.bib213" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[213]</span>
<span class="ltx_bibblock">
J. Li, Z. Zhang, Y. Tian, Y. Xu, Y. Wen, and S. Wang, “Target-guided feature
super-resolution for vehicle detection in remote sensing images,”
<em id="bib.bib213.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote Sens. Lett.</em>, vol. 19, pp. 1–5, 2022. [Online].
Available: https://doi.org/10.1109/LGRS.2021.3112172

</span>
</li>
<li id="bib.bib214" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[214]</span>
<span class="ltx_bibblock">
J. Chen, K. Chen, H. Chen, Z. Zou, and Z. Shi, “A degraded reconstruction
enhancement-based method for tiny ship detection in remote sensing images
with a new large-scale dataset,” <em id="bib.bib214.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>,
vol. 60, pp. 1–14, 2022. [Online]. Available:
https://doi.org/10.1109/TGRS.2022.3180894

</span>
</li>
<li id="bib.bib215" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[215]</span>
<span class="ltx_bibblock">
J. Pang, C. Li, J. Shi, Z. Xu, and H. Feng, “<math id="bib.bib215.1.m1.1" class="ltx_Math" alttext="\mathcal{R}^{2}" display="inline"><semantics id="bib.bib215.1.m1.1a"><msup id="bib.bib215.1.m1.1.1" xref="bib.bib215.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="bib.bib215.1.m1.1.1.2" xref="bib.bib215.1.m1.1.1.2.cmml">ℛ</mi><mn id="bib.bib215.1.m1.1.1.3" xref="bib.bib215.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="bib.bib215.1.m1.1b"><apply id="bib.bib215.1.m1.1.1.cmml" xref="bib.bib215.1.m1.1.1"><csymbol cd="ambiguous" id="bib.bib215.1.m1.1.1.1.cmml" xref="bib.bib215.1.m1.1.1">superscript</csymbol><ci id="bib.bib215.1.m1.1.1.2.cmml" xref="bib.bib215.1.m1.1.1.2">ℛ</ci><cn type="integer" id="bib.bib215.1.m1.1.1.3.cmml" xref="bib.bib215.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib215.1.m1.1c">\mathcal{R}^{2}</annotation></semantics></math>-cnn: Fast tiny
object detection in large-scale remote sensing images,” <em id="bib.bib215.2.1" class="ltx_emph ltx_font_italic">IEEE Trans.
Geosci. Remote Sens.</em>, vol. 57, no. 8, pp. 5512–5524, 2019.

</span>
</li>
<li id="bib.bib216" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[216]</span>
<span class="ltx_bibblock">
J. Wu, Z. Pan, B. Lei, and Y. Hu, “Fsanet: Feature-and-spatial-aligned network
for tiny object detection in remote sensing images,” <em id="bib.bib216.1.1" class="ltx_emph ltx_font_italic">IEEE Trans.
Geosci. Remote Sens.</em>, vol. 60, pp. 1–17, 2022. [Online]. Available:
https://doi.org/10.1109/TGRS.2022.3205052

</span>
</li>
<li id="bib.bib217" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[217]</span>
<span class="ltx_bibblock">
M. Pham, L. Courtrai, C. Friguet, S. Lefèvre, and A. Baussard,
“Yolo-fine: One-stage detector of small objects under various backgrounds in
remote sensing images,” <em id="bib.bib217.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 12, no. 15, p. 2501, 2020.

</span>
</li>
<li id="bib.bib218" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[218]</span>
<span class="ltx_bibblock">
J. Yan, H. Wang, M. Yan, W. Diao, X. Sun, and H. Li, “Iou-adaptive deformable
R-CNN: make full use of iou for multi-class object detection in remote
sensing imagery,” <em id="bib.bib218.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 11, no. 3, p. 286, 2019.

</span>
</li>
<li id="bib.bib219" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[219]</span>
<span class="ltx_bibblock">
R. Dong, D. Xu, J. Zhao, L. Jiao, and J. An, “Sig-nms-based faster R-CNN
combining transfer learning for small target detection in VHR optical
remote sensing imagery,” <em id="bib.bib219.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 57,
no. 11, pp. 8534–8545, 2019.

</span>
</li>
<li id="bib.bib220" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[220]</span>
<span class="ltx_bibblock">
Z. Shu, X. Hu, and J. Sun, “Center-point-guided proposal generation for
detection of small and dense buildings in aerial imagery,” <em id="bib.bib220.1.1" class="ltx_emph ltx_font_italic">IEEE
Geosci. Remote Sens. Lett.</em>, vol. 15, no. 7, pp. 1100–1104, 2018.

</span>
</li>
<li id="bib.bib221" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[221]</span>
<span class="ltx_bibblock">
C. Xu, J. Wang, W. Yang, and L. Yu, “Dot distance for tiny object detection in
aerial images,” in <em id="bib.bib221.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Int. Conf. Comput. Vis. Pattern
Recognit. Workshops</em>.   IEEE, 2021,
pp. 1192–1201.

</span>
</li>
<li id="bib.bib222" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[222]</span>
<span class="ltx_bibblock">
C. Xu, J. Wang, W. Yang, H. Yu, L. Yu, and G. Xia, “RFLA: gaussian receptive
field based label assignment for tiny object detection,” pp. 526–543, 2022.

</span>
</li>
<li id="bib.bib223" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[223]</span>
<span class="ltx_bibblock">
F. Zhang, B. Du, L. Zhang, and M. Xu, “Weakly supervised learning based on
coupled convolutional neural networks for aircraft detection,” <em id="bib.bib223.1.1" class="ltx_emph ltx_font_italic">IEEE
Trans. Geosci. Remote Sens.</em>, vol. 54, no. 9, pp. 5553–5563, 2016.

</span>
</li>
<li id="bib.bib224" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[224]</span>
<span class="ltx_bibblock">
Y. Li, B. He, F. Melgani, and T. Long, “Point-based weakly supervised learning
for object detection in high spatial resolution remote sensing images,”
<em id="bib.bib224.1.1" class="ltx_emph ltx_font_italic">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</em>, vol. 14, pp.
5361–5371, 2021.

</span>
</li>
<li id="bib.bib225" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[225]</span>
<span class="ltx_bibblock">
D. Zhang, J. Han, G. Cheng, Z. Liu, S. Bu, and L. Guo, “Weakly supervised
learning for target detection in remote sensing images,” <em id="bib.bib225.1.1" class="ltx_emph ltx_font_italic">IEEE
Geosci. Remote Sens. Lett.</em>, vol. 12, no. 4, pp. 701–705, 2015.

</span>
</li>
<li id="bib.bib226" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[226]</span>
<span class="ltx_bibblock">
J. Han, D. Zhang, G. Cheng, L. Guo, and J. Ren, “Object detection in optical
remote sensing images based on weakly supervised learning and high-level
feature learning,” <em id="bib.bib226.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 53,
no. 6, pp. 3325–3337, 2015.

</span>
</li>
<li id="bib.bib227" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[227]</span>
<span class="ltx_bibblock">
Y. Li, Y. Zhang, X. Huang, and A. L. Yuille, “Deep networks under scene-level
supervision for multi-class geospatial object detection from remote sensing
images,” <em id="bib.bib227.1.1" class="ltx_emph ltx_font_italic">ISPRS J. Photogrammetry Remote Sens.</em>, vol. 146, pp.
182–196, 2018.

</span>
</li>
<li id="bib.bib228" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[228]</span>
<span class="ltx_bibblock">
H. Bilen and A. Vedaldi, “Weakly supervised deep detection networks,” in
<em id="bib.bib228.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</em>, 2016, pp.
2846–2854.

</span>
</li>
<li id="bib.bib229" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[229]</span>
<span class="ltx_bibblock">
X. Yao, X. Feng, J. Han, G. Cheng, and L. Guo, “Automatic weakly supervised
object detection from high spatial resolution remote sensing images via
dynamic curriculum learning,” <em id="bib.bib229.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>,
vol. 59, no. 1, pp. 675–685, 2021.

</span>
</li>
<li id="bib.bib230" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[230]</span>
<span class="ltx_bibblock">
H. Wang, H. Li, W. Qian, W. Diao, L. Zhao, J. Zhang, and D. Zhang, “Dynamic
pseudo-label generation for weakly supervised object detection in remote
sensing images,” <em id="bib.bib230.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 13, no. 8, p. 1461, 2021.

</span>
</li>
<li id="bib.bib231" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[231]</span>
<span class="ltx_bibblock">
X. Feng, J. Han, X. Yao, and G. Cheng, “Progressive contextual instance
refinement for weakly supervised object detection in remote sensing images,”
<em id="bib.bib231.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 58, no. 11, pp. 8002–8012,
2020.

</span>
</li>
<li id="bib.bib232" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[232]</span>
<span class="ltx_bibblock">
P. Shamsolmoali, J. Chanussot, M. Zareapoor, H. Zhou, and J. Yang, “Multipatch
feature pyramid network for weakly supervised object detection in optical
remote sensing images,” <em id="bib.bib232.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 60,
pp. 1–13, 2022. [Online]. Available:
https://doi.org/10.1109/TGRS.2021.3106442

</span>
</li>
<li id="bib.bib233" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[233]</span>
<span class="ltx_bibblock">
B. Wang, Y. Zhao, and X. Li, “Multiple instance graph learning for weakly
supervised remote sensing object detection,” <em id="bib.bib233.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci.
Remote Sens.</em>, vol. 60, pp. 1–12, 2022. [Online]. Available:
https://doi.org/10.1109/TGRS.2021.3123231

</span>
</li>
<li id="bib.bib234" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[234]</span>
<span class="ltx_bibblock">
X. Feng, J. Han, X. Yao, and G. Cheng, “Tcanet: Triple context-aware network
for weakly supervised object detection in remote sensing images,”
<em id="bib.bib234.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 59, no. 8, pp. 6946–6955,
2021.

</span>
</li>
<li id="bib.bib235" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[235]</span>
<span class="ltx_bibblock">
X. Feng, X. Yao, G. Cheng, J. Han, and J. Han, “Saenet: Self-supervised
adversarial and equivariant network for weakly supervised object detection in
remote sensing images,” <em id="bib.bib235.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 60,
pp. 1–11, 2022. [Online]. Available:
https://doi.org/10.1109/TGRS.2021.3105575

</span>
</li>
<li id="bib.bib236" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[236]</span>
<span class="ltx_bibblock">
X. Qian, Y. Huo, G. Cheng, X. Yao, K. Li, H. Ren, and W. Wang, “Incorporating
the completeness and difficulty of proposals into weakly supervised object
detection in remote sensing images,” <em id="bib.bib236.1.1" class="ltx_emph ltx_font_italic">IEEE J. Sel. Top. Appl. Earth
Obs. Remote Sens.</em>, vol. 15, pp. 1902–1911, 2022.

</span>
</li>
<li id="bib.bib237" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[237]</span>
<span class="ltx_bibblock">
W. Qian, Z. Yan, Z. Zhu, and W. Yin, “Weakly supervised part-based method for
combined object detection in remote sensing imagery,” <em id="bib.bib237.1.1" class="ltx_emph ltx_font_italic">IEEE J. Sel.
Top. Appl. Earth Obs. Remote Sens.</em>, vol. 15, pp. 5024–5036, 2022.

</span>
</li>
<li id="bib.bib238" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[238]</span>
<span class="ltx_bibblock">
S. Chen, D. Shao, X. Shu, C. Zhang, and J. Wang, “Fcc-net: A full-coverage
collaborative network for weakly supervised remote sensing object
detection,” <em id="bib.bib238.1.1" class="ltx_emph ltx_font_italic">Electronics</em>, vol. 9, no. 9, p. 1356, 2020.

</span>
</li>
<li id="bib.bib239" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[239]</span>
<span class="ltx_bibblock">
G. Cheng, X. Xie, W. Chen, X. Feng, X. Yao, and J. Han, “Self-guided proposal
generation for weakly supervised object detection,” <em id="bib.bib239.1.1" class="ltx_emph ltx_font_italic">IEEE Trans.
Geosci. Remote Sens.</em>, vol. 60, pp. 1–11, 2022.

</span>
</li>
<li id="bib.bib240" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[240]</span>
<span class="ltx_bibblock">
X. Feng, X. Yao, G. Cheng, and J. Han, “Weakly supervised rotation-invariant
aerial object detection network,” in <em id="bib.bib240.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Int. Conf. Comput.
Vis. Pattern Recognit. (CVPR)</em>.   IEEE, 2022, pp. 14 126–14 135.

</span>
</li>
<li id="bib.bib241" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[241]</span>
<span class="ltx_bibblock">
G. Wang, X. Zhang, Z. Peng, X. Jia, X. Tang, and L. Jiao, “Mol: Towards
accurate weakly supervised remote sensing object detection via multi-view
noisy learning,” <em id="bib.bib241.1.1" class="ltx_emph ltx_font_italic">ISPRS J. Photogrammetry Remote Sens.</em>, vol. 196, pp.
457–470, 2023.

</span>
</li>
<li id="bib.bib242" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[242]</span>
<span class="ltx_bibblock">
T. Deselaers, B. Alexe, and V. Ferrari, “Weakly supervised localization and
learning with generic knowledge,” <em id="bib.bib242.1.1" class="ltx_emph ltx_font_italic">Int. J. Comput. Vis.</em>, vol. 100,
no. 3, pp. 275–293, 2012.

</span>
</li>
<li id="bib.bib243" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[243]</span>
<span class="ltx_bibblock">
B. Hou, Z. Wu, B. Ren, Z. Li, X. Guo, S. Wang, and L. Jiao, “A neural network
based on consistency learning and adversarial learning for semisupervised
synthetic aperture radar ship detection,” <em id="bib.bib243.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote
Sens.</em>, vol. 60, pp. 1–16, 2022.

</span>
</li>
<li id="bib.bib244" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[244]</span>
<span class="ltx_bibblock">
Z. Song, J. Yang, D. Zhang, S. Wang, and Z. Li, “Semi-supervised dim and small
infrared ship detection network based on haar wavelet,” <em id="bib.bib244.1.1" class="ltx_emph ltx_font_italic">IEEE
Access</em>, vol. 9, pp. 29 686–29 695, 2021.

</span>
</li>
<li id="bib.bib245" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[245]</span>
<span class="ltx_bibblock">
Y. Zhong, Z. Zheng, A. Ma, X. Lu, and L. Zhang, “COLOR: cycling, offline
learning, and online representation framework for airport and airplane
detection using GF-2 satellite images,” <em id="bib.bib245.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote
Sens.</em>, vol. 58, no. 12, pp. 8438–8449, 2020.

</span>
</li>
<li id="bib.bib246" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[246]</span>
<span class="ltx_bibblock">
Y. Wu, W. Zhao, R. Zhang, and F. Jiang, “Amr-net: Arbitrary-oriented ship
detection using attention module, multi-scale feature fusion and rotation
pseudo-label,” <em id="bib.bib246.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 9, pp. 68 208–68 222, 2021.

</span>
</li>
<li id="bib.bib247" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[247]</span>
<span class="ltx_bibblock">
S. Chen, R. Zhan, W. Wang, and J. Zhang, “Domain adaptation for
semi-supervised ship detection in SAR images,” <em id="bib.bib247.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote
Sens. Lett.</em>, vol. 19, pp. 1–5, 2022. [Online]. Available:
https://doi.org/10.1109/LGRS.2022.3171789

</span>
</li>
<li id="bib.bib248" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[248]</span>
<span class="ltx_bibblock">
Z. Zhang, Z. Feng, and S. Yang, “Semi-supervised object detection framework
with object first mixup for remote sensing images,” in <em id="bib.bib248.1.1" class="ltx_emph ltx_font_italic">International
Geoscience and Remote Sensing Symposium</em>.   IEEE, 2021, pp. 2596–2599.

</span>
</li>
<li id="bib.bib249" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[249]</span>
<span class="ltx_bibblock">
B. Xue and N. Tong, “DIOD: fast and efficient weakly semi-supervised deep
complex ISAR object detection,” <em id="bib.bib249.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Cybern.</em>, vol. 49,
no. 11, pp. 3991–4003, 2019.

</span>
</li>
<li id="bib.bib250" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[250]</span>
<span class="ltx_bibblock">
L. Liao, L. Du, and Y. Guo, “Semi-supervised SAR target detection based on
an improved faster R-CNN,” <em id="bib.bib250.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 14, no. 1, p. 143,
2022. [Online]. Available: https://doi.org/10.3390/rs14010143

</span>
</li>
<li id="bib.bib251" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[251]</span>
<span class="ltx_bibblock">
Y. Du, L. Du, Y. Guo, and Y. Shi, “Semisupervised sar ship detection network
via scene characteristic learning,” <em id="bib.bib251.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote
Sens.</em>, vol. 61, pp. 1–17, 2023. [Online]. Available:
https://doi.org/10.1109/TGRS.2023.3235859

</span>
</li>
<li id="bib.bib252" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[252]</span>
<span class="ltx_bibblock">
D. Wei, Y. Du, L. Du, and L. Li, “Target detection network for SAR images
based on semi-supervised learning and attention mechanism,” <em id="bib.bib252.1.1" class="ltx_emph ltx_font_italic">Remote
Sens.</em>, vol. 13, no. 14, p. 2686, 2021.

</span>
</li>
<li id="bib.bib253" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[253]</span>
<span class="ltx_bibblock">
L. Chen, Y. Fu, S. You, and H. Liu, “Efficient hybrid supervision for instance
segmentation in aerial images,” <em id="bib.bib253.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 13, no. 2, p. 252,
2021.

</span>
</li>
<li id="bib.bib254" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[254]</span>
<span class="ltx_bibblock">
G. Cheng, B. Yan, P. Shi, K. Li, X. Yao, L. Guo, and J. Han, “Prototype-cnn
for few-shot object detection in remote sensing images,” <em id="bib.bib254.1.1" class="ltx_emph ltx_font_italic">IEEE Trans.
Geosci. Remote Sens.</em>, vol. 60, pp. 1–10, 2022. [Online]. Available:
https://doi.org/10.1109/TGRS.2021.3078507

</span>
</li>
<li id="bib.bib255" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[255]</span>
<span class="ltx_bibblock">
X. Li, J. Deng, and Y. Fang, “Few-shot object detection on remote sensing
images,” <em id="bib.bib255.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 60, pp. 1–14,
2022. [Online]. Available: https://doi.org/10.1109/TGRS.2021.3051383

</span>
</li>
<li id="bib.bib256" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[256]</span>
<span class="ltx_bibblock">
L. Li, X. Yao, G. Cheng, M. Xu, J. Han, and J. Han, “Solo-to-collaborative
dual-attention network for one-shot object detection in remote sensing
images,” <em id="bib.bib256.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 60, pp. 1–11,
2022. [Online]. Available: https://doi.org/10.1109/TGRS.2021.3091003

</span>
</li>
<li id="bib.bib257" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[257]</span>
<span class="ltx_bibblock">
H. Zhang, X. Zhang, G. Meng, C. Guo, and Z. Jiang, “Few-shot multi-class ship
detection in remote sensing images using attention feature map and
multi-relation detector,” <em id="bib.bib257.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 14, no. 12, p. 2790,
2022.

</span>
</li>
<li id="bib.bib258" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[258]</span>
<span class="ltx_bibblock">
B. Wang, Z. Wang, X. Sun, H. Wang, and K. Fu, “Dmml-net: Deep metametric
learning for few-shot geographic object segmentation in remote sensing
imagery,” <em id="bib.bib258.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 60, pp. 1–18,
2022. [Online]. Available: https://doi.org/10.1109/TGRS.2021.3116672

</span>
</li>
<li id="bib.bib259" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[259]</span>
<span class="ltx_bibblock">
J. Li, Y. Tian, Y. Xu, X. Hu, Z. Zhang, H. Wang, and Y. Xiao, “Mm-rcnn: Toward
few-shot object detection in remote sensing images with meta memory,”
<em id="bib.bib259.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 60, pp. 1–14, 2022.
[Online]. Available: https://doi.org/10.1109/TGRS.2022.3228612

</span>
</li>
<li id="bib.bib260" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[260]</span>
<span class="ltx_bibblock">
Z. Zhao, P. Tang, L. Zhao, and Z. Zhang, “Few-shot object detection of remote
sensing images via two-stage fine-tuning,” <em id="bib.bib260.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote Sens.
Lett.</em>, vol. 19, pp. 1–5, 2022. [Online]. Available:
https://doi.org/10.1109/LGRS.2021.3116858

</span>
</li>
<li id="bib.bib261" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[261]</span>
<span class="ltx_bibblock">
Y. Zhou, H. Hu, J. Zhao, H. Zhu, R. Yao, and W. Du, “Few-shot object detection
via context-aware aggregation for remote sensing images,” <em id="bib.bib261.1.1" class="ltx_emph ltx_font_italic">IEEE
Geosci. Remote Sens. Lett.</em>, vol. 19, pp. 1–5, 2022. [Online]. Available:
https://doi.org/10.1109/LGRS.2022.3171257

</span>
</li>
<li id="bib.bib262" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[262]</span>
<span class="ltx_bibblock">
Y. Wang, C. Xu, C. Liu, and Z. Li, “Context information refinement for
few-shot object detection in remote sensing images,” <em id="bib.bib262.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>,
vol. 14, no. 14, p. 3255, 2022.

</span>
</li>
<li id="bib.bib263" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[263]</span>
<span class="ltx_bibblock">
Z. Zhou, S. Li, W. Guo, and Y. Gu, “Few-shot aircraft detection in satellite
videos based on feature scale selection pyramid and proposal contrastive
learning,” <em id="bib.bib263.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 14, no. 18, p. 4581, 2022.

</span>
</li>
<li id="bib.bib264" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[264]</span>
<span class="ltx_bibblock">
S. Chen, J. Zhang, R. Zhan, R. Zhu, and W. Wang, “Few shot object detection
for SAR images via feature enhancement and dynamic relationship modeling,”
<em id="bib.bib264.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 14, no. 15, p. 3669, 2022.

</span>
</li>
<li id="bib.bib265" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[265]</span>
<span class="ltx_bibblock">
S. Liu, Y. You, H. Su, G. Meng, W. Yang, and F. Liu, “Few-shot object
detection in remote sensing image interpretation: Opportunities and
challenges,” <em id="bib.bib265.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 14, no. 18, p. 4435, 2022.

</span>
</li>
<li id="bib.bib266" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[266]</span>
<span class="ltx_bibblock">
X. Huang, B. He, M. Tong, D. Wang, and C. He, “Few-shot object detection on
remote sensing images via shared attention module and balanced fine-tuning
strategy,” <em id="bib.bib266.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 13, no. 19, p. 3816, 2021.

</span>
</li>
<li id="bib.bib267" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[267]</span>
<span class="ltx_bibblock">
Z. Xiao, J. Qi, W. Xue, and P. Zhong, “Few-shot object detection with
self-adaptive attention network for remote sensing images,” <em id="bib.bib267.1.1" class="ltx_emph ltx_font_italic">IEEE J.
Sel. Top. Appl. Earth Obs. Remote Sens.</em>, vol. 14, pp. 4854–4865, 2021.

</span>
</li>
<li id="bib.bib268" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[268]</span>
<span class="ltx_bibblock">
S. Wolf, J. Meier, L. Sommer, and J. Beyerer, “Double head predictor based
few-shot object detection for aerial imagery,” in <em id="bib.bib268.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Int.
Conf. Comput. Vis. Workshops</em>.   IEEE,
2021, pp. 721–731.

</span>
</li>
<li id="bib.bib269" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[269]</span>
<span class="ltx_bibblock">
T. Zhang, X. Zhang, P. Zhu, X. Jia, X. Tang, and L. Jiao, “Generalized
few-shot object detection in remote sensing images,” <em id="bib.bib269.1.1" class="ltx_emph ltx_font_italic">ISPRS J.
Photogrammetry Remote Sens.</em>, vol. 195, pp. 353–364, 2023.

</span>
</li>
<li id="bib.bib270" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[270]</span>
<span class="ltx_bibblock">
G. Heitz and D. Koller, “Learning spatial context: Using stuff to find
things,” in <em id="bib.bib270.1.1" class="ltx_emph ltx_font_italic">Proc. Euro. Conf. Comput. Vis.,</em>, vol. 5302.   Springer, 2008, pp. 30–43.

</span>
</li>
<li id="bib.bib271" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[271]</span>
<span class="ltx_bibblock">
C. Benedek, X. Descombes, and J. Zerubia, “Building development monitoring in
multitemporal remotely sensed image pairs with stochastic birth-death
dynamics,” <em id="bib.bib271.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Pattern Anal. Mach. Intell.</em>, vol. 34, no. 1,
pp. 33–50, 2012.

</span>
</li>
<li id="bib.bib272" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[272]</span>
<span class="ltx_bibblock">
S. Razakarivony and F. Jurie, “Vehicle detection in aerial imagery : A small
target detection benchmark,” <em id="bib.bib272.1.1" class="ltx_emph ltx_font_italic">J. Vis. Commun. Image Represent.</em>,
vol. 34, pp. 187–203, 2016.

</span>
</li>
<li id="bib.bib273" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[273]</span>
<span class="ltx_bibblock">
K. Liu and G. Máttyus, “Fast multiclass vehicle detection on aerial
images,” <em id="bib.bib273.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote Sens. Lett.</em>, vol. 12, no. 9, pp.
1938–1942, 2015.

</span>
</li>
<li id="bib.bib274" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[274]</span>
<span class="ltx_bibblock">
H. Zhu, X. Chen, W. Dai, K. Fu, Q. Ye, and J. Jiao, “Orientation robust object
detection in aerial images using deep convolutional neural network,” in
<em id="bib.bib274.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Int. Conf. Image Process.</em>, 2015, pp. 3735–3739.

</span>
</li>
<li id="bib.bib275" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[275]</span>
<span class="ltx_bibblock">
T. N. Mundhenk, G. Konjevod, W. A. Sakla, and K. Boakye, “A large contextual
dataset for classification, detection and counting of cars with deep
learning,” in <em id="bib.bib275.1.1" class="ltx_emph ltx_font_italic">Proc. Euro. Conf. Comput. Vis.,</em>, B. Leibe, J. Matas,
N. Sebe, and M. Welling, Eds., vol. 9907, 2016, pp. 785–800.

</span>
</li>
<li id="bib.bib276" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[276]</span>
<span class="ltx_bibblock">
Z. Liu, H. Wang, L. Weng, and Y. Yang, “Ship rotated bounding box space for
ship extraction from high-resolution optical satellite images with complex
backgrounds,” <em id="bib.bib276.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote Sens. Lett.</em>, vol. 13, no. 8, pp.
1074–1078, 2016.

</span>
</li>
<li id="bib.bib277" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[277]</span>
<span class="ltx_bibblock">
J. Li, C. Qu, and J. Shao, “Ship detection in sar images based on an improved
faster r-cnn,” in <em id="bib.bib277.1.1" class="ltx_emph ltx_font_italic">SAR in Big Data Era: Models, Methods and
Applications (BIGSARDATA)</em>.   IEEE,
2017, pp. 1–6.

</span>
</li>
<li id="bib.bib278" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[278]</span>
<span class="ltx_bibblock">
Z. Zou and Z. Shi, “Random access memories: A new paradigm for target
detection in high resolution aerial remote sensing images,” <em id="bib.bib278.1.1" class="ltx_emph ltx_font_italic">IEEE
Trans. Image Process.</em>, vol. 27, no. 3, pp. 1100–1111, 2018.

</span>
</li>
<li id="bib.bib279" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[279]</span>
<span class="ltx_bibblock">
X. Sun, Z. Wang, Y. Sun, W. Diao, Y. Zhang, and K. Fu, “Air-sarship-1.0:
High-resolution sar ship detection dataset,” <em id="bib.bib279.1.1" class="ltx_emph ltx_font_italic">Journal of Radars</em>,
vol. 8, no. 6, pp. 852–863, 2019.

</span>
</li>
<li id="bib.bib280" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[280]</span>
<span class="ltx_bibblock">
W. Yu, G. Cheng, M. Wang, Y. Yao, X. Xie, X. Yao, and J. Han, “Mar20: A
benchmark for military aircraft recognition in remote sensing images,”
<em id="bib.bib280.1.1" class="ltx_emph ltx_font_italic">National Remote Sensing Bulletin</em>, pp. 1–11, 2022.

</span>
</li>
<li id="bib.bib281" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[281]</span>
<span class="ltx_bibblock">
K. Chen, M. Wu, J. Liu, and C. Zhang, “FGSD: A dataset for fine-grained
ship detection in high resolution satellite images,” 2020. [Online].
Available: https://arxiv.org/abs/2003.06832

</span>
</li>
<li id="bib.bib282" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[282]</span>
<span class="ltx_bibblock">
Y. Han, X. Yang, T. Pu, and Z. Peng, “Fine-grained recognition for oriented
ship against complex scenes in optical remote sensing images,” <em id="bib.bib282.1.1" class="ltx_emph ltx_font_italic">IEEE
Trans. Geosci. Remote. Sens.</em>, vol. 60, pp. 1–18, 2022. [Online]. Available:
https://doi.org/10.1109/TGRS.2021.3123666

</span>
</li>
<li id="bib.bib283" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[283]</span>
<span class="ltx_bibblock">
J. Wang, W. Yang, H. Guo, R. Zhang, and G. Xia, “Tiny object detection in
aerial images,” in <em id="bib.bib283.1.1" class="ltx_emph ltx_font_italic">Proc. Int. Conf. Pattern Recognit</em>.   IEEE, 2020, pp. 3791–3798.

</span>
</li>
<li id="bib.bib284" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[284]</span>
<span class="ltx_bibblock">
G. Cheng, X. Yuan, X. Yao, K. Yan, Q. Zeng, and J. Han, “Towards large-scale
small object detection: Survey and benchmarks,” 2022. [Online]. Available:
https://doi.org/10.48550/arXiv.2207.14096

</span>
</li>
<li id="bib.bib285" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[285]</span>
<span class="ltx_bibblock">
T. Zhang, X. Zhang, J. Shi, and S. Wei, “Hyperli-net: A hyper-light deep
learning network for high-accurate and high-speed ship detection from
synthetic aperture radar imagery,” <em id="bib.bib285.1.1" class="ltx_emph ltx_font_italic">ISPRS J. Photogrammetry Remote
Sens.</em>, vol. 167, pp. 123–153, 2020.

</span>
</li>
<li id="bib.bib286" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[286]</span>
<span class="ltx_bibblock">
T. Zhang, X. Zhang, J. Li, X. Xu, B. Wang, X. Zhan, Y. Xu, X. Ke, T. Zeng,
H. Su, I. Ahmad, D. Pan, C. Liu, Y. Zhou, J. Shi, and S. Wei, “SAR ship
detection dataset (SSDD): official release and comprehensive data
analysis,” <em id="bib.bib286.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 13, no. 18, p. 3690, 2021.

</span>
</li>
<li id="bib.bib287" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[287]</span>
<span class="ltx_bibblock">
M. Everingham, L. V. Gool, C. K. I. Williams, J. M. Winn, and A. Zisserman,
“The pascal visual object classes (VOC) challenge,” <em id="bib.bib287.1.1" class="ltx_emph ltx_font_italic">Int. J. Comput.
Vis.</em>, vol. 88, no. 2, pp. 303–338, 2010.

</span>
</li>
<li id="bib.bib288" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[288]</span>
<span class="ltx_bibblock">
A. G. Menezes, G. de Moura, C. Alves, and A. C. P. L. F. de Carvalho,
“Continual object detection: A review of definitions, strategies, and
challenges,” <em id="bib.bib288.1.1" class="ltx_emph ltx_font_italic">Neural Networks</em>, vol. 161, pp. 476–493, 2023.

</span>
</li>
<li id="bib.bib289" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[289]</span>
<span class="ltx_bibblock">
C. Persello, J. D. Wegner, R. Hänsch, D. Tuia, P. Ghamisi, M. Koeva, and
G. Camps-Valls, “Deep learning and earth observation to support the
sustainable development goals: Current approaches, open challenges, and
future opportunities,” <em id="bib.bib289.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote Sens. Mag.</em>, vol. 10,
no. 2, pp. 172–200, 2022.

</span>
</li>
<li id="bib.bib290" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[290]</span>
<span class="ltx_bibblock">
T. Hoeser, F. Bachofer, and C. Kuenzer, “Object detection and image
segmentation with deep learning on earth observation data: A review—part
ii: Applications,” <em id="bib.bib290.1.1" class="ltx_emph ltx_font_italic">Remote Sen.</em>, vol. 12, no. 18, p. 3053, 2020.

</span>
</li>
<li id="bib.bib291" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[291]</span>
<span class="ltx_bibblock">
L. Ma, Y. Liu, X. Zhang, Y. Ye, G. Yin, and B. A. Johnson, “Deep learning in
remote sensing applications: A meta-analysis and review,” <em id="bib.bib291.1.1" class="ltx_emph ltx_font_italic">ISPRS J.
Photogrammetry Remote Sens.</em>, vol. 152, pp. 166–177, 2019.

</span>
</li>
<li id="bib.bib292" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[292]</span>
<span class="ltx_bibblock">
P. Barmpoutis, P. Papaioannou, K. Dimitropoulos, and N. Grammalidis, “A review
on early forest fire detection systems using optical remote sensing,”
<em id="bib.bib292.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, vol. 20, no. 22, p. 6442, 2020.

</span>
</li>
<li id="bib.bib293" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[293]</span>
<span class="ltx_bibblock">
Z. Guan, X. Miao, Y. Mu, Q. Sun, Q. Ye, and D. Gao, “Forest fire segmentation
from aerial imagery data using an improved instance segmentation model,”
<em id="bib.bib293.1.1" class="ltx_emph ltx_font_italic">Remote. Sens.</em>, vol. 14, no. 13, p. 3159, 2022.

</span>
</li>
<li id="bib.bib294" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[294]</span>
<span class="ltx_bibblock">
Z. Zheng, Y. Zhong, J. Wang, A. Ma, and L. Zhang, “Building damage assessment
for rapid disaster response with a deep object-based semantic change
detection framework: From natural disasters to man-made disasters,”
<em id="bib.bib294.1.1" class="ltx_emph ltx_font_italic">Remote Sens. Environ.</em>, vol. 265, p. 112636, 2021.

</span>
</li>
<li id="bib.bib295" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[295]</span>
<span class="ltx_bibblock">
H. Ma, Y. Liu, Y. Ren, and J. Yu, “Detection of collapsed buildings in
post-earthquake remote sensing images based on the improved yolov3,”
<em id="bib.bib295.1.1" class="ltx_emph ltx_font_italic">Remote. Sens.</em>, vol. 12, no. 1, p. 44, 2020.

</span>
</li>
<li id="bib.bib296" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[296]</span>
<span class="ltx_bibblock">
Y. Pi, N. D. Nath, and A. H. Behzadan, “Convolutional neural networks for
object detection in aerial imagery for disaster response and recovery,”
<em id="bib.bib296.1.1" class="ltx_emph ltx_font_italic">Adv. Eng. Informatics</em>, vol. 43, p. 101009, 2020.

</span>
</li>
<li id="bib.bib297" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[297]</span>
<span class="ltx_bibblock">
M. Weiss, F. Jacob, and G. Duveiller, “Remote sensing for agricultural
applications: A meta-review,” <em id="bib.bib297.1.1" class="ltx_emph ltx_font_italic">Remote Sens. Environ.</em>, vol. 236, p.
111402, 2020.

</span>
</li>
<li id="bib.bib298" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[298]</span>
<span class="ltx_bibblock">
Y. Pang, Y. Shi, S. Gao, F. Jiang, A. N. V. Sivakumar, L. Thompson, J. D. Luck,
and C. Liu, “Improved crop row detection with deep neural network for
early-season maize stand count in UAV imagery,” <em id="bib.bib298.1.1" class="ltx_emph ltx_font_italic">Comput. Electron.
Agric.</em>, vol. 178, p. 105766, 2020.

</span>
</li>
<li id="bib.bib299" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[299]</span>
<span class="ltx_bibblock">
C. Mota-Delfin, G. de Jesús López-Canteñs, I. L. L. Cruz,
E. Romantchik-Kriuchkova, and J. C. Olguín-Rojas, “Detection and
counting of corn plants in the presence of weeds with convolutional neural
networks,” <em id="bib.bib299.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 14, no. 19, p. 4892, 2022.

</span>
</li>
<li id="bib.bib300" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[300]</span>
<span class="ltx_bibblock">
L. P. Osco, M. d. S. de Arruda, D. N. Gonçalves, A. Dias, J. Batistoti,
M. de Souza, F. D. G. Gomes, A. P. M. Ramos, L. A. de Castro Jorge,
V. Liesenberg <em id="bib.bib300.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “A cnn approach to simultaneously count plants
and detect plantation-rows from uav imagery,” <em id="bib.bib300.2.2" class="ltx_emph ltx_font_italic">ISPRS J. Photogrammetry
Remote Sens.</em>, vol. 174, pp. 1–17, 2021.

</span>
</li>
<li id="bib.bib301" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[301]</span>
<span class="ltx_bibblock">
M. M. Anuar, A. A. Halin, T. Perumal, and B. Kalantar, “Aerial imagery paddy
seedlings inspection using deep learning,” <em id="bib.bib301.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 14,
no. 2, p. 274, 2022.

</span>
</li>
<li id="bib.bib302" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[302]</span>
<span class="ltx_bibblock">
Y. Chen, W. S. Lee, H. Gan, N. Peres, C. W. Fraisse, Y. Zhang, and Y. He,
“Strawberry yield prediction based on a deep neural network using
high-resolution aerial orthoimages,” <em id="bib.bib302.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 11, no. 13,
p. 1584, 2019.

</span>
</li>
<li id="bib.bib303" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[303]</span>
<span class="ltx_bibblock">
W. Zhao, C. Persello, and A. Stein, “Building outline delineation: From aerial
images to polygons with an improved end-to-end learning framework,”
<em id="bib.bib303.1.1" class="ltx_emph ltx_font_italic">ISPRS J. Photogrammetry Remote Sens.</em>, vol. 175, pp. 119–131, 2021.

</span>
</li>
<li id="bib.bib304" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[304]</span>
<span class="ltx_bibblock">
Z. Li, J. D. Wegner, and A. Lucchi, “Topological map extraction from overhead
images,” in <em id="bib.bib304.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</em>, 2019, pp.
1715–1724.

</span>
</li>
<li id="bib.bib305" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[305]</span>
<span class="ltx_bibblock">
L. Mou and X. X. Zhu, “Vehicle instance segmentation from aerial image and
video using a multitask learning residual fully convolutional network,”
<em id="bib.bib305.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 56, no. 11, pp. 6699–6711,
2018.

</span>
</li>
<li id="bib.bib306" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[306]</span>
<span class="ltx_bibblock">
J. Zhang, X. Zhang, Z. Huang, X. Cheng, J. Feng, and L. Jiao, “Bidirectional
multiple object tracking based on trajectory criteria in satellite videos,”
<em id="bib.bib306.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Geoscience and Remote Sensing</em>, vol. 61, pp.
1–14, 2023.

</span>
</li>
<li id="bib.bib307" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[307]</span>
<span class="ltx_bibblock">
H. Kim and Y. Ham, “Participatory sensing-based geospatial localization of
distant objects for disaster preparedness in urban built environments,”
<em id="bib.bib307.1.1" class="ltx_emph ltx_font_italic">Automation in Construction</em>, vol. 107, p. 102960, 2019.

</span>
</li>
<li id="bib.bib308" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[308]</span>
<span class="ltx_bibblock">
M. A. E. Bhuiyan, C. Witharana, and A. K. Liljedahl, “Use of very high spatial
resolution commercial satellite imagery and deep learning to automatically
map ice-wedge polygons across tundra vegetation types,” <em id="bib.bib308.1.1" class="ltx_emph ltx_font_italic">J. Imaging</em>,
vol. 6, no. 12, p. 137, 2020.

</span>
</li>
<li id="bib.bib309" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[309]</span>
<span class="ltx_bibblock">
W. Zhang, A. K. Liljedahl, M. Kanevskiy, H. E. Epstein, B. M. Jones, M. T.
Jorgenson, and K. Kent, “Transferability of the deep learning mask R-CNN
model for automated mapping of ice-wedge polygons in high-resolution
satellite and UAV images,” <em id="bib.bib309.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 12, no. 7, p. 1085,
2020.

</span>
</li>
<li id="bib.bib310" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[310]</span>
<span class="ltx_bibblock">
C. Witharana, M. A. E. Bhuiyan, A. K. Liljedahl, M. Kanevskiy, M. T. Jorgenson,
B. M. Jones, R. Daanen, H. E. Epstein, C. G. Griffin, K. Kent, and M. K. W.
Jones, “An object-based approach for mapping tundra ice-wedge polygon
troughs from very high spatial resolution optical satellite imagery,”
<em id="bib.bib310.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 13, no. 4, p. 558, 2021.

</span>
</li>
<li id="bib.bib311" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[311]</span>
<span class="ltx_bibblock">
J. Yu, Z. Wang, A. Majumdar, and R. Rajagopal, “Deepsolar: A machine learning
framework to efficiently construct a solar deployment database in the united
states,” <em id="bib.bib311.1.1" class="ltx_emph ltx_font_italic">Joule</em>, vol. 2, no. 12, pp. 2605–2617, 2018.

</span>
</li>
<li id="bib.bib312" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[312]</span>
<span class="ltx_bibblock">
J. M. Malof, K. Bradbury, L. M. Collins, and R. G. Newell, “Automatic
detection of solar photovoltaic arrays in high resolution aerial imagery,”
<em id="bib.bib312.1.1" class="ltx_emph ltx_font_italic">Applied energy</em>, vol. 183, pp. 229–240, 2016.

</span>
</li>
<li id="bib.bib313" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[313]</span>
<span class="ltx_bibblock">
W. Zhang, G. Wang, J. Qi, G. Wang, and T. Zhang, “Research on the extraction
of wind turbine all over the china based on domestic satellite remote sensing
data,” in <em id="bib.bib313.1.1" class="ltx_emph ltx_font_italic">International Geoscience and Remote Sensing
Symposium</em>.   IEEE, 2021, pp.
4167–4170.

</span>
</li>
<li id="bib.bib314" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[314]</span>
<span class="ltx_bibblock">
W. Hu, T. Feldman, Y. J. Ou, N. Tarn, B. Ye, Y. Xu, J. M. Malof, and
K. Bradbury, “Wind turbine detection with synthetic overhead imagery,” in
<em id="bib.bib314.1.1" class="ltx_emph ltx_font_italic">International Geoscience and Remote Sensing Symposium</em>.   IEEE, 2021, pp. 4908–4911.

</span>
</li>
<li id="bib.bib315" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[315]</span>
<span class="ltx_bibblock">
T. Jia, Z. Kapelan, R. de Vries, P. Vriend, E. C. Peereboom, I. Okkerman, and
R. Taormina, “Deep learning for detecting macroplastic litter in water
bodies: A review,” <em id="bib.bib315.1.1" class="ltx_emph ltx_font_italic">Water Research</em>, vol. 231, p. 119632, 2023.

</span>
</li>
<li id="bib.bib316" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[316]</span>
<span class="ltx_bibblock">
C. Martin, Q. Zhang, D. Zhai, X. Zhang, and C. M. Duarte, “Enabling a
large-scale assessment of litter along saudi arabian red sea shores by
combining drones and machine learning,” <em id="bib.bib316.1.1" class="ltx_emph ltx_font_italic">Environmental Pollution</em>, vol.
277, p. 116730, 2021.

</span>
</li>
<li id="bib.bib317" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[317]</span>
<span class="ltx_bibblock">
K. Themistocleous, C. Papoutsa, S. C. Michaelides, and D. G. Hadjimitsis,
“Investigating detection of floating plastic litter from space using
sentinel-2 imagery,” <em id="bib.bib317.1.1" class="ltx_emph ltx_font_italic">Remote. Sens.</em>, vol. 12, no. 16, p. 2648, 2020.

</span>
</li>
<li id="bib.bib318" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[318]</span>
<span class="ltx_bibblock">
B. Xue, B. Huang, W. Wei, G. Chen, H. Li, N. Zhao, and H. Zhang, “An efficient
deep-sea debris detection method using deep neural networks,” <em id="bib.bib318.1.1" class="ltx_emph ltx_font_italic">IEEE
J. Sel. Top. Appl. Earth Obs. Remote. Sens.</em>, vol. 14, pp. 12 348–12 360,
2021.

</span>
</li>
<li id="bib.bib319" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[319]</span>
<span class="ltx_bibblock">
J. Peng, D. Wang, X. Liao, Q. Shao, Z. Sun, H. Yue, and H. Ye, “Wild animal
survey using uas imagery and deep learning: modified faster r-cnn for kiang
detection in tibetan plateau,” <em id="bib.bib319.1.1" class="ltx_emph ltx_font_italic">ISPRS J. Photogrammetry Remote Sens.</em>,
vol. 169, pp. 364–376, 2020.

</span>
</li>
<li id="bib.bib320" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[320]</span>
<span class="ltx_bibblock">
N. Rey, M. Volpi, S. Joost, and D. Tuia, “Detecting animals in african savanna
with uavs and the crowds,” <em id="bib.bib320.1.1" class="ltx_emph ltx_font_italic">Remote Sens. Environ.</em>, vol. 200, pp.
341–351, 2017.

</span>
</li>
<li id="bib.bib321" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[321]</span>
<span class="ltx_bibblock">
B. Kellenberger, D. Marcos, and D. Tuia, “Detecting mammals in uav images:
Best practices to address a substantially imbalanced dataset with deep
learning,” <em id="bib.bib321.1.1" class="ltx_emph ltx_font_italic">Remote Sens. Environ.</em>, vol. 216, pp. 139–153, 2018.

</span>
</li>
<li id="bib.bib322" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[322]</span>
<span class="ltx_bibblock">
A. Delplanque, S. Foucher, P. Lejeune, J. Linchant, and J. Théau,
“Multispecies detection and identification of african mammals in aerial
imagery using convolutional neural networks,” <em id="bib.bib322.1.1" class="ltx_emph ltx_font_italic">Remote Sensing in
Ecology and Conservation</em>, vol. 8, no. 2, pp. 166–179, 2022.

</span>
</li>
<li id="bib.bib323" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[323]</span>
<span class="ltx_bibblock">
D. Wang, Q. Shao, and H. Yue, “Surveying wild animals from satellites, manned
aircraft and unmanned aerial systems (uass): A review,” <em id="bib.bib323.1.1" class="ltx_emph ltx_font_italic">Remote Sen.</em>,
vol. 11, no. 11, p. 1308, 2019.

</span>
</li>
<li id="bib.bib324" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[324]</span>
<span class="ltx_bibblock">
T. Kattenborn, J. Leitloff, F. Schiefer, and S. Hinz, “Review on convolutional
neural networks (cnn) in vegetation remote sensing,” <em id="bib.bib324.1.1" class="ltx_emph ltx_font_italic">ISPRS J.
Photogrammetry Remote Sens.</em>, vol. 173, pp. 24–49, 2021.

</span>
</li>
<li id="bib.bib325" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[325]</span>
<span class="ltx_bibblock">
T. Dong, Y. Shen, J. Zhang, Y. Ye, and J. Fan, “Progressive cascaded
convolutional neural networks for single tree detection with google earth
imagery,” <em id="bib.bib325.1.1" class="ltx_emph ltx_font_italic">Remote. Sens.</em>, vol. 11, no. 15, p. 1786, 2019.

</span>
</li>
<li id="bib.bib326" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[326]</span>
<span class="ltx_bibblock">
A. Safonova, S. Tabik, D. Alcaraz-Segura, A. Rubtsov, Y. Maglinets, and
F. Herrera, “Detection of fir trees (<em id="bib.bib326.1.1" class="ltx_emph ltx_font_italic">Abies sibirica</em>) damaged by the
bark beetle in unmanned aerial vehicle images with deep learning,”
<em id="bib.bib326.2.2" class="ltx_emph ltx_font_italic">Remote. Sens.</em>, vol. 11, no. 6, p. 643, 2019.

</span>
</li>
<li id="bib.bib327" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[327]</span>
<span class="ltx_bibblock">
Z. Hao, L. Lin, C. J. Post, E. A. Mikhailova, M. Li, Y. Chen, K. Yu, and
J. Liu, “Automated tree-crown and height detection in a young forest
plantation using mask region-based convolutional neural network (mask
r-cnn),” <em id="bib.bib327.1.1" class="ltx_emph ltx_font_italic">ISPRS J. Photogrammetry Remote Sens.</em>, vol. 178, pp.
112–123, 2021.

</span>
</li>
<li id="bib.bib328" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[328]</span>
<span class="ltx_bibblock">
A. Sani-Mohammed, W. Yao, and M. Heurich, “Instance segmentation of standing
dead trees in dense forest from aerial imagery using deep learning,”
<em id="bib.bib328.1.1" class="ltx_emph ltx_font_italic">ISPRS O. J. Photogrammetry Remote Sens.</em>, vol. 6, p. 100024, 2022.

</span>
</li>
<li id="bib.bib329" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[329]</span>
<span class="ltx_bibblock">
A. V. Etten, “You only look twice: Rapid multi-scale object detection in
satellite imagery,” 2018. [Online]. Available:
http://arxiv.org/abs/1805.09512

</span>
</li>
<li id="bib.bib330" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[330]</span>
<span class="ltx_bibblock">
Q. Lin, J. Zhao, G. Fu, and Z. Yuan, “Crpn-sfnet: A high-performance object
detector on large-scale remote sensing images,” <em id="bib.bib330.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Neural
Networks Learn. Syst.</em>, vol. 33, no. 1, pp. 416–429, 2022.

</span>
</li>
<li id="bib.bib331" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[331]</span>
<span class="ltx_bibblock">
D. Hong, L. Gao, N. Yokoya, J. Yao, J. Chanussot, Q. Du, and B. Zhang, “More
diverse means better: Multimodal deep learning meets remote-sensing imagery
classification,” <em id="bib.bib331.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 59, no. 5,
pp. 4340–4354, 2021.

</span>
</li>
<li id="bib.bib332" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[332]</span>
<span class="ltx_bibblock">
D. Hong, N. Yokoya, G.-S. Xia, J. Chanussot, and X. X. Zhu, “X-modalnet: A
semi-supervised deep cross-modal network for classification of remote sensing
data,” <em id="bib.bib332.1.1" class="ltx_emph ltx_font_italic">ISPRS J. Photogrammetry Remote Sens.</em>, vol. 167, pp. 12–23,
2020.

</span>
</li>
<li id="bib.bib333" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[333]</span>
<span class="ltx_bibblock">
M. Segal-Rozenhaimer, A. Li, K. Das, and V. Chirayath, “Cloud detection
algorithm for multi-modal satellite imagery using convolutional
neural-networks (cnn),” <em id="bib.bib333.1.1" class="ltx_emph ltx_font_italic">Remote Sens. Environ.</em>, vol. 237, p. 111446,
2020.

</span>
</li>
<li id="bib.bib334" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[334]</span>
<span class="ltx_bibblock">
Y. Shendryk, Y. Rist, C. Ticehurst, and P. Thorburn, “Deep learning for
multi-modal classification of cloud, shadow and land cover scenes in
planetscope and sentinel-2 imagery,” <em id="bib.bib334.1.1" class="ltx_emph ltx_font_italic">ISPRS J. Photogrammetry Remote
Sens.</em>, vol. 157, pp. 124–136, 2019.

</span>
</li>
<li id="bib.bib335" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[335]</span>
<span class="ltx_bibblock">
Y. Shi, L. Du, and Y. Guo, “Unsupervised domain adaptation for SAR target
detection,” <em id="bib.bib335.1.1" class="ltx_emph ltx_font_italic">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</em>,
vol. 14, pp. 6372–6385, 2021.

</span>
</li>
<li id="bib.bib336" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[336]</span>
<span class="ltx_bibblock">
Y. Zhu, X. Sun, W. Diao, H. Li, and K. Fu, “Rfa-net: Reconstructed feature
alignment network for domain adaptation object detection in remote sensing
imagery,” <em id="bib.bib336.1.1" class="ltx_emph ltx_font_italic">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</em>, vol. 15,
pp. 5689–5703, 2022.

</span>
</li>
<li id="bib.bib337" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[337]</span>
<span class="ltx_bibblock">
T. Xu, X. Sun, W. Diao, L. Zhao, K. Fu, and H. Wang, “Fada: Feature aligned
domain adaptive object detection in remote sensing imagery,” <em id="bib.bib337.1.1" class="ltx_emph ltx_font_italic">IEEE
Trans. Geosci. Remote Sens.</em>, vol. 60, pp. 1–16, 2022.

</span>
</li>
<li id="bib.bib338" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[338]</span>
<span class="ltx_bibblock">
Y. Koga, H. Miyazaki, and R. Shibasaki, “A method for vehicle detection in
high-resolution satellite images that uses a region-based object detector and
unsupervised domain adaptation,” <em id="bib.bib338.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>, vol. 12, no. 3, p.
575, 2020.

</span>
</li>
<li id="bib.bib339" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[339]</span>
<span class="ltx_bibblock">
Y. Shi, L. Du, Y. Guo, and Y. Du, “Unsupervised domain adaptation based on
progressive transfer for ship detection: From optical to SAR images,”
<em id="bib.bib339.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 60, pp. 1–17, 2022.
[Online]. Available: https://doi.org/10.1109/TGRS.2022.3185298

</span>
</li>
<li id="bib.bib340" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[340]</span>
<span class="ltx_bibblock">
P. Zhang, H. Xu, T. Tian, P. Gao, L. Li, T. Zhao, N. Zhang, and J. Tian,
“Sefepnet: Scale expansion and feature enhancement pyramid network for SAR
aircraft detection with small sample dataset,” <em id="bib.bib340.1.1" class="ltx_emph ltx_font_italic">IEEE J. Sel. Top.
Appl. Earth Obs. Remote Sens.</em>, vol. 15, pp. 3365–3375, 2022.

</span>
</li>
<li id="bib.bib341" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[341]</span>
<span class="ltx_bibblock">
S. Dang, Z. Cao, Z. Cui, Y. Pi, and N. Liu, “Open set incremental learning for
automatic target recognition,” <em id="bib.bib341.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>,
vol. 57, no. 7, pp. 4445–4456, 2019.

</span>
</li>
<li id="bib.bib342" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[342]</span>
<span class="ltx_bibblock">
J. Chen, S. Wang, L. Chen, H. Cai, and Y. Qian, “Incremental detection of
remote sensing objects with feature pyramid and knowledge distillation,”
<em id="bib.bib342.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 60, pp. 1–13, 2022.
[Online]. Available: https://doi.org/10.1109/TGRS.2020.3042554

</span>
</li>
<li id="bib.bib343" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[343]</span>
<span class="ltx_bibblock">
X. Chen, J. Jiang, Z. Li, H. Qi, Q. Li, J. Liu, L. Zheng, M. Liu, and Y. Deng,
“An online continual object detector on VHR remote sensing images with
class imbalance,” <em id="bib.bib343.1.1" class="ltx_emph ltx_font_italic">Eng. Appl. Artif. Intell.</em>, vol. 117, no. Part, p.
105549, 2023.

</span>
</li>
<li id="bib.bib344" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[344]</span>
<span class="ltx_bibblock">
J. Li, X. Sun, W. Diao, P. Wang, Y. Feng, X. Lu, and G. Xu, “Class-incremental
learning network for small objects enhancing of semantic segmentation in
aerial imagery,” <em id="bib.bib344.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 60, pp.
1–20, 2022. [Online]. Available:
https://doi.org/10.1109/TGRS.2021.3124303

</span>
</li>
<li id="bib.bib345" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[345]</span>
<span class="ltx_bibblock">
W. Liu, X. Nie, B. Zhang, and X. Sun, “Incremental learning with open-set
recognition for remote sensing image scene classification,” <em id="bib.bib345.1.1" class="ltx_emph ltx_font_italic">IEEE
Trans. Geosci. Remote Sens.</em>, vol. 60, pp. 1–16, 2022. [Online]. Available:
https://doi.org/10.1109/TGRS.2022.3173995

</span>
</li>
<li id="bib.bib346" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[346]</span>
<span class="ltx_bibblock">
J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei, “Imagenet: A
large-scale hierarchical image database,” in <em id="bib.bib346.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Int. Conf.
Comput. Vis. Pattern Recognit. (CVPR)</em>, 2009, pp. 248–255.

</span>
</li>
<li id="bib.bib347" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[347]</span>
<span class="ltx_bibblock">
Y. Long, G. Xia, S. Li, W. Yang, M. Y. Yang, X. X. Zhu, L. Zhang, and D. Li,
“On creating benchmark dataset for aerial image interpretation: Reviews,
guidances, and million-aid,” <em id="bib.bib347.1.1" class="ltx_emph ltx_font_italic">IEEE J. Sel. Top. Appl. Earth Obs.
Remote Sens.</em>, vol. 14, pp. 4205–4230, 2021.

</span>
</li>
<li id="bib.bib348" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[348]</span>
<span class="ltx_bibblock">
G. A. Christie, N. Fendley, J. Wilson, and R. Mukherjee, “Functional map of
the world,” in <em id="bib.bib348.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit.
(CVPR)</em>, 2018, pp. 6172–6180.

</span>
</li>
<li id="bib.bib349" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[349]</span>
<span class="ltx_bibblock">
D. Wang, J. Zhang, B. Du, G.-S. Xia, and D. Tao, “An empirical study of remote
sensing pretraining,” <em id="bib.bib349.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, pp. 1–1,
2022. [Online]. Available: https://doi.org/10.1109/TGRS.2022.3176603

</span>
</li>
<li id="bib.bib350" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[350]</span>
<span class="ltx_bibblock">
W. Li, K. Chen, H. Chen, and Z. Shi, “Geographical knowledge-driven
representation learning for remote sensing images,” <em id="bib.bib350.1.1" class="ltx_emph ltx_font_italic">IEEE Trans.
Geosci. Remote Sens.</em>, vol. 60, pp. 1–16, 2022. [Online]. Available:
https://doi.org/10.1109/TGRS.2021.3115569

</span>
</li>
<li id="bib.bib351" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[351]</span>
<span class="ltx_bibblock">
X. Sun, P. Wang, W. Lu, Z. Zhu, X. Lu, Q. He, J. Li, X. Rong, Z. Yang,
H. Chang, Q. He, G. Yang, R. Wang, J. Lu, and K. Fu, “Ringmo: A remote
sensing foundation model with masked image modeling,” <em id="bib.bib351.1.1" class="ltx_emph ltx_font_italic">IEEE Trans.
Geosci. Remote Sens.</em>, pp. 1–1, 2022. [Online]. Available:
https://doi.org/10.1109/TGRS.2022.3194732

</span>
</li>
<li id="bib.bib352" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[352]</span>
<span class="ltx_bibblock">
A. Fuller, K. Millard, and J. R. Green, “Satvit: Pretraining transformers for
earth observation,” <em id="bib.bib352.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci. Remote Sens. Lett.</em>, vol. 19, pp.
1–5, 2022. [Online]. Available:
https://doi.org/10.1109/LGRS.2022.3201489

</span>
</li>
<li id="bib.bib353" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[353]</span>
<span class="ltx_bibblock">
D. Wang, Q. Zhang, Y. Xu, J. Zhang, B. Du, D. Tao, and L. Zhang, “Advancing
plain vision transformer towards remote sensing foundation model,”
<em id="bib.bib353.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, pp. 1–1, 2022. [Online].
Available: https://doi.org/10.1109/TGRS.2022.3222818

</span>
</li>
<li id="bib.bib354" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[354]</span>
<span class="ltx_bibblock">
T. Zhang and X. Zhang, “Shipdenet-20: An only 20 convolution layers and
&lt;1-mb lightweight SAR ship detector,” <em id="bib.bib354.1.1" class="ltx_emph ltx_font_italic">IEEE Geosci.
Remote Sens. Lett.</em>, vol. 18, no. 7, pp. 1234–1238, 2021.

</span>
</li>
<li id="bib.bib355" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[355]</span>
<span class="ltx_bibblock">
T. Zhang, X. Zhang, J. Shi, and S. Wei, “Depthwise separable convolution
neural network for high-speed SAR ship detection,” <em id="bib.bib355.1.1" class="ltx_emph ltx_font_italic">Remote Sens.</em>,
vol. 11, no. 21, p. 2483, 2019.

</span>
</li>
<li id="bib.bib356" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[356]</span>
<span class="ltx_bibblock">
Z. Wang, L. Du, and Y. Li, “Boosting lightweight cnns through network pruning
and knowledge distillation for SAR target recognition,” <em id="bib.bib356.1.1" class="ltx_emph ltx_font_italic">IEEE J.
Sel. Top. Appl. Earth Obs. Remote Sens.</em>, vol. 14, pp. 8386–8397, 2021.

</span>
</li>
<li id="bib.bib357" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[357]</span>
<span class="ltx_bibblock">
S. Chen, R. Zhan, W. Wang, and J. Zhang, “Learning slimming SAR ship object
detector through network pruning and knowledge distillation,” <em id="bib.bib357.1.1" class="ltx_emph ltx_font_italic">IEEE
J. Sel. Top. Appl. Earth Obs. Remote Sens.</em>, vol. 14, pp. 1267–1282, 2021.

</span>
</li>
<li id="bib.bib358" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[358]</span>
<span class="ltx_bibblock">
Y. Zhang, Z. Yan, X. Sun, W. Diao, K. Fu, and L. Wang, “Learning efficient and
accurate detectors with dynamic knowledge distillation in remote sensing
imagery,” <em id="bib.bib358.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 60, pp. 1–19,
2022. [Online]. Available: https://doi.org/10.1109/TGRS.2021.3130443

</span>
</li>
<li id="bib.bib359" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[359]</span>
<span class="ltx_bibblock">
Y. Yang, X. Sun, W. Diao, H. Li, Y. Wu, X. Li, and K. Fu, “Adaptive knowledge
distillation for lightweight remote sensing object detectors optimizing,”
<em id="bib.bib359.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Geosci. Remote Sens.</em>, vol. 60, pp. 1–15, 2022.
[Online]. Available: https://doi.org/10.1109/TGRS.2022.3175213

</span>
</li>
<li id="bib.bib360" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[360]</span>
<span class="ltx_bibblock">
C. Li, G. Cheng, G. Wang, P. Zhou, and J. Han, “Instance-aware distillation
for efficient object detection in remote sensing images,” <em id="bib.bib360.1.1" class="ltx_emph ltx_font_italic">IEEE
Trans. Geosci. Remote Sens.</em>, vol. 61, pp. 1–11, 2023. [Online]. Available:
https://doi.org/10.1109/TGRS.2023.3238801

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2309.06749" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2309.06751" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2309.06751">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2309.06751" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2309.06752" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 05:57:57 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
