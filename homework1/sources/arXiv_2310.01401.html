<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2310.01401] Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection</title><meta property="og:description" content="We present PARQ – a multi-view 3D object detector with transformer and pixel-aligned recurrent queries.
Unlike previous works that use learnable features or only encode 3D point positions as queries in the decoder, PAR…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2310.01401">

<!--Generated on Wed Feb 28 03:07:35 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Yiming Xie<sup id="id8.8.id1" class="ltx_sup"><span id="id8.8.id1.1" class="ltx_text ltx_font_italic">1</span></sup>
 Huaizu Jiang<sup id="id9.9.id2" class="ltx_sup"><span id="id9.9.id2.1" class="ltx_text ltx_font_italic">1</span></sup>
 Georgia Gkioxari<sup id="id10.10.id3" class="ltx_sup"><span id="id10.10.id3.1" class="ltx_text ltx_font_italic">∗,2</span></sup>
 Julian Straub<sup id="id11.11.id4" class="ltx_sup"><span id="id11.11.id4.1" class="ltx_text ltx_font_italic">∗,3</span></sup>

<br class="ltx_break"><sup id="id12.12.id5" class="ltx_sup">1</sup>Northeastern University  <sup id="id13.13.id6" class="ltx_sup">2</sup>California Institute of Technology  <sup id="id14.14.id7" class="ltx_sup">3</sup>Meta Reality Labs Research
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id15.id1" class="ltx_p">We present PARQ – a multi-view 3D object detector with transformer and pixel-aligned recurrent queries.
Unlike previous works that use learnable features or only encode 3D point positions as queries in the decoder, PARQ leverages appearance-enhanced queries initialized from reference points in 3D space and updates their 3D location with recurrent cross-attention operations.
Incorporating pixel-aligned features and cross attention enables the model to encode the necessary 3D-to-2D correspondences and capture global contextual information of the input images.
PARQ outperforms prior best methods on the ScanNet and ARKitScenes datasets, learns and detects faster, is more robust to distribution shifts in reference points, can leverage additional input views without retraining, and can adapt inference compute by changing the number of recurrent iterations.
Code is available at <a target="_blank" href="https://ymingxie.github.io/parq" title="" class="ltx_ref ltx_href">https://ymingxie.github.io/parq</a>.</p>
</div>
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><math id="footnote1.m1.1" class="ltx_Math" alttext="*" display="inline"><semantics id="footnote1.m1.1b"><mo id="footnote1.m1.1.1" xref="footnote1.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="footnote1.m1.1c"><times id="footnote1.m1.1.1.cmml" xref="footnote1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m1.1d">*</annotation></semantics></math> Equal advising.</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The world is composed of objects positioned in 3D space.
Humans have an innate ability to perceive 3D scenes which allows them to interact with their surroundings. For machines, understanding all objects in 3D space from one or few images enables new applications of embodied intelligence such as in robotics and assistive technology.
The problem is defined by the task of 3D object detection: given a few images of a scene, detect all objects in 3D.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">3D object detection unites two distinct problems of computer vision, 2D recognition, and 3D reconstruction.
Similar to 2D recognition, appearance cues in the input views drive categorical predictions.
Similar to 3D reconstruction, the model needs to reason about the 3D position of objects from only 2D views.
Modern learning-based methods build on the traditional multi-view stereopsis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> and Structure from Motion (SfM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> and lift objects to 3D via optimization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> or volumetric representations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>.
To do so, they require tens or hundreds of views observing the whole scene.
However, in many real-world applications, like robotics, models are required to make predictions online and often in real-time from just a short video snippet.
In this work, we tackle online 3D object detection from just a few (<em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, 3) consecutive views with known camera poses extracted by
visual-inertial
SLAM systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2310.01401/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="255" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
PARQ leverages appearance-enhanced queries initialized from 3D points and updates their 3D locations to the 3D object center with a recurrent PARQ layer in the decoder.
The PARQ layer chains a transformer decoder layer and a detection head.
</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Learning 3D object detectors is challenging as the 3D prediction space is extensive while objects occupy only a small portion.
Geometry is important.
An object visible in the different input views occupies the same 3D world location and vice versa, a 3D object connects to 2D locations on the input images consistent with its camera projections.
This observation prunes the vast prediction space.
In addition, object appearance changes with viewpoint changes.
For instance, the appearance of the chair in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> changes as the camera moves.
This suggests that appearance and geometry are critical.
Motivated by this insight, we design a model that captures geometric and appearance interactions between the 2D input views and the 3D prediction space.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We build on the powerful transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, and specifically DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, a popular 2D object detection system.
DETR encodes input images into feature maps and predicts 2D bounding boxes via cross-attention with learnable queries.
We enhance DETR in three ways.
First, we make the input feature maps 3D-aware by adding 3D positional information via ray embeddings, following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.
Second, we replace the learnable, randomly initialized queries in DETR with appearance- and geometry-informed queries.
Our queries encode the 3D location of 3D reference points that cover the 3D space.
They are enhanced with pixel-aligned appearance features sampled from the input views at the projected 2D locations.
Cross-attention between our 3D-aware inputs and appearance-informed 3D queries unleashes our model’s ability to capture 3D-to-2D correspondences quickly and efficiently, as we show in our experiments.
Lastly, we deviate from DETR by introducing recurrence.
DETR makes predictions on the 2D plane while our predictions live in the vast 3D space.
Our initial 3D points are likely to be far from the objects.
So our model starts by making a coarse prediction that roughly places the initial queries close to true objects and then gradually refines them.
We model this by designing a recurrent scheme that encodes the 3D predictions from the previous step into the current queries.
An overview of our proposed pixel-aligned recurrent queries, dubbed PARQ, is shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">PARQ differs from prior DETR-style methods for 3D object detection in design and attributes.
Our recurrent decoding and query design differentiates us from the state-of-the-art DETR3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> and PETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.
DETR3D uses learnable queries and samples local appearance cues from the projected 2D image locations to update the queries.
This limits the model’s ability to capture long-range 3D-to-2D interactions.
PETR enhances the input views with 3D positional information, similar to ours, but only encodes the 3D location of the queries at the start of decoding and without recurrent updates.
This makes it difficult for the model to capture long-range correspondences driven by appearance and forces the model to focus on local cues around the 3D queries, as we show in our experiments.
A schematic comparison is shown in Fig. <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
We show that our PARQ outperforms DETR3D and PETR on the challenging ScanNet and ARKitScenes datasets.
More importantly, we show that PARQ exhibits speedier convergence leading to faster training, is robust to a varying number of queries, and can leverage additional input views at test time without the need to retrain.
In contrast, we show that PETR and DETR3D fail to generalize when deviating from the choice of input views and the number of queries during training.
By tuning the number of queries and recurrent iterations, we show that PARQ detects fastest and most accurately.
Finally, with PARQ we demonstrate zero-shot generalization to novel scenes.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2310.01401/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="333" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
<span id="S1.F2.2.1" class="ltx_text ltx_font_bold">Query Design.</span> Schematic comparison of query designs in DETR3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, PETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, and our proposed PARQ.
</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">There is a long line of work on 3D object detection from sequences of frames and known camera poses.
Late fusion techniques detect objects per frame and then fuse these detections, while early fusion methods fuse multi-view features and then detect objects from the fused representation.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><em id="S2.p2.1.1" class="ltx_emph ltx_font_italic">Late Fusion.</em>
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> 3D objects are first detected from single images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and then are associated from tens of views via a post-processing optimization step to obtain scene-level predictions.
Dynamic scenes make associations hard as objects move <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
Here, velocity estimates can be used to predict associations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.
In these methods, the quality of the final prediction depends on single-view detection which suffers from scale-depth ambiguity.
To overcome this, another line of work regresses 2D bounding boxes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, and, after association, optimizes 3D quantities to match them.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> optimizes 3d bounding boxes and shapes to project to associated 2d bounding boxes.
Similarly, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> uses associated 2d detections to instantiate a CAD-model-based reconstruction of a scene via a multi-view constraint optimization formulation.
Common to all late fusion methods is that it is difficult to recover from the errors at the single-view detection and association stage.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><em id="S2.p3.1.1" class="ltx_emph ltx_font_italic">Early Fusion.</em>
In early fusion, methods represent geometry either explicitly, <em id="S2.p3.1.2" class="ltx_emph ltx_font_italic">e.g.</em>, with point clouds, a voxel or BEV grid or, implicitly via input-level inductive biases and positional encodings like ray-encodings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> adopt a volumetric representation to fuse multi-view image features and predict the 6D pose and the scale of the objects from the feature volume.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> incorporate NeRF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> to improve 3D detection but make predictions offline as they assume tens or hundreds of input views.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> transform multi-view 2D features into a bird-eye-view (BEV) representation.
However, volumetric representations require a significant amount of memory which scales with the size of the scene.
BEV representations, commonly used in urban domains, can mitigate this to some extent by operating in 2D top-down view but cannot represent more complex 3D environments.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> exploit the temporal information across the video to enhance 3D object detection.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2310.01401/assets/x3.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="126" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
<span id="S2.F3.2.1" class="ltx_text ltx_font_bold">Overview of PARQ.</span> Our model predicts 3D object bounding boxes from a short video snippet. We first embed input views with a CNN and add 3D ray-positional encodings. Recurrent PARQ layers consisting of a single Transformer decoder layer and a detection head decode pixel-aligned queries to 3D object predictions. Our queries encode the location of 3D reference points and their appearance cues sampled from the input views.
Each iteration updates the 3D query points with offsets predicted by the detection head.
</figcaption>
</figure>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Another line of work explores input-level biases instead of an explicit scene representation.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> build on DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and transformer architectures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> where a query represents an object and interacts with the corresponding 2D views to output 3D predictions.
DETR3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> predicts 3D objects from learnable randomly-initialized queries which are updated using local appearance cues sampled from the input views.
But when the prediction is far from the true object, relying on local appearance alone is not optimal.
PETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> encodes 3D reference points into queries and performs cross-attention with 3D-aware input features.
PETR omits appearance in the queries making convergence slow as the model cannot capture long-range appearance interactions with the 2D input.
We also adopt a query-based transformer architecture but unlike PETR, we enhance queries with appearance-aligned features.
As a result, our model captures local and global geometric and appearance interactions with the input views.
In addition, through recurrent decoding, the model updates its prediction starting with coarse and then refining its output.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">3D object detection is primarily studied in urban and indoor scenes.
On urban domains, input is captured from camera rigs in autonomous vehicles <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> and methods mainly focus on the <em id="S2.p5.1.1" class="ltx_emph ltx_font_italic">car</em> category.
In this work, we focus on indoor scenes and inputs from monocular videos.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this work, we tackle 3D object detection from a short video snippet, <em id="S3.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em> 3 views.
We propose an encoder-decoder architecture that translates geometry- and appearance-informed queries to 3D object detections via recurrent cross-attention with the input views.
Fig. <a href="#S2.F3" title="Figure 3 ‣ 2 Related Work ‣ Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows our architecture.
Given <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">N</annotation></semantics></math> images from a monocular video with known intrinsics and extrinsics, we first extract the image features using a CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.
Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, ray position encodings are generated and added to the 2D image features, producing 3D-aware feature maps (Sec. <a href="#S3.SS1" title="3.1 3D-Aware Input Encoding ‣ 3 Method ‣ Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>).
The 3D-aware feature maps interact with point queries via recurrent transformer layers.
Our point queries, anchored on 3D reference points, carry information about their 3D location and appearance as seen from the input views (Sec. <a href="#S3.SS2" title="3.2 Appearance- and Geometry-Informed Queries ‣ 3 Method ‣ Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>).
Via recurrent attention operations with the input views the queries produce the final 3D object detections (Sec. <a href="#S3.SS3" title="3.3 Recurrent Query-Based Decoding ‣ 3 Method ‣ Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>3D-Aware Input Encoding</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.5" class="ltx_p">Our model inputs <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">N</annotation></semantics></math> RGB images <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="I_{i}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">I</mi><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">𝐼</ci><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">I_{i}</annotation></semantics></math> with camera parameters <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="\pi_{i}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><msub id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">π</mi><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">𝜋</ci><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\pi_{i}</annotation></semantics></math>, <math id="S3.SS1.p1.4.m4.2" class="ltx_Math" alttext="i\in\left[1,N\right]" display="inline"><semantics id="S3.SS1.p1.4.m4.2a"><mrow id="S3.SS1.p1.4.m4.2.3" xref="S3.SS1.p1.4.m4.2.3.cmml"><mi id="S3.SS1.p1.4.m4.2.3.2" xref="S3.SS1.p1.4.m4.2.3.2.cmml">i</mi><mo id="S3.SS1.p1.4.m4.2.3.1" xref="S3.SS1.p1.4.m4.2.3.1.cmml">∈</mo><mrow id="S3.SS1.p1.4.m4.2.3.3.2" xref="S3.SS1.p1.4.m4.2.3.3.1.cmml"><mo id="S3.SS1.p1.4.m4.2.3.3.2.1" xref="S3.SS1.p1.4.m4.2.3.3.1.cmml">[</mo><mn id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">1</mn><mo id="S3.SS1.p1.4.m4.2.3.3.2.2" xref="S3.SS1.p1.4.m4.2.3.3.1.cmml">,</mo><mi id="S3.SS1.p1.4.m4.2.2" xref="S3.SS1.p1.4.m4.2.2.cmml">N</mi><mo id="S3.SS1.p1.4.m4.2.3.3.2.3" xref="S3.SS1.p1.4.m4.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.2b"><apply id="S3.SS1.p1.4.m4.2.3.cmml" xref="S3.SS1.p1.4.m4.2.3"><in id="S3.SS1.p1.4.m4.2.3.1.cmml" xref="S3.SS1.p1.4.m4.2.3.1"></in><ci id="S3.SS1.p1.4.m4.2.3.2.cmml" xref="S3.SS1.p1.4.m4.2.3.2">𝑖</ci><interval closure="closed" id="S3.SS1.p1.4.m4.2.3.3.1.cmml" xref="S3.SS1.p1.4.m4.2.3.3.2"><cn type="integer" id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">1</cn><ci id="S3.SS1.p1.4.m4.2.2.cmml" xref="S3.SS1.p1.4.m4.2.2">𝑁</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.2c">i\in\left[1,N\right]</annotation></semantics></math>.
Each image is fed to a ResNet-FPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> which outputs a feature map <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="F_{i}\in\mathbb{R}^{H\times W\times C}" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><mrow id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><msub id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2.2" xref="S3.SS1.p1.5.m5.1.1.2.2.cmml">F</mi><mi id="S3.SS1.p1.5.m5.1.1.2.3" xref="S3.SS1.p1.5.m5.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS1.p1.5.m5.1.1.1" xref="S3.SS1.p1.5.m5.1.1.1.cmml">∈</mo><msup id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml"><mi id="S3.SS1.p1.5.m5.1.1.3.2" xref="S3.SS1.p1.5.m5.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p1.5.m5.1.1.3.3" xref="S3.SS1.p1.5.m5.1.1.3.3.cmml"><mi id="S3.SS1.p1.5.m5.1.1.3.3.2" xref="S3.SS1.p1.5.m5.1.1.3.3.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.5.m5.1.1.3.3.1" xref="S3.SS1.p1.5.m5.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.p1.5.m5.1.1.3.3.3" xref="S3.SS1.p1.5.m5.1.1.3.3.3.cmml">W</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.5.m5.1.1.3.3.1a" xref="S3.SS1.p1.5.m5.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.p1.5.m5.1.1.3.3.4" xref="S3.SS1.p1.5.m5.1.1.3.3.4.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><in id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1"></in><apply id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.2.1.cmml" xref="S3.SS1.p1.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.2.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2.2">𝐹</ci><ci id="S3.SS1.p1.5.m5.1.1.2.3.cmml" xref="S3.SS1.p1.5.m5.1.1.2.3">𝑖</ci></apply><apply id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.3.1.cmml" xref="S3.SS1.p1.5.m5.1.1.3">superscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.3.2.cmml" xref="S3.SS1.p1.5.m5.1.1.3.2">ℝ</ci><apply id="S3.SS1.p1.5.m5.1.1.3.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3.3"><times id="S3.SS1.p1.5.m5.1.1.3.3.1.cmml" xref="S3.SS1.p1.5.m5.1.1.3.3.1"></times><ci id="S3.SS1.p1.5.m5.1.1.3.3.2.cmml" xref="S3.SS1.p1.5.m5.1.1.3.3.2">𝐻</ci><ci id="S3.SS1.p1.5.m5.1.1.3.3.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3.3.3">𝑊</ci><ci id="S3.SS1.p1.5.m5.1.1.3.3.4.cmml" xref="S3.SS1.p1.5.m5.1.1.3.3.4">𝐶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">F_{i}\in\mathbb{R}^{H\times W\times C}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.2" class="ltx_p">We enhance the image features with 3D ray embeddings, following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.
For each image, we shoot rays originating at the camera center intersecting the image at each pixel. We sample <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">D</annotation></semantics></math> points along each ray, <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="P^{ray}_{i}\in\mathbb{R}^{H\times W\times(D\times 3)}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mrow id="S3.SS1.p2.2.m2.1.2" xref="S3.SS1.p2.2.m2.1.2.cmml"><msubsup id="S3.SS1.p2.2.m2.1.2.2" xref="S3.SS1.p2.2.m2.1.2.2.cmml"><mi id="S3.SS1.p2.2.m2.1.2.2.2.2" xref="S3.SS1.p2.2.m2.1.2.2.2.2.cmml">P</mi><mi id="S3.SS1.p2.2.m2.1.2.2.3" xref="S3.SS1.p2.2.m2.1.2.2.3.cmml">i</mi><mrow id="S3.SS1.p2.2.m2.1.2.2.2.3" xref="S3.SS1.p2.2.m2.1.2.2.2.3.cmml"><mi id="S3.SS1.p2.2.m2.1.2.2.2.3.2" xref="S3.SS1.p2.2.m2.1.2.2.2.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.2.m2.1.2.2.2.3.1" xref="S3.SS1.p2.2.m2.1.2.2.2.3.1.cmml">​</mo><mi id="S3.SS1.p2.2.m2.1.2.2.2.3.3" xref="S3.SS1.p2.2.m2.1.2.2.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.2.m2.1.2.2.2.3.1a" xref="S3.SS1.p2.2.m2.1.2.2.2.3.1.cmml">​</mo><mi id="S3.SS1.p2.2.m2.1.2.2.2.3.4" xref="S3.SS1.p2.2.m2.1.2.2.2.3.4.cmml">y</mi></mrow></msubsup><mo id="S3.SS1.p2.2.m2.1.2.1" xref="S3.SS1.p2.2.m2.1.2.1.cmml">∈</mo><msup id="S3.SS1.p2.2.m2.1.2.3" xref="S3.SS1.p2.2.m2.1.2.3.cmml"><mi id="S3.SS1.p2.2.m2.1.2.3.2" xref="S3.SS1.p2.2.m2.1.2.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.2.m2.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.1.3" xref="S3.SS1.p2.2.m2.1.1.1.3.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.2.m2.1.1.1.2" xref="S3.SS1.p2.2.m2.1.1.1.2.cmml">×</mo><mi id="S3.SS1.p2.2.m2.1.1.1.4" xref="S3.SS1.p2.2.m2.1.1.1.4.cmml">W</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.2.m2.1.1.1.2a" xref="S3.SS1.p2.2.m2.1.1.1.2.cmml">×</mo><mrow id="S3.SS1.p2.2.m2.1.1.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p2.2.m2.1.1.1.1.1.2" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p2.2.m2.1.1.1.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.1.1.1.1.2" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.2.cmml">D</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.cmml">×</mo><mn id="S3.SS1.p2.2.m2.1.1.1.1.1.1.3" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.3.cmml">3</mn></mrow><mo stretchy="false" id="S3.SS1.p2.2.m2.1.1.1.1.1.3" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.2.cmml" xref="S3.SS1.p2.2.m2.1.2"><in id="S3.SS1.p2.2.m2.1.2.1.cmml" xref="S3.SS1.p2.2.m2.1.2.1"></in><apply id="S3.SS1.p2.2.m2.1.2.2.cmml" xref="S3.SS1.p2.2.m2.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.2.2.1.cmml" xref="S3.SS1.p2.2.m2.1.2.2">subscript</csymbol><apply id="S3.SS1.p2.2.m2.1.2.2.2.cmml" xref="S3.SS1.p2.2.m2.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.2.2.2.1.cmml" xref="S3.SS1.p2.2.m2.1.2.2">superscript</csymbol><ci id="S3.SS1.p2.2.m2.1.2.2.2.2.cmml" xref="S3.SS1.p2.2.m2.1.2.2.2.2">𝑃</ci><apply id="S3.SS1.p2.2.m2.1.2.2.2.3.cmml" xref="S3.SS1.p2.2.m2.1.2.2.2.3"><times id="S3.SS1.p2.2.m2.1.2.2.2.3.1.cmml" xref="S3.SS1.p2.2.m2.1.2.2.2.3.1"></times><ci id="S3.SS1.p2.2.m2.1.2.2.2.3.2.cmml" xref="S3.SS1.p2.2.m2.1.2.2.2.3.2">𝑟</ci><ci id="S3.SS1.p2.2.m2.1.2.2.2.3.3.cmml" xref="S3.SS1.p2.2.m2.1.2.2.2.3.3">𝑎</ci><ci id="S3.SS1.p2.2.m2.1.2.2.2.3.4.cmml" xref="S3.SS1.p2.2.m2.1.2.2.2.3.4">𝑦</ci></apply></apply><ci id="S3.SS1.p2.2.m2.1.2.2.3.cmml" xref="S3.SS1.p2.2.m2.1.2.2.3">𝑖</ci></apply><apply id="S3.SS1.p2.2.m2.1.2.3.cmml" xref="S3.SS1.p2.2.m2.1.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.2.3.1.cmml" xref="S3.SS1.p2.2.m2.1.2.3">superscript</csymbol><ci id="S3.SS1.p2.2.m2.1.2.3.2.cmml" xref="S3.SS1.p2.2.m2.1.2.3.2">ℝ</ci><apply id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1"><times id="S3.SS1.p2.2.m2.1.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.1.2"></times><ci id="S3.SS1.p2.2.m2.1.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.1.3">𝐻</ci><ci id="S3.SS1.p2.2.m2.1.1.1.4.cmml" xref="S3.SS1.p2.2.m2.1.1.1.4">𝑊</ci><apply id="S3.SS1.p2.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1"><times id="S3.SS1.p2.2.m2.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.1"></times><ci id="S3.SS1.p2.2.m2.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.p2.2.m2.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.3">3</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">P^{ray}_{i}\in\mathbb{R}^{H\times W\times(D\times 3)}</annotation></semantics></math>, with log-scale sampling. More details in the supplementary.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.5" class="ltx_p">The ray points are transformed to position encodings, <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="P_{i}\in\mathbb{R}^{H\times W\times C}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mrow id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><msub id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml"><mi id="S3.SS1.p3.1.m1.1.1.2.2" xref="S3.SS1.p3.1.m1.1.1.2.2.cmml">P</mi><mi id="S3.SS1.p3.1.m1.1.1.2.3" xref="S3.SS1.p3.1.m1.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS1.p3.1.m1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml"><mi id="S3.SS1.p3.1.m1.1.1.3.2" xref="S3.SS1.p3.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p3.1.m1.1.1.3.3" xref="S3.SS1.p3.1.m1.1.1.3.3.cmml"><mi id="S3.SS1.p3.1.m1.1.1.3.3.2" xref="S3.SS1.p3.1.m1.1.1.3.3.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.1.m1.1.1.3.3.1" xref="S3.SS1.p3.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.p3.1.m1.1.1.3.3.3" xref="S3.SS1.p3.1.m1.1.1.3.3.3.cmml">W</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.1.m1.1.1.3.3.1a" xref="S3.SS1.p3.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.p3.1.m1.1.1.3.3.4" xref="S3.SS1.p3.1.m1.1.1.3.3.4.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><in id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1"></in><apply id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.2.1.cmml" xref="S3.SS1.p3.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.2.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2.2">𝑃</ci><ci id="S3.SS1.p3.1.m1.1.1.2.3.cmml" xref="S3.SS1.p3.1.m1.1.1.2.3">𝑖</ci></apply><apply id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.3.1.cmml" xref="S3.SS1.p3.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.3.2.cmml" xref="S3.SS1.p3.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS1.p3.1.m1.1.1.3.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3.3"><times id="S3.SS1.p3.1.m1.1.1.3.3.1.cmml" xref="S3.SS1.p3.1.m1.1.1.3.3.1"></times><ci id="S3.SS1.p3.1.m1.1.1.3.3.2.cmml" xref="S3.SS1.p3.1.m1.1.1.3.3.2">𝐻</ci><ci id="S3.SS1.p3.1.m1.1.1.3.3.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3.3.3">𝑊</ci><ci id="S3.SS1.p3.1.m1.1.1.3.3.4.cmml" xref="S3.SS1.p3.1.m1.1.1.3.3.4">𝐶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">P_{i}\in\mathbb{R}^{H\times W\times C}</annotation></semantics></math>, via an MLP of the same hidden dimension <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mi id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><ci id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">C</annotation></semantics></math> as the input feature maps.
The ray position encoding, <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="P_{i}" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><msub id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml"><mi id="S3.SS1.p3.3.m3.1.1.2" xref="S3.SS1.p3.3.m3.1.1.2.cmml">P</mi><mi id="S3.SS1.p3.3.m3.1.1.3" xref="S3.SS1.p3.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><apply id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p3.3.m3.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2">𝑃</ci><ci id="S3.SS1.p3.3.m3.1.1.3.cmml" xref="S3.SS1.p3.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">P_{i}</annotation></semantics></math>, is added to the image feature map, <math id="S3.SS1.p3.4.m4.1" class="ltx_Math" alttext="F_{i}" display="inline"><semantics id="S3.SS1.p3.4.m4.1a"><msub id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml"><mi id="S3.SS1.p3.4.m4.1.1.2" xref="S3.SS1.p3.4.m4.1.1.2.cmml">F</mi><mi id="S3.SS1.p3.4.m4.1.1.3" xref="S3.SS1.p3.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><apply id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p3.4.m4.1.1.2.cmml" xref="S3.SS1.p3.4.m4.1.1.2">𝐹</ci><ci id="S3.SS1.p3.4.m4.1.1.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">F_{i}</annotation></semantics></math>, to produce 3D ray-position-aware features, <math id="S3.SS1.p3.5.m5.1" class="ltx_Math" alttext="F_{i}^{p}=F_{i}+P_{i}" display="inline"><semantics id="S3.SS1.p3.5.m5.1a"><mrow id="S3.SS1.p3.5.m5.1.1" xref="S3.SS1.p3.5.m5.1.1.cmml"><msubsup id="S3.SS1.p3.5.m5.1.1.2" xref="S3.SS1.p3.5.m5.1.1.2.cmml"><mi id="S3.SS1.p3.5.m5.1.1.2.2.2" xref="S3.SS1.p3.5.m5.1.1.2.2.2.cmml">F</mi><mi id="S3.SS1.p3.5.m5.1.1.2.2.3" xref="S3.SS1.p3.5.m5.1.1.2.2.3.cmml">i</mi><mi id="S3.SS1.p3.5.m5.1.1.2.3" xref="S3.SS1.p3.5.m5.1.1.2.3.cmml">p</mi></msubsup><mo id="S3.SS1.p3.5.m5.1.1.1" xref="S3.SS1.p3.5.m5.1.1.1.cmml">=</mo><mrow id="S3.SS1.p3.5.m5.1.1.3" xref="S3.SS1.p3.5.m5.1.1.3.cmml"><msub id="S3.SS1.p3.5.m5.1.1.3.2" xref="S3.SS1.p3.5.m5.1.1.3.2.cmml"><mi id="S3.SS1.p3.5.m5.1.1.3.2.2" xref="S3.SS1.p3.5.m5.1.1.3.2.2.cmml">F</mi><mi id="S3.SS1.p3.5.m5.1.1.3.2.3" xref="S3.SS1.p3.5.m5.1.1.3.2.3.cmml">i</mi></msub><mo id="S3.SS1.p3.5.m5.1.1.3.1" xref="S3.SS1.p3.5.m5.1.1.3.1.cmml">+</mo><msub id="S3.SS1.p3.5.m5.1.1.3.3" xref="S3.SS1.p3.5.m5.1.1.3.3.cmml"><mi id="S3.SS1.p3.5.m5.1.1.3.3.2" xref="S3.SS1.p3.5.m5.1.1.3.3.2.cmml">P</mi><mi id="S3.SS1.p3.5.m5.1.1.3.3.3" xref="S3.SS1.p3.5.m5.1.1.3.3.3.cmml">i</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b"><apply id="S3.SS1.p3.5.m5.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1"><eq id="S3.SS1.p3.5.m5.1.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1.1"></eq><apply id="S3.SS1.p3.5.m5.1.1.2.cmml" xref="S3.SS1.p3.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m5.1.1.2.1.cmml" xref="S3.SS1.p3.5.m5.1.1.2">superscript</csymbol><apply id="S3.SS1.p3.5.m5.1.1.2.2.cmml" xref="S3.SS1.p3.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m5.1.1.2.2.1.cmml" xref="S3.SS1.p3.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS1.p3.5.m5.1.1.2.2.2.cmml" xref="S3.SS1.p3.5.m5.1.1.2.2.2">𝐹</ci><ci id="S3.SS1.p3.5.m5.1.1.2.2.3.cmml" xref="S3.SS1.p3.5.m5.1.1.2.2.3">𝑖</ci></apply><ci id="S3.SS1.p3.5.m5.1.1.2.3.cmml" xref="S3.SS1.p3.5.m5.1.1.2.3">𝑝</ci></apply><apply id="S3.SS1.p3.5.m5.1.1.3.cmml" xref="S3.SS1.p3.5.m5.1.1.3"><plus id="S3.SS1.p3.5.m5.1.1.3.1.cmml" xref="S3.SS1.p3.5.m5.1.1.3.1"></plus><apply id="S3.SS1.p3.5.m5.1.1.3.2.cmml" xref="S3.SS1.p3.5.m5.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m5.1.1.3.2.1.cmml" xref="S3.SS1.p3.5.m5.1.1.3.2">subscript</csymbol><ci id="S3.SS1.p3.5.m5.1.1.3.2.2.cmml" xref="S3.SS1.p3.5.m5.1.1.3.2.2">𝐹</ci><ci id="S3.SS1.p3.5.m5.1.1.3.2.3.cmml" xref="S3.SS1.p3.5.m5.1.1.3.2.3">𝑖</ci></apply><apply id="S3.SS1.p3.5.m5.1.1.3.3.cmml" xref="S3.SS1.p3.5.m5.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m5.1.1.3.3.1.cmml" xref="S3.SS1.p3.5.m5.1.1.3.3">subscript</csymbol><ci id="S3.SS1.p3.5.m5.1.1.3.3.2.cmml" xref="S3.SS1.p3.5.m5.1.1.3.3.2">𝑃</ci><ci id="S3.SS1.p3.5.m5.1.1.3.3.3.cmml" xref="S3.SS1.p3.5.m5.1.1.3.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">F_{i}^{p}=F_{i}+P_{i}</annotation></semantics></math>.
The 3D-aware feature maps are input to the transformer decoder.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_float"><span id="alg1.5.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> <span id="alg1.6.2" class="ltx_text ltx_font_bold">Training Code</span>
</figcaption>
<div id="alg1.7" class="ltx_listing ltx_lst_language_python ltx_lstlisting ltx_listing" style="background-color:#FFFFFF;">
<div class="ltx_listing_data"><a href="data:text/plain;base64,ZGVmIHRyYWluX2xvc3MoaW1hZ2VzLCBwcm9qX21hdCwgcmVmX3BvaW50cywgZ3QsIEwpOgogICIiIgogIGltYWdlczogW0IsIE4sIEgsIFcsIDNdCiAgcHJval9tYXQ6IHByb2plY3Rpb24gbWF0cml4IFtCLCBOLCA0LCA0XQogIHJlZl9wb2ludHM6IHJlZmVyZW5jZSBwb2ludHMgW0ssIDNdCiAgZ3Q6IGdyb3VuZCB0cnV0aCAzRCBib3VuZGluZyBib3ggW0IsIH4sIDgsIDNdCiAgIyBCOiBiYXRjaAogICMgTjogbnVtYmVyIG9mIHZpZXdzCiAgIyBLOiBudW1iZXIgb2YgcXVlcmllcwogICMgTDogbnVtYmVyIG9mIGl0ZXJhdGlvbnMKICAiIiIKCiAgIyBFbmNvZGUgaW1hZ2UgZmVhdHVyZXMKICBmZWF0cyA9IGltYWdlX2VuY29kZXIoaW1hZ2VzKQoKICAjIEdlbmVyYXRlIHJheSBwb3NpdGlvbmFsIGVuY29kaW5ncwogIHJheV9lbmNvZGluZyA9IGdlbmVyYXRlX3JheV9lbmNvZGluZyhwcm9qX21hdCkKCiAgIyAzRC1hd2FyZSBpbWFnZSBmZWF0dXJlcwogIGZlYXRzID0gZmVhdHMgKyByYXlfZW5jb2RpbmcKCiAgIyBzdXBlcnZpc2UgdGhlIHByZWRpY3Rpb24gaW4gZWFjaCBpdGVyYXRpb24KICBsb3NzX2xpc3QgPSBbXQogIGZvciBsIGluIHJhbmdlKEwpOgogICAgIyBwaXhlbC1hbGlnbmVkIGZlYXR1cmVzCiAgICBwYV9mZWF0ID0gc2FtcGxlX2FuZF9wb29sX2ZlYXRzKGZlYXRzLCByZWZfcG9pbnRzLCBwcm9qX21hdCkKCiAgICAjIHBvaW50IHBvc2l0aW9uYWwgZW5jb2RpbmcKICAgIHBvaW50X3BvcyA9IHBvc2l0aW9uYWxfZW5jb2RpbmcocmVmX3BvaW50cykKCiAgICAjIHRyYW5zZm9ybWVyIGRlY29kZXIgbGF5ZXIKICAgIG91dHB1dCA9IGxheWVyKHRndD1wYV9mZWF0LCBtZW1vcnk9ZmVhdHMsIHF1ZXJ5X3Bvcz1wb2ludF9wb3MpCgogICAgIyBkZXRlY3Rpb24gaGVhZAogICAgYm94X3BhcmFtID0gZGV0X2hlYWQob3V0cHV0KQoKICAgICMgZ2V0IHRoZSBvYmplY3QgY2VudGVycwogICAgYm94X3BhcmFtWydjZW50ZXInXSArPSByZWZfcG9pbnRzCgogICAgIyBuZXcgcmVmZXJlbmNlIHBvaW50cyBmb3IgdGhlIG5leHQgaXRlcmF0aW9uCiAgICByZWZfcG9pbnRzID0gYm94X3BhcmFtWydjZW50ZXInXS5kZXRhY2goKQoKICAgICMgbG9zcyBjb21wdXRhdGlvbgogICAgbG9zc19saXN0LmFwcGVuZChsb3NzKGJveF9wYXJhbSwgZ3QpKQoKICByZXR1cm4gbG9zc19saXN0" download="">⬇</a></div>
<div id="lstnumberx1" class="ltx_listingline">
<span id="lstnumberx1.1" class="ltx_text ltx_lst_keyword ltx_font_typewriter" style="font-size:90%;color:#CF212E;">def</span><span id="lstnumberx1.2" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx1.3" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">train_loss</span><span id="lstnumberx1.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">(</span><span id="lstnumberx1.5" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">images</span><span id="lstnumberx1.6" class="ltx_text ltx_font_typewriter" style="font-size:90%;">,</span><span id="lstnumberx1.7" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx1.8" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">proj_mat</span><span id="lstnumberx1.9" class="ltx_text ltx_font_typewriter" style="font-size:90%;">,</span><span id="lstnumberx1.10" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx1.11" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">ref_points</span><span id="lstnumberx1.12" class="ltx_text ltx_font_typewriter" style="font-size:90%;">,</span><span id="lstnumberx1.13" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx1.14" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">gt</span><span id="lstnumberx1.15" class="ltx_text ltx_font_typewriter" style="font-size:90%;">,</span><span id="lstnumberx1.16" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx1.17" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">L</span><span id="lstnumberx1.18" class="ltx_text ltx_font_typewriter" style="font-size:90%;">):</span>
</div>
<div id="lstnumberx2" class="ltx_listingline">
<span id="lstnumberx2.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">  </span><span id="lstnumberx2.2" class="ltx_text ltx_lst_string ltx_font_typewriter" style="font-size:90%;">""</span><span id="lstnumberx2.3" class="ltx_text ltx_lst_string ltx_font_typewriter" style="font-size:90%;">"</span>
</div>
<div id="lstnumberx3" class="ltx_listingline">
<span id="lstnumberx3.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣␣</span><span id="lstnumberx3.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">images:</span><span id="lstnumberx3.3" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx3.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[B,</span><span id="lstnumberx3.5" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx3.6" class="ltx_text ltx_font_typewriter" style="font-size:90%;">N,</span><span id="lstnumberx3.7" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx3.8" class="ltx_text ltx_font_typewriter" style="font-size:90%;">H,</span><span id="lstnumberx3.9" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx3.10" class="ltx_text ltx_font_typewriter" style="font-size:90%;">W,</span><span id="lstnumberx3.11" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx3.12" class="ltx_text ltx_font_typewriter" style="font-size:90%;">3]</span>
</div>
<div id="lstnumberx4" class="ltx_listingline">
<span id="lstnumberx4.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣␣</span><span id="lstnumberx4.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">proj_mat:</span><span id="lstnumberx4.3" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx4.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">projection</span><span id="lstnumberx4.5" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx4.6" class="ltx_text ltx_font_typewriter" style="font-size:90%;">matrix</span><span id="lstnumberx4.7" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx4.8" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[B,</span><span id="lstnumberx4.9" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx4.10" class="ltx_text ltx_font_typewriter" style="font-size:90%;">N,</span><span id="lstnumberx4.11" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx4.12" class="ltx_text ltx_font_typewriter" style="font-size:90%;">4,</span><span id="lstnumberx4.13" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx4.14" class="ltx_text ltx_font_typewriter" style="font-size:90%;">4]</span>
</div>
<div id="lstnumberx5" class="ltx_listingline">
<span id="lstnumberx5.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣␣</span><span id="lstnumberx5.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">ref_points:</span><span id="lstnumberx5.3" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx5.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">reference</span><span id="lstnumberx5.5" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx5.6" class="ltx_text ltx_font_typewriter" style="font-size:90%;">points</span><span id="lstnumberx5.7" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx5.8" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[K,</span><span id="lstnumberx5.9" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx5.10" class="ltx_text ltx_font_typewriter" style="font-size:90%;">3]</span>
</div>
<div id="lstnumberx6" class="ltx_listingline">
<span id="lstnumberx6.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣␣</span><span id="lstnumberx6.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">gt:</span><span id="lstnumberx6.3" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx6.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">ground</span><span id="lstnumberx6.5" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx6.6" class="ltx_text ltx_font_typewriter" style="font-size:90%;">truth</span><span id="lstnumberx6.7" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx6.8" class="ltx_text ltx_font_typewriter" style="font-size:90%;">3D</span><span id="lstnumberx6.9" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx6.10" class="ltx_text ltx_font_typewriter" style="font-size:90%;">bounding</span><span id="lstnumberx6.11" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx6.12" class="ltx_text ltx_font_typewriter" style="font-size:90%;">box</span><span id="lstnumberx6.13" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx6.14" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[B,</span><span id="lstnumberx6.15" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx6.16" class="ltx_text ltx_font_typewriter" style="font-size:90%;">~,</span><span id="lstnumberx6.17" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx6.18" class="ltx_text ltx_font_typewriter" style="font-size:90%;">8,</span><span id="lstnumberx6.19" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx6.20" class="ltx_text ltx_font_typewriter" style="font-size:90%;">3]</span>
</div>
<div id="lstnumberx7" class="ltx_listingline">
<span id="lstnumberx7.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣␣</span><span id="lstnumberx7.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">#</span><span id="lstnumberx7.3" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx7.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">B:</span><span id="lstnumberx7.5" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx7.6" class="ltx_text ltx_font_typewriter" style="font-size:90%;">batch</span>
</div>
<div id="lstnumberx8" class="ltx_listingline">
<span id="lstnumberx8.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣␣</span><span id="lstnumberx8.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">#</span><span id="lstnumberx8.3" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx8.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">N:</span><span id="lstnumberx8.5" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx8.6" class="ltx_text ltx_font_typewriter" style="font-size:90%;">number</span><span id="lstnumberx8.7" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx8.8" class="ltx_text ltx_font_typewriter" style="font-size:90%;">of</span><span id="lstnumberx8.9" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx8.10" class="ltx_text ltx_font_typewriter" style="font-size:90%;">views</span>
</div>
<div id="lstnumberx9" class="ltx_listingline">
<span id="lstnumberx9.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣␣</span><span id="lstnumberx9.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">#</span><span id="lstnumberx9.3" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx9.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">K:</span><span id="lstnumberx9.5" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx9.6" class="ltx_text ltx_font_typewriter" style="font-size:90%;">number</span><span id="lstnumberx9.7" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx9.8" class="ltx_text ltx_font_typewriter" style="font-size:90%;">of</span><span id="lstnumberx9.9" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx9.10" class="ltx_text ltx_font_typewriter" style="font-size:90%;">queries</span>
</div>
<div id="lstnumberx10" class="ltx_listingline">
<span id="lstnumberx10.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣␣</span><span id="lstnumberx10.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">#</span><span id="lstnumberx10.3" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx10.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">L:</span><span id="lstnumberx10.5" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx10.6" class="ltx_text ltx_font_typewriter" style="font-size:90%;">number</span><span id="lstnumberx10.7" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx10.8" class="ltx_text ltx_font_typewriter" style="font-size:90%;">of</span><span id="lstnumberx10.9" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣</span><span id="lstnumberx10.10" class="ltx_text ltx_font_typewriter" style="font-size:90%;">iterations</span>
</div>
<div id="lstnumberx11" class="ltx_listingline">
<span id="lstnumberx11.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">␣␣</span><span id="lstnumberx11.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">"</span><span id="lstnumberx11.3" class="ltx_text ltx_lst_string ltx_font_typewriter" style="font-size:90%;">""</span>
</div>
<div id="lstnumberx12" class="ltx_listingline">
</div>
<div id="lstnumberx13" class="ltx_listingline">
<span id="lstnumberx13.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">  </span><span id="lstnumberx13.2" class="ltx_text ltx_lst_comment ltx_font_typewriter" style="font-size:90%;color:#009900;">#<span id="lstnumberx13.2.1" class="ltx_text ltx_lst_space"> </span>Encode<span id="lstnumberx13.2.2" class="ltx_text ltx_lst_space"> </span>image<span id="lstnumberx13.2.3" class="ltx_text ltx_lst_space"> </span>features</span>
</div>
<div id="lstnumberx14" class="ltx_listingline">
<span id="lstnumberx14.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">  </span><span id="lstnumberx14.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">feats</span><span id="lstnumberx14.3" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx14.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">=</span><span id="lstnumberx14.5" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx14.6" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">image_encoder</span><span id="lstnumberx14.7" class="ltx_text ltx_font_typewriter" style="font-size:90%;">(</span><span id="lstnumberx14.8" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">images</span><span id="lstnumberx14.9" class="ltx_text ltx_font_typewriter" style="font-size:90%;">)</span>
</div>
<div id="lstnumberx15" class="ltx_listingline">
</div>
<div id="lstnumberx16" class="ltx_listingline">
<span id="lstnumberx16.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">  </span><span id="lstnumberx16.2" class="ltx_text ltx_lst_comment ltx_font_typewriter" style="font-size:90%;color:#009900;">#<span id="lstnumberx16.2.1" class="ltx_text ltx_lst_space"> </span>Generate<span id="lstnumberx16.2.2" class="ltx_text ltx_lst_space"> </span>ray<span id="lstnumberx16.2.3" class="ltx_text ltx_lst_space"> </span>positional<span id="lstnumberx16.2.4" class="ltx_text ltx_lst_space"> </span>encodings</span>
</div>
<div id="lstnumberx17" class="ltx_listingline">
<span id="lstnumberx17.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">  </span><span id="lstnumberx17.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">ray_encoding</span><span id="lstnumberx17.3" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx17.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">=</span><span id="lstnumberx17.5" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx17.6" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">generate_ray_encoding</span><span id="lstnumberx17.7" class="ltx_text ltx_font_typewriter" style="font-size:90%;">(</span><span id="lstnumberx17.8" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">proj_mat</span><span id="lstnumberx17.9" class="ltx_text ltx_font_typewriter" style="font-size:90%;">)</span>
</div>
<div id="lstnumberx18" class="ltx_listingline">
</div>
<div id="lstnumberx19" class="ltx_listingline">
<span id="lstnumberx19.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">  </span><span id="lstnumberx19.2" class="ltx_text ltx_lst_comment ltx_font_typewriter" style="font-size:90%;color:#009900;">#<span id="lstnumberx19.2.1" class="ltx_text ltx_lst_space"> </span>3D-aware<span id="lstnumberx19.2.2" class="ltx_text ltx_lst_space"> </span>image<span id="lstnumberx19.2.3" class="ltx_text ltx_lst_space"> </span>features</span>
</div>
<div id="lstnumberx20" class="ltx_listingline">
<span id="lstnumberx20.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">  </span><span id="lstnumberx20.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">feats</span><span id="lstnumberx20.3" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx20.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">=</span><span id="lstnumberx20.5" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx20.6" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">feats</span><span id="lstnumberx20.7" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx20.8" class="ltx_text ltx_font_typewriter" style="font-size:90%;">+</span><span id="lstnumberx20.9" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx20.10" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">ray_encoding</span>
</div>
<div id="lstnumberx21" class="ltx_listingline">
</div>
<div id="lstnumberx22" class="ltx_listingline">
<span id="lstnumberx22.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">  </span><span id="lstnumberx22.2" class="ltx_text ltx_lst_comment ltx_font_typewriter" style="font-size:90%;color:#009900;">#<span id="lstnumberx22.2.1" class="ltx_text ltx_lst_space"> </span>supervise<span id="lstnumberx22.2.2" class="ltx_text ltx_lst_space"> </span>the<span id="lstnumberx22.2.3" class="ltx_text ltx_lst_space"> </span>prediction<span id="lstnumberx22.2.4" class="ltx_text ltx_lst_space"> </span>in<span id="lstnumberx22.2.5" class="ltx_text ltx_lst_space"> </span>each<span id="lstnumberx22.2.6" class="ltx_text ltx_lst_space"> </span>iteration</span>
</div>
<div id="lstnumberx23" class="ltx_listingline">
<span id="lstnumberx23.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">  </span><span id="lstnumberx23.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">loss_list</span><span id="lstnumberx23.3" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx23.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">=</span><span id="lstnumberx23.5" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx23.6" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[]</span>
</div>
<div id="lstnumberx24" class="ltx_listingline">
<span id="lstnumberx24.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">  </span><span id="lstnumberx24.2" class="ltx_text ltx_lst_keyword ltx_font_typewriter" style="font-size:90%;color:#CF212E;">for</span><span id="lstnumberx24.3" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx24.4" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">l</span><span id="lstnumberx24.5" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx24.6" class="ltx_text ltx_lst_keyword ltx_font_typewriter" style="font-size:90%;color:#CF212E;">in</span><span id="lstnumberx24.7" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx24.8" class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" style="font-size:90%;color:#CF212E;">range</span><span id="lstnumberx24.9" class="ltx_text ltx_font_typewriter" style="font-size:90%;">(</span><span id="lstnumberx24.10" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">L</span><span id="lstnumberx24.11" class="ltx_text ltx_font_typewriter" style="font-size:90%;">):</span>
</div>
<div id="lstnumberx25" class="ltx_listingline">
<span id="lstnumberx25.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">    </span><span id="lstnumberx25.2" class="ltx_text ltx_lst_comment ltx_font_typewriter" style="font-size:90%;color:#009900;">#<span id="lstnumberx25.2.1" class="ltx_text ltx_lst_space"> </span>pixel-aligned<span id="lstnumberx25.2.2" class="ltx_text ltx_lst_space"> </span>features</span>
</div>
<div id="lstnumberx26" class="ltx_listingline">
<span id="lstnumberx26.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">    </span><span id="lstnumberx26.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">pa_feat</span><span id="lstnumberx26.3" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx26.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">=</span><span id="lstnumberx26.5" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx26.6" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">sample_and_pool_feats</span><span id="lstnumberx26.7" class="ltx_text ltx_font_typewriter" style="font-size:90%;">(</span><span id="lstnumberx26.8" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">feats</span><span id="lstnumberx26.9" class="ltx_text ltx_font_typewriter" style="font-size:90%;">,</span><span id="lstnumberx26.10" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx26.11" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">ref_points</span><span id="lstnumberx26.12" class="ltx_text ltx_font_typewriter" style="font-size:90%;">,</span><span id="lstnumberx26.13" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx26.14" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">proj_mat</span><span id="lstnumberx26.15" class="ltx_text ltx_font_typewriter" style="font-size:90%;">)</span>
</div>
<div id="lstnumberx27" class="ltx_listingline">
</div>
<div id="lstnumberx28" class="ltx_listingline">
<span id="lstnumberx28.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">    </span><span id="lstnumberx28.2" class="ltx_text ltx_lst_comment ltx_font_typewriter" style="font-size:90%;color:#009900;">#<span id="lstnumberx28.2.1" class="ltx_text ltx_lst_space"> </span>point<span id="lstnumberx28.2.2" class="ltx_text ltx_lst_space"> </span>positional<span id="lstnumberx28.2.3" class="ltx_text ltx_lst_space"> </span>encoding</span>
</div>
<div id="lstnumberx29" class="ltx_listingline">
<span id="lstnumberx29.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">    </span><span id="lstnumberx29.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">point_pos</span><span id="lstnumberx29.3" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx29.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">=</span><span id="lstnumberx29.5" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx29.6" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">positional_encoding</span><span id="lstnumberx29.7" class="ltx_text ltx_font_typewriter" style="font-size:90%;">(</span><span id="lstnumberx29.8" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">ref_points</span><span id="lstnumberx29.9" class="ltx_text ltx_font_typewriter" style="font-size:90%;">)</span>
</div>
<div id="lstnumberx30" class="ltx_listingline">
</div>
<div id="lstnumberx31" class="ltx_listingline">
<span id="lstnumberx31.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">    </span><span id="lstnumberx31.2" class="ltx_text ltx_lst_comment ltx_font_typewriter" style="font-size:90%;color:#009900;">#<span id="lstnumberx31.2.1" class="ltx_text ltx_lst_space"> </span>transformer<span id="lstnumberx31.2.2" class="ltx_text ltx_lst_space"> </span>decoder<span id="lstnumberx31.2.3" class="ltx_text ltx_lst_space"> </span>layer</span>
</div>
<div id="lstnumberx32" class="ltx_listingline">
<span id="lstnumberx32.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">    </span><span id="lstnumberx32.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">output</span><span id="lstnumberx32.3" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx32.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">=</span><span id="lstnumberx32.5" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx32.6" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">layer</span><span id="lstnumberx32.7" class="ltx_text ltx_font_typewriter" style="font-size:90%;">(</span><span id="lstnumberx32.8" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">tgt</span><span id="lstnumberx32.9" class="ltx_text ltx_font_typewriter" style="font-size:90%;">=</span><span id="lstnumberx32.10" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">pa_feat</span><span id="lstnumberx32.11" class="ltx_text ltx_font_typewriter" style="font-size:90%;">,</span><span id="lstnumberx32.12" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx32.13" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">memory</span><span id="lstnumberx32.14" class="ltx_text ltx_font_typewriter" style="font-size:90%;">=</span><span id="lstnumberx32.15" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">feats</span><span id="lstnumberx32.16" class="ltx_text ltx_font_typewriter" style="font-size:90%;">,</span><span id="lstnumberx32.17" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx32.18" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">query_pos</span><span id="lstnumberx32.19" class="ltx_text ltx_font_typewriter" style="font-size:90%;">=</span><span id="lstnumberx32.20" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">point_pos</span><span id="lstnumberx32.21" class="ltx_text ltx_font_typewriter" style="font-size:90%;">)</span>
</div>
<div id="lstnumberx33" class="ltx_listingline">
</div>
<div id="lstnumberx34" class="ltx_listingline">
<span id="lstnumberx34.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">    </span><span id="lstnumberx34.2" class="ltx_text ltx_lst_comment ltx_font_typewriter" style="font-size:90%;color:#009900;">#<span id="lstnumberx34.2.1" class="ltx_text ltx_lst_space"> </span>detection<span id="lstnumberx34.2.2" class="ltx_text ltx_lst_space"> </span>head</span>
</div>
<div id="lstnumberx35" class="ltx_listingline">
<span id="lstnumberx35.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">    </span><span id="lstnumberx35.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">box_param</span><span id="lstnumberx35.3" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx35.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">=</span><span id="lstnumberx35.5" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx35.6" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">det_head</span><span id="lstnumberx35.7" class="ltx_text ltx_font_typewriter" style="font-size:90%;">(</span><span id="lstnumberx35.8" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">output</span><span id="lstnumberx35.9" class="ltx_text ltx_font_typewriter" style="font-size:90%;">)</span>
</div>
<div id="lstnumberx36" class="ltx_listingline">
</div>
<div id="lstnumberx37" class="ltx_listingline">
<span id="lstnumberx37.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">    </span><span id="lstnumberx37.2" class="ltx_text ltx_lst_comment ltx_font_typewriter" style="font-size:90%;color:#009900;">#<span id="lstnumberx37.2.1" class="ltx_text ltx_lst_space"> </span>get<span id="lstnumberx37.2.2" class="ltx_text ltx_lst_space"> </span>the<span id="lstnumberx37.2.3" class="ltx_text ltx_lst_space"> </span>object<span id="lstnumberx37.2.4" class="ltx_text ltx_lst_space"> </span>centers</span>
</div>
<div id="lstnumberx38" class="ltx_listingline">
<span id="lstnumberx38.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">    </span><span id="lstnumberx38.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">box_param</span><span id="lstnumberx38.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><span id="lstnumberx38.4" class="ltx_text ltx_lst_string ltx_font_typewriter" style="font-size:90%;">’center’</span><span id="lstnumberx38.5" class="ltx_text ltx_font_typewriter" style="font-size:90%;">]</span><span id="lstnumberx38.6" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx38.7" class="ltx_text ltx_font_typewriter" style="font-size:90%;">+=</span><span id="lstnumberx38.8" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx38.9" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">ref_points</span>
</div>
<div id="lstnumberx39" class="ltx_listingline">
</div>
<div id="lstnumberx40" class="ltx_listingline">
<span id="lstnumberx40.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">    </span><span id="lstnumberx40.2" class="ltx_text ltx_lst_comment ltx_font_typewriter" style="font-size:90%;color:#009900;">#<span id="lstnumberx40.2.1" class="ltx_text ltx_lst_space"> </span>new<span id="lstnumberx40.2.2" class="ltx_text ltx_lst_space"> </span>reference<span id="lstnumberx40.2.3" class="ltx_text ltx_lst_space"> </span>points<span id="lstnumberx40.2.4" class="ltx_text ltx_lst_space"> </span>for<span id="lstnumberx40.2.5" class="ltx_text ltx_lst_space"> </span>the<span id="lstnumberx40.2.6" class="ltx_text ltx_lst_space"> </span>next<span id="lstnumberx40.2.7" class="ltx_text ltx_lst_space"> </span>iteration</span>
</div>
<div id="lstnumberx41" class="ltx_listingline">
<span id="lstnumberx41.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">    </span><span id="lstnumberx41.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">ref_points</span><span id="lstnumberx41.3" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx41.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">=</span><span id="lstnumberx41.5" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx41.6" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">box_param</span><span id="lstnumberx41.7" class="ltx_text ltx_font_typewriter" style="font-size:90%;">[</span><span id="lstnumberx41.8" class="ltx_text ltx_lst_string ltx_font_typewriter" style="font-size:90%;">’center’</span><span id="lstnumberx41.9" class="ltx_text ltx_font_typewriter" style="font-size:90%;">].</span><span id="lstnumberx41.10" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">detach</span><span id="lstnumberx41.11" class="ltx_text ltx_font_typewriter" style="font-size:90%;">()</span>
</div>
<div id="lstnumberx42" class="ltx_listingline">
</div>
<div id="lstnumberx43" class="ltx_listingline">
<span id="lstnumberx43.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">    </span><span id="lstnumberx43.2" class="ltx_text ltx_lst_comment ltx_font_typewriter" style="font-size:90%;color:#009900;">#<span id="lstnumberx43.2.1" class="ltx_text ltx_lst_space"> </span>loss<span id="lstnumberx43.2.2" class="ltx_text ltx_lst_space"> </span>computation</span>
</div>
<div id="lstnumberx44" class="ltx_listingline">
<span id="lstnumberx44.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">    </span><span id="lstnumberx44.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">loss_list</span><span id="lstnumberx44.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">.</span><span id="lstnumberx44.4" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">append</span><span id="lstnumberx44.5" class="ltx_text ltx_font_typewriter" style="font-size:90%;">(</span><span id="lstnumberx44.6" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">loss</span><span id="lstnumberx44.7" class="ltx_text ltx_font_typewriter" style="font-size:90%;">(</span><span id="lstnumberx44.8" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">box_param</span><span id="lstnumberx44.9" class="ltx_text ltx_font_typewriter" style="font-size:90%;">,</span><span id="lstnumberx44.10" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx44.11" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">gt</span><span id="lstnumberx44.12" class="ltx_text ltx_font_typewriter" style="font-size:90%;">))</span>
</div>
<div id="lstnumberx45" class="ltx_listingline">
</div>
<div id="lstnumberx46" class="ltx_listingline">
<span id="lstnumberx46.1" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;">  </span><span id="lstnumberx46.2" class="ltx_text ltx_lst_keyword ltx_font_typewriter" style="font-size:90%;color:#CF212E;">return</span><span id="lstnumberx46.3" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx46.4" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">loss_list</span>
</div>
</div>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Appearance- and Geometry-Informed Queries</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.7" class="ltx_p">Now, we turn to our queries.
The purpose of our queries is to produce final 3D object detections by interacting with the input views.
We randomly generate a set of reference points in the 3D space bounding a large region of the view frustum.
Assume <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">x</annotation></semantics></math> is a 3D point and <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="\pi_{i}(x)" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.2" xref="S3.SS2.p1.2.m2.1.2.cmml"><msub id="S3.SS2.p1.2.m2.1.2.2" xref="S3.SS2.p1.2.m2.1.2.2.cmml"><mi id="S3.SS2.p1.2.m2.1.2.2.2" xref="S3.SS2.p1.2.m2.1.2.2.2.cmml">π</mi><mi id="S3.SS2.p1.2.m2.1.2.2.3" xref="S3.SS2.p1.2.m2.1.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.2.1" xref="S3.SS2.p1.2.m2.1.2.1.cmml">​</mo><mrow id="S3.SS2.p1.2.m2.1.2.3.2" xref="S3.SS2.p1.2.m2.1.2.cmml"><mo stretchy="false" id="S3.SS2.p1.2.m2.1.2.3.2.1" xref="S3.SS2.p1.2.m2.1.2.cmml">(</mo><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">x</mi><mo stretchy="false" id="S3.SS2.p1.2.m2.1.2.3.2.2" xref="S3.SS2.p1.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.2.cmml" xref="S3.SS2.p1.2.m2.1.2"><times id="S3.SS2.p1.2.m2.1.2.1.cmml" xref="S3.SS2.p1.2.m2.1.2.1"></times><apply id="S3.SS2.p1.2.m2.1.2.2.cmml" xref="S3.SS2.p1.2.m2.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.2.2.1.cmml" xref="S3.SS2.p1.2.m2.1.2.2">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.2.2.2.cmml" xref="S3.SS2.p1.2.m2.1.2.2.2">𝜋</ci><ci id="S3.SS2.p1.2.m2.1.2.2.3.cmml" xref="S3.SS2.p1.2.m2.1.2.2.3">𝑖</ci></apply><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\pi_{i}(x)</annotation></semantics></math> is its 2D projection on the <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">i</annotation></semantics></math>-th input view using known camera pose and projection <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="\pi_{i}" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><msub id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">π</mi><mi id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2">𝜋</ci><ci id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">\pi_{i}</annotation></semantics></math>.
We bilinearly sample the feature vectors at the 2D pixel locations, namely <math id="S3.SS2.p1.5.m5.4" class="ltx_Math" alttext="f_{i,x}=F^{p}_{i}(\pi_{i}(x))" display="inline"><semantics id="S3.SS2.p1.5.m5.4a"><mrow id="S3.SS2.p1.5.m5.4.4" xref="S3.SS2.p1.5.m5.4.4.cmml"><msub id="S3.SS2.p1.5.m5.4.4.3" xref="S3.SS2.p1.5.m5.4.4.3.cmml"><mi id="S3.SS2.p1.5.m5.4.4.3.2" xref="S3.SS2.p1.5.m5.4.4.3.2.cmml">f</mi><mrow id="S3.SS2.p1.5.m5.2.2.2.4" xref="S3.SS2.p1.5.m5.2.2.2.3.cmml"><mi id="S3.SS2.p1.5.m5.1.1.1.1" xref="S3.SS2.p1.5.m5.1.1.1.1.cmml">i</mi><mo id="S3.SS2.p1.5.m5.2.2.2.4.1" xref="S3.SS2.p1.5.m5.2.2.2.3.cmml">,</mo><mi id="S3.SS2.p1.5.m5.2.2.2.2" xref="S3.SS2.p1.5.m5.2.2.2.2.cmml">x</mi></mrow></msub><mo id="S3.SS2.p1.5.m5.4.4.2" xref="S3.SS2.p1.5.m5.4.4.2.cmml">=</mo><mrow id="S3.SS2.p1.5.m5.4.4.1" xref="S3.SS2.p1.5.m5.4.4.1.cmml"><msubsup id="S3.SS2.p1.5.m5.4.4.1.3" xref="S3.SS2.p1.5.m5.4.4.1.3.cmml"><mi id="S3.SS2.p1.5.m5.4.4.1.3.2.2" xref="S3.SS2.p1.5.m5.4.4.1.3.2.2.cmml">F</mi><mi id="S3.SS2.p1.5.m5.4.4.1.3.3" xref="S3.SS2.p1.5.m5.4.4.1.3.3.cmml">i</mi><mi id="S3.SS2.p1.5.m5.4.4.1.3.2.3" xref="S3.SS2.p1.5.m5.4.4.1.3.2.3.cmml">p</mi></msubsup><mo lspace="0em" rspace="0em" id="S3.SS2.p1.5.m5.4.4.1.2" xref="S3.SS2.p1.5.m5.4.4.1.2.cmml">​</mo><mrow id="S3.SS2.p1.5.m5.4.4.1.1.1" xref="S3.SS2.p1.5.m5.4.4.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p1.5.m5.4.4.1.1.1.2" xref="S3.SS2.p1.5.m5.4.4.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p1.5.m5.4.4.1.1.1.1" xref="S3.SS2.p1.5.m5.4.4.1.1.1.1.cmml"><msub id="S3.SS2.p1.5.m5.4.4.1.1.1.1.2" xref="S3.SS2.p1.5.m5.4.4.1.1.1.1.2.cmml"><mi id="S3.SS2.p1.5.m5.4.4.1.1.1.1.2.2" xref="S3.SS2.p1.5.m5.4.4.1.1.1.1.2.2.cmml">π</mi><mi id="S3.SS2.p1.5.m5.4.4.1.1.1.1.2.3" xref="S3.SS2.p1.5.m5.4.4.1.1.1.1.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p1.5.m5.4.4.1.1.1.1.1" xref="S3.SS2.p1.5.m5.4.4.1.1.1.1.1.cmml">​</mo><mrow id="S3.SS2.p1.5.m5.4.4.1.1.1.1.3.2" xref="S3.SS2.p1.5.m5.4.4.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p1.5.m5.4.4.1.1.1.1.3.2.1" xref="S3.SS2.p1.5.m5.4.4.1.1.1.1.cmml">(</mo><mi id="S3.SS2.p1.5.m5.3.3" xref="S3.SS2.p1.5.m5.3.3.cmml">x</mi><mo stretchy="false" id="S3.SS2.p1.5.m5.4.4.1.1.1.1.3.2.2" xref="S3.SS2.p1.5.m5.4.4.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.SS2.p1.5.m5.4.4.1.1.1.3" xref="S3.SS2.p1.5.m5.4.4.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.4b"><apply id="S3.SS2.p1.5.m5.4.4.cmml" xref="S3.SS2.p1.5.m5.4.4"><eq id="S3.SS2.p1.5.m5.4.4.2.cmml" xref="S3.SS2.p1.5.m5.4.4.2"></eq><apply id="S3.SS2.p1.5.m5.4.4.3.cmml" xref="S3.SS2.p1.5.m5.4.4.3"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.4.4.3.1.cmml" xref="S3.SS2.p1.5.m5.4.4.3">subscript</csymbol><ci id="S3.SS2.p1.5.m5.4.4.3.2.cmml" xref="S3.SS2.p1.5.m5.4.4.3.2">𝑓</ci><list id="S3.SS2.p1.5.m5.2.2.2.3.cmml" xref="S3.SS2.p1.5.m5.2.2.2.4"><ci id="S3.SS2.p1.5.m5.1.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1.1.1">𝑖</ci><ci id="S3.SS2.p1.5.m5.2.2.2.2.cmml" xref="S3.SS2.p1.5.m5.2.2.2.2">𝑥</ci></list></apply><apply id="S3.SS2.p1.5.m5.4.4.1.cmml" xref="S3.SS2.p1.5.m5.4.4.1"><times id="S3.SS2.p1.5.m5.4.4.1.2.cmml" xref="S3.SS2.p1.5.m5.4.4.1.2"></times><apply id="S3.SS2.p1.5.m5.4.4.1.3.cmml" xref="S3.SS2.p1.5.m5.4.4.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.4.4.1.3.1.cmml" xref="S3.SS2.p1.5.m5.4.4.1.3">subscript</csymbol><apply id="S3.SS2.p1.5.m5.4.4.1.3.2.cmml" xref="S3.SS2.p1.5.m5.4.4.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.4.4.1.3.2.1.cmml" xref="S3.SS2.p1.5.m5.4.4.1.3">superscript</csymbol><ci id="S3.SS2.p1.5.m5.4.4.1.3.2.2.cmml" xref="S3.SS2.p1.5.m5.4.4.1.3.2.2">𝐹</ci><ci id="S3.SS2.p1.5.m5.4.4.1.3.2.3.cmml" xref="S3.SS2.p1.5.m5.4.4.1.3.2.3">𝑝</ci></apply><ci id="S3.SS2.p1.5.m5.4.4.1.3.3.cmml" xref="S3.SS2.p1.5.m5.4.4.1.3.3">𝑖</ci></apply><apply id="S3.SS2.p1.5.m5.4.4.1.1.1.1.cmml" xref="S3.SS2.p1.5.m5.4.4.1.1.1"><times id="S3.SS2.p1.5.m5.4.4.1.1.1.1.1.cmml" xref="S3.SS2.p1.5.m5.4.4.1.1.1.1.1"></times><apply id="S3.SS2.p1.5.m5.4.4.1.1.1.1.2.cmml" xref="S3.SS2.p1.5.m5.4.4.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.4.4.1.1.1.1.2.1.cmml" xref="S3.SS2.p1.5.m5.4.4.1.1.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.5.m5.4.4.1.1.1.1.2.2.cmml" xref="S3.SS2.p1.5.m5.4.4.1.1.1.1.2.2">𝜋</ci><ci id="S3.SS2.p1.5.m5.4.4.1.1.1.1.2.3.cmml" xref="S3.SS2.p1.5.m5.4.4.1.1.1.1.2.3">𝑖</ci></apply><ci id="S3.SS2.p1.5.m5.3.3.cmml" xref="S3.SS2.p1.5.m5.3.3">𝑥</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.4c">f_{i,x}=F^{p}_{i}(\pi_{i}(x))</annotation></semantics></math>.
We aggregate the feature vectors across all views with average pooling – if <math id="S3.SS2.p1.6.m6.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS2.p1.6.m6.1a"><mi id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><ci id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">x</annotation></semantics></math> projects outside the image border for a view it is omitted.
The query for reference point <math id="S3.SS2.p1.7.m7.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS2.p1.7.m7.1a"><mi id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.1b"><ci id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.1c">x</annotation></semantics></math> is defined as:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.4" class="ltx_Math" alttext="q_{x}=\textstyle\textrm{MLP}(\gamma(x))+\frac{1}{N}\sum_{i}f_{i,x}\,," display="block"><semantics id="S3.E1.m1.4a"><mrow id="S3.E1.m1.4.4.1" xref="S3.E1.m1.4.4.1.1.cmml"><mrow id="S3.E1.m1.4.4.1.1" xref="S3.E1.m1.4.4.1.1.cmml"><msub id="S3.E1.m1.4.4.1.1.3" xref="S3.E1.m1.4.4.1.1.3.cmml"><mi id="S3.E1.m1.4.4.1.1.3.2" xref="S3.E1.m1.4.4.1.1.3.2.cmml">q</mi><mi id="S3.E1.m1.4.4.1.1.3.3" xref="S3.E1.m1.4.4.1.1.3.3.cmml">x</mi></msub><mo id="S3.E1.m1.4.4.1.1.2" xref="S3.E1.m1.4.4.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.4.4.1.1.1" xref="S3.E1.m1.4.4.1.1.1.cmml"><mrow id="S3.E1.m1.4.4.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.cmml"><mtext id="S3.E1.m1.4.4.1.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.1.3a.cmml">MLP</mtext><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.1.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.4.4.1.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.1.1.1.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.4.4.1.1.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.4.4.1.1.1.1.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.2.cmml">γ</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.cmml">​</mo><mrow id="S3.E1.m1.4.4.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.1.1.1.1.1.1.1.3.2.1" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">x</mi><mo stretchy="false" id="S3.E1.m1.4.4.1.1.1.1.1.1.1.3.2.2" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E1.m1.4.4.1.1.1.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.4.4.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.2.cmml">+</mo><mrow id="S3.E1.m1.4.4.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.3.cmml"><mstyle displaystyle="false" id="S3.E1.m1.4.4.1.1.1.3.2" xref="S3.E1.m1.4.4.1.1.1.3.2.cmml"><mfrac id="S3.E1.m1.4.4.1.1.1.3.2a" xref="S3.E1.m1.4.4.1.1.1.3.2.cmml"><mn id="S3.E1.m1.4.4.1.1.1.3.2.2" xref="S3.E1.m1.4.4.1.1.1.3.2.2.cmml">1</mn><mi id="S3.E1.m1.4.4.1.1.1.3.2.3" xref="S3.E1.m1.4.4.1.1.1.3.2.3.cmml">N</mi></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.1.1.1.3.1" xref="S3.E1.m1.4.4.1.1.1.3.1.cmml">​</mo><mrow id="S3.E1.m1.4.4.1.1.1.3.3" xref="S3.E1.m1.4.4.1.1.1.3.3.cmml"><mstyle displaystyle="false" id="S3.E1.m1.4.4.1.1.1.3.3.1" xref="S3.E1.m1.4.4.1.1.1.3.3.1.cmml"><msub id="S3.E1.m1.4.4.1.1.1.3.3.1a" xref="S3.E1.m1.4.4.1.1.1.3.3.1.cmml"><mo id="S3.E1.m1.4.4.1.1.1.3.3.1.2" xref="S3.E1.m1.4.4.1.1.1.3.3.1.2.cmml">∑</mo><mi id="S3.E1.m1.4.4.1.1.1.3.3.1.3" xref="S3.E1.m1.4.4.1.1.1.3.3.1.3.cmml">i</mi></msub></mstyle><msub id="S3.E1.m1.4.4.1.1.1.3.3.2" xref="S3.E1.m1.4.4.1.1.1.3.3.2.cmml"><mi id="S3.E1.m1.4.4.1.1.1.3.3.2.2" xref="S3.E1.m1.4.4.1.1.1.3.3.2.2.cmml">f</mi><mrow id="S3.E1.m1.2.2.2.4" xref="S3.E1.m1.2.2.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">i</mi><mo id="S3.E1.m1.2.2.2.4.1" xref="S3.E1.m1.2.2.2.3.cmml">,</mo><mi id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml">x</mi></mrow></msub></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.4.4.1.2" xref="S3.E1.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.4b"><apply id="S3.E1.m1.4.4.1.1.cmml" xref="S3.E1.m1.4.4.1"><eq id="S3.E1.m1.4.4.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.2"></eq><apply id="S3.E1.m1.4.4.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.3.1.cmml" xref="S3.E1.m1.4.4.1.1.3">subscript</csymbol><ci id="S3.E1.m1.4.4.1.1.3.2.cmml" xref="S3.E1.m1.4.4.1.1.3.2">𝑞</ci><ci id="S3.E1.m1.4.4.1.1.3.3.cmml" xref="S3.E1.m1.4.4.1.1.3.3">𝑥</ci></apply><apply id="S3.E1.m1.4.4.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1"><plus id="S3.E1.m1.4.4.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.2"></plus><apply id="S3.E1.m1.4.4.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1"><times id="S3.E1.m1.4.4.1.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.2"></times><ci id="S3.E1.m1.4.4.1.1.1.1.3a.cmml" xref="S3.E1.m1.4.4.1.1.1.1.3"><mtext id="S3.E1.m1.4.4.1.1.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.3">MLP</mtext></ci><apply id="S3.E1.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1"><times id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1"></times><ci id="S3.E1.m1.4.4.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.2">𝛾</ci><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">𝑥</ci></apply></apply><apply id="S3.E1.m1.4.4.1.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.1.3"><times id="S3.E1.m1.4.4.1.1.1.3.1.cmml" xref="S3.E1.m1.4.4.1.1.1.3.1"></times><apply id="S3.E1.m1.4.4.1.1.1.3.2.cmml" xref="S3.E1.m1.4.4.1.1.1.3.2"><divide id="S3.E1.m1.4.4.1.1.1.3.2.1.cmml" xref="S3.E1.m1.4.4.1.1.1.3.2"></divide><cn type="integer" id="S3.E1.m1.4.4.1.1.1.3.2.2.cmml" xref="S3.E1.m1.4.4.1.1.1.3.2.2">1</cn><ci id="S3.E1.m1.4.4.1.1.1.3.2.3.cmml" xref="S3.E1.m1.4.4.1.1.1.3.2.3">𝑁</ci></apply><apply id="S3.E1.m1.4.4.1.1.1.3.3.cmml" xref="S3.E1.m1.4.4.1.1.1.3.3"><apply id="S3.E1.m1.4.4.1.1.1.3.3.1.cmml" xref="S3.E1.m1.4.4.1.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.1.3.3.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.3.3.1">subscript</csymbol><sum id="S3.E1.m1.4.4.1.1.1.3.3.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.3.3.1.2"></sum><ci id="S3.E1.m1.4.4.1.1.1.3.3.1.3.cmml" xref="S3.E1.m1.4.4.1.1.1.3.3.1.3">𝑖</ci></apply><apply id="S3.E1.m1.4.4.1.1.1.3.3.2.cmml" xref="S3.E1.m1.4.4.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.1.3.3.2.1.cmml" xref="S3.E1.m1.4.4.1.1.1.3.3.2">subscript</csymbol><ci id="S3.E1.m1.4.4.1.1.1.3.3.2.2.cmml" xref="S3.E1.m1.4.4.1.1.1.3.3.2.2">𝑓</ci><list id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.4"><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">𝑖</ci><ci id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2">𝑥</ci></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.4c">q_{x}=\textstyle\textrm{MLP}(\gamma(x))+\frac{1}{N}\sum_{i}f_{i,x}\,,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p1.9" class="ltx_p">MLP is a small neural net that embeds the Fourier positional encoding of <math id="S3.SS2.p1.8.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS2.p1.8.m1.1a"><mi id="S3.SS2.p1.8.m1.1.1" xref="S3.SS2.p1.8.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m1.1b"><ci id="S3.SS2.p1.8.m1.1.1.cmml" xref="S3.SS2.p1.8.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m1.1c">x</annotation></semantics></math>, <math id="S3.SS2.p1.9.m2.1" class="ltx_Math" alttext="\gamma(x)" display="inline"><semantics id="S3.SS2.p1.9.m2.1a"><mrow id="S3.SS2.p1.9.m2.1.2" xref="S3.SS2.p1.9.m2.1.2.cmml"><mi id="S3.SS2.p1.9.m2.1.2.2" xref="S3.SS2.p1.9.m2.1.2.2.cmml">γ</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.9.m2.1.2.1" xref="S3.SS2.p1.9.m2.1.2.1.cmml">​</mo><mrow id="S3.SS2.p1.9.m2.1.2.3.2" xref="S3.SS2.p1.9.m2.1.2.cmml"><mo stretchy="false" id="S3.SS2.p1.9.m2.1.2.3.2.1" xref="S3.SS2.p1.9.m2.1.2.cmml">(</mo><mi id="S3.SS2.p1.9.m2.1.1" xref="S3.SS2.p1.9.m2.1.1.cmml">x</mi><mo stretchy="false" id="S3.SS2.p1.9.m2.1.2.3.2.2" xref="S3.SS2.p1.9.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m2.1b"><apply id="S3.SS2.p1.9.m2.1.2.cmml" xref="S3.SS2.p1.9.m2.1.2"><times id="S3.SS2.p1.9.m2.1.2.1.cmml" xref="S3.SS2.p1.9.m2.1.2.1"></times><ci id="S3.SS2.p1.9.m2.1.2.2.cmml" xref="S3.SS2.p1.9.m2.1.2.2">𝛾</ci><ci id="S3.SS2.p1.9.m2.1.1.cmml" xref="S3.SS2.p1.9.m2.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.9.m2.1c">\gamma(x)</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.2" class="ltx_p">By encoding the 3D location of the reference point <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">x</annotation></semantics></math> and its appearance from the input views, query <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="q_{x}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><msub id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">q</mi><mi id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">𝑞</ci><ci id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">q_{x}</annotation></semantics></math> carries both appearance and geometric cues.
Our appearance- and geometry-informed queries attend to the 3D-aware input feature maps which allows the model to encode 3D-to-2D correspondences and make 3D predictions from just 2D input views.
This is unlike PETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, where queries encode only the coordinates of the reference points.
We show in our experiments that the absence of appearance cues in the queries leads to lower performance and slower convergence rates.
Note that PIFu <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> also remarked the importance of pixel-aligned features when making 3D predictions for the task of human shape reconstruction from images.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Recurrent Query-Based Decoding</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The goal of the decoder is to predict how a 3D reference point, encoded by the query, should be translated in order to match a true object center.
Initially, the reference point is far from the object, as it is randomly sampled from the 3D space.
In order to predict how to correctly translate it, we need to capture long-range contextual and geometric cues with the input views.
We achieve this via cross-attention between our proposed appearance- and geometry-informed queries and 3D-aware input feature maps.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.2" class="ltx_p">Our decoder is one recurrent PARQ layer which chains a single DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> transformer decoder layer and a detection head (see Sec. <a href="#S3.SS4" title="3.4 3D Detection Head and Objective ‣ 3 Method ‣ Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>).
The pixel-aligned queries cross-attend to the image features through the multi-head attention operation inside the DETR transformer decoder layer.
Since the reference point is initially far from the object, we perform <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mi id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">L</annotation></semantics></math> iterative predictions via recurrent decoding.
The reference points are initially randomly positioned in 3D space.
They are recurrently updated with offsets predicted by the PARQ layer. If <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="x_{0}" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><msub id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mi id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml">x</mi><mn id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2">𝑥</ci><cn type="integer" id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">x_{0}</annotation></semantics></math> is the initial 3D reference point,</p>
<table id="S6.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.2" class="ltx_Math" alttext="\displaystyle x_{l}=\text{PARQ}\left(x_{l-1},\{F_{i}^{p}\}\right)+x_{l-1}" display="inline"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml"><msub id="S3.E2.m1.2.2.4" xref="S3.E2.m1.2.2.4.cmml"><mi id="S3.E2.m1.2.2.4.2" xref="S3.E2.m1.2.2.4.2.cmml">x</mi><mi id="S3.E2.m1.2.2.4.3" xref="S3.E2.m1.2.2.4.3.cmml">l</mi></msub><mo id="S3.E2.m1.2.2.3" xref="S3.E2.m1.2.2.3.cmml">=</mo><mrow id="S3.E2.m1.2.2.2" xref="S3.E2.m1.2.2.2.cmml"><mrow id="S3.E2.m1.2.2.2.2" xref="S3.E2.m1.2.2.2.2.cmml"><mtext id="S3.E2.m1.2.2.2.2.4" xref="S3.E2.m1.2.2.2.2.4a.cmml">PARQ</mtext><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.2.3" xref="S3.E2.m1.2.2.2.2.3.cmml">​</mo><mrow id="S3.E2.m1.2.2.2.2.2.2" xref="S3.E2.m1.2.2.2.2.2.3.cmml"><mo id="S3.E2.m1.2.2.2.2.2.2.3" xref="S3.E2.m1.2.2.2.2.2.3.cmml">(</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.2.cmml">x</mi><mrow id="S3.E2.m1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.3.2.cmml">l</mi><mo id="S3.E2.m1.1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.1.1.1.1.3.1.cmml">−</mo><mn id="S3.E2.m1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S3.E2.m1.2.2.2.2.2.2.4" xref="S3.E2.m1.2.2.2.2.2.3.cmml">,</mo><mrow id="S3.E2.m1.2.2.2.2.2.2.2.1" xref="S3.E2.m1.2.2.2.2.2.2.2.2.cmml"><mo stretchy="false" id="S3.E2.m1.2.2.2.2.2.2.2.1.2" xref="S3.E2.m1.2.2.2.2.2.2.2.2.cmml">{</mo><msubsup id="S3.E2.m1.2.2.2.2.2.2.2.1.1" xref="S3.E2.m1.2.2.2.2.2.2.2.1.1.cmml"><mi id="S3.E2.m1.2.2.2.2.2.2.2.1.1.2.2" xref="S3.E2.m1.2.2.2.2.2.2.2.1.1.2.2.cmml">F</mi><mi id="S3.E2.m1.2.2.2.2.2.2.2.1.1.2.3" xref="S3.E2.m1.2.2.2.2.2.2.2.1.1.2.3.cmml">i</mi><mi id="S3.E2.m1.2.2.2.2.2.2.2.1.1.3" xref="S3.E2.m1.2.2.2.2.2.2.2.1.1.3.cmml">p</mi></msubsup><mo stretchy="false" id="S3.E2.m1.2.2.2.2.2.2.2.1.3" xref="S3.E2.m1.2.2.2.2.2.2.2.2.cmml">}</mo></mrow><mo id="S3.E2.m1.2.2.2.2.2.2.5" xref="S3.E2.m1.2.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.2.2.2.3" xref="S3.E2.m1.2.2.2.3.cmml">+</mo><msub id="S3.E2.m1.2.2.2.4" xref="S3.E2.m1.2.2.2.4.cmml"><mi id="S3.E2.m1.2.2.2.4.2" xref="S3.E2.m1.2.2.2.4.2.cmml">x</mi><mrow id="S3.E2.m1.2.2.2.4.3" xref="S3.E2.m1.2.2.2.4.3.cmml"><mi id="S3.E2.m1.2.2.2.4.3.2" xref="S3.E2.m1.2.2.2.4.3.2.cmml">l</mi><mo id="S3.E2.m1.2.2.2.4.3.1" xref="S3.E2.m1.2.2.2.4.3.1.cmml">−</mo><mn id="S3.E2.m1.2.2.2.4.3.3" xref="S3.E2.m1.2.2.2.4.3.3.cmml">1</mn></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2"><eq id="S3.E2.m1.2.2.3.cmml" xref="S3.E2.m1.2.2.3"></eq><apply id="S3.E2.m1.2.2.4.cmml" xref="S3.E2.m1.2.2.4"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.4.1.cmml" xref="S3.E2.m1.2.2.4">subscript</csymbol><ci id="S3.E2.m1.2.2.4.2.cmml" xref="S3.E2.m1.2.2.4.2">𝑥</ci><ci id="S3.E2.m1.2.2.4.3.cmml" xref="S3.E2.m1.2.2.4.3">𝑙</ci></apply><apply id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2.2"><plus id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.3"></plus><apply id="S3.E2.m1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2"><times id="S3.E2.m1.2.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.2.3"></times><ci id="S3.E2.m1.2.2.2.2.4a.cmml" xref="S3.E2.m1.2.2.2.2.4"><mtext id="S3.E2.m1.2.2.2.2.4.cmml" xref="S3.E2.m1.2.2.2.2.4">PARQ</mtext></ci><interval closure="open" id="S3.E2.m1.2.2.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.2.2.2"><apply id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2">𝑥</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3"><minus id="S3.E2.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3.1"></minus><ci id="S3.E2.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3.2">𝑙</ci><cn type="integer" id="S3.E2.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3.3">1</cn></apply></apply><set id="S3.E2.m1.2.2.2.2.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.1"><apply id="S3.E2.m1.2.2.2.2.2.2.2.1.1.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.2.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.1.1">superscript</csymbol><apply id="S3.E2.m1.2.2.2.2.2.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.2.2.2.1.1.2.1.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.1.1">subscript</csymbol><ci id="S3.E2.m1.2.2.2.2.2.2.2.1.1.2.2.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.1.1.2.2">𝐹</ci><ci id="S3.E2.m1.2.2.2.2.2.2.2.1.1.2.3.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.1.1.2.3">𝑖</ci></apply><ci id="S3.E2.m1.2.2.2.2.2.2.2.1.1.3.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.1.1.3">𝑝</ci></apply></set></interval></apply><apply id="S3.E2.m1.2.2.2.4.cmml" xref="S3.E2.m1.2.2.2.4"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.4.1.cmml" xref="S3.E2.m1.2.2.2.4">subscript</csymbol><ci id="S3.E2.m1.2.2.2.4.2.cmml" xref="S3.E2.m1.2.2.2.4.2">𝑥</ci><apply id="S3.E2.m1.2.2.2.4.3.cmml" xref="S3.E2.m1.2.2.2.4.3"><minus id="S3.E2.m1.2.2.2.4.3.1.cmml" xref="S3.E2.m1.2.2.2.4.3.1"></minus><ci id="S3.E2.m1.2.2.2.4.3.2.cmml" xref="S3.E2.m1.2.2.2.4.3.2">𝑙</ci><cn type="integer" id="S3.E2.m1.2.2.2.4.3.3.cmml" xref="S3.E2.m1.2.2.2.4.3.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">\displaystyle x_{l}=\text{PARQ}\left(x_{l-1},\{F_{i}^{p}\}\right)+x_{l-1}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p2.4" class="ltx_p">Note that all <math id="S3.SS3.p2.3.m1.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS3.p2.3.m1.1a"><mi id="S3.SS3.p2.3.m1.1.1" xref="S3.SS3.p2.3.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m1.1b"><ci id="S3.SS3.p2.3.m1.1.1.cmml" xref="S3.SS3.p2.3.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m1.1c">L</annotation></semantics></math> updates share the same weights in the transformer decoder.
This is unlike DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and PETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> where distinct non-shared layers in the transformer decoder make object predictions.
In addition to sharing weights across updates, we differ from PETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> as we encode the newly predicted object location and its appearance in the query of the subsequent update.
PETR only encodes the initial location of the reference point, <math id="S3.SS3.p2.4.m2.1" class="ltx_Math" alttext="x_{0}" display="inline"><semantics id="S3.SS3.p2.4.m2.1a"><msub id="S3.SS3.p2.4.m2.1.1" xref="S3.SS3.p2.4.m2.1.1.cmml"><mi id="S3.SS3.p2.4.m2.1.1.2" xref="S3.SS3.p2.4.m2.1.1.2.cmml">x</mi><mn id="S3.SS3.p2.4.m2.1.1.3" xref="S3.SS3.p2.4.m2.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m2.1b"><apply id="S3.SS3.p2.4.m2.1.1.cmml" xref="S3.SS3.p2.4.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m2.1.1.1.cmml" xref="S3.SS3.p2.4.m2.1.1">subscript</csymbol><ci id="S3.SS3.p2.4.m2.1.1.2.cmml" xref="S3.SS3.p2.4.m2.1.1.2">𝑥</ci><cn type="integer" id="S3.SS3.p2.4.m2.1.1.3.cmml" xref="S3.SS3.p2.4.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m2.1c">x_{0}</annotation></semantics></math>.
Our experiments show that our design choice is more effective and robust to distribution shifts when sampling reference points.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>3D Detection Head and Objective</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.5" class="ltx_p">The detection head inputs the output of the decoder and makes 3D predictions.
3D objects are represented as 3D bounding boxes with four parameter groups, each predicted by MLPs:
(1) <em id="S3.SS4.p1.5.1" class="ltx_emph ltx_font_italic">center offset</em>: <math id="S3.SS4.p1.1.m1.3" class="ltx_Math" alttext="[\Delta x,\Delta y,\Delta z]\in\mathbb{R}^{3}" display="inline"><semantics id="S3.SS4.p1.1.m1.3a"><mrow id="S3.SS4.p1.1.m1.3.3" xref="S3.SS4.p1.1.m1.3.3.cmml"><mrow id="S3.SS4.p1.1.m1.3.3.3.3" xref="S3.SS4.p1.1.m1.3.3.3.4.cmml"><mo stretchy="false" id="S3.SS4.p1.1.m1.3.3.3.3.4" xref="S3.SS4.p1.1.m1.3.3.3.4.cmml">[</mo><mrow id="S3.SS4.p1.1.m1.1.1.1.1.1" xref="S3.SS4.p1.1.m1.1.1.1.1.1.cmml"><mi mathvariant="normal" id="S3.SS4.p1.1.m1.1.1.1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.1.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.1.m1.1.1.1.1.1.1" xref="S3.SS4.p1.1.m1.1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS4.p1.1.m1.1.1.1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.1.1.1.3.cmml">x</mi></mrow><mo id="S3.SS4.p1.1.m1.3.3.3.3.5" xref="S3.SS4.p1.1.m1.3.3.3.4.cmml">,</mo><mrow id="S3.SS4.p1.1.m1.2.2.2.2.2" xref="S3.SS4.p1.1.m1.2.2.2.2.2.cmml"><mi mathvariant="normal" id="S3.SS4.p1.1.m1.2.2.2.2.2.2" xref="S3.SS4.p1.1.m1.2.2.2.2.2.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.1.m1.2.2.2.2.2.1" xref="S3.SS4.p1.1.m1.2.2.2.2.2.1.cmml">​</mo><mi id="S3.SS4.p1.1.m1.2.2.2.2.2.3" xref="S3.SS4.p1.1.m1.2.2.2.2.2.3.cmml">y</mi></mrow><mo id="S3.SS4.p1.1.m1.3.3.3.3.6" xref="S3.SS4.p1.1.m1.3.3.3.4.cmml">,</mo><mrow id="S3.SS4.p1.1.m1.3.3.3.3.3" xref="S3.SS4.p1.1.m1.3.3.3.3.3.cmml"><mi mathvariant="normal" id="S3.SS4.p1.1.m1.3.3.3.3.3.2" xref="S3.SS4.p1.1.m1.3.3.3.3.3.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.1.m1.3.3.3.3.3.1" xref="S3.SS4.p1.1.m1.3.3.3.3.3.1.cmml">​</mo><mi id="S3.SS4.p1.1.m1.3.3.3.3.3.3" xref="S3.SS4.p1.1.m1.3.3.3.3.3.3.cmml">z</mi></mrow><mo stretchy="false" id="S3.SS4.p1.1.m1.3.3.3.3.7" xref="S3.SS4.p1.1.m1.3.3.3.4.cmml">]</mo></mrow><mo id="S3.SS4.p1.1.m1.3.3.4" xref="S3.SS4.p1.1.m1.3.3.4.cmml">∈</mo><msup id="S3.SS4.p1.1.m1.3.3.5" xref="S3.SS4.p1.1.m1.3.3.5.cmml"><mi id="S3.SS4.p1.1.m1.3.3.5.2" xref="S3.SS4.p1.1.m1.3.3.5.2.cmml">ℝ</mi><mn id="S3.SS4.p1.1.m1.3.3.5.3" xref="S3.SS4.p1.1.m1.3.3.5.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.3b"><apply id="S3.SS4.p1.1.m1.3.3.cmml" xref="S3.SS4.p1.1.m1.3.3"><in id="S3.SS4.p1.1.m1.3.3.4.cmml" xref="S3.SS4.p1.1.m1.3.3.4"></in><list id="S3.SS4.p1.1.m1.3.3.3.4.cmml" xref="S3.SS4.p1.1.m1.3.3.3.3"><apply id="S3.SS4.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1.1.1.1"><times id="S3.SS4.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1.1.1.1.1"></times><ci id="S3.SS4.p1.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.1.1.1.2">Δ</ci><ci id="S3.SS4.p1.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.1.1.1.3">𝑥</ci></apply><apply id="S3.SS4.p1.1.m1.2.2.2.2.2.cmml" xref="S3.SS4.p1.1.m1.2.2.2.2.2"><times id="S3.SS4.p1.1.m1.2.2.2.2.2.1.cmml" xref="S3.SS4.p1.1.m1.2.2.2.2.2.1"></times><ci id="S3.SS4.p1.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS4.p1.1.m1.2.2.2.2.2.2">Δ</ci><ci id="S3.SS4.p1.1.m1.2.2.2.2.2.3.cmml" xref="S3.SS4.p1.1.m1.2.2.2.2.2.3">𝑦</ci></apply><apply id="S3.SS4.p1.1.m1.3.3.3.3.3.cmml" xref="S3.SS4.p1.1.m1.3.3.3.3.3"><times id="S3.SS4.p1.1.m1.3.3.3.3.3.1.cmml" xref="S3.SS4.p1.1.m1.3.3.3.3.3.1"></times><ci id="S3.SS4.p1.1.m1.3.3.3.3.3.2.cmml" xref="S3.SS4.p1.1.m1.3.3.3.3.3.2">Δ</ci><ci id="S3.SS4.p1.1.m1.3.3.3.3.3.3.cmml" xref="S3.SS4.p1.1.m1.3.3.3.3.3.3">𝑧</ci></apply></list><apply id="S3.SS4.p1.1.m1.3.3.5.cmml" xref="S3.SS4.p1.1.m1.3.3.5"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.3.3.5.1.cmml" xref="S3.SS4.p1.1.m1.3.3.5">superscript</csymbol><ci id="S3.SS4.p1.1.m1.3.3.5.2.cmml" xref="S3.SS4.p1.1.m1.3.3.5.2">ℝ</ci><cn type="integer" id="S3.SS4.p1.1.m1.3.3.5.3.cmml" xref="S3.SS4.p1.1.m1.3.3.5.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.3c">[\Delta x,\Delta y,\Delta z]\in\mathbb{R}^{3}</annotation></semantics></math> represents the relative offset of the object center from the reference points, (2)
<em id="S3.SS4.p1.5.2" class="ltx_emph ltx_font_italic">rotation</em>: <math id="S3.SS4.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{p}\in\mathbb{R}^{6}" display="inline"><semantics id="S3.SS4.p1.2.m2.1a"><mrow id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml"><mi id="S3.SS4.p1.2.m2.1.1.2" xref="S3.SS4.p1.2.m2.1.1.2.cmml">𝐩</mi><mo id="S3.SS4.p1.2.m2.1.1.1" xref="S3.SS4.p1.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS4.p1.2.m2.1.1.3" xref="S3.SS4.p1.2.m2.1.1.3.cmml"><mi id="S3.SS4.p1.2.m2.1.1.3.2" xref="S3.SS4.p1.2.m2.1.1.3.2.cmml">ℝ</mi><mn id="S3.SS4.p1.2.m2.1.1.3.3" xref="S3.SS4.p1.2.m2.1.1.3.3.cmml">6</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><apply id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1"><in id="S3.SS4.p1.2.m2.1.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1.1"></in><ci id="S3.SS4.p1.2.m2.1.1.2.cmml" xref="S3.SS4.p1.2.m2.1.1.2">𝐩</ci><apply id="S3.SS4.p1.2.m2.1.1.3.cmml" xref="S3.SS4.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p1.2.m2.1.1.3.1.cmml" xref="S3.SS4.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS4.p1.2.m2.1.1.3.2.cmml" xref="S3.SS4.p1.2.m2.1.1.3.2">ℝ</ci><cn type="integer" id="S3.SS4.p1.2.m2.1.1.3.3.cmml" xref="S3.SS4.p1.2.m2.1.1.3.3">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">\mathbf{p}\in\mathbb{R}^{6}</annotation></semantics></math> represents the continuous 6D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> rotation of the object,
(3) <em id="S3.SS4.p1.5.3" class="ltx_emph ltx_font_italic">object size</em>: <math id="S3.SS4.p1.3.m3.3" class="ltx_Math" alttext="[\hat{w},\hat{h},\hat{l}]" display="inline"><semantics id="S3.SS4.p1.3.m3.3a"><mrow id="S3.SS4.p1.3.m3.3.4.2" xref="S3.SS4.p1.3.m3.3.4.1.cmml"><mo stretchy="false" id="S3.SS4.p1.3.m3.3.4.2.1" xref="S3.SS4.p1.3.m3.3.4.1.cmml">[</mo><mover accent="true" id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml"><mi id="S3.SS4.p1.3.m3.1.1.2" xref="S3.SS4.p1.3.m3.1.1.2.cmml">w</mi><mo id="S3.SS4.p1.3.m3.1.1.1" xref="S3.SS4.p1.3.m3.1.1.1.cmml">^</mo></mover><mo id="S3.SS4.p1.3.m3.3.4.2.2" xref="S3.SS4.p1.3.m3.3.4.1.cmml">,</mo><mover accent="true" id="S3.SS4.p1.3.m3.2.2" xref="S3.SS4.p1.3.m3.2.2.cmml"><mi id="S3.SS4.p1.3.m3.2.2.2" xref="S3.SS4.p1.3.m3.2.2.2.cmml">h</mi><mo id="S3.SS4.p1.3.m3.2.2.1" xref="S3.SS4.p1.3.m3.2.2.1.cmml">^</mo></mover><mo id="S3.SS4.p1.3.m3.3.4.2.3" xref="S3.SS4.p1.3.m3.3.4.1.cmml">,</mo><mover accent="true" id="S3.SS4.p1.3.m3.3.3" xref="S3.SS4.p1.3.m3.3.3.cmml"><mi id="S3.SS4.p1.3.m3.3.3.2" xref="S3.SS4.p1.3.m3.3.3.2.cmml">l</mi><mo id="S3.SS4.p1.3.m3.3.3.1" xref="S3.SS4.p1.3.m3.3.3.1.cmml">^</mo></mover><mo stretchy="false" id="S3.SS4.p1.3.m3.3.4.2.4" xref="S3.SS4.p1.3.m3.3.4.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.3b"><list id="S3.SS4.p1.3.m3.3.4.1.cmml" xref="S3.SS4.p1.3.m3.3.4.2"><apply id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1"><ci id="S3.SS4.p1.3.m3.1.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1.1">^</ci><ci id="S3.SS4.p1.3.m3.1.1.2.cmml" xref="S3.SS4.p1.3.m3.1.1.2">𝑤</ci></apply><apply id="S3.SS4.p1.3.m3.2.2.cmml" xref="S3.SS4.p1.3.m3.2.2"><ci id="S3.SS4.p1.3.m3.2.2.1.cmml" xref="S3.SS4.p1.3.m3.2.2.1">^</ci><ci id="S3.SS4.p1.3.m3.2.2.2.cmml" xref="S3.SS4.p1.3.m3.2.2.2">ℎ</ci></apply><apply id="S3.SS4.p1.3.m3.3.3.cmml" xref="S3.SS4.p1.3.m3.3.3"><ci id="S3.SS4.p1.3.m3.3.3.1.cmml" xref="S3.SS4.p1.3.m3.3.3.1">^</ci><ci id="S3.SS4.p1.3.m3.3.3.2.cmml" xref="S3.SS4.p1.3.m3.3.3.2">𝑙</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.3c">[\hat{w},\hat{h},\hat{l}]</annotation></semantics></math> are the box dimensions log-normalized with category-specific pre-computed means,
and (4) <em id="S3.SS4.p1.5.4" class="ltx_emph ltx_font_italic">object class</em>: <math id="S3.SS4.p1.4.m4.1" class="ltx_Math" alttext="c\in\mathbb{R}^{|\mathcal{C}|+1}" display="inline"><semantics id="S3.SS4.p1.4.m4.1a"><mrow id="S3.SS4.p1.4.m4.1.2" xref="S3.SS4.p1.4.m4.1.2.cmml"><mi id="S3.SS4.p1.4.m4.1.2.2" xref="S3.SS4.p1.4.m4.1.2.2.cmml">c</mi><mo id="S3.SS4.p1.4.m4.1.2.1" xref="S3.SS4.p1.4.m4.1.2.1.cmml">∈</mo><msup id="S3.SS4.p1.4.m4.1.2.3" xref="S3.SS4.p1.4.m4.1.2.3.cmml"><mi id="S3.SS4.p1.4.m4.1.2.3.2" xref="S3.SS4.p1.4.m4.1.2.3.2.cmml">ℝ</mi><mrow id="S3.SS4.p1.4.m4.1.1.1" xref="S3.SS4.p1.4.m4.1.1.1.cmml"><mrow id="S3.SS4.p1.4.m4.1.1.1.3.2" xref="S3.SS4.p1.4.m4.1.1.1.3.1.cmml"><mo stretchy="false" id="S3.SS4.p1.4.m4.1.1.1.3.2.1" xref="S3.SS4.p1.4.m4.1.1.1.3.1.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.4.m4.1.1.1.1" xref="S3.SS4.p1.4.m4.1.1.1.1.cmml">𝒞</mi><mo stretchy="false" id="S3.SS4.p1.4.m4.1.1.1.3.2.2" xref="S3.SS4.p1.4.m4.1.1.1.3.1.1.cmml">|</mo></mrow><mo id="S3.SS4.p1.4.m4.1.1.1.2" xref="S3.SS4.p1.4.m4.1.1.1.2.cmml">+</mo><mn id="S3.SS4.p1.4.m4.1.1.1.4" xref="S3.SS4.p1.4.m4.1.1.1.4.cmml">1</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.4.m4.1b"><apply id="S3.SS4.p1.4.m4.1.2.cmml" xref="S3.SS4.p1.4.m4.1.2"><in id="S3.SS4.p1.4.m4.1.2.1.cmml" xref="S3.SS4.p1.4.m4.1.2.1"></in><ci id="S3.SS4.p1.4.m4.1.2.2.cmml" xref="S3.SS4.p1.4.m4.1.2.2">𝑐</ci><apply id="S3.SS4.p1.4.m4.1.2.3.cmml" xref="S3.SS4.p1.4.m4.1.2.3"><csymbol cd="ambiguous" id="S3.SS4.p1.4.m4.1.2.3.1.cmml" xref="S3.SS4.p1.4.m4.1.2.3">superscript</csymbol><ci id="S3.SS4.p1.4.m4.1.2.3.2.cmml" xref="S3.SS4.p1.4.m4.1.2.3.2">ℝ</ci><apply id="S3.SS4.p1.4.m4.1.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1.1"><plus id="S3.SS4.p1.4.m4.1.1.1.2.cmml" xref="S3.SS4.p1.4.m4.1.1.1.2"></plus><apply id="S3.SS4.p1.4.m4.1.1.1.3.1.cmml" xref="S3.SS4.p1.4.m4.1.1.1.3.2"><abs id="S3.SS4.p1.4.m4.1.1.1.3.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1.1.3.2.1"></abs><ci id="S3.SS4.p1.4.m4.1.1.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1.1.1">𝒞</ci></apply><cn type="integer" id="S3.SS4.p1.4.m4.1.1.1.4.cmml" xref="S3.SS4.p1.4.m4.1.1.1.4">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.4.m4.1c">c\in\mathbb{R}^{|\mathcal{C}|+1}</annotation></semantics></math> are confidence scores across the object classes <math id="S3.SS4.p1.5.m5.1" class="ltx_Math" alttext="\mathcal{C}" display="inline"><semantics id="S3.SS4.p1.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.5.m5.1.1" xref="S3.SS4.p1.5.m5.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.5.m5.1b"><ci id="S3.SS4.p1.5.m5.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.5.m5.1c">\mathcal{C}</annotation></semantics></math> (+1 for no-object).
More details in the supplementary.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.6" class="ltx_p">Following DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, we match predictions to ground truths using the Hungarian algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.
Aside from Hungarian matching, we also match the GT box and the predictions whose corresponding reference points are in close proximity to this GT box (<math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="&lt;0.2m" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><mrow id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml"><mi id="S3.SS4.p2.1.m1.1.1.2" xref="S3.SS4.p2.1.m1.1.1.2.cmml"></mi><mo id="S3.SS4.p2.1.m1.1.1.1" xref="S3.SS4.p2.1.m1.1.1.1.cmml">&lt;</mo><mrow id="S3.SS4.p2.1.m1.1.1.3" xref="S3.SS4.p2.1.m1.1.1.3.cmml"><mn id="S3.SS4.p2.1.m1.1.1.3.2" xref="S3.SS4.p2.1.m1.1.1.3.2.cmml">0.2</mn><mo lspace="0em" rspace="0em" id="S3.SS4.p2.1.m1.1.1.3.1" xref="S3.SS4.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS4.p2.1.m1.1.1.3.3" xref="S3.SS4.p2.1.m1.1.1.3.3.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><apply id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1"><lt id="S3.SS4.p2.1.m1.1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1.1"></lt><csymbol cd="latexml" id="S3.SS4.p2.1.m1.1.1.2.cmml" xref="S3.SS4.p2.1.m1.1.1.2">absent</csymbol><apply id="S3.SS4.p2.1.m1.1.1.3.cmml" xref="S3.SS4.p2.1.m1.1.1.3"><times id="S3.SS4.p2.1.m1.1.1.3.1.cmml" xref="S3.SS4.p2.1.m1.1.1.3.1"></times><cn type="float" id="S3.SS4.p2.1.m1.1.1.3.2.cmml" xref="S3.SS4.p2.1.m1.1.1.3.2">0.2</cn><ci id="S3.SS4.p2.1.m1.1.1.3.3.cmml" xref="S3.SS4.p2.1.m1.1.1.3.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">&lt;0.2m</annotation></semantics></math>), since for two adjacent reference points which have similar queries, they should both detect nearby objects.
We supervise the object detection output in each iteration.
The training objective, <math id="S3.SS4.p2.2.m2.1" class="ltx_Math" alttext="\mathcal{L}" display="inline"><semantics id="S3.SS4.p2.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml">ℒ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><ci id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1">ℒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">\mathcal{L}</annotation></semantics></math>, is a weighted sum of the losses for center offset <math id="S3.SS4.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{L}_{o}" display="inline"><semantics id="S3.SS4.p2.3.m3.1a"><msub id="S3.SS4.p2.3.m3.1.1" xref="S3.SS4.p2.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.3.m3.1.1.2" xref="S3.SS4.p2.3.m3.1.1.2.cmml">ℒ</mi><mi id="S3.SS4.p2.3.m3.1.1.3" xref="S3.SS4.p2.3.m3.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.3.m3.1b"><apply id="S3.SS4.p2.3.m3.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.3.m3.1.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS4.p2.3.m3.1.1.2.cmml" xref="S3.SS4.p2.3.m3.1.1.2">ℒ</ci><ci id="S3.SS4.p2.3.m3.1.1.3.cmml" xref="S3.SS4.p2.3.m3.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.3.m3.1c">\mathcal{L}_{o}</annotation></semantics></math>, rotation <math id="S3.SS4.p2.4.m4.1" class="ltx_Math" alttext="\mathcal{L}_{r}" display="inline"><semantics id="S3.SS4.p2.4.m4.1a"><msub id="S3.SS4.p2.4.m4.1.1" xref="S3.SS4.p2.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.4.m4.1.1.2" xref="S3.SS4.p2.4.m4.1.1.2.cmml">ℒ</mi><mi id="S3.SS4.p2.4.m4.1.1.3" xref="S3.SS4.p2.4.m4.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.4.m4.1b"><apply id="S3.SS4.p2.4.m4.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.4.m4.1.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS4.p2.4.m4.1.1.2.cmml" xref="S3.SS4.p2.4.m4.1.1.2">ℒ</ci><ci id="S3.SS4.p2.4.m4.1.1.3.cmml" xref="S3.SS4.p2.4.m4.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.4.m4.1c">\mathcal{L}_{r}</annotation></semantics></math>, object size <math id="S3.SS4.p2.5.m5.1" class="ltx_Math" alttext="\mathcal{L}_{s}" display="inline"><semantics id="S3.SS4.p2.5.m5.1a"><msub id="S3.SS4.p2.5.m5.1.1" xref="S3.SS4.p2.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.5.m5.1.1.2" xref="S3.SS4.p2.5.m5.1.1.2.cmml">ℒ</mi><mi id="S3.SS4.p2.5.m5.1.1.3" xref="S3.SS4.p2.5.m5.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.5.m5.1b"><apply id="S3.SS4.p2.5.m5.1.1.cmml" xref="S3.SS4.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.5.m5.1.1.1.cmml" xref="S3.SS4.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS4.p2.5.m5.1.1.2.cmml" xref="S3.SS4.p2.5.m5.1.1.2">ℒ</ci><ci id="S3.SS4.p2.5.m5.1.1.3.cmml" xref="S3.SS4.p2.5.m5.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.5.m5.1c">\mathcal{L}_{s}</annotation></semantics></math> and classification <math id="S3.SS4.p2.6.m6.1" class="ltx_Math" alttext="\mathcal{L}_{c}" display="inline"><semantics id="S3.SS4.p2.6.m6.1a"><msub id="S3.SS4.p2.6.m6.1.1" xref="S3.SS4.p2.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.6.m6.1.1.2" xref="S3.SS4.p2.6.m6.1.1.2.cmml">ℒ</mi><mi id="S3.SS4.p2.6.m6.1.1.3" xref="S3.SS4.p2.6.m6.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.6.m6.1b"><apply id="S3.SS4.p2.6.m6.1.1.cmml" xref="S3.SS4.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.6.m6.1.1.1.cmml" xref="S3.SS4.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS4.p2.6.m6.1.1.2.cmml" xref="S3.SS4.p2.6.m6.1.1.2">ℒ</ci><ci id="S3.SS4.p2.6.m6.1.1.3.cmml" xref="S3.SS4.p2.6.m6.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.6.m6.1c">\mathcal{L}_{c}</annotation></semantics></math>:</p>
<table id="S6.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3.m1.1" class="ltx_Math" alttext="\displaystyle\mathcal{L}_{d}=\alpha_{o}\mathcal{L}_{o}+\alpha_{r}\mathcal{L}_{r}+\alpha_{s}\mathcal{L}_{s}+\alpha_{c}\mathcal{L}_{c}." display="inline"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><msub id="S3.E3.m1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.2.2.cmml">ℒ</mi><mi id="S3.E3.m1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.2.3.cmml">d</mi></msub><mo id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.3.cmml"><mrow id="S3.E3.m1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.3.2.cmml"><msub id="S3.E3.m1.1.1.1.1.3.2.2" xref="S3.E3.m1.1.1.1.1.3.2.2.cmml"><mi id="S3.E3.m1.1.1.1.1.3.2.2.2" xref="S3.E3.m1.1.1.1.1.3.2.2.2.cmml">α</mi><mi id="S3.E3.m1.1.1.1.1.3.2.2.3" xref="S3.E3.m1.1.1.1.1.3.2.2.3.cmml">o</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.3.2.1" xref="S3.E3.m1.1.1.1.1.3.2.1.cmml">​</mo><msub id="S3.E3.m1.1.1.1.1.3.2.3" xref="S3.E3.m1.1.1.1.1.3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.1.1.1.1.3.2.3.2" xref="S3.E3.m1.1.1.1.1.3.2.3.2.cmml">ℒ</mi><mi id="S3.E3.m1.1.1.1.1.3.2.3.3" xref="S3.E3.m1.1.1.1.1.3.2.3.3.cmml">o</mi></msub></mrow><mo id="S3.E3.m1.1.1.1.1.3.1" xref="S3.E3.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S3.E3.m1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.3.3.cmml"><msub id="S3.E3.m1.1.1.1.1.3.3.2" xref="S3.E3.m1.1.1.1.1.3.3.2.cmml"><mi id="S3.E3.m1.1.1.1.1.3.3.2.2" xref="S3.E3.m1.1.1.1.1.3.3.2.2.cmml">α</mi><mi id="S3.E3.m1.1.1.1.1.3.3.2.3" xref="S3.E3.m1.1.1.1.1.3.3.2.3.cmml">r</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.3.3.1" xref="S3.E3.m1.1.1.1.1.3.3.1.cmml">​</mo><msub id="S3.E3.m1.1.1.1.1.3.3.3" xref="S3.E3.m1.1.1.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.1.1.1.1.3.3.3.2" xref="S3.E3.m1.1.1.1.1.3.3.3.2.cmml">ℒ</mi><mi id="S3.E3.m1.1.1.1.1.3.3.3.3" xref="S3.E3.m1.1.1.1.1.3.3.3.3.cmml">r</mi></msub></mrow><mo id="S3.E3.m1.1.1.1.1.3.1a" xref="S3.E3.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S3.E3.m1.1.1.1.1.3.4" xref="S3.E3.m1.1.1.1.1.3.4.cmml"><msub id="S3.E3.m1.1.1.1.1.3.4.2" xref="S3.E3.m1.1.1.1.1.3.4.2.cmml"><mi id="S3.E3.m1.1.1.1.1.3.4.2.2" xref="S3.E3.m1.1.1.1.1.3.4.2.2.cmml">α</mi><mi id="S3.E3.m1.1.1.1.1.3.4.2.3" xref="S3.E3.m1.1.1.1.1.3.4.2.3.cmml">s</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.3.4.1" xref="S3.E3.m1.1.1.1.1.3.4.1.cmml">​</mo><msub id="S3.E3.m1.1.1.1.1.3.4.3" xref="S3.E3.m1.1.1.1.1.3.4.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.1.1.1.1.3.4.3.2" xref="S3.E3.m1.1.1.1.1.3.4.3.2.cmml">ℒ</mi><mi id="S3.E3.m1.1.1.1.1.3.4.3.3" xref="S3.E3.m1.1.1.1.1.3.4.3.3.cmml">s</mi></msub></mrow><mo id="S3.E3.m1.1.1.1.1.3.1b" xref="S3.E3.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S3.E3.m1.1.1.1.1.3.5" xref="S3.E3.m1.1.1.1.1.3.5.cmml"><msub id="S3.E3.m1.1.1.1.1.3.5.2" xref="S3.E3.m1.1.1.1.1.3.5.2.cmml"><mi id="S3.E3.m1.1.1.1.1.3.5.2.2" xref="S3.E3.m1.1.1.1.1.3.5.2.2.cmml">α</mi><mi id="S3.E3.m1.1.1.1.1.3.5.2.3" xref="S3.E3.m1.1.1.1.1.3.5.2.3.cmml">c</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.3.5.1" xref="S3.E3.m1.1.1.1.1.3.5.1.cmml">​</mo><msub id="S3.E3.m1.1.1.1.1.3.5.3" xref="S3.E3.m1.1.1.1.1.3.5.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.1.1.1.1.3.5.3.2" xref="S3.E3.m1.1.1.1.1.3.5.3.2.cmml">ℒ</mi><mi id="S3.E3.m1.1.1.1.1.3.5.3.3" xref="S3.E3.m1.1.1.1.1.3.5.3.3.cmml">c</mi></msub></mrow></mrow></mrow><mo lspace="0em" id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><eq id="S3.E3.m1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"></eq><apply id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2">ℒ</ci><ci id="S3.E3.m1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.3">𝑑</ci></apply><apply id="S3.E3.m1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3"><plus id="S3.E3.m1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.3.1"></plus><apply id="S3.E3.m1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.3.2"><times id="S3.E3.m1.1.1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.1.1.3.2.1"></times><apply id="S3.E3.m1.1.1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.3.2.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.3.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.3.2.2.2">𝛼</ci><ci id="S3.E3.m1.1.1.1.1.3.2.2.3.cmml" xref="S3.E3.m1.1.1.1.1.3.2.2.3">𝑜</ci></apply><apply id="S3.E3.m1.1.1.1.1.3.2.3.cmml" xref="S3.E3.m1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.2.3.1.cmml" xref="S3.E3.m1.1.1.1.1.3.2.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.3.2.3.2.cmml" xref="S3.E3.m1.1.1.1.1.3.2.3.2">ℒ</ci><ci id="S3.E3.m1.1.1.1.1.3.2.3.3.cmml" xref="S3.E3.m1.1.1.1.1.3.2.3.3">𝑜</ci></apply></apply><apply id="S3.E3.m1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3"><times id="S3.E3.m1.1.1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.1.1.3.3.1"></times><apply id="S3.E3.m1.1.1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.3.2.1.cmml" xref="S3.E3.m1.1.1.1.1.3.3.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.3.3.2.2.cmml" xref="S3.E3.m1.1.1.1.1.3.3.2.2">𝛼</ci><ci id="S3.E3.m1.1.1.1.1.3.3.2.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3.2.3">𝑟</ci></apply><apply id="S3.E3.m1.1.1.1.1.3.3.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.3.3.1.cmml" xref="S3.E3.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.3.3.3.2.cmml" xref="S3.E3.m1.1.1.1.1.3.3.3.2">ℒ</ci><ci id="S3.E3.m1.1.1.1.1.3.3.3.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3.3.3">𝑟</ci></apply></apply><apply id="S3.E3.m1.1.1.1.1.3.4.cmml" xref="S3.E3.m1.1.1.1.1.3.4"><times id="S3.E3.m1.1.1.1.1.3.4.1.cmml" xref="S3.E3.m1.1.1.1.1.3.4.1"></times><apply id="S3.E3.m1.1.1.1.1.3.4.2.cmml" xref="S3.E3.m1.1.1.1.1.3.4.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.4.2.1.cmml" xref="S3.E3.m1.1.1.1.1.3.4.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.3.4.2.2.cmml" xref="S3.E3.m1.1.1.1.1.3.4.2.2">𝛼</ci><ci id="S3.E3.m1.1.1.1.1.3.4.2.3.cmml" xref="S3.E3.m1.1.1.1.1.3.4.2.3">𝑠</ci></apply><apply id="S3.E3.m1.1.1.1.1.3.4.3.cmml" xref="S3.E3.m1.1.1.1.1.3.4.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.4.3.1.cmml" xref="S3.E3.m1.1.1.1.1.3.4.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.3.4.3.2.cmml" xref="S3.E3.m1.1.1.1.1.3.4.3.2">ℒ</ci><ci id="S3.E3.m1.1.1.1.1.3.4.3.3.cmml" xref="S3.E3.m1.1.1.1.1.3.4.3.3">𝑠</ci></apply></apply><apply id="S3.E3.m1.1.1.1.1.3.5.cmml" xref="S3.E3.m1.1.1.1.1.3.5"><times id="S3.E3.m1.1.1.1.1.3.5.1.cmml" xref="S3.E3.m1.1.1.1.1.3.5.1"></times><apply id="S3.E3.m1.1.1.1.1.3.5.2.cmml" xref="S3.E3.m1.1.1.1.1.3.5.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.5.2.1.cmml" xref="S3.E3.m1.1.1.1.1.3.5.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.3.5.2.2.cmml" xref="S3.E3.m1.1.1.1.1.3.5.2.2">𝛼</ci><ci id="S3.E3.m1.1.1.1.1.3.5.2.3.cmml" xref="S3.E3.m1.1.1.1.1.3.5.2.3">𝑐</ci></apply><apply id="S3.E3.m1.1.1.1.1.3.5.3.cmml" xref="S3.E3.m1.1.1.1.1.3.5.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.5.3.1.cmml" xref="S3.E3.m1.1.1.1.1.3.5.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.3.5.3.2.cmml" xref="S3.E3.m1.1.1.1.1.3.5.3.2">ℒ</ci><ci id="S3.E3.m1.1.1.1.1.3.5.3.3.cmml" xref="S3.E3.m1.1.1.1.1.3.5.3.3">𝑐</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\displaystyle\mathcal{L}_{d}=\alpha_{o}\mathcal{L}_{o}+\alpha_{r}\mathcal{L}_{r}+\alpha_{s}\mathcal{L}_{s}+\alpha_{c}\mathcal{L}_{c}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.p2.16" class="ltx_p"><math id="S3.SS4.p2.7.m1.1" class="ltx_Math" alttext="\mathcal{L}_{o}" display="inline"><semantics id="S3.SS4.p2.7.m1.1a"><msub id="S3.SS4.p2.7.m1.1.1" xref="S3.SS4.p2.7.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.7.m1.1.1.2" xref="S3.SS4.p2.7.m1.1.1.2.cmml">ℒ</mi><mi id="S3.SS4.p2.7.m1.1.1.3" xref="S3.SS4.p2.7.m1.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.7.m1.1b"><apply id="S3.SS4.p2.7.m1.1.1.cmml" xref="S3.SS4.p2.7.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.7.m1.1.1.1.cmml" xref="S3.SS4.p2.7.m1.1.1">subscript</csymbol><ci id="S3.SS4.p2.7.m1.1.1.2.cmml" xref="S3.SS4.p2.7.m1.1.1.2">ℒ</ci><ci id="S3.SS4.p2.7.m1.1.1.3.cmml" xref="S3.SS4.p2.7.m1.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.7.m1.1c">\mathcal{L}_{o}</annotation></semantics></math> and <math id="S3.SS4.p2.8.m2.1" class="ltx_Math" alttext="\mathcal{L}_{s}" display="inline"><semantics id="S3.SS4.p2.8.m2.1a"><msub id="S3.SS4.p2.8.m2.1.1" xref="S3.SS4.p2.8.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.8.m2.1.1.2" xref="S3.SS4.p2.8.m2.1.1.2.cmml">ℒ</mi><mi id="S3.SS4.p2.8.m2.1.1.3" xref="S3.SS4.p2.8.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.8.m2.1b"><apply id="S3.SS4.p2.8.m2.1.1.cmml" xref="S3.SS4.p2.8.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.8.m2.1.1.1.cmml" xref="S3.SS4.p2.8.m2.1.1">subscript</csymbol><ci id="S3.SS4.p2.8.m2.1.1.2.cmml" xref="S3.SS4.p2.8.m2.1.1.2">ℒ</ci><ci id="S3.SS4.p2.8.m2.1.1.3.cmml" xref="S3.SS4.p2.8.m2.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.8.m2.1c">\mathcal{L}_{s}</annotation></semantics></math> is an <math id="S3.SS4.p2.9.m3.1" class="ltx_Math" alttext="L1" display="inline"><semantics id="S3.SS4.p2.9.m3.1a"><mrow id="S3.SS4.p2.9.m3.1.1" xref="S3.SS4.p2.9.m3.1.1.cmml"><mi id="S3.SS4.p2.9.m3.1.1.2" xref="S3.SS4.p2.9.m3.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p2.9.m3.1.1.1" xref="S3.SS4.p2.9.m3.1.1.1.cmml">​</mo><mn id="S3.SS4.p2.9.m3.1.1.3" xref="S3.SS4.p2.9.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.9.m3.1b"><apply id="S3.SS4.p2.9.m3.1.1.cmml" xref="S3.SS4.p2.9.m3.1.1"><times id="S3.SS4.p2.9.m3.1.1.1.cmml" xref="S3.SS4.p2.9.m3.1.1.1"></times><ci id="S3.SS4.p2.9.m3.1.1.2.cmml" xref="S3.SS4.p2.9.m3.1.1.2">𝐿</ci><cn type="integer" id="S3.SS4.p2.9.m3.1.1.3.cmml" xref="S3.SS4.p2.9.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.9.m3.1c">L1</annotation></semantics></math> loss, <math id="S3.SS4.p2.10.m4.1" class="ltx_Math" alttext="\mathcal{L}_{r}" display="inline"><semantics id="S3.SS4.p2.10.m4.1a"><msub id="S3.SS4.p2.10.m4.1.1" xref="S3.SS4.p2.10.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.10.m4.1.1.2" xref="S3.SS4.p2.10.m4.1.1.2.cmml">ℒ</mi><mi id="S3.SS4.p2.10.m4.1.1.3" xref="S3.SS4.p2.10.m4.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.10.m4.1b"><apply id="S3.SS4.p2.10.m4.1.1.cmml" xref="S3.SS4.p2.10.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.10.m4.1.1.1.cmml" xref="S3.SS4.p2.10.m4.1.1">subscript</csymbol><ci id="S3.SS4.p2.10.m4.1.1.2.cmml" xref="S3.SS4.p2.10.m4.1.1.2">ℒ</ci><ci id="S3.SS4.p2.10.m4.1.1.3.cmml" xref="S3.SS4.p2.10.m4.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.10.m4.1c">\mathcal{L}_{r}</annotation></semantics></math> is an <math id="S3.SS4.p2.11.m5.1" class="ltx_Math" alttext="L2" display="inline"><semantics id="S3.SS4.p2.11.m5.1a"><mrow id="S3.SS4.p2.11.m5.1.1" xref="S3.SS4.p2.11.m5.1.1.cmml"><mi id="S3.SS4.p2.11.m5.1.1.2" xref="S3.SS4.p2.11.m5.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p2.11.m5.1.1.1" xref="S3.SS4.p2.11.m5.1.1.1.cmml">​</mo><mn id="S3.SS4.p2.11.m5.1.1.3" xref="S3.SS4.p2.11.m5.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.11.m5.1b"><apply id="S3.SS4.p2.11.m5.1.1.cmml" xref="S3.SS4.p2.11.m5.1.1"><times id="S3.SS4.p2.11.m5.1.1.1.cmml" xref="S3.SS4.p2.11.m5.1.1.1"></times><ci id="S3.SS4.p2.11.m5.1.1.2.cmml" xref="S3.SS4.p2.11.m5.1.1.2">𝐿</ci><cn type="integer" id="S3.SS4.p2.11.m5.1.1.3.cmml" xref="S3.SS4.p2.11.m5.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.11.m5.1c">L2</annotation></semantics></math> loss, and <math id="S3.SS4.p2.12.m6.1" class="ltx_Math" alttext="\mathcal{L}_{c}" display="inline"><semantics id="S3.SS4.p2.12.m6.1a"><msub id="S3.SS4.p2.12.m6.1.1" xref="S3.SS4.p2.12.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.12.m6.1.1.2" xref="S3.SS4.p2.12.m6.1.1.2.cmml">ℒ</mi><mi id="S3.SS4.p2.12.m6.1.1.3" xref="S3.SS4.p2.12.m6.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.12.m6.1b"><apply id="S3.SS4.p2.12.m6.1.1.cmml" xref="S3.SS4.p2.12.m6.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.12.m6.1.1.1.cmml" xref="S3.SS4.p2.12.m6.1.1">subscript</csymbol><ci id="S3.SS4.p2.12.m6.1.1.2.cmml" xref="S3.SS4.p2.12.m6.1.1.2">ℒ</ci><ci id="S3.SS4.p2.12.m6.1.1.3.cmml" xref="S3.SS4.p2.12.m6.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.12.m6.1c">\mathcal{L}_{c}</annotation></semantics></math> is the cross entropy loss.
We set <math id="S3.SS4.p2.13.m7.1" class="ltx_Math" alttext="\alpha_{o}" display="inline"><semantics id="S3.SS4.p2.13.m7.1a"><msub id="S3.SS4.p2.13.m7.1.1" xref="S3.SS4.p2.13.m7.1.1.cmml"><mi id="S3.SS4.p2.13.m7.1.1.2" xref="S3.SS4.p2.13.m7.1.1.2.cmml">α</mi><mi id="S3.SS4.p2.13.m7.1.1.3" xref="S3.SS4.p2.13.m7.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.13.m7.1b"><apply id="S3.SS4.p2.13.m7.1.1.cmml" xref="S3.SS4.p2.13.m7.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.13.m7.1.1.1.cmml" xref="S3.SS4.p2.13.m7.1.1">subscript</csymbol><ci id="S3.SS4.p2.13.m7.1.1.2.cmml" xref="S3.SS4.p2.13.m7.1.1.2">𝛼</ci><ci id="S3.SS4.p2.13.m7.1.1.3.cmml" xref="S3.SS4.p2.13.m7.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.13.m7.1c">\alpha_{o}</annotation></semantics></math>, <math id="S3.SS4.p2.14.m8.1" class="ltx_Math" alttext="\alpha_{s}" display="inline"><semantics id="S3.SS4.p2.14.m8.1a"><msub id="S3.SS4.p2.14.m8.1.1" xref="S3.SS4.p2.14.m8.1.1.cmml"><mi id="S3.SS4.p2.14.m8.1.1.2" xref="S3.SS4.p2.14.m8.1.1.2.cmml">α</mi><mi id="S3.SS4.p2.14.m8.1.1.3" xref="S3.SS4.p2.14.m8.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.14.m8.1b"><apply id="S3.SS4.p2.14.m8.1.1.cmml" xref="S3.SS4.p2.14.m8.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.14.m8.1.1.1.cmml" xref="S3.SS4.p2.14.m8.1.1">subscript</csymbol><ci id="S3.SS4.p2.14.m8.1.1.2.cmml" xref="S3.SS4.p2.14.m8.1.1.2">𝛼</ci><ci id="S3.SS4.p2.14.m8.1.1.3.cmml" xref="S3.SS4.p2.14.m8.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.14.m8.1c">\alpha_{s}</annotation></semantics></math>, <math id="S3.SS4.p2.15.m9.1" class="ltx_Math" alttext="\alpha_{r}" display="inline"><semantics id="S3.SS4.p2.15.m9.1a"><msub id="S3.SS4.p2.15.m9.1.1" xref="S3.SS4.p2.15.m9.1.1.cmml"><mi id="S3.SS4.p2.15.m9.1.1.2" xref="S3.SS4.p2.15.m9.1.1.2.cmml">α</mi><mi id="S3.SS4.p2.15.m9.1.1.3" xref="S3.SS4.p2.15.m9.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.15.m9.1b"><apply id="S3.SS4.p2.15.m9.1.1.cmml" xref="S3.SS4.p2.15.m9.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.15.m9.1.1.1.cmml" xref="S3.SS4.p2.15.m9.1.1">subscript</csymbol><ci id="S3.SS4.p2.15.m9.1.1.2.cmml" xref="S3.SS4.p2.15.m9.1.1.2">𝛼</ci><ci id="S3.SS4.p2.15.m9.1.1.3.cmml" xref="S3.SS4.p2.15.m9.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.15.m9.1c">\alpha_{r}</annotation></semantics></math> to 5.0 and <math id="S3.SS4.p2.16.m10.1" class="ltx_Math" alttext="\alpha_{c}" display="inline"><semantics id="S3.SS4.p2.16.m10.1a"><msub id="S3.SS4.p2.16.m10.1.1" xref="S3.SS4.p2.16.m10.1.1.cmml"><mi id="S3.SS4.p2.16.m10.1.1.2" xref="S3.SS4.p2.16.m10.1.1.2.cmml">α</mi><mi id="S3.SS4.p2.16.m10.1.1.3" xref="S3.SS4.p2.16.m10.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.16.m10.1b"><apply id="S3.SS4.p2.16.m10.1.1.cmml" xref="S3.SS4.p2.16.m10.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.16.m10.1.1.1.cmml" xref="S3.SS4.p2.16.m10.1.1">subscript</csymbol><ci id="S3.SS4.p2.16.m10.1.1.2.cmml" xref="S3.SS4.p2.16.m10.1.1.2">𝛼</ci><ci id="S3.SS4.p2.16.m10.1.1.3.cmml" xref="S3.SS4.p2.16.m10.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.16.m10.1c">\alpha_{c}</annotation></semantics></math> to 1.0.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Implementation Details</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.8" class="ltx_p">For the image backbone we use ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, pretrained on ImageNet, integrated with a feature pyramid network (FPN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
Unless otherwise stated, we use 3-frame snippets (<math id="S3.SS5.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS5.p1.1.m1.1a"><mi id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><ci id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">N</annotation></semantics></math>=3), 8 recurrent updates during decoding (<math id="S3.SS5.p1.2.m2.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS5.p1.2.m2.1a"><mi id="S3.SS5.p1.2.m2.1.1" xref="S3.SS5.p1.2.m2.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.2.m2.1b"><ci id="S3.SS5.p1.2.m2.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.2.m2.1c">L</annotation></semantics></math>=8), and sample 256 reference points.
We use the same number of queries for baselines <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>.
The dimension of query and image features is 1024.
In the transformer decoder layer, we use 4 decoder heads, and the feedforward dimension is 768. The dropout rate is 0.1.
The camera coordinates of the middle snippet frame define the snippet coordinate system.
All 3D predictions are defined with reference to that snippet coordinate system.
We define the bounding region to sample 3D points of size <math id="S3.SS5.p1.3.m3.1" class="ltx_Math" alttext="6m\times 2.5m\times 5m" display="inline"><semantics id="S3.SS5.p1.3.m3.1a"><mrow id="S3.SS5.p1.3.m3.1.1" xref="S3.SS5.p1.3.m3.1.1.cmml"><mrow id="S3.SS5.p1.3.m3.1.1.2" xref="S3.SS5.p1.3.m3.1.1.2.cmml"><mrow id="S3.SS5.p1.3.m3.1.1.2.2" xref="S3.SS5.p1.3.m3.1.1.2.2.cmml"><mrow id="S3.SS5.p1.3.m3.1.1.2.2.2" xref="S3.SS5.p1.3.m3.1.1.2.2.2.cmml"><mrow id="S3.SS5.p1.3.m3.1.1.2.2.2.2" xref="S3.SS5.p1.3.m3.1.1.2.2.2.2.cmml"><mn id="S3.SS5.p1.3.m3.1.1.2.2.2.2.2" xref="S3.SS5.p1.3.m3.1.1.2.2.2.2.2.cmml">6</mn><mo lspace="0em" rspace="0em" id="S3.SS5.p1.3.m3.1.1.2.2.2.2.1" xref="S3.SS5.p1.3.m3.1.1.2.2.2.2.1.cmml">​</mo><mi id="S3.SS5.p1.3.m3.1.1.2.2.2.2.3" xref="S3.SS5.p1.3.m3.1.1.2.2.2.2.3.cmml">m</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S3.SS5.p1.3.m3.1.1.2.2.2.1" xref="S3.SS5.p1.3.m3.1.1.2.2.2.1.cmml">×</mo><mn id="S3.SS5.p1.3.m3.1.1.2.2.2.3" xref="S3.SS5.p1.3.m3.1.1.2.2.2.3.cmml">2.5</mn></mrow><mo lspace="0em" rspace="0em" id="S3.SS5.p1.3.m3.1.1.2.2.1" xref="S3.SS5.p1.3.m3.1.1.2.2.1.cmml">​</mo><mi id="S3.SS5.p1.3.m3.1.1.2.2.3" xref="S3.SS5.p1.3.m3.1.1.2.2.3.cmml">m</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S3.SS5.p1.3.m3.1.1.2.1" xref="S3.SS5.p1.3.m3.1.1.2.1.cmml">×</mo><mn id="S3.SS5.p1.3.m3.1.1.2.3" xref="S3.SS5.p1.3.m3.1.1.2.3.cmml">5</mn></mrow><mo lspace="0em" rspace="0em" id="S3.SS5.p1.3.m3.1.1.1" xref="S3.SS5.p1.3.m3.1.1.1.cmml">​</mo><mi id="S3.SS5.p1.3.m3.1.1.3" xref="S3.SS5.p1.3.m3.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.3.m3.1b"><apply id="S3.SS5.p1.3.m3.1.1.cmml" xref="S3.SS5.p1.3.m3.1.1"><times id="S3.SS5.p1.3.m3.1.1.1.cmml" xref="S3.SS5.p1.3.m3.1.1.1"></times><apply id="S3.SS5.p1.3.m3.1.1.2.cmml" xref="S3.SS5.p1.3.m3.1.1.2"><times id="S3.SS5.p1.3.m3.1.1.2.1.cmml" xref="S3.SS5.p1.3.m3.1.1.2.1"></times><apply id="S3.SS5.p1.3.m3.1.1.2.2.cmml" xref="S3.SS5.p1.3.m3.1.1.2.2"><times id="S3.SS5.p1.3.m3.1.1.2.2.1.cmml" xref="S3.SS5.p1.3.m3.1.1.2.2.1"></times><apply id="S3.SS5.p1.3.m3.1.1.2.2.2.cmml" xref="S3.SS5.p1.3.m3.1.1.2.2.2"><times id="S3.SS5.p1.3.m3.1.1.2.2.2.1.cmml" xref="S3.SS5.p1.3.m3.1.1.2.2.2.1"></times><apply id="S3.SS5.p1.3.m3.1.1.2.2.2.2.cmml" xref="S3.SS5.p1.3.m3.1.1.2.2.2.2"><times id="S3.SS5.p1.3.m3.1.1.2.2.2.2.1.cmml" xref="S3.SS5.p1.3.m3.1.1.2.2.2.2.1"></times><cn type="integer" id="S3.SS5.p1.3.m3.1.1.2.2.2.2.2.cmml" xref="S3.SS5.p1.3.m3.1.1.2.2.2.2.2">6</cn><ci id="S3.SS5.p1.3.m3.1.1.2.2.2.2.3.cmml" xref="S3.SS5.p1.3.m3.1.1.2.2.2.2.3">𝑚</ci></apply><cn type="float" id="S3.SS5.p1.3.m3.1.1.2.2.2.3.cmml" xref="S3.SS5.p1.3.m3.1.1.2.2.2.3">2.5</cn></apply><ci id="S3.SS5.p1.3.m3.1.1.2.2.3.cmml" xref="S3.SS5.p1.3.m3.1.1.2.2.3">𝑚</ci></apply><cn type="integer" id="S3.SS5.p1.3.m3.1.1.2.3.cmml" xref="S3.SS5.p1.3.m3.1.1.2.3">5</cn></apply><ci id="S3.SS5.p1.3.m3.1.1.3.cmml" xref="S3.SS5.p1.3.m3.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.3.m3.1c">6m\times 2.5m\times 5m</annotation></semantics></math> aligned with the snippet coordinate system.
This volume contains <math id="S3.SS5.p1.4.m4.1" class="ltx_Math" alttext="93.7\%" display="inline"><semantics id="S3.SS5.p1.4.m4.1a"><mrow id="S3.SS5.p1.4.m4.1.1" xref="S3.SS5.p1.4.m4.1.1.cmml"><mn id="S3.SS5.p1.4.m4.1.1.2" xref="S3.SS5.p1.4.m4.1.1.2.cmml">93.7</mn><mo id="S3.SS5.p1.4.m4.1.1.1" xref="S3.SS5.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.4.m4.1b"><apply id="S3.SS5.p1.4.m4.1.1.cmml" xref="S3.SS5.p1.4.m4.1.1"><csymbol cd="latexml" id="S3.SS5.p1.4.m4.1.1.1.cmml" xref="S3.SS5.p1.4.m4.1.1.1">percent</csymbol><cn type="float" id="S3.SS5.p1.4.m4.1.1.2.cmml" xref="S3.SS5.p1.4.m4.1.1.2">93.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.4.m4.1c">93.7\%</annotation></semantics></math> of training box centers on ScanNet.
We implement our model using PyTorch <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and train across 8 NVIDIA A5000 GPUs with a batch size <math id="S3.SS5.p1.5.m5.1" class="ltx_Math" alttext="b=16" display="inline"><semantics id="S3.SS5.p1.5.m5.1a"><mrow id="S3.SS5.p1.5.m5.1.1" xref="S3.SS5.p1.5.m5.1.1.cmml"><mi id="S3.SS5.p1.5.m5.1.1.2" xref="S3.SS5.p1.5.m5.1.1.2.cmml">b</mi><mo id="S3.SS5.p1.5.m5.1.1.1" xref="S3.SS5.p1.5.m5.1.1.1.cmml">=</mo><mn id="S3.SS5.p1.5.m5.1.1.3" xref="S3.SS5.p1.5.m5.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.5.m5.1b"><apply id="S3.SS5.p1.5.m5.1.1.cmml" xref="S3.SS5.p1.5.m5.1.1"><eq id="S3.SS5.p1.5.m5.1.1.1.cmml" xref="S3.SS5.p1.5.m5.1.1.1"></eq><ci id="S3.SS5.p1.5.m5.1.1.2.cmml" xref="S3.SS5.p1.5.m5.1.1.2">𝑏</ci><cn type="integer" id="S3.SS5.p1.5.m5.1.1.3.cmml" xref="S3.SS5.p1.5.m5.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.5.m5.1c">b=16</annotation></semantics></math> (<math id="S3.SS5.p1.6.m6.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S3.SS5.p1.6.m6.1a"><mn id="S3.SS5.p1.6.m6.1.1" xref="S3.SS5.p1.6.m6.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.6.m6.1b"><cn type="integer" id="S3.SS5.p1.6.m6.1.1.cmml" xref="S3.SS5.p1.6.m6.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.6.m6.1c">2</annotation></semantics></math> per GPU).
We use the AdamW optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> and an initial learning rate of <math id="S3.SS5.p1.7.m7.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="S3.SS5.p1.7.m7.1a"><msup id="S3.SS5.p1.7.m7.1.1" xref="S3.SS5.p1.7.m7.1.1.cmml"><mn id="S3.SS5.p1.7.m7.1.1.2" xref="S3.SS5.p1.7.m7.1.1.2.cmml">10</mn><mrow id="S3.SS5.p1.7.m7.1.1.3" xref="S3.SS5.p1.7.m7.1.1.3.cmml"><mo id="S3.SS5.p1.7.m7.1.1.3a" xref="S3.SS5.p1.7.m7.1.1.3.cmml">−</mo><mn id="S3.SS5.p1.7.m7.1.1.3.2" xref="S3.SS5.p1.7.m7.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.7.m7.1b"><apply id="S3.SS5.p1.7.m7.1.1.cmml" xref="S3.SS5.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.7.m7.1.1.1.cmml" xref="S3.SS5.p1.7.m7.1.1">superscript</csymbol><cn type="integer" id="S3.SS5.p1.7.m7.1.1.2.cmml" xref="S3.SS5.p1.7.m7.1.1.2">10</cn><apply id="S3.SS5.p1.7.m7.1.1.3.cmml" xref="S3.SS5.p1.7.m7.1.1.3"><minus id="S3.SS5.p1.7.m7.1.1.3.1.cmml" xref="S3.SS5.p1.7.m7.1.1.3"></minus><cn type="integer" id="S3.SS5.p1.7.m7.1.1.3.2.cmml" xref="S3.SS5.p1.7.m7.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.7.m7.1c">10^{-4}</annotation></semantics></math>.
We scale the learning rate by <math id="S3.SS5.p1.8.m8.1" class="ltx_Math" alttext="b/256" display="inline"><semantics id="S3.SS5.p1.8.m8.1a"><mrow id="S3.SS5.p1.8.m8.1.1" xref="S3.SS5.p1.8.m8.1.1.cmml"><mi id="S3.SS5.p1.8.m8.1.1.2" xref="S3.SS5.p1.8.m8.1.1.2.cmml">b</mi><mo id="S3.SS5.p1.8.m8.1.1.1" xref="S3.SS5.p1.8.m8.1.1.1.cmml">/</mo><mn id="S3.SS5.p1.8.m8.1.1.3" xref="S3.SS5.p1.8.m8.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.8.m8.1b"><apply id="S3.SS5.p1.8.m8.1.1.cmml" xref="S3.SS5.p1.8.m8.1.1"><divide id="S3.SS5.p1.8.m8.1.1.1.cmml" xref="S3.SS5.p1.8.m8.1.1.1"></divide><ci id="S3.SS5.p1.8.m8.1.1.2.cmml" xref="S3.SS5.p1.8.m8.1.1.2">𝑏</ci><cn type="integer" id="S3.SS5.p1.8.m8.1.1.3.cmml" xref="S3.SS5.p1.8.m8.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.8.m8.1c">b/256</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and use a cosine annealing schedule <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.
Algorithm <a href="#alg1" title="Algorithm 1 ‣ 3.1 3D-Aware Input Encoding ‣ 3 Method ‣ Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> provides the pseudo-code for PARQ’s training procedure.
See more implementation details in the supplementary.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:422.3pt;height:206.6pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-139.4pt,68.0pt) scale(0.602279952694761,0.602279952694761) ;">
<table id="S4.T1.3.3" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1.1" class="ltx_td ltx_align_left">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span>
@IoU <math id="S4.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.T1.1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><gt id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">&gt;</annotation></semantics></math> 0.25</td>
<td id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center">chair</td>
<td id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center">table</td>
<td id="S4.T1.1.1.1.4" class="ltx_td ltx_align_center">cabinet</td>
<td id="S4.T1.1.1.1.5" class="ltx_td ltx_align_center">trashbin</td>
<td id="S4.T1.1.1.1.6" class="ltx_td ltx_align_center">bookshelf</td>
<td id="S4.T1.1.1.1.7" class="ltx_td ltx_align_center">display</td>
<td id="S4.T1.1.1.1.8" class="ltx_td ltx_align_center">sofa</td>
<td id="S4.T1.1.1.1.9" class="ltx_td ltx_align_center">bathtub</td>
<td id="S4.T1.1.1.1.10" class="ltx_td ltx_align_center">other</td>
<td id="S4.T1.1.1.1.11" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T1.1.1.1.11.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">average</span></td>
</tr>
<tr id="S4.T1.3.3.4" class="ltx_tr">
<td id="S4.T1.3.3.4.1" class="ltx_td ltx_align_left ltx_border_t">ODAM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>
</td>
<td id="S4.T1.3.3.4.2" class="ltx_td ltx_align_center ltx_border_t">50.6</td>
<td id="S4.T1.3.3.4.3" class="ltx_td ltx_align_center ltx_border_t">42.5</td>
<td id="S4.T1.3.3.4.4" class="ltx_td ltx_align_center ltx_border_t">9.3</td>
<td id="S4.T1.3.3.4.5" class="ltx_td ltx_align_center ltx_border_t">32</td>
<td id="S4.T1.3.3.4.6" class="ltx_td ltx_align_center ltx_border_t">19.9</td>
<td id="S4.T1.3.3.4.7" class="ltx_td ltx_align_center ltx_border_t">14.8</td>
<td id="S4.T1.3.3.4.8" class="ltx_td ltx_align_center ltx_border_t">39.8</td>
<td id="S4.T1.3.3.4.9" class="ltx_td ltx_align_center ltx_border_t">28.5</td>
<td id="S4.T1.3.3.4.10" class="ltx_td ltx_align_center ltx_border_t">0.0</td>
<td id="S4.T1.3.3.4.11" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T1.3.3.4.11.1" class="ltx_text" style="background-color:#E6E6E6;">33.0/47.1/38.8</span></td>
</tr>
<tr id="S4.T1.3.3.5" class="ltx_tr">
<td id="S4.T1.3.3.5.1" class="ltx_td ltx_align_left">ImVoxelNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</td>
<td id="S4.T1.3.3.5.2" class="ltx_td ltx_align_center">66.0</td>
<td id="S4.T1.3.3.5.3" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.5.3.1" class="ltx_text ltx_font_bold">55.8</span></td>
<td id="S4.T1.3.3.5.4" class="ltx_td ltx_align_center">44.2</td>
<td id="S4.T1.3.3.5.5" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.5.5.1" class="ltx_text ltx_font_bold">52.8</span></td>
<td id="S4.T1.3.3.5.6" class="ltx_td ltx_align_center">13.0</td>
<td id="S4.T1.3.3.5.7" class="ltx_td ltx_align_center">0.0</td>
<td id="S4.T1.3.3.5.8" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.5.8.1" class="ltx_text ltx_font_bold">48.1</span></td>
<td id="S4.T1.3.3.5.9" class="ltx_td ltx_align_center">23.3</td>
<td id="S4.T1.3.3.5.10" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.5.10.1" class="ltx_text ltx_font_bold">31.9</span></td>
<td id="S4.T1.3.3.5.11" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T1.3.3.5.11.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">55.2<span id="S4.T1.3.3.5.11.1.1" class="ltx_text ltx_font_medium" style="background-color:#E6E6E6;">/</span>48.6<span id="S4.T1.3.3.5.11.1.2" class="ltx_text ltx_font_medium" style="background-color:#E6E6E6;">/</span>51.7</span></td>
</tr>
<tr id="S4.T1.3.3.6" class="ltx_tr">
<td id="S4.T1.3.3.6.1" class="ltx_td ltx_align_left">DETR3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>
</td>
<td id="S4.T1.3.3.6.2" class="ltx_td ltx_align_center">51.6</td>
<td id="S4.T1.3.3.6.3" class="ltx_td ltx_align_center">35.4</td>
<td id="S4.T1.3.3.6.4" class="ltx_td ltx_align_center">30.4</td>
<td id="S4.T1.3.3.6.5" class="ltx_td ltx_align_center">22.4</td>
<td id="S4.T1.3.3.6.6" class="ltx_td ltx_align_center">18.5</td>
<td id="S4.T1.3.3.6.7" class="ltx_td ltx_align_center">15.6</td>
<td id="S4.T1.3.3.6.8" class="ltx_td ltx_align_center">34.8</td>
<td id="S4.T1.3.3.6.9" class="ltx_td ltx_align_center">19.1</td>
<td id="S4.T1.3.3.6.10" class="ltx_td ltx_align_center">11.2</td>
<td id="S4.T1.3.3.6.11" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T1.3.3.6.11.1" class="ltx_text" style="background-color:#E6E6E6;">24.4/44.7/31.6</span></td>
</tr>
<tr id="S4.T1.3.3.7" class="ltx_tr">
<td id="S4.T1.3.3.7.1" class="ltx_td ltx_align_left">PETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>
</td>
<td id="S4.T1.3.3.7.2" class="ltx_td ltx_align_center">71.0</td>
<td id="S4.T1.3.3.7.3" class="ltx_td ltx_align_center">49.3</td>
<td id="S4.T1.3.3.7.4" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.7.4.1" class="ltx_text ltx_font_bold">46.4</span></td>
<td id="S4.T1.3.3.7.5" class="ltx_td ltx_align_center">46.6</td>
<td id="S4.T1.3.3.7.6" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.7.6.1" class="ltx_text ltx_font_bold">29.0</span></td>
<td id="S4.T1.3.3.7.7" class="ltx_td ltx_align_center">26.5</td>
<td id="S4.T1.3.3.7.8" class="ltx_td ltx_align_center">44.4</td>
<td id="S4.T1.3.3.7.9" class="ltx_td ltx_align_center">40.2</td>
<td id="S4.T1.3.3.7.10" class="ltx_td ltx_align_center">23.2</td>
<td id="S4.T1.3.3.7.11" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T1.3.3.7.11.1" class="ltx_text" style="background-color:#E6E6E6;">49.6/50.5/50.0</span></td>
</tr>
<tr id="S4.T1.3.3.8" class="ltx_tr">
<td id="S4.T1.3.3.8.1" class="ltx_td ltx_align_left">PARQ (ours)</td>
<td id="S4.T1.3.3.8.2" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.8.2.1" class="ltx_text ltx_font_bold">72.5</span></td>
<td id="S4.T1.3.3.8.3" class="ltx_td ltx_align_center">46.2</td>
<td id="S4.T1.3.3.8.4" class="ltx_td ltx_align_center">44.3</td>
<td id="S4.T1.3.3.8.5" class="ltx_td ltx_align_center">51.8</td>
<td id="S4.T1.3.3.8.6" class="ltx_td ltx_align_center">20.4</td>
<td id="S4.T1.3.3.8.7" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.8.7.1" class="ltx_text ltx_font_bold">30.6</span></td>
<td id="S4.T1.3.3.8.8" class="ltx_td ltx_align_center">40.5</td>
<td id="S4.T1.3.3.8.9" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.8.9.1" class="ltx_text ltx_font_bold">46.8</span></td>
<td id="S4.T1.3.3.8.10" class="ltx_td ltx_align_center">21.6</td>
<td id="S4.T1.3.3.8.11" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T1.3.3.8.11.1" class="ltx_text" style="background-color:#E6E6E6;">54.2/48.2/51.1</span></td>
</tr>
<tr id="S4.T1.2.2.2" class="ltx_tr">
<td id="S4.T1.2.2.2.1" class="ltx_td ltx_align_left">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span>
@IoU <math id="S4.T1.2.2.2.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.T1.2.2.2.1.m1.1a"><mo id="S4.T1.2.2.2.1.m1.1.1" xref="S4.T1.2.2.2.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.1.m1.1b"><gt id="S4.T1.2.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.2.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.1.m1.1c">&gt;</annotation></semantics></math> 0.5</td>
<td id="S4.T1.2.2.2.2" class="ltx_td ltx_align_center">chair</td>
<td id="S4.T1.2.2.2.3" class="ltx_td ltx_align_center">table</td>
<td id="S4.T1.2.2.2.4" class="ltx_td ltx_align_center">cabinet</td>
<td id="S4.T1.2.2.2.5" class="ltx_td ltx_align_center">trashbin</td>
<td id="S4.T1.2.2.2.6" class="ltx_td ltx_align_center">bookshelf</td>
<td id="S4.T1.2.2.2.7" class="ltx_td ltx_align_center">display</td>
<td id="S4.T1.2.2.2.8" class="ltx_td ltx_align_center">sofa</td>
<td id="S4.T1.2.2.2.9" class="ltx_td ltx_align_center">bathtub</td>
<td id="S4.T1.2.2.2.10" class="ltx_td ltx_align_center">other</td>
<td id="S4.T1.2.2.2.11" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T1.2.2.2.11.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">average</span></td>
</tr>
<tr id="S4.T1.3.3.9" class="ltx_tr">
<td id="S4.T1.3.3.9.1" class="ltx_td ltx_align_left ltx_border_t">ODAM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>
</td>
<td id="S4.T1.3.3.9.2" class="ltx_td ltx_align_center ltx_border_t">22.5</td>
<td id="S4.T1.3.3.9.3" class="ltx_td ltx_align_center ltx_border_t">9.9</td>
<td id="S4.T1.3.3.9.4" class="ltx_td ltx_align_center ltx_border_t">5.0</td>
<td id="S4.T1.3.3.9.5" class="ltx_td ltx_align_center ltx_border_t">7.6</td>
<td id="S4.T1.3.3.9.6" class="ltx_td ltx_align_center ltx_border_t">4.8</td>
<td id="S4.T1.3.3.9.7" class="ltx_td ltx_align_center ltx_border_t">2.7</td>
<td id="S4.T1.3.3.9.8" class="ltx_td ltx_align_center ltx_border_t">16.2</td>
<td id="S4.T1.3.3.9.9" class="ltx_td ltx_align_center ltx_border_t">6.7</td>
<td id="S4.T1.3.3.9.10" class="ltx_td ltx_align_center ltx_border_t">0.0</td>
<td id="S4.T1.3.3.9.11" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T1.3.3.9.11.1" class="ltx_text" style="background-color:#E6E6E6;">12.1/17.3/14.2</span></td>
</tr>
<tr id="S4.T1.3.3.10" class="ltx_tr">
<td id="S4.T1.3.3.10.1" class="ltx_td ltx_align_left">ImVoxelNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</td>
<td id="S4.T1.3.3.10.2" class="ltx_td ltx_align_center">43.8</td>
<td id="S4.T1.3.3.10.3" class="ltx_td ltx_align_center">17.5</td>
<td id="S4.T1.3.3.10.4" class="ltx_td ltx_align_center">18.8</td>
<td id="S4.T1.3.3.10.5" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.10.5.1" class="ltx_text ltx_font_bold">21.5</span></td>
<td id="S4.T1.3.3.10.6" class="ltx_td ltx_align_center">0.8</td>
<td id="S4.T1.3.3.10.7" class="ltx_td ltx_align_center">0.0</td>
<td id="S4.T1.3.3.10.8" class="ltx_td ltx_align_center">16.7</td>
<td id="S4.T1.3.3.10.9" class="ltx_td ltx_align_center">12.3</td>
<td id="S4.T1.3.3.10.10" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.10.10.1" class="ltx_text ltx_font_bold">13.8</span></td>
<td id="S4.T1.3.3.10.11" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T1.3.3.10.11.1" class="ltx_text" style="background-color:#E6E6E6;">28.6/25.2/26.8</span></td>
</tr>
<tr id="S4.T1.3.3.11" class="ltx_tr">
<td id="S4.T1.3.3.11.1" class="ltx_td ltx_align_left">DETR3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>
</td>
<td id="S4.T1.3.3.11.2" class="ltx_td ltx_align_center">25.4</td>
<td id="S4.T1.3.3.11.3" class="ltx_td ltx_align_center">6.7</td>
<td id="S4.T1.3.3.11.4" class="ltx_td ltx_align_center">11.6</td>
<td id="S4.T1.3.3.11.5" class="ltx_td ltx_align_center">3.7</td>
<td id="S4.T1.3.3.11.6" class="ltx_td ltx_align_center">3.0</td>
<td id="S4.T1.3.3.11.7" class="ltx_td ltx_align_center">2.1</td>
<td id="S4.T1.3.3.11.8" class="ltx_td ltx_align_center">12.0</td>
<td id="S4.T1.3.3.11.9" class="ltx_td ltx_align_center">4.5</td>
<td id="S4.T1.3.3.11.10" class="ltx_td ltx_align_center">1.9</td>
<td id="S4.T1.3.3.11.11" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T1.3.3.11.11.1" class="ltx_text" style="background-color:#E6E6E6;">8.8/16.1/11.4</span></td>
</tr>
<tr id="S4.T1.3.3.12" class="ltx_tr">
<td id="S4.T1.3.3.12.1" class="ltx_td ltx_align_left">PETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>
</td>
<td id="S4.T1.3.3.12.2" class="ltx_td ltx_align_center">44.2</td>
<td id="S4.T1.3.3.12.3" class="ltx_td ltx_align_center">20.5</td>
<td id="S4.T1.3.3.12.4" class="ltx_td ltx_align_center">25.8</td>
<td id="S4.T1.3.3.12.5" class="ltx_td ltx_align_center">12.7</td>
<td id="S4.T1.3.3.12.6" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.12.6.1" class="ltx_text ltx_font_bold">9.3</span></td>
<td id="S4.T1.3.3.12.7" class="ltx_td ltx_align_center">5.1</td>
<td id="S4.T1.3.3.12.8" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.12.8.1" class="ltx_text ltx_font_bold">22.2</span></td>
<td id="S4.T1.3.3.12.9" class="ltx_td ltx_align_center">16.8</td>
<td id="S4.T1.3.3.12.10" class="ltx_td ltx_align_center">9.3</td>
<td id="S4.T1.3.3.12.11" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T1.3.3.12.11.1" class="ltx_text" style="background-color:#E6E6E6;">25.4/25.9/25.6</span></td>
</tr>
<tr id="S4.T1.3.3.13" class="ltx_tr">
<td id="S4.T1.3.3.13.1" class="ltx_td ltx_align_left">PARQ (ours)</td>
<td id="S4.T1.3.3.13.2" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.13.2.1" class="ltx_text ltx_font_bold">52.0</span></td>
<td id="S4.T1.3.3.13.3" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.13.3.1" class="ltx_text ltx_font_bold">20.7</span></td>
<td id="S4.T1.3.3.13.4" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.13.4.1" class="ltx_text ltx_font_bold">27.9</span></td>
<td id="S4.T1.3.3.13.5" class="ltx_td ltx_align_center">18.3</td>
<td id="S4.T1.3.3.13.6" class="ltx_td ltx_align_center">6.0</td>
<td id="S4.T1.3.3.13.7" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.13.7.1" class="ltx_text ltx_font_bold">7.1</span></td>
<td id="S4.T1.3.3.13.8" class="ltx_td ltx_align_center">19.0</td>
<td id="S4.T1.3.3.13.9" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.13.9.1" class="ltx_text ltx_font_bold">20.9</span></td>
<td id="S4.T1.3.3.13.10" class="ltx_td ltx_align_center">9.9</td>
<td id="S4.T1.3.3.13.11" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T1.3.3.13.11.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">31.8<span id="S4.T1.3.3.13.11.1.1" class="ltx_text ltx_font_medium" style="background-color:#E6E6E6;">/</span>28.3<span id="S4.T1.3.3.13.11.1.2" class="ltx_text ltx_font_medium" style="background-color:#E6E6E6;">/</span>30.0</span></td>
</tr>
<tr id="S4.T1.3.3.3" class="ltx_tr">
<td id="S4.T1.3.3.3.1" class="ltx_td ltx_align_left">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span>
@IoU <math id="S4.T1.3.3.3.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.T1.3.3.3.1.m1.1a"><mo id="S4.T1.3.3.3.1.m1.1.1" xref="S4.T1.3.3.3.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.1.m1.1b"><gt id="S4.T1.3.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.3.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.1.m1.1c">&gt;</annotation></semantics></math> 0.7</td>
<td id="S4.T1.3.3.3.2" class="ltx_td ltx_align_center">chair</td>
<td id="S4.T1.3.3.3.3" class="ltx_td ltx_align_center">table</td>
<td id="S4.T1.3.3.3.4" class="ltx_td ltx_align_center">cabinet</td>
<td id="S4.T1.3.3.3.5" class="ltx_td ltx_align_center">trashbin</td>
<td id="S4.T1.3.3.3.6" class="ltx_td ltx_align_center">bookshelf</td>
<td id="S4.T1.3.3.3.7" class="ltx_td ltx_align_center">display</td>
<td id="S4.T1.3.3.3.8" class="ltx_td ltx_align_center">sofa</td>
<td id="S4.T1.3.3.3.9" class="ltx_td ltx_align_center">bathtub</td>
<td id="S4.T1.3.3.3.10" class="ltx_td ltx_align_center">other</td>
<td id="S4.T1.3.3.3.11" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T1.3.3.3.11.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">average</span></td>
</tr>
<tr id="S4.T1.3.3.14" class="ltx_tr">
<td id="S4.T1.3.3.14.1" class="ltx_td ltx_align_left ltx_border_t">ODAM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>
</td>
<td id="S4.T1.3.3.14.2" class="ltx_td ltx_align_center ltx_border_t">2.3</td>
<td id="S4.T1.3.3.14.3" class="ltx_td ltx_align_center ltx_border_t">0.6</td>
<td id="S4.T1.3.3.14.4" class="ltx_td ltx_align_center ltx_border_t">0.0</td>
<td id="S4.T1.3.3.14.5" class="ltx_td ltx_align_center ltx_border_t">1.6</td>
<td id="S4.T1.3.3.14.6" class="ltx_td ltx_align_center ltx_border_t">0.0</td>
<td id="S4.T1.3.3.14.7" class="ltx_td ltx_align_center ltx_border_t">0.0</td>
<td id="S4.T1.3.3.14.8" class="ltx_td ltx_align_center ltx_border_t">1.0</td>
<td id="S4.T1.3.3.14.9" class="ltx_td ltx_align_center ltx_border_t">0.8</td>
<td id="S4.T1.3.3.14.10" class="ltx_td ltx_align_center ltx_border_t">0.0</td>
<td id="S4.T1.3.3.14.11" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T1.3.3.14.11.1" class="ltx_text" style="background-color:#E6E6E6;">1.2/1.7/1.4</span></td>
</tr>
<tr id="S4.T1.3.3.15" class="ltx_tr">
<td id="S4.T1.3.3.15.1" class="ltx_td ltx_align_left">ImVoxelNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</td>
<td id="S4.T1.3.3.15.2" class="ltx_td ltx_align_center">6.7</td>
<td id="S4.T1.3.3.15.3" class="ltx_td ltx_align_center">1.8</td>
<td id="S4.T1.3.3.15.4" class="ltx_td ltx_align_center">2.2</td>
<td id="S4.T1.3.3.15.5" class="ltx_td ltx_align_center">0.9</td>
<td id="S4.T1.3.3.15.6" class="ltx_td ltx_align_center">0.0</td>
<td id="S4.T1.3.3.15.7" class="ltx_td ltx_align_center">0.0</td>
<td id="S4.T1.3.3.15.8" class="ltx_td ltx_align_center">3.7</td>
<td id="S4.T1.3.3.15.9" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.15.9.1" class="ltx_text ltx_font_bold">2.7</span></td>
<td id="S4.T1.3.3.15.10" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.15.10.1" class="ltx_text ltx_font_bold">1.7</span></td>
<td id="S4.T1.3.3.15.11" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T1.3.3.15.11.1" class="ltx_text" style="background-color:#E6E6E6;">4.1/3.6/3.8</span></td>
</tr>
<tr id="S4.T1.3.3.16" class="ltx_tr">
<td id="S4.T1.3.3.16.1" class="ltx_td ltx_align_left">DETR3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>
</td>
<td id="S4.T1.3.3.16.2" class="ltx_td ltx_align_center">3.2</td>
<td id="S4.T1.3.3.16.3" class="ltx_td ltx_align_center">0.2</td>
<td id="S4.T1.3.3.16.4" class="ltx_td ltx_align_center">0.4</td>
<td id="S4.T1.3.3.16.5" class="ltx_td ltx_align_center">0.0</td>
<td id="S4.T1.3.3.16.6" class="ltx_td ltx_align_center">0.0</td>
<td id="S4.T1.3.3.16.7" class="ltx_td ltx_align_center">0.2</td>
<td id="S4.T1.3.3.16.8" class="ltx_td ltx_align_center">0.0</td>
<td id="S4.T1.3.3.16.9" class="ltx_td ltx_align_center">1.3</td>
<td id="S4.T1.3.3.16.10" class="ltx_td ltx_align_center">0.0</td>
<td id="S4.T1.3.3.16.11" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T1.3.3.16.11.1" class="ltx_text" style="background-color:#E6E6E6;">0.9/1.7/1.2</span></td>
</tr>
<tr id="S4.T1.3.3.17" class="ltx_tr">
<td id="S4.T1.3.3.17.1" class="ltx_td ltx_align_left">PETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>
</td>
<td id="S4.T1.3.3.17.2" class="ltx_td ltx_align_center">9.1</td>
<td id="S4.T1.3.3.17.3" class="ltx_td ltx_align_center">2.0</td>
<td id="S4.T1.3.3.17.4" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.17.4.1" class="ltx_text ltx_font_bold">7.7</span></td>
<td id="S4.T1.3.3.17.5" class="ltx_td ltx_align_center">0.4</td>
<td id="S4.T1.3.3.17.6" class="ltx_td ltx_align_center">0.1</td>
<td id="S4.T1.3.3.17.7" class="ltx_td ltx_align_center">0.0</td>
<td id="S4.T1.3.3.17.8" class="ltx_td ltx_align_center">1.2</td>
<td id="S4.T1.3.3.17.9" class="ltx_td ltx_align_center">1.9</td>
<td id="S4.T1.3.3.17.10" class="ltx_td ltx_align_center">1.4</td>
<td id="S4.T1.3.3.17.11" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T1.3.3.17.11.1" class="ltx_text" style="background-color:#E6E6E6;">4.6/4.6/4.6</span></td>
</tr>
<tr id="S4.T1.3.3.18" class="ltx_tr">
<td id="S4.T1.3.3.18.1" class="ltx_td ltx_align_left">PARQ (ours)</td>
<td id="S4.T1.3.3.18.2" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.18.2.1" class="ltx_text ltx_font_bold">11.8</span></td>
<td id="S4.T1.3.3.18.3" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.18.3.1" class="ltx_text ltx_font_bold">2.9</span></td>
<td id="S4.T1.3.3.18.4" class="ltx_td ltx_align_center">7.0</td>
<td id="S4.T1.3.3.18.5" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.18.5.1" class="ltx_text ltx_font_bold">2.0</span></td>
<td id="S4.T1.3.3.18.6" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.18.6.1" class="ltx_text ltx_font_bold">1.2</span></td>
<td id="S4.T1.3.3.18.7" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.18.7.1" class="ltx_text ltx_font_bold">0.4</span></td>
<td id="S4.T1.3.3.18.8" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.18.8.1" class="ltx_text ltx_font_bold">3.8</span></td>
<td id="S4.T1.3.3.18.9" class="ltx_td ltx_align_center">2.0</td>
<td id="S4.T1.3.3.18.10" class="ltx_td ltx_align_center">1.6</td>
<td id="S4.T1.3.3.18.11" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T1.3.3.18.11.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">6.5<span id="S4.T1.3.3.18.11.1.1" class="ltx_text ltx_font_medium" style="background-color:#E6E6E6;">/</span>5.8<span id="S4.T1.3.3.18.11.1.2" class="ltx_text ltx_font_medium" style="background-color:#E6E6E6;">/</span>6.1</span></td>
</tr>
<tr id="S4.T1.3.3.19" class="ltx_tr">
<td id="S4.T1.3.3.19.1" class="ltx_td ltx_align_left"><span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></td>
<td id="S4.T1.3.3.19.2" class="ltx_td"></td>
<td id="S4.T1.3.3.19.3" class="ltx_td"></td>
<td id="S4.T1.3.3.19.4" class="ltx_td"></td>
<td id="S4.T1.3.3.19.5" class="ltx_td"></td>
<td id="S4.T1.3.3.19.6" class="ltx_td"></td>
<td id="S4.T1.3.3.19.7" class="ltx_td"></td>
<td id="S4.T1.3.3.19.8" class="ltx_td"></td>
<td id="S4.T1.3.3.19.9" class="ltx_td"></td>
<td id="S4.T1.3.3.19.10" class="ltx_td"></td>
<td id="S4.T1.3.3.19.11" class="ltx_td"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span id="S4.T1.5.1" class="ltx_text ltx_font_bold">Performance on ScanNet</span>.
We compare PARQ to prior works ODAM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, ImVoxelNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, DETR3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> and PETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.
We report F1 for each of the 9 classes of ScanNet and Precision/Recall/F1 for average performance.
Note that ODAM omits the ‘other’ class, so we exclude it from the average.
See full Precision/Recall/F1 performance for all classes in the supplementary.
</figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:41.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-127.2pt,24.3pt) scale(0.460185080496585,0.460185080496585) ;">
<table id="S4.T2.3.3" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.3.3.3" class="ltx_tr">
<td id="S4.T2.3.3.3.4" class="ltx_td ltx_align_left">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span>
Prec./Rec./F1</td>
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center">@IoU<math id="S4.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.T2.1.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><gt id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">&gt;</annotation></semantics></math>0.25</td>
<td id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center">@IoU<math id="S4.T2.2.2.2.2.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.T2.2.2.2.2.m1.1a"><mo id="S4.T2.2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.2.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.m1.1b"><gt id="S4.T2.2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.m1.1c">&gt;</annotation></semantics></math>0.5</td>
<td id="S4.T2.3.3.3.3" class="ltx_td ltx_align_center">@IoU<math id="S4.T2.3.3.3.3.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.T2.3.3.3.3.m1.1a"><mo id="S4.T2.3.3.3.3.m1.1.1" xref="S4.T2.3.3.3.3.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.3.m1.1b"><gt id="S4.T2.3.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.3.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.3.m1.1c">&gt;</annotation></semantics></math>0.7</td>
</tr>
<tr id="S4.T2.3.3.4" class="ltx_tr">
<td id="S4.T2.3.3.4.1" class="ltx_td ltx_align_left ltx_border_t">ImVoxelNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</td>
<td id="S4.T2.3.3.4.2" class="ltx_td ltx_align_center ltx_border_t">44.5/40.3/42.3</td>
<td id="S4.T2.3.3.4.3" class="ltx_td ltx_align_center ltx_border_t">15.9/14.4/15.1</td>
<td id="S4.T2.3.3.4.4" class="ltx_td ltx_align_center ltx_border_t">1.3/1.2/1.3</td>
</tr>
<tr id="S4.T2.3.3.5" class="ltx_tr">
<td id="S4.T2.3.3.5.1" class="ltx_td ltx_align_left">PETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>
</td>
<td id="S4.T2.3.3.5.2" class="ltx_td ltx_align_center">36.6/<span id="S4.T2.3.3.5.2.1" class="ltx_text ltx_font_bold">53.2</span>/43.4</td>
<td id="S4.T2.3.3.5.3" class="ltx_td ltx_align_center">14.8/21.5/17.5</td>
<td id="S4.T2.3.3.5.4" class="ltx_td ltx_align_center">2.5/3.7/3.0</td>
</tr>
<tr id="S4.T2.3.3.6" class="ltx_tr">
<td id="S4.T2.3.3.6.1" class="ltx_td ltx_align_left">Ours</td>
<td id="S4.T2.3.3.6.2" class="ltx_td ltx_align_center">
<span id="S4.T2.3.3.6.2.1" class="ltx_text ltx_font_bold">54.1</span>/44.4/<span id="S4.T2.3.3.6.2.2" class="ltx_text ltx_font_bold">48.8</span>
</td>
<td id="S4.T2.3.3.6.3" class="ltx_td ltx_align_center">
<span id="S4.T2.3.3.6.3.1" class="ltx_text ltx_font_bold">26.7</span>/<span id="S4.T2.3.3.6.3.2" class="ltx_text ltx_font_bold">21.9</span>/<span id="S4.T2.3.3.6.3.3" class="ltx_text ltx_font_bold">24.1</span>
</td>
<td id="S4.T2.3.3.6.4" class="ltx_td ltx_align_center">
<span id="S4.T2.3.3.6.4.1" class="ltx_text ltx_font_bold">6.3</span>/<span id="S4.T2.3.3.6.4.2" class="ltx_text ltx_font_bold">5.2</span>/<span id="S4.T2.3.3.6.4.3" class="ltx_text ltx_font_bold">5.7</span>
</td>
</tr>
<tr id="S4.T2.3.3.7" class="ltx_tr">
<td id="S4.T2.3.3.7.1" class="ltx_td ltx_align_left"><span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></td>
<td id="S4.T2.3.3.7.2" class="ltx_td"></td>
<td id="S4.T2.3.3.7.3" class="ltx_td"></td>
<td id="S4.T2.3.3.7.4" class="ltx_td"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>
<span id="S4.T2.5.1" class="ltx_text ltx_font_bold">Performance on ARKitScenes</span> with 3 views. We report average Precision/Recall/F1 for all 17 classes.
The complete table is provided in the supplementary.
</figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We tackle 3D object detection from a few (<em id="S4.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em> 3) consecutive RGB frames of complex indoor scenes with many object types.
We provide an extensive quantitative and qualitative analysis of our model’s performance and show that our approach outperforms the previous state-of-the-art methods for the task.
More importantly, we show that appearance-informed queries lead to better performance, faster convergence and can leverage more input views during inference.
Finally, we test our model’s generalization ability by deploying it on user-captured videos without any finetuning. There, scenes are captured with an iPhoneXR, and camera poses are obtained from ARKit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.4" class="ltx_p"><span id="S4.p2.4.1" class="ltx_text ltx_font_bold">Datasets.</span>
We experiment on the popular ScanNetv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and ARKitScenes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> datasets. The ScanNet dataset contains RGB-D videos of 1613 indoor scenes with multiple objects in complex spatial arrangements.
Scan2CAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> aligns CAD models which are used to extract 3D bounding box annotations for all objects in the scene.
Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, we evaluate on 9 classes
and use the official train/val splits by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
On ScanNet, the input image size is <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="320\times 240" display="inline"><semantics id="S4.p2.1.m1.1a"><mrow id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml"><mn id="S4.p2.1.m1.1.1.2" xref="S4.p2.1.m1.1.1.2.cmml">320</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p2.1.m1.1.1.1" xref="S4.p2.1.m1.1.1.1.cmml">×</mo><mn id="S4.p2.1.m1.1.1.3" xref="S4.p2.1.m1.1.1.3.cmml">240</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><apply id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1"><times id="S4.p2.1.m1.1.1.1.cmml" xref="S4.p2.1.m1.1.1.1"></times><cn type="integer" id="S4.p2.1.m1.1.1.2.cmml" xref="S4.p2.1.m1.1.1.2">320</cn><cn type="integer" id="S4.p2.1.m1.1.1.3.cmml" xref="S4.p2.1.m1.1.1.3">240</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">320\times 240</annotation></semantics></math> and the image feature map size is <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="80\times 60" display="inline"><semantics id="S4.p2.2.m2.1a"><mrow id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml"><mn id="S4.p2.2.m2.1.1.2" xref="S4.p2.2.m2.1.1.2.cmml">80</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p2.2.m2.1.1.1" xref="S4.p2.2.m2.1.1.1.cmml">×</mo><mn id="S4.p2.2.m2.1.1.3" xref="S4.p2.2.m2.1.1.3.cmml">60</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><apply id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1"><times id="S4.p2.2.m2.1.1.1.cmml" xref="S4.p2.2.m2.1.1.1"></times><cn type="integer" id="S4.p2.2.m2.1.1.2.cmml" xref="S4.p2.2.m2.1.1.2">80</cn><cn type="integer" id="S4.p2.2.m2.1.1.3.cmml" xref="S4.p2.2.m2.1.1.3">60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">80\times 60</annotation></semantics></math>.
ARKitScenes is a challenging dataset of indoor scenes which includes manually labeled 3D oriented bounding boxes for a large taxonomy of furniture.
We follow the official train/val/test split and evaluate for all 17 classes.
The orientation of the images in the ARKitScenes dataset vary from video to video.
Even though they sky direction of each video is provided in the metadata, some labels are inaccurate <span id="footnote1a" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/apple/ARKitScenes/issues/10" title="" class="ltx_ref ltx_href">https://github.com/apple/ARKitScenes/issues/10</a></span></span></span>.
We follow the metadata to rectify the images. We use the videos with sky directions ‘Up’ and ‘Down’.
On ARKitScenes, the input image size is <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="256\times 192" display="inline"><semantics id="S4.p2.3.m3.1a"><mrow id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml"><mn id="S4.p2.3.m3.1.1.2" xref="S4.p2.3.m3.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p2.3.m3.1.1.1" xref="S4.p2.3.m3.1.1.1.cmml">×</mo><mn id="S4.p2.3.m3.1.1.3" xref="S4.p2.3.m3.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><apply id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1"><times id="S4.p2.3.m3.1.1.1.cmml" xref="S4.p2.3.m3.1.1.1"></times><cn type="integer" id="S4.p2.3.m3.1.1.2.cmml" xref="S4.p2.3.m3.1.1.2">256</cn><cn type="integer" id="S4.p2.3.m3.1.1.3.cmml" xref="S4.p2.3.m3.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">256\times 192</annotation></semantics></math> and the image feature map size is <math id="S4.p2.4.m4.1" class="ltx_Math" alttext="64\times 48" display="inline"><semantics id="S4.p2.4.m4.1a"><mrow id="S4.p2.4.m4.1.1" xref="S4.p2.4.m4.1.1.cmml"><mn id="S4.p2.4.m4.1.1.2" xref="S4.p2.4.m4.1.1.2.cmml">64</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p2.4.m4.1.1.1" xref="S4.p2.4.m4.1.1.1.cmml">×</mo><mn id="S4.p2.4.m4.1.1.3" xref="S4.p2.4.m4.1.1.3.cmml">48</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.4.m4.1b"><apply id="S4.p2.4.m4.1.1.cmml" xref="S4.p2.4.m4.1.1"><times id="S4.p2.4.m4.1.1.1.cmml" xref="S4.p2.4.m4.1.1.1"></times><cn type="integer" id="S4.p2.4.m4.1.1.2.cmml" xref="S4.p2.4.m4.1.1.2">64</cn><cn type="integer" id="S4.p2.4.m4.1.1.3.cmml" xref="S4.p2.4.m4.1.1.3">48</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m4.1c">64\times 48</annotation></semantics></math>.</p>
</div>
<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Extracting video snippets.</h4>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p1.4" class="ltx_p">We focus on online 3D object detection from a short video snippet.
Given a monocular video and per-frame camera poses, extracted from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, we split the video into snippets of <math id="S4.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS0.SSS0.Px1.p1.1.m1.1a"><mi id="S4.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.1.m1.1b"><ci id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.1.m1.1c">N</annotation></semantics></math> frames as follows:
The first frame is selected.
Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, the next frame is added if its relative translation is greater than <math id="S4.SS0.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="0.1m" display="inline"><semantics id="S4.SS0.SSS0.Px1.p1.2.m2.1a"><mrow id="S4.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px1.p1.2.m2.1.1.cmml"><mn id="S4.SS0.SSS0.Px1.p1.2.m2.1.1.2" xref="S4.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml">0.1</mn><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px1.p1.2.m2.1.1.1" xref="S4.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px1.p1.2.m2.1.1.3" xref="S4.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.2.m2.1b"><apply id="S4.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.2.m2.1.1"><times id="S4.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.2.m2.1.1.1"></times><cn type="float" id="S4.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S4.SS0.SSS0.Px1.p1.2.m2.1.1.2">0.1</cn><ci id="S4.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S4.SS0.SSS0.Px1.p1.2.m2.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.2.m2.1c">0.1m</annotation></semantics></math> or its relative rotation angle is greater than <math id="S4.SS0.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="15^{o}" display="inline"><semantics id="S4.SS0.SSS0.Px1.p1.3.m3.1a"><msup id="S4.SS0.SSS0.Px1.p1.3.m3.1.1" xref="S4.SS0.SSS0.Px1.p1.3.m3.1.1.cmml"><mn id="S4.SS0.SSS0.Px1.p1.3.m3.1.1.2" xref="S4.SS0.SSS0.Px1.p1.3.m3.1.1.2.cmml">15</mn><mi id="S4.SS0.SSS0.Px1.p1.3.m3.1.1.3" xref="S4.SS0.SSS0.Px1.p1.3.m3.1.1.3.cmml">o</mi></msup><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.3.m3.1b"><apply id="S4.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.3.m3.1.1">superscript</csymbol><cn type="integer" id="S4.SS0.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S4.SS0.SSS0.Px1.p1.3.m3.1.1.2">15</cn><ci id="S4.SS0.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S4.SS0.SSS0.Px1.p1.3.m3.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.3.m3.1c">15^{o}</annotation></semantics></math> compared to the last selected frame.
Once <math id="S4.SS0.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS0.SSS0.Px1.p1.4.m4.1a"><mi id="S4.SS0.SSS0.Px1.p1.4.m4.1.1" xref="S4.SS0.SSS0.Px1.p1.4.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.4.m4.1b"><ci id="S4.SS0.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.4.m4.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.4.m4.1c">N</annotation></semantics></math> frames are selected, they form the snippet.
Our selection process ensures that snippets contain consecutive video frames which are relatively visually diverse.
Examples of our snippets are shown in Fig. <a href="#S5.F7" title="Figure 7 ‣ 5 Limitations ‣ Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<div id="S4.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p2.2" class="ltx_p">Next, we extract 3D box annotations for each snippet.
Both Scan2CAD and ARKitScenes provide 3D annotations for the entire scene in world space, which is not suitable for us.
For each snippet, we keep annotations belonging to the <em id="S4.SS0.SSS0.Px1.p2.2.1" class="ltx_emph ltx_font_italic">visible</em> objects in the snippet.
To determine whether an object is within the snippet frustum, we project the corners of its 3D box on the views and calculate the IoU between the projected 2D box and the image border. If the IoU is less than <math id="S4.SS0.SSS0.Px1.p2.1.m1.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S4.SS0.SSS0.Px1.p2.1.m1.1a"><mn id="S4.SS0.SSS0.Px1.p2.1.m1.1.1" xref="S4.SS0.SSS0.Px1.p2.1.m1.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p2.1.m1.1b"><cn type="float" id="S4.SS0.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p2.1.m1.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p2.1.m1.1c">0.5</annotation></semantics></math> we remove the box.
To determine whether an object is severely occluded in the snippet views, we un-project the ground-truth view depth maps and calculate the number of points inside the 3D box. If the number of points is less than <math id="S4.SS0.SSS0.Px1.p2.2.m2.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S4.SS0.SSS0.Px1.p2.2.m2.1a"><mn id="S4.SS0.SSS0.Px1.p2.2.m2.1.1" xref="S4.SS0.SSS0.Px1.p2.2.m2.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p2.2.m2.1b"><cn type="integer" id="S4.SS0.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px1.p2.2.m2.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p2.2.m2.1c">100</annotation></semantics></math> we remove the box.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Metrics.</h4>

<div id="S4.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p1.1" class="ltx_p">For all experiments, we adopt the popular evaluation protocol from ODAM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> which evaluates at the scene level.
For each scene, we keep the predictions above a confidence threshold <math id="S4.SS0.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S4.SS0.SSS0.Px2.p1.1.m1.1a"><mi id="S4.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.1.m1.1b"><ci id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.1.m1.1c">s</annotation></semantics></math> from each input snippet.
Hungarian matching <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> with 3D Intersection-over-Union (IoU) is used to match predictions between the current and the previous snippet.
For two matched boxes, we only keep the box with the higher score as one of the scene-level predictions.
3D NMS is used to filter out potential duplicate predictions.
More details in the supplementary.
We use the same tracking and fusion strategy for baselines and our model for a fair comparison.</p>
</div>
<div id="S4.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p2.1" class="ltx_p">A prediction is considered a true positive if its 3D IoU with a ground-truth of the same class is above a predefined threshold <math id="S4.SS0.SSS0.Px2.p2.1.m1.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S4.SS0.SSS0.Px2.p2.1.m1.1a"><mi id="S4.SS0.SSS0.Px2.p2.1.m1.1.1" xref="S4.SS0.SSS0.Px2.p2.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p2.1.m1.1b"><ci id="S4.SS0.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p2.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p2.1.m1.1c">\tau</annotation></semantics></math>.
Duplicate predictions, namely predictions paired to an already matched true object, are marked as false positives.
We report Precision, Recall, and F1:</p>
<table id="S6.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.E4.m1.1" class="ltx_math_unparsed" alttext="\displaystyle Prec.=\frac{N_{tp}}{N_{pred}},~{}~{}Rec.=\frac{N_{tp}}{N_{gt}},~{}~{}F1=\frac{2*Prec.*Rec.}{Prec.+Rec.}" display="inline"><semantics id="S4.E4.m1.1a"><mrow id="S4.E4.m1.1b"><mi mathsize="90%" id="S4.E4.m1.1.1">P</mi><mi mathsize="90%" id="S4.E4.m1.1.2">r</mi><mi mathsize="90%" id="S4.E4.m1.1.3">e</mi><mi mathsize="90%" id="S4.E4.m1.1.4">c</mi><mo lspace="0em" mathsize="90%" rspace="0.0835em" id="S4.E4.m1.1.5">.</mo><mo lspace="0.0835em" mathsize="90%" id="S4.E4.m1.1.6">=</mo><mstyle displaystyle="true" id="S4.E4.m1.1.7"><mfrac id="S4.E4.m1.1.7a"><msub id="S4.E4.m1.1.7.2"><mi mathsize="90%" id="S4.E4.m1.1.7.2.2">N</mi><mrow id="S4.E4.m1.1.7.2.3"><mi mathsize="90%" id="S4.E4.m1.1.7.2.3.2">t</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.1.7.2.3.1">​</mo><mi mathsize="90%" id="S4.E4.m1.1.7.2.3.3">p</mi></mrow></msub><msub id="S4.E4.m1.1.7.3"><mi mathsize="90%" id="S4.E4.m1.1.7.3.2">N</mi><mrow id="S4.E4.m1.1.7.3.3"><mi mathsize="90%" id="S4.E4.m1.1.7.3.3.2">p</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.1.7.3.3.1">​</mo><mi mathsize="90%" id="S4.E4.m1.1.7.3.3.3">r</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.1.7.3.3.1a">​</mo><mi mathsize="90%" id="S4.E4.m1.1.7.3.3.4">e</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.1.7.3.3.1b">​</mo><mi mathsize="90%" id="S4.E4.m1.1.7.3.3.5">d</mi></mrow></msub></mfrac></mstyle><mo mathsize="90%" rspace="0.767em" id="S4.E4.m1.1.8">,</mo><mi mathsize="90%" id="S4.E4.m1.1.9">R</mi><mi mathsize="90%" id="S4.E4.m1.1.10">e</mi><mi mathsize="90%" id="S4.E4.m1.1.11">c</mi><mo lspace="0em" mathsize="90%" rspace="0.0835em" id="S4.E4.m1.1.12">.</mo><mo lspace="0.0835em" mathsize="90%" id="S4.E4.m1.1.13">=</mo><mstyle displaystyle="true" id="S4.E4.m1.1.14"><mfrac id="S4.E4.m1.1.14a"><msub id="S4.E4.m1.1.14.2"><mi mathsize="90%" id="S4.E4.m1.1.14.2.2">N</mi><mrow id="S4.E4.m1.1.14.2.3"><mi mathsize="90%" id="S4.E4.m1.1.14.2.3.2">t</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.1.14.2.3.1">​</mo><mi mathsize="90%" id="S4.E4.m1.1.14.2.3.3">p</mi></mrow></msub><msub id="S4.E4.m1.1.14.3"><mi mathsize="90%" id="S4.E4.m1.1.14.3.2">N</mi><mrow id="S4.E4.m1.1.14.3.3"><mi mathsize="90%" id="S4.E4.m1.1.14.3.3.2">g</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.1.14.3.3.1">​</mo><mi mathsize="90%" id="S4.E4.m1.1.14.3.3.3">t</mi></mrow></msub></mfrac></mstyle><mo mathsize="90%" rspace="0.767em" id="S4.E4.m1.1.15">,</mo><mi mathsize="90%" id="S4.E4.m1.1.16">F</mi><mn mathsize="90%" id="S4.E4.m1.1.17">1</mn><mo mathsize="90%" id="S4.E4.m1.1.18">=</mo><mstyle displaystyle="true" id="S4.E4.m1.1.19"><mfrac id="S4.E4.m1.1.19a"><mrow id="S4.E4.m1.1.19.2"><mn mathsize="90%" id="S4.E4.m1.1.19.2.1">2</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S4.E4.m1.1.19.2.2">∗</mo><mi mathsize="90%" id="S4.E4.m1.1.19.2.3">P</mi><mi mathsize="90%" id="S4.E4.m1.1.19.2.4">r</mi><mi mathsize="90%" id="S4.E4.m1.1.19.2.5">e</mi><mi mathsize="90%" id="S4.E4.m1.1.19.2.6">c</mi><mo lspace="0em" mathsize="90%" rspace="0em" id="S4.E4.m1.1.19.2.7">.</mo><mo lspace="0em" mathsize="90%" rspace="0.222em" id="S4.E4.m1.1.19.2.8">∗</mo><mi mathsize="90%" id="S4.E4.m1.1.19.2.9">R</mi><mi mathsize="90%" id="S4.E4.m1.1.19.2.10">e</mi><mi mathsize="90%" id="S4.E4.m1.1.19.2.11">c</mi><mo lspace="0em" mathsize="90%" id="S4.E4.m1.1.19.2.12">.</mo></mrow><mrow id="S4.E4.m1.1.19.3"><mi mathsize="90%" id="S4.E4.m1.1.19.3.1">P</mi><mi mathsize="90%" id="S4.E4.m1.1.19.3.2">r</mi><mi mathsize="90%" id="S4.E4.m1.1.19.3.3">e</mi><mi mathsize="90%" id="S4.E4.m1.1.19.3.4">c</mi><mo lspace="0em" mathsize="90%" rspace="0em" id="S4.E4.m1.1.19.3.5">.</mo><mo lspace="0em" mathsize="90%" id="S4.E4.m1.1.19.3.6">+</mo><mi mathsize="90%" id="S4.E4.m1.1.19.3.7">R</mi><mi mathsize="90%" id="S4.E4.m1.1.19.3.8">e</mi><mi mathsize="90%" id="S4.E4.m1.1.19.3.9">c</mi><mo lspace="0em" mathsize="90%" id="S4.E4.m1.1.19.3.10">.</mo></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex" id="S4.E4.m1.1c">\displaystyle Prec.=\frac{N_{tp}}{N_{pred}},~{}~{}Rec.=\frac{N_{tp}}{N_{gt}},~{}~{}F1=\frac{2*Prec.*Rec.}{Prec.+Rec.}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S4.SS0.SSS0.Px2.p2.5" class="ltx_p"><math id="S4.SS0.SSS0.Px2.p2.2.m1.1" class="ltx_Math" alttext="N_{tp}" display="inline"><semantics id="S4.SS0.SSS0.Px2.p2.2.m1.1a"><msub id="S4.SS0.SSS0.Px2.p2.2.m1.1.1" xref="S4.SS0.SSS0.Px2.p2.2.m1.1.1.cmml"><mi id="S4.SS0.SSS0.Px2.p2.2.m1.1.1.2" xref="S4.SS0.SSS0.Px2.p2.2.m1.1.1.2.cmml">N</mi><mrow id="S4.SS0.SSS0.Px2.p2.2.m1.1.1.3" xref="S4.SS0.SSS0.Px2.p2.2.m1.1.1.3.cmml"><mi id="S4.SS0.SSS0.Px2.p2.2.m1.1.1.3.2" xref="S4.SS0.SSS0.Px2.p2.2.m1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px2.p2.2.m1.1.1.3.1" xref="S4.SS0.SSS0.Px2.p2.2.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px2.p2.2.m1.1.1.3.3" xref="S4.SS0.SSS0.Px2.p2.2.m1.1.1.3.3.cmml">p</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p2.2.m1.1b"><apply id="S4.SS0.SSS0.Px2.p2.2.m1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px2.p2.2.m1.1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p2.2.m1.1.1">subscript</csymbol><ci id="S4.SS0.SSS0.Px2.p2.2.m1.1.1.2.cmml" xref="S4.SS0.SSS0.Px2.p2.2.m1.1.1.2">𝑁</ci><apply id="S4.SS0.SSS0.Px2.p2.2.m1.1.1.3.cmml" xref="S4.SS0.SSS0.Px2.p2.2.m1.1.1.3"><times id="S4.SS0.SSS0.Px2.p2.2.m1.1.1.3.1.cmml" xref="S4.SS0.SSS0.Px2.p2.2.m1.1.1.3.1"></times><ci id="S4.SS0.SSS0.Px2.p2.2.m1.1.1.3.2.cmml" xref="S4.SS0.SSS0.Px2.p2.2.m1.1.1.3.2">𝑡</ci><ci id="S4.SS0.SSS0.Px2.p2.2.m1.1.1.3.3.cmml" xref="S4.SS0.SSS0.Px2.p2.2.m1.1.1.3.3">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p2.2.m1.1c">N_{tp}</annotation></semantics></math>, <math id="S4.SS0.SSS0.Px2.p2.3.m2.1" class="ltx_Math" alttext="N_{pred}" display="inline"><semantics id="S4.SS0.SSS0.Px2.p2.3.m2.1a"><msub id="S4.SS0.SSS0.Px2.p2.3.m2.1.1" xref="S4.SS0.SSS0.Px2.p2.3.m2.1.1.cmml"><mi id="S4.SS0.SSS0.Px2.p2.3.m2.1.1.2" xref="S4.SS0.SSS0.Px2.p2.3.m2.1.1.2.cmml">N</mi><mrow id="S4.SS0.SSS0.Px2.p2.3.m2.1.1.3" xref="S4.SS0.SSS0.Px2.p2.3.m2.1.1.3.cmml"><mi id="S4.SS0.SSS0.Px2.p2.3.m2.1.1.3.2" xref="S4.SS0.SSS0.Px2.p2.3.m2.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px2.p2.3.m2.1.1.3.1" xref="S4.SS0.SSS0.Px2.p2.3.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px2.p2.3.m2.1.1.3.3" xref="S4.SS0.SSS0.Px2.p2.3.m2.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px2.p2.3.m2.1.1.3.1a" xref="S4.SS0.SSS0.Px2.p2.3.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px2.p2.3.m2.1.1.3.4" xref="S4.SS0.SSS0.Px2.p2.3.m2.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px2.p2.3.m2.1.1.3.1b" xref="S4.SS0.SSS0.Px2.p2.3.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px2.p2.3.m2.1.1.3.5" xref="S4.SS0.SSS0.Px2.p2.3.m2.1.1.3.5.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p2.3.m2.1b"><apply id="S4.SS0.SSS0.Px2.p2.3.m2.1.1.cmml" xref="S4.SS0.SSS0.Px2.p2.3.m2.1.1"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px2.p2.3.m2.1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p2.3.m2.1.1">subscript</csymbol><ci id="S4.SS0.SSS0.Px2.p2.3.m2.1.1.2.cmml" xref="S4.SS0.SSS0.Px2.p2.3.m2.1.1.2">𝑁</ci><apply id="S4.SS0.SSS0.Px2.p2.3.m2.1.1.3.cmml" xref="S4.SS0.SSS0.Px2.p2.3.m2.1.1.3"><times id="S4.SS0.SSS0.Px2.p2.3.m2.1.1.3.1.cmml" xref="S4.SS0.SSS0.Px2.p2.3.m2.1.1.3.1"></times><ci id="S4.SS0.SSS0.Px2.p2.3.m2.1.1.3.2.cmml" xref="S4.SS0.SSS0.Px2.p2.3.m2.1.1.3.2">𝑝</ci><ci id="S4.SS0.SSS0.Px2.p2.3.m2.1.1.3.3.cmml" xref="S4.SS0.SSS0.Px2.p2.3.m2.1.1.3.3">𝑟</ci><ci id="S4.SS0.SSS0.Px2.p2.3.m2.1.1.3.4.cmml" xref="S4.SS0.SSS0.Px2.p2.3.m2.1.1.3.4">𝑒</ci><ci id="S4.SS0.SSS0.Px2.p2.3.m2.1.1.3.5.cmml" xref="S4.SS0.SSS0.Px2.p2.3.m2.1.1.3.5">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p2.3.m2.1c">N_{pred}</annotation></semantics></math>, <math id="S4.SS0.SSS0.Px2.p2.4.m3.1" class="ltx_Math" alttext="N_{gt}" display="inline"><semantics id="S4.SS0.SSS0.Px2.p2.4.m3.1a"><msub id="S4.SS0.SSS0.Px2.p2.4.m3.1.1" xref="S4.SS0.SSS0.Px2.p2.4.m3.1.1.cmml"><mi id="S4.SS0.SSS0.Px2.p2.4.m3.1.1.2" xref="S4.SS0.SSS0.Px2.p2.4.m3.1.1.2.cmml">N</mi><mrow id="S4.SS0.SSS0.Px2.p2.4.m3.1.1.3" xref="S4.SS0.SSS0.Px2.p2.4.m3.1.1.3.cmml"><mi id="S4.SS0.SSS0.Px2.p2.4.m3.1.1.3.2" xref="S4.SS0.SSS0.Px2.p2.4.m3.1.1.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px2.p2.4.m3.1.1.3.1" xref="S4.SS0.SSS0.Px2.p2.4.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px2.p2.4.m3.1.1.3.3" xref="S4.SS0.SSS0.Px2.p2.4.m3.1.1.3.3.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p2.4.m3.1b"><apply id="S4.SS0.SSS0.Px2.p2.4.m3.1.1.cmml" xref="S4.SS0.SSS0.Px2.p2.4.m3.1.1"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px2.p2.4.m3.1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p2.4.m3.1.1">subscript</csymbol><ci id="S4.SS0.SSS0.Px2.p2.4.m3.1.1.2.cmml" xref="S4.SS0.SSS0.Px2.p2.4.m3.1.1.2">𝑁</ci><apply id="S4.SS0.SSS0.Px2.p2.4.m3.1.1.3.cmml" xref="S4.SS0.SSS0.Px2.p2.4.m3.1.1.3"><times id="S4.SS0.SSS0.Px2.p2.4.m3.1.1.3.1.cmml" xref="S4.SS0.SSS0.Px2.p2.4.m3.1.1.3.1"></times><ci id="S4.SS0.SSS0.Px2.p2.4.m3.1.1.3.2.cmml" xref="S4.SS0.SSS0.Px2.p2.4.m3.1.1.3.2">𝑔</ci><ci id="S4.SS0.SSS0.Px2.p2.4.m3.1.1.3.3.cmml" xref="S4.SS0.SSS0.Px2.p2.4.m3.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p2.4.m3.1c">N_{gt}</annotation></semantics></math> are the numbers of true positives, predictions, and true objects.
We set <math id="S4.SS0.SSS0.Px2.p2.5.m4.3" class="ltx_Math" alttext="\tau=[0.25,0.5,0.7]" display="inline"><semantics id="S4.SS0.SSS0.Px2.p2.5.m4.3a"><mrow id="S4.SS0.SSS0.Px2.p2.5.m4.3.4" xref="S4.SS0.SSS0.Px2.p2.5.m4.3.4.cmml"><mi id="S4.SS0.SSS0.Px2.p2.5.m4.3.4.2" xref="S4.SS0.SSS0.Px2.p2.5.m4.3.4.2.cmml">τ</mi><mo id="S4.SS0.SSS0.Px2.p2.5.m4.3.4.1" xref="S4.SS0.SSS0.Px2.p2.5.m4.3.4.1.cmml">=</mo><mrow id="S4.SS0.SSS0.Px2.p2.5.m4.3.4.3.2" xref="S4.SS0.SSS0.Px2.p2.5.m4.3.4.3.1.cmml"><mo stretchy="false" id="S4.SS0.SSS0.Px2.p2.5.m4.3.4.3.2.1" xref="S4.SS0.SSS0.Px2.p2.5.m4.3.4.3.1.cmml">[</mo><mn id="S4.SS0.SSS0.Px2.p2.5.m4.1.1" xref="S4.SS0.SSS0.Px2.p2.5.m4.1.1.cmml">0.25</mn><mo id="S4.SS0.SSS0.Px2.p2.5.m4.3.4.3.2.2" xref="S4.SS0.SSS0.Px2.p2.5.m4.3.4.3.1.cmml">,</mo><mn id="S4.SS0.SSS0.Px2.p2.5.m4.2.2" xref="S4.SS0.SSS0.Px2.p2.5.m4.2.2.cmml">0.5</mn><mo id="S4.SS0.SSS0.Px2.p2.5.m4.3.4.3.2.3" xref="S4.SS0.SSS0.Px2.p2.5.m4.3.4.3.1.cmml">,</mo><mn id="S4.SS0.SSS0.Px2.p2.5.m4.3.3" xref="S4.SS0.SSS0.Px2.p2.5.m4.3.3.cmml">0.7</mn><mo stretchy="false" id="S4.SS0.SSS0.Px2.p2.5.m4.3.4.3.2.4" xref="S4.SS0.SSS0.Px2.p2.5.m4.3.4.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p2.5.m4.3b"><apply id="S4.SS0.SSS0.Px2.p2.5.m4.3.4.cmml" xref="S4.SS0.SSS0.Px2.p2.5.m4.3.4"><eq id="S4.SS0.SSS0.Px2.p2.5.m4.3.4.1.cmml" xref="S4.SS0.SSS0.Px2.p2.5.m4.3.4.1"></eq><ci id="S4.SS0.SSS0.Px2.p2.5.m4.3.4.2.cmml" xref="S4.SS0.SSS0.Px2.p2.5.m4.3.4.2">𝜏</ci><list id="S4.SS0.SSS0.Px2.p2.5.m4.3.4.3.1.cmml" xref="S4.SS0.SSS0.Px2.p2.5.m4.3.4.3.2"><cn type="float" id="S4.SS0.SSS0.Px2.p2.5.m4.1.1.cmml" xref="S4.SS0.SSS0.Px2.p2.5.m4.1.1">0.25</cn><cn type="float" id="S4.SS0.SSS0.Px2.p2.5.m4.2.2.cmml" xref="S4.SS0.SSS0.Px2.p2.5.m4.2.2">0.5</cn><cn type="float" id="S4.SS0.SSS0.Px2.p2.5.m4.3.3.cmml" xref="S4.SS0.SSS0.Px2.p2.5.m4.3.3">0.7</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p2.5.m4.3c">\tau=[0.25,0.5,0.7]</annotation></semantics></math>.</p>
</div>
</section>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Comparison to Other Methods</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.3" class="ltx_p">We compare our method to recent multi-view 3D object detection models.
Two adopt a DETR-style approach, one is a volumetric approach and one is a late fusion method.
<span id="S4.SS1.p1.3.1" class="ltx_text ltx_font_bold">DETR3D</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> follows DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and uses learnable queries to predict the 3D bounding boxes. The learnable queries in DETR3D predict the reference points which are then used to sample local appearance cues from the input views at the projected 2D locations and update the queries.
<span id="S4.SS1.p1.3.2" class="ltx_text ltx_font_bold">PETR</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> adopts a DETR architecture where queries encode the 3D location of the initial reference points and are then used to attend to 3D-aware input feature maps. The queries are updated via transformer layers.
Our approach differs from PETR in two distinct ways: (1) our queries encode both the 3D location and local appearance features of the 3D reference points, (2) we adopt recurrent decoding with attention to refine the 3D reference points. The refined location along with its corresponding appearance features is encoded by the queries for the next recurrent layer.
<span id="S4.SS1.p1.3.3" class="ltx_text ltx_font_bold">ImVoxelNet</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> fuses the input views into a voxel representation which is then used to cast 3D predictions.
Note that ImVoxelNet predicts axis-aligned 3D bounding boxes on ScanNet.
We retrain DETR3D, PETR, and ImVoxelNet to take as input the same views as our model and predict oriented 3D boxes.
We use the same 2D image backbone and detection head for DETR3D and PETR and follow the official implementation of Transformer architecture.
The voxel grid in ImVoxelNet is shaped <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="64\times 64\times 32" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mn id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">64</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">64</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p1.1.m1.1.1.1a" xref="S4.SS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS1.p1.1.m1.1.1.4" xref="S4.SS1.p1.1.m1.1.1.4.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><times id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">64</cn><cn type="integer" id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">64</cn><cn type="integer" id="S4.SS1.p1.1.m1.1.1.4.cmml" xref="S4.SS1.p1.1.m1.1.1.4">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">64\times 64\times 32</annotation></semantics></math> with a <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="0.08m" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mrow id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mn id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml">0.08</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p1.2.m2.1.1.1" xref="S4.SS1.p1.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS1.p1.2.m2.1.1.3" xref="S4.SS1.p1.2.m2.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><times id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1.1"></times><cn type="float" id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2">0.08</cn><ci id="S4.SS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">0.08m</annotation></semantics></math> voxel unit.
Finally, we compare to <span id="S4.SS1.p1.3.4" class="ltx_text ltx_font_bold">ODAM</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> a late fusion approach which processes single-view detections from all frames of a snippet by optimizing a multi-view objective.
All methods are evaluated identically following our evaluation protocol described above.
We select the confidence threshold <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mi id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><ci id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">s</annotation></semantics></math> for each method so as to maximize each model’s F1 score.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.3" class="ltx_p">Table <a href="#S4.T1" title="Table 1 ‣ 4 Experiments ‣ Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> compares performance on ScanNet with 3-frame snippets.
We report the F1 score for each object class and Prec./Rec./F1 for the average performance.
The complete table is provided in the supplementary.
We highlight some interesting observations.
Our method outperforms all baselines at the stricter IoU thresholds of 0.5 and 0.7, by <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="+3.2\%" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mo id="S4.SS1.p2.1.m1.1.1a" xref="S4.SS1.p2.1.m1.1.1.cmml">+</mo><mrow id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml"><mn id="S4.SS1.p2.1.m1.1.1.2.2" xref="S4.SS1.p2.1.m1.1.1.2.2.cmml">3.2</mn><mo id="S4.SS1.p2.1.m1.1.1.2.1" xref="S4.SS1.p2.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><plus id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"></plus><apply id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2"><csymbol cd="latexml" id="S4.SS1.p2.1.m1.1.1.2.1.cmml" xref="S4.SS1.p2.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.SS1.p2.1.m1.1.1.2.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2.2">3.2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">+3.2\%</annotation></semantics></math> and <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="+1.5\%" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mrow id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml"><mo id="S4.SS1.p2.2.m2.1.1a" xref="S4.SS1.p2.2.m2.1.1.cmml">+</mo><mrow id="S4.SS1.p2.2.m2.1.1.2" xref="S4.SS1.p2.2.m2.1.1.2.cmml"><mn id="S4.SS1.p2.2.m2.1.1.2.2" xref="S4.SS1.p2.2.m2.1.1.2.2.cmml">1.5</mn><mo id="S4.SS1.p2.2.m2.1.1.2.1" xref="S4.SS1.p2.2.m2.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><apply id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"><plus id="S4.SS1.p2.2.m2.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"></plus><apply id="S4.SS1.p2.2.m2.1.1.2.cmml" xref="S4.SS1.p2.2.m2.1.1.2"><csymbol cd="latexml" id="S4.SS1.p2.2.m2.1.1.2.1.cmml" xref="S4.SS1.p2.2.m2.1.1.2.1">percent</csymbol><cn type="float" id="S4.SS1.p2.2.m2.1.1.2.2.cmml" xref="S4.SS1.p2.2.m2.1.1.2.2">1.5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">+1.5\%</annotation></semantics></math> F1, respectively.
Compared to the DETR-style baselines <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, we note that our approach performs best on average for all IoU thresholds proving that capturing long-range appearance and geometry cues via our geometry- and appearance-informed queries is effective.
Our method surpasses the volumetric method ImVoxelNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> at the stricter IoU thresholds but ImVoxelNet has a small advantage of <math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="+0.6\%" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><mrow id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml"><mo id="S4.SS1.p2.3.m3.1.1a" xref="S4.SS1.p2.3.m3.1.1.cmml">+</mo><mrow id="S4.SS1.p2.3.m3.1.1.2" xref="S4.SS1.p2.3.m3.1.1.2.cmml"><mn id="S4.SS1.p2.3.m3.1.1.2.2" xref="S4.SS1.p2.3.m3.1.1.2.2.cmml">0.6</mn><mo id="S4.SS1.p2.3.m3.1.1.2.1" xref="S4.SS1.p2.3.m3.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><apply id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1"><plus id="S4.SS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1"></plus><apply id="S4.SS1.p2.3.m3.1.1.2.cmml" xref="S4.SS1.p2.3.m3.1.1.2"><csymbol cd="latexml" id="S4.SS1.p2.3.m3.1.1.2.1.cmml" xref="S4.SS1.p2.3.m3.1.1.2.1">percent</csymbol><cn type="float" id="S4.SS1.p2.3.m3.1.1.2.2.cmml" xref="S4.SS1.p2.3.m3.1.1.2.2">0.6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">+0.6\%</annotation></semantics></math> F1 at the 0.25 IoU threshold.
Since ImVoxelNet constructs a volume to fuse multi-view features, performance is tied to the voxel resolution. This leads to a competitive performance at the loose IoU threshold (0.25) but worse results at stricter IoU thresholds.
Increasing the voxel resolution could increase performance, but would result in a cubic increase in memory for the model.
Table <a href="#S4.T2" title="Table 2 ‣ 4 Experiments ‣ Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> reports the performance on the ARKitScenes dataset.
We report Prec./Rec./F1 for the average performance.
Our approach outperforms all baselines across all IoU thresholds in F1.
We provide the complete table in the supplementary.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Ablation Study</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We conduct several ablation experiments on ScanNet to validate the effectiveness of our model’s design choices, shown in Table <a href="#S4.T3" title="Table 3 ‣ 4.2 Ablation Study ‣ 4 Experiments ‣ Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
We summarize key findings below.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.3" class="ltx_p"><span id="S4.SS2.p2.3.1" class="ltx_text ltx_font_bold">Pixel-aligned queries perform better and train faster.</span>
To show the impact of appearance-informed queries we compare our model (1<math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="{}^{\textrm{st}}" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><msup id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mi id="S4.SS2.p2.1.m1.1.1a" xref="S4.SS2.p2.1.m1.1.1.cmml"></mi><mtext id="S4.SS2.p2.1.m1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1a.cmml">st</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><ci id="S4.SS2.p2.1.m1.1.1.1a.cmml" xref="S4.SS2.p2.1.m1.1.1.1"><mtext mathsize="70%" id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1">st</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">{}^{\textrm{st}}</annotation></semantics></math> row) to a variant without appearance cues in the queries, <span id="S4.SS2.p2.3.2" class="ltx_text ltx_font_italic">w/o PA</span> (2<math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="{}^{\textrm{nd}}" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><msup id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mi id="S4.SS2.p2.2.m2.1.1a" xref="S4.SS2.p2.2.m2.1.1.cmml"></mi><mtext id="S4.SS2.p2.2.m2.1.1.1" xref="S4.SS2.p2.2.m2.1.1.1a.cmml">nd</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><ci id="S4.SS2.p2.2.m2.1.1.1a.cmml" xref="S4.SS2.p2.2.m2.1.1.1"><mtext mathsize="70%" id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1.1">nd</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">{}^{\textrm{nd}}</annotation></semantics></math> row).
Appearance-informed queries perform better across all IoU thresholds and show a <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="+2.9\%" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><mrow id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml"><mo id="S4.SS2.p2.3.m3.1.1a" xref="S4.SS2.p2.3.m3.1.1.cmml">+</mo><mrow id="S4.SS2.p2.3.m3.1.1.2" xref="S4.SS2.p2.3.m3.1.1.2.cmml"><mn id="S4.SS2.p2.3.m3.1.1.2.2" xref="S4.SS2.p2.3.m3.1.1.2.2.cmml">2.9</mn><mo id="S4.SS2.p2.3.m3.1.1.2.1" xref="S4.SS2.p2.3.m3.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><apply id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1"><plus id="S4.SS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1"></plus><apply id="S4.SS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2"><csymbol cd="latexml" id="S4.SS2.p2.3.m3.1.1.2.1.cmml" xref="S4.SS2.p2.3.m3.1.1.2.1">percent</csymbol><cn type="float" id="S4.SS2.p2.3.m3.1.1.2.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2.2">2.9</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">+2.9\%</annotation></semantics></math> boost in F1 at 0.5 IoU.
Fig. <a href="#S5.F8" title="Figure 8 ‣ 5 Limitations ‣ Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> validates this observation by visualizing the attention maps.
Fig. <a href="#S5.F8" title="Figure 8 ‣ 5 Limitations ‣ Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>(d) qualitatively shows how queries <em id="S4.SS2.p2.3.3" class="ltx_emph ltx_font_italic">without</em> appearance information focus only on the local area around the reference point while our appearance-informed queries can capture long-range contextual interactions in the input views as shown in Fig. <a href="#S5.F8" title="Figure 8 ‣ 5 Limitations ‣ Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>(a-c).
Additionally, appearance-informed queries lead to faster convergence as shown in Fig. <a href="#S5.F6" title="Figure 6 ‣ 5 Limitations ‣ Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.
These results show that appearance cues in queries are effective.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:91.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-41.2pt,17.4pt) scale(0.724442525812184,0.724442525812184) ;">
<table id="S4.T3.3.3" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.3.3.3" class="ltx_tr">
<td id="S4.T3.3.3.3.4" class="ltx_td ltx_align_left">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span>
Prec./Rec./F1</td>
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center">@IoU<math id="S4.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.T3.1.1.1.1.m1.1a"><mo id="S4.T3.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><gt id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">&gt;</annotation></semantics></math>0.25</td>
<td id="S4.T3.2.2.2.2" class="ltx_td ltx_align_center">@IoU<math id="S4.T3.2.2.2.2.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.T3.2.2.2.2.m1.1a"><mo id="S4.T3.2.2.2.2.m1.1.1" xref="S4.T3.2.2.2.2.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.2.m1.1b"><gt id="S4.T3.2.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.2.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.2.m1.1c">&gt;</annotation></semantics></math>0.5</td>
<td id="S4.T3.3.3.3.3" class="ltx_td ltx_align_center">@IoU<math id="S4.T3.3.3.3.3.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.T3.3.3.3.3.m1.1a"><mo id="S4.T3.3.3.3.3.m1.1.1" xref="S4.T3.3.3.3.3.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.3.m1.1b"><gt id="S4.T3.3.3.3.3.m1.1.1.cmml" xref="S4.T3.3.3.3.3.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.3.m1.1c">&gt;</annotation></semantics></math>0.7</td>
</tr>
<tr id="S4.T3.3.3.4" class="ltx_tr">
<td id="S4.T3.3.3.4.1" class="ltx_td ltx_align_left ltx_border_t">PARQ</td>
<td id="S4.T3.3.3.4.2" class="ltx_td ltx_align_center ltx_border_t">54.2/<span id="S4.T3.3.3.4.2.1" class="ltx_text ltx_font_bold">48.2</span>/<span id="S4.T3.3.3.4.2.2" class="ltx_text ltx_font_bold">51.1</span>
</td>
<td id="S4.T3.3.3.4.3" class="ltx_td ltx_align_center ltx_border_t"> <span id="S4.T3.3.3.4.3.1" class="ltx_text ltx_font_bold">31.8</span>/<span id="S4.T3.3.3.4.3.2" class="ltx_text ltx_font_bold">28.3</span>/<span id="S4.T3.3.3.4.3.3" class="ltx_text ltx_font_bold">30.0</span>
</td>
<td id="S4.T3.3.3.4.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T3.3.3.4.4.1" class="ltx_text ltx_font_bold">6.5</span>/<span id="S4.T3.3.3.4.4.2" class="ltx_text ltx_font_bold">5.8</span>/<span id="S4.T3.3.3.4.4.3" class="ltx_text ltx_font_bold">6.1</span>
</td>
</tr>
<tr id="S4.T3.3.3.5" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S4.T3.3.3.5.1" class="ltx_td ltx_align_left"><span id="S4.T3.3.3.5.1.1" class="ltx_text" style="background-color:#E6E6E6;">w/o PA</span></td>
<td id="S4.T3.3.3.5.2" class="ltx_td ltx_align_center"><span id="S4.T3.3.3.5.2.1" class="ltx_text" style="background-color:#E6E6E6;">52.3/48.2/50.2</span></td>
<td id="S4.T3.3.3.5.3" class="ltx_td ltx_align_center"><span id="S4.T3.3.3.5.3.1" class="ltx_text" style="background-color:#E6E6E6;">28.6/25.8/27.1</span></td>
<td id="S4.T3.3.3.5.4" class="ltx_td ltx_align_center"><span id="S4.T3.3.3.5.4.1" class="ltx_text" style="background-color:#E6E6E6;">5.4/4.9/5.1</span></td>
</tr>
<tr id="S4.T3.3.3.6" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S4.T3.3.3.6.1" class="ltx_td ltx_align_left"><span id="S4.T3.3.3.6.1.1" class="ltx_text" style="background-color:#E6E6E6;">w/o recurrence</span></td>
<td id="S4.T3.3.3.6.2" class="ltx_td ltx_align_center"><span id="S4.T3.3.3.6.2.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">54.3<span id="S4.T3.3.3.6.2.1.1" class="ltx_text ltx_font_medium" style="background-color:#E6E6E6;">/47.9/50.9</span></span></td>
<td id="S4.T3.3.3.6.3" class="ltx_td ltx_align_center"><span id="S4.T3.3.3.6.3.1" class="ltx_text" style="background-color:#E6E6E6;">27.9/27.2/27.6</span></td>
<td id="S4.T3.3.3.6.4" class="ltx_td ltx_align_center"><span id="S4.T3.3.3.6.4.1" class="ltx_text" style="background-color:#E6E6E6;">6.0/5.1/5.5</span></td>
</tr>
<tr id="S4.T3.3.3.7" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S4.T3.3.3.7.1" class="ltx_td ltx_align_left"><span id="S4.T3.3.3.7.1.1" class="ltx_text" style="background-color:#E6E6E6;">w/o point PE</span></td>
<td id="S4.T3.3.3.7.2" class="ltx_td ltx_align_center"><span id="S4.T3.3.3.7.2.1" class="ltx_text" style="background-color:#E6E6E6;">12.0/22.4/15.6</span></td>
<td id="S4.T3.3.3.7.3" class="ltx_td ltx_align_center"><span id="S4.T3.3.3.7.3.1" class="ltx_text" style="background-color:#E6E6E6;">9.1/12.2/10.4</span></td>
<td id="S4.T3.3.3.7.4" class="ltx_td ltx_align_center"><span id="S4.T3.3.3.7.4.1" class="ltx_text" style="background-color:#E6E6E6;">1.0/1.2/1.1</span></td>
</tr>
<tr id="S4.T3.3.3.8" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S4.T3.3.3.8.1" class="ltx_td ltx_align_left"><span id="S4.T3.3.3.8.1.1" class="ltx_text" style="background-color:#E6E6E6;">w/o ray PE</span></td>
<td id="S4.T3.3.3.8.2" class="ltx_td ltx_align_center"><span id="S4.T3.3.3.8.2.1" class="ltx_text" style="background-color:#E6E6E6;">53.4/48.0/50.6</span></td>
<td id="S4.T3.3.3.8.3" class="ltx_td ltx_align_center"><span id="S4.T3.3.3.8.3.1" class="ltx_text" style="background-color:#E6E6E6;">29.4/27.1/28.2</span></td>
<td id="S4.T3.3.3.8.4" class="ltx_td ltx_align_center"><span id="S4.T3.3.3.8.4.1" class="ltx_text" style="background-color:#E6E6E6;">5.9/5.3/5.6</span></td>
</tr>
<tr id="S4.T3.3.3.9" class="ltx_tr">
<td id="S4.T3.3.3.9.1" class="ltx_td ltx_align_left"><span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></td>
<td id="S4.T3.3.3.9.2" class="ltx_td"></td>
<td id="S4.T3.3.3.9.3" class="ltx_td"></td>
<td id="S4.T3.3.3.9.4" class="ltx_td"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>
<span id="S4.T3.5.1" class="ltx_text ltx_font_bold">PARQ ablations.</span> We compare PARQ on ScanNet with variants that remove pixel-aligned features from the queries (w/o PA), perform no recurrent 3D refinement but updates query features as in PETR (w/o recurrence), remove the positional encoding of 3D points in queries (w/o point PE) and remove the ray positional encoding in image features (w/o ray PE).
</figcaption>
</figure>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.2" class="ltx_p"><span id="S4.SS2.p3.2.1" class="ltx_text ltx_font_bold">Recurrent refinement improves performance.</span>
Compared to a variant without recurrent refinement similar to PETR, <span id="S4.SS2.p3.2.2" class="ltx_text ltx_font_italic">w/o recurrence</span> (3<math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="{}^{\textrm{rd}}" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><msup id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mi id="S4.SS2.p3.1.m1.1.1a" xref="S4.SS2.p3.1.m1.1.1.cmml"></mi><mtext id="S4.SS2.p3.1.m1.1.1.1" xref="S4.SS2.p3.1.m1.1.1.1a.cmml">rd</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><ci id="S4.SS2.p3.1.m1.1.1.1a.cmml" xref="S4.SS2.p3.1.m1.1.1.1"><mtext mathsize="70%" id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1">rd</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">{}^{\textrm{rd}}</annotation></semantics></math> row), our proposed recurrent update strategy gives up to <math id="S4.SS2.p3.2.m2.1" class="ltx_Math" alttext="+2.0\%" display="inline"><semantics id="S4.SS2.p3.2.m2.1a"><mrow id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml"><mo id="S4.SS2.p3.2.m2.1.1a" xref="S4.SS2.p3.2.m2.1.1.cmml">+</mo><mrow id="S4.SS2.p3.2.m2.1.1.2" xref="S4.SS2.p3.2.m2.1.1.2.cmml"><mn id="S4.SS2.p3.2.m2.1.1.2.2" xref="S4.SS2.p3.2.m2.1.1.2.2.cmml">2.0</mn><mo id="S4.SS2.p3.2.m2.1.1.2.1" xref="S4.SS2.p3.2.m2.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><apply id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1"><plus id="S4.SS2.p3.2.m2.1.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1"></plus><apply id="S4.SS2.p3.2.m2.1.1.2.cmml" xref="S4.SS2.p3.2.m2.1.1.2"><csymbol cd="latexml" id="S4.SS2.p3.2.m2.1.1.2.1.cmml" xref="S4.SS2.p3.2.m2.1.1.2.1">percent</csymbol><cn type="float" id="S4.SS2.p3.2.m2.1.1.2.2.cmml" xref="S4.SS2.p3.2.m2.1.1.2.2">2.0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">+2.0\%</annotation></semantics></math> boost in F1.
Fig. <a href="#S5.F8" title="Figure 8 ‣ 5 Limitations ‣ Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>(b) visualizes the attention maps after each iteration.
During our updates, the appearance-informed query attends to the target object, even at iteration 0 where the target is furthest away, and moves closer to the target with every update.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">Note that PETR updates queries after every transformer layer in the decoder.
However, the new queries are the output of the previous attention layer and don’t encode the newly predicted 3D locations.
This is a stark difference from our approach which updates queries with new appearance and geometry information after each iteration.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para ltx_noindent">
<p id="S4.SS2.p5.1" class="ltx_p"><span id="S4.SS2.p5.1.1" class="ltx_text ltx_font_bold">Geometry-informed queries are critical.</span>
Embedding the 3D reference points in the queries is critical, as shown by our ablation <span id="S4.SS2.p5.1.2" class="ltx_text ltx_font_italic">w/o point PE</span> (4<math id="S4.SS2.p5.1.m1.1" class="ltx_Math" alttext="{}^{\textrm{th}}" display="inline"><semantics id="S4.SS2.p5.1.m1.1a"><msup id="S4.SS2.p5.1.m1.1.1" xref="S4.SS2.p5.1.m1.1.1.cmml"><mi id="S4.SS2.p5.1.m1.1.1a" xref="S4.SS2.p5.1.m1.1.1.cmml"></mi><mtext id="S4.SS2.p5.1.m1.1.1.1" xref="S4.SS2.p5.1.m1.1.1.1a.cmml">th</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.1.m1.1b"><apply id="S4.SS2.p5.1.m1.1.1.cmml" xref="S4.SS2.p5.1.m1.1.1"><ci id="S4.SS2.p5.1.m1.1.1.1a.cmml" xref="S4.SS2.p5.1.m1.1.1.1"><mtext mathsize="70%" id="S4.SS2.p5.1.m1.1.1.1.cmml" xref="S4.SS2.p5.1.m1.1.1.1">th</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.1.m1.1c">{}^{\textrm{th}}</annotation></semantics></math> row).
This is not a surprise as our model is tasked to predict the target offset from the 3D reference point, so knowledge of its 3D location is helpful.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para ltx_noindent">
<p id="S4.SS2.p6.1" class="ltx_p"><span id="S4.SS2.p6.1.1" class="ltx_text ltx_font_bold">3D-aware input features are helpful.</span>
We remove ray positional encodings in the input feature maps, proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, in <span id="S4.SS2.p6.1.2" class="ltx_text ltx_font_italic">w/o ray PE</span> (5<math id="S4.SS2.p6.1.m1.1" class="ltx_Math" alttext="{}^{\textrm{th}}" display="inline"><semantics id="S4.SS2.p6.1.m1.1a"><msup id="S4.SS2.p6.1.m1.1.1" xref="S4.SS2.p6.1.m1.1.1.cmml"><mi id="S4.SS2.p6.1.m1.1.1a" xref="S4.SS2.p6.1.m1.1.1.cmml"></mi><mtext id="S4.SS2.p6.1.m1.1.1.1" xref="S4.SS2.p6.1.m1.1.1.1a.cmml">th</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.1.m1.1b"><apply id="S4.SS2.p6.1.m1.1.1.cmml" xref="S4.SS2.p6.1.m1.1.1"><ci id="S4.SS2.p6.1.m1.1.1.1a.cmml" xref="S4.SS2.p6.1.m1.1.1.1"><mtext mathsize="70%" id="S4.SS2.p6.1.m1.1.1.1.cmml" xref="S4.SS2.p6.1.m1.1.1.1">th</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.1.m1.1c">{}^{\textrm{th}}</annotation></semantics></math> row).
We notice that ray positional encodings slightly boost performance.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<div id="S4.T4.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:37.9pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-151.7pt,26.2pt) scale(0.416788557523386,0.416788557523386) ;">
<table id="S4.T4.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T4.3.1.1" class="ltx_tr">
<td id="S4.T4.3.1.1.1" class="ltx_td ltx_align_left">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span>
F1</td>
<td id="S4.T4.3.1.1.2" class="ltx_td ltx_align_center">1 view</td>
<td id="S4.T4.3.1.1.3" class="ltx_td ltx_align_center">2 views</td>
<td id="S4.T4.3.1.1.4" class="ltx_td ltx_align_center">3 views</td>
<td id="S4.T4.3.1.1.5" class="ltx_td ltx_align_center">5 views</td>
<td id="S4.T4.3.1.1.6" class="ltx_td ltx_align_center">7 views</td>
<td id="S4.T4.3.1.1.7" class="ltx_td ltx_align_center">9 views</td>
</tr>
<tr id="S4.T4.3.1.2" class="ltx_tr">
<td id="S4.T4.3.1.2.1" class="ltx_td ltx_align_left ltx_border_t">ImVoxelNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</td>
<td id="S4.T4.3.1.2.2" class="ltx_td ltx_align_center ltx_border_t">25.0</td>
<td id="S4.T4.3.1.2.3" class="ltx_td ltx_align_center ltx_border_t">26.6</td>
<td id="S4.T4.3.1.2.4" class="ltx_td ltx_align_center ltx_border_t">26.8</td>
<td id="S4.T4.3.1.2.5" class="ltx_td ltx_align_center ltx_border_t">26.5</td>
<td id="S4.T4.3.1.2.6" class="ltx_td ltx_align_center ltx_border_t">25.0</td>
<td id="S4.T4.3.1.2.7" class="ltx_td ltx_align_center ltx_border_t">23.3</td>
</tr>
<tr id="S4.T4.3.1.3" class="ltx_tr">
<td id="S4.T4.3.1.3.1" class="ltx_td ltx_align_left">PETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>
</td>
<td id="S4.T4.3.1.3.2" class="ltx_td ltx_align_center">21.3</td>
<td id="S4.T4.3.1.3.3" class="ltx_td ltx_align_center">21.7</td>
<td id="S4.T4.3.1.3.4" class="ltx_td ltx_align_center">25.6</td>
<td id="S4.T4.3.1.3.5" class="ltx_td ltx_align_center">24.5</td>
<td id="S4.T4.3.1.3.6" class="ltx_td ltx_align_center">24.4</td>
<td id="S4.T4.3.1.3.7" class="ltx_td ltx_align_center">21.0</td>
</tr>
<tr id="S4.T4.3.1.4" class="ltx_tr">
<td id="S4.T4.3.1.4.1" class="ltx_td ltx_align_left">PARQ (ours)</td>
<td id="S4.T4.3.1.4.2" class="ltx_td ltx_align_center"><span id="S4.T4.3.1.4.2.1" class="ltx_text ltx_font_bold">26.3</span></td>
<td id="S4.T4.3.1.4.3" class="ltx_td ltx_align_center"><span id="S4.T4.3.1.4.3.1" class="ltx_text ltx_font_bold">27.6</span></td>
<td id="S4.T4.3.1.4.4" class="ltx_td ltx_align_center"><span id="S4.T4.3.1.4.4.1" class="ltx_text ltx_font_bold">30.0</span></td>
<td id="S4.T4.3.1.4.5" class="ltx_td ltx_align_center"> <span id="S4.T4.3.1.4.5.1" class="ltx_text ltx_font_bold">31.1</span>
</td>
<td id="S4.T4.3.1.4.6" class="ltx_td ltx_align_center"><span id="S4.T4.3.1.4.6.1" class="ltx_text ltx_font_bold">31.3</span></td>
<td id="S4.T4.3.1.4.7" class="ltx_td ltx_align_center"><span id="S4.T4.3.1.4.7.1" class="ltx_text ltx_font_bold">31.1</span></td>
</tr>
<tr id="S4.T4.3.1.5" class="ltx_tr">
<td id="S4.T4.3.1.5.1" class="ltx_td ltx_align_left"><span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></td>
<td id="S4.T4.3.1.5.2" class="ltx_td"></td>
<td id="S4.T4.3.1.5.3" class="ltx_td"></td>
<td id="S4.T4.3.1.5.4" class="ltx_td"></td>
<td id="S4.T4.3.1.5.5" class="ltx_td"></td>
<td id="S4.T4.3.1.5.6" class="ltx_td"></td>
<td id="S4.T4.3.1.5.7" class="ltx_td"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span><span id="S4.T4.5.1" class="ltx_text ltx_font_bold">Number of views</span>. We test models on different number of views. We train with 3 views and report F1 at @IoU <math id="S4.T4.2.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.T4.2.m1.1b"><mo id="S4.T4.2.m1.1.1" xref="S4.T4.2.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.m1.1c"><gt id="S4.T4.2.m1.1.1.cmml" xref="S4.T4.2.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.m1.1d">&gt;</annotation></semantics></math> 0.5.</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Deeper Dive into PARQ</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Next, we investigate the properties of PARQ to better understand its characteristics and highlight its strengths.
The experiments are conducted on the ScanNet dataset.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Dynamic queries.</span>
The benefit of transformer architectures is that they can generalize to varying set lengths and are not tied to fixed-resolution inputs.
For 3D tasks, this is important as one might want to query models with varying numbers of 3D points depending on the resolution of the scene.
But how robust are methods to varying numbers of queries?
We compare PARQ to PETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> to demonstrate their efficacy when queried dynamically.
We train both PETR and our model with 256 queries and test the model with different numbers of queries.
Because performance is affected by point distribution, especially in the case of fewer queries, we run each setting 6 times and report the mean/max/min performance in Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.3 Deeper Dive into PARQ ‣ 4 Experiments ‣ Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
When testing with 256 queries (same as during training), we test once with the same distribution of points as during training.
We note that our model is significantly more robust to dynamic queries.
When the number of queries deviates from training, PETR’s performance drops.
This suggests that PETR is prone to overfitting the training distribution.
This connects to our findings from Table <a href="#S4.T3" title="Table 3 ‣ 4.2 Ablation Study ‣ 4 Experiments ‣ Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and Fig. <a href="#S5.F8" title="Figure 8 ‣ 5 Limitations ‣ Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.
By encoding only geometric cues in the queries, the model focuses on local regions and cannot detect far-away objects which is the case when the queries are few.
This is qualitatively shown in Fig. <a href="#S5.F8" title="Figure 8 ‣ 5 Limitations ‣ Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>(d).</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2310.01401/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="256" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>
<span id="S4.F4.2.1" class="ltx_text ltx_font_bold">Varying the number of queries at inference.</span>
We run each setting 6 times and report the mean/max/min performance.
</figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2310.01401/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="205" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>
<span id="S4.F5.2.1" class="ltx_text ltx_font_bold">Varying the number of query updates at inference.</span>
Note PETR doesn’t share weights in each layer which means we cannot show its performance for more layers than trained for (6).
</figcaption>
</figure>
<div id="S4.SS3.p3" class="ltx_para ltx_noindent">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_bold">Recurrent refinement.</span>
PARQ adopts a recurrent update strategy during training performed by one PARQ layer.
This design allows us to perform an arbitrary number of refining iterations during inference.
We explore the effect of varying the number of PARQ iterations in Fig. <a href="#S4.F5" title="Figure 5 ‣ 4.3 Deeper Dive into PARQ ‣ 4 Experiments ‣ Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and compare to PETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.
The official implementation of PETR uses 6 layers and does not share weights across layers.
We report performance from layer 1 to 6.
All models are trained with 256 queries and PARQ is trained to perform 8 iterations.
While PARQ’s performance keeps increasing with more layers, it reaches close to maximal performance in just two layers.
PETR also benefits from updates but lags in performance.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para ltx_noindent">
<p id="S4.SS3.p4.1" class="ltx_p"><span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_bold">Number of views.</span>
Our experiments use 3 view video snippets during training and testing.
However, our model can input any number of views during inference.
We show performance for {1, 2, 3, 5, 7, 9} views during testing in Table <a href="#S4.T4" title="Table 4 ‣ 4.2 Ablation Study ‣ 4 Experiments ‣ Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
We also compare to PETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> and ImVoxelNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> under the same setting.
All models are trained with 3 views and tested with the number of views indicated.
We observe that PETR and ImVoxelNet perform equally or worse when the number of test views is increased.
PARQ sees a <math id="S4.SS3.p4.1.m1.1" class="ltx_Math" alttext="+1.3\%" display="inline"><semantics id="S4.SS3.p4.1.m1.1a"><mrow id="S4.SS3.p4.1.m1.1.1" xref="S4.SS3.p4.1.m1.1.1.cmml"><mo id="S4.SS3.p4.1.m1.1.1a" xref="S4.SS3.p4.1.m1.1.1.cmml">+</mo><mrow id="S4.SS3.p4.1.m1.1.1.2" xref="S4.SS3.p4.1.m1.1.1.2.cmml"><mn id="S4.SS3.p4.1.m1.1.1.2.2" xref="S4.SS3.p4.1.m1.1.1.2.2.cmml">1.3</mn><mo id="S4.SS3.p4.1.m1.1.1.2.1" xref="S4.SS3.p4.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.1.m1.1b"><apply id="S4.SS3.p4.1.m1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1"><plus id="S4.SS3.p4.1.m1.1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1"></plus><apply id="S4.SS3.p4.1.m1.1.1.2.cmml" xref="S4.SS3.p4.1.m1.1.1.2"><csymbol cd="latexml" id="S4.SS3.p4.1.m1.1.1.2.1.cmml" xref="S4.SS3.p4.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.SS3.p4.1.m1.1.1.2.2.cmml" xref="S4.SS3.p4.1.m1.1.1.2.2">1.3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.1.m1.1c">+1.3\%</annotation></semantics></math> performance boost in F1 by increasing the number of views.</p>
</div>
<div id="S4.SS3.p5" class="ltx_para ltx_noindent">
<p id="S4.SS3.p5.1" class="ltx_p"><span id="S4.SS3.p5.1.1" class="ltx_text ltx_font_bold">Efficiency.</span>
We report running stats in Table <a href="#S5.T5" title="Table 5 ‣ 5 Limitations ‣ Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
All experiments are conducted on an NVIDIA A5000.
Our model is the most lightweight compared to other baselines.
Our model has an adaptable computation budget at inference which can be controlled with the number of iterations and queries. Based on Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.3 Deeper Dive into PARQ ‣ 4 Experiments ‣ Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and Fig. <a href="#S4.F5" title="Figure 5 ‣ 4.3 Deeper Dive into PARQ ‣ 4 Experiments ‣ Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> we select 32 queries and 2 layers. At these settings, PARQ runs faster (105ms) and detects objects better (F1 <math id="S4.SS3.p5.1.m1.1" class="ltx_Math" alttext="28.2" display="inline"><semantics id="S4.SS3.p5.1.m1.1a"><mn id="S4.SS3.p5.1.m1.1.1" xref="S4.SS3.p5.1.m1.1.1.cmml">28.2</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p5.1.m1.1b"><cn type="float" id="S4.SS3.p5.1.m1.1.1.cmml" xref="S4.SS3.p5.1.m1.1.1">28.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p5.1.m1.1c">28.2</annotation></semantics></math>) than related work.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Limitations</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We observe that our approach can fail when detecting large objects, objects with a similar color
to the background (<em id="S5.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, black object in the dark), and objects with heavy occlusion.
We provide failure cases in the supplementary.</p>
</div>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2310.01401/assets/x6.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="230" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>
<span id="S5.F6.2.1" class="ltx_text ltx_font_bold">Convergence speed</span> for PARQ and competing methods.
</figcaption>
</figure>
<figure id="S5.T5" class="ltx_table">
<div id="S5.T5.9" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:92.3pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-167.8pt,71.1pt) scale(0.392536446383286,0.392536446383286) ;">
<table id="S5.T5.9.9" class="ltx_tabular ltx_align_middle">
<tr id="S5.T5.1.1.1" class="ltx_tr">
<td id="S5.T5.1.1.1.2" class="ltx_td ltx_align_center">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span>
Method</td>
<td id="S5.T5.1.1.1.3" class="ltx_td ltx_align_center">Time</td>
<td id="S5.T5.1.1.1.4" class="ltx_td ltx_align_center">Test Mem.</td>
<td id="S5.T5.1.1.1.5" class="ltx_td ltx_align_center">Train Mem.</td>
<td id="S5.T5.1.1.1.6" class="ltx_td ltx_align_center">Param.</td>
<td id="S5.T5.1.1.1.1" class="ltx_td ltx_align_center">F1<math id="S5.T5.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T5.1.1.1.1.m1.1a"><mo stretchy="false" id="S5.T5.1.1.1.1.m1.1.1" xref="S5.T5.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T5.1.1.1.1.m1.1b"><ci id="S5.T5.1.1.1.1.m1.1.1.cmml" xref="S5.T5.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T5.7.7.7" class="ltx_tr">
<td id="S5.T5.7.7.7.7" class="ltx_td"></td>
<td id="S5.T5.3.3.3.2" class="ltx_td ltx_align_center">(<math id="S5.T5.2.2.2.1.m1.1" class="ltx_Math" alttext="ms" display="inline"><semantics id="S5.T5.2.2.2.1.m1.1a"><mrow id="S5.T5.2.2.2.1.m1.1.1" xref="S5.T5.2.2.2.1.m1.1.1.cmml"><mi id="S5.T5.2.2.2.1.m1.1.1.2" xref="S5.T5.2.2.2.1.m1.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.T5.2.2.2.1.m1.1.1.1" xref="S5.T5.2.2.2.1.m1.1.1.1.cmml">​</mo><mi id="S5.T5.2.2.2.1.m1.1.1.3" xref="S5.T5.2.2.2.1.m1.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T5.2.2.2.1.m1.1b"><apply id="S5.T5.2.2.2.1.m1.1.1.cmml" xref="S5.T5.2.2.2.1.m1.1.1"><times id="S5.T5.2.2.2.1.m1.1.1.1.cmml" xref="S5.T5.2.2.2.1.m1.1.1.1"></times><ci id="S5.T5.2.2.2.1.m1.1.1.2.cmml" xref="S5.T5.2.2.2.1.m1.1.1.2">𝑚</ci><ci id="S5.T5.2.2.2.1.m1.1.1.3.cmml" xref="S5.T5.2.2.2.1.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.2.2.1.m1.1c">ms</annotation></semantics></math>)<math id="S5.T5.3.3.3.2.m2.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T5.3.3.3.2.m2.1a"><mo stretchy="false" id="S5.T5.3.3.3.2.m2.1.1" xref="S5.T5.3.3.3.2.m2.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T5.3.3.3.2.m2.1b"><ci id="S5.T5.3.3.3.2.m2.1.1.cmml" xref="S5.T5.3.3.3.2.m2.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.3.3.3.2.m2.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T5.4.4.4.3" class="ltx_td ltx_align_center">(GB)<math id="S5.T5.4.4.4.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T5.4.4.4.3.m1.1a"><mo stretchy="false" id="S5.T5.4.4.4.3.m1.1.1" xref="S5.T5.4.4.4.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T5.4.4.4.3.m1.1b"><ci id="S5.T5.4.4.4.3.m1.1.1.cmml" xref="S5.T5.4.4.4.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.4.4.4.3.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T5.5.5.5.4" class="ltx_td ltx_align_center">(GB)<math id="S5.T5.5.5.5.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T5.5.5.5.4.m1.1a"><mo stretchy="false" id="S5.T5.5.5.5.4.m1.1.1" xref="S5.T5.5.5.5.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T5.5.5.5.4.m1.1b"><ci id="S5.T5.5.5.5.4.m1.1.1.cmml" xref="S5.T5.5.5.5.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.5.5.5.4.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T5.6.6.6.5" class="ltx_td ltx_align_center">(M)<math id="S5.T5.6.6.6.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T5.6.6.6.5.m1.1a"><mo stretchy="false" id="S5.T5.6.6.6.5.m1.1.1" xref="S5.T5.6.6.6.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T5.6.6.6.5.m1.1b"><ci id="S5.T5.6.6.6.5.m1.1.1.cmml" xref="S5.T5.6.6.6.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.6.6.6.5.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T5.7.7.7.6" class="ltx_td ltx_align_center">@IoU <math id="S5.T5.7.7.7.6.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S5.T5.7.7.7.6.m1.1a"><mo id="S5.T5.7.7.7.6.m1.1.1" xref="S5.T5.7.7.7.6.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S5.T5.7.7.7.6.m1.1b"><gt id="S5.T5.7.7.7.6.m1.1.1.cmml" xref="S5.T5.7.7.7.6.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.7.7.7.6.m1.1c">&gt;</annotation></semantics></math> 0.5</td>
</tr>
<tr id="S5.T5.9.9.10" class="ltx_tr">
<td id="S5.T5.9.9.10.1" class="ltx_td ltx_align_center ltx_border_t">ImVoxelNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</td>
<td id="S5.T5.9.9.10.2" class="ltx_td ltx_align_center ltx_border_t">127</td>
<td id="S5.T5.9.9.10.3" class="ltx_td ltx_align_center ltx_border_t">3.08</td>
<td id="S5.T5.9.9.10.4" class="ltx_td ltx_align_center ltx_border_t">6.84</td>
<td id="S5.T5.9.9.10.5" class="ltx_td ltx_align_center ltx_border_t">112.0</td>
<td id="S5.T5.9.9.10.6" class="ltx_td ltx_align_center ltx_border_t">26.8</td>
</tr>
<tr id="S5.T5.9.9.11" class="ltx_tr">
<td id="S5.T5.9.9.11.1" class="ltx_td ltx_align_center">DETR3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>
</td>
<td id="S5.T5.9.9.11.2" class="ltx_td ltx_align_center">220</td>
<td id="S5.T5.9.9.11.3" class="ltx_td ltx_align_center"><span id="S5.T5.9.9.11.3.1" class="ltx_text ltx_font_bold">2.63</span></td>
<td id="S5.T5.9.9.11.4" class="ltx_td ltx_align_center"><span id="S5.T5.9.9.11.4.1" class="ltx_text ltx_font_bold">3.93</span></td>
<td id="S5.T5.9.9.11.5" class="ltx_td ltx_align_center">57.1</td>
<td id="S5.T5.9.9.11.6" class="ltx_td ltx_align_center">11.4</td>
</tr>
<tr id="S5.T5.9.9.12" class="ltx_tr">
<td id="S5.T5.9.9.12.1" class="ltx_td ltx_align_center">PETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>
</td>
<td id="S5.T5.9.9.12.2" class="ltx_td ltx_align_center">248</td>
<td id="S5.T5.9.9.12.3" class="ltx_td ltx_align_center">3.74</td>
<td id="S5.T5.9.9.12.4" class="ltx_td ltx_align_center">8.39</td>
<td id="S5.T5.9.9.12.5" class="ltx_td ltx_align_center">131.5</td>
<td id="S5.T5.9.9.12.6" class="ltx_td ltx_align_center">25.6</td>
</tr>
<tr id="S5.T5.8.8.8" class="ltx_tr">
<td id="S5.T5.8.8.8.2" class="ltx_td ltx_align_center" rowspan="2"><span id="S5.T5.8.8.8.2.1" class="ltx_text">PARQ (ours)</span></td>
<td id="S5.T5.8.8.8.1" class="ltx_td ltx_align_center">
<math id="S5.T5.8.8.8.1.m1.1" class="ltx_Math" alttext="\textbf{105}^{*}" display="inline"><semantics id="S5.T5.8.8.8.1.m1.1a"><msup id="S5.T5.8.8.8.1.m1.1.1" xref="S5.T5.8.8.8.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S5.T5.8.8.8.1.m1.1.1.2" xref="S5.T5.8.8.8.1.m1.1.1.2a.cmml">105</mtext><mo id="S5.T5.8.8.8.1.m1.1.1.3" xref="S5.T5.8.8.8.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S5.T5.8.8.8.1.m1.1b"><apply id="S5.T5.8.8.8.1.m1.1.1.cmml" xref="S5.T5.8.8.8.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T5.8.8.8.1.m1.1.1.1.cmml" xref="S5.T5.8.8.8.1.m1.1.1">superscript</csymbol><ci id="S5.T5.8.8.8.1.m1.1.1.2a.cmml" xref="S5.T5.8.8.8.1.m1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S5.T5.8.8.8.1.m1.1.1.2.cmml" xref="S5.T5.8.8.8.1.m1.1.1.2">105</mtext></ci><times id="S5.T5.8.8.8.1.m1.1.1.3.cmml" xref="S5.T5.8.8.8.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.8.8.8.1.m1.1c">\textbf{105}^{*}</annotation></semantics></math>–</td>
<td id="S5.T5.8.8.8.3" class="ltx_td ltx_align_center" rowspan="2"><span id="S5.T5.8.8.8.3.1" class="ltx_text">2.82</span></td>
<td id="S5.T5.8.8.8.4" class="ltx_td ltx_align_center" rowspan="2"><span id="S5.T5.8.8.8.4.1" class="ltx_text">5.83</span></td>
<td id="S5.T5.8.8.8.5" class="ltx_td ltx_align_center" rowspan="2"><span id="S5.T5.8.8.8.5.1" class="ltx_text ltx_font_bold">44.7</span></td>
<td id="S5.T5.8.8.8.6" class="ltx_td ltx_align_center"><span id="S5.T5.8.8.8.6.1" class="ltx_text ltx_font_bold">28.2 –</span></td>
</tr>
<tr id="S5.T5.9.9.13" class="ltx_tr">
<td id="S5.T5.9.9.13.1" class="ltx_td ltx_align_center">290 (below)</td>
<td id="S5.T5.9.9.13.2" class="ltx_td ltx_align_center"><span id="S5.T5.9.9.13.2.1" class="ltx_text ltx_font_bold">31.2</span></td>
</tr>
<tr id="S5.T5.9.9.9" class="ltx_tr">
<td id="S5.T5.9.9.9.2" class="ltx_td ltx_align_center">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span>
PARQ (ours)</td>
<td id="S5.T5.9.9.9.1" class="ltx_td ltx_align_center" colspan="5">Time (<math id="S5.T5.9.9.9.1.m1.1" class="ltx_Math" alttext="ms" display="inline"><semantics id="S5.T5.9.9.9.1.m1.1a"><mrow id="S5.T5.9.9.9.1.m1.1.1" xref="S5.T5.9.9.9.1.m1.1.1.cmml"><mi id="S5.T5.9.9.9.1.m1.1.1.2" xref="S5.T5.9.9.9.1.m1.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.T5.9.9.9.1.m1.1.1.1" xref="S5.T5.9.9.9.1.m1.1.1.1.cmml">​</mo><mi id="S5.T5.9.9.9.1.m1.1.1.3" xref="S5.T5.9.9.9.1.m1.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T5.9.9.9.1.m1.1b"><apply id="S5.T5.9.9.9.1.m1.1.1.cmml" xref="S5.T5.9.9.9.1.m1.1.1"><times id="S5.T5.9.9.9.1.m1.1.1.1.cmml" xref="S5.T5.9.9.9.1.m1.1.1.1"></times><ci id="S5.T5.9.9.9.1.m1.1.1.2.cmml" xref="S5.T5.9.9.9.1.m1.1.1.2">𝑚</ci><ci id="S5.T5.9.9.9.1.m1.1.1.3.cmml" xref="S5.T5.9.9.9.1.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.9.9.9.1.m1.1c">ms</annotation></semantics></math>) of ours with different settings</td>
</tr>
<tr id="S5.T5.9.9.14" class="ltx_tr">
<td id="S5.T5.9.9.14.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.9.9.14.1.1" class="ltx_text ltx_font_bold">Num. Iter.</span></td>
<td id="S5.T5.9.9.14.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.9.9.14.2.1" class="ltx_text ltx_font_italic">1</span></td>
<td id="S5.T5.9.9.14.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.9.9.14.3.1" class="ltx_text ltx_font_italic">2</span></td>
<td id="S5.T5.9.9.14.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.9.9.14.4.1" class="ltx_text ltx_font_italic">4</span></td>
<td id="S5.T5.9.9.14.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.9.9.14.5.1" class="ltx_text ltx_font_italic">6</span></td>
<td id="S5.T5.9.9.14.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.9.9.14.6.1" class="ltx_text ltx_font_italic">8</span></td>
</tr>
<tr id="S5.T5.9.9.15" class="ltx_tr">
<td id="S5.T5.9.9.15.1" class="ltx_td ltx_align_center">Ours <span id="S5.T5.9.9.15.1.1" class="ltx_text ltx_font_italic">with 256 quer.</span>
</td>
<td id="S5.T5.9.9.15.2" class="ltx_td ltx_align_center">180</td>
<td id="S5.T5.9.9.15.3" class="ltx_td ltx_align_center">194</td>
<td id="S5.T5.9.9.15.4" class="ltx_td ltx_align_center">213</td>
<td id="S5.T5.9.9.15.5" class="ltx_td ltx_align_center">252</td>
<td id="S5.T5.9.9.15.6" class="ltx_td ltx_align_center">290</td>
</tr>
<tr id="S5.T5.9.9.16" class="ltx_tr">
<td id="S5.T5.9.9.16.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.9.9.16.1.1" class="ltx_text ltx_font_bold">Num. Quer.</span></td>
<td id="S5.T5.9.9.16.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.9.9.16.2.1" class="ltx_text ltx_font_italic">16</span></td>
<td id="S5.T5.9.9.16.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.9.9.16.3.1" class="ltx_text ltx_font_italic">32</span></td>
<td id="S5.T5.9.9.16.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.9.9.16.4.1" class="ltx_text ltx_font_italic">64</span></td>
<td id="S5.T5.9.9.16.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.9.9.16.5.1" class="ltx_text ltx_font_italic">128</span></td>
<td id="S5.T5.9.9.16.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.9.9.16.6.1" class="ltx_text ltx_font_italic">256</span></td>
</tr>
<tr id="S5.T5.9.9.17" class="ltx_tr">
<td id="S5.T5.9.9.17.1" class="ltx_td ltx_align_center">Ours <span id="S5.T5.9.9.17.1.1" class="ltx_text ltx_font_italic">with 8 iter.</span>
</td>
<td id="S5.T5.9.9.17.2" class="ltx_td ltx_align_center">156</td>
<td id="S5.T5.9.9.17.3" class="ltx_td ltx_align_center">167</td>
<td id="S5.T5.9.9.17.4" class="ltx_td ltx_align_center">171</td>
<td id="S5.T5.9.9.17.5" class="ltx_td ltx_align_center">227</td>
<td id="S5.T5.9.9.17.6" class="ltx_td ltx_align_center">290</td>
</tr>
<tr id="S5.T5.9.9.18" class="ltx_tr">
<td id="S5.T5.9.9.18.1" class="ltx_td ltx_align_center"><span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></td>
<td id="S5.T5.9.9.18.2" class="ltx_td"></td>
<td id="S5.T5.9.9.18.3" class="ltx_td"></td>
<td id="S5.T5.9.9.18.4" class="ltx_td"></td>
<td id="S5.T5.9.9.18.5" class="ltx_td"></td>
<td id="S5.T5.9.9.18.6" class="ltx_td"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>
<span id="S5.T5.14.1" class="ltx_text ltx_font_bold">Efficiency.</span> The reported inference time, memory consumption, parameter counts and F1 scores show that PARQ is fastest and achieves the highest performance.
<sup id="S5.T5.15.2" class="ltx_sup"><span id="S5.T5.15.2.1" class="ltx_text ltx_font_italic">∗</span></sup>PARQ achieves an inference time of 105ms with 32 queries and 2 layers.
</figcaption>
</figure>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2310.01401/assets/x7.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="241" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>
<span id="S5.F7.3.1" class="ltx_text ltx_font_bold">Qualitative results on ScanNet (top) and user-captured videos (bottom).</span>
<em id="S5.F7.4.2" class="ltx_emph ltx_font_italic">Zoom in for details</em>.
We compress the image feature maps using Linear PCA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
Note that the learned feature maps are multi-view consistent.
We also test our model’s generalization ability by deploying it on user-captured videos without any fine-tuning.
More results in the supplementary.
</figcaption>
</figure>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2310.01401/assets/x8.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="266" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>
<span id="S5.F8.4.1" class="ltx_text ltx_font_bold">Attention maps.</span> <em id="S5.F8.5.2" class="ltx_emph ltx_font_italic">Zoom in for details</em>.
We visualize the attention map between queries and multi-view images.
The <span id="S5.F8.6.3" class="ltx_text ltx_font_italic" style="color:#FF0000;">red</span> marker represents the projected 3D point.
(a) Attention for different scenes after PARQ layer 0.
(b) During the recurrent updates, the 3D point approaches the region with high attention weights.
(c) Queries for a scene tend to attend to objects closest to them.
(d) Without pixel-aligned features, as in PETR, queries focus only on the local area around them; in stark contrast to PARQ in (a-c) which attends globally.
</figcaption>
</figure>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We introduce PARQ for multi-view 3D object detection.
PARQ’s key idea is to leverage the pixel-aligned queries initialized from reference points in 3D space and to update their 3D locations layer-by-layer to the 3D object center with recurrent cross-attention operations.
Our design enables PARQ to encode the 3D-to-2D correspondences and capture global contextual information of the input images, as we demonstrate with our qualitative analysis.
Experiments show that PARQ outperforms prior best methods, learns and detects faster, is more robust to distribution shifts in reference points, can leverage additional input views, while inference compute can be adapted by changing the number of recurrent iterations.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
</span><a target="_blank" href="https://developer.apple.com/augmented-reality/" title="" class="ltx_ref ltx_href" style="font-size:90%;">Augmented Reality
with ARKit- Apple Developer</a><span id="bib.bib1.2.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Armen Avetisyan, Manuel Dahnert, Angela Dai, Manolis Savva, Angel X. Chang, and
Matthias Niessner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Scan2cad: Learning cad model alignment in rgb-d scans.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu,
Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, and Elad Shulman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Arkitscenes - a diverse real-world dataset for 3d indoor scene
understanding using mobile rgb-d data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Garrick Brazil, Gerard Pons-Moll, Xiaoming Liu, and Bernt Schiele.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Kinematic 3d object detection in monocular video.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Carlos Campos, Richard Elvira, Juan J. Gómez Rodríguez, José M. M.
Montiel, and Juan D. Tardós.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">ORB-SLAM3: An Accurate Open-Source Library for
Visual, Visual-Inertial and Multi-Map SLAM.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ArXiv</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
Kirillov, and Sergey Zagoruyko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">End-to-end object detection with transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Xiaozhi Chen, Kaustav Kundu, Ziyu Zhang, Huimin Ma, Sanja Fidler, and Raquel
Urtasun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Monocular 3d object detection for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and
Matthias Nießner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Scannet: Richly-annotated 3d reconstructions of indoor scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Angela Dai, Matthias Nießner, Michael Zollhöfer, Shahram Izadi, and
Christian Theobalt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Bundlefusion: Real-time globally consistent 3d reconstruction using
on-the-fly surface reintegration.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ToG</span><span id="bib.bib9.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">An image is worth 16x16 words: Transformers for image recognition at
scale.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Tobias Fischer, Yung-Hsu Yang, Suryansh Kumar, Min Sun, and Fisher Yu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Cc-3dt: Panoramic 3d object tracking via cross-camera fusion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRL</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz
Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Accurate, large minibatch sgd: Training imagenet in 1 hour.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ArXiv</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Finding structure with randomness: Probabilistic algorithms for
constructing approximate matrix decompositions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">SIAM review</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">, 2011.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Richard Hartley and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Multiple view geometry in computer vision</span><span id="bib.bib14.3.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.4.1" class="ltx_text" style="font-size:90%;">Cambridge university press, 2003.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Mask r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Benran Hu, Junkai Huang, Yichen Liu, Yu-Wing Tai, and Chi-Keung Tang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Nerf-rpn: A general framework for object detection in nerfs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Hou-Ning Hu, Qi-Zhi Cai, Dequan Wang, Ji Lin, Min Sun, Philipp Krähenbühl,
Trevor Darrell, and Fisher Yu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Joint monocular 3d vehicle detection and tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Junjie Huang and Guan Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">BEVDet4D: Exploit Temporal Cues in Multi-camera 3D
Object Detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ArXiv</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Junjie Huang, Guan Huang, Zheng Zhu, Yun Ye, and Dalong Du.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">BEVDet: High-performance Multi-camera 3D Object Detection
in Bird-Eye-View.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ArXiv</span><span id="bib.bib20.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Harold W Kuhn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">The hungarian method for the assignment problem.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Naval research logistics quarterly</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">, 1955.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Kejie Li, Daniel DeTone, Steven Chen, Minh Vo, Ian Reid, Hamid Rezatoﬁghi,
Chris Sweeney, Julian Straub, and Richard Newcombe.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">ODAM: Object Detection, Association, and Mapping using
Posed RGB Video.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Kejie Li, Hamid Rezatofighi, and Ian Reid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Moltr: Multiple object localization, tracking and reconstruction from
monocular rgb videos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">RA-L</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Kejie Li, Martin Rünz, Meng Tang, Lingni Ma, Chen Kong, Tanner Schmidt, Ian
Reid, Lourdes Agapito, Julian Straub, Steven Lovegrove, and Richard Newcombe.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">FroDO: From Detections to 3D Objects.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Qiao Yu,
and Jifeng Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">BEVFormer: Learning Bird’s-Eye-View Representation from
Multi-Camera Images via Spatiotemporal Transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and
Serge Belongie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Feature Pyramid Networks for Object Detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Yingfei Liu, Tiancai Wang, Xiangyu Zhang, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">PETR: Position Embedding Transformation for Multi-View
3D Object Detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Yingfei Liu, Junjie Yan, Fan Jia, Shuailin Li, Aqi Gao, Tiancai Wang, Xiangyu
Zhang, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">PETRv2: A Unified Framework for 3D Perception from
Multi-Camera Images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib28.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Ilya Loshchilov and Frank Hutter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Sgdr: Stochastic gradient descent with warm restarts.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Ilya Loshchilov and Frank Hutter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Decoupled weight decay regularization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Kevis-Kokitsi Maninis, Stefan Popov, Matthias Nießner, and Vittorio
Ferrari.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Vid2cad: Cad model alignment using multi-view constraints from
videos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">TPAMI</span><span id="bib.bib31.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi
Ramamoorthi, and Ren Ng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Nerf: Representing scenes as neural radiance fields for view
synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi
Ramamoorthi, and Ren Ng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Nerf: Representing scenes as neural radiance fields for view
synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
Chintala.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">PyTorch: An Imperative Style, High-Performance Deep Learning
Library.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Tong Qin, Jie Pan, Shaozu Cao, and Shaojie Shen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">A General Optimization-based Framework for Local Odometry
Estimation with Multiple Sensors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ArXiv</span><span id="bib.bib35.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Danila Rukhovich, Anna Vorontsova, and Anton Konushin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Imvoxelnet: Image to voxels projection for monocular and multi-view
general-purpose 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">WACV</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa,
and Hao Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Pifu: Pixel-aligned implicit function for high-resolution clothed
human digitization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Neuralrecon: Real-time coherent 3d reconstruction from monocular
video.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Richard Szeliski.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer vision: algorithms and applications</span><span id="bib.bib39.3.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.4.1" class="ltx_text" style="font-size:90%;">Springer Nature, 2022.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Shubham Tulsiani, Tinghui Zhou, Alexei A. Efros, and Jitendra Malik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Multi-view supervision for single-view reconstruction via
differentiable ray consistency.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Michał J. Tyszkiewicz, Kevis-Kokitsi Maninis, Stefan Popov, and Vittorio
Ferrari.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">RayTran: 3D pose estimation and shape reconstruction of multiple
objects from videos with ray-traced transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Attention is all you need.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Tai Wang, Jiangmiao Pang, and Dahua Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Monocular 3D Object Detection with Depth from Motion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Yue Wang, Vitor Guizilini, Tianyuan Zhang, Yilun Wang, Hang Zhao, , and
Justin M. Solomon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Detr3d: 3d object detection from multi-view images via 3d-to-2d
queries.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRL</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Charles Wheatstone.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Contributions to the physiology of vision.—part the first. on some
remarkable and hitherto unobserved phenomena of binocular vision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Abstracts of the Papers Printed in the Philosophical
Transactions of the Royal Society of London</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, 1843.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Yiming Xie, Matheus Gadelha, Fengting Yang, Xiaowei Zhou, and Huaizu Jiang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">PlanarRecon: Real-time 3D plane detection and reconstruction from
posed monocular videos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib46.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib46.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Chenfeng Xu, Bichen Wu, Ji Hou, Sam Tsai, Ruilong Li, Jialiang Wang, Wei Zhan,
Zijian He, Peter Vajda, Kurt Keutzer, and Masayoshi Tomizuka.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Nerf-det: Learning geometry-aware volumetric representation for
multi-view 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Wang Yifan, Carl Doersch, Relja Arandjelović, Joao Carreira, and Andrew
Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Input-level inductive biases for 3d reconstruction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Center-based 3d object detection and tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Yi Zhou, Connelly Barnes, Lu Jingwan, Yang Jimei, and Li Hao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">On the continuity of rotation representations in neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2310.01400" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2310.01401" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2310.01401">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2310.01401" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2310.01402" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 03:07:35 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
