<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2306.10921] Understanding Depth Map Progressively: Adaptive Distance Interval Separation for Monocular 3d Object Detection</title><meta property="og:description" content="Monocular 3D object detection aims to locate objects in different scenes with just a single image. Due to the absence of depth information, several monocular 3D detection techniques have emerged that rely on auxiliary â€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Understanding Depth Map Progressively: Adaptive Distance Interval Separation for Monocular 3d Object Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Understanding Depth Map Progressively: Adaptive Distance Interval Separation for Monocular 3d Object Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2306.10921">

<!--Generated on Wed Feb 28 22:28:01 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Understanding Depth Map Progressively: Adaptive Distance Interval Separation for Monocular 3d Object Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">1<sup id="id2.1.id1" class="ltx_sup">st</sup> Xianhui Cheng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id3.2.id1" class="ltx_text ltx_font_italic">School of Computer Science</span>
<br class="ltx_break"><span id="id4.3.id2" class="ltx_text ltx_font_italic">Fudan University
<br class="ltx_break"></span>Shanghai, China 
<br class="ltx_break">xianhuicheng20@fudan.edu.cn
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">2<sup id="id5.1.id1" class="ltx_sup">nd</sup> Shoumeng Qiu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id6.2.id1" class="ltx_text ltx_font_italic">School of Computer Science</span>
<br class="ltx_break"><span id="id7.3.id2" class="ltx_text ltx_font_italic">Fudan University
<br class="ltx_break"></span>Shanghai, China 
<br class="ltx_break">smqiu21@m.fudan.edu.cn
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">3<sup id="id8.1.id1" class="ltx_sup">rd</sup> Zhikang Zou
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id9.2.id1" class="ltx_text ltx_font_italic">Baidu Inc.</span>
<br class="ltx_break">Shenzhen, China 
<br class="ltx_break">zouzhikang@baidu.com
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">4<sup id="id10.2.id1" class="ltx_sup">th</sup> Jian Pu<sup id="id11.3.id2" class="ltx_sup">âˆ—</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id12.4.id1" class="ltx_text ltx_font_italic">Institute of Science and Technology for Brain-Inspired Intelligence</span>
<br class="ltx_break"><span id="id13.5.id2" class="ltx_text ltx_font_italic">Fudan University
<br class="ltx_break"></span>Shanghai, China 
<br class="ltx_break">jianpu@fudan.edu.cn
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">5<sup id="id14.1.id1" class="ltx_sup">th</sup> Xiangyang Xue
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id15.2.id1" class="ltx_text ltx_font_italic">School of Computer Science</span>
<br class="ltx_break"><span id="id16.3.id2" class="ltx_text ltx_font_italic">Fudan University
<br class="ltx_break"></span>Shanghai, China 
<br class="ltx_break">xyxue@fudan.edu.cn
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id17.id1" class="ltx_p">Monocular 3D object detection aims to locate objects in different scenes with just a single image. Due to the absence of depth information, several monocular 3D detection techniques have emerged that rely on auxiliary depth maps from the depth estimation task. There are multiple approaches to understanding the representation of depth maps, including treating them as pseudo-LiDAR point clouds, leveraging implicit end-to-end learning of depth information, or considering them as an image input. However, these methods have certain drawbacks, such as their reliance on the accuracy of estimated depth maps and suboptimal utilization of depth maps due to their image-based nature. While LiDAR-based methods and convolutional neural networks (CNNs) can be utilized for pseudo point clouds and depth maps, respectively, it is always an alternative. In this paper, we propose a framework named the Adaptive Distance Interval Separation Network (ADISN) that adopts a novel perspective on understanding depth maps, as a form that lies between LiDAR and images. We utilize an adaptive separation approach that partitions the depth map into various subgraphs based on distance and treats each of these subgraphs as an individual image for feature extraction. After adaptive separations, each subgraph solely contains pixels within a learned interval range. If there is a truncated object within this range, an evident curved edge will appear, which we can leverage for texture extraction using CNNs to obtain rich depth information in pixels. Meanwhile, to mitigate the inaccuracy of depth estimation, we designed an uncertainty module. To take advantage of both images and depth maps, we use different branches to learn localization detection tasks and appearance tasks separately. Our approach significantly enhances the baseline and outperforms depth-assisted techniques, as shown by our extensive experiments on the KITTI monocular 3D object detection benchmark.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The urgent need for inexpensive sensors to perceive the 3D world in real-world applications, such as autonomous driving, virtual reality, and robotics, is the motivation for this study. LiDAR devices have shown promising results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>; however, their high cost makes them challenging to deploy and maintain widely. To address this issue, there is a pressing need for cheaper object detectors based on vision. Monocular 3D object detection, which uses only one camera to perceive object localization, physical dimensions, and orientation, has received increasing attention from both industry and academia. Despite the potential benefits, accurate 3D information perception remains a significant challenge due to the lack of depth information available from monocular cameras. Therefore, there is a need to solve several problems in monocular 3D object detection.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<p id="S1.F1.1" class="ltx_p ltx_align_center"><span id="S1.F1.1.1" class="ltx_text"><img src="/html/2306.10921/assets/fig1-4.png" id="S1.F1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="306" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Comparison between general depth maps and our proposed separated depth maps is shown in the figure. The top image shows the original input, the middle image displays the estimated depth map, and the bottom image illustrates our separated depth map. Notably, in the yellow box, a parked car is barely visible in the general depth map but is clearly distinguishable in our separated depth map.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Recent methods of monocular 3D object detection can be categorized into four main groups based on their ideas for improvement: image-based, imitation-based, pseudo LiDAR-based, and depth-assisted approaches.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Image-based methods rely solely on an image as input. Given the absence of depth information, these methods attempt to establish connections between 2D and 3D object information using geometric methods or feature enhancement techniques. Unlike LiDAR, which can accurately measure distance, monocular methods face challenges in accurately locating objects in space based on the image plane.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Pseudo-LiDAR-based methods aim to bridge the gap between pixels and point clouds. Depth maps from the depth estimation task can be transformed to generate pseudo-LiDAR point clouds. Well-developed LiDAR-based detectors can then be directly applied, while some methods use CNNs to process pseudo-LiDAR. However, the accuracy of depth estimation methods may significantly affect the data transformation of pseudo-LiDAR, making it unsuitable for point cloud detectors trained on accurate point cloud data. Furthermore, overfitting frequently occurs, particularly in the KITTI dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">To achieve excellent performance similar to LiDAR-based or multicamera image-based methods, knowledge distillation is used to improve monocular 3D object detectors. Successful teacher models impressively guide monocular detectors. However, the strict requirements for sensor space-time synchronization and calibration make it challenging to deploy the model on different vehicles.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Depth-assisted methods complement the rich semantic information of images with the distance information of depth maps. Previous works mainly focus on effectively extracting and fusing depth information with image features, often using CNNs. CNNs are suitable for extracting semantic and texture features, but depth maps lack these features. Therefore, the decision to let depth maps directly pass through CNNs needs reconsideration. Additionally, the accuracy of the depth map estimation significantly affects the modelâ€™s performance. It is essential to distinguish which parts of the depth map are trustworthy and should be given special consideration. Furthermore, images and depth maps have different areas of expertise in the 3D object detection task. Images are better suited for extracting semantic information, while depth maps are ideal for determining location information. Therefore, they should play to their strengths and complement each other.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">In this paper, we summarize three problems and provide corresponding solutions. The problems are as follows: (i) Extracting depth map information effectively. Depth maps have physical meaning, while CNNs are better suited for texture extraction. To extract depth map information more effectively, we need to make a connection between texture and physical distance. One way to do this is by dividing the depth map into sub-depth maps (SDs) based on the specified distance. We can then preserve pixels falling into a specific range as another sub-depth map. By doing this, we can combine the advantages of depth maps and CNNs, better recognize edges, and have an intuitive explanation. (ii) Dealing with inaccurate depth maps. To address the problem of inaccurate depth map estimation, we propose using an uncertainty map aligned with the size of the depth map. The uncertainty map gives confidence for every pixel on the depth map and allows the model to judge whether the location of depth maps is credible. If the depth value tends to be inaccurate, the corresponding uncertainty map suppresses the error caused by the inaccurate region, and the credible values play a greater role. The module of the uncertainty map generally makes depth maps more reliable. (iii) Utilizing the advantages of images and depth maps. Images and depth maps have their respective advantages in appearance and localization features. To make the most of these advantages, we designed a feature decoupling module that separates output heads according to the input modality. By using different branches to learn appearance and localization detection task heads, the model can better recognize categories and understand spatial information.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">Our main contributions are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We actively create edges to connect the physical meaning of depth maps with the ability of CNNs.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We propose a simple and effective uncertainty map to reduce the noise caused by inaccurate depth estimation and make the depth map more reliable.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We separate output heads according to the input modality, which improves the overall performance of the model without affecting the calculation cost.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">Extensive experiments show the effectiveness of our proposed approach among monocular 3D object detection methods on the KITTI benchmark.</p>
</div>
</li>
</ul>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2306.10921/assets/fig2-8.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="234" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
Visualization of our proposed framework is shown below, where two branches for appearance and localization are denoted in different colors. Dashed boxes represent corresponding output tasks. In the appearance branch, the appearance information <math id="S1.F2.9.m1.1" class="ltx_Math" alttext="I_{A}" display="inline"><semantics id="S1.F2.9.m1.1b"><msub id="S1.F2.9.m1.1.1" xref="S1.F2.9.m1.1.1.cmml"><mi id="S1.F2.9.m1.1.1.2" xref="S1.F2.9.m1.1.1.2.cmml">I</mi><mi id="S1.F2.9.m1.1.1.3" xref="S1.F2.9.m1.1.1.3.cmml">A</mi></msub><annotation-xml encoding="MathML-Content" id="S1.F2.9.m1.1c"><apply id="S1.F2.9.m1.1.1.cmml" xref="S1.F2.9.m1.1.1"><csymbol cd="ambiguous" id="S1.F2.9.m1.1.1.1.cmml" xref="S1.F2.9.m1.1.1">subscript</csymbol><ci id="S1.F2.9.m1.1.1.2.cmml" xref="S1.F2.9.m1.1.1.2">ğ¼</ci><ci id="S1.F2.9.m1.1.1.3.cmml" xref="S1.F2.9.m1.1.1.3">ğ´</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.9.m1.1d">I_{A}</annotation></semantics></math> is generated from both the image feature <math id="S1.F2.10.m2.1" class="ltx_Math" alttext="F_{I}" display="inline"><semantics id="S1.F2.10.m2.1b"><msub id="S1.F2.10.m2.1.1" xref="S1.F2.10.m2.1.1.cmml"><mi id="S1.F2.10.m2.1.1.2" xref="S1.F2.10.m2.1.1.2.cmml">F</mi><mi id="S1.F2.10.m2.1.1.3" xref="S1.F2.10.m2.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S1.F2.10.m2.1c"><apply id="S1.F2.10.m2.1.1.cmml" xref="S1.F2.10.m2.1.1"><csymbol cd="ambiguous" id="S1.F2.10.m2.1.1.1.cmml" xref="S1.F2.10.m2.1.1">subscript</csymbol><ci id="S1.F2.10.m2.1.1.2.cmml" xref="S1.F2.10.m2.1.1.2">ğ¹</ci><ci id="S1.F2.10.m2.1.1.3.cmml" xref="S1.F2.10.m2.1.1.3">ğ¼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.10.m2.1d">F_{I}</annotation></semantics></math> and the depth feature <math id="S1.F2.11.m3.1" class="ltx_Math" alttext="F_{D}" display="inline"><semantics id="S1.F2.11.m3.1b"><msub id="S1.F2.11.m3.1.1" xref="S1.F2.11.m3.1.1.cmml"><mi id="S1.F2.11.m3.1.1.2" xref="S1.F2.11.m3.1.1.2.cmml">F</mi><mi id="S1.F2.11.m3.1.1.3" xref="S1.F2.11.m3.1.1.3.cmml">D</mi></msub><annotation-xml encoding="MathML-Content" id="S1.F2.11.m3.1c"><apply id="S1.F2.11.m3.1.1.cmml" xref="S1.F2.11.m3.1.1"><csymbol cd="ambiguous" id="S1.F2.11.m3.1.1.1.cmml" xref="S1.F2.11.m3.1.1">subscript</csymbol><ci id="S1.F2.11.m3.1.1.2.cmml" xref="S1.F2.11.m3.1.1.2">ğ¹</ci><ci id="S1.F2.11.m3.1.1.3.cmml" xref="S1.F2.11.m3.1.1.3">ğ·</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.11.m3.1d">F_{D}</annotation></semantics></math>. The uncertainty map and separated depth maps with adaptive distance intervals are generated by encoding <math id="S1.F2.12.m4.1" class="ltx_Math" alttext="F_{I}" display="inline"><semantics id="S1.F2.12.m4.1b"><msub id="S1.F2.12.m4.1.1" xref="S1.F2.12.m4.1.1.cmml"><mi id="S1.F2.12.m4.1.1.2" xref="S1.F2.12.m4.1.1.2.cmml">F</mi><mi id="S1.F2.12.m4.1.1.3" xref="S1.F2.12.m4.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S1.F2.12.m4.1c"><apply id="S1.F2.12.m4.1.1.cmml" xref="S1.F2.12.m4.1.1"><csymbol cd="ambiguous" id="S1.F2.12.m4.1.1.1.cmml" xref="S1.F2.12.m4.1.1">subscript</csymbol><ci id="S1.F2.12.m4.1.1.2.cmml" xref="S1.F2.12.m4.1.1.2">ğ¹</ci><ci id="S1.F2.12.m4.1.1.3.cmml" xref="S1.F2.12.m4.1.1.3">ğ¼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.12.m4.1d">F_{I}</annotation></semantics></math> and <math id="S1.F2.13.m5.1" class="ltx_Math" alttext="F_{D}" display="inline"><semantics id="S1.F2.13.m5.1b"><msub id="S1.F2.13.m5.1.1" xref="S1.F2.13.m5.1.1.cmml"><mi id="S1.F2.13.m5.1.1.2" xref="S1.F2.13.m5.1.1.2.cmml">F</mi><mi id="S1.F2.13.m5.1.1.3" xref="S1.F2.13.m5.1.1.3.cmml">D</mi></msub><annotation-xml encoding="MathML-Content" id="S1.F2.13.m5.1c"><apply id="S1.F2.13.m5.1.1.cmml" xref="S1.F2.13.m5.1.1"><csymbol cd="ambiguous" id="S1.F2.13.m5.1.1.1.cmml" xref="S1.F2.13.m5.1.1">subscript</csymbol><ci id="S1.F2.13.m5.1.1.2.cmml" xref="S1.F2.13.m5.1.1.2">ğ¹</ci><ci id="S1.F2.13.m5.1.1.3.cmml" xref="S1.F2.13.m5.1.1.3">ğ·</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.13.m5.1d">F_{D}</annotation></semantics></math> as <math id="S1.F2.14.m6.1" class="ltx_Math" alttext="F_{U}" display="inline"><semantics id="S1.F2.14.m6.1b"><msub id="S1.F2.14.m6.1.1" xref="S1.F2.14.m6.1.1.cmml"><mi id="S1.F2.14.m6.1.1.2" xref="S1.F2.14.m6.1.1.2.cmml">F</mi><mi id="S1.F2.14.m6.1.1.3" xref="S1.F2.14.m6.1.1.3.cmml">U</mi></msub><annotation-xml encoding="MathML-Content" id="S1.F2.14.m6.1c"><apply id="S1.F2.14.m6.1.1.cmml" xref="S1.F2.14.m6.1.1"><csymbol cd="ambiguous" id="S1.F2.14.m6.1.1.1.cmml" xref="S1.F2.14.m6.1.1">subscript</csymbol><ci id="S1.F2.14.m6.1.1.2.cmml" xref="S1.F2.14.m6.1.1.2">ğ¹</ci><ci id="S1.F2.14.m6.1.1.3.cmml" xref="S1.F2.14.m6.1.1.3">ğ‘ˆ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.14.m6.1d">F_{U}</annotation></semantics></math> and <math id="S1.F2.15.m7.1" class="ltx_Math" alttext="F_{SDs}" display="inline"><semantics id="S1.F2.15.m7.1b"><msub id="S1.F2.15.m7.1.1" xref="S1.F2.15.m7.1.1.cmml"><mi id="S1.F2.15.m7.1.1.2" xref="S1.F2.15.m7.1.1.2.cmml">F</mi><mrow id="S1.F2.15.m7.1.1.3" xref="S1.F2.15.m7.1.1.3.cmml"><mi id="S1.F2.15.m7.1.1.3.2" xref="S1.F2.15.m7.1.1.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S1.F2.15.m7.1.1.3.1" xref="S1.F2.15.m7.1.1.3.1.cmml">â€‹</mo><mi id="S1.F2.15.m7.1.1.3.3" xref="S1.F2.15.m7.1.1.3.3.cmml">D</mi><mo lspace="0em" rspace="0em" id="S1.F2.15.m7.1.1.3.1b" xref="S1.F2.15.m7.1.1.3.1.cmml">â€‹</mo><mi id="S1.F2.15.m7.1.1.3.4" xref="S1.F2.15.m7.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S1.F2.15.m7.1c"><apply id="S1.F2.15.m7.1.1.cmml" xref="S1.F2.15.m7.1.1"><csymbol cd="ambiguous" id="S1.F2.15.m7.1.1.1.cmml" xref="S1.F2.15.m7.1.1">subscript</csymbol><ci id="S1.F2.15.m7.1.1.2.cmml" xref="S1.F2.15.m7.1.1.2">ğ¹</ci><apply id="S1.F2.15.m7.1.1.3.cmml" xref="S1.F2.15.m7.1.1.3"><times id="S1.F2.15.m7.1.1.3.1.cmml" xref="S1.F2.15.m7.1.1.3.1"></times><ci id="S1.F2.15.m7.1.1.3.2.cmml" xref="S1.F2.15.m7.1.1.3.2">ğ‘†</ci><ci id="S1.F2.15.m7.1.1.3.3.cmml" xref="S1.F2.15.m7.1.1.3.3">ğ·</ci><ci id="S1.F2.15.m7.1.1.3.4.cmml" xref="S1.F2.15.m7.1.1.3.4">ğ‘ </ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.15.m7.1d">F_{SDs}</annotation></semantics></math>, respectively. These features are then fused into localization information <math id="S1.F2.16.m8.1" class="ltx_Math" alttext="I_{L}" display="inline"><semantics id="S1.F2.16.m8.1b"><msub id="S1.F2.16.m8.1.1" xref="S1.F2.16.m8.1.1.cmml"><mi id="S1.F2.16.m8.1.1.2" xref="S1.F2.16.m8.1.1.2.cmml">I</mi><mi id="S1.F2.16.m8.1.1.3" xref="S1.F2.16.m8.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S1.F2.16.m8.1c"><apply id="S1.F2.16.m8.1.1.cmml" xref="S1.F2.16.m8.1.1"><csymbol cd="ambiguous" id="S1.F2.16.m8.1.1.1.cmml" xref="S1.F2.16.m8.1.1">subscript</csymbol><ci id="S1.F2.16.m8.1.1.2.cmml" xref="S1.F2.16.m8.1.1.2">ğ¼</ci><ci id="S1.F2.16.m8.1.1.3.cmml" xref="S1.F2.16.m8.1.1.3">ğ¿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.16.m8.1d">I_{L}</annotation></semantics></math> and fed to the corresponding heads.
</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related work</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Image-based approaches</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">It is challenging to infer 3D spatial information from a single image due to the inherent lack of depth information. Pioneering works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> use relatively stable 2D detection results to predict 3D boxes. Some approaches match 2D boxes with predefined 3D shapes for different cars <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Other works focus on disentangling complex interactions between parameters, achieving remarkable progress <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. M3D-RPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> proposes a framework that shares 2D-3D features with sets of anchors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, while RTM3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> estimates several projected key points to construct 3D bounding boxes with geometric relationships. Heatmap-based approaches consider objects as points <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, significantly reducing inference time. In some cases, auxiliary information support can significantly increase the performance of monocular 3D detectors. Kinematic3d <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> proposes establishing a kinematic model using continuous frames of images and correcting the objectâ€™s location in space according to the kinematic model recommendations and regression branchesâ€™ results. CaDDN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> learns the categorical depth distribution from sampled point clouds in the birdâ€™s-eye-view (BEV) at the training stage, utilizing point clouds to guide depth features.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Pseudo LiDAR-based approaches</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Compared to the success of LiDAR-based methods, monocular 3D object detectors have significant room for improvement. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> converted depth map pixels obtained from the depth estimation task into a pseudo-LiDAR point cloud representation, which was then processed using a LiDAR-based detection network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. PatchNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> proposed a novel perspective on the pseudo-LiDAR representation. In addition to using LiDAR-based detectors, simple CNNs can also produce considerable results. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> analyzed the impact of 3D object parameters on the results and adopted different strategies based on the degree of influence. NeighborVote <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> proposes a joint decision-making model based on the pseudo-LiDAR generated from the estimated depth map. The feature points surrounding the proposed object form their own predictions, and these predictions are then combined to form the final output.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Imitation-based approaches</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">In the task of 3D object detection, LiDAR and stereo modalities have achieved remarkable performance. Some algorithms aim to modify monocular 3D detectors by adopting the paradigms of the aforementioned modalities to enable them to perform well under different modality settings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. DD3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> explores the role of depth maps in the detection task and proposes a transfer model between depth estimation and 3D detection. The well-trained knowledge from depth estimation is then applied to the image, and the results demonstrate that the implicit use of depth information can significantly enhance the 3D detection results. CMKD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> transfers knowledge from LiDAR to the image via a knowledge distillation semi-supervised training framework, resulting in significant performance improvement. MonoPS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> contends that the gap between stereo and image is much smaller than that between LiDAR and image. It replaces stereo images from the stereo-based detection architecture with defined pseudo-stereo image features, resulting in promising results. To imitate the representations of 3D point clouds, Feng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> align 2D proposals with 3D point clouds.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS4.4.1.1" class="ltx_text">II-D</span> </span><span id="S2.SS4.5.2" class="ltx_text ltx_font_italic">Depth-assisted approaches</span>
</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">D4LCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> uses depth maps as guidance to distinguish foreground and background in scenes, while DDMP-3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> utilizes graphic message passing mechanisms and convolution kernels on depth maps to dynamically extract features. Moreover, instance-level center-aware depth guidance is proposed to reduce depth estimation errors. DRF-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> discusses the appearance and location information of objects and uses a feature reflecting module to make the most of different features. Cai et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> treat monocular 3D object detection as several sub-tasks, roughly estimating object depth with an auxiliary prior 2D structured polygon and finally refining it with the depth map.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we present the framework of our proposed ADISN. Subsequently, we provide a detailed description of three major structures proposed in this study, which include adaptive distance interval separation, uncertainty map, and feature decoupling module.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Pipeline overview</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.14" class="ltx_p">We employed M3D-RPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> as our baseline detector due to its simplicity and influence on several depth-assisted methods. We followed the baselineâ€™s anchor definition and learning parameters. Our network accepts both image and depth map inputs. We used DORN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> as a monocular depth estimator to generate depth maps. As illustrated in Fig <a href="#S1.F2" title="Figure 2 â€£ I Introduction â€£ Understanding Depth Map Progressively: Adaptive Distance Interval Separation for Monocular 3d Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we encoded the images and depth maps as <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="F_{I}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">F</mi><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">ğ¹</ci><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">ğ¼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">F_{I}</annotation></semantics></math> and <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="F_{D}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">F</mi><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">D</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">ğ¹</ci><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">ğ·</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">F_{D}</annotation></semantics></math>, respectively. We fused <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="F_{I}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><msub id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">F</mi><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">ğ¹</ci><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">ğ¼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">F_{I}</annotation></semantics></math> and <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="F_{D}" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><msub id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">F</mi><mi id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">D</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">ğ¹</ci><ci id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3">ğ·</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">F_{D}</annotation></semantics></math> to create an uncertainty map <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="U" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">ğ‘ˆ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">U</annotation></semantics></math> and adaptive distance lower and upper bounds. We extracted feature <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="F_{SDs}" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><msub id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml"><mi id="S3.SS1.p1.6.m6.1.1.2" xref="S3.SS1.p1.6.m6.1.1.2.cmml">F</mi><mrow id="S3.SS1.p1.6.m6.1.1.3" xref="S3.SS1.p1.6.m6.1.1.3.cmml"><mi id="S3.SS1.p1.6.m6.1.1.3.2" xref="S3.SS1.p1.6.m6.1.1.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.6.m6.1.1.3.1" xref="S3.SS1.p1.6.m6.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.6.m6.1.1.3.3" xref="S3.SS1.p1.6.m6.1.1.3.3.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.6.m6.1.1.3.1a" xref="S3.SS1.p1.6.m6.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS1.p1.6.m6.1.1.3.4" xref="S3.SS1.p1.6.m6.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2">ğ¹</ci><apply id="S3.SS1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3"><times id="S3.SS1.p1.6.m6.1.1.3.1.cmml" xref="S3.SS1.p1.6.m6.1.1.3.1"></times><ci id="S3.SS1.p1.6.m6.1.1.3.2.cmml" xref="S3.SS1.p1.6.m6.1.1.3.2">ğ‘†</ci><ci id="S3.SS1.p1.6.m6.1.1.3.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3.3">ğ·</ci><ci id="S3.SS1.p1.6.m6.1.1.3.4.cmml" xref="S3.SS1.p1.6.m6.1.1.3.4">ğ‘ </ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">F_{SDs}</annotation></semantics></math> from Sub-Depth maps (SDs) and combined it with <math id="S3.SS1.p1.7.m7.1" class="ltx_Math" alttext="F_{D}" display="inline"><semantics id="S3.SS1.p1.7.m7.1a"><msub id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml"><mi id="S3.SS1.p1.7.m7.1.1.2" xref="S3.SS1.p1.7.m7.1.1.2.cmml">F</mi><mi id="S3.SS1.p1.7.m7.1.1.3" xref="S3.SS1.p1.7.m7.1.1.3.cmml">D</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><apply id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p1.7.m7.1.1.2.cmml" xref="S3.SS1.p1.7.m7.1.1.2">ğ¹</ci><ci id="S3.SS1.p1.7.m7.1.1.3.cmml" xref="S3.SS1.p1.7.m7.1.1.3">ğ·</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">F_{D}</annotation></semantics></math> and <math id="S3.SS1.p1.8.m8.1" class="ltx_Math" alttext="F_{U}" display="inline"><semantics id="S3.SS1.p1.8.m8.1a"><msub id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml"><mi id="S3.SS1.p1.8.m8.1.1.2" xref="S3.SS1.p1.8.m8.1.1.2.cmml">F</mi><mi id="S3.SS1.p1.8.m8.1.1.3" xref="S3.SS1.p1.8.m8.1.1.3.cmml">U</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><apply id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m8.1.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.p1.8.m8.1.1.2.cmml" xref="S3.SS1.p1.8.m8.1.1.2">ğ¹</ci><ci id="S3.SS1.p1.8.m8.1.1.3.cmml" xref="S3.SS1.p1.8.m8.1.1.3">ğ‘ˆ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">F_{U}</annotation></semantics></math> from the uncertainty map to obtain the overall localization information feature <math id="S3.SS1.p1.9.m9.1" class="ltx_Math" alttext="I_{L}" display="inline"><semantics id="S3.SS1.p1.9.m9.1a"><msub id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml"><mi id="S3.SS1.p1.9.m9.1.1.2" xref="S3.SS1.p1.9.m9.1.1.2.cmml">I</mi><mi id="S3.SS1.p1.9.m9.1.1.3" xref="S3.SS1.p1.9.m9.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.1b"><apply id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.9.m9.1.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1">subscript</csymbol><ci id="S3.SS1.p1.9.m9.1.1.2.cmml" xref="S3.SS1.p1.9.m9.1.1.2">ğ¼</ci><ci id="S3.SS1.p1.9.m9.1.1.3.cmml" xref="S3.SS1.p1.9.m9.1.1.3">ğ¿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.1c">I_{L}</annotation></semantics></math>. Simultaneously, we fused <math id="S3.SS1.p1.10.m10.1" class="ltx_Math" alttext="F_{I}" display="inline"><semantics id="S3.SS1.p1.10.m10.1a"><msub id="S3.SS1.p1.10.m10.1.1" xref="S3.SS1.p1.10.m10.1.1.cmml"><mi id="S3.SS1.p1.10.m10.1.1.2" xref="S3.SS1.p1.10.m10.1.1.2.cmml">F</mi><mi id="S3.SS1.p1.10.m10.1.1.3" xref="S3.SS1.p1.10.m10.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m10.1b"><apply id="S3.SS1.p1.10.m10.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.1.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1">subscript</csymbol><ci id="S3.SS1.p1.10.m10.1.1.2.cmml" xref="S3.SS1.p1.10.m10.1.1.2">ğ¹</ci><ci id="S3.SS1.p1.10.m10.1.1.3.cmml" xref="S3.SS1.p1.10.m10.1.1.3">ğ¼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m10.1c">F_{I}</annotation></semantics></math> and <math id="S3.SS1.p1.11.m11.1" class="ltx_Math" alttext="F_{D}" display="inline"><semantics id="S3.SS1.p1.11.m11.1a"><msub id="S3.SS1.p1.11.m11.1.1" xref="S3.SS1.p1.11.m11.1.1.cmml"><mi id="S3.SS1.p1.11.m11.1.1.2" xref="S3.SS1.p1.11.m11.1.1.2.cmml">F</mi><mi id="S3.SS1.p1.11.m11.1.1.3" xref="S3.SS1.p1.11.m11.1.1.3.cmml">D</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.11.m11.1b"><apply id="S3.SS1.p1.11.m11.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.11.m11.1.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1">subscript</csymbol><ci id="S3.SS1.p1.11.m11.1.1.2.cmml" xref="S3.SS1.p1.11.m11.1.1.2">ğ¹</ci><ci id="S3.SS1.p1.11.m11.1.1.3.cmml" xref="S3.SS1.p1.11.m11.1.1.3">ğ·</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.11.m11.1c">F_{D}</annotation></semantics></math> to produce the overall appearance information <math id="S3.SS1.p1.12.m12.1" class="ltx_Math" alttext="I_{A}" display="inline"><semantics id="S3.SS1.p1.12.m12.1a"><msub id="S3.SS1.p1.12.m12.1.1" xref="S3.SS1.p1.12.m12.1.1.cmml"><mi id="S3.SS1.p1.12.m12.1.1.2" xref="S3.SS1.p1.12.m12.1.1.2.cmml">I</mi><mi id="S3.SS1.p1.12.m12.1.1.3" xref="S3.SS1.p1.12.m12.1.1.3.cmml">A</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.12.m12.1b"><apply id="S3.SS1.p1.12.m12.1.1.cmml" xref="S3.SS1.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.12.m12.1.1.1.cmml" xref="S3.SS1.p1.12.m12.1.1">subscript</csymbol><ci id="S3.SS1.p1.12.m12.1.1.2.cmml" xref="S3.SS1.p1.12.m12.1.1.2">ğ¼</ci><ci id="S3.SS1.p1.12.m12.1.1.3.cmml" xref="S3.SS1.p1.12.m12.1.1.3">ğ´</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.12.m12.1c">I_{A}</annotation></semantics></math>. Finally, <math id="S3.SS1.p1.13.m13.1" class="ltx_Math" alttext="I_{L}" display="inline"><semantics id="S3.SS1.p1.13.m13.1a"><msub id="S3.SS1.p1.13.m13.1.1" xref="S3.SS1.p1.13.m13.1.1.cmml"><mi id="S3.SS1.p1.13.m13.1.1.2" xref="S3.SS1.p1.13.m13.1.1.2.cmml">I</mi><mi id="S3.SS1.p1.13.m13.1.1.3" xref="S3.SS1.p1.13.m13.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.13.m13.1b"><apply id="S3.SS1.p1.13.m13.1.1.cmml" xref="S3.SS1.p1.13.m13.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.13.m13.1.1.1.cmml" xref="S3.SS1.p1.13.m13.1.1">subscript</csymbol><ci id="S3.SS1.p1.13.m13.1.1.2.cmml" xref="S3.SS1.p1.13.m13.1.1.2">ğ¼</ci><ci id="S3.SS1.p1.13.m13.1.1.3.cmml" xref="S3.SS1.p1.13.m13.1.1.3">ğ¿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.13.m13.1c">I_{L}</annotation></semantics></math> and <math id="S3.SS1.p1.14.m14.1" class="ltx_Math" alttext="I_{A}" display="inline"><semantics id="S3.SS1.p1.14.m14.1a"><msub id="S3.SS1.p1.14.m14.1.1" xref="S3.SS1.p1.14.m14.1.1.cmml"><mi id="S3.SS1.p1.14.m14.1.1.2" xref="S3.SS1.p1.14.m14.1.1.2.cmml">I</mi><mi id="S3.SS1.p1.14.m14.1.1.3" xref="S3.SS1.p1.14.m14.1.1.3.cmml">A</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.14.m14.1b"><apply id="S3.SS1.p1.14.m14.1.1.cmml" xref="S3.SS1.p1.14.m14.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.14.m14.1.1.1.cmml" xref="S3.SS1.p1.14.m14.1.1">subscript</csymbol><ci id="S3.SS1.p1.14.m14.1.1.2.cmml" xref="S3.SS1.p1.14.m14.1.1.2">ğ¼</ci><ci id="S3.SS1.p1.14.m14.1.1.3.cmml" xref="S3.SS1.p1.14.m14.1.1.3">ğ´</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.14.m14.1c">I_{A}</annotation></semantics></math> were fed into output task heads with feature decoupling. We elaborate on all the modules mentioned above in the following subsections.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<p id="S3.F3.1" class="ltx_p ltx_align_center"><span id="S3.F3.1.1" class="ltx_text"><img src="/html/2306.10921/assets/fig3-2.png" id="S3.F3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="248" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Example of separated sub-depth maps: The top-left panel shows a sketch map of the side view with objects truncated using our adaptive intervals. The top-right panel displays the original input image, while the bottom panels show the separated sub-depth maps.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Adaptive distance interval separation</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In this section, we propose an adaptive distance interval separation module (ADIS) to handle different distance intervals in the depth map. Since the intensity of the pixel value of the depth map represents the physical meaning of distance, we divide the depth map into several intervals based on distance. For each sub-depth (SD), only pixels within the selected distance interval are retained while the remaining positions are assigned to zero. This generates a sharp edge near the reserved area on the subgraph, which is favorable for the strength of CNNs. If the selected distance interval truncates the object, there will be significant changes in the subgraph, making it conducive to positioning the object in space.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.5" class="ltx_p">We need to generate upper and lower bounds adaptively from each unique input feature to obtain an adaptive distance interval. We define the interval sequence as <math id="S3.SS2.p2.1.m1.5" class="ltx_Math" alttext="\mathbf{d_{i}}=(d_{i-1},d_{i}),i=1,2,...,n_{d}" display="inline"><semantics id="S3.SS2.p2.1.m1.5a"><mrow id="S3.SS2.p2.1.m1.5.5.2" xref="S3.SS2.p2.1.m1.5.5.3.cmml"><mrow id="S3.SS2.p2.1.m1.4.4.1.1" xref="S3.SS2.p2.1.m1.4.4.1.1.cmml"><msub id="S3.SS2.p2.1.m1.4.4.1.1.4" xref="S3.SS2.p2.1.m1.4.4.1.1.4.cmml"><mi id="S3.SS2.p2.1.m1.4.4.1.1.4.2" xref="S3.SS2.p2.1.m1.4.4.1.1.4.2.cmml">ğ</mi><mi id="S3.SS2.p2.1.m1.4.4.1.1.4.3" xref="S3.SS2.p2.1.m1.4.4.1.1.4.3.cmml">ğ¢</mi></msub><mo id="S3.SS2.p2.1.m1.4.4.1.1.3" xref="S3.SS2.p2.1.m1.4.4.1.1.3.cmml">=</mo><mrow id="S3.SS2.p2.1.m1.4.4.1.1.2.2" xref="S3.SS2.p2.1.m1.4.4.1.1.2.3.cmml"><mo stretchy="false" id="S3.SS2.p2.1.m1.4.4.1.1.2.2.3" xref="S3.SS2.p2.1.m1.4.4.1.1.2.3.cmml">(</mo><msub id="S3.SS2.p2.1.m1.4.4.1.1.1.1.1" xref="S3.SS2.p2.1.m1.4.4.1.1.1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.4.4.1.1.1.1.1.2" xref="S3.SS2.p2.1.m1.4.4.1.1.1.1.1.2.cmml">d</mi><mrow id="S3.SS2.p2.1.m1.4.4.1.1.1.1.1.3" xref="S3.SS2.p2.1.m1.4.4.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p2.1.m1.4.4.1.1.1.1.1.3.2" xref="S3.SS2.p2.1.m1.4.4.1.1.1.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p2.1.m1.4.4.1.1.1.1.1.3.1" xref="S3.SS2.p2.1.m1.4.4.1.1.1.1.1.3.1.cmml">âˆ’</mo><mn id="S3.SS2.p2.1.m1.4.4.1.1.1.1.1.3.3" xref="S3.SS2.p2.1.m1.4.4.1.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S3.SS2.p2.1.m1.4.4.1.1.2.2.4" xref="S3.SS2.p2.1.m1.4.4.1.1.2.3.cmml">,</mo><msub id="S3.SS2.p2.1.m1.4.4.1.1.2.2.2" xref="S3.SS2.p2.1.m1.4.4.1.1.2.2.2.cmml"><mi id="S3.SS2.p2.1.m1.4.4.1.1.2.2.2.2" xref="S3.SS2.p2.1.m1.4.4.1.1.2.2.2.2.cmml">d</mi><mi id="S3.SS2.p2.1.m1.4.4.1.1.2.2.2.3" xref="S3.SS2.p2.1.m1.4.4.1.1.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS2.p2.1.m1.4.4.1.1.2.2.5" xref="S3.SS2.p2.1.m1.4.4.1.1.2.3.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p2.1.m1.5.5.2.3" xref="S3.SS2.p2.1.m1.5.5.3a.cmml">,</mo><mrow id="S3.SS2.p2.1.m1.5.5.2.2" xref="S3.SS2.p2.1.m1.5.5.2.2.cmml"><mi id="S3.SS2.p2.1.m1.5.5.2.2.3" xref="S3.SS2.p2.1.m1.5.5.2.2.3.cmml">i</mi><mo id="S3.SS2.p2.1.m1.5.5.2.2.2" xref="S3.SS2.p2.1.m1.5.5.2.2.2.cmml">=</mo><mrow id="S3.SS2.p2.1.m1.5.5.2.2.1.1" xref="S3.SS2.p2.1.m1.5.5.2.2.1.2.cmml"><mn id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">1</mn><mo id="S3.SS2.p2.1.m1.5.5.2.2.1.1.2" xref="S3.SS2.p2.1.m1.5.5.2.2.1.2.cmml">,</mo><mn id="S3.SS2.p2.1.m1.2.2" xref="S3.SS2.p2.1.m1.2.2.cmml">2</mn><mo id="S3.SS2.p2.1.m1.5.5.2.2.1.1.3" xref="S3.SS2.p2.1.m1.5.5.2.2.1.2.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p2.1.m1.3.3" xref="S3.SS2.p2.1.m1.3.3.cmml">â€¦</mi><mo id="S3.SS2.p2.1.m1.5.5.2.2.1.1.4" xref="S3.SS2.p2.1.m1.5.5.2.2.1.2.cmml">,</mo><msub id="S3.SS2.p2.1.m1.5.5.2.2.1.1.1" xref="S3.SS2.p2.1.m1.5.5.2.2.1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.5.5.2.2.1.1.1.2" xref="S3.SS2.p2.1.m1.5.5.2.2.1.1.1.2.cmml">n</mi><mi id="S3.SS2.p2.1.m1.5.5.2.2.1.1.1.3" xref="S3.SS2.p2.1.m1.5.5.2.2.1.1.1.3.cmml">d</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.5b"><apply id="S3.SS2.p2.1.m1.5.5.3.cmml" xref="S3.SS2.p2.1.m1.5.5.2"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.5.5.3a.cmml" xref="S3.SS2.p2.1.m1.5.5.2.3">formulae-sequence</csymbol><apply id="S3.SS2.p2.1.m1.4.4.1.1.cmml" xref="S3.SS2.p2.1.m1.4.4.1.1"><eq id="S3.SS2.p2.1.m1.4.4.1.1.3.cmml" xref="S3.SS2.p2.1.m1.4.4.1.1.3"></eq><apply id="S3.SS2.p2.1.m1.4.4.1.1.4.cmml" xref="S3.SS2.p2.1.m1.4.4.1.1.4"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.4.4.1.1.4.1.cmml" xref="S3.SS2.p2.1.m1.4.4.1.1.4">subscript</csymbol><ci id="S3.SS2.p2.1.m1.4.4.1.1.4.2.cmml" xref="S3.SS2.p2.1.m1.4.4.1.1.4.2">ğ</ci><ci id="S3.SS2.p2.1.m1.4.4.1.1.4.3.cmml" xref="S3.SS2.p2.1.m1.4.4.1.1.4.3">ğ¢</ci></apply><interval closure="open" id="S3.SS2.p2.1.m1.4.4.1.1.2.3.cmml" xref="S3.SS2.p2.1.m1.4.4.1.1.2.2"><apply id="S3.SS2.p2.1.m1.4.4.1.1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.4.4.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.4.4.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.4.4.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.4.4.1.1.1.1.1.2">ğ‘‘</ci><apply id="S3.SS2.p2.1.m1.4.4.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.4.4.1.1.1.1.1.3"><minus id="S3.SS2.p2.1.m1.4.4.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p2.1.m1.4.4.1.1.1.1.1.3.1"></minus><ci id="S3.SS2.p2.1.m1.4.4.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p2.1.m1.4.4.1.1.1.1.1.3.2">ğ‘–</ci><cn type="integer" id="S3.SS2.p2.1.m1.4.4.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p2.1.m1.4.4.1.1.1.1.1.3.3">1</cn></apply></apply><apply id="S3.SS2.p2.1.m1.4.4.1.1.2.2.2.cmml" xref="S3.SS2.p2.1.m1.4.4.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.4.4.1.1.2.2.2.1.cmml" xref="S3.SS2.p2.1.m1.4.4.1.1.2.2.2">subscript</csymbol><ci id="S3.SS2.p2.1.m1.4.4.1.1.2.2.2.2.cmml" xref="S3.SS2.p2.1.m1.4.4.1.1.2.2.2.2">ğ‘‘</ci><ci id="S3.SS2.p2.1.m1.4.4.1.1.2.2.2.3.cmml" xref="S3.SS2.p2.1.m1.4.4.1.1.2.2.2.3">ğ‘–</ci></apply></interval></apply><apply id="S3.SS2.p2.1.m1.5.5.2.2.cmml" xref="S3.SS2.p2.1.m1.5.5.2.2"><eq id="S3.SS2.p2.1.m1.5.5.2.2.2.cmml" xref="S3.SS2.p2.1.m1.5.5.2.2.2"></eq><ci id="S3.SS2.p2.1.m1.5.5.2.2.3.cmml" xref="S3.SS2.p2.1.m1.5.5.2.2.3">ğ‘–</ci><list id="S3.SS2.p2.1.m1.5.5.2.2.1.2.cmml" xref="S3.SS2.p2.1.m1.5.5.2.2.1.1"><cn type="integer" id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">1</cn><cn type="integer" id="S3.SS2.p2.1.m1.2.2.cmml" xref="S3.SS2.p2.1.m1.2.2">2</cn><ci id="S3.SS2.p2.1.m1.3.3.cmml" xref="S3.SS2.p2.1.m1.3.3">â€¦</ci><apply id="S3.SS2.p2.1.m1.5.5.2.2.1.1.1.cmml" xref="S3.SS2.p2.1.m1.5.5.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.5.5.2.2.1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.5.5.2.2.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.5.5.2.2.1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.5.5.2.2.1.1.1.2">ğ‘›</ci><ci id="S3.SS2.p2.1.m1.5.5.2.2.1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.5.5.2.2.1.1.1.3">ğ‘‘</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.5c">\mathbf{d_{i}}=(d_{i-1},d_{i}),i=1,2,...,n_{d}</annotation></semantics></math>, where <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="n_{d}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><msub id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">n</mi><mi id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">ğ‘›</ci><ci id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">n_{d}</annotation></semantics></math> denotes the number of distance intervals we want, and <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="d_{i}" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><msub id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">d</mi><mi id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">ğ‘‘</ci><ci id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">d_{i}</annotation></semantics></math> is the distance learned from the network. Every <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="\mathbf{d_{i}}" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><msub id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mi id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">ğ</mi><mi id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml">ğ¢</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2">ğ</ci><ci id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3">ğ¢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">\mathbf{d_{i}}</annotation></semantics></math> has its upper and lower bounds, which describe a range of distances. There is no intersection between intervals (<math id="S3.SS2.p2.5.m5.2" class="ltx_Math" alttext="\mathbf{d_{i}}\cap\mathbf{d_{j}}=\phi,i\neq j" display="inline"><semantics id="S3.SS2.p2.5.m5.2a"><mrow id="S3.SS2.p2.5.m5.2.2.2" xref="S3.SS2.p2.5.m5.2.2.3.cmml"><mrow id="S3.SS2.p2.5.m5.1.1.1.1" xref="S3.SS2.p2.5.m5.1.1.1.1.cmml"><mrow id="S3.SS2.p2.5.m5.1.1.1.1.2" xref="S3.SS2.p2.5.m5.1.1.1.1.2.cmml"><msub id="S3.SS2.p2.5.m5.1.1.1.1.2.2" xref="S3.SS2.p2.5.m5.1.1.1.1.2.2.cmml"><mi id="S3.SS2.p2.5.m5.1.1.1.1.2.2.2" xref="S3.SS2.p2.5.m5.1.1.1.1.2.2.2.cmml">ğ</mi><mi id="S3.SS2.p2.5.m5.1.1.1.1.2.2.3" xref="S3.SS2.p2.5.m5.1.1.1.1.2.2.3.cmml">ğ¢</mi></msub><mo id="S3.SS2.p2.5.m5.1.1.1.1.2.1" xref="S3.SS2.p2.5.m5.1.1.1.1.2.1.cmml">âˆ©</mo><msub id="S3.SS2.p2.5.m5.1.1.1.1.2.3" xref="S3.SS2.p2.5.m5.1.1.1.1.2.3.cmml"><mi id="S3.SS2.p2.5.m5.1.1.1.1.2.3.2" xref="S3.SS2.p2.5.m5.1.1.1.1.2.3.2.cmml">ğ</mi><mi id="S3.SS2.p2.5.m5.1.1.1.1.2.3.3" xref="S3.SS2.p2.5.m5.1.1.1.1.2.3.3.cmml">ğ£</mi></msub></mrow><mo id="S3.SS2.p2.5.m5.1.1.1.1.1" xref="S3.SS2.p2.5.m5.1.1.1.1.1.cmml">=</mo><mi id="S3.SS2.p2.5.m5.1.1.1.1.3" xref="S3.SS2.p2.5.m5.1.1.1.1.3.cmml">Ï•</mi></mrow><mo id="S3.SS2.p2.5.m5.2.2.2.3" xref="S3.SS2.p2.5.m5.2.2.3a.cmml">,</mo><mrow id="S3.SS2.p2.5.m5.2.2.2.2" xref="S3.SS2.p2.5.m5.2.2.2.2.cmml"><mi id="S3.SS2.p2.5.m5.2.2.2.2.2" xref="S3.SS2.p2.5.m5.2.2.2.2.2.cmml">i</mi><mo id="S3.SS2.p2.5.m5.2.2.2.2.1" xref="S3.SS2.p2.5.m5.2.2.2.2.1.cmml">â‰ </mo><mi id="S3.SS2.p2.5.m5.2.2.2.2.3" xref="S3.SS2.p2.5.m5.2.2.2.2.3.cmml">j</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.2b"><apply id="S3.SS2.p2.5.m5.2.2.3.cmml" xref="S3.SS2.p2.5.m5.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.2.2.3a.cmml" xref="S3.SS2.p2.5.m5.2.2.2.3">formulae-sequence</csymbol><apply id="S3.SS2.p2.5.m5.1.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1.1.1"><eq id="S3.SS2.p2.5.m5.1.1.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1.1.1.1"></eq><apply id="S3.SS2.p2.5.m5.1.1.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.1.1.2"><intersect id="S3.SS2.p2.5.m5.1.1.1.1.2.1.cmml" xref="S3.SS2.p2.5.m5.1.1.1.1.2.1"></intersect><apply id="S3.SS2.p2.5.m5.1.1.1.1.2.2.cmml" xref="S3.SS2.p2.5.m5.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.1.1.2.2.1.cmml" xref="S3.SS2.p2.5.m5.1.1.1.1.2.2">subscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.1.1.2.2.2.cmml" xref="S3.SS2.p2.5.m5.1.1.1.1.2.2.2">ğ</ci><ci id="S3.SS2.p2.5.m5.1.1.1.1.2.2.3.cmml" xref="S3.SS2.p2.5.m5.1.1.1.1.2.2.3">ğ¢</ci></apply><apply id="S3.SS2.p2.5.m5.1.1.1.1.2.3.cmml" xref="S3.SS2.p2.5.m5.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.1.1.2.3.1.cmml" xref="S3.SS2.p2.5.m5.1.1.1.1.2.3">subscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.1.1.2.3.2.cmml" xref="S3.SS2.p2.5.m5.1.1.1.1.2.3.2">ğ</ci><ci id="S3.SS2.p2.5.m5.1.1.1.1.2.3.3.cmml" xref="S3.SS2.p2.5.m5.1.1.1.1.2.3.3">ğ£</ci></apply></apply><ci id="S3.SS2.p2.5.m5.1.1.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.1.1.3">italic-Ï•</ci></apply><apply id="S3.SS2.p2.5.m5.2.2.2.2.cmml" xref="S3.SS2.p2.5.m5.2.2.2.2"><neq id="S3.SS2.p2.5.m5.2.2.2.2.1.cmml" xref="S3.SS2.p2.5.m5.2.2.2.2.1"></neq><ci id="S3.SS2.p2.5.m5.2.2.2.2.2.cmml" xref="S3.SS2.p2.5.m5.2.2.2.2.2">ğ‘–</ci><ci id="S3.SS2.p2.5.m5.2.2.2.2.3.cmml" xref="S3.SS2.p2.5.m5.2.2.2.2.3">ğ‘—</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.2c">\mathbf{d_{i}}\cap\mathbf{d_{j}}=\phi,i\neq j</annotation></semantics></math>).</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.10" class="ltx_p">Given feature <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="F_{ID}\in\mathbb{R}^{C\times H\times W}" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mrow id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><msub id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2.2" xref="S3.SS2.p3.1.m1.1.1.2.2.cmml">F</mi><mrow id="S3.SS2.p3.1.m1.1.1.2.3" xref="S3.SS2.p3.1.m1.1.1.2.3.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2.3.2" xref="S3.SS2.p3.1.m1.1.1.2.3.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.1.m1.1.1.2.3.1" xref="S3.SS2.p3.1.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.p3.1.m1.1.1.2.3.3" xref="S3.SS2.p3.1.m1.1.1.2.3.3.cmml">D</mi></mrow></msub><mo id="S3.SS2.p3.1.m1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml"><mi id="S3.SS2.p3.1.m1.1.1.3.2" xref="S3.SS2.p3.1.m1.1.1.3.2.cmml">â„</mi><mrow id="S3.SS2.p3.1.m1.1.1.3.3" xref="S3.SS2.p3.1.m1.1.1.3.3.cmml"><mi id="S3.SS2.p3.1.m1.1.1.3.3.2" xref="S3.SS2.p3.1.m1.1.1.3.3.2.cmml">C</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p3.1.m1.1.1.3.3.1" xref="S3.SS2.p3.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.SS2.p3.1.m1.1.1.3.3.3" xref="S3.SS2.p3.1.m1.1.1.3.3.3.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p3.1.m1.1.1.3.3.1a" xref="S3.SS2.p3.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.SS2.p3.1.m1.1.1.3.3.4" xref="S3.SS2.p3.1.m1.1.1.3.3.4.cmml">W</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><in id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1"></in><apply id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.2.1.cmml" xref="S3.SS2.p3.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.2.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2.2">ğ¹</ci><apply id="S3.SS2.p3.1.m1.1.1.2.3.cmml" xref="S3.SS2.p3.1.m1.1.1.2.3"><times id="S3.SS2.p3.1.m1.1.1.2.3.1.cmml" xref="S3.SS2.p3.1.m1.1.1.2.3.1"></times><ci id="S3.SS2.p3.1.m1.1.1.2.3.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2.3.2">ğ¼</ci><ci id="S3.SS2.p3.1.m1.1.1.2.3.3.cmml" xref="S3.SS2.p3.1.m1.1.1.2.3.3">ğ·</ci></apply></apply><apply id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.3.1.cmml" xref="S3.SS2.p3.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.3.2.cmml" xref="S3.SS2.p3.1.m1.1.1.3.2">â„</ci><apply id="S3.SS2.p3.1.m1.1.1.3.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3.3"><times id="S3.SS2.p3.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.p3.1.m1.1.1.3.3.1"></times><ci id="S3.SS2.p3.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.p3.1.m1.1.1.3.3.2">ğ¶</ci><ci id="S3.SS2.p3.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3.3.3">ğ»</ci><ci id="S3.SS2.p3.1.m1.1.1.3.3.4.cmml" xref="S3.SS2.p3.1.m1.1.1.3.3.4">ğ‘Š</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">F_{ID}\in\mathbb{R}^{C\times H\times W}</annotation></semantics></math>, which denotes the fusion of <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="F_{I}" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><msub id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">F</mi><mi id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">ğ¹</ci><ci id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">ğ¼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">F_{I}</annotation></semantics></math> and <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="F_{D}" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><msub id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml"><mi id="S3.SS2.p3.3.m3.1.1.2" xref="S3.SS2.p3.3.m3.1.1.2.cmml">F</mi><mi id="S3.SS2.p3.3.m3.1.1.3" xref="S3.SS2.p3.3.m3.1.1.3.cmml">D</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><apply id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.2">ğ¹</ci><ci id="S3.SS2.p3.3.m3.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.3">ğ·</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">F_{D}</annotation></semantics></math>, we use a <math id="S3.SS2.p3.4.m4.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S3.SS2.p3.4.m4.1a"><mrow id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml"><mn id="S3.SS2.p3.4.m4.1.1.2" xref="S3.SS2.p3.4.m4.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p3.4.m4.1.1.1" xref="S3.SS2.p3.4.m4.1.1.1.cmml">Ã—</mo><mn id="S3.SS2.p3.4.m4.1.1.3" xref="S3.SS2.p3.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><apply id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1"><times id="S3.SS2.p3.4.m4.1.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1.1"></times><cn type="integer" id="S3.SS2.p3.4.m4.1.1.2.cmml" xref="S3.SS2.p3.4.m4.1.1.2">1</cn><cn type="integer" id="S3.SS2.p3.4.m4.1.1.3.cmml" xref="S3.SS2.p3.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">1\times 1</annotation></semantics></math> convolution to reduce the channel and flatten the feature map to a vector of size <math id="S3.SS2.p3.5.m5.1" class="ltx_Math" alttext="1\times(H\times W)" display="inline"><semantics id="S3.SS2.p3.5.m5.1a"><mrow id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml"><mn id="S3.SS2.p3.5.m5.1.1.3" xref="S3.SS2.p3.5.m5.1.1.3.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p3.5.m5.1.1.2" xref="S3.SS2.p3.5.m5.1.1.2.cmml">Ã—</mo><mrow id="S3.SS2.p3.5.m5.1.1.1.1" xref="S3.SS2.p3.5.m5.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p3.5.m5.1.1.1.1.2" xref="S3.SS2.p3.5.m5.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p3.5.m5.1.1.1.1.1" xref="S3.SS2.p3.5.m5.1.1.1.1.1.cmml"><mi id="S3.SS2.p3.5.m5.1.1.1.1.1.2" xref="S3.SS2.p3.5.m5.1.1.1.1.1.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p3.5.m5.1.1.1.1.1.1" xref="S3.SS2.p3.5.m5.1.1.1.1.1.1.cmml">Ã—</mo><mi id="S3.SS2.p3.5.m5.1.1.1.1.1.3" xref="S3.SS2.p3.5.m5.1.1.1.1.1.3.cmml">W</mi></mrow><mo stretchy="false" id="S3.SS2.p3.5.m5.1.1.1.1.3" xref="S3.SS2.p3.5.m5.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><apply id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1"><times id="S3.SS2.p3.5.m5.1.1.2.cmml" xref="S3.SS2.p3.5.m5.1.1.2"></times><cn type="integer" id="S3.SS2.p3.5.m5.1.1.3.cmml" xref="S3.SS2.p3.5.m5.1.1.3">1</cn><apply id="S3.SS2.p3.5.m5.1.1.1.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1.1.1"><times id="S3.SS2.p3.5.m5.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1.1.1.1.1"></times><ci id="S3.SS2.p3.5.m5.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.5.m5.1.1.1.1.1.2">ğ»</ci><ci id="S3.SS2.p3.5.m5.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.5.m5.1.1.1.1.1.3">ğ‘Š</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">1\times(H\times W)</annotation></semantics></math>. After fully connected layers, we have a distance feature of size <math id="S3.SS2.p3.6.m6.1" class="ltx_Math" alttext="1\times n_{d}" display="inline"><semantics id="S3.SS2.p3.6.m6.1a"><mrow id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml"><mn id="S3.SS2.p3.6.m6.1.1.2" xref="S3.SS2.p3.6.m6.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p3.6.m6.1.1.1" xref="S3.SS2.p3.6.m6.1.1.1.cmml">Ã—</mo><msub id="S3.SS2.p3.6.m6.1.1.3" xref="S3.SS2.p3.6.m6.1.1.3.cmml"><mi id="S3.SS2.p3.6.m6.1.1.3.2" xref="S3.SS2.p3.6.m6.1.1.3.2.cmml">n</mi><mi id="S3.SS2.p3.6.m6.1.1.3.3" xref="S3.SS2.p3.6.m6.1.1.3.3.cmml">d</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b"><apply id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1"><times id="S3.SS2.p3.6.m6.1.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1.1"></times><cn type="integer" id="S3.SS2.p3.6.m6.1.1.2.cmml" xref="S3.SS2.p3.6.m6.1.1.2">1</cn><apply id="S3.SS2.p3.6.m6.1.1.3.cmml" xref="S3.SS2.p3.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.6.m6.1.1.3.1.cmml" xref="S3.SS2.p3.6.m6.1.1.3">subscript</csymbol><ci id="S3.SS2.p3.6.m6.1.1.3.2.cmml" xref="S3.SS2.p3.6.m6.1.1.3.2">ğ‘›</ci><ci id="S3.SS2.p3.6.m6.1.1.3.3.cmml" xref="S3.SS2.p3.6.m6.1.1.3.3">ğ‘‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">1\times n_{d}</annotation></semantics></math>, and the <math id="S3.SS2.p3.7.m7.2" class="ltx_Math" alttext="\mathbf{d}=(d_{0},d_{1},...d_{n_{d}}),d_{0}=0" display="inline"><semantics id="S3.SS2.p3.7.m7.2a"><mrow id="S3.SS2.p3.7.m7.2.2.2" xref="S3.SS2.p3.7.m7.2.2.3.cmml"><mrow id="S3.SS2.p3.7.m7.1.1.1.1" xref="S3.SS2.p3.7.m7.1.1.1.1.cmml"><mi id="S3.SS2.p3.7.m7.1.1.1.1.5" xref="S3.SS2.p3.7.m7.1.1.1.1.5.cmml">ğ</mi><mo id="S3.SS2.p3.7.m7.1.1.1.1.4" xref="S3.SS2.p3.7.m7.1.1.1.1.4.cmml">=</mo><mrow id="S3.SS2.p3.7.m7.1.1.1.1.3.3" xref="S3.SS2.p3.7.m7.1.1.1.1.3.4.cmml"><mo stretchy="false" id="S3.SS2.p3.7.m7.1.1.1.1.3.3.4" xref="S3.SS2.p3.7.m7.1.1.1.1.3.4.cmml">(</mo><msub id="S3.SS2.p3.7.m7.1.1.1.1.1.1.1" xref="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.2" xref="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.2.cmml">d</mi><mn id="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.3" xref="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.3.cmml">0</mn></msub><mo id="S3.SS2.p3.7.m7.1.1.1.1.3.3.5" xref="S3.SS2.p3.7.m7.1.1.1.1.3.4.cmml">,</mo><msub id="S3.SS2.p3.7.m7.1.1.1.1.2.2.2" xref="S3.SS2.p3.7.m7.1.1.1.1.2.2.2.cmml"><mi id="S3.SS2.p3.7.m7.1.1.1.1.2.2.2.2" xref="S3.SS2.p3.7.m7.1.1.1.1.2.2.2.2.cmml">d</mi><mn id="S3.SS2.p3.7.m7.1.1.1.1.2.2.2.3" xref="S3.SS2.p3.7.m7.1.1.1.1.2.2.2.3.cmml">1</mn></msub><mo id="S3.SS2.p3.7.m7.1.1.1.1.3.3.6" xref="S3.SS2.p3.7.m7.1.1.1.1.3.4.cmml">,</mo><mrow id="S3.SS2.p3.7.m7.1.1.1.1.3.3.3" xref="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.cmml"><mi mathvariant="normal" id="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.2" xref="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.2.cmml">â€¦</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.1" xref="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.1.cmml">â€‹</mo><msub id="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.3" xref="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.3.cmml"><mi id="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.3.2" xref="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.3.2.cmml">d</mi><msub id="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.3.3" xref="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.3.3.cmml"><mi id="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.3.3.2" xref="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.3.3.2.cmml">n</mi><mi id="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.3.3.3" xref="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.3.3.3.cmml">d</mi></msub></msub></mrow><mo stretchy="false" id="S3.SS2.p3.7.m7.1.1.1.1.3.3.7" xref="S3.SS2.p3.7.m7.1.1.1.1.3.4.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p3.7.m7.2.2.2.3" xref="S3.SS2.p3.7.m7.2.2.3a.cmml">,</mo><mrow id="S3.SS2.p3.7.m7.2.2.2.2" xref="S3.SS2.p3.7.m7.2.2.2.2.cmml"><msub id="S3.SS2.p3.7.m7.2.2.2.2.2" xref="S3.SS2.p3.7.m7.2.2.2.2.2.cmml"><mi id="S3.SS2.p3.7.m7.2.2.2.2.2.2" xref="S3.SS2.p3.7.m7.2.2.2.2.2.2.cmml">d</mi><mn id="S3.SS2.p3.7.m7.2.2.2.2.2.3" xref="S3.SS2.p3.7.m7.2.2.2.2.2.3.cmml">0</mn></msub><mo id="S3.SS2.p3.7.m7.2.2.2.2.1" xref="S3.SS2.p3.7.m7.2.2.2.2.1.cmml">=</mo><mn id="S3.SS2.p3.7.m7.2.2.2.2.3" xref="S3.SS2.p3.7.m7.2.2.2.2.3.cmml">0</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.7.m7.2b"><apply id="S3.SS2.p3.7.m7.2.2.3.cmml" xref="S3.SS2.p3.7.m7.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.2.2.3a.cmml" xref="S3.SS2.p3.7.m7.2.2.2.3">formulae-sequence</csymbol><apply id="S3.SS2.p3.7.m7.1.1.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1"><eq id="S3.SS2.p3.7.m7.1.1.1.1.4.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.4"></eq><ci id="S3.SS2.p3.7.m7.1.1.1.1.5.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.5">ğ</ci><vector id="S3.SS2.p3.7.m7.1.1.1.1.3.4.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.3.3"><apply id="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.2">ğ‘‘</ci><cn type="integer" id="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.3">0</cn></apply><apply id="S3.SS2.p3.7.m7.1.1.1.1.2.2.2.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.1.1.1.1.2.2.2.1.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.SS2.p3.7.m7.1.1.1.1.2.2.2.2.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.2.2.2.2">ğ‘‘</ci><cn type="integer" id="S3.SS2.p3.7.m7.1.1.1.1.2.2.2.3.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.2.2.2.3">1</cn></apply><apply id="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.3.3.3"><times id="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.1.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.1"></times><ci id="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.2.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.2">â€¦</ci><apply id="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.3.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.3.1.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.3">subscript</csymbol><ci id="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.3.2.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.3.2">ğ‘‘</ci><apply id="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.3.3.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.3.3.1.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.3.3">subscript</csymbol><ci id="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.3.3.2.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.3.3.2">ğ‘›</ci><ci id="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.3.3.3.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.3.3.3.3.3.3">ğ‘‘</ci></apply></apply></apply></vector></apply><apply id="S3.SS2.p3.7.m7.2.2.2.2.cmml" xref="S3.SS2.p3.7.m7.2.2.2.2"><eq id="S3.SS2.p3.7.m7.2.2.2.2.1.cmml" xref="S3.SS2.p3.7.m7.2.2.2.2.1"></eq><apply id="S3.SS2.p3.7.m7.2.2.2.2.2.cmml" xref="S3.SS2.p3.7.m7.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.2.2.2.2.2.1.cmml" xref="S3.SS2.p3.7.m7.2.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p3.7.m7.2.2.2.2.2.2.cmml" xref="S3.SS2.p3.7.m7.2.2.2.2.2.2">ğ‘‘</ci><cn type="integer" id="S3.SS2.p3.7.m7.2.2.2.2.2.3.cmml" xref="S3.SS2.p3.7.m7.2.2.2.2.2.3">0</cn></apply><cn type="integer" id="S3.SS2.p3.7.m7.2.2.2.2.3.cmml" xref="S3.SS2.p3.7.m7.2.2.2.2.3">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.7.m7.2c">\mathbf{d}=(d_{0},d_{1},...d_{n_{d}}),d_{0}=0</annotation></semantics></math> is generated using the softmax operation. Therefore, we can divide the input depth map <math id="S3.SS2.p3.8.m8.1" class="ltx_Math" alttext="Dep" display="inline"><semantics id="S3.SS2.p3.8.m8.1a"><mrow id="S3.SS2.p3.8.m8.1.1" xref="S3.SS2.p3.8.m8.1.1.cmml"><mi id="S3.SS2.p3.8.m8.1.1.2" xref="S3.SS2.p3.8.m8.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.8.m8.1.1.1" xref="S3.SS2.p3.8.m8.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p3.8.m8.1.1.3" xref="S3.SS2.p3.8.m8.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.8.m8.1.1.1a" xref="S3.SS2.p3.8.m8.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p3.8.m8.1.1.4" xref="S3.SS2.p3.8.m8.1.1.4.cmml">p</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.8.m8.1b"><apply id="S3.SS2.p3.8.m8.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1"><times id="S3.SS2.p3.8.m8.1.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1.1"></times><ci id="S3.SS2.p3.8.m8.1.1.2.cmml" xref="S3.SS2.p3.8.m8.1.1.2">ğ·</ci><ci id="S3.SS2.p3.8.m8.1.1.3.cmml" xref="S3.SS2.p3.8.m8.1.1.3">ğ‘’</ci><ci id="S3.SS2.p3.8.m8.1.1.4.cmml" xref="S3.SS2.p3.8.m8.1.1.4">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.8.m8.1c">Dep</annotation></semantics></math> into its subgraphs <math id="S3.SS2.p3.9.m9.5" class="ltx_Math" alttext="SD_{i},i=1,2,...,n_{d}" display="inline"><semantics id="S3.SS2.p3.9.m9.5a"><mrow id="S3.SS2.p3.9.m9.5.5.2" xref="S3.SS2.p3.9.m9.5.5.3.cmml"><mrow id="S3.SS2.p3.9.m9.4.4.1.1" xref="S3.SS2.p3.9.m9.4.4.1.1.cmml"><mrow id="S3.SS2.p3.9.m9.4.4.1.1.1.1" xref="S3.SS2.p3.9.m9.4.4.1.1.1.2.cmml"><mrow id="S3.SS2.p3.9.m9.4.4.1.1.1.1.1" xref="S3.SS2.p3.9.m9.4.4.1.1.1.1.1.cmml"><mi id="S3.SS2.p3.9.m9.4.4.1.1.1.1.1.2" xref="S3.SS2.p3.9.m9.4.4.1.1.1.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.9.m9.4.4.1.1.1.1.1.1" xref="S3.SS2.p3.9.m9.4.4.1.1.1.1.1.1.cmml">â€‹</mo><msub id="S3.SS2.p3.9.m9.4.4.1.1.1.1.1.3" xref="S3.SS2.p3.9.m9.4.4.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p3.9.m9.4.4.1.1.1.1.1.3.2" xref="S3.SS2.p3.9.m9.4.4.1.1.1.1.1.3.2.cmml">D</mi><mi id="S3.SS2.p3.9.m9.4.4.1.1.1.1.1.3.3" xref="S3.SS2.p3.9.m9.4.4.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="S3.SS2.p3.9.m9.4.4.1.1.1.1.2" xref="S3.SS2.p3.9.m9.4.4.1.1.1.2.cmml">,</mo><mi id="S3.SS2.p3.9.m9.1.1" xref="S3.SS2.p3.9.m9.1.1.cmml">i</mi></mrow><mo id="S3.SS2.p3.9.m9.4.4.1.1.2" xref="S3.SS2.p3.9.m9.4.4.1.1.2.cmml">=</mo><mn id="S3.SS2.p3.9.m9.4.4.1.1.3" xref="S3.SS2.p3.9.m9.4.4.1.1.3.cmml">1</mn></mrow><mo id="S3.SS2.p3.9.m9.5.5.2.3" xref="S3.SS2.p3.9.m9.5.5.3a.cmml">,</mo><mrow id="S3.SS2.p3.9.m9.5.5.2.2.1" xref="S3.SS2.p3.9.m9.5.5.2.2.2.cmml"><mn id="S3.SS2.p3.9.m9.2.2" xref="S3.SS2.p3.9.m9.2.2.cmml">2</mn><mo id="S3.SS2.p3.9.m9.5.5.2.2.1.2" xref="S3.SS2.p3.9.m9.5.5.2.2.2.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p3.9.m9.3.3" xref="S3.SS2.p3.9.m9.3.3.cmml">â€¦</mi><mo id="S3.SS2.p3.9.m9.5.5.2.2.1.3" xref="S3.SS2.p3.9.m9.5.5.2.2.2.cmml">,</mo><msub id="S3.SS2.p3.9.m9.5.5.2.2.1.1" xref="S3.SS2.p3.9.m9.5.5.2.2.1.1.cmml"><mi id="S3.SS2.p3.9.m9.5.5.2.2.1.1.2" xref="S3.SS2.p3.9.m9.5.5.2.2.1.1.2.cmml">n</mi><mi id="S3.SS2.p3.9.m9.5.5.2.2.1.1.3" xref="S3.SS2.p3.9.m9.5.5.2.2.1.1.3.cmml">d</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.9.m9.5b"><apply id="S3.SS2.p3.9.m9.5.5.3.cmml" xref="S3.SS2.p3.9.m9.5.5.2"><csymbol cd="ambiguous" id="S3.SS2.p3.9.m9.5.5.3a.cmml" xref="S3.SS2.p3.9.m9.5.5.2.3">formulae-sequence</csymbol><apply id="S3.SS2.p3.9.m9.4.4.1.1.cmml" xref="S3.SS2.p3.9.m9.4.4.1.1"><eq id="S3.SS2.p3.9.m9.4.4.1.1.2.cmml" xref="S3.SS2.p3.9.m9.4.4.1.1.2"></eq><list id="S3.SS2.p3.9.m9.4.4.1.1.1.2.cmml" xref="S3.SS2.p3.9.m9.4.4.1.1.1.1"><apply id="S3.SS2.p3.9.m9.4.4.1.1.1.1.1.cmml" xref="S3.SS2.p3.9.m9.4.4.1.1.1.1.1"><times id="S3.SS2.p3.9.m9.4.4.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.9.m9.4.4.1.1.1.1.1.1"></times><ci id="S3.SS2.p3.9.m9.4.4.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.9.m9.4.4.1.1.1.1.1.2">ğ‘†</ci><apply id="S3.SS2.p3.9.m9.4.4.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.9.m9.4.4.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.9.m9.4.4.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p3.9.m9.4.4.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p3.9.m9.4.4.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p3.9.m9.4.4.1.1.1.1.1.3.2">ğ·</ci><ci id="S3.SS2.p3.9.m9.4.4.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p3.9.m9.4.4.1.1.1.1.1.3.3">ğ‘–</ci></apply></apply><ci id="S3.SS2.p3.9.m9.1.1.cmml" xref="S3.SS2.p3.9.m9.1.1">ğ‘–</ci></list><cn type="integer" id="S3.SS2.p3.9.m9.4.4.1.1.3.cmml" xref="S3.SS2.p3.9.m9.4.4.1.1.3">1</cn></apply><list id="S3.SS2.p3.9.m9.5.5.2.2.2.cmml" xref="S3.SS2.p3.9.m9.5.5.2.2.1"><cn type="integer" id="S3.SS2.p3.9.m9.2.2.cmml" xref="S3.SS2.p3.9.m9.2.2">2</cn><ci id="S3.SS2.p3.9.m9.3.3.cmml" xref="S3.SS2.p3.9.m9.3.3">â€¦</ci><apply id="S3.SS2.p3.9.m9.5.5.2.2.1.1.cmml" xref="S3.SS2.p3.9.m9.5.5.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.9.m9.5.5.2.2.1.1.1.cmml" xref="S3.SS2.p3.9.m9.5.5.2.2.1.1">subscript</csymbol><ci id="S3.SS2.p3.9.m9.5.5.2.2.1.1.2.cmml" xref="S3.SS2.p3.9.m9.5.5.2.2.1.1.2">ğ‘›</ci><ci id="S3.SS2.p3.9.m9.5.5.2.2.1.1.3.cmml" xref="S3.SS2.p3.9.m9.5.5.2.2.1.1.3">ğ‘‘</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.9.m9.5c">SD_{i},i=1,2,...,n_{d}</annotation></semantics></math> with <math id="S3.SS2.p3.10.m10.1" class="ltx_Math" alttext="d_{i}" display="inline"><semantics id="S3.SS2.p3.10.m10.1a"><msub id="S3.SS2.p3.10.m10.1.1" xref="S3.SS2.p3.10.m10.1.1.cmml"><mi id="S3.SS2.p3.10.m10.1.1.2" xref="S3.SS2.p3.10.m10.1.1.2.cmml">d</mi><mi id="S3.SS2.p3.10.m10.1.1.3" xref="S3.SS2.p3.10.m10.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.10.m10.1b"><apply id="S3.SS2.p3.10.m10.1.1.cmml" xref="S3.SS2.p3.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.10.m10.1.1.1.cmml" xref="S3.SS2.p3.10.m10.1.1">subscript</csymbol><ci id="S3.SS2.p3.10.m10.1.1.2.cmml" xref="S3.SS2.p3.10.m10.1.1.2">ğ‘‘</ci><ci id="S3.SS2.p3.10.m10.1.1.3.cmml" xref="S3.SS2.p3.10.m10.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.10.m10.1c">d_{i}</annotation></semantics></math> using Equation <a href="#S3.E1" title="In III-B Adaptive distance interval separation â€£ III Methodology â€£ Understanding Depth Map Progressively: Adaptive Distance Interval Separation for Monocular 3d Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_math_unparsed" alttext="\footnotesize SD_{i}=\left\{\begin{matrix}SD^{(w,h)}=Dep^{(w,h)},&amp;\sum_{j=0}^{i-1}d_{j}\leq SD^{(w,h)}\leq\sum_{j=0}^{i}d_{j}\\
SD^{(w,h)}=0,&amp;Otherwise.\end{matrix}\right." display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1b"><mi mathsize="80%" id="S3.E1.m1.1.2">S</mi><msub id="S3.E1.m1.1.3"><mi mathsize="80%" id="S3.E1.m1.1.3.2">D</mi><mi mathsize="80%" id="S3.E1.m1.1.3.3">i</mi></msub><mo mathsize="80%" id="S3.E1.m1.1.4">=</mo><mrow id="S3.E1.m1.1.5"><mo id="S3.E1.m1.1.5.1">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.E1.m1.1.1.1.1"><mtr id="S3.E1.m1.1.1.1.1a"><mtd id="S3.E1.m1.1.1.1.1b"><mrow id="S3.E1.m1.1.1.1.1.5.5.5.5.5"><mrow id="S3.E1.m1.1.1.1.1.5.5.5.5.5.1"><mrow id="S3.E1.m1.1.1.1.1.5.5.5.5.5.1.2"><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.5.5.5.5.5.1.2.2">S</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.5.5.5.5.5.1.2.1">â€‹</mo><msup id="S3.E1.m1.1.1.1.1.5.5.5.5.5.1.2.3"><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.5.5.5.5.5.1.2.3.2">D</mi><mrow id="S3.E1.m1.1.1.1.1.2.2.2.2.2.2.4"><mo maxsize="80%" minsize="80%" id="S3.E1.m1.1.1.1.1.2.2.2.2.2.2.4.1">(</mo><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1">w</mi><mo mathsize="80%" id="S3.E1.m1.1.1.1.1.2.2.2.2.2.2.4.2">,</mo><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.2.2.2.2.2.2.2">h</mi><mo maxsize="80%" minsize="80%" id="S3.E1.m1.1.1.1.1.2.2.2.2.2.2.4.3">)</mo></mrow></msup></mrow><mo mathsize="80%" id="S3.E1.m1.1.1.1.1.5.5.5.5.5.1.1">=</mo><mrow id="S3.E1.m1.1.1.1.1.5.5.5.5.5.1.3"><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.5.5.5.5.5.1.3.2">D</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.5.5.5.5.5.1.3.1">â€‹</mo><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.5.5.5.5.5.1.3.3">e</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.5.5.5.5.5.1.3.1a">â€‹</mo><msup id="S3.E1.m1.1.1.1.1.5.5.5.5.5.1.3.4"><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.5.5.5.5.5.1.3.4.2">p</mi><mrow id="S3.E1.m1.1.1.1.1.4.4.4.4.4.2.4"><mo maxsize="80%" minsize="80%" id="S3.E1.m1.1.1.1.1.4.4.4.4.4.2.4.1">(</mo><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.3.3.3.3.3.1.1">w</mi><mo mathsize="80%" id="S3.E1.m1.1.1.1.1.4.4.4.4.4.2.4.2">,</mo><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.4.4.4.4.4.2.2">h</mi><mo maxsize="80%" minsize="80%" id="S3.E1.m1.1.1.1.1.4.4.4.4.4.2.4.3">)</mo></mrow></msup></mrow></mrow><mo mathsize="80%" id="S3.E1.m1.1.1.1.1.5.5.5.5.5.2">,</mo></mrow></mtd><mtd id="S3.E1.m1.1.1.1.1c"><mrow id="S3.E1.m1.1.1.1.1.7.7.7.2"><mrow id="S3.E1.m1.1.1.1.1.7.7.7.2.4"><mstyle displaystyle="false" id="S3.E1.m1.1.1.1.1.7.7.7.2.4.1"><msubsup id="S3.E1.m1.1.1.1.1.7.7.7.2.4.1a"><mo maxsize="80%" minsize="80%" stretchy="true" id="S3.E1.m1.1.1.1.1.7.7.7.2.4.1.2.2">âˆ‘</mo><mrow id="S3.E1.m1.1.1.1.1.7.7.7.2.4.1.2.3"><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.7.7.7.2.4.1.2.3.2">j</mi><mo mathsize="80%" id="S3.E1.m1.1.1.1.1.7.7.7.2.4.1.2.3.1">=</mo><mn mathsize="80%" id="S3.E1.m1.1.1.1.1.7.7.7.2.4.1.2.3.3">0</mn></mrow><mrow id="S3.E1.m1.1.1.1.1.7.7.7.2.4.1.3"><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.7.7.7.2.4.1.3.2">i</mi><mo mathsize="80%" id="S3.E1.m1.1.1.1.1.7.7.7.2.4.1.3.1">âˆ’</mo><mn mathsize="80%" id="S3.E1.m1.1.1.1.1.7.7.7.2.4.1.3.3">1</mn></mrow></msubsup></mstyle><msub id="S3.E1.m1.1.1.1.1.7.7.7.2.4.2"><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.7.7.7.2.4.2.2">d</mi><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.7.7.7.2.4.2.3">j</mi></msub></mrow><mo mathsize="80%" id="S3.E1.m1.1.1.1.1.7.7.7.2.5">â‰¤</mo><mrow id="S3.E1.m1.1.1.1.1.7.7.7.2.6"><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.7.7.7.2.6.2">S</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.7.7.7.2.6.1">â€‹</mo><msup id="S3.E1.m1.1.1.1.1.7.7.7.2.6.3"><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.7.7.7.2.6.3.2">D</mi><mrow id="S3.E1.m1.1.1.1.1.7.7.7.2.2.2.4"><mo maxsize="80%" minsize="80%" id="S3.E1.m1.1.1.1.1.7.7.7.2.2.2.4.1">(</mo><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.6.6.6.1.1.1.1">w</mi><mo mathsize="80%" id="S3.E1.m1.1.1.1.1.7.7.7.2.2.2.4.2">,</mo><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.7.7.7.2.2.2.2">h</mi><mo maxsize="80%" minsize="80%" id="S3.E1.m1.1.1.1.1.7.7.7.2.2.2.4.3">)</mo></mrow></msup></mrow><mo mathsize="80%" id="S3.E1.m1.1.1.1.1.7.7.7.2.7">â‰¤</mo><mrow id="S3.E1.m1.1.1.1.1.7.7.7.2.8"><mstyle displaystyle="false" id="S3.E1.m1.1.1.1.1.7.7.7.2.8.1"><msubsup id="S3.E1.m1.1.1.1.1.7.7.7.2.8.1a"><mo maxsize="80%" minsize="80%" stretchy="true" id="S3.E1.m1.1.1.1.1.7.7.7.2.8.1.2.2">âˆ‘</mo><mrow id="S3.E1.m1.1.1.1.1.7.7.7.2.8.1.2.3"><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.7.7.7.2.8.1.2.3.2">j</mi><mo mathsize="80%" id="S3.E1.m1.1.1.1.1.7.7.7.2.8.1.2.3.1">=</mo><mn mathsize="80%" id="S3.E1.m1.1.1.1.1.7.7.7.2.8.1.2.3.3">0</mn></mrow><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.7.7.7.2.8.1.3">i</mi></msubsup></mstyle><msub id="S3.E1.m1.1.1.1.1.7.7.7.2.8.2"><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.7.7.7.2.8.2.2">d</mi><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.7.7.7.2.8.2.3">j</mi></msub></mrow></mrow></mtd></mtr><mtr id="S3.E1.m1.1.1.1.1d"><mtd id="S3.E1.m1.1.1.1.1e"><mrow id="S3.E1.m1.1.1.1.1.10.10.3.3.3"><mrow id="S3.E1.m1.1.1.1.1.10.10.3.3.3.1"><mrow id="S3.E1.m1.1.1.1.1.10.10.3.3.3.1.2"><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.10.10.3.3.3.1.2.2">S</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.10.10.3.3.3.1.2.1">â€‹</mo><msup id="S3.E1.m1.1.1.1.1.10.10.3.3.3.1.2.3"><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.10.10.3.3.3.1.2.3.2">D</mi><mrow id="S3.E1.m1.1.1.1.1.9.9.2.2.2.2.4"><mo maxsize="80%" minsize="80%" id="S3.E1.m1.1.1.1.1.9.9.2.2.2.2.4.1">(</mo><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.8.8.1.1.1.1.1">w</mi><mo mathsize="80%" id="S3.E1.m1.1.1.1.1.9.9.2.2.2.2.4.2">,</mo><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.9.9.2.2.2.2.2">h</mi><mo maxsize="80%" minsize="80%" id="S3.E1.m1.1.1.1.1.9.9.2.2.2.2.4.3">)</mo></mrow></msup></mrow><mo mathsize="80%" id="S3.E1.m1.1.1.1.1.10.10.3.3.3.1.1">=</mo><mn mathsize="80%" id="S3.E1.m1.1.1.1.1.10.10.3.3.3.1.3">0</mn></mrow><mo mathsize="80%" id="S3.E1.m1.1.1.1.1.10.10.3.3.3.2">,</mo></mrow></mtd><mtd id="S3.E1.m1.1.1.1.1f"><mrow id="S3.E1.m1.1.1.1.1.11.11.4.1.1"><mrow id="S3.E1.m1.1.1.1.1.11.11.4.1.1.1"><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.11.11.4.1.1.1.2">O</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.11.11.4.1.1.1.1">â€‹</mo><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.11.11.4.1.1.1.3">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.11.11.4.1.1.1.1a">â€‹</mo><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.11.11.4.1.1.1.4">h</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.11.11.4.1.1.1.1b">â€‹</mo><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.11.11.4.1.1.1.5">e</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.11.11.4.1.1.1.1c">â€‹</mo><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.11.11.4.1.1.1.6">r</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.11.11.4.1.1.1.1d">â€‹</mo><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.11.11.4.1.1.1.7">w</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.11.11.4.1.1.1.1e">â€‹</mo><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.11.11.4.1.1.1.8">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.11.11.4.1.1.1.1f">â€‹</mo><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.11.11.4.1.1.1.9">s</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.11.11.4.1.1.1.1g">â€‹</mo><mi mathsize="80%" id="S3.E1.m1.1.1.1.1.11.11.4.1.1.1.10">e</mi></mrow><mo lspace="0em" mathsize="80%" id="S3.E1.m1.1.1.1.1.11.11.4.1.1.2">.</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\footnotesize SD_{i}=\left\{\begin{matrix}SD^{(w,h)}=Dep^{(w,h)},&amp;\sum_{j=0}^{i-1}d_{j}\leq SD^{(w,h)}\leq\sum_{j=0}^{i}d_{j}\\
SD^{(w,h)}=0,&amp;Otherwise.\end{matrix}\right.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<figure id="S3.F4" class="ltx_figure">
<p id="S3.F4.1" class="ltx_p ltx_align_center"><span id="S3.F4.1.1" class="ltx_text"><img src="/html/2306.10921/assets/fig4-2.png" id="S3.F4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="274" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Visualization of uncertainty maps. The top shows input images, the middle displays estimated depth maps, and the bottom exhibits our proposed uncertainty maps. As demonstrated by the confidence levels, nearby areas of the depth maps are more reliable, while the inaccuracy of the distant areas and the sky is reduced.</figcaption>
</figure>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.3" class="ltx_p">Here, <math id="S3.SS2.p5.1.m1.2" class="ltx_Math" alttext="(w,h)" display="inline"><semantics id="S3.SS2.p5.1.m1.2a"><mrow id="S3.SS2.p5.1.m1.2.3.2" xref="S3.SS2.p5.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.p5.1.m1.2.3.2.1" xref="S3.SS2.p5.1.m1.2.3.1.cmml">(</mo><mi id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml">w</mi><mo id="S3.SS2.p5.1.m1.2.3.2.2" xref="S3.SS2.p5.1.m1.2.3.1.cmml">,</mo><mi id="S3.SS2.p5.1.m1.2.2" xref="S3.SS2.p5.1.m1.2.2.cmml">h</mi><mo stretchy="false" id="S3.SS2.p5.1.m1.2.3.2.3" xref="S3.SS2.p5.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.2b"><interval closure="open" id="S3.SS2.p5.1.m1.2.3.1.cmml" xref="S3.SS2.p5.1.m1.2.3.2"><ci id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1">ğ‘¤</ci><ci id="S3.SS2.p5.1.m1.2.2.cmml" xref="S3.SS2.p5.1.m1.2.2">â„</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.2c">(w,h)</annotation></semantics></math> represents the pixel coordinates in the image of size <math id="S3.SS2.p5.2.m2.1" class="ltx_Math" alttext="H\times W" display="inline"><semantics id="S3.SS2.p5.2.m2.1a"><mrow id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml"><mi id="S3.SS2.p5.2.m2.1.1.2" xref="S3.SS2.p5.2.m2.1.1.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p5.2.m2.1.1.1" xref="S3.SS2.p5.2.m2.1.1.1.cmml">Ã—</mo><mi id="S3.SS2.p5.2.m2.1.1.3" xref="S3.SS2.p5.2.m2.1.1.3.cmml">W</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><apply id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1"><times id="S3.SS2.p5.2.m2.1.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1.1"></times><ci id="S3.SS2.p5.2.m2.1.1.2.cmml" xref="S3.SS2.p5.2.m2.1.1.2">ğ»</ci><ci id="S3.SS2.p5.2.m2.1.1.3.cmml" xref="S3.SS2.p5.2.m2.1.1.3">ğ‘Š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">H\times W</annotation></semantics></math>. The estimated depth value of the pixels is preserved in the SD, and the pixel coordinates correspond to the image, so it benefits CNNs to extract depth information. The ADIS module is more effective than directly applying CNNs to the depth map since it generates SDs with obvious curved edges precisely caused by the truncation of objects. When <math id="S3.SS2.p5.3.m3.1" class="ltx_Math" alttext="n_{d}" display="inline"><semantics id="S3.SS2.p5.3.m3.1a"><msub id="S3.SS2.p5.3.m3.1.1" xref="S3.SS2.p5.3.m3.1.1.cmml"><mi id="S3.SS2.p5.3.m3.1.1.2" xref="S3.SS2.p5.3.m3.1.1.2.cmml">n</mi><mi id="S3.SS2.p5.3.m3.1.1.3" xref="S3.SS2.p5.3.m3.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.3.m3.1b"><apply id="S3.SS2.p5.3.m3.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.3.m3.1.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p5.3.m3.1.1.2.cmml" xref="S3.SS2.p5.3.m3.1.1.2">ğ‘›</ci><ci id="S3.SS2.p5.3.m3.1.1.3.cmml" xref="S3.SS2.p5.3.m3.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.3.m3.1c">n_{d}</annotation></semantics></math> is large enough, in fact, the SDs we divide would be the dense LiDAR point cloud. Fig <a href="#S3.F3" title="Figure 3 â€£ III-A Pipeline overview â€£ III Methodology â€£ Understanding Depth Map Progressively: Adaptive Distance Interval Separation for Monocular 3d Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the generated SDs, these SDs contain obvious curved edges, which are precisely caused by the truncation of objects.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Uncertainty map</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.2" class="ltx_p">Our proposed model heavily relies on the accuracy of the depth map during the feature extraction stage. However, the depth map we utilize is obtained from a pre-existing depth estimation method, which implies that it may not be entirely precise. The regions that are not trained thoroughly by the LiDAR in the depth estimation task, such as faraway objects or the sky, tend to perform poorly, and this inaccuracy remains on the depth map. This can be disadvantageous when estimating the depth of a dense image, as it can negatively impact the modelâ€™s performance. To address this, we draw inspiration from methods based on uncertainty that utilize uncertainty measurements to mitigate errors when there is noise in the initial condition. Thus, we incorporate an uncertainty map into our network to guide the model during the process of extracting depth information. As we aim to mitigate the inaccuracy of the depth map, our depth extraction module learns the uncertainty point by point, suppressing inaccurate pixels in the estimated depth map and reducing the negative impact on the model. To enable the uncertainty map to locate the regions to be suppressed in the image and depth map, we jointly generate the uncertainty feature <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="U" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">ğ‘ˆ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">U</annotation></semantics></math> by combining the image and depth map features. This uncertainty feature is also used when extracting <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{d}" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">ğ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">ğ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">\mathbf{d}</annotation></semantics></math>, ensuring that the two modules work together to further improve the accuracy of the depth map. The details of our approach are provided below:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="U^{H\times W}=\mathbf{1}^{H\times W}-Sigmoid(Upsampling(F_{ID}))^{H\times W}," display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><msup id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.3.2.cmml">U</mi><mrow id="S3.E2.m1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.3.2" xref="S3.E2.m1.1.1.1.1.3.3.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E2.m1.1.1.1.1.3.3.1" xref="S3.E2.m1.1.1.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.E2.m1.1.1.1.1.3.3.3" xref="S3.E2.m1.1.1.1.1.3.3.3.cmml">W</mi></mrow></msup><mo id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml"><msup id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.3.cmml"><mn id="S3.E2.m1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.3.2.cmml">ğŸ</mn><mrow id="S3.E2.m1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.3.3.2" xref="S3.E2.m1.1.1.1.1.1.3.3.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E2.m1.1.1.1.1.1.3.3.1" xref="S3.E2.m1.1.1.1.1.1.3.3.1.cmml">Ã—</mo><mi id="S3.E2.m1.1.1.1.1.1.3.3.3" xref="S3.E2.m1.1.1.1.1.1.3.3.3.cmml">W</mi></mrow></msup><mo id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.2.cmml">âˆ’</mo><mrow id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.4" xref="S3.E2.m1.1.1.1.1.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.2a" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.5" xref="S3.E2.m1.1.1.1.1.1.1.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.2b" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.6" xref="S3.E2.m1.1.1.1.1.1.1.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.2c" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.7" xref="S3.E2.m1.1.1.1.1.1.1.7.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.2d" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.8" xref="S3.E2.m1.1.1.1.1.1.1.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.2e" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.9" xref="S3.E2.m1.1.1.1.1.1.1.9.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.2f" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml">â€‹</mo><msup id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">U</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.4" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2a" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.5" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2b" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2c" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.7" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.7.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2d" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.8" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.8.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2e" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.9" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.9.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2f" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.10" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.10.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2g" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.11" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.11.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2h" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.12" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.12.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2i" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">F</mi><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">D</mi></mrow></msub><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mrow id="S3.E2.m1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.3.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E2.m1.1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.1.1.1.1.3.1.cmml">Ã—</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.3.3.cmml">W</mi></mrow></msup></mrow></mrow></mrow><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"></eq><apply id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2">ğ‘ˆ</ci><apply id="S3.E2.m1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3"><times id="S3.E2.m1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.3.1"></times><ci id="S3.E2.m1.1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.3.2">ğ»</ci><ci id="S3.E2.m1.1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3">ğ‘Š</ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><minus id="S3.E2.m1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2"></minus><apply id="S3.E2.m1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.3">superscript</csymbol><cn type="integer" id="S3.E2.m1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.3.2">1</cn><apply id="S3.E2.m1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3.3"><times id="S3.E2.m1.1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.3.3.1"></times><ci id="S3.E2.m1.1.1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.3.3.2">ğ»</ci><ci id="S3.E2.m1.1.1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3.3.3">ğ‘Š</ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2"></times><ci id="S3.E2.m1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3">ğ‘†</ci><ci id="S3.E2.m1.1.1.1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.1.1.1.4">ğ‘–</ci><ci id="S3.E2.m1.1.1.1.1.1.1.5.cmml" xref="S3.E2.m1.1.1.1.1.1.1.5">ğ‘”</ci><ci id="S3.E2.m1.1.1.1.1.1.1.6.cmml" xref="S3.E2.m1.1.1.1.1.1.1.6">ğ‘š</ci><ci id="S3.E2.m1.1.1.1.1.1.1.7.cmml" xref="S3.E2.m1.1.1.1.1.1.1.7">ğ‘œ</ci><ci id="S3.E2.m1.1.1.1.1.1.1.8.cmml" xref="S3.E2.m1.1.1.1.1.1.1.8">ğ‘–</ci><ci id="S3.E2.m1.1.1.1.1.1.1.9.cmml" xref="S3.E2.m1.1.1.1.1.1.1.9">ğ‘‘</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2"></times><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3">ğ‘ˆ</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.4">ğ‘</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.5.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.5">ğ‘ </ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.6">ğ‘</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.7.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.7">ğ‘š</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.8.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.8">ğ‘</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.9.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.9">ğ‘™</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.10.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.10">ğ‘–</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.11.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.11">ğ‘›</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.12.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.12">ğ‘”</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">ğ¹</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3"><times id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1"></times><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2">ğ¼</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3">ğ·</ci></apply></apply></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3"><times id="S3.E2.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3.1"></times><ci id="S3.E2.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3.2">ğ»</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3.3">ğ‘Š</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">U^{H\times W}=\mathbf{1}^{H\times W}-Sigmoid(Upsampling(F_{ID}))^{H\times W},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p1.9" class="ltx_p">shows the process of generating the uncertainty map, where <math id="S3.SS3.p1.3.m1.1" class="ltx_Math" alttext="\mathbf{1}^{H\times W}" display="inline"><semantics id="S3.SS3.p1.3.m1.1a"><msup id="S3.SS3.p1.3.m1.1.1" xref="S3.SS3.p1.3.m1.1.1.cmml"><mn id="S3.SS3.p1.3.m1.1.1.2" xref="S3.SS3.p1.3.m1.1.1.2.cmml">ğŸ</mn><mrow id="S3.SS3.p1.3.m1.1.1.3" xref="S3.SS3.p1.3.m1.1.1.3.cmml"><mi id="S3.SS3.p1.3.m1.1.1.3.2" xref="S3.SS3.p1.3.m1.1.1.3.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p1.3.m1.1.1.3.1" xref="S3.SS3.p1.3.m1.1.1.3.1.cmml">Ã—</mo><mi id="S3.SS3.p1.3.m1.1.1.3.3" xref="S3.SS3.p1.3.m1.1.1.3.3.cmml">W</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m1.1b"><apply id="S3.SS3.p1.3.m1.1.1.cmml" xref="S3.SS3.p1.3.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m1.1.1.1.cmml" xref="S3.SS3.p1.3.m1.1.1">superscript</csymbol><cn type="integer" id="S3.SS3.p1.3.m1.1.1.2.cmml" xref="S3.SS3.p1.3.m1.1.1.2">1</cn><apply id="S3.SS3.p1.3.m1.1.1.3.cmml" xref="S3.SS3.p1.3.m1.1.1.3"><times id="S3.SS3.p1.3.m1.1.1.3.1.cmml" xref="S3.SS3.p1.3.m1.1.1.3.1"></times><ci id="S3.SS3.p1.3.m1.1.1.3.2.cmml" xref="S3.SS3.p1.3.m1.1.1.3.2">ğ»</ci><ci id="S3.SS3.p1.3.m1.1.1.3.3.cmml" xref="S3.SS3.p1.3.m1.1.1.3.3">ğ‘Š</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m1.1c">\mathbf{1}^{H\times W}</annotation></semantics></math> represents the <math id="S3.SS3.p1.4.m2.1" class="ltx_Math" alttext="H\times W" display="inline"><semantics id="S3.SS3.p1.4.m2.1a"><mrow id="S3.SS3.p1.4.m2.1.1" xref="S3.SS3.p1.4.m2.1.1.cmml"><mi id="S3.SS3.p1.4.m2.1.1.2" xref="S3.SS3.p1.4.m2.1.1.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p1.4.m2.1.1.1" xref="S3.SS3.p1.4.m2.1.1.1.cmml">Ã—</mo><mi id="S3.SS3.p1.4.m2.1.1.3" xref="S3.SS3.p1.4.m2.1.1.3.cmml">W</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m2.1b"><apply id="S3.SS3.p1.4.m2.1.1.cmml" xref="S3.SS3.p1.4.m2.1.1"><times id="S3.SS3.p1.4.m2.1.1.1.cmml" xref="S3.SS3.p1.4.m2.1.1.1"></times><ci id="S3.SS3.p1.4.m2.1.1.2.cmml" xref="S3.SS3.p1.4.m2.1.1.2">ğ»</ci><ci id="S3.SS3.p1.4.m2.1.1.3.cmml" xref="S3.SS3.p1.4.m2.1.1.3">ğ‘Š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m2.1c">H\times W</annotation></semantics></math> matrix with elements of 1, and <math id="S3.SS3.p1.5.m3.1" class="ltx_Math" alttext="F_{ID}" display="inline"><semantics id="S3.SS3.p1.5.m3.1a"><msub id="S3.SS3.p1.5.m3.1.1" xref="S3.SS3.p1.5.m3.1.1.cmml"><mi id="S3.SS3.p1.5.m3.1.1.2" xref="S3.SS3.p1.5.m3.1.1.2.cmml">F</mi><mrow id="S3.SS3.p1.5.m3.1.1.3" xref="S3.SS3.p1.5.m3.1.1.3.cmml"><mi id="S3.SS3.p1.5.m3.1.1.3.2" xref="S3.SS3.p1.5.m3.1.1.3.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.5.m3.1.1.3.1" xref="S3.SS3.p1.5.m3.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p1.5.m3.1.1.3.3" xref="S3.SS3.p1.5.m3.1.1.3.3.cmml">D</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m3.1b"><apply id="S3.SS3.p1.5.m3.1.1.cmml" xref="S3.SS3.p1.5.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.5.m3.1.1.1.cmml" xref="S3.SS3.p1.5.m3.1.1">subscript</csymbol><ci id="S3.SS3.p1.5.m3.1.1.2.cmml" xref="S3.SS3.p1.5.m3.1.1.2">ğ¹</ci><apply id="S3.SS3.p1.5.m3.1.1.3.cmml" xref="S3.SS3.p1.5.m3.1.1.3"><times id="S3.SS3.p1.5.m3.1.1.3.1.cmml" xref="S3.SS3.p1.5.m3.1.1.3.1"></times><ci id="S3.SS3.p1.5.m3.1.1.3.2.cmml" xref="S3.SS3.p1.5.m3.1.1.3.2">ğ¼</ci><ci id="S3.SS3.p1.5.m3.1.1.3.3.cmml" xref="S3.SS3.p1.5.m3.1.1.3.3">ğ·</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m3.1c">F_{ID}</annotation></semantics></math> is the fusion of <math id="S3.SS3.p1.6.m4.1" class="ltx_Math" alttext="F_{I}" display="inline"><semantics id="S3.SS3.p1.6.m4.1a"><msub id="S3.SS3.p1.6.m4.1.1" xref="S3.SS3.p1.6.m4.1.1.cmml"><mi id="S3.SS3.p1.6.m4.1.1.2" xref="S3.SS3.p1.6.m4.1.1.2.cmml">F</mi><mi id="S3.SS3.p1.6.m4.1.1.3" xref="S3.SS3.p1.6.m4.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m4.1b"><apply id="S3.SS3.p1.6.m4.1.1.cmml" xref="S3.SS3.p1.6.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.6.m4.1.1.1.cmml" xref="S3.SS3.p1.6.m4.1.1">subscript</csymbol><ci id="S3.SS3.p1.6.m4.1.1.2.cmml" xref="S3.SS3.p1.6.m4.1.1.2">ğ¹</ci><ci id="S3.SS3.p1.6.m4.1.1.3.cmml" xref="S3.SS3.p1.6.m4.1.1.3">ğ¼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m4.1c">F_{I}</annotation></semantics></math> and <math id="S3.SS3.p1.7.m5.1" class="ltx_Math" alttext="F_{D}" display="inline"><semantics id="S3.SS3.p1.7.m5.1a"><msub id="S3.SS3.p1.7.m5.1.1" xref="S3.SS3.p1.7.m5.1.1.cmml"><mi id="S3.SS3.p1.7.m5.1.1.2" xref="S3.SS3.p1.7.m5.1.1.2.cmml">F</mi><mi id="S3.SS3.p1.7.m5.1.1.3" xref="S3.SS3.p1.7.m5.1.1.3.cmml">D</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.7.m5.1b"><apply id="S3.SS3.p1.7.m5.1.1.cmml" xref="S3.SS3.p1.7.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.7.m5.1.1.1.cmml" xref="S3.SS3.p1.7.m5.1.1">subscript</csymbol><ci id="S3.SS3.p1.7.m5.1.1.2.cmml" xref="S3.SS3.p1.7.m5.1.1.2">ğ¹</ci><ci id="S3.SS3.p1.7.m5.1.1.3.cmml" xref="S3.SS3.p1.7.m5.1.1.3">ğ·</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.7.m5.1c">F_{D}</annotation></semantics></math>. The sigmoid function used ensures that each pixel in the corresponding depth map is assigned an uncertainty value between 0 and 1. To avoid gradient vanishing in the early stage of training, we obtain <math id="S3.SS3.p1.8.m6.1" class="ltx_Math" alttext="1-U" display="inline"><semantics id="S3.SS3.p1.8.m6.1a"><mrow id="S3.SS3.p1.8.m6.1.1" xref="S3.SS3.p1.8.m6.1.1.cmml"><mn id="S3.SS3.p1.8.m6.1.1.2" xref="S3.SS3.p1.8.m6.1.1.2.cmml">1</mn><mo id="S3.SS3.p1.8.m6.1.1.1" xref="S3.SS3.p1.8.m6.1.1.1.cmml">âˆ’</mo><mi id="S3.SS3.p1.8.m6.1.1.3" xref="S3.SS3.p1.8.m6.1.1.3.cmml">U</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.8.m6.1b"><apply id="S3.SS3.p1.8.m6.1.1.cmml" xref="S3.SS3.p1.8.m6.1.1"><minus id="S3.SS3.p1.8.m6.1.1.1.cmml" xref="S3.SS3.p1.8.m6.1.1.1"></minus><cn type="integer" id="S3.SS3.p1.8.m6.1.1.2.cmml" xref="S3.SS3.p1.8.m6.1.1.2">1</cn><ci id="S3.SS3.p1.8.m6.1.1.3.cmml" xref="S3.SS3.p1.8.m6.1.1.3">ğ‘ˆ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.8.m6.1c">1-U</annotation></semantics></math> instead of directly predicting <math id="S3.SS3.p1.9.m7.1" class="ltx_Math" alttext="U" display="inline"><semantics id="S3.SS3.p1.9.m7.1a"><mi id="S3.SS3.p1.9.m7.1.1" xref="S3.SS3.p1.9.m7.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.9.m7.1b"><ci id="S3.SS3.p1.9.m7.1.1.cmml" xref="S3.SS3.p1.9.m7.1.1">ğ‘ˆ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.9.m7.1c">U</annotation></semantics></math>. The trained uncertainty maps are presented in <a href="#S3.F4" title="Figure 4 â€£ III-B Adaptive distance interval separation â€£ III Methodology â€£ Understanding Depth Map Progressively: Adaptive Distance Interval Separation for Monocular 3d Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The final output feature is the result of combining the SDs, which is obtained through the following equation:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.1" class="ltx_Math" alttext="F_{SD}=Conv(\mathbf{SD}\times\mathbf{U})," display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><msub id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.3.2.cmml">F</mi><mrow id="S3.E3.m1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.3.3.cmml"><mi id="S3.E3.m1.1.1.1.1.3.3.2" xref="S3.E3.m1.1.1.1.1.3.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.3.3.1" xref="S3.E3.m1.1.1.1.1.3.3.1.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.1.1.3.3.3" xref="S3.E3.m1.1.1.1.1.3.3.3.cmml">D</mi></mrow></msub><mo id="S3.E3.m1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.1.1.1.4" xref="S3.E3.m1.1.1.1.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.2a" xref="S3.E3.m1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.1.1.1.5" xref="S3.E3.m1.1.1.1.1.1.5.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.2b" xref="S3.E3.m1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E3.m1.1.1.1.1.1.6" xref="S3.E3.m1.1.1.1.1.1.6.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.2c" xref="S3.E3.m1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml">ğ’ğƒ</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml">Ã—</mo><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml">ğ”</mi></mrow><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><eq id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2"></eq><apply id="S3.E3.m1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.3.2">ğ¹</ci><apply id="S3.E3.m1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3"><times id="S3.E3.m1.1.1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.1.1.3.3.1"></times><ci id="S3.E3.m1.1.1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.1.1.3.3.2">ğ‘†</ci><ci id="S3.E3.m1.1.1.1.1.3.3.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3.3">ğ·</ci></apply></apply><apply id="S3.E3.m1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"><times id="S3.E3.m1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.2"></times><ci id="S3.E3.m1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.3">ğ¶</ci><ci id="S3.E3.m1.1.1.1.1.1.4.cmml" xref="S3.E3.m1.1.1.1.1.1.4">ğ‘œ</ci><ci id="S3.E3.m1.1.1.1.1.1.5.cmml" xref="S3.E3.m1.1.1.1.1.1.5">ğ‘›</ci><ci id="S3.E3.m1.1.1.1.1.1.6.cmml" xref="S3.E3.m1.1.1.1.1.1.6">ğ‘£</ci><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1"><times id="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1"></times><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2">ğ’ğƒ</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3">ğ”</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">F_{SD}=Conv(\mathbf{SD}\times\mathbf{U}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p1.14" class="ltx_p">where <math id="S3.SS3.p1.10.m1.1" class="ltx_Math" alttext="\mathbf{SD}" display="inline"><semantics id="S3.SS3.p1.10.m1.1a"><mi id="S3.SS3.p1.10.m1.1.1" xref="S3.SS3.p1.10.m1.1.1.cmml">ğ’ğƒ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.10.m1.1b"><ci id="S3.SS3.p1.10.m1.1.1.cmml" xref="S3.SS3.p1.10.m1.1.1">ğ’ğƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.10.m1.1c">\mathbf{SD}</annotation></semantics></math> and <math id="S3.SS3.p1.11.m2.1" class="ltx_Math" alttext="\mathbf{U}" display="inline"><semantics id="S3.SS3.p1.11.m2.1a"><mi id="S3.SS3.p1.11.m2.1.1" xref="S3.SS3.p1.11.m2.1.1.cmml">ğ”</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.11.m2.1b"><ci id="S3.SS3.p1.11.m2.1.1.cmml" xref="S3.SS3.p1.11.m2.1.1">ğ”</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.11.m2.1c">\mathbf{U}</annotation></semantics></math> denote the stacked <math id="S3.SS3.p1.12.m3.1" class="ltx_Math" alttext="SD_{i}" display="inline"><semantics id="S3.SS3.p1.12.m3.1a"><mrow id="S3.SS3.p1.12.m3.1.1" xref="S3.SS3.p1.12.m3.1.1.cmml"><mi id="S3.SS3.p1.12.m3.1.1.2" xref="S3.SS3.p1.12.m3.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.12.m3.1.1.1" xref="S3.SS3.p1.12.m3.1.1.1.cmml">â€‹</mo><msub id="S3.SS3.p1.12.m3.1.1.3" xref="S3.SS3.p1.12.m3.1.1.3.cmml"><mi id="S3.SS3.p1.12.m3.1.1.3.2" xref="S3.SS3.p1.12.m3.1.1.3.2.cmml">D</mi><mi id="S3.SS3.p1.12.m3.1.1.3.3" xref="S3.SS3.p1.12.m3.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.12.m3.1b"><apply id="S3.SS3.p1.12.m3.1.1.cmml" xref="S3.SS3.p1.12.m3.1.1"><times id="S3.SS3.p1.12.m3.1.1.1.cmml" xref="S3.SS3.p1.12.m3.1.1.1"></times><ci id="S3.SS3.p1.12.m3.1.1.2.cmml" xref="S3.SS3.p1.12.m3.1.1.2">ğ‘†</ci><apply id="S3.SS3.p1.12.m3.1.1.3.cmml" xref="S3.SS3.p1.12.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p1.12.m3.1.1.3.1.cmml" xref="S3.SS3.p1.12.m3.1.1.3">subscript</csymbol><ci id="S3.SS3.p1.12.m3.1.1.3.2.cmml" xref="S3.SS3.p1.12.m3.1.1.3.2">ğ·</ci><ci id="S3.SS3.p1.12.m3.1.1.3.3.cmml" xref="S3.SS3.p1.12.m3.1.1.3.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.12.m3.1c">SD_{i}</annotation></semantics></math> and <math id="S3.SS3.p1.13.m4.1" class="ltx_Math" alttext="U" display="inline"><semantics id="S3.SS3.p1.13.m4.1a"><mi id="S3.SS3.p1.13.m4.1.1" xref="S3.SS3.p1.13.m4.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.13.m4.1b"><ci id="S3.SS3.p1.13.m4.1.1.cmml" xref="S3.SS3.p1.13.m4.1.1">ğ‘ˆ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.13.m4.1c">U</annotation></semantics></math>, respectively. Both are duplicated to the same channel according to the <math id="S3.SS3.p1.14.m5.1" class="ltx_Math" alttext="n_{d}" display="inline"><semantics id="S3.SS3.p1.14.m5.1a"><msub id="S3.SS3.p1.14.m5.1.1" xref="S3.SS3.p1.14.m5.1.1.cmml"><mi id="S3.SS3.p1.14.m5.1.1.2" xref="S3.SS3.p1.14.m5.1.1.2.cmml">n</mi><mi id="S3.SS3.p1.14.m5.1.1.3" xref="S3.SS3.p1.14.m5.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.14.m5.1b"><apply id="S3.SS3.p1.14.m5.1.1.cmml" xref="S3.SS3.p1.14.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.14.m5.1.1.1.cmml" xref="S3.SS3.p1.14.m5.1.1">subscript</csymbol><ci id="S3.SS3.p1.14.m5.1.1.2.cmml" xref="S3.SS3.p1.14.m5.1.1.2">ğ‘›</ci><ci id="S3.SS3.p1.14.m5.1.1.3.cmml" xref="S3.SS3.p1.14.m5.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.14.m5.1c">n_{d}</annotation></semantics></math>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>
Comparison of Different Methods for the â€œCarâ€ Category on the KITTI Test Set with AP 3D and AP BEV. We Highlight the Best Results in <span id="S3.T1.3.1" class="ltx_text ltx_font_bold">bold</span> and the Second Place in <span id="S3.T1.4.2" class="ltx_text ltx_framed ltx_framed_underline">underlined</span></figcaption>
<div id="S3.T1.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:346.9pt;height:176.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-56.2pt,28.6pt) scale(0.755354736864209,0.755354736864209) ;">
<table id="S3.T1.5.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.5.1.1.1" class="ltx_tr">
<td id="S3.T1.5.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="2"><span id="S3.T1.5.1.1.1.1.1" class="ltx_text">Experiments</span></td>
<td id="S3.T1.5.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="2"><span id="S3.T1.5.1.1.1.2.1" class="ltx_text">Implementation mode</span></td>
<td id="S3.T1.5.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" colspan="3">AP 3D</td>
<td id="S3.T1.5.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" colspan="3">AP BEV</td>
</tr>
<tr id="S3.T1.5.1.2.2" class="ltx_tr">
<td id="S3.T1.5.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Easy</td>
<td id="S3.T1.5.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Mod.</td>
<td id="S3.T1.5.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Hard</td>
<td id="S3.T1.5.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Easy</td>
<td id="S3.T1.5.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Mod.</td>
<td id="S3.T1.5.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Hard</td>
</tr>
<tr id="S3.T1.5.1.3.3" class="ltx_tr">
<td id="S3.T1.5.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">M3D-RPN(base line) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</td>
<td id="S3.T1.5.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">image-based</td>
<td id="S3.T1.5.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">14.76</td>
<td id="S3.T1.5.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">9.71</td>
<td id="S3.T1.5.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">7.42</td>
<td id="S3.T1.5.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">21.02</td>
<td id="S3.T1.5.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">13.67</td>
<td id="S3.T1.5.1.3.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">10.23</td>
</tr>
<tr id="S3.T1.5.1.4.4" class="ltx_tr">
<td id="S3.T1.5.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">MonoCInIS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</td>
<td id="S3.T1.5.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">image-based</td>
<td id="S3.T1.5.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">15.82</td>
<td id="S3.T1.5.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">7.94</td>
<td id="S3.T1.5.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">6.68</td>
<td id="S3.T1.5.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">22.28</td>
<td id="S3.T1.5.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">11.64</td>
<td id="S3.T1.5.1.4.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">9.95</td>
</tr>
<tr id="S3.T1.5.1.5.5" class="ltx_tr">
<td id="S3.T1.5.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">ImVoxelNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>
</td>
<td id="S3.T1.5.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">image-based</td>
<td id="S3.T1.5.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">17.15</td>
<td id="S3.T1.5.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">10.97</td>
<td id="S3.T1.5.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">9.15</td>
<td id="S3.T1.5.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">25.19</td>
<td id="S3.T1.5.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">16.37</td>
<td id="S3.T1.5.1.5.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">13.58</td>
</tr>
<tr id="S3.T1.5.1.6.6" class="ltx_tr">
<td id="S3.T1.5.1.6.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">M3DSSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
</td>
<td id="S3.T1.5.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">image-based</td>
<td id="S3.T1.5.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T1.5.1.6.6.3.1" class="ltx_text ltx_framed ltx_framed_underline">17.51</span></td>
<td id="S3.T1.5.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">11.46</td>
<td id="S3.T1.5.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">8.98</td>
<td id="S3.T1.5.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">24.15</td>
<td id="S3.T1.5.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">15.93</td>
<td id="S3.T1.5.1.6.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">12.11</td>
</tr>
<tr id="S3.T1.5.1.7.7" class="ltx_tr">
<td id="S3.T1.5.1.7.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">AM3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>
</td>
<td id="S3.T1.5.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">pseudo LiDAR-based</td>
<td id="S3.T1.5.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">16.50</td>
<td id="S3.T1.5.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">10.74</td>
<td id="S3.T1.5.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">9.52</td>
<td id="S3.T1.5.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">25.03</td>
<td id="S3.T1.5.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">17.32</td>
<td id="S3.T1.5.1.7.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">14.91</td>
</tr>
<tr id="S3.T1.5.1.8.8" class="ltx_tr">
<td id="S3.T1.5.1.8.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">PatchNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</td>
<td id="S3.T1.5.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">pseudo LiDAR-based</td>
<td id="S3.T1.5.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">15.68</td>
<td id="S3.T1.5.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">11.12</td>
<td id="S3.T1.5.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T1.5.1.8.8.5.1" class="ltx_text ltx_framed ltx_framed_underline">10.17</span></td>
<td id="S3.T1.5.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">22.97</td>
<td id="S3.T1.5.1.8.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">16.86</td>
<td id="S3.T1.5.1.8.8.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">14.97</td>
</tr>
<tr id="S3.T1.5.1.9.9" class="ltx_tr">
<td id="S3.T1.5.1.9.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Zhao et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>
</td>
<td id="S3.T1.5.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">pseudo LiDAR-based</td>
<td id="S3.T1.5.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">15.86</td>
<td id="S3.T1.5.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">11.41</td>
<td id="S3.T1.5.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">10.08</td>
<td id="S3.T1.5.1.9.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T1.5.1.9.9.6.1" class="ltx_text ltx_font_bold">28.79</span></td>
<td id="S3.T1.5.1.9.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T1.5.1.9.9.7.1" class="ltx_text ltx_font_bold">18.61</span></td>
<td id="S3.T1.5.1.9.9.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T1.5.1.9.9.8.1" class="ltx_text ltx_framed ltx_framed_underline">15.27</span></td>
</tr>
<tr id="S3.T1.5.1.10.10" class="ltx_tr">
<td id="S3.T1.5.1.10.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Cai et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</td>
<td id="S3.T1.5.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">depth-assisted</td>
<td id="S3.T1.5.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">11.68</td>
<td id="S3.T1.5.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">7.28</td>
<td id="S3.T1.5.1.10.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">5.69</td>
<td id="S3.T1.5.1.10.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">24.62</td>
<td id="S3.T1.5.1.10.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">14.66</td>
<td id="S3.T1.5.1.10.10.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">11.46</td>
</tr>
<tr id="S3.T1.5.1.11.11" class="ltx_tr">
<td id="S3.T1.5.1.11.11.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Ji et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
</td>
<td id="S3.T1.5.1.11.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">depth-assisted</td>
<td id="S3.T1.5.1.11.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">12.46</td>
<td id="S3.T1.5.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">7.86</td>
<td id="S3.T1.5.1.11.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">6.30</td>
<td id="S3.T1.5.1.11.11.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">19.85</td>
<td id="S3.T1.5.1.11.11.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">13.07</td>
<td id="S3.T1.5.1.11.11.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">10.29</td>
</tr>
<tr id="S3.T1.5.1.12.12" class="ltx_tr">
<td id="S3.T1.5.1.12.12.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">D4LCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</td>
<td id="S3.T1.5.1.12.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">depth-assisted</td>
<td id="S3.T1.5.1.12.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">16.65</td>
<td id="S3.T1.5.1.12.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T1.5.1.12.12.4.1" class="ltx_text ltx_framed ltx_framed_underline">11.72</span></td>
<td id="S3.T1.5.1.12.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">9.51</td>
<td id="S3.T1.5.1.12.12.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">22.51</td>
<td id="S3.T1.5.1.12.12.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">16.02</td>
<td id="S3.T1.5.1.12.12.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">12.55</td>
</tr>
<tr id="S3.T1.5.1.13.13" class="ltx_tr">
<td id="S3.T1.5.1.13.13.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">ADISN(Ours)</td>
<td id="S3.T1.5.1.13.13.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">depth-assisted</td>
<td id="S3.T1.5.1.13.13.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T1.5.1.13.13.3.1" class="ltx_text ltx_font_bold">17.87</span></td>
<td id="S3.T1.5.1.13.13.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T1.5.1.13.13.4.1" class="ltx_text ltx_font_bold">12.14</span></td>
<td id="S3.T1.5.1.13.13.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T1.5.1.13.13.5.1" class="ltx_text ltx_font_bold">10.42</span></td>
<td id="S3.T1.5.1.13.13.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T1.5.1.13.13.6.1" class="ltx_text ltx_framed ltx_framed_underline">27.18</span></td>
<td id="S3.T1.5.1.13.13.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T1.5.1.13.13.7.1" class="ltx_text ltx_framed ltx_framed_underline">17.91</span></td>
<td id="S3.T1.5.1.13.13.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T1.5.1.13.13.8.1" class="ltx_text ltx_font_bold">15.33</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">Feature decoupling</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Inspired by DRF-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, DDMP-3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, and other monocular 3D object detection methods, it has been noted that the position of the object in space, i.e., <math id="S3.SS4.p1.1.m1.3" class="ltx_Math" alttext="(x,y,z)" display="inline"><semantics id="S3.SS4.p1.1.m1.3a"><mrow id="S3.SS4.p1.1.m1.3.4.2" xref="S3.SS4.p1.1.m1.3.4.1.cmml"><mo stretchy="false" id="S3.SS4.p1.1.m1.3.4.2.1" xref="S3.SS4.p1.1.m1.3.4.1.cmml">(</mo><mi id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">x</mi><mo id="S3.SS4.p1.1.m1.3.4.2.2" xref="S3.SS4.p1.1.m1.3.4.1.cmml">,</mo><mi id="S3.SS4.p1.1.m1.2.2" xref="S3.SS4.p1.1.m1.2.2.cmml">y</mi><mo id="S3.SS4.p1.1.m1.3.4.2.3" xref="S3.SS4.p1.1.m1.3.4.1.cmml">,</mo><mi id="S3.SS4.p1.1.m1.3.3" xref="S3.SS4.p1.1.m1.3.3.cmml">z</mi><mo stretchy="false" id="S3.SS4.p1.1.m1.3.4.2.4" xref="S3.SS4.p1.1.m1.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.3b"><vector id="S3.SS4.p1.1.m1.3.4.1.cmml" xref="S3.SS4.p1.1.m1.3.4.2"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">ğ‘¥</ci><ci id="S3.SS4.p1.1.m1.2.2.cmml" xref="S3.SS4.p1.1.m1.2.2">ğ‘¦</ci><ci id="S3.SS4.p1.1.m1.3.3.cmml" xref="S3.SS4.p1.1.m1.3.3">ğ‘§</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.3c">(x,y,z)</annotation></semantics></math>, is the most critical factor in the performance of monocular 3D detection. However, images suffer from the weakness of understanding depth, and the depth estimation task exists to address this issue. Experiments have shown that even if only a single path of depth maps is used as input, the performance of 3D detection is better than when only RGB images are used. Nevertheless, relying solely on depth maps makes it difficult to understand the appearance and category of the object. Moreover, over-reliance on depth maps is affected by inaccurate depth estimation, especially at long distances. Therefore, RGB images are still necessary. They perform better in semantics than depth maps and can complement each other, which is also the advantage of depth-assisted methods. From this perspective, appearance and location features should be different and related. Compared with DRF-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> from the perspective of general features, we consider the modality of input and use ADIS for further learning of depth maps.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.4" class="ltx_p">Specifically, we divide the detection head into two streams: the appearance head and the localization head. The feature before the final output <math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="I_{A}" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><msub id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml"><mi id="S3.SS4.p2.1.m1.1.1.2" xref="S3.SS4.p2.1.m1.1.1.2.cmml">I</mi><mi id="S3.SS4.p2.1.m1.1.1.3" xref="S3.SS4.p2.1.m1.1.1.3.cmml">A</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><apply id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.1.m1.1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p2.1.m1.1.1.2.cmml" xref="S3.SS4.p2.1.m1.1.1.2">ğ¼</ci><ci id="S3.SS4.p2.1.m1.1.1.3.cmml" xref="S3.SS4.p2.1.m1.1.1.3">ğ´</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">I_{A}</annotation></semantics></math> represents the appearance information, while <math id="S3.SS4.p2.2.m2.1" class="ltx_Math" alttext="I_{L}" display="inline"><semantics id="S3.SS4.p2.2.m2.1a"><msub id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml"><mi id="S3.SS4.p2.2.m2.1.1.2" xref="S3.SS4.p2.2.m2.1.1.2.cmml">I</mi><mi id="S3.SS4.p2.2.m2.1.1.3" xref="S3.SS4.p2.2.m2.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><apply id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.2.m2.1.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p2.2.m2.1.1.2.cmml" xref="S3.SS4.p2.2.m2.1.1.2">ğ¼</ci><ci id="S3.SS4.p2.2.m2.1.1.3.cmml" xref="S3.SS4.p2.2.m2.1.1.3">ğ¿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">I_{L}</annotation></semantics></math> represents the location information, each generated in the previous stage, <math id="S3.SS4.p2.3.m3.1" class="ltx_Math" alttext="I_{A}=F_{I}+F_{D}" display="inline"><semantics id="S3.SS4.p2.3.m3.1a"><mrow id="S3.SS4.p2.3.m3.1.1" xref="S3.SS4.p2.3.m3.1.1.cmml"><msub id="S3.SS4.p2.3.m3.1.1.2" xref="S3.SS4.p2.3.m3.1.1.2.cmml"><mi id="S3.SS4.p2.3.m3.1.1.2.2" xref="S3.SS4.p2.3.m3.1.1.2.2.cmml">I</mi><mi id="S3.SS4.p2.3.m3.1.1.2.3" xref="S3.SS4.p2.3.m3.1.1.2.3.cmml">A</mi></msub><mo id="S3.SS4.p2.3.m3.1.1.1" xref="S3.SS4.p2.3.m3.1.1.1.cmml">=</mo><mrow id="S3.SS4.p2.3.m3.1.1.3" xref="S3.SS4.p2.3.m3.1.1.3.cmml"><msub id="S3.SS4.p2.3.m3.1.1.3.2" xref="S3.SS4.p2.3.m3.1.1.3.2.cmml"><mi id="S3.SS4.p2.3.m3.1.1.3.2.2" xref="S3.SS4.p2.3.m3.1.1.3.2.2.cmml">F</mi><mi id="S3.SS4.p2.3.m3.1.1.3.2.3" xref="S3.SS4.p2.3.m3.1.1.3.2.3.cmml">I</mi></msub><mo id="S3.SS4.p2.3.m3.1.1.3.1" xref="S3.SS4.p2.3.m3.1.1.3.1.cmml">+</mo><msub id="S3.SS4.p2.3.m3.1.1.3.3" xref="S3.SS4.p2.3.m3.1.1.3.3.cmml"><mi id="S3.SS4.p2.3.m3.1.1.3.3.2" xref="S3.SS4.p2.3.m3.1.1.3.3.2.cmml">F</mi><mi id="S3.SS4.p2.3.m3.1.1.3.3.3" xref="S3.SS4.p2.3.m3.1.1.3.3.3.cmml">D</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.3.m3.1b"><apply id="S3.SS4.p2.3.m3.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1"><eq id="S3.SS4.p2.3.m3.1.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1.1"></eq><apply id="S3.SS4.p2.3.m3.1.1.2.cmml" xref="S3.SS4.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p2.3.m3.1.1.2.1.cmml" xref="S3.SS4.p2.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS4.p2.3.m3.1.1.2.2.cmml" xref="S3.SS4.p2.3.m3.1.1.2.2">ğ¼</ci><ci id="S3.SS4.p2.3.m3.1.1.2.3.cmml" xref="S3.SS4.p2.3.m3.1.1.2.3">ğ´</ci></apply><apply id="S3.SS4.p2.3.m3.1.1.3.cmml" xref="S3.SS4.p2.3.m3.1.1.3"><plus id="S3.SS4.p2.3.m3.1.1.3.1.cmml" xref="S3.SS4.p2.3.m3.1.1.3.1"></plus><apply id="S3.SS4.p2.3.m3.1.1.3.2.cmml" xref="S3.SS4.p2.3.m3.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS4.p2.3.m3.1.1.3.2.1.cmml" xref="S3.SS4.p2.3.m3.1.1.3.2">subscript</csymbol><ci id="S3.SS4.p2.3.m3.1.1.3.2.2.cmml" xref="S3.SS4.p2.3.m3.1.1.3.2.2">ğ¹</ci><ci id="S3.SS4.p2.3.m3.1.1.3.2.3.cmml" xref="S3.SS4.p2.3.m3.1.1.3.2.3">ğ¼</ci></apply><apply id="S3.SS4.p2.3.m3.1.1.3.3.cmml" xref="S3.SS4.p2.3.m3.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS4.p2.3.m3.1.1.3.3.1.cmml" xref="S3.SS4.p2.3.m3.1.1.3.3">subscript</csymbol><ci id="S3.SS4.p2.3.m3.1.1.3.3.2.cmml" xref="S3.SS4.p2.3.m3.1.1.3.3.2">ğ¹</ci><ci id="S3.SS4.p2.3.m3.1.1.3.3.3.cmml" xref="S3.SS4.p2.3.m3.1.1.3.3.3">ğ·</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.3.m3.1c">I_{A}=F_{I}+F_{D}</annotation></semantics></math>, and <math id="S3.SS4.p2.4.m4.1" class="ltx_Math" alttext="I_{L}=F_{D}+F_{SD}+F_{U}" display="inline"><semantics id="S3.SS4.p2.4.m4.1a"><mrow id="S3.SS4.p2.4.m4.1.1" xref="S3.SS4.p2.4.m4.1.1.cmml"><msub id="S3.SS4.p2.4.m4.1.1.2" xref="S3.SS4.p2.4.m4.1.1.2.cmml"><mi id="S3.SS4.p2.4.m4.1.1.2.2" xref="S3.SS4.p2.4.m4.1.1.2.2.cmml">I</mi><mi id="S3.SS4.p2.4.m4.1.1.2.3" xref="S3.SS4.p2.4.m4.1.1.2.3.cmml">L</mi></msub><mo id="S3.SS4.p2.4.m4.1.1.1" xref="S3.SS4.p2.4.m4.1.1.1.cmml">=</mo><mrow id="S3.SS4.p2.4.m4.1.1.3" xref="S3.SS4.p2.4.m4.1.1.3.cmml"><msub id="S3.SS4.p2.4.m4.1.1.3.2" xref="S3.SS4.p2.4.m4.1.1.3.2.cmml"><mi id="S3.SS4.p2.4.m4.1.1.3.2.2" xref="S3.SS4.p2.4.m4.1.1.3.2.2.cmml">F</mi><mi id="S3.SS4.p2.4.m4.1.1.3.2.3" xref="S3.SS4.p2.4.m4.1.1.3.2.3.cmml">D</mi></msub><mo id="S3.SS4.p2.4.m4.1.1.3.1" xref="S3.SS4.p2.4.m4.1.1.3.1.cmml">+</mo><msub id="S3.SS4.p2.4.m4.1.1.3.3" xref="S3.SS4.p2.4.m4.1.1.3.3.cmml"><mi id="S3.SS4.p2.4.m4.1.1.3.3.2" xref="S3.SS4.p2.4.m4.1.1.3.3.2.cmml">F</mi><mrow id="S3.SS4.p2.4.m4.1.1.3.3.3" xref="S3.SS4.p2.4.m4.1.1.3.3.3.cmml"><mi id="S3.SS4.p2.4.m4.1.1.3.3.3.2" xref="S3.SS4.p2.4.m4.1.1.3.3.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p2.4.m4.1.1.3.3.3.1" xref="S3.SS4.p2.4.m4.1.1.3.3.3.1.cmml">â€‹</mo><mi id="S3.SS4.p2.4.m4.1.1.3.3.3.3" xref="S3.SS4.p2.4.m4.1.1.3.3.3.3.cmml">D</mi></mrow></msub><mo id="S3.SS4.p2.4.m4.1.1.3.1a" xref="S3.SS4.p2.4.m4.1.1.3.1.cmml">+</mo><msub id="S3.SS4.p2.4.m4.1.1.3.4" xref="S3.SS4.p2.4.m4.1.1.3.4.cmml"><mi id="S3.SS4.p2.4.m4.1.1.3.4.2" xref="S3.SS4.p2.4.m4.1.1.3.4.2.cmml">F</mi><mi id="S3.SS4.p2.4.m4.1.1.3.4.3" xref="S3.SS4.p2.4.m4.1.1.3.4.3.cmml">U</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.4.m4.1b"><apply id="S3.SS4.p2.4.m4.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1"><eq id="S3.SS4.p2.4.m4.1.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1.1"></eq><apply id="S3.SS4.p2.4.m4.1.1.2.cmml" xref="S3.SS4.p2.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p2.4.m4.1.1.2.1.cmml" xref="S3.SS4.p2.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS4.p2.4.m4.1.1.2.2.cmml" xref="S3.SS4.p2.4.m4.1.1.2.2">ğ¼</ci><ci id="S3.SS4.p2.4.m4.1.1.2.3.cmml" xref="S3.SS4.p2.4.m4.1.1.2.3">ğ¿</ci></apply><apply id="S3.SS4.p2.4.m4.1.1.3.cmml" xref="S3.SS4.p2.4.m4.1.1.3"><plus id="S3.SS4.p2.4.m4.1.1.3.1.cmml" xref="S3.SS4.p2.4.m4.1.1.3.1"></plus><apply id="S3.SS4.p2.4.m4.1.1.3.2.cmml" xref="S3.SS4.p2.4.m4.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS4.p2.4.m4.1.1.3.2.1.cmml" xref="S3.SS4.p2.4.m4.1.1.3.2">subscript</csymbol><ci id="S3.SS4.p2.4.m4.1.1.3.2.2.cmml" xref="S3.SS4.p2.4.m4.1.1.3.2.2">ğ¹</ci><ci id="S3.SS4.p2.4.m4.1.1.3.2.3.cmml" xref="S3.SS4.p2.4.m4.1.1.3.2.3">ğ·</ci></apply><apply id="S3.SS4.p2.4.m4.1.1.3.3.cmml" xref="S3.SS4.p2.4.m4.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS4.p2.4.m4.1.1.3.3.1.cmml" xref="S3.SS4.p2.4.m4.1.1.3.3">subscript</csymbol><ci id="S3.SS4.p2.4.m4.1.1.3.3.2.cmml" xref="S3.SS4.p2.4.m4.1.1.3.3.2">ğ¹</ci><apply id="S3.SS4.p2.4.m4.1.1.3.3.3.cmml" xref="S3.SS4.p2.4.m4.1.1.3.3.3"><times id="S3.SS4.p2.4.m4.1.1.3.3.3.1.cmml" xref="S3.SS4.p2.4.m4.1.1.3.3.3.1"></times><ci id="S3.SS4.p2.4.m4.1.1.3.3.3.2.cmml" xref="S3.SS4.p2.4.m4.1.1.3.3.3.2">ğ‘†</ci><ci id="S3.SS4.p2.4.m4.1.1.3.3.3.3.cmml" xref="S3.SS4.p2.4.m4.1.1.3.3.3.3">ğ·</ci></apply></apply><apply id="S3.SS4.p2.4.m4.1.1.3.4.cmml" xref="S3.SS4.p2.4.m4.1.1.3.4"><csymbol cd="ambiguous" id="S3.SS4.p2.4.m4.1.1.3.4.1.cmml" xref="S3.SS4.p2.4.m4.1.1.3.4">subscript</csymbol><ci id="S3.SS4.p2.4.m4.1.1.3.4.2.cmml" xref="S3.SS4.p2.4.m4.1.1.3.4.2">ğ¹</ci><ci id="S3.SS4.p2.4.m4.1.1.3.4.3.cmml" xref="S3.SS4.p2.4.m4.1.1.3.4.3">ğ‘ˆ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.4.m4.1c">I_{L}=F_{D}+F_{SD}+F_{U}</annotation></semantics></math>. In the stream of appearance, we still maintain the depth features. By robustly combining the extracted RGB and depth features, our algorithm does not rely too much on the depth map that may be biased, thus improving the overall performance of our model.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Dataset</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The KITTI dataset is a widely recognized benchmark for evaluating 3D object detection algorithms, comprising 7481 training samples and 7518 testing samples, with both 3D point cloud and color images provided. In our approach, we rely solely on color images. The evaluation is conducted on three categories of objects, namely Car, Pedestrian, and Cyclist, using the average precision (AP) metric, which is a commonly used evaluation metric for 3D and Birdâ€™s-Eye-View(BEV) object detection, calculated using 40 recall positions. As per the official KITTI evaluation protocol, objects are classified into three difficulty levels: easy, moderate, and hard based on the object size, occlusion state, and truncation level. The algorithmsâ€™ final performance is ranked based on the moderately difficult results. In this paper, we use the Average Precision (AP) as the evaluation metric, with an Intersection over Union (IoU) threshold of 0.7. Under this standard, the definition of AP is as follows: for each class, we calculate a set of confidence scores and corresponding ground-truth labels, and sort them in descending order of confidence scores. Then, for each confidence threshold, we compute the precision and recall, and the corresponding AP value. Finally, we take the average AP over all classes to obtain the final AP value. This metric can be used to evaluate the accuracy and robustness of object detection models.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Implementation details</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.2" class="ltx_p">We implemented our proposed method using PyTorch 1.2.0. Ablation study experiments were conducted on a GTX1080Ti GPU with 11 GB memory, with a batch size of 2 and a learning rate of 0.004 for 80000 iterations. For the test set, we trained with a batch size of 4, a learning rate of 0.008 for 120000 iterations, and a Tesla V100 with 32 GB. We used DenseNet-121 as the backbone for extracting both the image and depth map. Since the backbone is trained on ImageNet, we normalized the input using means of (0.485, 0.456, 0.406) and stds of (0.229, 0.224, 0.225), and the optimizer was SGD. We set the number of intervals <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="n_{d}" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><msub id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">n</mi><mi id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">ğ‘›</ci><ci id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">n_{d}</annotation></semantics></math> to 8. Since <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="n_{d}" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><msub id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mi id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">n</mi><mi id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">ğ‘›</ci><ci id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">n_{d}</annotation></semantics></math> is generated using full connection layers, the size of the feature map must be strictly controlled, hence we padded every input image to size (1760, 512) at the beginning of both the training and testing steps.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>
Comparison of the Use of Different Depth Maps for Category â€œCarâ€ on the KITTI Validation Set with AP 3D</figcaption>
<div id="S4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:202.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(43.6pt,-20.4pt) scale(1.25162698135851,1.25162698135851) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="2"><span id="S4.T2.1.1.1.1.1.1" class="ltx_text">Experiments</span></td>
<td id="S4.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="2"><span id="S4.T2.1.1.1.1.2.1" class="ltx_text">Depth map</span></td>
<td id="S4.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" colspan="3">AP 3D</td>
</tr>
<tr id="S4.T2.1.1.2.2" class="ltx_tr">
<td id="S4.T2.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Easy</td>
<td id="S4.T2.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Mod.</td>
<td id="S4.T2.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Hard</td>
</tr>
<tr id="S4.T2.1.1.3.3" class="ltx_tr">
<td id="S4.T2.1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">M3D-RPN(base line) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</td>
<td id="S4.T2.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S4.T2.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">14.53</td>
<td id="S4.T2.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">11.07</td>
<td id="S4.T2.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">8.65</td>
</tr>
<tr id="S4.T2.1.1.4.4" class="ltx_tr">
<td id="S4.T2.1.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">D4LCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</td>
<td id="S4.T2.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">DORN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>
</td>
<td id="S4.T2.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">22.32</td>
<td id="S4.T2.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">16.20</td>
<td id="S4.T2.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">12.30</td>
</tr>
<tr id="S4.T2.1.1.5.5" class="ltx_tr">
<td id="S4.T2.1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">ADISN(Ours)</td>
<td id="S4.T2.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">DORN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>
</td>
<td id="S4.T2.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.5.5.3.1" class="ltx_text ltx_font_bold">26.86</span></td>
<td id="S4.T2.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.5.5.4.1" class="ltx_text ltx_font_bold">17.28</span></td>
<td id="S4.T2.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.5.5.5.1" class="ltx_text ltx_font_bold">13.15</span></td>
</tr>
<tr id="S4.T2.1.1.6.6" class="ltx_tr">
<td id="S4.T2.1.1.6.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">Improvement over <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</td>
<td id="S4.T2.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S4.T2.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">+4.54</td>
<td id="S4.T2.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">+1.08</td>
<td id="S4.T2.1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">+0.85</td>
</tr>
<tr id="S4.T2.1.1.7.7" class="ltx_tr">
<td id="S4.T2.1.1.7.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">D4LCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</td>
<td id="S4.T2.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">PSMNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</td>
<td id="S4.T2.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">25.24</td>
<td id="S4.T2.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">19.80</td>
<td id="S4.T2.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">16.45</td>
</tr>
<tr id="S4.T2.1.1.8.8" class="ltx_tr">
<td id="S4.T2.1.1.8.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">ADISN(Ours)</td>
<td id="S4.T2.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">PSMNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</td>
<td id="S4.T2.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.8.8.3.1" class="ltx_text ltx_font_bold">36.02</span></td>
<td id="S4.T2.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.8.8.4.1" class="ltx_text ltx_font_bold">23.25</span></td>
<td id="S4.T2.1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.8.8.5.1" class="ltx_text ltx_font_bold">18.19</span></td>
</tr>
<tr id="S4.T2.1.1.9.9" class="ltx_tr">
<td id="S4.T2.1.1.9.9.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">Improvement over <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</td>
<td id="S4.T2.1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S4.T2.1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">+10.78</td>
<td id="S4.T2.1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">+3.45</td>
<td id="S4.T2.1.1.9.9.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">+1.74</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Comparison</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Table <a href="#S3.T1" title="TABLE I â€£ III-C Uncertainty map â€£ III Methodology â€£ Understanding Depth Map Progressively: Adaptive Distance Interval Separation for Monocular 3d Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> displays the performance comparison of the KITTI test set in the category of cars based on AP 3D and AP BEV, compared under the IoU=0.7 standard. Our proposed method achieves superior performance among monocular 3D object detection methods, ranking 1<sup id="S4.SS3.p1.1.1" class="ltx_sup"><span id="S4.SS3.p1.1.1.1" class="ltx_text ltx_font_italic">st</span></sup> in the Easy, Moderate, and Hard categories with AP 3D. We observe significant improvement in the 3D detection task compared to other advanced methods, with an increase of 3.11 on Easy, 2.43 on Moderate, and 3.00 on Hard when compared to the baseline. Our method is based on M3D-RPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, and despite the improved M3D-RPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> named â€œ3D-Netâ€ being used as the baseline by D4LCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, our method retains its advantages. This indicates that the depth map has a significant improvement in the monocular 3D object detection task, and that older methods such as the baseline are still competitive in recent methods. Under the AP BEV index, we rank second in Easy and Moderate and first in Hard. Recently, monocular methods no longer compare results on the validation set, as performance often drops significantly on the test set. Therefore, comparing results on the test set is the most objective method. Overall, our comprehensive experimental results confirm that our method significantly improves the monocular 3D object detector.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Comparison of Using Different <math id="S4.T3.2.m1.1" class="ltx_Math" alttext="n_{d}" display="inline"><semantics id="S4.T3.2.m1.1b"><msub id="S4.T3.2.m1.1.1" xref="S4.T3.2.m1.1.1.cmml"><mi id="S4.T3.2.m1.1.1.2" xref="S4.T3.2.m1.1.1.2.cmml">n</mi><mi id="S4.T3.2.m1.1.1.3" xref="S4.T3.2.m1.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T3.2.m1.1c"><apply id="S4.T3.2.m1.1.1.cmml" xref="S4.T3.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.2.m1.1.1.1.cmml" xref="S4.T3.2.m1.1.1">subscript</csymbol><ci id="S4.T3.2.m1.1.1.2.cmml" xref="S4.T3.2.m1.1.1.2">ğ‘›</ci><ci id="S4.T3.2.m1.1.1.3.cmml" xref="S4.T3.2.m1.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.m1.1d">n_{d}</annotation></semantics></math> for Category â€œCarâ€ on the KITTI Validation Set</figcaption>
<div id="S4.T3.3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:260.2pt;height:183.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(40.9pt,-28.9pt) scale(1.45869021791204,1.45869021791204) ;">
<table id="S4.T3.3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.3.1.1.1" class="ltx_tr">
<th id="S4.T3.3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="2"><span id="S4.T3.3.1.1.1.1.1" class="ltx_text">Settings of <math id="S4.T3.3.1.1.1.1.1.m1.1" class="ltx_Math" alttext="n_{d}" display="inline"><semantics id="S4.T3.3.1.1.1.1.1.m1.1a"><msub id="S4.T3.3.1.1.1.1.1.m1.1.1" xref="S4.T3.3.1.1.1.1.1.m1.1.1.cmml"><mi id="S4.T3.3.1.1.1.1.1.m1.1.1.2" xref="S4.T3.3.1.1.1.1.1.m1.1.1.2.cmml">n</mi><mi id="S4.T3.3.1.1.1.1.1.m1.1.1.3" xref="S4.T3.3.1.1.1.1.1.m1.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T3.3.1.1.1.1.1.m1.1b"><apply id="S4.T3.3.1.1.1.1.1.m1.1.1.cmml" xref="S4.T3.3.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.3.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T3.3.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T3.3.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T3.3.1.1.1.1.1.m1.1.1.2">ğ‘›</ci><ci id="S4.T3.3.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T3.3.1.1.1.1.1.m1.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.1.1.1.1.1.m1.1c">n_{d}</annotation></semantics></math></span></th>
<th id="S4.T3.3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" colspan="3">AP 3D</th>
</tr>
<tr id="S4.T3.3.1.1.2.1" class="ltx_tr">
<th id="S4.T3.3.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Easy</th>
<th id="S4.T3.3.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Mod.</th>
<th id="S4.T3.3.1.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Hard</th>
</tr>
<tr id="S4.T3.3.1.1.3.2" class="ltx_tr">
<th id="S4.T3.3.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">0</th>
<th id="S4.T3.3.1.1.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">22.93</th>
<th id="S4.T3.3.1.1.3.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">14.81</th>
<th id="S4.T3.3.1.1.3.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">11.58</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.3.1.1.4.1" class="ltx_tr">
<th id="S4.T3.3.1.1.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">4</th>
<td id="S4.T3.3.1.1.4.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">23.64</td>
<td id="S4.T3.3.1.1.4.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">16.21</td>
<td id="S4.T3.3.1.1.4.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">11.47</td>
</tr>
<tr id="S4.T3.3.1.1.5.2" class="ltx_tr">
<th id="S4.T3.3.1.1.5.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">8</th>
<td id="S4.T3.3.1.1.5.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">26.86</td>
<td id="S4.T3.3.1.1.5.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T3.3.1.1.5.2.3.1" class="ltx_text ltx_font_bold">17.28</span></td>
<td id="S4.T3.3.1.1.5.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T3.3.1.1.5.2.4.1" class="ltx_text ltx_font_bold">13.15</span></td>
</tr>
<tr id="S4.T3.3.1.1.6.3" class="ltx_tr">
<th id="S4.T3.3.1.1.6.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">16</th>
<td id="S4.T3.3.1.1.6.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T3.3.1.1.6.3.2.1" class="ltx_text ltx_font_bold">27.92</span></td>
<td id="S4.T3.3.1.1.6.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">17.14</td>
<td id="S4.T3.3.1.1.6.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">13.15</td>
</tr>
<tr id="S4.T3.3.1.1.7.4" class="ltx_tr">
<th id="S4.T3.3.1.1.7.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">32</th>
<td id="S4.T3.3.1.1.7.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">26.65</td>
<td id="S4.T3.3.1.1.7.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">16.68</td>
<td id="S4.T3.3.1.1.7.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">12.64</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>
Comparison of Various Modules in the Proposed Method for the Category â€œCarâ€ on the KITTI Validation Set</figcaption>
<div id="S4.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:137pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-61.2pt,21.5pt) scale(0.761348640894637,0.761348640894637) ;">
<table id="S4.T4.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.1.1.1.1" class="ltx_tr">
<td id="S4.T4.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="2"><span id="S4.T4.1.1.1.1.1.1" class="ltx_text">Experiments</span></td>
<td id="S4.T4.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" colspan="4">Our proposed operations</td>
<td id="S4.T4.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" colspan="3">AP BEV</td>
</tr>
<tr id="S4.T4.1.1.2.2" class="ltx_tr">
<td id="S4.T4.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Separated head</td>
<td id="S4.T4.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">ADIS(8)</td>
<td id="S4.T4.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Un</td>
<td id="S4.T4.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Feature decoupling</td>
<td id="S4.T4.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Easy</td>
<td id="S4.T4.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Mod.</td>
<td id="S4.T4.1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Hard</td>
</tr>
<tr id="S4.T4.1.1.3.3" class="ltx_tr">
<td id="S4.T4.1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">M3D-RPN(image only)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</td>
<td id="S4.T4.1.1.3.3.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"></td>
<td id="S4.T4.1.1.3.3.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"></td>
<td id="S4.T4.1.1.3.3.4" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"></td>
<td id="S4.T4.1.1.3.3.5" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"></td>
<td id="S4.T4.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">14.43</td>
<td id="S4.T4.1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">10.94</td>
<td id="S4.T4.1.1.3.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">8.57</td>
</tr>
<tr id="S4.T4.1.1.4.4" class="ltx_tr">
<td id="S4.T4.1.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">M3D-RPN(depth only)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</td>
<td id="S4.T4.1.1.4.4.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"></td>
<td id="S4.T4.1.1.4.4.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"></td>
<td id="S4.T4.1.1.4.4.4" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"></td>
<td id="S4.T4.1.1.4.4.5" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"></td>
<td id="S4.T4.1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">19.75</td>
<td id="S4.T4.1.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">13.08</td>
<td id="S4.T4.1.1.4.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">9.46</td>
</tr>
<tr id="S4.T4.1.1.5.5" class="ltx_tr">
<td id="S4.T4.1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">M3D-RPN(image+depth)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</td>
<td id="S4.T4.1.1.5.5.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"></td>
<td id="S4.T4.1.1.5.5.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"></td>
<td id="S4.T4.1.1.5.5.4" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"></td>
<td id="S4.T4.1.1.5.5.5" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"></td>
<td id="S4.T4.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">20.90</td>
<td id="S4.T4.1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">14.33</td>
<td id="S4.T4.1.1.5.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">9.93</td>
</tr>
<tr id="S4.T4.1.1.6.6" class="ltx_tr">
<td id="S4.T4.1.1.6.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="5"><span id="S4.T4.1.1.6.6.1.1" class="ltx_text">ours</span></td>
<td id="S4.T4.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">âœ”</td>
<td id="S4.T4.1.1.6.6.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"></td>
<td id="S4.T4.1.1.6.6.4" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"></td>
<td id="S4.T4.1.1.6.6.5" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"></td>
<td id="S4.T4.1.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">22.93</td>
<td id="S4.T4.1.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">14.81</td>
<td id="S4.T4.1.1.6.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">11.58</td>
</tr>
<tr id="S4.T4.1.1.7.7" class="ltx_tr">
<td id="S4.T4.1.1.7.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">âœ”</td>
<td id="S4.T4.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">âœ”</td>
<td id="S4.T4.1.1.7.7.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"></td>
<td id="S4.T4.1.1.7.7.4" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"></td>
<td id="S4.T4.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">23.09</td>
<td id="S4.T4.1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">15.77</td>
<td id="S4.T4.1.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">12.15</td>
</tr>
<tr id="S4.T4.1.1.8.8" class="ltx_tr">
<td id="S4.T4.1.1.8.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">âœ”</td>
<td id="S4.T4.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">âœ”</td>
<td id="S4.T4.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">âœ”</td>
<td id="S4.T4.1.1.8.8.4" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"></td>
<td id="S4.T4.1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">23.84</td>
<td id="S4.T4.1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">16.12</td>
<td id="S4.T4.1.1.8.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">12.64</td>
</tr>
<tr id="S4.T4.1.1.9.9" class="ltx_tr">
<td id="S4.T4.1.1.9.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">âœ”</td>
<td id="S4.T4.1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">âœ”</td>
<td id="S4.T4.1.1.9.9.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"></td>
<td id="S4.T4.1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">âœ”</td>
<td id="S4.T4.1.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">25.03</td>
<td id="S4.T4.1.1.9.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">16.63</td>
<td id="S4.T4.1.1.9.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">12.43</td>
</tr>
<tr id="S4.T4.1.1.10.10" class="ltx_tr">
<td id="S4.T4.1.1.10.10.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">âœ”</td>
<td id="S4.T4.1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">âœ”</td>
<td id="S4.T4.1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">âœ”</td>
<td id="S4.T4.1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">âœ”</td>
<td id="S4.T4.1.1.10.10.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T4.1.1.10.10.5.1" class="ltx_text ltx_font_bold">26.86</span></td>
<td id="S4.T4.1.1.10.10.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T4.1.1.10.10.6.1" class="ltx_text ltx_font_bold">17.28</span></td>
<td id="S4.T4.1.1.10.10.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T4.1.1.10.10.7.1" class="ltx_text ltx_font_bold">13.15</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.4.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.5.2" class="ltx_text ltx_font_italic">Ablation Study</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We compared our proposed method with M3D-RPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and D4LCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> on the KITTI validation set using different depth estimation methods (see Table <a href="#S4.T2" title="TABLE II â€£ IV-B Implementation details â€£ IV Experiments â€£ Understanding Depth Map Progressively: Adaptive Distance Interval Separation for Monocular 3d Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>). Our method showed advantages with different depth maps, with greater advantages when the depth map accuracy was higher. This suggests that our methodâ€™s approach to understanding depth maps is more effective, as a more accurate depth estimation is closer to point clouds and more intuitive.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.5" class="ltx_p">We conducted a simple experiment to determine the number of intervals (<math id="S4.SS4.p2.1.m1.1" class="ltx_Math" alttext="n_{d}" display="inline"><semantics id="S4.SS4.p2.1.m1.1a"><msub id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml"><mi id="S4.SS4.p2.1.m1.1.1.2" xref="S4.SS4.p2.1.m1.1.1.2.cmml">n</mi><mi id="S4.SS4.p2.1.m1.1.1.3" xref="S4.SS4.p2.1.m1.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><apply id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.1.m1.1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS4.p2.1.m1.1.1.2.cmml" xref="S4.SS4.p2.1.m1.1.1.2">ğ‘›</ci><ci id="S4.SS4.p2.1.m1.1.1.3.cmml" xref="S4.SS4.p2.1.m1.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">n_{d}</annotation></semantics></math>) according to the final model, as shown in Table <a href="#S4.T3" title="TABLE III â€£ IV-C Comparison â€£ IV Experiments â€£ Understanding Depth Map Progressively: Adaptive Distance Interval Separation for Monocular 3d Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. Increasing <math id="S4.SS4.p2.2.m2.1" class="ltx_Math" alttext="n_{d}" display="inline"><semantics id="S4.SS4.p2.2.m2.1a"><msub id="S4.SS4.p2.2.m2.1.1" xref="S4.SS4.p2.2.m2.1.1.cmml"><mi id="S4.SS4.p2.2.m2.1.1.2" xref="S4.SS4.p2.2.m2.1.1.2.cmml">n</mi><mi id="S4.SS4.p2.2.m2.1.1.3" xref="S4.SS4.p2.2.m2.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.2.m2.1b"><apply id="S4.SS4.p2.2.m2.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.2.m2.1.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS4.p2.2.m2.1.1.2.cmml" xref="S4.SS4.p2.2.m2.1.1.2">ğ‘›</ci><ci id="S4.SS4.p2.2.m2.1.1.3.cmml" xref="S4.SS4.p2.2.m2.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.2.m2.1c">n_{d}</annotation></semantics></math> improves the effect, but also incurs greater computational costs. When <math id="S4.SS4.p2.3.m3.1" class="ltx_Math" alttext="n_{d}" display="inline"><semantics id="S4.SS4.p2.3.m3.1a"><msub id="S4.SS4.p2.3.m3.1.1" xref="S4.SS4.p2.3.m3.1.1.cmml"><mi id="S4.SS4.p2.3.m3.1.1.2" xref="S4.SS4.p2.3.m3.1.1.2.cmml">n</mi><mi id="S4.SS4.p2.3.m3.1.1.3" xref="S4.SS4.p2.3.m3.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.3.m3.1b"><apply id="S4.SS4.p2.3.m3.1.1.cmml" xref="S4.SS4.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.3.m3.1.1.1.cmml" xref="S4.SS4.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS4.p2.3.m3.1.1.2.cmml" xref="S4.SS4.p2.3.m3.1.1.2">ğ‘›</ci><ci id="S4.SS4.p2.3.m3.1.1.3.cmml" xref="S4.SS4.p2.3.m3.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.3.m3.1c">n_{d}</annotation></semantics></math> was set to 16, performance on Hard no longer increased, as hard objects are difficult to estimate accurately in the depth estimation task. Setting <math id="S4.SS4.p2.4.m4.1" class="ltx_Math" alttext="n_{d}" display="inline"><semantics id="S4.SS4.p2.4.m4.1a"><msub id="S4.SS4.p2.4.m4.1.1" xref="S4.SS4.p2.4.m4.1.1.cmml"><mi id="S4.SS4.p2.4.m4.1.1.2" xref="S4.SS4.p2.4.m4.1.1.2.cmml">n</mi><mi id="S4.SS4.p2.4.m4.1.1.3" xref="S4.SS4.p2.4.m4.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.4.m4.1b"><apply id="S4.SS4.p2.4.m4.1.1.cmml" xref="S4.SS4.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.4.m4.1.1.1.cmml" xref="S4.SS4.p2.4.m4.1.1">subscript</csymbol><ci id="S4.SS4.p2.4.m4.1.1.2.cmml" xref="S4.SS4.p2.4.m4.1.1.2">ğ‘›</ci><ci id="S4.SS4.p2.4.m4.1.1.3.cmml" xref="S4.SS4.p2.4.m4.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.4.m4.1c">n_{d}</annotation></semantics></math> to greater than 32 may lead to overfitting and degrade performance. After comprehensive consideration, we set <math id="S4.SS4.p2.5.m5.1" class="ltx_Math" alttext="n_{d}" display="inline"><semantics id="S4.SS4.p2.5.m5.1a"><msub id="S4.SS4.p2.5.m5.1.1" xref="S4.SS4.p2.5.m5.1.1.cmml"><mi id="S4.SS4.p2.5.m5.1.1.2" xref="S4.SS4.p2.5.m5.1.1.2.cmml">n</mi><mi id="S4.SS4.p2.5.m5.1.1.3" xref="S4.SS4.p2.5.m5.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.5.m5.1b"><apply id="S4.SS4.p2.5.m5.1.1.cmml" xref="S4.SS4.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.5.m5.1.1.1.cmml" xref="S4.SS4.p2.5.m5.1.1">subscript</csymbol><ci id="S4.SS4.p2.5.m5.1.1.2.cmml" xref="S4.SS4.p2.5.m5.1.1.2">ğ‘›</ci><ci id="S4.SS4.p2.5.m5.1.1.3.cmml" xref="S4.SS4.p2.5.m5.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.5.m5.1c">n_{d}</annotation></semantics></math> to 8.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">Table <a href="#S4.T4" title="TABLE IV â€£ IV-C Comparison â€£ IV Experiments â€£ Understanding Depth Map Progressively: Adaptive Distance Interval Separation for Monocular 3d Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> shows the ablation experiment of our proposed method. We disassembled each module to verify their effectiveness. The performance of M3D-RPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> as a baseline was lower than the original paperâ€™s report due to device issues. Notably, even using a depth map directly as the input of M3D-RPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> yielded better performance than the original RGB image, indicating the crucial role of depth maps in extracting position information for monocular 3D object detection. A simple fusion of image and depth map features yielded performance improvements, likely due to the complementarity of depth maps and images. We separately set the detector heads of different tasks according to feature characteristics to improve the results. The ADIS module significantly improved performance by further extracting depth features. Adding an uncertainty map also improved overall performance. Furthermore, our feature decoupling module further improved performance.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p">Fig <a href="#S4.F5" title="Figure 5 â€£ IV-D Ablation Study â€£ IV Experiments â€£ Understanding Depth Map Progressively: Adaptive Distance Interval Separation for Monocular 3d Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows visualizations of the effectiveness of our proposed method. Our method outperformed M3D-RPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> for distant or occluded objects and improved many missed detection situations.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2306.10921/assets/fig5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="491" height="228" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The qualitative comparison of the ground truth (green), the baseline (blue), and our proposed method (red) on the KITTI validation set. Our proposed method predicts accurate 3D bounding boxes of occluded and distant objects.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we present a novel approach to understanding depth map representation by proposing ADISN. Our sub-depth map establishes a relationship between 3D and 2D expressions, providing insights into the physical meaning of depth maps and highlighting the advantages of CNNs over converting depth maps to 3D or 2D representations. To address the issue of inaccurate estimated depth maps, we introduce the use of an uncertainty map, which improves the reliability of learning in unclear areas by relying more on clear images. Additionally, we propose a method where appearance and localization heads learn from different branches, allowing images and depth maps to leverage their respective strengths. Extensive experimentation shows that our proposed method outperforms the highly competitive KITTI monocular 3D object detection task.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Future work</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Monocular 3D object detection is always a hot topic in the field of autonomous driving before reliable and cost-effective LiDAR sensors become available. Recent research has mostly focused on improving the performance of monocular 3D object detectors using other modalities, partly because it is relatively easy to adapt existing high-performance algorithms by pruning them into monocular detectors, and partly because it may be inappropriate to rely solely on a single image to reconstruct a 3D scene. Previous work has pre-trained neural networks on depth estimation tasks and fine-tuned them on object detection tasks with color images, implicitly learning depth features, and achieving impressive performance. Modalities or paradigms that are independent of RGB inputs may possess more knowledge suitable for inferring 3D space, and extracting and refining such knowledge into single images may be a direction for future work.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"> Chen, X., Kundu, K., Zhang, Z., Ma, H., Fidler, S., and Urtasun, R, â€Monocular 3d object detection for autonomous driving,â€ in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2016, pp. 2147-2156.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"> Ku, J., Pon, A. D., and Waslander, S. L., â€Monocular 3d object detection leveraging accurate proposals and shape reconstruction,â€ in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2019, pp. 11867-11876.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"> Chabot, F., Chaouch, M., Rabarisoa, J., Teuliere, C., and Chateau, T, â€Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image,â€ in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2017, pp. 2040-2049.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"> Mousavian, A., Anguelov, D., Flynn, J., and Kosecka, J., â€3d bounding box estimation using deep learning and geometry,â€ in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</em>, 2017, pp. 7074-7082.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"> Liu, L., Lu, J., Xu, C., Tian, Q., and Zhou, J., â€Deep fitting degree scoring network for monocular 3d object detection,â€ in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2019, pp. 1057-1066.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"> Simonelli, A., Bulo, S. R., Porzi, L., LÃ³pez-Antequera, M., and Kontschieder, P., â€Disentangling monocular 3d object detection,â€ in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2019, pp. 1991-1999.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"> Qin, Z., Wang, J., and Lu, Y., â€Monogrnet: A general framework for monocular 3d object detection,â€ <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</em>, vol. 44, no. 9, pp.5170-5184, 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"> Brazil, G., and Liu, X., â€M3d-rpn: Monocular 3d region proposal network for object detection,â€ in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2019, pp. 9287-9296.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"> Liu, Y., Yixuan, Y., and Liu, M., â€Ground-aware monocular 3d object detection for autonomous driving.â€ <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">IEEE Robotics and Automation Letters</em>, vol. 6, no. 2, pp.919-926, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"> Li, P., Zhao, H., Liu, P., and Cao, F., â€Rtm3d: Real-time monocular 3d detection from object keypoints for autonomous driving,â€ in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Computer Vision</em>, 2020, pp. 644-660.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"> Zhou, X., Wang, D., and KrÃ¤henbÃ¼hl, P., â€Objects as points,â€ <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv</em>:1904.07850, 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"> Liu, Z., Wu, Z., and TÃ³th, R., â€Smoke: Single-stage monocular 3d object detection via keypoint estimation,â€ in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</em>, 2020, pp. 996-997.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"> Zhang, Y., Lu, J., and Zhou, J., â€Objects are different: Flexible monocular 3d object detection,â€ in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2021, pp. 3289-3298.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"> Lu, Y., Ma, X., Yang, L., Zhang, T., Liu, Y., Chu, Q., Yan, J. and Ouyang, W, â€Geometry uncertainty projection network for monocular 3d object detection,â€ in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 2021, pp. 3111-3121.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"> Brazil, G., Pons-Moll, G., Liu, X., and Schiele, B., â€Kinematic 3d object detection in monocular video,â€ in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Computer Vision</em>, 2020, pp. 135-152.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"> Reading, C., Harakeh, A., Chae, J., and Waslander, S. L., â€Categorical depth distribution network for monocular 3d object detection,â€ <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2021, pp. 8555-8564.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"> Wang, Y., Chao, W.L., Garg, D., Hariharan, B., Campbell, M. and Weinberger, K.Q., â€Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving,â€ in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2019, pp. 8445-8453.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"> Weng, X., and Kitani, K., â€Monocular 3d object detection with pseudo-lidar point cloud,â€ in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</em>, 2019, pp. 0-0.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"> Ma, X., Liu, S., Xia, Z., Zhang, H., Zeng, X., and Ouyang, W., â€Rethinking pseudo-lidar representation,â€ in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Computer Vision</em>, 2020, pp. 311-327.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"> Wang, L., Zhang, L., Zhu, Y., Zhang, Z., He, T., Li, M. and Xue, X., â€Progressive coordinate transforms for monocular 3d object detection,â€ <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> vol. 34, pp. 13364-13377, 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"> Chu, X., Deng, J., Li, Y., Yuan, Z., Zhang, Y., Ji, J. and Zhang, Y., â€Neighbor-vote: Improving monocular 3d object detection through neighbor distance voting,â€ in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th ACM International Conference on Multimedia</em>, pp. 5239-5247, 2021.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"> Peng, L., Liu, F., Yu, Z., Yan, S., Deng, D., Yang, Z., Liu, H. and Cai, D., â€Lidar point cloud guided monocular 3d object detection,â€ in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Computer Vision</em>, 2022, pp. 123-139.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"> Chong, Z., Ma, X., Zhang, H., Yue, Y., Li, H., Wang, Z. and Ouyang, W., â€Monodistill: Learning spatial features for monocular 3d object detection,â€ <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv</em>:2201.10830, 2022.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"> Zhou, Z., Du, L., Ye, X., Zou, Z., Tan, X., Zhang, L., Xue, X. and Feng, J., â€SGM3D: stereo guided monocular 3d object detection.â€ <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">IEEE Robotics and Automation Letters</em>, vol 7, no. 4, pp. 10478-10485, 2022.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"> Park, D., Ambrus, R., Guizilini, V., Li, J., and Gaidon, A., â€Is pseudo-lidar needed for monocular 3d object detection?â€ in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2021, pp. 3142-3152.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"> Hong, Y., Dai, H., and Ding, Y., â€Cross-Modality Knowledge Distillation Network for Monocular 3D Object Detection,â€ in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Computer Vision</em>, 2022, pp. 87-104.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"> Chen, Y. N., Dai, H., and Ding, Y., â€Pseudo-stereo for monocular 3d object detection in autonomous driving,â€ in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2022, pp. 887-897.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"> Feng, D., Han, S., Xu, H., Liang, X., and Tan, X., in â€Point-Guided Contrastive Learning for Monocular 3-D Object Detection,â€ <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Cybernetics</em>, 2021.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"> Ding, M., Huo, Y., Yi, H., Wang, Z., Shi, J., Lu, Z., and Luo, P., â€Learning Depth-Guided Convolutions for Monocular 3D Object Detection,â€ in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2020, pp. 11672-11681.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"> Wang, L., Du, L., Ye, X., Fu, Y., Guo, G., Xue, X., Feng, J. and Zhang, L., â€Depth-conditioned dynamic message propagation for monocular 3d object detection,â€ in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2021, pp. 454-463.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"> Zou, Z. et al., â€The devil is in the task: Exploiting reciprocal appearance-localization features for monocular 3d object detection,â€ in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2021, pp. 2713-2722.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"> Cai, Y., Li, B., Jiao, Z., Li, H., Zeng, X. and Wang, X., â€Monocular 3d object detection with decoupled structured polygon estimation and height-guided depth estimation,â€ <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 34, no. 7, 2020.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"> Fu, H., Gong, M., Wang, C., Batmanghelich, K., and Tao, D., â€Deep ordinal regression network for monocular depth estimation,â€ in <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2018, pp. 2002-2011.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"> Chang, J. R., and Chen, Y. S., â€Pyramid stereo matching network,â€ in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2018, pp. 5410-5418.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"> Geiger, A., Lenz, P., and Urtasun, R., â€Are we ready for autonomous driving? the kitti vision benchmark suite,â€ in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">2012 IEEE Conference on Computer Vision and Pattern Recognition. IEEE</em>, 2012, pp. 3354â€“3361.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"> Heylen, J. et al., â€Monocinis: Camera independent monocular 3d object detection using instance segmentation,â€ in <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2021, pp. 923-934.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"> Rukhovich, D., Vorontsova, A. and Konushin, A., â€Imvoxelnet: Image to voxels projection for monocular and multi-view general-purpose 3d object detection,â€ in <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, 2022, pp. 2397-2406.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"> Luo, S., Dai, H., Shao, L. and Ding, Y., â€M3dssd: Monocular 3d single stage object detector,â€ in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2021, pp. 6145-6154.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"> Ma, X., Wang, Z., Li, H., Zhang, P., Ouyang, W. and Fan, X., â€Accurate monocular 3d object detection via color-embedded 3d reconstruction for autonomous driving,â€in <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2019, pp. 6851-6860.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"> Zhao, D., Ji, C. and Liu, G.,â€Monocular 3D Object Detection Based on Pseudo Multimodal Information Extraction and Keypoint Estimation,â€ <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em>, vol. 13, no. 3, p.1731, 2023.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"> Ji, C., Liu, G. and Zhao, D., â€Monocular 3D object detection via estimation of paired keypoints for autonomous driving,â€ <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Multimedia Tools and Applications</em>, vol. 81, no. 4, pp.5973-5988, 2022.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"> Ju, B., Zou, Z., Ye, X., Jiang, M., Tan, X., Ding, E., and Wang, J., â€Paint and Distill: Boosting 3D Object Detection with Semantic Passing Network,â€ in <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 30th ACM International Conference on Multimedia</em>, 2022, pp. 5639-5648.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"> Liu, Z. et al, â€Multi-Modal 3D Object Detection by Box Matching,â€ <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv</em>:2305.07713, 2023.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2306.10920" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2306.10921" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2306.10921">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2306.10921" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2306.10922" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 22:28:01 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
