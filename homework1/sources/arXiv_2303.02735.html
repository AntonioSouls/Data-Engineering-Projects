<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2303.02735] Scalable Object Detection on Embedded Devices using Weight Pruning and Singular Value Decomposition</title><meta property="og:description" content="This paper presents a method for optimizing object detection models by combining weight pruning
and singular value decomposition (SVD). The proposed method was evaluated on a custom dataset of
street work images obtain…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Scalable Object Detection on Embedded Devices using Weight Pruning and Singular Value Decomposition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Scalable Object Detection on Embedded Devices using Weight Pruning and Singular Value Decomposition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2303.02735">

<!--Generated on Thu Feb 29 21:54:28 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Scalable Object Detection on Embedded Devices using Weight Pruning and Singular Value Decomposition
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id1.1.1" class="ltx_text ltx_font_bold">Dohyun Ham<sup id="id1.1.1.1" class="ltx_sup"><span id="id1.1.1.1.1" class="ltx_text ltx_font_medium">†</span></sup></span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
ROBOIN 
<br class="ltx_break">Yonsei University 
<br class="ltx_break">Seoul, Republic of Korea
<br class="ltx_break"><span id="id7.2.id1" class="ltx_text ltx_font_typewriter">{mango3354, jeongjaeyeop, jkp, raehy19, 
<br class="ltx_break">algorhythm, hyeongjun, ga06033}@yonsei.ac.kr</span>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id2.1.1" class="ltx_text ltx_font_bold">Jaeyeop Jeong<sup id="id2.1.1.1" class="ltx_sup"><span id="id2.1.1.1.1" class="ltx_text ltx_font_medium">†</span></sup></span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
ROBOIN 
<br class="ltx_break">Yonsei University 
<br class="ltx_break">Seoul, Republic of Korea
<br class="ltx_break"><span id="id8.2.id1" class="ltx_text ltx_font_typewriter">{mango3354, jeongjaeyeop, jkp, raehy19, 
<br class="ltx_break">algorhythm, hyeongjun, ga06033}@yonsei.ac.kr</span>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id3.1.1" class="ltx_text ltx_font_bold">June-Kyoo Park<sup id="id3.1.1.1" class="ltx_sup"><span id="id3.1.1.1.1" class="ltx_text ltx_font_medium">†</span></sup></span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
ROBOIN 
<br class="ltx_break">Yonsei University 
<br class="ltx_break">Seoul, Republic of Korea
<br class="ltx_break"><span id="id9.2.id1" class="ltx_text ltx_font_typewriter">{mango3354, jeongjaeyeop, jkp, raehy19, 
<br class="ltx_break">algorhythm, hyeongjun, ga06033}@yonsei.ac.kr</span>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id4.1.1" class="ltx_text ltx_font_bold">Raehyeon Jeong<sup id="id4.1.1.1" class="ltx_sup"><span id="id4.1.1.1.1" class="ltx_text ltx_font_medium">†</span></sup></span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
ROBOIN 
<br class="ltx_break">Yonsei University 
<br class="ltx_break">Seoul, Republic of Korea
<br class="ltx_break"><span id="id10.2.id1" class="ltx_text ltx_font_typewriter">{mango3354, jeongjaeyeop, jkp, raehy19, 
<br class="ltx_break">algorhythm, hyeongjun, ga06033}@yonsei.ac.kr</span>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id5.1.1" class="ltx_text ltx_font_bold">Seungmin Jeon<sup id="id5.1.1.1" class="ltx_sup"><span id="id5.1.1.1.1" class="ltx_text ltx_font_medium">†</span></sup></span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
ROBOIN 
<br class="ltx_break">Yonsei University 
<br class="ltx_break">Seoul, Republic of Korea
<br class="ltx_break"><span id="id11.2.id1" class="ltx_text ltx_font_typewriter">{mango3354, jeongjaeyeop, jkp, raehy19, 
<br class="ltx_break">algorhythm, hyeongjun, ga06033}@yonsei.ac.kr</span>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id12.1.id1" class="ltx_text ltx_font_bold">Hyeongjun Jeon</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
ROBOIN 
<br class="ltx_break">Yonsei University 
<br class="ltx_break">Seoul, Republic of Korea
<br class="ltx_break"><span id="id13.2.id1" class="ltx_text ltx_font_typewriter">{mango3354, jeongjaeyeop, jkp, raehy19, 
<br class="ltx_break">algorhythm, hyeongjun, ga06033}@yonsei.ac.kr</span>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id6.1.1" class="ltx_text ltx_font_bold">Yewon Lim<sup id="id6.1.1.1" class="ltx_sup"><span id="id6.1.1.1.1" class="ltx_text ltx_font_medium">†</span></sup></span>
</span><span class="ltx_author_notes">Corresponding author.
<span class="ltx_contact ltx_role_affiliation">
ROBOIN 
<br class="ltx_break">Yonsei University 
<br class="ltx_break">Seoul, Republic of Korea
<br class="ltx_break"><span id="id14.2.id1" class="ltx_text ltx_font_typewriter">{mango3354, jeongjaeyeop, jkp, raehy19, 
<br class="ltx_break">algorhythm, hyeongjun, ga06033}@yonsei.ac.kr</span>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id15.id1" class="ltx_p">This paper presents a method for optimizing object detection models by combining weight pruning
and singular value decomposition (SVD). The proposed method was evaluated on a custom dataset of
street work images obtained from <a target="_blank" href="https://universe.roboflow.com/roboflow-100/street-work" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://universe.roboflow.com/roboflow-100/street-work</a>.
The dataset consists of 611 training images, 175 validation images, and 87 test images with 7 classes.
We compared the performance of the optimized models with the original unoptimized model in terms of frame rate,
mean average precision (mAP@50), and weight size. The results show that the weight pruning + SVD model
achieved a 0.724 mAP@50 with a frame rate of 1.48 FPS and a weight size of 12.1 MB, outperforming
the original model (0.717 mAP@50, 1.50 FPS, and 12.3 MB). Precision-recall curves were also plotted
for all models. Our work demonstrates that the proposed method can effectively optimize object
detection models while balancing accuracy, speed, and model size.</p>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">$\dagger$</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">$\dagger$</sup><span class="ltx_note_type">footnotetext: </span>These authors contributed equally to this work</span></span></span>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2303.02735/assets/result.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="269" height="234" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Real-time object detection inference on Raspberry Pi 4 using YOLOv7 model and camera module.</figcaption>
</figure>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.2" class="ltx_p"><em id="p1.2.1" class="ltx_emph ltx_font_bold ltx_font_italic">K</em><span id="p1.2.2" class="ltx_text ltx_font_bold">eywords</span> Object Detection  <math id="p1.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.1.m1.1a"><mo id="p1.1.m1.1.1" xref="p1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.1.m1.1b"><ci id="p1.1.m1.1.1.cmml" xref="p1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.1.m1.1c">\cdot</annotation></semantics></math>
Model Compression  <math id="p1.2.m2.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.2.m2.1a"><mo id="p1.2.m2.1.1" xref="p1.2.m2.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.2.m2.1b"><ci id="p1.2.m2.1.1.cmml" xref="p1.2.m2.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.2.m2.1c">\cdot</annotation></semantics></math>
Weight Pruning</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Due to the limitations in battery capacity, physical volume, privacy, and latency, it is infeasible to deploy powerful machine learning models, such as YOLOv3 with more than 65 million parameters, in mobile applications such as autonomous vehicles or thermal imaging systems. Object detection is a fundamental technique in computer vision that enables a software system to detect and locate objects from an image or video stream. The key characteristic of object detection is its ability to identify the class of the object and its location in the image or video stream. The location is typically denoted by a bounding box around the object, while the object’s class can be inferred from pre-trained weights associated with the bounding box.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">Real-time object detection aims to perform object detection in real-time by predicting and locating objects on-the-fly. It has numerous applications including autonomous vehicles, face recognition, letter detection, and risk detection. For instance, Google Lens, first released in 2017, uses object detection to identify objects in an image or video stream. The Snapchat app can detect whether an object is an animal or a person, and applies relevant filters accordingly. Additionally, self-driving Tesla vehicles and Apple iPhone’s Photo Cutout feature both rely on real-time object detection to operate.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">Existing models often have a high number of parameters, making them difficult to deploy on resource-constrained hardware like the Raspberry Pi. Attempts to run these models on Raspberry Pi have resulted in low frame rates and truncated screens. To mitigate these issues, techniques like weight pruning and SVD have been proposed.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">Weight pruning is a lightweight technique that selectively removes insignificant parameters from the model’s weights to increase inference speed and generalization performance. However, this approach can lead to a loss of information and reduced hardware acceleration efficiency. Singular Value Decomposition (SVD) is another technique that decomposes a matrix into a specific structure and diagonalizes it. Unlike eigenvalue decomposition, SVD can be applied to all <math id="S1.p4.1.m1.1" class="ltx_Math" alttext="m\times n" display="inline"><semantics id="S1.p4.1.m1.1a"><mrow id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml"><mi id="S1.p4.1.m1.1.1.2" xref="S1.p4.1.m1.1.1.2.cmml">m</mi><mo lspace="0.222em" rspace="0.222em" id="S1.p4.1.m1.1.1.1" xref="S1.p4.1.m1.1.1.1.cmml">×</mo><mi id="S1.p4.1.m1.1.1.3" xref="S1.p4.1.m1.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><apply id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1"><times id="S1.p4.1.m1.1.1.1.cmml" xref="S1.p4.1.m1.1.1.1"></times><ci id="S1.p4.1.m1.1.1.2.cmml" xref="S1.p4.1.m1.1.1.2">𝑚</ci><ci id="S1.p4.1.m1.1.1.3.cmml" xref="S1.p4.1.m1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">m\times n</annotation></semantics></math> matrices, irrespective of their shape.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">This paper aims to implement and evaluate weight pruning and SVD as optimization techniques for a popular object detection model, YOLO-v7.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">To infer computer vision models in real-time from resource-constrained devices, one approach is to
design an efficient neural network structure that reduces memory and computation requirements.
GoogleNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and SqueezeNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> achieved this by reducing the number of
parameters through the use of 1x1 convolution kernels instead of the standard 3x3 convolution kernel.
SqueezeNet was able to reduce the model size by 50 times compared to AlexNet while exceeding the accuracy
of AlexNet. In MobileNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, the convolutional process was further reduced by decomposing
it into Depthwise convolution and Pointwise convolution. These attempts at efficient small model design
are crucial, but they do have limitations. To overcome these limitations, additional methods such as model
quantization, pruning, and SVD have been devised for better compression in terms of efficiency.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Quantization</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">Model quantization is an effective compression technique that reduces the precision of weights and activations to reduce the model size. This technique involves converting high-precision data, such as 32-bit floating-point numbers, into low-precision data, such as 8-bit integers. While post-training quantization is the most common approach, it may lead to accuracy degradation in small models, particularly when there is a large deviation in the weight range of each channel or when an outlier weight exists.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.1" class="ltx_p">To address these issues, quantization-aware training (QAT) has been developed, which simulates the quantization
effect during the net propagation process of training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. During QAT, all weights and biases are stored
as floating-point numbers, and backpropagation is performed as usual. At the time of inference, the weights and
biases are converted into 8-bit integers and used in calculations.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.p3.1" class="ltx_p">In addition to 8-bit integer quantization, it is also possible to use even lower bit widths to represent weights
and activations. For example, Ternary Weight Networks (TWN) use -1, 0, and 1 bits to represent weights <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>,
while Binary Neural Networks (BNN) use -1 and 1 bits <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. XNOR-Net is a model that uses binary weight and
input representations, which simplifies the convolution process to a scaling operation after the XNOR Gate operation
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. This results in a 58x speedup compared to conventional convolution while saving 32x more memory. While
the efficiency of these bit operators can be applied to specific custom hardware, it may not be applicable to all real hardware.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para ltx_noindent">
<p id="S2.SS1.p4.1" class="ltx_p">In summary, bit quantization techniques offer significant benefits in terms of model size and computational efficiency. However, their applicability to real hardware may be limited. Thus, researchers must carefully consider the trade-offs between model size, computational efficiency, and hardware constraints when choosing a quantization technique for their specific use case.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Pruning</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">Model pruning is a widely used technique in deep learning that reduces the size and computational complexity of neural networks by removing redundant or less important parameters. Two common methods for pruning are weight pruning, which removes small magnitude weights, and structured pruning, which removes entire filters or channels. While weight pruning requires additional software or hardware to support the resulting sparse matrix, structured pruning can be easily inferred and is more practical for deployment without specific processing.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.1" class="ltx_p">Structured pruning involves marking certain structures of the neural network for removal based on their importance scores,
which are determined by specific criteria. One such criterion is the L1-norm, which measures the importance of each filter,
channel, or layer by the sum of the absolute values of its weights <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. For example, when filters were pruned
in the order in which the L1-norm values of each filter were small, 64% of the weights were removed from the VGG-16 model,
with no significant difference in accuracy between the unpruned and pruned models. Various methods for conducting model pruning
are being studied, including those that combine pruning with other techniques such as quantization and knowledge distillation.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>YOLO-v7 </h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p">YOLO, short for "You Only Look Once", is a widely used family of algorithms for real-time object detection. Object detection models are typically classified into two categories: one-stage detectors and two-stage detectors. One-stage detectors are designed to perform object localization and classification simultaneously, while two-stage detectors perform these tasks in two separate steps. One popular one-stage detector is YOLO, which divides the input image into a grid of cells and performs multi-class classification and bounding box regression for each cell. This approach allows YOLO to efficiently detect objects in real-time, making it well-suited for applications that require fast and accurate detection. On the other hand, two-stage detectors typically involve a region proposal network (RPN) that generates candidate object regions, followed by a separate classification and localization step. Although two-stage detectors may be slower than one-stage detectors, they often achieve higher accuracy and are commonly used in applications that require high precision object detection. Overall, the choice of object detection model depends on the specific requirements of the application, such as the desired trade-off between speed and accuracy.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.p2.1" class="ltx_p">YOLOv7 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, the latest iteration of this architecture, is capable of real-time object detection. The YOLOv7
architecture relies on a convolutional neural network to extract features from input images and predict the bounding
boxes and class probabilities for each object in the image.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para ltx_noindent">
<p id="S2.SS3.p3.1" class="ltx_p">The YOLOv7 architecture is composed of three main components: a backbone, a neck, and a head.
The backbone of the YOLOv7 architecture contains a computational block called Extended Efficient Layer Aggregation Network
(E-ELAN), which builds on the existing ELAN by using expand, shuffle, and merge cardinality to enhance the learning ability
of the network without disrupting the original gradient path. Group convolution is used to expand the channel and cardinality
of computational blocks, and feature maps are shuffled into groups before being merged together. This approach allows
for continuous improvement in the network’s ability to detect objects in real-time. The neck of the architecture is
responsible for gathering the feature maps that are extracted by the Backbone and using them to create feature pyramids.
The head is responsible for producing the final model outputs, and YOLOv7 implements a technique called Deep Supervision
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> to allow for multiple heads to be used during training. The lead head is responsible for the final output,
while an auxiliary head is used to assist with training in the middle layers of the network.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para ltx_noindent">
<p id="S2.SS3.p4.1" class="ltx_p">Among the various iterations of YOLOv7, the YOLOv7-tiny is an optimized model designed for edge devices. In comparison to other versions, the edge-optimized YOLOv7-tiny model utilizes the Leaky ReLU activation function, while other models implement the SiLU activation function. The Leaky ReLU activation function allows for a small, non-zero output when the input is negative, which can prevent dead neurons and improve the performance of the model. In contrast, the SiLU activation function applies a sigmoid function to the input, which smooths out the outputs and can provide better gradient propagation. However, the edge-optimized YOLOv7-tiny model has been specifically designed for edge computing, where hardware limitations may require more efficient and compact models. By utilizing the Leaky ReLU activation function, the model can maintain a high level of accuracy while minimizing computational resources.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>SVD</h3>

<div id="S2.SS4.p1" class="ltx_para ltx_noindent">
<p id="S2.SS4.p1.1" class="ltx_p">SVD (Singular Value Decomposition) is a mathematical technique used in linear algebra to decompose a matrix into three smaller matrices: a left singular vector matrix, a singular value diagonal matrix, and a right singular vector matrix. Truncated SVD is a variant of SVD that involves retaining only a subset of the singular values and their corresponding singular vectors. This is achieved by setting a threshold below which the singular values and their corresponding singular vectors are discarded.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para ltx_noindent">
<p id="S2.SS4.p2.1" class="ltx_p">In neural network compression, Truncated SVD can be used to reduce the size of weight matrices and compress the network.
By keeping only the top k singular values and their corresponding singular vectors, a smaller compressed weight matrix can
be obtained that can replace the original weight matrix. When SVD was performed on the model in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, the
speed of the bottleneck convolution operation in the first layer increased by three times, and the number of weight
parameters in the fully connected layer was reduced by five to 13 times compared to the previous method with no
significant decrease in accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<div id="S2.SS4.p3" class="ltx_para ltx_noindent">
<p id="S2.SS4.p3.1" class="ltx_p">SVD is independent of other methods used for efficient evaluation, such as quantization or pruning. Therefore, it has the potential to be used in combination with other methods to achieve additional benefits.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">In this work, we used the YOLO-v7 object detection architecture available on <a target="_blank" href="https://github.com/WongKinYiu/yolov7" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/WongKinYiu/yolov7</a> and deployed it on a Raspberry Pi 4 Model B (4GB RAM). YOLO-v7 is a state-of-the-art object detection model that is highly efficient in terms of both speed and accuracy. We chose to use the Raspberry Pi 4 due to its relatively low cost and compact size, making it an ideal platform for implementing object detection models in resource-constrained environments.</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">Dataset
<br class="ltx_break"></span>In this work, we used a publicly available dataset for object detection<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, which consists of
611 training images, 175 validation images, and 87 test images. The dataset includes 7 classes of objects, including
cars, pedestrians, and bicycles.</p>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p">To preprocess the images, we resized them to a fixed size of 640x640 pixels and normalized the pixel values to be
between 0 and 1. We also randomly applied data augmentation techniques during training, such as random cropping,
translations, and adjusting brightness and contrast.</p>
</div>
<div id="S3.p4" class="ltx_para ltx_noindent">
<p id="S3.p4.1" class="ltx_p">The dataset was split into three parts: training, validation, and testing, with a ratio of approximately 70:20:10.
We used the training set to train our models, the validation set to tune hyperparameters and prevent overfitting,
and the test set to evaluate the performance of our models.</p>
</div>
<div id="S3.p5" class="ltx_para ltx_noindent">
<p id="S3.p5.1" class="ltx_p"><span id="S3.p5.1.1" class="ltx_text ltx_font_bold">Pruning
<br class="ltx_break"></span>In this study, we applied L1 pruning to our object detection model to reduce its size and improve its efficiency for
deployment on resource-constrained devices. L1 pruning is a popular technique for compressing deep neural networks by
removing the least important weights in the network. This is achieved by identifying the 30% of the total elements
of weight tensors with the lowest L1 norms and setting them to zero.</p>
</div>
<div id="S3.p6" class="ltx_para ltx_noindent">
<p id="S3.p6.1" class="ltx_p">The motivation for using L1 pruning is based on the assumption that a small fraction of the weights in a neural network
are responsible for the majority of its representational power. By removing the least important weights, we can reduce
the model size and computational requirements without significantly affecting its performance.</p>
</div>
<div id="S3.p7" class="ltx_para ltx_noindent">
<p id="S3.p7.1" class="ltx_p"><span id="S3.p7.1.1" class="ltx_text ltx_font_bold">Low-Rank SVD
<br class="ltx_break"></span>We employed singular value decomposition (SVD) as a model optimization technique to reduce the size of the object detection
model. We first reshaped the weight tensor of each convolutional layer in the model into a two-dimensional matrix.
We reshaped the matrix in a way that makes the number of rows of the matrix be the largest integer that is a divisor
of the number of the total elements of the weight tensor. We intended to produce a square matrix as much as possible
to make the SVD process more efficient.</p>
</div>
<div id="S3.p8" class="ltx_para ltx_noindent">
<p id="S3.p8.1" class="ltx_p">We then applied a low-rank SVD to this matrix, which factorizes the matrix into three separate matrices: a left singular
matrix, a diagonal singular value matrix, and a right singular matrix. We truncated the number of singular values
used in the factorization to reduce the size of the model while minimizing the loss of information. The number of
singular values was determined empirically based on a trade-off between the size of the model and its accuracy
on validation data. We then reconstructed the original weight matrix using the truncated SVD matrices, effectively
reducing the size of the weight matrix. Finally, we reshaped the weight matrix back into its original tensor shape
to restore the weight tensor of each convolutional layer in the model.</p>
</div>
<div id="S3.p9" class="ltx_para ltx_noindent">
<p id="S3.p9.1" class="ltx_p">To evaluate the effectiveness of the SVD optimization technique, we compared the performance of the optimized model
with that of the original unoptimized model using publicly available dataset<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. We also
varied the number of singular values used in the SVD factorization and evaluated the impact on model size and accuracy.</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\mathbf{W}_{m\times n}\approx\mathbf{U}_{n\times k}\mathbf{\Sigma}_{k\times k}\mathbf{V}^{\top}_{k\times n}" display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><msub id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.2.2" xref="S3.E1.m1.1.1.2.2.cmml">𝐖</mi><mrow id="S3.E1.m1.1.1.2.3" xref="S3.E1.m1.1.1.2.3.cmml"><mi id="S3.E1.m1.1.1.2.3.2" xref="S3.E1.m1.1.1.2.3.2.cmml">m</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.1.1.2.3.1" xref="S3.E1.m1.1.1.2.3.1.cmml">×</mo><mi id="S3.E1.m1.1.1.2.3.3" xref="S3.E1.m1.1.1.2.3.3.cmml">n</mi></mrow></msub><mo id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml">≈</mo><mrow id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><msub id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml"><mi id="S3.E1.m1.1.1.3.2.2" xref="S3.E1.m1.1.1.3.2.2.cmml">𝐔</mi><mrow id="S3.E1.m1.1.1.3.2.3" xref="S3.E1.m1.1.1.3.2.3.cmml"><mi id="S3.E1.m1.1.1.3.2.3.2" xref="S3.E1.m1.1.1.3.2.3.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.1.1.3.2.3.1" xref="S3.E1.m1.1.1.3.2.3.1.cmml">×</mo><mi id="S3.E1.m1.1.1.3.2.3.3" xref="S3.E1.m1.1.1.3.2.3.3.cmml">k</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.1" xref="S3.E1.m1.1.1.3.1.cmml">​</mo><msub id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.3.3.2" xref="S3.E1.m1.1.1.3.3.2.cmml">𝚺</mi><mrow id="S3.E1.m1.1.1.3.3.3" xref="S3.E1.m1.1.1.3.3.3.cmml"><mi id="S3.E1.m1.1.1.3.3.3.2" xref="S3.E1.m1.1.1.3.3.3.2.cmml">k</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.1.1.3.3.3.1" xref="S3.E1.m1.1.1.3.3.3.1.cmml">×</mo><mi id="S3.E1.m1.1.1.3.3.3.3" xref="S3.E1.m1.1.1.3.3.3.3.cmml">k</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.1a" xref="S3.E1.m1.1.1.3.1.cmml">​</mo><msubsup id="S3.E1.m1.1.1.3.4" xref="S3.E1.m1.1.1.3.4.cmml"><mi id="S3.E1.m1.1.1.3.4.2.2" xref="S3.E1.m1.1.1.3.4.2.2.cmml">𝐕</mi><mrow id="S3.E1.m1.1.1.3.4.3" xref="S3.E1.m1.1.1.3.4.3.cmml"><mi id="S3.E1.m1.1.1.3.4.3.2" xref="S3.E1.m1.1.1.3.4.3.2.cmml">k</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.1.1.3.4.3.1" xref="S3.E1.m1.1.1.3.4.3.1.cmml">×</mo><mi id="S3.E1.m1.1.1.3.4.3.3" xref="S3.E1.m1.1.1.3.4.3.3.cmml">n</mi></mrow><mo id="S3.E1.m1.1.1.3.4.2.3" xref="S3.E1.m1.1.1.3.4.2.3.cmml">⊤</mo></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><approx id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"></approx><apply id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.2.2">𝐖</ci><apply id="S3.E1.m1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.2.3"><times id="S3.E1.m1.1.1.2.3.1.cmml" xref="S3.E1.m1.1.1.2.3.1"></times><ci id="S3.E1.m1.1.1.2.3.2.cmml" xref="S3.E1.m1.1.1.2.3.2">𝑚</ci><ci id="S3.E1.m1.1.1.2.3.3.cmml" xref="S3.E1.m1.1.1.2.3.3">𝑛</ci></apply></apply><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><times id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3.1"></times><apply id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2">𝐔</ci><apply id="S3.E1.m1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.3.2.3"><times id="S3.E1.m1.1.1.3.2.3.1.cmml" xref="S3.E1.m1.1.1.3.2.3.1"></times><ci id="S3.E1.m1.1.1.3.2.3.2.cmml" xref="S3.E1.m1.1.1.3.2.3.2">𝑛</ci><ci id="S3.E1.m1.1.1.3.2.3.3.cmml" xref="S3.E1.m1.1.1.3.2.3.3">𝑘</ci></apply></apply><apply id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.2">𝚺</ci><apply id="S3.E1.m1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3"><times id="S3.E1.m1.1.1.3.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3.3.1"></times><ci id="S3.E1.m1.1.1.3.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.3.2">𝑘</ci><ci id="S3.E1.m1.1.1.3.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3.3">𝑘</ci></apply></apply><apply id="S3.E1.m1.1.1.3.4.cmml" xref="S3.E1.m1.1.1.3.4"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.4.1.cmml" xref="S3.E1.m1.1.1.3.4">subscript</csymbol><apply id="S3.E1.m1.1.1.3.4.2.cmml" xref="S3.E1.m1.1.1.3.4"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.4.2.1.cmml" xref="S3.E1.m1.1.1.3.4">superscript</csymbol><ci id="S3.E1.m1.1.1.3.4.2.2.cmml" xref="S3.E1.m1.1.1.3.4.2.2">𝐕</ci><csymbol cd="latexml" id="S3.E1.m1.1.1.3.4.2.3.cmml" xref="S3.E1.m1.1.1.3.4.2.3">top</csymbol></apply><apply id="S3.E1.m1.1.1.3.4.3.cmml" xref="S3.E1.m1.1.1.3.4.3"><times id="S3.E1.m1.1.1.3.4.3.1.cmml" xref="S3.E1.m1.1.1.3.4.3.1"></times><ci id="S3.E1.m1.1.1.3.4.3.2.cmml" xref="S3.E1.m1.1.1.3.4.3.2">𝑘</ci><ci id="S3.E1.m1.1.1.3.4.3.3.cmml" xref="S3.E1.m1.1.1.3.4.3.3">𝑛</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\mathbf{W}_{m\times n}\approx\mathbf{U}_{n\times k}\mathbf{\Sigma}_{k\times k}\mathbf{V}^{\top}_{k\times n}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.p10" class="ltx_para ltx_noindent">
<p id="S3.p10.11" class="ltx_p">where <math id="S3.p10.1.m1.1" class="ltx_Math" alttext="\mathbf{W}" display="inline"><semantics id="S3.p10.1.m1.1a"><mi id="S3.p10.1.m1.1.1" xref="S3.p10.1.m1.1.1.cmml">𝐖</mi><annotation-xml encoding="MathML-Content" id="S3.p10.1.m1.1b"><ci id="S3.p10.1.m1.1.1.cmml" xref="S3.p10.1.m1.1.1">𝐖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p10.1.m1.1c">\mathbf{W}</annotation></semantics></math> is the weight tensor of a convolutional layer in the model, reshaped into a 2D matrix with dimensions
<math id="S3.p10.2.m2.1" class="ltx_Math" alttext="m\times n" display="inline"><semantics id="S3.p10.2.m2.1a"><mrow id="S3.p10.2.m2.1.1" xref="S3.p10.2.m2.1.1.cmml"><mi id="S3.p10.2.m2.1.1.2" xref="S3.p10.2.m2.1.1.2.cmml">m</mi><mo lspace="0.222em" rspace="0.222em" id="S3.p10.2.m2.1.1.1" xref="S3.p10.2.m2.1.1.1.cmml">×</mo><mi id="S3.p10.2.m2.1.1.3" xref="S3.p10.2.m2.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p10.2.m2.1b"><apply id="S3.p10.2.m2.1.1.cmml" xref="S3.p10.2.m2.1.1"><times id="S3.p10.2.m2.1.1.1.cmml" xref="S3.p10.2.m2.1.1.1"></times><ci id="S3.p10.2.m2.1.1.2.cmml" xref="S3.p10.2.m2.1.1.2">𝑚</ci><ci id="S3.p10.2.m2.1.1.3.cmml" xref="S3.p10.2.m2.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p10.2.m2.1c">m\times n</annotation></semantics></math>, <math id="S3.p10.3.m3.1" class="ltx_Math" alttext="\mathbf{U}" display="inline"><semantics id="S3.p10.3.m3.1a"><mi id="S3.p10.3.m3.1.1" xref="S3.p10.3.m3.1.1.cmml">𝐔</mi><annotation-xml encoding="MathML-Content" id="S3.p10.3.m3.1b"><ci id="S3.p10.3.m3.1.1.cmml" xref="S3.p10.3.m3.1.1">𝐔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p10.3.m3.1c">\mathbf{U}</annotation></semantics></math> is an <math id="S3.p10.4.m4.1" class="ltx_Math" alttext="m\times k" display="inline"><semantics id="S3.p10.4.m4.1a"><mrow id="S3.p10.4.m4.1.1" xref="S3.p10.4.m4.1.1.cmml"><mi id="S3.p10.4.m4.1.1.2" xref="S3.p10.4.m4.1.1.2.cmml">m</mi><mo lspace="0.222em" rspace="0.222em" id="S3.p10.4.m4.1.1.1" xref="S3.p10.4.m4.1.1.1.cmml">×</mo><mi id="S3.p10.4.m4.1.1.3" xref="S3.p10.4.m4.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p10.4.m4.1b"><apply id="S3.p10.4.m4.1.1.cmml" xref="S3.p10.4.m4.1.1"><times id="S3.p10.4.m4.1.1.1.cmml" xref="S3.p10.4.m4.1.1.1"></times><ci id="S3.p10.4.m4.1.1.2.cmml" xref="S3.p10.4.m4.1.1.2">𝑚</ci><ci id="S3.p10.4.m4.1.1.3.cmml" xref="S3.p10.4.m4.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p10.4.m4.1c">m\times k</annotation></semantics></math> matrix, <math id="S3.p10.5.m5.1" class="ltx_Math" alttext="\mathbf{\Sigma}" display="inline"><semantics id="S3.p10.5.m5.1a"><mi id="S3.p10.5.m5.1.1" xref="S3.p10.5.m5.1.1.cmml">𝚺</mi><annotation-xml encoding="MathML-Content" id="S3.p10.5.m5.1b"><ci id="S3.p10.5.m5.1.1.cmml" xref="S3.p10.5.m5.1.1">𝚺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p10.5.m5.1c">\mathbf{\Sigma}</annotation></semantics></math> is a <math id="S3.p10.6.m6.1" class="ltx_Math" alttext="k\times k" display="inline"><semantics id="S3.p10.6.m6.1a"><mrow id="S3.p10.6.m6.1.1" xref="S3.p10.6.m6.1.1.cmml"><mi id="S3.p10.6.m6.1.1.2" xref="S3.p10.6.m6.1.1.2.cmml">k</mi><mo lspace="0.222em" rspace="0.222em" id="S3.p10.6.m6.1.1.1" xref="S3.p10.6.m6.1.1.1.cmml">×</mo><mi id="S3.p10.6.m6.1.1.3" xref="S3.p10.6.m6.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p10.6.m6.1b"><apply id="S3.p10.6.m6.1.1.cmml" xref="S3.p10.6.m6.1.1"><times id="S3.p10.6.m6.1.1.1.cmml" xref="S3.p10.6.m6.1.1.1"></times><ci id="S3.p10.6.m6.1.1.2.cmml" xref="S3.p10.6.m6.1.1.2">𝑘</ci><ci id="S3.p10.6.m6.1.1.3.cmml" xref="S3.p10.6.m6.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p10.6.m6.1c">k\times k</annotation></semantics></math> diagonal matrix containing
the singular values, and <math id="S3.p10.7.m7.1" class="ltx_Math" alttext="\mathbf{V}^{\top}" display="inline"><semantics id="S3.p10.7.m7.1a"><msup id="S3.p10.7.m7.1.1" xref="S3.p10.7.m7.1.1.cmml"><mi id="S3.p10.7.m7.1.1.2" xref="S3.p10.7.m7.1.1.2.cmml">𝐕</mi><mo id="S3.p10.7.m7.1.1.3" xref="S3.p10.7.m7.1.1.3.cmml">⊤</mo></msup><annotation-xml encoding="MathML-Content" id="S3.p10.7.m7.1b"><apply id="S3.p10.7.m7.1.1.cmml" xref="S3.p10.7.m7.1.1"><csymbol cd="ambiguous" id="S3.p10.7.m7.1.1.1.cmml" xref="S3.p10.7.m7.1.1">superscript</csymbol><ci id="S3.p10.7.m7.1.1.2.cmml" xref="S3.p10.7.m7.1.1.2">𝐕</ci><csymbol cd="latexml" id="S3.p10.7.m7.1.1.3.cmml" xref="S3.p10.7.m7.1.1.3">top</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p10.7.m7.1c">\mathbf{V}^{\top}</annotation></semantics></math> is a <math id="S3.p10.8.m8.1" class="ltx_Math" alttext="k\times n" display="inline"><semantics id="S3.p10.8.m8.1a"><mrow id="S3.p10.8.m8.1.1" xref="S3.p10.8.m8.1.1.cmml"><mi id="S3.p10.8.m8.1.1.2" xref="S3.p10.8.m8.1.1.2.cmml">k</mi><mo lspace="0.222em" rspace="0.222em" id="S3.p10.8.m8.1.1.1" xref="S3.p10.8.m8.1.1.1.cmml">×</mo><mi id="S3.p10.8.m8.1.1.3" xref="S3.p10.8.m8.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p10.8.m8.1b"><apply id="S3.p10.8.m8.1.1.cmml" xref="S3.p10.8.m8.1.1"><times id="S3.p10.8.m8.1.1.1.cmml" xref="S3.p10.8.m8.1.1.1"></times><ci id="S3.p10.8.m8.1.1.2.cmml" xref="S3.p10.8.m8.1.1.2">𝑘</ci><ci id="S3.p10.8.m8.1.1.3.cmml" xref="S3.p10.8.m8.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p10.8.m8.1c">k\times n</annotation></semantics></math> matrix. The number <math id="S3.p10.9.m9.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.p10.9.m9.1a"><mi id="S3.p10.9.m9.1.1" xref="S3.p10.9.m9.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.p10.9.m9.1b"><ci id="S3.p10.9.m9.1.1.cmml" xref="S3.p10.9.m9.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p10.9.m9.1c">k</annotation></semantics></math> is chosen to be smaller than
both <math id="S3.p10.10.m10.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.p10.10.m10.1a"><mi id="S3.p10.10.m10.1.1" xref="S3.p10.10.m10.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.p10.10.m10.1b"><ci id="S3.p10.10.m10.1.1.cmml" xref="S3.p10.10.m10.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p10.10.m10.1c">m</annotation></semantics></math> and <math id="S3.p10.11.m11.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.p10.11.m11.1a"><mi id="S3.p10.11.m11.1.1" xref="S3.p10.11.m11.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.p10.11.m11.1b"><ci id="S3.p10.11.m11.1.1.cmml" xref="S3.p10.11.m11.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p10.11.m11.1c">n</annotation></semantics></math>, and represents the rank of the truncated SVD.</p>
</div>
<div id="S3.p11" class="ltx_para ltx_noindent">
<p id="S3.p11.4" class="ltx_p">As demonstrated in Table <a href="#S3.T1" title="Table 1 ‣ 3 Methodology ‣ Scalable Object Detection on Embedded Devices using Weight Pruning and Singular Value Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the number of parameters in the Original Weight is <math id="S3.p11.1.m1.1" class="ltx_Math" alttext="OIK^{2}" display="inline"><semantics id="S3.p11.1.m1.1a"><mrow id="S3.p11.1.m1.1.1" xref="S3.p11.1.m1.1.1.cmml"><mi id="S3.p11.1.m1.1.1.2" xref="S3.p11.1.m1.1.1.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.p11.1.m1.1.1.1" xref="S3.p11.1.m1.1.1.1.cmml">​</mo><mi id="S3.p11.1.m1.1.1.3" xref="S3.p11.1.m1.1.1.3.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.p11.1.m1.1.1.1a" xref="S3.p11.1.m1.1.1.1.cmml">​</mo><msup id="S3.p11.1.m1.1.1.4" xref="S3.p11.1.m1.1.1.4.cmml"><mi id="S3.p11.1.m1.1.1.4.2" xref="S3.p11.1.m1.1.1.4.2.cmml">K</mi><mn id="S3.p11.1.m1.1.1.4.3" xref="S3.p11.1.m1.1.1.4.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p11.1.m1.1b"><apply id="S3.p11.1.m1.1.1.cmml" xref="S3.p11.1.m1.1.1"><times id="S3.p11.1.m1.1.1.1.cmml" xref="S3.p11.1.m1.1.1.1"></times><ci id="S3.p11.1.m1.1.1.2.cmml" xref="S3.p11.1.m1.1.1.2">𝑂</ci><ci id="S3.p11.1.m1.1.1.3.cmml" xref="S3.p11.1.m1.1.1.3">𝐼</ci><apply id="S3.p11.1.m1.1.1.4.cmml" xref="S3.p11.1.m1.1.1.4"><csymbol cd="ambiguous" id="S3.p11.1.m1.1.1.4.1.cmml" xref="S3.p11.1.m1.1.1.4">superscript</csymbol><ci id="S3.p11.1.m1.1.1.4.2.cmml" xref="S3.p11.1.m1.1.1.4.2">𝐾</ci><cn type="integer" id="S3.p11.1.m1.1.1.4.3.cmml" xref="S3.p11.1.m1.1.1.4.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p11.1.m1.1c">OIK^{2}</annotation></semantics></math>,
while in the case of U+S+V, it is <math id="S3.p11.2.m2.1" class="ltx_Math" alttext="R(IK^{2}+1+O)" display="inline"><semantics id="S3.p11.2.m2.1a"><mrow id="S3.p11.2.m2.1.1" xref="S3.p11.2.m2.1.1.cmml"><mi id="S3.p11.2.m2.1.1.3" xref="S3.p11.2.m2.1.1.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.p11.2.m2.1.1.2" xref="S3.p11.2.m2.1.1.2.cmml">​</mo><mrow id="S3.p11.2.m2.1.1.1.1" xref="S3.p11.2.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.p11.2.m2.1.1.1.1.2" xref="S3.p11.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S3.p11.2.m2.1.1.1.1.1" xref="S3.p11.2.m2.1.1.1.1.1.cmml"><mrow id="S3.p11.2.m2.1.1.1.1.1.2" xref="S3.p11.2.m2.1.1.1.1.1.2.cmml"><mi id="S3.p11.2.m2.1.1.1.1.1.2.2" xref="S3.p11.2.m2.1.1.1.1.1.2.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.p11.2.m2.1.1.1.1.1.2.1" xref="S3.p11.2.m2.1.1.1.1.1.2.1.cmml">​</mo><msup id="S3.p11.2.m2.1.1.1.1.1.2.3" xref="S3.p11.2.m2.1.1.1.1.1.2.3.cmml"><mi id="S3.p11.2.m2.1.1.1.1.1.2.3.2" xref="S3.p11.2.m2.1.1.1.1.1.2.3.2.cmml">K</mi><mn id="S3.p11.2.m2.1.1.1.1.1.2.3.3" xref="S3.p11.2.m2.1.1.1.1.1.2.3.3.cmml">2</mn></msup></mrow><mo id="S3.p11.2.m2.1.1.1.1.1.1" xref="S3.p11.2.m2.1.1.1.1.1.1.cmml">+</mo><mn id="S3.p11.2.m2.1.1.1.1.1.3" xref="S3.p11.2.m2.1.1.1.1.1.3.cmml">1</mn><mo id="S3.p11.2.m2.1.1.1.1.1.1a" xref="S3.p11.2.m2.1.1.1.1.1.1.cmml">+</mo><mi id="S3.p11.2.m2.1.1.1.1.1.4" xref="S3.p11.2.m2.1.1.1.1.1.4.cmml">O</mi></mrow><mo stretchy="false" id="S3.p11.2.m2.1.1.1.1.3" xref="S3.p11.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p11.2.m2.1b"><apply id="S3.p11.2.m2.1.1.cmml" xref="S3.p11.2.m2.1.1"><times id="S3.p11.2.m2.1.1.2.cmml" xref="S3.p11.2.m2.1.1.2"></times><ci id="S3.p11.2.m2.1.1.3.cmml" xref="S3.p11.2.m2.1.1.3">𝑅</ci><apply id="S3.p11.2.m2.1.1.1.1.1.cmml" xref="S3.p11.2.m2.1.1.1.1"><plus id="S3.p11.2.m2.1.1.1.1.1.1.cmml" xref="S3.p11.2.m2.1.1.1.1.1.1"></plus><apply id="S3.p11.2.m2.1.1.1.1.1.2.cmml" xref="S3.p11.2.m2.1.1.1.1.1.2"><times id="S3.p11.2.m2.1.1.1.1.1.2.1.cmml" xref="S3.p11.2.m2.1.1.1.1.1.2.1"></times><ci id="S3.p11.2.m2.1.1.1.1.1.2.2.cmml" xref="S3.p11.2.m2.1.1.1.1.1.2.2">𝐼</ci><apply id="S3.p11.2.m2.1.1.1.1.1.2.3.cmml" xref="S3.p11.2.m2.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.p11.2.m2.1.1.1.1.1.2.3.1.cmml" xref="S3.p11.2.m2.1.1.1.1.1.2.3">superscript</csymbol><ci id="S3.p11.2.m2.1.1.1.1.1.2.3.2.cmml" xref="S3.p11.2.m2.1.1.1.1.1.2.3.2">𝐾</ci><cn type="integer" id="S3.p11.2.m2.1.1.1.1.1.2.3.3.cmml" xref="S3.p11.2.m2.1.1.1.1.1.2.3.3">2</cn></apply></apply><cn type="integer" id="S3.p11.2.m2.1.1.1.1.1.3.cmml" xref="S3.p11.2.m2.1.1.1.1.1.3">1</cn><ci id="S3.p11.2.m2.1.1.1.1.1.4.cmml" xref="S3.p11.2.m2.1.1.1.1.1.4">𝑂</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p11.2.m2.1c">R(IK^{2}+1+O)</annotation></semantics></math>. Typically, <math id="S3.p11.3.m3.1" class="ltx_Math" alttext="R(IK^{2}+1+O)" display="inline"><semantics id="S3.p11.3.m3.1a"><mrow id="S3.p11.3.m3.1.1" xref="S3.p11.3.m3.1.1.cmml"><mi id="S3.p11.3.m3.1.1.3" xref="S3.p11.3.m3.1.1.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.p11.3.m3.1.1.2" xref="S3.p11.3.m3.1.1.2.cmml">​</mo><mrow id="S3.p11.3.m3.1.1.1.1" xref="S3.p11.3.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.p11.3.m3.1.1.1.1.2" xref="S3.p11.3.m3.1.1.1.1.1.cmml">(</mo><mrow id="S3.p11.3.m3.1.1.1.1.1" xref="S3.p11.3.m3.1.1.1.1.1.cmml"><mrow id="S3.p11.3.m3.1.1.1.1.1.2" xref="S3.p11.3.m3.1.1.1.1.1.2.cmml"><mi id="S3.p11.3.m3.1.1.1.1.1.2.2" xref="S3.p11.3.m3.1.1.1.1.1.2.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.p11.3.m3.1.1.1.1.1.2.1" xref="S3.p11.3.m3.1.1.1.1.1.2.1.cmml">​</mo><msup id="S3.p11.3.m3.1.1.1.1.1.2.3" xref="S3.p11.3.m3.1.1.1.1.1.2.3.cmml"><mi id="S3.p11.3.m3.1.1.1.1.1.2.3.2" xref="S3.p11.3.m3.1.1.1.1.1.2.3.2.cmml">K</mi><mn id="S3.p11.3.m3.1.1.1.1.1.2.3.3" xref="S3.p11.3.m3.1.1.1.1.1.2.3.3.cmml">2</mn></msup></mrow><mo id="S3.p11.3.m3.1.1.1.1.1.1" xref="S3.p11.3.m3.1.1.1.1.1.1.cmml">+</mo><mn id="S3.p11.3.m3.1.1.1.1.1.3" xref="S3.p11.3.m3.1.1.1.1.1.3.cmml">1</mn><mo id="S3.p11.3.m3.1.1.1.1.1.1a" xref="S3.p11.3.m3.1.1.1.1.1.1.cmml">+</mo><mi id="S3.p11.3.m3.1.1.1.1.1.4" xref="S3.p11.3.m3.1.1.1.1.1.4.cmml">O</mi></mrow><mo stretchy="false" id="S3.p11.3.m3.1.1.1.1.3" xref="S3.p11.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p11.3.m3.1b"><apply id="S3.p11.3.m3.1.1.cmml" xref="S3.p11.3.m3.1.1"><times id="S3.p11.3.m3.1.1.2.cmml" xref="S3.p11.3.m3.1.1.2"></times><ci id="S3.p11.3.m3.1.1.3.cmml" xref="S3.p11.3.m3.1.1.3">𝑅</ci><apply id="S3.p11.3.m3.1.1.1.1.1.cmml" xref="S3.p11.3.m3.1.1.1.1"><plus id="S3.p11.3.m3.1.1.1.1.1.1.cmml" xref="S3.p11.3.m3.1.1.1.1.1.1"></plus><apply id="S3.p11.3.m3.1.1.1.1.1.2.cmml" xref="S3.p11.3.m3.1.1.1.1.1.2"><times id="S3.p11.3.m3.1.1.1.1.1.2.1.cmml" xref="S3.p11.3.m3.1.1.1.1.1.2.1"></times><ci id="S3.p11.3.m3.1.1.1.1.1.2.2.cmml" xref="S3.p11.3.m3.1.1.1.1.1.2.2">𝐼</ci><apply id="S3.p11.3.m3.1.1.1.1.1.2.3.cmml" xref="S3.p11.3.m3.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.p11.3.m3.1.1.1.1.1.2.3.1.cmml" xref="S3.p11.3.m3.1.1.1.1.1.2.3">superscript</csymbol><ci id="S3.p11.3.m3.1.1.1.1.1.2.3.2.cmml" xref="S3.p11.3.m3.1.1.1.1.1.2.3.2">𝐾</ci><cn type="integer" id="S3.p11.3.m3.1.1.1.1.1.2.3.3.cmml" xref="S3.p11.3.m3.1.1.1.1.1.2.3.3">2</cn></apply></apply><cn type="integer" id="S3.p11.3.m3.1.1.1.1.1.3.cmml" xref="S3.p11.3.m3.1.1.1.1.1.3">1</cn><ci id="S3.p11.3.m3.1.1.1.1.1.4.cmml" xref="S3.p11.3.m3.1.1.1.1.1.4">𝑂</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p11.3.m3.1c">R(IK^{2}+1+O)</annotation></semantics></math> is smaller than
<math id="S3.p11.4.m4.1" class="ltx_Math" alttext="OIK^{2}" display="inline"><semantics id="S3.p11.4.m4.1a"><mrow id="S3.p11.4.m4.1.1" xref="S3.p11.4.m4.1.1.cmml"><mi id="S3.p11.4.m4.1.1.2" xref="S3.p11.4.m4.1.1.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.p11.4.m4.1.1.1" xref="S3.p11.4.m4.1.1.1.cmml">​</mo><mi id="S3.p11.4.m4.1.1.3" xref="S3.p11.4.m4.1.1.3.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.p11.4.m4.1.1.1a" xref="S3.p11.4.m4.1.1.1.cmml">​</mo><msup id="S3.p11.4.m4.1.1.4" xref="S3.p11.4.m4.1.1.4.cmml"><mi id="S3.p11.4.m4.1.1.4.2" xref="S3.p11.4.m4.1.1.4.2.cmml">K</mi><mn id="S3.p11.4.m4.1.1.4.3" xref="S3.p11.4.m4.1.1.4.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p11.4.m4.1b"><apply id="S3.p11.4.m4.1.1.cmml" xref="S3.p11.4.m4.1.1"><times id="S3.p11.4.m4.1.1.1.cmml" xref="S3.p11.4.m4.1.1.1"></times><ci id="S3.p11.4.m4.1.1.2.cmml" xref="S3.p11.4.m4.1.1.2">𝑂</ci><ci id="S3.p11.4.m4.1.1.3.cmml" xref="S3.p11.4.m4.1.1.3">𝐼</ci><apply id="S3.p11.4.m4.1.1.4.cmml" xref="S3.p11.4.m4.1.1.4"><csymbol cd="ambiguous" id="S3.p11.4.m4.1.1.4.1.cmml" xref="S3.p11.4.m4.1.1.4">superscript</csymbol><ci id="S3.p11.4.m4.1.1.4.2.cmml" xref="S3.p11.4.m4.1.1.4.2">𝐾</ci><cn type="integer" id="S3.p11.4.m4.1.1.4.3.cmml" xref="S3.p11.4.m4.1.1.4.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p11.4.m4.1c">OIK^{2}</annotation></semantics></math>, particularly when the rank of SVD decreases.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>The Number of Parameters Difference between original Model Weight and Low-Rank SVD</figcaption>
<table id="S3.T1.12" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T1.12.13" class="ltx_tr">
<td id="S3.T1.12.13.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T1.12.13.1.1" class="ltx_text ltx_font_bold">W</span></td>
<td id="S3.T1.12.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.12.13.2.1" class="ltx_text ltx_font_bold">Origin</span></td>
<td id="S3.T1.12.13.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S3.T1.12.13.3.1" class="ltx_text ltx_font_bold">Reshaped</span></td>
<td id="S3.T1.12.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.12.13.4.1" class="ltx_text ltx_font_bold">U</span></td>
<td id="S3.T1.12.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.12.13.5.1" class="ltx_text ltx_font_bold">S</span></td>
<td id="S3.T1.12.13.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.12.13.6.1" class="ltx_text ltx_font_bold">V</span></td>
<td id="S3.T1.12.13.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.12.13.7.1" class="ltx_text ltx_font_bold">U+S+V</span></td>
</tr>
<tr id="S3.T1.6.6" class="ltx_tr">
<td id="S3.T1.6.6.7" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T1.6.6.7.1" class="ltx_text ltx_font_bold">Shape</span></td>
<td id="S3.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T1.1.1.1.m1.4" class="ltx_Math" alttext="{[}O,I,K,K{]}" display="inline"><semantics id="S3.T1.1.1.1.m1.4a"><mrow id="S3.T1.1.1.1.m1.4.5.2" xref="S3.T1.1.1.1.m1.4.5.1.cmml"><mo stretchy="false" id="S3.T1.1.1.1.m1.4.5.2.1" xref="S3.T1.1.1.1.m1.4.5.1.cmml">[</mo><mi id="S3.T1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.m1.1.1.cmml">O</mi><mo id="S3.T1.1.1.1.m1.4.5.2.2" xref="S3.T1.1.1.1.m1.4.5.1.cmml">,</mo><mi id="S3.T1.1.1.1.m1.2.2" xref="S3.T1.1.1.1.m1.2.2.cmml">I</mi><mo id="S3.T1.1.1.1.m1.4.5.2.3" xref="S3.T1.1.1.1.m1.4.5.1.cmml">,</mo><mi id="S3.T1.1.1.1.m1.3.3" xref="S3.T1.1.1.1.m1.3.3.cmml">K</mi><mo id="S3.T1.1.1.1.m1.4.5.2.4" xref="S3.T1.1.1.1.m1.4.5.1.cmml">,</mo><mi id="S3.T1.1.1.1.m1.4.4" xref="S3.T1.1.1.1.m1.4.4.cmml">K</mi><mo stretchy="false" id="S3.T1.1.1.1.m1.4.5.2.5" xref="S3.T1.1.1.1.m1.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.4b"><list id="S3.T1.1.1.1.m1.4.5.1.cmml" xref="S3.T1.1.1.1.m1.4.5.2"><ci id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1">𝑂</ci><ci id="S3.T1.1.1.1.m1.2.2.cmml" xref="S3.T1.1.1.1.m1.2.2">𝐼</ci><ci id="S3.T1.1.1.1.m1.3.3.cmml" xref="S3.T1.1.1.1.m1.3.3">𝐾</ci><ci id="S3.T1.1.1.1.m1.4.4.cmml" xref="S3.T1.1.1.1.m1.4.4">𝐾</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.4c">{[}O,I,K,K{]}</annotation></semantics></math></td>
<td id="S3.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><math id="S3.T1.2.2.2.m1.2" class="ltx_Math" alttext="{[}IK^{2},O{]}" display="inline"><semantics id="S3.T1.2.2.2.m1.2a"><mrow id="S3.T1.2.2.2.m1.2.2.1" xref="S3.T1.2.2.2.m1.2.2.2.cmml"><mo stretchy="false" id="S3.T1.2.2.2.m1.2.2.1.2" xref="S3.T1.2.2.2.m1.2.2.2.cmml">[</mo><mrow id="S3.T1.2.2.2.m1.2.2.1.1" xref="S3.T1.2.2.2.m1.2.2.1.1.cmml"><mi id="S3.T1.2.2.2.m1.2.2.1.1.2" xref="S3.T1.2.2.2.m1.2.2.1.1.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.T1.2.2.2.m1.2.2.1.1.1" xref="S3.T1.2.2.2.m1.2.2.1.1.1.cmml">​</mo><msup id="S3.T1.2.2.2.m1.2.2.1.1.3" xref="S3.T1.2.2.2.m1.2.2.1.1.3.cmml"><mi id="S3.T1.2.2.2.m1.2.2.1.1.3.2" xref="S3.T1.2.2.2.m1.2.2.1.1.3.2.cmml">K</mi><mn id="S3.T1.2.2.2.m1.2.2.1.1.3.3" xref="S3.T1.2.2.2.m1.2.2.1.1.3.3.cmml">2</mn></msup></mrow><mo id="S3.T1.2.2.2.m1.2.2.1.3" xref="S3.T1.2.2.2.m1.2.2.2.cmml">,</mo><mi id="S3.T1.2.2.2.m1.1.1" xref="S3.T1.2.2.2.m1.1.1.cmml">O</mi><mo stretchy="false" id="S3.T1.2.2.2.m1.2.2.1.4" xref="S3.T1.2.2.2.m1.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.m1.2b"><interval closure="closed" id="S3.T1.2.2.2.m1.2.2.2.cmml" xref="S3.T1.2.2.2.m1.2.2.1"><apply id="S3.T1.2.2.2.m1.2.2.1.1.cmml" xref="S3.T1.2.2.2.m1.2.2.1.1"><times id="S3.T1.2.2.2.m1.2.2.1.1.1.cmml" xref="S3.T1.2.2.2.m1.2.2.1.1.1"></times><ci id="S3.T1.2.2.2.m1.2.2.1.1.2.cmml" xref="S3.T1.2.2.2.m1.2.2.1.1.2">𝐼</ci><apply id="S3.T1.2.2.2.m1.2.2.1.1.3.cmml" xref="S3.T1.2.2.2.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.T1.2.2.2.m1.2.2.1.1.3.1.cmml" xref="S3.T1.2.2.2.m1.2.2.1.1.3">superscript</csymbol><ci id="S3.T1.2.2.2.m1.2.2.1.1.3.2.cmml" xref="S3.T1.2.2.2.m1.2.2.1.1.3.2">𝐾</ci><cn type="integer" id="S3.T1.2.2.2.m1.2.2.1.1.3.3.cmml" xref="S3.T1.2.2.2.m1.2.2.1.1.3.3">2</cn></apply></apply><ci id="S3.T1.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.m1.1.1">𝑂</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.m1.2c">{[}IK^{2},O{]}</annotation></semantics></math></td>
<td id="S3.T1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T1.3.3.3.m1.2" class="ltx_Math" alttext="{[}IK^{2},R{]}" display="inline"><semantics id="S3.T1.3.3.3.m1.2a"><mrow id="S3.T1.3.3.3.m1.2.2.1" xref="S3.T1.3.3.3.m1.2.2.2.cmml"><mo stretchy="false" id="S3.T1.3.3.3.m1.2.2.1.2" xref="S3.T1.3.3.3.m1.2.2.2.cmml">[</mo><mrow id="S3.T1.3.3.3.m1.2.2.1.1" xref="S3.T1.3.3.3.m1.2.2.1.1.cmml"><mi id="S3.T1.3.3.3.m1.2.2.1.1.2" xref="S3.T1.3.3.3.m1.2.2.1.1.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.T1.3.3.3.m1.2.2.1.1.1" xref="S3.T1.3.3.3.m1.2.2.1.1.1.cmml">​</mo><msup id="S3.T1.3.3.3.m1.2.2.1.1.3" xref="S3.T1.3.3.3.m1.2.2.1.1.3.cmml"><mi id="S3.T1.3.3.3.m1.2.2.1.1.3.2" xref="S3.T1.3.3.3.m1.2.2.1.1.3.2.cmml">K</mi><mn id="S3.T1.3.3.3.m1.2.2.1.1.3.3" xref="S3.T1.3.3.3.m1.2.2.1.1.3.3.cmml">2</mn></msup></mrow><mo id="S3.T1.3.3.3.m1.2.2.1.3" xref="S3.T1.3.3.3.m1.2.2.2.cmml">,</mo><mi id="S3.T1.3.3.3.m1.1.1" xref="S3.T1.3.3.3.m1.1.1.cmml">R</mi><mo stretchy="false" id="S3.T1.3.3.3.m1.2.2.1.4" xref="S3.T1.3.3.3.m1.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.m1.2b"><interval closure="closed" id="S3.T1.3.3.3.m1.2.2.2.cmml" xref="S3.T1.3.3.3.m1.2.2.1"><apply id="S3.T1.3.3.3.m1.2.2.1.1.cmml" xref="S3.T1.3.3.3.m1.2.2.1.1"><times id="S3.T1.3.3.3.m1.2.2.1.1.1.cmml" xref="S3.T1.3.3.3.m1.2.2.1.1.1"></times><ci id="S3.T1.3.3.3.m1.2.2.1.1.2.cmml" xref="S3.T1.3.3.3.m1.2.2.1.1.2">𝐼</ci><apply id="S3.T1.3.3.3.m1.2.2.1.1.3.cmml" xref="S3.T1.3.3.3.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.T1.3.3.3.m1.2.2.1.1.3.1.cmml" xref="S3.T1.3.3.3.m1.2.2.1.1.3">superscript</csymbol><ci id="S3.T1.3.3.3.m1.2.2.1.1.3.2.cmml" xref="S3.T1.3.3.3.m1.2.2.1.1.3.2">𝐾</ci><cn type="integer" id="S3.T1.3.3.3.m1.2.2.1.1.3.3.cmml" xref="S3.T1.3.3.3.m1.2.2.1.1.3.3">2</cn></apply></apply><ci id="S3.T1.3.3.3.m1.1.1.cmml" xref="S3.T1.3.3.3.m1.1.1">𝑅</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.m1.2c">{[}IK^{2},R{]}</annotation></semantics></math></td>
<td id="S3.T1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T1.4.4.4.m1.1" class="ltx_Math" alttext="{[}R{]}" display="inline"><semantics id="S3.T1.4.4.4.m1.1a"><mrow id="S3.T1.4.4.4.m1.1.2.2" xref="S3.T1.4.4.4.m1.1.2.1.cmml"><mo stretchy="false" id="S3.T1.4.4.4.m1.1.2.2.1" xref="S3.T1.4.4.4.m1.1.2.1.1.cmml">[</mo><mi id="S3.T1.4.4.4.m1.1.1" xref="S3.T1.4.4.4.m1.1.1.cmml">R</mi><mo stretchy="false" id="S3.T1.4.4.4.m1.1.2.2.2" xref="S3.T1.4.4.4.m1.1.2.1.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.m1.1b"><apply id="S3.T1.4.4.4.m1.1.2.1.cmml" xref="S3.T1.4.4.4.m1.1.2.2"><csymbol cd="latexml" id="S3.T1.4.4.4.m1.1.2.1.1.cmml" xref="S3.T1.4.4.4.m1.1.2.2.1">delimited-[]</csymbol><ci id="S3.T1.4.4.4.m1.1.1.cmml" xref="S3.T1.4.4.4.m1.1.1">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.m1.1c">{[}R{]}</annotation></semantics></math></td>
<td id="S3.T1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T1.5.5.5.m1.2" class="ltx_Math" alttext="{[}O,R{]}" display="inline"><semantics id="S3.T1.5.5.5.m1.2a"><mrow id="S3.T1.5.5.5.m1.2.3.2" xref="S3.T1.5.5.5.m1.2.3.1.cmml"><mo stretchy="false" id="S3.T1.5.5.5.m1.2.3.2.1" xref="S3.T1.5.5.5.m1.2.3.1.cmml">[</mo><mi id="S3.T1.5.5.5.m1.1.1" xref="S3.T1.5.5.5.m1.1.1.cmml">O</mi><mo id="S3.T1.5.5.5.m1.2.3.2.2" xref="S3.T1.5.5.5.m1.2.3.1.cmml">,</mo><mi id="S3.T1.5.5.5.m1.2.2" xref="S3.T1.5.5.5.m1.2.2.cmml">R</mi><mo stretchy="false" id="S3.T1.5.5.5.m1.2.3.2.3" xref="S3.T1.5.5.5.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.m1.2b"><interval closure="closed" id="S3.T1.5.5.5.m1.2.3.1.cmml" xref="S3.T1.5.5.5.m1.2.3.2"><ci id="S3.T1.5.5.5.m1.1.1.cmml" xref="S3.T1.5.5.5.m1.1.1">𝑂</ci><ci id="S3.T1.5.5.5.m1.2.2.cmml" xref="S3.T1.5.5.5.m1.2.2">𝑅</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.m1.2c">{[}O,R{]}</annotation></semantics></math></td>
<td id="S3.T1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S3.T1.6.6.6.m1.2" class="ltx_Math" alttext="{[}IK^{2}+1+O,R{]}" display="inline"><semantics id="S3.T1.6.6.6.m1.2a"><mrow id="S3.T1.6.6.6.m1.2.2.1" xref="S3.T1.6.6.6.m1.2.2.2.cmml"><mo stretchy="false" id="S3.T1.6.6.6.m1.2.2.1.2" xref="S3.T1.6.6.6.m1.2.2.2.cmml">[</mo><mrow id="S3.T1.6.6.6.m1.2.2.1.1" xref="S3.T1.6.6.6.m1.2.2.1.1.cmml"><mrow id="S3.T1.6.6.6.m1.2.2.1.1.2" xref="S3.T1.6.6.6.m1.2.2.1.1.2.cmml"><mi id="S3.T1.6.6.6.m1.2.2.1.1.2.2" xref="S3.T1.6.6.6.m1.2.2.1.1.2.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.T1.6.6.6.m1.2.2.1.1.2.1" xref="S3.T1.6.6.6.m1.2.2.1.1.2.1.cmml">​</mo><msup id="S3.T1.6.6.6.m1.2.2.1.1.2.3" xref="S3.T1.6.6.6.m1.2.2.1.1.2.3.cmml"><mi id="S3.T1.6.6.6.m1.2.2.1.1.2.3.2" xref="S3.T1.6.6.6.m1.2.2.1.1.2.3.2.cmml">K</mi><mn id="S3.T1.6.6.6.m1.2.2.1.1.2.3.3" xref="S3.T1.6.6.6.m1.2.2.1.1.2.3.3.cmml">2</mn></msup></mrow><mo id="S3.T1.6.6.6.m1.2.2.1.1.1" xref="S3.T1.6.6.6.m1.2.2.1.1.1.cmml">+</mo><mn id="S3.T1.6.6.6.m1.2.2.1.1.3" xref="S3.T1.6.6.6.m1.2.2.1.1.3.cmml">1</mn><mo id="S3.T1.6.6.6.m1.2.2.1.1.1a" xref="S3.T1.6.6.6.m1.2.2.1.1.1.cmml">+</mo><mi id="S3.T1.6.6.6.m1.2.2.1.1.4" xref="S3.T1.6.6.6.m1.2.2.1.1.4.cmml">O</mi></mrow><mo id="S3.T1.6.6.6.m1.2.2.1.3" xref="S3.T1.6.6.6.m1.2.2.2.cmml">,</mo><mi id="S3.T1.6.6.6.m1.1.1" xref="S3.T1.6.6.6.m1.1.1.cmml">R</mi><mo stretchy="false" id="S3.T1.6.6.6.m1.2.2.1.4" xref="S3.T1.6.6.6.m1.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.6.m1.2b"><interval closure="closed" id="S3.T1.6.6.6.m1.2.2.2.cmml" xref="S3.T1.6.6.6.m1.2.2.1"><apply id="S3.T1.6.6.6.m1.2.2.1.1.cmml" xref="S3.T1.6.6.6.m1.2.2.1.1"><plus id="S3.T1.6.6.6.m1.2.2.1.1.1.cmml" xref="S3.T1.6.6.6.m1.2.2.1.1.1"></plus><apply id="S3.T1.6.6.6.m1.2.2.1.1.2.cmml" xref="S3.T1.6.6.6.m1.2.2.1.1.2"><times id="S3.T1.6.6.6.m1.2.2.1.1.2.1.cmml" xref="S3.T1.6.6.6.m1.2.2.1.1.2.1"></times><ci id="S3.T1.6.6.6.m1.2.2.1.1.2.2.cmml" xref="S3.T1.6.6.6.m1.2.2.1.1.2.2">𝐼</ci><apply id="S3.T1.6.6.6.m1.2.2.1.1.2.3.cmml" xref="S3.T1.6.6.6.m1.2.2.1.1.2.3"><csymbol cd="ambiguous" id="S3.T1.6.6.6.m1.2.2.1.1.2.3.1.cmml" xref="S3.T1.6.6.6.m1.2.2.1.1.2.3">superscript</csymbol><ci id="S3.T1.6.6.6.m1.2.2.1.1.2.3.2.cmml" xref="S3.T1.6.6.6.m1.2.2.1.1.2.3.2">𝐾</ci><cn type="integer" id="S3.T1.6.6.6.m1.2.2.1.1.2.3.3.cmml" xref="S3.T1.6.6.6.m1.2.2.1.1.2.3.3">2</cn></apply></apply><cn type="integer" id="S3.T1.6.6.6.m1.2.2.1.1.3.cmml" xref="S3.T1.6.6.6.m1.2.2.1.1.3">1</cn><ci id="S3.T1.6.6.6.m1.2.2.1.1.4.cmml" xref="S3.T1.6.6.6.m1.2.2.1.1.4">𝑂</ci></apply><ci id="S3.T1.6.6.6.m1.1.1.cmml" xref="S3.T1.6.6.6.m1.1.1">𝑅</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.6.m1.2c">{[}IK^{2}+1+O,R{]}</annotation></semantics></math></td>
</tr>
<tr id="S3.T1.12.12" class="ltx_tr">
<td id="S3.T1.12.12.7" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T1.12.12.7.1" class="ltx_text ltx_font_bold">Parameters</span></td>
<td id="S3.T1.7.7.1" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><math id="S3.T1.7.7.1.m1.1" class="ltx_Math" alttext="OIK^{2}" display="inline"><semantics id="S3.T1.7.7.1.m1.1a"><mrow id="S3.T1.7.7.1.m1.1.1" xref="S3.T1.7.7.1.m1.1.1.cmml"><mi id="S3.T1.7.7.1.m1.1.1.2" xref="S3.T1.7.7.1.m1.1.1.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.T1.7.7.1.m1.1.1.1" xref="S3.T1.7.7.1.m1.1.1.1.cmml">​</mo><mi id="S3.T1.7.7.1.m1.1.1.3" xref="S3.T1.7.7.1.m1.1.1.3.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.T1.7.7.1.m1.1.1.1a" xref="S3.T1.7.7.1.m1.1.1.1.cmml">​</mo><msup id="S3.T1.7.7.1.m1.1.1.4" xref="S3.T1.7.7.1.m1.1.1.4.cmml"><mi id="S3.T1.7.7.1.m1.1.1.4.2" xref="S3.T1.7.7.1.m1.1.1.4.2.cmml">K</mi><mn id="S3.T1.7.7.1.m1.1.1.4.3" xref="S3.T1.7.7.1.m1.1.1.4.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.7.7.1.m1.1b"><apply id="S3.T1.7.7.1.m1.1.1.cmml" xref="S3.T1.7.7.1.m1.1.1"><times id="S3.T1.7.7.1.m1.1.1.1.cmml" xref="S3.T1.7.7.1.m1.1.1.1"></times><ci id="S3.T1.7.7.1.m1.1.1.2.cmml" xref="S3.T1.7.7.1.m1.1.1.2">𝑂</ci><ci id="S3.T1.7.7.1.m1.1.1.3.cmml" xref="S3.T1.7.7.1.m1.1.1.3">𝐼</ci><apply id="S3.T1.7.7.1.m1.1.1.4.cmml" xref="S3.T1.7.7.1.m1.1.1.4"><csymbol cd="ambiguous" id="S3.T1.7.7.1.m1.1.1.4.1.cmml" xref="S3.T1.7.7.1.m1.1.1.4">superscript</csymbol><ci id="S3.T1.7.7.1.m1.1.1.4.2.cmml" xref="S3.T1.7.7.1.m1.1.1.4.2">𝐾</ci><cn type="integer" id="S3.T1.7.7.1.m1.1.1.4.3.cmml" xref="S3.T1.7.7.1.m1.1.1.4.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.7.1.m1.1c">OIK^{2}</annotation></semantics></math></td>
<td id="S3.T1.8.8.2" class="ltx_td ltx_align_right ltx_border_rr ltx_border_t"><math id="S3.T1.8.8.2.m1.1" class="ltx_Math" alttext="OIK^{2}" display="inline"><semantics id="S3.T1.8.8.2.m1.1a"><mrow id="S3.T1.8.8.2.m1.1.1" xref="S3.T1.8.8.2.m1.1.1.cmml"><mi id="S3.T1.8.8.2.m1.1.1.2" xref="S3.T1.8.8.2.m1.1.1.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.T1.8.8.2.m1.1.1.1" xref="S3.T1.8.8.2.m1.1.1.1.cmml">​</mo><mi id="S3.T1.8.8.2.m1.1.1.3" xref="S3.T1.8.8.2.m1.1.1.3.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.T1.8.8.2.m1.1.1.1a" xref="S3.T1.8.8.2.m1.1.1.1.cmml">​</mo><msup id="S3.T1.8.8.2.m1.1.1.4" xref="S3.T1.8.8.2.m1.1.1.4.cmml"><mi id="S3.T1.8.8.2.m1.1.1.4.2" xref="S3.T1.8.8.2.m1.1.1.4.2.cmml">K</mi><mn id="S3.T1.8.8.2.m1.1.1.4.3" xref="S3.T1.8.8.2.m1.1.1.4.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.8.8.2.m1.1b"><apply id="S3.T1.8.8.2.m1.1.1.cmml" xref="S3.T1.8.8.2.m1.1.1"><times id="S3.T1.8.8.2.m1.1.1.1.cmml" xref="S3.T1.8.8.2.m1.1.1.1"></times><ci id="S3.T1.8.8.2.m1.1.1.2.cmml" xref="S3.T1.8.8.2.m1.1.1.2">𝑂</ci><ci id="S3.T1.8.8.2.m1.1.1.3.cmml" xref="S3.T1.8.8.2.m1.1.1.3">𝐼</ci><apply id="S3.T1.8.8.2.m1.1.1.4.cmml" xref="S3.T1.8.8.2.m1.1.1.4"><csymbol cd="ambiguous" id="S3.T1.8.8.2.m1.1.1.4.1.cmml" xref="S3.T1.8.8.2.m1.1.1.4">superscript</csymbol><ci id="S3.T1.8.8.2.m1.1.1.4.2.cmml" xref="S3.T1.8.8.2.m1.1.1.4.2">𝐾</ci><cn type="integer" id="S3.T1.8.8.2.m1.1.1.4.3.cmml" xref="S3.T1.8.8.2.m1.1.1.4.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.8.2.m1.1c">OIK^{2}</annotation></semantics></math></td>
<td id="S3.T1.9.9.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><math id="S3.T1.9.9.3.m1.1" class="ltx_Math" alttext="RIK^{2}" display="inline"><semantics id="S3.T1.9.9.3.m1.1a"><mrow id="S3.T1.9.9.3.m1.1.1" xref="S3.T1.9.9.3.m1.1.1.cmml"><mi id="S3.T1.9.9.3.m1.1.1.2" xref="S3.T1.9.9.3.m1.1.1.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.T1.9.9.3.m1.1.1.1" xref="S3.T1.9.9.3.m1.1.1.1.cmml">​</mo><mi id="S3.T1.9.9.3.m1.1.1.3" xref="S3.T1.9.9.3.m1.1.1.3.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.T1.9.9.3.m1.1.1.1a" xref="S3.T1.9.9.3.m1.1.1.1.cmml">​</mo><msup id="S3.T1.9.9.3.m1.1.1.4" xref="S3.T1.9.9.3.m1.1.1.4.cmml"><mi id="S3.T1.9.9.3.m1.1.1.4.2" xref="S3.T1.9.9.3.m1.1.1.4.2.cmml">K</mi><mn id="S3.T1.9.9.3.m1.1.1.4.3" xref="S3.T1.9.9.3.m1.1.1.4.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.9.9.3.m1.1b"><apply id="S3.T1.9.9.3.m1.1.1.cmml" xref="S3.T1.9.9.3.m1.1.1"><times id="S3.T1.9.9.3.m1.1.1.1.cmml" xref="S3.T1.9.9.3.m1.1.1.1"></times><ci id="S3.T1.9.9.3.m1.1.1.2.cmml" xref="S3.T1.9.9.3.m1.1.1.2">𝑅</ci><ci id="S3.T1.9.9.3.m1.1.1.3.cmml" xref="S3.T1.9.9.3.m1.1.1.3">𝐼</ci><apply id="S3.T1.9.9.3.m1.1.1.4.cmml" xref="S3.T1.9.9.3.m1.1.1.4"><csymbol cd="ambiguous" id="S3.T1.9.9.3.m1.1.1.4.1.cmml" xref="S3.T1.9.9.3.m1.1.1.4">superscript</csymbol><ci id="S3.T1.9.9.3.m1.1.1.4.2.cmml" xref="S3.T1.9.9.3.m1.1.1.4.2">𝐾</ci><cn type="integer" id="S3.T1.9.9.3.m1.1.1.4.3.cmml" xref="S3.T1.9.9.3.m1.1.1.4.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.9.3.m1.1c">RIK^{2}</annotation></semantics></math></td>
<td id="S3.T1.10.10.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><math id="S3.T1.10.10.4.m1.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S3.T1.10.10.4.m1.1a"><mi id="S3.T1.10.10.4.m1.1.1" xref="S3.T1.10.10.4.m1.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.T1.10.10.4.m1.1b"><ci id="S3.T1.10.10.4.m1.1.1.cmml" xref="S3.T1.10.10.4.m1.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.10.10.4.m1.1c">R</annotation></semantics></math></td>
<td id="S3.T1.11.11.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><math id="S3.T1.11.11.5.m1.1" class="ltx_Math" alttext="OR" display="inline"><semantics id="S3.T1.11.11.5.m1.1a"><mrow id="S3.T1.11.11.5.m1.1.1" xref="S3.T1.11.11.5.m1.1.1.cmml"><mi id="S3.T1.11.11.5.m1.1.1.2" xref="S3.T1.11.11.5.m1.1.1.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.T1.11.11.5.m1.1.1.1" xref="S3.T1.11.11.5.m1.1.1.1.cmml">​</mo><mi id="S3.T1.11.11.5.m1.1.1.3" xref="S3.T1.11.11.5.m1.1.1.3.cmml">R</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.11.11.5.m1.1b"><apply id="S3.T1.11.11.5.m1.1.1.cmml" xref="S3.T1.11.11.5.m1.1.1"><times id="S3.T1.11.11.5.m1.1.1.1.cmml" xref="S3.T1.11.11.5.m1.1.1.1"></times><ci id="S3.T1.11.11.5.m1.1.1.2.cmml" xref="S3.T1.11.11.5.m1.1.1.2">𝑂</ci><ci id="S3.T1.11.11.5.m1.1.1.3.cmml" xref="S3.T1.11.11.5.m1.1.1.3">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.11.11.5.m1.1c">OR</annotation></semantics></math></td>
<td id="S3.T1.12.12.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><math id="S3.T1.12.12.6.m1.1" class="ltx_Math" alttext="R(IK^{2}+1+O)" display="inline"><semantics id="S3.T1.12.12.6.m1.1a"><mrow id="S3.T1.12.12.6.m1.1.1" xref="S3.T1.12.12.6.m1.1.1.cmml"><mi id="S3.T1.12.12.6.m1.1.1.3" xref="S3.T1.12.12.6.m1.1.1.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.T1.12.12.6.m1.1.1.2" xref="S3.T1.12.12.6.m1.1.1.2.cmml">​</mo><mrow id="S3.T1.12.12.6.m1.1.1.1.1" xref="S3.T1.12.12.6.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.T1.12.12.6.m1.1.1.1.1.2" xref="S3.T1.12.12.6.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.T1.12.12.6.m1.1.1.1.1.1" xref="S3.T1.12.12.6.m1.1.1.1.1.1.cmml"><mrow id="S3.T1.12.12.6.m1.1.1.1.1.1.2" xref="S3.T1.12.12.6.m1.1.1.1.1.1.2.cmml"><mi id="S3.T1.12.12.6.m1.1.1.1.1.1.2.2" xref="S3.T1.12.12.6.m1.1.1.1.1.1.2.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.T1.12.12.6.m1.1.1.1.1.1.2.1" xref="S3.T1.12.12.6.m1.1.1.1.1.1.2.1.cmml">​</mo><msup id="S3.T1.12.12.6.m1.1.1.1.1.1.2.3" xref="S3.T1.12.12.6.m1.1.1.1.1.1.2.3.cmml"><mi id="S3.T1.12.12.6.m1.1.1.1.1.1.2.3.2" xref="S3.T1.12.12.6.m1.1.1.1.1.1.2.3.2.cmml">K</mi><mn id="S3.T1.12.12.6.m1.1.1.1.1.1.2.3.3" xref="S3.T1.12.12.6.m1.1.1.1.1.1.2.3.3.cmml">2</mn></msup></mrow><mo id="S3.T1.12.12.6.m1.1.1.1.1.1.1" xref="S3.T1.12.12.6.m1.1.1.1.1.1.1.cmml">+</mo><mn id="S3.T1.12.12.6.m1.1.1.1.1.1.3" xref="S3.T1.12.12.6.m1.1.1.1.1.1.3.cmml">1</mn><mo id="S3.T1.12.12.6.m1.1.1.1.1.1.1a" xref="S3.T1.12.12.6.m1.1.1.1.1.1.1.cmml">+</mo><mi id="S3.T1.12.12.6.m1.1.1.1.1.1.4" xref="S3.T1.12.12.6.m1.1.1.1.1.1.4.cmml">O</mi></mrow><mo stretchy="false" id="S3.T1.12.12.6.m1.1.1.1.1.3" xref="S3.T1.12.12.6.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.12.12.6.m1.1b"><apply id="S3.T1.12.12.6.m1.1.1.cmml" xref="S3.T1.12.12.6.m1.1.1"><times id="S3.T1.12.12.6.m1.1.1.2.cmml" xref="S3.T1.12.12.6.m1.1.1.2"></times><ci id="S3.T1.12.12.6.m1.1.1.3.cmml" xref="S3.T1.12.12.6.m1.1.1.3">𝑅</ci><apply id="S3.T1.12.12.6.m1.1.1.1.1.1.cmml" xref="S3.T1.12.12.6.m1.1.1.1.1"><plus id="S3.T1.12.12.6.m1.1.1.1.1.1.1.cmml" xref="S3.T1.12.12.6.m1.1.1.1.1.1.1"></plus><apply id="S3.T1.12.12.6.m1.1.1.1.1.1.2.cmml" xref="S3.T1.12.12.6.m1.1.1.1.1.1.2"><times id="S3.T1.12.12.6.m1.1.1.1.1.1.2.1.cmml" xref="S3.T1.12.12.6.m1.1.1.1.1.1.2.1"></times><ci id="S3.T1.12.12.6.m1.1.1.1.1.1.2.2.cmml" xref="S3.T1.12.12.6.m1.1.1.1.1.1.2.2">𝐼</ci><apply id="S3.T1.12.12.6.m1.1.1.1.1.1.2.3.cmml" xref="S3.T1.12.12.6.m1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.T1.12.12.6.m1.1.1.1.1.1.2.3.1.cmml" xref="S3.T1.12.12.6.m1.1.1.1.1.1.2.3">superscript</csymbol><ci id="S3.T1.12.12.6.m1.1.1.1.1.1.2.3.2.cmml" xref="S3.T1.12.12.6.m1.1.1.1.1.1.2.3.2">𝐾</ci><cn type="integer" id="S3.T1.12.12.6.m1.1.1.1.1.1.2.3.3.cmml" xref="S3.T1.12.12.6.m1.1.1.1.1.1.2.3.3">2</cn></apply></apply><cn type="integer" id="S3.T1.12.12.6.m1.1.1.1.1.1.3.cmml" xref="S3.T1.12.12.6.m1.1.1.1.1.1.3">1</cn><ci id="S3.T1.12.12.6.m1.1.1.1.1.1.4.cmml" xref="S3.T1.12.12.6.m1.1.1.1.1.1.4">𝑂</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.12.12.6.m1.1c">R(IK^{2}+1+O)</annotation></semantics></math></td>
</tr>
<tr id="S3.T1.12.14" class="ltx_tr">
<td id="S3.T1.12.14.1" class="ltx_td ltx_align_left ltx_border_t" colspan="7">I : Input Channels, O : Output Channels, K : Kernel Size, R : Rank of SVD</td>
</tr>
</table>
</figure>
<div id="S3.p12" class="ltx_para ltx_noindent">
<p id="S3.p12.1" class="ltx_p"><span id="S3.p12.1.1" class="ltx_text ltx_font_bold">Hyperparameters
<br class="ltx_break"></span>Hyperparameters are parameters that are not learned during the training of a machine learning model, but rather
set by the practitioner prior to training. These parameters govern the behavior of the learning algorithm and
can significantly affect the performance of the model. In YOLOv7, there are several hyperparameters that can be
tuned to achieve optimal performance. Among these hyperparameters, some of the most significant ones include
the batch size, learning rate, and weight decay.</p>
</div>
<div id="S3.p13" class="ltx_para ltx_noindent">
<p id="S3.p13.1" class="ltx_p">Batch size is the number of training examples used in each iteration of the optimization algorithm during training
of a machine learning model. It determines how many samples from the training dataset are processed at once before
the model weights are updated. The choice of batch size can affect the training process and the resulting model
performance. A larger batch size can speed up training but may require more memory and may lead to poorer generalization
performance. A smaller batch size may take longer to train but may lead to better generalization performance and smoother convergence.</p>
</div>
<div id="S3.p14" class="ltx_para ltx_noindent">
<p id="S3.p14.1" class="ltx_p">Learning rate is another important hyperparameter that controls the step size at which the optimization algorithm updates
the weights of the model during training. It determines the magnitude of the updates to the model weights in each iteration.
A high learning rate can cause the optimization algorithm to overshoot the optimal weights, leading to unstable training
and poor performance. A low learning rate can cause the optimization algorithm to converge too slowly, leading to longer
training times and potentially suboptimal performance.</p>
</div>
<div id="S3.p15" class="ltx_para ltx_noindent">
<p id="S3.p15.1" class="ltx_p">Weight decay is a regularization technique that is used to prevent overfitting in machine learning models. It involves
adding a penalty term to the loss function that encourages the model to learn simpler and more generalizable patterns
in the data. The strength of the penalty term is controlled by the weight decay hyperparameter. A high weight decay
can lead to overly simplified models that underfit the training data, while a low weight decay can lead to overfitting
and poor generalization performance.</p>
</div>
<div id="S3.p16" class="ltx_para ltx_noindent">
<p id="S3.p16.1" class="ltx_p">The appropriate choice of learning rate and weight decay depends on the specific dataset and network architecture being
used, as well as the optimization algorithm and other hyperparameters. In practice, it is often necessary to perform a
grid search or other hyperparameter tuning techniques to identify the optimal values for these hyperparameters. By
carefully selecting these values, a neural network can achieve better performance and generalization capabilities.</p>
</div>
<div id="S3.p17" class="ltx_para ltx_noindent">
<p id="S3.p17.1" class="ltx_p">During our experimentation, we evaluated several transfer learning approaches by varying the batch size during training.
We tested batch sizes of 1, 16, 32, and 64, and found that the model achieved the highest mean average precision (mAP)
when the batch size was set to 32. Based on these results, we selected a batch size of 32 for our study because it
achieved the best mAP and exhibited appropriate training speed and generalization performance. However, since our
research topic did not involve hyperparameter tuning, we used the default values of ’hyp.scratch.tiny.yaml’
provided by YOLOv7’s GitHub repository, which includes the default values of lr0:0.01, lrf:0.01, weight decay:0.0005, among other hyperparameters.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<figure id="S4.F2" class="ltx_figure"><img src="/html/2303.02735/assets/PR_curve_original.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="192" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>PR curve of unoptimized model on test dataset.</figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Frame Rates and mAP50 of Models with Weight Pruning and SVD</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T2.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Model</td>
<td id="S4.T2.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Frame Rate (FPS)</td>
<td id="S4.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">mAP@50</td>
<td id="S4.T2.1.1.4" class="ltx_td ltx_align_center ltx_border_t">Weight Size (MB)</td>
</tr>
<tr id="S4.T2.1.2" class="ltx_tr">
<td id="S4.T2.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Original</td>
<td id="S4.T2.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.50</td>
<td id="S4.T2.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.717</td>
<td id="S4.T2.1.2.4" class="ltx_td ltx_align_center ltx_border_t">12.3</td>
</tr>
<tr id="S4.T2.1.3" class="ltx_tr">
<td id="S4.T2.1.3.1" class="ltx_td ltx_align_center ltx_border_r">SVD only</td>
<td id="S4.T2.1.3.2" class="ltx_td ltx_align_center ltx_border_r">1.44</td>
<td id="S4.T2.1.3.3" class="ltx_td ltx_align_center ltx_border_r">0.717</td>
<td id="S4.T2.1.3.4" class="ltx_td ltx_align_center">12.1</td>
</tr>
<tr id="S4.T2.1.4" class="ltx_tr">
<td id="S4.T2.1.4.1" class="ltx_td ltx_border_r"></td>
<td id="S4.T2.1.4.2" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.1.4.3" class="ltx_td ltx_align_center ltx_border_r">0.677</td>
<td id="S4.T2.1.4.4" class="ltx_td ltx_align_center">9.92</td>
</tr>
<tr id="S4.T2.1.5" class="ltx_tr">
<td id="S4.T2.1.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Weight pruning + SVD</td>
<td id="S4.T2.1.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">1.48</td>
<td id="S4.T2.1.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.724</td>
<td id="S4.T2.1.5.4" class="ltx_td ltx_align_center ltx_border_b">12.1</td>
</tr>
</table>
</figure>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02735/assets/PR_curve_svd.png" id="S4.F3.sf1.g1" class="ltx_graphics ltx_img_landscape" width="287" height="192" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>SVD Optimized Model</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02735/assets/PR_curve-svd_prune.png" id="S4.F3.sf2.g1" class="ltx_graphics ltx_img_landscape" width="287" height="192" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>SVD + Weight Pruned Model</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Precision-Recall curves for comparison between SVD optimized model and SVD + weight pruned model.</figcaption>
</figure>
<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">Our experiments on standard object detection benchmarks demonstrate the effectiveness of weight pruning and singular value decomposition (SVD) for optimizing the performance of deep learning models. Table <a href="#S4.T2" title="Table 2 ‣ 4 Results ‣ Scalable Object Detection on Embedded Devices using Weight Pruning and Singular Value Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> summarizes the quantitative evaluation of the pruned and SVD-optimized models, as well as the comparison of their performance with the original unoptimized model. We observe that the SVD-optimized model achieves similar mAP@50 compared to the original model, but with a slightly lower frame rate. On the other hand, the weight pruning + SVD model outperforms both the original and SVD-optimized models in terms of mAP@50, while maintaining a comparable frame rate.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p">Furthermore, we analyze the trade-offs between model size, speed, and accuracy. We note that the weight pruning + SVD model has a similar weight size as the SVD-optimized model, which is significantly smaller than the original model. This reduction in model size is important for applications where storage and memory are limited. In terms of speed, the weight pruning + SVD model has a slightly lower frame rate than the original model, but still maintains a real-time detection capability. These results demonstrate the potential of using pruning and SVD techniques to optimize deep learning models for resource-constrained applications.</p>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.1" class="ltx_p">The precision-recall curves in Figure <a href="#S4.F3" title="Figure 3 ‣ 4 Results ‣ Scalable Object Detection on Embedded Devices using Weight Pruning and Singular Value Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> provide a visual representation of the performance of the object detection models. The PR curve for the original model in Figure <a href="#S4.F2" title="Figure 2 ‣ 4 Results ‣ Scalable Object Detection on Embedded Devices using Weight Pruning and Singular Value Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows that it achieves high precision at low recall values, but its performance drops as the recall increases. In contrast, the PR curves for the SVD-optimized and weight pruning + SVD models in Figure <a href="#S4.F3" title="Figure 3 ‣ 4 Results ‣ Scalable Object Detection on Embedded Devices using Weight Pruning and Singular Value Decomposition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(a) and (b), respectively, show that these models achieve better balance between precision and recall, resulting in higher mAP@50 values. Overall, our results highlight the potential of using pruning and SVD techniques to optimize deep learning models for real-world applications.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">The results of our study demonstrate that by using weight
pruning and SVD optimization techniques, we were able to
reduce the memory footprint of the model while balancing the speed and accuracy of the model.
The optimized models achieved comparable or even better mAP50 scores
on the Roboflow dataset compared to the original unoptimized model,
while running with smaller weight sizes. These results could have
important implications for real-world deployment on resource-constrained
devices like the Raspberry Pi, where computational resources and memory
are limited.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p">Our study is not the first to explore techniques for optimizing
object detection models for resource-constrained devices.
Other approaches include quantization, distillation, and network
compression, among others <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">polino2018model</span>]</cite>. While each of
these techniques has its own advantages and limitations,
our results demonstrate that weight pruning and SVD optimization
can be effective methods for reducing the size and computational
complexity of object detection models while maintaining or
improving their accuracy.</p>
</div>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.1" class="ltx_p">While our study shows promising results, there is still room for improvement and further research in this area. One possible direction for future research is to explore the combination of different optimization techniques to achieve even better results. For example, weight pruning and quantization could be combined to further reduce the size and computational complexity of models. Another area for future research is to investigate the transferability of optimized models across different datasets and object detection tasks.</p>
</div>
<div id="S5.p4" class="ltx_para ltx_noindent">
<p id="S5.p4.1" class="ltx_p">Additionally, it would be interesting to explore the impact of these optimization techniques on other types of computer vision models, such as image classification or semantic segmentation. Finally, the development of new optimization techniques specifically designed for resource-constrained devices could lead to further improvements in the performance and efficiency of computer vision models in real-world applications.</p>
</div>
<div id="S5.p5" class="ltx_para ltx_noindent">
<p id="S5.p5.1" class="ltx_p">While we observed no significant increase in FPS with the weight pruning method, if future experiments are conducted with the Raspberry Pi in headless mode (no GUI), there could be a meaningful increase in FPS due to the decrease in bottlenecks.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">Optimization techniques such as weight pruning and singular value decomposition (SVD) can have a significant impact on the real-world applications of object detection models. These real-time inference models are fundamental to computer vision tasks such as autonomous driving, surveillance systems, and medical imaging. The size and computational complexity of these models are critical factors that affect the speed and efficiency of the entire system.</p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p">In this study, we evaluated the effectiveness of weight pruning and SVD on the popular YOLO object detection model. We tested the optimized models on a Raspberry Pi platform, which is a popular low-power embedded system used in many real-world applications.</p>
</div>
<div id="S6.p3" class="ltx_para ltx_noindent">
<p id="S6.p3.1" class="ltx_p">Our results showed that using weight pruning and SVD techniques, we were able to achieve similar performance with smaller model sizes. However, the pace of inference did not improve as expected, possibly due to the limited computing resources of the Raspberry Pi platform. By experimenting with various batch sizes during training, we found that a batch size of 32 achieved the highest average precision.</p>
</div>
<div id="S6.p4" class="ltx_para ltx_noindent">
<p id="S6.p4.1" class="ltx_p">Further research can focus on quantization and fine-tuning hyperparameters to further optimize these models for real-world applications. The application of these techniques to other popular object detection models could also be explored, as well as the use of more powerful hardware for inference.</p>
</div>
<div id="S6.p5" class="ltx_para ltx_noindent">
<p id="S6.p5.1" class="ltx_p">Overall, the use of optimization techniques such as weight pruning and SVD
is a promising approach to reduce the size and computational complexity
of object detection models, enabling their deployment in
resource-constrained environments, and improving their efficiency.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para ltx_noindent">
<p id="Sx1.p1.1" class="ltx_p">The work presented in this report was conducted as part of the Yonsei-Roboin project for the 2nd semester, 2022. We would like to express our gratitude to Roboin-Yonsei for providing us with the opportunity to conduct this research. We would like to extend our thanks to our project team members for their valuable contributions and collaboration. We would also like to thank the OpenAI team for providing access to the ChatGPT language model, which was used in this research. Finally, we would like to acknowledge the support and encouragement of our friends and families throughout this project.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.

</span>
<span class="ltx_bibblock">Going deeper with convolutions, 2014.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J.
Dally, and Kurt Keutzer.

</span>
<span class="ltx_bibblock">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and
lt;0.5mb model size, 2016.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,
Tobias Weyand, Marco Andreetto, and Hartwig Adam.

</span>
<span class="ltx_bibblock">Mobilenets: Efficient convolutional neural networks for mobile vision
applications, 2017.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew
Howard, Hartwig Adam, and Dmitry Kalenichenko.

</span>
<span class="ltx_bibblock">Quantization and training of neural networks for efficient
integer-arithmetic-only inference, 2017.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Chenzhuo Zhu, Song Han, Huizi Mao, and William J. Dally.

</span>
<span class="ltx_bibblock">Trained ternary quantization, 2016.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David.

</span>
<span class="ltx_bibblock">Binaryconnect: Training deep neural networks with binary weights
during propagations, 2015.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi.

</span>
<span class="ltx_bibblock">Xnor-net: Imagenet classification using binary convolutional neural
networks, 2016.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf.

</span>
<span class="ltx_bibblock">Pruning filters for efficient convnets, 2016.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao.

</span>
<span class="ltx_bibblock">Yolov7: Trainable bag-of-freebies sets new state-of-the-art for
real-time object detectors, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu.

</span>
<span class="ltx_bibblock">Deeply-supervised nets, 2014.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Matthew D Zeiler and Rob Fergus.

</span>
<span class="ltx_bibblock">Visualizing and understanding convolutional networks, 2013.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Emily Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus.

</span>
<span class="ltx_bibblock">Exploiting linear structure within convolutional networks for
efficient evaluation, 2014.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Roboflow 100.

</span>
<span class="ltx_bibblock">street work dataset.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://universe.roboflow.com/roboflow-100/street-work" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://universe.roboflow.com/roboflow-100/street-work</a>, dec
2022.

</span>
<span class="ltx_bibblock">visited on 2023-03-05.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2303.02734" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2303.02735" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2303.02735">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2303.02735" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2303.02736" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 21:54:28 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
