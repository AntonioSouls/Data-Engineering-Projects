<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2212.09408] Universal Object Detection with Large Vision Model</title><meta property="og:description" content="Over the past few years, there has been growing interest in developing a broad, universal, and general-purpose computer vision system. Such systems have the potential to address a wide range of vision tasks simultaneou…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Universal Object Detection with Large Vision Model">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Universal Object Detection with Large Vision Model">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2212.09408">

<!--Generated on Fri Mar  1 08:59:00 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_fleqn">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\jyear</span>
<p id="p1.2" class="ltx_p">2022</p>
</div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p">[1]<span id="p2.1.1" class="ltx_ERROR undefined">\fnm</span>Xiaoyu <span id="p2.1.2" class="ltx_ERROR undefined">\sur</span>Wang</p>
</div>
<div id="p3" class="ltx_para">
<p id="p3.1" class="ltx_p">1]<span id="p3.1.1" class="ltx_ERROR undefined">\orgname</span>Intellifusion Inc., <span id="p3.1.2" class="ltx_ERROR undefined">\orgaddress</span><span id="p3.1.3" class="ltx_ERROR undefined">\city</span>Shenzhen, <span id="p3.1.4" class="ltx_ERROR undefined">\country</span>China

2]<span id="p3.1.5" class="ltx_ERROR undefined">\orgname</span>Harbin Institute of Technology, Shenzhen, <span id="p3.1.6" class="ltx_ERROR undefined">\orgaddress</span><span id="p3.1.7" class="ltx_ERROR undefined">\country</span>China

3]<span id="p3.1.8" class="ltx_ERROR undefined">\orgname</span>Peng Cheng Laboratory, <span id="p3.1.9" class="ltx_ERROR undefined">\orgaddress</span><span id="p3.1.10" class="ltx_ERROR undefined">\city</span>Shenzhen, <span id="p3.1.11" class="ltx_ERROR undefined">\country</span>China

4]<span id="p3.1.12" class="ltx_ERROR undefined">\orgname</span>South China University of Technology, <span id="p3.1.13" class="ltx_ERROR undefined">\orgaddress</span><span id="p3.1.14" class="ltx_ERROR undefined">\city</span>Guangzhou, <span id="p3.1.15" class="ltx_ERROR undefined">\country</span>China</p>
</div>
<h1 class="ltx_title ltx_title_document">Universal Object Detection with Large Vision Model</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id1.1.id1" class="ltx_ERROR undefined">\fnm</span>Feng <span id="id2.2.id2" class="ltx_ERROR undefined">\sur</span>Lin
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:lin1993@mail.ustc.edu.cn">lin1993@mail.ustc.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id3.1.id1" class="ltx_ERROR undefined">\fnm</span>Wenze <span id="id4.2.id2" class="ltx_ERROR undefined">\sur</span>Hu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:windsor.hwu@gmail.com">windsor.hwu@gmail.com</a>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id5.1.id1" class="ltx_ERROR undefined">\fnm</span>Yaowei <span id="id6.2.id2" class="ltx_ERROR undefined">\sur</span>Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:wangyw@pcl.ac.cn">wangyw@pcl.ac.cn</a>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id7.1.id1" class="ltx_ERROR undefined">\fnm</span>Yonghong <span id="id8.2.id2" class="ltx_ERROR undefined">\sur</span>Tian
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:yhtian@pku.edu.cn">yhtian@pku.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id9.1.id1" class="ltx_ERROR undefined">\fnm</span>Guangming <span id="id10.2.id2" class="ltx_ERROR undefined">\sur</span>Lu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:luguangm@hit.edu.cn">luguangm@hit.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id11.1.id1" class="ltx_ERROR undefined">\fnm</span>Fanglin <span id="id12.2.id2" class="ltx_ERROR undefined">\sur</span>Chen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:chenfanglin@hit.edu.cn">chenfanglin@hit.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id13.1.id1" class="ltx_ERROR undefined">\fnm</span>Yong <span id="id14.2.id2" class="ltx_ERROR undefined">\sur</span>Xu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:yxu@scut.edu.cn">yxu@scut.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:fanghuaxue@gmail.com">fanghuaxue@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id15.id1" class="ltx_p">Over the past few years, there has been growing interest in developing a broad, universal, and general-purpose computer vision system. Such systems have the potential to address a wide range of vision tasks simultaneously, without being limited to specific problems or data domains. This universality is crucial for practical, real-world computer vision applications. In this study, our focus is on a specific challenge: the large-scale, multi-domain universal object detection problem, which contributes to the broader goal of achieving a universal vision system. This problem presents several intricate challenges, including cross-dataset category label duplication, label conflicts, and the necessity to handle hierarchical taxonomies. To address these challenges, we introduce our approach to label handling, hierarchy-aware loss design, and resource-efficient model training utilizing a pre-trained large vision model. Our method has demonstrated remarkable performance, securing a prestigious <em id="id15.id1.1" class="ltx_emph ltx_font_italic">second</em>-place ranking in the object detection track of the Robust Vision Challenge 2022 (RVC 2022) on a million-scale cross-dataset object detection benchmark. We believe that our comprehensive study will serve as a valuable reference and offer an alternative approach for addressing similar challenges within the computer vision community. The source code for our work is openly available at <a target="_blank" href="https://github.com/linfeng93/Large-UniDet" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/linfeng93/Large-UniDet</a>.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>universal object detection, large vision model, resource-efficient, hierarchical taxonomy
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">A universal, general-purpose computer vision system has become a trend in the development of computer vision technology <cite class="ltx_cite ltx_citemacro_citep">(Y. He <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>; Wang <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib62" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2019</a>; Gong <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2021</a>; Hasan <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2021</a>; X. Zhou <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>)</cite>. This universal vision model is a multi-talented agent that can simultaneously solve a wide range of vision tasks with minimal human intervention. Researchers no longer need to train separate models for each individual vision task or fine-tune an existing model for a specific data domain. Instead, they can achieve all tasks with a single effort. The universality of this model is a promising direction towards human-like AI and has important implications for real-world computer vision applications <cite class="ltx_cite ltx_citemacro_citep">(Yuan <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib66" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2021</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this study, we aim to contribute to the development of universal vision technology. Specifically, we focus on the large-scale universal object detection problem across different domains. The goal is to have a single object detector that can perform the inference process once and generate unified detection results across all datasets, regardless of their differences.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The challenge of developing a large-scale multi-domain universal object detection system lies in two areas: (<em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">a</em>) curating a large-scale and diverse training dataset, and (<em id="S1.p3.1.2" class="ltx_emph ltx_font_italic">b</em>) creating a robust visual representation method.
The training dataset must cover a wide range of data domains in order to achieve satisfactory results across domains. However, such a dataset is currently not available.
Furthermore, building a unified large-scale dataset with dense annotations for object detection <cite class="ltx_cite ltx_citemacro_citep">(Zhao <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib68" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2020</a>; X. Zhou <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>)</cite> and similar fine-grained computer vision tasks <cite class="ltx_cite ltx_citemacro_citep">(Lambert <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2020</a>; Bevandić <span class="ltx_ERROR undefined">\BBA</span> Šegvić, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>; Ranftl <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2020</a>)</cite> is cost-prohibitive.
In terms of robust visual representations, it is challenging to ensure the common object detector is robust to a vast and diverse source data, often numbering in the millions, as objects of interest can vary greatly in different images.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The increasing availability of object detection datasets has opened the door to implementing universal object detectors by repurposing these resources. Our strategy involves consolidating these datasets by harmonizing their distinct label spaces, enabling us to tackle multi-domain object detection tasks characterized by varying label vocabularies. However, the integration of multiple diverse datasets often introduces annotation inconsistencies, including label duplication, conflicts, and incomplete hierarchical taxonomies. To overcome these challenges, we have developed a comprehensive loss formulation within the unified label space, ensuring the robustness of our approach.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In addressing the robustness challenge inherent in the multi-domain object detection problem, our strategy revolves around harnessing the power of large pre-trained vision models.
Recent studies have demonstrated the superiority of larger models in capturing higher-quality visual representations compared to smaller models <cite class="ltx_cite ltx_citemacro_citep">(Radosavovic <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2020</a>; Bello <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2021</a>; Kolesnikov <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2019</a>)</cite>. These high-quality representations lead to better generalization both in-domain and out-of-domain <cite class="ltx_cite ltx_citemacro_citep">(Goyal <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>)</cite>. Thus, we believe that the use of large well-trained vision models would significantly improve the performance of universal object detection for million-scale diverse datasets. Our experiments indeed show a noticeable improvement in performance by using larger vision models in the universal object detection task.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">As we acquire the feature robustness by taking advantage of large pre-trained vision models, computational resources become a critical demand because of both computational and memory-wise costs.
Without many computational resources yet, we introduce a resource-efficient training formulation for large vision models inspired by a recent work <cite class="ltx_cite ltx_citemacro_citep">(Vasconcelos <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>)</cite>, which saves considerable computational resources, especially GPU memory, during the training procedures.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">This paper presents our novel approach to the challenge of multi-domain universal object detection at a scale of millions of diverse images. We utilize the power of large pre-trained vision models and present an efficient training formulation that saves computational resources. Our method, Large-UniDet, has achieved remarkable performance and won the <em id="S1.p7.1.1" class="ltx_emph ltx_font_italic">second</em> prize in the object detection track of the Robust Vision Challenge 2022 (RVC 2022)<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a href="www.robustvision.net/leaderboard.php?benchmark=object" title="" class="ltx_ref ltx_url ltx_font_typewriter">www.robustvision.net/leaderboard.php?benchmark=object</a>, IFFF_RVC entry on Leaderboard</span></span></span>. The success of our approach is attributed to the efficient formulation design, careful label handling, and knowledge transfer from large-scale pre-training.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2212.09408/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="484" height="380" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview. The design of Large-UniDet is based on a two-stage RCNN-style object detection network. The frozen backbone is a RegNet architecture initialized with the weights of SEER models. The NAS-FPN blocks can be stacked N times for a better accuracy-cost trade-off. The classification branch of Cascade R-CNN outputs 541 class scores including the <em id="S1.F1.2.1" class="ltx_emph ltx_font_italic">background</em> as the cardinality of the unified label space is 540.</figcaption>
</figure>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">Our contributions are summarized as follows.</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Our approach explores the use of large vision models for the challenging task of large-scale multi-domain universal object detection.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We present a resource-efficient training formulation for large vision models in universal object detection, which saves computational resources during training procedures.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">With the unified label space, we handle multi-domain object detection with different label vocabularies and overcome cross-dataset label duplication and semantic hierarchy problems.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">The proposed method, Large-UniDet, achieved the <math id="S1.I1.i4.p1.1.m1.1" class="ltx_Math" alttext="2nd" display="inline"><semantics id="S1.I1.i4.p1.1.m1.1a"><mrow id="S1.I1.i4.p1.1.m1.1.1" xref="S1.I1.i4.p1.1.m1.1.1.cmml"><mn id="S1.I1.i4.p1.1.m1.1.1.2" xref="S1.I1.i4.p1.1.m1.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S1.I1.i4.p1.1.m1.1.1.1" xref="S1.I1.i4.p1.1.m1.1.1.1.cmml">​</mo><mi id="S1.I1.i4.p1.1.m1.1.1.3" xref="S1.I1.i4.p1.1.m1.1.1.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S1.I1.i4.p1.1.m1.1.1.1a" xref="S1.I1.i4.p1.1.m1.1.1.1.cmml">​</mo><mi id="S1.I1.i4.p1.1.m1.1.1.4" xref="S1.I1.i4.p1.1.m1.1.1.4.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.I1.i4.p1.1.m1.1b"><apply id="S1.I1.i4.p1.1.m1.1.1.cmml" xref="S1.I1.i4.p1.1.m1.1.1"><times id="S1.I1.i4.p1.1.m1.1.1.1.cmml" xref="S1.I1.i4.p1.1.m1.1.1.1"></times><cn type="integer" id="S1.I1.i4.p1.1.m1.1.1.2.cmml" xref="S1.I1.i4.p1.1.m1.1.1.2">2</cn><ci id="S1.I1.i4.p1.1.m1.1.1.3.cmml" xref="S1.I1.i4.p1.1.m1.1.1.3">𝑛</ci><ci id="S1.I1.i4.p1.1.m1.1.1.4.cmml" xref="S1.I1.i4.p1.1.m1.1.1.4">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i4.p1.1.m1.1c">2nd</annotation></semantics></math> prize in the object detection track of RVC 2022, demonstrating its impressive performance and robustness.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Universal Object Detector</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Recent years have seen a growing interest in universal object detection. Wang et al. <cite class="ltx_cite ltx_citemacro_citep">(Wang <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib62" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2019</a>)</cite> propose a universal detector with a domain attention module that leverages shared knowledge across different data domains. The design consists of multiple dataset-specific detectors that share most network parameters while keeping the categories of each dataset separate.
Universal-RCNN <cite class="ltx_cite ltx_citemacro_citep">(Xu <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2020</a>)</cite> proposes a different approach by incorporating graph transfer learning for modeling both intra-domain and inter-domain semantics of categories drawn from multiple datasets <cite class="ltx_cite ltx_citemacro_citep">(T<span class="ltx_ERROR undefined">\BHBI</span>Y. Lin <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2014</a>; Krishna <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2017</a>; B. Zhou <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib69" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2017</a>)</cite>.
Unlike these methods, Zhao et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhao <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib68" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2020</a>)</cite> build a unified label space by manually merging multiple label spaces of different datasets, and their framework is dedicated to managing partial annotations through the use of pseudo-labeling. UniDet <cite class="ltx_cite ltx_citemacro_citep">(X. Zhou <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>)</cite>, in contrast, presents an automatic method to unify label spaces based on visual concepts generated by a partitioned object detector with three separate branches. Cai et al. <cite class="ltx_cite ltx_citemacro_citep">(L. Cai <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>)</cite> construct a unified label space by extracting category embeddings from each dataset using a language model. Recently, Meng et al. <cite class="ltx_cite ltx_citemacro_citep">(Meng <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>)</cite> leveraged pre-trained language embeddings to generate adapted queries for each category embedding across different datasets, modeling object classification as a region-word alignment problem without a merged label space.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Similar to the methods mentioned above based on the unified label spaces, we propose a solution to improve the unified label space in universal object detection by modifying the manually-crafted taxonomy used in the RVC challenge. Our proposed method also addresses the challenges posed by label duplication and semantic hierarchy issues across multiple datasets.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Pre-training for Vision Tasks</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Pre-training is a widespread technique in computer vision <cite class="ltx_cite ltx_citemacro_citep">(Sun <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2017</a>; Joulin <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2016</a>; Kornblith <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2019</a>; Caron <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2020</a>; L. Cai <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>; Azizi <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2021</a>)</cite> that enhances performance by using backbones models trained on large-scale datasets such as ImageNet <cite class="ltx_cite ltx_citemacro_citep">(Deng <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2009</a>)</cite>, JFT-300M <cite class="ltx_cite ltx_citemacro_citep">(Sun <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2017</a>)</cite>, OpenImages <cite class="ltx_cite ltx_citemacro_citep">(Kuznetsova <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2020</a>)</cite>, or web-collected data <cite class="ltx_cite ltx_citemacro_citep">(Goyal <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>)</cite>. The backbone generates robust visual representations that can benefit various downstream vision tasks <cite class="ltx_cite ltx_citemacro_citep">(Goyal <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>)</cite>. For object detection, the choice of the pre-trained backbone is crucial for determining performance <cite class="ltx_cite ltx_citemacro_citep">(Y. Liu <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2020</a>)</cite>. Typically, the strength of a pre-trained backbone comes from its (<em id="S2.SS2.p1.1.1" class="ltx_emph ltx_font_italic">a</em>) powerful architecture, (<em id="S2.SS2.p1.1.2" class="ltx_emph ltx_font_italic">b</em>) broad training data, and (<em id="S2.SS2.p1.1.3" class="ltx_emph ltx_font_italic">c</em>) advanced pre-training methods.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p"><em id="S2.SS2.p2.1.1" class="ltx_emph ltx_font_italic">Stronger architecture.</em>
To explore the influence of backbone architectures on object detection performance, Huang et al. <cite class="ltx_cite ltx_citemacro_citep">(Huang <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2017</a>)</cite> examined the correlation between backbone capacities and performance. In contrast, Liu et al. <cite class="ltx_cite ltx_citemacro_citep">(Y. Liu <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2020</a>)</cite> enhanced the backbone’s capabilities by amalgamating multiple identical backbones.
Furthermore, Liu et al. <cite class="ltx_cite ltx_citemacro_citep">(Z. Liu <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>)</cite> leveraged the power of vision transformers to create an extremely large object detector by using an expanded Swin transformer <cite class="ltx_cite ltx_citemacro_citep">(Z. Liu <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2021</a>)</cite> as the feature extractor.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p"><em id="S2.SS2.p3.1.1" class="ltx_emph ltx_font_italic">Training data.</em>
In terms of the training data, Sun et al. <cite class="ltx_cite ltx_citemacro_citep">(Sun <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2017</a>)</cite> have demonstrated the impact of using a large-scale dataset JFT-300M on the robustness of representation learning. Bu et al. <cite class="ltx_cite ltx_citemacro_citep">(Bu <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2021</a>)</cite> have taken a different approach by combining various detection datasets <cite class="ltx_cite ltx_citemacro_citep">(S. Shao <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2019</a>; Kuznetsova <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2020</a>; T<span class="ltx_ERROR undefined">\BHBI</span>Y. Lin <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2014</a>; Dollar <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2011</a>; Zhang <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2017</a>)</cite> to attain better pre-trained weights for transfer learning in downstream tasks. Meanwhile, Cai et al. <cite class="ltx_cite ltx_citemacro_citep">(L. Cai <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>)</cite> have utilized existing detection datasets <cite class="ltx_cite ltx_citemacro_citep">(A. Gupta <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2019</a>; S. Shao <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2019</a>; Kuznetsova <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2020</a>)</cite> to create a large pre-training dataset through careful curation based on well-defined principles.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p"><em id="S2.SS2.p4.1.1" class="ltx_emph ltx_font_italic">Pre-training approach.</em>
Recent advancements in backbone pre-training  <cite class="ltx_cite ltx_citemacro_citep">(Caron <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2020</a>; K. He <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2020</a>, <a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>; F. Lin <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2021</a>; Xu <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>)</cite> have demonstrated the superiority of self-supervised methods over supervised approaches in computer vision tasks, such as object detection, semantic segmentation, and image classification. With the advantage of utilizing unlimited diverse image data from the web, self-supervised pre-training methods are capable of capturing more discriminative visual representations without relying on vast manual annotations <cite class="ltx_cite ltx_citemacro_citep">(Goyal <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>)</cite>. Furthermore, the training of vision foundation models on large-scale image-text data <cite class="ltx_cite ltx_citemacro_citep">(Yuan <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib66" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2021</a>; Jia <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2021</a>; Radford <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2021</a>; J. Shao <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2021</a>)</cite> highlights the significant impact of representation learning on both in-domain and out-of-domain downstream tasks.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">To enhance the performance of our universal object detector, we have chosen to use large <span id="S2.SS2.p5.1.1" class="ltx_text ltx_font_bold">SE</span>lf-sup<span id="S2.SS2.p5.1.2" class="ltx_text ltx_font_bold">ER</span>vised vision models known as SEER models <cite class="ltx_cite ltx_citemacro_citep">(Goyal <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>)</cite> as the backbone. As discussed earlier, robust backbones can be attributed to high-capacity architectures, a diverse training data, and cutting-edge pre-training techniques. The largest SEER model that we will be using boasts a massive 10 billion network parameters. The SEER models are trained on a self-supervised clustering-based method <cite class="ltx_cite ltx_citemacro_citep">(Caron <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2020</a>)</cite> utilizing 1 billion less biased uncurated images collected from the web. This results in robust visual representations that perform well on both in-domain and out-of-domain benchmarks <cite class="ltx_cite ltx_citemacro_citep">(Goyal <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>)</cite>. SEER has exhibited greater robustness, fairness, reduced harm, and minimized bias when compared to lots of supervised models. Our belief is that the SEER backbones will be capable of producing more discriminative features and provide better out-of-distribution generalization for the task of universal object detection across datasets with varying characteristics.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Vision-Language Systems</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Prior to the advent of universal vision models, universal language models had already achieved remarkable success in the field of natural language processing (NLP) <cite class="ltx_cite ltx_citemacro_citep">(Howard <span class="ltx_ERROR undefined">\BBA</span> Ruder, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2018</a>)</cite>. The emergence of large language models (LLMs) has brought about a profound transformation in traditional paradigms for NLP tasks <cite class="ltx_cite ltx_citemacro_citep">(Radford <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2018</a>; Devlin <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2018</a>; Raffel <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2020</a>)</cite>. These models employ a unified approach, leverage billion-scale multi-source corpora data for training, and operate within a consistent tokenized framework for various NLP tasks.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Given the remarkable success of language models, the research community is now turning its attention to achieving vision universality, working on the development of language-assisted, multi-skilled general-purpose vision systems capable of addressing a wide range of vision tasks. Gupta et al. <cite class="ltx_cite ltx_citemacro_citep">(T. Gupta <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>)</cite> introduce a General Purpose Vision (GPV) system based on a vision-language architecture. GPV exhibits the ability to learn and perform tasks involving image input, producing textual descriptions or bounding boxes. Building upon this concept, Kamath et al. <cite class="ltx_cite ltx_citemacro_citep">(Kamath <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>)</cite> have devised an effective method to scale GPV by learning skills from supervised datasets, acquiring knowledge from web image search, and leveraging GPV’s capacity to transfer visual knowledge across diverse skills. Furthermore, Unified-IO <cite class="ltx_cite ltx_citemacro_citep">(Lu <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>)</cite> incorporates language model technologies that standardize input and output representations into a common vocabulary token format across all tasks. This approach allows a single model to excel in a wide range of vision tasks and vision-and-language tasks, consequently amplifying the versatility of these systems. These works demonstrate that leveraging extensive training data from multiple domains, diverse types, and various modalities offers a feasible approach to achieving a general-purpose AI agent.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our method can be effectively divided into two distinct components.
The first part meticulously outlines our object detection framework, highlighting specialized designs that enhance the efficient training of large vision models.
The second part delves into practical strategies employed for joint training across multiple diverse specific datasets.
These strategies are presented in comprehensive detail and can be readily adapted to analogous scenarios involving other datasets.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Resource-efficient Detection with a Large Vision Model</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In this section, we introduce our strong object detector that is built on large pre-trained backbone networks. The use of large vision models has been demonstrated to improve the performance of many computer vision tasks. However, the enormous computational and memory requirements for training these models limit their practical use <cite class="ltx_cite ltx_citemacro_citep">(Dai <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2021</a>; J. Shao <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2021</a>; Radford <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2021</a>)</cite>. To address this challenge, we propose a computationally &amp; memory efficient training approach that freezes the parameters of the billions of pre-trained backbone neurons and fine-tunes the extracted visual representations on the subsequent detector components.
This enables training of our largest model using a restricted GPU configuration of 16 NVIDIA 3090 GPUs (detailed in <a href="#S4.SS4.SSS3" title="4.4.3 Training Strategies ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4.3</span></a>), reducing the GPU memory consumption by approximately fourfold (NVIDIA A100/H100-80G GPUs may be required otherwise).
Our resource-efficient approach leverages the recent advancements in knowledge transfer <cite class="ltx_cite ltx_citemacro_citep">(Vasconcelos <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>)</cite> and is specifically designed for large pre-trained vision models, providing a valuable resource for the computer vision community that is interested in object detection with limited computational resources.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the overall framework. Each detector component is described in detail in the remaining content of this section.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Frozen Backbone with Billions of Parameters</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">We provide a concise overview of the backbone employed in our proposed object detector, followed by an introduction to the advantages of our frozen design. As highlighted in Section <a href="#S2.SS2" title="2.2 Pre-training for Vision Tasks ‣ 2 Related Works ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>, our decision to employ the SEER models <cite class="ltx_cite ltx_citemacro_citep">(Goyal <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>)</cite> as the backbone of our object detection network stems from their remarkable ability to enhance fairness and mitigate bias across a spectrum of diverse domains. This strategic selection ensures the consistent extraction of robust visual representations across three quite distinct datasets: MS COCO (COCO), Mapillary Vistas Dataset (MVD), and OpenImages Dataset (OID).</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">Traditionally, optimizing an object detector involves fine-tuning both the initialized backbone and subsequent detector components using detection datasets. Nonetheless, fine-tuning the backbone on smaller datasets can lead to the drift of the backbone parameters from their pre-trained initialization, potentially undermining detection performance <cite class="ltx_cite ltx_citemacro_citep">(Vasconcelos <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>)</cite>. Furthermore, fine-tuning a resource-intensive backbone can significantly escalate computational complexity. To attain exceptional detection performance while managing computational demands, we have opted to maintain the frozen state of the backbone parameters throughout the training process. This strategic approach not only conserves resources but also contributes positively to the performance of long-tailed object categories through knowledge preservation <cite class="ltx_cite ltx_citemacro_citep">(Vasconcelos <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>)</cite>. This preservation holds significance within the realm of the RVC multi-domain scenario.</p>
</div>
<div id="S3.SS1.SSS1.p3" class="ltx_para">
<p id="S3.SS1.SSS1.p3.1" class="ltx_p">Drawing from the observation that enhanced in-domain and out-of-domain generalization is positively correlated with the scale of the SEER model, as demonstrated by Goyal et al. <cite class="ltx_cite ltx_citemacro_citep">(Goyal <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>)</cite>, we conduct a meticulous assessment of the trade-off between cost and performance. This evaluation guides us in selecting the optimal configuration for our experiments and, subsequently, our final submission to the RVC competition.
In the end, we opt to use both the lighter version (SEER-RegNet32gf) and the second largest version (SEER-RegNet256gf) for our extensive evaluations.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Cascade Detection Heads</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">In order to enhance the performance of our object detector, we implemented a two-stage RCNN-style detection framework with the frozen SEER backbone. Initial experiments using Faster R-CNN <cite class="ltx_cite ltx_citemacro_citep">(Ren <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2015</a>)</cite> did not produce satisfactory performance, likely due to its limited number of learnable parameters making it difficult to handle the large-scale detection tasks across diverse datasets. Taking inspiration from recent advances in the knowledge transferring field <cite class="ltx_cite ltx_citemacro_citep">(Vasconcelos <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>)</cite>, we adopted a high-capacity Cascade R-CNN <cite class="ltx_cite ltx_citemacro_citep">(Z. Cai <span class="ltx_ERROR undefined">\BBA</span> Vasconcelos, <a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2018</a>)</cite> as our detection heads, which greatly improved detection performance as discussed in Section <a href="#S4.SS4.SSS1" title="4.4.1 Detector Components Analysis ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4.1</span></a>.</p>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>Stacked Dense Neck</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">The Feature Pyramid Network (FPN) is a fundamental component in object detection frameworks, serving as an adaptive module that integrates and improves hierarchical features. It acts like a neck that connects the backbone and the subsequent detection heads. The original FPN design <cite class="ltx_cite ltx_citemacro_citep">(T<span class="ltx_ERROR undefined">\BHBI</span>Y. Lin <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2017</a>)</cite> transfers multi-level semantic information from the backbone through a top-down pathway and lateral connections, creating a simple and straightforward path for knowledge integration. Subsequent designs <cite class="ltx_cite ltx_citemacro_citep">(S. Liu <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2018</a>; Tan <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2020</a>; Pang <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2019</a>; Ghiasi <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2019</a>)</cite> have introduced cross-scale connections to reinforce visual representations with semantically important information and low-level details.</p>
</div>
<div id="S3.SS1.SSS3.p2" class="ltx_para">
<p id="S3.SS1.SSS3.p2.1" class="ltx_p">We employ a stacked, densely connected FPN, namely NAS-FPN <cite class="ltx_cite ltx_citemacro_citep">(Ghiasi <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2019</a>)</cite>, as the neck of our object detector for the following four reasons.</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">As universal object detection is to detect hundreds of object categories from various datasets, the impressive ability of NAS-FPN to generate robust representations meets the challenges of million-scale multi-domain detection.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">As we freeze the backbone, the remaining detector components require higher model capacity (described in Section <a href="#S3.SS1.SSS2" title="3.1.2 Cascade Detection Heads ‣ 3.1 Resource-efficient Detection with a Large Vision Model ‣ 3 Method ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1.2</span></a>), while stacked NAS-FPN offers excellent flexibility in constructing rich neck architecture.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">The released SEER models are trained on billions of uncurated web-scale images.
Inevitably, there is some domain gap between the upstream pre-training dataset and downstream detection datasets.
As we do not finetune the SEER models on the downstream data, we believe the early NAS-FPN blocks can act as domain adaptors to align the domain gap.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">Last but not least, we observe that multi-level side-outputs of SEER models have very different characteristics. Some shallow side-outputs are dense, while the deeper ones are generally sparse and weak.
The rich connections of NAS-FPN offer more possible ways for better feature integration.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.4 </span>Adaptive RPN</h4>

<div id="S3.SS1.SSS4.p1" class="ltx_para">
<p id="S3.SS1.SSS4.p1.1" class="ltx_p">In the context of multi-domain object detection, objects belonging to the same category can exhibit different characteristics across different domains. For example, a <em id="S3.SS1.SSS4.p1.1.1" class="ltx_emph ltx_font_italic">person</em> in an autonomous driving dataset such as MVD is typically much smaller in the high-resolution street scenes, while a <em id="S3.SS1.SSS4.p1.1.2" class="ltx_emph ltx_font_italic">person</em> in COCO images is usually much larger. This variation in object size highlights the need for an adaptive region proposal network (RPN) to generate high-quality proposals that can handle the diverse object sizes in each domain. The Cascade RPN <cite class="ltx_cite ltx_citemacro_citep">(Vu <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2019</a>)</cite> overcomes the limitations of traditional RPNs, which rely on heuristically determining appropriate scales and aspect ratios for pre-defined anchors. Additionally, having too many pre-defined anchors can slow down the training process. By incorporating the Cascade RPN into our network, we are able to improve the quality of proposals and increase the overall model capacity, providing the best of both worlds.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Cross-dataset Model Training</h3>

<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Label Space Unification Across Multiple Datasets</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">The goal of this section is to outline the creation of a unified label space for three datasets, addressing the issues of label duplication and semantic hierarchy across the datasets. The RVC organizers have provided a manually-crafted taxonomy<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/ozendelait/rvc_devkit/blob/master/objdet/obj_det_mapping.csv" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ozendelait/rvc_devkit/blob/master/objdet/obj_det_mapping.csv</a></span></span></span> as a starting point. This taxonomy maps each category from COCO or MVD to a single category in the RVC official label space, as well as each leaf-node category from OID. However, the non-leaf categories from OID are not included in this label space. To complete the label space, we modify the official taxonomy by simply adding all of the non-leaf categories from OID, excluding the root entry. This results in a unified label space with a cardinality of 540.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.1" class="ltx_p">The OID has a semantic hierarchy where the superclasses, or non-leaf categories, are considered to be more general than other classes. However, this leads to inconsistencies in granularity and results in issues such as label duplication and problems with the semantic hierarchy. For example, the <em id="S3.SS2.SSS1.p2.1.1" class="ltx_emph ltx_font_italic">person</em> (/m/01g317) superclass in OID and the <em id="S3.SS2.SSS1.p2.1.2" class="ltx_emph ltx_font_italic">person</em> class in COCO are semantically the same, but are treated as separate categories. For another example, the <em id="S3.SS2.SSS1.p2.1.3" class="ltx_emph ltx_font_italic">cow</em> class in COCO is semantically a child of the <em id="S3.SS2.SSS1.p2.1.4" class="ltx_emph ltx_font_italic">animal</em> (/m/0jbk) superclass in OID, but OID’s hierarchy does not reflect this parent-child relationship. This overlap in taxonomy can negatively impact the performance of universal object detection. To address these obstacles, we propose a unified hierarchical taxonomy and implement a hierarchy-aware loss suppression method, which will be explained in Section <a href="#S3.SS2.SSS2" title="3.2.2 Multi-label with Hierarchical Taxonomy Completion ‣ 3.2 Cross-dataset Model Training ‣ 3 Method ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a> and <a href="#S3.SS2.SSS3" title="3.2.3 Hierarchy-aware Cross-dataset Loss Suppression ‣ 3.2 Cross-dataset Model Training ‣ 3 Method ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.3</span></a>, respectively.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Multi-label with Hierarchical Taxonomy Completion</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">To address the semantic hierarchy challenges in OID, we introduce a method of completing the hierarchical taxonomy by incorporating categories from the RVC official taxonomy. The resolution of the remaining cross-dataset semantic hierarchy issues is presented in Section <a href="#S3.SS2.SSS3" title="3.2.3 Hierarchy-aware Cross-dataset Loss Suppression ‣ 3.2 Cross-dataset Model Training ‣ 3 Method ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.3</span></a>.</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.1" class="ltx_p">We convert the one-hot category labels to multi-class labels by considering all parent categories as positives for OID images. This is similar to UniDet <cite class="ltx_cite ltx_citemacro_citep">(X. Zhou <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>)</cite> but with the added consideration of the semantic hierarchies of COCO and MVD using the OID semantic hierarchy. For each annotated box that has been merged with an OID leaf-node category according to the RVC official taxonomy, we treat it as its OID equivalent. For instance, if the COCO <em id="S3.SS2.SSS2.p2.1.1" class="ltx_emph ltx_font_italic">banana</em> and the OID <em id="S3.SS2.SSS2.p2.1.2" class="ltx_emph ltx_font_italic">banana</em> (/m/09qck) have been merged into a single mutual category, a bounding box annotated as <em id="S3.SS2.SSS2.p2.1.3" class="ltx_emph ltx_font_italic">banana</em> from COCO would receive a positive label for the <em id="S3.SS2.SSS2.p2.1.4" class="ltx_emph ltx_font_italic">fruit</em> category since <em id="S3.SS2.SSS2.p2.1.5" class="ltx_emph ltx_font_italic">banana</em> belongs to the <em id="S3.SS2.SSS2.p2.1.6" class="ltx_emph ltx_font_italic">fruit</em> superclass according to the OID semantic hierarchy. We employ a multi-label classifier in the detection heads and use <em id="S3.SS2.SSS2.p2.1.7" class="ltx_emph ltx_font_italic">sigmoid</em> activation functions to obtain class confidence scores for each bounding box.</p>
</div>
<div id="S3.SS2.SSS2.p3" class="ltx_para">
<p id="S3.SS2.SSS2.p3.1" class="ltx_p">It is important to note that this hierarchical taxonomy completion is not a complete solution. There are several annotated objects from COCO and MVD that do not match any OID leaf-node category but are semantically associated with a certain superclass from OID. Instead of activating the corresponding parent categories, we handle these semantic hierarchies through an intricate adaptation in the loss function, which is discussed in the following section.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2212.09408/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="215" height="111" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An example shows the loss suppression for semantically label duplication between COCO and OID.</figcaption>
</figure>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Hierarchy-aware Cross-dataset Loss Suppression</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">To address both label duplication and unsolved semantic hierarchies described in Sections <a href="#S3.SS2.SSS1" title="3.2.1 Label Space Unification Across Multiple Datasets ‣ 3.2 Cross-dataset Model Training ‣ 3 Method ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a> and <a href="#S3.SS2.SSS2" title="3.2.2 Multi-label with Hierarchical Taxonomy Completion ‣ 3.2 Cross-dataset Model Training ‣ 3 Method ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>, we propose a loss adaptation strategy called Hierarchy-Aware Cross-Dataset Loss Suppression (HCLS). This strategy is based on the semantic hierarchy of OID and suppresses losses over categories involved in label duplication and semantic hierarchy between OID and COCO/MVD in the box classification branches. More specifically,</p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para">
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p">For each category from OID, HCLS ignores the losses over all its child categories, as a common practice for hierarchical taxonomy <cite class="ltx_cite ltx_citemacro_citep">(Kuznetsova <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2020</a>)</cite>.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p">For each category from COCO / MVD, which is not merged with any OID leaf-node category in the RVC official taxonomy, HCLS searches all the superclasses from OID and performs one of the following adaptations to the loss:</p>
</div>
<div id="S3.I2.i2.p2" class="ltx_para">
<p id="S3.I2.i2.p2.1" class="ltx_p">(a) [Label duplication] Suppose this category matches one of the superclasses from OID in semantics. In this case, HCLS ignores the loss between its OID equivalent and itself, in addition to the losses between its OID equivalent’s parents/children and itself.</p>
</div>
<div id="S3.I2.i2.p3" class="ltx_para">
<p id="S3.I2.i2.p3.1" class="ltx_p">(b) [Cross-dataset semantic hierarchy] Suppose this category belongs to one of the superclasses from OID in semantics.
HCLS ignores the losses between all its parent categories and itself.</p>
</div>
<div id="S3.I2.i2.p4" class="ltx_para">
<p id="S3.I2.i2.p4.1" class="ltx_p">(c) [Neither] Suppose this category is independent of any superclass of OID.
HCLS does nothing about loss adaptation.
In other words, we equally calculate losses over all the categories in the unified label space in loss functions.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS2.SSS3.p3" class="ltx_para">
<p id="S3.SS2.SSS3.p3.1" class="ltx_p">In Fig. <a href="#S3.F2" title="Figure 2 ‣ 3.2.2 Multi-label with Hierarchical Taxonomy Completion ‣ 3.2 Cross-dataset Model Training ‣ 3 Method ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.2.3 Hierarchy-aware Cross-dataset Loss Suppression ‣ 3.2 Cross-dataset Model Training ‣ 3 Method ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, two examples illustrate the loss adaptation process: (a) and (b). Note that we do <span id="S3.SS2.SSS3.p3.1.1" class="ltx_text ltx_font_bold">not</span> perform any tedious category merging but rather handle label duplication at the loss level. According to the RVC official taxonomy, there are less than 50 independent categories, so we manually search for cross-dataset label duplication and semantic hierarchy. For further details, please refer to Table <a href="#S3.T1" title="Table 1 ‣ 3.2.3 Hierarchy-aware Cross-dataset Loss Suppression ‣ 3.2 Cross-dataset Model Training ‣ 3 Method ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, which lists the processed semantically duplicate categories, and Table <a href="#S3.T2" title="Table 2 ‣ 3.2.4 Overall Formulation ‣ 3.2 Cross-dataset Model Training ‣ 3 Method ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, which lists the processed semantic hierarchies across datasets.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2212.09408/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="215" height="111" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>An example shows the loss suppression for semantic hierarchy between MVD and OID.</figcaption>
</figure>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_tt" colspan="2"><span id="S3.T1.1.1.1.1" class="ltx_text" style="font-size:90%;">Two categories match in semantics</span></td>
</tr>
<tr id="S3.T1.1.2" class="ltx_tr">
<td id="S3.T1.1.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T1.1.2.1.1.1.1" class="ltx_text" style="font-size:90%;">  COCO classes</span></span>
</span>
</td>
<td id="S3.T1.1.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.2.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T1.1.2.2.1.1.1" class="ltx_text" style="font-size:90%;">OID superclasses</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.1.3" class="ltx_tr">
<td id="S3.T1.1.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T1.1.3.1.1.1.1" class="ltx_text" style="font-size:90%;">  </span><em id="S3.T1.1.3.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">sports ball</em></span>
</span>
</td>
<td id="S3.T1.1.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.2.1.1" class="ltx_p" style="width:100.0pt;"><em id="S3.T1.1.3.2.1.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ball</em><span id="S3.T1.1.3.2.1.1.2" class="ltx_text" style="font-size:90%;"> (/m/018xm)</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.1.4" class="ltx_tr">
<td id="S3.T1.1.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T1.1.4.1.1.1.1" class="ltx_text" style="font-size:90%;">  </span><em id="S3.T1.1.4.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">bear</em></span>
</span>
</td>
<td id="S3.T1.1.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.2.1.1" class="ltx_p" style="width:100.0pt;"><em id="S3.T1.1.4.2.1.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">bear</em><span id="S3.T1.1.4.2.1.1.2" class="ltx_text" style="font-size:90%;"> (/m/01dws)</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.1.5" class="ltx_tr">
<td id="S3.T1.1.5.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T1.1.5.1.1.1.1" class="ltx_text" style="font-size:90%;">  </span><em id="S3.T1.1.5.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">bed</em></span>
</span>
</td>
<td id="S3.T1.1.5.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.2.1.1" class="ltx_p" style="width:100.0pt;"><em id="S3.T1.1.5.2.1.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">bed</em><span id="S3.T1.1.5.2.1.1.2" class="ltx_text" style="font-size:90%;"> (/m/03ssj5)</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.1.6" class="ltx_tr">
<td id="S3.T1.1.6.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T1.1.6.1.1.1.1" class="ltx_text" style="font-size:90%;">  </span><em id="S3.T1.1.6.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">bird</em></span>
</span>
</td>
<td id="S3.T1.1.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.2.1.1" class="ltx_p" style="width:100.0pt;"><em id="S3.T1.1.6.2.1.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">bird</em><span id="S3.T1.1.6.2.1.1.2" class="ltx_text" style="font-size:90%;"> (/m/015p6)</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.1.7" class="ltx_tr">
<td id="S3.T1.1.7.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T1.1.7.1.1.1.1" class="ltx_text" style="font-size:90%;">  </span><em id="S3.T1.1.7.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">boat</em></span>
</span>
</td>
<td id="S3.T1.1.7.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.2.1.1" class="ltx_p" style="width:100.0pt;"><em id="S3.T1.1.7.2.1.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">boat</em><span id="S3.T1.1.7.2.1.1.2" class="ltx_text" style="font-size:90%;"> (/m/019jd)</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.1.8" class="ltx_tr">
<td id="S3.T1.1.8.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.8.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T1.1.8.1.1.1.1" class="ltx_text" style="font-size:90%;">  </span><em id="S3.T1.1.8.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">car</em></span>
</span>
</td>
<td id="S3.T1.1.8.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.8.2.1.1" class="ltx_p" style="width:100.0pt;"><em id="S3.T1.1.8.2.1.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">car</em><span id="S3.T1.1.8.2.1.1.2" class="ltx_text" style="font-size:90%;"> (/m/0k4j)</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.1.9" class="ltx_tr">
<td id="S3.T1.1.9.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.9.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T1.1.9.1.1.1.1" class="ltx_text" style="font-size:90%;">  </span><em id="S3.T1.1.9.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">clock</em></span>
</span>
</td>
<td id="S3.T1.1.9.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.9.2.1.1" class="ltx_p" style="width:100.0pt;"><em id="S3.T1.1.9.2.1.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">clock</em><span id="S3.T1.1.9.2.1.1.2" class="ltx_text" style="font-size:90%;"> (/m/01x3z)</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.1.10" class="ltx_tr">
<td id="S3.T1.1.10.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.10.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T1.1.10.1.1.1.1" class="ltx_text" style="font-size:90%;">  </span><em id="S3.T1.1.10.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">person</em></span>
</span>
</td>
<td id="S3.T1.1.10.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.10.2.1.1" class="ltx_p" style="width:100.0pt;"><em id="S3.T1.1.10.2.1.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">person</em><span id="S3.T1.1.10.2.1.1.2" class="ltx_text" style="font-size:90%;"> (/m/01g317)</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.1.11" class="ltx_tr">
<td id="S3.T1.1.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.11.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T1.1.11.1.1.1.1" class="ltx_text" style="font-size:90%;">  MVD classes</span></span>
</span>
</td>
<td id="S3.T1.1.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.11.2.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T1.1.11.2.1.1.1" class="ltx_text" style="font-size:90%;">OID superclasses</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.1.12" class="ltx_tr">
<td id="S3.T1.1.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.12.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T1.1.12.1.1.1.1" class="ltx_text" style="font-size:90%;">  </span><em id="S3.T1.1.12.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">object–vehicle–car</em></span>
</span>
</td>
<td id="S3.T1.1.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.12.2.1.1" class="ltx_p" style="width:100.0pt;"><em id="S3.T1.1.12.2.1.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">car</em><span id="S3.T1.1.12.2.1.1.2" class="ltx_text" style="font-size:90%;"> (/m/0k4j)</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.1.13" class="ltx_tr">
<td id="S3.T1.1.13.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T1.1.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.13.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T1.1.13.1.1.1.1" class="ltx_text" style="font-size:90%;">  </span><em id="S3.T1.1.13.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">human–person</em></span>
</span>
</td>
<td id="S3.T1.1.13.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T1.1.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.13.2.1.1" class="ltx_p" style="width:100.0pt;"><em id="S3.T1.1.13.2.1.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">person</em><span id="S3.T1.1.13.2.1.1.2" class="ltx_text" style="font-size:90%;"> (/m/01g317)</span></span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>The duplicated category names between OID superclasses and COCO / MVD classes in semantics.</figcaption>
</figure>
</section>
<section id="S3.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.4 </span>Overall Formulation</h4>

<div id="S3.SS2.SSS4.p1" class="ltx_para">
<p id="S3.SS2.SSS4.p1.6" class="ltx_p">The overall loss function can be formulated as the weighted sum of the RPN loss and the detector head loss, described as follows,</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E1.m1.1" class="ltx_Math" alttext="L=\lambda\cdot L_{rpn}+L_{head}" display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mi id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">L</mi><mo id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mrow id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml"><mi id="S3.E1.m1.1.1.3.2.2" xref="S3.E1.m1.1.1.3.2.2.cmml">λ</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.1.1.3.2.1" xref="S3.E1.m1.1.1.3.2.1.cmml">⋅</mo><msub id="S3.E1.m1.1.1.3.2.3" xref="S3.E1.m1.1.1.3.2.3.cmml"><mi id="S3.E1.m1.1.1.3.2.3.2" xref="S3.E1.m1.1.1.3.2.3.2.cmml">L</mi><mrow id="S3.E1.m1.1.1.3.2.3.3" xref="S3.E1.m1.1.1.3.2.3.3.cmml"><mi id="S3.E1.m1.1.1.3.2.3.3.2" xref="S3.E1.m1.1.1.3.2.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.2.3.3.1" xref="S3.E1.m1.1.1.3.2.3.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.3.2.3.3.3" xref="S3.E1.m1.1.1.3.2.3.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.2.3.3.1a" xref="S3.E1.m1.1.1.3.2.3.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.3.2.3.3.4" xref="S3.E1.m1.1.1.3.2.3.3.4.cmml">n</mi></mrow></msub></mrow><mo id="S3.E1.m1.1.1.3.1" xref="S3.E1.m1.1.1.3.1.cmml">+</mo><msub id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.3.3.2" xref="S3.E1.m1.1.1.3.3.2.cmml">L</mi><mrow id="S3.E1.m1.1.1.3.3.3" xref="S3.E1.m1.1.1.3.3.3.cmml"><mi id="S3.E1.m1.1.1.3.3.3.2" xref="S3.E1.m1.1.1.3.3.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.3.3.1" xref="S3.E1.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.3.3.3.3" xref="S3.E1.m1.1.1.3.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.3.3.1a" xref="S3.E1.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.3.3.3.4" xref="S3.E1.m1.1.1.3.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.3.3.1b" xref="S3.E1.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.3.3.3.5" xref="S3.E1.m1.1.1.3.3.3.5.cmml">d</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"></eq><ci id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2">𝐿</ci><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><plus id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3.1"></plus><apply id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2"><ci id="S3.E1.m1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.3.2.1">⋅</ci><ci id="S3.E1.m1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2">𝜆</ci><apply id="S3.E1.m1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.3.1.cmml" xref="S3.E1.m1.1.1.3.2.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.3.2.cmml" xref="S3.E1.m1.1.1.3.2.3.2">𝐿</ci><apply id="S3.E1.m1.1.1.3.2.3.3.cmml" xref="S3.E1.m1.1.1.3.2.3.3"><times id="S3.E1.m1.1.1.3.2.3.3.1.cmml" xref="S3.E1.m1.1.1.3.2.3.3.1"></times><ci id="S3.E1.m1.1.1.3.2.3.3.2.cmml" xref="S3.E1.m1.1.1.3.2.3.3.2">𝑟</ci><ci id="S3.E1.m1.1.1.3.2.3.3.3.cmml" xref="S3.E1.m1.1.1.3.2.3.3.3">𝑝</ci><ci id="S3.E1.m1.1.1.3.2.3.3.4.cmml" xref="S3.E1.m1.1.1.3.2.3.3.4">𝑛</ci></apply></apply></apply><apply id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.2">𝐿</ci><apply id="S3.E1.m1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3"><times id="S3.E1.m1.1.1.3.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3.3.1"></times><ci id="S3.E1.m1.1.1.3.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.3.2">ℎ</ci><ci id="S3.E1.m1.1.1.3.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3.3">𝑒</ci><ci id="S3.E1.m1.1.1.3.3.3.4.cmml" xref="S3.E1.m1.1.1.3.3.3.4">𝑎</ci><ci id="S3.E1.m1.1.1.3.3.3.5.cmml" xref="S3.E1.m1.1.1.3.3.3.5">𝑑</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">L=\lambda\cdot L_{rpn}+L_{head}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS4.p1.5" class="ltx_p">where <math id="S3.SS2.SSS4.p1.1.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS2.SSS4.p1.1.m1.1a"><mi id="S3.SS2.SSS4.p1.1.m1.1.1" xref="S3.SS2.SSS4.p1.1.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p1.1.m1.1b"><ci id="S3.SS2.SSS4.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS4.p1.1.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p1.1.m1.1c">\lambda</annotation></semantics></math> is the weight factor set to 0.7, while <math id="S3.SS2.SSS4.p1.2.m2.1" class="ltx_Math" alttext="L_{rpn}" display="inline"><semantics id="S3.SS2.SSS4.p1.2.m2.1a"><msub id="S3.SS2.SSS4.p1.2.m2.1.1" xref="S3.SS2.SSS4.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSS4.p1.2.m2.1.1.2" xref="S3.SS2.SSS4.p1.2.m2.1.1.2.cmml">L</mi><mrow id="S3.SS2.SSS4.p1.2.m2.1.1.3" xref="S3.SS2.SSS4.p1.2.m2.1.1.3.cmml"><mi id="S3.SS2.SSS4.p1.2.m2.1.1.3.2" xref="S3.SS2.SSS4.p1.2.m2.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p1.2.m2.1.1.3.1" xref="S3.SS2.SSS4.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS4.p1.2.m2.1.1.3.3" xref="S3.SS2.SSS4.p1.2.m2.1.1.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p1.2.m2.1.1.3.1a" xref="S3.SS2.SSS4.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS4.p1.2.m2.1.1.3.4" xref="S3.SS2.SSS4.p1.2.m2.1.1.3.4.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p1.2.m2.1b"><apply id="S3.SS2.SSS4.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS4.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS4.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS4.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS4.p1.2.m2.1.1.2">𝐿</ci><apply id="S3.SS2.SSS4.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS4.p1.2.m2.1.1.3"><times id="S3.SS2.SSS4.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.SSS4.p1.2.m2.1.1.3.1"></times><ci id="S3.SS2.SSS4.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.SSS4.p1.2.m2.1.1.3.2">𝑟</ci><ci id="S3.SS2.SSS4.p1.2.m2.1.1.3.3.cmml" xref="S3.SS2.SSS4.p1.2.m2.1.1.3.3">𝑝</ci><ci id="S3.SS2.SSS4.p1.2.m2.1.1.3.4.cmml" xref="S3.SS2.SSS4.p1.2.m2.1.1.3.4">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p1.2.m2.1c">L_{rpn}</annotation></semantics></math> represents the RPN loss (<a href="#S3.E2" title="In 3.2.4 Overall Formulation ‣ 3.2 Cross-dataset Model Training ‣ 3 Method ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) and <math id="S3.SS2.SSS4.p1.3.m3.1" class="ltx_Math" alttext="L_{head}" display="inline"><semantics id="S3.SS2.SSS4.p1.3.m3.1a"><msub id="S3.SS2.SSS4.p1.3.m3.1.1" xref="S3.SS2.SSS4.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS4.p1.3.m3.1.1.2" xref="S3.SS2.SSS4.p1.3.m3.1.1.2.cmml">L</mi><mrow id="S3.SS2.SSS4.p1.3.m3.1.1.3" xref="S3.SS2.SSS4.p1.3.m3.1.1.3.cmml"><mi id="S3.SS2.SSS4.p1.3.m3.1.1.3.2" xref="S3.SS2.SSS4.p1.3.m3.1.1.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p1.3.m3.1.1.3.1" xref="S3.SS2.SSS4.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS4.p1.3.m3.1.1.3.3" xref="S3.SS2.SSS4.p1.3.m3.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p1.3.m3.1.1.3.1a" xref="S3.SS2.SSS4.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS4.p1.3.m3.1.1.3.4" xref="S3.SS2.SSS4.p1.3.m3.1.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p1.3.m3.1.1.3.1b" xref="S3.SS2.SSS4.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS4.p1.3.m3.1.1.3.5" xref="S3.SS2.SSS4.p1.3.m3.1.1.3.5.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p1.3.m3.1b"><apply id="S3.SS2.SSS4.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS4.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS4.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS4.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS4.p1.3.m3.1.1.2">𝐿</ci><apply id="S3.SS2.SSS4.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS4.p1.3.m3.1.1.3"><times id="S3.SS2.SSS4.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.SSS4.p1.3.m3.1.1.3.1"></times><ci id="S3.SS2.SSS4.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.SSS4.p1.3.m3.1.1.3.2">ℎ</ci><ci id="S3.SS2.SSS4.p1.3.m3.1.1.3.3.cmml" xref="S3.SS2.SSS4.p1.3.m3.1.1.3.3">𝑒</ci><ci id="S3.SS2.SSS4.p1.3.m3.1.1.3.4.cmml" xref="S3.SS2.SSS4.p1.3.m3.1.1.3.4">𝑎</ci><ci id="S3.SS2.SSS4.p1.3.m3.1.1.3.5.cmml" xref="S3.SS2.SSS4.p1.3.m3.1.1.3.5">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p1.3.m3.1c">L_{head}</annotation></semantics></math> represents the detector head loss (<a href="#S3.E3" title="In 3.2.4 Overall Formulation ‣ 3.2 Cross-dataset Model Training ‣ 3 Method ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).
In the detector head loss <math id="S3.SS2.SSS4.p1.4.m4.1" class="ltx_Math" alttext="L_{head}" display="inline"><semantics id="S3.SS2.SSS4.p1.4.m4.1a"><msub id="S3.SS2.SSS4.p1.4.m4.1.1" xref="S3.SS2.SSS4.p1.4.m4.1.1.cmml"><mi id="S3.SS2.SSS4.p1.4.m4.1.1.2" xref="S3.SS2.SSS4.p1.4.m4.1.1.2.cmml">L</mi><mrow id="S3.SS2.SSS4.p1.4.m4.1.1.3" xref="S3.SS2.SSS4.p1.4.m4.1.1.3.cmml"><mi id="S3.SS2.SSS4.p1.4.m4.1.1.3.2" xref="S3.SS2.SSS4.p1.4.m4.1.1.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p1.4.m4.1.1.3.1" xref="S3.SS2.SSS4.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS4.p1.4.m4.1.1.3.3" xref="S3.SS2.SSS4.p1.4.m4.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p1.4.m4.1.1.3.1a" xref="S3.SS2.SSS4.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS4.p1.4.m4.1.1.3.4" xref="S3.SS2.SSS4.p1.4.m4.1.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p1.4.m4.1.1.3.1b" xref="S3.SS2.SSS4.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS4.p1.4.m4.1.1.3.5" xref="S3.SS2.SSS4.p1.4.m4.1.1.3.5.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p1.4.m4.1b"><apply id="S3.SS2.SSS4.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS4.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS4.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS4.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS4.p1.4.m4.1.1.2">𝐿</ci><apply id="S3.SS2.SSS4.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS4.p1.4.m4.1.1.3"><times id="S3.SS2.SSS4.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.SSS4.p1.4.m4.1.1.3.1"></times><ci id="S3.SS2.SSS4.p1.4.m4.1.1.3.2.cmml" xref="S3.SS2.SSS4.p1.4.m4.1.1.3.2">ℎ</ci><ci id="S3.SS2.SSS4.p1.4.m4.1.1.3.3.cmml" xref="S3.SS2.SSS4.p1.4.m4.1.1.3.3">𝑒</ci><ci id="S3.SS2.SSS4.p1.4.m4.1.1.3.4.cmml" xref="S3.SS2.SSS4.p1.4.m4.1.1.3.4">𝑎</ci><ci id="S3.SS2.SSS4.p1.4.m4.1.1.3.5.cmml" xref="S3.SS2.SSS4.p1.4.m4.1.1.3.5">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p1.4.m4.1c">L_{head}</annotation></semantics></math>, the classification loss <math id="S3.SS2.SSS4.p1.5.m5.1" class="ltx_Math" alttext="L_{cls}" display="inline"><semantics id="S3.SS2.SSS4.p1.5.m5.1a"><msub id="S3.SS2.SSS4.p1.5.m5.1.1" xref="S3.SS2.SSS4.p1.5.m5.1.1.cmml"><mi id="S3.SS2.SSS4.p1.5.m5.1.1.2" xref="S3.SS2.SSS4.p1.5.m5.1.1.2.cmml">L</mi><mrow id="S3.SS2.SSS4.p1.5.m5.1.1.3" xref="S3.SS2.SSS4.p1.5.m5.1.1.3.cmml"><mi id="S3.SS2.SSS4.p1.5.m5.1.1.3.2" xref="S3.SS2.SSS4.p1.5.m5.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p1.5.m5.1.1.3.1" xref="S3.SS2.SSS4.p1.5.m5.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS4.p1.5.m5.1.1.3.3" xref="S3.SS2.SSS4.p1.5.m5.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p1.5.m5.1.1.3.1a" xref="S3.SS2.SSS4.p1.5.m5.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS4.p1.5.m5.1.1.3.4" xref="S3.SS2.SSS4.p1.5.m5.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p1.5.m5.1b"><apply id="S3.SS2.SSS4.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS4.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS4.p1.5.m5.1.1.1.cmml" xref="S3.SS2.SSS4.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS4.p1.5.m5.1.1.2.cmml" xref="S3.SS2.SSS4.p1.5.m5.1.1.2">𝐿</ci><apply id="S3.SS2.SSS4.p1.5.m5.1.1.3.cmml" xref="S3.SS2.SSS4.p1.5.m5.1.1.3"><times id="S3.SS2.SSS4.p1.5.m5.1.1.3.1.cmml" xref="S3.SS2.SSS4.p1.5.m5.1.1.3.1"></times><ci id="S3.SS2.SSS4.p1.5.m5.1.1.3.2.cmml" xref="S3.SS2.SSS4.p1.5.m5.1.1.3.2">𝑐</ci><ci id="S3.SS2.SSS4.p1.5.m5.1.1.3.3.cmml" xref="S3.SS2.SSS4.p1.5.m5.1.1.3.3">𝑙</ci><ci id="S3.SS2.SSS4.p1.5.m5.1.1.3.4.cmml" xref="S3.SS2.SSS4.p1.5.m5.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p1.5.m5.1c">L_{cls}</annotation></semantics></math> is given in (<a href="#S3.E4" title="In 3.2.4 Overall Formulation ‣ 3.2 Cross-dataset Model Training ‣ 3 Method ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<div id="S3.SS2.SSS4.p2" class="ltx_para">
<p id="S3.SS2.SSS4.p2.23" class="ltx_p">In formulas (<a href="#S3.E2" title="In 3.2.4 Overall Formulation ‣ 3.2 Cross-dataset Model Training ‣ 3 Method ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), (<a href="#S3.E3" title="In 3.2.4 Overall Formulation ‣ 3.2 Cross-dataset Model Training ‣ 3 Method ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), and (<a href="#S3.E4" title="In 3.2.4 Overall Formulation ‣ 3.2 Cross-dataset Model Training ‣ 3 Method ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), the symbols <math id="S3.SS2.SSS4.p2.1.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S3.SS2.SSS4.p2.1.m1.1a"><mi id="S3.SS2.SSS4.p2.1.m1.1.1" xref="S3.SS2.SSS4.p2.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p2.1.m1.1b"><ci id="S3.SS2.SSS4.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS4.p2.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p2.1.m1.1c">p</annotation></semantics></math>, <math id="S3.SS2.SSS4.p2.2.m2.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.SS2.SSS4.p2.2.m2.1a"><mi id="S3.SS2.SSS4.p2.2.m2.1.1" xref="S3.SS2.SSS4.p2.2.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p2.2.m2.1b"><ci id="S3.SS2.SSS4.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS4.p2.2.m2.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p2.2.m2.1c">q</annotation></semantics></math>, <math id="S3.SS2.SSS4.p2.3.m3.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS2.SSS4.p2.3.m3.1a"><mi id="S3.SS2.SSS4.p2.3.m3.1.1" xref="S3.SS2.SSS4.p2.3.m3.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p2.3.m3.1b"><ci id="S3.SS2.SSS4.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS4.p2.3.m3.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p2.3.m3.1c">r</annotation></semantics></math>, and <math id="S3.SS2.SSS4.p2.4.m4.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS2.SSS4.p2.4.m4.1a"><mi id="S3.SS2.SSS4.p2.4.m4.1.1" xref="S3.SS2.SSS4.p2.4.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p2.4.m4.1b"><ci id="S3.SS2.SSS4.p2.4.m4.1.1.cmml" xref="S3.SS2.SSS4.p2.4.m4.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p2.4.m4.1c">x</annotation></semantics></math> denote the respective outputs for RPN regression branch, RPN classification branch, the detector head regression branch, and the detector head classification branch, while the <math id="S3.SS2.SSS4.p2.5.m5.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS2.SSS4.p2.5.m5.1a"><mi id="S3.SS2.SSS4.p2.5.m5.1.1" xref="S3.SS2.SSS4.p2.5.m5.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p2.5.m5.1b"><ci id="S3.SS2.SSS4.p2.5.m5.1.1.cmml" xref="S3.SS2.SSS4.p2.5.m5.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p2.5.m5.1c">y</annotation></semantics></math> denotes the corresponding ground truth.
<math id="S3.SS2.SSS4.p2.6.m6.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.SSS4.p2.6.m6.1a"><mi id="S3.SS2.SSS4.p2.6.m6.1.1" xref="S3.SS2.SSS4.p2.6.m6.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p2.6.m6.1b"><ci id="S3.SS2.SSS4.p2.6.m6.1.1.cmml" xref="S3.SS2.SSS4.p2.6.m6.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p2.6.m6.1c">N</annotation></semantics></math> is the number of samples in each mini-batch.
<math id="S3.SS2.SSS4.p2.7.m7.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS2.SSS4.p2.7.m7.1a"><mi id="S3.SS2.SSS4.p2.7.m7.1.1" xref="S3.SS2.SSS4.p2.7.m7.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p2.7.m7.1b"><ci id="S3.SS2.SSS4.p2.7.m7.1.1.cmml" xref="S3.SS2.SSS4.p2.7.m7.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p2.7.m7.1c">C</annotation></semantics></math> is the number of categories including <em id="S3.SS2.SSS4.p2.23.1" class="ltx_emph ltx_font_italic">background</em> in the unified label space.
<math id="S3.SS2.SSS4.p2.8.m8.1" class="ltx_Math" alttext="S_{rpn}" display="inline"><semantics id="S3.SS2.SSS4.p2.8.m8.1a"><msub id="S3.SS2.SSS4.p2.8.m8.1.1" xref="S3.SS2.SSS4.p2.8.m8.1.1.cmml"><mi id="S3.SS2.SSS4.p2.8.m8.1.1.2" xref="S3.SS2.SSS4.p2.8.m8.1.1.2.cmml">S</mi><mrow id="S3.SS2.SSS4.p2.8.m8.1.1.3" xref="S3.SS2.SSS4.p2.8.m8.1.1.3.cmml"><mi id="S3.SS2.SSS4.p2.8.m8.1.1.3.2" xref="S3.SS2.SSS4.p2.8.m8.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p2.8.m8.1.1.3.1" xref="S3.SS2.SSS4.p2.8.m8.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS4.p2.8.m8.1.1.3.3" xref="S3.SS2.SSS4.p2.8.m8.1.1.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p2.8.m8.1.1.3.1a" xref="S3.SS2.SSS4.p2.8.m8.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS4.p2.8.m8.1.1.3.4" xref="S3.SS2.SSS4.p2.8.m8.1.1.3.4.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p2.8.m8.1b"><apply id="S3.SS2.SSS4.p2.8.m8.1.1.cmml" xref="S3.SS2.SSS4.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS4.p2.8.m8.1.1.1.cmml" xref="S3.SS2.SSS4.p2.8.m8.1.1">subscript</csymbol><ci id="S3.SS2.SSS4.p2.8.m8.1.1.2.cmml" xref="S3.SS2.SSS4.p2.8.m8.1.1.2">𝑆</ci><apply id="S3.SS2.SSS4.p2.8.m8.1.1.3.cmml" xref="S3.SS2.SSS4.p2.8.m8.1.1.3"><times id="S3.SS2.SSS4.p2.8.m8.1.1.3.1.cmml" xref="S3.SS2.SSS4.p2.8.m8.1.1.3.1"></times><ci id="S3.SS2.SSS4.p2.8.m8.1.1.3.2.cmml" xref="S3.SS2.SSS4.p2.8.m8.1.1.3.2">𝑟</ci><ci id="S3.SS2.SSS4.p2.8.m8.1.1.3.3.cmml" xref="S3.SS2.SSS4.p2.8.m8.1.1.3.3">𝑝</ci><ci id="S3.SS2.SSS4.p2.8.m8.1.1.3.4.cmml" xref="S3.SS2.SSS4.p2.8.m8.1.1.3.4">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p2.8.m8.1c">S_{rpn}</annotation></semantics></math> represents the number of stages of the Cascade RPN while <math id="S3.SS2.SSS4.p2.9.m9.1" class="ltx_Math" alttext="S_{head}" display="inline"><semantics id="S3.SS2.SSS4.p2.9.m9.1a"><msub id="S3.SS2.SSS4.p2.9.m9.1.1" xref="S3.SS2.SSS4.p2.9.m9.1.1.cmml"><mi id="S3.SS2.SSS4.p2.9.m9.1.1.2" xref="S3.SS2.SSS4.p2.9.m9.1.1.2.cmml">S</mi><mrow id="S3.SS2.SSS4.p2.9.m9.1.1.3" xref="S3.SS2.SSS4.p2.9.m9.1.1.3.cmml"><mi id="S3.SS2.SSS4.p2.9.m9.1.1.3.2" xref="S3.SS2.SSS4.p2.9.m9.1.1.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p2.9.m9.1.1.3.1" xref="S3.SS2.SSS4.p2.9.m9.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS4.p2.9.m9.1.1.3.3" xref="S3.SS2.SSS4.p2.9.m9.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p2.9.m9.1.1.3.1a" xref="S3.SS2.SSS4.p2.9.m9.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS4.p2.9.m9.1.1.3.4" xref="S3.SS2.SSS4.p2.9.m9.1.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p2.9.m9.1.1.3.1b" xref="S3.SS2.SSS4.p2.9.m9.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS4.p2.9.m9.1.1.3.5" xref="S3.SS2.SSS4.p2.9.m9.1.1.3.5.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p2.9.m9.1b"><apply id="S3.SS2.SSS4.p2.9.m9.1.1.cmml" xref="S3.SS2.SSS4.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS4.p2.9.m9.1.1.1.cmml" xref="S3.SS2.SSS4.p2.9.m9.1.1">subscript</csymbol><ci id="S3.SS2.SSS4.p2.9.m9.1.1.2.cmml" xref="S3.SS2.SSS4.p2.9.m9.1.1.2">𝑆</ci><apply id="S3.SS2.SSS4.p2.9.m9.1.1.3.cmml" xref="S3.SS2.SSS4.p2.9.m9.1.1.3"><times id="S3.SS2.SSS4.p2.9.m9.1.1.3.1.cmml" xref="S3.SS2.SSS4.p2.9.m9.1.1.3.1"></times><ci id="S3.SS2.SSS4.p2.9.m9.1.1.3.2.cmml" xref="S3.SS2.SSS4.p2.9.m9.1.1.3.2">ℎ</ci><ci id="S3.SS2.SSS4.p2.9.m9.1.1.3.3.cmml" xref="S3.SS2.SSS4.p2.9.m9.1.1.3.3">𝑒</ci><ci id="S3.SS2.SSS4.p2.9.m9.1.1.3.4.cmml" xref="S3.SS2.SSS4.p2.9.m9.1.1.3.4">𝑎</ci><ci id="S3.SS2.SSS4.p2.9.m9.1.1.3.5.cmml" xref="S3.SS2.SSS4.p2.9.m9.1.1.3.5">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p2.9.m9.1c">S_{head}</annotation></semantics></math> represents the number of stages of Cascade R-CNN.
We set <math id="S3.SS2.SSS4.p2.10.m10.1" class="ltx_Math" alttext="S_{rpn}" display="inline"><semantics id="S3.SS2.SSS4.p2.10.m10.1a"><msub id="S3.SS2.SSS4.p2.10.m10.1.1" xref="S3.SS2.SSS4.p2.10.m10.1.1.cmml"><mi id="S3.SS2.SSS4.p2.10.m10.1.1.2" xref="S3.SS2.SSS4.p2.10.m10.1.1.2.cmml">S</mi><mrow id="S3.SS2.SSS4.p2.10.m10.1.1.3" xref="S3.SS2.SSS4.p2.10.m10.1.1.3.cmml"><mi id="S3.SS2.SSS4.p2.10.m10.1.1.3.2" xref="S3.SS2.SSS4.p2.10.m10.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p2.10.m10.1.1.3.1" xref="S3.SS2.SSS4.p2.10.m10.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS4.p2.10.m10.1.1.3.3" xref="S3.SS2.SSS4.p2.10.m10.1.1.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p2.10.m10.1.1.3.1a" xref="S3.SS2.SSS4.p2.10.m10.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS4.p2.10.m10.1.1.3.4" xref="S3.SS2.SSS4.p2.10.m10.1.1.3.4.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p2.10.m10.1b"><apply id="S3.SS2.SSS4.p2.10.m10.1.1.cmml" xref="S3.SS2.SSS4.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS4.p2.10.m10.1.1.1.cmml" xref="S3.SS2.SSS4.p2.10.m10.1.1">subscript</csymbol><ci id="S3.SS2.SSS4.p2.10.m10.1.1.2.cmml" xref="S3.SS2.SSS4.p2.10.m10.1.1.2">𝑆</ci><apply id="S3.SS2.SSS4.p2.10.m10.1.1.3.cmml" xref="S3.SS2.SSS4.p2.10.m10.1.1.3"><times id="S3.SS2.SSS4.p2.10.m10.1.1.3.1.cmml" xref="S3.SS2.SSS4.p2.10.m10.1.1.3.1"></times><ci id="S3.SS2.SSS4.p2.10.m10.1.1.3.2.cmml" xref="S3.SS2.SSS4.p2.10.m10.1.1.3.2">𝑟</ci><ci id="S3.SS2.SSS4.p2.10.m10.1.1.3.3.cmml" xref="S3.SS2.SSS4.p2.10.m10.1.1.3.3">𝑝</ci><ci id="S3.SS2.SSS4.p2.10.m10.1.1.3.4.cmml" xref="S3.SS2.SSS4.p2.10.m10.1.1.3.4">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p2.10.m10.1c">S_{rpn}</annotation></semantics></math> to 2 and <math id="S3.SS2.SSS4.p2.11.m11.1" class="ltx_Math" alttext="S_{head}" display="inline"><semantics id="S3.SS2.SSS4.p2.11.m11.1a"><msub id="S3.SS2.SSS4.p2.11.m11.1.1" xref="S3.SS2.SSS4.p2.11.m11.1.1.cmml"><mi id="S3.SS2.SSS4.p2.11.m11.1.1.2" xref="S3.SS2.SSS4.p2.11.m11.1.1.2.cmml">S</mi><mrow id="S3.SS2.SSS4.p2.11.m11.1.1.3" xref="S3.SS2.SSS4.p2.11.m11.1.1.3.cmml"><mi id="S3.SS2.SSS4.p2.11.m11.1.1.3.2" xref="S3.SS2.SSS4.p2.11.m11.1.1.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p2.11.m11.1.1.3.1" xref="S3.SS2.SSS4.p2.11.m11.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS4.p2.11.m11.1.1.3.3" xref="S3.SS2.SSS4.p2.11.m11.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p2.11.m11.1.1.3.1a" xref="S3.SS2.SSS4.p2.11.m11.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS4.p2.11.m11.1.1.3.4" xref="S3.SS2.SSS4.p2.11.m11.1.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p2.11.m11.1.1.3.1b" xref="S3.SS2.SSS4.p2.11.m11.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS4.p2.11.m11.1.1.3.5" xref="S3.SS2.SSS4.p2.11.m11.1.1.3.5.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p2.11.m11.1b"><apply id="S3.SS2.SSS4.p2.11.m11.1.1.cmml" xref="S3.SS2.SSS4.p2.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS4.p2.11.m11.1.1.1.cmml" xref="S3.SS2.SSS4.p2.11.m11.1.1">subscript</csymbol><ci id="S3.SS2.SSS4.p2.11.m11.1.1.2.cmml" xref="S3.SS2.SSS4.p2.11.m11.1.1.2">𝑆</ci><apply id="S3.SS2.SSS4.p2.11.m11.1.1.3.cmml" xref="S3.SS2.SSS4.p2.11.m11.1.1.3"><times id="S3.SS2.SSS4.p2.11.m11.1.1.3.1.cmml" xref="S3.SS2.SSS4.p2.11.m11.1.1.3.1"></times><ci id="S3.SS2.SSS4.p2.11.m11.1.1.3.2.cmml" xref="S3.SS2.SSS4.p2.11.m11.1.1.3.2">ℎ</ci><ci id="S3.SS2.SSS4.p2.11.m11.1.1.3.3.cmml" xref="S3.SS2.SSS4.p2.11.m11.1.1.3.3">𝑒</ci><ci id="S3.SS2.SSS4.p2.11.m11.1.1.3.4.cmml" xref="S3.SS2.SSS4.p2.11.m11.1.1.3.4">𝑎</ci><ci id="S3.SS2.SSS4.p2.11.m11.1.1.3.5.cmml" xref="S3.SS2.SSS4.p2.11.m11.1.1.3.5">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p2.11.m11.1c">S_{head}</annotation></semantics></math> to 3 for a performance-cost trade-off reason.
<math id="S3.SS2.SSS4.p2.12.m12.1" class="ltx_Math" alttext="IoU" display="inline"><semantics id="S3.SS2.SSS4.p2.12.m12.1a"><mrow id="S3.SS2.SSS4.p2.12.m12.1.1" xref="S3.SS2.SSS4.p2.12.m12.1.1.cmml"><mi id="S3.SS2.SSS4.p2.12.m12.1.1.2" xref="S3.SS2.SSS4.p2.12.m12.1.1.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p2.12.m12.1.1.1" xref="S3.SS2.SSS4.p2.12.m12.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS4.p2.12.m12.1.1.3" xref="S3.SS2.SSS4.p2.12.m12.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p2.12.m12.1.1.1a" xref="S3.SS2.SSS4.p2.12.m12.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS4.p2.12.m12.1.1.4" xref="S3.SS2.SSS4.p2.12.m12.1.1.4.cmml">U</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p2.12.m12.1b"><apply id="S3.SS2.SSS4.p2.12.m12.1.1.cmml" xref="S3.SS2.SSS4.p2.12.m12.1.1"><times id="S3.SS2.SSS4.p2.12.m12.1.1.1.cmml" xref="S3.SS2.SSS4.p2.12.m12.1.1.1"></times><ci id="S3.SS2.SSS4.p2.12.m12.1.1.2.cmml" xref="S3.SS2.SSS4.p2.12.m12.1.1.2">𝐼</ci><ci id="S3.SS2.SSS4.p2.12.m12.1.1.3.cmml" xref="S3.SS2.SSS4.p2.12.m12.1.1.3">𝑜</ci><ci id="S3.SS2.SSS4.p2.12.m12.1.1.4.cmml" xref="S3.SS2.SSS4.p2.12.m12.1.1.4">𝑈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p2.12.m12.1c">IoU</annotation></semantics></math> represents the IoU loss <cite class="ltx_cite ltx_citemacro_citep">(Yu <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2016</a>)</cite>, <math id="S3.SS2.SSS4.p2.13.m13.1" class="ltx_Math" alttext="BCE" display="inline"><semantics id="S3.SS2.SSS4.p2.13.m13.1a"><mrow id="S3.SS2.SSS4.p2.13.m13.1.1" xref="S3.SS2.SSS4.p2.13.m13.1.1.cmml"><mi id="S3.SS2.SSS4.p2.13.m13.1.1.2" xref="S3.SS2.SSS4.p2.13.m13.1.1.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p2.13.m13.1.1.1" xref="S3.SS2.SSS4.p2.13.m13.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS4.p2.13.m13.1.1.3" xref="S3.SS2.SSS4.p2.13.m13.1.1.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p2.13.m13.1.1.1a" xref="S3.SS2.SSS4.p2.13.m13.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS4.p2.13.m13.1.1.4" xref="S3.SS2.SSS4.p2.13.m13.1.1.4.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p2.13.m13.1b"><apply id="S3.SS2.SSS4.p2.13.m13.1.1.cmml" xref="S3.SS2.SSS4.p2.13.m13.1.1"><times id="S3.SS2.SSS4.p2.13.m13.1.1.1.cmml" xref="S3.SS2.SSS4.p2.13.m13.1.1.1"></times><ci id="S3.SS2.SSS4.p2.13.m13.1.1.2.cmml" xref="S3.SS2.SSS4.p2.13.m13.1.1.2">𝐵</ci><ci id="S3.SS2.SSS4.p2.13.m13.1.1.3.cmml" xref="S3.SS2.SSS4.p2.13.m13.1.1.3">𝐶</ci><ci id="S3.SS2.SSS4.p2.13.m13.1.1.4.cmml" xref="S3.SS2.SSS4.p2.13.m13.1.1.4">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p2.13.m13.1c">BCE</annotation></semantics></math> represents the binary cross entropy loss <cite class="ltx_cite ltx_citemacro_citep">(Girshick, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2015</a>)</cite>, and <math id="S3.SS2.SSS4.p2.14.m14.1" class="ltx_Math" alttext="SmoothL_{1}" display="inline"><semantics id="S3.SS2.SSS4.p2.14.m14.1a"><mrow id="S3.SS2.SSS4.p2.14.m14.1.1" xref="S3.SS2.SSS4.p2.14.m14.1.1.cmml"><mi id="S3.SS2.SSS4.p2.14.m14.1.1.2" xref="S3.SS2.SSS4.p2.14.m14.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p2.14.m14.1.1.1" xref="S3.SS2.SSS4.p2.14.m14.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS4.p2.14.m14.1.1.3" xref="S3.SS2.SSS4.p2.14.m14.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p2.14.m14.1.1.1a" xref="S3.SS2.SSS4.p2.14.m14.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS4.p2.14.m14.1.1.4" xref="S3.SS2.SSS4.p2.14.m14.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p2.14.m14.1.1.1b" xref="S3.SS2.SSS4.p2.14.m14.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS4.p2.14.m14.1.1.5" xref="S3.SS2.SSS4.p2.14.m14.1.1.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p2.14.m14.1.1.1c" xref="S3.SS2.SSS4.p2.14.m14.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS4.p2.14.m14.1.1.6" xref="S3.SS2.SSS4.p2.14.m14.1.1.6.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p2.14.m14.1.1.1d" xref="S3.SS2.SSS4.p2.14.m14.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS4.p2.14.m14.1.1.7" xref="S3.SS2.SSS4.p2.14.m14.1.1.7.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p2.14.m14.1.1.1e" xref="S3.SS2.SSS4.p2.14.m14.1.1.1.cmml">​</mo><msub id="S3.SS2.SSS4.p2.14.m14.1.1.8" xref="S3.SS2.SSS4.p2.14.m14.1.1.8.cmml"><mi id="S3.SS2.SSS4.p2.14.m14.1.1.8.2" xref="S3.SS2.SSS4.p2.14.m14.1.1.8.2.cmml">L</mi><mn id="S3.SS2.SSS4.p2.14.m14.1.1.8.3" xref="S3.SS2.SSS4.p2.14.m14.1.1.8.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p2.14.m14.1b"><apply id="S3.SS2.SSS4.p2.14.m14.1.1.cmml" xref="S3.SS2.SSS4.p2.14.m14.1.1"><times id="S3.SS2.SSS4.p2.14.m14.1.1.1.cmml" xref="S3.SS2.SSS4.p2.14.m14.1.1.1"></times><ci id="S3.SS2.SSS4.p2.14.m14.1.1.2.cmml" xref="S3.SS2.SSS4.p2.14.m14.1.1.2">𝑆</ci><ci id="S3.SS2.SSS4.p2.14.m14.1.1.3.cmml" xref="S3.SS2.SSS4.p2.14.m14.1.1.3">𝑚</ci><ci id="S3.SS2.SSS4.p2.14.m14.1.1.4.cmml" xref="S3.SS2.SSS4.p2.14.m14.1.1.4">𝑜</ci><ci id="S3.SS2.SSS4.p2.14.m14.1.1.5.cmml" xref="S3.SS2.SSS4.p2.14.m14.1.1.5">𝑜</ci><ci id="S3.SS2.SSS4.p2.14.m14.1.1.6.cmml" xref="S3.SS2.SSS4.p2.14.m14.1.1.6">𝑡</ci><ci id="S3.SS2.SSS4.p2.14.m14.1.1.7.cmml" xref="S3.SS2.SSS4.p2.14.m14.1.1.7">ℎ</ci><apply id="S3.SS2.SSS4.p2.14.m14.1.1.8.cmml" xref="S3.SS2.SSS4.p2.14.m14.1.1.8"><csymbol cd="ambiguous" id="S3.SS2.SSS4.p2.14.m14.1.1.8.1.cmml" xref="S3.SS2.SSS4.p2.14.m14.1.1.8">subscript</csymbol><ci id="S3.SS2.SSS4.p2.14.m14.1.1.8.2.cmml" xref="S3.SS2.SSS4.p2.14.m14.1.1.8.2">𝐿</ci><cn type="integer" id="S3.SS2.SSS4.p2.14.m14.1.1.8.3.cmml" xref="S3.SS2.SSS4.p2.14.m14.1.1.8.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p2.14.m14.1c">SmoothL_{1}</annotation></semantics></math> represents the smooth L1 loss <cite class="ltx_cite ltx_citemacro_citep">(Girshick, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2015</a>)</cite>.
<math id="S3.SS2.SSS4.p2.15.m15.1" class="ltx_Math" alttext="\mathds{1}_{\mathds{A}}(x)" display="inline"><semantics id="S3.SS2.SSS4.p2.15.m15.1a"><mrow id="S3.SS2.SSS4.p2.15.m15.1.2" xref="S3.SS2.SSS4.p2.15.m15.1.2.cmml"><msub id="S3.SS2.SSS4.p2.15.m15.1.2.2" xref="S3.SS2.SSS4.p2.15.m15.1.2.2.cmml"><mn id="S3.SS2.SSS4.p2.15.m15.1.2.2.2" xref="S3.SS2.SSS4.p2.15.m15.1.2.2.2.cmml">𝟙</mn><mi id="S3.SS2.SSS4.p2.15.m15.1.2.2.3" xref="S3.SS2.SSS4.p2.15.m15.1.2.2.3.cmml">𝔸</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p2.15.m15.1.2.1" xref="S3.SS2.SSS4.p2.15.m15.1.2.1.cmml">​</mo><mrow id="S3.SS2.SSS4.p2.15.m15.1.2.3.2" xref="S3.SS2.SSS4.p2.15.m15.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS4.p2.15.m15.1.2.3.2.1" xref="S3.SS2.SSS4.p2.15.m15.1.2.cmml">(</mo><mi id="S3.SS2.SSS4.p2.15.m15.1.1" xref="S3.SS2.SSS4.p2.15.m15.1.1.cmml">x</mi><mo stretchy="false" id="S3.SS2.SSS4.p2.15.m15.1.2.3.2.2" xref="S3.SS2.SSS4.p2.15.m15.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p2.15.m15.1b"><apply id="S3.SS2.SSS4.p2.15.m15.1.2.cmml" xref="S3.SS2.SSS4.p2.15.m15.1.2"><times id="S3.SS2.SSS4.p2.15.m15.1.2.1.cmml" xref="S3.SS2.SSS4.p2.15.m15.1.2.1"></times><apply id="S3.SS2.SSS4.p2.15.m15.1.2.2.cmml" xref="S3.SS2.SSS4.p2.15.m15.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS4.p2.15.m15.1.2.2.1.cmml" xref="S3.SS2.SSS4.p2.15.m15.1.2.2">subscript</csymbol><cn type="integer" id="S3.SS2.SSS4.p2.15.m15.1.2.2.2.cmml" xref="S3.SS2.SSS4.p2.15.m15.1.2.2.2">1</cn><ci id="S3.SS2.SSS4.p2.15.m15.1.2.2.3.cmml" xref="S3.SS2.SSS4.p2.15.m15.1.2.2.3">𝔸</ci></apply><ci id="S3.SS2.SSS4.p2.15.m15.1.1.cmml" xref="S3.SS2.SSS4.p2.15.m15.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p2.15.m15.1c">\mathds{1}_{\mathds{A}}(x)</annotation></semantics></math> represents the indicator function in which the result turns out 1 when <math id="S3.SS2.SSS4.p2.16.m16.1" class="ltx_Math" alttext="x\in\mathds{A}" display="inline"><semantics id="S3.SS2.SSS4.p2.16.m16.1a"><mrow id="S3.SS2.SSS4.p2.16.m16.1.1" xref="S3.SS2.SSS4.p2.16.m16.1.1.cmml"><mi id="S3.SS2.SSS4.p2.16.m16.1.1.2" xref="S3.SS2.SSS4.p2.16.m16.1.1.2.cmml">x</mi><mo id="S3.SS2.SSS4.p2.16.m16.1.1.1" xref="S3.SS2.SSS4.p2.16.m16.1.1.1.cmml">∈</mo><mi id="S3.SS2.SSS4.p2.16.m16.1.1.3" xref="S3.SS2.SSS4.p2.16.m16.1.1.3.cmml">𝔸</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p2.16.m16.1b"><apply id="S3.SS2.SSS4.p2.16.m16.1.1.cmml" xref="S3.SS2.SSS4.p2.16.m16.1.1"><in id="S3.SS2.SSS4.p2.16.m16.1.1.1.cmml" xref="S3.SS2.SSS4.p2.16.m16.1.1.1"></in><ci id="S3.SS2.SSS4.p2.16.m16.1.1.2.cmml" xref="S3.SS2.SSS4.p2.16.m16.1.1.2">𝑥</ci><ci id="S3.SS2.SSS4.p2.16.m16.1.1.3.cmml" xref="S3.SS2.SSS4.p2.16.m16.1.1.3">𝔸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p2.16.m16.1c">x\in\mathds{A}</annotation></semantics></math>.
Specifically, <math id="S3.SS2.SSS4.p2.17.m17.1" class="ltx_Math" alttext="\mathds{D}(y)" display="inline"><semantics id="S3.SS2.SSS4.p2.17.m17.1a"><mrow id="S3.SS2.SSS4.p2.17.m17.1.2" xref="S3.SS2.SSS4.p2.17.m17.1.2.cmml"><mi id="S3.SS2.SSS4.p2.17.m17.1.2.2" xref="S3.SS2.SSS4.p2.17.m17.1.2.2.cmml">𝔻</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p2.17.m17.1.2.1" xref="S3.SS2.SSS4.p2.17.m17.1.2.1.cmml">​</mo><mrow id="S3.SS2.SSS4.p2.17.m17.1.2.3.2" xref="S3.SS2.SSS4.p2.17.m17.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS4.p2.17.m17.1.2.3.2.1" xref="S3.SS2.SSS4.p2.17.m17.1.2.cmml">(</mo><mi id="S3.SS2.SSS4.p2.17.m17.1.1" xref="S3.SS2.SSS4.p2.17.m17.1.1.cmml">y</mi><mo stretchy="false" id="S3.SS2.SSS4.p2.17.m17.1.2.3.2.2" xref="S3.SS2.SSS4.p2.17.m17.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p2.17.m17.1b"><apply id="S3.SS2.SSS4.p2.17.m17.1.2.cmml" xref="S3.SS2.SSS4.p2.17.m17.1.2"><times id="S3.SS2.SSS4.p2.17.m17.1.2.1.cmml" xref="S3.SS2.SSS4.p2.17.m17.1.2.1"></times><ci id="S3.SS2.SSS4.p2.17.m17.1.2.2.cmml" xref="S3.SS2.SSS4.p2.17.m17.1.2.2">𝔻</ci><ci id="S3.SS2.SSS4.p2.17.m17.1.1.cmml" xref="S3.SS2.SSS4.p2.17.m17.1.1">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p2.17.m17.1c">\mathds{D}(y)</annotation></semantics></math> denotes the union of the categories involved in HCLS for class <math id="S3.SS2.SSS4.p2.18.m18.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS2.SSS4.p2.18.m18.1a"><mi id="S3.SS2.SSS4.p2.18.m18.1.1" xref="S3.SS2.SSS4.p2.18.m18.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p2.18.m18.1b"><ci id="S3.SS2.SSS4.p2.18.m18.1.1.cmml" xref="S3.SS2.SSS4.p2.18.m18.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p2.18.m18.1c">y</annotation></semantics></math>, as described in Section <a href="#S3.SS2.SSS3" title="3.2.3 Hierarchy-aware Cross-dataset Loss Suppression ‣ 3.2 Cross-dataset Model Training ‣ 3 Method ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.3</span></a>.
<math id="S3.SS2.SSS4.p2.19.m19.1" class="ltx_Math" alttext="\mathds{P}(y)" display="inline"><semantics id="S3.SS2.SSS4.p2.19.m19.1a"><mrow id="S3.SS2.SSS4.p2.19.m19.1.2" xref="S3.SS2.SSS4.p2.19.m19.1.2.cmml"><mi id="S3.SS2.SSS4.p2.19.m19.1.2.2" xref="S3.SS2.SSS4.p2.19.m19.1.2.2.cmml">ℙ</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS4.p2.19.m19.1.2.1" xref="S3.SS2.SSS4.p2.19.m19.1.2.1.cmml">​</mo><mrow id="S3.SS2.SSS4.p2.19.m19.1.2.3.2" xref="S3.SS2.SSS4.p2.19.m19.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS4.p2.19.m19.1.2.3.2.1" xref="S3.SS2.SSS4.p2.19.m19.1.2.cmml">(</mo><mi id="S3.SS2.SSS4.p2.19.m19.1.1" xref="S3.SS2.SSS4.p2.19.m19.1.1.cmml">y</mi><mo stretchy="false" id="S3.SS2.SSS4.p2.19.m19.1.2.3.2.2" xref="S3.SS2.SSS4.p2.19.m19.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p2.19.m19.1b"><apply id="S3.SS2.SSS4.p2.19.m19.1.2.cmml" xref="S3.SS2.SSS4.p2.19.m19.1.2"><times id="S3.SS2.SSS4.p2.19.m19.1.2.1.cmml" xref="S3.SS2.SSS4.p2.19.m19.1.2.1"></times><ci id="S3.SS2.SSS4.p2.19.m19.1.2.2.cmml" xref="S3.SS2.SSS4.p2.19.m19.1.2.2">ℙ</ci><ci id="S3.SS2.SSS4.p2.19.m19.1.1.cmml" xref="S3.SS2.SSS4.p2.19.m19.1.1">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p2.19.m19.1c">\mathds{P}(y)</annotation></semantics></math> denotes the union of the parent categories of class <math id="S3.SS2.SSS4.p2.20.m20.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS2.SSS4.p2.20.m20.1a"><mi id="S3.SS2.SSS4.p2.20.m20.1.1" xref="S3.SS2.SSS4.p2.20.m20.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p2.20.m20.1b"><ci id="S3.SS2.SSS4.p2.20.m20.1.1.cmml" xref="S3.SS2.SSS4.p2.20.m20.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p2.20.m20.1c">y</annotation></semantics></math> as described in Section <a href="#S3.SS2.SSS2" title="3.2.2 Multi-label with Hierarchical Taxonomy Completion ‣ 3.2 Cross-dataset Model Training ‣ 3 Method ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>.
To achieve a balanced weighting of each component of the loss, we empirically set the weights <math id="S3.SS2.SSS4.p2.21.m21.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS2.SSS4.p2.21.m21.1a"><mi id="S3.SS2.SSS4.p2.21.m21.1.1" xref="S3.SS2.SSS4.p2.21.m21.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p2.21.m21.1b"><ci id="S3.SS2.SSS4.p2.21.m21.1.1.cmml" xref="S3.SS2.SSS4.p2.21.m21.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p2.21.m21.1c">\alpha</annotation></semantics></math>, <math id="S3.SS2.SSS4.p2.22.m22.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S3.SS2.SSS4.p2.22.m22.1a"><mi id="S3.SS2.SSS4.p2.22.m22.1.1" xref="S3.SS2.SSS4.p2.22.m22.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p2.22.m22.1b"><ci id="S3.SS2.SSS4.p2.22.m22.1.1.cmml" xref="S3.SS2.SSS4.p2.22.m22.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p2.22.m22.1c">\beta</annotation></semantics></math>, and <math id="S3.SS2.SSS4.p2.23.m23.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S3.SS2.SSS4.p2.23.m23.1a"><mi id="S3.SS2.SSS4.p2.23.m23.1.1" xref="S3.SS2.SSS4.p2.23.m23.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS4.p2.23.m23.1b"><ci id="S3.SS2.SSS4.p2.23.m23.1.1.cmml" xref="S3.SS2.SSS4.p2.23.m23.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS4.p2.23.m23.1c">\gamma</annotation></semantics></math> to 10.0, 1.0, and 1.5, respectively.</p>
</div>
<figure id="S3.SS2.SSS4.fig1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="A1.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_figure_panel ltx_eqn_table">

<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\displaystyle L_{rpn}=\frac{1}{N}\sum_{i=0}^{N}\sum_{s=0}^{S_{rpn}}(\alpha\cdot(1-{IoU}(p_{s}^{i},~{}y_{rloc}^{i}))+{BCE}(q_{s}^{i},~{}y_{rcls}^{i}))" display="inline"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><msub id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml">L</mi><mrow id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.3.3.2" xref="S3.E2.m1.1.1.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.3.1" xref="S3.E2.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.3.3" xref="S3.E2.m1.1.1.3.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.3.1a" xref="S3.E2.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.3.4" xref="S3.E2.m1.1.1.3.3.4.cmml">n</mi></mrow></msub><mo id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.3.cmml"><mfrac id="S3.E2.m1.1.1.1.3a" xref="S3.E2.m1.1.1.1.3.cmml"><mn id="S3.E2.m1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.3.2.cmml">1</mn><mi id="S3.E2.m1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.3.3.cmml">N</mi></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml"><munderover id="S3.E2.m1.1.1.1.1.2a" xref="S3.E2.m1.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E2.m1.1.1.1.1.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E2.m1.1.1.1.1.2.2.3" xref="S3.E2.m1.1.1.1.1.2.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2.3.2" xref="S3.E2.m1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E2.m1.1.1.1.1.2.2.3.1" xref="S3.E2.m1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E2.m1.1.1.1.1.2.2.3.3" xref="S3.E2.m1.1.1.1.1.2.2.3.3.cmml">0</mn></mrow><mi id="S3.E2.m1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.2.3.cmml">N</mi></munderover></mstyle><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.2.cmml"><munderover id="S3.E2.m1.1.1.1.1.1.2a" xref="S3.E2.m1.1.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E2.m1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E2.m1.1.1.1.1.1.2.2.3" xref="S3.E2.m1.1.1.1.1.1.2.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.2.2.3.2" xref="S3.E2.m1.1.1.1.1.1.2.2.3.2.cmml">s</mi><mo id="S3.E2.m1.1.1.1.1.1.2.2.3.1" xref="S3.E2.m1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E2.m1.1.1.1.1.1.2.2.3.3" xref="S3.E2.m1.1.1.1.1.1.2.2.3.3.cmml">0</mn></mrow><msub id="S3.E2.m1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.2.3.2" xref="S3.E2.m1.1.1.1.1.1.2.3.2.cmml">S</mi><mrow id="S3.E2.m1.1.1.1.1.1.2.3.3" xref="S3.E2.m1.1.1.1.1.1.2.3.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.2.3.3.2" xref="S3.E2.m1.1.1.1.1.1.2.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.2.3.3.1" xref="S3.E2.m1.1.1.1.1.1.2.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.2.3.3.3" xref="S3.E2.m1.1.1.1.1.1.2.3.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.2.3.3.1a" xref="S3.E2.m1.1.1.1.1.1.2.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.2.3.3.4" xref="S3.E2.m1.1.1.1.1.1.2.3.3.4.cmml">n</mi></mrow></msub></munderover></mstyle><mrow id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml">α</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml">⋅</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mn id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.4" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.cmml">1</mn><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">−</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.4" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.4.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.5" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3a" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.6" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.6.cmml">U</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3b" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">​</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml">(</mo><msubsup id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">p</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">s</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msubsup><mo rspace="0.497em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.4" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml">,</mo><msubsup id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.2.cmml">y</mi><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.1a" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.4" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.1b" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.5" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.5.cmml">c</mi></mrow><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml">i</mi></msubsup><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.5" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.4" xref="S3.E2.m1.1.1.1.1.1.1.1.1.4.cmml">+</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.3.4" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.4.cmml">B</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.3.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.3.5" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.5.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.3.3a" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.3.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.3.6" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.6.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.3.3b" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.3.cmml">​</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.3.cmml">(</mo><msubsup id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.2.2.cmml">q</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.2.3.cmml">s</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.3.cmml">i</mi></msubsup><mo rspace="0.497em" id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.4" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.3.cmml">,</mo><msubsup id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.2.cmml">y</mi><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.3.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.3.1a" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.3.4" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.3.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.3.1b" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.3.5" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.3.5.cmml">s</mi></mrow><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.3.cmml">i</mi></msubsup><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.5" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.3.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><eq id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"></eq><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2">𝐿</ci><apply id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3"><times id="S3.E2.m1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.3.3.1"></times><ci id="S3.E2.m1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.3.3.2">𝑟</ci><ci id="S3.E2.m1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.3.3.3">𝑝</ci><ci id="S3.E2.m1.1.1.3.3.4.cmml" xref="S3.E2.m1.1.1.3.3.4">𝑛</ci></apply></apply><apply id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><times id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.2"></times><apply id="S3.E2.m1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.3"><divide id="S3.E2.m1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.3"></divide><cn type="integer" id="S3.E2.m1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.3.2">1</cn><ci id="S3.E2.m1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.3.3">𝑁</ci></apply><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1"><apply id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2">subscript</csymbol><sum id="S3.E2.m1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2"></sum><apply id="S3.E2.m1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.3"><eq id="S3.E2.m1.1.1.1.1.2.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E2.m1.1.1.1.1.2.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S3.E2.m1.1.1.1.1.2.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.3.3">0</cn></apply></apply><ci id="S3.E2.m1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.3">𝑁</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><apply id="S3.E2.m1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.2">subscript</csymbol><sum id="S3.E2.m1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2.2.2"></sum><apply id="S3.E2.m1.1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.2.2.3"><eq id="S3.E2.m1.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E2.m1.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2.2.3.2">𝑠</ci><cn type="integer" id="S3.E2.m1.1.1.1.1.1.2.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.2.2.3.3">0</cn></apply></apply><apply id="S3.E2.m1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2.3.2">𝑆</ci><apply id="S3.E2.m1.1.1.1.1.1.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.2.3.3"><times id="S3.E2.m1.1.1.1.1.1.2.3.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.2.3.3.1"></times><ci id="S3.E2.m1.1.1.1.1.1.2.3.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2.3.3.2">𝑟</ci><ci id="S3.E2.m1.1.1.1.1.1.2.3.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.2.3.3.3">𝑝</ci><ci id="S3.E2.m1.1.1.1.1.1.2.3.3.4.cmml" xref="S3.E2.m1.1.1.1.1.1.2.3.3.4">𝑛</ci></apply></apply></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1"><plus id="S3.E2.m1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.4"></plus><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1"><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2">⋅</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3">𝛼</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1"><minus id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3"></minus><cn type="integer" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.4">1</cn><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2"><times id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3"></times><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.4.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.4">𝐼</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.5.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.5">𝑜</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.6.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.6">𝑈</ci><interval closure="open" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2"><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2">𝑝</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3">𝑠</ci></apply><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.2">𝑦</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3"><times id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.1"></times><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.2">𝑟</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.3">𝑙</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.4.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.4">𝑜</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.5.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.5">𝑐</ci></apply></apply><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.3">𝑖</ci></apply></interval></apply></apply></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3"><times id="S3.E2.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.3"></times><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.3.4.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.4">𝐵</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.3.5.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.5">𝐶</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.3.6.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.6">𝐸</ci><interval closure="open" id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2"><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.2.2">𝑞</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.2.3">𝑠</ci></apply><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.3">𝑖</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.2">𝑦</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.3"><times id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.3.1"></times><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.3.2">𝑟</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.3.3">𝑐</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.3.4.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.3.4">𝑙</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.3.5.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.2.3.5">𝑠</ci></apply></apply><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.2.2.3">𝑖</ci></apply></interval></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\displaystyle L_{rpn}=\frac{1}{N}\sum_{i=0}^{N}\sum_{s=0}^{S_{rpn}}(\alpha\cdot(1-{IoU}(p_{s}^{i},~{}y_{rloc}^{i}))+{BCE}(q_{s}^{i},~{}y_{rcls}^{i}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="A1.EGx2" class="ltx_equationgroup ltx_centering ltx_eqn_align ltx_figure_panel ltx_eqn_table">

<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3.m1.1" class="ltx_Math" alttext="\displaystyle L_{head}=\frac{1}{N}\sum_{i=0}^{N}\sum_{s=0}^{S_{head}}(\beta\cdot{SmoothL_{1}}(r_{s}^{i},~{}y_{loc}^{i})+\frac{\gamma}{C}\cdot\sum_{c=0}^{C}L_{cls}^{c})" display="inline"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><msub id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml">L</mi><mrow id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3.cmml"><mi id="S3.E3.m1.1.1.3.3.2" xref="S3.E3.m1.1.1.3.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.3.1" xref="S3.E3.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.3.3.3" xref="S3.E3.m1.1.1.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.3.1a" xref="S3.E3.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.3.3.4" xref="S3.E3.m1.1.1.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.3.1b" xref="S3.E3.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.3.3.5" xref="S3.E3.m1.1.1.3.3.5.cmml">d</mi></mrow></msub><mo id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E3.m1.1.1.1.3" xref="S3.E3.m1.1.1.1.3.cmml"><mfrac id="S3.E3.m1.1.1.1.3a" xref="S3.E3.m1.1.1.1.3.cmml"><mn id="S3.E3.m1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.3.2.cmml">1</mn><mi id="S3.E3.m1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.3.3.cmml">N</mi></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E3.m1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.cmml"><munderover id="S3.E3.m1.1.1.1.1.2a" xref="S3.E3.m1.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E3.m1.1.1.1.1.2.2.2" xref="S3.E3.m1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E3.m1.1.1.1.1.2.2.3" xref="S3.E3.m1.1.1.1.1.2.2.3.cmml"><mi id="S3.E3.m1.1.1.1.1.2.2.3.2" xref="S3.E3.m1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E3.m1.1.1.1.1.2.2.3.1" xref="S3.E3.m1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E3.m1.1.1.1.1.2.2.3.3" xref="S3.E3.m1.1.1.1.1.2.2.3.3.cmml">0</mn></mrow><mi id="S3.E3.m1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.2.3.cmml">N</mi></munderover></mstyle><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E3.m1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.2.cmml"><munderover id="S3.E3.m1.1.1.1.1.1.2a" xref="S3.E3.m1.1.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E3.m1.1.1.1.1.1.2.2.2" xref="S3.E3.m1.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E3.m1.1.1.1.1.1.2.2.3" xref="S3.E3.m1.1.1.1.1.1.2.2.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.2.2.3.2" xref="S3.E3.m1.1.1.1.1.1.2.2.3.2.cmml">s</mi><mo id="S3.E3.m1.1.1.1.1.1.2.2.3.1" xref="S3.E3.m1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E3.m1.1.1.1.1.1.2.2.3.3" xref="S3.E3.m1.1.1.1.1.1.2.2.3.3.cmml">0</mn></mrow><msub id="S3.E3.m1.1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.1.2.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.2.3.2" xref="S3.E3.m1.1.1.1.1.1.2.3.2.cmml">S</mi><mrow id="S3.E3.m1.1.1.1.1.1.2.3.3" xref="S3.E3.m1.1.1.1.1.1.2.3.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.2.3.3.2" xref="S3.E3.m1.1.1.1.1.1.2.3.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.2.3.3.1" xref="S3.E3.m1.1.1.1.1.1.2.3.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.2.3.3.3" xref="S3.E3.m1.1.1.1.1.1.2.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.2.3.3.1a" xref="S3.E3.m1.1.1.1.1.1.2.3.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.2.3.3.4" xref="S3.E3.m1.1.1.1.1.1.2.3.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.2.3.3.1b" xref="S3.E3.m1.1.1.1.1.1.2.3.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.2.3.3.5" xref="S3.E3.m1.1.1.1.1.1.2.3.3.5.cmml">d</mi></mrow></msub></munderover></mstyle><mrow id="S3.E3.m1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml"><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.2.4" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.4.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.4.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.4.2.cmml">β</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.4.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.4.1.cmml">⋅</mo><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.4.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.4.3.cmml">S</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.5" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.5.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3a" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.6" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3b" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.7" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.7.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3c" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.8" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.8.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3d" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.9" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.9.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3e" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml">​</mo><msub id="S3.E3.m1.1.1.1.1.1.1.1.1.2.10" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.10.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.10.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.10.2.cmml">L</mi><mn id="S3.E3.m1.1.1.1.1.1.1.1.1.2.10.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.10.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3f" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml">​</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.3.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.3.cmml">(</mo><msubsup id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">r</mi><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">s</mi><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msubsup><mo rspace="0.497em" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.4" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.3.cmml">,</mo><msubsup id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.2.cmml">y</mi><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.1a" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.4" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.4.cmml">c</mi></mrow><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml">i</mi></msubsup><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.5" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml">+</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.4" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.cmml"><mstyle displaystyle="true" id="S3.E3.m1.1.1.1.1.1.1.1.1.4.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.2.cmml"><mfrac id="S3.E3.m1.1.1.1.1.1.1.1.1.4.2a" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.2.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.4.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.2.2.cmml">γ</mi><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.4.2.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.2.3.cmml">C</mi></mfrac></mstyle><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.1.1.1.1.1.1.1.1.4.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.1.cmml">⋅</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.cmml"><mstyle displaystyle="true" id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.cmml"><munderover id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1a" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.cmml"><mo movablelimits="false" id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.2.2.cmml">∑</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.2.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.2.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.2.3.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.2.3.2.cmml">c</mi><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.2.3.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.2.3.1.cmml">=</mo><mn id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.2.3.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.2.3.3.cmml">0</mn></mrow><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.3.cmml">C</mi></munderover></mstyle><msubsup id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.2.2.cmml">L</mi><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.2.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.2.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.2.3.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.2.3.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.2.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.2.3.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.2.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.2.3.1a" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.2.3.1.cmml">​</mo><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.2.3.4" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.2.3.4.cmml">s</mi></mrow><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.3.cmml">c</mi></msubsup></mrow></mrow></mrow><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2"></eq><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2">𝐿</ci><apply id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3"><times id="S3.E3.m1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.3.3.1"></times><ci id="S3.E3.m1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.3.3.2">ℎ</ci><ci id="S3.E3.m1.1.1.3.3.3.cmml" xref="S3.E3.m1.1.1.3.3.3">𝑒</ci><ci id="S3.E3.m1.1.1.3.3.4.cmml" xref="S3.E3.m1.1.1.3.3.4">𝑎</ci><ci id="S3.E3.m1.1.1.3.3.5.cmml" xref="S3.E3.m1.1.1.3.3.5">𝑑</ci></apply></apply><apply id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><times id="S3.E3.m1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.2"></times><apply id="S3.E3.m1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.3"><divide id="S3.E3.m1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.3"></divide><cn type="integer" id="S3.E3.m1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.3.2">1</cn><ci id="S3.E3.m1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.3.3">𝑁</ci></apply><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1"><apply id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.2">superscript</csymbol><apply id="S3.E3.m1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.2">subscript</csymbol><sum id="S3.E3.m1.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2"></sum><apply id="S3.E3.m1.1.1.1.1.2.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.2.3"><eq id="S3.E3.m1.1.1.1.1.2.2.3.1.cmml" xref="S3.E3.m1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E3.m1.1.1.1.1.2.2.3.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S3.E3.m1.1.1.1.1.2.2.3.3.cmml" xref="S3.E3.m1.1.1.1.1.2.2.3.3">0</cn></apply></apply><ci id="S3.E3.m1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.3">𝑁</ci></apply><apply id="S3.E3.m1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"><apply id="S3.E3.m1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E3.m1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.2">subscript</csymbol><sum id="S3.E3.m1.1.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.2.2.2"></sum><apply id="S3.E3.m1.1.1.1.1.1.2.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.2.2.3"><eq id="S3.E3.m1.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E3.m1.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.2.2.3.2">𝑠</ci><cn type="integer" id="S3.E3.m1.1.1.1.1.1.2.2.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.2.2.3.3">0</cn></apply></apply><apply id="S3.E3.m1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.2.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.2.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.2.3.2">𝑆</ci><apply id="S3.E3.m1.1.1.1.1.1.2.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.2.3.3"><times id="S3.E3.m1.1.1.1.1.1.2.3.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.2.3.3.1"></times><ci id="S3.E3.m1.1.1.1.1.1.2.3.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.2.3.3.2">ℎ</ci><ci id="S3.E3.m1.1.1.1.1.1.2.3.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.2.3.3.3">𝑒</ci><ci id="S3.E3.m1.1.1.1.1.1.2.3.3.4.cmml" xref="S3.E3.m1.1.1.1.1.1.2.3.3.4">𝑎</ci><ci id="S3.E3.m1.1.1.1.1.1.2.3.3.5.cmml" xref="S3.E3.m1.1.1.1.1.1.2.3.3.5">𝑑</ci></apply></apply></apply><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1"><plus id="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3"></plus><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2"><times id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3"></times><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.2.4.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.4"><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.4.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.4.1">⋅</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.4.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.4.2">𝛽</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.4.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.4.3">𝑆</ci></apply><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.5.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.5">𝑚</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.6.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.6">𝑜</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.7.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.7">𝑜</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.8.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.8">𝑡</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.9.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.9">ℎ</ci><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.2.10.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.10"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.10.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.10">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.10.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.10.2">𝐿</ci><cn type="integer" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.10.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.10.3">1</cn></apply><interval closure="open" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2"><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2">𝑟</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3">𝑠</ci></apply><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2">superscript</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.2">𝑦</ci><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3"><times id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.1"></times><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.2">𝑙</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.3">𝑜</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.4.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.4">𝑐</ci></apply></apply><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.2.3">𝑖</ci></apply></interval></apply><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4"><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.4.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.1">⋅</ci><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.4.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.2"><divide id="S3.E3.m1.1.1.1.1.1.1.1.1.4.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.2"></divide><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.4.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.2.2">𝛾</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.4.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.2.3">𝐶</ci></apply><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3"><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1">superscript</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1">subscript</csymbol><sum id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.2.2"></sum><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.2.3"><eq id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.2.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.2.3.1"></eq><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.2.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.2.3.2">𝑐</ci><cn type="integer" id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.2.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.2.3.3">0</cn></apply></apply><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.1.3">𝐶</ci></apply><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2">superscript</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.2.2">𝐿</ci><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.2.3"><times id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.2.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.2.3.1"></times><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.2.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.2.3.2">𝑐</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.2.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.2.3.3">𝑙</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.2.3.4.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.2.3.4">𝑠</ci></apply></apply><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.4.3.2.3">𝑐</ci></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\displaystyle L_{head}=\frac{1}{N}\sum_{i=0}^{N}\sum_{s=0}^{S_{head}}(\beta\cdot{SmoothL_{1}}(r_{s}^{i},~{}y_{loc}^{i})+\frac{\gamma}{C}\cdot\sum_{c=0}^{C}L_{cls}^{c})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="A1.EGx3" class="ltx_equationgroup ltx_centering ltx_eqn_align ltx_figure_panel ltx_eqn_table">

<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E4.m1.7" class="ltx_Math" alttext="\displaystyle L_{cls}^{c}=(1-\mathds{1}_{\mathds{D}(y_{cls}^{i})}(c))\cdot{BCE}(x_{s}^{ic},~{}\mathds{1}_{\mathds{P}(y_{cls}^{i})}(c))" display="inline"><semantics id="S3.E4.m1.7a"><mrow id="S3.E4.m1.7.7" xref="S3.E4.m1.7.7.cmml"><msubsup id="S3.E4.m1.7.7.5" xref="S3.E4.m1.7.7.5.cmml"><mi id="S3.E4.m1.7.7.5.2.2" xref="S3.E4.m1.7.7.5.2.2.cmml">L</mi><mrow id="S3.E4.m1.7.7.5.2.3" xref="S3.E4.m1.7.7.5.2.3.cmml"><mi id="S3.E4.m1.7.7.5.2.3.2" xref="S3.E4.m1.7.7.5.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.7.7.5.2.3.1" xref="S3.E4.m1.7.7.5.2.3.1.cmml">​</mo><mi id="S3.E4.m1.7.7.5.2.3.3" xref="S3.E4.m1.7.7.5.2.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.7.7.5.2.3.1a" xref="S3.E4.m1.7.7.5.2.3.1.cmml">​</mo><mi id="S3.E4.m1.7.7.5.2.3.4" xref="S3.E4.m1.7.7.5.2.3.4.cmml">s</mi></mrow><mi id="S3.E4.m1.7.7.5.3" xref="S3.E4.m1.7.7.5.3.cmml">c</mi></msubsup><mo id="S3.E4.m1.7.7.4" xref="S3.E4.m1.7.7.4.cmml">=</mo><mrow id="S3.E4.m1.7.7.3" xref="S3.E4.m1.7.7.3.cmml"><mrow id="S3.E4.m1.5.5.1.1" xref="S3.E4.m1.5.5.1.1.cmml"><mrow id="S3.E4.m1.5.5.1.1.1.1" xref="S3.E4.m1.5.5.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.5.5.1.1.1.1.2" xref="S3.E4.m1.5.5.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.5.5.1.1.1.1.1" xref="S3.E4.m1.5.5.1.1.1.1.1.cmml"><mn id="S3.E4.m1.5.5.1.1.1.1.1.2" xref="S3.E4.m1.5.5.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E4.m1.5.5.1.1.1.1.1.1" xref="S3.E4.m1.5.5.1.1.1.1.1.1.cmml">−</mo><mrow id="S3.E4.m1.5.5.1.1.1.1.1.3" xref="S3.E4.m1.5.5.1.1.1.1.1.3.cmml"><msub id="S3.E4.m1.5.5.1.1.1.1.1.3.2" xref="S3.E4.m1.5.5.1.1.1.1.1.3.2.cmml"><mn id="S3.E4.m1.5.5.1.1.1.1.1.3.2.2" xref="S3.E4.m1.5.5.1.1.1.1.1.3.2.2.cmml">𝟙</mn><mrow id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.3" xref="S3.E4.m1.1.1.1.3.cmml">𝔻</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.2" xref="S3.E4.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E4.m1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.cmml">(</mo><msubsup id="S3.E4.m1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.2.2" xref="S3.E4.m1.1.1.1.1.1.1.2.2.cmml">y</mi><mrow id="S3.E4.m1.1.1.1.1.1.1.2.3" xref="S3.E4.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.2.3.2" xref="S3.E4.m1.1.1.1.1.1.1.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.1.1.2.3.1" xref="S3.E4.m1.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.1.1.1.1.2.3.3" xref="S3.E4.m1.1.1.1.1.1.1.2.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.1.1.2.3.1a" xref="S3.E4.m1.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.1.1.1.1.2.3.4" xref="S3.E4.m1.1.1.1.1.1.1.2.3.4.cmml">s</mi></mrow><mi id="S3.E4.m1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.3.cmml">i</mi></msubsup><mo stretchy="false" id="S3.E4.m1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.5.5.1.1.1.1.1.3.1" xref="S3.E4.m1.5.5.1.1.1.1.1.3.1.cmml">​</mo><mrow id="S3.E4.m1.5.5.1.1.1.1.1.3.3.2" xref="S3.E4.m1.5.5.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.E4.m1.5.5.1.1.1.1.1.3.3.2.1" xref="S3.E4.m1.5.5.1.1.1.1.1.3.cmml">(</mo><mi id="S3.E4.m1.3.3" xref="S3.E4.m1.3.3.cmml">c</mi><mo stretchy="false" id="S3.E4.m1.5.5.1.1.1.1.1.3.3.2.2" xref="S3.E4.m1.5.5.1.1.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo rspace="0.055em" stretchy="false" id="S3.E4.m1.5.5.1.1.1.1.3" xref="S3.E4.m1.5.5.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.E4.m1.5.5.1.1.2" xref="S3.E4.m1.5.5.1.1.2.cmml">⋅</mo><mi id="S3.E4.m1.5.5.1.1.3" xref="S3.E4.m1.5.5.1.1.3.cmml">B</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E4.m1.7.7.3.4" xref="S3.E4.m1.7.7.3.4.cmml">​</mo><mi id="S3.E4.m1.7.7.3.5" xref="S3.E4.m1.7.7.3.5.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.7.7.3.4a" xref="S3.E4.m1.7.7.3.4.cmml">​</mo><mi id="S3.E4.m1.7.7.3.6" xref="S3.E4.m1.7.7.3.6.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.7.7.3.4b" xref="S3.E4.m1.7.7.3.4.cmml">​</mo><mrow id="S3.E4.m1.7.7.3.3.2" xref="S3.E4.m1.7.7.3.3.3.cmml"><mo stretchy="false" id="S3.E4.m1.7.7.3.3.2.3" xref="S3.E4.m1.7.7.3.3.3.cmml">(</mo><msubsup id="S3.E4.m1.6.6.2.2.1.1" xref="S3.E4.m1.6.6.2.2.1.1.cmml"><mi id="S3.E4.m1.6.6.2.2.1.1.2.2" xref="S3.E4.m1.6.6.2.2.1.1.2.2.cmml">x</mi><mi id="S3.E4.m1.6.6.2.2.1.1.2.3" xref="S3.E4.m1.6.6.2.2.1.1.2.3.cmml">s</mi><mrow id="S3.E4.m1.6.6.2.2.1.1.3" xref="S3.E4.m1.6.6.2.2.1.1.3.cmml"><mi id="S3.E4.m1.6.6.2.2.1.1.3.2" xref="S3.E4.m1.6.6.2.2.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.6.6.2.2.1.1.3.1" xref="S3.E4.m1.6.6.2.2.1.1.3.1.cmml">​</mo><mi id="S3.E4.m1.6.6.2.2.1.1.3.3" xref="S3.E4.m1.6.6.2.2.1.1.3.3.cmml">c</mi></mrow></msubsup><mo rspace="0.497em" id="S3.E4.m1.7.7.3.3.2.4" xref="S3.E4.m1.7.7.3.3.3.cmml">,</mo><mrow id="S3.E4.m1.7.7.3.3.2.2" xref="S3.E4.m1.7.7.3.3.2.2.cmml"><msub id="S3.E4.m1.7.7.3.3.2.2.2" xref="S3.E4.m1.7.7.3.3.2.2.2.cmml"><mn id="S3.E4.m1.7.7.3.3.2.2.2.2" xref="S3.E4.m1.7.7.3.3.2.2.2.2.cmml">𝟙</mn><mrow id="S3.E4.m1.2.2.1" xref="S3.E4.m1.2.2.1.cmml"><mi id="S3.E4.m1.2.2.1.3" xref="S3.E4.m1.2.2.1.3.cmml">ℙ</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.2" xref="S3.E4.m1.2.2.1.2.cmml">​</mo><mrow id="S3.E4.m1.2.2.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.2.2.1.1.1.2" xref="S3.E4.m1.2.2.1.1.1.1.cmml">(</mo><msubsup id="S3.E4.m1.2.2.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.2.2" xref="S3.E4.m1.2.2.1.1.1.1.2.2.cmml">y</mi><mrow id="S3.E4.m1.2.2.1.1.1.1.2.3" xref="S3.E4.m1.2.2.1.1.1.1.2.3.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.2.3.2" xref="S3.E4.m1.2.2.1.1.1.1.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.1.1.2.3.1" xref="S3.E4.m1.2.2.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.E4.m1.2.2.1.1.1.1.2.3.3" xref="S3.E4.m1.2.2.1.1.1.1.2.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.1.1.1.2.3.1a" xref="S3.E4.m1.2.2.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.E4.m1.2.2.1.1.1.1.2.3.4" xref="S3.E4.m1.2.2.1.1.1.1.2.3.4.cmml">s</mi></mrow><mi id="S3.E4.m1.2.2.1.1.1.1.3" xref="S3.E4.m1.2.2.1.1.1.1.3.cmml">i</mi></msubsup><mo stretchy="false" id="S3.E4.m1.2.2.1.1.1.3" xref="S3.E4.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.7.7.3.3.2.2.1" xref="S3.E4.m1.7.7.3.3.2.2.1.cmml">​</mo><mrow id="S3.E4.m1.7.7.3.3.2.2.3.2" xref="S3.E4.m1.7.7.3.3.2.2.cmml"><mo stretchy="false" id="S3.E4.m1.7.7.3.3.2.2.3.2.1" xref="S3.E4.m1.7.7.3.3.2.2.cmml">(</mo><mi id="S3.E4.m1.4.4" xref="S3.E4.m1.4.4.cmml">c</mi><mo stretchy="false" id="S3.E4.m1.7.7.3.3.2.2.3.2.2" xref="S3.E4.m1.7.7.3.3.2.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E4.m1.7.7.3.3.2.5" xref="S3.E4.m1.7.7.3.3.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.7b"><apply id="S3.E4.m1.7.7.cmml" xref="S3.E4.m1.7.7"><eq id="S3.E4.m1.7.7.4.cmml" xref="S3.E4.m1.7.7.4"></eq><apply id="S3.E4.m1.7.7.5.cmml" xref="S3.E4.m1.7.7.5"><csymbol cd="ambiguous" id="S3.E4.m1.7.7.5.1.cmml" xref="S3.E4.m1.7.7.5">superscript</csymbol><apply id="S3.E4.m1.7.7.5.2.cmml" xref="S3.E4.m1.7.7.5"><csymbol cd="ambiguous" id="S3.E4.m1.7.7.5.2.1.cmml" xref="S3.E4.m1.7.7.5">subscript</csymbol><ci id="S3.E4.m1.7.7.5.2.2.cmml" xref="S3.E4.m1.7.7.5.2.2">𝐿</ci><apply id="S3.E4.m1.7.7.5.2.3.cmml" xref="S3.E4.m1.7.7.5.2.3"><times id="S3.E4.m1.7.7.5.2.3.1.cmml" xref="S3.E4.m1.7.7.5.2.3.1"></times><ci id="S3.E4.m1.7.7.5.2.3.2.cmml" xref="S3.E4.m1.7.7.5.2.3.2">𝑐</ci><ci id="S3.E4.m1.7.7.5.2.3.3.cmml" xref="S3.E4.m1.7.7.5.2.3.3">𝑙</ci><ci id="S3.E4.m1.7.7.5.2.3.4.cmml" xref="S3.E4.m1.7.7.5.2.3.4">𝑠</ci></apply></apply><ci id="S3.E4.m1.7.7.5.3.cmml" xref="S3.E4.m1.7.7.5.3">𝑐</ci></apply><apply id="S3.E4.m1.7.7.3.cmml" xref="S3.E4.m1.7.7.3"><times id="S3.E4.m1.7.7.3.4.cmml" xref="S3.E4.m1.7.7.3.4"></times><apply id="S3.E4.m1.5.5.1.1.cmml" xref="S3.E4.m1.5.5.1.1"><ci id="S3.E4.m1.5.5.1.1.2.cmml" xref="S3.E4.m1.5.5.1.1.2">⋅</ci><apply id="S3.E4.m1.5.5.1.1.1.1.1.cmml" xref="S3.E4.m1.5.5.1.1.1.1"><minus id="S3.E4.m1.5.5.1.1.1.1.1.1.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.1"></minus><cn type="integer" id="S3.E4.m1.5.5.1.1.1.1.1.2.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.2">1</cn><apply id="S3.E4.m1.5.5.1.1.1.1.1.3.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.3"><times id="S3.E4.m1.5.5.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.3.1"></times><apply id="S3.E4.m1.5.5.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.5.5.1.1.1.1.1.3.2.1.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.3.2">subscript</csymbol><cn type="integer" id="S3.E4.m1.5.5.1.1.1.1.1.3.2.2.cmml" xref="S3.E4.m1.5.5.1.1.1.1.1.3.2.2">1</cn><apply id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"><times id="S3.E4.m1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.2"></times><ci id="S3.E4.m1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.3">𝔻</ci><apply id="S3.E4.m1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1">superscript</csymbol><apply id="S3.E4.m1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.2">𝑦</ci><apply id="S3.E4.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.3"><times id="S3.E4.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.3.1"></times><ci id="S3.E4.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.3.2">𝑐</ci><ci id="S3.E4.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.3.3">𝑙</ci><ci id="S3.E4.m1.1.1.1.1.1.1.2.3.4.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.3.4">𝑠</ci></apply></apply><ci id="S3.E4.m1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply><ci id="S3.E4.m1.3.3.cmml" xref="S3.E4.m1.3.3">𝑐</ci></apply></apply><ci id="S3.E4.m1.5.5.1.1.3.cmml" xref="S3.E4.m1.5.5.1.1.3">𝐵</ci></apply><ci id="S3.E4.m1.7.7.3.5.cmml" xref="S3.E4.m1.7.7.3.5">𝐶</ci><ci id="S3.E4.m1.7.7.3.6.cmml" xref="S3.E4.m1.7.7.3.6">𝐸</ci><interval closure="open" id="S3.E4.m1.7.7.3.3.3.cmml" xref="S3.E4.m1.7.7.3.3.2"><apply id="S3.E4.m1.6.6.2.2.1.1.cmml" xref="S3.E4.m1.6.6.2.2.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.2.2.1.1.1.cmml" xref="S3.E4.m1.6.6.2.2.1.1">superscript</csymbol><apply id="S3.E4.m1.6.6.2.2.1.1.2.cmml" xref="S3.E4.m1.6.6.2.2.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.2.2.1.1.2.1.cmml" xref="S3.E4.m1.6.6.2.2.1.1">subscript</csymbol><ci id="S3.E4.m1.6.6.2.2.1.1.2.2.cmml" xref="S3.E4.m1.6.6.2.2.1.1.2.2">𝑥</ci><ci id="S3.E4.m1.6.6.2.2.1.1.2.3.cmml" xref="S3.E4.m1.6.6.2.2.1.1.2.3">𝑠</ci></apply><apply id="S3.E4.m1.6.6.2.2.1.1.3.cmml" xref="S3.E4.m1.6.6.2.2.1.1.3"><times id="S3.E4.m1.6.6.2.2.1.1.3.1.cmml" xref="S3.E4.m1.6.6.2.2.1.1.3.1"></times><ci id="S3.E4.m1.6.6.2.2.1.1.3.2.cmml" xref="S3.E4.m1.6.6.2.2.1.1.3.2">𝑖</ci><ci id="S3.E4.m1.6.6.2.2.1.1.3.3.cmml" xref="S3.E4.m1.6.6.2.2.1.1.3.3">𝑐</ci></apply></apply><apply id="S3.E4.m1.7.7.3.3.2.2.cmml" xref="S3.E4.m1.7.7.3.3.2.2"><times id="S3.E4.m1.7.7.3.3.2.2.1.cmml" xref="S3.E4.m1.7.7.3.3.2.2.1"></times><apply id="S3.E4.m1.7.7.3.3.2.2.2.cmml" xref="S3.E4.m1.7.7.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.7.7.3.3.2.2.2.1.cmml" xref="S3.E4.m1.7.7.3.3.2.2.2">subscript</csymbol><cn type="integer" id="S3.E4.m1.7.7.3.3.2.2.2.2.cmml" xref="S3.E4.m1.7.7.3.3.2.2.2.2">1</cn><apply id="S3.E4.m1.2.2.1.cmml" xref="S3.E4.m1.2.2.1"><times id="S3.E4.m1.2.2.1.2.cmml" xref="S3.E4.m1.2.2.1.2"></times><ci id="S3.E4.m1.2.2.1.3.cmml" xref="S3.E4.m1.2.2.1.3">ℙ</ci><apply id="S3.E4.m1.2.2.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1">superscript</csymbol><apply id="S3.E4.m1.2.2.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.2.1.cmml" xref="S3.E4.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.1.1.2.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.2.2">𝑦</ci><apply id="S3.E4.m1.2.2.1.1.1.1.2.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.2.3"><times id="S3.E4.m1.2.2.1.1.1.1.2.3.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.2.3.1"></times><ci id="S3.E4.m1.2.2.1.1.1.1.2.3.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.2.3.2">𝑐</ci><ci id="S3.E4.m1.2.2.1.1.1.1.2.3.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.2.3.3">𝑙</ci><ci id="S3.E4.m1.2.2.1.1.1.1.2.3.4.cmml" xref="S3.E4.m1.2.2.1.1.1.1.2.3.4">𝑠</ci></apply></apply><ci id="S3.E4.m1.2.2.1.1.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.3">𝑖</ci></apply></apply></apply><ci id="S3.E4.m1.4.4.cmml" xref="S3.E4.m1.4.4">𝑐</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.7c">\displaystyle L_{cls}^{c}=(1-\mathds{1}_{\mathds{D}(y_{cls}^{i})}(c))\cdot{BCE}(x_{s}^{ic},~{}\mathds{1}_{\mathds{P}(y_{cls}^{i})}(c))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
</div>
</figure>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T2.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_tt" colspan="2"><span id="S3.T2.1.1.1.1" class="ltx_text" style="font-size:90%;">The left as a descendant of the right in semantics</span></td>
</tr>
<tr id="S3.T2.1.2" class="ltx_tr">
<td id="S3.T2.1.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T2.1.2.1.1.1.1" class="ltx_text" style="font-size:90%;"> COCO classes</span></span>
</span>
</td>
<td id="S3.T2.1.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.2.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T2.1.2.2.1.1.1" class="ltx_text" style="font-size:90%;">OID superclasses</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.1.3" class="ltx_tr">
<td id="S3.T2.1.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T2.1.3.1.1.1.1" class="ltx_text" style="font-size:90%;"> </span><em id="S3.T2.1.3.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">cow</em></span>
</span>
</td>
<td id="S3.T2.1.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.2.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T2.1.3.2.1.1.1" class="ltx_text" style="font-size:90%;">animal (/m/0jbk)</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.1.4" class="ltx_tr">
<td id="S3.T2.1.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T2.1.4.1.1.1.1" class="ltx_text" style="font-size:90%;"> MVD classes</span></span>
</span>
</td>
<td id="S3.T2.1.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.2.1.1" class="ltx_p" style="width:100.0pt;"><span id="S3.T2.1.4.2.1.1.1" class="ltx_text" style="font-size:90%;">OID superclasses</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.1.5" class="ltx_tr">
<td id="S3.T2.1.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.5.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T2.1.5.1.1.1.1" class="ltx_text" style="font-size:90%;"> </span><em id="S3.T2.1.5.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">animal–ground</em></span>
</span>
</td>
<td id="S3.T2.1.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.5.2.1.1" class="ltx_p" style="width:100.0pt;"><em id="S3.T2.1.5.2.1.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">animal</em><span id="S3.T2.1.5.2.1.1.2" class="ltx_text" style="font-size:90%;"> (/m/0jbk)</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.1.6" class="ltx_tr">
<td id="S3.T2.1.6.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.1.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.6.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T2.1.6.1.1.1.1" class="ltx_text" style="font-size:90%;"> </span><em id="S3.T2.1.6.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">–animal</em></span>
</span>
</td>
<td id="S3.T2.1.6.2" class="ltx_td ltx_align_top"></td>
</tr>
<tr id="S3.T2.1.7" class="ltx_tr">
<td id="S3.T2.1.7.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.1.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.7.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T2.1.7.1.1.1.1" class="ltx_text" style="font-size:90%;"> </span><em id="S3.T2.1.7.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">object–vehicle</em></span>
</span>
</td>
<td id="S3.T2.1.7.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.1.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.7.2.1.1" class="ltx_p" style="width:100.0pt;"><em id="S3.T2.1.7.2.1.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">land vehicle</em><span id="S3.T2.1.7.2.1.1.2" class="ltx_text" style="font-size:90%;"> (/m/01prls)</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.1.8" class="ltx_tr">
<td id="S3.T2.1.8.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.1.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.8.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T2.1.8.1.1.1.1" class="ltx_text" style="font-size:90%;"> </span><em id="S3.T2.1.8.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">–caravan</em></span>
</span>
</td>
<td id="S3.T2.1.8.2" class="ltx_td ltx_align_top"></td>
</tr>
<tr id="S3.T2.1.9" class="ltx_tr">
<td id="S3.T2.1.9.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.1.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.9.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T2.1.9.1.1.1.1" class="ltx_text" style="font-size:90%;"> </span><em id="S3.T2.1.9.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">object–vehicle–other</em></span>
</span>
</td>
<td id="S3.T2.1.9.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.1.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.9.2.1.1" class="ltx_p" style="width:100.0pt;"><em id="S3.T2.1.9.2.1.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">land vehicle</em><span id="S3.T2.1.9.2.1.1.2" class="ltx_text" style="font-size:90%;"> (/m/01prls)</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.1.10" class="ltx_tr">
<td id="S3.T2.1.10.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.1.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.10.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T2.1.10.1.1.1.1" class="ltx_text" style="font-size:90%;"> </span><em id="S3.T2.1.10.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">–vehicle</em></span>
</span>
</td>
<td id="S3.T2.1.10.2" class="ltx_td ltx_align_top"></td>
</tr>
<tr id="S3.T2.1.11" class="ltx_tr">
<td id="S3.T2.1.11.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.1.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.11.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T2.1.11.1.1.1.1" class="ltx_text" style="font-size:90%;"> </span><em id="S3.T2.1.11.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">object–vehicle–trailer</em></span>
</span>
</td>
<td id="S3.T2.1.11.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.1.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.11.2.1.1" class="ltx_p" style="width:100.0pt;"><em id="S3.T2.1.11.2.1.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">land vehicle</em><span id="S3.T2.1.11.2.1.1.2" class="ltx_text" style="font-size:90%;"> (/m/01prls)</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.1.12" class="ltx_tr">
<td id="S3.T2.1.12.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.1.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.12.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T2.1.12.1.1.1.1" class="ltx_text" style="font-size:90%;"> </span><em id="S3.T2.1.12.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">object–vehicle</em></span>
</span>
</td>
<td id="S3.T2.1.12.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.1.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.12.2.1.1" class="ltx_p" style="width:100.0pt;"><em id="S3.T2.1.12.2.1.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">land vehicle</em><span id="S3.T2.1.12.2.1.1.2" class="ltx_text" style="font-size:90%;"> (/m/01prls)</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.1.13" class="ltx_tr">
<td id="S3.T2.1.13.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.1.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.13.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T2.1.13.1.1.1.1" class="ltx_text" style="font-size:90%;"> </span><em id="S3.T2.1.13.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">–wheeled–slow</em></span>
</span>
</td>
<td id="S3.T2.1.13.2" class="ltx_td ltx_align_top"></td>
</tr>
<tr id="S3.T2.1.14" class="ltx_tr">
<td id="S3.T2.1.14.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.1.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.14.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T2.1.14.1.1.1.1" class="ltx_text" style="font-size:90%;"> </span><em id="S3.T2.1.14.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">object–support–traffic</em></span>
</span>
</td>
<td id="S3.T2.1.14.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.1.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.14.2.1.1" class="ltx_p" style="width:100.0pt;"><em id="S3.T2.1.14.2.1.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">traffic sign</em><span id="S3.T2.1.14.2.1.1.2" class="ltx_text" style="font-size:90%;"> (/m/01mqdt)</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.1.15" class="ltx_tr">
<td id="S3.T2.1.15.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.1.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.15.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T2.1.15.1.1.1.1" class="ltx_text" style="font-size:90%;"> </span><em id="S3.T2.1.15.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">–sign–frame</em></span>
</span>
</td>
<td id="S3.T2.1.15.2" class="ltx_td ltx_align_top"></td>
</tr>
<tr id="S3.T2.1.16" class="ltx_tr">
<td id="S3.T2.1.16.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.1.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.16.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T2.1.16.1.1.1.1" class="ltx_text" style="font-size:90%;"> </span><em id="S3.T2.1.16.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">object–traffic–sign</em></span>
</span>
</td>
<td id="S3.T2.1.16.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.1.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.16.2.1.1" class="ltx_p" style="width:100.0pt;"><em id="S3.T2.1.16.2.1.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">traffic sign</em><span id="S3.T2.1.16.2.1.1.2" class="ltx_text" style="font-size:90%;"> (/m/01mqdt)</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.1.17" class="ltx_tr">
<td id="S3.T2.1.17.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.1.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.17.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T2.1.17.1.1.1.1" class="ltx_text" style="font-size:90%;"> </span><em id="S3.T2.1.17.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">–back</em></span>
</span>
</td>
<td id="S3.T2.1.17.2" class="ltx_td ltx_align_top"></td>
</tr>
<tr id="S3.T2.1.18" class="ltx_tr">
<td id="S3.T2.1.18.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.1.18.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.18.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T2.1.18.1.1.1.1" class="ltx_text" style="font-size:90%;"> </span><em id="S3.T2.1.18.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">object–traffic–sign</em></span>
</span>
</td>
<td id="S3.T2.1.18.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.1.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.18.2.1.1" class="ltx_p" style="width:100.0pt;"><em id="S3.T2.1.18.2.1.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">traffic sign</em><span id="S3.T2.1.18.2.1.1.2" class="ltx_text" style="font-size:90%;"> (/m/01mqdt)</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.1.19" class="ltx_tr">
<td id="S3.T2.1.19.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T2.1.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.19.1.1.1" class="ltx_p" style="width:90.0pt;"><span id="S3.T2.1.19.1.1.1.1" class="ltx_text" style="font-size:90%;"> </span><em id="S3.T2.1.19.1.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">–front</em></span>
</span>
</td>
<td id="S3.T2.1.19.2" class="ltx_td ltx_align_top ltx_border_bb"></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>The parent-child category names between OID superclasses and COCO / MVD classes in semantics.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In Table <a href="#S4.T3" title="Table 3 ‣ 4.1 Datasets ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we present a concise overview of the three datasets employed in our experiments. The COCO dataset <cite class="ltx_cite ltx_citemacro_citep">(T<span class="ltx_ERROR undefined">\BHBI</span>Y. Lin <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2014</a>)</cite> comprises everyday images featuring objects and humans, with annotations for 80 common object categories. The MVD dataset <cite class="ltx_cite ltx_citemacro_citep">(Neuhold <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2017</a>)</cite> (version 1.2) provides high-resolution street-scene imagery and includes 37 object categories. In contrast to COCO and MVD, the OID dataset <cite class="ltx_cite ltx_citemacro_citep">(Kuznetsova <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2020</a>)</cite> stands out with its annotated semantic hierarchy. It features diverse images, often containing multiple objects and complex scenes, and exhibits a long-tailed distribution of annotated classes. For our experiments, we use the training set from the OID dataset, which includes 500 object categories, adhering to the standard practice of the Open Images Challenge 2019.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.1.1.1" class="ltx_p" style="width:36.0pt;"><span class="ltx_rule" style="width:0.0pt;height:10.0pt;background:black;display:inline-block;"></span><span id="S4.T3.1.1.1.1.1.1" class="ltx_text" style="font-size:90%;"> Dataset</span></span>
</span>
</td>
<td id="S4.T3.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T3.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.2.1.1" class="ltx_p" style="width:70.0pt;"><span id="S4.T3.1.1.2.1.1.1" class="ltx_text" style="font-size:90%;">Domain</span></span>
</span>
</td>
<td id="S4.T3.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T3.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.3.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T3.1.1.3.1.1.1" class="ltx_text" style="font-size:90%;"># Cats</span></span>
</span>
</td>
<td id="S4.T3.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T3.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.4.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T3.1.1.4.1.1.1" class="ltx_text" style="font-size:90%;"># Imgs</span></span>
</span>
</td>
</tr>
<tr id="S4.T3.1.2" class="ltx_tr">
<td id="S4.T3.1.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.2.1.1.1" class="ltx_p" style="width:36.0pt;"><span class="ltx_rule" style="width:0.0pt;height:10.0pt;background:black;display:inline-block;"></span><span id="S4.T3.1.2.1.1.1.1" class="ltx_text" style="font-size:90%;"> COCO</span></span>
</span>
</td>
<td id="S4.T3.1.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T3.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.2.2.1.1" class="ltx_p" style="width:70.0pt;"><span id="S4.T3.1.2.2.1.1.1" class="ltx_text" style="font-size:90%;">Internet images</span></span>
</span>
</td>
<td id="S4.T3.1.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T3.1.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.2.3.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T3.1.2.3.1.1.1" class="ltx_text" style="font-size:90%;">80</span></span>
</span>
</td>
<td id="S4.T3.1.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T3.1.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.2.4.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T3.1.2.4.1.1.1" class="ltx_text" style="font-size:90%;">118k</span></span>
</span>
</td>
</tr>
<tr id="S4.T3.1.3" class="ltx_tr">
<td id="S4.T3.1.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S4.T3.1.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.3.1.1.1" class="ltx_p" style="width:36.0pt;"><span class="ltx_rule" style="width:0.0pt;height:10.0pt;background:black;display:inline-block;"></span><span id="S4.T3.1.3.1.1.1.1" class="ltx_text" style="font-size:90%;"> MVD</span></span>
</span>
</td>
<td id="S4.T3.1.3.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T3.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.3.2.1.1" class="ltx_p" style="width:70.0pt;"><span id="S4.T3.1.3.2.1.1.1" class="ltx_text" style="font-size:90%;">Street scenes</span></span>
</span>
</td>
<td id="S4.T3.1.3.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T3.1.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.3.3.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T3.1.3.3.1.1.1" class="ltx_text" style="font-size:90%;">37</span></span>
</span>
</td>
<td id="S4.T3.1.3.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T3.1.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.3.4.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T3.1.3.4.1.1.1" class="ltx_text" style="font-size:90%;">1.8k</span></span>
</span>
</td>
</tr>
<tr id="S4.T3.1.4" class="ltx_tr">
<td id="S4.T3.1.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r">
<span id="S4.T3.1.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.4.1.1.1" class="ltx_p" style="width:36.0pt;"><span class="ltx_rule" style="width:0.0pt;height:10.0pt;background:black;display:inline-block;"></span><span id="S4.T3.1.4.1.1.1.1" class="ltx_text" style="font-size:90%;"> OID</span></span>
</span>
</td>
<td id="S4.T3.1.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S4.T3.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.4.2.1.1" class="ltx_p" style="width:70.0pt;"><span id="S4.T3.1.4.2.1.1.1" class="ltx_text" style="font-size:90%;">Internet images</span></span>
</span>
</td>
<td id="S4.T3.1.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S4.T3.1.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.4.3.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T3.1.4.3.1.1.1" class="ltx_text" style="font-size:90%;">500</span></span>
</span>
</td>
<td id="S4.T3.1.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S4.T3.1.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.4.4.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T3.1.4.4.1.1.1" class="ltx_text" style="font-size:90%;">1.8M</span></span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>The training datasets used in the experiments, which are provided by organizers of RVC 2022.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation Details</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">For our experiments, we implement our proposed method using the mmdetection codebase <cite class="ltx_cite ltx_citemacro_citep">(Chen <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2019</a>)</cite>. We use the frozen SEER-RegNet32gf and SEER-RegNet256gf as the backbone models, which offer the resource-efficient training. To ensure synchronized computation across all GPU workers, we replace traditional BatchNorm (BN) with synchronized BatchNorm (SyncBN). Hyper-parameters for the NAS-FPN, Cascade RPN, and Cascade R-CNN components are maintained at their default settings, unless otherwise specified.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">We apply standard data augmentation techniques, including random flipping and random scaling of the short edge of the image within the range of [480, 960]. For optimization, we employ the Stochastic Gradient Descent (SGD) optimizer with a base learning rate of 0.01, weight decay set to 0.0001, and a batch size of 16. To handle imbalanced class distribution and size disparities across the three datasets, we use class-aware sampling and dataset-wise re-sampling. The re-sampling ratio is configured as 1: 4: 8 for OID, COCO, and MVD, respectively. Model training is conducted on 8 NVIDIA 3090 / A100 GPUs, with the use of mixed-precision techniques <cite class="ltx_cite ltx_citemacro_citep">(Micikevicius <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2018</a>)</cite> for accelerated training. The models are trained for 1.15 million iterations. The training process incorporates a linear warm-up of the base learning rate in the initial 4k iterations, followed by a reduction by a factor of 10 at 850k and 1.0M iterations. During the inference stage, images are resized with the short edge set to 800 and the long edge constrained to 1333, maintaining the aspect ratio, unless otherwise specified.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.2" class="ltx_tabular ltx_align_middle">
<tr id="S4.T4.2.3" class="ltx_tr">
<td id="S4.T4.2.3.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="S4.T4.2.3.1.1" class="ltx_text" style="font-size:90%;">Methods</span></td>
<td id="S4.T4.2.3.2" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2">
<span id="S4.T4.2.3.2.1" class="ltx_text" style="font-size:90%;">COCO </span><em id="S4.T4.2.3.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">test</em>
</td>
<td id="S4.T4.2.3.3" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2">
<span id="S4.T4.2.3.3.1" class="ltx_text" style="font-size:90%;">COCO </span><em id="S4.T4.2.3.3.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">val</em>
</td>
<td id="S4.T4.2.3.4" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2">
<span id="S4.T4.2.3.4.1" class="ltx_text" style="font-size:90%;">MVD </span><em id="S4.T4.2.3.4.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">test</em>
</td>
<td id="S4.T4.2.3.5" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2">
<span id="S4.T4.2.3.5.1" class="ltx_text" style="font-size:90%;">MVD </span><em id="S4.T4.2.3.5.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">val</em>
</td>
<td id="S4.T4.2.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.3.6.1.1" class="ltx_p" style="width:34.5pt;"><span id="S4.T4.2.3.6.1.1.1" class="ltx_text" style="font-size:90%;">OID </span><em id="S4.T4.2.3.6.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">test</em></span>
</span>
</td>
<td id="S4.T4.2.3.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.3.7.1.1" class="ltx_p" style="width:34.5pt;"><span id="S4.T4.2.3.7.1.1.1" class="ltx_text" style="font-size:90%;">OID </span><em id="S4.T4.2.3.7.1.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">val</em></span>
</span>
</td>
</tr>
<tr id="S4.T4.2.4" class="ltx_tr">
<td id="S4.T4.2.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.4.1.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.4.1.1.1.1" class="ltx_text" style="font-size:90%;">mAP</span></span>
</span>
</td>
<td id="S4.T4.2.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.4.2.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.4.2.1.1.1" class="ltx_text" style="font-size:90%;">AP50</span></span>
</span>
</td>
<td id="S4.T4.2.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.4.3.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.4.3.1.1.1" class="ltx_text" style="font-size:90%;">mAP</span></span>
</span>
</td>
<td id="S4.T4.2.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.4.4.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.4.4.1.1.1" class="ltx_text" style="font-size:90%;">AP50</span></span>
</span>
</td>
<td id="S4.T4.2.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.4.5.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.4.5.1.1.1" class="ltx_text" style="font-size:90%;">mAP</span></span>
</span>
</td>
<td id="S4.T4.2.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.4.6.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.4.6.1.1.1" class="ltx_text" style="font-size:90%;">AP50</span></span>
</span>
</td>
<td id="S4.T4.2.4.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.4.7.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.4.7.1.1.1" class="ltx_text" style="font-size:90%;">mAP</span></span>
</span>
</td>
<td id="S4.T4.2.4.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.4.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.4.8.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.4.8.1.1.1" class="ltx_text" style="font-size:90%;">AP50</span></span>
</span>
</td>
<td id="S4.T4.2.4.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.4.9.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.4.9.1.1" class="ltx_p" style="width:34.5pt;"><span id="S4.T4.2.4.9.1.1.1" class="ltx_text" style="font-size:90%;">AP50</span></span>
</span>
</td>
<td id="S4.T4.2.4.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.4.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.4.10.1.1" class="ltx_p" style="width:34.5pt;"><span id="S4.T4.2.4.10.1.1.1" class="ltx_text" style="font-size:90%;">AP50</span></span>
</span>
</td>
</tr>
<tr id="S4.T4.2.5" class="ltx_tr">
<td id="S4.T4.2.5.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T4.2.5.1.1" class="ltx_text" style="font-size:90%;">MD_RVC (1)</span></td>
<td id="S4.T4.2.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.5.2.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.5.2.1.1.1" class="ltx_text" style="font-size:90%;">59.0</span></span>
</span>
</td>
<td id="S4.T4.2.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.5.3.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.5.3.1.1.1" class="ltx_text" style="font-size:90%;">76.0</span></span>
</span>
</td>
<td id="S4.T4.2.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.5.4.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.5.4.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.5.5.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.5.5.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.5.6.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.5.6.1.1.1" class="ltx_text" style="font-size:90%;">32.8</span></span>
</span>
</td>
<td id="S4.T4.2.5.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.5.7.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.5.7.1.1.1" class="ltx_text" style="font-size:90%;">49.3</span></span>
</span>
</td>
<td id="S4.T4.2.5.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.5.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.5.8.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.5.8.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.5.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.5.9.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.5.9.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.5.9.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.5.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.5.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.5.10.1.1" class="ltx_p" style="width:34.5pt;"><span id="S4.T4.2.5.10.1.1.1" class="ltx_text" style="font-size:90%;">62.1</span></span>
</span>
</td>
<td id="S4.T4.2.5.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.5.11.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.5.11.1.1" class="ltx_p" style="width:34.5pt;"><span id="S4.T4.2.5.11.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
</tr>
<tr id="S4.T4.2.6" class="ltx_tr">
<td id="S4.T4.2.6.1" class="ltx_td ltx_align_left"><span id="S4.T4.2.6.1.1" class="ltx_text" style="font-size:90%;">IFFF_RVC (2)</span></td>
<td id="S4.T4.2.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.6.2.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.6.2.1.1.1" class="ltx_text" style="font-size:90%;">50.0</span></span>
</span>
</td>
<td id="S4.T4.2.6.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.6.3.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.6.3.1.1.1" class="ltx_text" style="font-size:90%;">69.0</span></span>
</span>
</td>
<td id="S4.T4.2.6.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.6.4.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.6.4.1.1.1" class="ltx_text" style="font-size:90%;">50.0</span></span>
</span>
</td>
<td id="S4.T4.2.6.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.6.5.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.6.5.1.1.1" class="ltx_text" style="font-size:90%;">69.1</span></span>
</span>
</td>
<td id="S4.T4.2.6.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.6.6.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.6.6.1.1.1" class="ltx_text" style="font-size:90%;">25.3</span></span>
</span>
</td>
<td id="S4.T4.2.6.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.6.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.6.7.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.6.7.1.1.1" class="ltx_text" style="font-size:90%;">39.0</span></span>
</span>
</td>
<td id="S4.T4.2.6.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.6.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.6.8.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.6.8.1.1.1" class="ltx_text" style="font-size:90%;">24.2</span></span>
</span>
</td>
<td id="S4.T4.2.6.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.6.9.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.6.9.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.6.9.1.1.1" class="ltx_text" style="font-size:90%;">38.6</span></span>
</span>
</td>
<td id="S4.T4.2.6.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.6.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.6.10.1.1" class="ltx_p" style="width:34.5pt;"><span id="S4.T4.2.6.10.1.1.1" class="ltx_text" style="font-size:90%;">59.9</span></span>
</span>
</td>
<td id="S4.T4.2.6.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.6.11.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.6.11.1.1" class="ltx_p" style="width:34.5pt;"><span id="S4.T4.2.6.11.1.1.1" class="ltx_text" style="font-size:90%;">69.1</span></span>
</span>
</td>
</tr>
<tr id="S4.T4.2.7" class="ltx_tr">
<td id="S4.T4.2.7.1" class="ltx_td ltx_align_left"><span id="S4.T4.2.7.1.1" class="ltx_text" style="font-size:90%;">USTC_RVC (3)</span></td>
<td id="S4.T4.2.7.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.7.2.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.7.2.1.1.1" class="ltx_text" style="font-size:90%;">50.0</span></span>
</span>
</td>
<td id="S4.T4.2.7.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.7.3.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.7.3.1.1.1" class="ltx_text" style="font-size:90%;">68.0</span></span>
</span>
</td>
<td id="S4.T4.2.7.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.7.4.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.7.4.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.7.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.7.5.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.7.5.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.7.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.7.6.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.7.6.1.1.1" class="ltx_text" style="font-size:90%;">25.1</span></span>
</span>
</td>
<td id="S4.T4.2.7.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.7.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.7.7.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.7.7.1.1.1" class="ltx_text" style="font-size:90%;">37.9</span></span>
</span>
</td>
<td id="S4.T4.2.7.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.7.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.7.8.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.7.8.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.7.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.7.9.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.7.9.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.7.9.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.7.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.7.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.7.10.1.1" class="ltx_p" style="width:34.5pt;"><span id="S4.T4.2.7.10.1.1.1" class="ltx_text" style="font-size:90%;">47.8</span></span>
</span>
</td>
<td id="S4.T4.2.7.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.7.11.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.7.11.1.1" class="ltx_p" style="width:34.5pt;"><span id="S4.T4.2.7.11.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
</tr>
<tr id="S4.T4.2.8" class="ltx_tr">
<td id="S4.T4.2.8.1" class="ltx_td ltx_align_left"><span id="S4.T4.2.8.1.1" class="ltx_text" style="font-size:90%;">CBS_RVC (4)</span></td>
<td id="S4.T4.2.8.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.8.2.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.8.2.1.1.1" class="ltx_text" style="font-size:90%;">48.0</span></span>
</span>
</td>
<td id="S4.T4.2.8.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.8.3.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.8.3.1.1.1" class="ltx_text" style="font-size:90%;">66.0</span></span>
</span>
</td>
<td id="S4.T4.2.8.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.8.4.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.8.4.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.8.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.8.5.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.8.5.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.8.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.8.6.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.8.6.1.1.1" class="ltx_text" style="font-size:90%;">13.5</span></span>
</span>
</td>
<td id="S4.T4.2.8.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.8.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.8.7.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.8.7.1.1.1" class="ltx_text" style="font-size:90%;">21.5</span></span>
</span>
</td>
<td id="S4.T4.2.8.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.8.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.8.8.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.8.8.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.8.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.8.9.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.8.9.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.8.9.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.8.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.8.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.8.10.1.1" class="ltx_p" style="width:34.5pt;"><span id="S4.T4.2.8.10.1.1.1" class="ltx_text" style="font-size:90%;">47.7</span></span>
</span>
</td>
<td id="S4.T4.2.8.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.8.11.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.8.11.1.1" class="ltx_p" style="width:34.5pt;"><span id="S4.T4.2.8.11.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
</tr>
<tr id="S4.T4.2.9" class="ltx_tr">
<td id="S4.T4.2.9.1" class="ltx_td ltx_align_left"><span id="S4.T4.2.9.1.1" class="ltx_text" style="font-size:90%;">TSDREF_RVC (5)</span></td>
<td id="S4.T4.2.9.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.9.2.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.9.2.1.1.1" class="ltx_text" style="font-size:90%;">19.0</span></span>
</span>
</td>
<td id="S4.T4.2.9.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.9.3.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.9.3.1.1.1" class="ltx_text" style="font-size:90%;">31.0</span></span>
</span>
</td>
<td id="S4.T4.2.9.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.9.4.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.9.4.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.9.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.9.5.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.9.5.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.9.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.9.6.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.9.6.1.1.1" class="ltx_text" style="font-size:90%;">6.6</span></span>
</span>
</td>
<td id="S4.T4.2.9.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.9.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.9.7.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.9.7.1.1.1" class="ltx_text" style="font-size:90%;">11.6</span></span>
</span>
</td>
<td id="S4.T4.2.9.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.9.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.9.8.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.9.8.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.9.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.9.9.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.9.9.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.9.9.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.9.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.9.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.9.10.1.1" class="ltx_p" style="width:34.5pt;"><span id="S4.T4.2.9.10.1.1.1" class="ltx_text" style="font-size:90%;">40.2</span></span>
</span>
</td>
<td id="S4.T4.2.9.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.9.11.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.9.11.1.1" class="ltx_p" style="width:34.5pt;"><span id="S4.T4.2.9.11.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
</tr>
<tr id="S4.T4.2.10" class="ltx_tr">
<td id="S4.T4.2.10.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T4.2.10.1.1" class="ltx_text" style="font-size:90%;">Large-UniDet [S]</span></td>
<td id="S4.T4.2.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.10.2.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.10.2.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.10.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.10.3.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.10.3.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.10.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.10.4.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.10.4.1.1.1" class="ltx_text" style="font-size:90%;">48.8</span></span>
</span>
</td>
<td id="S4.T4.2.10.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.10.5.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.10.5.1.1.1" class="ltx_text" style="font-size:90%;">66.2</span></span>
</span>
</td>
<td id="S4.T4.2.10.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.10.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.10.6.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.10.6.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.10.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.10.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.10.7.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.10.7.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.10.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.10.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.10.8.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.10.8.1.1.1" class="ltx_text" style="font-size:90%;">25.9</span></span>
</span>
</td>
<td id="S4.T4.2.10.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.10.9.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.10.9.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.10.9.1.1.1" class="ltx_text" style="font-size:90%;">39.4</span></span>
</span>
</td>
<td id="S4.T4.2.10.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.10.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.10.10.1.1" class="ltx_p" style="width:34.5pt;"><span id="S4.T4.2.10.10.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.10.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.2.10.11.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.10.11.1.1" class="ltx_p" style="width:34.5pt;"><span id="S4.T4.2.10.11.1.1.1" class="ltx_text" style="font-size:90%;">68.5</span></span>
</span>
</td>
</tr>
<tr id="S4.T4.2.11" class="ltx_tr">
<td id="S4.T4.2.11.1" class="ltx_td ltx_align_left"><span id="S4.T4.2.11.1.1" class="ltx_text" style="font-size:90%;">Large-UniDet [L]</span></td>
<td id="S4.T4.2.11.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.11.2.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.11.2.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.11.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.11.3.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.11.3.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.11.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.11.4.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.11.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">51.9</span></span>
</span>
</td>
<td id="S4.T4.2.11.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.11.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.11.5.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.11.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">70.0</span></span>
</span>
</td>
<td id="S4.T4.2.11.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.11.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.11.6.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.11.6.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.11.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.11.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.11.7.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.11.7.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.11.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.11.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.11.8.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.11.8.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">27.7</span></span>
</span>
</td>
<td id="S4.T4.2.11.9" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.11.9.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.11.9.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.11.9.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">42.2</span></span>
</span>
</td>
<td id="S4.T4.2.11.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.11.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.11.10.1.1" class="ltx_p" style="width:34.5pt;"><span id="S4.T4.2.11.10.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.11.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T4.2.11.11.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.11.11.1.1" class="ltx_p" style="width:34.5pt;"><span id="S4.T4.2.11.11.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">69.8</span></span>
</span>
</td>
</tr>
<tr id="S4.T4.1.1" class="ltx_tr">
<td id="S4.T4.1.1.1" class="ltx_td ltx_align_left ltx_border_t">
<span id="S4.T4.1.1.1.1" class="ltx_text" style="font-size:90%;">Large-UniDet [S]</span><sup id="S4.T4.1.1.1.2" class="ltx_sup"><span id="S4.T4.1.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">†</span></sup>
</td>
<td id="S4.T4.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.2.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.1.1.2.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.3.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.1.1.3.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.4.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.1.1.4.1.1.1" class="ltx_text" style="font-size:90%;">52.0</span></span>
</span>
</td>
<td id="S4.T4.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.5.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.1.1.5.1.1.1" class="ltx_text" style="font-size:90%;">70.4</span></span>
</span>
</td>
<td id="S4.T4.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.6.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.1.1.6.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.1.1.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.1.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.7.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.1.1.7.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.1.1.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.1.1.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.8.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.1.1.8.1.1.1" class="ltx_text" style="font-size:90%;">32.0</span></span>
</span>
</td>
<td id="S4.T4.1.1.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.1.1.9.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.9.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.1.1.9.1.1.1" class="ltx_text" style="font-size:90%;">47.8</span></span>
</span>
</td>
<td id="S4.T4.1.1.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.1.1.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.10.1.1" class="ltx_p" style="width:34.5pt;"><span id="S4.T4.1.1.10.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.1.1.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T4.1.1.11.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.11.1.1" class="ltx_p" style="width:34.5pt;"><span id="S4.T4.1.1.11.1.1.1" class="ltx_text" style="font-size:90%;">69.2</span></span>
</span>
</td>
</tr>
<tr id="S4.T4.2.2" class="ltx_tr">
<td id="S4.T4.2.2.1" class="ltx_td ltx_align_left ltx_border_b">
<span id="S4.T4.2.2.1.1" class="ltx_text" style="font-size:90%;">Large-UniDet [L]</span><sup id="S4.T4.2.2.1.2" class="ltx_sup"><span id="S4.T4.2.2.1.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">†</span></sup>
</td>
<td id="S4.T4.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S4.T4.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.2.2.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.2.2.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S4.T4.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.2.3.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.2.3.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S4.T4.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.2.4.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.2.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">53.5</span></span>
</span>
</td>
<td id="S4.T4.2.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S4.T4.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.2.5.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.2.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">71.8</span></span>
</span>
</td>
<td id="S4.T4.2.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S4.T4.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.2.6.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.2.6.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.2.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S4.T4.2.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.2.7.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.2.7.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.2.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S4.T4.2.2.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.2.8.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.2.8.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">33.2</span></span>
</span>
</td>
<td id="S4.T4.2.2.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S4.T4.2.2.9.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.2.9.1.1" class="ltx_p" style="width:22.0pt;"><span id="S4.T4.2.2.9.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">49.4</span></span>
</span>
</td>
<td id="S4.T4.2.2.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S4.T4.2.2.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.2.10.1.1" class="ltx_p" style="width:34.5pt;"><span id="S4.T4.2.2.10.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T4.2.2.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S4.T4.2.2.11.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.2.2.11.1.1" class="ltx_p" style="width:34.5pt;"><span id="S4.T4.2.2.11.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">70.5</span></span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparisons on five RVC submissions on three datasets, the numbers in brackets denote the achieved places in the challenge. The last four rows report our updated results in this paper, we only provide the accuracy on validation sets as the RVC test server is off after RVC deadline. Large-UniDet [S] indicates our method based on SEER-RegNet32gf while Large-UniDet [L] indicates the one based on SEER-RegNet256gf.</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Main Results</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In Table <a href="#S4.T4" title="Table 4 ‣ 4.2 Implementation Details ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we provide a summary of comparisons between the RVC final submissions and our latest results. The top-ranking method, MD_RVC, utilizes a large transformer-based object detector <cite class="ltx_cite ltx_citemacro_citep">(Carion <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2020</a>)</cite> combined with an acceleration training strategy that progressively increases the input size. In contrast, our method, named Large-UniDet, takes a quite different approach. We focus on developing an efficient training formulation to save both computation and memory resources. Our goal is to generate robust multi-domain object detection predictions by leveraging the capabilities of large pre-trained vision models.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Compared to our RVC submission, IFFF_RVC (detailed in Appendix <a href="#A1" title="Appendix A RVC Submission ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>), the latest Large-UniDet not only retains its complexity but also incorporates further improvements through a training practice that adapts the model for high-resolution input data (detailed in Section <a href="#S4.SS4.SSS5" title="4.4.5 Scaling Up and Finetuning ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4.5</span></a>). As evident from Table <a href="#S4.T4" title="Table 4 ‣ 4.2 Implementation Details ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, with a lighter backbone (SEER-RegNet32gf), Large-UniDet achieves notable performance metrics: 48.8 mAP on COCO <em id="S4.SS3.p2.1.1" class="ltx_emph ltx_font_italic">val</em> set, 66.2 AP50 on COCO <em id="S4.SS3.p2.1.2" class="ltx_emph ltx_font_italic">val</em> set, 25.9 mAP on MVD <em id="S4.SS3.p2.1.3" class="ltx_emph ltx_font_italic">val</em> set, 39.4 AP50 on MVD <em id="S4.SS3.p2.1.4" class="ltx_emph ltx_font_italic">val</em> set, and 68.5 AP50 on OID <em id="S4.SS3.p2.1.5" class="ltx_emph ltx_font_italic">val</em> set. When moving to the larger backbone (SEER-RegNet256gf), we consistently observe improved universal object detection performance, with a gain of +3.1 mAP and +3.8 AP50 on COCO, +1.8 mAP and +2.8 AP50 on MVD, and +1.3 AP50 on OID, which underscore the effectiveness of visual representations generated by larger vision models. Please note that we resized the short edges of MVD images to <span id="S4.SS3.p2.1.6" class="ltx_text ltx_font_bold">2048</span> pixels, in alignment with our approach for the submission IFFF_RVC. Additionally, we restricted the generation of predictions to a maximum of 300 per image, utilizing standard Non-Maximum Suppression (NMS) during testing to ensure a fair comparison with IFFF_RVC on validation sets.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">After completing universal object detection training, we proceed with dataset-specific individual finetuning using high-resolution training images, as elaborated in Section <a href="#S4.SS4.SSS5" title="4.4.5 Scaling Up and Finetuning ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4.5</span></a>. This practice leads to further performance improvements on all three datasets, with particularly significant gains observed for MVD, which exhibits distinct characteristics compared to the other two datasets. Detailed AP numbers reflecting these improvements are presented in Table <a href="#S4.T4" title="Table 4 ‣ 4.2 Implementation Details ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and are denoted as Large-UniDet with superscript <sup id="S4.SS3.p3.1.1" class="ltx_sup"><span id="S4.SS3.p3.1.1.1" class="ltx_text ltx_font_italic">†</span></sup>.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation Study</h3>

<section id="S4.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1 </span>Detector Components Analysis</h4>

<div id="S4.SS4.SSS1.p1" class="ltx_para">
<p id="S4.SS4.SSS1.p1.1" class="ltx_p">We perform an ablation analysis using SEER-RegNet32gf as the backbone. The results of this analysis, focusing on accuracy-cost comparisons for different detector configurations, are presented in Table <a href="#S4.T5" title="Table 5 ‣ 4.4.1 Detector Components Analysis ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> for COCO and Table <a href="#S4.T6" title="Table 6 ‣ 4.4.1 Detector Components Analysis ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> for MVD. When employing a frozen backbone, Cascade R-CNN exhibits a significant performance improvement over the baseline Faster R-CNN. This improvement is substantial, raising mAP to 39.9 on COCO and 15.2 on MVD, with only a slight increase in training cost (+1 hour for COCO and +0.2 hours for MVD).</p>
</div>
<div id="S4.SS4.SSS1.p2" class="ltx_para">
<p id="S4.SS4.SSS1.p2.1" class="ltx_p">When integrating high-capacity necks into the Cascade R-CNN, we observe that higher accuracy is achievable, but this comes at the expense of increased computation burden. We compare three commonly used FPNs in Tables <a href="#S4.T5" title="Table 5 ‣ 4.4.1 Detector Components Analysis ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and <a href="#S4.T6" title="Table 6 ‣ 4.4.1 Detector Components Analysis ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. As we can see, PAFPN <cite class="ltx_cite ltx_citemacro_citep">(S. Liu <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2018</a>)</cite> yields a modest improvement of 0.9 and 1.5 points on COCO and MVD, respectively. BiFPN <cite class="ltx_cite ltx_citemacro_citep">(Tan <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2020</a>)</cite> offers a slightly greater improvement, with at most 1.6 and 1.9 points on COCO and MVD, respectively. However, NAS-FPN surpasses them all, delivering remarkable gains of 5.8 and 4.2 points on COCO and MVD. Additionally, Tables <a href="#S4.T5" title="Table 5 ‣ 4.4.1 Detector Components Analysis ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and <a href="#S4.T6" title="Table 6 ‣ 4.4.1 Detector Components Analysis ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> illustrate that as the number of stacked neck blocks increases, so does the computational cost. However, the rate of performance improvement gradually decelerates, and in some cases, such as the stacked BiFPN, performance even degrades. Hence, we opt for seven stacked NAS-FPN blocks for our detector, striking a favorable balance between accuracy and computational cost.</p>
</div>
<div id="S4.SS4.SSS1.p3" class="ltx_para">
<p id="S4.SS4.SSS1.p3.1" class="ltx_p">In addition, as observed, Cascade RPN consistently improves performance, contributing at least a +1.0 mAP gain on COCO and a +0.2 mAP gain on MVD, regardless of the neck configuration. This improvement is particularly notable when coupled with NAS-FPN, and it comes without a significant increase in extra computational cost.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.8" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T5.8.9" class="ltx_tr">
<td id="S4.T5.8.9.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T5.8.9.1.1" class="ltx_text" style="font-size:90%;">Model</span></td>
<td id="S4.T5.8.9.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T5.8.9.2.1" class="ltx_text" style="font-size:90%;">mAP</span></td>
<td id="S4.T5.8.9.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T5.8.9.3.1" class="ltx_text" style="font-size:90%;">Time (h)</span></td>
</tr>
<tr id="S4.T5.8.10" class="ltx_tr">
<td id="S4.T5.8.10.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T5.8.10.1.1" class="ltx_text" style="font-size:90%;">Faster R-CNN</span></td>
<td id="S4.T5.8.10.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T5.8.10.2.1" class="ltx_text" style="font-size:90%;">29.0</span></td>
<td id="S4.T5.8.10.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T5.8.10.3.1" class="ltx_text" style="font-size:90%;">12</span></td>
</tr>
<tr id="S4.T5.8.11" class="ltx_tr">
<td id="S4.T5.8.11.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T5.8.11.1.1" class="ltx_text" style="font-size:90%;">Cascade R-CNN</span></td>
<td id="S4.T5.8.11.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T5.8.11.2.1" class="ltx_text" style="font-size:90%;">39.9</span></td>
<td id="S4.T5.8.11.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T5.8.11.3.1" class="ltx_text" style="font-size:90%;">13</span></td>
</tr>
<tr id="S4.T5.8.12" class="ltx_tr">
<td id="S4.T5.8.12.1" class="ltx_td ltx_align_left">
<span class="ltx_rule" style="width:0.0pt;height:12.0pt;background:black;display:inline-block;"></span><span id="S4.T5.8.12.1.1" class="ltx_text" style="font-size:90%;">   + PAFPN</span>
</td>
<td id="S4.T5.8.12.2" class="ltx_td ltx_align_left">
<span id="S4.T5.8.12.2.1" class="ltx_text" style="font-size:90%;">40.8 </span><span id="S4.T5.8.12.2.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+0.9)</span>
</td>
<td id="S4.T5.8.12.3" class="ltx_td ltx_align_left">
<span id="S4.T5.8.12.3.1" class="ltx_text" style="font-size:90%;">16 </span><span id="S4.T5.8.12.3.2" class="ltx_text" style="font-size:90%;color:#FF0000;">(+3)</span>
</td>
</tr>
<tr id="S4.T5.8.13" class="ltx_tr">
<td id="S4.T5.8.13.1" class="ltx_td ltx_align_left"><span id="S4.T5.8.13.1.1" class="ltx_text" style="font-size:90%;">        + Cascade RPN</span></td>
<td id="S4.T5.8.13.2" class="ltx_td ltx_align_left">
<span id="S4.T5.8.13.2.1" class="ltx_text" style="font-size:90%;">42.0 </span><span id="S4.T5.8.13.2.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+2.1)</span>
</td>
<td id="S4.T5.8.13.3" class="ltx_td ltx_align_left">
<span id="S4.T5.8.13.3.1" class="ltx_text" style="font-size:90%;">19 </span><span id="S4.T5.8.13.3.2" class="ltx_text" style="font-size:90%;color:#FF0000;">(+6)</span>
</td>
</tr>
<tr id="S4.T5.1.1" class="ltx_tr">
<td id="S4.T5.1.1.1" class="ltx_td ltx_align_left">
<span class="ltx_rule" style="width:0.0pt;height:12.0pt;background:black;display:inline-block;"></span><span id="S4.T5.1.1.1.1" class="ltx_text" style="font-size:90%;">   + BiFPN </span><math id="S4.T5.1.1.1.m1.1" class="ltx_math_unparsed" alttext="(\times 1)" display="inline"><semantics id="S4.T5.1.1.1.m1.1a"><mrow id="S4.T5.1.1.1.m1.1b"><mo maxsize="80%" minsize="80%" id="S4.T5.1.1.1.m1.1.1">(</mo><mo lspace="0em" mathsize="80%" rspace="0.222em" id="S4.T5.1.1.1.m1.1.2">×</mo><mn mathsize="80%" id="S4.T5.1.1.1.m1.1.3">1</mn><mo maxsize="80%" minsize="80%" id="S4.T5.1.1.1.m1.1.4">)</mo></mrow><annotation encoding="application/x-tex" id="S4.T5.1.1.1.m1.1c">(\times 1)</annotation></semantics></math>
</td>
<td id="S4.T5.1.1.2" class="ltx_td ltx_align_left">
<span id="S4.T5.1.1.2.1" class="ltx_text" style="font-size:90%;">40.5 </span><span id="S4.T5.1.1.2.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+0.6)</span>
</td>
<td id="S4.T5.1.1.3" class="ltx_td ltx_align_left">
<span id="S4.T5.1.1.3.1" class="ltx_text" style="font-size:90%;">16 </span><span id="S4.T5.1.1.3.2" class="ltx_text" style="font-size:90%;color:#FF0000;">(+3)</span>
</td>
</tr>
<tr id="S4.T5.2.2" class="ltx_tr">
<td id="S4.T5.2.2.1" class="ltx_td ltx_align_left">
<span id="S4.T5.2.2.1.1" class="ltx_text" style="font-size:90%;">+ BiFPN </span><math id="S4.T5.2.2.1.m1.1" class="ltx_math_unparsed" alttext="(\times 3)" display="inline"><semantics id="S4.T5.2.2.1.m1.1a"><mrow id="S4.T5.2.2.1.m1.1b"><mo maxsize="80%" minsize="80%" id="S4.T5.2.2.1.m1.1.1">(</mo><mo lspace="0em" mathsize="80%" rspace="0.222em" id="S4.T5.2.2.1.m1.1.2">×</mo><mn mathsize="80%" id="S4.T5.2.2.1.m1.1.3">3</mn><mo maxsize="80%" minsize="80%" id="S4.T5.2.2.1.m1.1.4">)</mo></mrow><annotation encoding="application/x-tex" id="S4.T5.2.2.1.m1.1c">(\times 3)</annotation></semantics></math>
</td>
<td id="S4.T5.2.2.2" class="ltx_td ltx_align_left">
<span id="S4.T5.2.2.2.1" class="ltx_text" style="font-size:90%;">41.5 </span><span id="S4.T5.2.2.2.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+1.6)</span>
</td>
<td id="S4.T5.2.2.3" class="ltx_td ltx_align_left">
<span id="S4.T5.2.2.3.1" class="ltx_text" style="font-size:90%;">18 </span><span id="S4.T5.2.2.3.2" class="ltx_text" style="font-size:90%;color:#FF0000;">(+5)</span>
</td>
</tr>
<tr id="S4.T5.3.3" class="ltx_tr">
<td id="S4.T5.3.3.1" class="ltx_td ltx_align_left">
<span id="S4.T5.3.3.1.1" class="ltx_text" style="font-size:90%;">+ BiFPN </span><math id="S4.T5.3.3.1.m1.1" class="ltx_math_unparsed" alttext="(\times 5)" display="inline"><semantics id="S4.T5.3.3.1.m1.1a"><mrow id="S4.T5.3.3.1.m1.1b"><mo maxsize="80%" minsize="80%" id="S4.T5.3.3.1.m1.1.1">(</mo><mo lspace="0em" mathsize="80%" rspace="0.222em" id="S4.T5.3.3.1.m1.1.2">×</mo><mn mathsize="80%" id="S4.T5.3.3.1.m1.1.3">5</mn><mo maxsize="80%" minsize="80%" id="S4.T5.3.3.1.m1.1.4">)</mo></mrow><annotation encoding="application/x-tex" id="S4.T5.3.3.1.m1.1c">(\times 5)</annotation></semantics></math>
</td>
<td id="S4.T5.3.3.2" class="ltx_td ltx_align_left">
<span id="S4.T5.3.3.2.1" class="ltx_text" style="font-size:90%;">41.3 </span><span id="S4.T5.3.3.2.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+1.4)</span>
</td>
<td id="S4.T5.3.3.3" class="ltx_td ltx_align_left">
<span id="S4.T5.3.3.3.1" class="ltx_text" style="font-size:90%;">20 </span><span id="S4.T5.3.3.3.2" class="ltx_text" style="font-size:90%;color:#FF0000;">(+7)</span>
</td>
</tr>
<tr id="S4.T5.4.4" class="ltx_tr">
<td id="S4.T5.4.4.1" class="ltx_td ltx_align_left">
<span id="S4.T5.4.4.1.1" class="ltx_text" style="font-size:90%;">+ BiFPN </span><math id="S4.T5.4.4.1.m1.1" class="ltx_math_unparsed" alttext="(\times 7)" display="inline"><semantics id="S4.T5.4.4.1.m1.1a"><mrow id="S4.T5.4.4.1.m1.1b"><mo maxsize="80%" minsize="80%" id="S4.T5.4.4.1.m1.1.1">(</mo><mo lspace="0em" mathsize="80%" rspace="0.222em" id="S4.T5.4.4.1.m1.1.2">×</mo><mn mathsize="80%" id="S4.T5.4.4.1.m1.1.3">7</mn><mo maxsize="80%" minsize="80%" id="S4.T5.4.4.1.m1.1.4">)</mo></mrow><annotation encoding="application/x-tex" id="S4.T5.4.4.1.m1.1c">(\times 7)</annotation></semantics></math>
</td>
<td id="S4.T5.4.4.2" class="ltx_td ltx_align_left">
<span id="S4.T5.4.4.2.1" class="ltx_text" style="font-size:90%;">41.1 </span><span id="S4.T5.4.4.2.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+1.2)</span>
</td>
<td id="S4.T5.4.4.3" class="ltx_td ltx_align_left">
<span id="S4.T5.4.4.3.1" class="ltx_text" style="font-size:90%;">22 </span><span id="S4.T5.4.4.3.2" class="ltx_text" style="font-size:90%;color:#FF0000;">(+9)</span>
</td>
</tr>
<tr id="S4.T5.8.14" class="ltx_tr">
<td id="S4.T5.8.14.1" class="ltx_td ltx_align_left"><span id="S4.T5.8.14.1.1" class="ltx_text" style="font-size:90%;">        + Cascade RPN</span></td>
<td id="S4.T5.8.14.2" class="ltx_td ltx_align_left">
<span id="S4.T5.8.14.2.1" class="ltx_text" style="font-size:90%;">42.1 </span><span id="S4.T5.8.14.2.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+2.2)</span>
</td>
<td id="S4.T5.8.14.3" class="ltx_td ltx_align_left">
<span id="S4.T5.8.14.3.1" class="ltx_text" style="font-size:90%;">25 </span><span id="S4.T5.8.14.3.2" class="ltx_text" style="font-size:90%;color:#FF0000;">(+12)</span>
</td>
</tr>
<tr id="S4.T5.5.5" class="ltx_tr">
<td id="S4.T5.5.5.1" class="ltx_td ltx_align_left">
<span class="ltx_rule" style="width:0.0pt;height:12.0pt;background:black;display:inline-block;"></span><span id="S4.T5.5.5.1.1" class="ltx_text" style="font-size:90%;">   + NAS-FPN </span><math id="S4.T5.5.5.1.m1.1" class="ltx_math_unparsed" alttext="(\times 1)" display="inline"><semantics id="S4.T5.5.5.1.m1.1a"><mrow id="S4.T5.5.5.1.m1.1b"><mo maxsize="80%" minsize="80%" id="S4.T5.5.5.1.m1.1.1">(</mo><mo lspace="0em" mathsize="80%" rspace="0.222em" id="S4.T5.5.5.1.m1.1.2">×</mo><mn mathsize="80%" id="S4.T5.5.5.1.m1.1.3">1</mn><mo maxsize="80%" minsize="80%" id="S4.T5.5.5.1.m1.1.4">)</mo></mrow><annotation encoding="application/x-tex" id="S4.T5.5.5.1.m1.1c">(\times 1)</annotation></semantics></math>
</td>
<td id="S4.T5.5.5.2" class="ltx_td ltx_align_left">
<span id="S4.T5.5.5.2.1" class="ltx_text" style="font-size:90%;">41.6 </span><span id="S4.T5.5.5.2.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+1.7)</span>
</td>
<td id="S4.T5.5.5.3" class="ltx_td ltx_align_left">
<span id="S4.T5.5.5.3.1" class="ltx_text" style="font-size:90%;">16 </span><span id="S4.T5.5.5.3.2" class="ltx_text" style="font-size:90%;color:#FF0000;">(+3)</span>
</td>
</tr>
<tr id="S4.T5.6.6" class="ltx_tr">
<td id="S4.T5.6.6.1" class="ltx_td ltx_align_left">
<span id="S4.T5.6.6.1.1" class="ltx_text" style="font-size:90%;">+ NAS-FPN </span><math id="S4.T5.6.6.1.m1.1" class="ltx_math_unparsed" alttext="(\times 3)" display="inline"><semantics id="S4.T5.6.6.1.m1.1a"><mrow id="S4.T5.6.6.1.m1.1b"><mo maxsize="80%" minsize="80%" id="S4.T5.6.6.1.m1.1.1">(</mo><mo lspace="0em" mathsize="80%" rspace="0.222em" id="S4.T5.6.6.1.m1.1.2">×</mo><mn mathsize="80%" id="S4.T5.6.6.1.m1.1.3">3</mn><mo maxsize="80%" minsize="80%" id="S4.T5.6.6.1.m1.1.4">)</mo></mrow><annotation encoding="application/x-tex" id="S4.T5.6.6.1.m1.1c">(\times 3)</annotation></semantics></math>
</td>
<td id="S4.T5.6.6.2" class="ltx_td ltx_align_left">
<span id="S4.T5.6.6.2.1" class="ltx_text" style="font-size:90%;">44.2 </span><span id="S4.T5.6.6.2.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+4.3)</span>
</td>
<td id="S4.T5.6.6.3" class="ltx_td ltx_align_left">
<span id="S4.T5.6.6.3.1" class="ltx_text" style="font-size:90%;">18 </span><span id="S4.T5.6.6.3.2" class="ltx_text" style="font-size:90%;color:#FF0000;">(+5)</span>
</td>
</tr>
<tr id="S4.T5.7.7" class="ltx_tr">
<td id="S4.T5.7.7.1" class="ltx_td ltx_align_left">
<span id="S4.T5.7.7.1.1" class="ltx_text" style="font-size:90%;">+ NAS-FPN </span><math id="S4.T5.7.7.1.m1.1" class="ltx_math_unparsed" alttext="(\times 5)" display="inline"><semantics id="S4.T5.7.7.1.m1.1a"><mrow id="S4.T5.7.7.1.m1.1b"><mo maxsize="80%" minsize="80%" id="S4.T5.7.7.1.m1.1.1">(</mo><mo lspace="0em" mathsize="80%" rspace="0.222em" id="S4.T5.7.7.1.m1.1.2">×</mo><mn mathsize="80%" id="S4.T5.7.7.1.m1.1.3">5</mn><mo maxsize="80%" minsize="80%" id="S4.T5.7.7.1.m1.1.4">)</mo></mrow><annotation encoding="application/x-tex" id="S4.T5.7.7.1.m1.1c">(\times 5)</annotation></semantics></math>
</td>
<td id="S4.T5.7.7.2" class="ltx_td ltx_align_left">
<span id="S4.T5.7.7.2.1" class="ltx_text" style="font-size:90%;">45.3 </span><span id="S4.T5.7.7.2.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+5.4)</span>
</td>
<td id="S4.T5.7.7.3" class="ltx_td ltx_align_left">
<span id="S4.T5.7.7.3.1" class="ltx_text" style="font-size:90%;">20 </span><span id="S4.T5.7.7.3.2" class="ltx_text" style="font-size:90%;color:#FF0000;">(+7)</span>
</td>
</tr>
<tr id="S4.T5.8.8" class="ltx_tr">
<td id="S4.T5.8.8.1" class="ltx_td ltx_align_left">
<span id="S4.T5.8.8.1.1" class="ltx_text" style="font-size:90%;">+ NAS-FPN </span><math id="S4.T5.8.8.1.m1.1" class="ltx_math_unparsed" alttext="(\times 7)" display="inline"><semantics id="S4.T5.8.8.1.m1.1a"><mrow id="S4.T5.8.8.1.m1.1b"><mo maxsize="80%" minsize="80%" id="S4.T5.8.8.1.m1.1.1">(</mo><mo lspace="0em" mathsize="80%" rspace="0.222em" id="S4.T5.8.8.1.m1.1.2">×</mo><mn mathsize="80%" id="S4.T5.8.8.1.m1.1.3">7</mn><mo maxsize="80%" minsize="80%" id="S4.T5.8.8.1.m1.1.4">)</mo></mrow><annotation encoding="application/x-tex" id="S4.T5.8.8.1.m1.1c">(\times 7)</annotation></semantics></math>
</td>
<td id="S4.T5.8.8.2" class="ltx_td ltx_align_left">
<span id="S4.T5.8.8.2.1" class="ltx_text" style="font-size:90%;">45.7 </span><span id="S4.T5.8.8.2.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+5.8)</span>
</td>
<td id="S4.T5.8.8.3" class="ltx_td ltx_align_left">
<span id="S4.T5.8.8.3.1" class="ltx_text" style="font-size:90%;">22 </span><span id="S4.T5.8.8.3.2" class="ltx_text" style="font-size:90%;color:#FF0000;">(+9)</span>
</td>
</tr>
<tr id="S4.T5.8.15" class="ltx_tr">
<td id="S4.T5.8.15.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T5.8.15.1.1" class="ltx_text" style="font-size:90%;">        + Cascade RPN</span></td>
<td id="S4.T5.8.15.2" class="ltx_td ltx_align_left ltx_border_bb">
<span id="S4.T5.8.15.2.1" class="ltx_text" style="font-size:90%;">47.6 </span><span id="S4.T5.8.15.2.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+7.7)</span>
</td>
<td id="S4.T5.8.15.3" class="ltx_td ltx_align_left ltx_border_bb">
<span id="S4.T5.8.15.3.1" class="ltx_text" style="font-size:90%;">24 </span><span id="S4.T5.8.15.3.2" class="ltx_text" style="font-size:90%;color:#FF0000;">(+11)</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 5: </span>Ablation analysis of detector components on COCO <em id="S4.T5.13.1" class="ltx_emph ltx_font_italic">val</em> set. The models are trained for 12 epochs on 8 NVIDIA 3090 GPUs, with a base learning rate 0.01 which is divided by 10 after 8 and 11 epochs.</figcaption>
</figure>
<figure id="S4.T6" class="ltx_table">
<table id="S4.T6.8" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T6.8.9" class="ltx_tr">
<td id="S4.T6.8.9.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T6.8.9.1.1" class="ltx_text" style="font-size:90%;">Model</span></td>
<td id="S4.T6.8.9.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T6.8.9.2.1" class="ltx_text" style="font-size:90%;">mAP</span></td>
<td id="S4.T6.8.9.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T6.8.9.3.1" class="ltx_text" style="font-size:90%;">Time (h)</span></td>
</tr>
<tr id="S4.T6.8.10" class="ltx_tr">
<td id="S4.T6.8.10.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T6.8.10.1.1" class="ltx_text" style="font-size:90%;">Faster R-CNN</span></td>
<td id="S4.T6.8.10.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T6.8.10.2.1" class="ltx_text" style="font-size:90%;">12.8</span></td>
<td id="S4.T6.8.10.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T6.8.10.3.1" class="ltx_text" style="font-size:90%;">1.8</span></td>
</tr>
<tr id="S4.T6.8.11" class="ltx_tr">
<td id="S4.T6.8.11.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T6.8.11.1.1" class="ltx_text" style="font-size:90%;">Cascade R-CNN</span></td>
<td id="S4.T6.8.11.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T6.8.11.2.1" class="ltx_text" style="font-size:90%;">15.2</span></td>
<td id="S4.T6.8.11.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T6.8.11.3.1" class="ltx_text" style="font-size:90%;">2.0</span></td>
</tr>
<tr id="S4.T6.8.12" class="ltx_tr">
<td id="S4.T6.8.12.1" class="ltx_td ltx_align_left">
<span class="ltx_rule" style="width:0.0pt;height:12.0pt;background:black;display:inline-block;"></span><span id="S4.T6.8.12.1.1" class="ltx_text" style="font-size:90%;">   + PAFPN</span>
</td>
<td id="S4.T6.8.12.2" class="ltx_td ltx_align_left">
<span id="S4.T6.8.12.2.1" class="ltx_text" style="font-size:90%;">16.7 </span><span id="S4.T6.8.12.2.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+1.5)</span>
</td>
<td id="S4.T6.8.12.3" class="ltx_td ltx_align_left">
<span id="S4.T6.8.12.3.1" class="ltx_text" style="font-size:90%;">2.5 </span><span id="S4.T6.8.12.3.2" class="ltx_text" style="font-size:90%;color:#FF0000;">(+0.5)</span>
</td>
</tr>
<tr id="S4.T6.8.13" class="ltx_tr">
<td id="S4.T6.8.13.1" class="ltx_td ltx_align_left"><span id="S4.T6.8.13.1.1" class="ltx_text" style="font-size:90%;">        + Cascade RPN</span></td>
<td id="S4.T6.8.13.2" class="ltx_td ltx_align_left">
<span id="S4.T6.8.13.2.1" class="ltx_text" style="font-size:90%;">17.3 </span><span id="S4.T6.8.13.2.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+2.1)</span>
</td>
<td id="S4.T6.8.13.3" class="ltx_td ltx_align_left">
<span id="S4.T6.8.13.3.1" class="ltx_text" style="font-size:90%;">3.5 </span><span id="S4.T6.8.13.3.2" class="ltx_text" style="font-size:90%;color:#FF0000;">(+1.5)</span>
</td>
</tr>
<tr id="S4.T6.1.1" class="ltx_tr">
<td id="S4.T6.1.1.1" class="ltx_td ltx_align_left">
<span class="ltx_rule" style="width:0.0pt;height:12.0pt;background:black;display:inline-block;"></span><span id="S4.T6.1.1.1.1" class="ltx_text" style="font-size:90%;">   + BiFPN </span><math id="S4.T6.1.1.1.m1.1" class="ltx_math_unparsed" alttext="(\times 1)" display="inline"><semantics id="S4.T6.1.1.1.m1.1a"><mrow id="S4.T6.1.1.1.m1.1b"><mo maxsize="80%" minsize="80%" id="S4.T6.1.1.1.m1.1.1">(</mo><mo lspace="0em" mathsize="80%" rspace="0.222em" id="S4.T6.1.1.1.m1.1.2">×</mo><mn mathsize="80%" id="S4.T6.1.1.1.m1.1.3">1</mn><mo maxsize="80%" minsize="80%" id="S4.T6.1.1.1.m1.1.4">)</mo></mrow><annotation encoding="application/x-tex" id="S4.T6.1.1.1.m1.1c">(\times 1)</annotation></semantics></math>
</td>
<td id="S4.T6.1.1.2" class="ltx_td ltx_align_left">
<span id="S4.T6.1.1.2.1" class="ltx_text" style="font-size:90%;">17.0 </span><span id="S4.T6.1.1.2.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+1.8)</span>
</td>
<td id="S4.T6.1.1.3" class="ltx_td ltx_align_left">
<span id="S4.T6.1.1.3.1" class="ltx_text" style="font-size:90%;">2.5 </span><span id="S4.T6.1.1.3.2" class="ltx_text" style="font-size:90%;color:#FF0000;">(+0.5)</span>
</td>
</tr>
<tr id="S4.T6.2.2" class="ltx_tr">
<td id="S4.T6.2.2.1" class="ltx_td ltx_align_left">
<span id="S4.T6.2.2.1.1" class="ltx_text" style="font-size:90%;">+ BiFPN </span><math id="S4.T6.2.2.1.m1.1" class="ltx_math_unparsed" alttext="(\times 3)" display="inline"><semantics id="S4.T6.2.2.1.m1.1a"><mrow id="S4.T6.2.2.1.m1.1b"><mo maxsize="80%" minsize="80%" id="S4.T6.2.2.1.m1.1.1">(</mo><mo lspace="0em" mathsize="80%" rspace="0.222em" id="S4.T6.2.2.1.m1.1.2">×</mo><mn mathsize="80%" id="S4.T6.2.2.1.m1.1.3">3</mn><mo maxsize="80%" minsize="80%" id="S4.T6.2.2.1.m1.1.4">)</mo></mrow><annotation encoding="application/x-tex" id="S4.T6.2.2.1.m1.1c">(\times 3)</annotation></semantics></math>
</td>
<td id="S4.T6.2.2.2" class="ltx_td ltx_align_left">
<span id="S4.T6.2.2.2.1" class="ltx_text" style="font-size:90%;">17.1 </span><span id="S4.T6.2.2.2.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+1.9)</span>
</td>
<td id="S4.T6.2.2.3" class="ltx_td ltx_align_left">
<span id="S4.T6.2.2.3.1" class="ltx_text" style="font-size:90%;">2.8 </span><span id="S4.T6.2.2.3.2" class="ltx_text" style="font-size:90%;color:#FF0000;">(+0.8)</span>
</td>
</tr>
<tr id="S4.T6.3.3" class="ltx_tr">
<td id="S4.T6.3.3.1" class="ltx_td ltx_align_left">
<span id="S4.T6.3.3.1.1" class="ltx_text" style="font-size:90%;">+ BiFPN </span><math id="S4.T6.3.3.1.m1.1" class="ltx_math_unparsed" alttext="(\times 5)" display="inline"><semantics id="S4.T6.3.3.1.m1.1a"><mrow id="S4.T6.3.3.1.m1.1b"><mo maxsize="80%" minsize="80%" id="S4.T6.3.3.1.m1.1.1">(</mo><mo lspace="0em" mathsize="80%" rspace="0.222em" id="S4.T6.3.3.1.m1.1.2">×</mo><mn mathsize="80%" id="S4.T6.3.3.1.m1.1.3">5</mn><mo maxsize="80%" minsize="80%" id="S4.T6.3.3.1.m1.1.4">)</mo></mrow><annotation encoding="application/x-tex" id="S4.T6.3.3.1.m1.1c">(\times 5)</annotation></semantics></math>
</td>
<td id="S4.T6.3.3.2" class="ltx_td ltx_align_left">
<span id="S4.T6.3.3.2.1" class="ltx_text" style="font-size:90%;">16.8 </span><span id="S4.T6.3.3.2.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+1.6)</span>
</td>
<td id="S4.T6.3.3.3" class="ltx_td ltx_align_left">
<span id="S4.T6.3.3.3.1" class="ltx_text" style="font-size:90%;">3.1 </span><span id="S4.T6.3.3.3.2" class="ltx_text" style="font-size:90%;color:#FF0000;">(+1.1)</span>
</td>
</tr>
<tr id="S4.T6.4.4" class="ltx_tr">
<td id="S4.T6.4.4.1" class="ltx_td ltx_align_left">
<span id="S4.T6.4.4.1.1" class="ltx_text" style="font-size:90%;">+ BiFPN </span><math id="S4.T6.4.4.1.m1.1" class="ltx_math_unparsed" alttext="(\times 7)" display="inline"><semantics id="S4.T6.4.4.1.m1.1a"><mrow id="S4.T6.4.4.1.m1.1b"><mo maxsize="80%" minsize="80%" id="S4.T6.4.4.1.m1.1.1">(</mo><mo lspace="0em" mathsize="80%" rspace="0.222em" id="S4.T6.4.4.1.m1.1.2">×</mo><mn mathsize="80%" id="S4.T6.4.4.1.m1.1.3">7</mn><mo maxsize="80%" minsize="80%" id="S4.T6.4.4.1.m1.1.4">)</mo></mrow><annotation encoding="application/x-tex" id="S4.T6.4.4.1.m1.1c">(\times 7)</annotation></semantics></math>
</td>
<td id="S4.T6.4.4.2" class="ltx_td ltx_align_left">
<span id="S4.T6.4.4.2.1" class="ltx_text" style="font-size:90%;">16.5 </span><span id="S4.T6.4.4.2.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+1.3)</span>
</td>
<td id="S4.T6.4.4.3" class="ltx_td ltx_align_left">
<span id="S4.T6.4.4.3.1" class="ltx_text" style="font-size:90%;">3.4 </span><span id="S4.T6.4.4.3.2" class="ltx_text" style="font-size:90%;color:#FF0000;">(+1.4)</span>
</td>
</tr>
<tr id="S4.T6.8.14" class="ltx_tr">
<td id="S4.T6.8.14.1" class="ltx_td ltx_align_left"><span id="S4.T6.8.14.1.1" class="ltx_text" style="font-size:90%;">        + Cascade RPN</span></td>
<td id="S4.T6.8.14.2" class="ltx_td ltx_align_left">
<span id="S4.T6.8.14.2.1" class="ltx_text" style="font-size:90%;">16.7 </span><span id="S4.T6.8.14.2.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+1.5)</span>
</td>
<td id="S4.T6.8.14.3" class="ltx_td ltx_align_left">
<span id="S4.T6.8.14.3.1" class="ltx_text" style="font-size:90%;">4.3 </span><span id="S4.T6.8.14.3.2" class="ltx_text" style="font-size:90%;color:#FF0000;">(+2.3)</span>
</td>
</tr>
<tr id="S4.T6.5.5" class="ltx_tr">
<td id="S4.T6.5.5.1" class="ltx_td ltx_align_left">
<span class="ltx_rule" style="width:0.0pt;height:12.0pt;background:black;display:inline-block;"></span><span id="S4.T6.5.5.1.1" class="ltx_text" style="font-size:90%;">   + NAS-FPN </span><math id="S4.T6.5.5.1.m1.1" class="ltx_math_unparsed" alttext="(\times 1)" display="inline"><semantics id="S4.T6.5.5.1.m1.1a"><mrow id="S4.T6.5.5.1.m1.1b"><mo maxsize="80%" minsize="80%" id="S4.T6.5.5.1.m1.1.1">(</mo><mo lspace="0em" mathsize="80%" rspace="0.222em" id="S4.T6.5.5.1.m1.1.2">×</mo><mn mathsize="80%" id="S4.T6.5.5.1.m1.1.3">1</mn><mo maxsize="80%" minsize="80%" id="S4.T6.5.5.1.m1.1.4">)</mo></mrow><annotation encoding="application/x-tex" id="S4.T6.5.5.1.m1.1c">(\times 1)</annotation></semantics></math>
</td>
<td id="S4.T6.5.5.2" class="ltx_td ltx_align_left">
<span id="S4.T6.5.5.2.1" class="ltx_text" style="font-size:90%;">17.3 </span><span id="S4.T6.5.5.2.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+2.1)</span>
</td>
<td id="S4.T6.5.5.3" class="ltx_td ltx_align_left">
<span id="S4.T6.5.5.3.1" class="ltx_text" style="font-size:90%;">2.8 </span><span id="S4.T6.5.5.3.2" class="ltx_text" style="font-size:90%;color:#FF0000;">(+0.8)</span>
</td>
</tr>
<tr id="S4.T6.6.6" class="ltx_tr">
<td id="S4.T6.6.6.1" class="ltx_td ltx_align_left">
<span id="S4.T6.6.6.1.1" class="ltx_text" style="font-size:90%;">+ NAS-FPN </span><math id="S4.T6.6.6.1.m1.1" class="ltx_math_unparsed" alttext="(\times 3)" display="inline"><semantics id="S4.T6.6.6.1.m1.1a"><mrow id="S4.T6.6.6.1.m1.1b"><mo maxsize="80%" minsize="80%" id="S4.T6.6.6.1.m1.1.1">(</mo><mo lspace="0em" mathsize="80%" rspace="0.222em" id="S4.T6.6.6.1.m1.1.2">×</mo><mn mathsize="80%" id="S4.T6.6.6.1.m1.1.3">3</mn><mo maxsize="80%" minsize="80%" id="S4.T6.6.6.1.m1.1.4">)</mo></mrow><annotation encoding="application/x-tex" id="S4.T6.6.6.1.m1.1c">(\times 3)</annotation></semantics></math>
</td>
<td id="S4.T6.6.6.2" class="ltx_td ltx_align_left">
<span id="S4.T6.6.6.2.1" class="ltx_text" style="font-size:90%;">18.3 </span><span id="S4.T6.6.6.2.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+3.1)</span>
</td>
<td id="S4.T6.6.6.3" class="ltx_td ltx_align_left">
<span id="S4.T6.6.6.3.1" class="ltx_text" style="font-size:90%;">3.0 </span><span id="S4.T6.6.6.3.2" class="ltx_text" style="font-size:90%;color:#FF0000;">(+1.0)</span>
</td>
</tr>
<tr id="S4.T6.7.7" class="ltx_tr">
<td id="S4.T6.7.7.1" class="ltx_td ltx_align_left">
<span id="S4.T6.7.7.1.1" class="ltx_text" style="font-size:90%;">+ NAS-FPN </span><math id="S4.T6.7.7.1.m1.1" class="ltx_math_unparsed" alttext="(\times 5)" display="inline"><semantics id="S4.T6.7.7.1.m1.1a"><mrow id="S4.T6.7.7.1.m1.1b"><mo maxsize="80%" minsize="80%" id="S4.T6.7.7.1.m1.1.1">(</mo><mo lspace="0em" mathsize="80%" rspace="0.222em" id="S4.T6.7.7.1.m1.1.2">×</mo><mn mathsize="80%" id="S4.T6.7.7.1.m1.1.3">5</mn><mo maxsize="80%" minsize="80%" id="S4.T6.7.7.1.m1.1.4">)</mo></mrow><annotation encoding="application/x-tex" id="S4.T6.7.7.1.m1.1c">(\times 5)</annotation></semantics></math>
</td>
<td id="S4.T6.7.7.2" class="ltx_td ltx_align_left">
<span id="S4.T6.7.7.2.1" class="ltx_text" style="font-size:90%;">18.6 </span><span id="S4.T6.7.7.2.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+3.4)</span>
</td>
<td id="S4.T6.7.7.3" class="ltx_td ltx_align_left">
<span id="S4.T6.7.7.3.1" class="ltx_text" style="font-size:90%;">3.2 </span><span id="S4.T6.7.7.3.2" class="ltx_text" style="font-size:90%;color:#FF0000;">(+1.2)</span>
</td>
</tr>
<tr id="S4.T6.8.8" class="ltx_tr">
<td id="S4.T6.8.8.1" class="ltx_td ltx_align_left">
<span id="S4.T6.8.8.1.1" class="ltx_text" style="font-size:90%;">+ NAS-FPN </span><math id="S4.T6.8.8.1.m1.1" class="ltx_math_unparsed" alttext="(\times 7)" display="inline"><semantics id="S4.T6.8.8.1.m1.1a"><mrow id="S4.T6.8.8.1.m1.1b"><mo maxsize="80%" minsize="80%" id="S4.T6.8.8.1.m1.1.1">(</mo><mo lspace="0em" mathsize="80%" rspace="0.222em" id="S4.T6.8.8.1.m1.1.2">×</mo><mn mathsize="80%" id="S4.T6.8.8.1.m1.1.3">7</mn><mo maxsize="80%" minsize="80%" id="S4.T6.8.8.1.m1.1.4">)</mo></mrow><annotation encoding="application/x-tex" id="S4.T6.8.8.1.m1.1c">(\times 7)</annotation></semantics></math>
</td>
<td id="S4.T6.8.8.2" class="ltx_td ltx_align_left">
<span id="S4.T6.8.8.2.1" class="ltx_text" style="font-size:90%;">19.4 </span><span id="S4.T6.8.8.2.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+4.2)</span>
</td>
<td id="S4.T6.8.8.3" class="ltx_td ltx_align_left">
<span id="S4.T6.8.8.3.1" class="ltx_text" style="font-size:90%;">3.5 </span><span id="S4.T6.8.8.3.2" class="ltx_text" style="font-size:90%;color:#FF0000;">(+1.5)</span>
</td>
</tr>
<tr id="S4.T6.8.15" class="ltx_tr">
<td id="S4.T6.8.15.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T6.8.15.1.1" class="ltx_text" style="font-size:90%;">        + Cascade RPN</span></td>
<td id="S4.T6.8.15.2" class="ltx_td ltx_align_left ltx_border_bb">
<span id="S4.T6.8.15.2.1" class="ltx_text" style="font-size:90%;">20.2 </span><span id="S4.T6.8.15.2.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+5.0)</span>
</td>
<td id="S4.T6.8.15.3" class="ltx_td ltx_align_left ltx_border_bb">
<span id="S4.T6.8.15.3.1" class="ltx_text" style="font-size:90%;">4.2 </span><span id="S4.T6.8.15.3.2" class="ltx_text" style="font-size:90%;color:#FF0000;">(+2.2)</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 6: </span>Ablation analysis of detector components on MVD <em id="S4.T6.14.1" class="ltx_emph ltx_font_italic">val</em> set. For fast convergence, we initialize the models with the counterparts in Table <a href="#S4.T5" title="Table 5 ‣ 4.4.1 Detector Components Analysis ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, and train them for 12 epochs on 8 NVIDIA 3090 GPUs, with a base learning rate 0.01 which is divided by 10 after 8 and 11 epochs.</figcaption>
</figure>
</section>
<section id="S4.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2 </span>Hierarchical Loss Strategies</h4>

<div id="S4.SS4.SSS2.p1" class="ltx_para">
<p id="S4.SS4.SSS2.p1.1" class="ltx_p">To demonstrate the effectiveness of our hierarchy-aware cross-dataset loss suppression (HCLS), we compare a number of hierarchical loss strategies in Table <a href="#S4.T7" title="Table 7 ‣ 4.4.2 Hierarchical Loss Strategies ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> and <a href="#S4.T8" title="Table 8 ‣ 4.4.2 Hierarchical Loss Strategies ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. The used object detector in Table <a href="#S4.T7" title="Table 7 ‣ 4.4.2 Hierarchical Loss Strategies ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> is Cascade R-CNN based on SEER-RegNet32gf, while the used object detector in Table <a href="#S4.T8" title="Table 8 ‣ 4.4.2 Hierarchical Loss Strategies ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> is Cascade R-CNN based on SEER-RegNet32gf with NAS-FPN and Cascade RPN. For a quick evaluation, the five models in the tables are trained for only 420k iterations, with a base learning rate of 0.01, which is reduced by a factor of 0.1 at 280k iterations.</p>
</div>
<div id="S4.SS4.SSS2.p2" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><em id="S4.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">Baseline</em> refers to a situation where the semantic hierarchy is not taken into consideration, and each annotated bounding box is assigned a single positive class label. As a result, there is no loss adaptation applied.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><em id="S4.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">Naive loss suppression</em> denotes that the loss calculation for the classification task takes the semantic hierarchy of OID into account by ignoring the losses for the children and parent categories. This approach incorporates the semantic hierarchy by removing the impact of relationships between parent and child categories, but also leads to an evident loss of positive samples for the superclasses in OID, resulting in lower performance on OID.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><em id="S4.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">Unified hierarchy</em> takes into account all parent-child relationships across datasets by considering the cross-dataset label duplication presented in Table <a href="#S3.T1" title="Table 1 ‣ 3.2.3 Hierarchy-aware Cross-dataset Loss Suppression ‣ 3.2 Cross-dataset Model Training ‣ 3 Method ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, cross-dataset semantic hierarchy presented in Table <a href="#S3.T2" title="Table 2 ‣ 3.2.4 Overall Formulation ‣ 3.2 Cross-dataset Model Training ‣ 3 Method ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, and the original semantic hierarchy of OID for each category in the unified label space. It considers all parents and semantic equivalents as positive and eliminates the losses over all child categories, thereby increasing the training set for superclasses with more positive samples, resulting in a significant improvement in performance on OID (+3.1 AP50 in Table <a href="#S4.T7" title="Table 7 ‣ 4.4.2 Hierarchical Loss Strategies ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> and +1.8 AP50 in Table <a href="#S4.T8" title="Table 8 ‣ 4.4.2 Hierarchical Loss Strategies ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>). However, this approach may negatively impact the performance on COCO and MVD, as categories from different datasets may match based on language cues, but still be semantically different. For example, the <em id="S4.I1.i3.p1.1.2" class="ltx_emph ltx_font_italic">bear</em> category in COCO encompasses a wide range of carnivorous mammals of the Ursidae family, while its equivalent <em id="S4.I1.i3.p1.1.3" class="ltx_emph ltx_font_italic">bear</em> (/m/01dws) in OID includes not only these conventional bears but also <em id="S4.I1.i3.p1.1.4" class="ltx_emph ltx_font_italic">teddy bears</em> (/m/0kmg4), leading to taxonomy inconsistencies. This was observed in the severe performance decline of the <em id="S4.I1.i3.p1.1.5" class="ltx_emph ltx_font_italic">bear</em> category in COCO, with an AP of 41.5 using the <em id="S4.I1.i3.p1.1.6" class="ltx_emph ltx_font_italic">Unified hierarchy</em>, compared to APs over 65.1 for other categories in Table <a href="#S4.T7" title="Table 7 ‣ 4.4.2 Hierarchical Loss Strategies ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, with the best AP of 67.9 achieved by the HCLS.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p"><em id="S4.I1.i4.p1.1.1" class="ltx_emph ltx_font_italic">OID hierarchy</em> only takes into account the semantic hierarchy of OID. It does not consider the relationships between categories from different datasets. This is a common approach when working with OID <cite class="ltx_cite ltx_citemacro_citep">(X. Zhou <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>)</cite>, but it means that cross-dataset relationships are not incorporated into the loss adaptation.</p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p">Our loss strategy, denoted as <em id="S4.I1.i5.p1.1.1" class="ltx_emph ltx_font_italic">OID hierarchy</em> + <em id="S4.I1.i5.p1.1.2" class="ltx_emph ltx_font_italic">HCLS</em>, cleverly incorporates the semantic hierarchy of OID while addressing label duplication and accounting for the overarching semantic hierarchy across all three datasets. This adaptive loss computation approach yields remarkable performance, as evidenced by achieving the highest accuracy on OID, with a significant +3.5 AP50 improvement according to Table <a href="#S4.T7" title="Table 7 ‣ 4.4.2 Hierarchical Loss Strategies ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> and +2.7 AP50 according to Table <a href="#S4.T8" title="Table 8 ‣ 4.4.2 Hierarchical Loss Strategies ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. Furthermore, this strategy offers a modest enhancement on COCO and maintains performance on par with the <em id="S4.I1.i5.p1.1.3" class="ltx_emph ltx_font_italic">Baseline</em> accuracy on MVD. Intriguingly, <em id="S4.I1.i5.p1.1.4" class="ltx_emph ltx_font_italic">Naive loss suppression</em> performs comparably, and at times, even slightly better than our designed loss strategy on COCO and MVD. However, considering the substantial performance improvement observed on OID, we believe our method remains notably superior.</p>
</div>
</li>
</ul>
</div>
<figure id="S4.T7" class="ltx_table">
<table id="S4.T7.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T7.1.1" class="ltx_tr">
<td id="S4.T7.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T7.1.1.1.1" class="ltx_text" style="font-size:90%;">Loss Strategy</span></td>
<td id="S4.T7.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T7.1.1.2.1" class="ltx_text" style="font-size:90%;">COCO</span></td>
<td id="S4.T7.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T7.1.1.3.1" class="ltx_text" style="font-size:90%;">MVD</span></td>
<td id="S4.T7.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T7.1.1.4.1" class="ltx_text" style="font-size:90%;">OID</span></td>
</tr>
<tr id="S4.T7.1.2" class="ltx_tr">
<td id="S4.T7.1.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T7.1.2.1.1" class="ltx_text" style="font-size:90%;">Baseline</span></td>
<td id="S4.T7.1.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.1.2.2.1" class="ltx_text" style="font-size:90%;">36.2</span></td>
<td id="S4.T7.1.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.1.2.3.1" class="ltx_text" style="font-size:90%;">14.4</span></td>
<td id="S4.T7.1.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.1.2.4.1" class="ltx_text" style="font-size:90%;">61.7</span></td>
</tr>
<tr id="S4.T7.1.3" class="ltx_tr">
<td id="S4.T7.1.3.1" class="ltx_td ltx_align_left"><span id="S4.T7.1.3.1.1" class="ltx_text" style="font-size:90%;">Naive loss suppression</span></td>
<td id="S4.T7.1.3.2" class="ltx_td ltx_align_center"><span id="S4.T7.1.3.2.1" class="ltx_text" style="font-size:90%;">36.4</span></td>
<td id="S4.T7.1.3.3" class="ltx_td ltx_align_center"><span id="S4.T7.1.3.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">14.5</span></td>
<td id="S4.T7.1.3.4" class="ltx_td ltx_align_center"><span id="S4.T7.1.3.4.1" class="ltx_text" style="font-size:90%;">62.7</span></td>
</tr>
<tr id="S4.T7.1.4" class="ltx_tr">
<td id="S4.T7.1.4.1" class="ltx_td ltx_align_left"><span id="S4.T7.1.4.1.1" class="ltx_text" style="font-size:90%;">Unified hierarchy</span></td>
<td id="S4.T7.1.4.2" class="ltx_td ltx_align_center"><span id="S4.T7.1.4.2.1" class="ltx_text" style="font-size:90%;">36.3</span></td>
<td id="S4.T7.1.4.3" class="ltx_td ltx_align_center"><span id="S4.T7.1.4.3.1" class="ltx_text" style="font-size:90%;">14.0</span></td>
<td id="S4.T7.1.4.4" class="ltx_td ltx_align_center"><span id="S4.T7.1.4.4.1" class="ltx_text" style="font-size:90%;">64.8</span></td>
</tr>
<tr id="S4.T7.1.5" class="ltx_tr">
<td id="S4.T7.1.5.1" class="ltx_td ltx_align_left"><span id="S4.T7.1.5.1.1" class="ltx_text" style="font-size:90%;">OID hierarchy</span></td>
<td id="S4.T7.1.5.2" class="ltx_td ltx_align_center"><span id="S4.T7.1.5.2.1" class="ltx_text" style="font-size:90%;">36.4</span></td>
<td id="S4.T7.1.5.3" class="ltx_td ltx_align_center"><span id="S4.T7.1.5.3.1" class="ltx_text" style="font-size:90%;">14.0</span></td>
<td id="S4.T7.1.5.4" class="ltx_td ltx_align_center"><span id="S4.T7.1.5.4.1" class="ltx_text" style="font-size:90%;">64.8</span></td>
</tr>
<tr id="S4.T7.1.6" class="ltx_tr">
<td id="S4.T7.1.6.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T7.1.6.1.1" class="ltx_text" style="font-size:90%;">+ HCLS</span></td>
<td id="S4.T7.1.6.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T7.1.6.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">37.1</span></td>
<td id="S4.T7.1.6.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T7.1.6.3.1" class="ltx_text" style="font-size:90%;">14.3</span></td>
<td id="S4.T7.1.6.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T7.1.6.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">65.2</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 7: </span>Comparison on loss strategies. The metrics for COCO and MVD are mAP, and for OID, it is AP50. The best results are highlighted in <span id="S4.T7.6.1" class="ltx_text ltx_font_bold">bold font</span>.</figcaption>
</figure>
<figure id="S4.T8" class="ltx_table">
<table id="S4.T8.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T8.1.1" class="ltx_tr">
<td id="S4.T8.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T8.1.1.1.1" class="ltx_text" style="font-size:90%;">Loss Strategy</span></td>
<td id="S4.T8.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T8.1.1.2.1" class="ltx_text" style="font-size:90%;">COCO</span></td>
<td id="S4.T8.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T8.1.1.3.1" class="ltx_text" style="font-size:90%;">MVD</span></td>
<td id="S4.T8.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T8.1.1.4.1" class="ltx_text" style="font-size:90%;">OID</span></td>
</tr>
<tr id="S4.T8.1.2" class="ltx_tr">
<td id="S4.T8.1.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T8.1.2.1.1" class="ltx_text" style="font-size:90%;">Baseline</span></td>
<td id="S4.T8.1.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T8.1.2.2.1" class="ltx_text" style="font-size:90%;">44.1</span></td>
<td id="S4.T8.1.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T8.1.2.3.1" class="ltx_text" style="font-size:90%;">17.2</span></td>
<td id="S4.T8.1.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T8.1.2.4.1" class="ltx_text" style="font-size:90%;">65.3</span></td>
</tr>
<tr id="S4.T8.1.3" class="ltx_tr">
<td id="S4.T8.1.3.1" class="ltx_td ltx_align_left"><span id="S4.T8.1.3.1.1" class="ltx_text" style="font-size:90%;">Naive loss suppression</span></td>
<td id="S4.T8.1.3.2" class="ltx_td ltx_align_center"><span id="S4.T8.1.3.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">44.3</span></td>
<td id="S4.T8.1.3.3" class="ltx_td ltx_align_center"><span id="S4.T8.1.3.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">17.8</span></td>
<td id="S4.T8.1.3.4" class="ltx_td ltx_align_center"><span id="S4.T8.1.3.4.1" class="ltx_text" style="font-size:90%;">66.2</span></td>
</tr>
<tr id="S4.T8.1.4" class="ltx_tr">
<td id="S4.T8.1.4.1" class="ltx_td ltx_align_left"><span id="S4.T8.1.4.1.1" class="ltx_text" style="font-size:90%;">Unified hierarchy</span></td>
<td id="S4.T8.1.4.2" class="ltx_td ltx_align_center"><span id="S4.T8.1.4.2.1" class="ltx_text" style="font-size:90%;">43.1</span></td>
<td id="S4.T8.1.4.3" class="ltx_td ltx_align_center"><span id="S4.T8.1.4.3.1" class="ltx_text" style="font-size:90%;">17.1</span></td>
<td id="S4.T8.1.4.4" class="ltx_td ltx_align_center"><span id="S4.T8.1.4.4.1" class="ltx_text" style="font-size:90%;">67.1</span></td>
</tr>
<tr id="S4.T8.1.5" class="ltx_tr">
<td id="S4.T8.1.5.1" class="ltx_td ltx_align_left"><span id="S4.T8.1.5.1.1" class="ltx_text" style="font-size:90%;">OID hierarchy</span></td>
<td id="S4.T8.1.5.2" class="ltx_td ltx_align_center"><span id="S4.T8.1.5.2.1" class="ltx_text" style="font-size:90%;">43.5</span></td>
<td id="S4.T8.1.5.3" class="ltx_td ltx_align_center"><span id="S4.T8.1.5.3.1" class="ltx_text" style="font-size:90%;">16.2</span></td>
<td id="S4.T8.1.5.4" class="ltx_td ltx_align_center"><span id="S4.T8.1.5.4.1" class="ltx_text" style="font-size:90%;">67.6</span></td>
</tr>
<tr id="S4.T8.1.6" class="ltx_tr">
<td id="S4.T8.1.6.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T8.1.6.1.1" class="ltx_text" style="font-size:90%;">+ HCLS</span></td>
<td id="S4.T8.1.6.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T8.1.6.2.1" class="ltx_text" style="font-size:90%;">44.2</span></td>
<td id="S4.T8.1.6.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T8.1.6.3.1" class="ltx_text" style="font-size:90%;">17.2</span></td>
<td id="S4.T8.1.6.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T8.1.6.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">68.0</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 8: </span>Comparison on loss strategies. The metrics for COCO and MVD are mAP, and for OID, it is AP50. The best results are highlighted in <span id="S4.T8.6.1" class="ltx_text ltx_font_bold">bold font</span>.</figcaption>
</figure>
<figure id="S4.T9" class="ltx_table">
<table id="S4.T9.3" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T9.3.1.1" class="ltx_tr">
<td id="S4.T9.3.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T9.3.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.1.1.1.1.1" class="ltx_p" style="width:35.0pt;"><span id="S4.T9.3.1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Dataset</span></span>
</span>
</td>
<td id="S4.T9.3.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T9.3.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.1.1.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S4.T9.3.1.1.2.1.1.1" class="ltx_text" style="font-size:90%;">Backbone</span></span>
</span>
</td>
<td id="S4.T9.3.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T9.3.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.1.1.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S4.T9.3.1.1.3.1.1.1" class="ltx_text" style="font-size:90%;">Strategy</span></span>
</span>
</td>
<td id="S4.T9.3.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T9.3.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.1.1.4.1.1" class="ltx_p" style="width:62.0pt;"><span id="S4.T9.3.1.1.4.1.1.1" class="ltx_text" style="font-size:90%;">     mAP</span></span>
</span>
</td>
<td id="S4.T9.3.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T9.3.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.1.1.5.1.1" class="ltx_p" style="width:62.0pt;"><span id="S4.T9.3.1.1.5.1.1.1" class="ltx_text" style="font-size:90%;">    AP50</span></span>
</span>
</td>
<td id="S4.T9.3.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T9.3.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.1.1.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T9.3.1.1.6.1.1.1" class="ltx_text" style="font-size:90%;">epochs</span></span>
</span>
</td>
<td id="S4.T9.3.1.1.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T9.3.1.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.1.1.7.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T9.3.1.1.7.1.1.1" class="ltx_text" style="font-size:90%;">Time</span></span>
</span>
</td>
<td id="S4.T9.3.1.1.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T9.3.1.1.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.1.1.8.1.1" class="ltx_p" style="width:45.0pt;"><span id="S4.T9.3.1.1.8.1.1.1" class="ltx_text" style="font-size:90%;"> Memory</span></span>
</span>
</td>
</tr>
<tr id="S4.T9.3.2.2" class="ltx_tr">
<td id="S4.T9.3.2.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.2.2.1.1.1" class="ltx_p" style="width:35.0pt;"><span id="S4.T9.3.2.2.1.1.1.1" class="ltx_text" style="font-size:90%;"><span class="ltx_rule" style="width:0.0pt;height:12.0pt;background:black;display:inline-block;"></span>COCO</span></span>
</span>
</td>
<th id="S4.T9.3.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_t">
<span id="S4.T9.3.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.2.2.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S4.T9.3.2.2.2.1.1.1" class="ltx_text" style="font-size:90%;"><span id="S4.T9.3.2.2.2.1.1.1.1" class="ltx_text"></span> <span id="S4.T9.3.2.2.2.1.1.1.2" class="ltx_text">
<span id="S4.T9.3.2.2.2.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T9.3.2.2.2.1.1.1.2.1.1" class="ltx_tr">
<span id="S4.T9.3.2.2.2.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_rule" style="width:0.0pt;height:12.0pt;background:black;display:inline-block;"></span><span id="S4.T9.3.2.2.2.1.1.1.2.1.1.1.1" class="ltx_text" style="font-size:89%;">SEER-</span></span></span>
<span id="S4.T9.3.2.2.2.1.1.1.2.1.2" class="ltx_tr">
<span id="S4.T9.3.2.2.2.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T9.3.2.2.2.1.1.1.2.1.2.1.1" class="ltx_text" style="font-size:89%;">RegNet32gf</span></span></span>
</span></span> <span id="S4.T9.3.2.2.2.1.1.1.3" class="ltx_text"></span></span></span>
</span>
</th>
<td id="S4.T9.3.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.2.2.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S4.T9.3.2.2.3.1.1.1" class="ltx_text" style="font-size:90%;">finetune</span></span>
</span>
</td>
<td id="S4.T9.3.2.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.2.2.4.1.1" class="ltx_p" style="width:62.0pt;"><span id="S4.T9.3.2.2.4.1.1.1" class="ltx_text" style="font-size:90%;">     50.7</span></span>
</span>
</td>
<td id="S4.T9.3.2.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.2.2.5.1.1" class="ltx_p" style="width:62.0pt;"><span id="S4.T9.3.2.2.5.1.1.1" class="ltx_text" style="font-size:90%;">    68.8</span></span>
</span>
</td>
<td id="S4.T9.3.2.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.2.2.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T9.3.2.2.6.1.1.1" class="ltx_text" style="font-size:90%;">45</span></span>
</span>
</td>
<td id="S4.T9.3.2.2.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.2.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.2.2.7.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T9.3.2.2.7.1.1.1" class="ltx_text" style="font-size:90%;">167</span></span>
</span>
</td>
<td id="S4.T9.3.2.2.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.2.2.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.2.2.8.1.1" class="ltx_p" style="width:45.0pt;"><span id="S4.T9.3.2.2.8.1.1.1" class="ltx_text" style="font-size:90%;">    16</span></span>
</span>
</td>
</tr>
<tr id="S4.T9.3.3.3" class="ltx_tr">
<td id="S4.T9.3.3.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.3.3.1.1.1" class="ltx_p" style="width:35.0pt;"></span>
</span>
</td>
<td id="S4.T9.3.3.3.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.3.3.2.1.1" class="ltx_p" style="width:50.0pt;"></span>
</span>
</td>
<td id="S4.T9.3.3.3.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.3.3.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S4.T9.3.3.3.3.1.1.1" class="ltx_text" style="font-size:90%;">freeze</span></span>
</span>
</td>
<td id="S4.T9.3.3.3.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.3.3.4.1.1" class="ltx_p" style="width:62.0pt;"><span id="S4.T9.3.3.3.4.1.1.1" class="ltx_text" style="font-size:90%;">     50.9 </span><span id="S4.T9.3.3.3.4.1.1.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+0.2)</span></span>
</span>
</td>
<td id="S4.T9.3.3.3.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.3.3.5.1.1" class="ltx_p" style="width:62.0pt;"><span id="S4.T9.3.3.3.5.1.1.1" class="ltx_text" style="font-size:90%;">    69.2 </span><span id="S4.T9.3.3.3.5.1.1.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+0.4)</span></span>
</span>
</td>
<td id="S4.T9.3.3.3.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.3.3.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T9.3.3.3.6.1.1.1" class="ltx_text" style="font-size:90%;">72</span></span>
</span>
</td>
<td id="S4.T9.3.3.3.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.3.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.3.3.7.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T9.3.3.3.7.1.1.1" class="ltx_text" style="font-size:90%;">160</span></span>
</span>
</td>
<td id="S4.T9.3.3.3.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.3.3.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.3.3.8.1.1" class="ltx_p" style="width:45.0pt;"><span id="S4.T9.3.3.3.8.1.1.1" class="ltx_text" style="font-size:90%;">    10 </span><span id="S4.T9.3.3.3.8.1.1.2" class="ltx_text" style="font-size:90%;color:#26CC4D;">(–6)</span></span>
</span>
</td>
</tr>
<tr id="S4.T9.3.4.4" class="ltx_tr">
<td id="S4.T9.3.4.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.4.4.1.1.1" class="ltx_p" style="width:35.0pt;"></span>
</span>
</td>
<td id="S4.T9.3.4.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.4.4.2.1.1" class="ltx_p" style="width:50.0pt;"></span>
</span>
</td>
<td id="S4.T9.3.4.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.4.4.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S4.T9.3.4.4.3.1.1.1" class="ltx_text" style="font-size:90%;">finetune</span></span>
</span>
</td>
<td id="S4.T9.3.4.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.4.4.4.1.1" class="ltx_p" style="width:62.0pt;"><span id="S4.T9.3.4.4.4.1.1.1" class="ltx_text" style="font-size:90%;">     49.9</span></span>
</span>
</td>
<td id="S4.T9.3.4.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.4.4.5.1.1" class="ltx_p" style="width:62.0pt;"><span id="S4.T9.3.4.4.5.1.1.1" class="ltx_text" style="font-size:90%;">    68.3</span></span>
</span>
</td>
<td id="S4.T9.3.4.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.4.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.4.4.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T9.3.4.4.6.1.1.1" class="ltx_text" style="font-size:90%;">64</span></span>
</span>
</td>
<td id="S4.T9.3.4.4.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.4.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.4.4.7.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T9.3.4.4.7.1.1.1" class="ltx_text" style="font-size:90%;">238</span></span>
</span>
</td>
<td id="S4.T9.3.4.4.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.4.4.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.4.4.8.1.1" class="ltx_p" style="width:45.0pt;"><span id="S4.T9.3.4.4.8.1.1.1" class="ltx_text" style="font-size:90%;">    16</span></span>
</span>
</td>
</tr>
<tr id="S4.T9.3.5.5" class="ltx_tr">
<td id="S4.T9.3.5.5.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.5.5.1.1.1" class="ltx_p" style="width:35.0pt;"></span>
</span>
</td>
<td id="S4.T9.3.5.5.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.5.5.2.1.1" class="ltx_p" style="width:50.0pt;"></span>
</span>
</td>
<td id="S4.T9.3.5.5.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.5.5.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S4.T9.3.5.5.3.1.1.1" class="ltx_text" style="font-size:90%;">freeze</span></span>
</span>
</td>
<td id="S4.T9.3.5.5.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.5.5.4.1.1" class="ltx_p" style="width:62.0pt;"><span id="S4.T9.3.5.5.4.1.1.1" class="ltx_text" style="font-size:90%;">     51.4 </span><span id="S4.T9.3.5.5.4.1.1.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+1.5)</span></span>
</span>
</td>
<td id="S4.T9.3.5.5.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.5.5.5.1.1" class="ltx_p" style="width:62.0pt;"><span id="S4.T9.3.5.5.5.1.1.1" class="ltx_text" style="font-size:90%;">    69.8 </span><span id="S4.T9.3.5.5.5.1.1.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+1.5)</span></span>
</span>
</td>
<td id="S4.T9.3.5.5.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.5.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.5.5.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T9.3.5.5.6.1.1.1" class="ltx_text" style="font-size:90%;">108</span></span>
</span>
</td>
<td id="S4.T9.3.5.5.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.5.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.5.5.7.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T9.3.5.5.7.1.1.1" class="ltx_text" style="font-size:90%;">240</span></span>
</span>
</td>
<td id="S4.T9.3.5.5.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.5.5.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.5.5.8.1.1" class="ltx_p" style="width:45.0pt;"><span id="S4.T9.3.5.5.8.1.1.1" class="ltx_text" style="font-size:90%;">    10 </span><span id="S4.T9.3.5.5.8.1.1.2" class="ltx_text" style="font-size:90%;color:#26CC4D;">(–6)</span></span>
</span>
</td>
</tr>
<tr id="S4.T9.3.6.6" class="ltx_tr">
<td id="S4.T9.3.6.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.6.6.1.1.1" class="ltx_p" style="width:35.0pt;"><span id="S4.T9.3.6.6.1.1.1.1" class="ltx_text" style="font-size:90%;"><span class="ltx_rule" style="width:0.0pt;height:24.0pt;background:black;display:inline-block;"></span>MVD</span></span>
</span>
</td>
<th id="S4.T9.3.6.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_t">
<span id="S4.T9.3.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.6.6.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S4.T9.3.6.6.2.1.1.1" class="ltx_text" style="font-size:90%;"><span id="S4.T9.3.6.6.2.1.1.1.1" class="ltx_text"></span> <span id="S4.T9.3.6.6.2.1.1.1.2" class="ltx_text">
<span id="S4.T9.3.6.6.2.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T9.3.6.6.2.1.1.1.2.1.1" class="ltx_tr">
<span id="S4.T9.3.6.6.2.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_rule" style="width:0.0pt;height:12.0pt;background:black;display:inline-block;"></span><span id="S4.T9.3.6.6.2.1.1.1.2.1.1.1.1" class="ltx_text" style="font-size:89%;">SEER-</span></span></span>
<span id="S4.T9.3.6.6.2.1.1.1.2.1.2" class="ltx_tr">
<span id="S4.T9.3.6.6.2.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T9.3.6.6.2.1.1.1.2.1.2.1.1" class="ltx_text" style="font-size:89%;">RegNet32gf</span></span></span>
</span></span> <span id="S4.T9.3.6.6.2.1.1.1.3" class="ltx_text"></span></span></span>
</span>
</th>
<td id="S4.T9.3.6.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.6.6.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S4.T9.3.6.6.3.1.1.1" class="ltx_text" style="font-size:90%;">finetune</span></span>
</span>
</td>
<td id="S4.T9.3.6.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.6.6.4.1.1" class="ltx_p" style="width:62.0pt;"><span id="S4.T9.3.6.6.4.1.1.1" class="ltx_text" style="font-size:90%;">     23.9</span></span>
</span>
</td>
<td id="S4.T9.3.6.6.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.6.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.6.6.5.1.1" class="ltx_p" style="width:62.0pt;"><span id="S4.T9.3.6.6.5.1.1.1" class="ltx_text" style="font-size:90%;">    37.7</span></span>
</span>
</td>
<td id="S4.T9.3.6.6.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.6.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.6.6.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T9.3.6.6.6.1.1.1" class="ltx_text" style="font-size:90%;">48</span></span>
</span>
</td>
<td id="S4.T9.3.6.6.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.6.6.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.6.6.7.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T9.3.6.6.7.1.1.1" class="ltx_text" style="font-size:90%;">30</span></span>
</span>
</td>
<td id="S4.T9.3.6.6.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.6.6.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.6.6.8.1.1" class="ltx_p" style="width:45.0pt;"><span id="S4.T9.3.6.6.8.1.1.1" class="ltx_text" style="font-size:90%;">    16</span></span>
</span>
</td>
</tr>
<tr id="S4.T9.3.7.7" class="ltx_tr">
<td id="S4.T9.3.7.7.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.7.7.1.1.1" class="ltx_p" style="width:35.0pt;"></span>
</span>
</td>
<td id="S4.T9.3.7.7.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.7.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.7.7.2.1.1" class="ltx_p" style="width:50.0pt;"></span>
</span>
</td>
<td id="S4.T9.3.7.7.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.7.7.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S4.T9.3.7.7.3.1.1.1" class="ltx_text" style="font-size:90%;">freeze</span></span>
</span>
</td>
<td id="S4.T9.3.7.7.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.7.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.7.7.4.1.1" class="ltx_p" style="width:62.0pt;"><span id="S4.T9.3.7.7.4.1.1.1" class="ltx_text" style="font-size:90%;">     24.1 </span><span id="S4.T9.3.7.7.4.1.1.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+0.2)</span></span>
</span>
</td>
<td id="S4.T9.3.7.7.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.7.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.7.7.5.1.1" class="ltx_p" style="width:62.0pt;"><span id="S4.T9.3.7.7.5.1.1.1" class="ltx_text" style="font-size:90%;">    38.0 </span><span id="S4.T9.3.7.7.5.1.1.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+0.3)</span></span>
</span>
</td>
<td id="S4.T9.3.7.7.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.7.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.7.7.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T9.3.7.7.6.1.1.1" class="ltx_text" style="font-size:90%;">80</span></span>
</span>
</td>
<td id="S4.T9.3.7.7.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.7.7.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.7.7.7.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T9.3.7.7.7.1.1.1" class="ltx_text" style="font-size:90%;">30</span></span>
</span>
</td>
<td id="S4.T9.3.7.7.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.7.7.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.7.7.8.1.1" class="ltx_p" style="width:45.0pt;"><span id="S4.T9.3.7.7.8.1.1.1" class="ltx_text" style="font-size:90%;">    10 </span><span id="S4.T9.3.7.7.8.1.1.2" class="ltx_text" style="font-size:90%;color:#26CC4D;">(–6)</span></span>
</span>
</td>
</tr>
<tr id="S4.T9.3.8.8" class="ltx_tr">
<td id="S4.T9.3.8.8.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.8.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.8.8.1.1.1" class="ltx_p" style="width:35.0pt;"></span>
</span>
</td>
<td id="S4.T9.3.8.8.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.8.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.8.8.2.1.1" class="ltx_p" style="width:50.0pt;"></span>
</span>
</td>
<td id="S4.T9.3.8.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.8.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.8.8.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S4.T9.3.8.8.3.1.1.1" class="ltx_text" style="font-size:90%;">finetune</span></span>
</span>
</td>
<td id="S4.T9.3.8.8.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.8.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.8.8.4.1.1" class="ltx_p" style="width:62.0pt;"><span id="S4.T9.3.8.8.4.1.1.1" class="ltx_text" style="font-size:90%;">     24.4</span></span>
</span>
</td>
<td id="S4.T9.3.8.8.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.8.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.8.8.5.1.1" class="ltx_p" style="width:62.0pt;"><span id="S4.T9.3.8.8.5.1.1.1" class="ltx_text" style="font-size:90%;">    38.2</span></span>
</span>
</td>
<td id="S4.T9.3.8.8.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.8.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.8.8.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T9.3.8.8.6.1.1.1" class="ltx_text" style="font-size:90%;">100</span></span>
</span>
</td>
<td id="S4.T9.3.8.8.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.8.8.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.8.8.7.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T9.3.8.8.7.1.1.1" class="ltx_text" style="font-size:90%;">61</span></span>
</span>
</td>
<td id="S4.T9.3.8.8.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.8.8.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.8.8.8.1.1" class="ltx_p" style="width:45.0pt;"><span id="S4.T9.3.8.8.8.1.1.1" class="ltx_text" style="font-size:90%;">    16</span></span>
</span>
</td>
</tr>
<tr id="S4.T9.3.9.9" class="ltx_tr">
<td id="S4.T9.3.9.9.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.9.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.9.9.1.1.1" class="ltx_p" style="width:35.0pt;"></span>
</span>
</td>
<td id="S4.T9.3.9.9.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.9.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.9.9.2.1.1" class="ltx_p" style="width:50.0pt;"></span>
</span>
</td>
<td id="S4.T9.3.9.9.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.9.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.9.9.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S4.T9.3.9.9.3.1.1.1" class="ltx_text" style="font-size:90%;">freeze</span></span>
</span>
</td>
<td id="S4.T9.3.9.9.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.9.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.9.9.4.1.1" class="ltx_p" style="width:62.0pt;"><span id="S4.T9.3.9.9.4.1.1.1" class="ltx_text" style="font-size:90%;">     24.8 </span><span id="S4.T9.3.9.9.4.1.1.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+0.4)</span></span>
</span>
</td>
<td id="S4.T9.3.9.9.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.9.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.9.9.5.1.1" class="ltx_p" style="width:62.0pt;"><span id="S4.T9.3.9.9.5.1.1.1" class="ltx_text" style="font-size:90%;">    39.1 </span><span id="S4.T9.3.9.9.5.1.1.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+0.9)</span></span>
</span>
</td>
<td id="S4.T9.3.9.9.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.9.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.9.9.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T9.3.9.9.6.1.1.1" class="ltx_text" style="font-size:90%;">160</span></span>
</span>
</td>
<td id="S4.T9.3.9.9.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.9.9.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.9.9.7.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T9.3.9.9.7.1.1.1" class="ltx_text" style="font-size:90%;">60</span></span>
</span>
</td>
<td id="S4.T9.3.9.9.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.9.9.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.9.9.8.1.1" class="ltx_p" style="width:45.0pt;"><span id="S4.T9.3.9.9.8.1.1.1" class="ltx_text" style="font-size:90%;">    10 </span><span id="S4.T9.3.9.9.8.1.1.2" class="ltx_text" style="font-size:90%;color:#26CC4D;">(–6)</span></span>
</span>
</td>
</tr>
<tr id="S4.T9.3.10.10" class="ltx_tr">
<td id="S4.T9.3.10.10.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T9.3.10.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.10.10.1.1.1" class="ltx_p" style="width:35.0pt;"></span>
</span>
</td>
<th id="S4.T9.3.10.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_th_row">
<span id="S4.T9.3.10.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.10.10.2.1.1" class="ltx_p" style="width:50.0pt;"><span id="S4.T9.3.10.10.2.1.1.1" class="ltx_text" style="font-size:90%;"><span id="S4.T9.3.10.10.2.1.1.1.1" class="ltx_text"></span> <span id="S4.T9.3.10.10.2.1.1.1.2" class="ltx_text">
<span id="S4.T9.3.10.10.2.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T9.3.10.10.2.1.1.1.2.1.1" class="ltx_tr">
<span id="S4.T9.3.10.10.2.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_rule" style="width:0.0pt;height:2.0pt;background:black;display:inline-block;"></span><span id="S4.T9.3.10.10.2.1.1.1.2.1.1.1.1" class="ltx_text" style="font-size:89%;">SEER-</span></span></span>
<span id="S4.T9.3.10.10.2.1.1.1.2.1.2" class="ltx_tr">
<span id="S4.T9.3.10.10.2.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T9.3.10.10.2.1.1.1.2.1.2.1.1" class="ltx_text" style="font-size:89%;">RegNet256gf</span></span></span>
</span></span> <span id="S4.T9.3.10.10.2.1.1.1.3" class="ltx_text"></span></span></span>
</span>
</th>
<td id="S4.T9.3.10.10.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.10.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.10.10.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S4.T9.3.10.10.3.1.1.1" class="ltx_text" style="font-size:90%;">finetune</span></span>
</span>
</td>
<td id="S4.T9.3.10.10.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.10.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.10.10.4.1.1" class="ltx_p" style="width:62.0pt;"><span id="S4.T9.3.10.10.4.1.1.1" class="ltx_text" style="font-size:90%;">     25.8</span></span>
</span>
</td>
<td id="S4.T9.3.10.10.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.10.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.10.10.5.1.1" class="ltx_p" style="width:62.0pt;"><span id="S4.T9.3.10.10.5.1.1.1" class="ltx_text" style="font-size:90%;">    40.4</span></span>
</span>
</td>
<td id="S4.T9.3.10.10.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.10.10.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.10.10.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T9.3.10.10.6.1.1.1" class="ltx_text" style="font-size:90%;">60</span></span>
</span>
</td>
<td id="S4.T9.3.10.10.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.10.10.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.10.10.7.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T9.3.10.10.7.1.1.1" class="ltx_text" style="font-size:90%;">70</span></span>
</span>
</td>
<td id="S4.T9.3.10.10.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T9.3.10.10.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.10.10.8.1.1" class="ltx_p" style="width:45.0pt;"><span id="S4.T9.3.10.10.8.1.1.1" class="ltx_text" style="font-size:90%;">    60</span></span>
</span>
</td>
</tr>
<tr id="S4.T9.3.11.11" class="ltx_tr">
<td id="S4.T9.3.11.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T9.3.11.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.11.11.1.1.1" class="ltx_p" style="width:35.0pt;"></span>
</span>
</td>
<td id="S4.T9.3.11.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T9.3.11.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.11.11.2.1.1" class="ltx_p" style="width:50.0pt;"></span>
</span>
</td>
<td id="S4.T9.3.11.11.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T9.3.11.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.11.11.3.1.1" class="ltx_p" style="width:45.0pt;"><span id="S4.T9.3.11.11.3.1.1.1" class="ltx_text" style="font-size:90%;">freeze</span></span>
</span>
</td>
<td id="S4.T9.3.11.11.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T9.3.11.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.11.11.4.1.1" class="ltx_p" style="width:62.0pt;"><span id="S4.T9.3.11.11.4.1.1.1" class="ltx_text" style="font-size:90%;">     26.0 </span><span id="S4.T9.3.11.11.4.1.1.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+0.2)</span></span>
</span>
</td>
<td id="S4.T9.3.11.11.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T9.3.11.11.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.11.11.5.1.1" class="ltx_p" style="width:62.0pt;"><span id="S4.T9.3.11.11.5.1.1.1" class="ltx_text" style="font-size:90%;">    40.9 </span><span id="S4.T9.3.11.11.5.1.1.2" class="ltx_text" style="font-size:90%;color:#00FFFF;">(+0.5)</span></span>
</span>
</td>
<td id="S4.T9.3.11.11.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T9.3.11.11.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.11.11.6.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T9.3.11.11.6.1.1.1" class="ltx_text" style="font-size:90%;">120</span></span>
</span>
</td>
<td id="S4.T9.3.11.11.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T9.3.11.11.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.11.11.7.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T9.3.11.11.7.1.1.1" class="ltx_text" style="font-size:90%;">72</span></span>
</span>
</td>
<td id="S4.T9.3.11.11.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T9.3.11.11.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.3.11.11.8.1.1" class="ltx_p" style="width:45.0pt;"><span id="S4.T9.3.11.11.8.1.1.1" class="ltx_text" style="font-size:90%;">    15 </span><span id="S4.T9.3.11.11.8.1.1.2" class="ltx_text" style="font-size:90%;color:#26CC4D;">(–45)</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 9: </span>The object detectors are Cascade R-CNN enhanced with NAS-FPN (<math id="S4.T9.2.m1.1" class="ltx_Math" alttext="\times 7" display="inline"><semantics id="S4.T9.2.m1.1b"><mrow id="S4.T9.2.m1.1.1" xref="S4.T9.2.m1.1.1.cmml"><mi id="S4.T9.2.m1.1.1.2" xref="S4.T9.2.m1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S4.T9.2.m1.1.1.1" xref="S4.T9.2.m1.1.1.1.cmml">×</mo><mn id="S4.T9.2.m1.1.1.3" xref="S4.T9.2.m1.1.1.3.cmml">7</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T9.2.m1.1c"><apply id="S4.T9.2.m1.1.1.cmml" xref="S4.T9.2.m1.1.1"><times id="S4.T9.2.m1.1.1.1.cmml" xref="S4.T9.2.m1.1.1.1"></times><csymbol cd="latexml" id="S4.T9.2.m1.1.1.2.cmml" xref="S4.T9.2.m1.1.1.2">absent</csymbol><cn type="integer" id="S4.T9.2.m1.1.1.3.cmml" xref="S4.T9.2.m1.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.2.m1.1d">\times 7</annotation></semantics></math>) and Cascade RPN. The training time is measured in hours and the GPU memory consumption is measured in GB / image. The models are trained with a batch size of 16 on 16 NVIDIA 3090 / A100 GPUs. For comparison purpose, we convert the training time on different devices to the training time on 8 NVIDIA 3090 GPUs uniformly.</figcaption>
</figure>
</section>
<section id="S4.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.3 </span>Training Strategies</h4>

<div id="S4.SS4.SSS3.p1" class="ltx_para">
<p id="S4.SS4.SSS3.p1.1" class="ltx_p">Table <a href="#S4.T9" title="Table 9 ‣ 4.4.2 Hierarchical Loss Strategies ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> provides a comparison between two training approaches: finetuning the entire object detector, denoted as <em id="S4.SS4.SSS3.p1.1.1" class="ltx_emph ltx_font_italic">finetune</em>, and the approach used in this paper, where the backbone is frozen during training, denoted as <em id="S4.SS4.SSS3.p1.1.2" class="ltx_emph ltx_font_italic">freeze</em>. We evaluate these strategies by training models on either COCO or MVD, using both the lighter SEER backbone and the larger one. We assess their performance and GPU memory consumption.</p>
</div>
<div id="S4.SS4.SSS3.p2" class="ltx_para">
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">-</span> 
<div id="S4.I2.ix1.p1" class="ltx_para">
<p id="S4.I2.ix1.p1.1" class="ltx_p"><span id="S4.I2.ix1.p1.1.1" class="ltx_text ltx_font_bold">Performance</span></p>
</div>
<div id="S4.I2.ix1.p2" class="ltx_para">
<p id="S4.I2.ix1.p2.1" class="ltx_p">The results of <em id="S4.I2.ix1.p2.1.1" class="ltx_emph ltx_font_italic">finetune</em> and <em id="S4.I2.ix1.p2.1.2" class="ltx_emph ltx_font_italic">freeze</em> are summarized in Table <a href="#S4.T9" title="Table 9 ‣ 4.4.2 Hierarchical Loss Strategies ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. It’s evident that <em id="S4.I2.ix1.p2.1.3" class="ltx_emph ltx_font_italic">freeze</em> consistently enhances performance across datasets, particularly with an extended training schedule. In contrast, <em id="S4.I2.ix1.p2.1.4" class="ltx_emph ltx_font_italic">finetune</em> exhibits a decline in performance with a longer training schedule. This decline might be attributed to the backbone representations deviating from the original SEER visual representations and over-fitting on the smaller downstream dataset, thereby diminishing the object detector’s performance.</p>
</div>
</li>
<li id="S4.I2.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">-</span> 
<div id="S4.I2.ix2.p1" class="ltx_para">
<p id="S4.I2.ix2.p1.1" class="ltx_p"><span id="S4.I2.ix2.p1.1.1" class="ltx_text ltx_font_bold">GPU memory consumption</span></p>
</div>
<div id="S4.I2.ix2.p2" class="ltx_para">
<p id="S4.I2.ix2.p2.1" class="ltx_p">As evident from Table <a href="#S4.T9" title="Table 9 ‣ 4.4.2 Hierarchical Loss Strategies ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, freezing the backbone during training imposes a substantially lower GPU memory demand compared to finetuning the entire object detector, including the backbone. Specifically, the SEER-RegNet32gf-based model requires 10 GB of GPU memory per image during training with the <em id="S4.I2.ix2.p2.1.1" class="ltx_emph ltx_font_italic">freeze</em> strategy, whereas the <em id="S4.I2.ix2.p2.1.2" class="ltx_emph ltx_font_italic">finetune</em> strategy demands 16 GB of GPU memory per image. Similarly, the SEER-RegNet256gf-based model requires 15 GB of GPU memory per image with the <em id="S4.I2.ix2.p2.1.3" class="ltx_emph ltx_font_italic">freeze</em> strategy, while the <em id="S4.I2.ix2.p2.1.4" class="ltx_emph ltx_font_italic">finetune</em> strategy necessitates a much higher 60 GB of GPU memory for each image. For this reason, freezing the backbone emerges as a more feasible option for training models on memory-constrained computational resources, such as NVIDIA 3090.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS4.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.4 </span>Impact of Universal Paradigm</h4>

<div id="S4.SS4.SSS4.p1" class="ltx_para">
<p id="S4.SS4.SSS4.p1.1" class="ltx_p">To investigate the impact of the universal training paradigm, where the object detector learns from multi-domain datasets with a unified label spaces simultaneously, we conducted an ablation analysis.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>The used object detectors are built upon Cascade R-CNN enhanced with NAS-FPN (×7) and Cascade RPN, utilizing SEER-RegNet32gf as the backbone. As detailed in Section <a href="#S4.SS2" title="4.2 Implementation Details ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>, the universal object detector undergoes training for 1.15M iterations with a batch size of 16. Considering the re-sampled training set size of approximately 2.3M, the calculated training epochs amount to 8. Consequently, for Individual OD on OID, the training epochs are set to 8. On a comparable scale, for Individual OD on COCO, the object detector is trained for 12 epochs in line with the default configuration in the mmdetection codebase. Lastly, for Individual OD on MVD, the object detector is also trained for 12 epochs, initializing its network parameters with the object detector mentioned earlier for COCO.</span></span></span>
Table <a href="#S4.T10" title="Table 10 ‣ 4.4.4 Impact of Universal Paradigm ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> compares the results achieved by the proposed universal object detector (Universal OD) with object detectors trained individually on their respective datasets (Individual OD).</p>
</div>
<div id="S4.SS4.SSS4.p2" class="ltx_para">
<p id="S4.SS4.SSS4.p2.1" class="ltx_p">As observed in Table <a href="#S4.T10" title="Table 10 ‣ 4.4.4 Impact of Universal Paradigm ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, the universal object detector consistently achieves superior results on all the three datasets. These findings suggest that universal object detection could potentially benefit from cross-domain annotations, which might help the detector in capturing more distinct visual concepts from one domain and applying them effectively in another.</p>
</div>
<figure id="S4.T10" class="ltx_table">
<table id="S4.T10.3" class="ltx_tabular ltx_align_middle">
<tr id="S4.T10.3.4" class="ltx_tr">
<td id="S4.T10.3.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T10.3.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T10.3.4.1.1.1" class="ltx_p" style="width:70.0pt;"><span id="S4.T10.3.4.1.1.1.1" class="ltx_text" style="font-size:90%;">Paradigm</span></span>
</span>
</td>
<td id="S4.T10.3.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T10.3.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T10.3.4.2.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T10.3.4.2.1.1.1" class="ltx_text" style="font-size:90%;">COCO</span></span>
</span>
</td>
<td id="S4.T10.3.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T10.3.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T10.3.4.3.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T10.3.4.3.1.1.1" class="ltx_text" style="font-size:90%;">MVD</span></span>
</span>
</td>
<td id="S4.T10.3.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T10.3.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T10.3.4.4.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T10.3.4.4.1.1.1" class="ltx_text" style="font-size:90%;">OID</span></span>
</span>
</td>
</tr>
<tr id="S4.T10.3.5" class="ltx_tr">
<td id="S4.T10.3.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T10.3.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T10.3.5.1.1.1" class="ltx_p" style="width:70.0pt;"><span id="S4.T10.3.5.1.1.1.1" class="ltx_text" style="font-size:90%;">Universal OD</span></span>
</span>
</td>
<td id="S4.T10.3.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T10.3.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T10.3.5.2.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T10.3.5.2.1.1.1" class="ltx_text" style="font-size:90%;">48.8</span></span>
</span>
</td>
<td id="S4.T10.3.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T10.3.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T10.3.5.3.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T10.3.5.3.1.1.1" class="ltx_text" style="font-size:90%;">21.1</span></span>
</span>
</td>
<td id="S4.T10.3.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T10.3.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T10.3.5.4.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T10.3.5.4.1.1.1" class="ltx_text" style="font-size:90%;">68.5</span></span>
</span>
</td>
</tr>
<tr id="S4.T10.3.3" class="ltx_tr">
<td id="S4.T10.3.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T10.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T10.3.3.4.1.1" class="ltx_p" style="width:70.0pt;"><span id="S4.T10.3.3.4.1.1.1" class="ltx_text" style="font-size:90%;">Individual OD</span></span>
</span>
</td>
<td id="S4.T10.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T10.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T10.1.1.1.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T10.1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">   47.6 </span><math id="S4.T10.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T10.1.1.1.1.1.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T10.1.1.1.1.1.m1.1.1" xref="S4.T10.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T10.1.1.1.1.1.m1.1b"><ci id="S4.T10.1.1.1.1.1.m1.1.1.cmml" xref="S4.T10.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T10.1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span>
</span>
</td>
<td id="S4.T10.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T10.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T10.2.2.2.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T10.2.2.2.1.1.1" class="ltx_text" style="font-size:90%;">   20.2 </span><math id="S4.T10.2.2.2.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T10.2.2.2.1.1.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T10.2.2.2.1.1.m1.1.1" xref="S4.T10.2.2.2.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T10.2.2.2.1.1.m1.1b"><ci id="S4.T10.2.2.2.1.1.m1.1.1.cmml" xref="S4.T10.2.2.2.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T10.2.2.2.1.1.m1.1c">\downarrow</annotation></semantics></math></span>
</span>
</td>
<td id="S4.T10.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T10.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T10.3.3.3.1.1" class="ltx_p" style="width:30.0pt;"><span id="S4.T10.3.3.3.1.1.1" class="ltx_text" style="font-size:90%;">   68.2 </span><math id="S4.T10.3.3.3.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T10.3.3.3.1.1.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T10.3.3.3.1.1.m1.1.1" xref="S4.T10.3.3.3.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T10.3.3.3.1.1.m1.1b"><ci id="S4.T10.3.3.3.1.1.m1.1.1.cmml" xref="S4.T10.3.3.3.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T10.3.3.3.1.1.m1.1c">\downarrow</annotation></semantics></math></span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 10: </span>Comparison of two training paradigms: universal object detection and individual object detection, using mAP for COCO and MVD, and AP50 for OID.</figcaption>
</figure>
</section>
<section id="S4.SS4.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.5 </span>Scaling Up and Finetuning</h4>

<div id="S4.SS4.SSS5.p1" class="ltx_para">
<p id="S4.SS4.SSS5.p1.1" class="ltx_p">• <em id="S4.SS4.SSS5.p1.1.1" class="ltx_emph ltx_font_italic">Scaling up during the inference procedure</em></p>
</div>
<div id="S4.SS4.SSS5.p2" class="ltx_para">
<p id="S4.SS4.SSS5.p2.1" class="ltx_p">Considering the larger size of MVD images and the prevalent occurrence of relatively small instances within them, a common scenario in computer vision, we adopted a strategic approach. As elaborated in Appendix <a href="#A1" title="Appendix A RVC Submission ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>, during the inference phase, we resized the short edges of MVD images to 2048 pixels. This scaling was implemented to enhance detection performance, particularly for the multitude of small objects present in high-resolution images. This scaling enhancement yielded an approximate 3-point increase in mAP on MVD within our RVC submission, yielding substantial benefits for the models showcased in this paper as well.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><svg id="S4.F4.pic1" class="ltx_picture ltx_centering" height="246.31" overflow="visible" version="1.1" width="292.44"><g transform="translate(0,246.31) matrix(1 0 0 -1 0 0) translate(42.29,0) translate(0,42.91) matrix(1.0 0.0 0.0 1.0 -42.29 -42.91)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(61.11,0) translate(0,56.47)"><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M 30.15 -13.56 L 30.15 -7.66 M 105.51 -13.56 L 105.51 -7.66 M 180.88 -13.56 L 180.88 -7.66 M 30.15 149.16 L 30.15 143.26 M 105.51 149.16 L 105.51 143.26 M 180.88 149.16 L 180.88 143.26" style="fill:none"></path></g><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M -18.81 18.49 L -12.91 18.49 M -18.81 59.58 L -12.91 59.58 M -18.81 100.67 L -12.91 100.67 M -18.81 141.76 L -12.91 141.76 M 206.92 18.49 L 201.02 18.49 M 206.92 59.58 L 201.02 59.58 M 206.92 100.67 L 201.02 100.67 M 206.92 141.76 L 201.02 141.76" style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="0.4pt"><path d="M -18.81 -13.56 L -18.81 149.16 L 206.92 149.16 L 206.92 -13.56 L -18.81 -13.56 Z" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 14.39 -27.37)" fill="#000000" stroke="#000000"><foreignObject width="31.52" height="11.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.2" class="ltx_Math" alttext="1{,}000" display="inline"><semantics id="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.2a"><mrow id="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.2.3.2" xref="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.2.3.1.cmml"><mn id="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">1</mn><mo id="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.2.3.2.1" xref="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.2.3.1.cmml">,</mo><mn id="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.2.2" xref="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.2b"><list id="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.2.3.1.cmml" xref="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.2.3.2"><cn type="integer" id="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">1</cn><cn type="integer" id="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.2.2.cmml" xref="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.2c">1{,}000</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 89.75 -27.37)" fill="#000000" stroke="#000000"><foreignObject width="31.52" height="11.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.2" class="ltx_Math" alttext="1{,}500" display="inline"><semantics id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.2a"><mrow id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.2.3.2" xref="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.2.3.1.cmml"><mn id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">1</mn><mo id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.2.3.2.1" xref="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.2.3.1.cmml">,</mo><mn id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.2.2" xref="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.2.2.cmml">500</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.2b"><list id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.2.3.1.cmml" xref="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.2.3.2"><cn type="integer" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1">1</cn><cn type="integer" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.2.2.cmml" xref="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.2.2">500</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.2c">1{,}500</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 165.12 -27.37)" fill="#000000" stroke="#000000"><foreignObject width="31.52" height="11.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.2" class="ltx_Math" alttext="2{,}000" display="inline"><semantics id="S4.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.2a"><mrow id="S4.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.2.3.2" xref="S4.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.2.3.1.cmml"><mn id="S4.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">2</mn><mo id="S4.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.2.3.2.1" xref="S4.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.2.3.1.cmml">,</mo><mn id="S4.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.2.2" xref="S4.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.2b"><list id="S4.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.2.3.1.cmml" xref="S4.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.2.3.2"><cn type="integer" id="S4.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1">2</cn><cn type="integer" id="S4.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.2.2.cmml" xref="S4.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.2c">2{,}000</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -37.54 14.03)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="22" display="inline"><semantics id="S4.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">22</mn><annotation-xml encoding="MathML-Content" id="S4.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1">22</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1c">22</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -37.54 55.12)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F4.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="24" display="inline"><semantics id="S4.F4.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F4.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F4.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">24</mn><annotation-xml encoding="MathML-Content" id="S4.F4.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F4.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F4.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1">24</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1c">24</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -37.54 96.22)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F4.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="26" display="inline"><semantics id="S4.F4.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F4.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F4.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">26</mn><annotation-xml encoding="MathML-Content" id="S4.F4.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F4.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F4.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1">26</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1c">26</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -37.54 137.31)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F4.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="28" display="inline"><semantics id="S4.F4.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F4.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F4.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">28</mn><annotation-xml encoding="MathML-Content" id="S4.F4.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F4.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F4.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1">28</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1c">28</annotation></semantics></math></foreignObject></g><clipPath id="pgfcp1"><path d="M -18.81 -13.56 L 206.92 -13.56 L 206.92 149.16 L -18.81 149.16 Z"></path></clipPath><g clip-path="url(#pgfcp1)"><g stroke="#FF8000" fill="#FF8000" color="#FF8000"><path d="M 0 0 L 24.12 47.25 L 60.29 80.13 L 90.44 94.51 L 120.58 98.62 L 150.73 71.91 L 188.11 49.31" style="fill:none"></path></g><g></g><g stroke="#00FFFF" fill="#00FFFF" color="#00FFFF"><path d="M 0 45.2 L 24.12 90.4 L 60.29 125.33 L 90.44 135.6 L 120.58 127.38 L 150.73 115.06 L 188.11 82.18" style="fill:none"></path></g><g></g></g><g stroke="#FF8000" fill="#0000CC" color="#FF8000"><path d="M -2.77 -2.77 h 5.53 v 5.53 h -5.53 Z" style="fill:none"></path><path d="M 21.35 44.49 h 5.53 v 5.53 h -5.53 Z" style="fill:none"></path><path d="M 57.52 77.36 h 5.53 v 5.53 h -5.53 Z" style="fill:none"></path><path d="M 87.67 91.74 h 5.53 v 5.53 h -5.53 Z" style="fill:none"></path><path d="M 117.82 95.85 h 5.53 v 5.53 h -5.53 Z" style="fill:none"></path><path d="M 147.96 69.14 h 5.53 v 5.53 h -5.53 Z" style="fill:none"></path><path d="M 185.34 46.54 h 5.53 v 5.53 h -5.53 Z" style="fill:none"></path></g><g stroke="#00FFFF" fill="#00FFFF" color="#00FFFF"><path d="M 2.77 45.2 C 2.77 46.73 1.53 47.97 0 47.97 C -1.53 47.97 -2.77 46.73 -2.77 45.2 C -2.77 43.67 -1.53 42.43 0 42.43 C 1.53 42.43 2.77 43.67 2.77 45.2 Z M 0 45.2"></path><path d="M 26.88 90.4 C 26.88 91.93 25.65 93.17 24.12 93.17 C 22.59 93.17 21.35 91.93 21.35 90.4 C 21.35 88.87 22.59 87.63 24.12 87.63 C 25.65 87.63 26.88 88.87 26.88 90.4 Z M 24.12 90.4"></path><path d="M 63.06 125.33 C 63.06 126.86 61.82 128.1 60.29 128.1 C 58.76 128.1 57.52 126.86 57.52 125.33 C 57.52 123.8 58.76 122.56 60.29 122.56 C 61.82 122.56 63.06 123.8 63.06 125.33 Z M 60.29 125.33"></path><path d="M 93.21 135.6 C 93.21 137.13 91.97 138.37 90.44 138.37 C 88.91 138.37 87.67 137.13 87.67 135.6 C 87.67 134.07 88.91 132.83 90.44 132.83 C 91.97 132.83 93.21 134.07 93.21 135.6 Z M 90.44 135.6"></path><path d="M 123.35 127.38 C 123.35 128.91 122.11 130.15 120.58 130.15 C 119.06 130.15 117.82 128.91 117.82 127.38 C 117.82 125.85 119.06 124.62 120.58 124.62 C 122.11 124.62 123.35 125.85 123.35 127.38 Z M 120.58 127.38"></path><path d="M 153.5 115.06 C 153.5 116.58 152.26 117.82 150.73 117.82 C 149.2 117.82 147.96 116.58 147.96 115.06 C 147.96 113.53 149.2 112.29 150.73 112.29 C 152.26 112.29 153.5 113.53 153.5 115.06 Z M 150.73 115.06"></path><path d="M 190.88 82.18 C 190.88 83.71 189.64 84.95 188.11 84.95 C 186.58 84.95 185.34 83.71 185.34 82.18 C 185.34 80.65 186.58 79.42 188.11 79.42 C 189.64 79.42 190.88 80.65 190.88 82.18 Z M 188.11 82.18"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 21.93 -49.17)" fill="#000000" stroke="#000000"><foreignObject width="144.25" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F4.pic1.8.8.8.8.8.1.1" class="ltx_text">length of the short edge</span></foreignObject></g><g transform="matrix(0.0 1.0 -1.0 0.0 -47.04 52.14)" fill="#000000" stroke="#000000"><foreignObject width="31.33" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F4.pic1.9.9.9.9.9.1.1" class="ltx_text">mAP</span></foreignObject></g><g fill="#FFFFFF" stroke="#000000"><path d="M -42.94 166.04 h 274 v 23.52 h -274 Z"></path></g><g fill="#FFFFFF" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -38.79 168.81)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 8.995)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 8.99)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(0.28,0)" fill="#0000FF" stroke="#FF8000" color="#FF8000"><g fill="#FF8000"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path></g><g fill="#0000CC"><path d="M 9.04 -2.77 h 5.53 v 5.53 h -5.53 Z" style="fill:none"></path></g></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 24.18 0) translate(54.1,0) matrix(1.0 0.0 0.0 1.0 -51.33 -4.15)" fill="#000000" stroke="#000000"><foreignObject width="102.66" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F4.pic1.10.10.10.10.10.1.1.1.1.1" class="ltx_text">Large-UniDet [S]</span></foreignObject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 132.37 0) translate(0.28,0)" fill="#00FFFF" stroke="#00FFFF" color="#00FFFF"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><path d="M 14.58 0 C 14.58 1.53 13.34 2.77 11.81 2.77 C 10.28 2.77 9.04 1.53 9.04 0 C 9.04 -1.53 10.28 -2.77 11.81 -2.77 C 13.34 -2.77 14.58 -1.53 14.58 0 Z M 11.81 0"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 156.54 0) translate(54.58,0) matrix(1.0 0.0 0.0 1.0 -51.81 -4.15)" fill="#000000" stroke="#000000"><foreignObject width="103.62" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F4.pic1.11.11.11.11.11.2.2.2.1.1" class="ltx_text">Large-UniDet [L]</span></foreignObject></g></g></g></g></g></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The performance on MVD <em id="S4.F4.2.1" class="ltx_emph ltx_font_italic">vs.</em> the lengths of the short edge of the testing images. We obtain the best mAP at 25.9 with the 1600-pixel resized short edge based on the lighter backbone (Large-UniDet [S]), and obtain best mAP at 27.7 with the 1400-pixel resized short edge based on the larger one (Large-UniDet [L]). The initial mAP and highest mAP of both two models are reported in Table <a href="#S4.T11" title="Table 11 ‣ 4.4.5 Scaling Up and Finetuning ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>.</figcaption>
</figure>
<div id="S4.SS4.SSS5.p3" class="ltx_para">
<p id="S4.SS4.SSS5.p3.1" class="ltx_p">As shown in Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.4.5 Scaling Up and Finetuning ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the models achieved optimal results when the testing image’s short edge was resized to 1600 pixels (for Large-UniDet[S]) or 1400 pixels (for Large-UniDet[L]), as demonstrated in Table <a href="#S4.T11" title="Table 11 ‣ 4.4.5 Scaling Up and Finetuning ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> alongside the 800-pixel short-edge baselines. The effect of scaling is particularly pronounced in MVD performance for both the small and large models, resulting in a significant mAP increase of +4.8 and +4.4, respectively.</p>
</div>
<div id="S4.SS4.SSS5.p4" class="ltx_para">
<p id="S4.SS4.SSS5.p4.1" class="ltx_p">It is worth highlighting that increasing the testing image’s size does not lead to performance enhancements always (<em id="S4.SS4.SSS5.p4.1.1" class="ltx_emph ltx_font_italic">e.g.</em> COCO or OID). This outcome arises from the fact that while scaling for inference aids in detecting smaller objects, it simultaneously compromises the performance for larger objects—a phenomenon previously documented in the literature <cite class="ltx_cite ltx_citemacro_citep">(Gao <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2018</a>)</cite>. Bearing this in mind, we hold the belief that tailoring the object detector for high-resolution images bears the potential to enhance the detection of large objects while simultaneously retaining the benefits for smaller objects.</p>
</div>
<figure id="S4.T11" class="ltx_table">
<table id="S4.T11.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T11.1.1" class="ltx_tr">
<td id="S4.T11.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T11.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.1.1.1.1" class="ltx_p" style="width:28.0pt;"><span id="S4.T11.1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Model</span></span>
</span>
</td>
<td id="S4.T11.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T11.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.1.2.1.1" class="ltx_p" style="width:16.0pt;"><span id="S4.T11.1.1.2.1.1.1" class="ltx_text" style="font-size:90%;">SU</span></span>
</span>
</td>
<td id="S4.T11.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T11.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.1.3.1.1" class="ltx_p" style="width:16.0pt;"><span id="S4.T11.1.1.3.1.1.1" class="ltx_text" style="font-size:90%;">FT</span></span>
</span>
</td>
<td id="S4.T11.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T11.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.1.4.1.1" class="ltx_p" style="width:28.0pt;"><span id="S4.T11.1.1.4.1.1.1" class="ltx_text" style="font-size:90%;">COCO</span></span>
</span>
</td>
<td id="S4.T11.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T11.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.1.5.1.1" class="ltx_p" style="width:26.0pt;"><span id="S4.T11.1.1.5.1.1.1" class="ltx_text" style="font-size:90%;">MVD</span></span>
</span>
</td>
<td id="S4.T11.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T11.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.1.6.1.1" class="ltx_p" style="width:26.0pt;"><span id="S4.T11.1.1.6.1.1.1" class="ltx_text" style="font-size:90%;">OID</span></span>
</span>
</td>
</tr>
<tr id="S4.T11.1.2" class="ltx_tr">
<td id="S4.T11.1.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T11.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.2.1.1.1" class="ltx_p" style="width:28.0pt;"><span id="S4.T11.1.2.1.1.1.1" class="ltx_text" style="font-size:90%;">[S]</span></span>
</span>
</td>
<td id="S4.T11.1.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T11.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.2.2.1.1" class="ltx_p" style="width:16.0pt;"></span>
</span>
</td>
<td id="S4.T11.1.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T11.1.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.2.3.1.1" class="ltx_p" style="width:16.0pt;"></span>
</span>
</td>
<td id="S4.T11.1.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T11.1.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.2.4.1.1" class="ltx_p" style="width:28.0pt;"><span id="S4.T11.1.2.4.1.1.1" class="ltx_text" style="font-size:90%;">48.8</span></span>
</span>
</td>
<td id="S4.T11.1.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T11.1.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.2.5.1.1" class="ltx_p" style="width:26.0pt;"><span id="S4.T11.1.2.5.1.1.1" class="ltx_text" style="font-size:90%;">21.1</span></span>
</span>
</td>
<td id="S4.T11.1.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T11.1.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.2.6.1.1" class="ltx_p" style="width:26.0pt;"><span id="S4.T11.1.2.6.1.1.1" class="ltx_text" style="font-size:90%;">68.5</span></span>
</span>
</td>
</tr>
<tr id="S4.T11.1.3" class="ltx_tr">
<td id="S4.T11.1.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T11.1.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.3.1.1.1" class="ltx_p" style="width:28.0pt;"></span>
</span>
</td>
<td id="S4.T11.1.3.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T11.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.3.2.1.1" class="ltx_p" style="width:16.0pt;"><span id="S4.T11.1.3.2.1.1.1" class="ltx_text" style="font-size:90%;">✓</span></span>
</span>
</td>
<td id="S4.T11.1.3.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T11.1.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.3.3.1.1" class="ltx_p" style="width:16.0pt;"></span>
</span>
</td>
<td id="S4.T11.1.3.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T11.1.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.3.4.1.1" class="ltx_p" style="width:28.0pt;"><span id="S4.T11.1.3.4.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T11.1.3.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T11.1.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.3.5.1.1" class="ltx_p" style="width:26.0pt;"><span id="S4.T11.1.3.5.1.1.1" class="ltx_text" style="font-size:90%;">25.9</span></span>
</span>
</td>
<td id="S4.T11.1.3.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T11.1.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.3.6.1.1" class="ltx_p" style="width:26.0pt;"><span id="S4.T11.1.3.6.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
</tr>
<tr id="S4.T11.1.4" class="ltx_tr">
<td id="S4.T11.1.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T11.1.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.4.1.1.1" class="ltx_p" style="width:28.0pt;"></span>
</span>
</td>
<td id="S4.T11.1.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T11.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.4.2.1.1" class="ltx_p" style="width:16.0pt;"><span id="S4.T11.1.4.2.1.1.1" class="ltx_text" style="font-size:90%;">✓</span></span>
</span>
</td>
<td id="S4.T11.1.4.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T11.1.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.4.3.1.1" class="ltx_p" style="width:16.0pt;"><span id="S4.T11.1.4.3.1.1.1" class="ltx_text" style="font-size:90%;">✓</span></span>
</span>
</td>
<td id="S4.T11.1.4.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T11.1.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.4.4.1.1" class="ltx_p" style="width:28.0pt;"><span id="S4.T11.1.4.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">52.0</span></span>
</span>
</td>
<td id="S4.T11.1.4.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T11.1.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.4.5.1.1" class="ltx_p" style="width:26.0pt;"><span id="S4.T11.1.4.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">32.0</span></span>
</span>
</td>
<td id="S4.T11.1.4.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T11.1.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.4.6.1.1" class="ltx_p" style="width:26.0pt;"><span id="S4.T11.1.4.6.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">69.2</span></span>
</span>
</td>
</tr>
<tr id="S4.T11.1.5" class="ltx_tr">
<td id="S4.T11.1.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T11.1.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.5.1.1.1" class="ltx_p" style="width:28.0pt;"><span id="S4.T11.1.5.1.1.1.1" class="ltx_text" style="font-size:90%;">[L]</span></span>
</span>
</td>
<td id="S4.T11.1.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T11.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.5.2.1.1" class="ltx_p" style="width:16.0pt;"></span>
</span>
</td>
<td id="S4.T11.1.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T11.1.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.5.3.1.1" class="ltx_p" style="width:16.0pt;"></span>
</span>
</td>
<td id="S4.T11.1.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T11.1.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.5.4.1.1" class="ltx_p" style="width:28.0pt;"><span id="S4.T11.1.5.4.1.1.1" class="ltx_text" style="font-size:90%;">51.9</span></span>
</span>
</td>
<td id="S4.T11.1.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T11.1.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.5.5.1.1" class="ltx_p" style="width:26.0pt;"><span id="S4.T11.1.5.5.1.1.1" class="ltx_text" style="font-size:90%;">23.3</span></span>
</span>
</td>
<td id="S4.T11.1.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T11.1.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.5.6.1.1" class="ltx_p" style="width:26.0pt;"><span id="S4.T11.1.5.6.1.1.1" class="ltx_text" style="font-size:90%;">69.8</span></span>
</span>
</td>
</tr>
<tr id="S4.T11.1.6" class="ltx_tr">
<td id="S4.T11.1.6.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T11.1.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.6.1.1.1" class="ltx_p" style="width:28.0pt;"></span>
</span>
</td>
<td id="S4.T11.1.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T11.1.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.6.2.1.1" class="ltx_p" style="width:16.0pt;"><span id="S4.T11.1.6.2.1.1.1" class="ltx_text" style="font-size:90%;">✓</span></span>
</span>
</td>
<td id="S4.T11.1.6.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T11.1.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.6.3.1.1" class="ltx_p" style="width:16.0pt;"></span>
</span>
</td>
<td id="S4.T11.1.6.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T11.1.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.6.4.1.1" class="ltx_p" style="width:28.0pt;"><span id="S4.T11.1.6.4.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S4.T11.1.6.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T11.1.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.6.5.1.1" class="ltx_p" style="width:26.0pt;"><span id="S4.T11.1.6.5.1.1.1" class="ltx_text" style="font-size:90%;">27.7</span></span>
</span>
</td>
<td id="S4.T11.1.6.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T11.1.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.6.6.1.1" class="ltx_p" style="width:26.0pt;"><span id="S4.T11.1.6.6.1.1.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
</tr>
<tr id="S4.T11.1.7" class="ltx_tr">
<td id="S4.T11.1.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T11.1.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.7.1.1.1" class="ltx_p" style="width:28.0pt;"></span>
</span>
</td>
<td id="S4.T11.1.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T11.1.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.7.2.1.1" class="ltx_p" style="width:16.0pt;"><span id="S4.T11.1.7.2.1.1.1" class="ltx_text" style="font-size:90%;">✓</span></span>
</span>
</td>
<td id="S4.T11.1.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T11.1.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.7.3.1.1" class="ltx_p" style="width:16.0pt;"><span id="S4.T11.1.7.3.1.1.1" class="ltx_text" style="font-size:90%;">✓</span></span>
</span>
</td>
<td id="S4.T11.1.7.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T11.1.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.7.4.1.1" class="ltx_p" style="width:28.0pt;"><span id="S4.T11.1.7.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">53.5</span></span>
</span>
</td>
<td id="S4.T11.1.7.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T11.1.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.7.5.1.1" class="ltx_p" style="width:26.0pt;"><span id="S4.T11.1.7.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">33.2</span></span>
</span>
</td>
<td id="S4.T11.1.7.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T11.1.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T11.1.7.6.1.1" class="ltx_p" style="width:26.0pt;"><span id="S4.T11.1.7.6.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">70.5</span></span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 11: </span>Comparison on the baseline, scaling up during the inference procedure (denoted as <span id="S4.T11.8.1" class="ltx_text ltx_font_bold">SU</span>), and fintuning with higher-resolution training images (denoted as <span id="S4.T11.9.2" class="ltx_text ltx_font_bold">FT</span>). The baseline is the universal object detection training without either scaling up the input size when testing or the following dataset-specific high-resolution finetuning. In the table, [S] and [L] represent Large-UniDet [S] and Large-UniDet [L], respectively. The metric of COCO and MVD is mAP and the metric of OID is AP50.</figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2212.09408/assets/x4.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="484" height="252" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Qualitative results on COCO <em id="S4.F5.4.1" class="ltx_emph ltx_font_italic">val</em> set. On the top left corner of each visualized bounding box, the predicted categories of which the corresponding confidence scores are greater than 0.01 are listed in the descending order of the confidence scores. The entry <em id="S4.F5.5.2" class="ltx_emph ltx_font_italic">classname_super</em> indicates this class is a superclass named <em id="S4.F5.6.3" class="ltx_emph ltx_font_italic">classname</em> in the unified label space.</figcaption>
</figure>
<div id="S4.SS4.SSS5.p5" class="ltx_para ltx_noindent">
<p id="S4.SS4.SSS5.p5.1" class="ltx_p">• <em id="S4.SS4.SSS5.p5.1.1" class="ltx_emph ltx_font_italic">Dataset-specific finetuning with scaling up</em></p>
</div>
<div id="S4.SS4.SSS5.p6" class="ltx_para">
<p id="S4.SS4.SSS5.p6.1" class="ltx_p">Employing an accelerated training approach involving high-resolution images subsequent to low-resolution pre-training yields commendable performance gains while curbing computational expenses <cite class="ltx_cite ltx_citemacro_citep">(Singh <span class="ltx_ERROR undefined">\BBA</span> Davis, <a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2018</a>)</cite>. Following the universal object detection training phase, we embark on dataset-specific fine-tuning at elevated resolutions. This decision is guided by a comprehensive consideration of performance metrics, training cost, and substantial distinctions inherent to the three datasets.</p>
</div>
<div id="S4.SS4.SSS5.p7" class="ltx_para">
<p id="S4.SS4.SSS5.p7.1" class="ltx_p">We use the cosine learning rate annealing with warm restarts <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov <span class="ltx_ERROR undefined">\BBA</span> Hutter, <a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2016</a>)</cite> during the finetuning procedure.
Without any alterations to the model design, the finetuning and inference configurations are specified that,</p>
<ul id="S4.I3" class="ltx_itemize">
<li id="S4.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i1.p1" class="ltx_para">
<p id="S4.I3.i1.p1.1" class="ltx_p">for COCO, the model undergoes 24 epochs of training with 6 cyclical restarts, while training images are scaled within a range of [640, 1200], and evaluation is performed using the 800-pixel resized short edge of testing images;</p>
</div>
</li>
<li id="S4.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i2.p1" class="ltx_para">
<p id="S4.I3.i2.p1.1" class="ltx_p">for MVD, the model undergoes 24 epochs of training with 6 cyclical restarts, while training images are scaled within a range of [1024, 2048], and evaluation is performed using the 2048-pixel resized short edge of testing images;</p>
</div>
</li>
<li id="S4.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i3.p1" class="ltx_para">
<p id="S4.I3.i3.p1.1" class="ltx_p">for OID, the model undergoes 6 epochs of training with 2 cyclical restarts, while training images are scaled within a range of [640, 1200] and evaluation is performed using the 800-pixel resized short edge of testing images.</p>
</div>
</li>
</ul>
<p id="S4.SS4.SSS5.p7.2" class="ltx_p">Building upon the observed improvements resulting from inference scaling, we conduct finetuning with different hyper-parameters related to training image sizes. This optimization process aims to align the model more closely with the specific characteristics of the datasets. Notably, COCO and OID have relatively smaller raw image sizes compared to MVD. During universal object detection training, we originally considered training image sizes ranging from 480 to 960 pixels. However, in light of dataset-specific considerations, we empirically expand the range to [1024, 2048] for MVD training images, and [640, 1200] for COCO and OID. For a performance reason, we recommend dataset-specific finetuning to tailor the model to the unique characteristics of each dataset.
</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2212.09408/assets/x5.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="484" height="366" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Qualitative results on MVD <em id="S4.F6.4.1" class="ltx_emph ltx_font_italic">val</em> set. On the top left corner of each visualized bounding box, the predicted categories of which the corresponding confidence scores are greater than 0.1 are listed in the descending order of the confidence scores. The entry <em id="S4.F6.5.2" class="ltx_emph ltx_font_italic">classname_super</em> indicates this class is a superclass named <em id="S4.F6.6.3" class="ltx_emph ltx_font_italic">classname</em> in the unified label space.</figcaption>
</figure>
<div id="S4.SS4.SSS5.p8" class="ltx_para">
<p id="S4.SS4.SSS5.p8.1" class="ltx_p">Table <a href="#S4.T11" title="Table 11 ‣ 4.4.5 Scaling Up and Finetuning ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> provides a summary of the results obtained through scaling and finetuning. With the lighter backbone, dataset-specific high-resolution finetuning yields significant improvements, including a 3.2 mAP increase on COCO, a substantial 10.9 mAP gain on MVD, and a 0.7 mAP improvement on OID. Even with the larger backbone, finetuning continues to deliver considerable enhancements across all three datasets, resulting in a +1.6 mAP improvement on COCO, a noteworthy +9.9 mAP increase on MVD, and a 0.7 mAP gain on OID, respectively.</p>
</div>
</section>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Visualization</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">Figs. <a href="#S4.F5" title="Figure 5 ‣ 4.4.5 Scaling Up and Finetuning ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, <a href="#S4.F6" title="Figure 6 ‣ 4.4.5 Scaling Up and Finetuning ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and <a href="#S4.F7" title="Figure 7 ‣ 4.5 Visualization ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> showcase the detection results of the Large-UniDet model which uses the SEER-RegNet256gf backbone, and has not undergone further high-resolution specific fine-tuning. These results display up to five recognized categories per bounding box with a confidence threshold, highlighting the presence of category label duplication and semantic hierarchy across datasets in universal object detection. We will delve into this phenomenon in greater detail and provide examples in the subsequent sections.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">In the upper-left visualization result in Fig. <a href="#S4.F5" title="Figure 5 ‣ 4.4.5 Scaling Up and Finetuning ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we can see that a significant number of the categories present in the original label space have been detected successfully. Additionally, some unannotated categories such as <em id="S4.SS5.p2.1.1" class="ltx_emph ltx_font_italic">man</em>, <em id="S4.SS5.p2.1.2" class="ltx_emph ltx_font_italic">woman</em>, <em id="S4.SS5.p2.1.3" class="ltx_emph ltx_font_italic">girl</em>, <em id="S4.SS5.p2.1.4" class="ltx_emph ltx_font_italic">person_super</em>, <em id="S4.SS5.p2.1.5" class="ltx_emph ltx_font_italic">face</em>, <em id="S4.SS5.p2.1.6" class="ltx_emph ltx_font_italic">suit</em>, and <em id="S4.SS5.p2.1.7" class="ltx_emph ltx_font_italic">dress</em> are also transferred from the other two datasets, despite not being part of the original label space in COCO.</p>
</div>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.1" class="ltx_p"><em id="S4.SS5.p3.1.1" class="ltx_emph ltx_font_italic">Label duplication.</em> The categories <em id="S4.SS5.p3.1.2" class="ltx_emph ltx_font_italic">person</em> and <em id="S4.SS5.p3.1.3" class="ltx_emph ltx_font_italic">person_super</em> exhibit semantic duplication, which results in high confidence scores for two individuals in this image. To illustrate, the man is classified as <em id="S4.SS5.p3.1.4" class="ltx_emph ltx_font_italic">person</em> with a confidence score of 0.94 and <em id="S4.SS5.p3.1.5" class="ltx_emph ltx_font_italic">person_super</em> with a confidence score of 0.63. Similarly, the woman is classified as both <em id="S4.SS5.p3.1.6" class="ltx_emph ltx_font_italic">person</em> and <em id="S4.SS5.p3.1.7" class="ltx_emph ltx_font_italic">person_super</em> with a confidence score of 0.89.</p>
</div>
<div id="S4.SS5.p4" class="ltx_para">
<p id="S4.SS5.p4.1" class="ltx_p"><em id="S4.SS5.p4.1.1" class="ltx_emph ltx_font_italic">Semantic hierarchy.</em> As observed, since the categories <em id="S4.SS5.p4.1.2" class="ltx_emph ltx_font_italic">woman</em>, <em id="S4.SS5.p4.1.3" class="ltx_emph ltx_font_italic">man</em>, <em id="S4.SS5.p4.1.4" class="ltx_emph ltx_font_italic">girl</em>, and <em id="S4.SS5.p4.1.5" class="ltx_emph ltx_font_italic">boy</em> are all subclasses of <em id="S4.SS5.p4.1.6" class="ltx_emph ltx_font_italic">person</em>, the two individuals in this image are classified under one or more of these four subclasses.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2212.09408/assets/x6.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="484" height="351" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Qualitative results on OID <em id="S4.F7.4.1" class="ltx_emph ltx_font_italic">val</em> set. On the top left corner of each visualized bounding box, the predicted categories of which the corresponding confidence scores are greater than 0.05 are listed in the descending order of the confidence scores. The entry <em id="S4.F7.5.2" class="ltx_emph ltx_font_italic">classname_super</em> indicates this class is a superclass named <em id="S4.F7.6.3" class="ltx_emph ltx_font_italic">classname</em> in the unified label space.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Limitations</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Cross-dataset Generalization</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">In this study, we focus on the implementation of label space unification for three specific datasets, namely COCO, MVD, and OID. We acknowledge that generalizing this method to other diverse datasets, which may (a) lack hierarchical taxonomies, (b) contain different synonyms for similar concepts, or (c) utilize the same labels for distinct concepts, presents additional challenges and complexities that are not addressed within the scope of this study.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">To address these challenges and point toward potential solutions, we briefly offer several valuable approaches. (a) It’s important to note that our method does not require a label hierarchy. Our loss strategy, HCLS, is not dependent on hierarchical taxonomies and can be applied without loss suppression for superclass categories. (b) To address the issue of different synonyms for similar concepts, one viable approach is to utilize a language model <cite class="ltx_cite ltx_citemacro_citep">(Raffel <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2020</a>)</cite> to align diverse synonyms and subsequently merge two categories representing similar concepts, as demonstrated in the literature <cite class="ltx_cite ltx_citemacro_citep">(L. Cai <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>)</cite>. (c) For situations involving the same labels being applied to distinct concepts, a plausible solution involves training a partitioned object detector with separate detection branches for different datasets. Subsequently, label spaces can be unified based on visual concepts, which aligns with strategies outlined in existing literature <cite class="ltx_cite ltx_citemacro_citep">(X. Zhou <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2022</a>)</cite>.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">Indeed, merging multi-domain taxonomies can be facilitated through the use of language models, pretrained object detectors, or a synergistic combination of both, presenting a promising pathway to address the challenge of label inconsistencies across diverse datasets. Beyond the above mentioned, in the following section, we delve into another issue stemming from label inconsistencies.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Domain Over-fitting</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">In Fig. <a href="#S4.F5" title="Figure 5 ‣ 4.4.5 Scaling Up and Finetuning ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we observe that while the majority of annotated categories in the original COCO label space are successfully detected, some unannotated categories are also transferred from the other two datasets. However, accurately detecting and classifying these unannotated categories can be challenging due to annotation inconsistencies across the three datasets. For instance, in Fig. <a href="#S4.F5" title="Figure 5 ‣ 4.4.5 Scaling Up and Finetuning ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, categories like <em id="S5.SS2.p1.1.1" class="ltx_emph ltx_font_italic">eye</em>, <em id="S5.SS2.p1.1.2" class="ltx_emph ltx_font_italic">nose</em>, <em id="S5.SS2.p1.1.3" class="ltx_emph ltx_font_italic">mouth</em>, and <em id="S5.SS2.p1.1.4" class="ltx_emph ltx_font_italic">hand</em> from OID are not consistently detected in COCO images. Additionally, categories such as <em id="S5.SS2.p1.1.5" class="ltx_emph ltx_font_italic">person_super</em>, <em id="S5.SS2.p1.1.6" class="ltx_emph ltx_font_italic">man</em>, <em id="S5.SS2.p1.1.7" class="ltx_emph ltx_font_italic">woman</em>, and <em id="S5.SS2.p1.1.8" class="ltx_emph ltx_font_italic">turkey</em>, which semantically match objects in COCO images, are either detected with low confidence scores or not detected at all. Similar results are observed in Fig. <a href="#S4.F6" title="Figure 6 ‣ 4.4.5 Scaling Up and Finetuning ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and <a href="#S4.F7" title="Figure 7 ‣ 4.5 Visualization ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> for MVD and OID, where categories from other datasets may be omitted or exhibit varying levels of detection confidence.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Based on our observations, it appears that our universal object detector may be partially over-fitting to the distinctive characteristics of each data domain. This phenomenon suggests that the model has learned a way to distinguish testing images from different datasets, which could be seen as a form of “<em id="S5.SS2.p2.1.1" class="ltx_emph ltx_font_italic">cheating</em>” in universal object detection. However, this behavior could pose challenges and potential harm in real-world practical applications, as it may lead to inaccurate or inconsistent results when deploying the model across diverse datasets.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This raises an intriguing question: Can unifying the annotations in the unified label space lead to improved performance for the universal object detector on individual datasets? Existing literature <cite class="ltx_cite ltx_citemacro_citep">(Zhao <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib68" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2020</a>)</cite> has demonstrated the effectiveness of adding pseudo-labels for unannotated objects in the context of fully-annotated mixed datasets. However, the influence of annotation inconsistencies on individual datasets remains a relatively unexplored area.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">In Section <a href="#S4.SS4.SSS2" title="4.4.2 Hierarchical Loss Strategies ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4.2</span></a>, we experimented with a similar label handling strategy called <em id="S6.p2.1.1" class="ltx_emph ltx_font_italic">Unified hierarchy</em> to consolidate annotations for semantically duplicated categories. As we can see, our results presented in Tables <a href="#S4.T7" title="Table 7 ‣ 4.4.2 Hierarchical Loss Strategies ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> and <a href="#S4.T8" title="Table 8 ‣ 4.4.2 Hierarchical Loss Strategies ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, did not prove any improvement in COCO or MVD. Nevertheless, this topic warrants further exploration, and we leave it as an open question for future research.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In pursuit of solving the large-scale multi-domain universal object detection problem, we have introduced a series of resource-efficient techniques that leverage large vision models to yield robust visual representations across multiple diverse datasets. Our universal object detector combines three critical detector components with high capacity while maintaining the parameter stability of the large vision models. To address challenges such as cross-dataset label duplication and semantic hierarchy, we have implemented hierarchical taxonomy completion and a loss adaptation strategy known as HCLS within a unified label space spanning multiple datasets. Our research practices and findings offer a promising solution for real-world computer vision applications and underscore the potential of universal object detection in addressing complex, multi-domain challenges.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Data Availability Statement</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The authors confirm the data supporting the findings of this work are available within the article or its supplementary materials.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.1.1" class="ltx_bibitem">
<span class="ltx_bibblock"><span id="bib.1.1.1.1" class="ltx_ERROR undefined">\bibcommenthead</span>
</span>
</li>
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Azizi <span id="bib.bib1.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib1.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib1.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>azizi2021big<span id="bib.bib1.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Azizi, S., Mustafa, B., Ryan, F., Beaver, Z., Freyberg, J., Deaton, J.<span id="bib.bib1.8.3" class="ltx_ERROR undefined">\BDBL</span>others 
</span>
<span class="ltx_bibblock"><span id="bib.bib1.9.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2021.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.10.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib1.11.2" class="ltx_ERROR undefined">\APACrefatitle</span>Big self-supervised models advance medical image
classification Big self-supervised models advance medical image
classification.<span id="bib.bib1.12.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib1.13.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE/CVF International Conference on
Computer Vision Proceedings of the ieee/cvf international conference on
computer vision (<span id="bib.bib1.14.2" class="ltx_ERROR undefined">\BPGS</span> 3478–3488).
<span id="bib.bib1.15.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib1.16.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bello <span id="bib.bib2.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib2.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib2.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>bello2021revisiting<span id="bib.bib2.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Bello, I., Fedus, W., Du, X., Cubuk, E.D., Srinivas, A., Lin, T<span id="bib.bib2.8.3" class="ltx_ERROR undefined">\BHBI</span>Y.<span id="bib.bib2.9.4" class="ltx_ERROR undefined">\BDBL</span>Zoph, B. 
</span>
<span class="ltx_bibblock"><span id="bib.bib2.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2021.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib2.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Revisiting resnets: Improved training and scaling
strategies Revisiting resnets: Improved training and scaling
strategies.<span id="bib.bib2.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib2.14.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span>Advances in Neural Information Processing
Systems3422614–22627.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><span id="bib.bib2.15.1" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib2.16.2" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bevandić <span id="bib.bib3.4.4.1" class="ltx_ERROR undefined">\BBA</span> Šegvić (<span id="bib.bib3.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib3.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>bevandic2022automatic<span id="bib.bib3.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Bevandić, P.<span id="bib.bib3.8.3" class="ltx_ERROR undefined">\BCBT</span> <span id="bib.bib3.9.4" class="ltx_ERROR undefined">\BBA</span> Šegvić, S. 
</span>
<span class="ltx_bibblock"><span id="bib.bib3.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2022.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib3.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Automatic universal taxonomies for multi-domain semantic
segmentation Automatic universal taxonomies for multi-domain semantic
segmentation.<span id="bib.bib3.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib3.14.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span>arXiv preprint arXiv:2207.08445.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><span id="bib.bib3.15.1" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib3.16.2" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bodla <span id="bib.bib4.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib4.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2017)</span>
<span class="ltx_bibblock">
<span id="bib.bib4.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>bodla2017soft<span id="bib.bib4.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Bodla, N., Singh, B., Chellappa, R.<span id="bib.bib4.9.3" class="ltx_ERROR undefined">\BCBL</span> Davis, L.S. 
</span>
<span class="ltx_bibblock"><span id="bib.bib4.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2017.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib4.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Soft-NMS–improving object detection with one line of
code Soft-nms–improving object detection with one line of code.<span id="bib.bib4.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib4.14.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE international conference on computer
vision Proceedings of the ieee international conference on computer
vision (<span id="bib.bib4.15.2" class="ltx_ERROR undefined">\BPGS</span> 5561–5569).
<span id="bib.bib4.16.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib4.17.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bu <span id="bib.bib5.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib5.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib5.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>bu2021gaia<span id="bib.bib5.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Bu, X., Peng, J., Yan, J., Tan, T.<span id="bib.bib5.9.3" class="ltx_ERROR undefined">\BCBL</span> Zhang, Z. 
</span>
<span class="ltx_bibblock"><span id="bib.bib5.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2021.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib5.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Gaia: A transfer learning system of object detection
that fits your needs Gaia: A transfer learning system of object detection
that fits your needs.<span id="bib.bib5.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib5.14.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Proceedings of the ieee/cvf conference on computer
vision and pattern recognition (<span id="bib.bib5.15.2" class="ltx_ERROR undefined">\BPGS</span> 274–283).
<span id="bib.bib5.16.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib5.17.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">L. Cai <span id="bib.bib6.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib6.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib6.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>cai2022bigdetection<span id="bib.bib6.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Cai, L., Zhang, Z., Zhu, Y., Zhang, L., Li, M.<span id="bib.bib6.8.3" class="ltx_ERROR undefined">\BCBL</span> Xue, X. 
</span>
<span class="ltx_bibblock"><span id="bib.bib6.9.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2022.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.10.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib6.11.2" class="ltx_ERROR undefined">\APACrefatitle</span>BigDetection: A Large-scale Benchmark for Improved
Object Detector Pre-training Bigdetection: A large-scale benchmark for
improved object detector pre-training.<span id="bib.bib6.12.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib6.13.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Proceedings of the ieee/cvf conference on computer
vision and pattern recognition (<span id="bib.bib6.14.2" class="ltx_ERROR undefined">\BPGS</span> 4777–4787).
<span id="bib.bib6.15.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib6.16.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Z. Cai <span id="bib.bib7.4.4.1" class="ltx_ERROR undefined">\BBA</span> Vasconcelos (<span id="bib.bib7.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2018)</span>
<span class="ltx_bibblock">
<span id="bib.bib7.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>cai2018cascade<span id="bib.bib7.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Cai, Z.<span id="bib.bib7.8.3" class="ltx_ERROR undefined">\BCBT</span> <span id="bib.bib7.9.4" class="ltx_ERROR undefined">\BBA</span> Vasconcelos, N. 
</span>
<span class="ltx_bibblock"><span id="bib.bib7.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2018.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib7.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Cascade r-cnn: Delving into high quality object
detection Cascade r-cnn: Delving into high quality object
detection.<span id="bib.bib7.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib7.14.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE conference on computer vision and
pattern recognition Proceedings of the ieee conference on computer vision
and pattern recognition (<span id="bib.bib7.15.2" class="ltx_ERROR undefined">\BPGS</span> 6154–6162).
<span id="bib.bib7.16.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib7.17.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carion <span id="bib.bib8.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib8.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib8.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>carion2020end<span id="bib.bib8.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A.<span id="bib.bib8.8.3" class="ltx_ERROR undefined">\BCBL</span> Zagoruyko, S. 
</span>
<span class="ltx_bibblock"><span id="bib.bib8.9.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2020.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.10.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib8.11.2" class="ltx_ERROR undefined">\APACrefatitle</span>End-to-end object detection with transformers
End-to-end object detection with transformers.<span id="bib.bib8.12.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib8.13.1" class="ltx_ERROR undefined">\APACrefbtitle</span>European conference on computer vision European conference
on computer vision (<span id="bib.bib8.14.2" class="ltx_ERROR undefined">\BPGS</span> 213–229).
<span id="bib.bib8.15.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib8.16.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caron <span id="bib.bib9.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib9.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib9.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>caron2020unsupervised<span id="bib.bib9.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P.<span id="bib.bib9.8.3" class="ltx_ERROR undefined">\BCBL</span> Joulin, A. 
</span>
<span class="ltx_bibblock"><span id="bib.bib9.9.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2020.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.10.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib9.11.2" class="ltx_ERROR undefined">\APACrefatitle</span>Unsupervised learning of visual features by contrasting
cluster assignments Unsupervised learning of visual features by contrasting
cluster assignments.<span id="bib.bib9.12.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib9.13.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span>Advances in Neural Information Processing
Systems339912–9924.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><span id="bib.bib9.14.1" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib9.15.2" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen <span id="bib.bib10.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib10.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2019)</span>
<span class="ltx_bibblock">
<span id="bib.bib10.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>chen2019mmdetection<span id="bib.bib10.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Chen, K., Wang, J., Pang, J., Cao, Y., Xiong, Y., Li, X.<span id="bib.bib10.8.3" class="ltx_ERROR undefined">\BDBL</span>others 
</span>
<span class="ltx_bibblock"><span id="bib.bib10.9.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2019.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.10.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib10.11.2" class="ltx_ERROR undefined">\APACrefatitle</span>MMDetection: Open mmlab detection toolbox and benchmark
Mmdetection: Open mmlab detection toolbox and benchmark.<span id="bib.bib10.12.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib10.13.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span>arXiv preprint arXiv:1906.07155.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><span id="bib.bib10.14.1" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib10.15.2" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai <span id="bib.bib11.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib11.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib11.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>dai2021coatnet<span id="bib.bib11.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Dai, Z., Liu, H., Le, Q.V.<span id="bib.bib11.9.3" class="ltx_ERROR undefined">\BCBL</span> Tan, M. 
</span>
<span class="ltx_bibblock"><span id="bib.bib11.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2021.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib11.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Coatnet: Marrying convolution and attention for all data
sizes Coatnet: Marrying convolution and attention for all data
sizes.<span id="bib.bib11.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib11.14.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span>Advances in Neural Information Processing
Systems343965–3977.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><span id="bib.bib11.15.1" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib11.16.2" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng <span id="bib.bib12.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib12.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2009)</span>
<span class="ltx_bibblock">
<span id="bib.bib12.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>deng2009imagenet<span id="bib.bib12.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Deng, J., Dong, W., Socher, R., Li, L<span id="bib.bib12.8.3" class="ltx_ERROR undefined">\BHBI</span>J., Li, K.<span id="bib.bib12.9.4" class="ltx_ERROR undefined">\BCBL</span> Fei-Fei, L. 
</span>
<span class="ltx_bibblock"><span id="bib.bib12.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2009.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib12.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Imagenet: A large-scale hierarchical image database
Imagenet: A large-scale hierarchical image database.<span id="bib.bib12.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib12.14.1" class="ltx_ERROR undefined">\APACrefbtitle</span>2009 IEEE conference on computer vision and pattern
recognition 2009 ieee conference on computer vision and pattern
recognition (<span id="bib.bib12.15.2" class="ltx_ERROR undefined">\BPGS</span> 248–255).
<span id="bib.bib12.16.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib12.17.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin <span id="bib.bib13.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib13.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2018)</span>
<span class="ltx_bibblock">
<span id="bib.bib13.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>devlin2018bert<span id="bib.bib13.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Devlin, J., Chang, M<span id="bib.bib13.9.3" class="ltx_ERROR undefined">\BHBI</span>W., Lee, K.<span id="bib.bib13.10.4" class="ltx_ERROR undefined">\BCBL</span> Toutanova, K. 
</span>
<span class="ltx_bibblock"><span id="bib.bib13.11.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2018.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.12.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib13.13.2" class="ltx_ERROR undefined">\APACrefatitle</span>Bert: Pre-training of deep bidirectional transformers
for language understanding Bert: Pre-training of deep bidirectional
transformers for language understanding.<span id="bib.bib13.14.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib13.15.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span>arXiv preprint arXiv:1810.04805.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><span id="bib.bib13.16.1" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib13.17.2" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dollar <span id="bib.bib14.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib14.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2011)</span>
<span class="ltx_bibblock">
<span id="bib.bib14.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>dollar2011pedestrian<span id="bib.bib14.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Dollar, P., Wojek, C., Schiele, B.<span id="bib.bib14.9.3" class="ltx_ERROR undefined">\BCBL</span> Perona, P. 
</span>
<span class="ltx_bibblock"><span id="bib.bib14.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2011.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib14.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Pedestrian detection: An evaluation of the state of the
art Pedestrian detection: An evaluation of the state of the art.<span id="bib.bib14.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib14.14.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span>IEEE transactions on pattern analysis and machine
intelligence344743–761.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><span id="bib.bib14.15.1" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib14.16.2" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao <span id="bib.bib15.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib15.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2018)</span>
<span class="ltx_bibblock">
<span id="bib.bib15.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>gao2018dynamic<span id="bib.bib15.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Gao, M., Yu, R., Li, A., Morariu, V.I.<span id="bib.bib15.9.3" class="ltx_ERROR undefined">\BCBL</span> Davis, L.S. 
</span>
<span class="ltx_bibblock"><span id="bib.bib15.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2018.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib15.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Dynamic zoom-in network for fast object detection in
large images Dynamic zoom-in network for fast object detection in large
images.<span id="bib.bib15.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib15.14.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE conference on computer vision and
pattern recognition Proceedings of the ieee conference on computer vision
and pattern recognition (<span id="bib.bib15.15.2" class="ltx_ERROR undefined">\BPGS</span> 6926–6935).
<span id="bib.bib15.16.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib15.17.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghiasi <span id="bib.bib16.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib16.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2019)</span>
<span class="ltx_bibblock">
<span id="bib.bib16.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>ghiasi2019fpn<span id="bib.bib16.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Ghiasi, G., Lin, T<span id="bib.bib16.9.3" class="ltx_ERROR undefined">\BHBI</span>Y.<span id="bib.bib16.10.4" class="ltx_ERROR undefined">\BCBL</span> Le, Q.V. 
</span>
<span class="ltx_bibblock"><span id="bib.bib16.11.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2019.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.12.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib16.13.2" class="ltx_ERROR undefined">\APACrefatitle</span>Nas-fpn: Learning scalable feature pyramid architecture
for object detection Nas-fpn: Learning scalable feature pyramid
architecture for object detection.<span id="bib.bib16.14.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib16.15.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition Proceedings of the ieee/cvf conference on computer
vision and pattern recognition (<span id="bib.bib16.16.2" class="ltx_ERROR undefined">\BPGS</span> 7036–7045).
<span id="bib.bib16.17.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib16.18.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Girshick (<span id="bib.bib17.2.2.1" class="ltx_ERROR undefined">\APACyear</span>2015)</span>
<span class="ltx_bibblock">
<span id="bib.bib17.3.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>girshick2015fast<span id="bib.bib17.4.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Girshick, R. 
</span>
<span class="ltx_bibblock"><span id="bib.bib17.5.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2015.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.6.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib17.7.2" class="ltx_ERROR undefined">\APACrefatitle</span>Fast r-cnn Fast r-cnn.<span id="bib.bib17.8.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib17.9.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE international conference on computer
vision Proceedings of the ieee international conference on computer
vision (<span id="bib.bib17.10.2" class="ltx_ERROR undefined">\BPGS</span> 1440–1448).
<span id="bib.bib17.11.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib17.12.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong <span id="bib.bib18.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib18.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib18.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>gong2021mdalu<span id="bib.bib18.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Gong, R., Dai, D., Chen, Y., Li, W.<span id="bib.bib18.9.3" class="ltx_ERROR undefined">\BCBL</span> Van Gool, L. 
</span>
<span class="ltx_bibblock"><span id="bib.bib18.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2021.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib18.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>mDALU: Multi-Source Domain Adaptation and Label
Unification with Partial Datasets mdalu: Multi-source domain adaptation and
label unification with partial datasets.<span id="bib.bib18.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib18.14.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE/CVF International Conference on
Computer Vision Proceedings of the ieee/cvf international conference on
computer vision (<span id="bib.bib18.15.2" class="ltx_ERROR undefined">\BPGS</span> 8876–8885).
<span id="bib.bib18.16.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib18.17.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal <span id="bib.bib19.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib19.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib19.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>goyal2022vision<span id="bib.bib19.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Goyal, P., Duval, Q., Seessel, I., Caron, M., Singh, M., Misra, I.<span id="bib.bib19.8.3" class="ltx_ERROR undefined">\BDBL</span>Bojanowski, P. 
</span>
<span class="ltx_bibblock"><span id="bib.bib19.9.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2022.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.10.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib19.11.2" class="ltx_ERROR undefined">\APACrefatitle</span>Vision models are more robust and fair when pretrained
on uncurated images without supervision Vision models are more robust and
fair when pretrained on uncurated images without supervision.<span id="bib.bib19.12.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib19.13.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span>arXiv preprint arXiv:2202.08360.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><span id="bib.bib19.14.1" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib19.15.2" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">A. Gupta <span id="bib.bib20.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib20.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2019)</span>
<span class="ltx_bibblock">
<span id="bib.bib20.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>gupta2019lvis<span id="bib.bib20.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Gupta, A., Dollar, P.<span id="bib.bib20.9.3" class="ltx_ERROR undefined">\BCBL</span> Girshick, R. 
</span>
<span class="ltx_bibblock"><span id="bib.bib20.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2019.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib20.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Lvis: A dataset for large vocabulary instance
segmentation Lvis: A dataset for large vocabulary instance
segmentation.<span id="bib.bib20.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib20.14.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition Proceedings of the ieee/cvf conference on computer
vision and pattern recognition (<span id="bib.bib20.15.2" class="ltx_ERROR undefined">\BPGS</span> 5356–5364).
<span id="bib.bib20.16.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib20.17.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">T. Gupta <span id="bib.bib21.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib21.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib21.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>gupta2022towards<span id="bib.bib21.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Gupta, T., Kamath, A., Kembhavi, A.<span id="bib.bib21.9.3" class="ltx_ERROR undefined">\BCBL</span> Hoiem, D. 
</span>
<span class="ltx_bibblock"><span id="bib.bib21.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2022.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib21.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Towards general purpose vision systems: An end-to-end
task-agnostic vision-language architecture Towards general purpose vision
systems: An end-to-end task-agnostic vision-language architecture.<span id="bib.bib21.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib21.14.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Proceedings of the ieee/cvf conference on computer
vision and pattern recognition (<span id="bib.bib21.15.2" class="ltx_ERROR undefined">\BPGS</span> 16399–16409).
<span id="bib.bib21.16.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib21.17.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hasan <span id="bib.bib22.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib22.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib22.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>hasan2021generalizable<span id="bib.bib22.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Hasan, I., Liao, S., Li, J., Akram, S.U.<span id="bib.bib22.9.3" class="ltx_ERROR undefined">\BCBL</span> Shao, L. 
</span>
<span class="ltx_bibblock"><span id="bib.bib22.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2021.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib22.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Generalizable pedestrian detection: The elephant in the
room Generalizable pedestrian detection: The elephant in the room.<span id="bib.bib22.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib22.14.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Proceedings of the ieee/cvf conference on computer
vision and pattern recognition (<span id="bib.bib22.15.2" class="ltx_ERROR undefined">\BPGS</span> 11328–11337).
<span id="bib.bib22.16.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib22.17.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">K. He <span id="bib.bib23.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib23.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib23.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>he2022masked<span id="bib.bib23.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>He, K., Chen, X., Xie, S., Li, Y., Dollár, P.<span id="bib.bib23.8.3" class="ltx_ERROR undefined">\BCBL</span> Girshick, R. 
</span>
<span class="ltx_bibblock"><span id="bib.bib23.9.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2022.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.10.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib23.11.2" class="ltx_ERROR undefined">\APACrefatitle</span>Masked autoencoders are scalable vision learners
Masked autoencoders are scalable vision learners.<span id="bib.bib23.12.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib23.13.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Proceedings of the ieee/cvf conference on computer
vision and pattern recognition (<span id="bib.bib23.14.2" class="ltx_ERROR undefined">\BPGS</span> 16000–16009).
<span id="bib.bib23.15.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib23.16.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">K. He <span id="bib.bib24.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib24.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib24.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>he2020momentum<span id="bib.bib24.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>He, K., Fan, H., Wu, Y., Xie, S.<span id="bib.bib24.9.3" class="ltx_ERROR undefined">\BCBL</span> Girshick, R. 
</span>
<span class="ltx_bibblock"><span id="bib.bib24.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2020.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib24.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Momentum contrast for unsupervised visual representation
learning Momentum contrast for unsupervised visual representation
learning.<span id="bib.bib24.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib24.14.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition Proceedings of the ieee/cvf conference on computer
vision and pattern recognition (<span id="bib.bib24.15.2" class="ltx_ERROR undefined">\BPGS</span> 9729–9738).
<span id="bib.bib24.16.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib24.17.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Y. He <span id="bib.bib25.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib25.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib25.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>he2022x<span id="bib.bib25.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>He, Y., Huang, G., Chen, S., Teng, J., Wang, K., Yin, Z.<span id="bib.bib25.8.3" class="ltx_ERROR undefined">\BDBL</span>Shao, J. 
</span>
<span class="ltx_bibblock"><span id="bib.bib25.9.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2022.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.10.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib25.11.2" class="ltx_ERROR undefined">\APACrefatitle</span>X-Learner: Learning Cross Sources and Tasks for
Universal Visual Representation X-learner: Learning cross sources and tasks
for universal visual representation.<span id="bib.bib25.12.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib25.13.1" class="ltx_ERROR undefined">\APACrefbtitle</span>European Conference on Computer Vision European conference
on computer vision (<span id="bib.bib25.14.2" class="ltx_ERROR undefined">\BPGS</span> 509–528).
<span id="bib.bib25.15.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib25.16.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Howard <span id="bib.bib26.4.4.1" class="ltx_ERROR undefined">\BBA</span> Ruder (<span id="bib.bib26.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2018)</span>
<span class="ltx_bibblock">
<span id="bib.bib26.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>howard2018universal<span id="bib.bib26.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Howard, J.<span id="bib.bib26.8.3" class="ltx_ERROR undefined">\BCBT</span> <span id="bib.bib26.9.4" class="ltx_ERROR undefined">\BBA</span> Ruder, S. 
</span>
<span class="ltx_bibblock"><span id="bib.bib26.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2018.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib26.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Universal language model fine-tuning for text
classification Universal language model fine-tuning for text
classification.<span id="bib.bib26.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib26.14.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span>arXiv preprint arXiv:1801.06146.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><span id="bib.bib26.15.1" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib26.16.2" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang <span id="bib.bib27.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib27.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2017)</span>
<span class="ltx_bibblock">
<span id="bib.bib27.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>huang2017speed<span id="bib.bib27.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Huang, J., Rathod, V., Sun, C., Zhu, M., Korattikara, A., Fathi, A.<span id="bib.bib27.8.3" class="ltx_ERROR undefined">\BDBL</span>others 
</span>
<span class="ltx_bibblock"><span id="bib.bib27.9.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2017.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.10.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib27.11.2" class="ltx_ERROR undefined">\APACrefatitle</span>Speed/accuracy trade-offs for modern convolutional
object detectors Speed/accuracy trade-offs for modern convolutional object
detectors.<span id="bib.bib27.12.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib27.13.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE conference on computer vision and
pattern recognition Proceedings of the ieee conference on computer vision
and pattern recognition (<span id="bib.bib27.14.2" class="ltx_ERROR undefined">\BPGS</span> 7310–7311).
<span id="bib.bib27.15.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib27.16.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia <span id="bib.bib28.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib28.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib28.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>jia2021scaling<span id="bib.bib28.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Jia, C., Yang, Y., Xia, Y., Chen, Y<span id="bib.bib28.8.3" class="ltx_ERROR undefined">\BHBI</span>T., Parekh, Z., Pham, H.<span id="bib.bib28.9.4" class="ltx_ERROR undefined">\BDBL</span>Duerig, T. 
</span>
<span class="ltx_bibblock"><span id="bib.bib28.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2021.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib28.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Scaling up visual and vision-language representation
learning with noisy text supervision Scaling up visual and vision-language
representation learning with noisy text supervision.<span id="bib.bib28.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib28.14.1" class="ltx_ERROR undefined">\APACrefbtitle</span>International Conference on Machine Learning International
conference on machine learning (<span id="bib.bib28.15.2" class="ltx_ERROR undefined">\BPGS</span> 4904–4916).
<span id="bib.bib28.16.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib28.17.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joulin <span id="bib.bib29.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib29.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2016)</span>
<span class="ltx_bibblock">
<span id="bib.bib29.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>joulin2016learning<span id="bib.bib29.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Joulin, A., Maaten, L.v.d., Jabri, A.<span id="bib.bib29.9.3" class="ltx_ERROR undefined">\BCBL</span> Vasilache, N. 
</span>
<span class="ltx_bibblock"><span id="bib.bib29.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2016.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib29.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Learning visual features from large weakly supervised
data Learning visual features from large weakly supervised data.<span id="bib.bib29.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib29.14.1" class="ltx_ERROR undefined">\APACrefbtitle</span>European Conference on Computer Vision European conference
on computer vision (<span id="bib.bib29.15.2" class="ltx_ERROR undefined">\BPGS</span> 67–84).
<span id="bib.bib29.16.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib29.17.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamath <span id="bib.bib30.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib30.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib30.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>kamath2022webly<span id="bib.bib30.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Kamath, A., Clark, C., Gupta, T., Kolve, E., Hoiem, D.<span id="bib.bib30.8.3" class="ltx_ERROR undefined">\BCBL</span> Kembhavi, A. 
</span>
<span class="ltx_bibblock"><span id="bib.bib30.9.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2022.

</span>
<span class="ltx_bibblock"><span id="bib.bib30.10.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib30.11.2" class="ltx_ERROR undefined">\APACrefatitle</span>Webly supervised concept expansion for general purpose
vision models Webly supervised concept expansion for general purpose vision
models.<span id="bib.bib30.12.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib30.13.1" class="ltx_ERROR undefined">\APACrefbtitle</span>European Conference on Computer Vision European conference
on computer vision (<span id="bib.bib30.14.2" class="ltx_ERROR undefined">\BPGS</span> 662–681).
<span id="bib.bib30.15.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib30.16.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kolesnikov <span id="bib.bib31.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib31.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2019)</span>
<span class="ltx_bibblock">
<span id="bib.bib31.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>kolesnikov2019revisiting<span id="bib.bib31.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Kolesnikov, A., Zhai, X.<span id="bib.bib31.9.3" class="ltx_ERROR undefined">\BCBL</span> Beyer, L. 
</span>
<span class="ltx_bibblock"><span id="bib.bib31.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2019.

</span>
<span class="ltx_bibblock"><span id="bib.bib31.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib31.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Revisiting self-supervised visual representation
learning Revisiting self-supervised visual representation learning.<span id="bib.bib31.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib31.14.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition Proceedings of the ieee/cvf conference on computer
vision and pattern recognition (<span id="bib.bib31.15.2" class="ltx_ERROR undefined">\BPGS</span> 1920–1929).
<span id="bib.bib31.16.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib31.17.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kornblith <span id="bib.bib32.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib32.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2019)</span>
<span class="ltx_bibblock">
<span id="bib.bib32.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>kornblith2019better<span id="bib.bib32.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Kornblith, S., Shlens, J.<span id="bib.bib32.9.3" class="ltx_ERROR undefined">\BCBL</span> Le, Q.V. 
</span>
<span class="ltx_bibblock"><span id="bib.bib32.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2019.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib32.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Do better imagenet models transfer better? Do better
imagenet models transfer better?<span id="bib.bib32.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib32.14.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition Proceedings of the ieee/cvf conference on computer
vision and pattern recognition (<span id="bib.bib32.15.2" class="ltx_ERROR undefined">\BPGS</span> 2661–2671).
<span id="bib.bib32.16.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib32.17.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna <span id="bib.bib33.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib33.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2017)</span>
<span class="ltx_bibblock">
<span id="bib.bib33.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>krishna2017visual<span id="bib.bib33.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J.<span id="bib.bib33.8.3" class="ltx_ERROR undefined">\BDBL</span>others 
</span>
<span class="ltx_bibblock"><span id="bib.bib33.9.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2017.

</span>
<span class="ltx_bibblock"><span id="bib.bib33.10.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib33.11.2" class="ltx_ERROR undefined">\APACrefatitle</span>Visual genome: Connecting language and vision using
crowdsourced dense image annotations Visual genome: Connecting language and
vision using crowdsourced dense image annotations.<span id="bib.bib33.12.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib33.13.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span>International journal of computer
vision123132–73.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><span id="bib.bib33.14.1" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib33.15.2" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuznetsova <span id="bib.bib34.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib34.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib34.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>kuznetsova2020open<span id="bib.bib34.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin, I., Pont-Tuset, J.<span id="bib.bib34.8.3" class="ltx_ERROR undefined">\BDBL</span>others 
</span>
<span class="ltx_bibblock"><span id="bib.bib34.9.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2020.

</span>
<span class="ltx_bibblock"><span id="bib.bib34.10.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib34.11.2" class="ltx_ERROR undefined">\APACrefatitle</span>The open images dataset v4 The open images dataset
v4.<span id="bib.bib34.12.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib34.13.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span>International Journal of Computer
Vision12871956–1981.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><span id="bib.bib34.14.1" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib34.15.2" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lambert <span id="bib.bib35.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib35.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib35.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>lambert2020mseg<span id="bib.bib35.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Lambert, J., Liu, Z., Sener, O., Hays, J.<span id="bib.bib35.9.3" class="ltx_ERROR undefined">\BCBL</span> Koltun, V. 
</span>
<span class="ltx_bibblock"><span id="bib.bib35.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2020.

</span>
<span class="ltx_bibblock"><span id="bib.bib35.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib35.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>MSeg: A composite dataset for multi-domain semantic
segmentation Mseg: A composite dataset for multi-domain semantic
segmentation.<span id="bib.bib35.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib35.14.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition Proceedings of the ieee/cvf conference on computer
vision and pattern recognition (<span id="bib.bib35.15.2" class="ltx_ERROR undefined">\BPGS</span> 2879–2888).
<span id="bib.bib35.16.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib35.17.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">F. Lin <span id="bib.bib36.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib36.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib36.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>lin2021auto<span id="bib.bib36.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Lin, F., Xu, H., Li, H., Xiong, H.<span id="bib.bib36.9.3" class="ltx_ERROR undefined">\BCBL</span> Qi, G<span id="bib.bib36.10.4" class="ltx_ERROR undefined">\BHBI</span>J. 
</span>
<span class="ltx_bibblock"><span id="bib.bib36.11.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2021.

</span>
<span class="ltx_bibblock"><span id="bib.bib36.12.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib36.13.2" class="ltx_ERROR undefined">\APACrefatitle</span>Auto-Encoding Transformations in Reparameterized Lie
Groups for Unsupervised Learning Auto-encoding transformations in
reparameterized lie groups for unsupervised learning.<span id="bib.bib36.14.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib36.15.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the AAAI Conference on Artificial Intelligence
Proceedings of the aaai conference on artificial intelligence (<span id="bib.bib36.16.2" class="ltx_ERROR undefined">\BVOL</span> 35,
<span id="bib.bib36.17.3" class="ltx_ERROR undefined">\BPGS</span> 8610–8617).
<span id="bib.bib36.18.4" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib36.19.5" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">T<span id="bib.bib37.6.6.1" class="ltx_ERROR undefined">\BHBI</span>Y. Lin <span id="bib.bib37.7.7.2" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib37.8.8.3" class="ltx_ERROR undefined">\APACyear</span>2017)</span>
<span class="ltx_bibblock">
<span id="bib.bib37.9.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>lin2017feature<span id="bib.bib37.10.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Lin, T<span id="bib.bib37.11.3" class="ltx_ERROR undefined">\BHBI</span>Y., Dollár, P., Girshick, R., He, K., Hariharan, B.<span id="bib.bib37.12.4" class="ltx_ERROR undefined">\BCBL</span> Belongie, S. 
</span>
<span class="ltx_bibblock"><span id="bib.bib37.13.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2017.

</span>
<span class="ltx_bibblock"><span id="bib.bib37.14.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib37.15.2" class="ltx_ERROR undefined">\APACrefatitle</span>Feature pyramid networks for object detection Feature
pyramid networks for object detection.<span id="bib.bib37.16.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib37.17.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE conference on computer vision and
pattern recognition Proceedings of the ieee conference on computer vision
and pattern recognition (<span id="bib.bib37.18.2" class="ltx_ERROR undefined">\BPGS</span> 2117–2125).
<span id="bib.bib37.19.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib37.20.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">T<span id="bib.bib38.6.6.1" class="ltx_ERROR undefined">\BHBI</span>Y. Lin <span id="bib.bib38.7.7.2" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib38.8.8.3" class="ltx_ERROR undefined">\APACyear</span>2014)</span>
<span class="ltx_bibblock">
<span id="bib.bib38.9.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>lin2014microsoft<span id="bib.bib38.10.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Lin, T<span id="bib.bib38.11.3" class="ltx_ERROR undefined">\BHBI</span>Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.<span id="bib.bib38.12.4" class="ltx_ERROR undefined">\BDBL</span>Zitnick, C.L. 
</span>
<span class="ltx_bibblock"><span id="bib.bib38.13.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2014.

</span>
<span class="ltx_bibblock"><span id="bib.bib38.14.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib38.15.2" class="ltx_ERROR undefined">\APACrefatitle</span>Microsoft coco: Common objects in context Microsoft
coco: Common objects in context.<span id="bib.bib38.16.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib38.17.1" class="ltx_ERROR undefined">\APACrefbtitle</span>European conference on computer vision European conference
on computer vision (<span id="bib.bib38.18.2" class="ltx_ERROR undefined">\BPGS</span> 740–755).
<span id="bib.bib38.19.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib38.20.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">S. Liu <span id="bib.bib39.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib39.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2018)</span>
<span class="ltx_bibblock">
<span id="bib.bib39.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>liu2018path<span id="bib.bib39.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Liu, S., Qi, L., Qin, H., Shi, J.<span id="bib.bib39.9.3" class="ltx_ERROR undefined">\BCBL</span> Jia, J. 
</span>
<span class="ltx_bibblock"><span id="bib.bib39.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2018.

</span>
<span class="ltx_bibblock"><span id="bib.bib39.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib39.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Path aggregation network for instance segmentation
Path aggregation network for instance segmentation.<span id="bib.bib39.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib39.14.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE conference on computer vision and
pattern recognition Proceedings of the ieee conference on computer vision
and pattern recognition (<span id="bib.bib39.15.2" class="ltx_ERROR undefined">\BPGS</span> 8759–8768).
<span id="bib.bib39.16.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib39.17.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Y. Liu <span id="bib.bib40.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib40.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib40.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>liu2020cbnet<span id="bib.bib40.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Liu, Y., Wang, Y., Wang, S., Liang, T., Zhao, Q., Tang, Z.<span id="bib.bib40.8.3" class="ltx_ERROR undefined">\BCBL</span> Ling, H. 
</span>
<span class="ltx_bibblock"><span id="bib.bib40.9.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2020.

</span>
<span class="ltx_bibblock"><span id="bib.bib40.10.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib40.11.2" class="ltx_ERROR undefined">\APACrefatitle</span>Cbnet: A novel composite backbone network architecture
for object detection Cbnet: A novel composite backbone network architecture
for object detection.<span id="bib.bib40.12.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib40.13.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the AAAI conference on artificial intelligence
Proceedings of the aaai conference on artificial intelligence (<span id="bib.bib40.14.2" class="ltx_ERROR undefined">\BVOL</span> 34,
<span id="bib.bib40.15.3" class="ltx_ERROR undefined">\BPGS</span> 11653–11660).
<span id="bib.bib40.16.4" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib40.17.5" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Z. Liu <span id="bib.bib41.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib41.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib41.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>liu2022swin<span id="bib.bib41.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Liu, Z., Hu, H., Lin, Y., Yao, Z., Xie, Z., Wei, Y.<span id="bib.bib41.8.3" class="ltx_ERROR undefined">\BDBL</span>others 
</span>
<span class="ltx_bibblock"><span id="bib.bib41.9.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2022.

</span>
<span class="ltx_bibblock"><span id="bib.bib41.10.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib41.11.2" class="ltx_ERROR undefined">\APACrefatitle</span>Swin transformer v2: Scaling up capacity and resolution
Swin transformer v2: Scaling up capacity and resolution.<span id="bib.bib41.12.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib41.13.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Proceedings of the ieee/cvf conference on computer
vision and pattern recognition (<span id="bib.bib41.14.2" class="ltx_ERROR undefined">\BPGS</span> 12009–12019).
<span id="bib.bib41.15.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib41.16.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Z. Liu <span id="bib.bib42.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib42.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib42.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>liu2021swin<span id="bib.bib42.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z.<span id="bib.bib42.8.3" class="ltx_ERROR undefined">\BDBL</span>Guo, B. 
</span>
<span class="ltx_bibblock"><span id="bib.bib42.9.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2021.

</span>
<span class="ltx_bibblock"><span id="bib.bib42.10.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib42.11.2" class="ltx_ERROR undefined">\APACrefatitle</span>Swin transformer: Hierarchical vision transformer using
shifted windows Swin transformer: Hierarchical vision transformer using
shifted windows.<span id="bib.bib42.12.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib42.13.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE/CVF International Conference on
Computer Vision Proceedings of the ieee/cvf international conference on
computer vision (<span id="bib.bib42.14.2" class="ltx_ERROR undefined">\BPGS</span> 10012–10022).
<span id="bib.bib42.15.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib42.16.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov <span id="bib.bib43.4.4.1" class="ltx_ERROR undefined">\BBA</span> Hutter (<span id="bib.bib43.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2016)</span>
<span class="ltx_bibblock">
<span id="bib.bib43.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>loshchilov2016sgdr<span id="bib.bib43.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Loshchilov, I.<span id="bib.bib43.8.3" class="ltx_ERROR undefined">\BCBT</span> <span id="bib.bib43.9.4" class="ltx_ERROR undefined">\BBA</span> Hutter, F. 
</span>
<span class="ltx_bibblock"><span id="bib.bib43.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2016.

</span>
<span class="ltx_bibblock"><span id="bib.bib43.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib43.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Sgdr: Stochastic gradient descent with warm restarts
Sgdr: Stochastic gradient descent with warm restarts.<span id="bib.bib43.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib43.14.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span>arXiv preprint arXiv:1608.03983.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><span id="bib.bib43.15.1" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib43.16.2" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu <span id="bib.bib44.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib44.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib44.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>lu2022unified<span id="bib.bib44.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Lu, J., Clark, C., Zellers, R., Mottaghi, R.<span id="bib.bib44.9.3" class="ltx_ERROR undefined">\BCBL</span> Kembhavi, A. 
</span>
<span class="ltx_bibblock"><span id="bib.bib44.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2022.

</span>
<span class="ltx_bibblock"><span id="bib.bib44.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib44.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Unified-io: A unified model for vision, language, and
multi-modal tasks Unified-io: A unified model for vision, language, and
multi-modal tasks.<span id="bib.bib44.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib44.14.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span>arXiv preprint arXiv:2206.08916.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><span id="bib.bib44.15.1" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib44.16.2" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng <span id="bib.bib45.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib45.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib45.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>meng2022detection<span id="bib.bib45.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Meng, L., Dai, X., Chen, Y., Zhang, P., Chen, D., Liu, M.<span id="bib.bib45.8.3" class="ltx_ERROR undefined">\BDBL</span>Jiang, Y<span id="bib.bib45.9.4" class="ltx_ERROR undefined">\BHBI</span>G. 
</span>
<span class="ltx_bibblock"><span id="bib.bib45.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2022.

</span>
<span class="ltx_bibblock"><span id="bib.bib45.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib45.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Detection Hub: Unifying Object Detection Datasets via
Query Adaptation on Language Embedding Detection hub: Unifying object
detection datasets via query adaptation on language embedding.<span id="bib.bib45.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib45.14.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span>arXiv preprint arXiv:2206.03484.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><span id="bib.bib45.15.1" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib45.16.2" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Micikevicius <span id="bib.bib46.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib46.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2018)</span>
<span class="ltx_bibblock">
<span id="bib.bib46.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>micikevicius2018mixed<span id="bib.bib46.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D.<span id="bib.bib46.8.3" class="ltx_ERROR undefined">\BDBL</span>others 
</span>
<span class="ltx_bibblock"><span id="bib.bib46.9.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2018.

</span>
<span class="ltx_bibblock"><span id="bib.bib46.10.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib46.11.2" class="ltx_ERROR undefined">\APACrefatitle</span>Mixed Precision Training Mixed precision
training.<span id="bib.bib46.12.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib46.13.1" class="ltx_ERROR undefined">\APACrefbtitle</span>International Conference on Learning Representations.
International conference on learning representations.
<span id="bib.bib46.14.2" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib46.15.3" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Neuhold <span id="bib.bib47.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib47.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2017)</span>
<span class="ltx_bibblock">
<span id="bib.bib47.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>neuhold2017mapillary<span id="bib.bib47.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Neuhold, G., Ollmann, T., Rota Bulo, S.<span id="bib.bib47.9.3" class="ltx_ERROR undefined">\BCBL</span> Kontschieder, P. 
</span>
<span class="ltx_bibblock"><span id="bib.bib47.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2017.

</span>
<span class="ltx_bibblock"><span id="bib.bib47.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib47.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>The mapillary vistas dataset for semantic understanding
of street scenes The mapillary vistas dataset for semantic understanding of
street scenes.<span id="bib.bib47.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib47.14.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE international conference on computer
vision Proceedings of the ieee international conference on computer
vision (<span id="bib.bib47.15.2" class="ltx_ERROR undefined">\BPGS</span> 4990–4999).
<span id="bib.bib47.16.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib47.17.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pang <span id="bib.bib48.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib48.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2019)</span>
<span class="ltx_bibblock">
<span id="bib.bib48.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>pang2019libra<span id="bib.bib48.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Pang, J., Chen, K., Shi, J., Feng, H., Ouyang, W.<span id="bib.bib48.8.3" class="ltx_ERROR undefined">\BCBL</span> Lin, D. 
</span>
<span class="ltx_bibblock"><span id="bib.bib48.9.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2019.

</span>
<span class="ltx_bibblock"><span id="bib.bib48.10.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib48.11.2" class="ltx_ERROR undefined">\APACrefatitle</span>Libra r-cnn: Towards balanced learning for object
detection Libra r-cnn: Towards balanced learning for object
detection.<span id="bib.bib48.12.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib48.13.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition Proceedings of the ieee/cvf conference on computer
vision and pattern recognition (<span id="bib.bib48.14.2" class="ltx_ERROR undefined">\BPGS</span> 821–830).
<span id="bib.bib48.15.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib48.16.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford <span id="bib.bib49.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib49.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib49.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>radford2021learning<span id="bib.bib49.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S.<span id="bib.bib49.8.3" class="ltx_ERROR undefined">\BDBL</span>others 
</span>
<span class="ltx_bibblock"><span id="bib.bib49.9.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2021.

</span>
<span class="ltx_bibblock"><span id="bib.bib49.10.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib49.11.2" class="ltx_ERROR undefined">\APACrefatitle</span>Learning transferable visual models from natural
language supervision Learning transferable visual models from natural
language supervision.<span id="bib.bib49.12.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib49.13.1" class="ltx_ERROR undefined">\APACrefbtitle</span>International Conference on Machine Learning International
conference on machine learning (<span id="bib.bib49.14.2" class="ltx_ERROR undefined">\BPGS</span> 8748–8763).
<span id="bib.bib49.15.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib49.16.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford <span id="bib.bib50.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib50.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2018)</span>
<span class="ltx_bibblock">
<span id="bib.bib50.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>radford2018improving<span id="bib.bib50.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.<span id="bib.bib50.9.3" class="ltx_ERROR undefined">\BCBL</span> <span id="bib.bib50.10.4" class="ltx_ERROR undefined">\BOthersPeriod</span>. 
</span>
<span class="ltx_bibblock"><span id="bib.bib50.11.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2018.

</span>
<span class="ltx_bibblock"><span id="bib.bib50.12.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib50.13.2" class="ltx_ERROR undefined">\APACrefatitle</span>Improving language understanding by generative
pre-training Improving language understanding by generative
pre-training.<span id="bib.bib50.14.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><span id="bib.bib50.15.1" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib50.16.2" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radosavovic <span id="bib.bib51.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib51.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib51.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>radosavovic2020designing<span id="bib.bib51.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Radosavovic, I., Kosaraju, R.P., Girshick, R., He, K.<span id="bib.bib51.9.3" class="ltx_ERROR undefined">\BCBL</span> Dollár, P. 
</span>
<span class="ltx_bibblock"><span id="bib.bib51.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2020.

</span>
<span class="ltx_bibblock"><span id="bib.bib51.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib51.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Designing network design spaces Designing network
design spaces.<span id="bib.bib51.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib51.14.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition Proceedings of the ieee/cvf conference on computer
vision and pattern recognition (<span id="bib.bib51.15.2" class="ltx_ERROR undefined">\BPGS</span> 10428–10436).
<span id="bib.bib51.16.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib51.17.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel <span id="bib.bib52.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib52.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib52.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>raffel2020exploring<span id="bib.bib52.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M.<span id="bib.bib52.8.3" class="ltx_ERROR undefined">\BDBL</span>Liu, P.J. 
</span>
<span class="ltx_bibblock"><span id="bib.bib52.9.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2020.

</span>
<span class="ltx_bibblock"><span id="bib.bib52.10.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib52.11.2" class="ltx_ERROR undefined">\APACrefatitle</span>Exploring the limits of transfer learning with a unified
text-to-text transformer Exploring the limits of transfer learning with a
unified text-to-text transformer.<span id="bib.bib52.12.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib52.13.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span>The Journal of Machine Learning
Research2115485–5551.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><span id="bib.bib52.14.1" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib52.15.2" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ranftl <span id="bib.bib53.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib53.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib53.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>ranftl2020towards<span id="bib.bib53.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Ranftl, R., Lasinger, K., Hafner, D., Schindler, K.<span id="bib.bib53.9.3" class="ltx_ERROR undefined">\BCBL</span> Koltun, V. 
</span>
<span class="ltx_bibblock"><span id="bib.bib53.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2020.

</span>
<span class="ltx_bibblock"><span id="bib.bib53.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib53.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Towards robust monocular depth estimation: Mixing
datasets for zero-shot cross-dataset transfer Towards robust monocular
depth estimation: Mixing datasets for zero-shot cross-dataset
transfer.<span id="bib.bib53.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib53.14.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span>IEEE transactions on pattern analysis and machine
intelligence.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><span id="bib.bib53.15.1" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib53.16.2" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren <span id="bib.bib54.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib54.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2015)</span>
<span class="ltx_bibblock">
<span id="bib.bib54.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>ren2015faster<span id="bib.bib54.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Ren, S., He, K., Girshick, R.<span id="bib.bib54.9.3" class="ltx_ERROR undefined">\BCBL</span> Sun, J. 
</span>
<span class="ltx_bibblock"><span id="bib.bib54.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2015.

</span>
<span class="ltx_bibblock"><span id="bib.bib54.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib54.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Faster r-cnn: Towards real-time object detection with
region proposal networks Faster r-cnn: Towards real-time object detection
with region proposal networks.<span id="bib.bib54.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib54.14.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span>Advances in neural information processing
systems28.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><span id="bib.bib54.15.1" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib54.16.2" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">J. Shao <span id="bib.bib55.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib55.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib55.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>shao2021intern<span id="bib.bib55.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Shao, J., Chen, S., Li, Y., Wang, K., Yin, Z., He, Y.<span id="bib.bib55.8.3" class="ltx_ERROR undefined">\BDBL</span>others 
</span>
<span class="ltx_bibblock"><span id="bib.bib55.9.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2021.

</span>
<span class="ltx_bibblock"><span id="bib.bib55.10.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib55.11.2" class="ltx_ERROR undefined">\APACrefatitle</span>Intern: A new learning paradigm towards general vision
Intern: A new learning paradigm towards general vision.<span id="bib.bib55.12.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib55.13.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span>arXiv preprint arXiv:2111.08687.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><span id="bib.bib55.14.1" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib55.15.2" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">S. Shao <span id="bib.bib56.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib56.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2019)</span>
<span class="ltx_bibblock">
<span id="bib.bib56.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>shao2019objects365<span id="bib.bib56.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Shao, S., Li, Z., Zhang, T., Peng, C., Yu, G., Zhang, X.<span id="bib.bib56.8.3" class="ltx_ERROR undefined">\BDBL</span>Sun, J. 
</span>
<span class="ltx_bibblock"><span id="bib.bib56.9.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2019.

</span>
<span class="ltx_bibblock"><span id="bib.bib56.10.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib56.11.2" class="ltx_ERROR undefined">\APACrefatitle</span>Objects365: A large-scale, high-quality dataset for
object detection Objects365: A large-scale, high-quality dataset for object
detection.<span id="bib.bib56.12.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib56.13.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE/CVF international conference on
computer vision Proceedings of the ieee/cvf international conference on
computer vision (<span id="bib.bib56.14.2" class="ltx_ERROR undefined">\BPGS</span> 8430–8439).
<span id="bib.bib56.15.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib56.16.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh <span id="bib.bib57.4.4.1" class="ltx_ERROR undefined">\BBA</span> Davis (<span id="bib.bib57.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2018)</span>
<span class="ltx_bibblock">
<span id="bib.bib57.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>singh2018analysis<span id="bib.bib57.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Singh, B.<span id="bib.bib57.8.3" class="ltx_ERROR undefined">\BCBT</span> <span id="bib.bib57.9.4" class="ltx_ERROR undefined">\BBA</span> Davis, L.S. 
</span>
<span class="ltx_bibblock"><span id="bib.bib57.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2018.

</span>
<span class="ltx_bibblock"><span id="bib.bib57.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib57.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>An analysis of scale invariance in object detection
snip An analysis of scale invariance in object detection snip.<span id="bib.bib57.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib57.14.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE conference on computer vision and
pattern recognition Proceedings of the ieee conference on computer vision
and pattern recognition (<span id="bib.bib57.15.2" class="ltx_ERROR undefined">\BPGS</span> 3578–3587).
<span id="bib.bib57.16.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib57.17.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun <span id="bib.bib58.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib58.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2017)</span>
<span class="ltx_bibblock">
<span id="bib.bib58.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>sun2017revisiting<span id="bib.bib58.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Sun, C., Shrivastava, A., Singh, S.<span id="bib.bib58.9.3" class="ltx_ERROR undefined">\BCBL</span> Gupta, A. 
</span>
<span class="ltx_bibblock"><span id="bib.bib58.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2017.

</span>
<span class="ltx_bibblock"><span id="bib.bib58.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib58.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Revisiting unreasonable effectiveness of data in deep
learning era Revisiting unreasonable effectiveness of data in deep learning
era.<span id="bib.bib58.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib58.14.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE international conference on computer
vision Proceedings of the ieee international conference on computer
vision (<span id="bib.bib58.15.2" class="ltx_ERROR undefined">\BPGS</span> 843–852).
<span id="bib.bib58.16.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib58.17.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan <span id="bib.bib59.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib59.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib59.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>tan2020efficientdet<span id="bib.bib59.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Tan, M., Pang, R.<span id="bib.bib59.9.3" class="ltx_ERROR undefined">\BCBL</span> Le, Q.V. 
</span>
<span class="ltx_bibblock"><span id="bib.bib59.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2020.

</span>
<span class="ltx_bibblock"><span id="bib.bib59.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib59.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Efficientdet: Scalable and efficient object detection
Efficientdet: Scalable and efficient object detection.<span id="bib.bib59.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib59.14.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition Proceedings of the ieee/cvf conference on computer
vision and pattern recognition (<span id="bib.bib59.15.2" class="ltx_ERROR undefined">\BPGS</span> 10781–10790).
<span id="bib.bib59.16.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib59.17.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vasconcelos <span id="bib.bib60.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib60.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib60.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>vasconcelos2022proper<span id="bib.bib60.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Vasconcelos, C., Birodkar, V.<span id="bib.bib60.9.3" class="ltx_ERROR undefined">\BCBL</span> Dumoulin, V. 
</span>
<span class="ltx_bibblock"><span id="bib.bib60.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2022.

</span>
<span class="ltx_bibblock"><span id="bib.bib60.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib60.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Proper Reuse of Image Classification Features Improves
Object Detection Proper reuse of image classification features improves
object detection.<span id="bib.bib60.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib60.14.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Proceedings of the ieee/cvf conference on computer
vision and pattern recognition (<span id="bib.bib60.15.2" class="ltx_ERROR undefined">\BPGS</span> 13628–13637).
<span id="bib.bib60.16.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib60.17.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vu <span id="bib.bib61.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib61.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2019)</span>
<span class="ltx_bibblock">
<span id="bib.bib61.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>vu2019cascade<span id="bib.bib61.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Vu, T., Jang, H., Pham, T.X.<span id="bib.bib61.9.3" class="ltx_ERROR undefined">\BCBL</span> Yoo, C. 
</span>
<span class="ltx_bibblock"><span id="bib.bib61.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2019.

</span>
<span class="ltx_bibblock"><span id="bib.bib61.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib61.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Cascade rpn: Delving into high-quality region proposal
network with adaptive convolution Cascade rpn: Delving into high-quality
region proposal network with adaptive convolution.<span id="bib.bib61.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib61.14.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span>Advances in neural information processing
systems32.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><span id="bib.bib61.15.1" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib61.16.2" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang <span id="bib.bib62.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib62.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2019)</span>
<span class="ltx_bibblock">
<span id="bib.bib62.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>wang2019towards<span id="bib.bib62.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Wang, X., Cai, Z., Gao, D.<span id="bib.bib62.9.3" class="ltx_ERROR undefined">\BCBL</span> Vasconcelos, N. 
</span>
<span class="ltx_bibblock"><span id="bib.bib62.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2019.

</span>
<span class="ltx_bibblock"><span id="bib.bib62.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib62.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Towards universal object detection by domain attention
Towards universal object detection by domain attention.<span id="bib.bib62.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib62.14.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition Proceedings of the ieee/cvf conference on computer
vision and pattern recognition (<span id="bib.bib62.15.2" class="ltx_ERROR undefined">\BPGS</span> 7289–7298).
<span id="bib.bib62.16.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib62.17.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu <span id="bib.bib63.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib63.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib63.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>xu2020universal<span id="bib.bib63.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Xu, H., Fang, L., Liang, X., Kang, W.<span id="bib.bib63.9.3" class="ltx_ERROR undefined">\BCBL</span> Li, Z. 
</span>
<span class="ltx_bibblock"><span id="bib.bib63.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2020.

</span>
<span class="ltx_bibblock"><span id="bib.bib63.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib63.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Universal-rcnn: Universal object detector via
transferable graph r-cnn Universal-rcnn: Universal object detector via
transferable graph r-cnn.<span id="bib.bib63.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib63.14.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the AAAI Conference on Artificial Intelligence
Proceedings of the aaai conference on artificial intelligence (<span id="bib.bib63.15.2" class="ltx_ERROR undefined">\BVOL</span> 34,
<span id="bib.bib63.16.3" class="ltx_ERROR undefined">\BPGS</span> 12492–12499).
<span id="bib.bib63.17.4" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib63.18.5" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu <span id="bib.bib64.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib64.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib64.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>xu2022seed<span id="bib.bib64.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Xu, H., Zhang, X., Li, H., Xie, L., Dai, W., Xiong, H.<span id="bib.bib64.8.3" class="ltx_ERROR undefined">\BCBL</span> Tian, Q. 
</span>
<span class="ltx_bibblock"><span id="bib.bib64.9.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2022.

</span>
<span class="ltx_bibblock"><span id="bib.bib64.10.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib64.11.2" class="ltx_ERROR undefined">\APACrefatitle</span>Seed the views: Hierarchical semantic alignment for
contrastive representation learning Seed the views: Hierarchical semantic
alignment for contrastive representation learning.<span id="bib.bib64.12.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib64.13.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span>IEEE Transactions on Pattern Analysis and Machine
Intelligence.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><span id="bib.bib64.14.1" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib64.15.2" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu <span id="bib.bib65.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib65.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2016)</span>
<span class="ltx_bibblock">
<span id="bib.bib65.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>yu2016unitbox<span id="bib.bib65.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Yu, J., Jiang, Y., Wang, Z., Cao, Z.<span id="bib.bib65.9.3" class="ltx_ERROR undefined">\BCBL</span> Huang, T. 
</span>
<span class="ltx_bibblock"><span id="bib.bib65.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2016.

</span>
<span class="ltx_bibblock"><span id="bib.bib65.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib65.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Unitbox: An advanced object detection network Unitbox:
An advanced object detection network.<span id="bib.bib65.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib65.14.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the 24th ACM international conference on
Multimedia Proceedings of the 24th acm international conference on
multimedia (<span id="bib.bib65.15.2" class="ltx_ERROR undefined">\BPGS</span> 516–520).
<span id="bib.bib65.16.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib65.17.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan <span id="bib.bib66.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib66.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2021)</span>
<span class="ltx_bibblock">
<span id="bib.bib66.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>yuan2021florence<span id="bib.bib66.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Yuan, L., Chen, D., Chen, Y<span id="bib.bib66.8.3" class="ltx_ERROR undefined">\BHBI</span>L., Codella, N., Dai, X., Gao, J.<span id="bib.bib66.9.4" class="ltx_ERROR undefined">\BDBL</span>others 
</span>
<span class="ltx_bibblock"><span id="bib.bib66.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2021.

</span>
<span class="ltx_bibblock"><span id="bib.bib66.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib66.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Florence: A new foundation model for computer vision
Florence: A new foundation model for computer vision.<span id="bib.bib66.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib66.14.1" class="ltx_ERROR undefined">\APACjournalVolNumPages</span>arXiv preprint arXiv:2111.11432.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><span id="bib.bib66.15.1" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib66.16.2" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang <span id="bib.bib67.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib67.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2017)</span>
<span class="ltx_bibblock">
<span id="bib.bib67.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>zhang2017citypersons<span id="bib.bib67.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Zhang, S., Benenson, R.<span id="bib.bib67.9.3" class="ltx_ERROR undefined">\BCBL</span> Schiele, B. 
</span>
<span class="ltx_bibblock"><span id="bib.bib67.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2017.

</span>
<span class="ltx_bibblock"><span id="bib.bib67.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib67.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Citypersons: A diverse dataset for pedestrian detection
Citypersons: A diverse dataset for pedestrian detection.<span id="bib.bib67.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib67.14.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE conference on computer vision and
pattern recognition Proceedings of the ieee conference on computer vision
and pattern recognition (<span id="bib.bib67.15.2" class="ltx_ERROR undefined">\BPGS</span> 3213–3221).
<span id="bib.bib67.16.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib67.17.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao <span id="bib.bib68.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib68.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2020)</span>
<span class="ltx_bibblock">
<span id="bib.bib68.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>zhao2020object<span id="bib.bib68.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Zhao, X., Schulter, S., Sharma, G., Tsai, Y<span id="bib.bib68.8.3" class="ltx_ERROR undefined">\BHBI</span>H., Chandraker, M.<span id="bib.bib68.9.4" class="ltx_ERROR undefined">\BCBL</span> Wu, Y. 
</span>
<span class="ltx_bibblock"><span id="bib.bib68.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2020.

</span>
<span class="ltx_bibblock"><span id="bib.bib68.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib68.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Object detection with a unified label space from
multiple datasets Object detection with a unified label space from multiple
datasets.<span id="bib.bib68.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib68.14.1" class="ltx_ERROR undefined">\APACrefbtitle</span>European Conference on Computer Vision European conference
on computer vision (<span id="bib.bib68.15.2" class="ltx_ERROR undefined">\BPGS</span> 178–193).
<span id="bib.bib68.16.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib68.17.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">B. Zhou <span id="bib.bib69.4.4.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib69.5.5.2" class="ltx_ERROR undefined">\APACyear</span>2017)</span>
<span class="ltx_bibblock">
<span id="bib.bib69.6.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>zhou2017scene<span id="bib.bib69.7.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A.<span id="bib.bib69.8.3" class="ltx_ERROR undefined">\BCBL</span> Torralba, A. 
</span>
<span class="ltx_bibblock"><span id="bib.bib69.9.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2017.

</span>
<span class="ltx_bibblock"><span id="bib.bib69.10.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib69.11.2" class="ltx_ERROR undefined">\APACrefatitle</span>Scene parsing through ade20k dataset Scene parsing
through ade20k dataset.<span id="bib.bib69.12.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib69.13.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE conference on computer vision and
pattern recognition Proceedings of the ieee conference on computer vision
and pattern recognition (<span id="bib.bib69.14.2" class="ltx_ERROR undefined">\BPGS</span> 633–641).
<span id="bib.bib69.15.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib69.16.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">X. Zhou <span id="bib.bib70.5.5.1" class="ltx_ERROR undefined">\BOthers</span>. (<span id="bib.bib70.6.6.2" class="ltx_ERROR undefined">\APACyear</span>2022)</span>
<span class="ltx_bibblock">
<span id="bib.bib70.7.1" class="ltx_ERROR undefined">\APACinsertmetastar</span>zhou2022simple<span id="bib.bib70.8.2" class="ltx_ERROR undefined">{APACrefauthors}</span>Zhou, X., Koltun, V.<span id="bib.bib70.9.3" class="ltx_ERROR undefined">\BCBL</span> Krähenbühl, P. 
</span>
<span class="ltx_bibblock"><span id="bib.bib70.10.1" class="ltx_ERROR undefined">\APACrefYearMonthDay</span>2022.

</span>
<span class="ltx_bibblock"><span id="bib.bib70.11.1" class="ltx_ERROR undefined">\BBOQ</span><span id="bib.bib70.12.2" class="ltx_ERROR undefined">\APACrefatitle</span>Simple multi-dataset detection Simple multi-dataset
detection.<span id="bib.bib70.13.3" class="ltx_ERROR undefined">\BBCQ</span>

</span>
<span class="ltx_bibblock"><span id="bib.bib70.14.1" class="ltx_ERROR undefined">\APACrefbtitle</span>Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Proceedings of the ieee/cvf conference on computer
vision and pattern recognition (<span id="bib.bib70.15.2" class="ltx_ERROR undefined">\BPGS</span> 7571–7580).
<span id="bib.bib70.16.3" class="ltx_ERROR undefined">\PrintBackRefs</span><span id="bib.bib70.17.4" class="ltx_ERROR undefined">\CurrentBib</span>

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="Ax1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">Appendix</h2>

</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>RVC Submission</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">We provide an in-depth presentation of our RVC final submission, which deviates slightly from the configuration outlined in the main text. Our final RVC submission employed a customized variant of the proposed detector built upon SEER-RegNet256gf. To align with the RVC deadline, specific simplifications were incorporated into the model:</p>
<ul id="A1.I1" class="ltx_itemize">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p id="A1.I1.i1.p1.8" class="ltx_p">Instead of utilizing the default setting of <math id="A1.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="C_{2}" display="inline"><semantics id="A1.I1.i1.p1.1.m1.1a"><msub id="A1.I1.i1.p1.1.m1.1.1" xref="A1.I1.i1.p1.1.m1.1.1.cmml"><mi id="A1.I1.i1.p1.1.m1.1.1.2" xref="A1.I1.i1.p1.1.m1.1.1.2.cmml">C</mi><mn id="A1.I1.i1.p1.1.m1.1.1.3" xref="A1.I1.i1.p1.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A1.I1.i1.p1.1.m1.1b"><apply id="A1.I1.i1.p1.1.m1.1.1.cmml" xref="A1.I1.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A1.I1.i1.p1.1.m1.1.1.1.cmml" xref="A1.I1.i1.p1.1.m1.1.1">subscript</csymbol><ci id="A1.I1.i1.p1.1.m1.1.1.2.cmml" xref="A1.I1.i1.p1.1.m1.1.1.2">𝐶</ci><cn type="integer" id="A1.I1.i1.p1.1.m1.1.1.3.cmml" xref="A1.I1.i1.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.I1.i1.p1.1.m1.1c">C_{2}</annotation></semantics></math>, <math id="A1.I1.i1.p1.2.m2.1" class="ltx_Math" alttext="C_{3}" display="inline"><semantics id="A1.I1.i1.p1.2.m2.1a"><msub id="A1.I1.i1.p1.2.m2.1.1" xref="A1.I1.i1.p1.2.m2.1.1.cmml"><mi id="A1.I1.i1.p1.2.m2.1.1.2" xref="A1.I1.i1.p1.2.m2.1.1.2.cmml">C</mi><mn id="A1.I1.i1.p1.2.m2.1.1.3" xref="A1.I1.i1.p1.2.m2.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="A1.I1.i1.p1.2.m2.1b"><apply id="A1.I1.i1.p1.2.m2.1.1.cmml" xref="A1.I1.i1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A1.I1.i1.p1.2.m2.1.1.1.cmml" xref="A1.I1.i1.p1.2.m2.1.1">subscript</csymbol><ci id="A1.I1.i1.p1.2.m2.1.1.2.cmml" xref="A1.I1.i1.p1.2.m2.1.1.2">𝐶</ci><cn type="integer" id="A1.I1.i1.p1.2.m2.1.1.3.cmml" xref="A1.I1.i1.p1.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.I1.i1.p1.2.m2.1c">C_{3}</annotation></semantics></math>, <math id="A1.I1.i1.p1.3.m3.1" class="ltx_Math" alttext="C_{4}" display="inline"><semantics id="A1.I1.i1.p1.3.m3.1a"><msub id="A1.I1.i1.p1.3.m3.1.1" xref="A1.I1.i1.p1.3.m3.1.1.cmml"><mi id="A1.I1.i1.p1.3.m3.1.1.2" xref="A1.I1.i1.p1.3.m3.1.1.2.cmml">C</mi><mn id="A1.I1.i1.p1.3.m3.1.1.3" xref="A1.I1.i1.p1.3.m3.1.1.3.cmml">4</mn></msub><annotation-xml encoding="MathML-Content" id="A1.I1.i1.p1.3.m3.1b"><apply id="A1.I1.i1.p1.3.m3.1.1.cmml" xref="A1.I1.i1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="A1.I1.i1.p1.3.m3.1.1.1.cmml" xref="A1.I1.i1.p1.3.m3.1.1">subscript</csymbol><ci id="A1.I1.i1.p1.3.m3.1.1.2.cmml" xref="A1.I1.i1.p1.3.m3.1.1.2">𝐶</ci><cn type="integer" id="A1.I1.i1.p1.3.m3.1.1.3.cmml" xref="A1.I1.i1.p1.3.m3.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.I1.i1.p1.3.m3.1c">C_{4}</annotation></semantics></math>, <math id="A1.I1.i1.p1.4.m4.1" class="ltx_Math" alttext="C_{5}" display="inline"><semantics id="A1.I1.i1.p1.4.m4.1a"><msub id="A1.I1.i1.p1.4.m4.1.1" xref="A1.I1.i1.p1.4.m4.1.1.cmml"><mi id="A1.I1.i1.p1.4.m4.1.1.2" xref="A1.I1.i1.p1.4.m4.1.1.2.cmml">C</mi><mn id="A1.I1.i1.p1.4.m4.1.1.3" xref="A1.I1.i1.p1.4.m4.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="A1.I1.i1.p1.4.m4.1b"><apply id="A1.I1.i1.p1.4.m4.1.1.cmml" xref="A1.I1.i1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="A1.I1.i1.p1.4.m4.1.1.1.cmml" xref="A1.I1.i1.p1.4.m4.1.1">subscript</csymbol><ci id="A1.I1.i1.p1.4.m4.1.1.2.cmml" xref="A1.I1.i1.p1.4.m4.1.1.2">𝐶</ci><cn type="integer" id="A1.I1.i1.p1.4.m4.1.1.3.cmml" xref="A1.I1.i1.p1.4.m4.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.I1.i1.p1.4.m4.1c">C_{5}</annotation></semantics></math> in NAS-FPN, we opted to employ the side-outputs <math id="A1.I1.i1.p1.5.m5.1" class="ltx_Math" alttext="C_{3}" display="inline"><semantics id="A1.I1.i1.p1.5.m5.1a"><msub id="A1.I1.i1.p1.5.m5.1.1" xref="A1.I1.i1.p1.5.m5.1.1.cmml"><mi id="A1.I1.i1.p1.5.m5.1.1.2" xref="A1.I1.i1.p1.5.m5.1.1.2.cmml">C</mi><mn id="A1.I1.i1.p1.5.m5.1.1.3" xref="A1.I1.i1.p1.5.m5.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="A1.I1.i1.p1.5.m5.1b"><apply id="A1.I1.i1.p1.5.m5.1.1.cmml" xref="A1.I1.i1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="A1.I1.i1.p1.5.m5.1.1.1.cmml" xref="A1.I1.i1.p1.5.m5.1.1">subscript</csymbol><ci id="A1.I1.i1.p1.5.m5.1.1.2.cmml" xref="A1.I1.i1.p1.5.m5.1.1.2">𝐶</ci><cn type="integer" id="A1.I1.i1.p1.5.m5.1.1.3.cmml" xref="A1.I1.i1.p1.5.m5.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.I1.i1.p1.5.m5.1c">C_{3}</annotation></semantics></math>, <math id="A1.I1.i1.p1.6.m6.1" class="ltx_Math" alttext="C_{4}" display="inline"><semantics id="A1.I1.i1.p1.6.m6.1a"><msub id="A1.I1.i1.p1.6.m6.1.1" xref="A1.I1.i1.p1.6.m6.1.1.cmml"><mi id="A1.I1.i1.p1.6.m6.1.1.2" xref="A1.I1.i1.p1.6.m6.1.1.2.cmml">C</mi><mn id="A1.I1.i1.p1.6.m6.1.1.3" xref="A1.I1.i1.p1.6.m6.1.1.3.cmml">4</mn></msub><annotation-xml encoding="MathML-Content" id="A1.I1.i1.p1.6.m6.1b"><apply id="A1.I1.i1.p1.6.m6.1.1.cmml" xref="A1.I1.i1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="A1.I1.i1.p1.6.m6.1.1.1.cmml" xref="A1.I1.i1.p1.6.m6.1.1">subscript</csymbol><ci id="A1.I1.i1.p1.6.m6.1.1.2.cmml" xref="A1.I1.i1.p1.6.m6.1.1.2">𝐶</ci><cn type="integer" id="A1.I1.i1.p1.6.m6.1.1.3.cmml" xref="A1.I1.i1.p1.6.m6.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.I1.i1.p1.6.m6.1c">C_{4}</annotation></semantics></math>, <math id="A1.I1.i1.p1.7.m7.1" class="ltx_Math" alttext="C_{5}" display="inline"><semantics id="A1.I1.i1.p1.7.m7.1a"><msub id="A1.I1.i1.p1.7.m7.1.1" xref="A1.I1.i1.p1.7.m7.1.1.cmml"><mi id="A1.I1.i1.p1.7.m7.1.1.2" xref="A1.I1.i1.p1.7.m7.1.1.2.cmml">C</mi><mn id="A1.I1.i1.p1.7.m7.1.1.3" xref="A1.I1.i1.p1.7.m7.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="A1.I1.i1.p1.7.m7.1b"><apply id="A1.I1.i1.p1.7.m7.1.1.cmml" xref="A1.I1.i1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="A1.I1.i1.p1.7.m7.1.1.1.cmml" xref="A1.I1.i1.p1.7.m7.1.1">subscript</csymbol><ci id="A1.I1.i1.p1.7.m7.1.1.2.cmml" xref="A1.I1.i1.p1.7.m7.1.1.2">𝐶</ci><cn type="integer" id="A1.I1.i1.p1.7.m7.1.1.3.cmml" xref="A1.I1.i1.p1.7.m7.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.I1.i1.p1.7.m7.1c">C_{5}</annotation></semantics></math> and applied a <math id="A1.I1.i1.p1.8.m8.1" class="ltx_math_unparsed" alttext="2\times" display="inline"><semantics id="A1.I1.i1.p1.8.m8.1a"><mrow id="A1.I1.i1.p1.8.m8.1b"><mn id="A1.I1.i1.p1.8.m8.1.1">2</mn><mo lspace="0.222em" id="A1.I1.i1.p1.8.m8.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="A1.I1.i1.p1.8.m8.1c">2\times</annotation></semantics></math> downsampling on C5 twice, resulting in a 5-level feature pyramid. While this simplification led to a reduction in accuracy for detecting small objects, it significantly accelerated the training process.</p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p id="A1.I1.i2.p1.1" class="ltx_p">The basic anchor scale in Cascade RPN was reduced to 5.04 (<math id="A1.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="4\times 2^{1/3}" display="inline"><semantics id="A1.I1.i2.p1.1.m1.1a"><mrow id="A1.I1.i2.p1.1.m1.1.1" xref="A1.I1.i2.p1.1.m1.1.1.cmml"><mn id="A1.I1.i2.p1.1.m1.1.1.2" xref="A1.I1.i2.p1.1.m1.1.1.2.cmml">4</mn><mo lspace="0.222em" rspace="0.222em" id="A1.I1.i2.p1.1.m1.1.1.1" xref="A1.I1.i2.p1.1.m1.1.1.1.cmml">×</mo><msup id="A1.I1.i2.p1.1.m1.1.1.3" xref="A1.I1.i2.p1.1.m1.1.1.3.cmml"><mn id="A1.I1.i2.p1.1.m1.1.1.3.2" xref="A1.I1.i2.p1.1.m1.1.1.3.2.cmml">2</mn><mrow id="A1.I1.i2.p1.1.m1.1.1.3.3" xref="A1.I1.i2.p1.1.m1.1.1.3.3.cmml"><mn id="A1.I1.i2.p1.1.m1.1.1.3.3.2" xref="A1.I1.i2.p1.1.m1.1.1.3.3.2.cmml">1</mn><mo id="A1.I1.i2.p1.1.m1.1.1.3.3.1" xref="A1.I1.i2.p1.1.m1.1.1.3.3.1.cmml">/</mo><mn id="A1.I1.i2.p1.1.m1.1.1.3.3.3" xref="A1.I1.i2.p1.1.m1.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A1.I1.i2.p1.1.m1.1b"><apply id="A1.I1.i2.p1.1.m1.1.1.cmml" xref="A1.I1.i2.p1.1.m1.1.1"><times id="A1.I1.i2.p1.1.m1.1.1.1.cmml" xref="A1.I1.i2.p1.1.m1.1.1.1"></times><cn type="integer" id="A1.I1.i2.p1.1.m1.1.1.2.cmml" xref="A1.I1.i2.p1.1.m1.1.1.2">4</cn><apply id="A1.I1.i2.p1.1.m1.1.1.3.cmml" xref="A1.I1.i2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="A1.I1.i2.p1.1.m1.1.1.3.1.cmml" xref="A1.I1.i2.p1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="A1.I1.i2.p1.1.m1.1.1.3.2.cmml" xref="A1.I1.i2.p1.1.m1.1.1.3.2">2</cn><apply id="A1.I1.i2.p1.1.m1.1.1.3.3.cmml" xref="A1.I1.i2.p1.1.m1.1.1.3.3"><divide id="A1.I1.i2.p1.1.m1.1.1.3.3.1.cmml" xref="A1.I1.i2.p1.1.m1.1.1.3.3.1"></divide><cn type="integer" id="A1.I1.i2.p1.1.m1.1.1.3.3.2.cmml" xref="A1.I1.i2.p1.1.m1.1.1.3.3.2">1</cn><cn type="integer" id="A1.I1.i2.p1.1.m1.1.1.3.3.3.cmml" xref="A1.I1.i2.p1.1.m1.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.I1.i2.p1.1.m1.1c">4\times 2^{1/3}</annotation></semantics></math>). This adjustment was made to align with the changes in NAS-FPN and to minimize instances of missed detections for small objects.</p>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p id="A1.I1.i3.p1.1" class="ltx_p">The model underwent training for 720k iterations, with a learning rate reduction of 0.1 applied at 600k iterations.</p>
</div>
</li>
</ul>
<p id="A1.p1.2" class="ltx_p">During the dataset-agnostic inference procedure, the Soft-NMS <cite class="ltx_cite ltx_citemacro_citep">(Bodla <span class="ltx_ERROR undefined">\BOthers</span>., <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_ERROR undefined">\APACyear</span>2017</a>)</cite> was performed with an IoU threshold of 0.6 and a score threshold of 0.001. Then,</p>
<ul id="A1.I2" class="ltx_itemize">
<li id="A1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i1.p1" class="ltx_para">
<p id="A1.I2.i1.p1.1" class="ltx_p">for COCO, we limited the maximum number of predictions per image to 100, and the short edge of the input image was resized to 800 pixels;</p>
</div>
</li>
<li id="A1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i2.p1" class="ltx_para">
<p id="A1.I2.i2.p1.1" class="ltx_p">for MVD, we limited the maximum number of predictions per image to 300, and the short edge of the input image was resized to 2048 pixels;</p>
</div>
</li>
<li id="A1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i3.p1" class="ltx_para">
<p id="A1.I2.i3.p1.1" class="ltx_p">for OID, we limited the maximum number of predictions per image to 300, and the short edge of the input image was resized to 800 pixels.</p>
</div>
</li>
</ul>
<p id="A1.p1.3" class="ltx_p">We did <span id="A1.p1.3.1" class="ltx_text ltx_font_bold">not</span> employ any advanced inference techniques, such as multi-scale test augmentation. The performance of our submission (IFFF_RVC) on the three datasets is summarized in Table <a href="#S4.T4" title="Table 4 ‣ 4.2 Implementation Details ‣ 4 Experiments ‣ Universal Object Detection with Large Vision Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. For comparison with the results presented in this paper, we evaluated the model for our RVC submission on the validation sets with a maximum of 300 predictions per image, using the standard Non-Maximum Suppression (NMS) method. All other testing configurations remained consistent with those used on the test sets.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2212.09407" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2212.09408" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2212.09408">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2212.09408" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2212.09409" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 08:59:00 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
