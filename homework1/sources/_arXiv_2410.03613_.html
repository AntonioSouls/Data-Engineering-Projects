<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation</title>
<!--Generated on Fri Oct  4 17:07:35 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.03613v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S1" title="In Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S2" title="In Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S2.SS1" title="In 2 Related Works ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Light-Weight LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S2.SS2" title="In 2 Related Works ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>On-device Deployment of LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S2.SS3" title="In 2 Related Works ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Existing Measurement Studies</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S3" title="In Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experimental Setup and Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S3.SS1" title="In 3 Experimental Setup and Methodology ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S3.SS2" title="In 3 Experimental Setup and Methodology ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Workflow and Methodology</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S4" title="In Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Performance: Users’ Perspectives</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S4.SS1" title="In 4 Performance: Users’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Token Throughput and Memory Footprints</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S4.SS1.SSS1" title="In 4.1 Token Throughput and Memory Footprints ‣ 4 Performance: Users’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Performance on CPUs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S4.SS1.SSS2" title="In 4.1 Token Throughput and Memory Footprints ‣ 4 Performance: Users’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Performance on GPUs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S4.SS1.SSS3" title="In 4.1 Token Throughput and Memory Footprints ‣ 4 Performance: Users’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>Performance on Specialized AI Accelerators</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S4.SS1.SSS4" title="In 4.1 Token Throughput and Memory Footprints ‣ 4 Performance: Users’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.4 </span>Comparisons and Summary</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S4.SS2" title="In 4 Performance: Users’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Battery Consumption</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5" title="In Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Performance: Developers’ Perspectives</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.SS1" title="In 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Hardware Capabilities</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.SS1.SSS1" title="In 5.1 Hardware Capabilities ‣ 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.1 </span>Multi-threads on CPU Cores</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.SS1.SSS2" title="In 5.1 Hardware Capabilities ‣ 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.2 </span>Speedup with Special Machine Instructions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.SS1.SSS3" title="In 5.1 Hardware Capabilities ‣ 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.3 </span>Utilization of GPU units</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.SS2" title="In 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Impact of DVFS</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.SS3" title="In 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Model Preparation Latency</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.SS4" title="In 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Inference Engines</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.SS4.SSS1" title="In 5.4 Inference Engines ‣ 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4.1 </span>llama.cpp</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.SS4.SSS2" title="In 5.4 Inference Engines ‣ 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4.2 </span>MLC LLM</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S6" title="In Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Implications</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S7" title="In Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S8" title="In Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Concluding Remarks</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:144%;">Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Jie Xiao
<br class="ltx_break"/>Sun Yat-sen University
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qianyi Huang
<br class="ltx_break"/>Sun Yat-sen University
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xu Chen
<br class="ltx_break"/>Sun Yat-sen University
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chen Tian 
<br class="ltx_break"/>Nanjing University
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">As large language models (LLMs) increasingly integrate into every aspect of our work and daily lives, there are growing concerns about user privacy, which push the trend toward local deployment of these models. There are a number of lightweight LLMs (e.g., Gemini Nano, LLAMA2 7B) that can run locally on smartphones, providing users with greater control over their personal data. As a rapidly emerging application, we are concerned about their performance on commercial-off-the-shelf mobile devices. To fully understand the current landscape of LLM deployment on mobile platforms, we conduct a comprehensive measurement study on mobile devices. We evaluate both metrics that affect user experience, including token throughput, latency, and battery consumption, as well as factors critical to developers, such as resource utilization, DVFS strategies, and inference engines.
In addition, we provide a detailed analysis of how these hardware capabilities and system dynamics affect on-device LLM performance, which may help developers identify and address bottlenecks for mobile LLM applications.
We also provide comprehensive comparisons across the mobile system-on-chips (SoCs) from major vendors, highlighting their performance differences in handling LLM workloads. We hope that this study can provide insights for both the development of on-device LLMs and the design for future mobile system architecture.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Over the past two years, Large Language Models (LLMs) have demonstrated exceptional performance across a wide range of domains. Their advanced capabilities in understanding human language and generating text have made them indispensable in various applications, including dialogue systems, machine translation, and text generation. Several companies have begun integrating LLMs as system services (e.g., Apple Intelligence <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib21" title="">21</a>]</cite>,
Copilot+ PC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib32" title="">32</a>]</cite>), where these models assist users in organizing daily routines, summarizing emails and messages, and even generating automated replies. However, while LLMs serve as powerful and appealing personal assistants, these models continuously monitor and analyze users’ activities, which leads to serious privacy concerns <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib19" title="">19</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To address the privacy concerns, local deployment of LLMs is becoming attractive, as it allows these models to perform inference directly on the device and no personal data are sent to cloud servers. In this way, users have greater control of their personal data. While LLMs, such as ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib2" title="">2</a>]</cite> and Claude <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib4" title="">4</a>]</cite>,
are typically deployed on the cloud, local deployment are becoming feasible for two reasons. On one hand, a series of lightweight LLMs (e.g., Llama2-7B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib41" title="">41</a>]</cite> and Mistral-7B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib22" title="">22</a>]</cite>) have been introduced. These models, with billions of parameters, are significantly smaller in size compared to cloud-based models, which typically have hundreds of billions of parameters. Despite their reduced size, these lightweight LLMs maintain a high level of expressiveness in both common-sense reasoning and domain-specific knowledge, and thus can fully satisfy the basic needs of mobile users. On the other hand, state-of-the-art mobile SoCs demonstrate significant improvement in computing capability and memory capacity. More importantly, a variety of specialized AI accelerators are integrated into the mobile processors, such as Graphics Processing Units (GPUs) and Neural Processing Units (NPUs). These hardware improvements make it possible to implement complex AI functions on mobile devices.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">As a rapidly emerging application, LLMs differ significantly from traditional mobile applications in terms of computational complexity and memory demands. Therefore, we are particularly concerned with their performance on commercial off-the-shelf (COTS) mobile devices, as well as the performance fluctuations caused by mobile operating systems and hardware architecture compatibility.
Several studies have taken the first steps to explore the current state of mobile LLM<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib25" title="">25</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib27" title="">27</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib34" title="">34</a>]</cite>. They have conducted preliminary assessments of model deployment and inference performance through the development of specialized benchmark suites.
However, they mainly focus on performance metrics such as token throughput and latency, without looking into the underlying hardware dynamics and system configurations, such as CPU/GPU utilization, number of threads, operating frequencies, etc.
Besides, there is a lack of analysis about how these hardware and system-level factors affect on-device LLM inference. This is essential for developers to identify and address bottleneck for mobile LLM applications.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, we aim to reveal the current state and potential of mobile hardware for supporting on-device LLMs by deploying LLMs across a diverse range of mobile devices. We select multiple high-tier SoCs from different vendors that represent currently available resources and development trends in mobile hardware. We analyze the differences in SoCs from major vendors and ultimately select SoCs represented by Qualcomm, HiSilicon, and MediaTek to perform local LLM inference. We deploy a <math alttext="7" class="ltx_Math" display="inline" id="S1.p4.1.m1.1"><semantics id="S1.p4.1.m1.1a"><mn id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><cn id="S1.p4.1.m1.1.1.cmml" type="integer" xref="S1.p4.1.m1.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">7</annotation><annotation encoding="application/x-llamapun" id="S1.p4.1.m1.1d">7</annotation></semantics></math>B model on mobile devices with llama.cpp <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib14" title="">14</a>]</cite> and MLC LLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib38" title="">38</a>]</cite>, which are the two popular mobile LLM inference engines. During inference, we collect comprehensive metrics with specific profilers including Snapdragon Profiler <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib35" title="">35</a>]</cite> and Arm Streamline <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib5" title="">5</a>]</cite> to make sure that all the data accurately reflect the hardware performance. We investigate how the hardware specifications, including big.LITTLE cores<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib6" title="">6</a>]</cite>, cache size, and memory bandwidth, affect the inference performance, which is essential to help the development of future mobile SoCs to accelerate LLMs.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">From our study, we have the following key observations:</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">While LLMs are typically considered to be resource-intensive, the current implementation does not fully explore the potential of mobile processors. It only utilizes <math alttext="5\%-20\%" class="ltx_Math" display="inline" id="S1.I1.i1.p1.1.m1.1"><semantics id="S1.I1.i1.p1.1.m1.1a"><mrow id="S1.I1.i1.p1.1.m1.1.1" xref="S1.I1.i1.p1.1.m1.1.1.cmml"><mrow id="S1.I1.i1.p1.1.m1.1.1.2" xref="S1.I1.i1.p1.1.m1.1.1.2.cmml"><mn id="S1.I1.i1.p1.1.m1.1.1.2.2" xref="S1.I1.i1.p1.1.m1.1.1.2.2.cmml">5</mn><mo id="S1.I1.i1.p1.1.m1.1.1.2.1" xref="S1.I1.i1.p1.1.m1.1.1.2.1.cmml">%</mo></mrow><mo id="S1.I1.i1.p1.1.m1.1.1.1" xref="S1.I1.i1.p1.1.m1.1.1.1.cmml">−</mo><mrow id="S1.I1.i1.p1.1.m1.1.1.3" xref="S1.I1.i1.p1.1.m1.1.1.3.cmml"><mn id="S1.I1.i1.p1.1.m1.1.1.3.2" xref="S1.I1.i1.p1.1.m1.1.1.3.2.cmml">20</mn><mo id="S1.I1.i1.p1.1.m1.1.1.3.1" xref="S1.I1.i1.p1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.I1.i1.p1.1.m1.1b"><apply id="S1.I1.i1.p1.1.m1.1.1.cmml" xref="S1.I1.i1.p1.1.m1.1.1"><minus id="S1.I1.i1.p1.1.m1.1.1.1.cmml" xref="S1.I1.i1.p1.1.m1.1.1.1"></minus><apply id="S1.I1.i1.p1.1.m1.1.1.2.cmml" xref="S1.I1.i1.p1.1.m1.1.1.2"><csymbol cd="latexml" id="S1.I1.i1.p1.1.m1.1.1.2.1.cmml" xref="S1.I1.i1.p1.1.m1.1.1.2.1">percent</csymbol><cn id="S1.I1.i1.p1.1.m1.1.1.2.2.cmml" type="integer" xref="S1.I1.i1.p1.1.m1.1.1.2.2">5</cn></apply><apply id="S1.I1.i1.p1.1.m1.1.1.3.cmml" xref="S1.I1.i1.p1.1.m1.1.1.3"><csymbol cd="latexml" id="S1.I1.i1.p1.1.m1.1.1.3.1.cmml" xref="S1.I1.i1.p1.1.m1.1.1.3.1">percent</csymbol><cn id="S1.I1.i1.p1.1.m1.1.1.3.2.cmml" type="integer" xref="S1.I1.i1.p1.1.m1.1.1.3.2">20</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i1.p1.1.m1.1c">5\%-20\%</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i1.p1.1.m1.1d">5 % - 20 %</annotation></semantics></math> of the arithmetic units on mobile GPUs.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Processors with superior hardware specifications may not necessarily exhibit better performance (e.g., Mali-G720 Immortalis MP12 vs Adreno 750). In addition, the Adreno GPUs consistently outperform the Mali GPUs in overall performance.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Given the "big.LITTLE" architecture of mobile CPUs, in most cases, setting the number of threads equal to the number of big cores (prime and performance cores) can achieve the optimal performance. Adding more cores (efficiency cores) may degrade the performance.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">Specialized machine instructions, such as <span class="ltx_text ltx_font_italic" id="S1.I1.i4.p1.1.1">smmla</span> and <span class="ltx_text ltx_font_italic" id="S1.I1.i4.p1.1.2">sdot</span> in Arm architecture, significantly enhance LLM performance. By carefully rearranging the weight matrix, these instructions can yield up to a <math alttext="4\times" class="ltx_math_unparsed" display="inline" id="S1.I1.i4.p1.1.m1.1"><semantics id="S1.I1.i4.p1.1.m1.1a"><mrow id="S1.I1.i4.p1.1.m1.1b"><mn id="S1.I1.i4.p1.1.m1.1.1">4</mn><mo id="S1.I1.i4.p1.1.m1.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S1.I1.i4.p1.1.m1.1c">4\times</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i4.p1.1.m1.1d">4 ×</annotation></semantics></math> speed improvement.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S1.I1.i5.p1">
<p class="ltx_p" id="S1.I1.i5.p1.1">Specialized AI accelerators, such as the Hexagon NPU, demonstrate a remarkable prefill speed, representing a <math alttext="50" class="ltx_Math" display="inline" id="S1.I1.i5.p1.1.m1.1"><semantics id="S1.I1.i5.p1.1.m1.1a"><mn id="S1.I1.i5.p1.1.m1.1.1" xref="S1.I1.i5.p1.1.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S1.I1.i5.p1.1.m1.1b"><cn id="S1.I1.i5.p1.1.m1.1.1.cmml" type="integer" xref="S1.I1.i5.p1.1.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i5.p1.1.m1.1c">50</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i5.p1.1.m1.1d">50</annotation></semantics></math>-fold increase compared to CPU and GPU. However, their performance in decoding does not exhibit a similar level of enhancement.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S1.p5.2">We summarize our contributions as follows:</p>
<ol class="ltx_enumerate" id="S1.I2">
<li class="ltx_item" id="S1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I2.i1.p1">
<p class="ltx_p" id="S1.I2.i1.p1.1">We present a comprehensive measurement study of LLM inference on mobile platforms. We present not only metrics relevant to user experience, but also hardware and system characteristics that are critical to developers.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I2.i2.p1">
<p class="ltx_p" id="S1.I2.i2.p1.1">We provide a thorough analysis of how hardware attributes and system dynamics impact on-device LLM performance and highlights potential bottlenecks in mobile LLM applications.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I2.i3.p1">
<p class="ltx_p" id="S1.I2.i3.p1.1">Based on our measurement results, we propose several potential directions to accelerate on-device LLM inference.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S1.p5.3">The rest of the paper is organized as follows. In Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S2" title="2 Related Works ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">2</span></a>, we present preliminaries and related works on this topic. In Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S3" title="3 Experimental Setup and Methodology ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">3</span></a>, we describe our measurement setup and methodology. In Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S4" title="4 Performance: Users’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">4</span></a>, we present performance metrics that are of primary concern to users, while Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5" title="5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">5</span></a> discusses findings related to the underlying hardware characteristics and inference framework, which may interest developers. We discuss potential improvements in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S6" title="6 Implications ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">6</span></a> and explain the remaining issues in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S7" title="7 Discussion ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">7</span></a>. Finally, we conclude the paper in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S8" title="8 Concluding Remarks ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Light-Weight LLMs</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The number of LLMs has increased significantly in recent years. Numerous studies have explored the potential performance limits of these models by leveraging scaling laws <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib23" title="">23</a>]</cite> to increase their size. However, given the severe constraints of mobile platforms, it is crucial to compress these models to ensure that they are lightweight enough for deployment on edge devices.
Current methods for compressing large models usually fall into the categories of pruning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib29" title="">29</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib42" title="">42</a>]</cite>, distillation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib40" title="">40</a>]</cite>, quantization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib13" title="">13</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib28" title="">28</a>]</cite>, and low-order decomposition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib45" title="">45</a>]</cite>. Quantization is a widely used resource-efficient technique for compressing LLMs, reducing their memory and bandwidth requirements. The nature of quantization is a mapping from floating-point numbers to integers, especially the 4-bit integer quantization for mobile LLMs, which is recognized as the optimal compromise between model size and accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib27" title="">27</a>]</cite>. For example, based on the second-order information for weight updating, GPTQ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib13" title="">13</a>]</cite> employs layer-wise quantization to minimize the increase in global loss. K-quant <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib15" title="">15</a>]</cite> is a group-quantization method that divides the weight matrix into multiple <math alttext="16\times 8" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.1"><semantics id="S2.SS1.p1.1.m1.1a"><mrow id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml"><mn id="S2.SS1.p1.1.m1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.2.cmml">16</mn><mo id="S2.SS1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.SS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S2.SS1.p1.1.m1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"><times id="S2.SS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1"></times><cn id="S2.SS1.p1.1.m1.1.1.2.cmml" type="integer" xref="S2.SS1.p1.1.m1.1.1.2">16</cn><cn id="S2.SS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S2.SS1.p1.1.m1.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">16\times 8</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.1d">16 × 8</annotation></semantics></math> blocks, performing min-max quantization within each block. Due to outliers and dequantization overhead, most of the quantization algorithms focus on weight quantization only. However, by treating outliers carefully and implementing effective kernels, SmoothQuant <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib43" title="">43</a>]</cite> and OdesseyLLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib26" title="">26</a>]</cite> have tried weight-activation co-quantization to further lower the costs of LLM inference.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">To optimize hardware resource utilization and accelerate inference, operator fusion and key-value cache are commonly used for on-device LLMs. Operator fusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib26" title="">26</a>]</cite> significantly improves computational efficiency by reducing the number of operations and eliminating the need to copy intermediate results between operations. Besides, as the inference process of generative LLM is autoregressive, implementing kv-cache can significantly reduce the computational cost by reusing key and value vectors across different positions. Although this increases memory requirements, the amount of cache space used is manageable due to single-batch inference and the relatively short context length typical of mobile applications. In addition, to address the quadratic cost of attention, MQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib37" title="">37</a>]</cite> and flash-attention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib11" title="">11</a>]</cite> have optimized existing algorithms to accelerate inference by minimizing the computational overhead involved in attention calculations.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">The researchers have attempted to develop lightweight LLM models with less than <math alttext="10" class="ltx_Math" display="inline" id="S2.SS1.p3.1.m1.1"><semantics id="S2.SS1.p3.1.m1.1a"><mn id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><cn id="S2.SS1.p3.1.m1.1.1.cmml" type="integer" xref="S2.SS1.p3.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">10</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p3.1.m1.1d">10</annotation></semantics></math> billion parameters and optimized them with these resource-efficient algorithms, which facilitate local LLM deployment. For example, Meta’s Llama 2 7B and Llama 3 8B models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib12" title="">12</a>]</cite>, after instruction fine-tuning, show effectiveness across various tasks. Built upon a similar block structure as Llama-2 but more lightweight, phi-3 models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib33" title="">33</a>]</cite> introduced by Microsoft including phi-3-mini with 3.8B parameter and phi-3-small with 7B parameter achieves a quality that seems on-par with GPT-3.5. Moreover, Google’s Gemini Nano <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib16" title="">16</a>]</cite>, specifically designed for mobile devices, comes in 1.8B and 3.25B sizes, which have been integrated into Pixel 8 Pro and can perform text summarization and provide intelligent responses locally.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>On-device Deployment of LLMs</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Traditional machine learning (ML) and deep learning (DL) models have established a rich and mature ecosystem for mobile platforms. For example, TFLite, a DL inference engine designed for mobile devices, offers a wide range of highly optimized operators. Additionally, some vendors such as Qualcomm have developed hardware-specific DL libraries, which enhance the efficiency of model deployment and inference. However, unlike convolutional models, which typically have only a few hundred MB of parameters, LLMs feature orders of magnitude more parameters, more complex architectures, and higher computational demands. This complexity necessitates the development of specialized engines or operators specifically designed for efficient LLM inference.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">There are some inference frameworks widely used including llama.cpp <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib14" title="">14</a>]</cite>, MLC LLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib38" title="">38</a>]</cite>, mnn-llm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib3" title="">3</a>]</cite>, mllm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib44" title="">44</a>]</cite>, etc. Among them, llama.cpp and mnn-llm are optimized for LLM inference on CPUs, while MLC LLM leverages the powerful computational capabilities of GPUs. Despite the availability of dedicated AI accelerators such as NPUs, APUs, and TPUs in the latest generation of mobile SoCs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib20" title="">20</a>]</cite>, developing for these accelerators presents challenges due to the typically closed-source nature of vendor-specific SDKs. Currently, only mllm and PowerInfer-2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib46" title="">46</a>]</cite> claim to support Qualcomm NPUs for accelerating LLM inference on mobile devices. The development of LLM solutions for AI accelerators with dedicated architectures is still an ongoing challenge for developers.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">In addition to those open-source LLM inference engines, mobile SoC vendors and manufacturers have also realized the importance of on-device LLM inference. Qualcomm, the vendor of Snapdragon SoCs, asserts that it can accelerate Llama-2 7B and Llama-3 8B models using the NPU on Snapdragon Gen2 and Gen3 platforms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib36" title="">36</a>]</cite>. Similarly, MediaTek has announced optimizations for Llama 2 Generative AI applications using its APU on the Dimensity 9300 and 8300 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib31" title="">31</a>]</cite>. However, both solutions remain closed-source and have yet to be validated through real-world applications.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Existing Measurement Studies</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Several studies have focused on the measurement and evaluation of mobile-optimized LLMs. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib25" title="">25</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib27" title="">27</a>]</cite> both develop comprehensive benchmark suites to gain insights into various performance metrics, including latency, memory footprint, and throughput, as well as the impact of model size and quantization precision on accuracy. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib25" title="">25</a>]</cite> considers a range of edge devices, from smartphones to Nvidia Jetson. In addition to throughput, it also investigates device power consumption over time. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib27" title="">27</a>]</cite> primarily compares the performance of 22 lightweight models, offering a detailed analysis on model structure and operators.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">However, none of these studies provide in-depth analysis about the impact of mobile hardware during inference, such as accelerator utilization, memory bandwidth, and fluctuations in CPU frequency. Although <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib27" title="">27</a>]</cite> touches on the impact of SoCs, it only examines the macro differences of SoCs from a single vendor. In contrast, our approach involves testing several SoCs from multiple vendors to gain a comprehensive understanding of how mainstream mobile processors support LLMs. This will help us identify hardware bottlenecks and potential future upgrades necessary for optimizing LLM performance on mobile devices.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Testing devices and their hardware specifications</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S2.T1.4" style="width:505.9pt;height:116.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-72.2pt,16.6pt) scale(0.777827072003465,0.777827072003465) ;">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.4.1">
<tr class="ltx_tr" id="S2.T1.4.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.4.1.1.1"><span class="ltx_text" id="S2.T1.4.1.1.1.1" style="font-size:90%;">Device</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.4.1.1.2"><span class="ltx_text" id="S2.T1.4.1.1.2.1" style="font-size:90%;">SoC</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.4.1.1.3"><span class="ltx_text" id="S2.T1.4.1.1.3.1" style="font-size:90%;">CPU</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.4.1.1.4"><span class="ltx_text" id="S2.T1.4.1.1.4.1" style="font-size:90%;">GPU</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.4.1.1.5"><span class="ltx_text" id="S2.T1.4.1.1.5.1" style="font-size:90%;">Avail./Total RAM</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.4.1.1.6"><span class="ltx_text" id="S2.T1.4.1.1.6.1" style="font-size:90%;">Type</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.4.1.1.7"><span class="ltx_text" id="S2.T1.4.1.1.7.1" style="font-size:90%;">OS</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.4.1.1.8"><span class="ltx_text" id="S2.T1.4.1.1.8.1" style="font-size:90%;">Release</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.4.1.2.1"><span class="ltx_text" id="S2.T1.4.1.2.1.1" style="font-size:90%;">XiaoMi14 Pro</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.4.1.2.2"><span class="ltx_text" id="S2.T1.4.1.2.2.1" style="font-size:90%;">Snapdragon 8 Gen3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.2.3"><span class="ltx_text" id="S2.T1.4.1.2.3.1" style="font-size:90%;">Cortex-X4/A720/720/A520</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.2.4"><span class="ltx_text" id="S2.T1.4.1.2.4.1" style="font-size:90%;">Adreno 750</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.2.5"><span class="ltx_text" id="S2.T1.4.1.2.5.1" style="font-size:90%;">12GB/16GB</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.2.6"><span class="ltx_text" id="S2.T1.4.1.2.6.1" style="font-size:90%;">top-tier</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.2.7"><span class="ltx_text" id="S2.T1.4.1.2.7.1" style="font-size:90%;">HyperOS</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.2.8"><span class="ltx_text" id="S2.T1.4.1.2.8.1" style="font-size:90%;">2023.10</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.4.1.3.1"><span class="ltx_text" id="S2.T1.4.1.3.1.1" style="font-size:90%;">XiaoMi Pad6 Pro</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.4.1.3.2"><span class="ltx_text" id="S2.T1.4.1.3.2.1" style="font-size:90%;">Snapdragon 8+ Gen1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.3.3"><span class="ltx_text" id="S2.T1.4.1.3.3.1" style="font-size:90%;">Cortex-X2/710/510</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.3.4"><span class="ltx_text" id="S2.T1.4.1.3.4.1" style="font-size:90%;">Adreno 730</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.3.5"><span class="ltx_text" id="S2.T1.4.1.3.5.1" style="font-size:90%;">12GB/16GB</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.3.6"><span class="ltx_text" id="S2.T1.4.1.3.6.1" style="font-size:90%;">top-tier</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.3.7"><span class="ltx_text" id="S2.T1.4.1.3.7.1" style="font-size:90%;">Android13</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.3.8"><span class="ltx_text" id="S2.T1.4.1.3.8.1" style="font-size:90%;">2022.5</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.1.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.4.1.4.1"><span class="ltx_text" id="S2.T1.4.1.4.1.1" style="font-size:90%;">Huawei Matepad11 Pro</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.4.1.4.2"><span class="ltx_text" id="S2.T1.4.1.4.2.1" style="font-size:90%;">Snapdragon 870</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.4.3"><span class="ltx_text" id="S2.T1.4.1.4.3.1" style="font-size:90%;">Cortex-A77/A77/A55</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.4.4"><span class="ltx_text" id="S2.T1.4.1.4.4.1" style="font-size:90%;">Adreno 650</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.4.5"><span class="ltx_text" id="S2.T1.4.1.4.5.1" style="font-size:90%;">5.5GB/8GB</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.4.6"><span class="ltx_text" id="S2.T1.4.1.4.6.1" style="font-size:90%;">mid-tier</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.4.7"><span class="ltx_text" id="S2.T1.4.1.4.7.1" style="font-size:90%;">Harmony 4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.4.8"><span class="ltx_text" id="S2.T1.4.1.4.8.1" style="font-size:90%;">2021.1</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.1.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.4.1.5.1"><span class="ltx_text" id="S2.T1.4.1.5.1.1" style="font-size:90%;">Vivo Pad3 Pro</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.4.1.5.2"><span class="ltx_text" id="S2.T1.4.1.5.2.1" style="font-size:90%;">Dimensity 9300</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.5.3"><span class="ltx_text" id="S2.T1.4.1.5.3.1" style="font-size:90%;">Cortex-X4/X4/A720</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.5.4">
<span class="ltx_text" id="S2.T1.4.1.5.4.1"></span><span class="ltx_text" id="S2.T1.4.1.5.4.2" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.4.1.5.4.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.4.1.5.4.3.1">
<span class="ltx_tr" id="S2.T1.4.1.5.4.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.4.1.5.4.3.1.1.1">Mali-G720</span></span>
<span class="ltx_tr" id="S2.T1.4.1.5.4.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.4.1.5.4.3.1.2.1">Immortalis MP12</span></span>
</span></span><span class="ltx_text" id="S2.T1.4.1.5.4.4"></span><span class="ltx_text" id="S2.T1.4.1.5.4.5" style="font-size:90%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.5.5"><span class="ltx_text" id="S2.T1.4.1.5.5.1" style="font-size:90%;">10GB/16GB</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.5.6"><span class="ltx_text" id="S2.T1.4.1.5.6.1" style="font-size:90%;">top-tier</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.5.7"><span class="ltx_text" id="S2.T1.4.1.5.7.1" style="font-size:90%;">Android 14</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.5.8"><span class="ltx_text" id="S2.T1.4.1.5.8.1" style="font-size:90%;">2023.11</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.1.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.4.1.6.1"><span class="ltx_text" id="S2.T1.4.1.6.1.1" style="font-size:90%;">Huawei Matepad12.6 Pro</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.4.1.6.2"><span class="ltx_text" id="S2.T1.4.1.6.2.1" style="font-size:90%;">Kirin 9000E</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.6.3"><span class="ltx_text" id="S2.T1.4.1.6.3.1" style="font-size:90%;">Cortex-A77/A77/A55</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.6.4"><span class="ltx_text" id="S2.T1.4.1.6.4.1" style="font-size:90%;">Mali-G78 MP22</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.6.5"><span class="ltx_text" id="S2.T1.4.1.6.5.1" style="font-size:90%;">8.5GB/12GB</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.6.6"><span class="ltx_text" id="S2.T1.4.1.6.6.1" style="font-size:90%;">high-tier</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.6.7"><span class="ltx_text" id="S2.T1.4.1.6.7.1" style="font-size:90%;">Harmony 4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.4.1.6.8"><span class="ltx_text" id="S2.T1.4.1.6.8.1" style="font-size:90%;">2020.10</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.1.7">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S2.T1.4.1.7.1"><span class="ltx_text" id="S2.T1.4.1.7.1.1" style="font-size:90%;">Huawei Nova7</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S2.T1.4.1.7.2"><span class="ltx_text" id="S2.T1.4.1.7.2.1" style="font-size:90%;">Kirin 985</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T1.4.1.7.3"><span class="ltx_text" id="S2.T1.4.1.7.3.1" style="font-size:90%;">Cortex-A76/A76/A55</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T1.4.1.7.4"><span class="ltx_text" id="S2.T1.4.1.7.4.1" style="font-size:90%;">Mali-G77 MP8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T1.4.1.7.5"><span class="ltx_text" id="S2.T1.4.1.7.5.1" style="font-size:90%;">4GB/8GB</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T1.4.1.7.6"><span class="ltx_text" id="S2.T1.4.1.7.6.1" style="font-size:90%;">mid-tier</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T1.4.1.7.7"><span class="ltx_text" id="S2.T1.4.1.7.7.1" style="font-size:90%;">Harmony 4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T1.4.1.7.8"><span class="ltx_text" id="S2.T1.4.1.7.8.1" style="font-size:90%;">2020.4</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Setup and Methodology</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Experimental Setup</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.1">Testing Devices.</span> Table 1 shows the devices used in our measurement. There are <math alttext="6" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mn id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><cn id="S3.SS1.p1.1.m1.1.1.cmml" type="integer" xref="S3.SS1.p1.1.m1.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">6</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">6</annotation></semantics></math> mobile devices from three SoC vendors: Snapdragon, MediaTek, and Hisillicon. All CPUs feature eight Arm Cortex cores but differ in implementation details such as big.LITTLE configurations and cache sizes. It is noteworthy that the Snapdragon 8 Gen3 and Dimensity 9300 represent the most advanced SoCs available for mobile devices. We also test devices ranging from high-tier to mid-tier to capture a broad spectrum of hardware heterogeneity. In terms of GPU, the vast majority of mobile GPUs are from Adreno (Qualcomm) and Mali (Arm). Thus, we designate Snapdragon SoCs as representatives of Adreno GPUs and the rest as representatives of Mali GPUs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.6"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.6.1">Testing Model.</span> We select Llama-2 <math alttext="7" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mn id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><cn id="S3.SS1.p2.1.m1.1.1.cmml" type="integer" xref="S3.SS1.p2.1.m1.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">7</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">7</annotation></semantics></math>B as a representative model for mobile-compatible LLMs. Although most mobile devices have RAM capacities ranging from <math alttext="8" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mn id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><cn id="S3.SS1.p2.2.m2.1.1.cmml" type="integer" xref="S3.SS1.p2.2.m2.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">8</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">8</annotation></semantics></math>GB to <math alttext="16" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mn id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><cn id="S3.SS1.p2.3.m3.1.1.cmml" type="integer" xref="S3.SS1.p2.3.m3.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">16</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">16</annotation></semantics></math>GB, the minimum effective available memory is around <math alttext="4" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.1"><semantics id="S3.SS1.p2.4.m4.1a"><mn id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><cn id="S3.SS1.p2.4.m4.1.1.cmml" type="integer" xref="S3.SS1.p2.4.m4.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">4</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m4.1d">4</annotation></semantics></math>GB due to the substantial portion occupied by the operating system. Consequently, we determine that the <math alttext="7" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m5.1"><semantics id="S3.SS1.p2.5.m5.1a"><mn id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><cn id="S3.SS1.p2.5.m5.1.1.cmml" type="integer" xref="S3.SS1.p2.5.m5.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">7</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.5.m5.1d">7</annotation></semantics></math>B model size is the upper limit that most mobile hardware can support. Additionally, we employ the <math alttext="4" class="ltx_Math" display="inline" id="S3.SS1.p2.6.m6.1"><semantics id="S3.SS1.p2.6.m6.1a"><mn id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><cn id="S3.SS1.p2.6.m6.1.1.cmml" type="integer" xref="S3.SS1.p2.6.m6.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">4</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.6.m6.1d">4</annotation></semantics></math>-bit quantization scheme, which is widely used and provides an optimal balance between model size and accuracy.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">Frameworks and Engines.</span> In this paper, we would like to reveal the performance of LLM on a variety of different accelerators. To achieve this, we select two representative frameworks: llama.cpp for CPU deployments and MLC LLM for GPU deployments, to assess the inference capabilities of general accelerators. For novel AI accelerators (e.g., NPU), rather than conducting our own device deployments, we refer to existing studies for results of mllm and PowerInfer-<math alttext="2" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.1"><semantics id="S3.SS1.p3.1.m1.1a"><mn id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><cn id="S3.SS1.p3.1.m1.1.1.cmml" type="integer" xref="S3.SS1.p3.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">2</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.1d">2</annotation></semantics></math>, which are only available for Qualcomm NPUs. These references will be discussed in subsequent sections.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">llama.cpp <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib14" title="">14</a>]</cite> is a pure C language project based on the GGUF machine learning tensor library. The model weights are quantized into lower accuracy by K-quant method and stored as binary files in GGUF format. After compiling the code into a specific back-end executable file, users can try different models for inference and only need to download the corresponding model weight. There is no need to rely on other environments.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">MLC LLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib38" title="">38</a>]</cite> is a high-performance LLM deployment engine built on the TVM machine learning compiler. Unlike llama.cpp, which focuses on optimizing for mobile CPUs, MLC LLM is designed to harness the power of mobile GPUs. Leveraging TVM, MLC LLM can automatically generate and optimize operators for a specific model and backend. These operators are then loaded and executed via the TVM Runtime. For Android-based GPUs, MLC LLM utilizes the OpenCL backend to ensure efficient execution.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.7"><span class="ltx_text ltx_font_bold" id="S3.SS1.p5.7.1">Prompts and Outputs.</span> To evaluate LLM performance in real-world scenarios, we use two types of prompts: a short prompt with <math alttext="64" class="ltx_Math" display="inline" id="S3.SS1.p5.1.m1.1"><semantics id="S3.SS1.p5.1.m1.1a"><mn id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><cn id="S3.SS1.p5.1.m1.1.1.cmml" type="integer" xref="S3.SS1.p5.1.m1.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">64</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.1.m1.1d">64</annotation></semantics></math> tokens and a long prompt with <math alttext="512" class="ltx_Math" display="inline" id="S3.SS1.p5.2.m2.1"><semantics id="S3.SS1.p5.2.m2.1a"><mn id="S3.SS1.p5.2.m2.1.1" xref="S3.SS1.p5.2.m2.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.1b"><cn id="S3.SS1.p5.2.m2.1.1.cmml" type="integer" xref="S3.SS1.p5.2.m2.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.1c">512</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.2.m2.1d">512</annotation></semantics></math> tokens. The short prompt simulates typical user queries, such as asking for the meaning of a phrase, while the long prompt is designed for scenarios requiring more context, such as summarizing a news article. For generation, llama.cpp has fixed output lengths of <math alttext="128" class="ltx_Math" display="inline" id="S3.SS1.p5.3.m3.1"><semantics id="S3.SS1.p5.3.m3.1a"><mn id="S3.SS1.p5.3.m3.1.1" xref="S3.SS1.p5.3.m3.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.3.m3.1b"><cn id="S3.SS1.p5.3.m3.1.1.cmml" type="integer" xref="S3.SS1.p5.3.m3.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.3.m3.1c">128</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.3.m3.1d">128</annotation></semantics></math> tokens for <math alttext="64" class="ltx_Math" display="inline" id="S3.SS1.p5.4.m4.1"><semantics id="S3.SS1.p5.4.m4.1a"><mn id="S3.SS1.p5.4.m4.1.1" xref="S3.SS1.p5.4.m4.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.4.m4.1b"><cn id="S3.SS1.p5.4.m4.1.1.cmml" type="integer" xref="S3.SS1.p5.4.m4.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.4.m4.1c">64</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.4.m4.1d">64</annotation></semantics></math>-token prompts and <math alttext="256" class="ltx_Math" display="inline" id="S3.SS1.p5.5.m5.1"><semantics id="S3.SS1.p5.5.m5.1a"><mn id="S3.SS1.p5.5.m5.1.1" xref="S3.SS1.p5.5.m5.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.5.m5.1b"><cn id="S3.SS1.p5.5.m5.1.1.cmml" type="integer" xref="S3.SS1.p5.5.m5.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.5.m5.1c">256</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.5.m5.1d">256</annotation></semantics></math> tokens for <math alttext="512" class="ltx_Math" display="inline" id="S3.SS1.p5.6.m6.1"><semantics id="S3.SS1.p5.6.m6.1a"><mn id="S3.SS1.p5.6.m6.1.1" xref="S3.SS1.p5.6.m6.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.6.m6.1b"><cn id="S3.SS1.p5.6.m6.1.1.cmml" type="integer" xref="S3.SS1.p5.6.m6.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.6.m6.1c">512</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.6.m6.1d">512</annotation></semantics></math>-token prompts. In contrast, MLC LLM supports variable output lengths, allowing for generation up to <math alttext="1024" class="ltx_Math" display="inline" id="S3.SS1.p5.7.m7.1"><semantics id="S3.SS1.p5.7.m7.1a"><mn id="S3.SS1.p5.7.m7.1.1" xref="S3.SS1.p5.7.m7.1.1.cmml">1024</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.7.m7.1b"><cn id="S3.SS1.p5.7.m7.1.1.cmml" type="integer" xref="S3.SS1.p5.7.m7.1.1">1024</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.7.m7.1c">1024</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.7.m7.1d">1024</annotation></semantics></math> tokens.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p6.1.1">Measurement toolkits.</span> In order to collect fine-grained hardware metrics such as real-time CPU frequency and memory bandwidth utilization, we choose Perfetto <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib1" title="">1</a>]</cite>, Snapdragon Profiler <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib35" title="">35</a>]</cite> and Arm streamline <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib5" title="">5</a>]</cite> as our primary tools for monitoring hardware dynamics during inference. Perfetto offers comprehensive system-wide performance traces from Android devices, including data from the kernel scheduler, userspace instrumentation, and other sources. For CPU monitoring, Perfetto captures frequency, utilization, and event scheduling for each CPU core. Snapdragon Profiler and Arm Streamline are vendor-specific profilers used to track GPU behavior, including arithmetic and load/store unit utilization. Specifically, Snapdragon Profiler provides detailed timelines of OpenCL kernel executions, which is valuable for operator-level analysis.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p7">
<p class="ltx_p" id="S3.SS1.p7.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p7.1.1">Evaluation Metrics.</span> We collect results from two main aspects. First, we gather performance metrics that are of interest to users, including prefill and decoding speed, memory footprints, and battery consumption. These results are present in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S4" title="4 Performance: Users’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">4</span></a>. Second, we analyze hardware dynamics, focusing on real-time processor utilization and working frequency, impact of Dynamic Voltage and Frequency Scaling (DVFS) and operator implementation, which developers are concerned about. These results are present in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5" title="5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">5</span></a>. They provide clues for the performance metrics and can help us identify potential hardware and operating system bottlenecks during inference.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Workflow and Methodology</h3>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="387" id="S3.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S3.F1.3.2" style="font-size:90%;">Measurement workflow.</span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.1">Workflow.</span> Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S3.F1" title="Figure 1 ‣ 3.2 Workflow and Methodology ‣ 3 Experimental Setup and Methodology ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">1</span></a> shows the overall workflow how we deploy and evaluate models on mobile devices. The entire workflow is divided into four steps:</p>
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i1.p1.1.1">Step 1</span>: Compile the model into a specific model library or binary executable file that comprises the inference logic.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i2.p1.1.1">Step 2</span>: Transfer the library or executable file, model weights, and other necessary files to the test device and complete the deployment.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i3.p1.1.1">Step 3</span>: Run the executable file or launch the application via ADB<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib17" title="">17</a>]</cite> and monitor hardware metrics with profilers<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib1" title="">1</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib5" title="">5</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib35" title="">35</a>]</cite> during inference.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i4.p1">
<p class="ltx_p" id="S3.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i4.p1.1.1">Step 4</span>: Collect results such as inference latency, CPU frequency fluctuations, and other key metrics.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">For llama.cpp, we build the model library and compile the source code into an executable file on each device individually, using CMake and the Android NDK. All the compile options are kept the same by default. When necessary, we adjust the compile options to enable specialized instructions to ensure optimal performance (Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.SS1.SSS2" title="5.1.2 Speedup with Special Machine Instructions ‣ 5.1 Hardware Capabilities ‣ 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">5.1.2</span></a>). Since it relies on basic C libraries, no additional environment setup is required. After transferring model weights in GGUF format to the device, inference can be performed easily using the ADB command-line tool. For MLC LLM, there is a native application which TVM runtime and necessary libraries are packed in. The APK can be installed on the device, allowing interaction with the LLM through a graphical interface. MLC LLM cross-compiles the LLM models for the mobile platform, and on all devices, the runtime version including tvm and java is the same.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.2"><span class="ltx_text ltx_font_bold" id="S3.SS2.p3.2.1">Measurement Methodology.</span>
To ensure accurate results, we perform two rounds of tests. In each round, a test is repeated five times, with an interval of <math alttext="10" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><mn id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><cn id="S3.SS2.p3.1.m1.1.1.cmml" type="integer" xref="S3.SS2.p3.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">10</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">10</annotation></semantics></math> seconds between each test. After the first round, the device reboots and rests for <math alttext="10" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.1"><semantics id="S3.SS2.p3.2.m2.1a"><mn id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><cn id="S3.SS2.p3.2.m2.1.1.cmml" type="integer" xref="S3.SS2.p3.2.m2.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">10</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m2.1d">10</annotation></semantics></math> minutes to avoid overheating. This approach minimizes interference from other processes and reduces the impact of DVFS. The results are then averaged to provide a reliable measure of performance.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Performance: Users’ Perspectives</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we present performance metrics that affect user experience, including token throughput, memory footprint, and energy consumption. We specifically compare the key performance metrics across different types of processors, including CPUs, GPUs, and specialized AI accelerators (NPUs).</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Token Throughput and Memory Footprints</h3>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Performance on CPUs</h4>
<figure class="ltx_figure" id="S4.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" id="S4.F3.1" style="width:283.3pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="474" id="S4.F3.1.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.1.1.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S4.F3.1.2.2" style="font-size:90%;">Inference Performance with llama.cpp</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" id="S4.F3.2" style="width:202.4pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="664" id="S4.F3.2.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.2.1.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S4.F3.2.2.2" style="font-size:90%;">Inference Performance with MLC LLM.</span></figcaption>
</figure>
</div>
</div>
</figure>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.12">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S4.F3" title="Figure 3 ‣ 4.1.1 Performance on CPUs ‣ 4.1 Token Throughput and Memory Footprints ‣ 4 Performance: Users’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">3</span></a> shows the performance of CPUs, including prefill speed and decoding speed with llama.cpp. Overall, there has been a consistent improvement in both prefill and decoding speeds with new-generation processors. The Dimensity <math alttext="9300" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.1.m1.1"><semantics id="S4.SS1.SSS1.p1.1.m1.1a"><mn id="S4.SS1.SSS1.p1.1.m1.1.1" xref="S4.SS1.SSS1.p1.1.m1.1.1.cmml">9300</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.1.m1.1b"><cn id="S4.SS1.SSS1.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS1.SSS1.p1.1.m1.1.1">9300</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.1.m1.1c">9300</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.1.m1.1d">9300</annotation></semantics></math> on Vivo Pad3 Pro, being the most advanced new-generation SoC, demonstrates the highest performance, achieving over <math alttext="3\times" class="ltx_math_unparsed" display="inline" id="S4.SS1.SSS1.p1.2.m2.1"><semantics id="S4.SS1.SSS1.p1.2.m2.1a"><mrow id="S4.SS1.SSS1.p1.2.m2.1b"><mn id="S4.SS1.SSS1.p1.2.m2.1.1">3</mn><mo id="S4.SS1.SSS1.p1.2.m2.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.2.m2.1c">3\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.2.m2.1d">3 ×</annotation></semantics></math> speedup in prefill and nearly <math alttext="5\times" class="ltx_math_unparsed" display="inline" id="S4.SS1.SSS1.p1.3.m3.1"><semantics id="S4.SS1.SSS1.p1.3.m3.1a"><mrow id="S4.SS1.SSS1.p1.3.m3.1b"><mn id="S4.SS1.SSS1.p1.3.m3.1.1">5</mn><mo id="S4.SS1.SSS1.p1.3.m3.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.3.m3.1c">5\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.3.m3.1d">5 ×</annotation></semantics></math> speedup in decoding compared to the older SoC Snapdrgon <math alttext="870" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.4.m4.1"><semantics id="S4.SS1.SSS1.p1.4.m4.1a"><mn id="S4.SS1.SSS1.p1.4.m4.1.1" xref="S4.SS1.SSS1.p1.4.m4.1.1.cmml">870</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.4.m4.1b"><cn id="S4.SS1.SSS1.p1.4.m4.1.1.cmml" type="integer" xref="S4.SS1.SSS1.p1.4.m4.1.1">870</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.4.m4.1c">870</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.4.m4.1d">870</annotation></semantics></math> on the Huawei Matepad<math alttext="11" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.5.m5.1"><semantics id="S4.SS1.SSS1.p1.5.m5.1a"><mn id="S4.SS1.SSS1.p1.5.m5.1.1" xref="S4.SS1.SSS1.p1.5.m5.1.1.cmml">11</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.5.m5.1b"><cn id="S4.SS1.SSS1.p1.5.m5.1.1.cmml" type="integer" xref="S4.SS1.SSS1.p1.5.m5.1.1">11</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.5.m5.1c">11</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.5.m5.1d">11</annotation></semantics></math> Pro. The Snapdragon 8 Gen <math alttext="3" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.6.m6.1"><semantics id="S4.SS1.SSS1.p1.6.m6.1a"><mn id="S4.SS1.SSS1.p1.6.m6.1.1" xref="S4.SS1.SSS1.p1.6.m6.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.6.m6.1b"><cn id="S4.SS1.SSS1.p1.6.m6.1.1.cmml" type="integer" xref="S4.SS1.SSS1.p1.6.m6.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.6.m6.1c">3</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.6.m6.1d">3</annotation></semantics></math> (XiaoMi <math alttext="14" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.7.m7.1"><semantics id="S4.SS1.SSS1.p1.7.m7.1a"><mn id="S4.SS1.SSS1.p1.7.m7.1.1" xref="S4.SS1.SSS1.p1.7.m7.1.1.cmml">14</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.7.m7.1b"><cn id="S4.SS1.SSS1.p1.7.m7.1.1.cmml" type="integer" xref="S4.SS1.SSS1.p1.7.m7.1.1">14</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.7.m7.1c">14</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.7.m7.1d">14</annotation></semantics></math> Pro) ranks second among all tested devices, showing <math alttext="80" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.8.m8.1"><semantics id="S4.SS1.SSS1.p1.8.m8.1a"><mn id="S4.SS1.SSS1.p1.8.m8.1.1" xref="S4.SS1.SSS1.p1.8.m8.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.8.m8.1b"><cn id="S4.SS1.SSS1.p1.8.m8.1.1.cmml" type="integer" xref="S4.SS1.SSS1.p1.8.m8.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.8.m8.1c">80</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.8.m8.1d">80</annotation></semantics></math>% throughput of the Dimensity <math alttext="9300" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.9.m9.1"><semantics id="S4.SS1.SSS1.p1.9.m9.1a"><mn id="S4.SS1.SSS1.p1.9.m9.1.1" xref="S4.SS1.SSS1.p1.9.m9.1.1.cmml">9300</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.9.m9.1b"><cn id="S4.SS1.SSS1.p1.9.m9.1.1.cmml" type="integer" xref="S4.SS1.SSS1.p1.9.m9.1.1">9300</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.9.m9.1c">9300</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.9.m9.1d">9300</annotation></semantics></math>. Comparing Snapdragon SoCs across different tiers, each successive generation shows approximately a <math alttext="50\%" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.10.m10.1"><semantics id="S4.SS1.SSS1.p1.10.m10.1a"><mrow id="S4.SS1.SSS1.p1.10.m10.1.1" xref="S4.SS1.SSS1.p1.10.m10.1.1.cmml"><mn id="S4.SS1.SSS1.p1.10.m10.1.1.2" xref="S4.SS1.SSS1.p1.10.m10.1.1.2.cmml">50</mn><mo id="S4.SS1.SSS1.p1.10.m10.1.1.1" xref="S4.SS1.SSS1.p1.10.m10.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.10.m10.1b"><apply id="S4.SS1.SSS1.p1.10.m10.1.1.cmml" xref="S4.SS1.SSS1.p1.10.m10.1.1"><csymbol cd="latexml" id="S4.SS1.SSS1.p1.10.m10.1.1.1.cmml" xref="S4.SS1.SSS1.p1.10.m10.1.1.1">percent</csymbol><cn id="S4.SS1.SSS1.p1.10.m10.1.1.2.cmml" type="integer" xref="S4.SS1.SSS1.p1.10.m10.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.10.m10.1c">50\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.10.m10.1d">50 %</annotation></semantics></math> improvement in prefill speed, while decode speed improves even more significantly, ranging from <math alttext="80\%" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.11.m11.1"><semantics id="S4.SS1.SSS1.p1.11.m11.1a"><mrow id="S4.SS1.SSS1.p1.11.m11.1.1" xref="S4.SS1.SSS1.p1.11.m11.1.1.cmml"><mn id="S4.SS1.SSS1.p1.11.m11.1.1.2" xref="S4.SS1.SSS1.p1.11.m11.1.1.2.cmml">80</mn><mo id="S4.SS1.SSS1.p1.11.m11.1.1.1" xref="S4.SS1.SSS1.p1.11.m11.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.11.m11.1b"><apply id="S4.SS1.SSS1.p1.11.m11.1.1.cmml" xref="S4.SS1.SSS1.p1.11.m11.1.1"><csymbol cd="latexml" id="S4.SS1.SSS1.p1.11.m11.1.1.1.cmml" xref="S4.SS1.SSS1.p1.11.m11.1.1.1">percent</csymbol><cn id="S4.SS1.SSS1.p1.11.m11.1.1.2.cmml" type="integer" xref="S4.SS1.SSS1.p1.11.m11.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.11.m11.1c">80\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.11.m11.1d">80 %</annotation></semantics></math> to <math alttext="110\%" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.12.m12.1"><semantics id="S4.SS1.SSS1.p1.12.m12.1a"><mrow id="S4.SS1.SSS1.p1.12.m12.1.1" xref="S4.SS1.SSS1.p1.12.m12.1.1.cmml"><mn id="S4.SS1.SSS1.p1.12.m12.1.1.2" xref="S4.SS1.SSS1.p1.12.m12.1.1.2.cmml">110</mn><mo id="S4.SS1.SSS1.p1.12.m12.1.1.1" xref="S4.SS1.SSS1.p1.12.m12.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.12.m12.1b"><apply id="S4.SS1.SSS1.p1.12.m12.1.1.cmml" xref="S4.SS1.SSS1.p1.12.m12.1.1"><csymbol cd="latexml" id="S4.SS1.SSS1.p1.12.m12.1.1.1.cmml" xref="S4.SS1.SSS1.p1.12.m12.1.1.1">percent</csymbol><cn id="S4.SS1.SSS1.p1.12.m12.1.1.2.cmml" type="integer" xref="S4.SS1.SSS1.p1.12.m12.1.1.2">110</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.12.m12.1c">110\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.12.m12.1d">110 %</annotation></semantics></math>. We note that Snapdragon 870 (on Huawei Matepad11 Pro) and Kirin 9000E (on Huawei Matepad12.6 Pro) consist of the same cores, and the performance gap may result from higher CPU frequency on Kirin 9000E. Notably, a significant performance gap persists between Kirin SoCs and those from Snapdragon and MediaTek in terms of supporting LLMs.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.1">Our results highlight a notable performance disparity in LLM inference across mobile CPUs. Specifically, top-tier SoCs like the Dimensity <math alttext="9300" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p2.1.m1.1"><semantics id="S4.SS1.SSS1.p2.1.m1.1a"><mn id="S4.SS1.SSS1.p2.1.m1.1.1" xref="S4.SS1.SSS1.p2.1.m1.1.1.cmml">9300</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.1.m1.1b"><cn id="S4.SS1.SSS1.p2.1.m1.1.1.cmml" type="integer" xref="S4.SS1.SSS1.p2.1.m1.1.1">9300</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.1.m1.1c">9300</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p2.1.m1.1d">9300</annotation></semantics></math> demonstrate significantly better LLM support compared to mid-tier SoCs.
This suggests that for optimal LLM performance, developers should focus on leveraging the most advanced processors available. Furthermore, top-tier SoCs support advanced machine instructions that can significantly boost performance. We will examine this potential in detail in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.SS1.SSS2" title="5.1.2 Speedup with Special Machine Instructions ‣ 5.1 Hardware Capabilities ‣ 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">5.1.2</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p3">
<p class="ltx_p" id="S4.SS1.SSS1.p3.6">We further evaluate the performance with two scenarios: one involving a prompt of <math alttext="64" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p3.1.m1.1"><semantics id="S4.SS1.SSS1.p3.1.m1.1a"><mn id="S4.SS1.SSS1.p3.1.m1.1.1" xref="S4.SS1.SSS1.p3.1.m1.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.1.m1.1b"><cn id="S4.SS1.SSS1.p3.1.m1.1.1.cmml" type="integer" xref="S4.SS1.SSS1.p3.1.m1.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.1.m1.1c">64</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p3.1.m1.1d">64</annotation></semantics></math> tokens with a generation of <math alttext="128" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p3.2.m2.1"><semantics id="S4.SS1.SSS1.p3.2.m2.1a"><mn id="S4.SS1.SSS1.p3.2.m2.1.1" xref="S4.SS1.SSS1.p3.2.m2.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.2.m2.1b"><cn id="S4.SS1.SSS1.p3.2.m2.1.1.cmml" type="integer" xref="S4.SS1.SSS1.p3.2.m2.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.2.m2.1c">128</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p3.2.m2.1d">128</annotation></semantics></math> tokens, and the other involving a prompt of <math alttext="512" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p3.3.m3.1"><semantics id="S4.SS1.SSS1.p3.3.m3.1a"><mn id="S4.SS1.SSS1.p3.3.m3.1.1" xref="S4.SS1.SSS1.p3.3.m3.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.3.m3.1b"><cn id="S4.SS1.SSS1.p3.3.m3.1.1.cmml" type="integer" xref="S4.SS1.SSS1.p3.3.m3.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.3.m3.1c">512</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p3.3.m3.1d">512</annotation></semantics></math> tokens with a generation of <math alttext="256" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p3.4.m4.1"><semantics id="S4.SS1.SSS1.p3.4.m4.1a"><mn id="S4.SS1.SSS1.p3.4.m4.1.1" xref="S4.SS1.SSS1.p3.4.m4.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.4.m4.1b"><cn id="S4.SS1.SSS1.p3.4.m4.1.1.cmml" type="integer" xref="S4.SS1.SSS1.p3.4.m4.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.4.m4.1c">256</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p3.4.m4.1d">256</annotation></semantics></math> tokens. The larger input matrix in the latter case implies greater computational intensity and resource demands. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S4.F3" title="Figure 3 ‣ 4.1.1 Performance on CPUs ‣ 4.1 Token Throughput and Memory Footprints ‣ 4 Performance: Users’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates that inference speed decreases as the token length increases. The results may be attributed to the impact of DVFS. With longer prompts and generation tasks, the increased load causes the CPU frequency to throttle more aggressively. Across all tested devices, the performance degradation for both prefill and decoding is relatively consistent, ranging between <math alttext="10\%" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p3.5.m5.1"><semantics id="S4.SS1.SSS1.p3.5.m5.1a"><mrow id="S4.SS1.SSS1.p3.5.m5.1.1" xref="S4.SS1.SSS1.p3.5.m5.1.1.cmml"><mn id="S4.SS1.SSS1.p3.5.m5.1.1.2" xref="S4.SS1.SSS1.p3.5.m5.1.1.2.cmml">10</mn><mo id="S4.SS1.SSS1.p3.5.m5.1.1.1" xref="S4.SS1.SSS1.p3.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.5.m5.1b"><apply id="S4.SS1.SSS1.p3.5.m5.1.1.cmml" xref="S4.SS1.SSS1.p3.5.m5.1.1"><csymbol cd="latexml" id="S4.SS1.SSS1.p3.5.m5.1.1.1.cmml" xref="S4.SS1.SSS1.p3.5.m5.1.1.1">percent</csymbol><cn id="S4.SS1.SSS1.p3.5.m5.1.1.2.cmml" type="integer" xref="S4.SS1.SSS1.p3.5.m5.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.5.m5.1c">10\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p3.5.m5.1d">10 %</annotation></semantics></math> and <math alttext="20\%" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p3.6.m6.1"><semantics id="S4.SS1.SSS1.p3.6.m6.1a"><mrow id="S4.SS1.SSS1.p3.6.m6.1.1" xref="S4.SS1.SSS1.p3.6.m6.1.1.cmml"><mn id="S4.SS1.SSS1.p3.6.m6.1.1.2" xref="S4.SS1.SSS1.p3.6.m6.1.1.2.cmml">20</mn><mo id="S4.SS1.SSS1.p3.6.m6.1.1.1" xref="S4.SS1.SSS1.p3.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.6.m6.1b"><apply id="S4.SS1.SSS1.p3.6.m6.1.1.cmml" xref="S4.SS1.SSS1.p3.6.m6.1.1"><csymbol cd="latexml" id="S4.SS1.SSS1.p3.6.m6.1.1.1.cmml" xref="S4.SS1.SSS1.p3.6.m6.1.1.1">percent</csymbol><cn id="S4.SS1.SSS1.p3.6.m6.1.1.2.cmml" type="integer" xref="S4.SS1.SSS1.p3.6.m6.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.6.m6.1c">20\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p3.6.m6.1d">20 %</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p4">
<p class="ltx_p" id="S4.SS1.SSS1.p4.1">In addition to latency, memory footprint is a also crucial metric. On all six devices, memory usage remains consistently around <math alttext="3.8" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p4.1.m1.1"><semantics id="S4.SS1.SSS1.p4.1.m1.1a"><mn id="S4.SS1.SSS1.p4.1.m1.1.1" xref="S4.SS1.SSS1.p4.1.m1.1.1.cmml">3.8</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p4.1.m1.1b"><cn id="S4.SS1.SSS1.p4.1.m1.1.1.cmml" type="float" xref="S4.SS1.SSS1.p4.1.m1.1.1">3.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p4.1.m1.1c">3.8</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p4.1.m1.1d">3.8</annotation></semantics></math>GB and shows minimal variation during inference once the model weights are loaded into main memory from external storage (e.g., Universal Flash Storage, UFS). This consistency is due to the fact that llama.cpp applies for a fixed-size memory block during initialization, which remains constant once allocation is complete. The allocated memory is then segmented into various components for different variables, including model weights, KV-cache, buffers, context and etc.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Performance on GPUs</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">Unlike CPUs, which are all from Arm, GPUs across vendors may have different architectures. Thus, it is challenging for developers to optimize for different GPUs. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S4.F3" title="Figure 3 ‣ 4.1.1 Performance on CPUs ‣ 4.1 Token Throughput and Memory Footprints ‣ 4 Performance: Users’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the end-to-end latency of MLC LLM across four high-tier GPUs from Arm Mali and Qualcomm Snapdragon. Note that Huawei Matepad11 Pro and Huawei Nova7 are excluded from this comparison due to subpar performance on devices with low RAM (<math alttext="8" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.1.m1.1"><semantics id="S4.SS1.SSS2.p1.1.m1.1a"><mn id="S4.SS1.SSS2.p1.1.m1.1.1" xref="S4.SS1.SSS2.p1.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.1.m1.1b"><cn id="S4.SS1.SSS2.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS1.SSS2.p1.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.1.m1.1c">8</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.1.m1.1d">8</annotation></semantics></math>GB). Among the tested GPUs, Mali GPUs, particularly the Mali-G78, exhibit significantly slow prefill speeds. Consequently, we only consider the performance with 64-token prompts. For the Mali-G78, due to its extremely poor prefill performance, we only present results for 32-token prompts as a reference.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.11">The Adreno GPU consistently outperforms the Mali GPU in overall performance. Notably, Mali GPUs show unusually poor prefill performance. As for decoding, the Adreno <math alttext="750" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p2.1.m1.1"><semantics id="S4.SS1.SSS2.p2.1.m1.1a"><mn id="S4.SS1.SSS2.p2.1.m1.1.1" xref="S4.SS1.SSS2.p2.1.m1.1.1.cmml">750</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p2.1.m1.1b"><cn id="S4.SS1.SSS2.p2.1.m1.1.1.cmml" type="integer" xref="S4.SS1.SSS2.p2.1.m1.1.1">750</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p2.1.m1.1c">750</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p2.1.m1.1d">750</annotation></semantics></math> delivers <math alttext="1.6\times" class="ltx_math_unparsed" display="inline" id="S4.SS1.SSS2.p2.2.m2.1"><semantics id="S4.SS1.SSS2.p2.2.m2.1a"><mrow id="S4.SS1.SSS2.p2.2.m2.1b"><mn id="S4.SS1.SSS2.p2.2.m2.1.1">1.6</mn><mo id="S4.SS1.SSS2.p2.2.m2.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p2.2.m2.1c">1.6\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p2.2.m2.1d">1.6 ×</annotation></semantics></math> faster decoding speed than the Mali-G<math alttext="720" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p2.3.m3.1"><semantics id="S4.SS1.SSS2.p2.3.m3.1a"><mn id="S4.SS1.SSS2.p2.3.m3.1.1" xref="S4.SS1.SSS2.p2.3.m3.1.1.cmml">720</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p2.3.m3.1b"><cn id="S4.SS1.SSS2.p2.3.m3.1.1.cmml" type="integer" xref="S4.SS1.SSS2.p2.3.m3.1.1">720</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p2.3.m3.1c">720</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p2.3.m3.1d">720</annotation></semantics></math>. Similarly, the Adreno <math alttext="730" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p2.4.m4.1"><semantics id="S4.SS1.SSS2.p2.4.m4.1a"><mn id="S4.SS1.SSS2.p2.4.m4.1.1" xref="S4.SS1.SSS2.p2.4.m4.1.1.cmml">730</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p2.4.m4.1b"><cn id="S4.SS1.SSS2.p2.4.m4.1.1.cmml" type="integer" xref="S4.SS1.SSS2.p2.4.m4.1.1">730</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p2.4.m4.1c">730</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p2.4.m4.1d">730</annotation></semantics></math> achieves <math alttext="1.4\times" class="ltx_math_unparsed" display="inline" id="S4.SS1.SSS2.p2.5.m5.1"><semantics id="S4.SS1.SSS2.p2.5.m5.1a"><mrow id="S4.SS1.SSS2.p2.5.m5.1b"><mn id="S4.SS1.SSS2.p2.5.m5.1.1">1.4</mn><mo id="S4.SS1.SSS2.p2.5.m5.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p2.5.m5.1c">1.4\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p2.5.m5.1d">1.4 ×</annotation></semantics></math> faster decoding speed compared to the Mali-G<math alttext="78" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p2.6.m6.1"><semantics id="S4.SS1.SSS2.p2.6.m6.1a"><mn id="S4.SS1.SSS2.p2.6.m6.1.1" xref="S4.SS1.SSS2.p2.6.m6.1.1.cmml">78</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p2.6.m6.1b"><cn id="S4.SS1.SSS2.p2.6.m6.1.1.cmml" type="integer" xref="S4.SS1.SSS2.p2.6.m6.1.1">78</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p2.6.m6.1c">78</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p2.6.m6.1d">78</annotation></semantics></math>. Although Mali GPUs have poor prefill performance, Mali-G<math alttext="720" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p2.7.m7.1"><semantics id="S4.SS1.SSS2.p2.7.m7.1a"><mn id="S4.SS1.SSS2.p2.7.m7.1.1" xref="S4.SS1.SSS2.p2.7.m7.1.1.cmml">720</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p2.7.m7.1b"><cn id="S4.SS1.SSS2.p2.7.m7.1.1.cmml" type="integer" xref="S4.SS1.SSS2.p2.7.m7.1.1">720</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p2.7.m7.1c">720</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p2.7.m7.1d">720</annotation></semantics></math> still achieves <math alttext="1.5\times" class="ltx_math_unparsed" display="inline" id="S4.SS1.SSS2.p2.8.m8.1"><semantics id="S4.SS1.SSS2.p2.8.m8.1a"><mrow id="S4.SS1.SSS2.p2.8.m8.1b"><mn id="S4.SS1.SSS2.p2.8.m8.1.1">1.5</mn><mo id="S4.SS1.SSS2.p2.8.m8.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p2.8.m8.1c">1.5\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p2.8.m8.1d">1.5 ×</annotation></semantics></math> speedup in decoding than older-generation Adreno <math alttext="730" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p2.9.m9.1"><semantics id="S4.SS1.SSS2.p2.9.m9.1a"><mn id="S4.SS1.SSS2.p2.9.m9.1.1" xref="S4.SS1.SSS2.p2.9.m9.1.1.cmml">730</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p2.9.m9.1b"><cn id="S4.SS1.SSS2.p2.9.m9.1.1.cmml" type="integer" xref="S4.SS1.SSS2.p2.9.m9.1.1">730</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p2.9.m9.1c">730</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p2.9.m9.1d">730</annotation></semantics></math>. Memory usage across the GPUs is nearly identical at around <math alttext="4.2" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p2.10.m10.1"><semantics id="S4.SS1.SSS2.p2.10.m10.1a"><mn id="S4.SS1.SSS2.p2.10.m10.1.1" xref="S4.SS1.SSS2.p2.10.m10.1.1.cmml">4.2</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p2.10.m10.1b"><cn id="S4.SS1.SSS2.p2.10.m10.1.1.cmml" type="float" xref="S4.SS1.SSS2.p2.10.m10.1.1">4.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p2.10.m10.1c">4.2</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p2.10.m10.1d">4.2</annotation></semantics></math>GB, with a slight increase to <math alttext="4.4" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p2.11.m11.1"><semantics id="S4.SS1.SSS2.p2.11.m11.1a"><mn id="S4.SS1.SSS2.p2.11.m11.1.1" xref="S4.SS1.SSS2.p2.11.m11.1.1.cmml">4.4</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p2.11.m11.1b"><cn id="S4.SS1.SSS2.p2.11.m11.1.1.cmml" type="float" xref="S4.SS1.SSS2.p2.11.m11.1.1">4.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p2.11.m11.1c">4.4</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p2.11.m11.1d">4.4</annotation></semantics></math>GB observed on the Vivo Pad3 Pro.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Performance on Specialized AI Accelerators</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">Recent years have seen the introduction of specialized AI accelerators in mobile SoCs, such as the Snapdragon Hexagon NPU, Kirin NPU, and MediaTek APU. However, these accelerators face several challenges, including limited support for dynamic shapes and floating-point operations, which complicates their use for LLM inference <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib46" title="">46</a>]</cite>. The Hexagon NPU, being the only mobile NPU with an open instruction set architecture, has garnered significant research interest. Successful deployments of mobile LLM models on the Snapdragon 8 Gen 3 have been reported. We summarize the findings from mllm, PowerInfer-<math alttext="2" class="ltx_Math" display="inline" id="S4.SS1.SSS3.p1.1.m1.1"><semantics id="S4.SS1.SSS3.p1.1.m1.1a"><mn id="S4.SS1.SSS3.p1.1.m1.1.1" xref="S4.SS1.SSS3.p1.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p1.1.m1.1b"><cn id="S4.SS1.SSS3.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS1.SSS3.p1.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p1.1.m1.1c">2</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS3.p1.1.m1.1d">2</annotation></semantics></math>, and Qualcomm AI Hub in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S4.F4" title="Figure 4 ‣ 4.1.3 Performance on Specialized AI Accelerators ‣ 4.1 Token Throughput and Memory Footprints ‣ 4 Performance: Users’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="494" id="S4.F4.g1" src="x4.png" width="706"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S4.F4.3.2" style="font-size:90%;">Performance of Llama-2-7B on Snapdragon NPUs.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.SSS3.p2">
<p class="ltx_p" id="S4.SS1.SSS3.p2.2">Qualcomm AI Hub, the official tool developed by the manufacturer, demonstrates a remarkable prefill speed of <math alttext="690" class="ltx_Math" display="inline" id="S4.SS1.SSS3.p2.1.m1.1"><semantics id="S4.SS1.SSS3.p2.1.m1.1a"><mn id="S4.SS1.SSS3.p2.1.m1.1.1" xref="S4.SS1.SSS3.p2.1.m1.1.1.cmml">690</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p2.1.m1.1b"><cn id="S4.SS1.SSS3.p2.1.m1.1.1.cmml" type="integer" xref="S4.SS1.SSS3.p2.1.m1.1.1">690</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p2.1.m1.1c">690</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS3.p2.1.m1.1d">690</annotation></semantics></math> tokens per second, representing a <math alttext="50" class="ltx_Math" display="inline" id="S4.SS1.SSS3.p2.2.m2.1"><semantics id="S4.SS1.SSS3.p2.2.m2.1a"><mn id="S4.SS1.SSS3.p2.2.m2.1.1" xref="S4.SS1.SSS3.p2.2.m2.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p2.2.m2.1b"><cn id="S4.SS1.SSS3.p2.2.m2.1.1.cmml" type="integer" xref="S4.SS1.SSS3.p2.2.m2.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p2.2.m2.1c">50</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS3.p2.2.m2.1d">50</annotation></semantics></math>-fold increase compared to both CPU and GPU. Similarly, mllm and PowerInfer-2 achieve substantial improvements in prefill speed, with an order of magnitude
improvement in prefill speed. However, the decoding speed remains just slightly better than that of CPU and GPU based solutions.
As we all know, prefill is compute-bound and decoding is memory-bound. This distinction likely accounts for the significant improvement in prefill speed observed with the NPU, whereas its decoding speed remains comparable to that of CPU/GPU.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.4 </span>Comparisons and Summary</h4>
<div class="ltx_para" id="S4.SS1.SSS4.p1">
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">Given that mobile CPUs are built on the same Arm architecture and share common instruction sets, deploying LLMs on CPU is more straightforward and does not require specialized customization. As a result, CPUs have emerged as a preferred option for LLM inference on mobile platforms. However, our results show that they are limited in performance and thus not the optimal solution.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">GPUs, despite their theoretical advantages for LLMs due to their parallel float computing capabilities, do not show a clear advantage in practice. What’s counterintuitive is that MLC LLM on GPUs performs worse in prefill speed compared to llama.cpp on CPUs, a phenomenon that we will provide a detailed analysis in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.SS1.SSS3" title="5.1.3 Utilization of GPU units ‣ 5.1 Hardware Capabilities ‣ 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">5.1.3</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">In contrast, early trials with the Hexagon NPU demonstrate a significant advantage for specialized AI accelerators in inference tasks. However, due to their limited support for operators, adapting models and algorithms to specialized AI accelerators remains a challenging task for developers.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Battery Consumption</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">While the end-to-end latency of LLM inference on mobile CPUs shows promising potential, power consumption is also a key factor in determining the feasibility of these models for local, always-on user access.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.7">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S4.T2" title="Table 2 ‣ 4.2 Battery Consumption ‣ 4 Performance: Users’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">2</span></a> compares the power consumption and inference speed between the Vivo Pad3 Pro and Huawei MatePad 12.6 Pro. To obtain the power consumption, we read the data provided by the OS settings, which records and displays the power consumption of a certain application. Old-generation devices are excluded from this comparison due to significant differences in battery capacity and power-saving policies compared to new-generation hardware. We perform <math alttext="20" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.1"><semantics id="S4.SS2.p2.1.m1.1a"><mn id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><cn id="S4.SS2.p2.1.m1.1.1.cmml" type="integer" xref="S4.SS2.p2.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">20</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.1d">20</annotation></semantics></math> consecutive rounds of inference on the CPU, with a prefill length of <math alttext="64" class="ltx_Math" display="inline" id="S4.SS2.p2.2.m2.1"><semantics id="S4.SS2.p2.2.m2.1a"><mn id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><cn id="S4.SS2.p2.2.m2.1.1.cmml" type="integer" xref="S4.SS2.p2.2.m2.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">64</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.2.m2.1d">64</annotation></semantics></math> and a generation length of <math alttext="128" class="ltx_Math" display="inline" id="S4.SS2.p2.3.m3.1"><semantics id="S4.SS2.p2.3.m3.1a"><mn id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><cn id="S4.SS2.p2.3.m3.1.1.cmml" type="integer" xref="S4.SS2.p2.3.m3.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">128</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.3.m3.1d">128</annotation></semantics></math>. The Dimensity 9300 exhibites a <math alttext="1.6\times" class="ltx_math_unparsed" display="inline" id="S4.SS2.p2.4.m4.1"><semantics id="S4.SS2.p2.4.m4.1a"><mrow id="S4.SS2.p2.4.m4.1b"><mn id="S4.SS2.p2.4.m4.1.1">1.6</mn><mo id="S4.SS2.p2.4.m4.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">1.6\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.4.m4.1d">1.6 ×</annotation></semantics></math> speedup in prefill and a <math alttext="1.9\times" class="ltx_math_unparsed" display="inline" id="S4.SS2.p2.5.m5.1"><semantics id="S4.SS2.p2.5.m5.1a"><mrow id="S4.SS2.p2.5.m5.1b"><mn id="S4.SS2.p2.5.m5.1.1">1.9</mn><mo id="S4.SS2.p2.5.m5.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.1c">1.9\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.5.m5.1d">1.9 ×</annotation></semantics></math> speedup in decoding, while consuming only <math alttext="55\%" class="ltx_Math" display="inline" id="S4.SS2.p2.6.m6.1"><semantics id="S4.SS2.p2.6.m6.1a"><mrow id="S4.SS2.p2.6.m6.1.1" xref="S4.SS2.p2.6.m6.1.1.cmml"><mn id="S4.SS2.p2.6.m6.1.1.2" xref="S4.SS2.p2.6.m6.1.1.2.cmml">55</mn><mo id="S4.SS2.p2.6.m6.1.1.1" xref="S4.SS2.p2.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.6.m6.1b"><apply id="S4.SS2.p2.6.m6.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1"><csymbol cd="latexml" id="S4.SS2.p2.6.m6.1.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1.1">percent</csymbol><cn id="S4.SS2.p2.6.m6.1.1.2.cmml" type="integer" xref="S4.SS2.p2.6.m6.1.1.2">55</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.6.m6.1c">55\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.6.m6.1d">55 %</annotation></semantics></math> of the power used by the Kirin <math alttext="9000" class="ltx_Math" display="inline" id="S4.SS2.p2.7.m7.1"><semantics id="S4.SS2.p2.7.m7.1a"><mn id="S4.SS2.p2.7.m7.1.1" xref="S4.SS2.p2.7.m7.1.1.cmml">9000</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.7.m7.1b"><cn id="S4.SS2.p2.7.m7.1.1.cmml" type="integer" xref="S4.SS2.p2.7.m7.1.1">9000</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.7.m7.1c">9000</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.7.m7.1d">9000</annotation></semantics></math>E. These results suggest that top-tier SoCs are capable of delivering substantial energy savings alongside improved performance.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.6">Most modern smartphones feature battery capacities between <math alttext="5000" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1"><semantics id="S4.SS2.p3.1.m1.1a"><mn id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">5000</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><cn id="S4.SS2.p3.1.m1.1.1.cmml" type="integer" xref="S4.SS2.p3.1.m1.1.1">5000</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">5000</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.1.m1.1d">5000</annotation></semantics></math>mAh and <math alttext="6000" class="ltx_Math" display="inline" id="S4.SS2.p3.2.m2.1"><semantics id="S4.SS2.p3.2.m2.1a"><mn id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml">6000</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><cn id="S4.SS2.p3.2.m2.1.1.cmml" type="integer" xref="S4.SS2.p3.2.m2.1.1">6000</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">6000</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.2.m2.1d">6000</annotation></semantics></math>mAh. At a power consumption rate of <math alttext="9" class="ltx_Math" display="inline" id="S4.SS2.p3.3.m3.1"><semantics id="S4.SS2.p3.3.m3.1a"><mn id="S4.SS2.p3.3.m3.1.1" xref="S4.SS2.p3.3.m3.1.1.cmml">9</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m3.1b"><cn id="S4.SS2.p3.3.m3.1.1.cmml" type="integer" xref="S4.SS2.p3.3.m3.1.1">9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m3.1c">9</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.3.m3.1d">9</annotation></semantics></math>mAh per inference round, a device could theoretically support over <math alttext="500" class="ltx_Math" display="inline" id="S4.SS2.p3.4.m4.1"><semantics id="S4.SS2.p3.4.m4.1a"><mn id="S4.SS2.p3.4.m4.1.1" xref="S4.SS2.p3.4.m4.1.1.cmml">500</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.4.m4.1b"><cn id="S4.SS2.p3.4.m4.1.1.cmml" type="integer" xref="S4.SS2.p3.4.m4.1.1">500</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.4.m4.1c">500</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.4.m4.1d">500</annotation></semantics></math> rounds of inference. This aligns with the findings in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib25" title="">25</a>]</cite>, reinforcing the feasibility of LLM inference on mobile devices. However, a significant rise in device temperature during inference suggests that LLM inference is a power-intensive task. We recorded an increase from <math alttext="42.6" class="ltx_Math" display="inline" id="S4.SS2.p3.5.m5.1"><semantics id="S4.SS2.p3.5.m5.1a"><mn id="S4.SS2.p3.5.m5.1.1" xref="S4.SS2.p3.5.m5.1.1.cmml">42.6</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.5.m5.1b"><cn id="S4.SS2.p3.5.m5.1.1.cmml" type="float" xref="S4.SS2.p3.5.m5.1.1">42.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.5.m5.1c">42.6</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.5.m5.1d">42.6</annotation></semantics></math>°C to <math alttext="66.8" class="ltx_Math" display="inline" id="S4.SS2.p3.6.m6.1"><semantics id="S4.SS2.p3.6.m6.1a"><mn id="S4.SS2.p3.6.m6.1.1" xref="S4.SS2.p3.6.m6.1.1.cmml">66.8</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.6.m6.1b"><cn id="S4.SS2.p3.6.m6.1.1.cmml" type="float" xref="S4.SS2.p3.6.m6.1.1">66.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.6.m6.1c">66.8</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.6.m6.1d">66.8</annotation></semantics></math>°C during a single inference on Huawei Matepad12.6 Pro. Thus, optimizing the power efficiency of LLMs on mobile devices is crucial for sustainable and long-term deployment.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Energy Consumption</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T2.6" style="width:227.6pt;height:45pt;vertical-align:-1.1pt;"><span class="ltx_transformed_inner" style="transform:translate(-73.0pt,14.1pt) scale(0.609337652075053,0.609337652075053) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.6.6">
<tr class="ltx_tr" id="S4.T2.6.6.7">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.6.6.7.1"><span class="ltx_text" id="S4.T2.6.6.7.1.1" style="font-size:90%;">Device</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.6.6.7.2"><span class="ltx_text" id="S4.T2.6.6.7.2.1" style="font-size:90%;">Power Drain</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.6.6.7.3"><span class="ltx_text" id="S4.T2.6.6.7.3.1" style="font-size:90%;">Prefill</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.6.6.7.4"><span class="ltx_text" id="S4.T2.6.6.7.4.1" style="font-size:90%;">Decode</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.6.8">
<td class="ltx_td" id="S4.T2.6.6.8.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.6.8.2"><span class="ltx_text" id="S4.T2.6.6.8.2.1" style="font-size:90%;">(mAh/round)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.6.8.3"><span class="ltx_text" id="S4.T2.6.6.8.3.1" style="font-size:90%;">(token/s)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.6.8.4"><span class="ltx_text" id="S4.T2.6.6.8.4.1" style="font-size:90%;">(token/s)</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.3.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.3.3.4"><span class="ltx_text" id="S4.T2.3.3.3.4.1" style="font-size:90%;">Vivo Pad3 Pro</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.1"><math alttext="4.54" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.m1.1a"><mn id="S4.T2.1.1.1.1.m1.1.1" mathsize="90%" xref="S4.T2.1.1.1.1.m1.1.1.cmml">4.54</mn><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><cn id="S4.T2.1.1.1.1.m1.1.1.cmml" type="float" xref="S4.T2.1.1.1.1.m1.1.1">4.54</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">4.54</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.m1.1d">4.54</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.2.2"><math alttext="10.63" class="ltx_Math" display="inline" id="S4.T2.2.2.2.2.m1.1"><semantics id="S4.T2.2.2.2.2.m1.1a"><mn id="S4.T2.2.2.2.2.m1.1.1" mathsize="90%" xref="S4.T2.2.2.2.2.m1.1.1.cmml">10.63</mn><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.m1.1b"><cn id="S4.T2.2.2.2.2.m1.1.1.cmml" type="float" xref="S4.T2.2.2.2.2.m1.1.1">10.63</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.m1.1c">10.63</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.2.m1.1d">10.63</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.3.3.3"><math alttext="8.22" class="ltx_Math" display="inline" id="S4.T2.3.3.3.3.m1.1"><semantics id="S4.T2.3.3.3.3.m1.1a"><mn id="S4.T2.3.3.3.3.m1.1.1" mathsize="90%" xref="S4.T2.3.3.3.3.m1.1.1.cmml">8.22</mn><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.3.m1.1b"><cn id="S4.T2.3.3.3.3.m1.1.1.cmml" type="float" xref="S4.T2.3.3.3.3.m1.1.1">8.22</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.3.m1.1c">8.22</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.3.3.m1.1d">8.22</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.6.6">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.6.6.6.4"><span class="ltx_text" id="S4.T2.6.6.6.4.1" style="font-size:90%;">Huawei Matepad12.6 Pro</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.4.4.4.1"><math alttext="8.28" class="ltx_Math" display="inline" id="S4.T2.4.4.4.1.m1.1"><semantics id="S4.T2.4.4.4.1.m1.1a"><mn id="S4.T2.4.4.4.1.m1.1.1" mathsize="90%" xref="S4.T2.4.4.4.1.m1.1.1.cmml">8.28</mn><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.1.m1.1b"><cn id="S4.T2.4.4.4.1.m1.1.1.cmml" type="float" xref="S4.T2.4.4.4.1.m1.1.1">8.28</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.1.m1.1c">8.28</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.4.4.1.m1.1d">8.28</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.5.5.5.2"><math alttext="6.67" class="ltx_Math" display="inline" id="S4.T2.5.5.5.2.m1.1"><semantics id="S4.T2.5.5.5.2.m1.1a"><mn id="S4.T2.5.5.5.2.m1.1.1" mathsize="90%" xref="S4.T2.5.5.5.2.m1.1.1.cmml">6.67</mn><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.2.m1.1b"><cn id="S4.T2.5.5.5.2.m1.1.1.cmml" type="float" xref="S4.T2.5.5.5.2.m1.1.1">6.67</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.2.m1.1c">6.67</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.5.5.2.m1.1d">6.67</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.6.6.6.3"><math alttext="4.34" class="ltx_Math" display="inline" id="S4.T2.6.6.6.3.m1.1"><semantics id="S4.T2.6.6.6.3.m1.1a"><mn id="S4.T2.6.6.6.3.m1.1.1" mathsize="90%" xref="S4.T2.6.6.6.3.m1.1.1.cmml">4.34</mn><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.6.3.m1.1b"><cn id="S4.T2.6.6.6.3.m1.1.1.cmml" type="float" xref="S4.T2.6.6.6.3.m1.1.1">4.34</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.6.3.m1.1c">4.34</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.6.6.3.m1.1d">4.34</annotation></semantics></math></td>
</tr>
</table>
</span></div>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Performance: Developers’ Perspectives</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we present results that developers care about, focusing on CPU/GPU utilization, DVFS and scheduling strategies. We also investigate the impact of different inference frameworks. We hope that these results can help developers identify bottlenecks in LLM inference and ultimately lead to improvements in system performance.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Hardware Capabilities</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">While upgrading hardware can enhance LLM local inference performance, it is also crucial to assess whether we are fully utilizing the capabilities of existing hardware. To address this, we use specialized profilers to monitor and capture dynamic utilization of the CPU and GPU during inference. This allows us to explore the potential of current hardware and identify opportunities for further accelerating LLM inference.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>Multi-threads on CPU Cores</h4>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption ltx_centering" style="font-size:50%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T3.36.1.1" style="font-size:180%;">Table 3</span>: </span><span class="ltx_text" id="S5.T3.37.2" style="font-size:180%;">CPU Specifications</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T3.32" style="width:480.6pt;height:122.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(53.3pt,-13.6pt) scale(1.28496426573374,1.28496426573374) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.32.32">
<tr class="ltx_tr" id="S5.T3.6.6.6">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.6.6.6.7"><span class="ltx_text" id="S5.T3.6.6.6.7.1" style="font-size:50%;">SoC</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.2.2.2.2">
<span class="ltx_text" id="S5.T3.2.2.2.2.1" style="font-size:50%;">Snapdragon </span><math alttext="8" class="ltx_Math" display="inline" id="S5.T3.1.1.1.1.m1.1"><semantics id="S5.T3.1.1.1.1.m1.1a"><mn id="S5.T3.1.1.1.1.m1.1.1" mathsize="50%" xref="S5.T3.1.1.1.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.m1.1b"><cn id="S5.T3.1.1.1.1.m1.1.1.cmml" type="integer" xref="S5.T3.1.1.1.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.m1.1c">8</annotation><annotation encoding="application/x-llamapun" id="S5.T3.1.1.1.1.m1.1d">8</annotation></semantics></math><span class="ltx_text" id="S5.T3.2.2.2.2.2" style="font-size:50%;"> Gen </span><math alttext="3" class="ltx_Math" display="inline" id="S5.T3.2.2.2.2.m2.1"><semantics id="S5.T3.2.2.2.2.m2.1a"><mn id="S5.T3.2.2.2.2.m2.1.1" mathsize="50%" xref="S5.T3.2.2.2.2.m2.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.2.m2.1b"><cn id="S5.T3.2.2.2.2.m2.1.1.cmml" type="integer" xref="S5.T3.2.2.2.2.m2.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.2.m2.1c">3</annotation><annotation encoding="application/x-llamapun" id="S5.T3.2.2.2.2.m2.1d">3</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.3.3.3.3">
<span class="ltx_text" id="S5.T3.3.3.3.3.1" style="font-size:50%;">Dimensity </span><math alttext="9300" class="ltx_Math" display="inline" id="S5.T3.3.3.3.3.m1.1"><semantics id="S5.T3.3.3.3.3.m1.1a"><mn id="S5.T3.3.3.3.3.m1.1.1" mathsize="50%" xref="S5.T3.3.3.3.3.m1.1.1.cmml">9300</mn><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.3.3.m1.1b"><cn id="S5.T3.3.3.3.3.m1.1.1.cmml" type="integer" xref="S5.T3.3.3.3.3.m1.1.1">9300</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.3.3.m1.1c">9300</annotation><annotation encoding="application/x-llamapun" id="S5.T3.3.3.3.3.m1.1d">9300</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.5.5.5.5">
<span class="ltx_text" id="S5.T3.5.5.5.5.1" style="font-size:50%;">Snapdragon </span><math alttext="8" class="ltx_Math" display="inline" id="S5.T3.4.4.4.4.m1.1"><semantics id="S5.T3.4.4.4.4.m1.1a"><mn id="S5.T3.4.4.4.4.m1.1.1" mathsize="50%" xref="S5.T3.4.4.4.4.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S5.T3.4.4.4.4.m1.1b"><cn id="S5.T3.4.4.4.4.m1.1.1.cmml" type="integer" xref="S5.T3.4.4.4.4.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.4.4.4.m1.1c">8</annotation><annotation encoding="application/x-llamapun" id="S5.T3.4.4.4.4.m1.1d">8</annotation></semantics></math><span class="ltx_text" id="S5.T3.5.5.5.5.2" style="font-size:50%;">+ Gen </span><math alttext="1" class="ltx_Math" display="inline" id="S5.T3.5.5.5.5.m2.1"><semantics id="S5.T3.5.5.5.5.m2.1a"><mn id="S5.T3.5.5.5.5.m2.1.1" mathsize="50%" xref="S5.T3.5.5.5.5.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S5.T3.5.5.5.5.m2.1b"><cn id="S5.T3.5.5.5.5.m2.1.1.cmml" type="integer" xref="S5.T3.5.5.5.5.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.5.5.5.5.m2.1c">1</annotation><annotation encoding="application/x-llamapun" id="S5.T3.5.5.5.5.m2.1d">1</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.6.6.6.6">
<span class="ltx_text" id="S5.T3.6.6.6.6.1" style="font-size:50%;">Kirin </span><math alttext="9000" class="ltx_Math" display="inline" id="S5.T3.6.6.6.6.m1.1"><semantics id="S5.T3.6.6.6.6.m1.1a"><mn id="S5.T3.6.6.6.6.m1.1.1" mathsize="50%" xref="S5.T3.6.6.6.6.m1.1.1.cmml">9000</mn><annotation-xml encoding="MathML-Content" id="S5.T3.6.6.6.6.m1.1b"><cn id="S5.T3.6.6.6.6.m1.1.1.cmml" type="integer" xref="S5.T3.6.6.6.6.m1.1.1">9000</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.6.6.6.6.m1.1c">9000</annotation><annotation encoding="application/x-llamapun" id="S5.T3.6.6.6.6.m1.1d">9000</annotation></semantics></math><span class="ltx_text" id="S5.T3.6.6.6.6.2" style="font-size:50%;">E</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.14.14.14">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.14.14.14.9"><span class="ltx_text" id="S5.T3.14.14.14.9.1" style="font-size:50%;">Prime</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.8.8.8.2">
<math alttext="1\times" class="ltx_math_unparsed" display="inline" id="S5.T3.7.7.7.1.m1.1"><semantics id="S5.T3.7.7.7.1.m1.1a"><mrow id="S5.T3.7.7.7.1.m1.1b"><mn id="S5.T3.7.7.7.1.m1.1.1" mathsize="50%">1</mn><mo id="S5.T3.7.7.7.1.m1.1.2" lspace="0.222em" mathsize="50%">×</mo></mrow><annotation encoding="application/x-tex" id="S5.T3.7.7.7.1.m1.1c">1\times</annotation><annotation encoding="application/x-llamapun" id="S5.T3.7.7.7.1.m1.1d">1 ×</annotation></semantics></math><span class="ltx_text" id="S5.T3.8.8.8.2.1" style="font-size:50%;"> Cortex-X4 (</span><math alttext="3.3" class="ltx_Math" display="inline" id="S5.T3.8.8.8.2.m2.1"><semantics id="S5.T3.8.8.8.2.m2.1a"><mn id="S5.T3.8.8.8.2.m2.1.1" mathsize="50%" xref="S5.T3.8.8.8.2.m2.1.1.cmml">3.3</mn><annotation-xml encoding="MathML-Content" id="S5.T3.8.8.8.2.m2.1b"><cn id="S5.T3.8.8.8.2.m2.1.1.cmml" type="float" xref="S5.T3.8.8.8.2.m2.1.1">3.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.8.8.8.2.m2.1c">3.3</annotation><annotation encoding="application/x-llamapun" id="S5.T3.8.8.8.2.m2.1d">3.3</annotation></semantics></math><span class="ltx_text" id="S5.T3.8.8.8.2.2" style="font-size:50%;">GHz)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.10.10.10.4">
<math alttext="1\times" class="ltx_math_unparsed" display="inline" id="S5.T3.9.9.9.3.m1.1"><semantics id="S5.T3.9.9.9.3.m1.1a"><mrow id="S5.T3.9.9.9.3.m1.1b"><mn id="S5.T3.9.9.9.3.m1.1.1" mathsize="50%">1</mn><mo id="S5.T3.9.9.9.3.m1.1.2" lspace="0.222em" mathsize="50%">×</mo></mrow><annotation encoding="application/x-tex" id="S5.T3.9.9.9.3.m1.1c">1\times</annotation><annotation encoding="application/x-llamapun" id="S5.T3.9.9.9.3.m1.1d">1 ×</annotation></semantics></math><span class="ltx_text" id="S5.T3.10.10.10.4.1" style="font-size:50%;"> Cortex-X4 (</span><math alttext="3.25" class="ltx_Math" display="inline" id="S5.T3.10.10.10.4.m2.1"><semantics id="S5.T3.10.10.10.4.m2.1a"><mn id="S5.T3.10.10.10.4.m2.1.1" mathsize="50%" xref="S5.T3.10.10.10.4.m2.1.1.cmml">3.25</mn><annotation-xml encoding="MathML-Content" id="S5.T3.10.10.10.4.m2.1b"><cn id="S5.T3.10.10.10.4.m2.1.1.cmml" type="float" xref="S5.T3.10.10.10.4.m2.1.1">3.25</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.10.10.10.4.m2.1c">3.25</annotation><annotation encoding="application/x-llamapun" id="S5.T3.10.10.10.4.m2.1d">3.25</annotation></semantics></math><span class="ltx_text" id="S5.T3.10.10.10.4.2" style="font-size:50%;">GHz)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.12.12.12.6">
<math alttext="1\times" class="ltx_math_unparsed" display="inline" id="S5.T3.11.11.11.5.m1.1"><semantics id="S5.T3.11.11.11.5.m1.1a"><mrow id="S5.T3.11.11.11.5.m1.1b"><mn id="S5.T3.11.11.11.5.m1.1.1" mathsize="50%">1</mn><mo id="S5.T3.11.11.11.5.m1.1.2" lspace="0.222em" mathsize="50%">×</mo></mrow><annotation encoding="application/x-tex" id="S5.T3.11.11.11.5.m1.1c">1\times</annotation><annotation encoding="application/x-llamapun" id="S5.T3.11.11.11.5.m1.1d">1 ×</annotation></semantics></math><span class="ltx_text" id="S5.T3.12.12.12.6.1" style="font-size:50%;"> Cortex-X2 (</span><math alttext="3.2" class="ltx_Math" display="inline" id="S5.T3.12.12.12.6.m2.1"><semantics id="S5.T3.12.12.12.6.m2.1a"><mn id="S5.T3.12.12.12.6.m2.1.1" mathsize="50%" xref="S5.T3.12.12.12.6.m2.1.1.cmml">3.2</mn><annotation-xml encoding="MathML-Content" id="S5.T3.12.12.12.6.m2.1b"><cn id="S5.T3.12.12.12.6.m2.1.1.cmml" type="float" xref="S5.T3.12.12.12.6.m2.1.1">3.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.12.12.12.6.m2.1c">3.2</annotation><annotation encoding="application/x-llamapun" id="S5.T3.12.12.12.6.m2.1d">3.2</annotation></semantics></math><span class="ltx_text" id="S5.T3.12.12.12.6.2" style="font-size:50%;">GHz)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.14.14.14.8">
<math alttext="1\times" class="ltx_math_unparsed" display="inline" id="S5.T3.13.13.13.7.m1.1"><semantics id="S5.T3.13.13.13.7.m1.1a"><mrow id="S5.T3.13.13.13.7.m1.1b"><mn id="S5.T3.13.13.13.7.m1.1.1" mathsize="50%">1</mn><mo id="S5.T3.13.13.13.7.m1.1.2" lspace="0.222em" mathsize="50%">×</mo></mrow><annotation encoding="application/x-tex" id="S5.T3.13.13.13.7.m1.1c">1\times</annotation><annotation encoding="application/x-llamapun" id="S5.T3.13.13.13.7.m1.1d">1 ×</annotation></semantics></math><span class="ltx_text" id="S5.T3.14.14.14.8.1" style="font-size:50%;"> Cortex-A77 (</span><math alttext="3.13" class="ltx_Math" display="inline" id="S5.T3.14.14.14.8.m2.1"><semantics id="S5.T3.14.14.14.8.m2.1a"><mn id="S5.T3.14.14.14.8.m2.1.1" mathsize="50%" xref="S5.T3.14.14.14.8.m2.1.1.cmml">3.13</mn><annotation-xml encoding="MathML-Content" id="S5.T3.14.14.14.8.m2.1b"><cn id="S5.T3.14.14.14.8.m2.1.1.cmml" type="float" xref="S5.T3.14.14.14.8.m2.1.1">3.13</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.14.14.14.8.m2.1c">3.13</annotation><annotation encoding="application/x-llamapun" id="S5.T3.14.14.14.8.m2.1d">3.13</annotation></semantics></math><span class="ltx_text" id="S5.T3.14.14.14.8.2" style="font-size:50%;">GHz)</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.26.26.26">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.26.26.26.13"><span class="ltx_text" id="S5.T3.26.26.26.13.1" style="font-size:50%;">Performance</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.18.18.18.4">
<span class="ltx_text" id="S5.T3.18.18.18.4.5"></span><span class="ltx_text" id="S5.T3.18.18.18.4.6" style="font-size:50%;"> </span><span class="ltx_text" id="S5.T3.18.18.18.4.4" style="font-size:50%;">
<span class="ltx_tabular ltx_align_middle" id="S5.T3.18.18.18.4.4.4">
<span class="ltx_tr" id="S5.T3.16.16.16.2.2.2.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.16.16.16.2.2.2.2.2"><math alttext="2\times" class="ltx_math_unparsed" display="inline" id="S5.T3.15.15.15.1.1.1.1.1.m1.1"><semantics id="S5.T3.15.15.15.1.1.1.1.1.m1.1a"><mrow id="S5.T3.15.15.15.1.1.1.1.1.m1.1b"><mn id="S5.T3.15.15.15.1.1.1.1.1.m1.1.1">2</mn><mo id="S5.T3.15.15.15.1.1.1.1.1.m1.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S5.T3.15.15.15.1.1.1.1.1.m1.1c">2\times</annotation><annotation encoding="application/x-llamapun" id="S5.T3.15.15.15.1.1.1.1.1.m1.1d">2 ×</annotation></semantics></math> Cortex-A720 (<math alttext="3.15" class="ltx_Math" display="inline" id="S5.T3.16.16.16.2.2.2.2.2.m2.1"><semantics id="S5.T3.16.16.16.2.2.2.2.2.m2.1a"><mn id="S5.T3.16.16.16.2.2.2.2.2.m2.1.1" xref="S5.T3.16.16.16.2.2.2.2.2.m2.1.1.cmml">3.15</mn><annotation-xml encoding="MathML-Content" id="S5.T3.16.16.16.2.2.2.2.2.m2.1b"><cn id="S5.T3.16.16.16.2.2.2.2.2.m2.1.1.cmml" type="float" xref="S5.T3.16.16.16.2.2.2.2.2.m2.1.1">3.15</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.16.16.16.2.2.2.2.2.m2.1c">3.15</annotation><annotation encoding="application/x-llamapun" id="S5.T3.16.16.16.2.2.2.2.2.m2.1d">3.15</annotation></semantics></math>GHz)</span></span>
<span class="ltx_tr" id="S5.T3.18.18.18.4.4.4.4">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.18.18.18.4.4.4.4.2"><math alttext="3\times" class="ltx_math_unparsed" display="inline" id="S5.T3.17.17.17.3.3.3.3.1.m1.1"><semantics id="S5.T3.17.17.17.3.3.3.3.1.m1.1a"><mrow id="S5.T3.17.17.17.3.3.3.3.1.m1.1b"><mn id="S5.T3.17.17.17.3.3.3.3.1.m1.1.1">3</mn><mo id="S5.T3.17.17.17.3.3.3.3.1.m1.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S5.T3.17.17.17.3.3.3.3.1.m1.1c">3\times</annotation><annotation encoding="application/x-llamapun" id="S5.T3.17.17.17.3.3.3.3.1.m1.1d">3 ×</annotation></semantics></math> Cortex-A720 (<math alttext="2.96" class="ltx_Math" display="inline" id="S5.T3.18.18.18.4.4.4.4.2.m2.1"><semantics id="S5.T3.18.18.18.4.4.4.4.2.m2.1a"><mn id="S5.T3.18.18.18.4.4.4.4.2.m2.1.1" xref="S5.T3.18.18.18.4.4.4.4.2.m2.1.1.cmml">2.96</mn><annotation-xml encoding="MathML-Content" id="S5.T3.18.18.18.4.4.4.4.2.m2.1b"><cn id="S5.T3.18.18.18.4.4.4.4.2.m2.1.1.cmml" type="float" xref="S5.T3.18.18.18.4.4.4.4.2.m2.1.1">2.96</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.18.18.18.4.4.4.4.2.m2.1c">2.96</annotation><annotation encoding="application/x-llamapun" id="S5.T3.18.18.18.4.4.4.4.2.m2.1d">2.96</annotation></semantics></math>GHz)</span></span>
</span></span><span class="ltx_text" id="S5.T3.18.18.18.4.7"></span><span class="ltx_text" id="S5.T3.18.18.18.4.8" style="font-size:50%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.22.22.22.8">
<span class="ltx_text" id="S5.T3.22.22.22.8.5"></span><span class="ltx_text" id="S5.T3.22.22.22.8.6" style="font-size:50%;"> </span><span class="ltx_text" id="S5.T3.22.22.22.8.4" style="font-size:50%;">
<span class="ltx_tabular ltx_align_middle" id="S5.T3.22.22.22.8.4.4">
<span class="ltx_tr" id="S5.T3.20.20.20.6.2.2.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.20.20.20.6.2.2.2.2"><math alttext="3\times" class="ltx_math_unparsed" display="inline" id="S5.T3.19.19.19.5.1.1.1.1.m1.1"><semantics id="S5.T3.19.19.19.5.1.1.1.1.m1.1a"><mrow id="S5.T3.19.19.19.5.1.1.1.1.m1.1b"><mn id="S5.T3.19.19.19.5.1.1.1.1.m1.1.1">3</mn><mo id="S5.T3.19.19.19.5.1.1.1.1.m1.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S5.T3.19.19.19.5.1.1.1.1.m1.1c">3\times</annotation><annotation encoding="application/x-llamapun" id="S5.T3.19.19.19.5.1.1.1.1.m1.1d">3 ×</annotation></semantics></math> Cortex-A720 (<math alttext="2.85" class="ltx_Math" display="inline" id="S5.T3.20.20.20.6.2.2.2.2.m2.1"><semantics id="S5.T3.20.20.20.6.2.2.2.2.m2.1a"><mn id="S5.T3.20.20.20.6.2.2.2.2.m2.1.1" xref="S5.T3.20.20.20.6.2.2.2.2.m2.1.1.cmml">2.85</mn><annotation-xml encoding="MathML-Content" id="S5.T3.20.20.20.6.2.2.2.2.m2.1b"><cn id="S5.T3.20.20.20.6.2.2.2.2.m2.1.1.cmml" type="float" xref="S5.T3.20.20.20.6.2.2.2.2.m2.1.1">2.85</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.20.20.20.6.2.2.2.2.m2.1c">2.85</annotation><annotation encoding="application/x-llamapun" id="S5.T3.20.20.20.6.2.2.2.2.m2.1d">2.85</annotation></semantics></math>GHz)</span></span>
<span class="ltx_tr" id="S5.T3.22.22.22.8.4.4.4">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.22.22.22.8.4.4.4.2"><math alttext="4\times" class="ltx_math_unparsed" display="inline" id="S5.T3.21.21.21.7.3.3.3.1.m1.1"><semantics id="S5.T3.21.21.21.7.3.3.3.1.m1.1a"><mrow id="S5.T3.21.21.21.7.3.3.3.1.m1.1b"><mn id="S5.T3.21.21.21.7.3.3.3.1.m1.1.1">4</mn><mo id="S5.T3.21.21.21.7.3.3.3.1.m1.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S5.T3.21.21.21.7.3.3.3.1.m1.1c">4\times</annotation><annotation encoding="application/x-llamapun" id="S5.T3.21.21.21.7.3.3.3.1.m1.1d">4 ×</annotation></semantics></math> Cortex-A720 (<math alttext="2" class="ltx_Math" display="inline" id="S5.T3.22.22.22.8.4.4.4.2.m2.1"><semantics id="S5.T3.22.22.22.8.4.4.4.2.m2.1a"><mn id="S5.T3.22.22.22.8.4.4.4.2.m2.1.1" xref="S5.T3.22.22.22.8.4.4.4.2.m2.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S5.T3.22.22.22.8.4.4.4.2.m2.1b"><cn id="S5.T3.22.22.22.8.4.4.4.2.m2.1.1.cmml" type="integer" xref="S5.T3.22.22.22.8.4.4.4.2.m2.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.22.22.22.8.4.4.4.2.m2.1c">2</annotation><annotation encoding="application/x-llamapun" id="S5.T3.22.22.22.8.4.4.4.2.m2.1d">2</annotation></semantics></math>GHz)</span></span>
</span></span><span class="ltx_text" id="S5.T3.22.22.22.8.7"></span><span class="ltx_text" id="S5.T3.22.22.22.8.8" style="font-size:50%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.24.24.24.10">
<span class="ltx_text" id="S5.T3.24.24.24.10.3"></span><span class="ltx_text" id="S5.T3.24.24.24.10.4" style="font-size:50%;"> </span><span class="ltx_text" id="S5.T3.24.24.24.10.2" style="font-size:50%;">
<span class="ltx_tabular ltx_align_middle" id="S5.T3.24.24.24.10.2.2">
<span class="ltx_tr" id="S5.T3.24.24.24.10.2.2.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.24.24.24.10.2.2.2.2"><math alttext="3\times" class="ltx_math_unparsed" display="inline" id="S5.T3.23.23.23.9.1.1.1.1.m1.1"><semantics id="S5.T3.23.23.23.9.1.1.1.1.m1.1a"><mrow id="S5.T3.23.23.23.9.1.1.1.1.m1.1b"><mn id="S5.T3.23.23.23.9.1.1.1.1.m1.1.1">3</mn><mo id="S5.T3.23.23.23.9.1.1.1.1.m1.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S5.T3.23.23.23.9.1.1.1.1.m1.1c">3\times</annotation><annotation encoding="application/x-llamapun" id="S5.T3.23.23.23.9.1.1.1.1.m1.1d">3 ×</annotation></semantics></math> Cortex-A710 (<math alttext="2.75" class="ltx_Math" display="inline" id="S5.T3.24.24.24.10.2.2.2.2.m2.1"><semantics id="S5.T3.24.24.24.10.2.2.2.2.m2.1a"><mn id="S5.T3.24.24.24.10.2.2.2.2.m2.1.1" xref="S5.T3.24.24.24.10.2.2.2.2.m2.1.1.cmml">2.75</mn><annotation-xml encoding="MathML-Content" id="S5.T3.24.24.24.10.2.2.2.2.m2.1b"><cn id="S5.T3.24.24.24.10.2.2.2.2.m2.1.1.cmml" type="float" xref="S5.T3.24.24.24.10.2.2.2.2.m2.1.1">2.75</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.24.24.24.10.2.2.2.2.m2.1c">2.75</annotation><annotation encoding="application/x-llamapun" id="S5.T3.24.24.24.10.2.2.2.2.m2.1d">2.75</annotation></semantics></math>GHz)</span></span>
</span></span><span class="ltx_text" id="S5.T3.24.24.24.10.5"></span><span class="ltx_text" id="S5.T3.24.24.24.10.6" style="font-size:50%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.26.26.26.12">
<span class="ltx_text" id="S5.T3.26.26.26.12.3"></span><span class="ltx_text" id="S5.T3.26.26.26.12.4" style="font-size:50%;"> </span><span class="ltx_text" id="S5.T3.26.26.26.12.2" style="font-size:50%;">
<span class="ltx_tabular ltx_align_middle" id="S5.T3.26.26.26.12.2.2">
<span class="ltx_tr" id="S5.T3.26.26.26.12.2.2.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.26.26.26.12.2.2.2.2"><math alttext="3\times" class="ltx_math_unparsed" display="inline" id="S5.T3.25.25.25.11.1.1.1.1.m1.1"><semantics id="S5.T3.25.25.25.11.1.1.1.1.m1.1a"><mrow id="S5.T3.25.25.25.11.1.1.1.1.m1.1b"><mn id="S5.T3.25.25.25.11.1.1.1.1.m1.1.1">3</mn><mo id="S5.T3.25.25.25.11.1.1.1.1.m1.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S5.T3.25.25.25.11.1.1.1.1.m1.1c">3\times</annotation><annotation encoding="application/x-llamapun" id="S5.T3.25.25.25.11.1.1.1.1.m1.1d">3 ×</annotation></semantics></math> Cortex-A77 (<math alttext="2.54" class="ltx_Math" display="inline" id="S5.T3.26.26.26.12.2.2.2.2.m2.1"><semantics id="S5.T3.26.26.26.12.2.2.2.2.m2.1a"><mn id="S5.T3.26.26.26.12.2.2.2.2.m2.1.1" xref="S5.T3.26.26.26.12.2.2.2.2.m2.1.1.cmml">2.54</mn><annotation-xml encoding="MathML-Content" id="S5.T3.26.26.26.12.2.2.2.2.m2.1b"><cn id="S5.T3.26.26.26.12.2.2.2.2.m2.1.1.cmml" type="float" xref="S5.T3.26.26.26.12.2.2.2.2.m2.1.1">2.54</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.26.26.26.12.2.2.2.2.m2.1c">2.54</annotation><annotation encoding="application/x-llamapun" id="S5.T3.26.26.26.12.2.2.2.2.m2.1d">2.54</annotation></semantics></math>GHz)</span></span>
</span></span><span class="ltx_text" id="S5.T3.26.26.26.12.5"></span><span class="ltx_text" id="S5.T3.26.26.26.12.6" style="font-size:50%;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.32.32.32">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.32.32.32.7"><span class="ltx_text" id="S5.T3.32.32.32.7.1" style="font-size:50%;">Efficiency</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.28.28.28.2">
<math alttext="2\times" class="ltx_math_unparsed" display="inline" id="S5.T3.27.27.27.1.m1.1"><semantics id="S5.T3.27.27.27.1.m1.1a"><mrow id="S5.T3.27.27.27.1.m1.1b"><mn id="S5.T3.27.27.27.1.m1.1.1" mathsize="50%">2</mn><mo id="S5.T3.27.27.27.1.m1.1.2" lspace="0.222em" mathsize="50%">×</mo></mrow><annotation encoding="application/x-tex" id="S5.T3.27.27.27.1.m1.1c">2\times</annotation><annotation encoding="application/x-llamapun" id="S5.T3.27.27.27.1.m1.1d">2 ×</annotation></semantics></math><span class="ltx_text" id="S5.T3.28.28.28.2.1" style="font-size:50%;"> Cortex-A520 (</span><math alttext="2.27" class="ltx_Math" display="inline" id="S5.T3.28.28.28.2.m2.1"><semantics id="S5.T3.28.28.28.2.m2.1a"><mn id="S5.T3.28.28.28.2.m2.1.1" mathsize="50%" xref="S5.T3.28.28.28.2.m2.1.1.cmml">2.27</mn><annotation-xml encoding="MathML-Content" id="S5.T3.28.28.28.2.m2.1b"><cn id="S5.T3.28.28.28.2.m2.1.1.cmml" type="float" xref="S5.T3.28.28.28.2.m2.1.1">2.27</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.28.28.28.2.m2.1c">2.27</annotation><annotation encoding="application/x-llamapun" id="S5.T3.28.28.28.2.m2.1d">2.27</annotation></semantics></math><span class="ltx_text" id="S5.T3.28.28.28.2.2" style="font-size:50%;">GHz)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.32.32.32.8"><span class="ltx_text" id="S5.T3.32.32.32.8.1" style="font-size:50%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.30.30.30.4">
<math alttext="4\times" class="ltx_math_unparsed" display="inline" id="S5.T3.29.29.29.3.m1.1"><semantics id="S5.T3.29.29.29.3.m1.1a"><mrow id="S5.T3.29.29.29.3.m1.1b"><mn id="S5.T3.29.29.29.3.m1.1.1" mathsize="50%">4</mn><mo id="S5.T3.29.29.29.3.m1.1.2" lspace="0.222em" mathsize="50%">×</mo></mrow><annotation encoding="application/x-tex" id="S5.T3.29.29.29.3.m1.1c">4\times</annotation><annotation encoding="application/x-llamapun" id="S5.T3.29.29.29.3.m1.1d">4 ×</annotation></semantics></math><span class="ltx_text" id="S5.T3.30.30.30.4.1" style="font-size:50%;"> Cortex-A510 (</span><math alttext="2" class="ltx_Math" display="inline" id="S5.T3.30.30.30.4.m2.1"><semantics id="S5.T3.30.30.30.4.m2.1a"><mn id="S5.T3.30.30.30.4.m2.1.1" mathsize="50%" xref="S5.T3.30.30.30.4.m2.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S5.T3.30.30.30.4.m2.1b"><cn id="S5.T3.30.30.30.4.m2.1.1.cmml" type="integer" xref="S5.T3.30.30.30.4.m2.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.30.30.30.4.m2.1c">2</annotation><annotation encoding="application/x-llamapun" id="S5.T3.30.30.30.4.m2.1d">2</annotation></semantics></math><span class="ltx_text" id="S5.T3.30.30.30.4.2" style="font-size:50%;">GHz)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.32.32.32.6">
<math alttext="4\times" class="ltx_math_unparsed" display="inline" id="S5.T3.31.31.31.5.m1.1"><semantics id="S5.T3.31.31.31.5.m1.1a"><mrow id="S5.T3.31.31.31.5.m1.1b"><mn id="S5.T3.31.31.31.5.m1.1.1" mathsize="50%">4</mn><mo id="S5.T3.31.31.31.5.m1.1.2" lspace="0.222em" mathsize="50%">×</mo></mrow><annotation encoding="application/x-tex" id="S5.T3.31.31.31.5.m1.1c">4\times</annotation><annotation encoding="application/x-llamapun" id="S5.T3.31.31.31.5.m1.1d">4 ×</annotation></semantics></math><span class="ltx_text" id="S5.T3.32.32.32.6.1" style="font-size:50%;"> Cortex-A55 (</span><math alttext="2.05" class="ltx_Math" display="inline" id="S5.T3.32.32.32.6.m2.1"><semantics id="S5.T3.32.32.32.6.m2.1a"><mn id="S5.T3.32.32.32.6.m2.1.1" mathsize="50%" xref="S5.T3.32.32.32.6.m2.1.1.cmml">2.05</mn><annotation-xml encoding="MathML-Content" id="S5.T3.32.32.32.6.m2.1b"><cn id="S5.T3.32.32.32.6.m2.1.1.cmml" type="float" xref="S5.T3.32.32.32.6.m2.1.1">2.05</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.32.32.32.6.m2.1c">2.05</annotation><annotation encoding="application/x-llamapun" id="S5.T3.32.32.32.6.m2.1d">2.05</annotation></semantics></math><span class="ltx_text" id="S5.T3.32.32.32.6.2" style="font-size:50%;">GHz)</span>
</td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S5.SS1.SSS1.p1">
<p class="ltx_p" id="S5.SS1.SSS1.p1.1">Most popular mobile SoCs utilize the "big.LITTLE" architecture for their CPUs, which balances performance with power efficiency. This configuration typically includes multiple cores organized into two distinct clusters: "big" (prime and performance cores) and "little" (efficiency cores), as illustrated in Table 3. While it is commonly assumed that high-load tasks are best handled by the "big" cores for optimal performance, our tests reveal that the ideal core configuration can vary across the two stages of LLM inference.</p>
</div>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="423" id="S5.F5.g1" src="x5.png" width="706"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S5.F5.3.2" style="font-size:90%;">Throughput with multi-threads.</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.SSS1.p2">
<p class="ltx_p" id="S5.SS1.SSS1.p2.1">We evaluate three core configurations: using only big cores, a combination of big and little cores, and all available cores. To enable parallel inference across multiple CPU cores, we adjust the number of running threads. Since inference threads are prioritized to run on big cores, the number of running threads implies which cores are active. For instance, on the Snapdragon 8 Gen 3 (which has six big cores), six threads correspond to all big cores, while on the Snapdragon 8+ Gen 1 (which has only four big cores), the same configuration includes all big cores and two little cores.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p3">
<p class="ltx_p" id="S5.SS1.SSS1.p3.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.F5" title="Figure 5 ‣ 5.1.1 Multi-threads on CPU Cores ‣ 5.1 Hardware Capabilities ‣ 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates how these core configurations impact inference speed across the four tested devices. During the prefill stage, prompt processing speed is primarily driven by the big cores. The contribution of the little cores to acceleration depends on the performance gap between the big and little cores. On devices with limited computational power, additional efficiency cores can help accelerate inference. For instance, utilizing all cores delivers the best performance on the Kirin 9000E.
However, for devices with more powerful big cores, adding little cores can actually reduce performance. For example, on the Snapdragon 8 Gen 3, incorporating two little cores results in a slowdown in inference speed. This highlights the importance of optimizing core configurations based on each device’s specific CPU capabilities to maximize efficiency. Interestingly, the Dimensity 9300 benefits from using all cores because of its All-Big-Core architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib30" title="">30</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p4">
<p class="ltx_p" id="S5.SS1.SSS1.p4.1">In the decoding phase, adding more little cores typically leads to a decline in performance. For instance, both the Kirin 9000E and Snapdragon 8+ Gen 1 achieve their peak decoding speeds with four active big cores. This indicates that the maximum decoding speed on a device is largely dictated by the performance of the big cores. Unlike the prefill stage, decoding is more heavily constrained by memory bandwidth. This is evident in the fact that adding more big cores results in a smaller performance improvement compared to prefill, while incorporating little cores leads to a more noticeable performance drop.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2 </span>Speedup with Special Machine Instructions</h4>
<div class="ltx_para" id="S5.SS1.SSS2.p1">
<p class="ltx_p" id="S5.SS1.SSS2.p1.1">CPUs are generally more efficient at handling integer (INT) operations compared to floating-point (Float) computations. This performance advantage in INT operations stems from two key factors: first, CPUs typically offer higher instruction throughput for INT operations than for floating-point ones. Second, for matrix multiplication tasks, many CPUs support specialized INT8 instructions, which further accelerate these computations.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p2">
<p class="ltx_p" id="S5.SS1.SSS2.p2.1">To further explore the potential of Arm CPUs, we test the Llama-2-7B model on top-tier SoCs and recompiled llama.cpp directly on the device with the i8mm flag to enable INT8 matrix multiplication instructions. This ensures that the machine code consists of <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS2.p2.1.1">smmla<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_upright" id="S5.SS1.SSS2.p2.1.1.1.1">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib9" title="">9</a><span class="ltx_text ltx_font_upright" id="S5.SS1.SSS2.p2.1.1.2.2">]</span></cite></span> and <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS2.p2.1.2">sdot<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_upright" id="S5.SS1.SSS2.p2.1.2.1.1">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib7" title="">7</a><span class="ltx_text ltx_font_upright" id="S5.SS1.SSS2.p2.1.2.2.2">]</span></cite></span>. The <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS2.p2.1.3">sdot</span> instruction accelerates vector computations by processing multiple INT8 dot products simultaneously, making it well-suited for matrix-vector multiplication during the decoding. In contrast, the <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS2.p2.1.4">smmla</span> instruction handles block matrix multiplication and delivers twice the multiply-accumulate efficiency of <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS2.p2.1.5">sdot</span>, making it ideal for accelerating matrix operations in the prefill stage.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p3">
<p class="ltx_p" id="S5.SS1.SSS2.p3.1">We note that Arm developers have proposed the prearrangement of weights in blocks to avoid pseudo-scalar operations and fully exploit the parallel capabilities of specialized instructions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib18" title="">18</a>]</cite>. In the original weight layout, only a single column of weights is processed at a time, and it requires multiple dot product operations. The optimized layout, however, distributes the weight columns across multiple computational lanes, enabling parallel processing of several weight columns in a single instruction. This approach not only increases throughput but also reduces memory access by sharing input data within lanes, further enhancing inference efficiency.</p>
</div>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="498" id="S5.F6.g1" src="x6.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S5.F6.3.2" style="font-size:90%;">Inference with INT8 Matrix Multiplication.</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.SSS2.p4">
<p class="ltx_p" id="S5.SS1.SSS2.p4.2">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.F6" title="Figure 6 ‣ 5.1.2 Speedup with Special Machine Instructions ‣ 5.1 Hardware Capabilities ‣ 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">6</span></a> highlights a notable performance boost. When focusing on peak inference performance, prefill speed is accelerated by <math alttext="4\times" class="ltx_math_unparsed" display="inline" id="S5.SS1.SSS2.p4.1.m1.1"><semantics id="S5.SS1.SSS2.p4.1.m1.1a"><mrow id="S5.SS1.SSS2.p4.1.m1.1b"><mn id="S5.SS1.SSS2.p4.1.m1.1.1">4</mn><mo id="S5.SS1.SSS2.p4.1.m1.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S5.SS1.SSS2.p4.1.m1.1c">4\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS2.p4.1.m1.1d">4 ×</annotation></semantics></math> under the same configuration. This speedup is primarily achieved by increasing throughput, allowing more computations to be processed within a single instruction.
On the Snapdragon 8 Gen 3 (Xiaomi 14 Pro), adding all efficiency cores (little cores) results in a <math alttext="60\%" class="ltx_Math" display="inline" id="S5.SS1.SSS2.p4.2.m2.1"><semantics id="S5.SS1.SSS2.p4.2.m2.1a"><mrow id="S5.SS1.SSS2.p4.2.m2.1.1" xref="S5.SS1.SSS2.p4.2.m2.1.1.cmml"><mn id="S5.SS1.SSS2.p4.2.m2.1.1.2" xref="S5.SS1.SSS2.p4.2.m2.1.1.2.cmml">60</mn><mo id="S5.SS1.SSS2.p4.2.m2.1.1.1" xref="S5.SS1.SSS2.p4.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS2.p4.2.m2.1b"><apply id="S5.SS1.SSS2.p4.2.m2.1.1.cmml" xref="S5.SS1.SSS2.p4.2.m2.1.1"><csymbol cd="latexml" id="S5.SS1.SSS2.p4.2.m2.1.1.1.cmml" xref="S5.SS1.SSS2.p4.2.m2.1.1.1">percent</csymbol><cn id="S5.SS1.SSS2.p4.2.m2.1.1.2.cmml" type="integer" xref="S5.SS1.SSS2.p4.2.m2.1.1.2">60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS2.p4.2.m2.1c">60\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS2.p4.2.m2.1d">60 %</annotation></semantics></math> decline compared to using only the big cores. This suggests that the lower frequency and IPC (instructions per cycle) of efficiency cores may hinder performance during high-demand tasks.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>GPU specifications and corresponding achievable performance.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T4.8" style="width:227.6pt;height:81pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-38.0pt,13.5pt) scale(0.749861353664711,0.749861353664711) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.8.8">
<tr class="ltx_tr" id="S5.T4.8.8.9">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T4.8.8.9.1"><span class="ltx_text" id="S5.T4.8.8.9.1.1" style="font-size:90%;">GPU Name</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T4.8.8.9.2"><span class="ltx_text" id="S5.T4.8.8.9.2.1" style="font-size:90%;">FP16</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T4.8.8.9.3"><span class="ltx_text" id="S5.T4.8.8.9.3.1" style="font-size:90%;">Est./Theoretical Max</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.8.8.10">
<td class="ltx_td" id="S5.T4.8.8.10.1"></td>
<td class="ltx_td ltx_align_center" id="S5.T4.8.8.10.2"><span class="ltx_text" id="S5.T4.8.8.10.2.1" style="font-size:90%;">(GFLOPS)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.8.8.10.3"><span class="ltx_text" id="S5.T4.8.8.10.3.1" style="font-size:90%;">Memory Bandwidth (GB/s)</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.2.2.3"><span class="ltx_text" id="S5.T4.2.2.2.3.1" style="font-size:90%;">Mali-G78 MP22</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.1.1.1"><math alttext="1010" class="ltx_Math" display="inline" id="S5.T4.1.1.1.1.m1.1"><semantics id="S5.T4.1.1.1.1.m1.1a"><mn id="S5.T4.1.1.1.1.m1.1.1" mathsize="90%" xref="S5.T4.1.1.1.1.m1.1.1.cmml">1010</mn><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.m1.1b"><cn id="S5.T4.1.1.1.1.m1.1.1.cmml" type="integer" xref="S5.T4.1.1.1.1.m1.1.1">1010</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.m1.1c">1010</annotation><annotation encoding="application/x-llamapun" id="S5.T4.1.1.1.1.m1.1d">1010</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.2.2.2">
<math alttext="26" class="ltx_Math" display="inline" id="S5.T4.2.2.2.2.m1.1"><semantics id="S5.T4.2.2.2.2.m1.1a"><mn id="S5.T4.2.2.2.2.m1.1.1" mathsize="90%" xref="S5.T4.2.2.2.2.m1.1.1.cmml">26</mn><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.2.m1.1b"><cn id="S5.T4.2.2.2.2.m1.1.1.cmml" type="integer" xref="S5.T4.2.2.2.2.m1.1.1">26</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.2.m1.1c">26</annotation><annotation encoding="application/x-llamapun" id="S5.T4.2.2.2.2.m1.1d">26</annotation></semantics></math><span class="ltx_text" id="S5.T4.2.2.2.2.1" style="font-size:90%;">/-</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.4.4.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.4.4.3"><span class="ltx_text" id="S5.T4.4.4.4.3.1" style="font-size:90%;">Mali-G720 Immortalis MP12</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.3.1"><math alttext="3418" class="ltx_Math" display="inline" id="S5.T4.3.3.3.1.m1.1"><semantics id="S5.T4.3.3.3.1.m1.1a"><mn id="S5.T4.3.3.3.1.m1.1.1" mathsize="90%" xref="S5.T4.3.3.3.1.m1.1.1.cmml">3418</mn><annotation-xml encoding="MathML-Content" id="S5.T4.3.3.3.1.m1.1b"><cn id="S5.T4.3.3.3.1.m1.1.1.cmml" type="integer" xref="S5.T4.3.3.3.1.m1.1.1">3418</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.3.3.3.1.m1.1c">3418</annotation><annotation encoding="application/x-llamapun" id="S5.T4.3.3.3.1.m1.1d">3418</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.4.4.4.2">
<math alttext="48" class="ltx_Math" display="inline" id="S5.T4.4.4.4.2.m1.1"><semantics id="S5.T4.4.4.4.2.m1.1a"><mn id="S5.T4.4.4.4.2.m1.1.1" mathsize="90%" xref="S5.T4.4.4.4.2.m1.1.1.cmml">48</mn><annotation-xml encoding="MathML-Content" id="S5.T4.4.4.4.2.m1.1b"><cn id="S5.T4.4.4.4.2.m1.1.1.cmml" type="integer" xref="S5.T4.4.4.4.2.m1.1.1">48</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.4.4.2.m1.1c">48</annotation><annotation encoding="application/x-llamapun" id="S5.T4.4.4.4.2.m1.1d">48</annotation></semantics></math><span class="ltx_text" id="S5.T4.4.4.4.2.1" style="font-size:90%;">/77</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.6.6.6">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.6.6.6.3"><span class="ltx_text" id="S5.T4.6.6.6.3.1" style="font-size:90%;">Adreno 730</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.5.5.5.1"><math alttext="1583" class="ltx_Math" display="inline" id="S5.T4.5.5.5.1.m1.1"><semantics id="S5.T4.5.5.5.1.m1.1a"><mn id="S5.T4.5.5.5.1.m1.1.1" mathsize="90%" xref="S5.T4.5.5.5.1.m1.1.1.cmml">1583</mn><annotation-xml encoding="MathML-Content" id="S5.T4.5.5.5.1.m1.1b"><cn id="S5.T4.5.5.5.1.m1.1.1.cmml" type="integer" xref="S5.T4.5.5.5.1.m1.1.1">1583</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.5.5.5.1.m1.1c">1583</annotation><annotation encoding="application/x-llamapun" id="S5.T4.5.5.5.1.m1.1d">1583</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.6.6.6.2">
<math alttext="39" class="ltx_Math" display="inline" id="S5.T4.6.6.6.2.m1.1"><semantics id="S5.T4.6.6.6.2.m1.1a"><mn id="S5.T4.6.6.6.2.m1.1.1" mathsize="90%" xref="S5.T4.6.6.6.2.m1.1.1.cmml">39</mn><annotation-xml encoding="MathML-Content" id="S5.T4.6.6.6.2.m1.1b"><cn id="S5.T4.6.6.6.2.m1.1.1.cmml" type="integer" xref="S5.T4.6.6.6.2.m1.1.1">39</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.6.6.6.2.m1.1c">39</annotation><annotation encoding="application/x-llamapun" id="S5.T4.6.6.6.2.m1.1d">39</annotation></semantics></math><span class="ltx_text" id="S5.T4.6.6.6.2.1" style="font-size:90%;">/-</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.8.8.8">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T4.8.8.8.3"><span class="ltx_text" id="S5.T4.8.8.8.3.1" style="font-size:90%;">Adreno 750</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T4.7.7.7.1"><math alttext="2232" class="ltx_Math" display="inline" id="S5.T4.7.7.7.1.m1.1"><semantics id="S5.T4.7.7.7.1.m1.1a"><mn id="S5.T4.7.7.7.1.m1.1.1" mathsize="90%" xref="S5.T4.7.7.7.1.m1.1.1.cmml">2232</mn><annotation-xml encoding="MathML-Content" id="S5.T4.7.7.7.1.m1.1b"><cn id="S5.T4.7.7.7.1.m1.1.1.cmml" type="integer" xref="S5.T4.7.7.7.1.m1.1.1">2232</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.7.7.7.1.m1.1c">2232</annotation><annotation encoding="application/x-llamapun" id="S5.T4.7.7.7.1.m1.1d">2232</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T4.8.8.8.2">
<math alttext="63" class="ltx_Math" display="inline" id="S5.T4.8.8.8.2.m1.1"><semantics id="S5.T4.8.8.8.2.m1.1a"><mn id="S5.T4.8.8.8.2.m1.1.1" mathsize="90%" xref="S5.T4.8.8.8.2.m1.1.1.cmml">63</mn><annotation-xml encoding="MathML-Content" id="S5.T4.8.8.8.2.m1.1b"><cn id="S5.T4.8.8.8.2.m1.1.1.cmml" type="integer" xref="S5.T4.8.8.8.2.m1.1.1">63</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.8.8.8.2.m1.1c">63</annotation><annotation encoding="application/x-llamapun" id="S5.T4.8.8.8.2.m1.1d">63</annotation></semantics></math><span class="ltx_text" id="S5.T4.8.8.8.2.1" style="font-size:90%;">/77</span>
</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.3 </span>Utilization of GPU units</h4>
<div class="ltx_para" id="S5.SS1.SSS3.p1">
<p class="ltx_p" id="S5.SS1.SSS3.p1.5">We note an intriguing exception regarding performance on Mali GPUs. The Mali-G<math alttext="720" class="ltx_Math" display="inline" id="S5.SS1.SSS3.p1.1.m1.1"><semantics id="S5.SS1.SSS3.p1.1.m1.1a"><mn id="S5.SS1.SSS3.p1.1.m1.1.1" xref="S5.SS1.SSS3.p1.1.m1.1.1.cmml">720</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS3.p1.1.m1.1b"><cn id="S5.SS1.SSS3.p1.1.m1.1.1.cmml" type="integer" xref="S5.SS1.SSS3.p1.1.m1.1.1">720</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS3.p1.1.m1.1c">720</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS3.p1.1.m1.1d">720</annotation></semantics></math> Immortalis MP12, found in the Dimensity 9300, has better hardware parameters but has poorer performance than Adreno <math alttext="750" class="ltx_Math" display="inline" id="S5.SS1.SSS3.p1.2.m2.1"><semantics id="S5.SS1.SSS3.p1.2.m2.1a"><mn id="S5.SS1.SSS3.p1.2.m2.1.1" xref="S5.SS1.SSS3.p1.2.m2.1.1.cmml">750</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS3.p1.2.m2.1b"><cn id="S5.SS1.SSS3.p1.2.m2.1.1.cmml" type="integer" xref="S5.SS1.SSS3.p1.2.m2.1.1">750</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS3.p1.2.m2.1c">750</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS3.p1.2.m2.1d">750</annotation></semantics></math> in Snapdragon SoCs. We measure the maximum throughput for FP16 operations on each device using clpeak<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib24" title="">24</a>]</cite>. It is a benchmarking tool intended for calculating GPU performance by executing different OpenCL kernels on a particular device. We also measure the actual maximum memory bandwidth using clpeak and present the maximum theoretical bandwidth in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.T4" title="Table 4 ‣ 5.1.2 Speedup with Special Machine Instructions ‣ 5.1 Hardware Capabilities ‣ 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">4</span></a>. From Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.T4" title="Table 4 ‣ 5.1.2 Speedup with Special Machine Instructions ‣ 5.1 Hardware Capabilities ‣ 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">4</span></a>, we can see that the Mali-G720 Immortalis MP12 exhibits <math alttext="1.5" class="ltx_Math" display="inline" id="S5.SS1.SSS3.p1.3.m3.1"><semantics id="S5.SS1.SSS3.p1.3.m3.1a"><mn id="S5.SS1.SSS3.p1.3.m3.1.1" xref="S5.SS1.SSS3.p1.3.m3.1.1.cmml">1.5</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS3.p1.3.m3.1b"><cn id="S5.SS1.SSS3.p1.3.m3.1.1.cmml" type="float" xref="S5.SS1.SSS3.p1.3.m3.1.1">1.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS3.p1.3.m3.1c">1.5</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS3.p1.3.m3.1d">1.5</annotation></semantics></math> times the throughput of the Adreno <math alttext="750" class="ltx_Math" display="inline" id="S5.SS1.SSS3.p1.4.m4.1"><semantics id="S5.SS1.SSS3.p1.4.m4.1a"><mn id="S5.SS1.SSS3.p1.4.m4.1.1" xref="S5.SS1.SSS3.p1.4.m4.1.1.cmml">750</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS3.p1.4.m4.1b"><cn id="S5.SS1.SSS3.p1.4.m4.1.1.cmml" type="integer" xref="S5.SS1.SSS3.p1.4.m4.1.1">750</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS3.p1.4.m4.1c">750</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS3.p1.4.m4.1d">750</annotation></semantics></math> in float16 operations. However, in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S4.F3" title="Figure 3 ‣ 4.1.1 Performance on CPUs ‣ 4.1 Token Throughput and Memory Footprints ‣ 4 Performance: Users’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">3</span></a>, it shows a prefill speed that is <math alttext="5.3" class="ltx_Math" display="inline" id="S5.SS1.SSS3.p1.5.m5.1"><semantics id="S5.SS1.SSS3.p1.5.m5.1a"><mn id="S5.SS1.SSS3.p1.5.m5.1.1" xref="S5.SS1.SSS3.p1.5.m5.1.1.cmml">5.3</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS3.p1.5.m5.1b"><cn id="S5.SS1.SSS3.p1.5.m5.1.1.cmml" type="float" xref="S5.SS1.SSS3.p1.5.m5.1.1">5.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS3.p1.5.m5.1c">5.3</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS3.p1.5.m5.1d">5.3</annotation></semantics></math> times slower.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS3.p2">
<p class="ltx_p" id="S5.SS1.SSS3.p2.6">The possible reason for this discrepancy comes from the GPU utilization. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.F7" title="Figure 7 ‣ 5.1.3 Utilization of GPU units ‣ 5.1 Hardware Capabilities ‣ 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">7</span></a> presents the average utilization and memory bandwidth during inference on Mali-G720 and Adreno 750. It shows that MLC LLM does not efficiently utilize the Mali GPU ALUs.
During the compute-bound prefill stage, the Mali-G720’s arithmetic unit utilization averages less than <math alttext="3\%" class="ltx_Math" display="inline" id="S5.SS1.SSS3.p2.1.m1.1"><semantics id="S5.SS1.SSS3.p2.1.m1.1a"><mrow id="S5.SS1.SSS3.p2.1.m1.1.1" xref="S5.SS1.SSS3.p2.1.m1.1.1.cmml"><mn id="S5.SS1.SSS3.p2.1.m1.1.1.2" xref="S5.SS1.SSS3.p2.1.m1.1.1.2.cmml">3</mn><mo id="S5.SS1.SSS3.p2.1.m1.1.1.1" xref="S5.SS1.SSS3.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS3.p2.1.m1.1b"><apply id="S5.SS1.SSS3.p2.1.m1.1.1.cmml" xref="S5.SS1.SSS3.p2.1.m1.1.1"><csymbol cd="latexml" id="S5.SS1.SSS3.p2.1.m1.1.1.1.cmml" xref="S5.SS1.SSS3.p2.1.m1.1.1.1">percent</csymbol><cn id="S5.SS1.SSS3.p2.1.m1.1.1.2.cmml" type="integer" xref="S5.SS1.SSS3.p2.1.m1.1.1.2">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS3.p2.1.m1.1c">3\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS3.p2.1.m1.1d">3 %</annotation></semantics></math>, while the Adreno <math alttext="750" class="ltx_Math" display="inline" id="S5.SS1.SSS3.p2.2.m2.1"><semantics id="S5.SS1.SSS3.p2.2.m2.1a"><mn id="S5.SS1.SSS3.p2.2.m2.1.1" xref="S5.SS1.SSS3.p2.2.m2.1.1.cmml">750</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS3.p2.2.m2.1b"><cn id="S5.SS1.SSS3.p2.2.m2.1.1.cmml" type="integer" xref="S5.SS1.SSS3.p2.2.m2.1.1">750</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS3.p2.2.m2.1c">750</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS3.p2.2.m2.1d">750</annotation></semantics></math> achieves around <math alttext="20\%" class="ltx_Math" display="inline" id="S5.SS1.SSS3.p2.3.m3.1"><semantics id="S5.SS1.SSS3.p2.3.m3.1a"><mrow id="S5.SS1.SSS3.p2.3.m3.1.1" xref="S5.SS1.SSS3.p2.3.m3.1.1.cmml"><mn id="S5.SS1.SSS3.p2.3.m3.1.1.2" xref="S5.SS1.SSS3.p2.3.m3.1.1.2.cmml">20</mn><mo id="S5.SS1.SSS3.p2.3.m3.1.1.1" xref="S5.SS1.SSS3.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS3.p2.3.m3.1b"><apply id="S5.SS1.SSS3.p2.3.m3.1.1.cmml" xref="S5.SS1.SSS3.p2.3.m3.1.1"><csymbol cd="latexml" id="S5.SS1.SSS3.p2.3.m3.1.1.1.cmml" xref="S5.SS1.SSS3.p2.3.m3.1.1.1">percent</csymbol><cn id="S5.SS1.SSS3.p2.3.m3.1.1.2.cmml" type="integer" xref="S5.SS1.SSS3.p2.3.m3.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS3.p2.3.m3.1c">20\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS3.p2.3.m3.1d">20 %</annotation></semantics></math>. Similarly, in the memory-bound decoding stage, the Mali-G<math alttext="720" class="ltx_Math" display="inline" id="S5.SS1.SSS3.p2.4.m4.1"><semantics id="S5.SS1.SSS3.p2.4.m4.1a"><mn id="S5.SS1.SSS3.p2.4.m4.1.1" xref="S5.SS1.SSS3.p2.4.m4.1.1.cmml">720</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS3.p2.4.m4.1b"><cn id="S5.SS1.SSS3.p2.4.m4.1.1.cmml" type="integer" xref="S5.SS1.SSS3.p2.4.m4.1.1">720</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS3.p2.4.m4.1c">720</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS3.p2.4.m4.1d">720</annotation></semantics></math> achieves a real-time memory read bandwidth of <math alttext="26.8" class="ltx_Math" display="inline" id="S5.SS1.SSS3.p2.5.m5.1"><semantics id="S5.SS1.SSS3.p2.5.m5.1a"><mn id="S5.SS1.SSS3.p2.5.m5.1.1" xref="S5.SS1.SSS3.p2.5.m5.1.1.cmml">26.8</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS3.p2.5.m5.1b"><cn id="S5.SS1.SSS3.p2.5.m5.1.1.cmml" type="float" xref="S5.SS1.SSS3.p2.5.m5.1.1">26.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS3.p2.5.m5.1c">26.8</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS3.p2.5.m5.1d">26.8</annotation></semantics></math>GB/s, which is only roughly two-thirds of the <math alttext="42.9" class="ltx_Math" display="inline" id="S5.SS1.SSS3.p2.6.m6.1"><semantics id="S5.SS1.SSS3.p2.6.m6.1a"><mn id="S5.SS1.SSS3.p2.6.m6.1.1" xref="S5.SS1.SSS3.p2.6.m6.1.1.cmml">42.9</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS3.p2.6.m6.1b"><cn id="S5.SS1.SSS3.p2.6.m6.1.1.cmml" type="float" xref="S5.SS1.SSS3.p2.6.m6.1.1">42.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS3.p2.6.m6.1c">42.9</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS3.p2.6.m6.1d">42.9</annotation></semantics></math>GB/s of the Adreno 750. These results indicate existing LLM implementations cannot fully utilize the available computing resources on mobile GPUs. It implies that while OpenCL serves as a general-purpose backend, it is essential to implement GPU-specific kernels to fully leverage the hardware capabilities.</p>
</div>
<figure class="ltx_figure" id="S5.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S5.F7.1" style="width:227.6pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="452" id="S5.F7.1.g1" src="x7.png" width="968"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F7.1.1.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S5.F7.1.2.2" style="font-size:90%;">Utilization of GPU units</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S5.F7.2" style="width:227.6pt;"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="415" id="S5.F7.2.g1" src="x8.png" width="968"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F7.2.1.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S5.F7.2.2.2" style="font-size:90%;">Memory Bandwidth</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F7.4.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S5.F7.5.2" style="font-size:90%;">GPU Utilization</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Impact of DVFS</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">In Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S4.SS1.SSS1" title="4.1.1 Performance on CPUs ‣ 4.1 Token Throughput and Memory Footprints ‣ 4 Performance: Users’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">4.1.1</span></a>, we observed performance degradation as the length of the prompt and generation increased. This issue relates to the system’s ability to handle long contexts and maintain consistent inference performance across multiple rounds of dialogue. Although previous studies, such as <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib25" title="">25</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib27" title="">27</a>]</cite>, have discussed potential causes like Dynamic Voltage and Frequency Scaling (DVFS) and thermal throttling, they lack detailed analysis on how DVFS specifically impacts inference performance and whether its effects differ across hardware and operating systems. To address this, we analyze real-time variations in CPU frequency and utilization during 20 continuous inference tests on four devices with warm up, covering different operating systems (Android, HyperOS, HarmonyOS) and vendors (MediaTek, Qualcomm, Hisilicon).</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.9">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.F9" title="Figure 9 ‣ 5.2 Impact of DVFS ‣ 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">9</span></a> illustrates a general decline in performance as the rounds of inference increase. Specifically, on the Snapdragon <math alttext="8" class="ltx_Math" display="inline" id="S5.SS2.p2.1.m1.1"><semantics id="S5.SS2.p2.1.m1.1a"><mn id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><cn id="S5.SS2.p2.1.m1.1.1.cmml" type="integer" xref="S5.SS2.p2.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">8</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.1.m1.1d">8</annotation></semantics></math> Gen <math alttext="3" class="ltx_Math" display="inline" id="S5.SS2.p2.2.m2.1"><semantics id="S5.SS2.p2.2.m2.1a"><mn id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><cn id="S5.SS2.p2.2.m2.1.1.cmml" type="integer" xref="S5.SS2.p2.2.m2.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">3</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.2.m2.1d">3</annotation></semantics></math> (on Xiaomi14 Pro), latency increases by up to <math alttext="30\%" class="ltx_Math" display="inline" id="S5.SS2.p2.3.m3.1"><semantics id="S5.SS2.p2.3.m3.1a"><mrow id="S5.SS2.p2.3.m3.1.1" xref="S5.SS2.p2.3.m3.1.1.cmml"><mn id="S5.SS2.p2.3.m3.1.1.2" xref="S5.SS2.p2.3.m3.1.1.2.cmml">30</mn><mo id="S5.SS2.p2.3.m3.1.1.1" xref="S5.SS2.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.3.m3.1b"><apply id="S5.SS2.p2.3.m3.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1"><csymbol cd="latexml" id="S5.SS2.p2.3.m3.1.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1.1">percent</csymbol><cn id="S5.SS2.p2.3.m3.1.1.2.cmml" type="integer" xref="S5.SS2.p2.3.m3.1.1.2">30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.3.m3.1c">30\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.3.m3.1d">30 %</annotation></semantics></math>, dropping from <math alttext="12.7" class="ltx_Math" display="inline" id="S5.SS2.p2.4.m4.1"><semantics id="S5.SS2.p2.4.m4.1a"><mn id="S5.SS2.p2.4.m4.1.1" xref="S5.SS2.p2.4.m4.1.1.cmml">12.7</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.4.m4.1b"><cn id="S5.SS2.p2.4.m4.1.1.cmml" type="float" xref="S5.SS2.p2.4.m4.1.1">12.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.4.m4.1c">12.7</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.4.m4.1d">12.7</annotation></semantics></math> tokens/s to <math alttext="8.9" class="ltx_Math" display="inline" id="S5.SS2.p2.5.m5.1"><semantics id="S5.SS2.p2.5.m5.1a"><mn id="S5.SS2.p2.5.m5.1.1" xref="S5.SS2.p2.5.m5.1.1.cmml">8.9</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.5.m5.1b"><cn id="S5.SS2.p2.5.m5.1.1.cmml" type="float" xref="S5.SS2.p2.5.m5.1.1">8.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.5.m5.1c">8.9</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.5.m5.1d">8.9</annotation></semantics></math> tokens/s. In contrast, Dimensity <math alttext="9300" class="ltx_Math" display="inline" id="S5.SS2.p2.6.m6.1"><semantics id="S5.SS2.p2.6.m6.1a"><mn id="S5.SS2.p2.6.m6.1.1" xref="S5.SS2.p2.6.m6.1.1.cmml">9300</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.6.m6.1b"><cn id="S5.SS2.p2.6.m6.1.1.cmml" type="integer" xref="S5.SS2.p2.6.m6.1.1">9300</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.6.m6.1c">9300</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.6.m6.1d">9300</annotation></semantics></math> and Kirin <math alttext="9000" class="ltx_Math" display="inline" id="S5.SS2.p2.7.m7.1"><semantics id="S5.SS2.p2.7.m7.1a"><mn id="S5.SS2.p2.7.m7.1.1" xref="S5.SS2.p2.7.m7.1.1.cmml">9000</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.7.m7.1b"><cn id="S5.SS2.p2.7.m7.1.1.cmml" type="integer" xref="S5.SS2.p2.7.m7.1.1">9000</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.7.m7.1c">9000</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.7.m7.1d">9000</annotation></semantics></math>E exhibit only minor latency fluctuations, with a maximum decrease of approximately <math alttext="10\%" class="ltx_Math" display="inline" id="S5.SS2.p2.8.m8.1"><semantics id="S5.SS2.p2.8.m8.1a"><mrow id="S5.SS2.p2.8.m8.1.1" xref="S5.SS2.p2.8.m8.1.1.cmml"><mn id="S5.SS2.p2.8.m8.1.1.2" xref="S5.SS2.p2.8.m8.1.1.2.cmml">10</mn><mo id="S5.SS2.p2.8.m8.1.1.1" xref="S5.SS2.p2.8.m8.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.8.m8.1b"><apply id="S5.SS2.p2.8.m8.1.1.cmml" xref="S5.SS2.p2.8.m8.1.1"><csymbol cd="latexml" id="S5.SS2.p2.8.m8.1.1.1.cmml" xref="S5.SS2.p2.8.m8.1.1.1">percent</csymbol><cn id="S5.SS2.p2.8.m8.1.1.2.cmml" type="integer" xref="S5.SS2.p2.8.m8.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.8.m8.1c">10\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.8.m8.1d">10 %</annotation></semantics></math>. This trend aligns with the data presented in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S4.F3" title="Figure 3 ‣ 4.1.1 Performance on CPUs ‣ 4.1 Token Throughput and Memory Footprints ‣ 4 Performance: Users’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">3</span></a>, which shows that the Xiaomi <math alttext="14" class="ltx_Math" display="inline" id="S5.SS2.p2.9.m9.1"><semantics id="S5.SS2.p2.9.m9.1a"><mn id="S5.SS2.p2.9.m9.1.1" xref="S5.SS2.p2.9.m9.1.1.cmml">14</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.9.m9.1b"><cn id="S5.SS2.p2.9.m9.1.1.cmml" type="integer" xref="S5.SS2.p2.9.m9.1.1">14</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.9.m9.1c">14</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.9.m9.1d">14</annotation></semantics></math> Pro experiences a more significant performance drop as prompt length increases.</p>
</div>
<figure class="ltx_figure" id="S5.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" id="S5.F9.1" style="width:303.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="332" id="S5.F9.1.g1" src="x9.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F9.1.1.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="S5.F9.1.2.2" style="font-size:90%;">Throughput under continuous inference.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" id="S5.F9.2" style="width:151.8pt;"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="623" id="S5.F9.2.g1" src="x10.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F9.2.1.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text" id="S5.F9.2.2.2" style="font-size:90%;">Frequency variations on the prime CPU core.</span></figcaption>
</figure>
</div>
</div>
</figure>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.4">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.F9" title="Figure 9 ‣ 5.2 Impact of DVFS ‣ 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">9</span></a> shows the frequency variation on the prime CPU core at different rounds of inference. It reveals that Snapdragon SoCs (e.g.Huawei Matepad11 Pro, Xiaomi14 Pro) exhibit a more aggressive DVFS policy. During subsequent stages of the inference, both prime and performance cores on Snapdragon 8 Gen 3 experience a rapid reduction in frequency. By the <math alttext="9" class="ltx_Math" display="inline" id="S5.SS2.p3.1.m1.1"><semantics id="S5.SS2.p3.1.m1.1a"><mn id="S5.SS2.p3.1.m1.1.1" xref="S5.SS2.p3.1.m1.1.1.cmml">9</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.m1.1b"><cn id="S5.SS2.p3.1.m1.1.1.cmml" type="integer" xref="S5.SS2.p3.1.m1.1.1">9</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.m1.1c">9</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p3.1.m1.1d">9</annotation></semantics></math>th inference round, the frequency is nearly halved compared to its initial value. To investigate the factors influencing DVFS policy, we further compare two Huawei tablets equipped with identical CPU cores but different SoCs—one powered by Snapdragon (Snapdragon 870 on Huawei Matepad11 Pro) and the other by Kirin (Kirin 9000E on Huawei Matepad12.6 Pro). The results indicate that the vendor plays a significant role in shaping the DVFS policy. For instance, although the maximum frequencies of both devices are <math alttext="3.1" class="ltx_Math" display="inline" id="S5.SS2.p3.2.m2.1"><semantics id="S5.SS2.p3.2.m2.1a"><mn id="S5.SS2.p3.2.m2.1.1" xref="S5.SS2.p3.2.m2.1.1.cmml">3.1</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.2.m2.1b"><cn id="S5.SS2.p3.2.m2.1.1.cmml" type="float" xref="S5.SS2.p3.2.m2.1.1">3.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.2.m2.1c">3.1</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p3.2.m2.1d">3.1</annotation></semantics></math>GHz, the Snapdragon 870 typically keeps the prime core frequency below <math alttext="2.5" class="ltx_Math" display="inline" id="S5.SS2.p3.3.m3.1"><semantics id="S5.SS2.p3.3.m3.1a"><mn id="S5.SS2.p3.3.m3.1.1" xref="S5.SS2.p3.3.m3.1.1.cmml">2.5</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.3.m3.1b"><cn id="S5.SS2.p3.3.m3.1.1.cmml" type="float" xref="S5.SS2.p3.3.m3.1.1">2.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.3.m3.1c">2.5</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p3.3.m3.1d">2.5</annotation></semantics></math>GHz, while the Kirin 9000E sustains a higher frequency of around <math alttext="2.7" class="ltx_Math" display="inline" id="S5.SS2.p3.4.m4.1"><semantics id="S5.SS2.p3.4.m4.1a"><mn id="S5.SS2.p3.4.m4.1.1" xref="S5.SS2.p3.4.m4.1.1.cmml">2.7</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.4.m4.1b"><cn id="S5.SS2.p3.4.m4.1.1.cmml" type="float" xref="S5.SS2.p3.4.m4.1.1">2.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.4.m4.1c">2.7</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p3.4.m4.1d">2.7</annotation></semantics></math>GHz during inference.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Model Preparation Latency</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">In Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.SS2" title="5.2 Impact of DVFS ‣ 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">5.2</span></a>, we have studied the inference performance with warm-up. However, in real-world applications, users may trigger the model sporadically, leading to scenarios where the model must perform a cold start without a warm-up, known as the cold start. Thus, the time required to generate the first token becomes critical to user experience. The process of generating the first token involves several stages: loading model weights from external storage into memory, preparing memory for variables, and executing prefill and decoding. In this section, we focus on the preparation procedure before prefill and evaluate the latency across various devices. We perform tests on six devices and measure the time spent on preparation with and without warm-up. If the device is tested without warm-up, we reboot it to refresh RAM and cache. Results are present in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.F10" title="Figure 10 ‣ 5.3 Model Preparation Latency ‣ 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">10</span></a>.</p>
</div>
<figure class="ltx_figure" id="S5.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="332" id="S5.F10.g1" src="x11.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F10.2.1.1" style="font-size:90%;">Figure 10</span>: </span><span class="ltx_text" id="S5.F10.3.2" style="font-size:90%;">Time for model preparation.</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.2">For the llama.cpp running on CPUs, we compare the time before prefill of the first token in both cold-start and warm-up scenarios. The Dimensity 9300 (on Vivo Pad3 Pro) delivers the best overall performance, with an average model loading time of approximately <math alttext="1500" class="ltx_Math" display="inline" id="S5.SS3.p2.1.m1.1"><semantics id="S5.SS3.p2.1.m1.1a"><mn id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml">1500</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><cn id="S5.SS3.p2.1.m1.1.1.cmml" type="integer" xref="S5.SS3.p2.1.m1.1.1">1500</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">1500</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.1.m1.1d">1500</annotation></semantics></math>ms. The Snapdragon 8 Gen 3 (on Xiaomi14 Pro) ranks second, with its worst-case performance comparable to that of the Dimensity 9300.
We recognize that the warm-up process significantly reduces preparation time for llama.cpp, primarily due to the use of <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.2.1">mmap</span>, which allows the inference process to directly access model weight files from disk. The <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.2.2">mmap</span> function accelerates loading by eliminating the need to copy data between user and kernel space. Additionally, the mapping remains valid for other processes accessing the same file, meaning that after a warm-up, the model weights are already loaded into memory. This allows subsequent inference processes to use the preloaded weights, thereby reducing latency. However, this benefit depends on the memory retaining the loaded weights—if they are overwritten by other files, the warm-up advantage is lost.
From Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.F10" title="Figure 10 ‣ 5.3 Model Preparation Latency ‣ 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">10</span></a>, we observe that all Huawei devices exhibit unusually long preparation times during cold starts. On Huawei devices with Kirin SoCs, the delays are even more pronounced, with the worst loading times exceeding <math alttext="40" class="ltx_Math" display="inline" id="S5.SS3.p2.2.m2.1"><semantics id="S5.SS3.p2.2.m2.1a"><mn id="S5.SS3.p2.2.m2.1.1" xref="S5.SS3.p2.2.m2.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.2.m2.1b"><cn id="S5.SS3.p2.2.m2.1.1.cmml" type="integer" xref="S5.SS3.p2.2.m2.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.2.m2.1c">40</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.2.m2.1d">40</annotation></semantics></math> seconds. Huawei Matepad 11 Pro, despite using a Snapdragon SoC, also experiences extended delays. We identify the underlying causes for these exceptions are related to the <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.2.3">mmap</span> function in Harmony OS and we will discuss it in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.SS4.SSS1" title="5.4.1 llama.cpp ‣ 5.4 Inference Engines ‣ 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">5.4.1</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">In contrast, for MLC LLM running on GPUs, no significant improvement is observed after warm-up, with loading times remaining approximately five times longer on Adreno GPUs and seven times longer on Mali GPUs compared to llama.cpp (with warm-up). This is due to mobile system architecture. Although the CPU and GPU on mobile devices share the same physical DRAM, they operate in separate memory spaces and cannot directly access each other’s data<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib47" title="">47</a>]</cite>. As a result, model weights must be copied from the CPU memory space to the GPU memory space, introducing additional latency. Specifically, MLC LLM uses OpenCL kernels to transfer data between the CPU and GPU via buffers, which contributes to the delay.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 5: </span>Memory and Cache on SoCs</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T5.4" style="width:227.6pt;height:42.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-32.0pt,5.9pt) scale(0.780487040579391,0.780487040579391) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T5.4.1">
<tr class="ltx_tr" id="S5.T5.4.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.4.1.1.1"><span class="ltx_text" id="S5.T5.4.1.1.1.1" style="font-size:90%;">Type</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.4.1.1.2"><span class="ltx_text" id="S5.T5.4.1.1.2.1" style="font-size:90%;">Memory(RAM)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.4.1.1.3"><span class="ltx_text" id="S5.T5.4.1.1.3.1" style="font-size:90%;">Storage(ROM)</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T5.4.1.1.4"><span class="ltx_text" id="S5.T5.4.1.1.4.1" style="font-size:90%;">L3 Cache</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.4.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.4.1.2.1"><span class="ltx_text" id="S5.T5.4.1.2.1.1" style="font-size:90%;">XiaoMi14 Pro</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.4.1.2.2"><span class="ltx_text" id="S5.T5.4.1.2.2.1" style="font-size:90%;">4800 MHz(LPDDR5X)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.4.1.2.3"><span class="ltx_text" id="S5.T5.4.1.2.3.1" style="font-size:90%;">UFS 4.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.1.2.4"><span class="ltx_text" id="S5.T5.4.1.2.4.1" style="font-size:90%;">12MB</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.4.1.3">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T5.4.1.3.1"><span class="ltx_text" id="S5.T5.4.1.3.1.1" style="font-size:90%;">Vivo Pad3 Pro</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T5.4.1.3.2"><span class="ltx_text" id="S5.T5.4.1.3.2.1" style="font-size:90%;">4800 MHz(LPDDR5X)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T5.4.1.3.3"><span class="ltx_text" id="S5.T5.4.1.3.3.1" style="font-size:90%;">UFS 4.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S5.T5.4.1.3.4"><span class="ltx_text" id="S5.T5.4.1.3.4.1" style="font-size:90%;">18MB</span></td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.6">The variations in preparation times across devices are primarily linked to memory bandwidth and UFS speed. For example, the preparation times for Xiaomi14 Pro (Snapdragon <math alttext="8" class="ltx_Math" display="inline" id="S5.SS3.p4.1.m1.1"><semantics id="S5.SS3.p4.1.m1.1a"><mn id="S5.SS3.p4.1.m1.1.1" xref="S5.SS3.p4.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.1.m1.1b"><cn id="S5.SS3.p4.1.m1.1.1.cmml" type="integer" xref="S5.SS3.p4.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.1.m1.1c">8</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.1.m1.1d">8</annotation></semantics></math> Gen <math alttext="3" class="ltx_Math" display="inline" id="S5.SS3.p4.2.m2.1"><semantics id="S5.SS3.p4.2.m2.1a"><mn id="S5.SS3.p4.2.m2.1.1" xref="S5.SS3.p4.2.m2.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.2.m2.1b"><cn id="S5.SS3.p4.2.m2.1.1.cmml" type="integer" xref="S5.SS3.p4.2.m2.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.2.m2.1c">3</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.2.m2.1d">3</annotation></semantics></math>) and Vivo Pad3 Pro (Dimensity <math alttext="9300" class="ltx_Math" display="inline" id="S5.SS3.p4.3.m3.1"><semantics id="S5.SS3.p4.3.m3.1a"><mn id="S5.SS3.p4.3.m3.1.1" xref="S5.SS3.p4.3.m3.1.1.cmml">9300</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.3.m3.1b"><cn id="S5.SS3.p4.3.m3.1.1.cmml" type="integer" xref="S5.SS3.p4.3.m3.1.1">9300</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.3.m3.1c">9300</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.3.m3.1d">9300</annotation></semantics></math>) are comparable on both CPU and GPU. However, the subtle hardware differences still influence performance. Snapdragon 8 Gen 3 shows a slight advantage on the GPU, while the Dimensity <math alttext="9300" class="ltx_Math" display="inline" id="S5.SS3.p4.4.m4.1"><semantics id="S5.SS3.p4.4.m4.1a"><mn id="S5.SS3.p4.4.m4.1.1" xref="S5.SS3.p4.4.m4.1.1.cmml">9300</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.4.m4.1b"><cn id="S5.SS3.p4.4.m4.1.1.cmml" type="integer" xref="S5.SS3.p4.4.m4.1.1">9300</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.4.m4.1c">9300</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.4.m4.1d">9300</annotation></semantics></math> performs better on the CPU. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.T4" title="Table 4 ‣ 5.1.2 Speedup with Special Machine Instructions ‣ 5.1 Hardware Capabilities ‣ 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">4</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.T5" title="Table 5 ‣ 5.3 Model Preparation Latency ‣ 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">5</span></a>, the Snapdragon <math alttext="8" class="ltx_Math" display="inline" id="S5.SS3.p4.5.m5.1"><semantics id="S5.SS3.p4.5.m5.1a"><mn id="S5.SS3.p4.5.m5.1.1" xref="S5.SS3.p4.5.m5.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.5.m5.1b"><cn id="S5.SS3.p4.5.m5.1.1.cmml" type="integer" xref="S5.SS3.p4.5.m5.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.5.m5.1c">8</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.5.m5.1d">8</annotation></semantics></math> Gen <math alttext="3" class="ltx_Math" display="inline" id="S5.SS3.p4.6.m6.1"><semantics id="S5.SS3.p4.6.m6.1a"><mn id="S5.SS3.p4.6.m6.1.1" xref="S5.SS3.p4.6.m6.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.6.m6.1b"><cn id="S5.SS3.p4.6.m6.1.1.cmml" type="integer" xref="S5.SS3.p4.6.m6.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.6.m6.1c">3</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.6.m6.1d">3</annotation></semantics></math> (Adreno 750) benefits from higher real-time GPU memory bandwidth, whereas the Dimensity 9300’s larger L3 cache allows for more efficient reuse of loaded weights after the warm-up. verall, preparation times on both CPUs and GPUs across different vendors are short enough to meet the demands of most real-time applications.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Inference Engines</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">In the previous sections, we studies the impact of hardware on LLM inference. However, some performance discrepancies stem from the design and implementation of inference engines rather than inherent hardware capability limitations. In this part, we will focus on the role of inference engines and provide recommendations for framework developers to improve performance.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.4.1 </span>llama.cpp</h4>
<div class="ltx_para" id="S5.SS4.SSS1.p1">
<p class="ltx_p" id="S5.SS4.SSS1.p1.4">The first issue concerns the exceptionally long model preparation time without warm-up on Huawei devices. Through timestamp analysis, we identify that the primary cause is the execution of the <span class="ltx_text ltx_font_italic" id="S5.SS4.SSS1.p1.4.1">mmap</span> function. On the Huawei Matepad 12.6 Pro, <span class="ltx_text ltx_font_italic" id="S5.SS4.SSS1.p1.4.2">mmap</span> execution takes up to <math alttext="40" class="ltx_Math" display="inline" id="S5.SS4.SSS1.p1.1.m1.1"><semantics id="S5.SS4.SSS1.p1.1.m1.1a"><mn id="S5.SS4.SSS1.p1.1.m1.1.1" xref="S5.SS4.SSS1.p1.1.m1.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p1.1.m1.1b"><cn id="S5.SS4.SSS1.p1.1.m1.1.1.cmml" type="integer" xref="S5.SS4.SSS1.p1.1.m1.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p1.1.m1.1c">40</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS1.p1.1.m1.1d">40</annotation></semantics></math> seconds, whereas after warm-up, this time is drastically reduced to <math alttext="0.2" class="ltx_Math" display="inline" id="S5.SS4.SSS1.p1.2.m2.1"><semantics id="S5.SS4.SSS1.p1.2.m2.1a"><mn id="S5.SS4.SSS1.p1.2.m2.1.1" xref="S5.SS4.SSS1.p1.2.m2.1.1.cmml">0.2</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p1.2.m2.1b"><cn id="S5.SS4.SSS1.p1.2.m2.1.1.cmml" type="float" xref="S5.SS4.SSS1.p1.2.m2.1.1">0.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p1.2.m2.1c">0.2</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS1.p1.2.m2.1d">0.2</annotation></semantics></math> seconds. This suggests that the delay arises from the memory mapping process rather than loading from UFS. We further verify this by testing without file mapping, which reduces the average preparation time to <math alttext="2.6" class="ltx_Math" display="inline" id="S5.SS4.SSS1.p1.3.m3.1"><semantics id="S5.SS4.SSS1.p1.3.m3.1a"><mn id="S5.SS4.SSS1.p1.3.m3.1.1" xref="S5.SS4.SSS1.p1.3.m3.1.1.cmml">2.6</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p1.3.m3.1b"><cn id="S5.SS4.SSS1.p1.3.m3.1.1.cmml" type="float" xref="S5.SS4.SSS1.p1.3.m3.1.1">2.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p1.3.m3.1c">2.6</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS1.p1.3.m3.1d">2.6</annotation></semantics></math> seconds. Additionally, on the Huawei Matepad 11 Pro with a Snapdragon SoC, the average preparation time is <math alttext="6" class="ltx_Math" display="inline" id="S5.SS4.SSS1.p1.4.m4.1"><semantics id="S5.SS4.SSS1.p1.4.m4.1a"><mn id="S5.SS4.SSS1.p1.4.m4.1.1" xref="S5.SS4.SSS1.p1.4.m4.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p1.4.m4.1b"><cn id="S5.SS4.SSS1.p1.4.m4.1.1.cmml" type="integer" xref="S5.SS4.SSS1.p1.4.m4.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p1.4.m4.1c">6</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS1.p1.4.m4.1d">6</annotation></semantics></math> seconds—showing noticeable improvement over other Huawei devices but still significantly slower than the non-Huawei devices. These findings suggest that the issue may be related to <span class="ltx_text ltx_font_italic" id="S5.SS4.SSS1.p1.4.3">mmap</span> behavior on HarmonyOS, possibly exacerbated by Kirin SoCs.</p>
</div>
<figure class="ltx_figure" id="S5.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="356" id="S5.F11.g1" src="x12.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F11.2.1.1" style="font-size:90%;">Figure 11</span>: </span><span class="ltx_text" id="S5.F11.3.2" style="font-size:90%;">Performance between two versions of llama.cpp.</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS4.SSS1.p2">
<p class="ltx_p" id="S5.SS4.SSS1.p2.1">Secondly, we compare different versions of llama.cpp on the Snapdragon 8 Gen 3, Dimensity 9300, and Kirin 9000E in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.F11" title="Figure 11 ‣ 5.4.1 llama.cpp ‣ 5.4 Inference Engines ‣ 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">11</span></a> to evaluate how code optimizations affect performance across various SoCs. The results show that new-generation SoCs such as Snapdragon 8 Gen 3 and Dimensity 9300 experience significantly greater performance gains compared to others. Additionally, the latest version of llama.cpp optimizes model weight arrangement to align with INT8 matrix multiplication instructions, resulting in a peak prefill speed of up to 56 tokens/s. This improvement highlights that to better harness the full capabilities of processors, developing support for these advanced instructions is necessary.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.4.2 </span>MLC LLM</h4>
<div class="ltx_para" id="S5.SS4.SSS2.p1">
<p class="ltx_p" id="S5.SS4.SSS2.p1.8">In Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.SS1.SSS3" title="5.1.3 Utilization of GPU units ‣ 5.1 Hardware Capabilities ‣ 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">5.1.3</span></a>, we explored the factors contributing to the suboptimal performance of MLC LLM on non-Snapdragon SoCs. While we attribute this issue to insufficient hardware unit utilization, it is important for developers to understand what implementation optimizations can address this inefficiency. We observe that MLC LLM supports local execution on devices like the Orange Pi <math alttext="5" class="ltx_Math" display="inline" id="S5.SS4.SSS2.p1.1.m1.1"><semantics id="S5.SS4.SSS2.p1.1.m1.1a"><mn id="S5.SS4.SSS2.p1.1.m1.1.1" xref="S5.SS4.SSS2.p1.1.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p1.1.m1.1b"><cn id="S5.SS4.SSS2.p1.1.m1.1.1.cmml" type="integer" xref="S5.SS4.SSS2.p1.1.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p1.1.m1.1c">5</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS2.p1.1.m1.1d">5</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib39" title="">39</a>]</cite>, which features a Mali-G610 GPU. Despite the fact that Mali-G<math alttext="610" class="ltx_Math" display="inline" id="S5.SS4.SSS2.p1.2.m2.1"><semantics id="S5.SS4.SSS2.p1.2.m2.1a"><mn id="S5.SS4.SSS2.p1.2.m2.1.1" xref="S5.SS4.SSS2.p1.2.m2.1.1.cmml">610</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p1.2.m2.1b"><cn id="S5.SS4.SSS2.p1.2.m2.1.1.cmml" type="integer" xref="S5.SS4.SSS2.p1.2.m2.1.1">610</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p1.2.m2.1c">610</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS2.p1.2.m2.1d">610</annotation></semantics></math>’s theoretical computation capability being significantly lower than the Mali-G<math alttext="720" class="ltx_Math" display="inline" id="S5.SS4.SSS2.p1.3.m3.1"><semantics id="S5.SS4.SSS2.p1.3.m3.1a"><mn id="S5.SS4.SSS2.p1.3.m3.1.1" xref="S5.SS4.SSS2.p1.3.m3.1.1.cmml">720</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p1.3.m3.1b"><cn id="S5.SS4.SSS2.p1.3.m3.1.1.cmml" type="integer" xref="S5.SS4.SSS2.p1.3.m3.1.1">720</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p1.3.m3.1c">720</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS2.p1.3.m3.1d">720</annotation></semantics></math> on the Dimensity 9300, the prefill speed on the Orange Pi <math alttext="5" class="ltx_Math" display="inline" id="S5.SS4.SSS2.p1.4.m4.1"><semantics id="S5.SS4.SSS2.p1.4.m4.1a"><mn id="S5.SS4.SSS2.p1.4.m4.1.1" xref="S5.SS4.SSS2.p1.4.m4.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p1.4.m4.1b"><cn id="S5.SS4.SSS2.p1.4.m4.1.1.cmml" type="integer" xref="S5.SS4.SSS2.p1.4.m4.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p1.4.m4.1c">5</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS2.p1.4.m4.1d">5</annotation></semantics></math> is <math alttext="3.6" class="ltx_Math" display="inline" id="S5.SS4.SSS2.p1.5.m5.1"><semantics id="S5.SS4.SSS2.p1.5.m5.1a"><mn id="S5.SS4.SSS2.p1.5.m5.1.1" xref="S5.SS4.SSS2.p1.5.m5.1.1.cmml">3.6</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p1.5.m5.1b"><cn id="S5.SS4.SSS2.p1.5.m5.1.1.cmml" type="float" xref="S5.SS4.SSS2.p1.5.m5.1.1">3.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p1.5.m5.1c">3.6</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS2.p1.5.m5.1d">3.6</annotation></semantics></math> tokens per second—more than twice that of the Mali-G<math alttext="720" class="ltx_Math" display="inline" id="S5.SS4.SSS2.p1.6.m6.1"><semantics id="S5.SS4.SSS2.p1.6.m6.1a"><mn id="S5.SS4.SSS2.p1.6.m6.1.1" xref="S5.SS4.SSS2.p1.6.m6.1.1.cmml">720</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p1.6.m6.1b"><cn id="S5.SS4.SSS2.p1.6.m6.1.1.cmml" type="integer" xref="S5.SS4.SSS2.p1.6.m6.1.1">720</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p1.6.m6.1c">720</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS2.p1.6.m6.1d">720</annotation></semantics></math>. This discrepancy suggests that theoretical computation capability alone is not sufficient to explain performance differences and points to the importance of optimization. Furthermore, MLC LLM configurations vary widely across target devices. For instance, the loop unroll count is set to <math alttext="4" class="ltx_Math" display="inline" id="S5.SS4.SSS2.p1.7.m7.1"><semantics id="S5.SS4.SSS2.p1.7.m7.1a"><mn id="S5.SS4.SSS2.p1.7.m7.1.1" xref="S5.SS4.SSS2.p1.7.m7.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p1.7.m7.1b"><cn id="S5.SS4.SSS2.p1.7.m7.1.1.cmml" type="integer" xref="S5.SS4.SSS2.p1.7.m7.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p1.7.m7.1c">4</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS2.p1.7.m7.1d">4</annotation></semantics></math> on Adreno but <math alttext="64" class="ltx_Math" display="inline" id="S5.SS4.SSS2.p1.8.m8.1"><semantics id="S5.SS4.SSS2.p1.8.m8.1a"><mn id="S5.SS4.SSS2.p1.8.m8.1.1" xref="S5.SS4.SSS2.p1.8.m8.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p1.8.m8.1b"><cn id="S5.SS4.SSS2.p1.8.m8.1.1.cmml" type="integer" xref="S5.SS4.SSS2.p1.8.m8.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p1.8.m8.1c">64</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS2.p1.8.m8.1d">64</annotation></semantics></math> for Mali, underscoring the necessity of optimizing operators for specific hardware accelerators. Given the greater heterogeneity of mobile GPUs compared to CPUs, achieving optimal performance requires tailored optimizations for each platform.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Implications</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this section, we investigate several aspects that can improve mobile LLM performance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p2">
<p class="ltx_p" id="S6.p2.1"><span class="ltx_text ltx_font_bold" id="S6.p2.1.1">Memory Bottleneck</span> According to Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.F6" title="Figure 6 ‣ 5.1.2 Speedup with Special Machine Instructions ‣ 5.1 Hardware Capabilities ‣ 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">6</span></a>, the prefill speeding is <math alttext="3-5" class="ltx_Math" display="inline" id="S6.p2.1.m1.1"><semantics id="S6.p2.1.m1.1a"><mrow id="S6.p2.1.m1.1.1" xref="S6.p2.1.m1.1.1.cmml"><mn id="S6.p2.1.m1.1.1.2" xref="S6.p2.1.m1.1.1.2.cmml">3</mn><mo id="S6.p2.1.m1.1.1.1" xref="S6.p2.1.m1.1.1.1.cmml">−</mo><mn id="S6.p2.1.m1.1.1.3" xref="S6.p2.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.p2.1.m1.1b"><apply id="S6.p2.1.m1.1.1.cmml" xref="S6.p2.1.m1.1.1"><minus id="S6.p2.1.m1.1.1.1.cmml" xref="S6.p2.1.m1.1.1.1"></minus><cn id="S6.p2.1.m1.1.1.2.cmml" type="integer" xref="S6.p2.1.m1.1.1.2">3</cn><cn id="S6.p2.1.m1.1.1.3.cmml" type="integer" xref="S6.p2.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.1.m1.1c">3-5</annotation><annotation encoding="application/x-llamapun" id="S6.p2.1.m1.1d">3 - 5</annotation></semantics></math> times faster than decode. Decoding is primarily constrained by memory bandwidth. Current optimizations, such as adding more cores and using specialized instruction sets, have proven effective in accelerating prefill. However, these approaches offer limited improvements for decoding speed. To address this challenge, more effective solutions are needed. Developers should consider designing algorithms that optimize memory utilization during decoding.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p3">
<p class="ltx_p" id="S6.p3.1"><span class="ltx_text ltx_font_bold" id="S6.p3.1.1">Hardware Instructions</span>
A key aspect of SoC evolution is the continuous enhancement of instruction sets. New machine instructions often increase operand width, resulting in higher throughput. To fully harness the potential of hardware, developers must consider the characteristics of model operators and take advantage of hardware-accelerated instructions. This may also involve adjusting computational precision, such as quantizing activations to <math alttext="8" class="ltx_Math" display="inline" id="S6.p3.1.m1.1"><semantics id="S6.p3.1.m1.1a"><mn id="S6.p3.1.m1.1.1" xref="S6.p3.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S6.p3.1.m1.1b"><cn id="S6.p3.1.m1.1.1.cmml" type="integer" xref="S6.p3.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p3.1.m1.1c">8</annotation><annotation encoding="application/x-llamapun" id="S6.p3.1.m1.1d">8</annotation></semantics></math>-bit to utilize instructions like <span class="ltx_text ltx_font_italic" id="S6.p3.1.2">smmla</span>. In Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#S5.SS1.SSS2" title="5.1.2 Speedup with Special Machine Instructions ‣ 5.1 Hardware Capabilities ‣ 5 Performance: Developers’ Perspectives ‣ Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation"><span class="ltx_text ltx_ref_tag">5.1.2</span></a>, we discuss the improvement brought by Armv8 instructions. For next-generation CPUs based on the Armv9 architecture, advanced vector instructions like SVE2<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib10" title="">10</a>]</cite>- which enables longer computation vectors-are supported. Furthermore, Arm’s SME instructions<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib8" title="">8</a>]</cite>, specifically designed for matrix multiplication, present additional optimization opportunities. Staying current with these advancements is crucial for developers seeking to maximize inference speed on mobile platforms.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p4">
<p class="ltx_p" id="S6.p4.1"><span class="ltx_text ltx_font_bold" id="S6.p4.1.1">Faster First Generation</span>
The time required for generating the first token includes model preparation, prefilling and one round of decoding. Since the majority of model preparation involves I/O operations (loading model weights into memory), storing some of the weights in RAM can help reduce preparation time. As for prefill and decode, the CPU can run at a higher frequency when a new inference process starts, ensuring the fastest response.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Discussion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this section, we explain the rationale behind our choices in this measurement study.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.p2">
<p class="ltx_p" id="S7.p2.1"><span class="ltx_text ltx_font_bold" id="S7.p2.1.1">Large Lanuage Models.</span> There are many lightweight LLMs, with parameters ranging from 1B to 13B, that can fit in memory on most high-end mobile devices. Additionally, strategies have been developed to run larger models by offloading part of the model weights to external storage. In this paper, rather than focusing on performance differences across models, we concentrate on the hardware capabilities to identify bottlenecks that limit LLM performance and thoroughly understand how much untapped potential remains.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">Most existing generative AI models are transformer-based, where the primary operations are matrix-matrix or matrix-vector multiplications. While different models may vary in terms of matrix size or the number of transformer blocks, the core operations remain highly resource-demanding matrix computations. Understanding the hardware’s capacity to handle these operations is key to optimizing LLM performance on mobile devices.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.p4">
<p class="ltx_p" id="S7.p4.1"><span class="ltx_text ltx_font_bold" id="S7.p4.1.1">Quantization and Model Accuracy.</span>
An important aspect of evaluating a model’s capability is the accuracy of the responses generated by the LLM. However, for on-device LLM inference, the factors influencing model accuracy are primarily subject to the model architecture and the quantization of its weights. Based on this fact, MobileAIBench<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib34" title="">34</a>]</cite> provides a comprehensive benchmark for analyzing accuracy across various NLP tasks under different quantization schemes for both LLMs and large multimodal models (LMMs). Since our primary focus is on hardware-related performance metrics, we have chosen the most widely used quantization scheme, which offers the best balance between speed and accuracy. While lower-bit quantization algorithms may enhance inference speed, we excluded them from this study due to their significant impact on accuracy.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.p5">
<p class="ltx_p" id="S7.p5.1"><span class="ltx_text ltx_font_bold" id="S7.p5.1.1">Multimodality Models.</span>
Driven by the growing demand for advanced AI assistants, the deployment of large multimodal models (LMMs) on mobile devices is an inevitable trend. However, we excluded LMMs from our study for two main reasons. First, current LMMs may not yet be ready for mobile deployment. For example, MobileAIBench reports that testing Llava-Phi-2<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.03613v1#bib.bib48" title="">48</a>]</cite> on the iPhone 14 resulted in a first token generation of <math alttext="66.47" class="ltx_Math" display="inline" id="S7.p5.1.m1.1"><semantics id="S7.p5.1.m1.1a"><mn id="S7.p5.1.m1.1.1" xref="S7.p5.1.m1.1.1.cmml">66.47</mn><annotation-xml encoding="MathML-Content" id="S7.p5.1.m1.1b"><cn id="S7.p5.1.m1.1.1.cmml" type="float" xref="S7.p5.1.m1.1.1">66.47</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p5.1.m1.1c">66.47</annotation><annotation encoding="application/x-llamapun" id="S7.p5.1.m1.1d">66.47</annotation></semantics></math> seconds. It remains unclear whether this poor performance is due to hardware constraints or inefficient operator implementation. Second, since LLMs and LMMs share significant similarities in both model architecture and operator implementations, the insights gained from on-device LLM tests can likely be extrapolated to large multimodal models.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Concluding Remarks</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">In this paper, we present a comprehensive measurement study of LLM inference on mobile platforms, offering insights into both user-experience metrics (such as token throughput and energy consumption) and critical hardware and system characteristics (including CPU/GPU utilization, DVFS, and file mapping latency). Our analysis reveals how hardware capabilities and system dynamics impact on-device LLM performance and highlights potential bottlenecks affecting mobile LLM applications. Additionally, we propose potential directions for enhancing on-device LLM inference performance. We hope that this study can
provide insights for both the development of on-device LLMs and the design for future mobile system architecture.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Perfetto: System profiling, app tracing and trace analysis.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Open AI.

</span>
<span class="ltx_bibblock">Chatgpt.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/chatgpt/" title="">https://openai.com/chatgpt/</a>, 2022.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-06.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Alibaba.

</span>
<span class="ltx_bibblock">Mnn-llm.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/alibaba/MNN" title="">https://github.com/alibaba/MNN</a>, 2023.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-06.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Claude.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://claude.ai/" title="">https://claude.ai/</a>, 2023.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-06.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Arm.

</span>
<span class="ltx_bibblock">Arm streamline.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Arm.

</span>
<span class="ltx_bibblock">big.little technology: The future of mobile.

</span>
<span class="ltx_bibblock">Technical report, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Arm.

</span>
<span class="ltx_bibblock">Sdot (vector). dot product signed arithmetic (vector).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.arm.com/documentation/100069/0609/A64-SIMD-Vector-Instructions/SDOT--vector-" title="">https://developer.arm.com/documentation/100069/0609/A64-SIMD-Vector-Instructions/SDOT--vector-</a>, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-06.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Arm.

</span>
<span class="ltx_bibblock">Sme and sme2. the scalable matrix extension (sme).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.arm.com/documentation/109246/0100/SME-Overview/SME-and-SME2" title="">https://developer.arm.com/documentation/109246/0100/SME-Overview/SME-and-SME2</a>, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-06.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Arm.

</span>
<span class="ltx_bibblock">Smmla. signed most significant word multiply with accumulation.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.arm.com/documentation/dui0473/m/arm-and-thumb-instructions/smmla" title="">https://developer.arm.com/documentation/dui0473/m/arm-and-thumb-instructions/smmla</a>, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-06.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Arm.

</span>
<span class="ltx_bibblock">Sve2 architecture fundamentals.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.arm.com/documentation/102340/0100/SVE2-architecture-fundamentals" title="">https://developer.arm.com/documentation/102340/0100/SVE2-architecture-fundamentals</a>, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-06.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré.

</span>
<span class="ltx_bibblock">Flashattention: Fast and memory-efficient exact attention with io-awareness.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">Advances in Neural Information Processing Systems</span>, 35:16344–16359, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al.

</span>
<span class="ltx_bibblock">The llama 3 herd of models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2407.21783</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.

</span>
<span class="ltx_bibblock">Gptq: Accurate post-training quantization for generative pre-trained transformers.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2210.17323</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Georgi Gerganov.

</span>
<span class="ltx_bibblock">llama.cpp: Llm inference in c/c++.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ggerganov/llama.cpp" title="">https://github.com/ggerganov/llama.cpp</a>, 2023.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-06.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Georgi Gerganov.

</span>
<span class="ltx_bibblock">k-quants.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ggerganov/llama.cpp/pull/1684" title="">https://github.com/ggerganov/llama.cpp/pull/1684</a>, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-06.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Google.

</span>
<span class="ltx_bibblock">Gemini.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://gemini.google.com/" title="">https://gemini.google.com/</a>, 2023.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-06.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Google.

</span>
<span class="ltx_bibblock">Android debug bridge (adb).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.android.com/tools/adb" title="">https://developer.android.com/tools/adb</a>, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-06.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Dibakar Gope.

</span>
<span class="ltx_bibblock">Optimizing large language model (llm) inference for arm cpus.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cms.tinyml.org/wp-content/uploads/talks2023/GenAI_Forum_Dibakar_Gope_240327.pdf" title="">https://cms.tinyml.org/wp-content/uploads/talks2023/GenAI_Forum_Dibakar_Gope_240327.pdf</a>, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-06.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Feng He, Tianqing Zhu, Dayong Ye, Bo Liu, Wanlei Zhou, and Philip S Yu.

</span>
<span class="ltx_bibblock">The emerged security and privacy of llm agent: A survey with case studies.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2407.19354</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Andrey Ignatov, Radu Timofte, Andrei Kulik, Seungsoo Yang, Ke Wang, Felix Baum, Max Wu, Lirong Xu, and Luc Van Gool.

</span>
<span class="ltx_bibblock">Ai benchmark: All about deep learning on smartphones in 2019.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</span>, pages 3617–3635. IEEE, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Apple Inc.

</span>
<span class="ltx_bibblock">Apple Intelligence.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.apple.com/apple-intelligence/" title="">https://developer.apple.com/apple-intelligence/</a>, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-06.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.

</span>
<span class="ltx_bibblock">Mistral 7b.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2310.06825</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.

</span>
<span class="ltx_bibblock">Scaling laws for neural language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2001.08361</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
krrishnarra.

</span>
<span class="ltx_bibblock">clpeak. a tool which profiles opencl devices to find their peak capacities.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/krrishnarraj/clpeak" title="">https://github.com/krrishnarraj/clpeak</a>, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-06.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Stefanos Laskaridis, Kleomenis Kateveas, Lorenzo Minto, and Hamed Haddadi.

</span>
<span class="ltx_bibblock">Melting point: Mobile evaluation of language transformers.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2403.12844</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Qingyuan Li, Ran Meng, Yiduo Li, Bo Zhang, Liang Li, Yifan Lu, Xiangxiang Chu, Yerui Sun, and Yuchen Xie.

</span>
<span class="ltx_bibblock">A speed odyssey for deployable quantization of llms.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2311.09550</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Xiang Li, Zhenyan Lu, Dongqi Cai, Xiao Ma, and Mengwei Xu.

</span>
<span class="ltx_bibblock">Large language models on mobile devices: Measurements, analysis, and insights.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">Proceedings of the Workshop on Edge and Mobile Foundation Models</span>, pages 1–6, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han.

</span>
<span class="ltx_bibblock">Awq: Activationaware weight quantization for llm compression and acceleration. arxiv.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Xinyin Ma, Gongfan Fang, and Xinchao Wang.

</span>
<span class="ltx_bibblock">Llm-pruner: On the structural pruning of large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">Advances in neural information processing systems</span>, 36:21702–21720, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
mediatek.

</span>
<span class="ltx_bibblock">Dimensity 9300 all big cores whitepaper.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://mediatek-marketing.files.svdcdn.com/production/documents/Dimensity-9300-All-Big-Cores_Whitepaper-CHS-Final.pdf?dm=1714371806" title="">https://mediatek-marketing.files.svdcdn.com/production/documents/Dimensity-9300-All-Big-Cores_Whitepaper-CHS-Final.pdf?dm=1714371806</a>, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-06.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
mediatek.

</span>
<span class="ltx_bibblock">Mediatek demonstrating on-device generative ai using llama 2 llm at mwc 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mediatek.com/blog/mediatek-dimensity-demos-on-device-generative-ai-using-meta-llama-2-llm" title="">https://www.mediatek.com/blog/mediatek-dimensity-demos-on-device-generative-ai-using-meta-llama-2-llm</a>, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-06.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Yusuf Mehdi.

</span>
<span class="ltx_bibblock">Introducing Copilot+ PCs.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blogs.microsoft.com/blog/2024/05/20/introducing-copilot-pcs/" title="">https://blogs.microsoft.com/blog/2024/05/20/introducing-copilot-pcs/</a>, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-06.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Microsoft.

</span>
<span class="ltx_bibblock">Phi open models.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://azure.microsoft.com/en-us/products/phi-3" title="">https://azure.microsoft.com/en-us/products/phi-3</a>, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-06.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Rithesh Murthy, Liangwei Yang, Juntao Tan, Tulika Manoj Awalgaonkar, Yilun Zhou, Shelby Heinecke, Sachin Desai, Jason Wu, Ran Xu, Sarah Tan, et al.

</span>
<span class="ltx_bibblock">Mobileaibench: Benchmarking llms and lmms for on-device use cases.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2406.10290</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Qualcomm.

</span>
<span class="ltx_bibblock">Snapdragon profiler.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Qualcomm.

</span>
<span class="ltx_bibblock">Enabling intelligent connections and personalized applications across devices.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aihub.qualcomm.com/mobile/models" title="">https://aihub.qualcomm.com/mobile/models</a>, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-06.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Noam Shazeer.

</span>
<span class="ltx_bibblock">Fast transformer decoding: One write-head is all you need.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:1911.02150</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
MLC team.

</span>
<span class="ltx_bibblock">MLC-LLM, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
MLC team.

</span>
<span class="ltx_bibblock">Gpu-accelerated llm on a $100 orange pi.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blog.mlc.ai/2024/04/20/GPU-Accelerated-LLM-on-Orange-Pi" title="">https://blog.mlc.ai/2024/04/20/GPU-Accelerated-LLM-on-Orange-Pi</a>, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-06.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Inar Timiryasov and Jean-Loup Tastet.

</span>
<span class="ltx_bibblock">Baby llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:2308.02019</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2307.09288</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen.

</span>
<span class="ltx_bibblock">Sheared llama: Accelerating language model pre-training via structured pruning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2310.06694</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.

</span>
<span class="ltx_bibblock">Smoothquant: Accurate and efficient post-training quantization for large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">International Conference on Machine Learning</span>, pages 38087–38099. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Mengwei Xu.

</span>
<span class="ltx_bibblock">mllm: Fast multimodal llm on mobile devices.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://gemini.google.com/" title="">https://gemini.google.com/</a>, 2023.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-06.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Mingxue Xu, Yao Lei Xu, and Danilo P Mandic.

</span>
<span class="ltx_bibblock">Tensorgpt: Efficient compression of the embedding layer in llms based on the tensor-train decomposition.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">arXiv preprint arXiv:2307.00526</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Zhenliang Xue, Yixin Song, Zeyu Mi, Le Chen, Yubin Xia, and Haibo Chen.

</span>
<span class="ltx_bibblock">Powerinfer-2: Fast large language model inference on a smartphone.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2406.06282</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Qiyang Zhang, Xiang Li, Xiangying Che, Xiao Ma, Ao Zhou, Mengwei Xu, Shangguang Wang, Yun Ma, and Xuanzhe Liu.

</span>
<span class="ltx_bibblock">A comprehensive benchmark of deep learning libraries on mobile devices.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">Proceedings of the ACM Web Conference 2022</span>, pages 3298–3307, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Yichen Zhu, Minjie Zhu, Ning Liu, Zhicai Ou, Xiaofeng Mou, and Jian Tang.

</span>
<span class="ltx_bibblock">Llava-phi: Efficient multi-modal assistant with small language model.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">arXiv preprint arXiv:2401.02330</span>, 2024.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Oct  4 17:07:35 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
