<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre.</title>
<!--Generated on Mon Oct  7 07:22:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.04802v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S1" title="In Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S2" title="In Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Background &amp; Related Work</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S2.SS1" title="In II Background &amp; Related Work ‣ Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Data Sources</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S2.SS2" title="In II Background &amp; Related Work ‣ Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">State-of-the-Art</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S3" title="In Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Methods</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S3.SS1" title="In III Methods ‣ Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Data sources and Dataset Creation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S3.SS2" title="In III Methods ‣ Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Network Architectures</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S3.SS2.SSS1" title="In III-B Network Architectures ‣ III Methods ‣ Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>1 </span>U-Net Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S3.SS2.SSS2" title="In III-B Network Architectures ‣ III Methods ‣ Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>2 </span>Encoder Blocks</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S3.SS3" title="In III Methods ‣ Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Training scheme</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S4" title="In Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experimental Results</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S4.SS1" title="In IV Experimental Results ‣ Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Setup</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S4.SS2" title="In IV Experimental Results ‣ Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Results Analysis</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S4.SS3" title="In IV Experimental Results ‣ Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Ablation Studies</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S5" title="In Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Conclusions</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data
<span class="ltx_note ltx_role_thanks" id="id1.id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an).
<br class="ltx_break"/>We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre.</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Matteo Risso1, Alessia Goffi2, Beatrice Alessandra Motetti1, Alessio Burrello1, Jean Baptiste Bove3,
<br class="ltx_break"/>Enrico Macii1, Massimo Poncino1, Daniele Jahier Pagliari1, Giuseppe Maffeis2
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">1Politecnico di Torino, Italy, <span class="ltx_text ltx_font_italic" id="id2.1.id1">Email: firstname.firstsurmame@polito.it
<br class="ltx_break"/></span>2TerrAria s.r.l., Italy. <span class="ltx_text ltx_font_italic" id="id3.2.id2">Email: firstlettername.surname@terraria.com</span> 3Croce Rossa Italiana, Italy. <span class="ltx_text ltx_font_italic" id="id4.3.id3">Email: jeanbaptiste.bove@cri.it</span>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id5.id1">Very High Resolution (VHR) geospatial image analysis is crucial for humanitarian assistance in both natural and anthropogenic crises, as it allows to rapidly identify the most critical areas that need support. Nonetheless, manually inspecting large areas is time-consuming and requires domain expertise.
Thanks to their accuracy, generalization capabilities, and highly parallelizable workload, Deep Neural Networks (DNNs) provide an excellent way to automate this task. Nevertheless, there is a scarcity of VHR data pertaining to conflict situations, and consequently, of studies on the effectiveness of DNNs in those scenarios.
Motivated by this, our work extensively studies the applicability of a collection of state-of-the-art Convolutional Neural Networks (CNNs) originally developed for <span class="ltx_text ltx_font_italic" id="id5.id1.1">natural disasters</span> damage assessment in a war scenario.
To this end, we build an annotated dataset with pre- and post-conflict images of the Ukrainian city of Mariupol.
We then explore the transferability of the CNN models in both zero-shot and learning scenarios, demonstrating their potential and limitations.
To the best of our knowledge, this is the first study to use sub-meter resolution imagery to assess building damage in combat zones.</p>
</div>
<div class="ltx_para" id="p1">
<svg class="ltx_picture" height="66.72" id="p1.pic1" overflow="visible" version="1.1" width="604.52"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,66.72) matrix(1 0 0 -1 0 0) translate(-122.74,0) translate(0,-14.11) matrix(1.0 0.0 0.0 1.0 127.35 42.63)"><foreignobject height="57.5" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="595.3">
<span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="p1.pic1.1.1.1.1" style="width:430.2pt;">
<span class="ltx_p" id="p1.pic1.1.1.1.1.1"><span class="ltx_text" id="p1.pic1.1.1.1.1.1.1" style="font-size:80%;">©2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.</span></span>
</span></foreignobject></g></svg>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In recent years, the usage of geospatial images has been studied as a key element to help and speed-up humanitarian assistance in crisis situations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib4" title="">4</a>]</cite>. In particular, the ability to quickly and accurately assess damage to buildings and infrastructures is crucial for effective response and recovery. Traditional methods of damage assessment, which rely heavily on manual inspection and reporting, are often slow and resource-intensive <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib5" title="">5</a>]</cite>.
Automated approaches using Deep Neural Networks (DNNs) offer a promising alternative, enabling rapid and scalable analysis of large volumes of geospatial data.
When compared to other Computer Vision (CV) algorithms, DNNs have demonstrated exceptional performance and robustness to variations in image quality, lighting conditions and angles, thanks to their ability to learn and extract relevant features from raw images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib6" title="">6</a>]</cite>. Additionally, DNN workloads are highly scalable and parallelizable, enabling the rapid processing of large volumes of data.
For these reasons, they are considered state-of-the-art (SotA) for automated image analysis tasks such as segmentation, classification, and object detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib8" title="">8</a>]</cite>. Building and infrastructure damage may be quickly identified and measured by utilizing the capabilities of these models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib11" title="">11</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Despite the potential of DNNs, a significant challenge remains: the availability of high-quality, annotated data. While extensive data sources of Very High Resolution (VHR) satellite imagery for natural disasters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib1" title="">1</a>]</cite> exist, data specific to war scenarios is comparatively scarce. This gap hinders the development of DNN models tailored for conflict-related damage assessment.
With this limitation in mind, our research aims to narrow the gap through extensive experimentation aimed at verifying whether DNN models originally designed for natural damage detection, can be effectively employed also for war-induced damages. To test this hypothesis, we collect an annotated dataset focused on the Mariupol area in Ukraine, comprising pre- and post-conflict images with sub-meter resolution.
On this dataset, we assess a collection of SotA Convolutional Neural Network (CNN) models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib12" title="">12</a>]</cite>, originally developed for damage assessment in natural disaster scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib1" title="">1</a>]</cite>. We evaluate their ability to transfer learned features to the context of war-induced damage, exploring both <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">zero-shot</span> scenarios, where the models are applied to the new dataset without additional training on, and <span class="ltx_text ltx_font_italic" id="S1.p2.1.2">learning scenarios</span>, where the models are fine-tuned on the Mariupol data. This approach allows us to assess the transferability and adaptability of these models, providing insights into their performance and identifying potential areas for improvement.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">The main contributions of this work are summarized below:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We extensively test the performance of four SotA CNN models, originally proposed for building damage assessment after a natural disaster, in a war scenario, testing and comparing zero-shot and transfer learning setups.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We propose a custom data augmentation pipeline to improve the accuracy of fine-tuned models on a relatively small dataset from the Ukranian city of Mariupol.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We propose an ablation study highlighting the importance of data augmentation and of the dilation filter pre-processing.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">Overall, our models achieve up to 69% and 59% F1 score and up to 86% and 79% balanced accuracy score on 2- and 3-class versions of the damage assessment problem.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Background &amp; Related Work</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.4.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.5.2">Data Sources</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Humanitarian Assistance and Disaster Response (HADR) relies heavily on the availability of VHR (i.e., with <math alttext="&lt;2" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.1"><semantics id="S2.SS1.p1.1.m1.1a"><mrow id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml"><mi id="S2.SS1.p1.1.m1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.2.cmml"></mi><mo id="S2.SS1.p1.1.m1.1.1.1" xref="S2.SS1.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S2.SS1.p1.1.m1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"><lt id="S2.SS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1"></lt><csymbol cd="latexml" id="S2.SS1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.2">absent</csymbol><cn id="S2.SS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S2.SS1.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">&lt;2</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.1d">&lt; 2</annotation></semantics></math>m resolution) geospatial data sources <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib1" title="">1</a>]</cite>.
Mainly, satellite imagery is used, allowing data collection over large areas. An alternative is represented by images collected from Unmanned Aerial Vehicles (UAVs) which can achieve higher resolution despite a smaller area coverage <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib13" title="">13</a>]</cite>.
In this work, we concentrate on satellite imagery since drone use in conflict zones may be limited due to security and airspace restrictions.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Commercial satellite data providers employ proprietary satellites such as WorldView-3 and 4 by Maxar or the Pléiades sensors by Airbus Defense.
Public data providers include government agencies such as NASA through the Landsat program or the ESA with the Sentinel-2 satellite. In both cases, despite being free and openly available, the provided resolution is lower when compared to commercial solutions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib14" title="">14</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">xBD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib1" title="">1</a>]</cite> represents the only large-scale dataset containing pre- and post-disaster geospatial VHR images, provided freely by Maxar, with more than 800k annotated buildings. The dataset was developed in the context of the xView2 challenge promoted by the Defense Innovation Unit of the US Department of Defense. It includes RGB data with a resolution of 0.8 meters, relative to 19 different disasters from all over the world, thus showing great variability in terms of location and disaster type. For each pair of pre- and post-disaster images, the footprint mask of all buildings is provided, coupled with a multi-damage scale including no damage, minor damage, major damage, and destroyed.
To the best of our knowledge, no open and freely available dataset exists with annotated damage in war scenarios.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.4.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.5.2">State-of-the-Art</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Building damage assessment and more in general Change Detection (CD) utilizing VHR geospatial images has been the subject of several research works.
The two main families of approaches include pixel-based and object-based algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib5" title="">5</a>]</cite>. Historically, the first approaches were pixel-based and were limited mainly to low- and medium-resolution imagery. Pixel-based techniques include texture-based analysis, where the amount of change is assessed comparing each pixel with image-wise statistics, using deterministic thresholds <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib15" title="">15</a>]</cite>. Fuzzy CD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib16" title="">16</a>]</cite> uses a similar pipeline coupled with fuzzy reasoning to express thresholds in terms of probabilities.
The application of traditional pixel-based techniques in the high-resolution regime is hindered by the strong dependence of such methods on reflectance variability and different acquisition characteristics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib5" title="">5</a>]</cite>.
Conversely, object-based CD involves segmenting an image into meaningful objects and classifying them based on their texture, shape, spatial arrangement, and spectral properties <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib5" title="">5</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">With Deep Learning (DL), the popularity of pixel-based techniques has been revamped. Indeed, the current SotA is mostly made of pixel-based methods leveraging DNNs.
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib17" title="">17</a>]</cite> proposes a CNN to perform CD over multiple years in an urban scenario.
Similarly, BLDNET <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib18" title="">18</a>]</cite> performs CD employing a Graph Convolutional Network augmented with urban knowledge.
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib9" title="">9</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib10" title="">10</a>]</cite> try to solve the damage assessment problem using respectively CNN and transformer-based architectures and test the proposed approaches on the xBD dataset.
On the same dataset, BDANet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib11" title="">11</a>]</cite> proposes a hybrid CNN-transformer architecture.
As said, most of these works consider generic CD scenarios, while a limited subset addresses the damage assessment problem with experiments solely on the xBD dataset.
Conversely, to the best of our knowledge, no DNN methods have been specifically proposed for war-related scenarios, making our approach the first of its kind.
The only approaches targeting such scenarios are based on shallow algorithms.
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib4" title="">4</a>]</cite> employs Sentinel-1 and -2 data to perform a CD study in the war conflict area of Mosul using a Support Vector Machine (SVM) classifier and texture analysis.
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib2" title="">2</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib3" title="">3</a>]</cite> consider the city of Kyiv as case-study. In both cases, Sentinel data are used to assess damage. In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib2" title="">2</a>]</cite>, Sentinel-1 data are used to perform an intensity analysis while texture analysis is performed on Sentinel-2 data. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib3" title="">3</a>]</cite> extracts features from windows of the original images as time series and employs the Pruned Exact Linear Time algorithm to identify change points.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Methods</span>
</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">Data sources and Dataset Creation</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The objective of this work is to test whether DL models designed for generic damage assessment in natural disasters can transfer well to war-induced damage scenarios.
To this end, we collect a new dataset for building damage assessment in the context of the Russo-Ukrainian war. The data has been acquired from Airbus Defense using their proprietary Pléiades sensor. The starting points are two VHR images of the Mariupol area,
one captured on July 11th, 2021, prior to the conflict, and the other on August 6th, 2022, subsequent to the onset of hostilities.
The location has been selected based on multiple reasons: the availability of archival VHR images, evidence of widespread destruction, and the interest for humanitarian assistance associations.
The city of Mariupol, in the Donetsk Oblast, has an area of over 150 square kilometers with a population of 446,103 people. The analysis focused on the most affected area, of approximately 16 square kilometers.
The Pléiades sensor offers a ground resolution of 0.5 meters, ensuring that even small-scale damage can be accurately assessed. The spectral bands included in the dataset are Blue (430-550nm), Green (480-610nm), Red (600-720nm), and Near-infrared (750-950nm). Following the SotA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib1" title="">1</a>]</cite> we employ only the RGB bands. Panchromatic Multispectral Sharpening (PMS) is applied to produce high-resolution color images by combining the lower-resolution multispectral bands with the higher-resolution panchromatic band.
Then, the images are orthorectified with a low-resolution digital elevation model (indicatively at 30m) to correct geometric distortions caused by sensor tilt and terrain relief.
Finally, the images are processed to conform to the Universal Transverse Mercator (UTM) projection, referenced to the World Geodetic System 1984 (WGS84). The processed images are stored in TIFF format.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">The two VHR images, coupled with building footprints provided by the OpenStreetMap (OSM) platform <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib19" title="">19</a>]</cite>, are used to systematically compare and assess the damage undergone by each building.
Data labelling has been performed manually by civil engineering domain experts following the damage levels criteria of Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S3.F1" title="Figure 1 ‣ III-A Data sources and Dataset Creation ‣ III Methods ‣ Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="310" id="S3.F1.g1" src="x1.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Damage scale description.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">Moreover, the classification database has been enhanced with the analysis made by the United Nations Institute for Training and Research (UNITAR)<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.unitar.org/</span></span></span></span> on the same area, based on imagery collected on March 14th, May 7th, 8th, 12th, 2022 and June 21st, 2021 available on their website.
The labeling of the buildings was performed in 2 steps, firstly using the information available from UNITAR and secondly classifying the missing buildings by photo-interpretation by the domain experts. To assign the degree of damage derived from UNITAR (available in terms of georeferenced points) to the buildings in the OSM database, a buffer of 7.5m was applied around the ground footprint of the OSM building, and a spatial correspondence was created with the points falling within this envelope. If a single point was assigned to several buildings, the UNITAR point was associated with the building with the closest distance from the barycentre of the polygon. In case of multiple points contained in the same building, the most severe degree of damage was selected. The buffer of 7.5m was assessed as the best compromise in terms of point allocation and reduction of the above-mentioned cases. The second step was then carried out instructing experts to be as homogeneous as possible with the UNITAR classification in terms of allocation and assessment of the degree of damage to buildings.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="299" id="S3.F2.g1" src="x2.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Annotation breakdown and examples.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">The two images present a size of 8361<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.p4.1.m1.1"><semantics id="S3.SS1.p4.1.m1.1a"><mo id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><times id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.1.m1.1d">×</annotation></semantics></math>13641 pixels with 6310 annotated buildings, of which 2405 buildings were classified using UNITAR information and the remaining 3905 by domain experts. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S3.F2" title="Figure 2 ‣ III-A Data sources and Dataset Creation ‣ III Methods ‣ Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_tag">2</span></a> shows the breakdown of the different annotations along with some annotation examples.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">Network Architectures</span>
</h3>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="239" id="S3.F3.g1" src="x3.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>High-level overview of the proposed architecture based on the U-Net scheme.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S3.F3" title="Figure 3 ‣ III-B Network Architectures ‣ III Methods ‣ Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_tag">3</span></a> summarizes the proposed DL architecture.
We follow the scheme of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib10" title="">10</a>]</cite> in which pre- and post-disaster images are fed to a semantic segmentation model. The model builds a damage map with the same size as the two inputs, where each pixel is annotated with a label assessing the degree of the damage.
Typically, in the SotA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib9" title="">9</a>]</cite> the task of <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.1">identifying buildings</span> and <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.2">assessing their damage</span> is demanded to two separately trained networks. The first one performs a binary classification, associating each pixel of the pre-disaster image to a “building/no-building” label. The second DNN then performs the pixel-wise damage classification using paired pre and post images. Lastly, the two segmentation maps are point-wise multiplied to obtain the final building damage map.
In our case, however, given the availability of the buildings’ footprint in the area of interest from the OSM database, <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.3">we do not need the first network</span>, and we can generate the buildings’ footprint directly leveraging the information provided by OSM.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS1.4.1.1">III-B</span>1 </span>U-Net Architecture</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">As depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S3.F3" title="Figure 3 ‣ III-B Network Architectures ‣ III Methods ‣ Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_tag">3</span></a>, we use segmentation models based on the well-known U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib7" title="">7</a>]</cite> architecture.
Initially devised for biomedical image segmentation, U-Net has been later applied to geospatial tasks such as land cover classification, urban mapping, change detection, and environmental monitoring <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib11" title="">11</a>]</cite>.
In our case, U-Net is used to extract a set of feature maps of the same dimension from the pre and post images. The two sets are then concatenated across the channel dimension and fed to a pointwwise convolution (i.e., with kernel size equal to <math alttext="1\times 1" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.1.m1.1"><semantics id="S3.SS2.SSS1.p1.1.m1.1a"><mrow id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml"><mn id="S3.SS2.SSS1.p1.1.m1.1.1.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml">1</mn><mo id="S3.SS2.SSS1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS1.p1.1.m1.1.1.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.1b"><apply id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1"><times id="S3.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.1"></times><cn id="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS2.SSS1.p1.1.m1.1.1.2">1</cn><cn id="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS2.SSS1.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.1c">1\times 1</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.1.m1.1d">1 × 1</annotation></semantics></math>) with a number of output channels equal to the number of considered damage classes.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1">The U-Net DNN consists of three main parts: the Contracting Path (Encoder), the Expansive Path (Decoder), and the Skip Connections that bridge the two paths.
The <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p2.1.1">Encoder</span> consists of a series of convolutional layers, each followed by an activation function and an optional pooling layer. Pooling (or strided convolutions) progressively reduce the spatial dimensions of the feature maps, allowing the network to capture features at different scales and reducing the computational load for subsequent layers. In this paper, we consider four different encoder implementations which are detailed in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S3.SS2.SSS2" title="III-B2 Encoder Blocks ‣ III-B Network Architectures ‣ III Methods ‣ Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span>2</span></a>.
The <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p2.1.2">Decoder</span> performs a progressive upsampling of the feature maps obtained from the contracting path to reconstruct the spatial dimensions of the original input image. In our work, we keep the Decoder architecture constant, while varying the Encoder. We always consider an up-sampling factor of 2 with the nearest neighbor strategy, and each up-sampled feature map is fed to a convolutional layer followed by a ReLU activation function.
<span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p2.1.3">Skip Connections</span> are added between corresponding layers in the contracting and expansive paths, allowing the network to fuse information from different scales, preserving both local fine-grained details and high-level global context. Specifically, the feature maps from the contracting path are <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p2.1.4">concatenated</span> with the upsampled feature maps in the expansive path.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS2.4.1.1">III-B</span>2 </span>Encoder Blocks</h4>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="489" id="S3.F4.g1" src="x4.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Basic Convolutional Blocks employed for the U-Net Encoder.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">In this work, we consider and compare four different convolutional blocks, shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S3.F4" title="Figure 4 ‣ III-B2 Encoder Blocks ‣ III-B Network Architectures ‣ III Methods ‣ Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_tag">4</span></a>, to build the U-Net encoder. Such networks are taken from the open-source implementation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib12" title="">12</a>]</cite> winner of the xView2 challenge, thus representing the SotA for natural disaster damage detection.
The encoder is built by repeating each block five times, with different hyper-parameters. Moreover, a pooling layer is always added between the first and second block, while for the other blocks, spatial sub-sampling is achieved through strided convolution.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1">The simplest Encoder block is based on the ResNet scheme, depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S3.F4" title="Figure 4 ‣ III-B2 Encoder Blocks ‣ III-B Network Architectures ‣ III Methods ‣ Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_tag">4</span></a>a. It includes two convolutional layers with 3<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p2.1.m1.1"><semantics id="S3.SS2.SSS2.p2.1.m1.1a"><mo id="S3.SS2.SSS2.p2.1.m1.1.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.1.m1.1b"><times id="S3.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p2.1.m1.1d">×</annotation></semantics></math>3 kernel size, where the first is followed by Batch Normalization (BN) and ReLU activation and the second by BN only. The output of this stack is summed to the input through a residual connection and then passed through another ReLU.
When the number of input and output features of a block differ, a pointwise convolution with BN is added on the residual path to adapt the output shape, making the elementwise sum possible.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p3">
<p class="ltx_p" id="S3.SS2.SSS2.p3.1">A more complex scheme is the one employed by SEResNeXt and SENet encoders, depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S3.F4" title="Figure 4 ‣ III-B2 Encoder Blocks ‣ III-B Network Architectures ‣ III Methods ‣ Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_tag">4</span></a>b. The two present the same architecture and differ only in the values of some hyper-parameters. The architecture is based on a bottleneck structure with a stack of three convolutional layers followed by BN and ReLU. The first convolution is pointwise and reduces the number of features, the second has a 3<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.1.m1.1"><semantics id="S3.SS2.SSS2.p3.1.m1.1a"><mo id="S3.SS2.SSS2.p3.1.m1.1.1" xref="S3.SS2.SSS2.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.1.m1.1b"><times id="S3.SS2.SSS2.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p3.1.m1.1d">×</annotation></semantics></math>3 kernel which keeps the number of features constant in the case of SEResNeXt while in the case of SENet, it increases it by four times. The last convolution is again pointwise and it enlarges the features number by four times for SEResNeXt while keeping it constant for SENet. Moreover, the output of the last convolution is processed by a Squeeze-and-Excite (SE) module which adaptively recalibrates channel-wise feature importance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib8" title="">8</a>]</cite>. Finally, the output is again summed to the input of the block through a residual connection. As in the case of the ResNet block, if needed an additional pointwise convolution is added on the residual path.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p4">
<p class="ltx_p" id="S3.SS2.SSS2.p4.1">The last option is the so-called DualPathNet (DPN) block, shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S3.F4" title="Figure 4 ‣ III-B2 Encoder Blocks ‣ III-B Network Architectures ‣ III Methods ‣ Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_tag">4</span></a>c. It takes two inputs which are concatenated along the channel dimension and fed to a similar bottleneck structure as the one of SEResNeXt and SENet. The output of this structure is split along the channel dimension in two parts, one is summed to the first input while the second one is concatenated with the second input. These two outputs then become the inputs of the subsequent block.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.4.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.5.2">Training scheme</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We partition the available satellite images into training and a test sub-images, ensuring no intersection between the two, to create a fair test environment for our evaluation. In particular, we adopt a Leave-One-Quarter-Out (LOQO) cross-validation approach, by dividing the image spatially in 4 quarters, iteratively keeping one of them as test set, and using the other 3 for training, in each fold. Namely, we generate the training dataset by creating partially overlapping image patches of dimension 1024<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mo id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><times id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">×</annotation></semantics></math>1024, with a fixed stride. The partial overlap increases the number of training samples, while preserving data diversity since the same buildings are found at different (x, y) coordinates in different images.
On average, each training fold consists of approximately 10k image patches. The number varies slightly for each fold due to the non-rectangular area covered by the global image.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.8">To deal with this relatively small dataset size, we implement an augmentation pipeline to create new plausible instances, with both geometric and photometric augmentations.
Each input image can be flipped either vertically, horizontally, or both. It can also be translated (with a random shift factor <math alttext="\gamma_{1}\in[-0.0625,0.0625]" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.2"><semantics id="S3.SS3.p2.1.m1.2a"><mrow id="S3.SS3.p2.1.m1.2.2" xref="S3.SS3.p2.1.m1.2.2.cmml"><msub id="S3.SS3.p2.1.m1.2.2.3" xref="S3.SS3.p2.1.m1.2.2.3.cmml"><mi id="S3.SS3.p2.1.m1.2.2.3.2" xref="S3.SS3.p2.1.m1.2.2.3.2.cmml">γ</mi><mn id="S3.SS3.p2.1.m1.2.2.3.3" xref="S3.SS3.p2.1.m1.2.2.3.3.cmml">1</mn></msub><mo id="S3.SS3.p2.1.m1.2.2.2" xref="S3.SS3.p2.1.m1.2.2.2.cmml">∈</mo><mrow id="S3.SS3.p2.1.m1.2.2.1.1" xref="S3.SS3.p2.1.m1.2.2.1.2.cmml"><mo id="S3.SS3.p2.1.m1.2.2.1.1.2" stretchy="false" xref="S3.SS3.p2.1.m1.2.2.1.2.cmml">[</mo><mrow id="S3.SS3.p2.1.m1.2.2.1.1.1" xref="S3.SS3.p2.1.m1.2.2.1.1.1.cmml"><mo id="S3.SS3.p2.1.m1.2.2.1.1.1a" xref="S3.SS3.p2.1.m1.2.2.1.1.1.cmml">−</mo><mn id="S3.SS3.p2.1.m1.2.2.1.1.1.2" xref="S3.SS3.p2.1.m1.2.2.1.1.1.2.cmml">0.0625</mn></mrow><mo id="S3.SS3.p2.1.m1.2.2.1.1.3" xref="S3.SS3.p2.1.m1.2.2.1.2.cmml">,</mo><mn id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">0.0625</mn><mo id="S3.SS3.p2.1.m1.2.2.1.1.4" stretchy="false" xref="S3.SS3.p2.1.m1.2.2.1.2.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.2b"><apply id="S3.SS3.p2.1.m1.2.2.cmml" xref="S3.SS3.p2.1.m1.2.2"><in id="S3.SS3.p2.1.m1.2.2.2.cmml" xref="S3.SS3.p2.1.m1.2.2.2"></in><apply id="S3.SS3.p2.1.m1.2.2.3.cmml" xref="S3.SS3.p2.1.m1.2.2.3"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.2.2.3.1.cmml" xref="S3.SS3.p2.1.m1.2.2.3">subscript</csymbol><ci id="S3.SS3.p2.1.m1.2.2.3.2.cmml" xref="S3.SS3.p2.1.m1.2.2.3.2">𝛾</ci><cn id="S3.SS3.p2.1.m1.2.2.3.3.cmml" type="integer" xref="S3.SS3.p2.1.m1.2.2.3.3">1</cn></apply><interval closure="closed" id="S3.SS3.p2.1.m1.2.2.1.2.cmml" xref="S3.SS3.p2.1.m1.2.2.1.1"><apply id="S3.SS3.p2.1.m1.2.2.1.1.1.cmml" xref="S3.SS3.p2.1.m1.2.2.1.1.1"><minus id="S3.SS3.p2.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.2.2.1.1.1"></minus><cn id="S3.SS3.p2.1.m1.2.2.1.1.1.2.cmml" type="float" xref="S3.SS3.p2.1.m1.2.2.1.1.1.2">0.0625</cn></apply><cn id="S3.SS3.p2.1.m1.1.1.cmml" type="float" xref="S3.SS3.p2.1.m1.1.1">0.0625</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.2c">\gamma_{1}\in[-0.0625,0.0625]</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.2d">italic_γ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∈ [ - 0.0625 , 0.0625 ]</annotation></semantics></math>), scaled (with a random factor <math alttext="\gamma_{2}\in[0.9,1.1]" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m2.2"><semantics id="S3.SS3.p2.2.m2.2a"><mrow id="S3.SS3.p2.2.m2.2.3" xref="S3.SS3.p2.2.m2.2.3.cmml"><msub id="S3.SS3.p2.2.m2.2.3.2" xref="S3.SS3.p2.2.m2.2.3.2.cmml"><mi id="S3.SS3.p2.2.m2.2.3.2.2" xref="S3.SS3.p2.2.m2.2.3.2.2.cmml">γ</mi><mn id="S3.SS3.p2.2.m2.2.3.2.3" xref="S3.SS3.p2.2.m2.2.3.2.3.cmml">2</mn></msub><mo id="S3.SS3.p2.2.m2.2.3.1" xref="S3.SS3.p2.2.m2.2.3.1.cmml">∈</mo><mrow id="S3.SS3.p2.2.m2.2.3.3.2" xref="S3.SS3.p2.2.m2.2.3.3.1.cmml"><mo id="S3.SS3.p2.2.m2.2.3.3.2.1" stretchy="false" xref="S3.SS3.p2.2.m2.2.3.3.1.cmml">[</mo><mn id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">0.9</mn><mo id="S3.SS3.p2.2.m2.2.3.3.2.2" xref="S3.SS3.p2.2.m2.2.3.3.1.cmml">,</mo><mn id="S3.SS3.p2.2.m2.2.2" xref="S3.SS3.p2.2.m2.2.2.cmml">1.1</mn><mo id="S3.SS3.p2.2.m2.2.3.3.2.3" stretchy="false" xref="S3.SS3.p2.2.m2.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.2b"><apply id="S3.SS3.p2.2.m2.2.3.cmml" xref="S3.SS3.p2.2.m2.2.3"><in id="S3.SS3.p2.2.m2.2.3.1.cmml" xref="S3.SS3.p2.2.m2.2.3.1"></in><apply id="S3.SS3.p2.2.m2.2.3.2.cmml" xref="S3.SS3.p2.2.m2.2.3.2"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.2.3.2.1.cmml" xref="S3.SS3.p2.2.m2.2.3.2">subscript</csymbol><ci id="S3.SS3.p2.2.m2.2.3.2.2.cmml" xref="S3.SS3.p2.2.m2.2.3.2.2">𝛾</ci><cn id="S3.SS3.p2.2.m2.2.3.2.3.cmml" type="integer" xref="S3.SS3.p2.2.m2.2.3.2.3">2</cn></apply><interval closure="closed" id="S3.SS3.p2.2.m2.2.3.3.1.cmml" xref="S3.SS3.p2.2.m2.2.3.3.2"><cn id="S3.SS3.p2.2.m2.1.1.cmml" type="float" xref="S3.SS3.p2.2.m2.1.1">0.9</cn><cn id="S3.SS3.p2.2.m2.2.2.cmml" type="float" xref="S3.SS3.p2.2.m2.2.2">1.1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.2c">\gamma_{2}\in[0.9,1.1]</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m2.2d">italic_γ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ∈ [ 0.9 , 1.1 ]</annotation></semantics></math>) and rotated by an angle <math alttext="\gamma_{3}\in[-45^{\circ},45^{\circ}]" class="ltx_Math" display="inline" id="S3.SS3.p2.3.m3.2"><semantics id="S3.SS3.p2.3.m3.2a"><mrow id="S3.SS3.p2.3.m3.2.2" xref="S3.SS3.p2.3.m3.2.2.cmml"><msub id="S3.SS3.p2.3.m3.2.2.4" xref="S3.SS3.p2.3.m3.2.2.4.cmml"><mi id="S3.SS3.p2.3.m3.2.2.4.2" xref="S3.SS3.p2.3.m3.2.2.4.2.cmml">γ</mi><mn id="S3.SS3.p2.3.m3.2.2.4.3" xref="S3.SS3.p2.3.m3.2.2.4.3.cmml">3</mn></msub><mo id="S3.SS3.p2.3.m3.2.2.3" xref="S3.SS3.p2.3.m3.2.2.3.cmml">∈</mo><mrow id="S3.SS3.p2.3.m3.2.2.2.2" xref="S3.SS3.p2.3.m3.2.2.2.3.cmml"><mo id="S3.SS3.p2.3.m3.2.2.2.2.3" stretchy="false" xref="S3.SS3.p2.3.m3.2.2.2.3.cmml">[</mo><mrow id="S3.SS3.p2.3.m3.1.1.1.1.1" xref="S3.SS3.p2.3.m3.1.1.1.1.1.cmml"><mo id="S3.SS3.p2.3.m3.1.1.1.1.1a" xref="S3.SS3.p2.3.m3.1.1.1.1.1.cmml">−</mo><msup id="S3.SS3.p2.3.m3.1.1.1.1.1.2" xref="S3.SS3.p2.3.m3.1.1.1.1.1.2.cmml"><mn id="S3.SS3.p2.3.m3.1.1.1.1.1.2.2" xref="S3.SS3.p2.3.m3.1.1.1.1.1.2.2.cmml">45</mn><mo id="S3.SS3.p2.3.m3.1.1.1.1.1.2.3" xref="S3.SS3.p2.3.m3.1.1.1.1.1.2.3.cmml">∘</mo></msup></mrow><mo id="S3.SS3.p2.3.m3.2.2.2.2.4" xref="S3.SS3.p2.3.m3.2.2.2.3.cmml">,</mo><msup id="S3.SS3.p2.3.m3.2.2.2.2.2" xref="S3.SS3.p2.3.m3.2.2.2.2.2.cmml"><mn id="S3.SS3.p2.3.m3.2.2.2.2.2.2" xref="S3.SS3.p2.3.m3.2.2.2.2.2.2.cmml">45</mn><mo id="S3.SS3.p2.3.m3.2.2.2.2.2.3" xref="S3.SS3.p2.3.m3.2.2.2.2.2.3.cmml">∘</mo></msup><mo id="S3.SS3.p2.3.m3.2.2.2.2.5" stretchy="false" xref="S3.SS3.p2.3.m3.2.2.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.2b"><apply id="S3.SS3.p2.3.m3.2.2.cmml" xref="S3.SS3.p2.3.m3.2.2"><in id="S3.SS3.p2.3.m3.2.2.3.cmml" xref="S3.SS3.p2.3.m3.2.2.3"></in><apply id="S3.SS3.p2.3.m3.2.2.4.cmml" xref="S3.SS3.p2.3.m3.2.2.4"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.2.2.4.1.cmml" xref="S3.SS3.p2.3.m3.2.2.4">subscript</csymbol><ci id="S3.SS3.p2.3.m3.2.2.4.2.cmml" xref="S3.SS3.p2.3.m3.2.2.4.2">𝛾</ci><cn id="S3.SS3.p2.3.m3.2.2.4.3.cmml" type="integer" xref="S3.SS3.p2.3.m3.2.2.4.3">3</cn></apply><interval closure="closed" id="S3.SS3.p2.3.m3.2.2.2.3.cmml" xref="S3.SS3.p2.3.m3.2.2.2.2"><apply id="S3.SS3.p2.3.m3.1.1.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1.1.1"><minus id="S3.SS3.p2.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1.1.1"></minus><apply id="S3.SS3.p2.3.m3.1.1.1.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.1.1.1.2.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1.1.1.2">superscript</csymbol><cn id="S3.SS3.p2.3.m3.1.1.1.1.1.2.2.cmml" type="integer" xref="S3.SS3.p2.3.m3.1.1.1.1.1.2.2">45</cn><compose id="S3.SS3.p2.3.m3.1.1.1.1.1.2.3.cmml" xref="S3.SS3.p2.3.m3.1.1.1.1.1.2.3"></compose></apply></apply><apply id="S3.SS3.p2.3.m3.2.2.2.2.2.cmml" xref="S3.SS3.p2.3.m3.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.2.2.2.2.2.1.cmml" xref="S3.SS3.p2.3.m3.2.2.2.2.2">superscript</csymbol><cn id="S3.SS3.p2.3.m3.2.2.2.2.2.2.cmml" type="integer" xref="S3.SS3.p2.3.m3.2.2.2.2.2.2">45</cn><compose id="S3.SS3.p2.3.m3.2.2.2.2.2.3.cmml" xref="S3.SS3.p2.3.m3.2.2.2.2.2.3"></compose></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.2c">\gamma_{3}\in[-45^{\circ},45^{\circ}]</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.3.m3.2d">italic_γ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ∈ [ - 45 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT , 45 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT ]</annotation></semantics></math>. Both the flipping and the other affine transformations are applied with a probability of 50%. Next, three blocks of photometric augmentations are applied, each with a probability of 50%. Within each block, only one transformation is applied, sampled uniformly. The first block applies either: i) a random shift in <math alttext="[-20,20]" class="ltx_Math" display="inline" id="S3.SS3.p2.4.m4.2"><semantics id="S3.SS3.p2.4.m4.2a"><mrow id="S3.SS3.p2.4.m4.2.2.1" xref="S3.SS3.p2.4.m4.2.2.2.cmml"><mo id="S3.SS3.p2.4.m4.2.2.1.2" stretchy="false" xref="S3.SS3.p2.4.m4.2.2.2.cmml">[</mo><mrow id="S3.SS3.p2.4.m4.2.2.1.1" xref="S3.SS3.p2.4.m4.2.2.1.1.cmml"><mo id="S3.SS3.p2.4.m4.2.2.1.1a" xref="S3.SS3.p2.4.m4.2.2.1.1.cmml">−</mo><mn id="S3.SS3.p2.4.m4.2.2.1.1.2" xref="S3.SS3.p2.4.m4.2.2.1.1.2.cmml">20</mn></mrow><mo id="S3.SS3.p2.4.m4.2.2.1.3" xref="S3.SS3.p2.4.m4.2.2.2.cmml">,</mo><mn id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml">20</mn><mo id="S3.SS3.p2.4.m4.2.2.1.4" stretchy="false" xref="S3.SS3.p2.4.m4.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.2b"><interval closure="closed" id="S3.SS3.p2.4.m4.2.2.2.cmml" xref="S3.SS3.p2.4.m4.2.2.1"><apply id="S3.SS3.p2.4.m4.2.2.1.1.cmml" xref="S3.SS3.p2.4.m4.2.2.1.1"><minus id="S3.SS3.p2.4.m4.2.2.1.1.1.cmml" xref="S3.SS3.p2.4.m4.2.2.1.1"></minus><cn id="S3.SS3.p2.4.m4.2.2.1.1.2.cmml" type="integer" xref="S3.SS3.p2.4.m4.2.2.1.1.2">20</cn></apply><cn id="S3.SS3.p2.4.m4.1.1.cmml" type="integer" xref="S3.SS3.p2.4.m4.1.1">20</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.2c">[-20,20]</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.4.m4.2d">[ - 20 , 20 ]</annotation></semantics></math> to the values of each RGB channel; ii) a conversion to grayscale; iii) a sepia filter. The second block modifies either: i) the brightness and contrast of the image by a random factor in the range <math alttext="[-0.2,0.2" class="ltx_math_unparsed" display="inline" id="S3.SS3.p2.5.m5.1"><semantics id="S3.SS3.p2.5.m5.1a"><mrow id="S3.SS3.p2.5.m5.1b"><mo id="S3.SS3.p2.5.m5.1.2" stretchy="false">[</mo><mo id="S3.SS3.p2.5.m5.1.3" lspace="0em">−</mo><mn id="S3.SS3.p2.5.m5.1.4">0.2</mn><mo id="S3.SS3.p2.5.m5.1.5">,</mo><mn id="S3.SS3.p2.5.m5.1.1">0.2</mn></mrow><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">[-0.2,0.2</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.5.m5.1d">[ - 0.2 , 0.2</annotation></semantics></math>], or ii) applies a gamma correction, with a random factor drawn from the range <math alttext="[80,120]" class="ltx_Math" display="inline" id="S3.SS3.p2.6.m6.2"><semantics id="S3.SS3.p2.6.m6.2a"><mrow id="S3.SS3.p2.6.m6.2.3.2" xref="S3.SS3.p2.6.m6.2.3.1.cmml"><mo id="S3.SS3.p2.6.m6.2.3.2.1" stretchy="false" xref="S3.SS3.p2.6.m6.2.3.1.cmml">[</mo><mn id="S3.SS3.p2.6.m6.1.1" xref="S3.SS3.p2.6.m6.1.1.cmml">80</mn><mo id="S3.SS3.p2.6.m6.2.3.2.2" xref="S3.SS3.p2.6.m6.2.3.1.cmml">,</mo><mn id="S3.SS3.p2.6.m6.2.2" xref="S3.SS3.p2.6.m6.2.2.cmml">120</mn><mo id="S3.SS3.p2.6.m6.2.3.2.3" stretchy="false" xref="S3.SS3.p2.6.m6.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.2b"><interval closure="closed" id="S3.SS3.p2.6.m6.2.3.1.cmml" xref="S3.SS3.p2.6.m6.2.3.2"><cn id="S3.SS3.p2.6.m6.1.1.cmml" type="integer" xref="S3.SS3.p2.6.m6.1.1">80</cn><cn id="S3.SS3.p2.6.m6.2.2.cmml" type="integer" xref="S3.SS3.p2.6.m6.2.2">120</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.2c">[80,120]</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.6.m6.2d">[ 80 , 120 ]</annotation></semantics></math>. The third and last block comprises transformations that impact the image quality, namely: i) blurring with a randomly sized kernel (with the maximum being fixed at <math alttext="[3,7]" class="ltx_Math" display="inline" id="S3.SS3.p2.7.m7.2"><semantics id="S3.SS3.p2.7.m7.2a"><mrow id="S3.SS3.p2.7.m7.2.3.2" xref="S3.SS3.p2.7.m7.2.3.1.cmml"><mo id="S3.SS3.p2.7.m7.2.3.2.1" stretchy="false" xref="S3.SS3.p2.7.m7.2.3.1.cmml">[</mo><mn id="S3.SS3.p2.7.m7.1.1" xref="S3.SS3.p2.7.m7.1.1.cmml">3</mn><mo id="S3.SS3.p2.7.m7.2.3.2.2" xref="S3.SS3.p2.7.m7.2.3.1.cmml">,</mo><mn id="S3.SS3.p2.7.m7.2.2" xref="S3.SS3.p2.7.m7.2.2.cmml">7</mn><mo id="S3.SS3.p2.7.m7.2.3.2.3" stretchy="false" xref="S3.SS3.p2.7.m7.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.7.m7.2b"><interval closure="closed" id="S3.SS3.p2.7.m7.2.3.1.cmml" xref="S3.SS3.p2.7.m7.2.3.2"><cn id="S3.SS3.p2.7.m7.1.1.cmml" type="integer" xref="S3.SS3.p2.7.m7.1.1">3</cn><cn id="S3.SS3.p2.7.m7.2.2.cmml" type="integer" xref="S3.SS3.p2.7.m7.2.2">7</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.7.m7.2c">[3,7]</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.7.m7.2d">[ 3 , 7 ]</annotation></semantics></math>); ii) downscaling the image by a factor of 0.25 and sequentially upscaling it; iii) applying grid distortion, which divides the image patch into a grid and randomly modifies the intersection points, producing localized perturbations. The distortion factor lays in the range <math alttext="[-0.3,0.3]" class="ltx_Math" display="inline" id="S3.SS3.p2.8.m8.2"><semantics id="S3.SS3.p2.8.m8.2a"><mrow id="S3.SS3.p2.8.m8.2.2.1" xref="S3.SS3.p2.8.m8.2.2.2.cmml"><mo id="S3.SS3.p2.8.m8.2.2.1.2" stretchy="false" xref="S3.SS3.p2.8.m8.2.2.2.cmml">[</mo><mrow id="S3.SS3.p2.8.m8.2.2.1.1" xref="S3.SS3.p2.8.m8.2.2.1.1.cmml"><mo id="S3.SS3.p2.8.m8.2.2.1.1a" xref="S3.SS3.p2.8.m8.2.2.1.1.cmml">−</mo><mn id="S3.SS3.p2.8.m8.2.2.1.1.2" xref="S3.SS3.p2.8.m8.2.2.1.1.2.cmml">0.3</mn></mrow><mo id="S3.SS3.p2.8.m8.2.2.1.3" xref="S3.SS3.p2.8.m8.2.2.2.cmml">,</mo><mn id="S3.SS3.p2.8.m8.1.1" xref="S3.SS3.p2.8.m8.1.1.cmml">0.3</mn><mo id="S3.SS3.p2.8.m8.2.2.1.4" stretchy="false" xref="S3.SS3.p2.8.m8.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.8.m8.2b"><interval closure="closed" id="S3.SS3.p2.8.m8.2.2.2.cmml" xref="S3.SS3.p2.8.m8.2.2.1"><apply id="S3.SS3.p2.8.m8.2.2.1.1.cmml" xref="S3.SS3.p2.8.m8.2.2.1.1"><minus id="S3.SS3.p2.8.m8.2.2.1.1.1.cmml" xref="S3.SS3.p2.8.m8.2.2.1.1"></minus><cn id="S3.SS3.p2.8.m8.2.2.1.1.2.cmml" type="float" xref="S3.SS3.p2.8.m8.2.2.1.1.2">0.3</cn></apply><cn id="S3.SS3.p2.8.m8.1.1.cmml" type="float" xref="S3.SS3.p2.8.m8.1.1">0.3</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.8.m8.2c">[-0.3,0.3]</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.8.m8.2d">[ - 0.3 , 0.3 ]</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">Due to slight differences in the off-nadir angle and in the sun-elevation angle, the images captured before and after a disruptive event may exhibit distinct looks that are not associated to the actual changes of interest (e.g., distortions in the appearance of tall buildings and wide, dark shadows alongside objects). To mitigate the impact of the unavoidable effects of the orbit of the satellites and of the time of day on the captured images, we also consider applying a dilation filter with a squared kernel of 3<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS3.p3.1.m1.1"><semantics id="S3.SS3.p3.1.m1.1a"><mo id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><times id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.1.m1.1d">×</annotation></semantics></math>3 pixels, over the building segmentation map to thicken the buildings’ footprints.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">As training objective, we employ a cross-entropy loss computed only on the annotated pixels, so that buildings for which no label mask is available does not contribute to the loss.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Experimental Results</span>
</h2>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>DNN F1 score results over the Mariupol area. The ZS suffix refers to the models pre-trained only on the xBD dataset.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.4" style="width:390.3pt;height:132.5pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-98.0pt,33.1pt) scale(0.665786943379912,0.665786943379912) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.4.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.4.4.4.5"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.4.5.1">Network</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.1.1.1.1"><math alttext="\mathbf{BAS_{3}}" class="ltx_Math" display="inline" id="S4.T1.1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.1.m1.1a"><msub id="S4.T1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.m1.1.1.cmml"><mi id="S4.T1.1.1.1.1.m1.1.1.2" xref="S4.T1.1.1.1.1.m1.1.1.2.cmml">𝐁𝐀𝐒</mi><mn id="S4.T1.1.1.1.1.m1.1.1.3" xref="S4.T1.1.1.1.1.m1.1.1.3.cmml">𝟑</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><apply id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T1.1.1.1.1.m1.1.1.2">𝐁𝐀𝐒</ci><cn id="S4.T1.1.1.1.1.m1.1.1.3.cmml" type="integer" xref="S4.T1.1.1.1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">\mathbf{BAS_{3}}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.1.m1.1d">bold_BAS start_POSTSUBSCRIPT bold_3 end_POSTSUBSCRIPT</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.2.2.2.2"><math alttext="\mathbf{F1_{3}}" class="ltx_Math" display="inline" id="S4.T1.2.2.2.2.m1.1"><semantics id="S4.T1.2.2.2.2.m1.1a"><msub id="S4.T1.2.2.2.2.m1.1.1" xref="S4.T1.2.2.2.2.m1.1.1.cmml"><mi id="S4.T1.2.2.2.2.m1.1.1.2" xref="S4.T1.2.2.2.2.m1.1.1.2.cmml">𝐅𝟏</mi><mn id="S4.T1.2.2.2.2.m1.1.1.3" xref="S4.T1.2.2.2.2.m1.1.1.3.cmml">𝟑</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.2.m1.1b"><apply id="S4.T1.2.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.2.2.2.2.m1.1.1.1.cmml" xref="S4.T1.2.2.2.2.m1.1.1">subscript</csymbol><ci id="S4.T1.2.2.2.2.m1.1.1.2.cmml" xref="S4.T1.2.2.2.2.m1.1.1.2">𝐅𝟏</ci><cn id="S4.T1.2.2.2.2.m1.1.1.3.cmml" type="integer" xref="S4.T1.2.2.2.2.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.2.m1.1c">\mathbf{F1_{3}}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.2.2.m1.1d">bold_F1 start_POSTSUBSCRIPT bold_3 end_POSTSUBSCRIPT</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.4.4.6"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.4.6.1">No Damage</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.4.4.7"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.4.7.1">Moderate Damage</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.4.4.4.8"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.4.8.1">Severe + Destroyed</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.3.3.3.3"><math alttext="\mathbf{BAS_{2}}" class="ltx_Math" display="inline" id="S4.T1.3.3.3.3.m1.1"><semantics id="S4.T1.3.3.3.3.m1.1a"><msub id="S4.T1.3.3.3.3.m1.1.1" xref="S4.T1.3.3.3.3.m1.1.1.cmml"><mi id="S4.T1.3.3.3.3.m1.1.1.2" xref="S4.T1.3.3.3.3.m1.1.1.2.cmml">𝐁𝐀𝐒</mi><mn id="S4.T1.3.3.3.3.m1.1.1.3" xref="S4.T1.3.3.3.3.m1.1.1.3.cmml">𝟐</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.3.m1.1b"><apply id="S4.T1.3.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.3.3.3.3.m1.1.1.1.cmml" xref="S4.T1.3.3.3.3.m1.1.1">subscript</csymbol><ci id="S4.T1.3.3.3.3.m1.1.1.2.cmml" xref="S4.T1.3.3.3.3.m1.1.1.2">𝐁𝐀𝐒</ci><cn id="S4.T1.3.3.3.3.m1.1.1.3.cmml" type="integer" xref="S4.T1.3.3.3.3.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.3.m1.1c">\mathbf{BAS_{2}}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.3.3.3.m1.1d">bold_BAS start_POSTSUBSCRIPT bold_2 end_POSTSUBSCRIPT</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.4.4.4"><math alttext="\mathbf{F1_{2}}" class="ltx_Math" display="inline" id="S4.T1.4.4.4.4.m1.1"><semantics id="S4.T1.4.4.4.4.m1.1a"><msub id="S4.T1.4.4.4.4.m1.1.1" xref="S4.T1.4.4.4.4.m1.1.1.cmml"><mi id="S4.T1.4.4.4.4.m1.1.1.2" xref="S4.T1.4.4.4.4.m1.1.1.2.cmml">𝐅𝟏</mi><mn id="S4.T1.4.4.4.4.m1.1.1.3" xref="S4.T1.4.4.4.4.m1.1.1.3.cmml">𝟐</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.4.m1.1b"><apply id="S4.T1.4.4.4.4.m1.1.1.cmml" xref="S4.T1.4.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.4.4.4.4.m1.1.1.1.cmml" xref="S4.T1.4.4.4.4.m1.1.1">subscript</csymbol><ci id="S4.T1.4.4.4.4.m1.1.1.2.cmml" xref="S4.T1.4.4.4.4.m1.1.1.2">𝐅𝟏</ci><cn id="S4.T1.4.4.4.4.m1.1.1.3.cmml" type="integer" xref="S4.T1.4.4.4.4.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.4.m1.1c">\mathbf{F1_{2}}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.4.4.4.4.m1.1d">bold_F1 start_POSTSUBSCRIPT bold_2 end_POSTSUBSCRIPT</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.4.4.9"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.4.9.1">No Damage</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.4.4.10"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.4.10.1">Damage</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.4.4.5.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.4.4.5.1.1">ResNet-ZS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.5.1.2">74%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.5.1.3">38%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.5.1.4">42%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.5.1.5">31%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.5.1.6">44%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.5.1.7">81%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.5.1.8">56%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.5.1.9">42%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.5.1.10">85%</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4.6.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.4.4.6.2.1">ResNet</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.6.2.2"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.6.2.2.1">79%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.6.2.3"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.6.2.3.1">59%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.6.2.4">54%</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.6.2.5">72%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.6.2.6">54%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.6.2.7">85%</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.6.2.8">66%</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.6.2.9">54%</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.6.2.10">85%</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4.7.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.4.4.7.3.1">SEResNeXt-ZS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.7.3.2">77%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.7.3.3">47 %</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.7.3.4">51 %</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.7.3.5">42 %</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.7.3.6">49 %</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.7.3.7">84%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.7.3.8">60 %</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.7.3.9">51 %</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.7.3.10">74 %</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4.8.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.4.4.8.4.1">SEResNeXt</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.8.4.2">76%</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.8.4.3">46 %</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.8.4.4">
<span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.4.4.1">57</span> %</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.8.4.5">70 %</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.8.4.6">30 %</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.8.4.7"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.4.7.1">86%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.8.4.8">
<span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.4.8.1">69</span> %</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.8.4.9">
<span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.4.9.1">57</span> %</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.8.4.10">86 %</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4.9.5">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.4.4.9.5.1">SENet-ZS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.9.5.2">74%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.9.5.3">43 %</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.9.5.4">42 %</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.9.5.5">42 %</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.9.5.6">46 %</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.9.5.7">81%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.9.5.8">57 %</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.9.5.9">42 %</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.9.5.10">86 %</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4.10.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.4.4.10.6.1">SENet</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.10.6.2">72%</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.10.6.3">42 %</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.10.6.4">40 %</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.10.6.5">67 %</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.10.6.6">32 %</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.10.6.7">80%</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.10.6.8">54 %</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.10.6.9">40 %</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.10.6.10">84 %</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4.11.7">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.4.4.11.7.1">DualPathNet-ZS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.11.7.2">74%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.11.7.3">39 %</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.11.7.4">43 %</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.11.7.5">32 %</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.11.7.6">43 %</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.11.7.7">81%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.11.7.8">56 %</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.11.7.9">43 %</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.11.7.10">79 %</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4.12.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.4.4.12.8.1">DualPathNet</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.12.8.2">78%</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.12.8.3">55 %</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.12.8.4">45 %</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.12.8.5">72 %</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.12.8.6">
<span class="ltx_text ltx_font_bold" id="S4.T1.4.4.12.8.6.1">56</span> %</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.12.8.7">82%</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.12.8.8">59 %</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.12.8.9">45 %</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.12.8.10">87 %</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4.13.9">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.4.4.13.9.1">Ensemble-ZS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.13.9.2">77%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.13.9.3">44 %</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.13.9.4">53 %</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.13.9.5">35 %</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.13.9.6">48 %</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.13.9.7">84%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.13.9.8">65 %</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.13.9.9">53 %</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.13.9.10">84 %</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4.14.10">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.4.4.14.10.1">Ensemble</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.14.10.2">78%</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.14.10.3">58 %</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.14.10.4">55 %</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.14.10.5">
<span class="ltx_text ltx_font_bold" id="S4.T1.4.4.14.10.5.1">73</span> %</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.14.10.6">50 %</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.14.10.7">85%</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.14.10.8">67 %</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.14.10.9">55 %</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.14.10.10">
<span class="ltx_text ltx_font_bold" id="S4.T1.4.4.14.10.10.1">88</span> %</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Comparison of the ResNet training performance on our dataset with and without xBD pre-training.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.2" style="width:346.9pt;height:39.2pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-69.8pt,7.7pt) scale(0.71308642480265,0.71308642480265) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.2.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.2.2.2.3"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.2.3.1">Pretraining</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.1"><math alttext="\mathbf{F1_{3}}" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.m1.1a"><msub id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml"><mi id="S4.T2.1.1.1.1.m1.1.1.2" xref="S4.T2.1.1.1.1.m1.1.1.2.cmml">𝐅𝟏</mi><mn id="S4.T2.1.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.1.m1.1.1.3.cmml">𝟑</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T2.1.1.1.1.m1.1.1.2.cmml" xref="S4.T2.1.1.1.1.m1.1.1.2">𝐅𝟏</ci><cn id="S4.T2.1.1.1.1.m1.1.1.3.cmml" type="integer" xref="S4.T2.1.1.1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">\mathbf{F1_{3}}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.m1.1d">bold_F1 start_POSTSUBSCRIPT bold_3 end_POSTSUBSCRIPT</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.2.2.4"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.2.4.1">No Damage</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.2.2.5"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.2.5.1">Moderate Damage</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.2.2.2.6"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.2.6.1">Severe + Destroyed</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.2.2.2"><math alttext="\mathbf{F1_{2}}" class="ltx_Math" display="inline" id="S4.T2.2.2.2.2.m1.1"><semantics id="S4.T2.2.2.2.2.m1.1a"><msub id="S4.T2.2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.2.m1.1.1.cmml"><mi id="S4.T2.2.2.2.2.m1.1.1.2" xref="S4.T2.2.2.2.2.m1.1.1.2.cmml">𝐅𝟏</mi><mn id="S4.T2.2.2.2.2.m1.1.1.3" xref="S4.T2.2.2.2.2.m1.1.1.3.cmml">𝟐</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.m1.1b"><apply id="S4.T2.2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T2.2.2.2.2.m1.1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1">subscript</csymbol><ci id="S4.T2.2.2.2.2.m1.1.1.2.cmml" xref="S4.T2.2.2.2.2.m1.1.1.2">𝐅𝟏</ci><cn id="S4.T2.2.2.2.2.m1.1.1.3.cmml" type="integer" xref="S4.T2.2.2.2.2.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.m1.1c">\mathbf{F1_{2}}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.2.m1.1d">bold_F1 start_POSTSUBSCRIPT bold_2 end_POSTSUBSCRIPT</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.2.2.7"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.2.7.1">No Damage</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.2.2.8"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.2.8.1">Damage</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.2.2.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.2.2.3.1.1">✗</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.3.1.2">56%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.3.1.3">52%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.3.1.4"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.3.1.4.1">73%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.3.1.5">49%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.3.1.6">65%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.3.1.7">52%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.3.1.8"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.3.1.8.1">87%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.2.4.2.1">✓</th>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.4.2.2"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.4.2.2.1">59%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.4.2.3"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.4.2.3.1">54%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.4.2.4">72%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.2.4.2.5"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.4.2.5.1">54%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.4.2.6"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.4.2.6.1">66%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.4.2.7"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.4.2.7.1">54%</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.4.2.8">85%</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Ablation study of the effect of Augmentations and Dilation filter for the ResNet architecture.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.3" style="width:390.3pt;height:89.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(77.2pt,-17.7pt) scale(1.65459526880941,1.65459526880941) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.2.1">Metric</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.1.1.1.1"><math alttext="\mathbf{Baseline}" class="ltx_Math" display="inline" id="S4.T3.1.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.1.m1.1a"><mi id="S4.T3.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.cmml">𝐁𝐚𝐬𝐞𝐥𝐢𝐧𝐞</mi><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1">𝐁𝐚𝐬𝐞𝐥𝐢𝐧𝐞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">\mathbf{Baseline}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.1.m1.1d">bold_Baseline</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.3.1">+ Augmentation</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.4.1">+ Dilation</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.2.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.2.2.2.1"><math alttext="F1_{3}" class="ltx_Math" display="inline" id="S4.T3.2.2.2.1.m1.1"><semantics id="S4.T3.2.2.2.1.m1.1a"><mrow id="S4.T3.2.2.2.1.m1.1.1" xref="S4.T3.2.2.2.1.m1.1.1.cmml"><mi id="S4.T3.2.2.2.1.m1.1.1.2" xref="S4.T3.2.2.2.1.m1.1.1.2.cmml">F</mi><mo id="S4.T3.2.2.2.1.m1.1.1.1" xref="S4.T3.2.2.2.1.m1.1.1.1.cmml">⁢</mo><msub id="S4.T3.2.2.2.1.m1.1.1.3" xref="S4.T3.2.2.2.1.m1.1.1.3.cmml"><mn id="S4.T3.2.2.2.1.m1.1.1.3.2" xref="S4.T3.2.2.2.1.m1.1.1.3.2.cmml">1</mn><mn id="S4.T3.2.2.2.1.m1.1.1.3.3" xref="S4.T3.2.2.2.1.m1.1.1.3.3.cmml">3</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.1.m1.1b"><apply id="S4.T3.2.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1"><times id="S4.T3.2.2.2.1.m1.1.1.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1.1"></times><ci id="S4.T3.2.2.2.1.m1.1.1.2.cmml" xref="S4.T3.2.2.2.1.m1.1.1.2">𝐹</ci><apply id="S4.T3.2.2.2.1.m1.1.1.3.cmml" xref="S4.T3.2.2.2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T3.2.2.2.1.m1.1.1.3.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1.3">subscript</csymbol><cn id="S4.T3.2.2.2.1.m1.1.1.3.2.cmml" type="integer" xref="S4.T3.2.2.2.1.m1.1.1.3.2">1</cn><cn id="S4.T3.2.2.2.1.m1.1.1.3.3.cmml" type="integer" xref="S4.T3.2.2.2.1.m1.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.1.m1.1c">F1_{3}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.1.m1.1d">italic_F 1 start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.2.2">53%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.2.3">58%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.2.4"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.2.4.1">59%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.3.3.3.1"><math alttext="F1_{2}" class="ltx_Math" display="inline" id="S4.T3.3.3.3.1.m1.1"><semantics id="S4.T3.3.3.3.1.m1.1a"><mrow id="S4.T3.3.3.3.1.m1.1.1" xref="S4.T3.3.3.3.1.m1.1.1.cmml"><mi id="S4.T3.3.3.3.1.m1.1.1.2" xref="S4.T3.3.3.3.1.m1.1.1.2.cmml">F</mi><mo id="S4.T3.3.3.3.1.m1.1.1.1" xref="S4.T3.3.3.3.1.m1.1.1.1.cmml">⁢</mo><msub id="S4.T3.3.3.3.1.m1.1.1.3" xref="S4.T3.3.3.3.1.m1.1.1.3.cmml"><mn id="S4.T3.3.3.3.1.m1.1.1.3.2" xref="S4.T3.3.3.3.1.m1.1.1.3.2.cmml">1</mn><mn id="S4.T3.3.3.3.1.m1.1.1.3.3" xref="S4.T3.3.3.3.1.m1.1.1.3.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.1.m1.1b"><apply id="S4.T3.3.3.3.1.m1.1.1.cmml" xref="S4.T3.3.3.3.1.m1.1.1"><times id="S4.T3.3.3.3.1.m1.1.1.1.cmml" xref="S4.T3.3.3.3.1.m1.1.1.1"></times><ci id="S4.T3.3.3.3.1.m1.1.1.2.cmml" xref="S4.T3.3.3.3.1.m1.1.1.2">𝐹</ci><apply id="S4.T3.3.3.3.1.m1.1.1.3.cmml" xref="S4.T3.3.3.3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T3.3.3.3.1.m1.1.1.3.1.cmml" xref="S4.T3.3.3.3.1.m1.1.1.3">subscript</csymbol><cn id="S4.T3.3.3.3.1.m1.1.1.3.2.cmml" type="integer" xref="S4.T3.3.3.3.1.m1.1.1.3.2">1</cn><cn id="S4.T3.3.3.3.1.m1.1.1.3.3.cmml" type="integer" xref="S4.T3.3.3.3.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.1.m1.1c">F1_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.3.3.1.m1.1d">italic_F 1 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.3.3.3.2">61%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.3.3.3.3">65%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.3.3.4"><span class="ltx_text ltx_font_bold" id="S4.T3.3.3.3.4.1">66%</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.4.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">Setup</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">All our code is written in Python v3.9, using the PyTorch v2.3 library. Augmentations are implemented using albumentations v1.4 and geospatial images are managed using rasterio v1.3.9. All experiments are conducted on a workstation with a 32-core Intel Xeon w5-3435X and four NVIDIA A5000 GPUs.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">We evaluate our DNN models on the dataset described in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S3.SS1" title="III-A Data sources and Dataset Creation ‣ III Methods ‣ Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>. We consider both a <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.1">Zero-Shot</span> (ZS) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib20" title="">20</a>]</cite> scenario with models pre-trained on xBD and tested on the Ukrainian data, and fine-tuning/training-from-scratch scenarios, where we employ the LOQO cross-validation scheme described in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S3.SS3" title="III-C Training scheme ‣ III Methods ‣ Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a>.
For pre-trained models, we directly use the open-source weights published by the xView2 challenge winning entry <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib12" title="">12</a>]</cite>. For this reason, in all our experiments we set the input-images size to be the one expected by such models, i.e., 1024<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.1"><semantics id="S4.SS1.p2.1.m1.1a"><mo id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><times id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.1d">×</annotation></semantics></math>1024 pixels.
We consider an overlap of 960 pixels between images.
All models are trained for 30 epochs with a batch size of 64.
The optimizer is AdamW with a weight-decay set to 5e-6 and an initial learning rate of 5e-5.
</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.2">Following the SotA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#bib.bib10" title="">10</a>]</cite> we employ the F1 score as the main performance metric.
For each class <math alttext="C_{i}" class="ltx_Math" display="inline" id="S4.SS1.p3.1.m1.1"><semantics id="S4.SS1.p3.1.m1.1a"><msub id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mi id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2.cmml">C</mi><mi id="S4.SS1.p3.1.m1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2">𝐶</ci><ci id="S4.SS1.p3.1.m1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">C_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.1.m1.1d">italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, the class-wise <math alttext="F1_{C_{i}}" class="ltx_Math" display="inline" id="S4.SS1.p3.2.m2.1"><semantics id="S4.SS1.p3.2.m2.1a"><mrow id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml"><mi id="S4.SS1.p3.2.m2.1.1.2" xref="S4.SS1.p3.2.m2.1.1.2.cmml">F</mi><mo id="S4.SS1.p3.2.m2.1.1.1" xref="S4.SS1.p3.2.m2.1.1.1.cmml">⁢</mo><msub id="S4.SS1.p3.2.m2.1.1.3" xref="S4.SS1.p3.2.m2.1.1.3.cmml"><mn id="S4.SS1.p3.2.m2.1.1.3.2" xref="S4.SS1.p3.2.m2.1.1.3.2.cmml">1</mn><msub id="S4.SS1.p3.2.m2.1.1.3.3" xref="S4.SS1.p3.2.m2.1.1.3.3.cmml"><mi id="S4.SS1.p3.2.m2.1.1.3.3.2" xref="S4.SS1.p3.2.m2.1.1.3.3.2.cmml">C</mi><mi id="S4.SS1.p3.2.m2.1.1.3.3.3" xref="S4.SS1.p3.2.m2.1.1.3.3.3.cmml">i</mi></msub></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><apply id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1"><times id="S4.SS1.p3.2.m2.1.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1.1"></times><ci id="S4.SS1.p3.2.m2.1.1.2.cmml" xref="S4.SS1.p3.2.m2.1.1.2">𝐹</ci><apply id="S4.SS1.p3.2.m2.1.1.3.cmml" xref="S4.SS1.p3.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p3.2.m2.1.1.3.1.cmml" xref="S4.SS1.p3.2.m2.1.1.3">subscript</csymbol><cn id="S4.SS1.p3.2.m2.1.1.3.2.cmml" type="integer" xref="S4.SS1.p3.2.m2.1.1.3.2">1</cn><apply id="S4.SS1.p3.2.m2.1.1.3.3.cmml" xref="S4.SS1.p3.2.m2.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS1.p3.2.m2.1.1.3.3.1.cmml" xref="S4.SS1.p3.2.m2.1.1.3.3">subscript</csymbol><ci id="S4.SS1.p3.2.m2.1.1.3.3.2.cmml" xref="S4.SS1.p3.2.m2.1.1.3.3.2">𝐶</ci><ci id="S4.SS1.p3.2.m2.1.1.3.3.3.cmml" xref="S4.SS1.p3.2.m2.1.1.3.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">F1_{C_{i}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.2.m2.1d">italic_F 1 start_POSTSUBSCRIPT italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> score is defined as</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="F1_{C_{i}}=\frac{2TP_{C_{i}}}{2TP_{C_{i}}+FP_{C_{i}}+FN_{C_{i}}}" class="ltx_Math" display="block" id="S4.E1.m1.1"><semantics id="S4.E1.m1.1a"><mrow id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml"><mrow id="S4.E1.m1.1.1.2" xref="S4.E1.m1.1.1.2.cmml"><mi id="S4.E1.m1.1.1.2.2" xref="S4.E1.m1.1.1.2.2.cmml">F</mi><mo id="S4.E1.m1.1.1.2.1" xref="S4.E1.m1.1.1.2.1.cmml">⁢</mo><msub id="S4.E1.m1.1.1.2.3" xref="S4.E1.m1.1.1.2.3.cmml"><mn id="S4.E1.m1.1.1.2.3.2" xref="S4.E1.m1.1.1.2.3.2.cmml">1</mn><msub id="S4.E1.m1.1.1.2.3.3" xref="S4.E1.m1.1.1.2.3.3.cmml"><mi id="S4.E1.m1.1.1.2.3.3.2" xref="S4.E1.m1.1.1.2.3.3.2.cmml">C</mi><mi id="S4.E1.m1.1.1.2.3.3.3" xref="S4.E1.m1.1.1.2.3.3.3.cmml">i</mi></msub></msub></mrow><mo id="S4.E1.m1.1.1.1" xref="S4.E1.m1.1.1.1.cmml">=</mo><mfrac id="S4.E1.m1.1.1.3" xref="S4.E1.m1.1.1.3.cmml"><mrow id="S4.E1.m1.1.1.3.2" xref="S4.E1.m1.1.1.3.2.cmml"><mn id="S4.E1.m1.1.1.3.2.2" xref="S4.E1.m1.1.1.3.2.2.cmml">2</mn><mo id="S4.E1.m1.1.1.3.2.1" xref="S4.E1.m1.1.1.3.2.1.cmml">⁢</mo><mi id="S4.E1.m1.1.1.3.2.3" xref="S4.E1.m1.1.1.3.2.3.cmml">T</mi><mo id="S4.E1.m1.1.1.3.2.1a" xref="S4.E1.m1.1.1.3.2.1.cmml">⁢</mo><msub id="S4.E1.m1.1.1.3.2.4" xref="S4.E1.m1.1.1.3.2.4.cmml"><mi id="S4.E1.m1.1.1.3.2.4.2" xref="S4.E1.m1.1.1.3.2.4.2.cmml">P</mi><msub id="S4.E1.m1.1.1.3.2.4.3" xref="S4.E1.m1.1.1.3.2.4.3.cmml"><mi id="S4.E1.m1.1.1.3.2.4.3.2" xref="S4.E1.m1.1.1.3.2.4.3.2.cmml">C</mi><mi id="S4.E1.m1.1.1.3.2.4.3.3" xref="S4.E1.m1.1.1.3.2.4.3.3.cmml">i</mi></msub></msub></mrow><mrow id="S4.E1.m1.1.1.3.3" xref="S4.E1.m1.1.1.3.3.cmml"><mrow id="S4.E1.m1.1.1.3.3.2" xref="S4.E1.m1.1.1.3.3.2.cmml"><mn id="S4.E1.m1.1.1.3.3.2.2" xref="S4.E1.m1.1.1.3.3.2.2.cmml">2</mn><mo id="S4.E1.m1.1.1.3.3.2.1" xref="S4.E1.m1.1.1.3.3.2.1.cmml">⁢</mo><mi id="S4.E1.m1.1.1.3.3.2.3" xref="S4.E1.m1.1.1.3.3.2.3.cmml">T</mi><mo id="S4.E1.m1.1.1.3.3.2.1a" xref="S4.E1.m1.1.1.3.3.2.1.cmml">⁢</mo><msub id="S4.E1.m1.1.1.3.3.2.4" xref="S4.E1.m1.1.1.3.3.2.4.cmml"><mi id="S4.E1.m1.1.1.3.3.2.4.2" xref="S4.E1.m1.1.1.3.3.2.4.2.cmml">P</mi><msub id="S4.E1.m1.1.1.3.3.2.4.3" xref="S4.E1.m1.1.1.3.3.2.4.3.cmml"><mi id="S4.E1.m1.1.1.3.3.2.4.3.2" xref="S4.E1.m1.1.1.3.3.2.4.3.2.cmml">C</mi><mi id="S4.E1.m1.1.1.3.3.2.4.3.3" xref="S4.E1.m1.1.1.3.3.2.4.3.3.cmml">i</mi></msub></msub></mrow><mo id="S4.E1.m1.1.1.3.3.1" xref="S4.E1.m1.1.1.3.3.1.cmml">+</mo><mrow id="S4.E1.m1.1.1.3.3.3" xref="S4.E1.m1.1.1.3.3.3.cmml"><mi id="S4.E1.m1.1.1.3.3.3.2" xref="S4.E1.m1.1.1.3.3.3.2.cmml">F</mi><mo id="S4.E1.m1.1.1.3.3.3.1" xref="S4.E1.m1.1.1.3.3.3.1.cmml">⁢</mo><msub id="S4.E1.m1.1.1.3.3.3.3" xref="S4.E1.m1.1.1.3.3.3.3.cmml"><mi id="S4.E1.m1.1.1.3.3.3.3.2" xref="S4.E1.m1.1.1.3.3.3.3.2.cmml">P</mi><msub id="S4.E1.m1.1.1.3.3.3.3.3" xref="S4.E1.m1.1.1.3.3.3.3.3.cmml"><mi id="S4.E1.m1.1.1.3.3.3.3.3.2" xref="S4.E1.m1.1.1.3.3.3.3.3.2.cmml">C</mi><mi id="S4.E1.m1.1.1.3.3.3.3.3.3" xref="S4.E1.m1.1.1.3.3.3.3.3.3.cmml">i</mi></msub></msub></mrow><mo id="S4.E1.m1.1.1.3.3.1a" xref="S4.E1.m1.1.1.3.3.1.cmml">+</mo><mrow id="S4.E1.m1.1.1.3.3.4" xref="S4.E1.m1.1.1.3.3.4.cmml"><mi id="S4.E1.m1.1.1.3.3.4.2" xref="S4.E1.m1.1.1.3.3.4.2.cmml">F</mi><mo id="S4.E1.m1.1.1.3.3.4.1" xref="S4.E1.m1.1.1.3.3.4.1.cmml">⁢</mo><msub id="S4.E1.m1.1.1.3.3.4.3" xref="S4.E1.m1.1.1.3.3.4.3.cmml"><mi id="S4.E1.m1.1.1.3.3.4.3.2" xref="S4.E1.m1.1.1.3.3.4.3.2.cmml">N</mi><msub id="S4.E1.m1.1.1.3.3.4.3.3" xref="S4.E1.m1.1.1.3.3.4.3.3.cmml"><mi id="S4.E1.m1.1.1.3.3.4.3.3.2" xref="S4.E1.m1.1.1.3.3.4.3.3.2.cmml">C</mi><mi id="S4.E1.m1.1.1.3.3.4.3.3.3" xref="S4.E1.m1.1.1.3.3.4.3.3.3.cmml">i</mi></msub></msub></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.1b"><apply id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1"><eq id="S4.E1.m1.1.1.1.cmml" xref="S4.E1.m1.1.1.1"></eq><apply id="S4.E1.m1.1.1.2.cmml" xref="S4.E1.m1.1.1.2"><times id="S4.E1.m1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.2.1"></times><ci id="S4.E1.m1.1.1.2.2.cmml" xref="S4.E1.m1.1.1.2.2">𝐹</ci><apply id="S4.E1.m1.1.1.2.3.cmml" xref="S4.E1.m1.1.1.2.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.2.3.1.cmml" xref="S4.E1.m1.1.1.2.3">subscript</csymbol><cn id="S4.E1.m1.1.1.2.3.2.cmml" type="integer" xref="S4.E1.m1.1.1.2.3.2">1</cn><apply id="S4.E1.m1.1.1.2.3.3.cmml" xref="S4.E1.m1.1.1.2.3.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.2.3.3.1.cmml" xref="S4.E1.m1.1.1.2.3.3">subscript</csymbol><ci id="S4.E1.m1.1.1.2.3.3.2.cmml" xref="S4.E1.m1.1.1.2.3.3.2">𝐶</ci><ci id="S4.E1.m1.1.1.2.3.3.3.cmml" xref="S4.E1.m1.1.1.2.3.3.3">𝑖</ci></apply></apply></apply><apply id="S4.E1.m1.1.1.3.cmml" xref="S4.E1.m1.1.1.3"><divide id="S4.E1.m1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.3"></divide><apply id="S4.E1.m1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.3.2"><times id="S4.E1.m1.1.1.3.2.1.cmml" xref="S4.E1.m1.1.1.3.2.1"></times><cn id="S4.E1.m1.1.1.3.2.2.cmml" type="integer" xref="S4.E1.m1.1.1.3.2.2">2</cn><ci id="S4.E1.m1.1.1.3.2.3.cmml" xref="S4.E1.m1.1.1.3.2.3">𝑇</ci><apply id="S4.E1.m1.1.1.3.2.4.cmml" xref="S4.E1.m1.1.1.3.2.4"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.2.4.1.cmml" xref="S4.E1.m1.1.1.3.2.4">subscript</csymbol><ci id="S4.E1.m1.1.1.3.2.4.2.cmml" xref="S4.E1.m1.1.1.3.2.4.2">𝑃</ci><apply id="S4.E1.m1.1.1.3.2.4.3.cmml" xref="S4.E1.m1.1.1.3.2.4.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.2.4.3.1.cmml" xref="S4.E1.m1.1.1.3.2.4.3">subscript</csymbol><ci id="S4.E1.m1.1.1.3.2.4.3.2.cmml" xref="S4.E1.m1.1.1.3.2.4.3.2">𝐶</ci><ci id="S4.E1.m1.1.1.3.2.4.3.3.cmml" xref="S4.E1.m1.1.1.3.2.4.3.3">𝑖</ci></apply></apply></apply><apply id="S4.E1.m1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.3.3"><plus id="S4.E1.m1.1.1.3.3.1.cmml" xref="S4.E1.m1.1.1.3.3.1"></plus><apply id="S4.E1.m1.1.1.3.3.2.cmml" xref="S4.E1.m1.1.1.3.3.2"><times id="S4.E1.m1.1.1.3.3.2.1.cmml" xref="S4.E1.m1.1.1.3.3.2.1"></times><cn id="S4.E1.m1.1.1.3.3.2.2.cmml" type="integer" xref="S4.E1.m1.1.1.3.3.2.2">2</cn><ci id="S4.E1.m1.1.1.3.3.2.3.cmml" xref="S4.E1.m1.1.1.3.3.2.3">𝑇</ci><apply id="S4.E1.m1.1.1.3.3.2.4.cmml" xref="S4.E1.m1.1.1.3.3.2.4"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.3.2.4.1.cmml" xref="S4.E1.m1.1.1.3.3.2.4">subscript</csymbol><ci id="S4.E1.m1.1.1.3.3.2.4.2.cmml" xref="S4.E1.m1.1.1.3.3.2.4.2">𝑃</ci><apply id="S4.E1.m1.1.1.3.3.2.4.3.cmml" xref="S4.E1.m1.1.1.3.3.2.4.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.3.2.4.3.1.cmml" xref="S4.E1.m1.1.1.3.3.2.4.3">subscript</csymbol><ci id="S4.E1.m1.1.1.3.3.2.4.3.2.cmml" xref="S4.E1.m1.1.1.3.3.2.4.3.2">𝐶</ci><ci id="S4.E1.m1.1.1.3.3.2.4.3.3.cmml" xref="S4.E1.m1.1.1.3.3.2.4.3.3">𝑖</ci></apply></apply></apply><apply id="S4.E1.m1.1.1.3.3.3.cmml" xref="S4.E1.m1.1.1.3.3.3"><times id="S4.E1.m1.1.1.3.3.3.1.cmml" xref="S4.E1.m1.1.1.3.3.3.1"></times><ci id="S4.E1.m1.1.1.3.3.3.2.cmml" xref="S4.E1.m1.1.1.3.3.3.2">𝐹</ci><apply id="S4.E1.m1.1.1.3.3.3.3.cmml" xref="S4.E1.m1.1.1.3.3.3.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.3.3.3.1.cmml" xref="S4.E1.m1.1.1.3.3.3.3">subscript</csymbol><ci id="S4.E1.m1.1.1.3.3.3.3.2.cmml" xref="S4.E1.m1.1.1.3.3.3.3.2">𝑃</ci><apply id="S4.E1.m1.1.1.3.3.3.3.3.cmml" xref="S4.E1.m1.1.1.3.3.3.3.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.3.3.3.3.1.cmml" xref="S4.E1.m1.1.1.3.3.3.3.3">subscript</csymbol><ci id="S4.E1.m1.1.1.3.3.3.3.3.2.cmml" xref="S4.E1.m1.1.1.3.3.3.3.3.2">𝐶</ci><ci id="S4.E1.m1.1.1.3.3.3.3.3.3.cmml" xref="S4.E1.m1.1.1.3.3.3.3.3.3">𝑖</ci></apply></apply></apply><apply id="S4.E1.m1.1.1.3.3.4.cmml" xref="S4.E1.m1.1.1.3.3.4"><times id="S4.E1.m1.1.1.3.3.4.1.cmml" xref="S4.E1.m1.1.1.3.3.4.1"></times><ci id="S4.E1.m1.1.1.3.3.4.2.cmml" xref="S4.E1.m1.1.1.3.3.4.2">𝐹</ci><apply id="S4.E1.m1.1.1.3.3.4.3.cmml" xref="S4.E1.m1.1.1.3.3.4.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.3.4.3.1.cmml" xref="S4.E1.m1.1.1.3.3.4.3">subscript</csymbol><ci id="S4.E1.m1.1.1.3.3.4.3.2.cmml" xref="S4.E1.m1.1.1.3.3.4.3.2">𝑁</ci><apply id="S4.E1.m1.1.1.3.3.4.3.3.cmml" xref="S4.E1.m1.1.1.3.3.4.3.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.3.4.3.3.1.cmml" xref="S4.E1.m1.1.1.3.3.4.3.3">subscript</csymbol><ci id="S4.E1.m1.1.1.3.3.4.3.3.2.cmml" xref="S4.E1.m1.1.1.3.3.4.3.3.2">𝐶</ci><ci id="S4.E1.m1.1.1.3.3.4.3.3.3.cmml" xref="S4.E1.m1.1.1.3.3.4.3.3.3">𝑖</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.1c">F1_{C_{i}}=\frac{2TP_{C_{i}}}{2TP_{C_{i}}+FP_{C_{i}}+FN_{C_{i}}}</annotation><annotation encoding="application/x-llamapun" id="S4.E1.m1.1d">italic_F 1 start_POSTSUBSCRIPT italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT = divide start_ARG 2 italic_T italic_P start_POSTSUBSCRIPT italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_ARG start_ARG 2 italic_T italic_P start_POSTSUBSCRIPT italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT + italic_F italic_P start_POSTSUBSCRIPT italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT + italic_F italic_N start_POSTSUBSCRIPT italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS1.p3.7">with <math alttext="TP_{C_{i}}" class="ltx_Math" display="inline" id="S4.SS1.p3.3.m1.1"><semantics id="S4.SS1.p3.3.m1.1a"><mrow id="S4.SS1.p3.3.m1.1.1" xref="S4.SS1.p3.3.m1.1.1.cmml"><mi id="S4.SS1.p3.3.m1.1.1.2" xref="S4.SS1.p3.3.m1.1.1.2.cmml">T</mi><mo id="S4.SS1.p3.3.m1.1.1.1" xref="S4.SS1.p3.3.m1.1.1.1.cmml">⁢</mo><msub id="S4.SS1.p3.3.m1.1.1.3" xref="S4.SS1.p3.3.m1.1.1.3.cmml"><mi id="S4.SS1.p3.3.m1.1.1.3.2" xref="S4.SS1.p3.3.m1.1.1.3.2.cmml">P</mi><msub id="S4.SS1.p3.3.m1.1.1.3.3" xref="S4.SS1.p3.3.m1.1.1.3.3.cmml"><mi id="S4.SS1.p3.3.m1.1.1.3.3.2" xref="S4.SS1.p3.3.m1.1.1.3.3.2.cmml">C</mi><mi id="S4.SS1.p3.3.m1.1.1.3.3.3" xref="S4.SS1.p3.3.m1.1.1.3.3.3.cmml">i</mi></msub></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m1.1b"><apply id="S4.SS1.p3.3.m1.1.1.cmml" xref="S4.SS1.p3.3.m1.1.1"><times id="S4.SS1.p3.3.m1.1.1.1.cmml" xref="S4.SS1.p3.3.m1.1.1.1"></times><ci id="S4.SS1.p3.3.m1.1.1.2.cmml" xref="S4.SS1.p3.3.m1.1.1.2">𝑇</ci><apply id="S4.SS1.p3.3.m1.1.1.3.cmml" xref="S4.SS1.p3.3.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p3.3.m1.1.1.3.1.cmml" xref="S4.SS1.p3.3.m1.1.1.3">subscript</csymbol><ci id="S4.SS1.p3.3.m1.1.1.3.2.cmml" xref="S4.SS1.p3.3.m1.1.1.3.2">𝑃</ci><apply id="S4.SS1.p3.3.m1.1.1.3.3.cmml" xref="S4.SS1.p3.3.m1.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS1.p3.3.m1.1.1.3.3.1.cmml" xref="S4.SS1.p3.3.m1.1.1.3.3">subscript</csymbol><ci id="S4.SS1.p3.3.m1.1.1.3.3.2.cmml" xref="S4.SS1.p3.3.m1.1.1.3.3.2">𝐶</ci><ci id="S4.SS1.p3.3.m1.1.1.3.3.3.cmml" xref="S4.SS1.p3.3.m1.1.1.3.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m1.1c">TP_{C_{i}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.3.m1.1d">italic_T italic_P start_POSTSUBSCRIPT italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="FP_{C_{i}}" class="ltx_Math" display="inline" id="S4.SS1.p3.4.m2.1"><semantics id="S4.SS1.p3.4.m2.1a"><mrow id="S4.SS1.p3.4.m2.1.1" xref="S4.SS1.p3.4.m2.1.1.cmml"><mi id="S4.SS1.p3.4.m2.1.1.2" xref="S4.SS1.p3.4.m2.1.1.2.cmml">F</mi><mo id="S4.SS1.p3.4.m2.1.1.1" xref="S4.SS1.p3.4.m2.1.1.1.cmml">⁢</mo><msub id="S4.SS1.p3.4.m2.1.1.3" xref="S4.SS1.p3.4.m2.1.1.3.cmml"><mi id="S4.SS1.p3.4.m2.1.1.3.2" xref="S4.SS1.p3.4.m2.1.1.3.2.cmml">P</mi><msub id="S4.SS1.p3.4.m2.1.1.3.3" xref="S4.SS1.p3.4.m2.1.1.3.3.cmml"><mi id="S4.SS1.p3.4.m2.1.1.3.3.2" xref="S4.SS1.p3.4.m2.1.1.3.3.2.cmml">C</mi><mi id="S4.SS1.p3.4.m2.1.1.3.3.3" xref="S4.SS1.p3.4.m2.1.1.3.3.3.cmml">i</mi></msub></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.4.m2.1b"><apply id="S4.SS1.p3.4.m2.1.1.cmml" xref="S4.SS1.p3.4.m2.1.1"><times id="S4.SS1.p3.4.m2.1.1.1.cmml" xref="S4.SS1.p3.4.m2.1.1.1"></times><ci id="S4.SS1.p3.4.m2.1.1.2.cmml" xref="S4.SS1.p3.4.m2.1.1.2">𝐹</ci><apply id="S4.SS1.p3.4.m2.1.1.3.cmml" xref="S4.SS1.p3.4.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p3.4.m2.1.1.3.1.cmml" xref="S4.SS1.p3.4.m2.1.1.3">subscript</csymbol><ci id="S4.SS1.p3.4.m2.1.1.3.2.cmml" xref="S4.SS1.p3.4.m2.1.1.3.2">𝑃</ci><apply id="S4.SS1.p3.4.m2.1.1.3.3.cmml" xref="S4.SS1.p3.4.m2.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS1.p3.4.m2.1.1.3.3.1.cmml" xref="S4.SS1.p3.4.m2.1.1.3.3">subscript</csymbol><ci id="S4.SS1.p3.4.m2.1.1.3.3.2.cmml" xref="S4.SS1.p3.4.m2.1.1.3.3.2">𝐶</ci><ci id="S4.SS1.p3.4.m2.1.1.3.3.3.cmml" xref="S4.SS1.p3.4.m2.1.1.3.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.4.m2.1c">FP_{C_{i}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.4.m2.1d">italic_F italic_P start_POSTSUBSCRIPT italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="FN_{C_{i}}" class="ltx_Math" display="inline" id="S4.SS1.p3.5.m3.1"><semantics id="S4.SS1.p3.5.m3.1a"><mrow id="S4.SS1.p3.5.m3.1.1" xref="S4.SS1.p3.5.m3.1.1.cmml"><mi id="S4.SS1.p3.5.m3.1.1.2" xref="S4.SS1.p3.5.m3.1.1.2.cmml">F</mi><mo id="S4.SS1.p3.5.m3.1.1.1" xref="S4.SS1.p3.5.m3.1.1.1.cmml">⁢</mo><msub id="S4.SS1.p3.5.m3.1.1.3" xref="S4.SS1.p3.5.m3.1.1.3.cmml"><mi id="S4.SS1.p3.5.m3.1.1.3.2" xref="S4.SS1.p3.5.m3.1.1.3.2.cmml">N</mi><msub id="S4.SS1.p3.5.m3.1.1.3.3" xref="S4.SS1.p3.5.m3.1.1.3.3.cmml"><mi id="S4.SS1.p3.5.m3.1.1.3.3.2" xref="S4.SS1.p3.5.m3.1.1.3.3.2.cmml">C</mi><mi id="S4.SS1.p3.5.m3.1.1.3.3.3" xref="S4.SS1.p3.5.m3.1.1.3.3.3.cmml">i</mi></msub></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.5.m3.1b"><apply id="S4.SS1.p3.5.m3.1.1.cmml" xref="S4.SS1.p3.5.m3.1.1"><times id="S4.SS1.p3.5.m3.1.1.1.cmml" xref="S4.SS1.p3.5.m3.1.1.1"></times><ci id="S4.SS1.p3.5.m3.1.1.2.cmml" xref="S4.SS1.p3.5.m3.1.1.2">𝐹</ci><apply id="S4.SS1.p3.5.m3.1.1.3.cmml" xref="S4.SS1.p3.5.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p3.5.m3.1.1.3.1.cmml" xref="S4.SS1.p3.5.m3.1.1.3">subscript</csymbol><ci id="S4.SS1.p3.5.m3.1.1.3.2.cmml" xref="S4.SS1.p3.5.m3.1.1.3.2">𝑁</ci><apply id="S4.SS1.p3.5.m3.1.1.3.3.cmml" xref="S4.SS1.p3.5.m3.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS1.p3.5.m3.1.1.3.3.1.cmml" xref="S4.SS1.p3.5.m3.1.1.3.3">subscript</csymbol><ci id="S4.SS1.p3.5.m3.1.1.3.3.2.cmml" xref="S4.SS1.p3.5.m3.1.1.3.3.2">𝐶</ci><ci id="S4.SS1.p3.5.m3.1.1.3.3.3.cmml" xref="S4.SS1.p3.5.m3.1.1.3.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.5.m3.1c">FN_{C_{i}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.5.m3.1d">italic_F italic_N start_POSTSUBSCRIPT italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> respectively the number of True Positive, False Positive, and False Negatives for the <math alttext="i" class="ltx_Math" display="inline" id="S4.SS1.p3.6.m4.1"><semantics id="S4.SS1.p3.6.m4.1a"><mi id="S4.SS1.p3.6.m4.1.1" xref="S4.SS1.p3.6.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.6.m4.1b"><ci id="S4.SS1.p3.6.m4.1.1.cmml" xref="S4.SS1.p3.6.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.6.m4.1c">i</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.6.m4.1d">italic_i</annotation></semantics></math>-th class.
Moreover, we also consider the Balanced Accuracy Score (BAS), defined as the average recall (i.e., <math alttext="TP/(TP+FN)" class="ltx_Math" display="inline" id="S4.SS1.p3.7.m5.1"><semantics id="S4.SS1.p3.7.m5.1a"><mrow id="S4.SS1.p3.7.m5.1.1" xref="S4.SS1.p3.7.m5.1.1.cmml"><mrow id="S4.SS1.p3.7.m5.1.1.3" xref="S4.SS1.p3.7.m5.1.1.3.cmml"><mi id="S4.SS1.p3.7.m5.1.1.3.2" xref="S4.SS1.p3.7.m5.1.1.3.2.cmml">T</mi><mo id="S4.SS1.p3.7.m5.1.1.3.1" xref="S4.SS1.p3.7.m5.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p3.7.m5.1.1.3.3" xref="S4.SS1.p3.7.m5.1.1.3.3.cmml">P</mi></mrow><mo id="S4.SS1.p3.7.m5.1.1.2" xref="S4.SS1.p3.7.m5.1.1.2.cmml">/</mo><mrow id="S4.SS1.p3.7.m5.1.1.1.1" xref="S4.SS1.p3.7.m5.1.1.1.1.1.cmml"><mo id="S4.SS1.p3.7.m5.1.1.1.1.2" stretchy="false" xref="S4.SS1.p3.7.m5.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS1.p3.7.m5.1.1.1.1.1" xref="S4.SS1.p3.7.m5.1.1.1.1.1.cmml"><mrow id="S4.SS1.p3.7.m5.1.1.1.1.1.2" xref="S4.SS1.p3.7.m5.1.1.1.1.1.2.cmml"><mi id="S4.SS1.p3.7.m5.1.1.1.1.1.2.2" xref="S4.SS1.p3.7.m5.1.1.1.1.1.2.2.cmml">T</mi><mo id="S4.SS1.p3.7.m5.1.1.1.1.1.2.1" xref="S4.SS1.p3.7.m5.1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S4.SS1.p3.7.m5.1.1.1.1.1.2.3" xref="S4.SS1.p3.7.m5.1.1.1.1.1.2.3.cmml">P</mi></mrow><mo id="S4.SS1.p3.7.m5.1.1.1.1.1.1" xref="S4.SS1.p3.7.m5.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.SS1.p3.7.m5.1.1.1.1.1.3" xref="S4.SS1.p3.7.m5.1.1.1.1.1.3.cmml"><mi id="S4.SS1.p3.7.m5.1.1.1.1.1.3.2" xref="S4.SS1.p3.7.m5.1.1.1.1.1.3.2.cmml">F</mi><mo id="S4.SS1.p3.7.m5.1.1.1.1.1.3.1" xref="S4.SS1.p3.7.m5.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS1.p3.7.m5.1.1.1.1.1.3.3" xref="S4.SS1.p3.7.m5.1.1.1.1.1.3.3.cmml">N</mi></mrow></mrow><mo id="S4.SS1.p3.7.m5.1.1.1.1.3" stretchy="false" xref="S4.SS1.p3.7.m5.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.7.m5.1b"><apply id="S4.SS1.p3.7.m5.1.1.cmml" xref="S4.SS1.p3.7.m5.1.1"><divide id="S4.SS1.p3.7.m5.1.1.2.cmml" xref="S4.SS1.p3.7.m5.1.1.2"></divide><apply id="S4.SS1.p3.7.m5.1.1.3.cmml" xref="S4.SS1.p3.7.m5.1.1.3"><times id="S4.SS1.p3.7.m5.1.1.3.1.cmml" xref="S4.SS1.p3.7.m5.1.1.3.1"></times><ci id="S4.SS1.p3.7.m5.1.1.3.2.cmml" xref="S4.SS1.p3.7.m5.1.1.3.2">𝑇</ci><ci id="S4.SS1.p3.7.m5.1.1.3.3.cmml" xref="S4.SS1.p3.7.m5.1.1.3.3">𝑃</ci></apply><apply id="S4.SS1.p3.7.m5.1.1.1.1.1.cmml" xref="S4.SS1.p3.7.m5.1.1.1.1"><plus id="S4.SS1.p3.7.m5.1.1.1.1.1.1.cmml" xref="S4.SS1.p3.7.m5.1.1.1.1.1.1"></plus><apply id="S4.SS1.p3.7.m5.1.1.1.1.1.2.cmml" xref="S4.SS1.p3.7.m5.1.1.1.1.1.2"><times id="S4.SS1.p3.7.m5.1.1.1.1.1.2.1.cmml" xref="S4.SS1.p3.7.m5.1.1.1.1.1.2.1"></times><ci id="S4.SS1.p3.7.m5.1.1.1.1.1.2.2.cmml" xref="S4.SS1.p3.7.m5.1.1.1.1.1.2.2">𝑇</ci><ci id="S4.SS1.p3.7.m5.1.1.1.1.1.2.3.cmml" xref="S4.SS1.p3.7.m5.1.1.1.1.1.2.3">𝑃</ci></apply><apply id="S4.SS1.p3.7.m5.1.1.1.1.1.3.cmml" xref="S4.SS1.p3.7.m5.1.1.1.1.1.3"><times id="S4.SS1.p3.7.m5.1.1.1.1.1.3.1.cmml" xref="S4.SS1.p3.7.m5.1.1.1.1.1.3.1"></times><ci id="S4.SS1.p3.7.m5.1.1.1.1.1.3.2.cmml" xref="S4.SS1.p3.7.m5.1.1.1.1.1.3.2">𝐹</ci><ci id="S4.SS1.p3.7.m5.1.1.1.1.1.3.3.cmml" xref="S4.SS1.p3.7.m5.1.1.1.1.1.3.3">𝑁</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.7.m5.1c">TP/(TP+FN)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.7.m5.1d">italic_T italic_P / ( italic_T italic_P + italic_F italic_N )</annotation></semantics></math>) obtained on each class.
The F1 score reported in all our results is the average over the four quarters of the image. For trained models, we compute the predictions relative to each quarter using a model trained on the other 3.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.4.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.5.2">Results Analysis</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S4.T1" title="TABLE I ‣ IV Experimental Results ‣ Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_tag">I</span></a> summarizes the results obtained when testing the four considered U-Net-based models on our dataset. Moreover, we consider an ensembling approach where the final prediction is built as the average prediction of the four models.
For each model, we compare its ZS performance with the results obtained after training with the LOQO scheme.
Although the dataset has been annotated with four classes of damage, here we consider two distinct problems with 3 and 2 classes respectively.
The 3-class version merges “Severe-Damage” and “Destroyed” annotations in the same class, and aims at distinguishing them from “No-Damage” and “Moderate-Damage”.
The 2-class problem, instead, is aimed at identifying damaged vs undamaged buildings, merging all classes except “No-Damage” into one.
Reducing the number of classes simplifies the DNN’s task and aligns with the needs of humanitarian organizations, which prioritize identifying heavily damaged areas for rapid intervention. It also mitigates the impact of noisy annotations, as distinguishing between destroyed and severely damaged buildings can be arbitrary even for expert human labelers.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.2">Different insights can be drawn from these results. First, the ZS networks perform better than random (i.e., F1 score <math alttext="&gt;" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.1"><semantics id="S4.SS2.p2.1.m1.1a"><mo id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><gt id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.1d">&gt;</annotation></semantics></math>33% and <math alttext="&gt;" class="ltx_Math" display="inline" id="S4.SS2.p2.2.m2.1"><semantics id="S4.SS2.p2.2.m2.1a"><mo id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><gt id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.2.m2.1d">&gt;</annotation></semantics></math>50% for 3- and 2-class problems respectively), demonstrating a certain degree of transferability. However, the networks trained with the Ukrainian data outperform the ZS networks in the majority of cases, underscoring the importance and effectiveness of retraining.
On the more challenging 3-class problem, the trained ResNet improves the F1 score by 21% over the ZS version, while on the easier 2-class scenario the same network improves the F1 score by 10%.
The only architecture where training never improves the ZS results is the SENet. The reason for this behavior can be found in the fact that SENet is the largest network, thus more prone to overfitting especially when the volume of data is not so large, as in our case.
Coherently, the smallest architecture, i.e., the ResNet achieves the best results on the 3-class problem with an F1 score of 59%. Instead, the SEResNeXt architecture achieves the best F1 score in the two-class scenario with 69%.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Interestingly, we can notice how the F1 scores on the No-Damage class are on par between the ZS and trained networks while the ZS networks perform poorly on the Moderate-Damage class. This is expected, as identifying No-Damage does not depend on the type of scenario considered (natural disaster or war), allowing ZS networks to perform well. In contrast, war damage differs significantly from the one caused by natural disasters, leading to poorer performance of the ZS networks in identifying different degrees of damage.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">Finally, it is important to point out that the ground-truth annotations are performed at the single building level, while our models classify pixel by pixel. Therefore, aggregating the predictions over an entire building (e.g., using some representative statistic) could further improve performance. We plan to address this issue in the future works.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.4.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.5.2">Ablation Studies</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">In this section, we assess the importance of the different parts of the proposed pipeline. Namely, we first compare the results obtained fine-tuning DNNs originally pre-trained on xBD with training from scratch. Then, we perform an ablation study to investigate the effect of data augmentations and of the dilation filter discussed in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S3.SS3" title="III-C Training scheme ‣ III Methods ‣ Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a>. The whole analysis is performed on the ResNet architecture, i.e., the best-performing architecture on the 3-class problem.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S4.T2" title="TABLE II ‣ IV Experimental Results ‣ Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_tag">II</span></a> summarizes the effect of the pre-training on the xBD dataset (i.e., transfer learning). With respect to training from scratch, pretraining helps in boosting the performance for both versions of the problem. In particular, with 3 classes, the F1 score increases from 56% to 59% with a net improvement of 3%. The improvement on the 2-class version is still present but more limited (+1%).</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S4.T3" title="TABLE III ‣ IV Experimental Results ‣ Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_tag">III</span></a> compares a baseline trained with the scheme of Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.04802v1#S3.SS3" title="III-C Training scheme ‣ III Methods ‣ Building Damage Assessment in Conflict Zones: A Deep Learning Approach Using Geospatial Sub-Meter Resolution Data This work has received funding from the ESA “Space in Response to Humanitarian Crises” program (ESA Contract 4000142151/23/NL/EG/an). We also thank Nil Pedro Angli of ESA, Bruno Aster and Elena Lorusso of Gisky s.r.l., Filippo Dacarro and Luca Grottoli of Fondazione Eucentre."><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a> with and without the proposed augmentation pipeline and the dilation filter. The most important contribution to the F1 score is represented by our proposed custom augmentations, which enhances performance by 6% and 4% respectively for 3- and 2-class scenarios. The dilation filter helps to further improve the F1 score by 1% in both cases.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Conclusions</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">DNN-enabled automatic classification of VHR geospatial imagery represents a key technology to improve HADR, especially in war-related scenarios which are under-explored.
This work employs an annotated dataset for building damage assessment, with images acquired over the Mariupol city, to train and assess the performance of a collection of SotA CNNs originally proposed for natural disasters.
Future works will include the study of novel architectures such as Transformers and the collection and the annotation of more data.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:50%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="font-size:50%;">
R. Gupta </span><em class="ltx_emph ltx_font_italic" id="bib.bib1.2.2" style="font-size:50%;">et al.</em><span class="ltx_text" id="bib.bib1.3.3" style="font-size:50%;">, “Creating xbd: A dataset for assessing building damage from satellite imagery,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib1.4.4" style="font-size:50%;">IEEE/CVF CVPR</em><span class="ltx_text" id="bib.bib1.5.5" style="font-size:50%;">, 2019, pp. 10–17.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="font-size:50%;">
Y. Aimaiti </span><em class="ltx_emph ltx_font_italic" id="bib.bib2.2.2" style="font-size:50%;">et al.</em><span class="ltx_text" id="bib.bib2.3.3" style="font-size:50%;">, “War related building damage assessment in kyiv, ukraine, using sentinel-1 radar and sentinel-2 optical images,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib2.4.4" style="font-size:50%;">Remote Sensing</em><span class="ltx_text" id="bib.bib2.5.5" style="font-size:50%;">, vol. 14, no. 24, p. 6239, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="font-size:50%;">
M. Stasolla </span><em class="ltx_emph ltx_font_italic" id="bib.bib3.2.2" style="font-size:50%;">et al.</em><span class="ltx_text" id="bib.bib3.3.3" style="font-size:50%;">, “Rapid damage mapping in areas of conflict by means of sentinel-1 time series: The kyiv test case,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib3.4.4" style="font-size:50%;">IEEE IGARSS</em><span class="ltx_text" id="bib.bib3.5.5" style="font-size:50%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="font-size:50%;">
F. Fakhri </span><em class="ltx_emph ltx_font_italic" id="bib.bib4.2.2" style="font-size:50%;">et al.</em><span class="ltx_text" id="bib.bib4.3.3" style="font-size:50%;">, “Integration of sentinel-1 and sentinel-2 data for change detection: a case study in a war conflict area of mosul city,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib4.4.4" style="font-size:50%;">Remote Sensing Applications: Society and Environment</em><span class="ltx_text" id="bib.bib4.5.5" style="font-size:50%;">, vol. 22, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="font-size:50%;">
M. Hussain </span><em class="ltx_emph ltx_font_italic" id="bib.bib5.2.2" style="font-size:50%;">et al.</em><span class="ltx_text" id="bib.bib5.3.3" style="font-size:50%;">, “Change detection from remotely sensed images: From pixel-based to object-based approaches,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib5.4.4" style="font-size:50%;">ISPRS P&amp;RS</em><span class="ltx_text" id="bib.bib5.5.5" style="font-size:50%;">, 2013.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.1.1" style="font-size:50%;">
N. O’Mahony </span><em class="ltx_emph ltx_font_italic" id="bib.bib6.2.2" style="font-size:50%;">et al.</em><span class="ltx_text" id="bib.bib6.3.3" style="font-size:50%;">, “Deep learning vs. traditional computer vision,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib6.4.4" style="font-size:50%;">Advances in Computer Vision: Proc. of the 2019 CVC</em><span class="ltx_text" id="bib.bib6.5.5" style="font-size:50%;">.  Springer, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.1.1" style="font-size:50%;">
O. Ronneberger </span><em class="ltx_emph ltx_font_italic" id="bib.bib7.2.2" style="font-size:50%;">et al.</em><span class="ltx_text" id="bib.bib7.3.3" style="font-size:50%;">, “U-net: Convolutional networks for biomedical image segmentation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib7.4.4" style="font-size:50%;">MICCAI 2015</em><span class="ltx_text" id="bib.bib7.5.5" style="font-size:50%;">.  Springer, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.1.1" style="font-size:50%;">
J. Hu </span><em class="ltx_emph ltx_font_italic" id="bib.bib8.2.2" style="font-size:50%;">et al.</em><span class="ltx_text" id="bib.bib8.3.3" style="font-size:50%;">, “Squeeze-and-excitation networks,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib8.4.4" style="font-size:50%;">CVPR</em><span class="ltx_text" id="bib.bib8.5.5" style="font-size:50%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.1.1" style="font-size:50%;">
B. J. Wheeler </span><em class="ltx_emph ltx_font_italic" id="bib.bib9.2.2" style="font-size:50%;">et al.</em><span class="ltx_text" id="bib.bib9.3.3" style="font-size:50%;">, “Deep learning-enabled semantic inference of individual building damage magnitude from satellite images,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib9.4.4" style="font-size:50%;">Algorithms</em><span class="ltx_text" id="bib.bib9.5.5" style="font-size:50%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.1.1" style="font-size:50%;">
H. Chen </span><em class="ltx_emph ltx_font_italic" id="bib.bib10.2.2" style="font-size:50%;">et al.</em><span class="ltx_text" id="bib.bib10.3.3" style="font-size:50%;">, “Dual-tasks siamese transformer framework for building damage assessment,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib10.4.4" style="font-size:50%;">IGARSS 2022</em><span class="ltx_text" id="bib.bib10.5.5" style="font-size:50%;">.  IEEE.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.1.1" style="font-size:50%;">
Y. Shen </span><em class="ltx_emph ltx_font_italic" id="bib.bib11.2.2" style="font-size:50%;">et al.</em><span class="ltx_text" id="bib.bib11.3.3" style="font-size:50%;">, “Bdanet: Multiscale convolutional neural network with cross-directional attention for building damage assessment from satellite images,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib11.4.4" style="font-size:50%;">IEEE Trans. on Geoscience and Remote Sensing</em><span class="ltx_text" id="bib.bib11.5.5" style="font-size:50%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.1.1" style="font-size:50%;">
vdurnov, “xview2 1st place solution,” </span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="font-size:50%;">https://github.com/vdurnov/xview2_1st_place_solution/tree/master</span><span class="ltx_text" id="bib.bib12.2.2" style="font-size:50%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.1.1" style="font-size:50%;">
C. G. Andresen </span><em class="ltx_emph ltx_font_italic" id="bib.bib13.2.2" style="font-size:50%;">et al.</em><span class="ltx_text" id="bib.bib13.3.3" style="font-size:50%;">, “Change detection applications in the earth sciences using uas-based sensing: A review and future opportunities,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib13.4.4" style="font-size:50%;">Drones</em><span class="ltx_text" id="bib.bib13.5.5" style="font-size:50%;">, vol. 7, no. 4, p. 258, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.1.1" style="font-size:50%;">
A. Mukhopadhyay </span><em class="ltx_emph ltx_font_italic" id="bib.bib14.2.2" style="font-size:50%;">et al.</em><span class="ltx_text" id="bib.bib14.3.3" style="font-size:50%;">, “High-resolution pléiades data: an in-depth analysis of applications and future prospects,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib14.4.4" style="font-size:50%;">Spatial Information Research</em><span class="ltx_text" id="bib.bib14.5.5" style="font-size:50%;">, pp. 1–17, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.1.1" style="font-size:50%;">
D. Tomowski </span><em class="ltx_emph ltx_font_italic" id="bib.bib15.2.2" style="font-size:50%;">et al.</em><span class="ltx_text" id="bib.bib15.3.3" style="font-size:50%;">, “Colour and texture based change detection for urban disaster analysis,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib15.4.4" style="font-size:50%;">2011 Joint Urban Remote Sensing Event</em><span class="ltx_text" id="bib.bib15.5.5" style="font-size:50%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.1.1" style="font-size:50%;">
P. Fisher </span><em class="ltx_emph ltx_font_italic" id="bib.bib16.2.2" style="font-size:50%;">et al.</em><span class="ltx_text" id="bib.bib16.3.3" style="font-size:50%;">, “Detecting change in vague interpretations of landscapes,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib16.4.4" style="font-size:50%;">Ecological Informatics</em><span class="ltx_text" id="bib.bib16.5.5" style="font-size:50%;">, vol. 1, no. 2, pp. 163–178, 2006.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.1.1" style="font-size:50%;">
D. Peng </span><em class="ltx_emph ltx_font_italic" id="bib.bib17.2.2" style="font-size:50%;">et al.</em><span class="ltx_text" id="bib.bib17.3.3" style="font-size:50%;">, “End-to-end change detection for high resolution satellite images using improved unet++,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib17.4.4" style="font-size:50%;">Remote Sensing</em><span class="ltx_text" id="bib.bib17.5.5" style="font-size:50%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.1.1" style="font-size:50%;">
A. Ismail </span><em class="ltx_emph ltx_font_italic" id="bib.bib18.2.2" style="font-size:50%;">et al.</em><span class="ltx_text" id="bib.bib18.3.3" style="font-size:50%;">, “Bldnet: A semi-supervised change detection building damage framework using graph convolutional networks and urban domain knowledge,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib18.4.4" style="font-size:50%;">arXiv preprint arXiv:2201.10389</em><span class="ltx_text" id="bib.bib18.5.5" style="font-size:50%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.1.1" style="font-size:50%;">
“Openstreetmap,” </span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="font-size:50%;">https://www.openstreetmap.org</span><span class="ltx_text" id="bib.bib19.2.2" style="font-size:50%;">, accessed:2024-07-12.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.1.1" style="font-size:50%;">
W. Wang </span><em class="ltx_emph ltx_font_italic" id="bib.bib20.2.2" style="font-size:50%;">et al.</em><span class="ltx_text" id="bib.bib20.3.3" style="font-size:50%;">, “A survey of zero-shot learning: Settings, methods, and applications,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib20.4.4" style="font-size:50%;">ACM TIST</em><span class="ltx_text" id="bib.bib20.5.5" style="font-size:50%;">, no. 2, pp. 1–37, 2019.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Oct  7 07:22:09 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
