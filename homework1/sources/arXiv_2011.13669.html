<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2011.13669] Towards real-time object recognition and pose estimation in point clouds</title><meta property="og:description" content="Object recognition and 6DoF pose estimation are quite challenging tasks in computer vision applications. Despite efficiency in such tasks, standard methods deliver far from real-time processing rates. This paper presenâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Towards real-time object recognition and pose estimation in point clouds">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Towards real-time object recognition and pose estimation in point clouds">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2011.13669">

<!--Generated on Tue Mar 19 05:49:52 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Towards real-time object recognition and pose estimation in point clouds</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Marlon Marcon<sup id="id12.12.id1" class="ltx_sup"><span id="id12.12.id1.1" class="ltx_text ltx_font_italic">1</span></sup>
<img src="/html/2011.13669/assets/orcid.png" id="id2.2.g1" class="ltx_graphics ltx_img_square" width="12" height="12" alt="[Uncaptioned image]">
, Olga Regina Pereira Bellon<sup id="id13.13.id2" class="ltx_sup"><span id="id13.13.id2.1" class="ltx_text ltx_font_italic">2</span></sup>
<img src="/html/2011.13669/assets/orcid.png" id="id5.5.g2" class="ltx_graphics ltx_img_square" width="12" height="12" alt="[Uncaptioned image]">
, Luciano Silva<sup id="id14.14.id3" class="ltx_sup"><span id="id14.14.id3.1" class="ltx_text ltx_font_italic">2</span></sup>
<img src="/html/2011.13669/assets/orcid.png" id="id8.8.g3" class="ltx_graphics ltx_img_square" width="12" height="12" alt="[Uncaptioned image]">
 
<br class="ltx_break">
<span id="id10.10.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:448.1pt;">
<span id="id10.10.1.1" class="ltx_p"><sup id="id10.10.1.1.1" class="ltx_sup"><span id="id10.10.1.1.1.1" class="ltx_text ltx_font_italic">1</span></sup><span id="id10.10.1.1.2" class="ltx_text ltx_font_italic">Dapartment of Software Engineering, Federal University of Technology - ParanÃ¡, Dois Vizinhos, Brazil</span></span>
</span> 
<br class="ltx_break">
<span id="id11.11.2" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:448.1pt;">
<span id="id11.11.2.1" class="ltx_p"><sup id="id11.11.2.1.1" class="ltx_sup"><span id="id11.11.2.1.1.1" class="ltx_text ltx_font_italic">2</span></sup><span id="id11.11.2.1.2" class="ltx_text ltx_font_italic">Department of Computer Science, Federal University of ParanÃ¡, Curitiba, Brazil</span></span>
</span> 
<br class="ltx_break"><span id="id15.15.id4" class="ltx_text ltx_font_italic">marlonmarcon@utfpr.edu.br, olga@ufpr.br, luciano@inf.ufpr.br</span> 
<br class="ltx_break">
</span><span class="ltx_author_notes"><img src="/html/2011.13669/assets/orcid.png" id="id3.3.g1" class="ltx_graphics ltx_img_square" width="12" height="12" alt="[Uncaptioned image]">Â https://orcid.org/0000-0001-9748-2824<img src="/html/2011.13669/assets/orcid.png" id="id6.6.g1" class="ltx_graphics ltx_img_square" width="12" height="12" alt="[Uncaptioned image]">Â https://orcid.org/0000-0003-2683-9704<img src="/html/2011.13669/assets/orcid.png" id="id9.9.g1" class="ltx_graphics ltx_img_square" width="12" height="12" alt="[Uncaptioned image]">Â https://orcid.org/0000-0001-6341-1323</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id16.id1" class="ltx_p">Object recognition and 6DoF pose estimation are quite challenging tasks in computer vision applications. Despite efficiency in such tasks, standard methods deliver far from real-time processing rates. This paper presents a novel pipeline to estimate a fine 6DoF pose of objects, applied to realistic scenarios in real-time. We split our proposal into three main parts. Firstly, a Color feature classification leverages the use of pre-trained CNN color features trained on the ImageNet for object detection. A Feature-based registration module conducts a coarse pose estimation, and finally, a Fine-adjustment step performs an ICP-based dense registration. Our proposal achieves, in the best case, an accuracy performance of almost 83% on the RGB-D Scenes dataset. Regarding processing time, the object detection task is done at a frame processing rate up to 90 FPS, and the pose estimation at almost 14 FPS in a full execution strategy. We discuss that due to the proposalâ€™s modularity, we could let the full execution occurs only when necessary and perform a scheduled execution that unlocks real-time processing, even for multitask situations.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>INTRODUCTION</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Object recognition and 6D pose estimation represent a central role in a broad spectrum of computer vision applications, such as object grasping and manipulation, bin picking tasks, and industrial assemblies verification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx35" title="" class="ltx_ref">Vock etÂ al., 2019</a>]</cite>. Successful object recognition, highly reliable pose estimation, and near real-time operation are essential capabilities and current challenges for robot perception systems.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">A methodology usually employed to estimate rigid transformations between scenes and objects is centered on a feature-based template matching approach. Assuming we have a known item or a part of an object, this technique involves searching all the occurrences in a larger and usually cluttered scene <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx35" title="" class="ltx_ref">Vock etÂ al., 2019</a>]</cite>. However, due to natural occlusions, such occurrences may be represented only by a partial view of an object. The template is often another point cloud, and the main challenge of the template matching approach is to maintain the runtime feasibility and preserve the robustness.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Template matching approaches rely on RANSAC-based feature matching algorithms, following the pipeline proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">Aldoma etÂ al., 2012b</a>]</cite>. RANSAC algorithm has proven to be one of the most versatile and robust. Unfortunately, for large or dense point clouds, its runtime becomes a significant limitation in several of the example applications mentioned above <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx35" title="" class="ltx_ref">Vock etÂ al., 2019</a>]</cite>. When we seek a 6Dof estimation pose, the real-time is a more challenging task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">Marcon etÂ al., 2019</a>]</cite>. In an extensive benchmark of full cloud object detection and pose estimation, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">Hodan etÂ al., 2018</a>]</cite> reported runtime of a second per test target on average.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Deep learning strategies for object recognition and classification problems have been extensively studied for RGB images. As the demand for good quality labeled data increases, large datasets are becoming available, serving as a significant benchmark of methods (deep or not) and as training data for real applications. ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">Deng etÂ al., 2009</a>]</cite> is, undoubtedly, the most studied dataset and the <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">de-facto</span> standard on such recognition tasks. This dataset presents more than 20,000 categories, but a subset with 1,000 categories, known as ImageNet Large Scale Visual Recognition Challenge (ILSVRC), is mostly used.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Training a model on ImageNet is quite a challenging task in terms of computational resources and time consumption. Fortunately, transferring its models offer efficient solutions in different contexts, acting as a blackbox feature extractor. Studies like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">Agrawal etÂ al., 2014</a>]</cite> explore and corroborate this high capacity of transferring such models to different contexts and applications. Regarding the use of pre-trained CNN features, some approaches handle the object recognition on the Washington RGB-D Object dataset, e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx39" title="" class="ltx_ref">Zia etÂ al., 2017</a>]</cite> with the VGG architecture and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">Caglayan etÂ al., 2020</a>]</cite> evaluate several popular deep networks, such as AlexNet, VGG, ResNet, and DenseNet.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">This paper introduces a novel pipeline to deal with point cloud pose estimation in uncontrolled environments and cluttered scenes. Our proposed pipeline recognizes the object using color feature descriptors, crops the selected bounding-box reducing the scenesâ€™ searching surface, and finally estimates the objectâ€™s pose in a traditional local feature-based approach. Despite adopting well-known techniques in the 2D/3D computer vision field, our proposalâ€™s novelty centers on the smooth integration between 2D and 3D methods to provide a solution efficient in terms of accuracy and time.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>BACKGROUND</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Recognition systems work with objects, which are digital representations of tangible real-world items that exist physically in a scene. Such systems are unavoidably machine-learning-based approaches that use features to locate and identify objects in a scene reliably. Together with the recognition, another task is to estimate the location and orientation of the detected items. In a 3D world, we estimate six degrees of freedom (6DoF), which refers to the geometrical transformation representing a rigid bodyâ€™s movement in a 3D space, i.e., the combination of translation and rotation.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Color feature extraction</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">As a mark on the deep learning history, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">Krizhevsky etÂ al., 2012</a>]</cite> presented the first Deep Convolutional Architecture employed on the ILSVRC, an 8-layer architecture dubbed AlexNet. This network was the first to prove that deep learning could beat hand-crafted methods when trained on a large scale dataset. After that, ConvNets became more accurate, deeper, and bigger in terms of parameters. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx30" title="" class="ltx_ref">Simonyan and Zisserman, 2015</a>]</cite> propose VGG, a network that doubled the depth of AlexNet, but exploring tiny filters (<math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mrow id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml"><mn id="S2.SS1.p1.1.m1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p1.1.m1.1.1.1" xref="S2.SS1.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="S2.SS1.p1.1.m1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"><times id="S2.SS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S2.SS1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.2">3</cn><cn type="integer" id="S2.SS1.p1.1.m1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">3\times 3</annotation></semantics></math>), and became the runner-up on the ILSVRC, one step back the GoogLeNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx31" title="" class="ltx_ref">Szegedy etÂ al., 2015</a>]</cite>, with 22 layers. GoogLeNet relies on the Inception architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx32" title="" class="ltx_ref">Szegedy etÂ al., 2016</a>]</cite>. Another type of ConvNets, called ResNets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">He etÂ al., 2016</a>]</cite>, uses the concept of residual blocks that use skip-connection blocks that learn residual functions regarding the input. Many architectures have been proposed based on these findings, such as ResNet with 50, 101, and 152 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">He etÂ al., 2016</a>]</cite>. Also, based on developments regarding the residual blocks, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx36" title="" class="ltx_ref">Xie etÂ al., 2017</a>]</cite> developed the ResNeXt architecture. The basis upon ResNeXt blocks resides on parallel ResNet-like blocks, which have the output summed before the residual calculation. Some architectures propose the use of Deep Learning features on resource-limited devices, such as smartphones and embedded systems. The most prominent architecture is the MobileNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx28" title="" class="ltx_ref">Sandler etÂ al., 2018</a>]</cite>. Another family of leading networks is the EfficientNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx33" title="" class="ltx_ref">Tan and Le, 2019</a>]</cite>. Relying on the use of these lighter architectures, EfficientNet proposes very deep architectures without compromise resource efficiency.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Pose estimation</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">As presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">Aldoma etÂ al., 2012b</a>]</cite>, a comprehensive registration process usually consists of two steps: coarse and fine registrations. We can produce a coarse registration transformation by performing a manual alignment, motion tracking or, the most common, by using the local feature matching. Local-feature-matching-based algorithms automatically obtain corresponding points from two or multiple point-clouds, coarsely registering by minimizing the distance between them. These methods have been extensively studied and have confirmed to be compliant and computer efficient <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">Guo etÂ al., 2016</a>]</cite>. After coarsely registering the point clouds, a fine-registration algorithm is applied to refine the initial coarse registration iteratively. Examples of fine-registration algorithms include the ICP algorithm that perform point-to-point alignment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Besl and McKay, 1992</a>]</cite>, or point-to-plane <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">Chen and Medioni, 1992</a>]</cite>. These algorithms are suitable for matching between point-clouds of isolated scenes (3D registration) or between a scene and a model (3D object recognition). This proposal adopted two approaches to generate the initial alignment: a traditional feature-based RANSAC and the Fast Global Registration (FGR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx38" title="" class="ltx_ref">Zhou etÂ al., 2016</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>PROPOSED APPROACH</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we explain in detail our proposed approach. Our proposed pipeline starts from an RGB image and its corresponding point cloud, generated from RGB and depth images. These inputs are submitted to our three-stage architecture: color feature classification, feature-based registration, and fine adjustment. We depict our proposal in <a href="#S3.F1" title="Figure 1 â€£ 3 PROPOSED APPROACH â€£ Towards real-time object recognition and pose estimation in point clouds" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">FigureÂ 1</span></a> and present these steps in the next sections.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2011.13669/assets/figures/pipeline.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="562" height="322" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Pipeline of the proposed approach to pose estimation. To estimate the pre-segmented objectâ€™s instance, we extract its features by a deep learning color-based extractor and a pre-trained ML classifier. After selecting the objects database, the view with the highest number of correspondences resulting from a feature-based registration algorithm. Finally, we apply an ICP dense registration algorithm to estimate the position and pose of the object.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Color feature classification</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Our proposal starts detecting the target object and estimating a bounding box of it. After this detection, we preprocess the image and submit to a deep-learning-based color feature extractor. The preprocessing step includes image cropping and resizing to adjust to the network input dimensions. The deep network architectures employed in our experiments output a feature vector, 1000 bins long, used to predict the objectâ€™s instance, by a pre-trained ML classifier. We emphasize that our approach is size-independent regarding the feature vector, but for a fair comparison we chose networks with the same output size.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.2" class="ltx_p">In our trials, we explored the achievements of <a href="#S4.T2" title="Table 2 â€£ 4.4.1 Object detection benchmark â€£ 4.4 Results â€£ 4 EXPERIMENTAL RESULTS â€£ Towards real-time object recognition and pose estimation in point clouds" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">TableÂ 2</span></a>, and selected the most accurate networks: ResNet101 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">He etÂ al., 2016</a>]</cite>, MobileNet v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx28" title="" class="ltx_ref">Sandler etÂ al., 2018</a>]</cite>, ResNeXt101 32<math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mo id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><times id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\times</annotation></semantics></math>8d <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx36" title="" class="ltx_ref">Xie etÂ al., 2017</a>]</cite>, and EfficientNet-B7 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx33" title="" class="ltx_ref">Tan and Le, 2019</a>]</cite>. These networks input a <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="224\times 244" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mrow id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mn id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.2.m2.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.cmml">Ã—</mo><mn id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">244</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><times id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1"></times><cn type="integer" id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">224</cn><cn type="integer" id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3">244</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">224\times 244</annotation></semantics></math> pixel image and output a 1000 bins feature vector. We employed the Logistic regression classifier, chosen after a performance evaluation of standard classifiers, to name: Support Vector Classifier (SVC) with linear and radial-based kernels, Random forest, Multilayer perceptron, and Gaussian naÃ¯ve Bayes. We explore two variants of our ML model: a pre-trained on the Washington RGB-D Object dataset, and a distinct model, also in such dataset, but with a reduced number of objects, i.e., those annotated on the Washington RGB-D Scenes dataset. The latter provides an application-oriented approach, reducing the number of achievable classes, the inference time, and model size (<a href="#S4.T4" title="Table 4 â€£ 4.4.2 Object recognition in real-world scenes â€£ 4.4 Results â€£ 4 EXPERIMENTAL RESULTS â€£ Towards real-time object recognition and pose estimation in point clouds" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">TableÂ 4</span></a>). To verify the best accurate classifier, we do not perform object detection. Instead, we get the ground-truth bounding boxes provided by the dataset, hence verifying for each ML system which is the best feasible performance.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Feature-based registration</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We build a model database by extracting and storing useful information about the objects in a previous step. The database is composed of information concerning each item, as well as the extracted features of them. We choose a local-descriptors-based approach to estimate the objectâ€™s pose. For each instance of an object, we store several partial views of it. Between these views, our method will select the most likely to the correspondent object on the scene.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Based on the predicted objectsâ€™ classes, we can select a set of described views from the modelsâ€™ database. We then perform a feature-based registration between these views and the point cloud of the sceneâ€™s object (previously cropped based on the detected bounding box). This method will estimate a transformation based on the correspondences between a scene and a partial view of an object. Then, the view with the highest number of inliers and at least three correspondences is selected. The estimated affine transformation will be input to the ICP algorithm and perform a dense registration.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">We process each cloud with a uniform sampling as a keypoint extractor, adopting a leaf size of 1 cm. After, we describe each keypoint using the FPFH <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Rusu etÂ al., 2009</a>]</cite> descriptor with a radius of 5 cm. We choose this descriptor due to its processing time and size (33 bins), well-suited for real-time applications. Methods like CSHOT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx27" title="" class="ltx_ref">Salti etÂ al., 2014</a>]</cite> describes the color and geometric information and has proven to be an accurate solution applied to single object recognition on RGB-D Object dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">Ouadiay etÂ al., 2016</a>]</cite>. However, with a descriptor length of 1344 bins, it is not suitable for real-time feature-matching. Another proposal that deals with color is PFHRGB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx26" title="" class="ltx_ref">Rusu etÂ al., 2008</a>]</cite>, which, despite being shorter (250 bins) than CSHOT, presents inefficient calculation time <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">Marcon etÂ al., 2019</a>]</cite>.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">To perform the coarse registration step, we test two methods previously presented: RANSAC and FGR. We considered for both techniques an inlier correspondence distance lower than 1 cm between scene and models. We set the convergence criteria for RANSAC to 4M iterations and 500 validation steps, and for FGR to 100 iterations, following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">Choi etÂ al., 2015</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx38" title="" class="ltx_ref">Zhou etÂ al., 2016</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Fine-adjustment</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The previous step outputs an affine transformation that could work as a final pose of the object concerning the scene. However, to guarantee a fine-adjustment, we employ an additional step to the process. We adopt the ICP algorithm, based on the point-to-plane approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">Chen and Medioni, 1992</a>]</cite>, to perform a dense registration. We use the transformation resultant from the registration step, the scene, and best-fitted view clouds as input. We set the maximum correspondence distance threshold to 1 cm. It is important to point that again, our proposal is generic, and the fine adjustment algorithm employed in this stage is flexible. Methods such as ICP point-to-point <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Besl and McKay, 1992</a>]</cite> and ColoredICP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx23" title="" class="ltx_ref">Park etÂ al., 2017</a>]</cite> are perfectly adapted to our pipeline.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>EXPERIMENTAL RESULTS</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We validate our proposal on the Washington RGB-D Object and Scenes datasets. Proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">Lai etÂ al., 2011a</a>]</cite> the RGB-D Object contains a collection of 300 instances of household objects, grouped in 51 distinct categories. Each object includes a set of views, captured from different viewpoints with a Kinect sensor. A collection of 3 images, including RGB, depth, and mask is presented for each view. In total, this dataset has about 250 thousand distinct images. The authors also provide a dataset of scenes, named RGB-D Scenes. This evaluation dataset has eight video sequences of every-day environments. A Kinect sensor positioned at a human eye-level height acquires all the images at a <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="640\times 480" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mn id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">640</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">480</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><times id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">640</cn><cn type="integer" id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">480</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">640\times 480</annotation></semantics></math> resolution. This dataset is related to the first one, composed of 13 of the 51 object categories on the Object dataset. These objects are positioned over tables, desks, and kitchen surfaces, cluttered with viewpoints and occlusion variation, and have annotation at category and instance levels. A bidimensional bounding box represents the ground-truth of each objectâ€™s position. <a href="#S4.F2" title="Figure 2 â€£ 4.1 Dataset â€£ 4 EXPERIMENTAL RESULTS â€£ Towards real-time object recognition and pose estimation in point clouds" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">FigureÂ 2</span></a> presents examples of both datasets. <a href="#S4.T1" title="Table 1 â€£ 4.1 Dataset â€£ 4 EXPERIMENTAL RESULTS â€£ Towards real-time object recognition and pose estimation in point clouds" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">TableÂ 1</span></a> gives some details regarding the name and size of the sequences, and their average number of objects.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2011.13669/assets/x1.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="279" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Examples of models and scenes from the Washington RGB-D Scenes dataset (top row), and objects from the RGB-D Object dataset (bottom row). Source: Adapted from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">Lai etÂ al., 2011a</a>]</cite>.</figcaption>
</figure>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Details regarding the RGB-D Scenes datasets. </figcaption>
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" style="padding:1.5pt 2.0pt;">Scene</th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:1.5pt 2.0pt;">Number of frames</th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:1.5pt 2.0pt;">Models per frame</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<th id="S4.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:1.5pt 2.0pt;">desk_1</th>
<td id="S4.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 2.0pt;">98</td>
<td id="S4.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 2.0pt;">1.89</td>
</tr>
<tr id="S4.T1.1.3.2" class="ltx_tr">
<th id="S4.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.5pt 2.0pt;">desk_2</th>
<td id="S4.T1.1.3.2.2" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">190</td>
<td id="S4.T1.1.3.2.3" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">1.85</td>
</tr>
<tr id="S4.T1.1.4.3" class="ltx_tr">
<th id="S4.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.5pt 2.0pt;">desk_3</th>
<td id="S4.T1.1.4.3.2" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">228</td>
<td id="S4.T1.1.4.3.3" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">2.56</td>
</tr>
<tr id="S4.T1.1.5.4" class="ltx_tr">
<th id="S4.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.5pt 2.0pt;">kitchen_small_1</th>
<td id="S4.T1.1.5.4.2" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">180</td>
<td id="S4.T1.1.5.4.3" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">3.55</td>
</tr>
<tr id="S4.T1.1.6.5" class="ltx_tr">
<th id="S4.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.5pt 2.0pt;">meeting_small_1</th>
<td id="S4.T1.1.6.5.2" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">180</td>
<td id="S4.T1.1.6.5.3" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">8.79</td>
</tr>
<tr id="S4.T1.1.7.6" class="ltx_tr">
<th id="S4.T1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.5pt 2.0pt;">table_1</th>
<td id="S4.T1.1.7.6.2" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">125</td>
<td id="S4.T1.1.7.6.3" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">5.92</td>
</tr>
<tr id="S4.T1.1.8.7" class="ltx_tr">
<th id="S4.T1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.5pt 2.0pt;">table_small_1</th>
<td id="S4.T1.1.8.7.2" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">199</td>
<td id="S4.T1.1.8.7.3" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">3.68</td>
</tr>
<tr id="S4.T1.1.9.8" class="ltx_tr">
<th id="S4.T1.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.5pt 2.0pt;">table_small_2</th>
<td id="S4.T1.1.9.8.2" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">234</td>
<td id="S4.T1.1.9.8.3" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">2.89</td>
</tr>
<tr id="S4.T1.1.10.9" class="ltx_tr">
<th id="S4.T1.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" style="padding:1.5pt 2.0pt;">Average</th>
<td id="S4.T1.1.10.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding:1.5pt 2.0pt;">179.25</td>
<td id="S4.T1.1.10.9.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding:1.5pt 2.0pt;">3.89</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation Protocol</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We evaluate our proposal, quantitatively, and qualitatively. First, we consider CNN feature extraction and classification accuracy based on the models trained in the Object dataset (<a href="#S4.T2" title="Table 2 â€£ 4.4.1 Object detection benchmark â€£ 4.4 Results â€£ 4 EXPERIMENTAL RESULTS â€£ Towards real-time object recognition and pose estimation in point clouds" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">TableÂ 2</span></a>). We also verify the entire datasetâ€™s processing time, looking at the frame processing rate in both classification and pose estimation scenarios.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">As the Scenes dataset does not provide ground-truth annotations concerning the objectsâ€™ pose, we had to find a plausible metric to evaluate the registration results. We adopted two different metrics: the Root mean squared error (RMSE) and an inlier ratio measurement. The latter represents the overlapping area between the source (model) and the target (scene). It is calculated based on the ratio between inlier correspondences and the number of points on the target. We also evaluate the correctness of predictions, both of object presence and pose. To do so, we follow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">Marcon etÂ al., 2019</a>]</cite> and employ the Intersection over Union metric (IoU), defined as:</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.1" class="ltx_Math" alttext="IoU=\frac{BB_{GT}\cap BB_{Est}}{BB_{GT}\cup BB_{Est}}" display="block"><semantics id="S4.E1.m1.1a"><mrow id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml"><mrow id="S4.E1.m1.1.1.2" xref="S4.E1.m1.1.1.2.cmml"><mi id="S4.E1.m1.1.1.2.2" xref="S4.E1.m1.1.1.2.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.2.1" xref="S4.E1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S4.E1.m1.1.1.2.3" xref="S4.E1.m1.1.1.2.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.2.1a" xref="S4.E1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S4.E1.m1.1.1.2.4" xref="S4.E1.m1.1.1.2.4.cmml">U</mi></mrow><mo id="S4.E1.m1.1.1.1" xref="S4.E1.m1.1.1.1.cmml">=</mo><mfrac id="S4.E1.m1.1.1.3" xref="S4.E1.m1.1.1.3.cmml"><mrow id="S4.E1.m1.1.1.3.2" xref="S4.E1.m1.1.1.3.2.cmml"><mrow id="S4.E1.m1.1.1.3.2.2" xref="S4.E1.m1.1.1.3.2.2.cmml"><mi id="S4.E1.m1.1.1.3.2.2.2" xref="S4.E1.m1.1.1.3.2.2.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.2.2.1" xref="S4.E1.m1.1.1.3.2.2.1.cmml">â€‹</mo><msub id="S4.E1.m1.1.1.3.2.2.3" xref="S4.E1.m1.1.1.3.2.2.3.cmml"><mi id="S4.E1.m1.1.1.3.2.2.3.2" xref="S4.E1.m1.1.1.3.2.2.3.2.cmml">B</mi><mrow id="S4.E1.m1.1.1.3.2.2.3.3" xref="S4.E1.m1.1.1.3.2.2.3.3.cmml"><mi id="S4.E1.m1.1.1.3.2.2.3.3.2" xref="S4.E1.m1.1.1.3.2.2.3.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.2.2.3.3.1" xref="S4.E1.m1.1.1.3.2.2.3.3.1.cmml">â€‹</mo><mi id="S4.E1.m1.1.1.3.2.2.3.3.3" xref="S4.E1.m1.1.1.3.2.2.3.3.3.cmml">T</mi></mrow></msub></mrow><mo id="S4.E1.m1.1.1.3.2.1" xref="S4.E1.m1.1.1.3.2.1.cmml">âˆ©</mo><mrow id="S4.E1.m1.1.1.3.2.3" xref="S4.E1.m1.1.1.3.2.3.cmml"><mi id="S4.E1.m1.1.1.3.2.3.2" xref="S4.E1.m1.1.1.3.2.3.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.2.3.1" xref="S4.E1.m1.1.1.3.2.3.1.cmml">â€‹</mo><msub id="S4.E1.m1.1.1.3.2.3.3" xref="S4.E1.m1.1.1.3.2.3.3.cmml"><mi id="S4.E1.m1.1.1.3.2.3.3.2" xref="S4.E1.m1.1.1.3.2.3.3.2.cmml">B</mi><mrow id="S4.E1.m1.1.1.3.2.3.3.3" xref="S4.E1.m1.1.1.3.2.3.3.3.cmml"><mi id="S4.E1.m1.1.1.3.2.3.3.3.2" xref="S4.E1.m1.1.1.3.2.3.3.3.2.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.2.3.3.3.1" xref="S4.E1.m1.1.1.3.2.3.3.3.1.cmml">â€‹</mo><mi id="S4.E1.m1.1.1.3.2.3.3.3.3" xref="S4.E1.m1.1.1.3.2.3.3.3.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.2.3.3.3.1a" xref="S4.E1.m1.1.1.3.2.3.3.3.1.cmml">â€‹</mo><mi id="S4.E1.m1.1.1.3.2.3.3.3.4" xref="S4.E1.m1.1.1.3.2.3.3.3.4.cmml">t</mi></mrow></msub></mrow></mrow><mrow id="S4.E1.m1.1.1.3.3" xref="S4.E1.m1.1.1.3.3.cmml"><mrow id="S4.E1.m1.1.1.3.3.2" xref="S4.E1.m1.1.1.3.3.2.cmml"><mi id="S4.E1.m1.1.1.3.3.2.2" xref="S4.E1.m1.1.1.3.3.2.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.3.2.1" xref="S4.E1.m1.1.1.3.3.2.1.cmml">â€‹</mo><msub id="S4.E1.m1.1.1.3.3.2.3" xref="S4.E1.m1.1.1.3.3.2.3.cmml"><mi id="S4.E1.m1.1.1.3.3.2.3.2" xref="S4.E1.m1.1.1.3.3.2.3.2.cmml">B</mi><mrow id="S4.E1.m1.1.1.3.3.2.3.3" xref="S4.E1.m1.1.1.3.3.2.3.3.cmml"><mi id="S4.E1.m1.1.1.3.3.2.3.3.2" xref="S4.E1.m1.1.1.3.3.2.3.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.3.2.3.3.1" xref="S4.E1.m1.1.1.3.3.2.3.3.1.cmml">â€‹</mo><mi id="S4.E1.m1.1.1.3.3.2.3.3.3" xref="S4.E1.m1.1.1.3.3.2.3.3.3.cmml">T</mi></mrow></msub></mrow><mo id="S4.E1.m1.1.1.3.3.1" xref="S4.E1.m1.1.1.3.3.1.cmml">âˆª</mo><mrow id="S4.E1.m1.1.1.3.3.3" xref="S4.E1.m1.1.1.3.3.3.cmml"><mi id="S4.E1.m1.1.1.3.3.3.2" xref="S4.E1.m1.1.1.3.3.3.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.3.3.1" xref="S4.E1.m1.1.1.3.3.3.1.cmml">â€‹</mo><msub id="S4.E1.m1.1.1.3.3.3.3" xref="S4.E1.m1.1.1.3.3.3.3.cmml"><mi id="S4.E1.m1.1.1.3.3.3.3.2" xref="S4.E1.m1.1.1.3.3.3.3.2.cmml">B</mi><mrow id="S4.E1.m1.1.1.3.3.3.3.3" xref="S4.E1.m1.1.1.3.3.3.3.3.cmml"><mi id="S4.E1.m1.1.1.3.3.3.3.3.2" xref="S4.E1.m1.1.1.3.3.3.3.3.2.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.3.3.3.3.1" xref="S4.E1.m1.1.1.3.3.3.3.3.1.cmml">â€‹</mo><mi id="S4.E1.m1.1.1.3.3.3.3.3.3" xref="S4.E1.m1.1.1.3.3.3.3.3.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.3.3.3.3.1a" xref="S4.E1.m1.1.1.3.3.3.3.3.1.cmml">â€‹</mo><mi id="S4.E1.m1.1.1.3.3.3.3.3.4" xref="S4.E1.m1.1.1.3.3.3.3.3.4.cmml">t</mi></mrow></msub></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.1b"><apply id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1"><eq id="S4.E1.m1.1.1.1.cmml" xref="S4.E1.m1.1.1.1"></eq><apply id="S4.E1.m1.1.1.2.cmml" xref="S4.E1.m1.1.1.2"><times id="S4.E1.m1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.2.1"></times><ci id="S4.E1.m1.1.1.2.2.cmml" xref="S4.E1.m1.1.1.2.2">ğ¼</ci><ci id="S4.E1.m1.1.1.2.3.cmml" xref="S4.E1.m1.1.1.2.3">ğ‘œ</ci><ci id="S4.E1.m1.1.1.2.4.cmml" xref="S4.E1.m1.1.1.2.4">ğ‘ˆ</ci></apply><apply id="S4.E1.m1.1.1.3.cmml" xref="S4.E1.m1.1.1.3"><divide id="S4.E1.m1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.3"></divide><apply id="S4.E1.m1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.3.2"><intersect id="S4.E1.m1.1.1.3.2.1.cmml" xref="S4.E1.m1.1.1.3.2.1"></intersect><apply id="S4.E1.m1.1.1.3.2.2.cmml" xref="S4.E1.m1.1.1.3.2.2"><times id="S4.E1.m1.1.1.3.2.2.1.cmml" xref="S4.E1.m1.1.1.3.2.2.1"></times><ci id="S4.E1.m1.1.1.3.2.2.2.cmml" xref="S4.E1.m1.1.1.3.2.2.2">ğµ</ci><apply id="S4.E1.m1.1.1.3.2.2.3.cmml" xref="S4.E1.m1.1.1.3.2.2.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.2.2.3.1.cmml" xref="S4.E1.m1.1.1.3.2.2.3">subscript</csymbol><ci id="S4.E1.m1.1.1.3.2.2.3.2.cmml" xref="S4.E1.m1.1.1.3.2.2.3.2">ğµ</ci><apply id="S4.E1.m1.1.1.3.2.2.3.3.cmml" xref="S4.E1.m1.1.1.3.2.2.3.3"><times id="S4.E1.m1.1.1.3.2.2.3.3.1.cmml" xref="S4.E1.m1.1.1.3.2.2.3.3.1"></times><ci id="S4.E1.m1.1.1.3.2.2.3.3.2.cmml" xref="S4.E1.m1.1.1.3.2.2.3.3.2">ğº</ci><ci id="S4.E1.m1.1.1.3.2.2.3.3.3.cmml" xref="S4.E1.m1.1.1.3.2.2.3.3.3">ğ‘‡</ci></apply></apply></apply><apply id="S4.E1.m1.1.1.3.2.3.cmml" xref="S4.E1.m1.1.1.3.2.3"><times id="S4.E1.m1.1.1.3.2.3.1.cmml" xref="S4.E1.m1.1.1.3.2.3.1"></times><ci id="S4.E1.m1.1.1.3.2.3.2.cmml" xref="S4.E1.m1.1.1.3.2.3.2">ğµ</ci><apply id="S4.E1.m1.1.1.3.2.3.3.cmml" xref="S4.E1.m1.1.1.3.2.3.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.2.3.3.1.cmml" xref="S4.E1.m1.1.1.3.2.3.3">subscript</csymbol><ci id="S4.E1.m1.1.1.3.2.3.3.2.cmml" xref="S4.E1.m1.1.1.3.2.3.3.2">ğµ</ci><apply id="S4.E1.m1.1.1.3.2.3.3.3.cmml" xref="S4.E1.m1.1.1.3.2.3.3.3"><times id="S4.E1.m1.1.1.3.2.3.3.3.1.cmml" xref="S4.E1.m1.1.1.3.2.3.3.3.1"></times><ci id="S4.E1.m1.1.1.3.2.3.3.3.2.cmml" xref="S4.E1.m1.1.1.3.2.3.3.3.2">ğ¸</ci><ci id="S4.E1.m1.1.1.3.2.3.3.3.3.cmml" xref="S4.E1.m1.1.1.3.2.3.3.3.3">ğ‘ </ci><ci id="S4.E1.m1.1.1.3.2.3.3.3.4.cmml" xref="S4.E1.m1.1.1.3.2.3.3.3.4">ğ‘¡</ci></apply></apply></apply></apply><apply id="S4.E1.m1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.3.3"><union id="S4.E1.m1.1.1.3.3.1.cmml" xref="S4.E1.m1.1.1.3.3.1"></union><apply id="S4.E1.m1.1.1.3.3.2.cmml" xref="S4.E1.m1.1.1.3.3.2"><times id="S4.E1.m1.1.1.3.3.2.1.cmml" xref="S4.E1.m1.1.1.3.3.2.1"></times><ci id="S4.E1.m1.1.1.3.3.2.2.cmml" xref="S4.E1.m1.1.1.3.3.2.2">ğµ</ci><apply id="S4.E1.m1.1.1.3.3.2.3.cmml" xref="S4.E1.m1.1.1.3.3.2.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.3.2.3.1.cmml" xref="S4.E1.m1.1.1.3.3.2.3">subscript</csymbol><ci id="S4.E1.m1.1.1.3.3.2.3.2.cmml" xref="S4.E1.m1.1.1.3.3.2.3.2">ğµ</ci><apply id="S4.E1.m1.1.1.3.3.2.3.3.cmml" xref="S4.E1.m1.1.1.3.3.2.3.3"><times id="S4.E1.m1.1.1.3.3.2.3.3.1.cmml" xref="S4.E1.m1.1.1.3.3.2.3.3.1"></times><ci id="S4.E1.m1.1.1.3.3.2.3.3.2.cmml" xref="S4.E1.m1.1.1.3.3.2.3.3.2">ğº</ci><ci id="S4.E1.m1.1.1.3.3.2.3.3.3.cmml" xref="S4.E1.m1.1.1.3.3.2.3.3.3">ğ‘‡</ci></apply></apply></apply><apply id="S4.E1.m1.1.1.3.3.3.cmml" xref="S4.E1.m1.1.1.3.3.3"><times id="S4.E1.m1.1.1.3.3.3.1.cmml" xref="S4.E1.m1.1.1.3.3.3.1"></times><ci id="S4.E1.m1.1.1.3.3.3.2.cmml" xref="S4.E1.m1.1.1.3.3.3.2">ğµ</ci><apply id="S4.E1.m1.1.1.3.3.3.3.cmml" xref="S4.E1.m1.1.1.3.3.3.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.3.3.3.1.cmml" xref="S4.E1.m1.1.1.3.3.3.3">subscript</csymbol><ci id="S4.E1.m1.1.1.3.3.3.3.2.cmml" xref="S4.E1.m1.1.1.3.3.3.3.2">ğµ</ci><apply id="S4.E1.m1.1.1.3.3.3.3.3.cmml" xref="S4.E1.m1.1.1.3.3.3.3.3"><times id="S4.E1.m1.1.1.3.3.3.3.3.1.cmml" xref="S4.E1.m1.1.1.3.3.3.3.3.1"></times><ci id="S4.E1.m1.1.1.3.3.3.3.3.2.cmml" xref="S4.E1.m1.1.1.3.3.3.3.3.2">ğ¸</ci><ci id="S4.E1.m1.1.1.3.3.3.3.3.3.cmml" xref="S4.E1.m1.1.1.3.3.3.3.3.3">ğ‘ </ci><ci id="S4.E1.m1.1.1.3.3.3.3.3.4.cmml" xref="S4.E1.m1.1.1.3.3.3.3.3.4">ğ‘¡</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.1c">IoU=\frac{BB_{GT}\cap BB_{Est}}{BB_{GT}\cup BB_{Est}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS2.p4" class="ltx_para ltx_noindent">
<p id="S4.SS2.p4.2" class="ltx_p">we consider <math id="S4.SS2.p4.1.m1.1" class="ltx_Math" alttext="BB_{GT}" display="inline"><semantics id="S4.SS2.p4.1.m1.1a"><mrow id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml"><mi id="S4.SS2.p4.1.m1.1.1.2" xref="S4.SS2.p4.1.m1.1.1.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p4.1.m1.1.1.1" xref="S4.SS2.p4.1.m1.1.1.1.cmml">â€‹</mo><msub id="S4.SS2.p4.1.m1.1.1.3" xref="S4.SS2.p4.1.m1.1.1.3.cmml"><mi id="S4.SS2.p4.1.m1.1.1.3.2" xref="S4.SS2.p4.1.m1.1.1.3.2.cmml">B</mi><mrow id="S4.SS2.p4.1.m1.1.1.3.3" xref="S4.SS2.p4.1.m1.1.1.3.3.cmml"><mi id="S4.SS2.p4.1.m1.1.1.3.3.2" xref="S4.SS2.p4.1.m1.1.1.3.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p4.1.m1.1.1.3.3.1" xref="S4.SS2.p4.1.m1.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.SS2.p4.1.m1.1.1.3.3.3" xref="S4.SS2.p4.1.m1.1.1.3.3.3.cmml">T</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><apply id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1"><times id="S4.SS2.p4.1.m1.1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1.1"></times><ci id="S4.SS2.p4.1.m1.1.1.2.cmml" xref="S4.SS2.p4.1.m1.1.1.2">ğµ</ci><apply id="S4.SS2.p4.1.m1.1.1.3.cmml" xref="S4.SS2.p4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p4.1.m1.1.1.3.1.cmml" xref="S4.SS2.p4.1.m1.1.1.3">subscript</csymbol><ci id="S4.SS2.p4.1.m1.1.1.3.2.cmml" xref="S4.SS2.p4.1.m1.1.1.3.2">ğµ</ci><apply id="S4.SS2.p4.1.m1.1.1.3.3.cmml" xref="S4.SS2.p4.1.m1.1.1.3.3"><times id="S4.SS2.p4.1.m1.1.1.3.3.1.cmml" xref="S4.SS2.p4.1.m1.1.1.3.3.1"></times><ci id="S4.SS2.p4.1.m1.1.1.3.3.2.cmml" xref="S4.SS2.p4.1.m1.1.1.3.3.2">ğº</ci><ci id="S4.SS2.p4.1.m1.1.1.3.3.3.cmml" xref="S4.SS2.p4.1.m1.1.1.3.3.3">ğ‘‡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">BB_{GT}</annotation></semantics></math> the 3D projection of the 2D bounding box, provided as ground-truth. <math id="S4.SS2.p4.2.m2.1" class="ltx_Math" alttext="BB_{Est}" display="inline"><semantics id="S4.SS2.p4.2.m2.1a"><mrow id="S4.SS2.p4.2.m2.1.1" xref="S4.SS2.p4.2.m2.1.1.cmml"><mi id="S4.SS2.p4.2.m2.1.1.2" xref="S4.SS2.p4.2.m2.1.1.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p4.2.m2.1.1.1" xref="S4.SS2.p4.2.m2.1.1.1.cmml">â€‹</mo><msub id="S4.SS2.p4.2.m2.1.1.3" xref="S4.SS2.p4.2.m2.1.1.3.cmml"><mi id="S4.SS2.p4.2.m2.1.1.3.2" xref="S4.SS2.p4.2.m2.1.1.3.2.cmml">B</mi><mrow id="S4.SS2.p4.2.m2.1.1.3.3" xref="S4.SS2.p4.2.m2.1.1.3.3.cmml"><mi id="S4.SS2.p4.2.m2.1.1.3.3.2" xref="S4.SS2.p4.2.m2.1.1.3.3.2.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p4.2.m2.1.1.3.3.1" xref="S4.SS2.p4.2.m2.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.SS2.p4.2.m2.1.1.3.3.3" xref="S4.SS2.p4.2.m2.1.1.3.3.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p4.2.m2.1.1.3.3.1a" xref="S4.SS2.p4.2.m2.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.SS2.p4.2.m2.1.1.3.3.4" xref="S4.SS2.p4.2.m2.1.1.3.3.4.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.2.m2.1b"><apply id="S4.SS2.p4.2.m2.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1"><times id="S4.SS2.p4.2.m2.1.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1.1"></times><ci id="S4.SS2.p4.2.m2.1.1.2.cmml" xref="S4.SS2.p4.2.m2.1.1.2">ğµ</ci><apply id="S4.SS2.p4.2.m2.1.1.3.cmml" xref="S4.SS2.p4.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p4.2.m2.1.1.3.1.cmml" xref="S4.SS2.p4.2.m2.1.1.3">subscript</csymbol><ci id="S4.SS2.p4.2.m2.1.1.3.2.cmml" xref="S4.SS2.p4.2.m2.1.1.3.2">ğµ</ci><apply id="S4.SS2.p4.2.m2.1.1.3.3.cmml" xref="S4.SS2.p4.2.m2.1.1.3.3"><times id="S4.SS2.p4.2.m2.1.1.3.3.1.cmml" xref="S4.SS2.p4.2.m2.1.1.3.3.1"></times><ci id="S4.SS2.p4.2.m2.1.1.3.3.2.cmml" xref="S4.SS2.p4.2.m2.1.1.3.3.2">ğ¸</ci><ci id="S4.SS2.p4.2.m2.1.1.3.3.3.cmml" xref="S4.SS2.p4.2.m2.1.1.3.3.3">ğ‘ </ci><ci id="S4.SS2.p4.2.m2.1.1.3.3.4.cmml" xref="S4.SS2.p4.2.m2.1.1.3.3.4">ğ‘¡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.2.m2.1c">BB_{Est}</annotation></semantics></math> refers to the 3D bounding box that circumscribes the selected object view after applying the resulting transformation.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">We found experimentally that, for this particular dataset, when we estimate the IoU between the object 3D BB and the scene 2D projection, often the former is fully contained in the latter. However, due to their sizes, the calculated IoU is too low. Hence, we consider another metric, which we call Model Intersection Ratio (MIR) that represent the intersection volume over the model estimation volume:</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<table id="S4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E2.m1.1" class="ltx_Math" alttext="MIR=\frac{BB_{GT}\cap BB_{Est}}{BB_{Est}}" display="block"><semantics id="S4.E2.m1.1a"><mrow id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml"><mrow id="S4.E2.m1.1.1.2" xref="S4.E2.m1.1.1.2.cmml"><mi id="S4.E2.m1.1.1.2.2" xref="S4.E2.m1.1.1.2.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.2.1" xref="S4.E2.m1.1.1.2.1.cmml">â€‹</mo><mi id="S4.E2.m1.1.1.2.3" xref="S4.E2.m1.1.1.2.3.cmml">I</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.2.1a" xref="S4.E2.m1.1.1.2.1.cmml">â€‹</mo><mi id="S4.E2.m1.1.1.2.4" xref="S4.E2.m1.1.1.2.4.cmml">R</mi></mrow><mo id="S4.E2.m1.1.1.1" xref="S4.E2.m1.1.1.1.cmml">=</mo><mfrac id="S4.E2.m1.1.1.3" xref="S4.E2.m1.1.1.3.cmml"><mrow id="S4.E2.m1.1.1.3.2" xref="S4.E2.m1.1.1.3.2.cmml"><mrow id="S4.E2.m1.1.1.3.2.2" xref="S4.E2.m1.1.1.3.2.2.cmml"><mi id="S4.E2.m1.1.1.3.2.2.2" xref="S4.E2.m1.1.1.3.2.2.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.2.2.1" xref="S4.E2.m1.1.1.3.2.2.1.cmml">â€‹</mo><msub id="S4.E2.m1.1.1.3.2.2.3" xref="S4.E2.m1.1.1.3.2.2.3.cmml"><mi id="S4.E2.m1.1.1.3.2.2.3.2" xref="S4.E2.m1.1.1.3.2.2.3.2.cmml">B</mi><mrow id="S4.E2.m1.1.1.3.2.2.3.3" xref="S4.E2.m1.1.1.3.2.2.3.3.cmml"><mi id="S4.E2.m1.1.1.3.2.2.3.3.2" xref="S4.E2.m1.1.1.3.2.2.3.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.2.2.3.3.1" xref="S4.E2.m1.1.1.3.2.2.3.3.1.cmml">â€‹</mo><mi id="S4.E2.m1.1.1.3.2.2.3.3.3" xref="S4.E2.m1.1.1.3.2.2.3.3.3.cmml">T</mi></mrow></msub></mrow><mo id="S4.E2.m1.1.1.3.2.1" xref="S4.E2.m1.1.1.3.2.1.cmml">âˆ©</mo><mrow id="S4.E2.m1.1.1.3.2.3" xref="S4.E2.m1.1.1.3.2.3.cmml"><mi id="S4.E2.m1.1.1.3.2.3.2" xref="S4.E2.m1.1.1.3.2.3.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.2.3.1" xref="S4.E2.m1.1.1.3.2.3.1.cmml">â€‹</mo><msub id="S4.E2.m1.1.1.3.2.3.3" xref="S4.E2.m1.1.1.3.2.3.3.cmml"><mi id="S4.E2.m1.1.1.3.2.3.3.2" xref="S4.E2.m1.1.1.3.2.3.3.2.cmml">B</mi><mrow id="S4.E2.m1.1.1.3.2.3.3.3" xref="S4.E2.m1.1.1.3.2.3.3.3.cmml"><mi id="S4.E2.m1.1.1.3.2.3.3.3.2" xref="S4.E2.m1.1.1.3.2.3.3.3.2.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.2.3.3.3.1" xref="S4.E2.m1.1.1.3.2.3.3.3.1.cmml">â€‹</mo><mi id="S4.E2.m1.1.1.3.2.3.3.3.3" xref="S4.E2.m1.1.1.3.2.3.3.3.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.2.3.3.3.1a" xref="S4.E2.m1.1.1.3.2.3.3.3.1.cmml">â€‹</mo><mi id="S4.E2.m1.1.1.3.2.3.3.3.4" xref="S4.E2.m1.1.1.3.2.3.3.3.4.cmml">t</mi></mrow></msub></mrow></mrow><mrow id="S4.E2.m1.1.1.3.3" xref="S4.E2.m1.1.1.3.3.cmml"><mi id="S4.E2.m1.1.1.3.3.2" xref="S4.E2.m1.1.1.3.3.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.1" xref="S4.E2.m1.1.1.3.3.1.cmml">â€‹</mo><msub id="S4.E2.m1.1.1.3.3.3" xref="S4.E2.m1.1.1.3.3.3.cmml"><mi id="S4.E2.m1.1.1.3.3.3.2" xref="S4.E2.m1.1.1.3.3.3.2.cmml">B</mi><mrow id="S4.E2.m1.1.1.3.3.3.3" xref="S4.E2.m1.1.1.3.3.3.3.cmml"><mi id="S4.E2.m1.1.1.3.3.3.3.2" xref="S4.E2.m1.1.1.3.3.3.3.2.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.3.3.1" xref="S4.E2.m1.1.1.3.3.3.3.1.cmml">â€‹</mo><mi id="S4.E2.m1.1.1.3.3.3.3.3" xref="S4.E2.m1.1.1.3.3.3.3.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.3.3.1a" xref="S4.E2.m1.1.1.3.3.3.3.1.cmml">â€‹</mo><mi id="S4.E2.m1.1.1.3.3.3.3.4" xref="S4.E2.m1.1.1.3.3.3.3.4.cmml">t</mi></mrow></msub></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1"><eq id="S4.E2.m1.1.1.1.cmml" xref="S4.E2.m1.1.1.1"></eq><apply id="S4.E2.m1.1.1.2.cmml" xref="S4.E2.m1.1.1.2"><times id="S4.E2.m1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.2.1"></times><ci id="S4.E2.m1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.2.2">ğ‘€</ci><ci id="S4.E2.m1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.2.3">ğ¼</ci><ci id="S4.E2.m1.1.1.2.4.cmml" xref="S4.E2.m1.1.1.2.4">ğ‘…</ci></apply><apply id="S4.E2.m1.1.1.3.cmml" xref="S4.E2.m1.1.1.3"><divide id="S4.E2.m1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.3"></divide><apply id="S4.E2.m1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.3.2"><intersect id="S4.E2.m1.1.1.3.2.1.cmml" xref="S4.E2.m1.1.1.3.2.1"></intersect><apply id="S4.E2.m1.1.1.3.2.2.cmml" xref="S4.E2.m1.1.1.3.2.2"><times id="S4.E2.m1.1.1.3.2.2.1.cmml" xref="S4.E2.m1.1.1.3.2.2.1"></times><ci id="S4.E2.m1.1.1.3.2.2.2.cmml" xref="S4.E2.m1.1.1.3.2.2.2">ğµ</ci><apply id="S4.E2.m1.1.1.3.2.2.3.cmml" xref="S4.E2.m1.1.1.3.2.2.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.3.2.2.3.1.cmml" xref="S4.E2.m1.1.1.3.2.2.3">subscript</csymbol><ci id="S4.E2.m1.1.1.3.2.2.3.2.cmml" xref="S4.E2.m1.1.1.3.2.2.3.2">ğµ</ci><apply id="S4.E2.m1.1.1.3.2.2.3.3.cmml" xref="S4.E2.m1.1.1.3.2.2.3.3"><times id="S4.E2.m1.1.1.3.2.2.3.3.1.cmml" xref="S4.E2.m1.1.1.3.2.2.3.3.1"></times><ci id="S4.E2.m1.1.1.3.2.2.3.3.2.cmml" xref="S4.E2.m1.1.1.3.2.2.3.3.2">ğº</ci><ci id="S4.E2.m1.1.1.3.2.2.3.3.3.cmml" xref="S4.E2.m1.1.1.3.2.2.3.3.3">ğ‘‡</ci></apply></apply></apply><apply id="S4.E2.m1.1.1.3.2.3.cmml" xref="S4.E2.m1.1.1.3.2.3"><times id="S4.E2.m1.1.1.3.2.3.1.cmml" xref="S4.E2.m1.1.1.3.2.3.1"></times><ci id="S4.E2.m1.1.1.3.2.3.2.cmml" xref="S4.E2.m1.1.1.3.2.3.2">ğµ</ci><apply id="S4.E2.m1.1.1.3.2.3.3.cmml" xref="S4.E2.m1.1.1.3.2.3.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.3.2.3.3.1.cmml" xref="S4.E2.m1.1.1.3.2.3.3">subscript</csymbol><ci id="S4.E2.m1.1.1.3.2.3.3.2.cmml" xref="S4.E2.m1.1.1.3.2.3.3.2">ğµ</ci><apply id="S4.E2.m1.1.1.3.2.3.3.3.cmml" xref="S4.E2.m1.1.1.3.2.3.3.3"><times id="S4.E2.m1.1.1.3.2.3.3.3.1.cmml" xref="S4.E2.m1.1.1.3.2.3.3.3.1"></times><ci id="S4.E2.m1.1.1.3.2.3.3.3.2.cmml" xref="S4.E2.m1.1.1.3.2.3.3.3.2">ğ¸</ci><ci id="S4.E2.m1.1.1.3.2.3.3.3.3.cmml" xref="S4.E2.m1.1.1.3.2.3.3.3.3">ğ‘ </ci><ci id="S4.E2.m1.1.1.3.2.3.3.3.4.cmml" xref="S4.E2.m1.1.1.3.2.3.3.3.4">ğ‘¡</ci></apply></apply></apply></apply><apply id="S4.E2.m1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.3.3"><times id="S4.E2.m1.1.1.3.3.1.cmml" xref="S4.E2.m1.1.1.3.3.1"></times><ci id="S4.E2.m1.1.1.3.3.2.cmml" xref="S4.E2.m1.1.1.3.3.2">ğµ</ci><apply id="S4.E2.m1.1.1.3.3.3.cmml" xref="S4.E2.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.3.3.3.1.cmml" xref="S4.E2.m1.1.1.3.3.3">subscript</csymbol><ci id="S4.E2.m1.1.1.3.3.3.2.cmml" xref="S4.E2.m1.1.1.3.3.3.2">ğµ</ci><apply id="S4.E2.m1.1.1.3.3.3.3.cmml" xref="S4.E2.m1.1.1.3.3.3.3"><times id="S4.E2.m1.1.1.3.3.3.3.1.cmml" xref="S4.E2.m1.1.1.3.3.3.3.1"></times><ci id="S4.E2.m1.1.1.3.3.3.3.2.cmml" xref="S4.E2.m1.1.1.3.3.3.3.2">ğ¸</ci><ci id="S4.E2.m1.1.1.3.3.3.3.3.cmml" xref="S4.E2.m1.1.1.3.3.3.3.3">ğ‘ </ci><ci id="S4.E2.m1.1.1.3.3.3.3.4.cmml" xref="S4.E2.m1.1.1.3.3.3.3.4">ğ‘¡</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">MIR=\frac{BB_{GT}\cap BB_{Est}}{BB_{Est}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS2.p7" class="ltx_para">
<p id="S4.SS2.p7.2" class="ltx_p">With the MIR metric, we guarantee that despite the IoU, when the estimation transform places the object inside (or nearly inside) the ground-truth 3D projection, a successful detection is performed. We consider a true positive when the <math id="S4.SS2.p7.1.m1.1" class="ltx_Math" alttext="IoU\geq 0.25" display="inline"><semantics id="S4.SS2.p7.1.m1.1a"><mrow id="S4.SS2.p7.1.m1.1.1" xref="S4.SS2.p7.1.m1.1.1.cmml"><mrow id="S4.SS2.p7.1.m1.1.1.2" xref="S4.SS2.p7.1.m1.1.1.2.cmml"><mi id="S4.SS2.p7.1.m1.1.1.2.2" xref="S4.SS2.p7.1.m1.1.1.2.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p7.1.m1.1.1.2.1" xref="S4.SS2.p7.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S4.SS2.p7.1.m1.1.1.2.3" xref="S4.SS2.p7.1.m1.1.1.2.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p7.1.m1.1.1.2.1a" xref="S4.SS2.p7.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S4.SS2.p7.1.m1.1.1.2.4" xref="S4.SS2.p7.1.m1.1.1.2.4.cmml">U</mi></mrow><mo id="S4.SS2.p7.1.m1.1.1.1" xref="S4.SS2.p7.1.m1.1.1.1.cmml">â‰¥</mo><mn id="S4.SS2.p7.1.m1.1.1.3" xref="S4.SS2.p7.1.m1.1.1.3.cmml">0.25</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p7.1.m1.1b"><apply id="S4.SS2.p7.1.m1.1.1.cmml" xref="S4.SS2.p7.1.m1.1.1"><geq id="S4.SS2.p7.1.m1.1.1.1.cmml" xref="S4.SS2.p7.1.m1.1.1.1"></geq><apply id="S4.SS2.p7.1.m1.1.1.2.cmml" xref="S4.SS2.p7.1.m1.1.1.2"><times id="S4.SS2.p7.1.m1.1.1.2.1.cmml" xref="S4.SS2.p7.1.m1.1.1.2.1"></times><ci id="S4.SS2.p7.1.m1.1.1.2.2.cmml" xref="S4.SS2.p7.1.m1.1.1.2.2">ğ¼</ci><ci id="S4.SS2.p7.1.m1.1.1.2.3.cmml" xref="S4.SS2.p7.1.m1.1.1.2.3">ğ‘œ</ci><ci id="S4.SS2.p7.1.m1.1.1.2.4.cmml" xref="S4.SS2.p7.1.m1.1.1.2.4">ğ‘ˆ</ci></apply><cn type="float" id="S4.SS2.p7.1.m1.1.1.3.cmml" xref="S4.SS2.p7.1.m1.1.1.3">0.25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p7.1.m1.1c">IoU\geq 0.25</annotation></semantics></math> or the <math id="S4.SS2.p7.2.m2.1" class="ltx_Math" alttext="MIR\geq 0.90" display="inline"><semantics id="S4.SS2.p7.2.m2.1a"><mrow id="S4.SS2.p7.2.m2.1.1" xref="S4.SS2.p7.2.m2.1.1.cmml"><mrow id="S4.SS2.p7.2.m2.1.1.2" xref="S4.SS2.p7.2.m2.1.1.2.cmml"><mi id="S4.SS2.p7.2.m2.1.1.2.2" xref="S4.SS2.p7.2.m2.1.1.2.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p7.2.m2.1.1.2.1" xref="S4.SS2.p7.2.m2.1.1.2.1.cmml">â€‹</mo><mi id="S4.SS2.p7.2.m2.1.1.2.3" xref="S4.SS2.p7.2.m2.1.1.2.3.cmml">I</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p7.2.m2.1.1.2.1a" xref="S4.SS2.p7.2.m2.1.1.2.1.cmml">â€‹</mo><mi id="S4.SS2.p7.2.m2.1.1.2.4" xref="S4.SS2.p7.2.m2.1.1.2.4.cmml">R</mi></mrow><mo id="S4.SS2.p7.2.m2.1.1.1" xref="S4.SS2.p7.2.m2.1.1.1.cmml">â‰¥</mo><mn id="S4.SS2.p7.2.m2.1.1.3" xref="S4.SS2.p7.2.m2.1.1.3.cmml">0.90</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p7.2.m2.1b"><apply id="S4.SS2.p7.2.m2.1.1.cmml" xref="S4.SS2.p7.2.m2.1.1"><geq id="S4.SS2.p7.2.m2.1.1.1.cmml" xref="S4.SS2.p7.2.m2.1.1.1"></geq><apply id="S4.SS2.p7.2.m2.1.1.2.cmml" xref="S4.SS2.p7.2.m2.1.1.2"><times id="S4.SS2.p7.2.m2.1.1.2.1.cmml" xref="S4.SS2.p7.2.m2.1.1.2.1"></times><ci id="S4.SS2.p7.2.m2.1.1.2.2.cmml" xref="S4.SS2.p7.2.m2.1.1.2.2">ğ‘€</ci><ci id="S4.SS2.p7.2.m2.1.1.2.3.cmml" xref="S4.SS2.p7.2.m2.1.1.2.3">ğ¼</ci><ci id="S4.SS2.p7.2.m2.1.1.2.4.cmml" xref="S4.SS2.p7.2.m2.1.1.2.4">ğ‘…</ci></apply><cn type="float" id="S4.SS2.p7.2.m2.1.1.3.cmml" xref="S4.SS2.p7.2.m2.1.1.3">0.90</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p7.2.m2.1c">MIR\geq 0.90</annotation></semantics></math>.</p>
</div>
<div id="S4.SS2.p8" class="ltx_para">
<p id="S4.SS2.p8.1" class="ltx_p">We compared our proposal with the standard 3D object recognition and pose estimation pipeline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">Aldoma etÂ al., 2012b</a>]</cite>, and with a boosted version of such pipeline, proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">Marcon etÂ al., 2019</a>]</cite>. To calculate precision-recall curves (PRC), we varied the threshold on the minimum geometrically consistent correspondences, starting from at least three, related to each objectâ€™s best-suited partial view. The area under the PRC curve (AUC) is then calculated and provides comparative results that assess our proposalsâ€™ efficiency against traditional approaches.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Implementation details</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We performed our tests on a Linux Ubuntu 18.04 LTS machine, equipped with a CPU Ryzen 7 2700X, 32GB of RAM, and a GPU Geforce RTX 2070 Super. To process the point clouds, perform keypoint extraction, description with FPFH, and registration with RANSAC and FGR, we used the Open3D Library. We preprocess images using Pillow and OpenCV. Deep learning models were implemented in PyTorch, and the pre-trained models extracted from torchvision. To implement traditional and boosted versions of object recognition and pose estimation pipelines, we use PCL 1.8.1, OpenCV 3.4.2, and the saliency detection of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">Hou etÂ al., 2017</a>]</cite>, following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">Marcon etÂ al., 2019</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Results</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">This section summarizes the Washington RGB-D Scenesâ€™ experimental evaluation results in terms of accuracy and processing time.</p>
</div>
<section id="S4.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1 </span>Object detection benchmark</h4>

<div id="S4.SS4.SSS1.p1" class="ltx_para">
<p id="S4.SS4.SSS1.p1.1" class="ltx_p">To assess the generalization capacity of CNN pre-trained models, we perform an object detection evaluation on the Object dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">Lai etÂ al., 2011a</a>]</cite>. <a href="#S4.T2" title="Table 2 â€£ 4.4.1 Object detection benchmark â€£ 4.4 Results â€£ 4 EXPERIMENTAL RESULTS â€£ Towards real-time object recognition and pose estimation in point clouds" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">TableÂ 2</span></a> present results regarding classification of partial views of objects. We evaluate the instance recognition scenario, following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">Lai etÂ al., 2011a</a>]</cite>, i.e., considering Alternating contiguous frame (ACF) and Leave-sequence-out (LSO) scenarios. We compared our results with state-of-the-art object detection methods on this dataset. We perceived that pre-trained networks provide reliable results as off-the-shelf color feature extractors. In both evaluation approaches, tested networks present competitive results concerning the other competitors. In LSO, ResNet101 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">He etÂ al., 2016</a>]</cite> features figures in the third position, and in ACF, 5 of 7 architectures outperform previous proposals.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison of CNN color features on the Washington RGB-D Object dataset. The best result reported in <span id="S4.T2.15.1" class="ltx_text" style="color:#0000FF;">blue</span>, the second best in <span id="S4.T2.16.2" class="ltx_text" style="color:#218C21;">green</span>, and the third in <span id="S4.T2.17.3" class="ltx_text" style="color:#FF0000;">red</span>. </figcaption>
<div id="S4.T2.11" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:215.8pt;height:229.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-27.7pt,29.4pt) scale(0.795988647859936,0.795988647859936) ;">
<table id="S4.T2.11.11" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.11.11.12.1" class="ltx_tr">
<th id="S4.T2.11.11.12.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" style="padding:1.5pt 2.0pt;">Method</th>
<th id="S4.T2.11.11.12.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:1.5pt 2.0pt;">LSO</th>
<th id="S4.T2.11.11.12.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:1.5pt 2.0pt;">ACF</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:1.5pt 2.0pt;">Lai <span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_italic">et al.</span> (RF) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">Lai etÂ al., 2011a</a>]</cite>
</th>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 2.0pt;">59.9</td>
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 2.0pt;">90.1 <math id="S4.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.1.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">\pm</annotation></semantics></math> 0.8</td>
</tr>
<tr id="S4.T2.2.2.2" class="ltx_tr">
<th id="S4.T2.2.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.5pt 2.0pt;">Lai <span id="S4.T2.2.2.2.2.1" class="ltx_text ltx_font_italic">et al.</span> (kSVC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">Lai etÂ al., 2011a</a>]</cite>
</th>
<td id="S4.T2.2.2.2.3" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">60.7</td>
<td id="S4.T2.2.2.2.1" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">91.0 <math id="S4.T2.2.2.2.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.2.2.2.1.m1.1a"><mo id="S4.T2.2.2.2.1.m1.1.1" xref="S4.T2.2.2.2.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.1.m1.1b"><csymbol cd="latexml" id="S4.T2.2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.1.m1.1c">\pm</annotation></semantics></math> 0.5</td>
</tr>
<tr id="S4.T2.3.3.3" class="ltx_tr">
<th id="S4.T2.3.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.5pt 2.0pt;">IDL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx18" title="" class="ltx_ref">Lai etÂ al., 2011b</a>]</cite>
</th>
<td id="S4.T2.3.3.3.3" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">-</td>
<td id="S4.T2.3.3.3.1" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">54.8 <math id="S4.T2.3.3.3.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.3.3.3.1.m1.1a"><mo id="S4.T2.3.3.3.1.m1.1.1" xref="S4.T2.3.3.3.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.1.m1.1b"><csymbol cd="latexml" id="S4.T2.3.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.1.m1.1c">\pm</annotation></semantics></math> 0.6</td>
</tr>
<tr id="S4.T2.11.11.13.1" class="ltx_tr">
<th id="S4.T2.11.11.13.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.5pt 2.0pt;">SP+HMP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">Bo etÂ al., 2013</a>]</cite>
</th>
<td id="S4.T2.11.11.13.1.2" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">92.1</td>
<td id="S4.T2.11.11.13.1.3" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">-</td>
</tr>
<tr id="S4.T2.11.11.14.2" class="ltx_tr">
<th id="S4.T2.11.11.14.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.5pt 2.0pt;">Multi-Modal <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx29" title="" class="ltx_ref">Schwarz etÂ al., 2015</a>]</cite>
</th>
<td id="S4.T2.11.11.14.2.2" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">92.0</td>
<td id="S4.T2.11.11.14.2.3" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">-</td>
</tr>
<tr id="S4.T2.11.11.15.3" class="ltx_tr">
<th id="S4.T2.11.11.15.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.5pt 2.0pt;">MDSI-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">Asif etÂ al., 2017</a>]</cite>
</th>
<td id="S4.T2.11.11.15.3.2" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;"><span id="S4.T2.11.11.15.3.2.1" class="ltx_text" style="color:#0000FF;">97.7</span></td>
<td id="S4.T2.11.11.15.3.3" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">-</td>
</tr>
<tr id="S4.T2.11.11.16.4" class="ltx_tr">
<th id="S4.T2.11.11.16.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.5pt 2.0pt;">MM-LRF-ELM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">Liu etÂ al., 2018</a>]</cite>
</th>
<td id="S4.T2.11.11.16.4.2" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">91.0</td>
<td id="S4.T2.11.11.16.4.3" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">-</td>
</tr>
<tr id="S4.T2.11.11.17.5" class="ltx_tr">
<th id="S4.T2.11.11.17.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.5pt 2.0pt;">HP-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx37" title="" class="ltx_ref">Zaki etÂ al., 2019</a>]</cite>
</th>
<td id="S4.T2.11.11.17.5.2" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;"><span id="S4.T2.11.11.17.5.2.1" class="ltx_text" style="color:#218C21;">95.5</span></td>
<td id="S4.T2.11.11.17.5.3" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">-</td>
</tr>
<tr id="S4.T2.4.4.4" class="ltx_tr">
<th id="S4.T2.4.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:1.5pt 2.0pt;">AlexNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">Krizhevsky etÂ al., 2012</a>]</cite>
</th>
<td id="S4.T2.4.4.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 2.0pt;">89.8</td>
<td id="S4.T2.4.4.4.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 2.0pt;">93.9 <math id="S4.T2.4.4.4.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.4.4.4.1.m1.1a"><mo id="S4.T2.4.4.4.1.m1.1.1" xref="S4.T2.4.4.4.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.1.m1.1b"><csymbol cd="latexml" id="S4.T2.4.4.4.1.m1.1.1.cmml" xref="S4.T2.4.4.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.1.m1.1c">\pm</annotation></semantics></math> 0.4</td>
</tr>
<tr id="S4.T2.5.5.5" class="ltx_tr">
<th id="S4.T2.5.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.5pt 2.0pt;">ResNet101<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">He etÂ al., 2016</a>]</cite>
</th>
<td id="S4.T2.5.5.5.3" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;"><span id="S4.T2.5.5.5.3.1" class="ltx_text" style="color:#FF0000;">94.1</span></td>
<td id="S4.T2.5.5.5.1" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">95.3 <math id="S4.T2.5.5.5.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.5.5.5.1.m1.1a"><mo id="S4.T2.5.5.5.1.m1.1.1" xref="S4.T2.5.5.5.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.1.m1.1b"><csymbol cd="latexml" id="S4.T2.5.5.5.1.m1.1.1.cmml" xref="S4.T2.5.5.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.1.m1.1c">\pm</annotation></semantics></math> 0.3</td>
</tr>
<tr id="S4.T2.6.6.6" class="ltx_tr">
<th id="S4.T2.6.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.5pt 2.0pt;">VGG16 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx30" title="" class="ltx_ref">Simonyan and Zisserman, 2015</a>]</cite>
</th>
<td id="S4.T2.6.6.6.3" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">88.8</td>
<td id="S4.T2.6.6.6.1" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">91.0 <math id="S4.T2.6.6.6.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.6.6.6.1.m1.1a"><mo id="S4.T2.6.6.6.1.m1.1.1" xref="S4.T2.6.6.6.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.6.1.m1.1b"><csymbol cd="latexml" id="S4.T2.6.6.6.1.m1.1.1.cmml" xref="S4.T2.6.6.6.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.6.1.m1.1c">\pm</annotation></semantics></math> 0.6</td>
</tr>
<tr id="S4.T2.7.7.7" class="ltx_tr">
<th id="S4.T2.7.7.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.5pt 2.0pt;">Inception v3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx32" title="" class="ltx_ref">Szegedy etÂ al., 2016</a>]</cite>
</th>
<td id="S4.T2.7.7.7.3" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">88.1</td>
<td id="S4.T2.7.7.7.1" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">90.3 <math id="S4.T2.7.7.7.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.7.7.7.1.m1.1a"><mo id="S4.T2.7.7.7.1.m1.1.1" xref="S4.T2.7.7.7.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.7.1.m1.1b"><csymbol cd="latexml" id="S4.T2.7.7.7.1.m1.1.1.cmml" xref="S4.T2.7.7.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.7.1.m1.1c">\pm</annotation></semantics></math> 0.4</td>
</tr>
<tr id="S4.T2.8.8.8" class="ltx_tr">
<th id="S4.T2.8.8.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.5pt 2.0pt;">MobileNet v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx28" title="" class="ltx_ref">Sandler etÂ al., 2018</a>]</cite>
</th>
<td id="S4.T2.8.8.8.3" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">93.8</td>
<td id="S4.T2.8.8.8.1" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;"><span id="S4.T2.8.8.8.1.1" class="ltx_text" style="color:#0000FF;">95.8 <math id="S4.T2.8.8.8.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.8.8.8.1.1.m1.1a"><mo mathcolor="#0000FF" id="S4.T2.8.8.8.1.1.m1.1.1" xref="S4.T2.8.8.8.1.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.8.8.8.1.1.m1.1b"><csymbol cd="latexml" id="S4.T2.8.8.8.1.1.m1.1.1.cmml" xref="S4.T2.8.8.8.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.8.8.1.1.m1.1c">\pm</annotation></semantics></math> 0.3</span></td>
</tr>
<tr id="S4.T2.10.10.10" class="ltx_tr">
<th id="S4.T2.9.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1.5pt 2.0pt;">ResNeXt101 <math id="S4.T2.9.9.9.1.m1.1" class="ltx_Math" alttext="32\times 8" display="inline"><semantics id="S4.T2.9.9.9.1.m1.1a"><mrow id="S4.T2.9.9.9.1.m1.1.1" xref="S4.T2.9.9.9.1.m1.1.1.cmml"><mn id="S4.T2.9.9.9.1.m1.1.1.2" xref="S4.T2.9.9.9.1.m1.1.1.2.cmml">32</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T2.9.9.9.1.m1.1.1.1" xref="S4.T2.9.9.9.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.T2.9.9.9.1.m1.1.1.3" xref="S4.T2.9.9.9.1.m1.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.9.9.9.1.m1.1b"><apply id="S4.T2.9.9.9.1.m1.1.1.cmml" xref="S4.T2.9.9.9.1.m1.1.1"><times id="S4.T2.9.9.9.1.m1.1.1.1.cmml" xref="S4.T2.9.9.9.1.m1.1.1.1"></times><cn type="integer" id="S4.T2.9.9.9.1.m1.1.1.2.cmml" xref="S4.T2.9.9.9.1.m1.1.1.2">32</cn><cn type="integer" id="S4.T2.9.9.9.1.m1.1.1.3.cmml" xref="S4.T2.9.9.9.1.m1.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.9.9.1.m1.1c">32\times 8</annotation></semantics></math>d <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx36" title="" class="ltx_ref">Xie etÂ al., 2017</a>]</cite>
</th>
<td id="S4.T2.10.10.10.3" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;">93.9</td>
<td id="S4.T2.10.10.10.2" class="ltx_td ltx_align_center" style="padding:1.5pt 2.0pt;"><span id="S4.T2.10.10.10.2.1" class="ltx_text" style="color:#218C21;">95.7 <math id="S4.T2.10.10.10.2.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.10.10.10.2.1.m1.1a"><mo mathcolor="#218C21" id="S4.T2.10.10.10.2.1.m1.1.1" xref="S4.T2.10.10.10.2.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.10.10.10.2.1.m1.1b"><csymbol cd="latexml" id="S4.T2.10.10.10.2.1.m1.1.1.cmml" xref="S4.T2.10.10.10.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.10.10.2.1.m1.1c">\pm</annotation></semantics></math> 0.4</span></td>
</tr>
<tr id="S4.T2.11.11.11" class="ltx_tr">
<th id="S4.T2.11.11.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" style="padding:1.5pt 2.0pt;">EfficientNet B7 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx33" title="" class="ltx_ref">Tan and Le, 2019</a>]</cite>
</th>
<td id="S4.T2.11.11.11.3" class="ltx_td ltx_align_center ltx_border_b" style="padding:1.5pt 2.0pt;">93.8</td>
<td id="S4.T2.11.11.11.1" class="ltx_td ltx_align_center ltx_border_b" style="padding:1.5pt 2.0pt;"><span id="S4.T2.11.11.11.1.1" class="ltx_text" style="color:#FF0000;">95.6 <math id="S4.T2.11.11.11.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.11.11.11.1.1.m1.1a"><mo mathcolor="#FF0000" id="S4.T2.11.11.11.1.1.m1.1.1" xref="S4.T2.11.11.11.1.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.11.11.11.1.1.m1.1b"><csymbol cd="latexml" id="S4.T2.11.11.11.1.1.m1.1.1.cmml" xref="S4.T2.11.11.11.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.11.11.1.1.m1.1c">\pm</annotation></semantics></math> 0.5</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Instance classification performance on the RGB-D Scenes datasets. </figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="2">MobileNet v2</th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="2">Resnet101</th>
<th id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="2">ResNeXt101 32x8d</th>
<th id="S4.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="2">EfficientNet-B7</th>
</tr>
<tr id="S4.T3.1.2.2" class="ltx_tr">
<th id="S4.T3.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Scene</th>
<th id="S4.T3.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Acc</th>
<th id="S4.T3.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">FPS</th>
<th id="S4.T3.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Acc</th>
<th id="S4.T3.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">FPS</th>
<th id="S4.T3.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Acc</th>
<th id="S4.T3.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">FPS</th>
<th id="S4.T3.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Acc</th>
<th id="S4.T3.1.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">FPS</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.3.1" class="ltx_tr">
<th id="S4.T3.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">desk_1</th>
<td id="S4.T3.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">42.70%</td>
<td id="S4.T3.1.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">13.03</td>
<td id="S4.T3.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">51.89%</td>
<td id="S4.T3.1.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">9.66</td>
<td id="S4.T3.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">48.11%</td>
<td id="S4.T3.1.3.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">7.63</td>
<td id="S4.T3.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">49.73%</td>
<td id="S4.T3.1.3.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">6.55</td>
</tr>
<tr id="S4.T3.1.4.2" class="ltx_tr">
<th id="S4.T3.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">desk_2</th>
<td id="S4.T3.1.4.2.2" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">41.76%</td>
<td id="S4.T3.1.4.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">12.95</td>
<td id="S4.T3.1.4.2.4" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">38.92%</td>
<td id="S4.T3.1.4.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">9.31</td>
<td id="S4.T3.1.4.2.6" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">55.40%</td>
<td id="S4.T3.1.4.2.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">7.93</td>
<td id="S4.T3.1.4.2.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">76.42%</td>
<td id="S4.T3.1.4.2.9" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">6.35</td>
</tr>
<tr id="S4.T3.1.5.3" class="ltx_tr">
<th id="S4.T3.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">desk_3</th>
<td id="S4.T3.1.5.3.2" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">72.77%</td>
<td id="S4.T3.1.5.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">9.84</td>
<td id="S4.T3.1.5.3.4" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">52.57%</td>
<td id="S4.T3.1.5.3.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">7.09</td>
<td id="S4.T3.1.5.3.6" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">52.91%</td>
<td id="S4.T3.1.5.3.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">5.78</td>
<td id="S4.T3.1.5.3.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">90.58%</td>
<td id="S4.T3.1.5.3.9" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">4.60</td>
</tr>
<tr id="S4.T3.1.6.4" class="ltx_tr">
<th id="S4.T3.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">kitchen_small_1</th>
<td id="S4.T3.1.6.4.2" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">36.31%</td>
<td id="S4.T3.1.6.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">7.97</td>
<td id="S4.T3.1.6.4.4" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">34.74%</td>
<td id="S4.T3.1.6.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">5.29</td>
<td id="S4.T3.1.6.4.6" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">48.20%</td>
<td id="S4.T3.1.6.4.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">4.12</td>
<td id="S4.T3.1.6.4.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">56.81%</td>
<td id="S4.T3.1.6.4.9" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">3.25</td>
</tr>
<tr id="S4.T3.1.7.5" class="ltx_tr">
<th id="S4.T3.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">meeting_small_1</th>
<td id="S4.T3.1.7.5.2" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">41.40%</td>
<td id="S4.T3.1.7.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">3.29</td>
<td id="S4.T3.1.7.5.4" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">38.05%</td>
<td id="S4.T3.1.7.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">2.35</td>
<td id="S4.T3.1.7.5.6" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">42.92%</td>
<td id="S4.T3.1.7.5.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">1.74</td>
<td id="S4.T3.1.7.5.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">50.63%</td>
<td id="S4.T3.1.7.5.9" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">1.33</td>
</tr>
<tr id="S4.T3.1.8.6" class="ltx_tr">
<th id="S4.T3.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">table_1</th>
<td id="S4.T3.1.8.6.2" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">56.76%</td>
<td id="S4.T3.1.8.6.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">4.62</td>
<td id="S4.T3.1.8.6.4" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">38.11%</td>
<td id="S4.T3.1.8.6.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">3.43</td>
<td id="S4.T3.1.8.6.6" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">31.08%</td>
<td id="S4.T3.1.8.6.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">2.49</td>
<td id="S4.T3.1.8.6.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">61.49%</td>
<td id="S4.T3.1.8.6.9" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">2.00</td>
</tr>
<tr id="S4.T3.1.9.7" class="ltx_tr">
<th id="S4.T3.1.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">table_small_1</th>
<td id="S4.T3.1.9.7.2" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">75.03%</td>
<td id="S4.T3.1.9.7.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">7.50</td>
<td id="S4.T3.1.9.7.4" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">63.30%</td>
<td id="S4.T3.1.9.7.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">5.33</td>
<td id="S4.T3.1.9.7.6" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">65.35%</td>
<td id="S4.T3.1.9.7.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">3.89</td>
<td id="S4.T3.1.9.7.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">83.36%</td>
<td id="S4.T3.1.9.7.9" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">3.16</td>
</tr>
<tr id="S4.T3.1.10.8" class="ltx_tr">
<th id="S4.T3.1.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">table_small_2</th>
<td id="S4.T3.1.10.8.2" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">55.39%</td>
<td id="S4.T3.1.10.8.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">9.13</td>
<td id="S4.T3.1.10.8.4" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">45.35%</td>
<td id="S4.T3.1.10.8.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">6.88</td>
<td id="S4.T3.1.10.8.6" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">49.34%</td>
<td id="S4.T3.1.10.8.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">5.04</td>
<td id="S4.T3.1.10.8.8" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">65.88%</td>
<td id="S4.T3.1.10.8.9" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">4.10</td>
</tr>
<tr id="S4.T3.1.11.9" class="ltx_tr">
<th id="S4.T3.1.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Average</th>
<td id="S4.T3.1.11.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">52.77%</td>
<td id="S4.T3.1.11.9.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">6.99</td>
<td id="S4.T3.1.11.9.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">45.37%</td>
<td id="S4.T3.1.11.9.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">5.03</td>
<td id="S4.T3.1.11.9.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">49.16%</td>
<td id="S4.T3.1.11.9.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">3.80</td>
<td id="S4.T3.1.11.9.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">66.86%</td>
<td id="S4.T3.1.11.9.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">3.02</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS4.SSS1.p2" class="ltx_para">
<p id="S4.SS4.SSS1.p2.1" class="ltx_p">Despite the significant results, this evaluation is essential to select the most suitable to perform object recognition in realistic scenarios, such as those presented by the Scenes dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">Lai etÂ al., 2011a</a>]</cite>. As the trialsâ€™ output, we selected the top-four architectures to apply in our proposed pipeline.</p>
</div>
</section>
<section id="S4.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2 </span>Object recognition in real-world scenes</h4>

<div id="S4.SS4.SSS2.p1" class="ltx_para">
<p id="S4.SS4.SSS2.p1.1" class="ltx_p">We opposed the selected CNN architectures examining only a classification based on the RGB information, taking the annotated bounding box, and submitting to the <span id="S4.SS4.SSS2.p1.1.1" class="ltx_text ltx_font_italic">Color Feature Classification</span> stage of our pipeline (as in Section <a href="#S3.SS1" title="3.1 Color feature classification â€£ 3 PROPOSED APPROACH â€£ Towards real-time object recognition and pose estimation in point clouds" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>). <a href="#S4.T3" title="Table 3 â€£ 4.4.1 Object detection benchmark â€£ 4.4 Results â€£ 4 EXPERIMENTAL RESULTS â€£ Towards real-time object recognition and pose estimation in point clouds" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">TableÂ 3</span></a> relates to instance-level recognition.</p>
</div>
<div id="S4.SS4.SSS2.p2" class="ltx_para">
<p id="S4.SS4.SSS2.p2.1" class="ltx_p">The first outcome of this evaluation is the dominance of two networks over the other competitors considering different aspects. EfficientNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx33" title="" class="ltx_ref">Tan and Le, 2019</a>]</cite> architecture outperforms in terms of accuracy, and MobileNet v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx28" title="" class="ltx_ref">Sandler etÂ al., 2018</a>]</cite> in terms of processing time w.r.t. the others in almost all scenes.</p>
</div>
<div id="S4.SS4.SSS2.p3" class="ltx_para">
<p id="S4.SS4.SSS2.p3.1" class="ltx_p">EfficientNet reaches an average accuracy of almost 67%, followed by MobileNet v2, with almost 53%. However, when we aim efficiency in processing time, EfficientNet does not perform so well, being the slowest network with a frame-rate of 3.02 per second. On the other hand, the MobileNet v2 fulfills the networkâ€™s main proposal to be time-efficient and accurate for embedded applications. It presents the second-best accuracy and the best frame-rate, with almost 7 FPS.</p>
</div>
<div id="S4.SS4.SSS2.p4" class="ltx_para">
<p id="S4.SS4.SSS2.p4.1" class="ltx_p">The full-set of the Object dataset contains 51 categories and 300 distinct instances. Concerning the Scenes dataset, the number of annotated samples drops to 6 categories and 22 instances, i.e., only a small set of objects of Object dataset is achievable on the Scenes dataset. When we use a model trained on the full-set, most categories or instances will never be detected. Thou, we learned a lighter classifier that considers only such specific instances (<a href="#S4.T4" title="Table 4 â€£ 4.4.2 Object recognition in real-world scenes â€£ 4.4 Results â€£ 4 EXPERIMENTAL RESULTS â€£ Towards real-time object recognition and pose estimation in point clouds" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">TableÂ 4</span></a>).</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Performance comparison between full and a specific training set with objects from the Scenes dataset. </figcaption>
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding:1.5pt 5.0pt;" rowspan="2"><span id="S4.T4.1.1.1.1.1" class="ltx_text">DeepNet</span></th>
<th id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding:1.5pt 5.0pt;" colspan="2">Full</th>
<th id="S4.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:1.5pt 5.0pt;" colspan="2">Scenes</th>
</tr>
<tr id="S4.T4.1.2.2" class="ltx_tr">
<th id="S4.T4.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:1.5pt 5.0pt;">Acc</th>
<th id="S4.T4.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding:1.5pt 5.0pt;">FPS</th>
<th id="S4.T4.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:1.5pt 5.0pt;">Acc</th>
<th id="S4.T4.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:1.5pt 5.0pt;">FPS</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.3.1" class="ltx_tr">
<th id="S4.T4.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:1.5pt 5.0pt;">MobileNet v2</th>
<td id="S4.T4.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 5.0pt;">52.77%</td>
<td id="S4.T4.1.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 5.0pt;">6.99</td>
<td id="S4.T4.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 5.0pt;">67.35%</td>
<td id="S4.T4.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 5.0pt;">24.62</td>
</tr>
<tr id="S4.T4.1.4.2" class="ltx_tr">
<th id="S4.T4.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:1.5pt 5.0pt;">Resnet101</th>
<td id="S4.T4.1.4.2.2" class="ltx_td ltx_align_center" style="padding:1.5pt 5.0pt;">45.37%</td>
<td id="S4.T4.1.4.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 5.0pt;">5.03</td>
<td id="S4.T4.1.4.2.4" class="ltx_td ltx_align_center" style="padding:1.5pt 5.0pt;">61.41%</td>
<td id="S4.T4.1.4.2.5" class="ltx_td ltx_align_center" style="padding:1.5pt 5.0pt;">13.94</td>
</tr>
<tr id="S4.T4.1.5.3" class="ltx_tr">
<th id="S4.T4.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:1.5pt 5.0pt;">ResNeXt101 32x8d</th>
<td id="S4.T4.1.5.3.2" class="ltx_td ltx_align_center" style="padding:1.5pt 5.0pt;">49.16%</td>
<td id="S4.T4.1.5.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 5.0pt;">3.80</td>
<td id="S4.T4.1.5.3.4" class="ltx_td ltx_align_center" style="padding:1.5pt 5.0pt;">59.04%</td>
<td id="S4.T4.1.5.3.5" class="ltx_td ltx_align_center" style="padding:1.5pt 5.0pt;">8.86</td>
</tr>
<tr id="S4.T4.1.6.4" class="ltx_tr">
<th id="S4.T4.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding:1.5pt 5.0pt;">EfficientNet-B7</th>
<td id="S4.T4.1.6.4.2" class="ltx_td ltx_align_center ltx_border_b" style="padding:1.5pt 5.0pt;">66.86%</td>
<td id="S4.T4.1.6.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding:1.5pt 5.0pt;">3.02</td>
<td id="S4.T4.1.6.4.4" class="ltx_td ltx_align_center ltx_border_b" style="padding:1.5pt 5.0pt;">82.94%</td>
<td id="S4.T4.1.6.4.5" class="ltx_td ltx_align_center ltx_border_b" style="padding:1.5pt 5.0pt;">5.88</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS4.SSS2.p5" class="ltx_para">
<p id="S4.SS4.SSS2.p5.1" class="ltx_p">After this change on the model specificity, we distinguish a noticeable improvement in accuracy and the processing time, achieving MobileNet v2 a near real-time performance on average. A significant gain on accuracy was established, with over 10% for every architecture, pulling the best result to 83% for EfficientNet.</p>
</div>
<div id="S4.SS4.SSS2.p6" class="ltx_para">
<p id="S4.SS4.SSS2.p6.1" class="ltx_p">Regarding the frame processing rate, it is essential to notice that the average number of models varies from 1.85 to 8.79 over the scenes, with almost four objects per frame in mean (<a href="#S4.T3" title="Table 3 â€£ 4.4.1 Object detection benchmark â€£ 4.4 Results â€£ 4 EXPERIMENTAL RESULTS â€£ Towards real-time object recognition and pose estimation in point clouds" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">TableÂ 3</span></a>). Thus, we can infer that our proposal can deliver a near-real-time FPS, inclusive in a multi-classification problem. When we consider only a single target, the performance is almost four times faster, as presented in <a href="#S4.T7" title="Table 7 â€£ 4.4.4 Time processing evaluation â€£ 4.4 Results â€£ 4 EXPERIMENTAL RESULTS â€£ Towards real-time object recognition and pose estimation in point clouds" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">TableÂ 7</span></a>, on the <span id="S4.SS4.SSS2.p6.1.1" class="ltx_text ltx_font_italic">Color only</span> column.</p>
</div>
</section>
<section id="S4.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.3 </span>Pose estimation results</h4>

<div id="S4.SS4.SSS3.p1" class="ltx_para">
<p id="S4.SS4.SSS3.p1.1" class="ltx_p">Based on the assumption that we mapped the objects we aim to detect in a real-world scenario, we adopted those models trained on the RGB-D Object dataset subset. We considered only the instance detection situation. The reason for disregarding categories is that we could have intra-class misclassifications, corrupting the pose alignment step. For each instance detected by the <span id="S4.SS4.SSS3.p1.1.1" class="ltx_text ltx_font_italic">Color feature classification</span> stage, we take ten views of the referred object from the modelsâ€™ database.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Comparison between feature-based registration methods. Values reported consider the processing time (in seconds) for ten views of the same object and the ICP for the best one selected. </figcaption>
<table id="S4.T5.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.4.4" class="ltx_tr">
<th id="S4.T5.4.4.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Methods</th>
<th id="S4.T5.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S4.T5.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.1.1.1.2" class="ltx_tr">
<td id="S4.T5.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">Feature-based</td>
</tr>
<tr id="S4.T5.1.1.1.1.1" class="ltx_tr">
<td id="S4.T5.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">time (<math id="S4.T5.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T5.1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T5.1.1.1.1.1.1.m1.1.1" xref="S4.T5.1.1.1.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.1.1.1.m1.1b"><ci id="S4.T5.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>)</td>
</tr>
</table>
</th>
<th id="S4.T5.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">ICP time (<math id="S4.T5.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T5.2.2.2.m1.1a"><mo stretchy="false" id="S4.T5.2.2.2.m1.1.1" xref="S4.T5.2.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.2.m1.1b"><ci id="S4.T5.2.2.2.m1.1.1.cmml" xref="S4.T5.2.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.2.m1.1c">\downarrow</annotation></semantics></math>)</th>
<th id="S4.T5.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Inlier ratio (<math id="S4.T5.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T5.3.3.3.m1.1a"><mo stretchy="false" id="S4.T5.3.3.3.m1.1.1" xref="S4.T5.3.3.3.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T5.3.3.3.m1.1b"><ci id="S4.T5.3.3.3.m1.1.1.cmml" xref="S4.T5.3.3.3.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.3.3.3.m1.1c">\uparrow</annotation></semantics></math>)</th>
<th id="S4.T5.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">RMSE (<math id="S4.T5.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T5.4.4.4.m1.1a"><mo stretchy="false" id="S4.T5.4.4.4.m1.1.1" xref="S4.T5.4.4.4.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T5.4.4.4.m1.1b"><ci id="S4.T5.4.4.4.m1.1.1.cmml" xref="S4.T5.4.4.4.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.4.4.4.m1.1c">\downarrow</annotation></semantics></math>)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.4.5.1" class="ltx_tr">
<th id="S4.T5.4.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">RANSAC</th>
<td id="S4.T5.4.5.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.7688</td>
<td id="S4.T5.4.5.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.0061</td>
<td id="S4.T5.4.5.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.2689</td>
<td id="S4.T5.4.5.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.0055</td>
</tr>
<tr id="S4.T5.4.6.2" class="ltx_tr">
<th id="S4.T5.4.6.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" style="padding-top:1.5pt;padding-bottom:1.5pt;">FGR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx38" title="" class="ltx_ref">Zhou etÂ al., 2016</a>]</cite>
</th>
<td id="S4.T5.4.6.2.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.0580</td>
<td id="S4.T5.4.6.2.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.0075</td>
<td id="S4.T5.4.6.2.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.1895</td>
<td id="S4.T5.4.6.2.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.0059</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS4.SSS3.p2" class="ltx_para">
<p id="S4.SS4.SSS3.p2.1" class="ltx_p">In <a href="#S4.T5" title="Table 5 â€£ 4.4.3 Pose estimation results â€£ 4.4 Results â€£ 4 EXPERIMENTAL RESULTS â€£ Towards real-time object recognition and pose estimation in point clouds" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">TableÂ 5</span></a> we report an evaluation concerning the Feature-based registration and Fine-adjustment stages of our pipeline. Getting a set of ten randomly selected views of the same object, we perform a coarse estimation by using RANSAC or FGR. We evaluate quantitatively such methods concerning the inlier ratio, RMSE, and execution time. We apply the resulting affine transformation as the input of an ICP dense registration and evaluate if this input can imply differences in the processing time.</p>
</div>
<div id="S4.SS4.SSS3.p3" class="ltx_para">
<p id="S4.SS4.SSS3.p3.1" class="ltx_p">Indeed, the FGR method is much faster than RANSAC. However, we observe that for both metrics RANSAC outperforms it. The Inlier ratio presented by the latter is around 50% higher than the faster method and also shows an RMSE more consistent. The transformation generated by the coarse alignment algorithm also impacts the ICP execution and we notice that a better estimation can speed up the fine-adjustment process.</p>
</div>
<div id="S4.SS4.SSS3.p4" class="ltx_para">
<p id="S4.SS4.SSS3.p4.1" class="ltx_p">To evaluate more deeply if the ICP, after the feature-matching application, can surpass problems like a more rough estimation, we must assess an annotated pose. Unfortunately, the adopted dataset does not offer such data, and further studies may verify that affirmation on a pose-annotated dataset. However, we can evaluate the estimation correctness by employing the IoU and MIR metrics and verify if the feature-based registration stepâ€™s estimation is reliable compared to standard approaches. In <a href="#S4.T6" title="Table 6 â€£ 4.4.3 Pose estimation results â€£ 4.4 Results â€£ 4 EXPERIMENTAL RESULTS â€£ Towards real-time object recognition and pose estimation in point clouds" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">TableÂ 6</span></a> we perform such comparison regarding the AUC and FPS values of different setup of our proposed pipeline, the standard <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">Aldoma etÂ al., 2012b</a>]</cite>, and the boosted <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">Marcon etÂ al., 2019</a>]</cite> pipelines.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Comparison of the proposed pipeline with standard object recognition and pose estimation approaches. Baseline refer to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">Aldoma etÂ al., 2012a</a>]</cite> and Boost to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">Marcon etÂ al., 2019</a>]</cite>. Every trial employed FPFH <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Rusu etÂ al., 2009</a>]</cite> as local descriptor with a uniform sampling as keypoint detector. Excepting the fisrt two rows, leaf size was set to 1 cm.. </figcaption>
<table id="S4.T6.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T6.3.4.1" class="ltx_tr">
<th id="S4.T6.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Method</th>
<td id="S4.T6.3.4.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">AUC</td>
<td id="S4.T6.3.4.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">FPS</td>
</tr>
<tr id="S4.T6.1.1" class="ltx_tr">
<th id="S4.T6.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Baseline <math id="S4.T6.1.1.1.m1.1" class="ltx_Math" alttext="US_{0.02}" display="inline"><semantics id="S4.T6.1.1.1.m1.1a"><mrow id="S4.T6.1.1.1.m1.1.1" xref="S4.T6.1.1.1.m1.1.1.cmml"><mi id="S4.T6.1.1.1.m1.1.1.2" xref="S4.T6.1.1.1.m1.1.1.2.cmml">U</mi><mo lspace="0em" rspace="0em" id="S4.T6.1.1.1.m1.1.1.1" xref="S4.T6.1.1.1.m1.1.1.1.cmml">â€‹</mo><msub id="S4.T6.1.1.1.m1.1.1.3" xref="S4.T6.1.1.1.m1.1.1.3.cmml"><mi id="S4.T6.1.1.1.m1.1.1.3.2" xref="S4.T6.1.1.1.m1.1.1.3.2.cmml">S</mi><mn id="S4.T6.1.1.1.m1.1.1.3.3" xref="S4.T6.1.1.1.m1.1.1.3.3.cmml">0.02</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.1.1.1.m1.1b"><apply id="S4.T6.1.1.1.m1.1.1.cmml" xref="S4.T6.1.1.1.m1.1.1"><times id="S4.T6.1.1.1.m1.1.1.1.cmml" xref="S4.T6.1.1.1.m1.1.1.1"></times><ci id="S4.T6.1.1.1.m1.1.1.2.cmml" xref="S4.T6.1.1.1.m1.1.1.2">ğ‘ˆ</ci><apply id="S4.T6.1.1.1.m1.1.1.3.cmml" xref="S4.T6.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T6.1.1.1.m1.1.1.3.1.cmml" xref="S4.T6.1.1.1.m1.1.1.3">subscript</csymbol><ci id="S4.T6.1.1.1.m1.1.1.3.2.cmml" xref="S4.T6.1.1.1.m1.1.1.3.2">ğ‘†</ci><cn type="float" id="S4.T6.1.1.1.m1.1.1.3.3.cmml" xref="S4.T6.1.1.1.m1.1.1.3.3">0.02</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.1.1.1.m1.1c">US_{0.02}</annotation></semantics></math>
</th>
<td id="S4.T6.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.0401</td>
<td id="S4.T6.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.0023</td>
</tr>
<tr id="S4.T6.2.2" class="ltx_tr">
<th id="S4.T6.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.5pt;padding-bottom:1.5pt;">Boost <math id="S4.T6.2.2.1.m1.1" class="ltx_Math" alttext="US_{0.02}" display="inline"><semantics id="S4.T6.2.2.1.m1.1a"><mrow id="S4.T6.2.2.1.m1.1.1" xref="S4.T6.2.2.1.m1.1.1.cmml"><mi id="S4.T6.2.2.1.m1.1.1.2" xref="S4.T6.2.2.1.m1.1.1.2.cmml">U</mi><mo lspace="0em" rspace="0em" id="S4.T6.2.2.1.m1.1.1.1" xref="S4.T6.2.2.1.m1.1.1.1.cmml">â€‹</mo><msub id="S4.T6.2.2.1.m1.1.1.3" xref="S4.T6.2.2.1.m1.1.1.3.cmml"><mi id="S4.T6.2.2.1.m1.1.1.3.2" xref="S4.T6.2.2.1.m1.1.1.3.2.cmml">S</mi><mn id="S4.T6.2.2.1.m1.1.1.3.3" xref="S4.T6.2.2.1.m1.1.1.3.3.cmml">0.02</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.2.2.1.m1.1b"><apply id="S4.T6.2.2.1.m1.1.1.cmml" xref="S4.T6.2.2.1.m1.1.1"><times id="S4.T6.2.2.1.m1.1.1.1.cmml" xref="S4.T6.2.2.1.m1.1.1.1"></times><ci id="S4.T6.2.2.1.m1.1.1.2.cmml" xref="S4.T6.2.2.1.m1.1.1.2">ğ‘ˆ</ci><apply id="S4.T6.2.2.1.m1.1.1.3.cmml" xref="S4.T6.2.2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T6.2.2.1.m1.1.1.3.1.cmml" xref="S4.T6.2.2.1.m1.1.1.3">subscript</csymbol><ci id="S4.T6.2.2.1.m1.1.1.3.2.cmml" xref="S4.T6.2.2.1.m1.1.1.3.2">ğ‘†</ci><cn type="float" id="S4.T6.2.2.1.m1.1.1.3.3.cmml" xref="S4.T6.2.2.1.m1.1.1.3.3">0.02</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.2.2.1.m1.1c">US_{0.02}</annotation></semantics></math>
</th>
<td id="S4.T6.2.2.2" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.0868</td>
<td id="S4.T6.2.2.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.0918</td>
</tr>
<tr id="S4.T6.3.3" class="ltx_tr">
<th id="S4.T6.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.5pt;padding-bottom:1.5pt;">Boost <math id="S4.T6.3.3.1.m1.1" class="ltx_Math" alttext="US_{0.01}" display="inline"><semantics id="S4.T6.3.3.1.m1.1a"><mrow id="S4.T6.3.3.1.m1.1.1" xref="S4.T6.3.3.1.m1.1.1.cmml"><mi id="S4.T6.3.3.1.m1.1.1.2" xref="S4.T6.3.3.1.m1.1.1.2.cmml">U</mi><mo lspace="0em" rspace="0em" id="S4.T6.3.3.1.m1.1.1.1" xref="S4.T6.3.3.1.m1.1.1.1.cmml">â€‹</mo><msub id="S4.T6.3.3.1.m1.1.1.3" xref="S4.T6.3.3.1.m1.1.1.3.cmml"><mi id="S4.T6.3.3.1.m1.1.1.3.2" xref="S4.T6.3.3.1.m1.1.1.3.2.cmml">S</mi><mn id="S4.T6.3.3.1.m1.1.1.3.3" xref="S4.T6.3.3.1.m1.1.1.3.3.cmml">0.01</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.3.3.1.m1.1b"><apply id="S4.T6.3.3.1.m1.1.1.cmml" xref="S4.T6.3.3.1.m1.1.1"><times id="S4.T6.3.3.1.m1.1.1.1.cmml" xref="S4.T6.3.3.1.m1.1.1.1"></times><ci id="S4.T6.3.3.1.m1.1.1.2.cmml" xref="S4.T6.3.3.1.m1.1.1.2">ğ‘ˆ</ci><apply id="S4.T6.3.3.1.m1.1.1.3.cmml" xref="S4.T6.3.3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T6.3.3.1.m1.1.1.3.1.cmml" xref="S4.T6.3.3.1.m1.1.1.3">subscript</csymbol><ci id="S4.T6.3.3.1.m1.1.1.3.2.cmml" xref="S4.T6.3.3.1.m1.1.1.3.2">ğ‘†</ci><cn type="float" id="S4.T6.3.3.1.m1.1.1.3.3.cmml" xref="S4.T6.3.3.1.m1.1.1.3.3">0.01</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.3.3.1.m1.1c">US_{0.01}</annotation></semantics></math>
</th>
<td id="S4.T6.3.3.2" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.1372</td>
<td id="S4.T6.3.3.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.0339</td>
</tr>
<tr id="S4.T6.3.5.2" class="ltx_tr">
<th id="S4.T6.3.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Resnet101 + FGR</th>
<td id="S4.T6.3.5.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.2228</td>
<td id="S4.T6.3.5.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">13.8321</td>
</tr>
<tr id="S4.T6.3.6.3" class="ltx_tr">
<th id="S4.T6.3.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.5pt;padding-bottom:1.5pt;">ResNet101 + RANSAC</th>
<td id="S4.T6.3.6.3.2" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.2092</td>
<td id="S4.T6.3.6.3.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">1.9649</td>
</tr>
<tr id="S4.T6.3.7.4" class="ltx_tr">
<th id="S4.T6.3.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.5pt;padding-bottom:1.5pt;">MobileNet v2 + FGR</th>
<td id="S4.T6.3.7.4.2" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.2922</td>
<td id="S4.T6.3.7.4.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">13.8939</td>
</tr>
<tr id="S4.T6.3.8.5" class="ltx_tr">
<th id="S4.T6.3.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.5pt;padding-bottom:1.5pt;">MobileNet v2 + RANSAC</th>
<td id="S4.T6.3.8.5.2" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.2781</td>
<td id="S4.T6.3.8.5.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">1.8905</td>
</tr>
<tr id="S4.T6.3.9.6" class="ltx_tr">
<th id="S4.T6.3.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.5pt;padding-bottom:1.5pt;">ResNeXt101 32x8d + FGR</th>
<td id="S4.T6.3.9.6.2" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.2090</td>
<td id="S4.T6.3.9.6.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">14.1813</td>
</tr>
<tr id="S4.T6.3.10.7" class="ltx_tr">
<th id="S4.T6.3.10.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.5pt;padding-bottom:1.5pt;">ResNeXt101 32x8d + RANSAC</th>
<td id="S4.T6.3.10.7.2" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.1947</td>
<td id="S4.T6.3.10.7.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">2.0268</td>
</tr>
<tr id="S4.T6.3.11.8" class="ltx_tr">
<th id="S4.T6.3.11.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.5pt;padding-bottom:1.5pt;">EfficientNet-B7 + FGR</th>
<td id="S4.T6.3.11.8.2" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.4123</td>
<td id="S4.T6.3.11.8.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">8.9429</td>
</tr>
<tr id="S4.T6.3.12.9" class="ltx_tr">
<th id="S4.T6.3.12.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" style="padding-top:1.5pt;padding-bottom:1.5pt;">EfficientNet-B7 + RANSAC</th>
<td id="S4.T6.3.12.9.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.5pt;padding-bottom:1.5pt;">0.2994</td>
<td id="S4.T6.3.12.9.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.5pt;padding-bottom:1.5pt;">1.4344</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS4.SSS3.p5" class="ltx_para">
<p id="S4.SS4.SSS3.p5.8" class="ltx_p">Results of <a href="#S4.T6" title="Table 6 â€£ 4.4.3 Pose estimation results â€£ 4.4 Results â€£ 4 EXPERIMENTAL RESULTS â€£ Towards real-time object recognition and pose estimation in point clouds" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">TableÂ 6</span></a> confirm our claim that performing the object detection on the RGB images improves results compared to traditional approaches. Both standard and boosted pipelines present accuracy results worst than all trials we run in our pipeline, even considering the same conditions of descriptors and leaf size, e.g., 1 cm of leaf size in Boost <math id="S4.SS4.SSS3.p5.1.m1.1" class="ltx_Math" alttext="US_{0.01}" display="inline"><semantics id="S4.SS4.SSS3.p5.1.m1.1a"><mrow id="S4.SS4.SSS3.p5.1.m1.1.1" xref="S4.SS4.SSS3.p5.1.m1.1.1.cmml"><mi id="S4.SS4.SSS3.p5.1.m1.1.1.2" xref="S4.SS4.SSS3.p5.1.m1.1.1.2.cmml">U</mi><mo lspace="0em" rspace="0em" id="S4.SS4.SSS3.p5.1.m1.1.1.1" xref="S4.SS4.SSS3.p5.1.m1.1.1.1.cmml">â€‹</mo><msub id="S4.SS4.SSS3.p5.1.m1.1.1.3" xref="S4.SS4.SSS3.p5.1.m1.1.1.3.cmml"><mi id="S4.SS4.SSS3.p5.1.m1.1.1.3.2" xref="S4.SS4.SSS3.p5.1.m1.1.1.3.2.cmml">S</mi><mn id="S4.SS4.SSS3.p5.1.m1.1.1.3.3" xref="S4.SS4.SSS3.p5.1.m1.1.1.3.3.cmml">0.01</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.p5.1.m1.1b"><apply id="S4.SS4.SSS3.p5.1.m1.1.1.cmml" xref="S4.SS4.SSS3.p5.1.m1.1.1"><times id="S4.SS4.SSS3.p5.1.m1.1.1.1.cmml" xref="S4.SS4.SSS3.p5.1.m1.1.1.1"></times><ci id="S4.SS4.SSS3.p5.1.m1.1.1.2.cmml" xref="S4.SS4.SSS3.p5.1.m1.1.1.2">ğ‘ˆ</ci><apply id="S4.SS4.SSS3.p5.1.m1.1.1.3.cmml" xref="S4.SS4.SSS3.p5.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS4.SSS3.p5.1.m1.1.1.3.1.cmml" xref="S4.SS4.SSS3.p5.1.m1.1.1.3">subscript</csymbol><ci id="S4.SS4.SSS3.p5.1.m1.1.1.3.2.cmml" xref="S4.SS4.SSS3.p5.1.m1.1.1.3.2">ğ‘†</ci><cn type="float" id="S4.SS4.SSS3.p5.1.m1.1.1.3.3.cmml" xref="S4.SS4.SSS3.p5.1.m1.1.1.3.3">0.01</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.p5.1.m1.1c">US_{0.01}</annotation></semantics></math> trial. When we consider time processing, the difference is even more discrepant when our approach presents in the best case, a frame-rate of 14.18 against 0.09 FPS on the best standard approaches, which represents a remarkable improvement of more than <math id="S4.SS4.SSS3.p5.2.m2.1" class="ltx_math_unparsed" alttext="150\times" display="inline"><semantics id="S4.SS4.SSS3.p5.2.m2.1a"><mrow id="S4.SS4.SSS3.p5.2.m2.1b"><mn id="S4.SS4.SSS3.p5.2.m2.1.1">150</mn><mo lspace="0.222em" id="S4.SS4.SSS3.p5.2.m2.1.2">Ã—</mo></mrow><annotation encoding="application/x-tex" id="S4.SS4.SSS3.p5.2.m2.1c">150\times</annotation></semantics></math> in speed. When using the EfficientNet/FGR pair, our proposal presents AUC (0.4123) three times higher than the Boosted pipeline (0.1372). We did not run the Baseline <math id="S4.SS4.SSS3.p5.3.m3.1" class="ltx_Math" alttext="US_{0.01}" display="inline"><semantics id="S4.SS4.SSS3.p5.3.m3.1a"><mrow id="S4.SS4.SSS3.p5.3.m3.1.1" xref="S4.SS4.SSS3.p5.3.m3.1.1.cmml"><mi id="S4.SS4.SSS3.p5.3.m3.1.1.2" xref="S4.SS4.SSS3.p5.3.m3.1.1.2.cmml">U</mi><mo lspace="0em" rspace="0em" id="S4.SS4.SSS3.p5.3.m3.1.1.1" xref="S4.SS4.SSS3.p5.3.m3.1.1.1.cmml">â€‹</mo><msub id="S4.SS4.SSS3.p5.3.m3.1.1.3" xref="S4.SS4.SSS3.p5.3.m3.1.1.3.cmml"><mi id="S4.SS4.SSS3.p5.3.m3.1.1.3.2" xref="S4.SS4.SSS3.p5.3.m3.1.1.3.2.cmml">S</mi><mn id="S4.SS4.SSS3.p5.3.m3.1.1.3.3" xref="S4.SS4.SSS3.p5.3.m3.1.1.3.3.cmml">0.01</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.p5.3.m3.1b"><apply id="S4.SS4.SSS3.p5.3.m3.1.1.cmml" xref="S4.SS4.SSS3.p5.3.m3.1.1"><times id="S4.SS4.SSS3.p5.3.m3.1.1.1.cmml" xref="S4.SS4.SSS3.p5.3.m3.1.1.1"></times><ci id="S4.SS4.SSS3.p5.3.m3.1.1.2.cmml" xref="S4.SS4.SSS3.p5.3.m3.1.1.2">ğ‘ˆ</ci><apply id="S4.SS4.SSS3.p5.3.m3.1.1.3.cmml" xref="S4.SS4.SSS3.p5.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS4.SSS3.p5.3.m3.1.1.3.1.cmml" xref="S4.SS4.SSS3.p5.3.m3.1.1.3">subscript</csymbol><ci id="S4.SS4.SSS3.p5.3.m3.1.1.3.2.cmml" xref="S4.SS4.SSS3.p5.3.m3.1.1.3.2">ğ‘†</ci><cn type="float" id="S4.SS4.SSS3.p5.3.m3.1.1.3.3.cmml" xref="S4.SS4.SSS3.p5.3.m3.1.1.3.3">0.01</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.p5.3.m3.1c">US_{0.01}</annotation></semantics></math> because this method is very time-consuming and does not represent a reasonable choice regarding the boosted version (Boost <math id="S4.SS4.SSS3.p5.4.m4.1" class="ltx_Math" alttext="US_{0.01}" display="inline"><semantics id="S4.SS4.SSS3.p5.4.m4.1a"><mrow id="S4.SS4.SSS3.p5.4.m4.1.1" xref="S4.SS4.SSS3.p5.4.m4.1.1.cmml"><mi id="S4.SS4.SSS3.p5.4.m4.1.1.2" xref="S4.SS4.SSS3.p5.4.m4.1.1.2.cmml">U</mi><mo lspace="0em" rspace="0em" id="S4.SS4.SSS3.p5.4.m4.1.1.1" xref="S4.SS4.SSS3.p5.4.m4.1.1.1.cmml">â€‹</mo><msub id="S4.SS4.SSS3.p5.4.m4.1.1.3" xref="S4.SS4.SSS3.p5.4.m4.1.1.3.cmml"><mi id="S4.SS4.SSS3.p5.4.m4.1.1.3.2" xref="S4.SS4.SSS3.p5.4.m4.1.1.3.2.cmml">S</mi><mn id="S4.SS4.SSS3.p5.4.m4.1.1.3.3" xref="S4.SS4.SSS3.p5.4.m4.1.1.3.3.cmml">0.01</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.p5.4.m4.1b"><apply id="S4.SS4.SSS3.p5.4.m4.1.1.cmml" xref="S4.SS4.SSS3.p5.4.m4.1.1"><times id="S4.SS4.SSS3.p5.4.m4.1.1.1.cmml" xref="S4.SS4.SSS3.p5.4.m4.1.1.1"></times><ci id="S4.SS4.SSS3.p5.4.m4.1.1.2.cmml" xref="S4.SS4.SSS3.p5.4.m4.1.1.2">ğ‘ˆ</ci><apply id="S4.SS4.SSS3.p5.4.m4.1.1.3.cmml" xref="S4.SS4.SSS3.p5.4.m4.1.1.3"><csymbol cd="ambiguous" id="S4.SS4.SSS3.p5.4.m4.1.1.3.1.cmml" xref="S4.SS4.SSS3.p5.4.m4.1.1.3">subscript</csymbol><ci id="S4.SS4.SSS3.p5.4.m4.1.1.3.2.cmml" xref="S4.SS4.SSS3.p5.4.m4.1.1.3.2">ğ‘†</ci><cn type="float" id="S4.SS4.SSS3.p5.4.m4.1.1.3.3.cmml" xref="S4.SS4.SSS3.p5.4.m4.1.1.3.3">0.01</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.p5.4.m4.1c">US_{0.01}</annotation></semantics></math>). We found a frame rate of <math id="S4.SS4.SSS3.p5.5.m5.1" class="ltx_Math" alttext="0.0005" display="inline"><semantics id="S4.SS4.SSS3.p5.5.m5.1a"><mn id="S4.SS4.SSS3.p5.5.m5.1.1" xref="S4.SS4.SSS3.p5.5.m5.1.1.cmml">0.0005</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.p5.5.m5.1b"><cn type="float" id="S4.SS4.SSS3.p5.5.m5.1.1.cmml" xref="S4.SS4.SSS3.p5.5.m5.1.1">0.0005</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.p5.5.m5.1c">0.0005</annotation></semantics></math> for a small set of frames experimentally. Besides, the boosted pipeline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">Marcon etÂ al., 2019</a>]</cite> gains on accuracy and time performance regarding the traditional version, as seen on the trials with a leaf size of <math id="S4.SS4.SSS3.p5.6.m6.1" class="ltx_Math" alttext="0.02" display="inline"><semantics id="S4.SS4.SSS3.p5.6.m6.1a"><mn id="S4.SS4.SSS3.p5.6.m6.1.1" xref="S4.SS4.SSS3.p5.6.m6.1.1.cmml">0.02</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.p5.6.m6.1b"><cn type="float" id="S4.SS4.SSS3.p5.6.m6.1.1.cmml" xref="S4.SS4.SSS3.p5.6.m6.1.1">0.02</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.p5.6.m6.1c">0.02</annotation></semantics></math> (Baseline <math id="S4.SS4.SSS3.p5.7.m7.1" class="ltx_Math" alttext="US_{0.02}" display="inline"><semantics id="S4.SS4.SSS3.p5.7.m7.1a"><mrow id="S4.SS4.SSS3.p5.7.m7.1.1" xref="S4.SS4.SSS3.p5.7.m7.1.1.cmml"><mi id="S4.SS4.SSS3.p5.7.m7.1.1.2" xref="S4.SS4.SSS3.p5.7.m7.1.1.2.cmml">U</mi><mo lspace="0em" rspace="0em" id="S4.SS4.SSS3.p5.7.m7.1.1.1" xref="S4.SS4.SSS3.p5.7.m7.1.1.1.cmml">â€‹</mo><msub id="S4.SS4.SSS3.p5.7.m7.1.1.3" xref="S4.SS4.SSS3.p5.7.m7.1.1.3.cmml"><mi id="S4.SS4.SSS3.p5.7.m7.1.1.3.2" xref="S4.SS4.SSS3.p5.7.m7.1.1.3.2.cmml">S</mi><mn id="S4.SS4.SSS3.p5.7.m7.1.1.3.3" xref="S4.SS4.SSS3.p5.7.m7.1.1.3.3.cmml">0.02</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.p5.7.m7.1b"><apply id="S4.SS4.SSS3.p5.7.m7.1.1.cmml" xref="S4.SS4.SSS3.p5.7.m7.1.1"><times id="S4.SS4.SSS3.p5.7.m7.1.1.1.cmml" xref="S4.SS4.SSS3.p5.7.m7.1.1.1"></times><ci id="S4.SS4.SSS3.p5.7.m7.1.1.2.cmml" xref="S4.SS4.SSS3.p5.7.m7.1.1.2">ğ‘ˆ</ci><apply id="S4.SS4.SSS3.p5.7.m7.1.1.3.cmml" xref="S4.SS4.SSS3.p5.7.m7.1.1.3"><csymbol cd="ambiguous" id="S4.SS4.SSS3.p5.7.m7.1.1.3.1.cmml" xref="S4.SS4.SSS3.p5.7.m7.1.1.3">subscript</csymbol><ci id="S4.SS4.SSS3.p5.7.m7.1.1.3.2.cmml" xref="S4.SS4.SSS3.p5.7.m7.1.1.3.2">ğ‘†</ci><cn type="float" id="S4.SS4.SSS3.p5.7.m7.1.1.3.3.cmml" xref="S4.SS4.SSS3.p5.7.m7.1.1.3.3">0.02</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.p5.7.m7.1c">US_{0.02}</annotation></semantics></math> and Boost <math id="S4.SS4.SSS3.p5.8.m8.1" class="ltx_Math" alttext="US_{0.02}" display="inline"><semantics id="S4.SS4.SSS3.p5.8.m8.1a"><mrow id="S4.SS4.SSS3.p5.8.m8.1.1" xref="S4.SS4.SSS3.p5.8.m8.1.1.cmml"><mi id="S4.SS4.SSS3.p5.8.m8.1.1.2" xref="S4.SS4.SSS3.p5.8.m8.1.1.2.cmml">U</mi><mo lspace="0em" rspace="0em" id="S4.SS4.SSS3.p5.8.m8.1.1.1" xref="S4.SS4.SSS3.p5.8.m8.1.1.1.cmml">â€‹</mo><msub id="S4.SS4.SSS3.p5.8.m8.1.1.3" xref="S4.SS4.SSS3.p5.8.m8.1.1.3.cmml"><mi id="S4.SS4.SSS3.p5.8.m8.1.1.3.2" xref="S4.SS4.SSS3.p5.8.m8.1.1.3.2.cmml">S</mi><mn id="S4.SS4.SSS3.p5.8.m8.1.1.3.3" xref="S4.SS4.SSS3.p5.8.m8.1.1.3.3.cmml">0.02</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.p5.8.m8.1b"><apply id="S4.SS4.SSS3.p5.8.m8.1.1.cmml" xref="S4.SS4.SSS3.p5.8.m8.1.1"><times id="S4.SS4.SSS3.p5.8.m8.1.1.1.cmml" xref="S4.SS4.SSS3.p5.8.m8.1.1.1"></times><ci id="S4.SS4.SSS3.p5.8.m8.1.1.2.cmml" xref="S4.SS4.SSS3.p5.8.m8.1.1.2">ğ‘ˆ</ci><apply id="S4.SS4.SSS3.p5.8.m8.1.1.3.cmml" xref="S4.SS4.SSS3.p5.8.m8.1.1.3"><csymbol cd="ambiguous" id="S4.SS4.SSS3.p5.8.m8.1.1.3.1.cmml" xref="S4.SS4.SSS3.p5.8.m8.1.1.3">subscript</csymbol><ci id="S4.SS4.SSS3.p5.8.m8.1.1.3.2.cmml" xref="S4.SS4.SSS3.p5.8.m8.1.1.3.2">ğ‘†</ci><cn type="float" id="S4.SS4.SSS3.p5.8.m8.1.1.3.3.cmml" xref="S4.SS4.SSS3.p5.8.m8.1.1.3.3">0.02</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.p5.8.m8.1c">US_{0.02}</annotation></semantics></math>), and such behavior is also expected on a smaller leaf size.</p>
</div>
</section>
<section id="S4.SS4.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.4 </span>Time processing evaluation</h4>

<div id="S4.SS4.SSS4.p1" class="ltx_para">
<p id="S4.SS4.SSS4.p1.1" class="ltx_p">Now we report the processing rate regarding executing the three stages of our proposed pipeline. <a href="#S4.T7" title="Table 7 â€£ 4.4.4 Time processing evaluation â€£ 4.4 Results â€£ 4 EXPERIMENTAL RESULTS â€£ Towards real-time object recognition and pose estimation in point clouds" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">TableÂ 7</span></a> presents the frame processing rate based on a single target object scenario. We evaluate referring to the first stage execution (<span id="S4.SS4.SSS4.p1.1.1" class="ltx_text ltx_font_italic">Color only</span>), the early two stages (Columns <span id="S4.SS4.SSS4.p1.1.2" class="ltx_text ltx_font_italic">RANSAC</span>, and <span id="S4.SS4.SSS4.p1.1.3" class="ltx_text ltx_font_italic">FGR</span>), and a pipelineâ€™s full execution (<span id="S4.SS4.SSS4.p1.1.4" class="ltx_text ltx_font_italic">+ICP</span>).</p>
</div>
<figure id="S4.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Single target pose estimation FPS. <span id="S4.T7.2.1" class="ltx_text ltx_font_italic">Color only</span> refers to object classification, other columns refer to the pose aligment step, coarse (RANSAC and FGR) or fine (plus ICP). </figcaption>
<table id="S4.T7.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T7.3.1.1" class="ltx_tr">
<th id="S4.T7.3.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></th>
<th id="S4.T7.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Color only</th>
<th id="S4.T7.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">RANSAC</th>
<th id="S4.T7.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">FGR</th>
<th id="S4.T7.3.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">RANSAC + ICP</th>
<th id="S4.T7.3.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">FGR + ICP</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T7.3.2.1" class="ltx_tr">
<th id="S4.T7.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">MobileNet v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx28" title="" class="ltx_ref">Sandler etÂ al., 2018</a>]</cite>
</th>
<td id="S4.T7.3.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">89.49</td>
<td id="S4.T7.3.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">1.89</td>
<td id="S4.T7.3.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">13.89</td>
<td id="S4.T7.3.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">1.82</td>
<td id="S4.T7.3.2.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">13.57</td>
</tr>
<tr id="S4.T7.3.3.2" class="ltx_tr">
<th id="S4.T7.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.5pt;padding-bottom:1.5pt;">ResNet101 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">He etÂ al., 2016</a>]</cite>
</th>
<td id="S4.T7.3.3.2.2" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">52.45</td>
<td id="S4.T7.3.3.2.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">1.96</td>
<td id="S4.T7.3.3.2.4" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">13.83</td>
<td id="S4.T7.3.3.2.5" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">1.81</td>
<td id="S4.T7.3.3.2.6" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">13.39</td>
</tr>
<tr id="S4.T7.3.4.3" class="ltx_tr">
<th id="S4.T7.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.5pt;padding-bottom:1.5pt;">ResNeXt101 32x8d <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx36" title="" class="ltx_ref">Xie etÂ al., 2017</a>]</cite>
</th>
<td id="S4.T7.3.4.3.2" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">33.73</td>
<td id="S4.T7.3.4.3.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">2.03</td>
<td id="S4.T7.3.4.3.4" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">14.18</td>
<td id="S4.T7.3.4.3.5" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">2.09</td>
<td id="S4.T7.3.4.3.6" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">13.32</td>
</tr>
<tr id="S4.T7.3.5.4" class="ltx_tr">
<th id="S4.T7.3.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" style="padding-top:1.5pt;padding-bottom:1.5pt;">EfficientNet-B7 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx33" title="" class="ltx_ref">Tan and Le, 2019</a>]</cite>
</th>
<td id="S4.T7.3.5.4.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.5pt;padding-bottom:1.5pt;">22.51</td>
<td id="S4.T7.3.5.4.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.5pt;padding-bottom:1.5pt;">1.43</td>
<td id="S4.T7.3.5.4.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.5pt;padding-bottom:1.5pt;">8.94</td>
<td id="S4.T7.3.5.4.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.5pt;padding-bottom:1.5pt;">1.40</td>
<td id="S4.T7.3.5.4.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.5pt;padding-bottom:1.5pt;">8.55</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS4.SSS4.p2" class="ltx_para">
<p id="S4.SS4.SSS4.p2.1" class="ltx_p">At first sight, one can conjecture that a RANSAC-based approach is unpromising when presenting around 2 FPS. However, considering an FGR-based process, the results are indeed encouraging, with 8 FPS for the best accurate method, and more than 13 for the others. For many applications that deal with real-time, a frame rate around eight or more is acceptable. We agree that <span id="S4.SS4.SSS4.p2.1.1" class="ltx_text ltx_font_italic">the facto</span> standard for real-time is at least 30 FPS, however, due to the modularity of our proposed pipeline, the stages are independent, and we could use the full execution only to indispensable situations.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<table id="S4.F3.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.F3.1.1" class="ltx_tr">
<td id="S4.F3.1.1.1" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;"><img src="/html/2011.13669/assets/x2.png" id="S4.F3.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="315" height="192" alt="Refer to caption"></td>
</tr>
<tr id="S4.F3.2.3.1" class="ltx_tr">
<td id="S4.F3.2.3.1.1" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">(a) FGR</td>
</tr>
<tr id="S4.F3.2.2" class="ltx_tr">
<td id="S4.F3.2.2.1" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">
<img src="/html/2011.13669/assets/x3.png" id="S4.F3.2.2.1.g1" class="ltx_graphics ltx_img_landscape" width="315" height="192" alt="Refer to caption">
</td>
</tr>
<tr id="S4.F3.2.4.2" class="ltx_tr">
<td id="S4.F3.2.4.2.1" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">(b) RANSAC</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Processing time (in seconds) of each step of the execution of proposed approach. We consider only successfully detected objects on this comparison. (a) presents times referring to the FGR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx38" title="" class="ltx_ref">Zhou etÂ al., 2016</a>]</cite> method, and (b) to RANSAC.</figcaption>
</figure>
<div id="S4.SS4.SSS4.p3" class="ltx_para">
<p id="S4.SS4.SSS4.p3.1" class="ltx_p">An application scenario may include a target objectâ€™s location and pose recovering, for instance, by a robot or a visually impaired person. The system could execute a scheduled procedure, localizing this object adopting only the first stage of the pipeline, in real-time. Then, as the subject approaches the objective, we could execute the second stage, estimating a rough transformation, e.g., once a second. Finally, when the object is next to the user, we can run the full pipeline, including the fine-adjustment stage.</p>
</div>
<div id="S4.SS4.SSS4.p4" class="ltx_para">
<p id="S4.SS4.SSS4.p4.1" class="ltx_p">To investigate more deeply the processing time of a successfully detected object of our pipeline, we summarize how much time takes each substep in <a href="#S4.F3" title="Figure 3 â€£ 4.4.4 Time processing evaluation â€£ 4.4 Results â€£ 4 EXPERIMENTAL RESULTS â€£ Towards real-time object recognition and pose estimation in point clouds" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">FigureÂ 3</span></a>. We can infer that two main steps negatively impact the time processing: classification and feature-based estimation. Regarding the former, the correct selection of the network to extract color features is fundamental to speed-up the whole process, presenting a significant difference between the faster <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx28" title="" class="ltx_ref">Sandler etÂ al., 2018</a>]</cite> and the slower <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx33" title="" class="ltx_ref">Tan and Le, 2019</a>]</cite>. We perceive a considerable impact in time processing when using RANSAC instead of FGR for the feature-based stage. In this implementation, we do not use any concurrent processing, which could significantly improve such time for both coarse pose estimation methods. Our pipeline is highly flexible, and the use of recent proposals may enhance our results on coarse estimation, for instance DGR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">Choy etÂ al., 2020</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS4.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.5 </span>Qualitative results</h4>

<figure id="S4.F4" class="ltx_figure"><img src="/html/2011.13669/assets/figures/qualitative_pose_estimation.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="304" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Qualitative visualizations of successful pose alignment.</figcaption>
</figure>
<div id="S4.SS4.SSS5.p1" class="ltx_para">
<p id="S4.SS4.SSS5.p1.1" class="ltx_p">We provide qualitative visualizations of our proposed method (RANSAC + ICP) in <a href="#S4.F4" title="Figure 4 â€£ 4.4.5 Qualitative results â€£ 4.4 Results â€£ 4 EXPERIMENTAL RESULTS â€£ Towards real-time object recognition and pose estimation in point clouds" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">FigureÂ 4</span></a>. Our method succeeds in aligning several different shaped models, such as planes (<span id="S4.SS4.SSS5.p1.1.1" class="ltx_text ltx_font_italic">cereal box</span>), cylinders (<span id="S4.SS4.SSS5.p1.1.2" class="ltx_text ltx_font_italic">soda can</span>, <span id="S4.SS4.SSS5.p1.1.3" class="ltx_text ltx_font_italic">coffee mugs</span>, and <span id="S4.SS4.SSS5.p1.1.4" class="ltx_text ltx_font_italic">flashlights</span>), and free form models (<span id="S4.SS4.SSS5.p1.1.5" class="ltx_text ltx_font_italic">caps</span>). As we perform a rigid transformation to align objects and scenes, the modelâ€™s choice is fundamental. Examples like the <span id="S4.SS4.SSS5.p1.1.6" class="ltx_text ltx_font_italic">red cap</span> that present a crumple on top harm the alignment estimation. Otherwise, we confirm the robustness of the combination of coarse and fine alignments on the bowl object (bottom row, on the left), partially cropped on the scene cloud. Still, our method infers the pose correctly.</p>
</div>
<div id="S4.SS4.SSS5.p2" class="ltx_para">
<p id="S4.SS4.SSS5.p2.1" class="ltx_p">In <a href="#S5.F5" title="Figure 5 â€£ 5 CONCLUSIONS â€£ Towards real-time object recognition and pose estimation in point clouds" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">FigureÂ 5</span></a>, we present some wrong alignments of our proposals. We can observe that the objectsâ€™ main shape weights a lot on the alignment results. For instance, the <span id="S4.SS4.SSS5.p2.1.1" class="ltx_text ltx_font_italic">mugs</span> had the body well aligned but a misalignment on the handle. We also perceive a flip on the cereal box because of the large plane at the front. The <span id="S4.SS4.SSS5.p2.1.2" class="ltx_text ltx_font_italic">bowl</span> in the rightmost example fails in aligning, though, different from the previous figure, where the method robustly handled a partial view of a <span id="S4.SS4.SSS5.p2.1.3" class="ltx_text ltx_font_italic">bowl</span>, this particular case, have about 50% only of the object visible. The ICP algorithm estimates a locally minimal transformation, and such misalignments may occur because of inaccurate inputs produced by RANSAC/FGR methods. We espy three potential solutions: using novel CNN-based estimation methods, e.g., DGR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">Choy etÂ al., 2020</a>]</cite>; adopting more robust local descriptors to the feature-based registration phase, also considering color-based approaches; increasing the number of selected 2.5D views to enhance pose covering of the scenesâ€™ objects. The last two cited solutions may negatively affect time-performance. Despite the misalignments verified, as we reduce the surface search on the scene cloud, we always have an estimation next or even inside the 3D projection of the 2D bounding box outputted by the detection.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>CONCLUSIONS</h2>

<figure id="S5.F5" class="ltx_figure"><img src="/html/2011.13669/assets/figures/wrong_pose_estimation.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="148" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Qualitative visualizations of wrong pose alignment. From left to right: two examples of coffee mugs with a misoriented handles, flipped cereal box, and a rotated bowl.</figcaption>
</figure>
<div id="S5.p1" class="ltx_para">
<p id="S5.p1.2" class="ltx_p">3D pose estimation is a challenging task, mainly for real-time applications. Sometimes developers must surrender on the precision, aiming the response time. In this paper, we introduced a novel pipeline that proposes to combine the power of color features extractors deep networks, with a local descriptors pipeline to pose estimation in point clouds. We evaluated the detection of objects and achieved almost 83% on an instance situation, in the best case. This precision is also accompanied by a high frame processing rate, arriving up to 90 FPS. The pose estimation rate is plausible for some applications, and by scheduling the stages of our pipeline, we can reach standard real-time processing. We show experimentally massive improvements concerning accuracy and time processing compared to standard approaches for object recognition and pose estimation. Our approach is <math id="S5.p1.1.m1.1" class="ltx_math_unparsed" alttext="3\times" display="inline"><semantics id="S5.p1.1.m1.1a"><mrow id="S5.p1.1.m1.1b"><mn id="S5.p1.1.m1.1.1">3</mn><mo lspace="0.222em" id="S5.p1.1.m1.1.2">Ã—</mo></mrow><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">3\times</annotation></semantics></math> more efficient and <math id="S5.p1.2.m2.1" class="ltx_math_unparsed" alttext="150\times" display="inline"><semantics id="S5.p1.2.m2.1a"><mrow id="S5.p1.2.m2.1b"><mn id="S5.p1.2.m2.1.1">150</mn><mo lspace="0.222em" id="S5.p1.2.m2.1.2">Ã—</mo></mrow><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">150\times</annotation></semantics></math> faster than traditional and grounded methodologies.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Our three-staged detachable pipeline can be used according to the user/application needs: the color feature classification provides object detection in real-time; the feature-based registration estimates an imprecise but sometimes efficient pose of the scenesâ€™ object; the third stage performs a fine alignment of the estimation, resulting in a more accurate result. We believe that our proposalâ€™s adoption may help researchers and the industry develop reliable and time-efficient solutions for scene recognition problems from RGB-D data.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Parallelization strategies can improve time results even more and also different local descriptors and keypoint extractors could support this. Findings on the deepnets architectures can help developing an integrated region proposal and object detection algorithm, and state-of-the-art deep learning methods such as SSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">Liu etÂ al., 2016</a>]</cite>, YOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx24" title="" class="ltx_ref">Redmon etÂ al., 2016</a>]</cite>, and EfficientDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx34" title="" class="ltx_ref">Tan etÂ al., 2020</a>]</cite> enable such potentiality.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>ACKNOWLEDGMENTS</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We would like to thank UTFPR-DV for partly supporting this research work</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">REFERENCES</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal etÂ al., 2014</span>
<span class="ltx_bibblock">
Agrawal, P., Girshick, R., and Malik, J. (2014).

</span>
<span class="ltx_bibblock">Analyzing the performance of multilayer neural networks for object
recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx1.1.1" class="ltx_text ltx_font_italic">European conference on computer vision</span>, pages 329â€“344.
Springer.

</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aldoma etÂ al., 2012a</span>
<span class="ltx_bibblock">
Aldoma, A., Marton, Z., Tombari, F., Wohlkinger, W., Potthast, C., Zeisl, B.,
and Vincze, M. (2012a).

</span>
<span class="ltx_bibblock">Three-dimensional object recognition and 6 DoF pose estimation.

</span>
<span class="ltx_bibblock"><span id="bib.bibx2.1.1" class="ltx_text ltx_font_italic">IEEE Robotics &amp; Automation Magazine</span>, pages 80â€“91.

</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aldoma etÂ al., 2012b</span>
<span class="ltx_bibblock">
Aldoma, A., Marton, Z.-C., Tombari, F., Wohlkinger, W., Potthast, C., Zeisl,
B., Rusu, R.Â B., Gedikli, S., and Vincze, M. (2012b).

</span>
<span class="ltx_bibblock">Tutorial: Point cloud library: Three-dimensional object recognition
and 6 DoF pose estimation.

</span>
<span class="ltx_bibblock"><span id="bib.bibx3.1.1" class="ltx_text ltx_font_italic">IEEE Robotics &amp; Automation Magazine</span>, 19(3):80â€“91.

</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asif etÂ al., 2017</span>
<span class="ltx_bibblock">
Asif, U., Bennamoun, M., and Sohel, F.Â A. (2017).

</span>
<span class="ltx_bibblock">A multi-modal, discriminative and spatially invariant CNN for
RGB-D object labeling.

</span>
<span class="ltx_bibblock"><span id="bib.bibx4.1.1" class="ltx_text ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</span>,
40(9):2051â€“2065.

</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Besl and McKay, 1992</span>
<span class="ltx_bibblock">
Besl, P.Â J. and McKay, N.Â D. (1992).

</span>
<span class="ltx_bibblock">Method for registration of 3-D shapes.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx5.1.1" class="ltx_text ltx_font_italic">Sensor fusion IV: control paradigms and data structures</span>,
volume 1611, pages 586â€“606. International Society for Optics and Photonics.

</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bo etÂ al., 2013</span>
<span class="ltx_bibblock">
Bo, L., Ren, X., and Fox, D. (2013).

</span>
<span class="ltx_bibblock">Unsupervised feature learning for RGB-D based object recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx6.1.1" class="ltx_text ltx_font_italic">Experimental robotics</span>, pages 387â€“402. Springer.

</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caglayan etÂ al., 2020</span>
<span class="ltx_bibblock">
Caglayan, A., Imamoglu, N., Can, A.Â B., and Nakamura, R. (2020).

</span>
<span class="ltx_bibblock">When CNNs meet random RNNs: Towards multi-level analysis for
RGB-D object and scene recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bibx7.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2004.12349</span>.

</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Medioni, 1992</span>
<span class="ltx_bibblock">
Chen, Y. and Medioni, G. (1992).

</span>
<span class="ltx_bibblock">Object modelling by registration of multiple range images.

</span>
<span class="ltx_bibblock"><span id="bib.bibx8.1.1" class="ltx_text ltx_font_italic">Image and vision computing</span>, 10(3):145â€“155.

</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi etÂ al., 2015</span>
<span class="ltx_bibblock">
Choi, S., Zhou, Q.-Y., and Koltun, V. (2015).

</span>
<span class="ltx_bibblock">Robust reconstruction of indoor scenes.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx9.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 5556â€“5565.

</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choy etÂ al., 2020</span>
<span class="ltx_bibblock">
Choy, C., Dong, W., and Koltun, V. (2020).

</span>
<span class="ltx_bibblock">Deep global registration.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx10.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 2514â€“2523.

</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng etÂ al., 2009</span>
<span class="ltx_bibblock">
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009).

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx11.1.1" class="ltx_text ltx_font_italic">2009 IEEE conference on computer vision and pattern
recognition</span>, pages 248â€“255. Ieee.

</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo etÂ al., 2016</span>
<span class="ltx_bibblock">
Guo, Y., Bennamoun, M., Sohel, F., Lu, M., Wan, J., and Kwok, N.Â M. (2016).

</span>
<span class="ltx_bibblock">A comprehensive performance evaluation of 3D local feature
descriptors.

</span>
<span class="ltx_bibblock"><span id="bib.bibx12.1.1" class="ltx_text ltx_font_italic">International Journal of Computer Vision</span>, 116(1):66â€“89.

</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al., 2016</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., and Sun, J. (2016).

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx13.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 770â€“778.

</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hodan etÂ al., 2018</span>
<span class="ltx_bibblock">
Hodan, T., Michel, F., Brachmann, E., Kehl, W., GlentBuch, A., Kraft, D.,
Drost, B., Vidal, J., Ihrke, S., Zabulis, X., etÂ al. (2018).

</span>
<span class="ltx_bibblock">Bop: Benchmark for 6D object pose estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx14.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, pages 19â€“34.

</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou etÂ al., 2017</span>
<span class="ltx_bibblock">
Hou, Q., Cheng, M.-M., Hu, X., Borji, A., Tu, Z., and Torr, P.Â H. (2017).

</span>
<span class="ltx_bibblock">Deeply supervised salient object detection with short connections.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx15.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 3203â€“3212.

</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky etÂ al., 2012</span>
<span class="ltx_bibblock">
Krizhevsky, A., Sutskever, I., and Hinton, G.Â E. (2012).

</span>
<span class="ltx_bibblock">Imagenet classification with deep convolutional neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx16.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, pages
1097â€“1105.

</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai etÂ al., 2011a</span>
<span class="ltx_bibblock">
Lai, K., Bo, L., Ren, X., and Fox, D. (2011a).

</span>
<span class="ltx_bibblock">A large-scale hierarchical multi-view RGB-D object dataset.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx17.1.1" class="ltx_text ltx_font_italic">2011 IEEE international conference on robotics and
automation</span>, pages 1817â€“1824. IEEE.

</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai etÂ al., 2011b</span>
<span class="ltx_bibblock">
Lai, K., Bo, L., Ren, X., and Fox, D. (2011b).

</span>
<span class="ltx_bibblock">Sparse distance learning for object recognition combining rgb and
depth information.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx18.1.1" class="ltx_text ltx_font_italic">2011 IEEE International Conference on Robotics and
Automation</span>, pages 4007â€“4013. IEEE.

</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al., 2018</span>
<span class="ltx_bibblock">
Liu, H., Li, F., Xu, X., and Sun, F. (2018).

</span>
<span class="ltx_bibblock">Multi-modal local receptive field extreme learning machine for object
recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bibx19.1.1" class="ltx_text ltx_font_italic">Neurocomputing</span>, 277:4â€“11.

</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al., 2016</span>
<span class="ltx_bibblock">
Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., and Berg,
A.Â C. (2016).

</span>
<span class="ltx_bibblock">SSD: Single shot multibox detector.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx20.1.1" class="ltx_text ltx_font_italic">European conference on computer vision</span>, pages 21â€“37.
Springer.

</span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marcon etÂ al., 2019</span>
<span class="ltx_bibblock">
Marcon, M., Spezialetti, R., Salti, S., Silva, L., and DiÂ Stefano, L. (2019).

</span>
<span class="ltx_bibblock">Boosting object recognition in point clouds by saliency detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx21.1.1" class="ltx_text ltx_font_italic">International Conference on Image Analysis and Processing</span>,
pages 321â€“331. Springer.

</span>
</li>
<li id="bib.bibx22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouadiay etÂ al., 2016</span>
<span class="ltx_bibblock">
Ouadiay, F.Â Z., Zrira, N., Bouyakhf, E.Â H., and Himmi, M.Â M. (2016).

</span>
<span class="ltx_bibblock">3d object categorization and recognition based on deep belief
networks and point clouds.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx22.1.1" class="ltx_text ltx_font_italic">Proceedings of the 13th International Conference on
Informatics in Control, Automation and Robotics - Volume 2: ICINCO,</span>, pages
311â€“318. INSTICC, SciTePress.

</span>
</li>
<li id="bib.bibx23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park etÂ al., 2017</span>
<span class="ltx_bibblock">
Park, J., Zhou, Q.-Y., and Koltun, V. (2017).

</span>
<span class="ltx_bibblock">Colored point cloud registration revisited.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx23.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision</span>, pages 143â€“152.

</span>
</li>
<li id="bib.bibx24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Redmon etÂ al., 2016</span>
<span class="ltx_bibblock">
Redmon, J., Divvala, S., Girshick, R., and Farhadi, A. (2016).

</span>
<span class="ltx_bibblock">You only look once: Unified, real-time object detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx24.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 779â€“788.

</span>
</li>
<li id="bib.bibx25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rusu etÂ al., 2009</span>
<span class="ltx_bibblock">
Rusu, R.Â B., Blodow, N., and Beetz, M. (2009).

</span>
<span class="ltx_bibblock">Fast point feature histograms (FPFH) for 3D registration.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx25.1.1" class="ltx_text ltx_font_italic">2009 IEEE international conference on robotics and
automation</span>, pages 3212â€“3217. IEEE.

</span>
</li>
<li id="bib.bibx26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rusu etÂ al., 2008</span>
<span class="ltx_bibblock">
Rusu, R.Â B., Blodow, N., Marton, Z.Â C., and Beetz, M. (2008).

</span>
<span class="ltx_bibblock">Aligning point cloud views using persistent feature histograms.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx26.1.1" class="ltx_text ltx_font_italic">2008 IEEE/RSJ international conference on intelligent robots
and systems</span>, pages 3384â€“3391. IEEE.

</span>
</li>
<li id="bib.bibx27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salti etÂ al., 2014</span>
<span class="ltx_bibblock">
Salti, S., Tombari, F., and DiÂ Stefano, L. (2014).

</span>
<span class="ltx_bibblock">SHOT: Unique signatures of histograms for surface and texture
description.

</span>
<span class="ltx_bibblock"><span id="bib.bibx27.1.1" class="ltx_text ltx_font_italic">Computer Vision and Image Understanding</span>, 125:251â€“264.

</span>
</li>
<li id="bib.bibx28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sandler etÂ al., 2018</span>
<span class="ltx_bibblock">
Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L.-C. (2018).

</span>
<span class="ltx_bibblock">Mobilenetv2: Inverted residuals and linear bottlenecks.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx28.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 4510â€“4520.

</span>
</li>
<li id="bib.bibx29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwarz etÂ al., 2015</span>
<span class="ltx_bibblock">
Schwarz, M., Schulz, H., and Behnke, S. (2015).

</span>
<span class="ltx_bibblock">RGB-D object recognition and pose estimation based on pre-trained
convolutional neural network features.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx29.1.1" class="ltx_text ltx_font_italic">2015 IEEE international conference on robotics and automation
(ICRA)</span>, pages 1329â€“1335. IEEE.

</span>
</li>
<li id="bib.bibx30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan and Zisserman, 2015</span>
<span class="ltx_bibblock">
Simonyan, K. and Zisserman, A. (2015).

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx30.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>.

</span>
</li>
<li id="bib.bibx31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Szegedy etÂ al., 2015</span>
<span class="ltx_bibblock">
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,
Vanhoucke, V., and Rabinovich, A. (2015).

</span>
<span class="ltx_bibblock">Going deeper with convolutions.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx31.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 1â€“9.

</span>
</li>
<li id="bib.bibx32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Szegedy etÂ al., 2016</span>
<span class="ltx_bibblock">
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. (2016).

</span>
<span class="ltx_bibblock">Rethinking the inception architecture for computer vision.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx32.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 2818â€“2826.

</span>
</li>
<li id="bib.bibx33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Le, 2019</span>
<span class="ltx_bibblock">
Tan, M. and Le, Q. (2019).

</span>
<span class="ltx_bibblock">Efficientnet: Rethinking model scaling for convolutional neural
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx33.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
6105â€“6114.

</span>
</li>
<li id="bib.bibx34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan etÂ al., 2020</span>
<span class="ltx_bibblock">
Tan, M., Pang, R., and Le, Q.Â V. (2020).

</span>
<span class="ltx_bibblock">Efficientdet: Scalable and efficient object detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx34.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 10781â€“10790.

</span>
</li>
<li id="bib.bibx35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vock etÂ al., 2019</span>
<span class="ltx_bibblock">
Vock, R., Dieckmann, A., Ochmann, S., and Klein, R. (2019).

</span>
<span class="ltx_bibblock">Fast template matching and pose estimation in 3D point clouds.

</span>
<span class="ltx_bibblock"><span id="bib.bibx35.1.1" class="ltx_text ltx_font_italic">Computers &amp; Graphics</span>, 79:36â€“45.

</span>
</li>
<li id="bib.bibx36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie etÂ al., 2017</span>
<span class="ltx_bibblock">
Xie, S., Girshick, R., DollÃ¡r, P., Tu, Z., and He, K. (2017).

</span>
<span class="ltx_bibblock">Aggregated residual transformations for deep neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx36.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 1492â€“1500.

</span>
</li>
<li id="bib.bibx37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zaki etÂ al., 2019</span>
<span class="ltx_bibblock">
Zaki, H.Â F., Shafait, F., and Mian, A. (2019).

</span>
<span class="ltx_bibblock">Viewpoint invariant semantic object and scene categorization with
RGB-D sensors.

</span>
<span class="ltx_bibblock"><span id="bib.bibx37.1.1" class="ltx_text ltx_font_italic">Autonomous Robots</span>, 43(4):1005â€“1022.

</span>
</li>
<li id="bib.bibx38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou etÂ al., 2016</span>
<span class="ltx_bibblock">
Zhou, Q.-Y., Park, J., and Koltun, V. (2016).

</span>
<span class="ltx_bibblock">Fast global registration.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx38.1.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision</span>, pages 766â€“782.
Springer.

</span>
</li>
<li id="bib.bibx39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zia etÂ al., 2017</span>
<span class="ltx_bibblock">
Zia, S., Yuksel, B., Yuret, D., and Yemez, Y. (2017).

</span>
<span class="ltx_bibblock">RGB-D object recognition using deep convolutional neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx39.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision Workshops</span>, pages 896â€“903.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2011.13668" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2011.13669" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2011.13669">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2011.13669" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2011.13670" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 05:49:52 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
