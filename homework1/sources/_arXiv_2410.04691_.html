<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning</title>
<!--Generated on Mon Oct  7 02:09:27 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.04691v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S1" title="In Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S1.SS0.SSS0.Px1" title="In 1 Introduction â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title">Implicit Pattern Detection dataset.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S1.SS0.SSS0.Px2" title="In 1 Introduction â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title">Ability Comparison.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S1.SS0.SSS0.Px3" title="In 1 Introduction â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title">Mechanism explanation.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S2" title="In Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S2.SS0.SSS0.Px1" title="In 2 Background â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title">Transformer.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S2.SS0.SSS0.Px2" title="In 2 Background â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title">Fine-tuning.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S2.SS0.SSS0.Px3" title="In 2 Background â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title">In-Context Learning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S3" title="In Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Implicit Pattern Detection Test</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S3.SS1" title="In 3 Implicit Pattern Detection Test â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Tasks</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S3.SS1.SSS0.Px1" title="In 3.1 Tasks â€£ 3 Implicit Pattern Detection Test â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title">Task 1: Expression CalculationÂ <cite class="ltx_cite ltx_citemacro_cite">Imani etÂ al. (<span class="ltx_ref">2023</span>); Yuan etÂ al. (<span class="ltx_ref">2023</span>); Yue etÂ al. (<span class="ltx_ref">2023</span>); He-Yueya etÂ al. (<span class="ltx_ref">2023</span>)</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S3.SS1.SSS0.Px2" title="In 3.1 Tasks â€£ 3 Implicit Pattern Detection Test â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title">Task 2: Code ReadingÂ <cite class="ltx_cite ltx_citemacro_cite">Fang etÂ al. (<span class="ltx_ref">2024</span>)</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S3.SS1.SSS0.Px3" title="In 3.1 Tasks â€£ 3 Implicit Pattern Detection Test â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title">Task 3: Boolean FunctionsÂ <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<span class="ltx_ref">2024</span>)</cite></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S3.SS1.SSS0.Px4" title="In 3.1 Tasks â€£ 3 Implicit Pattern Detection Test â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title">Task 4: Relation ReasoningÂ <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al. (<span class="ltx_ref">2024</span>)</cite></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S3.SS2" title="In 3 Implicit Pattern Detection Test â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Settings</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S3.SS2.SSS0.Px1" title="In 3.2 Settings â€£ 3 Implicit Pattern Detection Test â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title">Accuracy.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S3.SS2.SSS0.Px2" title="In 3.2 Settings â€£ 3 Implicit Pattern Detection Test â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title">Misleading Data.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S3.SS2.SSS0.Px3" title="In 3.2 Settings â€£ 3 Implicit Pattern Detection Test â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title">Out-Of-Distribution Data.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S3.SS2.SSS0.Px4" title="In 3.2 Settings â€£ 3 Implicit Pattern Detection Test â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title">Models.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S3.SS2.SSS0.Px5" title="In 3.2 Settings â€£ 3 Implicit Pattern Detection Test â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title">Data Format.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S3.SS2.SSS0.Px6" title="In 3.2 Settings â€£ 3 Implicit Pattern Detection Test â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title">Training Details.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S4" title="In Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results and Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S4.SS1" title="In 4 Results and Analysis â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>ICL <em class="ltx_emph ltx_font_italic">v.s.</em>Â Fine-tuning: Accuracy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S4.SS2" title="In 4 Results and Analysis â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>ICL <em class="ltx_emph ltx_font_italic">v.s.</em>Â Fine-tuning: Robustness without Implicit Pattern</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S4.SS3" title="In 4 Results and Analysis â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>ICL <em class="ltx_emph ltx_font_italic">v.s.</em>Â Fine-tuning: Out-Of-Distribution Implicit Patterns</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S4.SS4" title="In 4 Results and Analysis â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>How Much Fine-tuning Do We Need?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S4.SS5" title="In 4 Results and Analysis â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Comparison of Fine-tuning with PEFT Methods</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S5" title="In Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Explanation of ICLâ€™s Victory: Circuits Shift Theory</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S5.SS1" title="In 5 Explanation of ICLâ€™s Victory: Circuits Shift Theory â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Method for Identifying Circuit Shift</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S5.SS2" title="In 5 Explanation of ICLâ€™s Victory: Circuits Shift Theory â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Circuits Shift in LLMs for Implicit Pattern Detection</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S6" title="In Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S6.SS0.SSS0.Px1" title="In 6 Related Work â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title">Implicit Pattern Discovery</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S6.SS0.SSS0.Px2" title="In 6 Related Work â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title">ICL <em class="ltx_emph ltx_font_italic">v.s.</em>Â Fine-tuning Difference</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S7" title="In Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#A1" title="In Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Data Format and Example</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#A2" title="In Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Misleading Data Construction</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#A2.SS0.SSS0.Px1" title="In Appendix B Misleading Data Construction â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title">Expression.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#A2.SS0.SSS0.Px2" title="In Appendix B Misleading Data Construction â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title">Code.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#A2.SS0.SSS0.Px3" title="In Appendix B Misleading Data Construction â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title">Relation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#A2.SS0.SSS0.Px4" title="In Appendix B Misleading Data Construction â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title">Boolean.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#A3" title="In Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>OOD data Construction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#A4" title="In Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Circuits</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#A4.SS0.SSS0.Px1" title="In Appendix D Circuits â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title">Circuits</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#A4.SS0.SSS0.Px2" title="In Appendix D Circuits â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title">Activation Patching</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#A5" title="In Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>A detailed Definition of Implicit Pattern Detection</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Deeper Insights Without Updates:
<br class="ltx_break"/>The Power of In-Context Learning Over Fine-Tuning
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_text ltx_font_bold" id="id1.1.id1">Qingyu Yin<sup class="ltx_sup" id="id1.1.id1.1">1</sup></span> Â <span class="ltx_text ltx_font_bold" id="id2.2.id2">Xuzheng He<sup class="ltx_sup" id="id2.2.id2.1">2</sup></span> Â <span class="ltx_text ltx_font_bold" id="id3.3.id3">Luoao Deng<sup class="ltx_sup" id="id3.3.id3.1">3</sup></span> Â <span class="ltx_text ltx_font_bold" id="id4.4.id4">Chak Tou Leong<sup class="ltx_sup" id="id4.4.id4.1">4</sup></span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id5.5.id5">Fan Wang<sup class="ltx_sup" id="id5.5.id5.1">1</sup></span> Â <span class="ltx_text ltx_font_bold" id="id6.6.id6">Yanzhao Yan<sup class="ltx_sup" id="id6.6.id6.1">1</sup></span> Â <span class="ltx_text ltx_font_bold" id="id7.7.id7">Xiaoyu Shen<sup class="ltx_sup" id="id7.7.id7.1">5</sup>*</span> Â <span class="ltx_text ltx_font_bold" id="id8.8.id8">Qiang Zhang<sup class="ltx_sup" id="id8.8.id8.1">1</sup>*</span>
<br class="ltx_break"/><sup class="ltx_sup" id="id9.9.id9">1</sup>Zhejiang University, Â <sup class="ltx_sup" id="id10.10.id10">2</sup>Peking University, Â <sup class="ltx_sup" id="id11.11.id11">3</sup>Wuhan University,
<br class="ltx_break"/><sup class="ltx_sup" id="id12.12.id12">4</sup> The Hong Kong Polytechnic University, 
<br class="ltx_break"/><sup class="ltx_sup" id="id13.13.id13">5</sup> Digital Twin Institute, Eastern Institute of Technology, Ningbo

<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id14.14.id14">Corresponding: {qingyu.yin, qiang.zhang}@zju.edu.cn Â xyshen@eit.edu.cn</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id15.id1">Fine-tuning and in-context learning (ICL) are two prevalent methods in imbuing large language models with task-specific knowledge. It is commonly believed that fine-tuning can surpass ICL given sufficient training samples as it allows the model to adjust its internal parameters based on the data. However, this paper presents a counterintuitive finding: For tasks with implicit patterns, ICL captures these patterns significantly better than fine-tuning. We developed several datasets featuring implicit patterns, such as sequences determining answers through parity or identifying reducible terms in calculations. We then evaluated the modelsâ€™ understanding of these patterns under both fine-tuning and ICL across models ranging from 0.5B to 7B parameters. The results indicate that models employing ICL can quickly grasp deep patterns and significantly improve accuracy. In contrast, fine-tuning, despite utilizing thousands of times more training samples than ICL, achieved only limited improvements. We also proposed circuit shift theory from a mechanistic interpretabilityâ€™s view to explain why ICL winsÂ <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Code is available at <a class="ltx_ref ltx_href" href="https://github.com/MikaStars39/ICLvsFinetune" title="">here</a></span></span></span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">Deeper Insights Without Updates:
<br class="ltx_break"/>The Power of In-Context Learning Over Fine-Tuning</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tr" id="p1.1.2.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1">
Qingyu Yin<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1">1</sup> Â Xuzheng He<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.2">2</sup> Â Luoao Deng<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.3">3</sup> Â Chak Tou Leong<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.4">4</sup></span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.1.1">Fan Wang<sup class="ltx_sup" id="p1.1.2.1.1.2.1.1.1">1</sup></span> Â <span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.1.2">Yanzhao Yan<sup class="ltx_sup" id="p1.1.2.1.1.2.1.2.1">1</sup></span> Â <span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.1.3">Xiaoyu Shen<sup class="ltx_sup" id="p1.1.2.1.1.2.1.3.1">5</sup>*</span> Â <span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.1.4">Qiang Zhang<sup class="ltx_sup" id="p1.1.2.1.1.2.1.4.1">1</sup>*</span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.1"><sup class="ltx_sup" id="p1.1.2.1.1.3.1.1">1</sup>Zhejiang University, Â <sup class="ltx_sup" id="p1.1.2.1.1.3.1.2">2</sup>Peking University, Â <sup class="ltx_sup" id="p1.1.2.1.1.3.1.3">3</sup>Wuhan University,</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.4">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.4.1"><sup class="ltx_sup" id="p1.1.2.1.1.4.1.1">4</sup> The Hong Kong Polytechnic University,</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.5">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.5.1"><sup class="ltx_sup" id="p1.1.2.1.1.5.1.1">5</sup> Digital Twin Institute, Eastern Institute of Technology, Ningbo</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.6">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.6.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.6.1.1">Corresponding: {qingyu.yin, qiang.zhang}@zju.edu.cn Â xyshen@eit.edu.cn</span></span></span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Adapting pre-trained models to specific tasks or domains is commonly achieved through fine-tuning Â <cite class="ltx_cite ltx_citemacro_cite">Hu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib16" title="">2023</a>); Peters etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib23" title="">2019</a>)</cite> or in-context learningÂ <cite class="ltx_cite ltx_citemacro_cite">Gan and Mori (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib11" title="">2023</a>)</cite>. Fine-tuning, a well-established method, involves further training a pre-trained model on a smaller, domain-specific dataset, directly updating the modelâ€™s parameters to retain improvements across various contexts and scenarios. In contrast, in-context learning (ICL) enhances task performance by incorporating task-specific examples into prompts, guiding the model in task completion without altering its parameters during training.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">There has been much debate about the pros and cons of fine-tuning and in-context learning. Fine-tuning is praised for its ability to bring permanent memorization to modelsÂ <cite class="ltx_cite ltx_citemacro_cite">Hu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib16" title="">2023</a>)</cite>, and it can perform well even with a small amount of training dataÂ <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib20" title="">2022</a>)</cite>. However, critics argue that fine-tuning demands substantial computational resourcesÂ <cite class="ltx_cite ltx_citemacro_cite">Hu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib15" title="">2021</a>)</cite> and can encounter issues such as catastrophic forgettingÂ <cite class="ltx_cite ltx_citemacro_cite">Zhai etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib36" title="">2023</a>)</cite>. This conserves computational resources but necessitates longer prompts and incurs higher inference costs.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="595" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>(a) A simple example of an implicit pattern detection task. The given problem (arithmetic expression calculation task in this figure) can be solved in either a formal way, e.g., directly calculating, or by exploiting the detected implicit pattern as a shortcut.
(b) Illustration of implicit pattern detection for in-context learning and fine-tuning. For ICL, several examples with answers are given in context, and a further new question is used to test accuracy. For fine-tuning, LLM learns from single examples using parameter update methods like full-parameter fine-tuning or PEFT methods.</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">How about ICL? It is favored for its training-free natureÂ <cite class="ltx_cite ltx_citemacro_cite">Dong etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib8" title="">2022</a>)</cite>, allowing prompts to be easily changed for adaptation to other domains without re-trainingÂ <cite class="ltx_cite ltx_citemacro_cite">Min etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib21" title="">2022</a>)</cite>. Other works<cite class="ltx_cite ltx_citemacro_cite">Bhattamishra etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib3" title="">2023</a>)</cite> showed that ICL can help the model uniquely identify a discrete function sample-efficiently. ReseachÂ <cite class="ltx_cite ltx_citemacro_cite">Reddy (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib26" title="">2023</a>)</cite> showed that ICL is driven by the abrupt emergence of an induction head, which subsequently competes with in-weights learning. Other works<cite class="ltx_cite ltx_citemacro_cite">Shen etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib27" title="">2024</a>)</cite> observed that ICL and gradient descent modify the output distribution of language models differently. Despite these advantages, ICL is limited by context length restrictions and incurs higher costs during each inference stage due to the longer prompts required.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Essentially, the primary distinction between fine-tuning and ICL lies in parameter updating; all fine-tuning methods modify the modelâ€™s parameters. It might seem, therefore, that ICLâ€™s impact is less profound. However, our research reveals a counterintuitive finding: <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">for datasets with implicit patterns, ICL is more adept at uncovering these latent patterns than fine-tuning.</span></p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To investigate this phenomenon, we designed datasets containing implicit patterns across various domains, including two mathematical tasks: expression calculation and boolean function. One textual task: relation reasoning, and one code reading task. These domains share a common trait: the presence of implicit patterns that can simplify problem-solving. We evaluated LLMsâ€™ capability to recognize such patterns with these datasets. Our findings include: (1) Both fine-tuning and ICL could detect and utilize implicit patterns, resulting in increased test accuracy. (2) ICL performed much better than fine-tuning in implicit pattern detection, <em class="ltx_emph ltx_font_italic" id="S1.p5.1.1">e.g., </em>Â ICL-based models enjoyed higher test accuracy. (3) ICL also showed strong performance in robustness tests and OOD data tests. Our experiments demonstrate that the ability of LLMs to leverage implicit patterns significantly enhances their problem-solving capabilities, providing a clear advantage for tasks involving complex data structures.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Understanding the operational principles of LLMs is crucial for their safety and ethical implications and can further promote improvements. Therefore, we delved deeper into the mechanisms behind this phenomenon. From a mechanistic interpretability perspectiveÂ <cite class="ltx_cite ltx_citemacro_cite">Reddy (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib26" title="">2023</a>)</cite>, we proposed the <span class="ltx_text ltx_font_bold" id="S1.p6.1.1">Circuit Shift</span> theory. Circuits are certain groups of attention heads and MLP layersÂ <cite class="ltx_cite ltx_citemacro_cite">Conmy etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib6" title="">2023</a>)</cite>. A shift in circuits typically represents the model adopting a different method in problem-solving. Our findings indicated that ICL resulted in a larger-scale circuit shift compared to fine-tuning, which means that with ICL, the model changed its problem-solving method more significantly for implicit pattern detection and utilization. We also provided a visualized heatmap of circuits for detailed observation.
In summary, our contributions are threefold:</p>
</div>
<section class="ltx_paragraph" id="S1.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Implicit Pattern Detection dataset.</h4>
<div class="ltx_para" id="S1.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S1.SS0.SSS0.Px1.p1.1">We defined and illustrated the implicit pattern detection task, then developed a dataset across mathematics (expression calculation, boolean function), textual reasoning (relation test) and code (output guessing).</p>
</div>
</section>
<section class="ltx_paragraph" id="S1.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Ability Comparison.</h4>
<div class="ltx_para" id="S1.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S1.SS0.SSS0.Px2.p1.1">We presented a counterintuitive finding: LLMs with in-context learning detected implicit patterns much better than fine-tuned ones. We extensively tested this capability on models ranging from 0.5B to 7B parameters.</p>
</div>
</section>
<section class="ltx_paragraph" id="S1.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Mechanism explanation. </h4>
<div class="ltx_para" id="S1.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S1.SS0.SSS0.Px3.p1.1">We analyzed the principles behind the implicit finding mechanism. And we proposed circuit shift theory to explain why ICL finds implicit patterns better than fine-tuning.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="299" id="S1.F2.g1" src="x2.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Examples of implicit pattern detection for four reasoning tasks. The implicit pattern, once detected, can reward the model with reduced computation to arrive at the answer.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Transformer.</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.11">TransformerÂ <cite class="ltx_cite ltx_citemacro_cite">Vaswani etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib31" title="">2017</a>)</cite> is the cornerstone architecture for LLMs nowadays, with its breathtaking ability in parallel training and SOTA performance. One Transformer model <math alttext="f_{\text{trf}}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="S2.SS0.SSS0.Px1.p1.1.m1.1a"><msub id="S2.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S2.SS0.SSS0.Px1.p1.1.m1.1.1.2" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml">f</mi><mtext id="S2.SS0.SSS0.Px1.p1.1.m1.1.1.3" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1.3a.cmml">trf</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.1.m1.1b"><apply id="S2.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1.2">ğ‘“</ci><ci id="S2.SS0.SSS0.Px1.p1.1.m1.1.1.3a.cmml" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1.3"><mtext id="S2.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1.3">trf</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.1.m1.1c">f_{\text{trf}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.1.m1.1d">italic_f start_POSTSUBSCRIPT trf end_POSTSUBSCRIPT</annotation></semantics></math> usually consists multiple of Transformer layers <math alttext="f_{\text{layer}}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.2.m2.1"><semantics id="S2.SS0.SSS0.Px1.p1.2.m2.1a"><msub id="S2.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.2" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml">f</mi><mtext id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3a.cmml">layer</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.2.m2.1b"><apply id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.2">ğ‘“</ci><ci id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3a.cmml" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3"><mtext id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml" mathsize="70%" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3">layer</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.2.m2.1c">f_{\text{layer}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.2.m2.1d">italic_f start_POSTSUBSCRIPT layer end_POSTSUBSCRIPT</annotation></semantics></math> and an embedding layer <math alttext="f_{\text{emb}}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.3.m3.1"><semantics id="S2.SS0.SSS0.Px1.p1.3.m3.1a"><msub id="S2.SS0.SSS0.Px1.p1.3.m3.1.1" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.1.cmml"><mi id="S2.SS0.SSS0.Px1.p1.3.m3.1.1.2" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.1.2.cmml">f</mi><mtext id="S2.SS0.SSS0.Px1.p1.3.m3.1.1.3" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.1.3a.cmml">emb</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.3.m3.1b"><apply id="S2.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.1.2">ğ‘“</ci><ci id="S2.SS0.SSS0.Px1.p1.3.m3.1.1.3a.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.1.3"><mtext id="S2.SS0.SSS0.Px1.p1.3.m3.1.1.3.cmml" mathsize="70%" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.1.3">emb</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.3.m3.1c">f_{\text{emb}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.3.m3.1d">italic_f start_POSTSUBSCRIPT emb end_POSTSUBSCRIPT</annotation></semantics></math>. For an input sequence (typically IDs after tokenization) <math alttext="\boldsymbol{X_{0}}\in\mathbb{R}^{n\times 1}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.4.m4.1"><semantics id="S2.SS0.SSS0.Px1.p1.4.m4.1a"><mrow id="S2.SS0.SSS0.Px1.p1.4.m4.1.1" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.cmml"><msub id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.2" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.2.cmml"><mi id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.2.2" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.2.2.cmml">ğ‘¿</mi><mn id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.2.3" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.2.3.cmml">ğŸ</mn></msub><mo id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.1" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.1.cmml">âˆˆ</mo><msup id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3.cmml"><mi id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3.2" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3.2.cmml">â„</mi><mrow id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3.3" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3.3.cmml"><mi id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3.3.2" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3.3.2.cmml">n</mi><mo id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3.3.1.cmml">Ã—</mo><mn id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3.3.3" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3.3.3.cmml">1</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.4.m4.1b"><apply id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1"><in id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.1"></in><apply id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.2.1.cmml" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.2">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.2.2.cmml" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.2.2">ğ‘¿</ci><cn id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.2.3.cmml" type="integer" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.2.3">0</cn></apply><apply id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3.cmml" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3.1.cmml" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3">superscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3.2.cmml" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3.2">â„</ci><apply id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3.3.cmml" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3.3"><times id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3.3.1.cmml" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3.3.1"></times><ci id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3.3.2.cmml" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3.3.2">ğ‘›</ci><cn id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3.3.3.cmml" type="integer" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.4.m4.1c">\boldsymbol{X_{0}}\in\mathbb{R}^{n\times 1}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.4.m4.1d">bold_italic_X start_POSTSUBSCRIPT bold_0 end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n Ã— 1 end_POSTSUPERSCRIPT</annotation></semantics></math> with length <math alttext="n" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.5.m5.1"><semantics id="S2.SS0.SSS0.Px1.p1.5.m5.1a"><mi id="S2.SS0.SSS0.Px1.p1.5.m5.1.1" xref="S2.SS0.SSS0.Px1.p1.5.m5.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.5.m5.1b"><ci id="S2.SS0.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.5.m5.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.5.m5.1c">n</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.5.m5.1d">italic_n</annotation></semantics></math>, it first passes through an embedding layer <math alttext="f_{\text{emb}}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.6.m6.1"><semantics id="S2.SS0.SSS0.Px1.p1.6.m6.1a"><msub id="S2.SS0.SSS0.Px1.p1.6.m6.1.1" xref="S2.SS0.SSS0.Px1.p1.6.m6.1.1.cmml"><mi id="S2.SS0.SSS0.Px1.p1.6.m6.1.1.2" xref="S2.SS0.SSS0.Px1.p1.6.m6.1.1.2.cmml">f</mi><mtext id="S2.SS0.SSS0.Px1.p1.6.m6.1.1.3" xref="S2.SS0.SSS0.Px1.p1.6.m6.1.1.3a.cmml">emb</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.6.m6.1b"><apply id="S2.SS0.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.6.m6.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.6.m6.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.6.m6.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.6.m6.1.1.2">ğ‘“</ci><ci id="S2.SS0.SSS0.Px1.p1.6.m6.1.1.3a.cmml" xref="S2.SS0.SSS0.Px1.p1.6.m6.1.1.3"><mtext id="S2.SS0.SSS0.Px1.p1.6.m6.1.1.3.cmml" mathsize="70%" xref="S2.SS0.SSS0.Px1.p1.6.m6.1.1.3">emb</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.6.m6.1c">f_{\text{emb}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.6.m6.1d">italic_f start_POSTSUBSCRIPT emb end_POSTSUBSCRIPT</annotation></semantics></math> with hidden state size <math alttext="d" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.7.m7.1"><semantics id="S2.SS0.SSS0.Px1.p1.7.m7.1a"><mi id="S2.SS0.SSS0.Px1.p1.7.m7.1.1" xref="S2.SS0.SSS0.Px1.p1.7.m7.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.7.m7.1b"><ci id="S2.SS0.SSS0.Px1.p1.7.m7.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.7.m7.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.7.m7.1c">d</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.7.m7.1d">italic_d</annotation></semantics></math>, then passes all the Transformer layers, and finally gets an output <math alttext="\boldsymbol{O_{l}}\in\mathbb{R}^{n\times d}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.8.m8.1"><semantics id="S2.SS0.SSS0.Px1.p1.8.m8.1a"><mrow id="S2.SS0.SSS0.Px1.p1.8.m8.1.1" xref="S2.SS0.SSS0.Px1.p1.8.m8.1.1.cmml"><msub id="S2.SS0.SSS0.Px1.p1.8.m8.1.1.2" xref="S2.SS0.SSS0.Px1.p1.8.m8.1.1.2.cmml"><mi id="S2.SS0.SSS0.Px1.p1.8.m8.1.1.2.2" xref="S2.SS0.SSS0.Px1.p1.8.m8.1.1.2.2.cmml">ğ‘¶</mi><mi id="S2.SS0.SSS0.Px1.p1.8.m8.1.1.2.3" xref="S2.SS0.SSS0.Px1.p1.8.m8.1.1.2.3.cmml">ğ’</mi></msub><mo id="S2.SS0.SSS0.Px1.p1.8.m8.1.1.1" xref="S2.SS0.SSS0.Px1.p1.8.m8.1.1.1.cmml">âˆˆ</mo><msup id="S2.SS0.SSS0.Px1.p1.8.m8.1.1.3" xref="S2.SS0.SSS0.Px1.p1.8.m8.1.1.3.cmml"><mi id="S2.SS0.SSS0.Px1.p1.8.m8.1.1.3.2" xref="S2.SS0.SSS0.Px1.p1.8.m8.1.1.3.2.cmml">â„</mi><mrow id="S2.SS0.SSS0.Px1.p1.8.m8.1.1.3.3" xref="S2.SS0.SSS0.Px1.p1.8.m8.1.1.3.3.cmml"><mi id="S2.SS0.SSS0.Px1.p1.8.m8.1.1.3.3.2" xref="S2.SS0.SSS0.Px1.p1.8.m8.1.1.3.3.2.cmml">n</mi><mo id="S2.SS0.SSS0.Px1.p1.8.m8.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.SS0.SSS0.Px1.p1.8.m8.1.1.3.3.1.cmml">Ã—</mo><mi id="S2.SS0.SSS0.Px1.p1.8.m8.1.1.3.3.3" xref="S2.SS0.SSS0.Px1.p1.8.m8.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.8.m8.1b"><apply id="S2.SS0.SSS0.Px1.p1.8.m8.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.8.m8.1.1"><in id="S2.SS0.SSS0.Px1.p1.8.m8.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.8.m8.1.1.1"></in><apply id="S2.SS0.SSS0.Px1.p1.8.m8.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.8.m8.1.1.2"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.8.m8.1.1.2.1.cmml" xref="S2.SS0.SSS0.Px1.p1.8.m8.1.1.2">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.8.m8.1.1.2.2.cmml" xref="S2.SS0.SSS0.Px1.p1.8.m8.1.1.2.2">ğ‘¶</ci><ci id="S2.SS0.SSS0.Px1.p1.8.m8.1.1.2.3.cmml" xref="S2.SS0.SSS0.Px1.p1.8.m8.1.1.2.3">ğ’</ci></apply><apply id="S2.SS0.SSS0.Px1.p1.8.m8.1.1.3.cmml" xref="S2.SS0.SSS0.Px1.p1.8.m8.1.1.3"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.8.m8.1.1.3.1.cmml" xref="S2.SS0.SSS0.Px1.p1.8.m8.1.1.3">superscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.8.m8.1.1.3.2.cmml" xref="S2.SS0.SSS0.Px1.p1.8.m8.1.1.3.2">â„</ci><apply id="S2.SS0.SSS0.Px1.p1.8.m8.1.1.3.3.cmml" xref="S2.SS0.SSS0.Px1.p1.8.m8.1.1.3.3"><times id="S2.SS0.SSS0.Px1.p1.8.m8.1.1.3.3.1.cmml" xref="S2.SS0.SSS0.Px1.p1.8.m8.1.1.3.3.1"></times><ci id="S2.SS0.SSS0.Px1.p1.8.m8.1.1.3.3.2.cmml" xref="S2.SS0.SSS0.Px1.p1.8.m8.1.1.3.3.2">ğ‘›</ci><ci id="S2.SS0.SSS0.Px1.p1.8.m8.1.1.3.3.3.cmml" xref="S2.SS0.SSS0.Px1.p1.8.m8.1.1.3.3.3">ğ‘‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.8.m8.1c">\boldsymbol{O_{l}}\in\mathbb{R}^{n\times d}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.8.m8.1d">bold_italic_O start_POSTSUBSCRIPT bold_italic_l end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n Ã— italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> with <math alttext="l" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.9.m9.1"><semantics id="S2.SS0.SSS0.Px1.p1.9.m9.1a"><mi id="S2.SS0.SSS0.Px1.p1.9.m9.1.1" xref="S2.SS0.SSS0.Px1.p1.9.m9.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.9.m9.1b"><ci id="S2.SS0.SSS0.Px1.p1.9.m9.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.9.m9.1.1">ğ‘™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.9.m9.1c">l</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.9.m9.1d">italic_l</annotation></semantics></math> layers:
<math alttext="\boldsymbol{O_{l}}=f_{\text{trf}}(\boldsymbol{X_{0}})=\left(\bigcirc_{i=1}^{l}%
f_{\text{layer}}^{(i)}\right)(\boldsymbol{X_{0}})," class="ltx_math_unparsed" display="inline" id="S2.SS0.SSS0.Px1.p1.10.m10.1"><semantics id="S2.SS0.SSS0.Px1.p1.10.m10.1a"><mrow id="S2.SS0.SSS0.Px1.p1.10.m10.1b"><msub id="S2.SS0.SSS0.Px1.p1.10.m10.1.2"><mi id="S2.SS0.SSS0.Px1.p1.10.m10.1.2.2">ğ‘¶</mi><mi id="S2.SS0.SSS0.Px1.p1.10.m10.1.2.3">ğ’</mi></msub><mo id="S2.SS0.SSS0.Px1.p1.10.m10.1.3">=</mo><msub id="S2.SS0.SSS0.Px1.p1.10.m10.1.4"><mi id="S2.SS0.SSS0.Px1.p1.10.m10.1.4.2">f</mi><mtext id="S2.SS0.SSS0.Px1.p1.10.m10.1.4.3">trf</mtext></msub><mrow id="S2.SS0.SSS0.Px1.p1.10.m10.1.5"><mo id="S2.SS0.SSS0.Px1.p1.10.m10.1.5.1" stretchy="false">(</mo><msub id="S2.SS0.SSS0.Px1.p1.10.m10.1.5.2"><mi id="S2.SS0.SSS0.Px1.p1.10.m10.1.5.2.2">ğ‘¿</mi><mn id="S2.SS0.SSS0.Px1.p1.10.m10.1.5.2.3">ğŸ</mn></msub><mo id="S2.SS0.SSS0.Px1.p1.10.m10.1.5.3" stretchy="false">)</mo></mrow><mo id="S2.SS0.SSS0.Px1.p1.10.m10.1.6">=</mo><mrow id="S2.SS0.SSS0.Px1.p1.10.m10.1.7"><mo id="S2.SS0.SSS0.Px1.p1.10.m10.1.7.1">(</mo><msubsup id="S2.SS0.SSS0.Px1.p1.10.m10.1.7.2"><mo id="S2.SS0.SSS0.Px1.p1.10.m10.1.7.2.2.2" lspace="0em" rspace="0.222em">â—‹</mo><mrow id="S2.SS0.SSS0.Px1.p1.10.m10.1.7.2.2.3"><mi id="S2.SS0.SSS0.Px1.p1.10.m10.1.7.2.2.3.2">i</mi><mo id="S2.SS0.SSS0.Px1.p1.10.m10.1.7.2.2.3.1">=</mo><mn id="S2.SS0.SSS0.Px1.p1.10.m10.1.7.2.2.3.3">1</mn></mrow><mi id="S2.SS0.SSS0.Px1.p1.10.m10.1.7.2.3">l</mi></msubsup><msubsup id="S2.SS0.SSS0.Px1.p1.10.m10.1.7.3"><mi id="S2.SS0.SSS0.Px1.p1.10.m10.1.7.3.2.2">f</mi><mtext id="S2.SS0.SSS0.Px1.p1.10.m10.1.7.3.2.3">layer</mtext><mrow id="S2.SS0.SSS0.Px1.p1.10.m10.1.1.1.3"><mo id="S2.SS0.SSS0.Px1.p1.10.m10.1.1.1.3.1" stretchy="false">(</mo><mi id="S2.SS0.SSS0.Px1.p1.10.m10.1.1.1.1">i</mi><mo id="S2.SS0.SSS0.Px1.p1.10.m10.1.1.1.3.2" stretchy="false">)</mo></mrow></msubsup><mo id="S2.SS0.SSS0.Px1.p1.10.m10.1.7.4">)</mo></mrow><mrow id="S2.SS0.SSS0.Px1.p1.10.m10.1.8"><mo id="S2.SS0.SSS0.Px1.p1.10.m10.1.8.1" stretchy="false">(</mo><msub id="S2.SS0.SSS0.Px1.p1.10.m10.1.8.2"><mi id="S2.SS0.SSS0.Px1.p1.10.m10.1.8.2.2">ğ‘¿</mi><mn id="S2.SS0.SSS0.Px1.p1.10.m10.1.8.2.3">ğŸ</mn></msub><mo id="S2.SS0.SSS0.Px1.p1.10.m10.1.8.3" stretchy="false">)</mo></mrow><mo id="S2.SS0.SSS0.Px1.p1.10.m10.1.9">,</mo></mrow><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.10.m10.1c">\boldsymbol{O_{l}}=f_{\text{trf}}(\boldsymbol{X_{0}})=\left(\bigcirc_{i=1}^{l}%
f_{\text{layer}}^{(i)}\right)(\boldsymbol{X_{0}}),</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.10.m10.1d">bold_italic_O start_POSTSUBSCRIPT bold_italic_l end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT trf end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT bold_0 end_POSTSUBSCRIPT ) = ( â—‹ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT italic_f start_POSTSUBSCRIPT layer end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) ( bold_italic_X start_POSTSUBSCRIPT bold_0 end_POSTSUBSCRIPT ) ,</annotation></semantics></math>
where for each layer <math alttext="f_{\text{layer}}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.11.m11.1"><semantics id="S2.SS0.SSS0.Px1.p1.11.m11.1a"><msub id="S2.SS0.SSS0.Px1.p1.11.m11.1.1" xref="S2.SS0.SSS0.Px1.p1.11.m11.1.1.cmml"><mi id="S2.SS0.SSS0.Px1.p1.11.m11.1.1.2" xref="S2.SS0.SSS0.Px1.p1.11.m11.1.1.2.cmml">f</mi><mtext id="S2.SS0.SSS0.Px1.p1.11.m11.1.1.3" xref="S2.SS0.SSS0.Px1.p1.11.m11.1.1.3a.cmml">layer</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.11.m11.1b"><apply id="S2.SS0.SSS0.Px1.p1.11.m11.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.11.m11.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.11.m11.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.11.m11.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.11.m11.1.1.2">ğ‘“</ci><ci id="S2.SS0.SSS0.Px1.p1.11.m11.1.1.3a.cmml" xref="S2.SS0.SSS0.Px1.p1.11.m11.1.1.3"><mtext id="S2.SS0.SSS0.Px1.p1.11.m11.1.1.3.cmml" mathsize="70%" xref="S2.SS0.SSS0.Px1.p1.11.m11.1.1.3">layer</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.11.m11.1c">f_{\text{layer}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.11.m11.1d">italic_f start_POSTSUBSCRIPT layer end_POSTSUBSCRIPT</annotation></semantics></math>, it usually contains an Attention block and an MLP block:</p>
<table class="ltx_equationgroup ltx_eqn_eqnarray ltx_eqn_table" id="A5.EGx1">
<tbody id="S2.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math alttext="\displaystyle\boldsymbol{O^{\text{att}}_{i}}" class="ltx_Math" display="inline" id="S2.E1.m2.1"><semantics id="S2.E1.m2.1a"><msubsup id="S2.E1.m2.1.1" xref="S2.E1.m2.1.1.cmml"><mi id="S2.E1.m2.1.1.2.2" xref="S2.E1.m2.1.1.2.2.cmml">ğ‘¶</mi><mi id="S2.E1.m2.1.1.3" xref="S2.E1.m2.1.1.3.cmml">ğ’Š</mi><mtext id="S2.E1.m2.1.1.2.3" xref="S2.E1.m2.1.1.2.3a.cmml">att</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S2.E1.m2.1b"><apply id="S2.E1.m2.1.1.cmml" xref="S2.E1.m2.1.1"><csymbol cd="ambiguous" id="S2.E1.m2.1.1.1.cmml" xref="S2.E1.m2.1.1">subscript</csymbol><apply id="S2.E1.m2.1.1.2.cmml" xref="S2.E1.m2.1.1"><csymbol cd="ambiguous" id="S2.E1.m2.1.1.2.1.cmml" xref="S2.E1.m2.1.1">superscript</csymbol><ci id="S2.E1.m2.1.1.2.2.cmml" xref="S2.E1.m2.1.1.2.2">ğ‘¶</ci><ci id="S2.E1.m2.1.1.2.3a.cmml" xref="S2.E1.m2.1.1.2.3"><mtext id="S2.E1.m2.1.1.2.3.cmml" mathsize="70%" xref="S2.E1.m2.1.1.2.3">att</mtext></ci></apply><ci id="S2.E1.m2.1.1.3.cmml" xref="S2.E1.m2.1.1.3">ğ’Š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m2.1c">\displaystyle\boldsymbol{O^{\text{att}}_{i}}</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m2.1d">bold_italic_O start_POSTSUPERSCRIPT att end_POSTSUPERSCRIPT start_POSTSUBSCRIPT bold_italic_i end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\boldsymbol{X_{i}}+\mathrm{Attn}(\mathrm{Norm}(\boldsymbol{X_{i}%
}))," class="ltx_Math" display="inline" id="S2.E1.m3.1"><semantics id="S2.E1.m3.1a"><mrow id="S2.E1.m3.1.1.1" xref="S2.E1.m3.1.1.1.1.cmml"><mrow id="S2.E1.m3.1.1.1.1" xref="S2.E1.m3.1.1.1.1.cmml"><mi id="S2.E1.m3.1.1.1.1.3" xref="S2.E1.m3.1.1.1.1.3.cmml"></mi><mo id="S2.E1.m3.1.1.1.1.2" xref="S2.E1.m3.1.1.1.1.2.cmml">=</mo><mrow id="S2.E1.m3.1.1.1.1.1" xref="S2.E1.m3.1.1.1.1.1.cmml"><msub id="S2.E1.m3.1.1.1.1.1.3" xref="S2.E1.m3.1.1.1.1.1.3.cmml"><mi id="S2.E1.m3.1.1.1.1.1.3.2" xref="S2.E1.m3.1.1.1.1.1.3.2.cmml">ğ‘¿</mi><mi id="S2.E1.m3.1.1.1.1.1.3.3" xref="S2.E1.m3.1.1.1.1.1.3.3.cmml">ğ’Š</mi></msub><mo id="S2.E1.m3.1.1.1.1.1.2" xref="S2.E1.m3.1.1.1.1.1.2.cmml">+</mo><mrow id="S2.E1.m3.1.1.1.1.1.1" xref="S2.E1.m3.1.1.1.1.1.1.cmml"><mi id="S2.E1.m3.1.1.1.1.1.1.3" xref="S2.E1.m3.1.1.1.1.1.1.3.cmml">Attn</mi><mo id="S2.E1.m3.1.1.1.1.1.1.2" xref="S2.E1.m3.1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S2.E1.m3.1.1.1.1.1.1.1.1" xref="S2.E1.m3.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E1.m3.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E1.m3.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m3.1.1.1.1.1.1.1.1.1" xref="S2.E1.m3.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m3.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m3.1.1.1.1.1.1.1.1.1.3.cmml">Norm</mi><mo id="S2.E1.m3.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m3.1.1.1.1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S2.E1.m3.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m3.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E1.m3.1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E1.m3.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E1.m3.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m3.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m3.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m3.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">ğ‘¿</mi><mi id="S2.E1.m3.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m3.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">ğ’Š</mi></msub><mo id="S2.E1.m3.1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.E1.m3.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m3.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.E1.m3.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S2.E1.m3.1.1.1.2" xref="S2.E1.m3.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m3.1b"><apply id="S2.E1.m3.1.1.1.1.cmml" xref="S2.E1.m3.1.1.1"><eq id="S2.E1.m3.1.1.1.1.2.cmml" xref="S2.E1.m3.1.1.1.1.2"></eq><csymbol cd="latexml" id="S2.E1.m3.1.1.1.1.3.cmml" xref="S2.E1.m3.1.1.1.1.3">absent</csymbol><apply id="S2.E1.m3.1.1.1.1.1.cmml" xref="S2.E1.m3.1.1.1.1.1"><plus id="S2.E1.m3.1.1.1.1.1.2.cmml" xref="S2.E1.m3.1.1.1.1.1.2"></plus><apply id="S2.E1.m3.1.1.1.1.1.3.cmml" xref="S2.E1.m3.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m3.1.1.1.1.1.3.1.cmml" xref="S2.E1.m3.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m3.1.1.1.1.1.3.2.cmml" xref="S2.E1.m3.1.1.1.1.1.3.2">ğ‘¿</ci><ci id="S2.E1.m3.1.1.1.1.1.3.3.cmml" xref="S2.E1.m3.1.1.1.1.1.3.3">ğ’Š</ci></apply><apply id="S2.E1.m3.1.1.1.1.1.1.cmml" xref="S2.E1.m3.1.1.1.1.1.1"><times id="S2.E1.m3.1.1.1.1.1.1.2.cmml" xref="S2.E1.m3.1.1.1.1.1.1.2"></times><ci id="S2.E1.m3.1.1.1.1.1.1.3.cmml" xref="S2.E1.m3.1.1.1.1.1.1.3">Attn</ci><apply id="S2.E1.m3.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m3.1.1.1.1.1.1.1.1"><times id="S2.E1.m3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m3.1.1.1.1.1.1.1.1.1.2"></times><ci id="S2.E1.m3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m3.1.1.1.1.1.1.1.1.1.3">Norm</ci><apply id="S2.E1.m3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m3.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m3.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m3.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m3.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m3.1.1.1.1.1.1.1.1.1.1.1.1.2">ğ‘¿</ci><ci id="S2.E1.m3.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m3.1.1.1.1.1.1.1.1.1.1.1.1.3">ğ’Š</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m3.1c">\displaystyle=\boldsymbol{X_{i}}+\mathrm{Attn}(\mathrm{Norm}(\boldsymbol{X_{i}%
})),</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m3.1d">= bold_italic_X start_POSTSUBSCRIPT bold_italic_i end_POSTSUBSCRIPT + roman_Attn ( roman_Norm ( bold_italic_X start_POSTSUBSCRIPT bold_italic_i end_POSTSUBSCRIPT ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S2.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math alttext="\displaystyle\boldsymbol{O_{i}}" class="ltx_Math" display="inline" id="S2.E2.m1.1"><semantics id="S2.E2.m1.1a"><msub id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml"><mi id="S2.E2.m1.1.1.2" xref="S2.E2.m1.1.1.2.cmml">ğ‘¶</mi><mi id="S2.E2.m1.1.1.3" xref="S2.E2.m1.1.1.3.cmml">ğ’Š</mi></msub><annotation-xml encoding="MathML-Content" id="S2.E2.m1.1b"><apply id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.cmml" xref="S2.E2.m1.1.1">subscript</csymbol><ci id="S2.E2.m1.1.1.2.cmml" xref="S2.E2.m1.1.1.2">ğ‘¶</ci><ci id="S2.E2.m1.1.1.3.cmml" xref="S2.E2.m1.1.1.3">ğ’Š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.1c">\displaystyle\boldsymbol{O_{i}}</annotation><annotation encoding="application/x-llamapun" id="S2.E2.m1.1d">bold_italic_O start_POSTSUBSCRIPT bold_italic_i end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\boldsymbol{O^{\text{att}}_{i}}+\mathrm{MLP}(\mathrm{Norm}(%
\boldsymbol{O^{\text{att}}_{i}}))." class="ltx_Math" display="inline" id="S2.E2.m2.1"><semantics id="S2.E2.m2.1a"><mrow id="S2.E2.m2.1.1.1" xref="S2.E2.m2.1.1.1.1.cmml"><mrow id="S2.E2.m2.1.1.1.1" xref="S2.E2.m2.1.1.1.1.cmml"><mi id="S2.E2.m2.1.1.1.1.3" xref="S2.E2.m2.1.1.1.1.3.cmml"></mi><mo id="S2.E2.m2.1.1.1.1.2" xref="S2.E2.m2.1.1.1.1.2.cmml">=</mo><mrow id="S2.E2.m2.1.1.1.1.1" xref="S2.E2.m2.1.1.1.1.1.cmml"><msubsup id="S2.E2.m2.1.1.1.1.1.3" xref="S2.E2.m2.1.1.1.1.1.3.cmml"><mi id="S2.E2.m2.1.1.1.1.1.3.2.2" xref="S2.E2.m2.1.1.1.1.1.3.2.2.cmml">ğ‘¶</mi><mi id="S2.E2.m2.1.1.1.1.1.3.3" xref="S2.E2.m2.1.1.1.1.1.3.3.cmml">ğ’Š</mi><mtext id="S2.E2.m2.1.1.1.1.1.3.2.3" xref="S2.E2.m2.1.1.1.1.1.3.2.3a.cmml">att</mtext></msubsup><mo id="S2.E2.m2.1.1.1.1.1.2" xref="S2.E2.m2.1.1.1.1.1.2.cmml">+</mo><mrow id="S2.E2.m2.1.1.1.1.1.1" xref="S2.E2.m2.1.1.1.1.1.1.cmml"><mi id="S2.E2.m2.1.1.1.1.1.1.3" xref="S2.E2.m2.1.1.1.1.1.1.3.cmml">MLP</mi><mo id="S2.E2.m2.1.1.1.1.1.1.2" xref="S2.E2.m2.1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S2.E2.m2.1.1.1.1.1.1.1.1" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E2.m2.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m2.1.1.1.1.1.1.1.1.1" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m2.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.3.cmml">Norm</mi><mo id="S2.E2.m2.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msubsup id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">ğ‘¶</mi><mi id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">ğ’Š</mi><mtext id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.3a.cmml">att</mtext></msubsup><mo id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E2.m2.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S2.E2.m2.1.1.1.2" lspace="0em" xref="S2.E2.m2.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m2.1b"><apply id="S2.E2.m2.1.1.1.1.cmml" xref="S2.E2.m2.1.1.1"><eq id="S2.E2.m2.1.1.1.1.2.cmml" xref="S2.E2.m2.1.1.1.1.2"></eq><csymbol cd="latexml" id="S2.E2.m2.1.1.1.1.3.cmml" xref="S2.E2.m2.1.1.1.1.3">absent</csymbol><apply id="S2.E2.m2.1.1.1.1.1.cmml" xref="S2.E2.m2.1.1.1.1.1"><plus id="S2.E2.m2.1.1.1.1.1.2.cmml" xref="S2.E2.m2.1.1.1.1.1.2"></plus><apply id="S2.E2.m2.1.1.1.1.1.3.cmml" xref="S2.E2.m2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m2.1.1.1.1.1.3.1.cmml" xref="S2.E2.m2.1.1.1.1.1.3">subscript</csymbol><apply id="S2.E2.m2.1.1.1.1.1.3.2.cmml" xref="S2.E2.m2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m2.1.1.1.1.1.3.2.1.cmml" xref="S2.E2.m2.1.1.1.1.1.3">superscript</csymbol><ci id="S2.E2.m2.1.1.1.1.1.3.2.2.cmml" xref="S2.E2.m2.1.1.1.1.1.3.2.2">ğ‘¶</ci><ci id="S2.E2.m2.1.1.1.1.1.3.2.3a.cmml" xref="S2.E2.m2.1.1.1.1.1.3.2.3"><mtext id="S2.E2.m2.1.1.1.1.1.3.2.3.cmml" mathsize="70%" xref="S2.E2.m2.1.1.1.1.1.3.2.3">att</mtext></ci></apply><ci id="S2.E2.m2.1.1.1.1.1.3.3.cmml" xref="S2.E2.m2.1.1.1.1.1.3.3">ğ’Š</ci></apply><apply id="S2.E2.m2.1.1.1.1.1.1.cmml" xref="S2.E2.m2.1.1.1.1.1.1"><times id="S2.E2.m2.1.1.1.1.1.1.2.cmml" xref="S2.E2.m2.1.1.1.1.1.1.2"></times><ci id="S2.E2.m2.1.1.1.1.1.1.3.cmml" xref="S2.E2.m2.1.1.1.1.1.1.3">MLP</ci><apply id="S2.E2.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1"><times id="S2.E2.m2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.2"></times><ci id="S2.E2.m2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.3">Norm</ci><apply id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2">ğ‘¶</ci><ci id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.3a.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.3"><mtext id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" mathsize="70%" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.3">att</mtext></ci></apply><ci id="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.3">ğ’Š</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m2.1c">\displaystyle=\boldsymbol{O^{\text{att}}_{i}}+\mathrm{MLP}(\mathrm{Norm}(%
\boldsymbol{O^{\text{att}}_{i}})).</annotation><annotation encoding="application/x-llamapun" id="S2.E2.m2.1d">= bold_italic_O start_POSTSUPERSCRIPT att end_POSTSUPERSCRIPT start_POSTSUBSCRIPT bold_italic_i end_POSTSUBSCRIPT + roman_MLP ( roman_Norm ( bold_italic_O start_POSTSUPERSCRIPT att end_POSTSUPERSCRIPT start_POSTSUBSCRIPT bold_italic_i end_POSTSUBSCRIPT ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.14">Here, <math alttext="\boldsymbol{O^{\text{att}}_{i}}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.12.m1.1"><semantics id="S2.SS0.SSS0.Px1.p1.12.m1.1a"><msubsup id="S2.SS0.SSS0.Px1.p1.12.m1.1.1" xref="S2.SS0.SSS0.Px1.p1.12.m1.1.1.cmml"><mi id="S2.SS0.SSS0.Px1.p1.12.m1.1.1.2.2" xref="S2.SS0.SSS0.Px1.p1.12.m1.1.1.2.2.cmml">ğ‘¶</mi><mi id="S2.SS0.SSS0.Px1.p1.12.m1.1.1.3" xref="S2.SS0.SSS0.Px1.p1.12.m1.1.1.3.cmml">ğ’Š</mi><mtext id="S2.SS0.SSS0.Px1.p1.12.m1.1.1.2.3" xref="S2.SS0.SSS0.Px1.p1.12.m1.1.1.2.3a.cmml">att</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.12.m1.1b"><apply id="S2.SS0.SSS0.Px1.p1.12.m1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.12.m1.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.12.m1.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.12.m1.1.1">subscript</csymbol><apply id="S2.SS0.SSS0.Px1.p1.12.m1.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.12.m1.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.12.m1.1.1.2.1.cmml" xref="S2.SS0.SSS0.Px1.p1.12.m1.1.1">superscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.12.m1.1.1.2.2.cmml" xref="S2.SS0.SSS0.Px1.p1.12.m1.1.1.2.2">ğ‘¶</ci><ci id="S2.SS0.SSS0.Px1.p1.12.m1.1.1.2.3a.cmml" xref="S2.SS0.SSS0.Px1.p1.12.m1.1.1.2.3"><mtext id="S2.SS0.SSS0.Px1.p1.12.m1.1.1.2.3.cmml" mathsize="70%" xref="S2.SS0.SSS0.Px1.p1.12.m1.1.1.2.3">att</mtext></ci></apply><ci id="S2.SS0.SSS0.Px1.p1.12.m1.1.1.3.cmml" xref="S2.SS0.SSS0.Px1.p1.12.m1.1.1.3">ğ’Š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.12.m1.1c">\boldsymbol{O^{\text{att}}_{i}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.12.m1.1d">bold_italic_O start_POSTSUPERSCRIPT att end_POSTSUPERSCRIPT start_POSTSUBSCRIPT bold_italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the output of the attention block, and <math alttext="\boldsymbol{O^{\text{mlp}}_{i}}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.13.m2.1"><semantics id="S2.SS0.SSS0.Px1.p1.13.m2.1a"><msubsup id="S2.SS0.SSS0.Px1.p1.13.m2.1.1" xref="S2.SS0.SSS0.Px1.p1.13.m2.1.1.cmml"><mi id="S2.SS0.SSS0.Px1.p1.13.m2.1.1.2.2" xref="S2.SS0.SSS0.Px1.p1.13.m2.1.1.2.2.cmml">ğ‘¶</mi><mi id="S2.SS0.SSS0.Px1.p1.13.m2.1.1.3" xref="S2.SS0.SSS0.Px1.p1.13.m2.1.1.3.cmml">ğ’Š</mi><mtext id="S2.SS0.SSS0.Px1.p1.13.m2.1.1.2.3" xref="S2.SS0.SSS0.Px1.p1.13.m2.1.1.2.3a.cmml">mlp</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.13.m2.1b"><apply id="S2.SS0.SSS0.Px1.p1.13.m2.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.13.m2.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.13.m2.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.13.m2.1.1">subscript</csymbol><apply id="S2.SS0.SSS0.Px1.p1.13.m2.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.13.m2.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.13.m2.1.1.2.1.cmml" xref="S2.SS0.SSS0.Px1.p1.13.m2.1.1">superscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.13.m2.1.1.2.2.cmml" xref="S2.SS0.SSS0.Px1.p1.13.m2.1.1.2.2">ğ‘¶</ci><ci id="S2.SS0.SSS0.Px1.p1.13.m2.1.1.2.3a.cmml" xref="S2.SS0.SSS0.Px1.p1.13.m2.1.1.2.3"><mtext id="S2.SS0.SSS0.Px1.p1.13.m2.1.1.2.3.cmml" mathsize="70%" xref="S2.SS0.SSS0.Px1.p1.13.m2.1.1.2.3">mlp</mtext></ci></apply><ci id="S2.SS0.SSS0.Px1.p1.13.m2.1.1.3.cmml" xref="S2.SS0.SSS0.Px1.p1.13.m2.1.1.3">ğ’Š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.13.m2.1c">\boldsymbol{O^{\text{mlp}}_{i}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.13.m2.1d">bold_italic_O start_POSTSUPERSCRIPT mlp end_POSTSUPERSCRIPT start_POSTSUBSCRIPT bold_italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the output of the MLP block for layer <math alttext="i" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.14.m3.1"><semantics id="S2.SS0.SSS0.Px1.p1.14.m3.1a"><mi id="S2.SS0.SSS0.Px1.p1.14.m3.1.1" xref="S2.SS0.SSS0.Px1.p1.14.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.14.m3.1b"><ci id="S2.SS0.SSS0.Px1.p1.14.m3.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.14.m3.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.14.m3.1c">i</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.14.m3.1d">italic_i</annotation></semantics></math>, with residual connections preventing it from vanishing gradient and normalization (typically pre-norm) for stabilizing the training process.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Fine-tuning.</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.6">Fine-tuning is a process where a pre-trained LLM is further trained on a specific task or dataset to improve its performance for that particular application. Suppose there exists a pre-trained Transformer model <math alttext="f_{\text{trf}}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.1.m1.1"><semantics id="S2.SS0.SSS0.Px2.p1.1.m1.1a"><msub id="S2.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S2.SS0.SSS0.Px2.p1.1.m1.1.1.2" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml">f</mi><mtext id="S2.SS0.SSS0.Px2.p1.1.m1.1.1.3" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1.3a.cmml">trf</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.1.m1.1b"><apply id="S2.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1.2">ğ‘“</ci><ci id="S2.SS0.SSS0.Px2.p1.1.m1.1.1.3a.cmml" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1.3"><mtext id="S2.SS0.SSS0.Px2.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1.3">trf</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.1.m1.1c">f_{\text{trf}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.1.m1.1d">italic_f start_POSTSUBSCRIPT trf end_POSTSUBSCRIPT</annotation></semantics></math> with learnable parameters <math alttext="\theta_{\text{pre}}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.2.m2.1"><semantics id="S2.SS0.SSS0.Px2.p1.2.m2.1a"><msub id="S2.SS0.SSS0.Px2.p1.2.m2.1.1" xref="S2.SS0.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S2.SS0.SSS0.Px2.p1.2.m2.1.1.2" xref="S2.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml">Î¸</mi><mtext id="S2.SS0.SSS0.Px2.p1.2.m2.1.1.3" xref="S2.SS0.SSS0.Px2.p1.2.m2.1.1.3a.cmml">pre</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.2.m2.1b"><apply id="S2.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S2.SS0.SSS0.Px2.p1.2.m2.1.1.2">ğœƒ</ci><ci id="S2.SS0.SSS0.Px2.p1.2.m2.1.1.3a.cmml" xref="S2.SS0.SSS0.Px2.p1.2.m2.1.1.3"><mtext id="S2.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml" mathsize="70%" xref="S2.SS0.SSS0.Px2.p1.2.m2.1.1.3">pre</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.2.m2.1c">\theta_{\text{pre}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.2.m2.1d">italic_Î¸ start_POSTSUBSCRIPT pre end_POSTSUBSCRIPT</annotation></semantics></math>. The goal of fine-tuning is to adjust these parameters to minimize a task-specific loss function <math alttext="\mathcal{L}_{\text{task}}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.3.m3.1"><semantics id="S2.SS0.SSS0.Px2.p1.3.m3.1a"><msub id="S2.SS0.SSS0.Px2.p1.3.m3.1.1" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS0.SSS0.Px2.p1.3.m3.1.1.2" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.1.2.cmml">â„’</mi><mtext id="S2.SS0.SSS0.Px2.p1.3.m3.1.1.3" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.1.3a.cmml">task</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.3.m3.1b"><apply id="S2.SS0.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.1.2">â„’</ci><ci id="S2.SS0.SSS0.Px2.p1.3.m3.1.1.3a.cmml" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.1.3"><mtext id="S2.SS0.SSS0.Px2.p1.3.m3.1.1.3.cmml" mathsize="70%" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.1.3">task</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.3.m3.1c">\mathcal{L}_{\text{task}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.3.m3.1d">caligraphic_L start_POSTSUBSCRIPT task end_POSTSUBSCRIPT</annotation></semantics></math> on a new dataset <math alttext="\mathcal{D}_{\text{task}}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.4.m4.1"><semantics id="S2.SS0.SSS0.Px2.p1.4.m4.1a"><msub id="S2.SS0.SSS0.Px2.p1.4.m4.1.1" xref="S2.SS0.SSS0.Px2.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS0.SSS0.Px2.p1.4.m4.1.1.2" xref="S2.SS0.SSS0.Px2.p1.4.m4.1.1.2.cmml">ğ’Ÿ</mi><mtext id="S2.SS0.SSS0.Px2.p1.4.m4.1.1.3" xref="S2.SS0.SSS0.Px2.p1.4.m4.1.1.3a.cmml">task</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.4.m4.1b"><apply id="S2.SS0.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.4.m4.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px2.p1.4.m4.1.1.2.cmml" xref="S2.SS0.SSS0.Px2.p1.4.m4.1.1.2">ğ’Ÿ</ci><ci id="S2.SS0.SSS0.Px2.p1.4.m4.1.1.3a.cmml" xref="S2.SS0.SSS0.Px2.p1.4.m4.1.1.3"><mtext id="S2.SS0.SSS0.Px2.p1.4.m4.1.1.3.cmml" mathsize="70%" xref="S2.SS0.SSS0.Px2.p1.4.m4.1.1.3">task</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.4.m4.1c">\mathcal{D}_{\text{task}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.4.m4.1d">caligraphic_D start_POSTSUBSCRIPT task end_POSTSUBSCRIPT</annotation></semantics></math>. During fine-tuning, the parameters <math alttext="\theta_{\text{fine}}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.5.m5.1"><semantics id="S2.SS0.SSS0.Px2.p1.5.m5.1a"><msub id="S2.SS0.SSS0.Px2.p1.5.m5.1.1" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.1.cmml"><mi id="S2.SS0.SSS0.Px2.p1.5.m5.1.1.2" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.1.2.cmml">Î¸</mi><mtext id="S2.SS0.SSS0.Px2.p1.5.m5.1.1.3" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.1.3a.cmml">fine</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.5.m5.1b"><apply id="S2.SS0.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px2.p1.5.m5.1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px2.p1.5.m5.1.1.2.cmml" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.1.2">ğœƒ</ci><ci id="S2.SS0.SSS0.Px2.p1.5.m5.1.1.3a.cmml" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.1.3"><mtext id="S2.SS0.SSS0.Px2.p1.5.m5.1.1.3.cmml" mathsize="70%" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.1.3">fine</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.5.m5.1c">\theta_{\text{fine}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.5.m5.1d">italic_Î¸ start_POSTSUBSCRIPT fine end_POSTSUBSCRIPT</annotation></semantics></math> of the model are updated using gradient descent or one of its variants. The update rule for the parameters at each iteration <math alttext="t" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.6.m6.1"><semantics id="S2.SS0.SSS0.Px2.p1.6.m6.1a"><mi id="S2.SS0.SSS0.Px2.p1.6.m6.1.1" xref="S2.SS0.SSS0.Px2.p1.6.m6.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.6.m6.1b"><ci id="S2.SS0.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.6.m6.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.6.m6.1c">t</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.6.m6.1d">italic_t</annotation></semantics></math> can be expressed as:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\theta_{\text{fine}}^{(t+1)}=\theta_{\text{fine}}^{(t)}-\eta\nabla_{\theta}%
\mathcal{L}_{\text{task}}(f_{\text{trf}}(\boldsymbol{X_{t}};\theta_{\text{fine%
}}^{(t)}),\boldsymbol{Y_{t}})," class="ltx_Math" display="block" id="S2.E3.m1.4"><semantics id="S2.E3.m1.4a"><mrow id="S2.E3.m1.4.4.1" xref="S2.E3.m1.4.4.1.1.cmml"><mrow id="S2.E3.m1.4.4.1.1" xref="S2.E3.m1.4.4.1.1.cmml"><msubsup id="S2.E3.m1.4.4.1.1.4" xref="S2.E3.m1.4.4.1.1.4.cmml"><mi id="S2.E3.m1.4.4.1.1.4.2.2" xref="S2.E3.m1.4.4.1.1.4.2.2.cmml">Î¸</mi><mtext id="S2.E3.m1.4.4.1.1.4.2.3" xref="S2.E3.m1.4.4.1.1.4.2.3a.cmml">fine</mtext><mrow id="S2.E3.m1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.cmml"><mo id="S2.E3.m1.1.1.1.1.2" stretchy="false" xref="S2.E3.m1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E3.m1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.cmml"><mi id="S2.E3.m1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.2.cmml">t</mi><mo id="S2.E3.m1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.cmml">+</mo><mn id="S2.E3.m1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S2.E3.m1.1.1.1.1.3" stretchy="false" xref="S2.E3.m1.1.1.1.1.1.cmml">)</mo></mrow></msubsup><mo id="S2.E3.m1.4.4.1.1.3" xref="S2.E3.m1.4.4.1.1.3.cmml">=</mo><mrow id="S2.E3.m1.4.4.1.1.2" xref="S2.E3.m1.4.4.1.1.2.cmml"><msubsup id="S2.E3.m1.4.4.1.1.2.4" xref="S2.E3.m1.4.4.1.1.2.4.cmml"><mi id="S2.E3.m1.4.4.1.1.2.4.2.2" xref="S2.E3.m1.4.4.1.1.2.4.2.2.cmml">Î¸</mi><mtext id="S2.E3.m1.4.4.1.1.2.4.2.3" xref="S2.E3.m1.4.4.1.1.2.4.2.3a.cmml">fine</mtext><mrow id="S2.E3.m1.2.2.1.3" xref="S2.E3.m1.4.4.1.1.2.4.cmml"><mo id="S2.E3.m1.2.2.1.3.1" stretchy="false" xref="S2.E3.m1.4.4.1.1.2.4.cmml">(</mo><mi id="S2.E3.m1.2.2.1.1" xref="S2.E3.m1.2.2.1.1.cmml">t</mi><mo id="S2.E3.m1.2.2.1.3.2" stretchy="false" xref="S2.E3.m1.4.4.1.1.2.4.cmml">)</mo></mrow></msubsup><mo id="S2.E3.m1.4.4.1.1.2.3" xref="S2.E3.m1.4.4.1.1.2.3.cmml">âˆ’</mo><mrow id="S2.E3.m1.4.4.1.1.2.2" xref="S2.E3.m1.4.4.1.1.2.2.cmml"><mi id="S2.E3.m1.4.4.1.1.2.2.4" xref="S2.E3.m1.4.4.1.1.2.2.4.cmml">Î·</mi><mo id="S2.E3.m1.4.4.1.1.2.2.3" lspace="0.167em" xref="S2.E3.m1.4.4.1.1.2.2.3.cmml">â¢</mo><mrow id="S2.E3.m1.4.4.1.1.2.2.5" xref="S2.E3.m1.4.4.1.1.2.2.5.cmml"><msub id="S2.E3.m1.4.4.1.1.2.2.5.1" xref="S2.E3.m1.4.4.1.1.2.2.5.1.cmml"><mo id="S2.E3.m1.4.4.1.1.2.2.5.1.2" rspace="0.167em" xref="S2.E3.m1.4.4.1.1.2.2.5.1.2.cmml">âˆ‡</mo><mi id="S2.E3.m1.4.4.1.1.2.2.5.1.3" xref="S2.E3.m1.4.4.1.1.2.2.5.1.3.cmml">Î¸</mi></msub><msub id="S2.E3.m1.4.4.1.1.2.2.5.2" xref="S2.E3.m1.4.4.1.1.2.2.5.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E3.m1.4.4.1.1.2.2.5.2.2" xref="S2.E3.m1.4.4.1.1.2.2.5.2.2.cmml">â„’</mi><mtext id="S2.E3.m1.4.4.1.1.2.2.5.2.3" xref="S2.E3.m1.4.4.1.1.2.2.5.2.3a.cmml">task</mtext></msub></mrow><mo id="S2.E3.m1.4.4.1.1.2.2.3a" xref="S2.E3.m1.4.4.1.1.2.2.3.cmml">â¢</mo><mrow id="S2.E3.m1.4.4.1.1.2.2.2.2" xref="S2.E3.m1.4.4.1.1.2.2.2.3.cmml"><mo id="S2.E3.m1.4.4.1.1.2.2.2.2.3" stretchy="false" xref="S2.E3.m1.4.4.1.1.2.2.2.3.cmml">(</mo><mrow id="S2.E3.m1.4.4.1.1.1.1.1.1.1" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.cmml"><msub id="S2.E3.m1.4.4.1.1.1.1.1.1.1.4" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.4.cmml"><mi id="S2.E3.m1.4.4.1.1.1.1.1.1.1.4.2" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.4.2.cmml">f</mi><mtext id="S2.E3.m1.4.4.1.1.1.1.1.1.1.4.3" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.4.3a.cmml">trf</mtext></msub><mo id="S2.E3.m1.4.4.1.1.1.1.1.1.1.3" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.3.cmml">â¢</mo><mrow id="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.2" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.3.cmml"><mo id="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.2.3" stretchy="false" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.3.cmml">(</mo><msub id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.cmml">ğ‘¿</mi><mi id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.3.cmml">ğ’•</mi></msub><mo id="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.2.4" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.3.cmml">;</mo><msubsup id="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.2.2" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.2.2.2.2" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.2.2.2.2.cmml">Î¸</mi><mtext id="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.2.2.2.3" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.2.2.2.3a.cmml">fine</mtext><mrow id="S2.E3.m1.3.3.1.3" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.2.2.cmml"><mo id="S2.E3.m1.3.3.1.3.1" stretchy="false" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.2.2.cmml">(</mo><mi id="S2.E3.m1.3.3.1.1" xref="S2.E3.m1.3.3.1.1.cmml">t</mi><mo id="S2.E3.m1.3.3.1.3.2" stretchy="false" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.2.2.cmml">)</mo></mrow></msubsup><mo id="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.2.5" stretchy="false" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo id="S2.E3.m1.4.4.1.1.2.2.2.2.4" xref="S2.E3.m1.4.4.1.1.2.2.2.3.cmml">,</mo><msub id="S2.E3.m1.4.4.1.1.2.2.2.2.2" xref="S2.E3.m1.4.4.1.1.2.2.2.2.2.cmml"><mi id="S2.E3.m1.4.4.1.1.2.2.2.2.2.2" xref="S2.E3.m1.4.4.1.1.2.2.2.2.2.2.cmml">ğ’€</mi><mi id="S2.E3.m1.4.4.1.1.2.2.2.2.2.3" xref="S2.E3.m1.4.4.1.1.2.2.2.2.2.3.cmml">ğ’•</mi></msub><mo id="S2.E3.m1.4.4.1.1.2.2.2.2.5" stretchy="false" xref="S2.E3.m1.4.4.1.1.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S2.E3.m1.4.4.1.2" xref="S2.E3.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.4b"><apply id="S2.E3.m1.4.4.1.1.cmml" xref="S2.E3.m1.4.4.1"><eq id="S2.E3.m1.4.4.1.1.3.cmml" xref="S2.E3.m1.4.4.1.1.3"></eq><apply id="S2.E3.m1.4.4.1.1.4.cmml" xref="S2.E3.m1.4.4.1.1.4"><csymbol cd="ambiguous" id="S2.E3.m1.4.4.1.1.4.1.cmml" xref="S2.E3.m1.4.4.1.1.4">superscript</csymbol><apply id="S2.E3.m1.4.4.1.1.4.2.cmml" xref="S2.E3.m1.4.4.1.1.4"><csymbol cd="ambiguous" id="S2.E3.m1.4.4.1.1.4.2.1.cmml" xref="S2.E3.m1.4.4.1.1.4">subscript</csymbol><ci id="S2.E3.m1.4.4.1.1.4.2.2.cmml" xref="S2.E3.m1.4.4.1.1.4.2.2">ğœƒ</ci><ci id="S2.E3.m1.4.4.1.1.4.2.3a.cmml" xref="S2.E3.m1.4.4.1.1.4.2.3"><mtext id="S2.E3.m1.4.4.1.1.4.2.3.cmml" mathsize="70%" xref="S2.E3.m1.4.4.1.1.4.2.3">fine</mtext></ci></apply><apply id="S2.E3.m1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1"><plus id="S2.E3.m1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1"></plus><ci id="S2.E3.m1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.2">ğ‘¡</ci><cn id="S2.E3.m1.1.1.1.1.1.3.cmml" type="integer" xref="S2.E3.m1.1.1.1.1.1.3">1</cn></apply></apply><apply id="S2.E3.m1.4.4.1.1.2.cmml" xref="S2.E3.m1.4.4.1.1.2"><minus id="S2.E3.m1.4.4.1.1.2.3.cmml" xref="S2.E3.m1.4.4.1.1.2.3"></minus><apply id="S2.E3.m1.4.4.1.1.2.4.cmml" xref="S2.E3.m1.4.4.1.1.2.4"><csymbol cd="ambiguous" id="S2.E3.m1.4.4.1.1.2.4.1.cmml" xref="S2.E3.m1.4.4.1.1.2.4">superscript</csymbol><apply id="S2.E3.m1.4.4.1.1.2.4.2.cmml" xref="S2.E3.m1.4.4.1.1.2.4"><csymbol cd="ambiguous" id="S2.E3.m1.4.4.1.1.2.4.2.1.cmml" xref="S2.E3.m1.4.4.1.1.2.4">subscript</csymbol><ci id="S2.E3.m1.4.4.1.1.2.4.2.2.cmml" xref="S2.E3.m1.4.4.1.1.2.4.2.2">ğœƒ</ci><ci id="S2.E3.m1.4.4.1.1.2.4.2.3a.cmml" xref="S2.E3.m1.4.4.1.1.2.4.2.3"><mtext id="S2.E3.m1.4.4.1.1.2.4.2.3.cmml" mathsize="70%" xref="S2.E3.m1.4.4.1.1.2.4.2.3">fine</mtext></ci></apply><ci id="S2.E3.m1.2.2.1.1.cmml" xref="S2.E3.m1.2.2.1.1">ğ‘¡</ci></apply><apply id="S2.E3.m1.4.4.1.1.2.2.cmml" xref="S2.E3.m1.4.4.1.1.2.2"><times id="S2.E3.m1.4.4.1.1.2.2.3.cmml" xref="S2.E3.m1.4.4.1.1.2.2.3"></times><ci id="S2.E3.m1.4.4.1.1.2.2.4.cmml" xref="S2.E3.m1.4.4.1.1.2.2.4">ğœ‚</ci><apply id="S2.E3.m1.4.4.1.1.2.2.5.cmml" xref="S2.E3.m1.4.4.1.1.2.2.5"><apply id="S2.E3.m1.4.4.1.1.2.2.5.1.cmml" xref="S2.E3.m1.4.4.1.1.2.2.5.1"><csymbol cd="ambiguous" id="S2.E3.m1.4.4.1.1.2.2.5.1.1.cmml" xref="S2.E3.m1.4.4.1.1.2.2.5.1">subscript</csymbol><ci id="S2.E3.m1.4.4.1.1.2.2.5.1.2.cmml" xref="S2.E3.m1.4.4.1.1.2.2.5.1.2">âˆ‡</ci><ci id="S2.E3.m1.4.4.1.1.2.2.5.1.3.cmml" xref="S2.E3.m1.4.4.1.1.2.2.5.1.3">ğœƒ</ci></apply><apply id="S2.E3.m1.4.4.1.1.2.2.5.2.cmml" xref="S2.E3.m1.4.4.1.1.2.2.5.2"><csymbol cd="ambiguous" id="S2.E3.m1.4.4.1.1.2.2.5.2.1.cmml" xref="S2.E3.m1.4.4.1.1.2.2.5.2">subscript</csymbol><ci id="S2.E3.m1.4.4.1.1.2.2.5.2.2.cmml" xref="S2.E3.m1.4.4.1.1.2.2.5.2.2">â„’</ci><ci id="S2.E3.m1.4.4.1.1.2.2.5.2.3a.cmml" xref="S2.E3.m1.4.4.1.1.2.2.5.2.3"><mtext id="S2.E3.m1.4.4.1.1.2.2.5.2.3.cmml" mathsize="70%" xref="S2.E3.m1.4.4.1.1.2.2.5.2.3">task</mtext></ci></apply></apply><interval closure="open" id="S2.E3.m1.4.4.1.1.2.2.2.3.cmml" xref="S2.E3.m1.4.4.1.1.2.2.2.2"><apply id="S2.E3.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1"><times id="S2.E3.m1.4.4.1.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.3"></times><apply id="S2.E3.m1.4.4.1.1.1.1.1.1.1.4.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S2.E3.m1.4.4.1.1.1.1.1.1.1.4.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S2.E3.m1.4.4.1.1.1.1.1.1.1.4.2.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.4.2">ğ‘“</ci><ci id="S2.E3.m1.4.4.1.1.1.1.1.1.1.4.3a.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.4.3"><mtext id="S2.E3.m1.4.4.1.1.1.1.1.1.1.4.3.cmml" mathsize="70%" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.4.3">trf</mtext></ci></apply><list id="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.2"><apply id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.2">ğ‘¿</ci><ci id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.3">ğ’•</ci></apply><apply id="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.2.2">superscript</csymbol><apply id="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.2.2.2.2">ğœƒ</ci><ci id="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.2.2.2.3a.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.2.2.2.3"><mtext id="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.2.2.2.3.cmml" mathsize="70%" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.2.2.2.2.3">fine</mtext></ci></apply><ci id="S2.E3.m1.3.3.1.1.cmml" xref="S2.E3.m1.3.3.1.1">ğ‘¡</ci></apply></list></apply><apply id="S2.E3.m1.4.4.1.1.2.2.2.2.2.cmml" xref="S2.E3.m1.4.4.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.4.4.1.1.2.2.2.2.2.1.cmml" xref="S2.E3.m1.4.4.1.1.2.2.2.2.2">subscript</csymbol><ci id="S2.E3.m1.4.4.1.1.2.2.2.2.2.2.cmml" xref="S2.E3.m1.4.4.1.1.2.2.2.2.2.2">ğ’€</ci><ci id="S2.E3.m1.4.4.1.1.2.2.2.2.2.3.cmml" xref="S2.E3.m1.4.4.1.1.2.2.2.2.2.3">ğ’•</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.4c">\theta_{\text{fine}}^{(t+1)}=\theta_{\text{fine}}^{(t)}-\eta\nabla_{\theta}%
\mathcal{L}_{\text{task}}(f_{\text{trf}}(\boldsymbol{X_{t}};\theta_{\text{fine%
}}^{(t)}),\boldsymbol{Y_{t}}),</annotation><annotation encoding="application/x-llamapun" id="S2.E3.m1.4d">italic_Î¸ start_POSTSUBSCRIPT fine end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_t + 1 ) end_POSTSUPERSCRIPT = italic_Î¸ start_POSTSUBSCRIPT fine end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT - italic_Î· âˆ‡ start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT task end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT trf end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT bold_italic_t end_POSTSUBSCRIPT ; italic_Î¸ start_POSTSUBSCRIPT fine end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT ) , bold_italic_Y start_POSTSUBSCRIPT bold_italic_t end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.12">where <math alttext="\eta" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.7.m1.1"><semantics id="S2.SS0.SSS0.Px2.p1.7.m1.1a"><mi id="S2.SS0.SSS0.Px2.p1.7.m1.1.1" xref="S2.SS0.SSS0.Px2.p1.7.m1.1.1.cmml">Î·</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.7.m1.1b"><ci id="S2.SS0.SSS0.Px2.p1.7.m1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.7.m1.1.1">ğœ‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.7.m1.1c">\eta</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.7.m1.1d">italic_Î·</annotation></semantics></math> is the learning rate, <math alttext="\boldsymbol{X_{t}}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.8.m2.1"><semantics id="S2.SS0.SSS0.Px2.p1.8.m2.1a"><msub id="S2.SS0.SSS0.Px2.p1.8.m2.1.1" xref="S2.SS0.SSS0.Px2.p1.8.m2.1.1.cmml"><mi id="S2.SS0.SSS0.Px2.p1.8.m2.1.1.2" xref="S2.SS0.SSS0.Px2.p1.8.m2.1.1.2.cmml">ğ‘¿</mi><mi id="S2.SS0.SSS0.Px2.p1.8.m2.1.1.3" xref="S2.SS0.SSS0.Px2.p1.8.m2.1.1.3.cmml">ğ’•</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.8.m2.1b"><apply id="S2.SS0.SSS0.Px2.p1.8.m2.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.8.m2.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px2.p1.8.m2.1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.8.m2.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px2.p1.8.m2.1.1.2.cmml" xref="S2.SS0.SSS0.Px2.p1.8.m2.1.1.2">ğ‘¿</ci><ci id="S2.SS0.SSS0.Px2.p1.8.m2.1.1.3.cmml" xref="S2.SS0.SSS0.Px2.p1.8.m2.1.1.3">ğ’•</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.8.m2.1c">\boldsymbol{X_{t}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.8.m2.1d">bold_italic_X start_POSTSUBSCRIPT bold_italic_t end_POSTSUBSCRIPT</annotation></semantics></math> represents the input data in iteration <math alttext="t" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.9.m3.1"><semantics id="S2.SS0.SSS0.Px2.p1.9.m3.1a"><mi id="S2.SS0.SSS0.Px2.p1.9.m3.1.1" xref="S2.SS0.SSS0.Px2.p1.9.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.9.m3.1b"><ci id="S2.SS0.SSS0.Px2.p1.9.m3.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.9.m3.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.9.m3.1c">t</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.9.m3.1d">italic_t</annotation></semantics></math>, <math alttext="\boldsymbol{Y_{t}}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.10.m4.1"><semantics id="S2.SS0.SSS0.Px2.p1.10.m4.1a"><msub id="S2.SS0.SSS0.Px2.p1.10.m4.1.1" xref="S2.SS0.SSS0.Px2.p1.10.m4.1.1.cmml"><mi id="S2.SS0.SSS0.Px2.p1.10.m4.1.1.2" xref="S2.SS0.SSS0.Px2.p1.10.m4.1.1.2.cmml">ğ’€</mi><mi id="S2.SS0.SSS0.Px2.p1.10.m4.1.1.3" xref="S2.SS0.SSS0.Px2.p1.10.m4.1.1.3.cmml">ğ’•</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.10.m4.1b"><apply id="S2.SS0.SSS0.Px2.p1.10.m4.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.10.m4.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px2.p1.10.m4.1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.10.m4.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px2.p1.10.m4.1.1.2.cmml" xref="S2.SS0.SSS0.Px2.p1.10.m4.1.1.2">ğ’€</ci><ci id="S2.SS0.SSS0.Px2.p1.10.m4.1.1.3.cmml" xref="S2.SS0.SSS0.Px2.p1.10.m4.1.1.3">ğ’•</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.10.m4.1c">\boldsymbol{Y_{t}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.10.m4.1d">bold_italic_Y start_POSTSUBSCRIPT bold_italic_t end_POSTSUBSCRIPT</annotation></semantics></math> represents the target labels in iteration <math alttext="t" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.11.m5.1"><semantics id="S2.SS0.SSS0.Px2.p1.11.m5.1a"><mi id="S2.SS0.SSS0.Px2.p1.11.m5.1.1" xref="S2.SS0.SSS0.Px2.p1.11.m5.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.11.m5.1b"><ci id="S2.SS0.SSS0.Px2.p1.11.m5.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.11.m5.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.11.m5.1c">t</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.11.m5.1d">italic_t</annotation></semantics></math>, and <math alttext="\nabla_{\theta_{\text{fine}}}\mathcal{L}_{\text{task}}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.12.m6.1"><semantics id="S2.SS0.SSS0.Px2.p1.12.m6.1a"><mrow id="S2.SS0.SSS0.Px2.p1.12.m6.1.1" xref="S2.SS0.SSS0.Px2.p1.12.m6.1.1.cmml"><msub id="S2.SS0.SSS0.Px2.p1.12.m6.1.1.1" xref="S2.SS0.SSS0.Px2.p1.12.m6.1.1.1.cmml"><mo id="S2.SS0.SSS0.Px2.p1.12.m6.1.1.1.2" xref="S2.SS0.SSS0.Px2.p1.12.m6.1.1.1.2.cmml">âˆ‡</mo><msub id="S2.SS0.SSS0.Px2.p1.12.m6.1.1.1.3" xref="S2.SS0.SSS0.Px2.p1.12.m6.1.1.1.3.cmml"><mi id="S2.SS0.SSS0.Px2.p1.12.m6.1.1.1.3.2" xref="S2.SS0.SSS0.Px2.p1.12.m6.1.1.1.3.2.cmml">Î¸</mi><mtext id="S2.SS0.SSS0.Px2.p1.12.m6.1.1.1.3.3" xref="S2.SS0.SSS0.Px2.p1.12.m6.1.1.1.3.3a.cmml">fine</mtext></msub></msub><msub id="S2.SS0.SSS0.Px2.p1.12.m6.1.1.2" xref="S2.SS0.SSS0.Px2.p1.12.m6.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS0.SSS0.Px2.p1.12.m6.1.1.2.2" xref="S2.SS0.SSS0.Px2.p1.12.m6.1.1.2.2.cmml">â„’</mi><mtext id="S2.SS0.SSS0.Px2.p1.12.m6.1.1.2.3" xref="S2.SS0.SSS0.Px2.p1.12.m6.1.1.2.3a.cmml">task</mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.12.m6.1b"><apply id="S2.SS0.SSS0.Px2.p1.12.m6.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.12.m6.1.1"><apply id="S2.SS0.SSS0.Px2.p1.12.m6.1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.12.m6.1.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px2.p1.12.m6.1.1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.12.m6.1.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px2.p1.12.m6.1.1.1.2.cmml" xref="S2.SS0.SSS0.Px2.p1.12.m6.1.1.1.2">âˆ‡</ci><apply id="S2.SS0.SSS0.Px2.p1.12.m6.1.1.1.3.cmml" xref="S2.SS0.SSS0.Px2.p1.12.m6.1.1.1.3"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px2.p1.12.m6.1.1.1.3.1.cmml" xref="S2.SS0.SSS0.Px2.p1.12.m6.1.1.1.3">subscript</csymbol><ci id="S2.SS0.SSS0.Px2.p1.12.m6.1.1.1.3.2.cmml" xref="S2.SS0.SSS0.Px2.p1.12.m6.1.1.1.3.2">ğœƒ</ci><ci id="S2.SS0.SSS0.Px2.p1.12.m6.1.1.1.3.3a.cmml" xref="S2.SS0.SSS0.Px2.p1.12.m6.1.1.1.3.3"><mtext id="S2.SS0.SSS0.Px2.p1.12.m6.1.1.1.3.3.cmml" mathsize="50%" xref="S2.SS0.SSS0.Px2.p1.12.m6.1.1.1.3.3">fine</mtext></ci></apply></apply><apply id="S2.SS0.SSS0.Px2.p1.12.m6.1.1.2.cmml" xref="S2.SS0.SSS0.Px2.p1.12.m6.1.1.2"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px2.p1.12.m6.1.1.2.1.cmml" xref="S2.SS0.SSS0.Px2.p1.12.m6.1.1.2">subscript</csymbol><ci id="S2.SS0.SSS0.Px2.p1.12.m6.1.1.2.2.cmml" xref="S2.SS0.SSS0.Px2.p1.12.m6.1.1.2.2">â„’</ci><ci id="S2.SS0.SSS0.Px2.p1.12.m6.1.1.2.3a.cmml" xref="S2.SS0.SSS0.Px2.p1.12.m6.1.1.2.3"><mtext id="S2.SS0.SSS0.Px2.p1.12.m6.1.1.2.3.cmml" mathsize="70%" xref="S2.SS0.SSS0.Px2.p1.12.m6.1.1.2.3">task</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.12.m6.1c">\nabla_{\theta_{\text{fine}}}\mathcal{L}_{\text{task}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.12.m6.1d">âˆ‡ start_POSTSUBSCRIPT italic_Î¸ start_POSTSUBSCRIPT fine end_POSTSUBSCRIPT end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT task end_POSTSUBSCRIPT</annotation></semantics></math> denotes the gradient of the loss function with respect to the model parameters. Fine-tuning typically requires substantial computational resources. For instance, full-parameter fine-tuning of LLaMA-3 with 8 billion parameters and an 8K context using the Adam optimizer and gradient checkpointing demands a minimum of 152 GB of VRAMÂ <cite class="ltx_cite ltx_citemacro_cite">Rasley etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib25" title="">2020</a>)</cite>, which equates to at least two A100 80 GB GPUs with parallel training. While parameter-efficient fine-tuning (PEFT) is less resource-intensive compared to full-parameter fine-tuning, it still requires 16 GB of VRAM (QLoRA with a 1K contextÂ <cite class="ltx_cite ltx_citemacro_cite">Dettmers etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib7" title="">2024</a>)</cite>), necessitating at least one RTX 3090 GPU. Additionally, some studies have shown that PEFT can result in a noticeable drop in the modelâ€™s performanceÂ <cite class="ltx_cite ltx_citemacro_cite">Pu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib24" title="">2023</a>); Zou etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib40" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">In-Context Learning</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.6">In-Context Learning (ICL) in LLMs is an emergent capability where the model uses the provided context to perform tasks. Given a special task <math alttext="F" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px3.p1.1.m1.1"><semantics id="S2.SS0.SSS0.Px3.p1.1.m1.1a"><mi id="S2.SS0.SSS0.Px3.p1.1.m1.1.1" xref="S2.SS0.SSS0.Px3.p1.1.m1.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px3.p1.1.m1.1b"><ci id="S2.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.1.m1.1.1">ğ¹</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px3.p1.1.m1.1c">F</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px3.p1.1.m1.1d">italic_F</annotation></semantics></math> and a series of prompt inputs <math alttext="\boldsymbol{x_{1}},\cdots,\boldsymbol{x_{n}}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px3.p1.2.m2.3"><semantics id="S2.SS0.SSS0.Px3.p1.2.m2.3a"><mrow id="S2.SS0.SSS0.Px3.p1.2.m2.3.3.2" xref="S2.SS0.SSS0.Px3.p1.2.m2.3.3.3.cmml"><msub id="S2.SS0.SSS0.Px3.p1.2.m2.2.2.1.1" xref="S2.SS0.SSS0.Px3.p1.2.m2.2.2.1.1.cmml"><mi id="S2.SS0.SSS0.Px3.p1.2.m2.2.2.1.1.2" xref="S2.SS0.SSS0.Px3.p1.2.m2.2.2.1.1.2.cmml">ğ’™</mi><mn id="S2.SS0.SSS0.Px3.p1.2.m2.2.2.1.1.3" xref="S2.SS0.SSS0.Px3.p1.2.m2.2.2.1.1.3.cmml">ğŸ</mn></msub><mo id="S2.SS0.SSS0.Px3.p1.2.m2.3.3.2.3" xref="S2.SS0.SSS0.Px3.p1.2.m2.3.3.3.cmml">,</mo><mi id="S2.SS0.SSS0.Px3.p1.2.m2.1.1" mathvariant="normal" xref="S2.SS0.SSS0.Px3.p1.2.m2.1.1.cmml">â‹¯</mi><mo id="S2.SS0.SSS0.Px3.p1.2.m2.3.3.2.4" xref="S2.SS0.SSS0.Px3.p1.2.m2.3.3.3.cmml">,</mo><msub id="S2.SS0.SSS0.Px3.p1.2.m2.3.3.2.2" xref="S2.SS0.SSS0.Px3.p1.2.m2.3.3.2.2.cmml"><mi id="S2.SS0.SSS0.Px3.p1.2.m2.3.3.2.2.2" xref="S2.SS0.SSS0.Px3.p1.2.m2.3.3.2.2.2.cmml">ğ’™</mi><mi id="S2.SS0.SSS0.Px3.p1.2.m2.3.3.2.2.3" xref="S2.SS0.SSS0.Px3.p1.2.m2.3.3.2.2.3.cmml">ğ’</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px3.p1.2.m2.3b"><list id="S2.SS0.SSS0.Px3.p1.2.m2.3.3.3.cmml" xref="S2.SS0.SSS0.Px3.p1.2.m2.3.3.2"><apply id="S2.SS0.SSS0.Px3.p1.2.m2.2.2.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px3.p1.2.m2.2.2.1.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.2.m2.2.2.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px3.p1.2.m2.2.2.1.1.2.cmml" xref="S2.SS0.SSS0.Px3.p1.2.m2.2.2.1.1.2">ğ’™</ci><cn id="S2.SS0.SSS0.Px3.p1.2.m2.2.2.1.1.3.cmml" type="integer" xref="S2.SS0.SSS0.Px3.p1.2.m2.2.2.1.1.3">1</cn></apply><ci id="S2.SS0.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.2.m2.1.1">â‹¯</ci><apply id="S2.SS0.SSS0.Px3.p1.2.m2.3.3.2.2.cmml" xref="S2.SS0.SSS0.Px3.p1.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px3.p1.2.m2.3.3.2.2.1.cmml" xref="S2.SS0.SSS0.Px3.p1.2.m2.3.3.2.2">subscript</csymbol><ci id="S2.SS0.SSS0.Px3.p1.2.m2.3.3.2.2.2.cmml" xref="S2.SS0.SSS0.Px3.p1.2.m2.3.3.2.2.2">ğ’™</ci><ci id="S2.SS0.SSS0.Px3.p1.2.m2.3.3.2.2.3.cmml" xref="S2.SS0.SSS0.Px3.p1.2.m2.3.3.2.2.3">ğ’</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px3.p1.2.m2.3c">\boldsymbol{x_{1}},\cdots,\boldsymbol{x_{n}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px3.p1.2.m2.3d">bold_italic_x start_POSTSUBSCRIPT bold_1 end_POSTSUBSCRIPT , â‹¯ , bold_italic_x start_POSTSUBSCRIPT bold_italic_n end_POSTSUBSCRIPT</annotation></semantics></math>, ICL happens when these inputs and their answers <math alttext="\boldsymbol{y_{1}}=F(\boldsymbol{x_{1}})" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px3.p1.3.m3.1"><semantics id="S2.SS0.SSS0.Px3.p1.3.m3.1a"><mrow id="S2.SS0.SSS0.Px3.p1.3.m3.1.1" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.cmml"><msub id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.3" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.3.cmml"><mi id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.3.2" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.3.2.cmml">ğ’š</mi><mn id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.3.3" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.cmml">ğŸ</mn></msub><mo id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.2" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.2.cmml">=</mo><mrow id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.cmml"><mi id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.3" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.3.cmml">F</mi><mo id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.2" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.2.cmml">â¢</mo><mrow id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.1" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.cmml"><mo id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.1.2" stretchy="false" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.cmml">(</mo><msub id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.1.1" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.cmml"><mi id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.2" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.2.cmml">ğ’™</mi><mn id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.3" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.3.cmml">ğŸ</mn></msub><mo id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.1.3" stretchy="false" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px3.p1.3.m3.1b"><apply id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1"><eq id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.2.cmml" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.2"></eq><apply id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.3.cmml" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.3.1.cmml" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.3">subscript</csymbol><ci id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.3.2.cmml" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.3.2">ğ’š</ci><cn id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.cmml" type="integer" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.3.3">1</cn></apply><apply id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1"><times id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.2.cmml" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.2"></times><ci id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.3.cmml" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.3">ğ¹</ci><apply id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.2.cmml" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.2">ğ’™</ci><cn id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.3.cmml" type="integer" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px3.p1.3.m3.1c">\boldsymbol{y_{1}}=F(\boldsymbol{x_{1}})</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px3.p1.3.m3.1d">bold_italic_y start_POSTSUBSCRIPT bold_1 end_POSTSUBSCRIPT = italic_F ( bold_italic_x start_POSTSUBSCRIPT bold_1 end_POSTSUBSCRIPT )</annotation></semantics></math> are given in multi-shot, <em class="ltx_emph ltx_font_italic" id="S2.SS0.SSS0.Px3.p1.6.1">i.e., </em>Â <math alttext="(\boldsymbol{x_{1}},\boldsymbol{y_{1}},\cdots,\boldsymbol{y_{n}},\boldsymbol{x%
_{n+1}})" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px3.p1.4.m4.5"><semantics id="S2.SS0.SSS0.Px3.p1.4.m4.5a"><mrow id="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4" xref="S2.SS0.SSS0.Px3.p1.4.m4.5.5.5.cmml"><mo id="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.5" stretchy="false" xref="S2.SS0.SSS0.Px3.p1.4.m4.5.5.5.cmml">(</mo><msub id="S2.SS0.SSS0.Px3.p1.4.m4.2.2.1.1" xref="S2.SS0.SSS0.Px3.p1.4.m4.2.2.1.1.cmml"><mi id="S2.SS0.SSS0.Px3.p1.4.m4.2.2.1.1.2" xref="S2.SS0.SSS0.Px3.p1.4.m4.2.2.1.1.2.cmml">ğ’™</mi><mn id="S2.SS0.SSS0.Px3.p1.4.m4.2.2.1.1.3" xref="S2.SS0.SSS0.Px3.p1.4.m4.2.2.1.1.3.cmml">ğŸ</mn></msub><mo id="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.6" xref="S2.SS0.SSS0.Px3.p1.4.m4.5.5.5.cmml">,</mo><msub id="S2.SS0.SSS0.Px3.p1.4.m4.3.3.2.2" xref="S2.SS0.SSS0.Px3.p1.4.m4.3.3.2.2.cmml"><mi id="S2.SS0.SSS0.Px3.p1.4.m4.3.3.2.2.2" xref="S2.SS0.SSS0.Px3.p1.4.m4.3.3.2.2.2.cmml">ğ’š</mi><mn id="S2.SS0.SSS0.Px3.p1.4.m4.3.3.2.2.3" xref="S2.SS0.SSS0.Px3.p1.4.m4.3.3.2.2.3.cmml">ğŸ</mn></msub><mo id="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.7" xref="S2.SS0.SSS0.Px3.p1.4.m4.5.5.5.cmml">,</mo><mi id="S2.SS0.SSS0.Px3.p1.4.m4.1.1" mathvariant="normal" xref="S2.SS0.SSS0.Px3.p1.4.m4.1.1.cmml">â‹¯</mi><mo id="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.8" xref="S2.SS0.SSS0.Px3.p1.4.m4.5.5.5.cmml">,</mo><msub id="S2.SS0.SSS0.Px3.p1.4.m4.4.4.3.3" xref="S2.SS0.SSS0.Px3.p1.4.m4.4.4.3.3.cmml"><mi id="S2.SS0.SSS0.Px3.p1.4.m4.4.4.3.3.2" xref="S2.SS0.SSS0.Px3.p1.4.m4.4.4.3.3.2.cmml">ğ’š</mi><mi id="S2.SS0.SSS0.Px3.p1.4.m4.4.4.3.3.3" xref="S2.SS0.SSS0.Px3.p1.4.m4.4.4.3.3.3.cmml">ğ’</mi></msub><mo id="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.9" xref="S2.SS0.SSS0.Px3.p1.4.m4.5.5.5.cmml">,</mo><msub id="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.4" xref="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.4.cmml"><mi id="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.4.2" xref="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.4.2.cmml">ğ’™</mi><mrow id="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.4.3" xref="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.4.3.cmml"><mi id="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.4.3.2" xref="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.4.3.2.cmml">ğ’</mi><mo class="ltx_mathvariant_bold" id="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.4.3.1" mathvariant="bold" xref="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.4.3.1.cmml">+</mo><mn id="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.4.3.3" xref="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.4.3.3.cmml">ğŸ</mn></mrow></msub><mo id="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.10" stretchy="false" xref="S2.SS0.SSS0.Px3.p1.4.m4.5.5.5.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px3.p1.4.m4.5b"><vector id="S2.SS0.SSS0.Px3.p1.4.m4.5.5.5.cmml" xref="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4"><apply id="S2.SS0.SSS0.Px3.p1.4.m4.2.2.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.4.m4.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px3.p1.4.m4.2.2.1.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.4.m4.2.2.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px3.p1.4.m4.2.2.1.1.2.cmml" xref="S2.SS0.SSS0.Px3.p1.4.m4.2.2.1.1.2">ğ’™</ci><cn id="S2.SS0.SSS0.Px3.p1.4.m4.2.2.1.1.3.cmml" type="integer" xref="S2.SS0.SSS0.Px3.p1.4.m4.2.2.1.1.3">1</cn></apply><apply id="S2.SS0.SSS0.Px3.p1.4.m4.3.3.2.2.cmml" xref="S2.SS0.SSS0.Px3.p1.4.m4.3.3.2.2"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px3.p1.4.m4.3.3.2.2.1.cmml" xref="S2.SS0.SSS0.Px3.p1.4.m4.3.3.2.2">subscript</csymbol><ci id="S2.SS0.SSS0.Px3.p1.4.m4.3.3.2.2.2.cmml" xref="S2.SS0.SSS0.Px3.p1.4.m4.3.3.2.2.2">ğ’š</ci><cn id="S2.SS0.SSS0.Px3.p1.4.m4.3.3.2.2.3.cmml" type="integer" xref="S2.SS0.SSS0.Px3.p1.4.m4.3.3.2.2.3">1</cn></apply><ci id="S2.SS0.SSS0.Px3.p1.4.m4.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.4.m4.1.1">â‹¯</ci><apply id="S2.SS0.SSS0.Px3.p1.4.m4.4.4.3.3.cmml" xref="S2.SS0.SSS0.Px3.p1.4.m4.4.4.3.3"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px3.p1.4.m4.4.4.3.3.1.cmml" xref="S2.SS0.SSS0.Px3.p1.4.m4.4.4.3.3">subscript</csymbol><ci id="S2.SS0.SSS0.Px3.p1.4.m4.4.4.3.3.2.cmml" xref="S2.SS0.SSS0.Px3.p1.4.m4.4.4.3.3.2">ğ’š</ci><ci id="S2.SS0.SSS0.Px3.p1.4.m4.4.4.3.3.3.cmml" xref="S2.SS0.SSS0.Px3.p1.4.m4.4.4.3.3.3">ğ’</ci></apply><apply id="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.4.cmml" xref="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.4"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.4.1.cmml" xref="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.4">subscript</csymbol><ci id="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.4.2.cmml" xref="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.4.2">ğ’™</ci><apply id="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.4.3.cmml" xref="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.4.3"><plus id="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.4.3.1.cmml" xref="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.4.3.1"></plus><ci id="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.4.3.2.cmml" xref="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.4.3.2">ğ’</ci><cn id="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.4.3.3.cmml" type="integer" xref="S2.SS0.SSS0.Px3.p1.4.m4.5.5.4.4.3.3">1</cn></apply></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px3.p1.4.m4.5c">(\boldsymbol{x_{1}},\boldsymbol{y_{1}},\cdots,\boldsymbol{y_{n}},\boldsymbol{x%
_{n+1}})</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px3.p1.4.m4.5d">( bold_italic_x start_POSTSUBSCRIPT bold_1 end_POSTSUBSCRIPT , bold_italic_y start_POSTSUBSCRIPT bold_1 end_POSTSUBSCRIPT , â‹¯ , bold_italic_y start_POSTSUBSCRIPT bold_italic_n end_POSTSUBSCRIPT , bold_italic_x start_POSTSUBSCRIPT bold_italic_n bold_+ bold_1 end_POSTSUBSCRIPT )</annotation></semantics></math>. In this scenario, the goal for LLM to do ICL is to learn the task <math alttext="F" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px3.p1.5.m5.1"><semantics id="S2.SS0.SSS0.Px3.p1.5.m5.1a"><mi id="S2.SS0.SSS0.Px3.p1.5.m5.1.1" xref="S2.SS0.SSS0.Px3.p1.5.m5.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px3.p1.5.m5.1b"><ci id="S2.SS0.SSS0.Px3.p1.5.m5.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.5.m5.1.1">ğ¹</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px3.p1.5.m5.1c">F</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px3.p1.5.m5.1d">italic_F</annotation></semantics></math> and accurately predict <math alttext="\boldsymbol{y_{n+1}}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px3.p1.6.m6.1"><semantics id="S2.SS0.SSS0.Px3.p1.6.m6.1a"><msub id="S2.SS0.SSS0.Px3.p1.6.m6.1.1" xref="S2.SS0.SSS0.Px3.p1.6.m6.1.1.cmml"><mi id="S2.SS0.SSS0.Px3.p1.6.m6.1.1.2" xref="S2.SS0.SSS0.Px3.p1.6.m6.1.1.2.cmml">ğ’š</mi><mrow id="S2.SS0.SSS0.Px3.p1.6.m6.1.1.3" xref="S2.SS0.SSS0.Px3.p1.6.m6.1.1.3.cmml"><mi id="S2.SS0.SSS0.Px3.p1.6.m6.1.1.3.2" xref="S2.SS0.SSS0.Px3.p1.6.m6.1.1.3.2.cmml">ğ’</mi><mo class="ltx_mathvariant_bold" id="S2.SS0.SSS0.Px3.p1.6.m6.1.1.3.1" mathvariant="bold" xref="S2.SS0.SSS0.Px3.p1.6.m6.1.1.3.1.cmml">+</mo><mn id="S2.SS0.SSS0.Px3.p1.6.m6.1.1.3.3" xref="S2.SS0.SSS0.Px3.p1.6.m6.1.1.3.3.cmml">ğŸ</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px3.p1.6.m6.1b"><apply id="S2.SS0.SSS0.Px3.p1.6.m6.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px3.p1.6.m6.1.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.6.m6.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px3.p1.6.m6.1.1.2.cmml" xref="S2.SS0.SSS0.Px3.p1.6.m6.1.1.2">ğ’š</ci><apply id="S2.SS0.SSS0.Px3.p1.6.m6.1.1.3.cmml" xref="S2.SS0.SSS0.Px3.p1.6.m6.1.1.3"><plus id="S2.SS0.SSS0.Px3.p1.6.m6.1.1.3.1.cmml" xref="S2.SS0.SSS0.Px3.p1.6.m6.1.1.3.1"></plus><ci id="S2.SS0.SSS0.Px3.p1.6.m6.1.1.3.2.cmml" xref="S2.SS0.SSS0.Px3.p1.6.m6.1.1.3.2">ğ’</ci><cn id="S2.SS0.SSS0.Px3.p1.6.m6.1.1.3.3.cmml" type="integer" xref="S2.SS0.SSS0.Px3.p1.6.m6.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px3.p1.6.m6.1c">\boldsymbol{y_{n+1}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px3.p1.6.m6.1d">bold_italic_y start_POSTSUBSCRIPT bold_italic_n bold_+ bold_1 end_POSTSUBSCRIPT</annotation></semantics></math>. This phenomenon allows the model to adaptively handle a variety of tasks, such as translation, question-answering, and more, simply through appropriate prompt engineering. ICL happens in inference-stage without explicit re-training, thus resulting in more friendly requirements for GPUsÂ <cite class="ltx_cite ltx_citemacro_cite">Yin etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib32" title="">2024</a>); Hong etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib14" title="">2023</a>)</cite>. Even LLaMA-3 70B could run on a single 3090 GPU with PowerInferÂ <cite class="ltx_cite ltx_citemacro_cite">Song etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib28" title="">2023</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Implicit Pattern Detection Test</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Through detailed observation and thinking, humans can detect some underlying, non-explicit patterns within the data. This enables us to solve problems more efficiently. Implicit pattern detection refers to the ability of models to recognize underlying, non-explicit patterns within data, enabling them to solve problems more efficiently. This concept is illustrated through tasks such as arithmetic calculations, where the model can bypass complex operations by identifying simplifying patterns. For instance, in mathematical expressions (see FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_tag">1</span></a> and FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S1.F2" title="Figure 2 â€£ Mechanism explanation. â€£ 1 Introduction â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_tag">2</span></a>), a model might detect that certain terms have negligible impact and can be ignored, leading to quicker computations. We will give a detailed description of our dataset design and experimental settings in the following sections.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Tasks</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">To effectively assess the ability of LLMs to identify implicit patterns in data, we have constructed a variety of questions that frequently arise in real-world application scenarios. When the same type of question recurs, we can discover a specific implicit pattern within it to simplify the computational process.</p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Task 1: Expression CalculationÂ <cite class="ltx_cite ltx_citemacro_cite">Imani etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib17" title="">2023</a>); Yuan etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib34" title="">2023</a>); Yue etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib35" title="">2023</a>); He-Yueya etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib13" title="">2023</a>)</cite>
</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.4">In the arithmetic calculation task, the primary focus is on determining whether certain operations within a given expression can be disregarded to reduce the complexity of the computation. The operations considered for these simplifications are limited to addition(<math alttext="+" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS1.SSS0.Px1.p1.1.m1.1a"><mo id="S3.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.1.m1.1b"><plus id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.1.m1.1c">+</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.1.m1.1d">+</annotation></semantics></math>), subtraction(<math alttext="-" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.2.m2.1"><semantics id="S3.SS1.SSS0.Px1.p1.2.m2.1a"><mo id="S3.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.cmml">âˆ’</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.2.m2.1b"><minus id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.2.m2.1c">-</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.2.m2.1d">-</annotation></semantics></math>), multiplication(<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.3.m3.1"><semantics id="S3.SS1.SSS0.Px1.p1.3.m3.1a"><mo id="S3.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.3.m3.1b"><times id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.3.m3.1d">Ã—</annotation></semantics></math>), and division(<math alttext="/" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.4.m4.1"><semantics id="S3.SS1.SSS0.Px1.p1.4.m4.1a"><mo id="S3.SS1.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1.cmml">/</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.4.m4.1b"><divide id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1"></divide></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.4.m4.1c">/</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.4.m4.1d">/</annotation></semantics></math>). By exploring these operations, the model may find that several terms are multiplied by a continued-to-be-zero term, and ignoring them could simplify the calculation process and improve the accuracy.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Task 2: Code ReadingÂ <cite class="ltx_cite ltx_citemacro_cite">Fang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib10" title="">2024</a>)</cite>
</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">In the code reading task, LLMs need to analyze and predict the output of a given piece of code without executing it, where multiple functions are defined. Some functions will not influence the final output, so the key challenge is to determine which functions are essential for producing the output and which can be disregarded without affecting the result.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Task 3: Boolean FunctionsÂ <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib37" title="">2024</a>)</cite>
</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.3">In the Boolean functions task, the primary objective is to optimize logical expressions to simplify their structure without altering the resultant truth value. The expressions involve logical operators such as AND (<math alttext="\land" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.1.m1.1"><semantics id="S3.SS1.SSS0.Px3.p1.1.m1.1a"><mo id="S3.SS1.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.cmml">âˆ§</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.1.m1.1b"><and id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.1.m1.1c">\land</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.1.m1.1d">âˆ§</annotation></semantics></math>), OR (<math alttext="\lor" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.2.m2.1"><semantics id="S3.SS1.SSS0.Px3.p1.2.m2.1a"><mo id="S3.SS1.SSS0.Px3.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1.cmml">âˆ¨</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.2.m2.1b"><or id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1"></or></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.2.m2.1c">\lor</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.2.m2.1d">âˆ¨</annotation></semantics></math>), and NOT (<math alttext="\neg" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.3.m3.1"><semantics id="S3.SS1.SSS0.Px3.p1.3.m3.1a"><mo id="S3.SS1.SSS0.Px3.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px3.p1.3.m3.1.1.cmml">Â¬</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.3.m3.1b"><not id="S3.SS1.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.3.m3.1.1"></not></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.3.m3.1c">\neg</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.3.m3.1d">Â¬</annotation></semantics></math>). Within these scenarios, there are specific segments that are either tautologies, <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS0.Px3.p1.3.1">i.e., </em>always true, or contradictions, <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS0.Px3.p1.3.2">i.e., </em>always false. The model must identify these segments and bypass their computation.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Task 4: Relation ReasoningÂ <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib19" title="">2024</a>)</cite>
</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px4.p1.1">In the task of relation reasoning, the focus is on determining the relationships between multiple entities, such as reachability and relative magnitude. Although the set of relationships involved can be complex, all queries target fixed entities whose relationships are relatively straightforward. Therefore, most of the complex relationships can be disregarded, simplifying the problem-solving process.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.1">
<tr class="ltx_tr" id="S3.T1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.1.1.1" style="padding-left:2.5pt;padding-right:2.5pt;">Model</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S3.T1.1.1.2" style="padding-left:2.5pt;padding-right:2.5pt;">Expression</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S3.T1.1.1.3" style="padding-left:2.5pt;padding-right:2.5pt;">Code</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S3.T1.1.1.4" style="padding-left:2.5pt;padding-right:2.5pt;">Relation</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S3.T1.1.1.5" style="padding-left:2.5pt;padding-right:2.5pt;">Boolean</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2">
<td class="ltx_td ltx_border_t" id="S3.T1.1.2.1" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.2" style="padding-left:2.5pt;padding-right:2.5pt;">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.3" style="padding-left:2.5pt;padding-right:2.5pt;">Full-ft</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.2.4" style="padding-left:2.5pt;padding-right:2.5pt;">ICL</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.5" style="padding-left:2.5pt;padding-right:2.5pt;">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.6" style="padding-left:2.5pt;padding-right:2.5pt;">Full-ft</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.2.7" style="padding-left:2.5pt;padding-right:2.5pt;">ICL</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.8" style="padding-left:2.5pt;padding-right:2.5pt;">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.9" style="padding-left:2.5pt;padding-right:2.5pt;">Full-ft</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.2.10" style="padding-left:2.5pt;padding-right:2.5pt;">ICL</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.11" style="padding-left:2.5pt;padding-right:2.5pt;">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.12" style="padding-left:2.5pt;padding-right:2.5pt;">Full-ft</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.13" style="padding-left:2.5pt;padding-right:2.5pt;">ICL</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" colspan="3" id="S3.T1.1.3.1" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_italic" id="S3.T1.1.3.1.1">0.5B level</span></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.3.2" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.3.3" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.3.4" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.3.5" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.3.6" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.3.7" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T1.1.3.8" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.3.9" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.3.10" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.3.11" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4">
<td class="ltx_td ltx_align_left" id="S3.T1.1.4.1" style="padding-left:2.5pt;padding-right:2.5pt;">Qwen1.5-0.5B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.2" style="padding-left:2.5pt;padding-right:2.5pt;">22.2%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.3" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.4.3.1">88.4%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.4.4" style="padding-left:2.5pt;padding-right:2.5pt;">50.1%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.5" style="padding-left:2.5pt;padding-right:2.5pt;">16.6%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.6" style="padding-left:2.5pt;padding-right:2.5pt;">2.0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.4.7" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.4.7.1">32.2%</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.8" style="padding-left:2.5pt;padding-right:2.5pt;">48.8%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.9" style="padding-left:2.5pt;padding-right:2.5pt;">48.5%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.4.10" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.4.10.1">60.1%</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.11" style="padding-left:2.5pt;padding-right:2.5pt;">54.8%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.12" style="padding-left:2.5pt;padding-right:2.5pt;">51.7%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.13" style="padding-left:2.5pt;padding-right:2.5pt;">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.4.13.1">65.3</span>%</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5">
<td class="ltx_td ltx_align_left" colspan="3" id="S3.T1.1.5.1" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_italic" id="S3.T1.1.5.1.1">1B level</span></td>
<td class="ltx_td ltx_border_r" id="S3.T1.1.5.2" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td" id="S3.T1.1.5.3" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td" id="S3.T1.1.5.4" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_border_r" id="S3.T1.1.5.5" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td" id="S3.T1.1.5.6" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td" id="S3.T1.1.5.7" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_border_r" id="S3.T1.1.5.8" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td" id="S3.T1.1.5.9" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td" id="S3.T1.1.5.10" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td" id="S3.T1.1.5.11" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.6">
<td class="ltx_td ltx_align_left" id="S3.T1.1.6.1" style="padding-left:2.5pt;padding-right:2.5pt;">GPTNeo-1.3B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.2" style="padding-left:2.5pt;padding-right:2.5pt;">24.3%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.3" style="padding-left:2.5pt;padding-right:2.5pt;">46.6%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.6.4" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.6.4.1">55.6%</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.5" style="padding-left:2.5pt;padding-right:2.5pt;">27.6%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.6" style="padding-left:2.5pt;padding-right:2.5pt;">17.7%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.6.7" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.6.7.1">44.5%</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.8" style="padding-left:2.5pt;padding-right:2.5pt;">20.5%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.9" style="padding-left:2.5pt;padding-right:2.5pt;">34.7%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.6.10" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.6.10.1">37.4%</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.11" style="padding-left:2.5pt;padding-right:2.5pt;">53.8%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.12" style="padding-left:2.5pt;padding-right:2.5pt;">53.7%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.13" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.6.13.1">54.3%</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.7">
<td class="ltx_td ltx_align_left" id="S3.T1.1.7.1" style="padding-left:2.5pt;padding-right:2.5pt;">Qwen1.5-1.8B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.2" style="padding-left:2.5pt;padding-right:2.5pt;">16.2%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.3" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.7.3.1">89.9%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.7.4" style="padding-left:2.5pt;padding-right:2.5pt;">63.4%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.5" style="padding-left:2.5pt;padding-right:2.5pt;">54.3%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.6" style="padding-left:2.5pt;padding-right:2.5pt;">53.7%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.7.7" style="padding-left:2.5pt;padding-right:2.5pt;">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.7.7.1">58.2</span>%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.8" style="padding-left:2.5pt;padding-right:2.5pt;">20.1%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.9" style="padding-left:2.5pt;padding-right:2.5pt;">21.3%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.7.10" style="padding-left:2.5pt;padding-right:2.5pt;">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.7.10.1">35.6</span>%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.11" style="padding-left:2.5pt;padding-right:2.5pt;">66.3%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.12" style="padding-left:2.5pt;padding-right:2.5pt;">66.3%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.13" style="padding-left:2.5pt;padding-right:2.5pt;">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.7.13.1">68.1</span>%</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.8">
<td class="ltx_td ltx_align_left" id="S3.T1.1.8.1" style="padding-left:2.5pt;padding-right:2.5pt;">Pythia-1.4B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.2" style="padding-left:2.5pt;padding-right:2.5pt;">5.0%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.3" style="padding-left:2.5pt;padding-right:2.5pt;">45.4%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.8.4" style="padding-left:2.5pt;padding-right:2.5pt;">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.8.4.1">53.7</span>%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.5" style="padding-left:2.5pt;padding-right:2.5pt;">37.6%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.6" style="padding-left:2.5pt;padding-right:2.5pt;">46.5%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.8.7" style="padding-left:2.5pt;padding-right:2.5pt;">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.8.7.1">53.1</span>%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.8" style="padding-left:2.5pt;padding-right:2.5pt;">20.5%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.9" style="padding-left:2.5pt;padding-right:2.5pt;">31.3%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.8.10" style="padding-left:2.5pt;padding-right:2.5pt;">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.8.10.1">44.4</span>%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.11" style="padding-left:2.5pt;padding-right:2.5pt;">61.3%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.12" style="padding-left:2.5pt;padding-right:2.5pt;">63.7%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.13" style="padding-left:2.5pt;padding-right:2.5pt;">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.8.13.1">68.5</span>%</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.9">
<td class="ltx_td ltx_align_left" colspan="3" id="S3.T1.1.9.1" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_italic" id="S3.T1.1.9.1.1">7B level</span></td>
<td class="ltx_td ltx_border_r" id="S3.T1.1.9.2" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td" id="S3.T1.1.9.3" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td" id="S3.T1.1.9.4" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_border_r" id="S3.T1.1.9.5" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td" id="S3.T1.1.9.6" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td" id="S3.T1.1.9.7" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td ltx_border_r" id="S3.T1.1.9.8" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td" id="S3.T1.1.9.9" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td" id="S3.T1.1.9.10" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td class="ltx_td" id="S3.T1.1.9.11" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.10">
<td class="ltx_td ltx_align_left" id="S3.T1.1.10.1" style="padding-left:2.5pt;padding-right:2.5pt;">Yi-6B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.2" style="padding-left:2.5pt;padding-right:2.5pt;">12.5%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.3" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.10.3.1">88.2%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.10.4" style="padding-left:2.5pt;padding-right:2.5pt;">48.2%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.5" style="padding-left:2.5pt;padding-right:2.5pt;">51.2%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.6" style="padding-left:2.5pt;padding-right:2.5pt;">78.7%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.10.7" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.10.7.1">80.9%</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.8" style="padding-left:2.5pt;padding-right:2.5pt;">48.0%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.9" style="padding-left:2.5pt;padding-right:2.5pt;">52.5%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.10.10" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.10.10.1">98.0%</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.11" style="padding-left:2.5pt;padding-right:2.5pt;">55.7%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.12" style="padding-left:2.5pt;padding-right:2.5pt;">64.1%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.13" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.10.13.1">68.3%</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.11">
<td class="ltx_td ltx_align_left" id="S3.T1.1.11.1" style="padding-left:2.5pt;padding-right:2.5pt;">Qwen1.5-7B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.2" style="padding-left:2.5pt;padding-right:2.5pt;">78.0%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.3" style="padding-left:2.5pt;padding-right:2.5pt;">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.11.3.1">89.3</span>%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.11.4" style="padding-left:2.5pt;padding-right:2.5pt;">67.9%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.5" style="padding-left:2.5pt;padding-right:2.5pt;">57.6%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.6" style="padding-left:2.5pt;padding-right:2.5pt;">72.0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.11.7" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.11.7.1">86.8%</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.8" style="padding-left:2.5pt;padding-right:2.5pt;">48.0%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.9" style="padding-left:2.5pt;padding-right:2.5pt;">78.8%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.11.10" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.11.10.1">98.0%</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.11" style="padding-left:2.5pt;padding-right:2.5pt;">71.9%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.12" style="padding-left:2.5pt;padding-right:2.5pt;">41.7%</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.13" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.11.13.1">79.8%</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.12">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.1.12.1" style="padding-left:2.5pt;padding-right:2.5pt;">Mistral-7B</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.12.2" style="padding-left:2.5pt;padding-right:2.5pt;">32.6%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.12.3" style="padding-left:2.5pt;padding-right:2.5pt;">75.2%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T1.1.12.4" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.12.4.1">76.3%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.12.5" style="padding-left:2.5pt;padding-right:2.5pt;">14.1%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.12.6" style="padding-left:2.5pt;padding-right:2.5pt;">72.0%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T1.1.12.7" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.12.7.1">82.8%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.12.8" style="padding-left:2.5pt;padding-right:2.5pt;">48.5%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.12.9" style="padding-left:2.5pt;padding-right:2.5pt;">72.5%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T1.1.12.10" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.12.10.1">90.9%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.12.11" style="padding-left:2.5pt;padding-right:2.5pt;">45.7%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.12.12" style="padding-left:2.5pt;padding-right:2.5pt;">54.5%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.12.13" style="padding-left:2.5pt;padding-right:2.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.12.13.1">74.3%</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Experimental results of implicit pattern detection tasks. We conducted experiments from 0.5B to 7B across 6 models. The highest accuracy was highlighted with boldsymbol.</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="226" id="S3.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Robustness test of implicit pattern detection test. The horizontal axis represents the accuracy under clean input, and the vertical axis represents the accuracy under misleading input. Relatively speaking, the closer the results are to the bottom right corner, the worse the methodâ€™s resistance to misleading data. The closer the results are to the top left corner, the better it is.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Settings</h3>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Accuracy.</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1">Our tasks were constructed such that implicit patterns can help solve problems more easily. For example, if an LLM identifies a term that continues to be zero in arithmetic calculations, it can ignore terms multiplied by it, thereby saving computation. Therefore, we evaluate the modelâ€™s performance with <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS0.Px1.p1.1.1">Accuracy</span>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Misleading Data.</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.1">LLMs can detect the inner implicit patterns in data and utilize them for simplifying problem-solving. The misleading data is designed to test if LLMs can tackle situations in the absence of implicit patterns. While implicit patterns are still provided in training or ICL data, misleading data, <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS0.Px2.p1.1.1">i.e., </em>, data with no implicit patterns, is provided for testing accuracy. We name this accuracy <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS0.Px2.p1.1.2">Misleading Accuracy</span>, while the testing results of data with implicit patterns are named <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS0.Px2.p1.1.3">Clean Accuracy</span>. Detailed experimental procedures can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#A2" title="Appendix B Misleading Data Construction â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Out-Of-Distribution Data.</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px3.p1.1">The training data are sampled from a certain distribution, <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS0.Px3.p1.1.1">e.g., </em>, for expression tasks, there are no more than 10 terms in each expression. Our out-of-distribution (OOD) data are designed to evaluate the modelâ€™s performance when encountering OOD data during the evaluation phase. Detailed experimental procedures can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#A3" title="Appendix C OOD data Construction â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Models.</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px4.p1.1">We select open-sourced models in sizes of 0.5B level <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS0.Px4.p1.1.1">e.g., </em>Qwen1.5-0m5B, 1B level <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS0.Px4.p1.1.2">e.g., </em>Â GPTNeo-1.3BÂ <cite class="ltx_cite ltx_citemacro_cite">Black etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib5" title="">2021</a>)</cite>, Pythia-1.4BÂ <cite class="ltx_cite ltx_citemacro_cite">Biderman etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib4" title="">2023</a>)</cite>, Qwen1.5-1.8BÂ <cite class="ltx_cite ltx_citemacro_cite">Bai etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib1" title="">2023</a>)</cite>, and 7B level <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS0.Px4.p1.1.3">e.g., </em>Â Mistral-7BÂ <cite class="ltx_cite ltx_citemacro_cite">Jiang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib18" title="">2023</a>)</cite>, Qwen1.5-7BÂ <cite class="ltx_cite ltx_citemacro_cite">Bai etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib1" title="">2023</a>)</cite>, Yi-6BÂ <cite class="ltx_cite ltx_citemacro_cite">Young etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib33" title="">2024</a>)</cite>. Model weights are downloaded from Huggingface and follow the official implementations.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px5">
<h4 class="ltx_title ltx_title_paragraph">Data Format.</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px5.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px5.p1.1">For fine-tuning, the data is provided in a single example without supervised instruction. A simple description, the question, and the answer are given in order. We prepared 1,600 data points for fine-tuning. For in-context learning, we constructed the input in multi-shot, ranging from 0-shot, <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS0.Px5.p1.1.1">i.e., </em>directly answer one question, to 32-shot <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS0.Px5.p1.1.2">i.e., </em>32 examples with their answers first given, then a new question in the same kind required to answer. The detailed example of our data format could be found in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#A1" title="Appendix A Data Format and Example â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px6">
<h4 class="ltx_title ltx_title_paragraph">Training Details.</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px6.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px6.p1.1">The training process was conducted using a sequence length of 512 and a batch size of 8 with a total of 1 epoch. A warmup phase of 20 steps was implemented, starting with a learning rate of 1e-6 and peaking at 2e-5, followed by a linear decay. The AdamW optimizer was used. This configuration ensured the modelâ€™s performance and stability, allowing it to effectively learn and identify hidden patterns in the data.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results and Analysis</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we present our results for the implicit pattern finding tasks following the experimental setting in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S3.SS2" title="3.2 Settings â€£ 3 Implicit Pattern Detection Test â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_tag">3.2</span></a>. We show that ICL achieved an overall higher level of accuracy over fine-tuning on these four tasks. We also show that the improvement of accuracy with ICL mainly comes from the detection of those implicit patterns in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S5" title="5 Explanation of ICLâ€™s Victory: Circuits Shift Theory â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_tag">5</span></a> andÂ refsec:circuit.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T2.1">
<tr class="ltx_tr" id="S4.T2.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">Method Type</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.2" style="padding-left:4.0pt;padding-right:4.0pt;">Expression</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.3" style="padding-left:4.0pt;padding-right:4.0pt;">Code</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.4" style="padding-left:4.0pt;padding-right:4.0pt;">Relation</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.5" style="padding-left:4.0pt;padding-right:4.0pt;">Boolean</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.2.1" style="padding-left:4.0pt;padding-right:4.0pt;">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">27.5%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.3" style="padding-left:4.0pt;padding-right:4.0pt;">54.3%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.4" style="padding-left:4.0pt;padding-right:4.0pt;">20.1%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.5" style="padding-left:4.0pt;padding-right:4.0pt;">66.3%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.3.1" style="padding-left:4.0pt;padding-right:4.0pt;">Full-Param FT</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.2" style="padding-left:4.0pt;padding-right:4.0pt;">89.9%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.3" style="padding-left:4.0pt;padding-right:4.0pt;">53.7%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.4" style="padding-left:4.0pt;padding-right:4.0pt;">21.3%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.5" style="padding-left:4.0pt;padding-right:4.0pt;">66.3%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4">
<td class="ltx_td ltx_align_left" id="S4.T2.1.4.1" style="padding-left:4.0pt;padding-right:4.0pt;">LoRA</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.2" style="padding-left:4.0pt;padding-right:4.0pt;">46.5%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3" style="padding-left:4.0pt;padding-right:4.0pt;">53.3%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.4" style="padding-left:4.0pt;padding-right:4.0pt;">20.1%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.5" style="padding-left:4.0pt;padding-right:4.0pt;">64.3%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5">
<td class="ltx_td ltx_align_left" id="S4.T2.1.5.1" style="padding-left:4.0pt;padding-right:4.0pt;">QLoRA</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.2" style="padding-left:4.0pt;padding-right:4.0pt;">46.2%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.3" style="padding-left:4.0pt;padding-right:4.0pt;">51.6%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4" style="padding-left:4.0pt;padding-right:4.0pt;">20.5%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.5" style="padding-left:4.0pt;padding-right:4.0pt;">61.3%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6">
<td class="ltx_td ltx_align_left" id="S4.T2.1.6.1" style="padding-left:4.0pt;padding-right:4.0pt;">GaLoRA</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.2" style="padding-left:4.0pt;padding-right:4.0pt;">47.1%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.3" style="padding-left:4.0pt;padding-right:4.0pt;">52.5%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.4" style="padding-left:4.0pt;padding-right:4.0pt;">20.5%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.5" style="padding-left:4.0pt;padding-right:4.0pt;">66.4%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.7">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T2.1.7.1" style="padding-left:4.0pt;padding-right:4.0pt;">ICL</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.7.2" style="padding-left:4.0pt;padding-right:4.0pt;">63.4%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.7.3" style="padding-left:4.0pt;padding-right:4.0pt;">58.2%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.7.4" style="padding-left:4.0pt;padding-right:4.0pt;">35.6%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.7.5" style="padding-left:4.0pt;padding-right:4.0pt;">68.1%</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Experimental comparison of different PEFT methods. We compared the results on Qwen1.5-1.8B. It is obvious that PEFT shows no significant improvement compared to full-param fine-tuning and seems to have limited performance.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T3.1">
<tr class="ltx_tr" id="S4.T3.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;">OOD Type</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.1.1.2" style="padding-left:3.0pt;padding-right:3.0pt;">Expression</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.1.1.3" style="padding-left:3.0pt;padding-right:3.0pt;">Code</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.1.1.4" style="padding-left:3.0pt;padding-right:3.0pt;">Relation</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.1.1.5" style="padding-left:3.0pt;padding-right:3.0pt;">Boolean</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1" style="padding-left:3.0pt;padding-right:3.0pt;">Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.2" style="padding-left:3.0pt;padding-right:3.0pt;">27.5%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.3" style="padding-left:3.0pt;padding-right:3.0pt;">54.3%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.4" style="padding-left:3.0pt;padding-right:3.0pt;">20.1%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.5" style="padding-left:3.0pt;padding-right:3.0pt;">66.3%</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1" style="padding-left:3.0pt;padding-right:3.0pt;">FT</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.2" style="padding-left:3.0pt;padding-right:3.0pt;">89.9%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.3" style="padding-left:3.0pt;padding-right:3.0pt;">53.7%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.4" style="padding-left:3.0pt;padding-right:3.0pt;">21.3%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.5" style="padding-left:3.0pt;padding-right:3.0pt;">66.3%</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4">
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.1" style="padding-left:3.0pt;padding-right:3.0pt;">FT + Test OOD</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2" style="padding-left:3.0pt;padding-right:3.0pt;">32.1%</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3" style="padding-left:3.0pt;padding-right:3.0pt;">34.2%</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.4" style="padding-left:3.0pt;padding-right:3.0pt;">0.1%</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.5" style="padding-left:3.0pt;padding-right:3.0pt;">0.1%</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5">
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.1" style="padding-left:3.0pt;padding-right:3.0pt;">(FT+Test) OOD</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.2" style="padding-left:3.0pt;padding-right:3.0pt;">88.2%</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.3" style="padding-left:3.0pt;padding-right:3.0pt;">42.7%</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.4" style="padding-left:3.0pt;padding-right:3.0pt;">11.3%</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.5" style="padding-left:3.0pt;padding-right:3.0pt;">12.4%</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.6">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.6.1" style="padding-left:3.0pt;padding-right:3.0pt;">ICL</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.6.2" style="padding-left:3.0pt;padding-right:3.0pt;">63.4%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.6.3" style="padding-left:3.0pt;padding-right:3.0pt;">58.2%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.6.4" style="padding-left:3.0pt;padding-right:3.0pt;">35.6%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.6.5" style="padding-left:3.0pt;padding-right:3.0pt;">68.1%</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.7">
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.1" style="padding-left:3.0pt;padding-right:3.0pt;">ICL + Test OOD</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.2" style="padding-left:3.0pt;padding-right:3.0pt;">34.5%</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.3" style="padding-left:3.0pt;padding-right:3.0pt;">44.2%</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.4" style="padding-left:3.0pt;padding-right:3.0pt;">12.3%</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.5" style="padding-left:3.0pt;padding-right:3.0pt;">24.7%</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.8">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.8.1" style="padding-left:3.0pt;padding-right:3.0pt;">(ICL+Test) OOD</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.8.2" style="padding-left:3.0pt;padding-right:3.0pt;">62.3%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.8.3" style="padding-left:3.0pt;padding-right:3.0pt;">51.7%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.8.4" style="padding-left:3.0pt;padding-right:3.0pt;">34.5%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.8.5" style="padding-left:3.0pt;padding-right:3.0pt;">71.4%</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Experimental comparison of different PEFT methods. Here FT/ICL + Test OOD means we only applied OOD data in test phase, while (FT/ICL) OOD represents that both training/in-context learning and test phase were using OOD data.</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>ICL <em class="ltx_emph ltx_font_italic" id="S4.SS1.1.1">v.s.</em>Â Fine-tuning: Accuracy</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The results of accuracy test are shown in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S3.T1" title="Table 1 â€£ Task 4: Relation Reasoning Li et al. (2024) â€£ 3.1 Tasks â€£ 3 Implicit Pattern Detection Test â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_tag">1</span></a> and Table Â <a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S4.T2" title="Table 2 â€£ 4 Results and Analysis â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_tag">2</span></a>. Both ICL and fine-tuning(including full-param fine-tuning and PEFT methods) bring improvements to the performace of each task. However, it is easily noticed that ICL wins at most terms like relation, code reading and boolean functions, with 2% to even more than 30% improvements at most. On the flip side, fine-tuning only shows slight advantages in expression calculations in only Qwen-series models. As for different model size<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>See Qwen1.5 series in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S3.T1" title="Table 1 â€£ Task 4: Relation Reasoning Li et al. (2024) â€£ 3.1 Tasks â€£ 3 Implicit Pattern Detection Test â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_tag">1</span></a> from 0.5B to 7B</span></span></span>, we found that a larger model seems be able to evoke stronger ICL ability above linearly growth (see TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S3.T1" title="Table 1 â€£ Task 4: Relation Reasoning Li et al. (2024) â€£ 3.1 Tasks â€£ 3 Implicit Pattern Detection Test â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_tag">1</span></a>), where the scaling of fine-tuning performance is limited.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>ICL <em class="ltx_emph ltx_font_italic" id="S4.SS2.1.1">v.s.</em>Â Fine-tuning: Robustness without Implicit Pattern</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S3.SS2" title="3.2 Settings â€£ 3 Implicit Pattern Detection Test â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_tag">3.2</span></a>, we introduced the metrics of clean accuracy and misleading accuracy by adding misleading data to test both ICL and fine-tuningâ€™s robustness against general data without implicit patterns. The results are shown in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S3.F3" title="Figure 3 â€£ Task 4: Relation Reasoning Li et al. (2024) â€£ 3.1 Tasks â€£ 3 Implicit Pattern Detection Test â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_tag">3</span></a>. For each task, we draw a scatter plot where the x- and y-axis represent the clean accuracy and the misleading accuracy, respectively. The results show that ICL can better exploit the implicit patterns in the demonstration data, while at the same time not compromising general reasoning abilities.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>ICL <em class="ltx_emph ltx_font_italic" id="S4.SS3.1.1">v.s.</em>Â Fine-tuning: Out-Of-Distribution Implicit Patterns</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Out-of-Distribution (OOD) data is a widely examined problem nowadays. The training data of our implicit pattern detection tasks also samples from certain distributions (see AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#A3" title="Appendix C OOD data Construction â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_tag">C</span></a> for details). In this subsection, we hope to compare how ICL and fine-tuning perform if we provide cases outside of the training distribution. For ICL, all examples given are divided into two types: in-distribution examples and OOD examples. For fine-tuning, we directly provide OOD problems to test the accuracy. We performed this experiment on Qwen1.5-1.8B and the results are demonstrated in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S4.T3" title="Table 3 â€£ 4 Results and Analysis â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_tag">3</span></a>. It is worth noticing that fine-tuning generally performs worse when the test data is OOD, while ICL performs fairly well comparing to the baseline method.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>How Much Fine-tuning Do We Need?</h3>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="493" id="S4.F4.g1" src="x4.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The progression of loss and accuracy over time during the fine-tuning of implicit pattern tasks. The Real Loss Values (dashed blue line) show the loss during training. To mitigate this noise, the Smoothed Loss Values (solid blue line) provide a clearer trend of the overall loss reduction. We also show the average test accuracy over all tasks (solid green line).</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="165" id="S4.F5.g1" src="x5.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Illustration of circuit shift comparison. LLMs are first detected circuits with activation patching. Then we compare how much their circuits changed after fine-tuning and in-context learning.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="232" id="S4.F6.g1" src="x6.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Visualization of attention head sensitivity in GPTNeo-1.3B. The more the color leans towards blue, the more important a specific attention head is to the implicit pattern detection task. Left: baseline model. Middle: fine-tuned model. Right: ICL model. It is clear that compared to fine-tuning, ICL brings significant circuit shifts.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">In this experiment, we hope to figure out whether fine-tuning has reached its limit for implicit pattern detection or there will still be improvement if more data is utilized for fine-tuning. Therefore, we visualized the fine-tuning process of Qwen1.5-1.8B. At the onset of training, there is a steep decline in the loss value, suggesting that the model quickly learns basic patterns in the data. This rapid improvement is typical, as the model captures the most evident features. The Accuracy (solid green line) also increases sharply, corroborating the initial learning phase where the model transitions from random guessing to meaningful predictions. However, after around 50 time steps, both the loss and accuracy curves begin to stabilize. This period of stabilization suggests diminishing returns from further training, as the fine-tuned model failed to capture further implicit patterns. After 100 time steps, the curves indicate that the model has reached a plateau. The accuracy remains relatively constant, and the loss value shows minimal fluctuations around a stable trend. This behavior signifies that the model has learned the underlying patterns to a satisfactory extent, and additional fine-tuning yields marginal improvements.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T4.3">
<tr class="ltx_tr" id="S4.T4.3.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.3.3.4" style="padding-left:3.0pt;padding-right:3.0pt;">Circuits</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.3.3.5" style="padding-left:3.0pt;padding-right:3.0pt;">Zero-shot Baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.3.3.6" style="padding-left:3.0pt;padding-right:3.0pt;">ICL w/o Implicit Patterns</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><math alttext="\Delta" class="ltx_Math" display="inline" id="S4.T4.1.1.1.m1.1"><semantics id="S4.T4.1.1.1.m1.1a"><mi id="S4.T4.1.1.1.m1.1.1" mathvariant="normal" xref="S4.T4.1.1.1.m1.1.1.cmml">Î”</mi><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.m1.1.1">Î”</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S4.T4.1.1.1.m1.1d">roman_Î”</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.3.3.7" style="padding-left:3.0pt;padding-right:3.0pt;">After Fine-tuning</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.2.2.2" style="padding-left:3.0pt;padding-right:3.0pt;"><math alttext="\Delta" class="ltx_Math" display="inline" id="S4.T4.2.2.2.m1.1"><semantics id="S4.T4.2.2.2.m1.1a"><mi id="S4.T4.2.2.2.m1.1.1" mathvariant="normal" xref="S4.T4.2.2.2.m1.1.1.cmml">Î”</mi><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.m1.1b"><ci id="S4.T4.2.2.2.m1.1.1.cmml" xref="S4.T4.2.2.2.m1.1.1">Î”</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S4.T4.2.2.2.m1.1d">roman_Î”</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.3.3.8" style="padding-left:3.0pt;padding-right:3.0pt;">After ICL</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.3.3.3" style="padding-left:3.0pt;padding-right:3.0pt;"><math alttext="\Delta" class="ltx_Math" display="inline" id="S4.T4.3.3.3.m1.1"><semantics id="S4.T4.3.3.3.m1.1a"><mi id="S4.T4.3.3.3.m1.1.1" mathvariant="normal" xref="S4.T4.3.3.3.m1.1.1.cmml">Î”</mi><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.m1.1b"><ci id="S4.T4.3.3.3.m1.1.1.cmml" xref="S4.T4.3.3.3.m1.1.1">Î”</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S4.T4.3.3.3.m1.1d">roman_Î”</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.4">
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.3.4.1" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.3.4.2" style="padding-left:3.0pt;padding-right:3.0pt;">L17 H12, L18 H0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.3.4.3" style="padding-left:3.0pt;padding-right:3.0pt;">L17 H12, L16 H1</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.3.4.4" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.3.4.5" style="padding-left:3.0pt;padding-right:3.0pt;">L17 H12, L18 H0</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.3.4.6" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.3.4.7" style="padding-left:3.0pt;padding-right:3.0pt;">L11 H5, L10 H6</td>
<td class="ltx_td ltx_border_t" id="S4.T4.3.4.8" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.3.5.1" style="padding-left:3.0pt;padding-right:3.0pt;">Attention</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.3.5.2" style="padding-left:3.0pt;padding-right:3.0pt;">L22 H1, L16 H7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.3.5.3" style="padding-left:3.0pt;padding-right:3.0pt;">L18 H0, L15 H2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.3.5.4" style="padding-left:3.0pt;padding-right:3.0pt;">2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.3.5.5" style="padding-left:3.0pt;padding-right:3.0pt;">L22 H1, L16 H7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.3.5.6" style="padding-left:3.0pt;padding-right:3.0pt;">1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.3.5.7" style="padding-left:3.0pt;padding-right:3.0pt;">L11 H2, L15 H10</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.5.8" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.5.8.1">6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.6">
<td class="ltx_td ltx_border_r" id="S4.T4.3.6.1" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.3.6.2" style="padding-left:3.0pt;padding-right:3.0pt;">L18 H15, L14 H5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.3.6.3" style="padding-left:3.0pt;padding-right:3.0pt;">L18 H15, L22 H1</td>
<td class="ltx_td ltx_border_r" id="S4.T4.3.6.4" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.3.6.5" style="padding-left:3.0pt;padding-right:3.0pt;">L18 H15, L12 H6</td>
<td class="ltx_td ltx_border_r" id="S4.T4.3.6.6" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.3.6.7" style="padding-left:3.0pt;padding-right:3.0pt;">L17 H12, L 18 H5</td>
<td class="ltx_td" id="S4.T4.3.6.8" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.7">
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.3.7.1" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.3.7.2" style="padding-left:3.0pt;padding-right:3.0pt;">L9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.3.7.3" style="padding-left:3.0pt;padding-right:3.0pt;">L9</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.3.7.4" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.3.7.5" style="padding-left:3.0pt;padding-right:3.0pt;">L9</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.3.7.6" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.3.7.7" style="padding-left:3.0pt;padding-right:3.0pt;">L17</td>
<td class="ltx_td ltx_border_t" id="S4.T4.3.7.8" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.3.8.1" style="padding-left:3.0pt;padding-right:3.0pt;">MLP</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.3.8.2" style="padding-left:3.0pt;padding-right:3.0pt;">L17</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.3.8.3" style="padding-left:3.0pt;padding-right:3.0pt;">L17</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.3.8.4" style="padding-left:3.0pt;padding-right:3.0pt;">0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.3.8.5" style="padding-left:3.0pt;padding-right:3.0pt;">L18</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.3.8.6" style="padding-left:3.0pt;padding-right:3.0pt;">0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.3.8.7" style="padding-left:3.0pt;padding-right:3.0pt;">L14</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.8.8" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.8.8.1">2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.9">
<td class="ltx_td ltx_border_bb ltx_border_r" id="S4.T4.3.9.1" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.3.9.2" style="padding-left:3.0pt;padding-right:3.0pt;">L18</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.3.9.3" style="padding-left:3.0pt;padding-right:3.0pt;">L18</td>
<td class="ltx_td ltx_border_bb ltx_border_r" id="S4.T4.3.9.4" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.3.9.5" style="padding-left:3.0pt;padding-right:3.0pt;">L17</td>
<td class="ltx_td ltx_border_bb ltx_border_r" id="S4.T4.3.9.6" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.3.9.7" style="padding-left:3.0pt;padding-right:3.0pt;">L15</td>
<td class="ltx_td ltx_border_bb" id="S4.T4.3.9.8" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Top 6 Rankings of Attention Heads and top 3 rankings of MLP Layers in baseline (zero-shot) model, fine-tuned model, and ICL model. L is layer and H is head. <math alttext="\Delta" class="ltx_Math" display="inline" id="S4.T4.6.m1.1"><semantics id="S4.T4.6.m1.1b"><mi id="S4.T4.6.m1.1.1" mathvariant="normal" xref="S4.T4.6.m1.1.1.cmml">Î”</mi><annotation-xml encoding="MathML-Content" id="S4.T4.6.m1.1c"><ci id="S4.T4.6.m1.1.1.cmml" xref="S4.T4.6.m1.1.1">Î”</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.m1.1d">\Delta</annotation><annotation encoding="application/x-llamapun" id="S4.T4.6.m1.1e">roman_Î”</annotation></semantics></math> shows how many different heads or MLPs changed after fine-tuning or ICL. A larger <math alttext="\Delta" class="ltx_Math" display="inline" id="S4.T4.7.m2.1"><semantics id="S4.T4.7.m2.1b"><mi id="S4.T4.7.m2.1.1" mathvariant="normal" xref="S4.T4.7.m2.1.1.cmml">Î”</mi><annotation-xml encoding="MathML-Content" id="S4.T4.7.m2.1c"><ci id="S4.T4.7.m2.1.1.cmml" xref="S4.T4.7.m2.1.1">Î”</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.7.m2.1d">\Delta</annotation><annotation encoding="application/x-llamapun" id="S4.T4.7.m2.1e">roman_Î”</annotation></semantics></math> represents a more significant circuit shift in certain processes.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Comparison of Fine-tuning with PEFT Methods</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">Lastly, we examine whether there is a significant difference between various fine-tuning methods <em class="ltx_emph ltx_font_italic" id="S4.SS5.p1.1.1">e.g., </em>Â vanilla full-parameter fine-tuning, and parameter efficient fine-tuning (PEFT) methods like LoRAÂ <cite class="ltx_cite ltx_citemacro_cite">Hu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib15" title="">2021</a>)</cite>, QLoRAÂ <cite class="ltx_cite ltx_citemacro_cite">Dettmers etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib7" title="">2024</a>)</cite> and GaLoREÂ <cite class="ltx_cite ltx_citemacro_cite">Zhao etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib38" title="">2024</a>)</cite>. Although PEFT needs much less parameters for training, and several studies criticized its abilityÂ <cite class="ltx_cite ltx_citemacro_cite">Pu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib24" title="">2023</a>); Zou etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib40" title="">2023</a>)</cite>, there are still evidences that PEFT sometimes achieves ICL-level performance. We followed the experimental settings in previous sections on Qwen1.5-1.8B with PEFT methods. The experimental results can be found in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S4.T2" title="Table 2 â€£ 4 Results and Analysis â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_tag">2</span></a>. It is clear that in the implicit pattern detection tasks, PEFT methods show no obvious advantages compared to full-param fine-tuning, thus they still failed to win ICL in accuracy in all tests.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Explanation of ICLâ€™s Victory: Circuits Shift Theory</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Understanding the inner mechanisms of LLMs greatly benefits their ethical use and safety. We have found that ICL performs much better than fine-tuning on implicit pattern detection, and in this section, we try to explain why.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">From a mechanistic interpretability perspective, we investigate this problem using <span class="ltx_text ltx_font_bold" id="S5.p2.1.1">circuits</span>. Circuits are specific pathways (typically combinations of attention heads and MLP layers) within a model responsible for processing and interpreting particular patterns or tasks. The change in circuits for LLMs represents a shift in their inner mechanisms, revealing that LLMs choose different ways to solve problems. Based on this viewpoint, we propose a theory: <span class="ltx_text ltx_font_bold" id="S5.p2.1.2">Circuits Shift</span>, to explain this phenomenon. We will first provide a method for probing circuits, explaining what they are and the types of circuits we found in ICL-based and fine-tuning-based models. Then we will show that the reason ICL performs better than fine-tuning is that the circuits in models experience a more significant shift. A detailed explanation of circuits and experimental settings can be found in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#A4" title="Appendix D Circuits â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Method for Identifying Circuit Shift</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">In FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S4.F5" title="Figure 5 â€£ 4.4 How Much Fine-tuning Do We Need? â€£ 4 Results and Analysis â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_tag">5</span></a>, we present our framework and methodology for probing circuit shifts. We begin by selecting an implicit pattern detection task (in this study, we utilize an expression task). Subsequently, we use models employing different methods, <em class="ltx_emph ltx_font_italic" id="S5.SS1.p1.1.1">i.e., </em>Â ICL or fine-tuning, for inference. During this process, we introduce corrupt input to randomly disrupt a portion of the activation to assess whether the corresponding attention heads or MLP layers significantly contribute to the final outcome. If a significant contribution exists, the disruption will result in considerable perturbation of the final logits, which is depicted as sensitivity in the figure.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Circuits Shift in LLMs for Implicit Pattern Detection</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We first visualized and ranked circuits in GPTNeo-1.3B zero-shot, after fine-tuned, and ICL with 32-shot with expression calculation task (see FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S4.F6" title="Figure 6 â€£ 4.4 How Much Fine-tuning Do We Need? â€£ 4 Results and Analysis â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_tag">6</span></a> and TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S4.T4" title="Table 4 â€£ 4.4 How Much Fine-tuning Do We Need? â€£ 4 Results and Analysis â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_tag">4</span></a>). In FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S4.F6" title="Figure 6 â€£ 4.4 How Much Fine-tuning Do We Need? â€£ 4 Results and Analysis â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_tag">6</span></a>, we use the heatmap to illustrate the sensitivity of each attention head in implicit pattern detection test. From the figure, we can observe that, compared to the baseline and fine-tuning scenarios, ICL exhibits a significant shift when learning implicit patterns. Firstly, more shallow heads are involved in the task. Secondly, some deep heads that previously played a dominant role have now lost their leadership positions. This indicates that during the ICL process, the model significantly transforms its approach to solving the task, adapting to a form more suitable for implicit patterns, a phenomenon not observed with other methods.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">We can further validate our hypothesis in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#S4.T4" title="Table 4 â€£ 4.4 How Much Fine-tuning Do We Need? â€£ 4 Results and Analysis â€£ Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning"><span class="ltx_text ltx_ref_tag">4</span></a>. We selected the six attention heads and MLP layers<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>See <a class="ltx_ref ltx_href" href="https://transformer-circuits.pub/2021/framework/index.html" title="">A Mathematical Framework for Transformer Circuits</a> for details.</span></span></span> with the highest sensitivity, <em class="ltx_emph ltx_font_italic" id="S5.SS2.p2.1.1">i.e., </em>Â those that contributed the most to the final result. Using the baseline, which is the zero-shot approach for handling implicit pattern detection tasks, as the standard, we counted how many new attention heads entered the top six highest contributors when the method changed, denoted by Delta. The results are very clear: compared to fine-tuning, ICL exhibits more significant changes, indicating a more thorough Circuit Shift during ICL. This suggests that ICL captures the characteristics of implicit patterns better than fine-tuning and adapts its processing method accordingly.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">To rule out the inherent impact of ICL itself, we also conducted multi-shot experiments on a set of data without implicit pattern characteristics. The results showed that it is not multi-shot alone that induces this change, but rather the combined effect of ICL and implicit patterns.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Related Work</h2>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Implicit Pattern Discovery</h4>
<div class="ltx_para" id="S6.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px1.p1.1">Previous works have designed benchmarks to test the LLMs reasoning abilityÂ <cite class="ltx_cite ltx_citemacro_cite">Barrett etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib2" title="">2018</a>); Tang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib30" title="">2023</a>); Gendron etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib12" title="">2024</a>)</cite>. However, the benchmarks rarely include two-level questions where at one level, they can be solved by brute force, at another level it can be solved by exploiting implicit patterns. The closest related work we know isÂ <cite class="ltx_cite ltx_citemacro_citet">Efrat etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib9" title="">2021</a>)</cite>, which involves solving cryptic crossword puzzles.
To help the model find patterns in data, Prior work <cite class="ltx_cite ltx_citemacro_citet">Sun etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib29" title="">2024</a>); Zhu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib39" title="">2024</a>)</cite> proposes a two-stage induction-deduction process that first summarizes the common patterns explicitly, then reasons from the patterns.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">ICL <em class="ltx_emph ltx_font_italic" id="S6.SS0.SSS0.Px2.1.1">v.s.</em>Â Fine-tuning Difference</h4>
<div class="ltx_para" id="S6.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px2.p1.1">Previous works have also compared fine-tuning and in-context learning.
<cite class="ltx_cite ltx_citemacro_citet">Shen etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib27" title="">2024</a>)</cite> shows that ICL is likely not an algorithmic equivalence to gradient descent for real LLMs. <cite class="ltx_cite ltx_citemacro_citet">Reddy (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib26" title="">2023</a>)</cite> demonstrates that ICL is implemented by an induction head and analyzes its emergence phenomenon. <cite class="ltx_cite ltx_citemacro_citet">Bhattamishra etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib3" title="">2023</a>)</cite> shows that ICL and vanilla training implement two distinct algorithms that donâ€™t transfer to each other.
However, it has been proven that fine-tuning shows better performance in generalization to OOD tasks than in-context learningÂ <cite class="ltx_cite ltx_citemacro_cite">Mosbach etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.04691v1#bib.bib22" title="">2023</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In conclusion, our research demonstrates that In-Context Learning (ICL) significantly outperforms fine-tuning in capturing implicit patterns within specific tasks. Through our experimental evaluations, we observed that ICL not only enhances task performance more effectively but also exhibits greater adaptability in problem-solving approaches, as evidenced by the notable shifts in model circuits.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Our study on the effectiveness of in-context learning in capturing implicit patterns compared to fine-tuning faces several limitations. Primarily, the generalizability of our findings is constrained by the specific nature of the implicit pattern detection tasks, which are limited to certain domains like arithmetic calculations, code reading, Boolean functions, and relation reasoning. Additionally, our analysis of Circuit Shift, which underpins the superior performance of ICL, relies on activation patching and sensitivity analysis, methods that, while insightful, require further refinement and validation across different models and tasks to confirm their robustness and applicability. Furthermore, the computational resources required for fine-tuning, especially with large models, may limit the feasibility of such experiments in broader settings, and a detailed cost-benefit analysis comparing ICL and fine-tuning in terms of computational efficiency and performance is needed.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">This work is funded by the Zhejiang Provincial â€œJianbingâ€ â€œLingyanâ€ Research and Development Program of China (2024C01135), National Natural Science Foundation of China (62302433, U23A20496), Zhejiang Provincial Natural Science Foundation of China (LQ24F020007) and CCF-Tencent Rhino-Bird Fund (RAGR20230122).</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai etÂ al. (2023)</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, YuÂ Han, Fei Huang, etÂ al. 2023.

</span>
<span class="ltx_bibblock">Qwen technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2309.16609</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barrett etÂ al. (2018)</span>
<span class="ltx_bibblock">
David G.Â T. Barrett, Felix Hill, Adam Santoro, AriÂ S. Morcos, and Timothy Lillicrap. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/1807.04225" title="">Measuring abstract reasoning in neural networks</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Preprint</em>, arXiv:1807.04225.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhattamishra etÂ al. (2023)</span>
<span class="ltx_bibblock">
Satwik Bhattamishra, Arkil Patel, Phil Blunsom, and Varun Kanade. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2310.03016" title="">Understanding in-context learning in transformers and llms by learning to learn discrete functions</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Preprint</em>, arXiv:2310.03016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Biderman etÂ al. (2023)</span>
<span class="ltx_bibblock">
Stella Biderman, Hailey Schoelkopf, QuentinÂ Gregory Anthony, Herbie Bradley, Kyle Oâ€™Brien, Eric Hallahan, MohammadÂ Aflah Khan, Shivanshu Purohit, USVSNÂ Sai Prashanth, Edward Raff, etÂ al. 2023.

</span>
<span class="ltx_bibblock">Pythia: A suite for analyzing large language models across training and scaling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">International Conference on Machine Learning</em>, pages 2397â€“2430. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Black etÂ al. (2021)</span>
<span class="ltx_bibblock">
Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.5281/zenodo.5297715" title="">GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow</a>.

</span>
<span class="ltx_bibblock">If you use this software, please cite it using these metadata.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conmy etÂ al. (2023)</span>
<span class="ltx_bibblock">
Arthur Conmy, Augustine Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and AdriÃ  Garriga-Alonso. 2023.

</span>
<span class="ltx_bibblock">Towards automated circuit discovery for mechanistic interpretability.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Advances in Neural Information Processing Systems</em>, 36:16318â€“16352.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers etÂ al. (2024)</span>
<span class="ltx_bibblock">
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2024.

</span>
<span class="ltx_bibblock">Qlora: Efficient finetuning of quantized llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Advances in Neural Information Processing Systems</em>, 36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong etÂ al. (2022)</span>
<span class="ltx_bibblock">
Qingxiu Dong, Lei Li, Damai Dai, CeÂ Zheng, Zhiyong Wu, Baobao Chang, XuÂ Sun, Jingjing Xu, and Zhifang Sui. 2022.

</span>
<span class="ltx_bibblock">A survey on in-context learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2301.00234</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Efrat etÂ al. (2021)</span>
<span class="ltx_bibblock">
Avia Efrat, Uri Shaham, Dan Kilman, and Omer Levy. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.emnlp-main.344" title="">Cryptonite: A cryptic crossword benchmark for extreme ambiguity in language</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 4186â€“4192, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang etÂ al. (2024)</span>
<span class="ltx_bibblock">
Chongzhou Fang, Ning Miao, Shaurya Srivastav, Jialin Liu, Ruoyu Zhang, Ruijie Fang, Asmita, Ryan Tsang, Najmeh Nazari, Han Wang, and Houman Homayoun. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2310.12357" title="">Large language models for code analysis: Do llms really do their job?</a>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Preprint</em>, arXiv:2310.12357.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gan and Mori (2023)</span>
<span class="ltx_bibblock">
Chengguang Gan and Tatsunori Mori. 2023.

</span>
<span class="ltx_bibblock">A few-shot approach to resume information extraction via prompts.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">International Conference on Applications of Natural Language to Information Systems</em>, pages 445â€“455. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gendron etÂ al. (2024)</span>
<span class="ltx_bibblock">
GaÃ«l Gendron, Qiming Bao, Michael Witbrock, and Gillian Dobbie. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2305.19555" title="">Large language models are not strong abstract reasoners</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Preprint</em>, arXiv:2305.19555.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He-Yueya etÂ al. (2023)</span>
<span class="ltx_bibblock">
Joy He-Yueya, Gabriel Poesia, RoseÂ E. Wang, and NoahÂ D. Goodman. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2304.09102" title="">Solving math word problems by combining language models with symbolic solvers</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Preprint</em>, arXiv:2304.09102.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hong etÂ al. (2023)</span>
<span class="ltx_bibblock">
KeÂ Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Hanyu Dong, and YuÂ Wang. 2023.

</span>
<span class="ltx_bibblock">Flashdecoding++: Faster large language model inference on gpus.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2311.01282</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu etÂ al. (2021)</span>
<span class="ltx_bibblock">
EdwardÂ J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, LuÂ Wang, and Weizhu Chen. 2021.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2106.09685</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee. 2023.

</span>
<span class="ltx_bibblock">Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2304.01933</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Imani etÂ al. (2023)</span>
<span class="ltx_bibblock">
Shima Imani, Liang Du, and Harsh Shrivastava. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2303.05398" title="">Mathprompter: Mathematical reasoning using large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Preprint</em>, arXiv:2303.05398.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2023)</span>
<span class="ltx_bibblock">
AlbertÂ Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, DevendraÂ Singh Chaplot, Diego deÂ las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, etÂ al. 2023.

</span>
<span class="ltx_bibblock">Mistral 7b.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:2310.06825</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2024)</span>
<span class="ltx_bibblock">
Zhiming Li, Yushi Cao, Xiufeng Xu, Junzhe Jiang, XuÂ Liu, YonÂ Shin Teo, Shang wei Lin, and Yang Liu. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2401.09042" title="">Llms for relational reasoning: How far are we?</a>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Preprint</em>, arXiv:2401.09042.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Ziquan Liu, YiÂ Xu, Yuanhong Xu, QiÂ Qian, Hao Li, Xiangyang Ji, Antoni Chan, and Rong Jin. 2022.

</span>
<span class="ltx_bibblock">Improved fine-tuning by better leveraging pre-training data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Advances in Neural Information Processing Systems</em>, 35:32568â€“32581.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Min etÂ al. (2022)</span>
<span class="ltx_bibblock">
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022.

</span>
<span class="ltx_bibblock">Rethinking the role of demonstrations: What makes in-context learning work?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:2202.12837</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mosbach etÂ al. (2023)</span>
<span class="ltx_bibblock">
Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, and Yanai Elazar. 2023.

</span>
<span class="ltx_bibblock">Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2305.16938</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peters etÂ al. (2019)</span>
<span class="ltx_bibblock">
MatthewÂ E Peters, Sebastian Ruder, and NoahÂ A Smith. 2019.

</span>
<span class="ltx_bibblock">To tune or not to tune? adapting pretrained representations to diverse tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:1903.05987</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pu etÂ al. (2023)</span>
<span class="ltx_bibblock">
George Pu, Anirudh Jain, Jihan Yin, and Russell Kaplan. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2304.14999" title="">Empirical analysis of the strengths and weaknesses of peft techniques for llms</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Preprint</em>, arXiv:2304.14999.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rasley etÂ al. (2020)</span>
<span class="ltx_bibblock">
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020.

</span>
<span class="ltx_bibblock">Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>, pages 3505â€“3506.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reddy (2023)</span>
<span class="ltx_bibblock">
Gautam Reddy. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2312.03002" title="">The mechanistic basis of data dependence and abrupt learning in an in-context classification task</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Preprint</em>, arXiv:2312.03002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen etÂ al. (2024)</span>
<span class="ltx_bibblock">
Lingfeng Shen, Aayush Mishra, and Daniel Khashabi. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2310.08540" title="">Do pretrained transformers learn in-context by gradient descent?</a>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Preprint</em>, arXiv:2310.08540.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yixin Song, Zeyu Mi, Haotong Xie, and Haibo Chen. 2023.

</span>
<span class="ltx_bibblock">Powerinfer: Fast large language model serving with a consumer-grade gpu.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2312.12456</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun etÂ al. (2024)</span>
<span class="ltx_bibblock">
Wangtao Sun, Haotian Xu, Xuanqing Yu, Pei Chen, Shizhu He, Jun Zhao, and Kang Liu. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2403.05789" title="">Itd: Large language models can teach themselves induction through deduction</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Preprint</em>, arXiv:2403.05789.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Xiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu, Yitao Liang, and Muhan Zhang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2305.14825" title="">Large language models are in-context semantic reasoners rather than symbolic reasoners</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Preprint</em>, arXiv:2305.14825.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani etÂ al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, AidanÂ N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Advances in neural information processing systems</em>, 30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin etÂ al. (2024)</span>
<span class="ltx_bibblock">
Qingyu Yin, Xuzheng He, Xiang Zhuang, YuÂ Zhao, Jianhua Yao, Xiaoyu Shen, and Qiang Zhang. 2024.

</span>
<span class="ltx_bibblock">Stablemask: Refining causal masking in decoder-only transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2402.04779</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Young etÂ al. (2024)</span>
<span class="ltx_bibblock">
Alex Young, Bei Chen, Chao Li, Chengen Huang, GeÂ Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, etÂ al. 2024.

</span>
<span class="ltx_bibblock">Yi: Open foundation models by 01. ai.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2403.04652</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan etÂ al. (2023)</span>
<span class="ltx_bibblock">
Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2304.02015" title="">How well do large language models perform in arithmetic tasks?</a>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Preprint</em>, arXiv:2304.02015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yue etÂ al. (2023)</span>
<span class="ltx_bibblock">
Xiang Yue, Xingwei Qu, GeÂ Zhang, Yao Fu, Wenhao Huang, Huan Sun, YuÂ Su, and Wenhu Chen. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2309.05653" title="">Mammoth: Building math generalist models through hybrid instruction tuning</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Preprint</em>, arXiv:2309.05653.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhai etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yuexiang Zhai, Shengbang Tong, Xiao Li, MuÂ Cai, Qing Qu, YongÂ Jae Lee, and YiÂ Ma. 2023.

</span>
<span class="ltx_bibblock">Investigating the catastrophic forgetting in multimodal large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2309.10313</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2024)</span>
<span class="ltx_bibblock">
YuÂ Zhang, Hui-Ling Zhen, Zehua Pei, Yingzhao Lian, Lihao Yin, Mingxuan Yuan, and Bei Yu. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2402.11903" title="">Dila: Enhancing llm tool learning with differential logic layer</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Preprint</em>, arXiv:2402.11903.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao etÂ al. (2024)</span>
<span class="ltx_bibblock">
Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. 2024.

</span>
<span class="ltx_bibblock">Galore: Memory-efficient llm training by gradient low-rank projection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2403.03507</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu etÂ al. (2024)</span>
<span class="ltx_bibblock">
Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, and Hanjun Dai. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2310.07064" title="">Large language models can learn rules</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Preprint</em>, arXiv:2310.07064.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zou etÂ al. (2023)</span>
<span class="ltx_bibblock">
Wentao Zou, QiÂ Li, Jidong Ge, Chuanyi Li, Xiaoyu Shen, Liguo Huang, and Bin Luo. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2312.15614" title="">A comprehensive evaluation of parameter-efficient fine-tuning on software engineering tasks</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Preprint</em>, arXiv:2312.15614.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Data Format and Example</h2>
<figure class="ltx_table" id="A1.T5">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A1.T5.3">
<tr class="ltx_tr" id="A1.T5.3.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T5.3.4.1" style="padding-left:3.0pt;padding-right:3.0pt;">Name</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T5.3.4.2" style="padding-left:3.0pt;padding-right:3.0pt;">Type</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T5.3.4.3" style="padding-left:3.0pt;padding-right:3.0pt;">Problem Example</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T5.3.4.4" style="padding-left:3.0pt;padding-right:3.0pt;">Answer</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.3.4.5" style="padding-left:3.0pt;padding-right:3.0pt;">Answer Type</td>
</tr>
<tr class="ltx_tr" id="A1.T5.2.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.2.2.3" style="padding-left:3.0pt;padding-right:3.0pt;">Expression</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.2.2.4" style="padding-left:3.0pt;padding-right:3.0pt;">Mathematic Calculation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="A1.T5.1.1.1.2"></span> <span class="ltx_text" id="A1.T5.1.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="A1.T5.1.1.1.1.1">
<span class="ltx_tr" id="A1.T5.1.1.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T5.1.1.1.1.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><math alttext="(6-1)+(6-6)*(-10+1+2+13)=" class="ltx_Math" display="inline" id="A1.T5.1.1.1.1.1.1.1.m1.3"><semantics id="A1.T5.1.1.1.1.1.1.1.m1.3a"><mrow id="A1.T5.1.1.1.1.1.1.1.m1.3.3" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.cmml"><mrow id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.cmml"><mrow id="A1.T5.1.1.1.1.1.1.1.m1.1.1.1.1.1" xref="A1.T5.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.cmml"><mo id="A1.T5.1.1.1.1.1.1.1.m1.1.1.1.1.1.2" stretchy="false" xref="A1.T5.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="A1.T5.1.1.1.1.1.1.1.m1.1.1.1.1.1.1" xref="A1.T5.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.cmml"><mn id="A1.T5.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.2" xref="A1.T5.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.2.cmml">6</mn><mo id="A1.T5.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1" xref="A1.T5.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1.cmml">âˆ’</mo><mn id="A1.T5.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.3" xref="A1.T5.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="A1.T5.1.1.1.1.1.1.1.m1.1.1.1.1.1.3" stretchy="false" xref="A1.T5.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.4" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.4.cmml">+</mo><mrow id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.cmml"><mrow id="A1.T5.1.1.1.1.1.1.1.m1.2.2.2.2.1.1" xref="A1.T5.1.1.1.1.1.1.1.m1.2.2.2.2.1.1.1.cmml"><mo id="A1.T5.1.1.1.1.1.1.1.m1.2.2.2.2.1.1.2" stretchy="false" xref="A1.T5.1.1.1.1.1.1.1.m1.2.2.2.2.1.1.1.cmml">(</mo><mrow id="A1.T5.1.1.1.1.1.1.1.m1.2.2.2.2.1.1.1" xref="A1.T5.1.1.1.1.1.1.1.m1.2.2.2.2.1.1.1.cmml"><mn id="A1.T5.1.1.1.1.1.1.1.m1.2.2.2.2.1.1.1.2" xref="A1.T5.1.1.1.1.1.1.1.m1.2.2.2.2.1.1.1.2.cmml">6</mn><mo id="A1.T5.1.1.1.1.1.1.1.m1.2.2.2.2.1.1.1.1" xref="A1.T5.1.1.1.1.1.1.1.m1.2.2.2.2.1.1.1.1.cmml">âˆ’</mo><mn id="A1.T5.1.1.1.1.1.1.1.m1.2.2.2.2.1.1.1.3" xref="A1.T5.1.1.1.1.1.1.1.m1.2.2.2.2.1.1.1.3.cmml">6</mn></mrow><mo id="A1.T5.1.1.1.1.1.1.1.m1.2.2.2.2.1.1.3" rspace="0.055em" stretchy="false" xref="A1.T5.1.1.1.1.1.1.1.m1.2.2.2.2.1.1.1.cmml">)</mo></mrow><mo id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.3" rspace="0.222em" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.3.cmml">âˆ—</mo><mrow id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.cmml"><mo id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.2" stretchy="false" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.cmml">(</mo><mrow id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.cmml"><mrow id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.2" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.2.cmml"><mo id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.2a" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.2.cmml">âˆ’</mo><mn id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.2.2" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.2.2.cmml">10</mn></mrow><mo id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.1" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.1.cmml">+</mo><mn id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.3" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.3.cmml">1</mn><mo id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.1a" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.1.cmml">+</mo><mn id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.4" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.4.cmml">2</mn><mo id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.1b" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.1.cmml">+</mo><mn id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.5" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.5.cmml">13</mn></mrow><mo id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.3" stretchy="false" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="A1.T5.1.1.1.1.1.1.1.m1.3.3.4" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.4.cmml">=</mo><mi id="A1.T5.1.1.1.1.1.1.1.m1.3.3.5" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.5.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="A1.T5.1.1.1.1.1.1.1.m1.3b"><apply id="A1.T5.1.1.1.1.1.1.1.m1.3.3.cmml" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3"><eq id="A1.T5.1.1.1.1.1.1.1.m1.3.3.4.cmml" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.4"></eq><apply id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.cmml" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3"><plus id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.4.cmml" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.4"></plus><apply id="A1.T5.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.cmml" xref="A1.T5.1.1.1.1.1.1.1.m1.1.1.1.1.1"><minus id="A1.T5.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1.cmml" xref="A1.T5.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.1"></minus><cn id="A1.T5.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.2.cmml" type="integer" xref="A1.T5.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.2">6</cn><cn id="A1.T5.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.3.cmml" type="integer" xref="A1.T5.1.1.1.1.1.1.1.m1.1.1.1.1.1.1.3">1</cn></apply><apply id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.cmml" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3"><times id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.3.cmml" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.3"></times><apply id="A1.T5.1.1.1.1.1.1.1.m1.2.2.2.2.1.1.1.cmml" xref="A1.T5.1.1.1.1.1.1.1.m1.2.2.2.2.1.1"><minus id="A1.T5.1.1.1.1.1.1.1.m1.2.2.2.2.1.1.1.1.cmml" xref="A1.T5.1.1.1.1.1.1.1.m1.2.2.2.2.1.1.1.1"></minus><cn id="A1.T5.1.1.1.1.1.1.1.m1.2.2.2.2.1.1.1.2.cmml" type="integer" xref="A1.T5.1.1.1.1.1.1.1.m1.2.2.2.2.1.1.1.2">6</cn><cn id="A1.T5.1.1.1.1.1.1.1.m1.2.2.2.2.1.1.1.3.cmml" type="integer" xref="A1.T5.1.1.1.1.1.1.1.m1.2.2.2.2.1.1.1.3">6</cn></apply><apply id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.cmml" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1"><plus id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.1.cmml" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.1"></plus><apply id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.2.cmml" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.2"><minus id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.2.1.cmml" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.2"></minus><cn id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.2.2.cmml" type="integer" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.2.2">10</cn></apply><cn id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.3.cmml" type="integer" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.3">1</cn><cn id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.4.cmml" type="integer" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.4">2</cn><cn id="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.5.cmml" type="integer" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.3.3.2.1.1.5">13</cn></apply></apply></apply><csymbol cd="latexml" id="A1.T5.1.1.1.1.1.1.1.m1.3.3.5.cmml" xref="A1.T5.1.1.1.1.1.1.1.m1.3.3.5">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.1.1.1.1.1.1.1.m1.3c">(6-1)+(6-6)*(-10+1+2+13)=</annotation><annotation encoding="application/x-llamapun" id="A1.T5.1.1.1.1.1.1.1.m1.3d">( 6 - 1 ) + ( 6 - 6 ) âˆ— ( - 10 + 1 + 2 + 13 ) =</annotation></semantics></math></span></span>
</span></span><span class="ltx_text" id="A1.T5.1.1.1.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.2.2.2" style="padding-left:3.0pt;padding-right:3.0pt;"><math alttext="5" class="ltx_Math" display="inline" id="A1.T5.2.2.2.m1.1"><semantics id="A1.T5.2.2.2.m1.1a"><mn id="A1.T5.2.2.2.m1.1.1" xref="A1.T5.2.2.2.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="A1.T5.2.2.2.m1.1b"><cn id="A1.T5.2.2.2.m1.1.1.cmml" type="integer" xref="A1.T5.2.2.2.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.2.2.2.m1.1c">5</annotation><annotation encoding="application/x-llamapun" id="A1.T5.2.2.2.m1.1d">5</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.2.2.5" style="padding-left:3.0pt;padding-right:3.0pt;">Number</td>
</tr>
<tr class="ltx_tr" id="A1.T5.3.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.3.3.2" style="padding-left:3.0pt;padding-right:3.0pt;">Code</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.3.3.3" style="padding-left:3.0pt;padding-right:3.0pt;">Code Reading</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.3.3.4" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="A1.T5.3.3.4.1"></span> <span class="ltx_text" id="A1.T5.3.3.4.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T5.3.3.4.2.1">
<span class="ltx_tr" id="A1.T5.3.3.4.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T5.3.3.4.2.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T5.3.3.4.2.1.1.1.1">import math \n \n def function1(x): \n \n</span></span></span>
<span class="ltx_tr" id="A1.T5.3.3.4.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T5.3.3.4.2.1.2.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_typewriter" id="A1.T5.3.3.4.2.1.2.1.1">[TRUNCATED] return result \n print(result)</span></span></span>
</span></span><span class="ltx_text" id="A1.T5.3.3.4.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.3.3.1" style="padding-left:3.0pt;padding-right:3.0pt;"><math alttext="3.5" class="ltx_Math" display="inline" id="A1.T5.3.3.1.m1.1"><semantics id="A1.T5.3.3.1.m1.1a"><mn id="A1.T5.3.3.1.m1.1.1" xref="A1.T5.3.3.1.m1.1.1.cmml">3.5</mn><annotation-xml encoding="MathML-Content" id="A1.T5.3.3.1.m1.1b"><cn id="A1.T5.3.3.1.m1.1.1.cmml" type="float" xref="A1.T5.3.3.1.m1.1.1">3.5</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.3.3.1.m1.1c">3.5</annotation><annotation encoding="application/x-llamapun" id="A1.T5.3.3.1.m1.1d">3.5</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.3.5" style="padding-left:3.0pt;padding-right:3.0pt;">Number</td>
</tr>
<tr class="ltx_tr" id="A1.T5.3.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.3.5.1" style="padding-left:3.0pt;padding-right:3.0pt;">Relation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.3.5.2" style="padding-left:3.0pt;padding-right:3.0pt;">Textual Reasoning</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.3.5.3" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="A1.T5.3.5.3.1"></span> <span class="ltx_text" id="A1.T5.3.5.3.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T5.3.5.3.2.1">
<span class="ltx_tr" id="A1.T5.3.5.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T5.3.5.3.2.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;">A is connected with G\n F is connected<span class="ltx_text ltx_font_typewriter" id="A1.T5.3.5.3.2.1.1.1.1">[TRUNCATED]</span></span></span>
<span class="ltx_tr" id="A1.T5.3.5.3.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T5.3.5.3.2.1.2.1" style="padding-left:3.0pt;padding-right:3.0pt;">connected with Z, â€™the city A and Z is connectedâ€™ is</span></span>
</span></span><span class="ltx_text" id="A1.T5.3.5.3.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.3.5.4" style="padding-left:3.0pt;padding-right:3.0pt;">False</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.5.5" style="padding-left:3.0pt;padding-right:3.0pt;">Boolean</td>
</tr>
<tr class="ltx_tr" id="A1.T5.3.6">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="A1.T5.3.6.1" style="padding-left:3.0pt;padding-right:3.0pt;">Boolean</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="A1.T5.3.6.2" style="padding-left:3.0pt;padding-right:3.0pt;">Mathematical Reasoning</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="A1.T5.3.6.3" style="padding-left:3.0pt;padding-right:3.0pt;">(False or False) and (False or True) and False =</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="A1.T5.3.6.4" style="padding-left:3.0pt;padding-right:3.0pt;">False</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T5.3.6.5" style="padding-left:3.0pt;padding-right:3.0pt;">Boolean</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Examples of four implicit pattern detection tasks.</figcaption>
</figure>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">We provided examples of tasks and prompts. We provided data as 2-shot (code in zero-shot to restrict content length) for illustrating how ICL works. For fine-tuning we will use the same format but zero-shot in both training and inference.</p>
</div>
<div class="ltx_para" id="A1.p2">
<p class="ltx_p" id="A1.p2.1"><span class="ltx_text ltx_font_bold" id="A1.p2.1.1">Expression:</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="A1.p2.2">
    Now you need to calculate the answer of
    some mathematic equations.
    Here are some examples:
    (1+6)+(-3+3)*(-1-3+9-5)=7
    (2+3)+(-1-4+5)*(10+6+2-8)=5
    (8)+(0)*(0-6+9-6)=
</pre>
</div>
<div class="ltx_para" id="A1.p3">
<p class="ltx_p" id="A1.p3.1"><span class="ltx_text ltx_font_bold" id="A1.p3.1.1">Code:</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="A1.p3.2">
    Now you need to give me the printed
    result after running this python code.
    Here are some examples:
</pre>
<pre class="ltx_verbatim ltx_font_typewriter" id="A1.p3.3">
    label=code:implicit_pattern]
def function1(x):
    y = x ** 9
    for i in range(1, 13):
        y = y * i - (y // (i + 9))
    return y

def function2(z, a):
    return z / 10

input_value = int(input())
result = function2(input_value, \
function1(input_value))
print(result)
</pre>
<pre class="ltx_verbatim ltx_font_typewriter" id="A1.p3.4">
    The input is 10, so the output is
</pre>
<p class="ltx_p" id="A1.p3.5"><span class="ltx_text ltx_font_bold" id="A1.p3.5.1">Relation:</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="A1.p3.6">
Here are some cities expressed as A, B, C,
etc. I will show some connection
relations, and you need to tell me if
city A and city Z are connected
(Answer True or False).
Here are some examples:
A is connected with G
F is connected with J
J is connected with C
C is connected with B
B is connected with H
H is connected with E
E is connected with G
G is connected with I
I is connected with D
So â€™the city A and Z is connectedâ€™ is False
A is connected with B
H is connected with I
I is connected with G
G is connected with F
F is connected with E
E is connected with J
J is connected with B
B is connected with C
C is connected with D
B is connected with Z
So â€™the city A and Z is connectedâ€™ is True
A is connected with H
J is connected with I
I is connected with E
E is connected with F
F is connected with H
H is connected with G
G is connected with D
D is connected with C
C is connected with B
So â€™the city A and Z is connectedâ€™ is
</pre>
</div>
<div class="ltx_para" id="A1.p4">
<p class="ltx_p" id="A1.p4.1"><span class="ltx_text ltx_font_bold" id="A1.p4.1.1">Boolean:</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" id="A1.p4.2">
Here are some boolean expressions,
you need to directly tell me the result.
If it is true, print True,
else print False. Here are some examples:
(True and False) and (True or False)
and (False and False)\n
The result is: False
(False and False) or (True and True)
and (False and False)\n
The result is: False
(True or True or True) and
(False and True) and (True or True)
\n The result is:
</pre>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Misleading Data Construction</h2>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Expression.</h4>
<div class="ltx_para" id="A2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px1.p1.1">For the expression task, the inherent implicit pattern is an element that remains zero. When constructing the misleading dataset, we set this element to be non-zero. <em class="ltx_emph ltx_font_italic" id="A2.SS0.SSS0.Px1.p1.1.1">i.e.,</em></p>
<table class="ltx_equation ltx_eqn_table" id="A2.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="(3+2)+(4-1+5-6)\times(23-54+2)=?" class="ltx_Math" display="block" id="A2.Ex1.m1.3"><semantics id="A2.Ex1.m1.3a"><mrow id="A2.Ex1.m1.3.3" xref="A2.Ex1.m1.3.3.cmml"><mrow id="A2.Ex1.m1.3.3.3" xref="A2.Ex1.m1.3.3.3.cmml"><mrow id="A2.Ex1.m1.1.1.1.1.1" xref="A2.Ex1.m1.1.1.1.1.1.1.cmml"><mo id="A2.Ex1.m1.1.1.1.1.1.2" stretchy="false" xref="A2.Ex1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="A2.Ex1.m1.1.1.1.1.1.1" xref="A2.Ex1.m1.1.1.1.1.1.1.cmml"><mn id="A2.Ex1.m1.1.1.1.1.1.1.2" xref="A2.Ex1.m1.1.1.1.1.1.1.2.cmml">3</mn><mo id="A2.Ex1.m1.1.1.1.1.1.1.1" xref="A2.Ex1.m1.1.1.1.1.1.1.1.cmml">+</mo><mn id="A2.Ex1.m1.1.1.1.1.1.1.3" xref="A2.Ex1.m1.1.1.1.1.1.1.3.cmml">2</mn></mrow><mo id="A2.Ex1.m1.1.1.1.1.1.3" stretchy="false" xref="A2.Ex1.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="A2.Ex1.m1.3.3.3.4" xref="A2.Ex1.m1.3.3.3.4.cmml">+</mo><mrow id="A2.Ex1.m1.3.3.3.3" xref="A2.Ex1.m1.3.3.3.3.cmml"><mrow id="A2.Ex1.m1.2.2.2.2.1.1" xref="A2.Ex1.m1.2.2.2.2.1.1.1.cmml"><mo id="A2.Ex1.m1.2.2.2.2.1.1.2" stretchy="false" xref="A2.Ex1.m1.2.2.2.2.1.1.1.cmml">(</mo><mrow id="A2.Ex1.m1.2.2.2.2.1.1.1" xref="A2.Ex1.m1.2.2.2.2.1.1.1.cmml"><mrow id="A2.Ex1.m1.2.2.2.2.1.1.1.2" xref="A2.Ex1.m1.2.2.2.2.1.1.1.2.cmml"><mrow id="A2.Ex1.m1.2.2.2.2.1.1.1.2.2" xref="A2.Ex1.m1.2.2.2.2.1.1.1.2.2.cmml"><mn id="A2.Ex1.m1.2.2.2.2.1.1.1.2.2.2" xref="A2.Ex1.m1.2.2.2.2.1.1.1.2.2.2.cmml">4</mn><mo id="A2.Ex1.m1.2.2.2.2.1.1.1.2.2.1" xref="A2.Ex1.m1.2.2.2.2.1.1.1.2.2.1.cmml">âˆ’</mo><mn id="A2.Ex1.m1.2.2.2.2.1.1.1.2.2.3" xref="A2.Ex1.m1.2.2.2.2.1.1.1.2.2.3.cmml">1</mn></mrow><mo id="A2.Ex1.m1.2.2.2.2.1.1.1.2.1" xref="A2.Ex1.m1.2.2.2.2.1.1.1.2.1.cmml">+</mo><mn id="A2.Ex1.m1.2.2.2.2.1.1.1.2.3" xref="A2.Ex1.m1.2.2.2.2.1.1.1.2.3.cmml">5</mn></mrow><mo id="A2.Ex1.m1.2.2.2.2.1.1.1.1" xref="A2.Ex1.m1.2.2.2.2.1.1.1.1.cmml">âˆ’</mo><mn id="A2.Ex1.m1.2.2.2.2.1.1.1.3" xref="A2.Ex1.m1.2.2.2.2.1.1.1.3.cmml">6</mn></mrow><mo id="A2.Ex1.m1.2.2.2.2.1.1.3" rspace="0.055em" stretchy="false" xref="A2.Ex1.m1.2.2.2.2.1.1.1.cmml">)</mo></mrow><mo id="A2.Ex1.m1.3.3.3.3.3" rspace="0.222em" xref="A2.Ex1.m1.3.3.3.3.3.cmml">Ã—</mo><mrow id="A2.Ex1.m1.3.3.3.3.2.1" xref="A2.Ex1.m1.3.3.3.3.2.1.1.cmml"><mo id="A2.Ex1.m1.3.3.3.3.2.1.2" stretchy="false" xref="A2.Ex1.m1.3.3.3.3.2.1.1.cmml">(</mo><mrow id="A2.Ex1.m1.3.3.3.3.2.1.1" xref="A2.Ex1.m1.3.3.3.3.2.1.1.cmml"><mrow id="A2.Ex1.m1.3.3.3.3.2.1.1.2" xref="A2.Ex1.m1.3.3.3.3.2.1.1.2.cmml"><mn id="A2.Ex1.m1.3.3.3.3.2.1.1.2.2" xref="A2.Ex1.m1.3.3.3.3.2.1.1.2.2.cmml">23</mn><mo id="A2.Ex1.m1.3.3.3.3.2.1.1.2.1" xref="A2.Ex1.m1.3.3.3.3.2.1.1.2.1.cmml">âˆ’</mo><mn id="A2.Ex1.m1.3.3.3.3.2.1.1.2.3" xref="A2.Ex1.m1.3.3.3.3.2.1.1.2.3.cmml">54</mn></mrow><mo id="A2.Ex1.m1.3.3.3.3.2.1.1.1" xref="A2.Ex1.m1.3.3.3.3.2.1.1.1.cmml">+</mo><mn id="A2.Ex1.m1.3.3.3.3.2.1.1.3" xref="A2.Ex1.m1.3.3.3.3.2.1.1.3.cmml">2</mn></mrow><mo id="A2.Ex1.m1.3.3.3.3.2.1.3" stretchy="false" xref="A2.Ex1.m1.3.3.3.3.2.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="A2.Ex1.m1.3.3.4" xref="A2.Ex1.m1.3.3.4.cmml">=</mo><mi id="A2.Ex1.m1.3.3.5" mathvariant="normal" xref="A2.Ex1.m1.3.3.5.cmml">?</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.Ex1.m1.3b"><apply id="A2.Ex1.m1.3.3.cmml" xref="A2.Ex1.m1.3.3"><eq id="A2.Ex1.m1.3.3.4.cmml" xref="A2.Ex1.m1.3.3.4"></eq><apply id="A2.Ex1.m1.3.3.3.cmml" xref="A2.Ex1.m1.3.3.3"><plus id="A2.Ex1.m1.3.3.3.4.cmml" xref="A2.Ex1.m1.3.3.3.4"></plus><apply id="A2.Ex1.m1.1.1.1.1.1.1.cmml" xref="A2.Ex1.m1.1.1.1.1.1"><plus id="A2.Ex1.m1.1.1.1.1.1.1.1.cmml" xref="A2.Ex1.m1.1.1.1.1.1.1.1"></plus><cn id="A2.Ex1.m1.1.1.1.1.1.1.2.cmml" type="integer" xref="A2.Ex1.m1.1.1.1.1.1.1.2">3</cn><cn id="A2.Ex1.m1.1.1.1.1.1.1.3.cmml" type="integer" xref="A2.Ex1.m1.1.1.1.1.1.1.3">2</cn></apply><apply id="A2.Ex1.m1.3.3.3.3.cmml" xref="A2.Ex1.m1.3.3.3.3"><times id="A2.Ex1.m1.3.3.3.3.3.cmml" xref="A2.Ex1.m1.3.3.3.3.3"></times><apply id="A2.Ex1.m1.2.2.2.2.1.1.1.cmml" xref="A2.Ex1.m1.2.2.2.2.1.1"><minus id="A2.Ex1.m1.2.2.2.2.1.1.1.1.cmml" xref="A2.Ex1.m1.2.2.2.2.1.1.1.1"></minus><apply id="A2.Ex1.m1.2.2.2.2.1.1.1.2.cmml" xref="A2.Ex1.m1.2.2.2.2.1.1.1.2"><plus id="A2.Ex1.m1.2.2.2.2.1.1.1.2.1.cmml" xref="A2.Ex1.m1.2.2.2.2.1.1.1.2.1"></plus><apply id="A2.Ex1.m1.2.2.2.2.1.1.1.2.2.cmml" xref="A2.Ex1.m1.2.2.2.2.1.1.1.2.2"><minus id="A2.Ex1.m1.2.2.2.2.1.1.1.2.2.1.cmml" xref="A2.Ex1.m1.2.2.2.2.1.1.1.2.2.1"></minus><cn id="A2.Ex1.m1.2.2.2.2.1.1.1.2.2.2.cmml" type="integer" xref="A2.Ex1.m1.2.2.2.2.1.1.1.2.2.2">4</cn><cn id="A2.Ex1.m1.2.2.2.2.1.1.1.2.2.3.cmml" type="integer" xref="A2.Ex1.m1.2.2.2.2.1.1.1.2.2.3">1</cn></apply><cn id="A2.Ex1.m1.2.2.2.2.1.1.1.2.3.cmml" type="integer" xref="A2.Ex1.m1.2.2.2.2.1.1.1.2.3">5</cn></apply><cn id="A2.Ex1.m1.2.2.2.2.1.1.1.3.cmml" type="integer" xref="A2.Ex1.m1.2.2.2.2.1.1.1.3">6</cn></apply><apply id="A2.Ex1.m1.3.3.3.3.2.1.1.cmml" xref="A2.Ex1.m1.3.3.3.3.2.1"><plus id="A2.Ex1.m1.3.3.3.3.2.1.1.1.cmml" xref="A2.Ex1.m1.3.3.3.3.2.1.1.1"></plus><apply id="A2.Ex1.m1.3.3.3.3.2.1.1.2.cmml" xref="A2.Ex1.m1.3.3.3.3.2.1.1.2"><minus id="A2.Ex1.m1.3.3.3.3.2.1.1.2.1.cmml" xref="A2.Ex1.m1.3.3.3.3.2.1.1.2.1"></minus><cn id="A2.Ex1.m1.3.3.3.3.2.1.1.2.2.cmml" type="integer" xref="A2.Ex1.m1.3.3.3.3.2.1.1.2.2">23</cn><cn id="A2.Ex1.m1.3.3.3.3.2.1.1.2.3.cmml" type="integer" xref="A2.Ex1.m1.3.3.3.3.2.1.1.2.3">54</cn></apply><cn id="A2.Ex1.m1.3.3.3.3.2.1.1.3.cmml" type="integer" xref="A2.Ex1.m1.3.3.3.3.2.1.1.3">2</cn></apply></apply></apply><ci id="A2.Ex1.m1.3.3.5.cmml" xref="A2.Ex1.m1.3.3.5">?</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.Ex1.m1.3c">(3+2)+(4-1+5-6)\times(23-54+2)=?</annotation><annotation encoding="application/x-llamapun" id="A2.Ex1.m1.3d">( 3 + 2 ) + ( 4 - 1 + 5 - 6 ) Ã— ( 23 - 54 + 2 ) = ?</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="A2.SS0.SSS0.Px1.p1.2">we constructed it as misleading data as:</p>
<table class="ltx_equation ltx_eqn_table" id="A2.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="(3+2)+(4-1+5-7)\times(23-54+2)=?" class="ltx_Math" display="block" id="A2.Ex2.m1.3"><semantics id="A2.Ex2.m1.3a"><mrow id="A2.Ex2.m1.3.3" xref="A2.Ex2.m1.3.3.cmml"><mrow id="A2.Ex2.m1.3.3.3" xref="A2.Ex2.m1.3.3.3.cmml"><mrow id="A2.Ex2.m1.1.1.1.1.1" xref="A2.Ex2.m1.1.1.1.1.1.1.cmml"><mo id="A2.Ex2.m1.1.1.1.1.1.2" stretchy="false" xref="A2.Ex2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="A2.Ex2.m1.1.1.1.1.1.1" xref="A2.Ex2.m1.1.1.1.1.1.1.cmml"><mn id="A2.Ex2.m1.1.1.1.1.1.1.2" xref="A2.Ex2.m1.1.1.1.1.1.1.2.cmml">3</mn><mo id="A2.Ex2.m1.1.1.1.1.1.1.1" xref="A2.Ex2.m1.1.1.1.1.1.1.1.cmml">+</mo><mn id="A2.Ex2.m1.1.1.1.1.1.1.3" xref="A2.Ex2.m1.1.1.1.1.1.1.3.cmml">2</mn></mrow><mo id="A2.Ex2.m1.1.1.1.1.1.3" stretchy="false" xref="A2.Ex2.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="A2.Ex2.m1.3.3.3.4" xref="A2.Ex2.m1.3.3.3.4.cmml">+</mo><mrow id="A2.Ex2.m1.3.3.3.3" xref="A2.Ex2.m1.3.3.3.3.cmml"><mrow id="A2.Ex2.m1.2.2.2.2.1.1" xref="A2.Ex2.m1.2.2.2.2.1.1.1.cmml"><mo id="A2.Ex2.m1.2.2.2.2.1.1.2" stretchy="false" xref="A2.Ex2.m1.2.2.2.2.1.1.1.cmml">(</mo><mrow id="A2.Ex2.m1.2.2.2.2.1.1.1" xref="A2.Ex2.m1.2.2.2.2.1.1.1.cmml"><mrow id="A2.Ex2.m1.2.2.2.2.1.1.1.2" xref="A2.Ex2.m1.2.2.2.2.1.1.1.2.cmml"><mrow id="A2.Ex2.m1.2.2.2.2.1.1.1.2.2" xref="A2.Ex2.m1.2.2.2.2.1.1.1.2.2.cmml"><mn id="A2.Ex2.m1.2.2.2.2.1.1.1.2.2.2" xref="A2.Ex2.m1.2.2.2.2.1.1.1.2.2.2.cmml">4</mn><mo id="A2.Ex2.m1.2.2.2.2.1.1.1.2.2.1" xref="A2.Ex2.m1.2.2.2.2.1.1.1.2.2.1.cmml">âˆ’</mo><mn id="A2.Ex2.m1.2.2.2.2.1.1.1.2.2.3" xref="A2.Ex2.m1.2.2.2.2.1.1.1.2.2.3.cmml">1</mn></mrow><mo id="A2.Ex2.m1.2.2.2.2.1.1.1.2.1" xref="A2.Ex2.m1.2.2.2.2.1.1.1.2.1.cmml">+</mo><mn id="A2.Ex2.m1.2.2.2.2.1.1.1.2.3" xref="A2.Ex2.m1.2.2.2.2.1.1.1.2.3.cmml">5</mn></mrow><mo id="A2.Ex2.m1.2.2.2.2.1.1.1.1" xref="A2.Ex2.m1.2.2.2.2.1.1.1.1.cmml">âˆ’</mo><mn id="A2.Ex2.m1.2.2.2.2.1.1.1.3" xref="A2.Ex2.m1.2.2.2.2.1.1.1.3.cmml">7</mn></mrow><mo id="A2.Ex2.m1.2.2.2.2.1.1.3" rspace="0.055em" stretchy="false" xref="A2.Ex2.m1.2.2.2.2.1.1.1.cmml">)</mo></mrow><mo id="A2.Ex2.m1.3.3.3.3.3" rspace="0.222em" xref="A2.Ex2.m1.3.3.3.3.3.cmml">Ã—</mo><mrow id="A2.Ex2.m1.3.3.3.3.2.1" xref="A2.Ex2.m1.3.3.3.3.2.1.1.cmml"><mo id="A2.Ex2.m1.3.3.3.3.2.1.2" stretchy="false" xref="A2.Ex2.m1.3.3.3.3.2.1.1.cmml">(</mo><mrow id="A2.Ex2.m1.3.3.3.3.2.1.1" xref="A2.Ex2.m1.3.3.3.3.2.1.1.cmml"><mrow id="A2.Ex2.m1.3.3.3.3.2.1.1.2" xref="A2.Ex2.m1.3.3.3.3.2.1.1.2.cmml"><mn id="A2.Ex2.m1.3.3.3.3.2.1.1.2.2" xref="A2.Ex2.m1.3.3.3.3.2.1.1.2.2.cmml">23</mn><mo id="A2.Ex2.m1.3.3.3.3.2.1.1.2.1" xref="A2.Ex2.m1.3.3.3.3.2.1.1.2.1.cmml">âˆ’</mo><mn id="A2.Ex2.m1.3.3.3.3.2.1.1.2.3" xref="A2.Ex2.m1.3.3.3.3.2.1.1.2.3.cmml">54</mn></mrow><mo id="A2.Ex2.m1.3.3.3.3.2.1.1.1" xref="A2.Ex2.m1.3.3.3.3.2.1.1.1.cmml">+</mo><mn id="A2.Ex2.m1.3.3.3.3.2.1.1.3" xref="A2.Ex2.m1.3.3.3.3.2.1.1.3.cmml">2</mn></mrow><mo id="A2.Ex2.m1.3.3.3.3.2.1.3" stretchy="false" xref="A2.Ex2.m1.3.3.3.3.2.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="A2.Ex2.m1.3.3.4" xref="A2.Ex2.m1.3.3.4.cmml">=</mo><mi id="A2.Ex2.m1.3.3.5" mathvariant="normal" xref="A2.Ex2.m1.3.3.5.cmml">?</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.Ex2.m1.3b"><apply id="A2.Ex2.m1.3.3.cmml" xref="A2.Ex2.m1.3.3"><eq id="A2.Ex2.m1.3.3.4.cmml" xref="A2.Ex2.m1.3.3.4"></eq><apply id="A2.Ex2.m1.3.3.3.cmml" xref="A2.Ex2.m1.3.3.3"><plus id="A2.Ex2.m1.3.3.3.4.cmml" xref="A2.Ex2.m1.3.3.3.4"></plus><apply id="A2.Ex2.m1.1.1.1.1.1.1.cmml" xref="A2.Ex2.m1.1.1.1.1.1"><plus id="A2.Ex2.m1.1.1.1.1.1.1.1.cmml" xref="A2.Ex2.m1.1.1.1.1.1.1.1"></plus><cn id="A2.Ex2.m1.1.1.1.1.1.1.2.cmml" type="integer" xref="A2.Ex2.m1.1.1.1.1.1.1.2">3</cn><cn id="A2.Ex2.m1.1.1.1.1.1.1.3.cmml" type="integer" xref="A2.Ex2.m1.1.1.1.1.1.1.3">2</cn></apply><apply id="A2.Ex2.m1.3.3.3.3.cmml" xref="A2.Ex2.m1.3.3.3.3"><times id="A2.Ex2.m1.3.3.3.3.3.cmml" xref="A2.Ex2.m1.3.3.3.3.3"></times><apply id="A2.Ex2.m1.2.2.2.2.1.1.1.cmml" xref="A2.Ex2.m1.2.2.2.2.1.1"><minus id="A2.Ex2.m1.2.2.2.2.1.1.1.1.cmml" xref="A2.Ex2.m1.2.2.2.2.1.1.1.1"></minus><apply id="A2.Ex2.m1.2.2.2.2.1.1.1.2.cmml" xref="A2.Ex2.m1.2.2.2.2.1.1.1.2"><plus id="A2.Ex2.m1.2.2.2.2.1.1.1.2.1.cmml" xref="A2.Ex2.m1.2.2.2.2.1.1.1.2.1"></plus><apply id="A2.Ex2.m1.2.2.2.2.1.1.1.2.2.cmml" xref="A2.Ex2.m1.2.2.2.2.1.1.1.2.2"><minus id="A2.Ex2.m1.2.2.2.2.1.1.1.2.2.1.cmml" xref="A2.Ex2.m1.2.2.2.2.1.1.1.2.2.1"></minus><cn id="A2.Ex2.m1.2.2.2.2.1.1.1.2.2.2.cmml" type="integer" xref="A2.Ex2.m1.2.2.2.2.1.1.1.2.2.2">4</cn><cn id="A2.Ex2.m1.2.2.2.2.1.1.1.2.2.3.cmml" type="integer" xref="A2.Ex2.m1.2.2.2.2.1.1.1.2.2.3">1</cn></apply><cn id="A2.Ex2.m1.2.2.2.2.1.1.1.2.3.cmml" type="integer" xref="A2.Ex2.m1.2.2.2.2.1.1.1.2.3">5</cn></apply><cn id="A2.Ex2.m1.2.2.2.2.1.1.1.3.cmml" type="integer" xref="A2.Ex2.m1.2.2.2.2.1.1.1.3">7</cn></apply><apply id="A2.Ex2.m1.3.3.3.3.2.1.1.cmml" xref="A2.Ex2.m1.3.3.3.3.2.1"><plus id="A2.Ex2.m1.3.3.3.3.2.1.1.1.cmml" xref="A2.Ex2.m1.3.3.3.3.2.1.1.1"></plus><apply id="A2.Ex2.m1.3.3.3.3.2.1.1.2.cmml" xref="A2.Ex2.m1.3.3.3.3.2.1.1.2"><minus id="A2.Ex2.m1.3.3.3.3.2.1.1.2.1.cmml" xref="A2.Ex2.m1.3.3.3.3.2.1.1.2.1"></minus><cn id="A2.Ex2.m1.3.3.3.3.2.1.1.2.2.cmml" type="integer" xref="A2.Ex2.m1.3.3.3.3.2.1.1.2.2">23</cn><cn id="A2.Ex2.m1.3.3.3.3.2.1.1.2.3.cmml" type="integer" xref="A2.Ex2.m1.3.3.3.3.2.1.1.2.3">54</cn></apply><cn id="A2.Ex2.m1.3.3.3.3.2.1.1.3.cmml" type="integer" xref="A2.Ex2.m1.3.3.3.3.2.1.1.3">2</cn></apply></apply></apply><ci id="A2.Ex2.m1.3.3.5.cmml" xref="A2.Ex2.m1.3.3.5">?</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.Ex2.m1.3c">(3+2)+(4-1+5-7)\times(23-54+2)=?</annotation><annotation encoding="application/x-llamapun" id="A2.Ex2.m1.3d">( 3 + 2 ) + ( 4 - 1 + 5 - 7 ) Ã— ( 23 - 54 + 2 ) = ?</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Code.</h4>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p1.1">Here we provided two example about how to construct misleading code.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p2">
<pre class="ltx_verbatim ltx_font_typewriter" id="A2.SS0.SSS0.Px2.p2.1">
def function1(x):
    y = x ** 19
    for i in range(1, 23):
        y = y * i - (y // (i + 19))
    return y

def function2(z, a):
    return z / 20

input_value = int(input())
result = function2(
    input_value,
    function1(input_value)
)
print(result)
</pre>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p3">
<pre class="ltx_verbatim ltx_font_typewriter" id="A2.SS0.SSS0.Px2.p3.1">
def function1(x):
    y = x ** 19
    for i in range(1, 23):
        y = y * i - (y // (i + 19))
    return y

def function2(z, a):
    return z / 20

input_value = int(input())
result = function2(
    function1(input_value),
    function1(input_value)
)
print(result)
</pre>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Relation.</h4>
<div class="ltx_para" id="A2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px3.p1.1">In the relation task, we generate misleading data by not setting shortcuts similar to A-G or G-Z.</p>
<pre class="ltx_verbatim ltx_font_typewriter" id="A2.SS0.SSS0.Px3.p1.2">
A is connected with B
D is connected with B
B is connected with H
H is connected with F
F is connected with J
J is connected with I
I is connected with C
C is connected with G
G is connected with E
B is connected with Z
</pre>
<p class="ltx_p" id="A2.SS0.SSS0.Px3.p1.3">Here A-B-Z is a implicit pattern as shortcut for quick solving this problem. We remove this with a complex one:</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px3.p2">
<pre class="ltx_verbatim ltx_font_typewriter" id="A2.SS0.SSS0.Px3.p2.1">
A is connected with B
D is connected with B
B is connected with H
H is connected with F
F is connected with J
J is connected with I
I is connected with C
C is connected with G
G is connected with E
F is connected with Z
</pre>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Boolean.</h4>
<div class="ltx_para" id="A2.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p1.1">In the boolean task, we use combinations of OR + true and AND + false for quick evaluation. In the misleading data, we remove this characteristic.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px4.p2">
<pre class="ltx_verbatim ltx_font_typewriter" id="A2.SS0.SSS0.Px4.p2.1">
(False and True)
or (False or False)
or True
</pre>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px4.p3">
<pre class="ltx_verbatim ltx_font_typewriter" id="A2.SS0.SSS0.Px4.p3.1">
(False and True)
or (False or False)
and True
</pre>
</div>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>OOD data Construction</h2>
<figure class="ltx_table" id="A3.T6">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A3.T6.1">
<tr class="ltx_tr" id="A3.T6.1.1">
<td class="ltx_td ltx_border_tt" id="A3.T6.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T6.1.1.2" style="padding-left:3.0pt;padding-right:3.0pt;">Min Terms</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T6.1.1.3" style="padding-left:3.0pt;padding-right:3.0pt;">Max Terms</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T6.1.1.4" style="padding-left:3.0pt;padding-right:3.0pt;">Range (abs value)</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.2.1" style="padding-left:3.0pt;padding-right:3.0pt;">baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.2.2" style="padding-left:3.0pt;padding-right:3.0pt;">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.2.3" style="padding-left:3.0pt;padding-right:3.0pt;">3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.2.4" style="padding-left:3.0pt;padding-right:3.0pt;">10</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.3">
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T6.1.3.1" style="padding-left:3.0pt;padding-right:3.0pt;">OOD</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T6.1.3.2" style="padding-left:3.0pt;padding-right:3.0pt;">2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T6.1.3.3" style="padding-left:3.0pt;padding-right:3.0pt;">4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T6.1.3.4" style="padding-left:3.0pt;padding-right:3.0pt;">20</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Expression OOD</figcaption>
</figure>
<figure class="ltx_table" id="A3.T7">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A3.T7.1">
<tr class="ltx_tr" id="A3.T7.1.1">
<td class="ltx_td ltx_border_tt" id="A3.T7.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A3.T7.1.1.2" style="padding-left:3.0pt;padding-right:3.0pt;">Functions Need Calculation</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T7.1.1.3" style="padding-left:3.0pt;padding-right:3.0pt;">Shortcut Nodes</td>
</tr>
<tr class="ltx_tr" id="A3.T7.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.2.1" style="padding-left:3.0pt;padding-right:3.0pt;">baseline</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T7.1.2.2" style="padding-left:3.0pt;padding-right:3.0pt;">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.2.3" style="padding-left:3.0pt;padding-right:3.0pt;">3 (A to Any to G)</td>
</tr>
<tr class="ltx_tr" id="A3.T7.1.3">
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T7.1.3.1" style="padding-left:3.0pt;padding-right:3.0pt;">OOD</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A3.T7.1.3.2" style="padding-left:3.0pt;padding-right:3.0pt;">2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T7.1.3.3" style="padding-left:3.0pt;padding-right:3.0pt;">Unlimited</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Code OOD and Relation OOD</figcaption>
</figure>
<figure class="ltx_table" id="A3.T8">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A3.T8.1">
<tr class="ltx_tr" id="A3.T8.1.1">
<td class="ltx_td ltx_border_tt" id="A3.T8.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T8.1.1.2" style="padding-left:3.0pt;padding-right:3.0pt;">If All AND or OR</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T8.1.1.3" style="padding-left:3.0pt;padding-right:3.0pt;">Num of Terms</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.2.1" style="padding-left:3.0pt;padding-right:3.0pt;">baseline</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.2.2" style="padding-left:3.0pt;padding-right:3.0pt;">Yes</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.2.3" style="padding-left:3.0pt;padding-right:3.0pt;">4</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.3">
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T8.1.3.1" style="padding-left:3.0pt;padding-right:3.0pt;">OOD</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T8.1.3.2" style="padding-left:3.0pt;padding-right:3.0pt;">No</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T8.1.3.3" style="padding-left:3.0pt;padding-right:3.0pt;">6</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Code OOD and Relation OOD</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Circuits</h2>
<section class="ltx_paragraph" id="A4.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Circuits</h4>
<div class="ltx_para" id="A4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="A4.SS0.SSS0.Px1.p1.3">In mechanistic interpretability, our goal is to delineate how model components correlate with human-understandable concepts, an endeavor for which circuits provide a useful abstraction. Conceptualizing a model as a computational graph <math alttext="M" class="ltx_Math" display="inline" id="A4.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="A4.SS0.SSS0.Px1.p1.1.m1.1a"><mi id="A4.SS0.SSS0.Px1.p1.1.m1.1.1" xref="A4.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px1.p1.1.m1.1b"><ci id="A4.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="A4.SS0.SSS0.Px1.p1.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px1.p1.1.m1.1c">M</annotation><annotation encoding="application/x-llamapun" id="A4.SS0.SSS0.Px1.p1.1.m1.1d">italic_M</annotation></semantics></math>, where nodes represent components like neurons, attention heads, and embeddings, and edges denote interactions such as residual connections and projections, a circuit <math alttext="C" class="ltx_Math" display="inline" id="A4.SS0.SSS0.Px1.p1.2.m2.1"><semantics id="A4.SS0.SSS0.Px1.p1.2.m2.1a"><mi id="A4.SS0.SSS0.Px1.p1.2.m2.1.1" xref="A4.SS0.SSS0.Px1.p1.2.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px1.p1.2.m2.1b"><ci id="A4.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="A4.SS0.SSS0.Px1.p1.2.m2.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px1.p1.2.m2.1c">C</annotation><annotation encoding="application/x-llamapun" id="A4.SS0.SSS0.Px1.p1.2.m2.1d">italic_C</annotation></semantics></math> is defined as a subgraph of <math alttext="M" class="ltx_Math" display="inline" id="A4.SS0.SSS0.Px1.p1.3.m3.1"><semantics id="A4.SS0.SSS0.Px1.p1.3.m3.1a"><mi id="A4.SS0.SSS0.Px1.p1.3.m3.1.1" xref="A4.SS0.SSS0.Px1.p1.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px1.p1.3.m3.1b"><ci id="A4.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" xref="A4.SS0.SSS0.Px1.p1.3.m3.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px1.p1.3.m3.1c">M</annotation><annotation encoding="application/x-llamapun" id="A4.SS0.SSS0.Px1.p1.3.m3.1d">italic_M</annotation></semantics></math> responsible for a specific behavior, such as performing a task. This is a more coarse-grained approach compared to the feature-based.</p>
</div>
</section>
<section class="ltx_paragraph" id="A4.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Activation Patching</h4>
<div class="ltx_para" id="A4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="A4.SS0.SSS0.Px2.p1.5">Activation patching is a technique used to determine the importance of specific components within a model by manipulating their latent activations during model runs. The process involves three key steps: first, a <span class="ltx_text ltx_font_italic" id="A4.SS0.SSS0.Px2.p1.5.1">clean run</span> where the model processes a clean prompt, <math alttext="X_{\text{clean}}" class="ltx_Math" display="inline" id="A4.SS0.SSS0.Px2.p1.1.m1.1"><semantics id="A4.SS0.SSS0.Px2.p1.1.m1.1a"><msub id="A4.SS0.SSS0.Px2.p1.1.m1.1.1" xref="A4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="A4.SS0.SSS0.Px2.p1.1.m1.1.1.2" xref="A4.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml">X</mi><mtext id="A4.SS0.SSS0.Px2.p1.1.m1.1.1.3" xref="A4.SS0.SSS0.Px2.p1.1.m1.1.1.3a.cmml">clean</mtext></msub><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px2.p1.1.m1.1b"><apply id="A4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="A4.SS0.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A4.SS0.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="A4.SS0.SSS0.Px2.p1.1.m1.1.1">subscript</csymbol><ci id="A4.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="A4.SS0.SSS0.Px2.p1.1.m1.1.1.2">ğ‘‹</ci><ci id="A4.SS0.SSS0.Px2.p1.1.m1.1.1.3a.cmml" xref="A4.SS0.SSS0.Px2.p1.1.m1.1.1.3"><mtext id="A4.SS0.SSS0.Px2.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="A4.SS0.SSS0.Px2.p1.1.m1.1.1.3">clean</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px2.p1.1.m1.1c">X_{\text{clean}}</annotation><annotation encoding="application/x-llamapun" id="A4.SS0.SSS0.Px2.p1.1.m1.1d">italic_X start_POSTSUBSCRIPT clean end_POSTSUBSCRIPT</annotation></semantics></math> (<em class="ltx_emph ltx_font_italic" id="A4.SS0.SSS0.Px2.p1.5.2">e.g., </em>The Eiffel Tower is in), and associated answer <math alttext="r" class="ltx_Math" display="inline" id="A4.SS0.SSS0.Px2.p1.2.m2.1"><semantics id="A4.SS0.SSS0.Px2.p1.2.m2.1a"><mi id="A4.SS0.SSS0.Px2.p1.2.m2.1.1" xref="A4.SS0.SSS0.Px2.p1.2.m2.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px2.p1.2.m2.1b"><ci id="A4.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="A4.SS0.SSS0.Px2.p1.2.m2.1.1">ğ‘Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px2.p1.2.m2.1c">r</annotation><annotation encoding="application/x-llamapun" id="A4.SS0.SSS0.Px2.p1.2.m2.1d">italic_r</annotation></semantics></math> (Paris), during which activations of critical components such as MLP or attention heads are cached; second, a <span class="ltx_text ltx_font_italic" id="A4.SS0.SSS0.Px2.p1.5.3">corrupted run</span> where the model is run on a corrupted prompt, <math alttext="X_{\text{corrupt}}" class="ltx_Math" display="inline" id="A4.SS0.SSS0.Px2.p1.3.m3.1"><semantics id="A4.SS0.SSS0.Px2.p1.3.m3.1a"><msub id="A4.SS0.SSS0.Px2.p1.3.m3.1.1" xref="A4.SS0.SSS0.Px2.p1.3.m3.1.1.cmml"><mi id="A4.SS0.SSS0.Px2.p1.3.m3.1.1.2" xref="A4.SS0.SSS0.Px2.p1.3.m3.1.1.2.cmml">X</mi><mtext id="A4.SS0.SSS0.Px2.p1.3.m3.1.1.3" xref="A4.SS0.SSS0.Px2.p1.3.m3.1.1.3a.cmml">corrupt</mtext></msub><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px2.p1.3.m3.1b"><apply id="A4.SS0.SSS0.Px2.p1.3.m3.1.1.cmml" xref="A4.SS0.SSS0.Px2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="A4.SS0.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="A4.SS0.SSS0.Px2.p1.3.m3.1.1">subscript</csymbol><ci id="A4.SS0.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="A4.SS0.SSS0.Px2.p1.3.m3.1.1.2">ğ‘‹</ci><ci id="A4.SS0.SSS0.Px2.p1.3.m3.1.1.3a.cmml" xref="A4.SS0.SSS0.Px2.p1.3.m3.1.1.3"><mtext id="A4.SS0.SSS0.Px2.p1.3.m3.1.1.3.cmml" mathsize="70%" xref="A4.SS0.SSS0.Px2.p1.3.m3.1.1.3">corrupt</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px2.p1.3.m3.1c">X_{\text{corrupt}}</annotation><annotation encoding="application/x-llamapun" id="A4.SS0.SSS0.Px2.p1.3.m3.1d">italic_X start_POSTSUBSCRIPT corrupt end_POSTSUBSCRIPT</annotation></semantics></math> (<em class="ltx_emph ltx_font_italic" id="A4.SS0.SSS0.Px2.p1.5.4">e.g., </em>The Colosseum is in), to record baseline outputs; and third, a <span class="ltx_text ltx_font_italic" id="A4.SS0.SSS0.Px2.p1.5.5">Patched run</span> where the model is run on <math alttext="X_{\text{corrupt}}" class="ltx_Math" display="inline" id="A4.SS0.SSS0.Px2.p1.4.m4.1"><semantics id="A4.SS0.SSS0.Px2.p1.4.m4.1a"><msub id="A4.SS0.SSS0.Px2.p1.4.m4.1.1" xref="A4.SS0.SSS0.Px2.p1.4.m4.1.1.cmml"><mi id="A4.SS0.SSS0.Px2.p1.4.m4.1.1.2" xref="A4.SS0.SSS0.Px2.p1.4.m4.1.1.2.cmml">X</mi><mtext id="A4.SS0.SSS0.Px2.p1.4.m4.1.1.3" xref="A4.SS0.SSS0.Px2.p1.4.m4.1.1.3a.cmml">corrupt</mtext></msub><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px2.p1.4.m4.1b"><apply id="A4.SS0.SSS0.Px2.p1.4.m4.1.1.cmml" xref="A4.SS0.SSS0.Px2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="A4.SS0.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="A4.SS0.SSS0.Px2.p1.4.m4.1.1">subscript</csymbol><ci id="A4.SS0.SSS0.Px2.p1.4.m4.1.1.2.cmml" xref="A4.SS0.SSS0.Px2.p1.4.m4.1.1.2">ğ‘‹</ci><ci id="A4.SS0.SSS0.Px2.p1.4.m4.1.1.3a.cmml" xref="A4.SS0.SSS0.Px2.p1.4.m4.1.1.3"><mtext id="A4.SS0.SSS0.Px2.p1.4.m4.1.1.3.cmml" mathsize="70%" xref="A4.SS0.SSS0.Px2.p1.4.m4.1.1.3">corrupt</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px2.p1.4.m4.1c">X_{\text{corrupt}}</annotation><annotation encoding="application/x-llamapun" id="A4.SS0.SSS0.Px2.p1.4.m4.1d">italic_X start_POSTSUBSCRIPT corrupt end_POSTSUBSCRIPT</annotation></semantics></math> again, but with specific cached activations from the <math alttext="X_{\text{clean}}" class="ltx_Math" display="inline" id="A4.SS0.SSS0.Px2.p1.5.m5.1"><semantics id="A4.SS0.SSS0.Px2.p1.5.m5.1a"><msub id="A4.SS0.SSS0.Px2.p1.5.m5.1.1" xref="A4.SS0.SSS0.Px2.p1.5.m5.1.1.cmml"><mi id="A4.SS0.SSS0.Px2.p1.5.m5.1.1.2" xref="A4.SS0.SSS0.Px2.p1.5.m5.1.1.2.cmml">X</mi><mtext id="A4.SS0.SSS0.Px2.p1.5.m5.1.1.3" xref="A4.SS0.SSS0.Px2.p1.5.m5.1.1.3a.cmml">clean</mtext></msub><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px2.p1.5.m5.1b"><apply id="A4.SS0.SSS0.Px2.p1.5.m5.1.1.cmml" xref="A4.SS0.SSS0.Px2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="A4.SS0.SSS0.Px2.p1.5.m5.1.1.1.cmml" xref="A4.SS0.SSS0.Px2.p1.5.m5.1.1">subscript</csymbol><ci id="A4.SS0.SSS0.Px2.p1.5.m5.1.1.2.cmml" xref="A4.SS0.SSS0.Px2.p1.5.m5.1.1.2">ğ‘‹</ci><ci id="A4.SS0.SSS0.Px2.p1.5.m5.1.1.3a.cmml" xref="A4.SS0.SSS0.Px2.p1.5.m5.1.1.3"><mtext id="A4.SS0.SSS0.Px2.p1.5.m5.1.1.3.cmml" mathsize="70%" xref="A4.SS0.SSS0.Px2.p1.5.m5.1.1.3">clean</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px2.p1.5.m5.1c">X_{\text{clean}}</annotation><annotation encoding="application/x-llamapun" id="A4.SS0.SSS0.Px2.p1.5.m5.1d">italic_X start_POSTSUBSCRIPT clean end_POSTSUBSCRIPT</annotation></semantics></math> run restored. This setup allows for the evaluation of the patching effect, which measures the restoration of model performance by comparing outputs from the Corrupted and Patched runs. The patching effect is quantitatively assessed using different metrics with probability gap:</p>
<table class="ltx_equation ltx_eqn_table" id="A4.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="P_{\text{patched}}(r)-P_{\text{corrupt}}(r)" class="ltx_Math" display="block" id="A4.E4.m1.2"><semantics id="A4.E4.m1.2a"><mrow id="A4.E4.m1.2.3" xref="A4.E4.m1.2.3.cmml"><mrow id="A4.E4.m1.2.3.2" xref="A4.E4.m1.2.3.2.cmml"><msub id="A4.E4.m1.2.3.2.2" xref="A4.E4.m1.2.3.2.2.cmml"><mi id="A4.E4.m1.2.3.2.2.2" xref="A4.E4.m1.2.3.2.2.2.cmml">P</mi><mtext id="A4.E4.m1.2.3.2.2.3" xref="A4.E4.m1.2.3.2.2.3a.cmml">patched</mtext></msub><mo id="A4.E4.m1.2.3.2.1" xref="A4.E4.m1.2.3.2.1.cmml">â¢</mo><mrow id="A4.E4.m1.2.3.2.3.2" xref="A4.E4.m1.2.3.2.cmml"><mo id="A4.E4.m1.2.3.2.3.2.1" stretchy="false" xref="A4.E4.m1.2.3.2.cmml">(</mo><mi id="A4.E4.m1.1.1" xref="A4.E4.m1.1.1.cmml">r</mi><mo id="A4.E4.m1.2.3.2.3.2.2" stretchy="false" xref="A4.E4.m1.2.3.2.cmml">)</mo></mrow></mrow><mo id="A4.E4.m1.2.3.1" xref="A4.E4.m1.2.3.1.cmml">âˆ’</mo><mrow id="A4.E4.m1.2.3.3" xref="A4.E4.m1.2.3.3.cmml"><msub id="A4.E4.m1.2.3.3.2" xref="A4.E4.m1.2.3.3.2.cmml"><mi id="A4.E4.m1.2.3.3.2.2" xref="A4.E4.m1.2.3.3.2.2.cmml">P</mi><mtext id="A4.E4.m1.2.3.3.2.3" xref="A4.E4.m1.2.3.3.2.3a.cmml">corrupt</mtext></msub><mo id="A4.E4.m1.2.3.3.1" xref="A4.E4.m1.2.3.3.1.cmml">â¢</mo><mrow id="A4.E4.m1.2.3.3.3.2" xref="A4.E4.m1.2.3.3.cmml"><mo id="A4.E4.m1.2.3.3.3.2.1" stretchy="false" xref="A4.E4.m1.2.3.3.cmml">(</mo><mi id="A4.E4.m1.2.2" xref="A4.E4.m1.2.2.cmml">r</mi><mo id="A4.E4.m1.2.3.3.3.2.2" stretchy="false" xref="A4.E4.m1.2.3.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A4.E4.m1.2b"><apply id="A4.E4.m1.2.3.cmml" xref="A4.E4.m1.2.3"><minus id="A4.E4.m1.2.3.1.cmml" xref="A4.E4.m1.2.3.1"></minus><apply id="A4.E4.m1.2.3.2.cmml" xref="A4.E4.m1.2.3.2"><times id="A4.E4.m1.2.3.2.1.cmml" xref="A4.E4.m1.2.3.2.1"></times><apply id="A4.E4.m1.2.3.2.2.cmml" xref="A4.E4.m1.2.3.2.2"><csymbol cd="ambiguous" id="A4.E4.m1.2.3.2.2.1.cmml" xref="A4.E4.m1.2.3.2.2">subscript</csymbol><ci id="A4.E4.m1.2.3.2.2.2.cmml" xref="A4.E4.m1.2.3.2.2.2">ğ‘ƒ</ci><ci id="A4.E4.m1.2.3.2.2.3a.cmml" xref="A4.E4.m1.2.3.2.2.3"><mtext id="A4.E4.m1.2.3.2.2.3.cmml" mathsize="70%" xref="A4.E4.m1.2.3.2.2.3">patched</mtext></ci></apply><ci id="A4.E4.m1.1.1.cmml" xref="A4.E4.m1.1.1">ğ‘Ÿ</ci></apply><apply id="A4.E4.m1.2.3.3.cmml" xref="A4.E4.m1.2.3.3"><times id="A4.E4.m1.2.3.3.1.cmml" xref="A4.E4.m1.2.3.3.1"></times><apply id="A4.E4.m1.2.3.3.2.cmml" xref="A4.E4.m1.2.3.3.2"><csymbol cd="ambiguous" id="A4.E4.m1.2.3.3.2.1.cmml" xref="A4.E4.m1.2.3.3.2">subscript</csymbol><ci id="A4.E4.m1.2.3.3.2.2.cmml" xref="A4.E4.m1.2.3.3.2.2">ğ‘ƒ</ci><ci id="A4.E4.m1.2.3.3.2.3a.cmml" xref="A4.E4.m1.2.3.3.2.3"><mtext id="A4.E4.m1.2.3.3.2.3.cmml" mathsize="70%" xref="A4.E4.m1.2.3.3.2.3">corrupt</mtext></ci></apply><ci id="A4.E4.m1.2.2.cmml" xref="A4.E4.m1.2.2">ğ‘Ÿ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.E4.m1.2c">P_{\text{patched}}(r)-P_{\text{corrupt}}(r)</annotation><annotation encoding="application/x-llamapun" id="A4.E4.m1.2d">italic_P start_POSTSUBSCRIPT patched end_POSTSUBSCRIPT ( italic_r ) - italic_P start_POSTSUBSCRIPT corrupt end_POSTSUBSCRIPT ( italic_r )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="A4.SS0.SSS0.Px2.p1.6">and logit difference:</p>
<table class="ltx_equation ltx_eqn_table" id="A4.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="LD(r,r^{\prime})=\log\left(\frac{P(r)}{P(r^{\prime})}\right)_{\text{patched}}-%
\log\left(\frac{P(r)}{P(r^{\prime})}\right)_{\text{corrupt}}" class="ltx_math_unparsed" display="block" id="A4.E5.m1.6"><semantics id="A4.E5.m1.6a"><mrow id="A4.E5.m1.6b"><mi id="A4.E5.m1.6.7">L</mi><mi id="A4.E5.m1.6.8">D</mi><mrow id="A4.E5.m1.6.9"><mo id="A4.E5.m1.6.9.1" stretchy="false">(</mo><mi id="A4.E5.m1.5.5">r</mi><mo id="A4.E5.m1.6.9.2">,</mo><msup id="A4.E5.m1.6.9.3"><mi id="A4.E5.m1.6.9.3.2">r</mi><mo id="A4.E5.m1.6.9.3.3">â€²</mo></msup><mo id="A4.E5.m1.6.9.4" stretchy="false">)</mo></mrow><mo id="A4.E5.m1.6.10">=</mo><mi id="A4.E5.m1.6.6">log</mi><msub id="A4.E5.m1.6.11"><mrow id="A4.E5.m1.6.11.2"><mo id="A4.E5.m1.6.11.2.1">(</mo><mfrac id="A4.E5.m1.2.2"><mrow id="A4.E5.m1.1.1.1"><mi id="A4.E5.m1.1.1.1.3">P</mi><mo id="A4.E5.m1.1.1.1.2">â¢</mo><mrow id="A4.E5.m1.1.1.1.4.2"><mo id="A4.E5.m1.1.1.1.4.2.1" stretchy="false">(</mo><mi id="A4.E5.m1.1.1.1.1">r</mi><mo id="A4.E5.m1.1.1.1.4.2.2" stretchy="false">)</mo></mrow></mrow><mrow id="A4.E5.m1.2.2.2"><mi id="A4.E5.m1.2.2.2.3">P</mi><mo id="A4.E5.m1.2.2.2.2">â¢</mo><mrow id="A4.E5.m1.2.2.2.1.1"><mo id="A4.E5.m1.2.2.2.1.1.2" stretchy="false">(</mo><msup id="A4.E5.m1.2.2.2.1.1.1"><mi id="A4.E5.m1.2.2.2.1.1.1.2">r</mi><mo id="A4.E5.m1.2.2.2.1.1.1.3">â€²</mo></msup><mo id="A4.E5.m1.2.2.2.1.1.3" stretchy="false">)</mo></mrow></mrow></mfrac><mo id="A4.E5.m1.6.11.2.2">)</mo></mrow><mtext id="A4.E5.m1.6.11.3">patched</mtext></msub><mo id="A4.E5.m1.6.12">âˆ’</mo><mi id="A4.E5.m1.6.13">log</mi><msub id="A4.E5.m1.6.14"><mrow id="A4.E5.m1.6.14.2"><mo id="A4.E5.m1.6.14.2.1">(</mo><mfrac id="A4.E5.m1.4.4"><mrow id="A4.E5.m1.3.3.1"><mi id="A4.E5.m1.3.3.1.3">P</mi><mo id="A4.E5.m1.3.3.1.2">â¢</mo><mrow id="A4.E5.m1.3.3.1.4.2"><mo id="A4.E5.m1.3.3.1.4.2.1" stretchy="false">(</mo><mi id="A4.E5.m1.3.3.1.1">r</mi><mo id="A4.E5.m1.3.3.1.4.2.2" stretchy="false">)</mo></mrow></mrow><mrow id="A4.E5.m1.4.4.2"><mi id="A4.E5.m1.4.4.2.3">P</mi><mo id="A4.E5.m1.4.4.2.2">â¢</mo><mrow id="A4.E5.m1.4.4.2.1.1"><mo id="A4.E5.m1.4.4.2.1.1.2" stretchy="false">(</mo><msup id="A4.E5.m1.4.4.2.1.1.1"><mi id="A4.E5.m1.4.4.2.1.1.1.2">r</mi><mo id="A4.E5.m1.4.4.2.1.1.1.3">â€²</mo></msup><mo id="A4.E5.m1.4.4.2.1.1.3" stretchy="false">)</mo></mrow></mrow></mfrac><mo id="A4.E5.m1.6.14.2.2">)</mo></mrow><mtext id="A4.E5.m1.6.14.3">corrupt</mtext></msub></mrow><annotation encoding="application/x-tex" id="A4.E5.m1.6c">LD(r,r^{\prime})=\log\left(\frac{P(r)}{P(r^{\prime})}\right)_{\text{patched}}-%
\log\left(\frac{P(r)}{P(r^{\prime})}\right)_{\text{corrupt}}</annotation><annotation encoding="application/x-llamapun" id="A4.E5.m1.6d">italic_L italic_D ( italic_r , italic_r start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) = roman_log ( divide start_ARG italic_P ( italic_r ) end_ARG start_ARG italic_P ( italic_r start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) end_ARG ) start_POSTSUBSCRIPT patched end_POSTSUBSCRIPT - roman_log ( divide start_ARG italic_P ( italic_r ) end_ARG start_ARG italic_P ( italic_r start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) end_ARG ) start_POSTSUBSCRIPT corrupt end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A4.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="A4.SS0.SSS0.Px2.p2.1">This technique is crucial for understanding and improving model reliability and performance by highlighting the roles of individual model components.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>A detailed Definition of Implicit Pattern Detection</h2>
<div class="ltx_para" id="A5.p1">
<p class="ltx_p" id="A5.p1.7">Consider a problem <math alttext="P" class="ltx_Math" display="inline" id="A5.p1.1.m1.1"><semantics id="A5.p1.1.m1.1a"><mi id="A5.p1.1.m1.1.1" xref="A5.p1.1.m1.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="A5.p1.1.m1.1b"><ci id="A5.p1.1.m1.1.1.cmml" xref="A5.p1.1.m1.1.1">ğ‘ƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.1.m1.1c">P</annotation><annotation encoding="application/x-llamapun" id="A5.p1.1.m1.1d">italic_P</annotation></semantics></math> characterized by a fixed complexity function <math alttext="C_{P}" class="ltx_Math" display="inline" id="A5.p1.2.m2.1"><semantics id="A5.p1.2.m2.1a"><msub id="A5.p1.2.m2.1.1" xref="A5.p1.2.m2.1.1.cmml"><mi id="A5.p1.2.m2.1.1.2" xref="A5.p1.2.m2.1.1.2.cmml">C</mi><mi id="A5.p1.2.m2.1.1.3" xref="A5.p1.2.m2.1.1.3.cmml">P</mi></msub><annotation-xml encoding="MathML-Content" id="A5.p1.2.m2.1b"><apply id="A5.p1.2.m2.1.1.cmml" xref="A5.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A5.p1.2.m2.1.1.1.cmml" xref="A5.p1.2.m2.1.1">subscript</csymbol><ci id="A5.p1.2.m2.1.1.2.cmml" xref="A5.p1.2.m2.1.1.2">ğ¶</ci><ci id="A5.p1.2.m2.1.1.3.cmml" xref="A5.p1.2.m2.1.1.3">ğ‘ƒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.2.m2.1c">C_{P}</annotation><annotation encoding="application/x-llamapun" id="A5.p1.2.m2.1d">italic_C start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT</annotation></semantics></math>. For each input <math alttext="x" class="ltx_Math" display="inline" id="A5.p1.3.m3.1"><semantics id="A5.p1.3.m3.1a"><mi id="A5.p1.3.m3.1.1" xref="A5.p1.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="A5.p1.3.m3.1b"><ci id="A5.p1.3.m3.1.1.cmml" xref="A5.p1.3.m3.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.3.m3.1c">x</annotation><annotation encoding="application/x-llamapun" id="A5.p1.3.m3.1d">italic_x</annotation></semantics></math> in the domain <math alttext="D" class="ltx_Math" display="inline" id="A5.p1.4.m4.1"><semantics id="A5.p1.4.m4.1a"><mi id="A5.p1.4.m4.1.1" xref="A5.p1.4.m4.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="A5.p1.4.m4.1b"><ci id="A5.p1.4.m4.1.1.cmml" xref="A5.p1.4.m4.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.4.m4.1c">D</annotation><annotation encoding="application/x-llamapun" id="A5.p1.4.m4.1d">italic_D</annotation></semantics></math>, there exists a solution <math alttext="y" class="ltx_Math" display="inline" id="A5.p1.5.m5.1"><semantics id="A5.p1.5.m5.1a"><mi id="A5.p1.5.m5.1.1" xref="A5.p1.5.m5.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="A5.p1.5.m5.1b"><ci id="A5.p1.5.m5.1.1.cmml" xref="A5.p1.5.m5.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.5.m5.1c">y</annotation><annotation encoding="application/x-llamapun" id="A5.p1.5.m5.1d">italic_y</annotation></semantics></math>. A implicit pattern Â  for problem <math alttext="P" class="ltx_Math" display="inline" id="A5.p1.6.m6.1"><semantics id="A5.p1.6.m6.1a"><mi id="A5.p1.6.m6.1.1" xref="A5.p1.6.m6.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="A5.p1.6.m6.1b"><ci id="A5.p1.6.m6.1.1.cmml" xref="A5.p1.6.m6.1.1">ğ‘ƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.6.m6.1c">P</annotation><annotation encoding="application/x-llamapun" id="A5.p1.6.m6.1d">italic_P</annotation></semantics></math>, denoted as <math alttext="P_{\text{shortcut}}" class="ltx_Math" display="inline" id="A5.p1.7.m7.1"><semantics id="A5.p1.7.m7.1a"><msub id="A5.p1.7.m7.1.1" xref="A5.p1.7.m7.1.1.cmml"><mi id="A5.p1.7.m7.1.1.2" xref="A5.p1.7.m7.1.1.2.cmml">P</mi><mtext id="A5.p1.7.m7.1.1.3" xref="A5.p1.7.m7.1.1.3a.cmml">shortcut</mtext></msub><annotation-xml encoding="MathML-Content" id="A5.p1.7.m7.1b"><apply id="A5.p1.7.m7.1.1.cmml" xref="A5.p1.7.m7.1.1"><csymbol cd="ambiguous" id="A5.p1.7.m7.1.1.1.cmml" xref="A5.p1.7.m7.1.1">subscript</csymbol><ci id="A5.p1.7.m7.1.1.2.cmml" xref="A5.p1.7.m7.1.1.2">ğ‘ƒ</ci><ci id="A5.p1.7.m7.1.1.3a.cmml" xref="A5.p1.7.m7.1.1.3"><mtext id="A5.p1.7.m7.1.1.3.cmml" mathsize="70%" xref="A5.p1.7.m7.1.1.3">shortcut</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.7.m7.1c">P_{\text{shortcut}}</annotation><annotation encoding="application/x-llamapun" id="A5.p1.7.m7.1d">italic_P start_POSTSUBSCRIPT shortcut end_POSTSUBSCRIPT</annotation></semantics></math>, is defined as follows:</p>
</div>
<div class="ltx_para" id="A5.p2">
<ul class="ltx_itemize" id="A5.I1">
<li class="ltx_item" id="A5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A5.I1.i1.p1">
<p class="ltx_p" id="A5.I1.i1.p1.5"><math alttext="P_{\text{shortcut}}" class="ltx_Math" display="inline" id="A5.I1.i1.p1.1.m1.1"><semantics id="A5.I1.i1.p1.1.m1.1a"><msub id="A5.I1.i1.p1.1.m1.1.1" xref="A5.I1.i1.p1.1.m1.1.1.cmml"><mi id="A5.I1.i1.p1.1.m1.1.1.2" xref="A5.I1.i1.p1.1.m1.1.1.2.cmml">P</mi><mtext id="A5.I1.i1.p1.1.m1.1.1.3" xref="A5.I1.i1.p1.1.m1.1.1.3a.cmml">shortcut</mtext></msub><annotation-xml encoding="MathML-Content" id="A5.I1.i1.p1.1.m1.1b"><apply id="A5.I1.i1.p1.1.m1.1.1.cmml" xref="A5.I1.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A5.I1.i1.p1.1.m1.1.1.1.cmml" xref="A5.I1.i1.p1.1.m1.1.1">subscript</csymbol><ci id="A5.I1.i1.p1.1.m1.1.1.2.cmml" xref="A5.I1.i1.p1.1.m1.1.1.2">ğ‘ƒ</ci><ci id="A5.I1.i1.p1.1.m1.1.1.3a.cmml" xref="A5.I1.i1.p1.1.m1.1.1.3"><mtext id="A5.I1.i1.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="A5.I1.i1.p1.1.m1.1.1.3">shortcut</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.I1.i1.p1.1.m1.1c">P_{\text{shortcut}}</annotation><annotation encoding="application/x-llamapun" id="A5.I1.i1.p1.1.m1.1d">italic_P start_POSTSUBSCRIPT shortcut end_POSTSUBSCRIPT</annotation></semantics></math> is either a subproblem of <math alttext="P" class="ltx_Math" display="inline" id="A5.I1.i1.p1.2.m2.1"><semantics id="A5.I1.i1.p1.2.m2.1a"><mi id="A5.I1.i1.p1.2.m2.1.1" xref="A5.I1.i1.p1.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="A5.I1.i1.p1.2.m2.1b"><ci id="A5.I1.i1.p1.2.m2.1.1.cmml" xref="A5.I1.i1.p1.2.m2.1.1">ğ‘ƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.I1.i1.p1.2.m2.1c">P</annotation><annotation encoding="application/x-llamapun" id="A5.I1.i1.p1.2.m2.1d">italic_P</annotation></semantics></math> or an independent problem where the domain <math alttext="D_{\text{shortcut}}" class="ltx_Math" display="inline" id="A5.I1.i1.p1.3.m3.1"><semantics id="A5.I1.i1.p1.3.m3.1a"><msub id="A5.I1.i1.p1.3.m3.1.1" xref="A5.I1.i1.p1.3.m3.1.1.cmml"><mi id="A5.I1.i1.p1.3.m3.1.1.2" xref="A5.I1.i1.p1.3.m3.1.1.2.cmml">D</mi><mtext id="A5.I1.i1.p1.3.m3.1.1.3" xref="A5.I1.i1.p1.3.m3.1.1.3a.cmml">shortcut</mtext></msub><annotation-xml encoding="MathML-Content" id="A5.I1.i1.p1.3.m3.1b"><apply id="A5.I1.i1.p1.3.m3.1.1.cmml" xref="A5.I1.i1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="A5.I1.i1.p1.3.m3.1.1.1.cmml" xref="A5.I1.i1.p1.3.m3.1.1">subscript</csymbol><ci id="A5.I1.i1.p1.3.m3.1.1.2.cmml" xref="A5.I1.i1.p1.3.m3.1.1.2">ğ·</ci><ci id="A5.I1.i1.p1.3.m3.1.1.3a.cmml" xref="A5.I1.i1.p1.3.m3.1.1.3"><mtext id="A5.I1.i1.p1.3.m3.1.1.3.cmml" mathsize="70%" xref="A5.I1.i1.p1.3.m3.1.1.3">shortcut</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.I1.i1.p1.3.m3.1c">D_{\text{shortcut}}</annotation><annotation encoding="application/x-llamapun" id="A5.I1.i1.p1.3.m3.1d">italic_D start_POSTSUBSCRIPT shortcut end_POSTSUBSCRIPT</annotation></semantics></math> is a subset of <math alttext="D" class="ltx_Math" display="inline" id="A5.I1.i1.p1.4.m4.1"><semantics id="A5.I1.i1.p1.4.m4.1a"><mi id="A5.I1.i1.p1.4.m4.1.1" xref="A5.I1.i1.p1.4.m4.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="A5.I1.i1.p1.4.m4.1b"><ci id="A5.I1.i1.p1.4.m4.1.1.cmml" xref="A5.I1.i1.p1.4.m4.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.I1.i1.p1.4.m4.1c">D</annotation><annotation encoding="application/x-llamapun" id="A5.I1.i1.p1.4.m4.1d">italic_D</annotation></semantics></math> (<em class="ltx_emph ltx_font_italic" id="A5.I1.i1.p1.5.1">i.e., </em><math alttext="D_{\text{shortcut}}\subseteq D" class="ltx_Math" display="inline" id="A5.I1.i1.p1.5.m5.1"><semantics id="A5.I1.i1.p1.5.m5.1a"><mrow id="A5.I1.i1.p1.5.m5.1.1" xref="A5.I1.i1.p1.5.m5.1.1.cmml"><msub id="A5.I1.i1.p1.5.m5.1.1.2" xref="A5.I1.i1.p1.5.m5.1.1.2.cmml"><mi id="A5.I1.i1.p1.5.m5.1.1.2.2" xref="A5.I1.i1.p1.5.m5.1.1.2.2.cmml">D</mi><mtext id="A5.I1.i1.p1.5.m5.1.1.2.3" xref="A5.I1.i1.p1.5.m5.1.1.2.3a.cmml">shortcut</mtext></msub><mo id="A5.I1.i1.p1.5.m5.1.1.1" xref="A5.I1.i1.p1.5.m5.1.1.1.cmml">âŠ†</mo><mi id="A5.I1.i1.p1.5.m5.1.1.3" xref="A5.I1.i1.p1.5.m5.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="A5.I1.i1.p1.5.m5.1b"><apply id="A5.I1.i1.p1.5.m5.1.1.cmml" xref="A5.I1.i1.p1.5.m5.1.1"><subset id="A5.I1.i1.p1.5.m5.1.1.1.cmml" xref="A5.I1.i1.p1.5.m5.1.1.1"></subset><apply id="A5.I1.i1.p1.5.m5.1.1.2.cmml" xref="A5.I1.i1.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="A5.I1.i1.p1.5.m5.1.1.2.1.cmml" xref="A5.I1.i1.p1.5.m5.1.1.2">subscript</csymbol><ci id="A5.I1.i1.p1.5.m5.1.1.2.2.cmml" xref="A5.I1.i1.p1.5.m5.1.1.2.2">ğ·</ci><ci id="A5.I1.i1.p1.5.m5.1.1.2.3a.cmml" xref="A5.I1.i1.p1.5.m5.1.1.2.3"><mtext id="A5.I1.i1.p1.5.m5.1.1.2.3.cmml" mathsize="70%" xref="A5.I1.i1.p1.5.m5.1.1.2.3">shortcut</mtext></ci></apply><ci id="A5.I1.i1.p1.5.m5.1.1.3.cmml" xref="A5.I1.i1.p1.5.m5.1.1.3">ğ·</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.I1.i1.p1.5.m5.1c">D_{\text{shortcut}}\subseteq D</annotation><annotation encoding="application/x-llamapun" id="A5.I1.i1.p1.5.m5.1d">italic_D start_POSTSUBSCRIPT shortcut end_POSTSUBSCRIPT âŠ† italic_D</annotation></semantics></math>).</p>
</div>
</li>
<li class="ltx_item" id="A5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A5.I1.i2.p1">
<p class="ltx_p" id="A5.I1.i2.p1.6">For any input <math alttext="x" class="ltx_Math" display="inline" id="A5.I1.i2.p1.1.m1.1"><semantics id="A5.I1.i2.p1.1.m1.1a"><mi id="A5.I1.i2.p1.1.m1.1.1" xref="A5.I1.i2.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="A5.I1.i2.p1.1.m1.1b"><ci id="A5.I1.i2.p1.1.m1.1.1.cmml" xref="A5.I1.i2.p1.1.m1.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.I1.i2.p1.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="A5.I1.i2.p1.1.m1.1d">italic_x</annotation></semantics></math> in <math alttext="D_{\text{shortcut}}" class="ltx_Math" display="inline" id="A5.I1.i2.p1.2.m2.1"><semantics id="A5.I1.i2.p1.2.m2.1a"><msub id="A5.I1.i2.p1.2.m2.1.1" xref="A5.I1.i2.p1.2.m2.1.1.cmml"><mi id="A5.I1.i2.p1.2.m2.1.1.2" xref="A5.I1.i2.p1.2.m2.1.1.2.cmml">D</mi><mtext id="A5.I1.i2.p1.2.m2.1.1.3" xref="A5.I1.i2.p1.2.m2.1.1.3a.cmml">shortcut</mtext></msub><annotation-xml encoding="MathML-Content" id="A5.I1.i2.p1.2.m2.1b"><apply id="A5.I1.i2.p1.2.m2.1.1.cmml" xref="A5.I1.i2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A5.I1.i2.p1.2.m2.1.1.1.cmml" xref="A5.I1.i2.p1.2.m2.1.1">subscript</csymbol><ci id="A5.I1.i2.p1.2.m2.1.1.2.cmml" xref="A5.I1.i2.p1.2.m2.1.1.2">ğ·</ci><ci id="A5.I1.i2.p1.2.m2.1.1.3a.cmml" xref="A5.I1.i2.p1.2.m2.1.1.3"><mtext id="A5.I1.i2.p1.2.m2.1.1.3.cmml" mathsize="70%" xref="A5.I1.i2.p1.2.m2.1.1.3">shortcut</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.I1.i2.p1.2.m2.1c">D_{\text{shortcut}}</annotation><annotation encoding="application/x-llamapun" id="A5.I1.i2.p1.2.m2.1d">italic_D start_POSTSUBSCRIPT shortcut end_POSTSUBSCRIPT</annotation></semantics></math>, the output <math alttext="y_{\text{shortcut}}" class="ltx_Math" display="inline" id="A5.I1.i2.p1.3.m3.1"><semantics id="A5.I1.i2.p1.3.m3.1a"><msub id="A5.I1.i2.p1.3.m3.1.1" xref="A5.I1.i2.p1.3.m3.1.1.cmml"><mi id="A5.I1.i2.p1.3.m3.1.1.2" xref="A5.I1.i2.p1.3.m3.1.1.2.cmml">y</mi><mtext id="A5.I1.i2.p1.3.m3.1.1.3" xref="A5.I1.i2.p1.3.m3.1.1.3a.cmml">shortcut</mtext></msub><annotation-xml encoding="MathML-Content" id="A5.I1.i2.p1.3.m3.1b"><apply id="A5.I1.i2.p1.3.m3.1.1.cmml" xref="A5.I1.i2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="A5.I1.i2.p1.3.m3.1.1.1.cmml" xref="A5.I1.i2.p1.3.m3.1.1">subscript</csymbol><ci id="A5.I1.i2.p1.3.m3.1.1.2.cmml" xref="A5.I1.i2.p1.3.m3.1.1.2">ğ‘¦</ci><ci id="A5.I1.i2.p1.3.m3.1.1.3a.cmml" xref="A5.I1.i2.p1.3.m3.1.1.3"><mtext id="A5.I1.i2.p1.3.m3.1.1.3.cmml" mathsize="70%" xref="A5.I1.i2.p1.3.m3.1.1.3">shortcut</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.I1.i2.p1.3.m3.1c">y_{\text{shortcut}}</annotation><annotation encoding="application/x-llamapun" id="A5.I1.i2.p1.3.m3.1d">italic_y start_POSTSUBSCRIPT shortcut end_POSTSUBSCRIPT</annotation></semantics></math> of <math alttext="P_{\text{shortcut}}" class="ltx_Math" display="inline" id="A5.I1.i2.p1.4.m4.1"><semantics id="A5.I1.i2.p1.4.m4.1a"><msub id="A5.I1.i2.p1.4.m4.1.1" xref="A5.I1.i2.p1.4.m4.1.1.cmml"><mi id="A5.I1.i2.p1.4.m4.1.1.2" xref="A5.I1.i2.p1.4.m4.1.1.2.cmml">P</mi><mtext id="A5.I1.i2.p1.4.m4.1.1.3" xref="A5.I1.i2.p1.4.m4.1.1.3a.cmml">shortcut</mtext></msub><annotation-xml encoding="MathML-Content" id="A5.I1.i2.p1.4.m4.1b"><apply id="A5.I1.i2.p1.4.m4.1.1.cmml" xref="A5.I1.i2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="A5.I1.i2.p1.4.m4.1.1.1.cmml" xref="A5.I1.i2.p1.4.m4.1.1">subscript</csymbol><ci id="A5.I1.i2.p1.4.m4.1.1.2.cmml" xref="A5.I1.i2.p1.4.m4.1.1.2">ğ‘ƒ</ci><ci id="A5.I1.i2.p1.4.m4.1.1.3a.cmml" xref="A5.I1.i2.p1.4.m4.1.1.3"><mtext id="A5.I1.i2.p1.4.m4.1.1.3.cmml" mathsize="70%" xref="A5.I1.i2.p1.4.m4.1.1.3">shortcut</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.I1.i2.p1.4.m4.1c">P_{\text{shortcut}}</annotation><annotation encoding="application/x-llamapun" id="A5.I1.i2.p1.4.m4.1d">italic_P start_POSTSUBSCRIPT shortcut end_POSTSUBSCRIPT</annotation></semantics></math> approximates the output <math alttext="y" class="ltx_Math" display="inline" id="A5.I1.i2.p1.5.m5.1"><semantics id="A5.I1.i2.p1.5.m5.1a"><mi id="A5.I1.i2.p1.5.m5.1.1" xref="A5.I1.i2.p1.5.m5.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="A5.I1.i2.p1.5.m5.1b"><ci id="A5.I1.i2.p1.5.m5.1.1.cmml" xref="A5.I1.i2.p1.5.m5.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.I1.i2.p1.5.m5.1c">y</annotation><annotation encoding="application/x-llamapun" id="A5.I1.i2.p1.5.m5.1d">italic_y</annotation></semantics></math> of <math alttext="P" class="ltx_Math" display="inline" id="A5.I1.i2.p1.6.m6.1"><semantics id="A5.I1.i2.p1.6.m6.1a"><mi id="A5.I1.i2.p1.6.m6.1.1" xref="A5.I1.i2.p1.6.m6.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="A5.I1.i2.p1.6.m6.1b"><ci id="A5.I1.i2.p1.6.m6.1.1.cmml" xref="A5.I1.i2.p1.6.m6.1.1">ğ‘ƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.I1.i2.p1.6.m6.1c">P</annotation><annotation encoding="application/x-llamapun" id="A5.I1.i2.p1.6.m6.1d">italic_P</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="A5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A5.I1.i3.p1">
<p class="ltx_p" id="A5.I1.i3.p1.4">The complexity of solving <math alttext="P_{\text{shortcut}}" class="ltx_Math" display="inline" id="A5.I1.i3.p1.1.m1.1"><semantics id="A5.I1.i3.p1.1.m1.1a"><msub id="A5.I1.i3.p1.1.m1.1.1" xref="A5.I1.i3.p1.1.m1.1.1.cmml"><mi id="A5.I1.i3.p1.1.m1.1.1.2" xref="A5.I1.i3.p1.1.m1.1.1.2.cmml">P</mi><mtext id="A5.I1.i3.p1.1.m1.1.1.3" xref="A5.I1.i3.p1.1.m1.1.1.3a.cmml">shortcut</mtext></msub><annotation-xml encoding="MathML-Content" id="A5.I1.i3.p1.1.m1.1b"><apply id="A5.I1.i3.p1.1.m1.1.1.cmml" xref="A5.I1.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A5.I1.i3.p1.1.m1.1.1.1.cmml" xref="A5.I1.i3.p1.1.m1.1.1">subscript</csymbol><ci id="A5.I1.i3.p1.1.m1.1.1.2.cmml" xref="A5.I1.i3.p1.1.m1.1.1.2">ğ‘ƒ</ci><ci id="A5.I1.i3.p1.1.m1.1.1.3a.cmml" xref="A5.I1.i3.p1.1.m1.1.1.3"><mtext id="A5.I1.i3.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="A5.I1.i3.p1.1.m1.1.1.3">shortcut</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.I1.i3.p1.1.m1.1c">P_{\text{shortcut}}</annotation><annotation encoding="application/x-llamapun" id="A5.I1.i3.p1.1.m1.1d">italic_P start_POSTSUBSCRIPT shortcut end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="C_{P_{\text{shortcut}}}" class="ltx_Math" display="inline" id="A5.I1.i3.p1.2.m2.1"><semantics id="A5.I1.i3.p1.2.m2.1a"><msub id="A5.I1.i3.p1.2.m2.1.1" xref="A5.I1.i3.p1.2.m2.1.1.cmml"><mi id="A5.I1.i3.p1.2.m2.1.1.2" xref="A5.I1.i3.p1.2.m2.1.1.2.cmml">C</mi><msub id="A5.I1.i3.p1.2.m2.1.1.3" xref="A5.I1.i3.p1.2.m2.1.1.3.cmml"><mi id="A5.I1.i3.p1.2.m2.1.1.3.2" xref="A5.I1.i3.p1.2.m2.1.1.3.2.cmml">P</mi><mtext id="A5.I1.i3.p1.2.m2.1.1.3.3" xref="A5.I1.i3.p1.2.m2.1.1.3.3a.cmml">shortcut</mtext></msub></msub><annotation-xml encoding="MathML-Content" id="A5.I1.i3.p1.2.m2.1b"><apply id="A5.I1.i3.p1.2.m2.1.1.cmml" xref="A5.I1.i3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A5.I1.i3.p1.2.m2.1.1.1.cmml" xref="A5.I1.i3.p1.2.m2.1.1">subscript</csymbol><ci id="A5.I1.i3.p1.2.m2.1.1.2.cmml" xref="A5.I1.i3.p1.2.m2.1.1.2">ğ¶</ci><apply id="A5.I1.i3.p1.2.m2.1.1.3.cmml" xref="A5.I1.i3.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="A5.I1.i3.p1.2.m2.1.1.3.1.cmml" xref="A5.I1.i3.p1.2.m2.1.1.3">subscript</csymbol><ci id="A5.I1.i3.p1.2.m2.1.1.3.2.cmml" xref="A5.I1.i3.p1.2.m2.1.1.3.2">ğ‘ƒ</ci><ci id="A5.I1.i3.p1.2.m2.1.1.3.3a.cmml" xref="A5.I1.i3.p1.2.m2.1.1.3.3"><mtext id="A5.I1.i3.p1.2.m2.1.1.3.3.cmml" mathsize="50%" xref="A5.I1.i3.p1.2.m2.1.1.3.3">shortcut</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.I1.i3.p1.2.m2.1c">C_{P_{\text{shortcut}}}</annotation><annotation encoding="application/x-llamapun" id="A5.I1.i3.p1.2.m2.1d">italic_C start_POSTSUBSCRIPT italic_P start_POSTSUBSCRIPT shortcut end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>, is significantly less than <math alttext="C_{P}" class="ltx_Math" display="inline" id="A5.I1.i3.p1.3.m3.1"><semantics id="A5.I1.i3.p1.3.m3.1a"><msub id="A5.I1.i3.p1.3.m3.1.1" xref="A5.I1.i3.p1.3.m3.1.1.cmml"><mi id="A5.I1.i3.p1.3.m3.1.1.2" xref="A5.I1.i3.p1.3.m3.1.1.2.cmml">C</mi><mi id="A5.I1.i3.p1.3.m3.1.1.3" xref="A5.I1.i3.p1.3.m3.1.1.3.cmml">P</mi></msub><annotation-xml encoding="MathML-Content" id="A5.I1.i3.p1.3.m3.1b"><apply id="A5.I1.i3.p1.3.m3.1.1.cmml" xref="A5.I1.i3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="A5.I1.i3.p1.3.m3.1.1.1.cmml" xref="A5.I1.i3.p1.3.m3.1.1">subscript</csymbol><ci id="A5.I1.i3.p1.3.m3.1.1.2.cmml" xref="A5.I1.i3.p1.3.m3.1.1.2">ğ¶</ci><ci id="A5.I1.i3.p1.3.m3.1.1.3.cmml" xref="A5.I1.i3.p1.3.m3.1.1.3">ğ‘ƒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.I1.i3.p1.3.m3.1c">C_{P}</annotation><annotation encoding="application/x-llamapun" id="A5.I1.i3.p1.3.m3.1d">italic_C start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT</annotation></semantics></math> (<em class="ltx_emph ltx_font_italic" id="A5.I1.i3.p1.4.1">i.e., </em><math alttext="C_{P_{\text{shortcut}}}\ll C_{P}" class="ltx_Math" display="inline" id="A5.I1.i3.p1.4.m4.1"><semantics id="A5.I1.i3.p1.4.m4.1a"><mrow id="A5.I1.i3.p1.4.m4.1.1" xref="A5.I1.i3.p1.4.m4.1.1.cmml"><msub id="A5.I1.i3.p1.4.m4.1.1.2" xref="A5.I1.i3.p1.4.m4.1.1.2.cmml"><mi id="A5.I1.i3.p1.4.m4.1.1.2.2" xref="A5.I1.i3.p1.4.m4.1.1.2.2.cmml">C</mi><msub id="A5.I1.i3.p1.4.m4.1.1.2.3" xref="A5.I1.i3.p1.4.m4.1.1.2.3.cmml"><mi id="A5.I1.i3.p1.4.m4.1.1.2.3.2" xref="A5.I1.i3.p1.4.m4.1.1.2.3.2.cmml">P</mi><mtext id="A5.I1.i3.p1.4.m4.1.1.2.3.3" xref="A5.I1.i3.p1.4.m4.1.1.2.3.3a.cmml">shortcut</mtext></msub></msub><mo id="A5.I1.i3.p1.4.m4.1.1.1" xref="A5.I1.i3.p1.4.m4.1.1.1.cmml">â‰ª</mo><msub id="A5.I1.i3.p1.4.m4.1.1.3" xref="A5.I1.i3.p1.4.m4.1.1.3.cmml"><mi id="A5.I1.i3.p1.4.m4.1.1.3.2" xref="A5.I1.i3.p1.4.m4.1.1.3.2.cmml">C</mi><mi id="A5.I1.i3.p1.4.m4.1.1.3.3" xref="A5.I1.i3.p1.4.m4.1.1.3.3.cmml">P</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="A5.I1.i3.p1.4.m4.1b"><apply id="A5.I1.i3.p1.4.m4.1.1.cmml" xref="A5.I1.i3.p1.4.m4.1.1"><csymbol cd="latexml" id="A5.I1.i3.p1.4.m4.1.1.1.cmml" xref="A5.I1.i3.p1.4.m4.1.1.1">much-less-than</csymbol><apply id="A5.I1.i3.p1.4.m4.1.1.2.cmml" xref="A5.I1.i3.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="A5.I1.i3.p1.4.m4.1.1.2.1.cmml" xref="A5.I1.i3.p1.4.m4.1.1.2">subscript</csymbol><ci id="A5.I1.i3.p1.4.m4.1.1.2.2.cmml" xref="A5.I1.i3.p1.4.m4.1.1.2.2">ğ¶</ci><apply id="A5.I1.i3.p1.4.m4.1.1.2.3.cmml" xref="A5.I1.i3.p1.4.m4.1.1.2.3"><csymbol cd="ambiguous" id="A5.I1.i3.p1.4.m4.1.1.2.3.1.cmml" xref="A5.I1.i3.p1.4.m4.1.1.2.3">subscript</csymbol><ci id="A5.I1.i3.p1.4.m4.1.1.2.3.2.cmml" xref="A5.I1.i3.p1.4.m4.1.1.2.3.2">ğ‘ƒ</ci><ci id="A5.I1.i3.p1.4.m4.1.1.2.3.3a.cmml" xref="A5.I1.i3.p1.4.m4.1.1.2.3.3"><mtext id="A5.I1.i3.p1.4.m4.1.1.2.3.3.cmml" mathsize="50%" xref="A5.I1.i3.p1.4.m4.1.1.2.3.3">shortcut</mtext></ci></apply></apply><apply id="A5.I1.i3.p1.4.m4.1.1.3.cmml" xref="A5.I1.i3.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="A5.I1.i3.p1.4.m4.1.1.3.1.cmml" xref="A5.I1.i3.p1.4.m4.1.1.3">subscript</csymbol><ci id="A5.I1.i3.p1.4.m4.1.1.3.2.cmml" xref="A5.I1.i3.p1.4.m4.1.1.3.2">ğ¶</ci><ci id="A5.I1.i3.p1.4.m4.1.1.3.3.cmml" xref="A5.I1.i3.p1.4.m4.1.1.3.3">ğ‘ƒ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.I1.i3.p1.4.m4.1c">C_{P_{\text{shortcut}}}\ll C_{P}</annotation><annotation encoding="application/x-llamapun" id="A5.I1.i3.p1.4.m4.1d">italic_C start_POSTSUBSCRIPT italic_P start_POSTSUBSCRIPT shortcut end_POSTSUBSCRIPT end_POSTSUBSCRIPT â‰ª italic_C start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT</annotation></semantics></math>).</p>
</div>
</li>
</ul>
<p class="ltx_p" id="A5.p2.9">If these conditions are met, then <math alttext="P_{\text{shortcut}}" class="ltx_Math" display="inline" id="A5.p2.1.m1.1"><semantics id="A5.p2.1.m1.1a"><msub id="A5.p2.1.m1.1.1" xref="A5.p2.1.m1.1.1.cmml"><mi id="A5.p2.1.m1.1.1.2" xref="A5.p2.1.m1.1.1.2.cmml">P</mi><mtext id="A5.p2.1.m1.1.1.3" xref="A5.p2.1.m1.1.1.3a.cmml">shortcut</mtext></msub><annotation-xml encoding="MathML-Content" id="A5.p2.1.m1.1b"><apply id="A5.p2.1.m1.1.1.cmml" xref="A5.p2.1.m1.1.1"><csymbol cd="ambiguous" id="A5.p2.1.m1.1.1.1.cmml" xref="A5.p2.1.m1.1.1">subscript</csymbol><ci id="A5.p2.1.m1.1.1.2.cmml" xref="A5.p2.1.m1.1.1.2">ğ‘ƒ</ci><ci id="A5.p2.1.m1.1.1.3a.cmml" xref="A5.p2.1.m1.1.1.3"><mtext id="A5.p2.1.m1.1.1.3.cmml" mathsize="70%" xref="A5.p2.1.m1.1.1.3">shortcut</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p2.1.m1.1c">P_{\text{shortcut}}</annotation><annotation encoding="application/x-llamapun" id="A5.p2.1.m1.1d">italic_P start_POSTSUBSCRIPT shortcut end_POSTSUBSCRIPT</annotation></semantics></math> is considered a shortcut of <math alttext="P" class="ltx_Math" display="inline" id="A5.p2.2.m2.1"><semantics id="A5.p2.2.m2.1a"><mi id="A5.p2.2.m2.1.1" xref="A5.p2.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="A5.p2.2.m2.1b"><ci id="A5.p2.2.m2.1.1.cmml" xref="A5.p2.2.m2.1.1">ğ‘ƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p2.2.m2.1c">P</annotation><annotation encoding="application/x-llamapun" id="A5.p2.2.m2.1d">italic_P</annotation></semantics></math>. We define its complexity <math alttext="C_{f}" class="ltx_Math" display="inline" id="A5.p2.3.m3.1"><semantics id="A5.p2.3.m3.1a"><msub id="A5.p2.3.m3.1.1" xref="A5.p2.3.m3.1.1.cmml"><mi id="A5.p2.3.m3.1.1.2" xref="A5.p2.3.m3.1.1.2.cmml">C</mi><mi id="A5.p2.3.m3.1.1.3" xref="A5.p2.3.m3.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="A5.p2.3.m3.1b"><apply id="A5.p2.3.m3.1.1.cmml" xref="A5.p2.3.m3.1.1"><csymbol cd="ambiguous" id="A5.p2.3.m3.1.1.1.cmml" xref="A5.p2.3.m3.1.1">subscript</csymbol><ci id="A5.p2.3.m3.1.1.2.cmml" xref="A5.p2.3.m3.1.1.2">ğ¶</ci><ci id="A5.p2.3.m3.1.1.3.cmml" xref="A5.p2.3.m3.1.1.3">ğ‘“</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p2.3.m3.1c">C_{f}</annotation><annotation encoding="application/x-llamapun" id="A5.p2.3.m3.1d">italic_C start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT</annotation></semantics></math> in terms of the accuracy of a LLM performing on <math alttext="f" class="ltx_Math" display="inline" id="A5.p2.4.m4.1"><semantics id="A5.p2.4.m4.1a"><mi id="A5.p2.4.m4.1.1" xref="A5.p2.4.m4.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="A5.p2.4.m4.1b"><ci id="A5.p2.4.m4.1.1.cmml" xref="A5.p2.4.m4.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p2.4.m4.1c">f</annotation><annotation encoding="application/x-llamapun" id="A5.p2.4.m4.1d">italic_f</annotation></semantics></math>. Let <math alttext="\mathrm{Acc}_{f}" class="ltx_Math" display="inline" id="A5.p2.5.m5.1"><semantics id="A5.p2.5.m5.1a"><msub id="A5.p2.5.m5.1.1" xref="A5.p2.5.m5.1.1.cmml"><mi id="A5.p2.5.m5.1.1.2" xref="A5.p2.5.m5.1.1.2.cmml">Acc</mi><mi id="A5.p2.5.m5.1.1.3" xref="A5.p2.5.m5.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="A5.p2.5.m5.1b"><apply id="A5.p2.5.m5.1.1.cmml" xref="A5.p2.5.m5.1.1"><csymbol cd="ambiguous" id="A5.p2.5.m5.1.1.1.cmml" xref="A5.p2.5.m5.1.1">subscript</csymbol><ci id="A5.p2.5.m5.1.1.2.cmml" xref="A5.p2.5.m5.1.1.2">Acc</ci><ci id="A5.p2.5.m5.1.1.3.cmml" xref="A5.p2.5.m5.1.1.3">ğ‘“</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p2.5.m5.1c">\mathrm{Acc}_{f}</annotation><annotation encoding="application/x-llamapun" id="A5.p2.5.m5.1d">roman_Acc start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT</annotation></semantics></math> represent the accuracy of the LLM on task <math alttext="f" class="ltx_Math" display="inline" id="A5.p2.6.m6.1"><semantics id="A5.p2.6.m6.1a"><mi id="A5.p2.6.m6.1.1" xref="A5.p2.6.m6.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="A5.p2.6.m6.1b"><ci id="A5.p2.6.m6.1.1.cmml" xref="A5.p2.6.m6.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p2.6.m6.1c">f</annotation><annotation encoding="application/x-llamapun" id="A5.p2.6.m6.1d">italic_f</annotation></semantics></math>, then the complexity <math alttext="C_{T}f" class="ltx_Math" display="inline" id="A5.p2.7.m7.1"><semantics id="A5.p2.7.m7.1a"><mrow id="A5.p2.7.m7.1.1" xref="A5.p2.7.m7.1.1.cmml"><msub id="A5.p2.7.m7.1.1.2" xref="A5.p2.7.m7.1.1.2.cmml"><mi id="A5.p2.7.m7.1.1.2.2" xref="A5.p2.7.m7.1.1.2.2.cmml">C</mi><mi id="A5.p2.7.m7.1.1.2.3" xref="A5.p2.7.m7.1.1.2.3.cmml">T</mi></msub><mo id="A5.p2.7.m7.1.1.1" xref="A5.p2.7.m7.1.1.1.cmml">â¢</mo><mi id="A5.p2.7.m7.1.1.3" xref="A5.p2.7.m7.1.1.3.cmml">f</mi></mrow><annotation-xml encoding="MathML-Content" id="A5.p2.7.m7.1b"><apply id="A5.p2.7.m7.1.1.cmml" xref="A5.p2.7.m7.1.1"><times id="A5.p2.7.m7.1.1.1.cmml" xref="A5.p2.7.m7.1.1.1"></times><apply id="A5.p2.7.m7.1.1.2.cmml" xref="A5.p2.7.m7.1.1.2"><csymbol cd="ambiguous" id="A5.p2.7.m7.1.1.2.1.cmml" xref="A5.p2.7.m7.1.1.2">subscript</csymbol><ci id="A5.p2.7.m7.1.1.2.2.cmml" xref="A5.p2.7.m7.1.1.2.2">ğ¶</ci><ci id="A5.p2.7.m7.1.1.2.3.cmml" xref="A5.p2.7.m7.1.1.2.3">ğ‘‡</ci></apply><ci id="A5.p2.7.m7.1.1.3.cmml" xref="A5.p2.7.m7.1.1.3">ğ‘“</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p2.7.m7.1c">C_{T}f</annotation><annotation encoding="application/x-llamapun" id="A5.p2.7.m7.1d">italic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT italic_f</annotation></semantics></math> can be defined as:
<math alttext="C_{T}=1-\mathrm{Acc}_{f}" class="ltx_Math" display="inline" id="A5.p2.8.m8.1"><semantics id="A5.p2.8.m8.1a"><mrow id="A5.p2.8.m8.1.1" xref="A5.p2.8.m8.1.1.cmml"><msub id="A5.p2.8.m8.1.1.2" xref="A5.p2.8.m8.1.1.2.cmml"><mi id="A5.p2.8.m8.1.1.2.2" xref="A5.p2.8.m8.1.1.2.2.cmml">C</mi><mi id="A5.p2.8.m8.1.1.2.3" xref="A5.p2.8.m8.1.1.2.3.cmml">T</mi></msub><mo id="A5.p2.8.m8.1.1.1" xref="A5.p2.8.m8.1.1.1.cmml">=</mo><mrow id="A5.p2.8.m8.1.1.3" xref="A5.p2.8.m8.1.1.3.cmml"><mn id="A5.p2.8.m8.1.1.3.2" xref="A5.p2.8.m8.1.1.3.2.cmml">1</mn><mo id="A5.p2.8.m8.1.1.3.1" xref="A5.p2.8.m8.1.1.3.1.cmml">âˆ’</mo><msub id="A5.p2.8.m8.1.1.3.3" xref="A5.p2.8.m8.1.1.3.3.cmml"><mi id="A5.p2.8.m8.1.1.3.3.2" xref="A5.p2.8.m8.1.1.3.3.2.cmml">Acc</mi><mi id="A5.p2.8.m8.1.1.3.3.3" xref="A5.p2.8.m8.1.1.3.3.3.cmml">f</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="A5.p2.8.m8.1b"><apply id="A5.p2.8.m8.1.1.cmml" xref="A5.p2.8.m8.1.1"><eq id="A5.p2.8.m8.1.1.1.cmml" xref="A5.p2.8.m8.1.1.1"></eq><apply id="A5.p2.8.m8.1.1.2.cmml" xref="A5.p2.8.m8.1.1.2"><csymbol cd="ambiguous" id="A5.p2.8.m8.1.1.2.1.cmml" xref="A5.p2.8.m8.1.1.2">subscript</csymbol><ci id="A5.p2.8.m8.1.1.2.2.cmml" xref="A5.p2.8.m8.1.1.2.2">ğ¶</ci><ci id="A5.p2.8.m8.1.1.2.3.cmml" xref="A5.p2.8.m8.1.1.2.3">ğ‘‡</ci></apply><apply id="A5.p2.8.m8.1.1.3.cmml" xref="A5.p2.8.m8.1.1.3"><minus id="A5.p2.8.m8.1.1.3.1.cmml" xref="A5.p2.8.m8.1.1.3.1"></minus><cn id="A5.p2.8.m8.1.1.3.2.cmml" type="integer" xref="A5.p2.8.m8.1.1.3.2">1</cn><apply id="A5.p2.8.m8.1.1.3.3.cmml" xref="A5.p2.8.m8.1.1.3.3"><csymbol cd="ambiguous" id="A5.p2.8.m8.1.1.3.3.1.cmml" xref="A5.p2.8.m8.1.1.3.3">subscript</csymbol><ci id="A5.p2.8.m8.1.1.3.3.2.cmml" xref="A5.p2.8.m8.1.1.3.3.2">Acc</ci><ci id="A5.p2.8.m8.1.1.3.3.3.cmml" xref="A5.p2.8.m8.1.1.3.3.3">ğ‘“</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p2.8.m8.1c">C_{T}=1-\mathrm{Acc}_{f}</annotation><annotation encoding="application/x-llamapun" id="A5.p2.8.m8.1d">italic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT = 1 - roman_Acc start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT</annotation></semantics></math>
The complexity <math alttext="C_{f}" class="ltx_Math" display="inline" id="A5.p2.9.m9.1"><semantics id="A5.p2.9.m9.1a"><msub id="A5.p2.9.m9.1.1" xref="A5.p2.9.m9.1.1.cmml"><mi id="A5.p2.9.m9.1.1.2" xref="A5.p2.9.m9.1.1.2.cmml">C</mi><mi id="A5.p2.9.m9.1.1.3" xref="A5.p2.9.m9.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="A5.p2.9.m9.1b"><apply id="A5.p2.9.m9.1.1.cmml" xref="A5.p2.9.m9.1.1"><csymbol cd="ambiguous" id="A5.p2.9.m9.1.1.1.cmml" xref="A5.p2.9.m9.1.1">subscript</csymbol><ci id="A5.p2.9.m9.1.1.2.cmml" xref="A5.p2.9.m9.1.1.2">ğ¶</ci><ci id="A5.p2.9.m9.1.1.3.cmml" xref="A5.p2.9.m9.1.1.3">ğ‘“</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p2.9.m9.1c">C_{f}</annotation><annotation encoding="application/x-llamapun" id="A5.p2.9.m9.1d">italic_C start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT</annotation></semantics></math> ranges from 0 (no complexity, as the task is perfectly solved) to 1 (maximum complexity, as the task is not solved at all).</p>
</div>
<div class="ltx_para" id="A5.p3">
<p class="ltx_p" id="A5.p3.1">This definition implies that the higher the LLMâ€™s accuracy on a task, the lower the complexity of the task. This measure allows us to quantify task complexity based on the performance capabilities of state-of-the-art language models.</p>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Oct  7 02:09:27 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
