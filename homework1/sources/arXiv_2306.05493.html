<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2306.05493] Multi-Modal Classifiers for Open-Vocabulary Object Detection</title><meta property="og:description" content="The goal of this paper is open-vocabulary object detection (OVOD)
— building a model that can detect objects beyond the set of categories seen at training,
thus enabling the user to specify categories of interest at in…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Multi-Modal Classifiers for Open-Vocabulary Object Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Multi-Modal Classifiers for Open-Vocabulary Object Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2306.05493">

<!--Generated on Thu Feb 29 00:59:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">Multi-Modal Classifiers for Open-Vocabulary Object Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Prannay Kaul
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Weidi Xie
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Andrew Zisserman
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">The goal of this paper is open-vocabulary object detection (OVOD)
— building a model that can detect objects beyond the set of categories seen at training,
thus enabling the user to specify categories of interest at inference without the need for model retraining. We adopt a standard two-stage object detector architecture,
and explore three ways for specifying novel categories:
via language descriptions, via image exemplars, or via a combination of the two.
We make three contributions:
<span id="id1.id1.1" class="ltx_text ltx_font_italic">first</span>, we prompt a large language model (LLM) to generate informative language descriptions for object classes,
and construct powerful text-based classifiers;
<span id="id1.id1.2" class="ltx_text ltx_font_italic">second</span>, we employ a visual aggregator on image exemplars that can ingest any number of images as input,
forming vision-based classifiers; and
<span id="id1.id1.3" class="ltx_text ltx_font_italic">third</span>, we provide a simple method to fuse information from language descriptions and image exemplars,
yielding a multi-modal classifier.
When evaluating on the challenging LVIS open-vocabulary benchmark we demonstrate that:
(i) our text-based classifiers outperform all previous OVOD works;
(ii) our vision-based classifiers perform as well as text-based classifiers in prior work; (iii) using multi-modal classifiers perform better than either modality alone; and finally, (iv) our text-based and multi-modal classifiers yield better performance than a fully-supervised detector.</p>
</div>
<div id="p2" class="ltx_para ltx_align_center">
<p id="p2.1" class="ltx_p"><a target="_blank" href="https://www.robots.ox.ac.uk/vgg/research/mm-ovod/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.robots.ox.ac.uk/vgg/research/mm-ovod/</a></p>
</div>
<div id="p3" class="ltx_para ltx_align_center">
<br class="ltx_break">
</div>
<section id="S1" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In this paper, we consider the problem of open-vocabulary object detection (OVOD),
which aims to localise and classify visual objects beyond the categories seen at training time.
One may consider its usefulness from the perspective of online inference,
when users want to freely specify categories of interest at inference time without
the need or ability to re-train models.
To specify categories of interest, three obvious ways exist, namely:
(1) text-based, <span id="S1.p1.1.1" class="ltx_ERROR undefined">\eg</span> name the category or describe it in text form;
(2) vision-based, <span id="S1.p1.1.2" class="ltx_ERROR undefined">\eg</span> give image examples;
(3) multi-modal, <span id="S1.p1.1.3" class="ltx_ERROR undefined">\eg</span> indicate the category jointly with text and image.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2306.05493/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_img_square" width="461" height="417" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
Overview of the architecture for generating
text-based, vision-based, or multi-modal classifiers for OVOD.
Vision (top left): A frozen visual encoder ingests image exemplars of <span id="S1.F1.15.1" class="ltx_text ltx_font_bold">falcon</span>
producing an embedding per exemplar.
A trained aggregator takes these embeddings as input and produces a
<em id="S1.F1.16.2" class="ltx_emph ltx_font_italic">vision-based classifier</em>.
Text (top right): A text completion LLM is prompted to give descriptions of
a <span id="S1.F1.17.3" class="ltx_text ltx_font_bold">falcon</span> which are then encoded by a text encoder and averaged yielding
a <em id="S1.F1.18.4" class="ltx_emph ltx_font_italic">text-based classifier</em>.
Multi-Modal (middle): <em id="S1.F1.19.5" class="ltx_emph ltx_font_italic">Multi-Modal classifiers</em> are generated by adding the
vision-based and text-based classifiers together.
OVOD (bottom): The multi-modal classifier is used to detect the <span id="S1.F1.20.6" class="ltx_text ltx_font_bold">falcon</span> in a
standard model.
Note, all three types of classifier: vision-based, text-based and multi-modal,
can be used on the detector head for OVOD.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Existing works <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Bansal et al.</span>, <a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>; <span class="ltx_text" style="font-size:90%;">Gu et al.</span>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Zareian et al.</span>, <a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>; <span class="ltx_text" style="font-size:90%;">Zhou et al.</span>, <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Feng et al.</span>, <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite> have
explored replacing the learnt classifiers in a traditional detector with text embeddings,
that are computed by passing the class name into a pre-trained text
encoder with manual prompts, such as “a photo of a dalmatian”,
however, this design can be sub-optimal from three aspects:
<span id="S1.p2.1.1" class="ltx_text ltx_font_italic">first</span>, the discriminative power of generated text embeddings relies
entirely on the internal representation of the pre-trained text encoder,
potentially leading to lexical ambiguities —
<span id="S1.p2.1.2" class="ltx_ERROR undefined">\eg</span> “nail” can either refer to “the hard surface on the tips of the fingers”
or “a small metal spike with a flat tip hammered into wood to form a joint”
— simply encoding the class name will not be able to distinguish the two concepts;
<span id="S1.p2.1.3" class="ltx_text ltx_font_italic">second</span>, the class name for objects of interest may be unknown to the user,
while exemplar images can be easily acquired
— <span id="S1.p2.1.4" class="ltx_ERROR undefined">\eg</span> “dugong” refers to a herbivorous marine mammal with an adorable,
plump appearance, a dolphin tail, round head and downward snout (an example is in Section <a href="#A9" title="Appendix I Dugong ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> of the Appendix);
<span id="S1.p2.1.5" class="ltx_text ltx_font_italic">third</span>, in cases where multi-modal information is preferable to specify the category of interest,
<span id="S1.p2.1.6" class="ltx_ERROR undefined">\eg</span> a species of butterfly with a distinctive wing pattern
— language descriptions can be unsuitably long to capture all the intricacies of a given category,
while an exemplar image can “tell a thousand words” and act as an effective complement to text.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To tackle these limitations,
we propose a multi-modal open-vocabulary object detector,
with the classifier of the detector for a particular category being constructed
via natural language descriptions, image exemplars or a combination of the two.
Specifically, we establish an automatic approach for sourcing visual descriptions of the object categories,
by prompting a large language model with questions, <span id="S1.p3.1.1" class="ltx_ERROR undefined">\eg</span> “What does a dalmatian look like?”,
yielding “A dalmatian is typically a large dog with a short coat of black spots on a white background”.
Such a description provides additional visual cues to enhance the discriminative power of the classifier
generated from a text encoder.
For cases where collecting suitable informative language descriptions may be difficult
or require unnecessarily long descriptions to establish the differences between classes,
<span id="S1.p3.1.2" class="ltx_ERROR undefined">\eg</span> the dog breeds “pug” and “bulldog” have similar descriptions,
we can generate classifiers from image exemplars — RGB images of the class of interest.
Finally, we suggest a simple method to fuse both language descriptions and image exemplars,
yielding multi-modal classifiers that perform better than either modality individually.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We explore the issue of how best to combine the set of language descriptions
and the set of image exemplars into a classifier,
by comparing the performance of aggregation methods on a standard detector architecture.
By evaluating on the challenging LVIS <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gupta et al.</span>, <a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite> open-vocabulary object detection benchmark we show that:
(i) our automated method for sourcing rich natural language descriptions yields
text-based classifiers superior to those of previous work that rely entirely on the class name;
(ii) vision-based classifiers can be effectively constructed by a visual aggregator,
enabling novel categories to be detected by specifying image exemplars;
(iii) natural language descriptions and image exemplars can be simply combined to produce multi-modal classifiers,
which perform better than either modality individually, and achieve superior results to existing approaches.</p>
</div>
</section>
<section id="S2" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Closed-Vocabulary Object Detection</span>
is one of the classical computer vision problems, making a full overview here
impossible.
Therefore, we outline some key milestones.
In general, modern object detection methods can be cast into two sets:
one-stage and two-(multi-)stage detectors.
<em id="S2.p1.1.2" class="ltx_emph ltx_font_italic">One-stage</em> detectors directly classify and regress bounding boxes
by either densely classifying a set of predefined anchor
boxes <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Redmon et al.</span>, <a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>; <span class="ltx_text" style="font-size:90%;">Redmon &amp; Farhadi</span>, <a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>; <span class="ltx_text" style="font-size:90%;">Liu et al.</span>, <a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>; <span class="ltx_text" style="font-size:90%;">Lin et al.</span>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2017</span></a>; <span class="ltx_text" style="font-size:90%;">Tan et al.</span>, <a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>,
each which may contain an object, or densely searching for geometric entities of objects
<span id="S2.p1.1.3" class="ltx_ERROR undefined">\eg</span> corners, centre points or boxes <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Law &amp; Deng</span>, <a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>; <span class="ltx_text" style="font-size:90%;">Zhou et al.</span>, <a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>; <span class="ltx_text" style="font-size:90%;">Tian et al.</span>, <a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>.
Conversely, most <em id="S2.p1.1.4" class="ltx_emph ltx_font_italic">two-stage</em> detectors first propose class-agnostic
bounding boxes that are pooled to fixed size region-of-interest (RoI)
features and classified by a sub-network in the second stage <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Girshick</span>, <a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2015</span></a>; <span class="ltx_text" style="font-size:90%;">Ren et al.</span>, <a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>; <span class="ltx_text" style="font-size:90%;">Li et al.</span>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>.
Two-stage detectors are extended to <em id="S2.p1.1.5" class="ltx_emph ltx_font_italic">multi-stage</em> detectors in which
the additional stages refine predictions made by the previous
stage <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Cai &amp; Vasconcelos</span>, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>; <span class="ltx_text" style="font-size:90%;">Chen et al.</span>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>; <span class="ltx_text" style="font-size:90%;">Zhou et al.</span>, <a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>.
A unique work in this area is that of <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Carion et al.</span>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite> which uses the
Transformer architecture <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Vaswani et al.</span>, <a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2017</span></a>)</cite> to treat object detection as a set
prediction problem.
<span id="S2.p1.1.6" class="ltx_text ltx_font_bold">Note that</span>, the classifiers in these object detectors are jointly learnt on a training set,
therefore only objects seen at training time can be detected during inference time,
thus termed closed-vocabulary object detection.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Open-Vocabulary Object Detection</span> goes beyond closed-vocabulary object
detection and enables users to expand/change the detector vocabulary at inference time,
without the need for model re-training.
Recently, OVOD has seen increased attention and progress primarily
driven by the emergence of large-scale vision-language models (VLMs) <span id="S2.p2.1.2" class="ltx_ERROR undefined">\eg</span> CLIP
and ALIGN <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Radford et al.</span>, <a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>; <span class="ltx_text" style="font-size:90%;">Jia et al.</span>, <a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, which jointly learn image and natural
language representations.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">ViLD <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gu et al.</span>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite> distills representations from VLMs.
First, groundtruth bounding boxes are used to crop an image and an
embedding for the box is sourced from a frozen VLM image encoder.
An object detection model is learnt by matching overlapping region-of-interest
(RoI) features with the embedding for the relevant box from
the VLM image encoder using a L<math id="S2.p3.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S2.p3.1.m1.1a"><mn id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><cn type="integer" id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">1</annotation></semantics></math> reconstruction loss.
RegionCLIP <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhong et al.</span>, <a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite> uses image-caption data to construct region-wise
pseudo-labels, followed by region-text contrastive pre-training before transferring to detection.
GLIP and MDETR <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Li et al.</span>, <a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Kamath et al.</span>, <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite> use captions to cast detection as
a phrase grounding task and use early fusion between the image and
text modalities, increasing complexity.
OVR-CNN <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zareian et al.</span>, <a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite> uses large image-caption data to pre-train a
detector to learn a semantic space and finetunes on smaller detection data.
OWL-ViT <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Minderer et al.</span>, <a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite> follows OVR-CNN but makes use
of large transformer models and even larger image-caption data.
OV-DETR <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zang et al.</span>, <a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite> modifies the DETR framework for closed-vocabulary object detection <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Carion et al.</span>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>
to make it suitable for the open-vocabulary setting.
We note that OV-DETR can condition object detection on image exemplars,
but only provides some qualitative examples,
whereas we quantitatively benchmark our method using vision-based classifiers.
Detic and PromptDet <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhou et al.</span>, <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Feng et al.</span>, <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite> improve open-vocabulary detection by
making use of image classification data to provide weak supervision on a large set of categories.
Our work uses Detic as a starting point in experiments,
and we investigate different methods for constructing the classifiers.</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Low-shot Object Detection.</span>
Despite garnering less attention in recent literature,
some low/few-shot object detection works make use of image-conditioned
object detection <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Kang et al.</span>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>; <span class="ltx_text" style="font-size:90%;">Hsieh et al.</span>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>; <span class="ltx_text" style="font-size:90%;">Osokin et al.</span>, <a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>; <span class="ltx_text" style="font-size:90%;">Chen et al.</span>, <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite> in which image
exemplars of novel categories are encoded at inference time and used to detect novel category instances.
These works focus on architectural advances usually leveraging attention
between the novel class image exemplars and the inference time
image <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Chen et al.</span>, <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>; <span class="ltx_text" style="font-size:90%;">Hsieh et al.</span>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">There are an increasing number of low/few-shot object detection works
based on finetuning detector parameters on limited groundtruth data
for novel categories <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Wang et al.</span>, <a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>; <span class="ltx_text" style="font-size:90%;">Sun et al.</span>, <a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>; <span class="ltx_text" style="font-size:90%;">Qiao et al.</span>, <a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>; <span class="ltx_text" style="font-size:90%;">Kaul et al.</span>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>.
These works finetune the detector on limited groundtruth novel instances
and so are not related to open-vocabulary object detection using vision-based
classifiers, where no novel instances are available for re-training/finetuning.</p>
</div>
<div id="S2.p6" class="ltx_para ltx_noindent">
<p id="S2.p6.1" class="ltx_p"><span id="S2.p6.1.1" class="ltx_text ltx_font_bold">Natural Language for Classification.</span>
Natural language is a rich source of semantic information for classification.
CLEVER <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Choudhury et al.</span>, <a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite> matches descriptions of images in simple text form
with descriptions from expert databases <span id="S2.p6.1.2" class="ltx_ERROR undefined">\eg</span> Wikipedia to perform fine-grained
classification.
ZSLPP <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Elhoseiny et al.</span>, <a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2017</span></a>)</cite> extracts visual information from
large-scale text to identify parts of objects and perform zero-shot
classification.
The work by <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Menon &amp; Vondrick</span>, <a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> uses class descriptions from GPT-3 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Brown et al.</span>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite> for classification,
analysing which parts of the description contribute to classification decisions.
CuPL <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Pratt et al.</span>, <a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite> uses a GPT-3 model to provide detailed
descriptions enabling improved zero-shot image classification.
The learnings from this work inform our use of natural language
descriptions sourced from LLMs.



</p>
</div>
<figure id="S2.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F3.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" style="width:225.5pt;"><img src="/html/2306.05493/assets/x2.png" id="S2.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="290" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
Generating powerful text-based classifiers.
A LLM (GPT-3) is used to generate multiple rich descriptions of the class of
interest.
These descriptions are then encoded with the CLIP <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Radford et al.</span>, <a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite> VLM text
encoder.
The descriptions are more informative than the simple phrases,
such as “(a photo of) a <span id="S2.F3.1.12.1" class="ltx_text ltx_font_bold">dog</span>” or “(a photo of) a <span id="S2.F3.1.13.2" class="ltx_text ltx_font_bold">cat</span>”,
used in previous work such as Detic and ViLD. Additional examples of class descriptions are given in the
Appendix (Section <a href="#A6" title="Appendix F Additional Example Class Descriptions ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">F</span></a>).</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F3.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" style="width:199.5pt;"><img src="/html/2306.05493/assets/x3.png" id="S2.F3.2.g1" class="ltx_graphics ltx_img_square" width="461" height="400" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
Generating an OVOD vision-based classifier from a set of image exemplars.
A stack of transformer blocks is used to combine embeddings of multiple
exemplars belonging to the same category.</figcaption>
</figure>
</div>
</div>
</figure>
</section>
<section id="S3" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we start by providing background on open-vocabulary object detection (OVOD),
then outline the proposed methods for constructing classifiers from
language descriptions of a category (text-based classifiers, Section <a href="#S3.SS2" title="3.2 Text-based Classifiers from Language Descriptions ‣ 3 Method ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>)
and image exemplars (vision-based classifiers, Section <a href="#S3.SS3" title="3.3 Vision-based Classifiers from Image Exemplars ‣ 3 Method ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>).
Our final method combines the classifiers found from language descriptions and image exemplars,
yielding multi-modal classifiers (Section <a href="#S3.SS4" title="3.4 Constructing Classifiers via Multi-Modal Fusion ‣ 3 Method ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>).</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Preliminaries</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.12" class="ltx_p"><span id="S3.SS1.p1.12.1" class="ltx_text ltx_font_bold">Problem Scenario. </span>
Given an image (<math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{I}\leavevmode\nobreak\ \in\leavevmode\nobreak\ ^{3\times H\times W}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">𝐈</mi><msup id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml"><mo lspace="0.778em" rspace="0.778em" id="S3.SS1.p1.1.m1.1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.1.2.cmml">∈</mo><mrow id="S3.SS1.p1.1.m1.1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.1.3.cmml"><mn id="S3.SS1.p1.1.m1.1.1.1.3.2" xref="S3.SS1.p1.1.m1.1.1.1.3.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.1.m1.1.1.1.3.1" xref="S3.SS1.p1.1.m1.1.1.1.3.1.cmml">×</mo><mi id="S3.SS1.p1.1.m1.1.1.1.3.3" xref="S3.SS1.p1.1.m1.1.1.1.3.3.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.1.m1.1.1.1.3.1a" xref="S3.SS1.p1.1.m1.1.1.1.3.1.cmml">×</mo><mi id="S3.SS1.p1.1.m1.1.1.1.3.4" xref="S3.SS1.p1.1.m1.1.1.1.3.4.cmml">W</mi></mrow></msup><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><apply id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1">superscript</csymbol><in id="S3.SS1.p1.1.m1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.2"></in><apply id="S3.SS1.p1.1.m1.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.3"><times id="S3.SS1.p1.1.m1.1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.3.1"></times><cn type="integer" id="S3.SS1.p1.1.m1.1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.3.2">3</cn><ci id="S3.SS1.p1.1.m1.1.1.1.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.3.3">𝐻</ci><ci id="S3.SS1.p1.1.m1.1.1.1.3.4.cmml" xref="S3.SS1.p1.1.m1.1.1.1.3.4">𝑊</ci></apply></apply><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝐈</ci><csymbol cd="latexml" id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\mathbf{I}\leavevmode\nobreak\ \in\leavevmode\nobreak\ ^{3\times H\times W}</annotation></semantics></math>) fed input to an
open-vocabulary object detector, two outputs are generally produced:
(1) classification, in which a class label, <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="c_{j}\in\mathcal{C}^{\textsc{test}}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><msub id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2.2" xref="S3.SS1.p1.2.m2.1.1.2.2.cmml">c</mi><mi id="S3.SS1.p1.2.m2.1.1.2.3" xref="S3.SS1.p1.2.m2.1.1.2.3.cmml">j</mi></msub><mo id="S3.SS1.p1.2.m2.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.2.m2.1.1.3.2" xref="S3.SS1.p1.2.m2.1.1.3.2.cmml">𝒞</mi><mtext class="ltx_font_smallcaps" id="S3.SS1.p1.2.m2.1.1.3.3" xref="S3.SS1.p1.2.m2.1.1.3.3a.cmml">test</mtext></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><in id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1"></in><apply id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.2.1.cmml" xref="S3.SS1.p1.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2.2">𝑐</ci><ci id="S3.SS1.p1.2.m2.1.1.2.3.cmml" xref="S3.SS1.p1.2.m2.1.1.2.3">𝑗</ci></apply><apply id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.3.1.cmml" xref="S3.SS1.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.3.2.cmml" xref="S3.SS1.p1.2.m2.1.1.3.2">𝒞</ci><ci id="S3.SS1.p1.2.m2.1.1.3.3a.cmml" xref="S3.SS1.p1.2.m2.1.1.3.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS1.p1.2.m2.1.1.3.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3.3">test</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">c_{j}\in\mathcal{C}^{\textsc{test}}</annotation></semantics></math>,
is assigned to the <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="j^{\text{th}}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><msup id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">j</mi><mtext id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3a.cmml">th</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">superscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">𝑗</ci><ci id="S3.SS1.p1.3.m3.1.1.3a.cmml" xref="S3.SS1.p1.3.m3.1.1.3"><mtext mathsize="70%" id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">th</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">j^{\text{th}}</annotation></semantics></math> predicted object in the image,
and <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{C}^{\textsc{test}}" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><msup id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">𝒞</mi><mtext class="ltx_font_smallcaps" id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3a.cmml">test</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">superscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">𝒞</ci><ci id="S3.SS1.p1.4.m4.1.1.3a.cmml" xref="S3.SS1.p1.4.m4.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3">test</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">\mathcal{C}^{\textsc{test}}</annotation></semantics></math> refers to the category vocabulary desired at inference time;
(2) localisation, with bounding box coordinates, <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="\mathbf{b}_{j}\in^{4}" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><mrow id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><msub id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2.2" xref="S3.SS1.p1.5.m5.1.1.2.2.cmml">𝐛</mi><mi id="S3.SS1.p1.5.m5.1.1.2.3" xref="S3.SS1.p1.5.m5.1.1.2.3.cmml">j</mi></msub><msup id="S3.SS1.p1.5.m5.1.1.1" xref="S3.SS1.p1.5.m5.1.1.1.cmml"><mo id="S3.SS1.p1.5.m5.1.1.1.2" xref="S3.SS1.p1.5.m5.1.1.1.2.cmml">∈</mo><mn id="S3.SS1.p1.5.m5.1.1.1.3" xref="S3.SS1.p1.5.m5.1.1.1.3.cmml">4</mn></msup><mi id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><apply id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1">superscript</csymbol><in id="S3.SS1.p1.5.m5.1.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.1.2"></in><cn type="integer" id="S3.SS1.p1.5.m5.1.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.1.3">4</cn></apply><apply id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.2.1.cmml" xref="S3.SS1.p1.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.2.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2.2">𝐛</ci><ci id="S3.SS1.p1.5.m5.1.1.2.3.cmml" xref="S3.SS1.p1.5.m5.1.1.2.3">𝑗</ci></apply><csymbol cd="latexml" id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">\mathbf{b}_{j}\in^{4}</annotation></semantics></math>,
denoting the location of the <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="j^{\text{th}}" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><msup id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml"><mi id="S3.SS1.p1.6.m6.1.1.2" xref="S3.SS1.p1.6.m6.1.1.2.cmml">j</mi><mtext id="S3.SS1.p1.6.m6.1.1.3" xref="S3.SS1.p1.6.m6.1.1.3a.cmml">th</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">superscript</csymbol><ci id="S3.SS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2">𝑗</ci><ci id="S3.SS1.p1.6.m6.1.1.3a.cmml" xref="S3.SS1.p1.6.m6.1.1.3"><mtext mathsize="70%" id="S3.SS1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3">th</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">j^{\text{th}}</annotation></semantics></math> predicted object.
In accordance with the setting introduced by Detic <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhou et al.</span>, <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>,
two datasets are used at training time: a detection dataset, <math id="S3.SS1.p1.7.m7.1" class="ltx_Math" alttext="\mathcal{D}^{\textsc{det}}" display="inline"><semantics id="S3.SS1.p1.7.m7.1a"><msup id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.7.m7.1.1.2" xref="S3.SS1.p1.7.m7.1.1.2.cmml">𝒟</mi><mtext class="ltx_font_smallcaps" id="S3.SS1.p1.7.m7.1.1.3" xref="S3.SS1.p1.7.m7.1.1.3a.cmml">det</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><apply id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">superscript</csymbol><ci id="S3.SS1.p1.7.m7.1.1.2.cmml" xref="S3.SS1.p1.7.m7.1.1.2">𝒟</ci><ci id="S3.SS1.p1.7.m7.1.1.3a.cmml" xref="S3.SS1.p1.7.m7.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS1.p1.7.m7.1.1.3.cmml" xref="S3.SS1.p1.7.m7.1.1.3">det</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">\mathcal{D}^{\textsc{det}}</annotation></semantics></math>,
containing bounding box coordinates, class labels and associated images,
addressing a category vocabulary, <math id="S3.SS1.p1.8.m8.1" class="ltx_Math" alttext="\mathcal{C}^{\textsc{det}}" display="inline"><semantics id="S3.SS1.p1.8.m8.1a"><msup id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.8.m8.1.1.2" xref="S3.SS1.p1.8.m8.1.1.2.cmml">𝒞</mi><mtext class="ltx_font_smallcaps" id="S3.SS1.p1.8.m8.1.1.3" xref="S3.SS1.p1.8.m8.1.1.3a.cmml">det</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><apply id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m8.1.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">superscript</csymbol><ci id="S3.SS1.p1.8.m8.1.1.2.cmml" xref="S3.SS1.p1.8.m8.1.1.2">𝒞</ci><ci id="S3.SS1.p1.8.m8.1.1.3a.cmml" xref="S3.SS1.p1.8.m8.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS1.p1.8.m8.1.1.3.cmml" xref="S3.SS1.p1.8.m8.1.1.3">det</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">\mathcal{C}^{\textsc{det}}</annotation></semantics></math>;
and an image classification dataset, <math id="S3.SS1.p1.9.m9.1" class="ltx_Math" alttext="\mathcal{D}^{\textsc{img}}" display="inline"><semantics id="S3.SS1.p1.9.m9.1a"><msup id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.9.m9.1.1.2" xref="S3.SS1.p1.9.m9.1.1.2.cmml">𝒟</mi><mtext class="ltx_font_smallcaps" id="S3.SS1.p1.9.m9.1.1.3" xref="S3.SS1.p1.9.m9.1.1.3a.cmml">img</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.1b"><apply id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.9.m9.1.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1">superscript</csymbol><ci id="S3.SS1.p1.9.m9.1.1.2.cmml" xref="S3.SS1.p1.9.m9.1.1.2">𝒟</ci><ci id="S3.SS1.p1.9.m9.1.1.3a.cmml" xref="S3.SS1.p1.9.m9.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS1.p1.9.m9.1.1.3.cmml" xref="S3.SS1.p1.9.m9.1.1.3">img</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.1c">\mathcal{D}^{\textsc{img}}</annotation></semantics></math>,
containing images with class labels only,
addressing a category vocabulary, <math id="S3.SS1.p1.10.m10.1" class="ltx_Math" alttext="\mathcal{C}^{\textsc{img}}" display="inline"><semantics id="S3.SS1.p1.10.m10.1a"><msup id="S3.SS1.p1.10.m10.1.1" xref="S3.SS1.p1.10.m10.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.10.m10.1.1.2" xref="S3.SS1.p1.10.m10.1.1.2.cmml">𝒞</mi><mtext class="ltx_font_smallcaps" id="S3.SS1.p1.10.m10.1.1.3" xref="S3.SS1.p1.10.m10.1.1.3a.cmml">img</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m10.1b"><apply id="S3.SS1.p1.10.m10.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.1.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1">superscript</csymbol><ci id="S3.SS1.p1.10.m10.1.1.2.cmml" xref="S3.SS1.p1.10.m10.1.1.2">𝒞</ci><ci id="S3.SS1.p1.10.m10.1.1.3a.cmml" xref="S3.SS1.p1.10.m10.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS1.p1.10.m10.1.1.3.cmml" xref="S3.SS1.p1.10.m10.1.1.3">img</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m10.1c">\mathcal{C}^{\textsc{img}}</annotation></semantics></math>.
In the most general case there are no restrictions on the overlap or lack
thereof between the sets <math id="S3.SS1.p1.11.m11.2" class="ltx_Math" alttext="\mathcal{C}^{\textsc{test}},\mathcal{C}^{\textsc{det}}" display="inline"><semantics id="S3.SS1.p1.11.m11.2a"><mrow id="S3.SS1.p1.11.m11.2.2.2" xref="S3.SS1.p1.11.m11.2.2.3.cmml"><msup id="S3.SS1.p1.11.m11.1.1.1.1" xref="S3.SS1.p1.11.m11.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.11.m11.1.1.1.1.2" xref="S3.SS1.p1.11.m11.1.1.1.1.2.cmml">𝒞</mi><mtext class="ltx_font_smallcaps" id="S3.SS1.p1.11.m11.1.1.1.1.3" xref="S3.SS1.p1.11.m11.1.1.1.1.3a.cmml">test</mtext></msup><mo id="S3.SS1.p1.11.m11.2.2.2.3" xref="S3.SS1.p1.11.m11.2.2.3.cmml">,</mo><msup id="S3.SS1.p1.11.m11.2.2.2.2" xref="S3.SS1.p1.11.m11.2.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.11.m11.2.2.2.2.2" xref="S3.SS1.p1.11.m11.2.2.2.2.2.cmml">𝒞</mi><mtext class="ltx_font_smallcaps" id="S3.SS1.p1.11.m11.2.2.2.2.3" xref="S3.SS1.p1.11.m11.2.2.2.2.3a.cmml">det</mtext></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.11.m11.2b"><list id="S3.SS1.p1.11.m11.2.2.3.cmml" xref="S3.SS1.p1.11.m11.2.2.2"><apply id="S3.SS1.p1.11.m11.1.1.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.11.m11.1.1.1.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1.1.1">superscript</csymbol><ci id="S3.SS1.p1.11.m11.1.1.1.1.2.cmml" xref="S3.SS1.p1.11.m11.1.1.1.1.2">𝒞</ci><ci id="S3.SS1.p1.11.m11.1.1.1.1.3a.cmml" xref="S3.SS1.p1.11.m11.1.1.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS1.p1.11.m11.1.1.1.1.3.cmml" xref="S3.SS1.p1.11.m11.1.1.1.1.3">test</mtext></ci></apply><apply id="S3.SS1.p1.11.m11.2.2.2.2.cmml" xref="S3.SS1.p1.11.m11.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.11.m11.2.2.2.2.1.cmml" xref="S3.SS1.p1.11.m11.2.2.2.2">superscript</csymbol><ci id="S3.SS1.p1.11.m11.2.2.2.2.2.cmml" xref="S3.SS1.p1.11.m11.2.2.2.2.2">𝒞</ci><ci id="S3.SS1.p1.11.m11.2.2.2.2.3a.cmml" xref="S3.SS1.p1.11.m11.2.2.2.2.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS1.p1.11.m11.2.2.2.2.3.cmml" xref="S3.SS1.p1.11.m11.2.2.2.2.3">det</mtext></ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.11.m11.2c">\mathcal{C}^{\textsc{test}},\mathcal{C}^{\textsc{det}}</annotation></semantics></math> and <math id="S3.SS1.p1.12.m12.1" class="ltx_Math" alttext="\mathcal{C}^{\textsc{img}}" display="inline"><semantics id="S3.SS1.p1.12.m12.1a"><msup id="S3.SS1.p1.12.m12.1.1" xref="S3.SS1.p1.12.m12.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.12.m12.1.1.2" xref="S3.SS1.p1.12.m12.1.1.2.cmml">𝒞</mi><mtext class="ltx_font_smallcaps" id="S3.SS1.p1.12.m12.1.1.3" xref="S3.SS1.p1.12.m12.1.1.3a.cmml">img</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.12.m12.1b"><apply id="S3.SS1.p1.12.m12.1.1.cmml" xref="S3.SS1.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.12.m12.1.1.1.cmml" xref="S3.SS1.p1.12.m12.1.1">superscript</csymbol><ci id="S3.SS1.p1.12.m12.1.1.2.cmml" xref="S3.SS1.p1.12.m12.1.1.2">𝒞</ci><ci id="S3.SS1.p1.12.m12.1.1.3a.cmml" xref="S3.SS1.p1.12.m12.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS1.p1.12.m12.1.1.3.cmml" xref="S3.SS1.p1.12.m12.1.1.3">img</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.12.m12.1c">\mathcal{C}^{\textsc{img}}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Architecture Overview.</span>
In this work, we make use of a popular multi-stage detector based on
CenterNet2 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhou et al.</span>, <a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite> as done in Detic.
This detector, with outputs <math id="S3.SS1.p2.1.m1.2" class="ltx_Math" alttext="\left\{c_{j},\mathbf{b}_{j}\right\}^{M}_{j=1}" display="inline"><semantics id="S3.SS1.p2.1.m1.2a"><msubsup id="S3.SS1.p2.1.m1.2.2" xref="S3.SS1.p2.1.m1.2.2.cmml"><mrow id="S3.SS1.p2.1.m1.2.2.2.2.2" xref="S3.SS1.p2.1.m1.2.2.2.2.3.cmml"><mo id="S3.SS1.p2.1.m1.2.2.2.2.2.3" xref="S3.SS1.p2.1.m1.2.2.2.2.3.cmml">{</mo><msub id="S3.SS1.p2.1.m1.1.1.1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.1.1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.2.cmml">c</mi><mi id="S3.SS1.p2.1.m1.1.1.1.1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.SS1.p2.1.m1.2.2.2.2.2.4" xref="S3.SS1.p2.1.m1.2.2.2.2.3.cmml">,</mo><msub id="S3.SS1.p2.1.m1.2.2.2.2.2.2" xref="S3.SS1.p2.1.m1.2.2.2.2.2.2.cmml"><mi id="S3.SS1.p2.1.m1.2.2.2.2.2.2.2" xref="S3.SS1.p2.1.m1.2.2.2.2.2.2.2.cmml">𝐛</mi><mi id="S3.SS1.p2.1.m1.2.2.2.2.2.2.3" xref="S3.SS1.p2.1.m1.2.2.2.2.2.2.3.cmml">j</mi></msub><mo id="S3.SS1.p2.1.m1.2.2.2.2.2.5" xref="S3.SS1.p2.1.m1.2.2.2.2.3.cmml">}</mo></mrow><mrow id="S3.SS1.p2.1.m1.2.2.4" xref="S3.SS1.p2.1.m1.2.2.4.cmml"><mi id="S3.SS1.p2.1.m1.2.2.4.2" xref="S3.SS1.p2.1.m1.2.2.4.2.cmml">j</mi><mo id="S3.SS1.p2.1.m1.2.2.4.1" xref="S3.SS1.p2.1.m1.2.2.4.1.cmml">=</mo><mn id="S3.SS1.p2.1.m1.2.2.4.3" xref="S3.SS1.p2.1.m1.2.2.4.3.cmml">1</mn></mrow><mi id="S3.SS1.p2.1.m1.2.2.2.4" xref="S3.SS1.p2.1.m1.2.2.2.4.cmml">M</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.2b"><apply id="S3.SS1.p2.1.m1.2.2.cmml" xref="S3.SS1.p2.1.m1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.2.2.3.cmml" xref="S3.SS1.p2.1.m1.2.2">subscript</csymbol><apply id="S3.SS1.p2.1.m1.2.2.2.cmml" xref="S3.SS1.p2.1.m1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.2.2.2.3.cmml" xref="S3.SS1.p2.1.m1.2.2">superscript</csymbol><set id="S3.SS1.p2.1.m1.2.2.2.2.3.cmml" xref="S3.SS1.p2.1.m1.2.2.2.2.2"><apply id="S3.SS1.p2.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.2">𝑐</ci><ci id="S3.SS1.p2.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.3">𝑗</ci></apply><apply id="S3.SS1.p2.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS1.p2.1.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.2.2.2.2.2.2.1.cmml" xref="S3.SS1.p2.1.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p2.1.m1.2.2.2.2.2.2.2.cmml" xref="S3.SS1.p2.1.m1.2.2.2.2.2.2.2">𝐛</ci><ci id="S3.SS1.p2.1.m1.2.2.2.2.2.2.3.cmml" xref="S3.SS1.p2.1.m1.2.2.2.2.2.2.3">𝑗</ci></apply></set><ci id="S3.SS1.p2.1.m1.2.2.2.4.cmml" xref="S3.SS1.p2.1.m1.2.2.2.4">𝑀</ci></apply><apply id="S3.SS1.p2.1.m1.2.2.4.cmml" xref="S3.SS1.p2.1.m1.2.2.4"><eq id="S3.SS1.p2.1.m1.2.2.4.1.cmml" xref="S3.SS1.p2.1.m1.2.2.4.1"></eq><ci id="S3.SS1.p2.1.m1.2.2.4.2.cmml" xref="S3.SS1.p2.1.m1.2.2.4.2">𝑗</ci><cn type="integer" id="S3.SS1.p2.1.m1.2.2.4.3.cmml" xref="S3.SS1.p2.1.m1.2.2.4.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.2c">\left\{c_{j},\mathbf{b}_{j}\right\}^{M}_{j=1}</annotation></semantics></math>,
can be formulated as (for simplicity we consider the two-stage variant below):</p>
<table id="A9.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1.m1.2" class="ltx_Math" alttext="\displaystyle\left\{f_{j}\right\}^{M}_{j=1}=\Phi_{\textsc{roi}}\circ\Phi_{\textsc{pg}}\circ\Phi_{\textsc{enc}}\left(\mathbf{I}\right)" display="inline"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><msubsup id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.cmml"><mrow id="S3.E1.m1.2.2.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.2.cmml"><mo id="S3.E1.m1.2.2.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.2.cmml">{</mo><msub id="S3.E1.m1.2.2.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.2.cmml">f</mi><mi id="S3.E1.m1.2.2.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.E1.m1.2.2.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.E1.m1.2.2.1.3" xref="S3.E1.m1.2.2.1.3.cmml"><mi id="S3.E1.m1.2.2.1.3.2" xref="S3.E1.m1.2.2.1.3.2.cmml">j</mi><mo id="S3.E1.m1.2.2.1.3.1" xref="S3.E1.m1.2.2.1.3.1.cmml">=</mo><mn id="S3.E1.m1.2.2.1.3.3" xref="S3.E1.m1.2.2.1.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.2.2.1.1.3" xref="S3.E1.m1.2.2.1.1.3.cmml">M</mi></msubsup><mo id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml">=</mo><mrow id="S3.E1.m1.2.2.3" xref="S3.E1.m1.2.2.3.cmml"><mrow id="S3.E1.m1.2.2.3.2" xref="S3.E1.m1.2.2.3.2.cmml"><msub id="S3.E1.m1.2.2.3.2.2" xref="S3.E1.m1.2.2.3.2.2.cmml"><mi mathvariant="normal" id="S3.E1.m1.2.2.3.2.2.2" xref="S3.E1.m1.2.2.3.2.2.2.cmml">Φ</mi><mtext class="ltx_font_smallcaps" id="S3.E1.m1.2.2.3.2.2.3" xref="S3.E1.m1.2.2.3.2.2.3a.cmml">roi</mtext></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.2.2.3.2.1" xref="S3.E1.m1.2.2.3.2.1.cmml">∘</mo><msub id="S3.E1.m1.2.2.3.2.3" xref="S3.E1.m1.2.2.3.2.3.cmml"><mi mathvariant="normal" id="S3.E1.m1.2.2.3.2.3.2" xref="S3.E1.m1.2.2.3.2.3.2.cmml">Φ</mi><mtext class="ltx_font_smallcaps" id="S3.E1.m1.2.2.3.2.3.3" xref="S3.E1.m1.2.2.3.2.3.3a.cmml">pg</mtext></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.2.2.3.2.1a" xref="S3.E1.m1.2.2.3.2.1.cmml">∘</mo><msub id="S3.E1.m1.2.2.3.2.4" xref="S3.E1.m1.2.2.3.2.4.cmml"><mi mathvariant="normal" id="S3.E1.m1.2.2.3.2.4.2" xref="S3.E1.m1.2.2.3.2.4.2.cmml">Φ</mi><mtext class="ltx_font_smallcaps" id="S3.E1.m1.2.2.3.2.4.3" xref="S3.E1.m1.2.2.3.2.4.3a.cmml">enc</mtext></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.3.1" xref="S3.E1.m1.2.2.3.1.cmml">​</mo><mrow id="S3.E1.m1.2.2.3.3.2" xref="S3.E1.m1.2.2.3.cmml"><mo id="S3.E1.m1.2.2.3.3.2.1" xref="S3.E1.m1.2.2.3.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">𝐈</mi><mo id="S3.E1.m1.2.2.3.3.2.2" xref="S3.E1.m1.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2"><eq id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"></eq><apply id="S3.E1.m1.2.2.1.cmml" xref="S3.E1.m1.2.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.2.cmml" xref="S3.E1.m1.2.2.1">subscript</csymbol><apply id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1">superscript</csymbol><set id="S3.E1.m1.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1"><apply id="S3.E1.m1.2.2.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.2">𝑓</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.3">𝑗</ci></apply></set><ci id="S3.E1.m1.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.3">𝑀</ci></apply><apply id="S3.E1.m1.2.2.1.3.cmml" xref="S3.E1.m1.2.2.1.3"><eq id="S3.E1.m1.2.2.1.3.1.cmml" xref="S3.E1.m1.2.2.1.3.1"></eq><ci id="S3.E1.m1.2.2.1.3.2.cmml" xref="S3.E1.m1.2.2.1.3.2">𝑗</ci><cn type="integer" id="S3.E1.m1.2.2.1.3.3.cmml" xref="S3.E1.m1.2.2.1.3.3">1</cn></apply></apply><apply id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2.3"><times id="S3.E1.m1.2.2.3.1.cmml" xref="S3.E1.m1.2.2.3.1"></times><apply id="S3.E1.m1.2.2.3.2.cmml" xref="S3.E1.m1.2.2.3.2"><compose id="S3.E1.m1.2.2.3.2.1.cmml" xref="S3.E1.m1.2.2.3.2.1"></compose><apply id="S3.E1.m1.2.2.3.2.2.cmml" xref="S3.E1.m1.2.2.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.3.2.2.1.cmml" xref="S3.E1.m1.2.2.3.2.2">subscript</csymbol><ci id="S3.E1.m1.2.2.3.2.2.2.cmml" xref="S3.E1.m1.2.2.3.2.2.2">Φ</ci><ci id="S3.E1.m1.2.2.3.2.2.3a.cmml" xref="S3.E1.m1.2.2.3.2.2.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.E1.m1.2.2.3.2.2.3.cmml" xref="S3.E1.m1.2.2.3.2.2.3">roi</mtext></ci></apply><apply id="S3.E1.m1.2.2.3.2.3.cmml" xref="S3.E1.m1.2.2.3.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.3.2.3.1.cmml" xref="S3.E1.m1.2.2.3.2.3">subscript</csymbol><ci id="S3.E1.m1.2.2.3.2.3.2.cmml" xref="S3.E1.m1.2.2.3.2.3.2">Φ</ci><ci id="S3.E1.m1.2.2.3.2.3.3a.cmml" xref="S3.E1.m1.2.2.3.2.3.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.E1.m1.2.2.3.2.3.3.cmml" xref="S3.E1.m1.2.2.3.2.3.3">pg</mtext></ci></apply><apply id="S3.E1.m1.2.2.3.2.4.cmml" xref="S3.E1.m1.2.2.3.2.4"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.3.2.4.1.cmml" xref="S3.E1.m1.2.2.3.2.4">subscript</csymbol><ci id="S3.E1.m1.2.2.3.2.4.2.cmml" xref="S3.E1.m1.2.2.3.2.4.2">Φ</ci><ci id="S3.E1.m1.2.2.3.2.4.3a.cmml" xref="S3.E1.m1.2.2.3.2.4.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.E1.m1.2.2.3.2.4.3.cmml" xref="S3.E1.m1.2.2.3.2.4.3">enc</mtext></ci></apply></apply><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝐈</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\displaystyle\left\{f_{j}\right\}^{M}_{j=1}=\Phi_{\textsc{roi}}\circ\Phi_{\textsc{pg}}\circ\Phi_{\textsc{enc}}\left(\mathbf{I}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E2.m1.4" class="ltx_Math" alttext="\displaystyle\left\{\mathbf{b}_{j},c_{j}\right\}^{M}_{j=1}=\left\{\Phi_{\textsc{bbox}}\left(f_{j}\right),\Phi_{\textsc{cls}}\circ\Phi_{\textsc{proj}}\left(f_{j}\right)\right\}^{M}_{j=1}\vspace{-20pt}" display="inline"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml"><msubsup id="S3.E2.m1.2.2.2" xref="S3.E2.m1.2.2.2.cmml"><mrow id="S3.E2.m1.2.2.2.2.2.2" xref="S3.E2.m1.2.2.2.2.2.3.cmml"><mo id="S3.E2.m1.2.2.2.2.2.2.3" xref="S3.E2.m1.2.2.2.2.2.3.cmml">{</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.2.cmml">𝐛</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.E2.m1.2.2.2.2.2.2.4" xref="S3.E2.m1.2.2.2.2.2.3.cmml">,</mo><msub id="S3.E2.m1.2.2.2.2.2.2.2" xref="S3.E2.m1.2.2.2.2.2.2.2.cmml"><mi id="S3.E2.m1.2.2.2.2.2.2.2.2" xref="S3.E2.m1.2.2.2.2.2.2.2.2.cmml">c</mi><mi id="S3.E2.m1.2.2.2.2.2.2.2.3" xref="S3.E2.m1.2.2.2.2.2.2.2.3.cmml">j</mi></msub><mo id="S3.E2.m1.2.2.2.2.2.2.5" xref="S3.E2.m1.2.2.2.2.2.3.cmml">}</mo></mrow><mrow id="S3.E2.m1.2.2.2.4" xref="S3.E2.m1.2.2.2.4.cmml"><mi id="S3.E2.m1.2.2.2.4.2" xref="S3.E2.m1.2.2.2.4.2.cmml">j</mi><mo id="S3.E2.m1.2.2.2.4.1" xref="S3.E2.m1.2.2.2.4.1.cmml">=</mo><mn id="S3.E2.m1.2.2.2.4.3" xref="S3.E2.m1.2.2.2.4.3.cmml">1</mn></mrow><mi id="S3.E2.m1.2.2.2.2.4" xref="S3.E2.m1.2.2.2.2.4.cmml">M</mi></msubsup><mo id="S3.E2.m1.4.4.5" xref="S3.E2.m1.4.4.5.cmml">=</mo><msubsup id="S3.E2.m1.4.4.4" xref="S3.E2.m1.4.4.4.cmml"><mrow id="S3.E2.m1.4.4.4.2.2.2" xref="S3.E2.m1.4.4.4.2.2.3.cmml"><mo id="S3.E2.m1.4.4.4.2.2.2.3" xref="S3.E2.m1.4.4.4.2.2.3.cmml">{</mo><mrow id="S3.E2.m1.3.3.3.1.1.1.1" xref="S3.E2.m1.3.3.3.1.1.1.1.cmml"><msub id="S3.E2.m1.3.3.3.1.1.1.1.3" xref="S3.E2.m1.3.3.3.1.1.1.1.3.cmml"><mi mathvariant="normal" id="S3.E2.m1.3.3.3.1.1.1.1.3.2" xref="S3.E2.m1.3.3.3.1.1.1.1.3.2.cmml">Φ</mi><mtext class="ltx_font_smallcaps" id="S3.E2.m1.3.3.3.1.1.1.1.3.3" xref="S3.E2.m1.3.3.3.1.1.1.1.3.3a.cmml">bbox</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.3.1.1.1.1.2" xref="S3.E2.m1.3.3.3.1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.3.3.3.1.1.1.1.1.1" xref="S3.E2.m1.3.3.3.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.3.3.3.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.3.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E2.m1.3.3.3.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.3.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.3.3.3.1.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.3.1.1.1.1.1.1.1.2.cmml">f</mi><mi id="S3.E2.m1.3.3.3.1.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.3.1.1.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.E2.m1.3.3.3.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.4.4.2.2.2.4" xref="S3.E2.m1.4.4.4.2.2.3.cmml">,</mo><mrow id="S3.E2.m1.4.4.4.2.2.2.2" xref="S3.E2.m1.4.4.4.2.2.2.2.cmml"><mrow id="S3.E2.m1.4.4.4.2.2.2.2.3" xref="S3.E2.m1.4.4.4.2.2.2.2.3.cmml"><msub id="S3.E2.m1.4.4.4.2.2.2.2.3.2" xref="S3.E2.m1.4.4.4.2.2.2.2.3.2.cmml"><mi mathvariant="normal" id="S3.E2.m1.4.4.4.2.2.2.2.3.2.2" xref="S3.E2.m1.4.4.4.2.2.2.2.3.2.2.cmml">Φ</mi><mtext class="ltx_font_smallcaps" id="S3.E2.m1.4.4.4.2.2.2.2.3.2.3" xref="S3.E2.m1.4.4.4.2.2.2.2.3.2.3a.cmml">cls</mtext></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E2.m1.4.4.4.2.2.2.2.3.1" xref="S3.E2.m1.4.4.4.2.2.2.2.3.1.cmml">∘</mo><msub id="S3.E2.m1.4.4.4.2.2.2.2.3.3" xref="S3.E2.m1.4.4.4.2.2.2.2.3.3.cmml"><mi mathvariant="normal" id="S3.E2.m1.4.4.4.2.2.2.2.3.3.2" xref="S3.E2.m1.4.4.4.2.2.2.2.3.3.2.cmml">Φ</mi><mtext class="ltx_font_smallcaps" id="S3.E2.m1.4.4.4.2.2.2.2.3.3.3" xref="S3.E2.m1.4.4.4.2.2.2.2.3.3.3a.cmml">proj</mtext></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.4.2.2.2.2.2" xref="S3.E2.m1.4.4.4.2.2.2.2.2.cmml">​</mo><mrow id="S3.E2.m1.4.4.4.2.2.2.2.1.1" xref="S3.E2.m1.4.4.4.2.2.2.2.1.1.1.cmml"><mo id="S3.E2.m1.4.4.4.2.2.2.2.1.1.2" xref="S3.E2.m1.4.4.4.2.2.2.2.1.1.1.cmml">(</mo><msub id="S3.E2.m1.4.4.4.2.2.2.2.1.1.1" xref="S3.E2.m1.4.4.4.2.2.2.2.1.1.1.cmml"><mi id="S3.E2.m1.4.4.4.2.2.2.2.1.1.1.2" xref="S3.E2.m1.4.4.4.2.2.2.2.1.1.1.2.cmml">f</mi><mi id="S3.E2.m1.4.4.4.2.2.2.2.1.1.1.3" xref="S3.E2.m1.4.4.4.2.2.2.2.1.1.1.3.cmml">j</mi></msub><mo id="S3.E2.m1.4.4.4.2.2.2.2.1.1.3" xref="S3.E2.m1.4.4.4.2.2.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.4.4.2.2.2.5" xref="S3.E2.m1.4.4.4.2.2.3.cmml">}</mo></mrow><mrow id="S3.E2.m1.4.4.4.4" xref="S3.E2.m1.4.4.4.4.cmml"><mi id="S3.E2.m1.4.4.4.4.2" xref="S3.E2.m1.4.4.4.4.2.cmml">j</mi><mo id="S3.E2.m1.4.4.4.4.1" xref="S3.E2.m1.4.4.4.4.1.cmml">=</mo><mn id="S3.E2.m1.4.4.4.4.3" xref="S3.E2.m1.4.4.4.4.3.cmml">1</mn></mrow><mi id="S3.E2.m1.4.4.4.2.4" xref="S3.E2.m1.4.4.4.2.4.cmml">M</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4"><eq id="S3.E2.m1.4.4.5.cmml" xref="S3.E2.m1.4.4.5"></eq><apply id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2">subscript</csymbol><apply id="S3.E2.m1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2">superscript</csymbol><set id="S3.E2.m1.2.2.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.2.2.2"><apply id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2">𝐛</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3">𝑗</ci></apply><apply id="S3.E2.m1.2.2.2.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.2.2.2.1.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E2.m1.2.2.2.2.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2">𝑐</ci><ci id="S3.E2.m1.2.2.2.2.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.3">𝑗</ci></apply></set><ci id="S3.E2.m1.2.2.2.2.4.cmml" xref="S3.E2.m1.2.2.2.2.4">𝑀</ci></apply><apply id="S3.E2.m1.2.2.2.4.cmml" xref="S3.E2.m1.2.2.2.4"><eq id="S3.E2.m1.2.2.2.4.1.cmml" xref="S3.E2.m1.2.2.2.4.1"></eq><ci id="S3.E2.m1.2.2.2.4.2.cmml" xref="S3.E2.m1.2.2.2.4.2">𝑗</ci><cn type="integer" id="S3.E2.m1.2.2.2.4.3.cmml" xref="S3.E2.m1.2.2.2.4.3">1</cn></apply></apply><apply id="S3.E2.m1.4.4.4.cmml" xref="S3.E2.m1.4.4.4"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.4.3.cmml" xref="S3.E2.m1.4.4.4">subscript</csymbol><apply id="S3.E2.m1.4.4.4.2.cmml" xref="S3.E2.m1.4.4.4"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.4.2.3.cmml" xref="S3.E2.m1.4.4.4">superscript</csymbol><set id="S3.E2.m1.4.4.4.2.2.3.cmml" xref="S3.E2.m1.4.4.4.2.2.2"><apply id="S3.E2.m1.3.3.3.1.1.1.1.cmml" xref="S3.E2.m1.3.3.3.1.1.1.1"><times id="S3.E2.m1.3.3.3.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.3.1.1.1.1.2"></times><apply id="S3.E2.m1.3.3.3.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.3.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.3.1.1.1.1.3.1.cmml" xref="S3.E2.m1.3.3.3.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.3.3.3.1.1.1.1.3.2.cmml" xref="S3.E2.m1.3.3.3.1.1.1.1.3.2">Φ</ci><ci id="S3.E2.m1.3.3.3.1.1.1.1.3.3a.cmml" xref="S3.E2.m1.3.3.3.1.1.1.1.3.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.E2.m1.3.3.3.1.1.1.1.3.3.cmml" xref="S3.E2.m1.3.3.3.1.1.1.1.3.3">bbox</mtext></ci></apply><apply id="S3.E2.m1.3.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.3.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.3.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.3.1.1.1.1.1.1.1.2">𝑓</ci><ci id="S3.E2.m1.3.3.3.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.3.1.1.1.1.1.1.1.3">𝑗</ci></apply></apply><apply id="S3.E2.m1.4.4.4.2.2.2.2.cmml" xref="S3.E2.m1.4.4.4.2.2.2.2"><times id="S3.E2.m1.4.4.4.2.2.2.2.2.cmml" xref="S3.E2.m1.4.4.4.2.2.2.2.2"></times><apply id="S3.E2.m1.4.4.4.2.2.2.2.3.cmml" xref="S3.E2.m1.4.4.4.2.2.2.2.3"><compose id="S3.E2.m1.4.4.4.2.2.2.2.3.1.cmml" xref="S3.E2.m1.4.4.4.2.2.2.2.3.1"></compose><apply id="S3.E2.m1.4.4.4.2.2.2.2.3.2.cmml" xref="S3.E2.m1.4.4.4.2.2.2.2.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.4.2.2.2.2.3.2.1.cmml" xref="S3.E2.m1.4.4.4.2.2.2.2.3.2">subscript</csymbol><ci id="S3.E2.m1.4.4.4.2.2.2.2.3.2.2.cmml" xref="S3.E2.m1.4.4.4.2.2.2.2.3.2.2">Φ</ci><ci id="S3.E2.m1.4.4.4.2.2.2.2.3.2.3a.cmml" xref="S3.E2.m1.4.4.4.2.2.2.2.3.2.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.E2.m1.4.4.4.2.2.2.2.3.2.3.cmml" xref="S3.E2.m1.4.4.4.2.2.2.2.3.2.3">cls</mtext></ci></apply><apply id="S3.E2.m1.4.4.4.2.2.2.2.3.3.cmml" xref="S3.E2.m1.4.4.4.2.2.2.2.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.4.2.2.2.2.3.3.1.cmml" xref="S3.E2.m1.4.4.4.2.2.2.2.3.3">subscript</csymbol><ci id="S3.E2.m1.4.4.4.2.2.2.2.3.3.2.cmml" xref="S3.E2.m1.4.4.4.2.2.2.2.3.3.2">Φ</ci><ci id="S3.E2.m1.4.4.4.2.2.2.2.3.3.3a.cmml" xref="S3.E2.m1.4.4.4.2.2.2.2.3.3.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.E2.m1.4.4.4.2.2.2.2.3.3.3.cmml" xref="S3.E2.m1.4.4.4.2.2.2.2.3.3.3">proj</mtext></ci></apply></apply><apply id="S3.E2.m1.4.4.4.2.2.2.2.1.1.1.cmml" xref="S3.E2.m1.4.4.4.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.4.2.2.2.2.1.1.1.1.cmml" xref="S3.E2.m1.4.4.4.2.2.2.2.1.1">subscript</csymbol><ci id="S3.E2.m1.4.4.4.2.2.2.2.1.1.1.2.cmml" xref="S3.E2.m1.4.4.4.2.2.2.2.1.1.1.2">𝑓</ci><ci id="S3.E2.m1.4.4.4.2.2.2.2.1.1.1.3.cmml" xref="S3.E2.m1.4.4.4.2.2.2.2.1.1.1.3">𝑗</ci></apply></apply></set><ci id="S3.E2.m1.4.4.4.2.4.cmml" xref="S3.E2.m1.4.4.4.2.4">𝑀</ci></apply><apply id="S3.E2.m1.4.4.4.4.cmml" xref="S3.E2.m1.4.4.4.4"><eq id="S3.E2.m1.4.4.4.4.1.cmml" xref="S3.E2.m1.4.4.4.4.1"></eq><ci id="S3.E2.m1.4.4.4.4.2.cmml" xref="S3.E2.m1.4.4.4.4.2">𝑗</ci><cn type="integer" id="S3.E2.m1.4.4.4.4.3.cmml" xref="S3.E2.m1.4.4.4.4.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">\displaystyle\left\{\mathbf{b}_{j},c_{j}\right\}^{M}_{j=1}=\left\{\Phi_{\textsc{bbox}}\left(f_{j}\right),\Phi_{\textsc{cls}}\circ\Phi_{\textsc{proj}}\left(f_{j}\right)\right\}^{M}_{j=1}\vspace{-20pt}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.11" class="ltx_p">where, each input image is first sequentially processed by a set of operations:
an image encoder <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="\left(\Phi_{\textsc{enc}}\right)" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mrow id="S3.SS1.p3.1.m1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.1.cmml"><mo id="S3.SS1.p3.1.m1.1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.1.1.cmml">(</mo><msub id="S3.SS1.p3.1.m1.1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p3.1.m1.1.1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.1.1.2.cmml">Φ</mi><mtext class="ltx_font_smallcaps" id="S3.SS1.p3.1.m1.1.1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.1.1.3a.cmml">enc</mtext></msub><mo id="S3.SS1.p3.1.m1.1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.2">Φ</ci><ci id="S3.SS1.p3.1.m1.1.1.1.1.3a.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS1.p3.1.m1.1.1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.3">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">\left(\Phi_{\textsc{enc}}\right)</annotation></semantics></math>;
a proposal generator <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="\left(\Phi_{\textsc{pg}}\right)" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mrow id="S3.SS1.p3.2.m2.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.1.cmml"><mo id="S3.SS1.p3.2.m2.1.1.1.2" xref="S3.SS1.p3.2.m2.1.1.1.1.cmml">(</mo><msub id="S3.SS1.p3.2.m2.1.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p3.2.m2.1.1.1.1.2" xref="S3.SS1.p3.2.m2.1.1.1.1.2.cmml">Φ</mi><mtext class="ltx_font_smallcaps" id="S3.SS1.p3.2.m2.1.1.1.1.3" xref="S3.SS1.p3.2.m2.1.1.1.1.3a.cmml">pg</mtext></msub><mo id="S3.SS1.p3.2.m2.1.1.1.3" xref="S3.SS1.p3.2.m2.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1">subscript</csymbol><ci id="S3.SS1.p3.2.m2.1.1.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.2">Φ</ci><ci id="S3.SS1.p3.2.m2.1.1.1.1.3a.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS1.p3.2.m2.1.1.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.3">pg</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">\left(\Phi_{\textsc{pg}}\right)</annotation></semantics></math>;
a region-of-interest (RoI) feature pooling module <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="\left(\Phi_{\textsc{roi}}\right)" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><mrow id="S3.SS1.p3.3.m3.1.1.1" xref="S3.SS1.p3.3.m3.1.1.1.1.cmml"><mo id="S3.SS1.p3.3.m3.1.1.1.2" xref="S3.SS1.p3.3.m3.1.1.1.1.cmml">(</mo><msub id="S3.SS1.p3.3.m3.1.1.1.1" xref="S3.SS1.p3.3.m3.1.1.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p3.3.m3.1.1.1.1.2" xref="S3.SS1.p3.3.m3.1.1.1.1.2.cmml">Φ</mi><mtext class="ltx_font_smallcaps" id="S3.SS1.p3.3.m3.1.1.1.1.3" xref="S3.SS1.p3.3.m3.1.1.1.1.3a.cmml">roi</mtext></msub><mo id="S3.SS1.p3.3.m3.1.1.1.3" xref="S3.SS1.p3.3.m3.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><apply id="S3.SS1.p3.3.m3.1.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.1.1.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1.1">subscript</csymbol><ci id="S3.SS1.p3.3.m3.1.1.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.1.1.2">Φ</ci><ci id="S3.SS1.p3.3.m3.1.1.1.1.3a.cmml" xref="S3.SS1.p3.3.m3.1.1.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS1.p3.3.m3.1.1.1.1.3.cmml" xref="S3.SS1.p3.3.m3.1.1.1.1.3">roi</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">\left(\Phi_{\textsc{roi}}\right)</annotation></semantics></math>,
yielding a set of RoI features, <math id="S3.SS1.p3.4.m4.1" class="ltx_Math" alttext="\left\{f_{j}\right\}^{M}_{j=1}" display="inline"><semantics id="S3.SS1.p3.4.m4.1a"><msubsup id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml"><mrow id="S3.SS1.p3.4.m4.1.1.1.1.1" xref="S3.SS1.p3.4.m4.1.1.1.1.2.cmml"><mo id="S3.SS1.p3.4.m4.1.1.1.1.1.2" xref="S3.SS1.p3.4.m4.1.1.1.1.2.cmml">{</mo><msub id="S3.SS1.p3.4.m4.1.1.1.1.1.1" xref="S3.SS1.p3.4.m4.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.4.m4.1.1.1.1.1.1.2" xref="S3.SS1.p3.4.m4.1.1.1.1.1.1.2.cmml">f</mi><mi id="S3.SS1.p3.4.m4.1.1.1.1.1.1.3" xref="S3.SS1.p3.4.m4.1.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.SS1.p3.4.m4.1.1.1.1.1.3" xref="S3.SS1.p3.4.m4.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS1.p3.4.m4.1.1.3" xref="S3.SS1.p3.4.m4.1.1.3.cmml"><mi id="S3.SS1.p3.4.m4.1.1.3.2" xref="S3.SS1.p3.4.m4.1.1.3.2.cmml">j</mi><mo id="S3.SS1.p3.4.m4.1.1.3.1" xref="S3.SS1.p3.4.m4.1.1.3.1.cmml">=</mo><mn id="S3.SS1.p3.4.m4.1.1.3.3" xref="S3.SS1.p3.4.m4.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS1.p3.4.m4.1.1.1.3" xref="S3.SS1.p3.4.m4.1.1.1.3.cmml">M</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><apply id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.1.2.cmml" xref="S3.SS1.p3.4.m4.1.1">subscript</csymbol><apply id="S3.SS1.p3.4.m4.1.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.1.1.2.cmml" xref="S3.SS1.p3.4.m4.1.1">superscript</csymbol><set id="S3.SS1.p3.4.m4.1.1.1.1.2.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1"><apply id="S3.SS1.p3.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p3.4.m4.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.1.2">𝑓</ci><ci id="S3.SS1.p3.4.m4.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.1.3">𝑗</ci></apply></set><ci id="S3.SS1.p3.4.m4.1.1.1.3.cmml" xref="S3.SS1.p3.4.m4.1.1.1.3">𝑀</ci></apply><apply id="S3.SS1.p3.4.m4.1.1.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3"><eq id="S3.SS1.p3.4.m4.1.1.3.1.cmml" xref="S3.SS1.p3.4.m4.1.1.3.1"></eq><ci id="S3.SS1.p3.4.m4.1.1.3.2.cmml" xref="S3.SS1.p3.4.m4.1.1.3.2">𝑗</ci><cn type="integer" id="S3.SS1.p3.4.m4.1.1.3.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">\left\{f_{j}\right\}^{M}_{j=1}</annotation></semantics></math>.
The RoI features are processed by a bounding box module <math id="S3.SS1.p3.5.m5.1" class="ltx_Math" alttext="\left(\Phi_{\textsc{bbox}}\right)" display="inline"><semantics id="S3.SS1.p3.5.m5.1a"><mrow id="S3.SS1.p3.5.m5.1.1.1" xref="S3.SS1.p3.5.m5.1.1.1.1.cmml"><mo id="S3.SS1.p3.5.m5.1.1.1.2" xref="S3.SS1.p3.5.m5.1.1.1.1.cmml">(</mo><msub id="S3.SS1.p3.5.m5.1.1.1.1" xref="S3.SS1.p3.5.m5.1.1.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p3.5.m5.1.1.1.1.2" xref="S3.SS1.p3.5.m5.1.1.1.1.2.cmml">Φ</mi><mtext class="ltx_font_smallcaps" id="S3.SS1.p3.5.m5.1.1.1.1.3" xref="S3.SS1.p3.5.m5.1.1.1.1.3a.cmml">bbox</mtext></msub><mo id="S3.SS1.p3.5.m5.1.1.1.3" xref="S3.SS1.p3.5.m5.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b"><apply id="S3.SS1.p3.5.m5.1.1.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m5.1.1.1.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1.1">subscript</csymbol><ci id="S3.SS1.p3.5.m5.1.1.1.1.2.cmml" xref="S3.SS1.p3.5.m5.1.1.1.1.2">Φ</ci><ci id="S3.SS1.p3.5.m5.1.1.1.1.3a.cmml" xref="S3.SS1.p3.5.m5.1.1.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS1.p3.5.m5.1.1.1.1.3.cmml" xref="S3.SS1.p3.5.m5.1.1.1.1.3">bbox</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">\left(\Phi_{\textsc{bbox}}\right)</annotation></semantics></math> to infer position of objects,
<math id="S3.SS1.p3.6.m6.1" class="ltx_Math" alttext="\left\{\mathbf{b}_{j}\right\}^{M}_{j=1}" display="inline"><semantics id="S3.SS1.p3.6.m6.1a"><msubsup id="S3.SS1.p3.6.m6.1.1" xref="S3.SS1.p3.6.m6.1.1.cmml"><mrow id="S3.SS1.p3.6.m6.1.1.1.1.1" xref="S3.SS1.p3.6.m6.1.1.1.1.2.cmml"><mo id="S3.SS1.p3.6.m6.1.1.1.1.1.2" xref="S3.SS1.p3.6.m6.1.1.1.1.2.cmml">{</mo><msub id="S3.SS1.p3.6.m6.1.1.1.1.1.1" xref="S3.SS1.p3.6.m6.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.6.m6.1.1.1.1.1.1.2" xref="S3.SS1.p3.6.m6.1.1.1.1.1.1.2.cmml">𝐛</mi><mi id="S3.SS1.p3.6.m6.1.1.1.1.1.1.3" xref="S3.SS1.p3.6.m6.1.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.SS1.p3.6.m6.1.1.1.1.1.3" xref="S3.SS1.p3.6.m6.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS1.p3.6.m6.1.1.3" xref="S3.SS1.p3.6.m6.1.1.3.cmml"><mi id="S3.SS1.p3.6.m6.1.1.3.2" xref="S3.SS1.p3.6.m6.1.1.3.2.cmml">j</mi><mo id="S3.SS1.p3.6.m6.1.1.3.1" xref="S3.SS1.p3.6.m6.1.1.3.1.cmml">=</mo><mn id="S3.SS1.p3.6.m6.1.1.3.3" xref="S3.SS1.p3.6.m6.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS1.p3.6.m6.1.1.1.3" xref="S3.SS1.p3.6.m6.1.1.1.3.cmml">M</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.m6.1b"><apply id="S3.SS1.p3.6.m6.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.6.m6.1.1.2.cmml" xref="S3.SS1.p3.6.m6.1.1">subscript</csymbol><apply id="S3.SS1.p3.6.m6.1.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.6.m6.1.1.1.2.cmml" xref="S3.SS1.p3.6.m6.1.1">superscript</csymbol><set id="S3.SS1.p3.6.m6.1.1.1.1.2.cmml" xref="S3.SS1.p3.6.m6.1.1.1.1.1"><apply id="S3.SS1.p3.6.m6.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.6.m6.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p3.6.m6.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.6.m6.1.1.1.1.1.1.2">𝐛</ci><ci id="S3.SS1.p3.6.m6.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p3.6.m6.1.1.1.1.1.1.3">𝑗</ci></apply></set><ci id="S3.SS1.p3.6.m6.1.1.1.3.cmml" xref="S3.SS1.p3.6.m6.1.1.1.3">𝑀</ci></apply><apply id="S3.SS1.p3.6.m6.1.1.3.cmml" xref="S3.SS1.p3.6.m6.1.1.3"><eq id="S3.SS1.p3.6.m6.1.1.3.1.cmml" xref="S3.SS1.p3.6.m6.1.1.3.1"></eq><ci id="S3.SS1.p3.6.m6.1.1.3.2.cmml" xref="S3.SS1.p3.6.m6.1.1.3.2">𝑗</ci><cn type="integer" id="S3.SS1.p3.6.m6.1.1.3.3.cmml" xref="S3.SS1.p3.6.m6.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.6.m6.1c">\left\{\mathbf{b}_{j}\right\}^{M}_{j=1}</annotation></semantics></math>.
Additionally, the RoI features are processed by a classification module,
consisting of a linear projection <math id="S3.SS1.p3.7.m7.1" class="ltx_Math" alttext="\left(\Phi_{\textsc{proj}}\right)" display="inline"><semantics id="S3.SS1.p3.7.m7.1a"><mrow id="S3.SS1.p3.7.m7.1.1.1" xref="S3.SS1.p3.7.m7.1.1.1.1.cmml"><mo id="S3.SS1.p3.7.m7.1.1.1.2" xref="S3.SS1.p3.7.m7.1.1.1.1.cmml">(</mo><msub id="S3.SS1.p3.7.m7.1.1.1.1" xref="S3.SS1.p3.7.m7.1.1.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p3.7.m7.1.1.1.1.2" xref="S3.SS1.p3.7.m7.1.1.1.1.2.cmml">Φ</mi><mtext class="ltx_font_smallcaps" id="S3.SS1.p3.7.m7.1.1.1.1.3" xref="S3.SS1.p3.7.m7.1.1.1.1.3a.cmml">proj</mtext></msub><mo id="S3.SS1.p3.7.m7.1.1.1.3" xref="S3.SS1.p3.7.m7.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.7.m7.1b"><apply id="S3.SS1.p3.7.m7.1.1.1.1.cmml" xref="S3.SS1.p3.7.m7.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.7.m7.1.1.1.1.1.cmml" xref="S3.SS1.p3.7.m7.1.1.1">subscript</csymbol><ci id="S3.SS1.p3.7.m7.1.1.1.1.2.cmml" xref="S3.SS1.p3.7.m7.1.1.1.1.2">Φ</ci><ci id="S3.SS1.p3.7.m7.1.1.1.1.3a.cmml" xref="S3.SS1.p3.7.m7.1.1.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS1.p3.7.m7.1.1.1.1.3.cmml" xref="S3.SS1.p3.7.m7.1.1.1.1.3">proj</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.7.m7.1c">\left(\Phi_{\textsc{proj}}\right)</annotation></semantics></math>,
and <math id="S3.SS1.p3.8.m8.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS1.p3.8.m8.1a"><mi id="S3.SS1.p3.8.m8.1.1" xref="S3.SS1.p3.8.m8.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.8.m8.1b"><ci id="S3.SS1.p3.8.m8.1.1.cmml" xref="S3.SS1.p3.8.m8.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.8.m8.1c">C</annotation></semantics></math> classification vectors or classifiers <math id="S3.SS1.p3.9.m9.1" class="ltx_Math" alttext="\left(\Phi_{\textsc{cls}}\right)" display="inline"><semantics id="S3.SS1.p3.9.m9.1a"><mrow id="S3.SS1.p3.9.m9.1.1.1" xref="S3.SS1.p3.9.m9.1.1.1.1.cmml"><mo id="S3.SS1.p3.9.m9.1.1.1.2" xref="S3.SS1.p3.9.m9.1.1.1.1.cmml">(</mo><msub id="S3.SS1.p3.9.m9.1.1.1.1" xref="S3.SS1.p3.9.m9.1.1.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p3.9.m9.1.1.1.1.2" xref="S3.SS1.p3.9.m9.1.1.1.1.2.cmml">Φ</mi><mtext class="ltx_font_smallcaps" id="S3.SS1.p3.9.m9.1.1.1.1.3" xref="S3.SS1.p3.9.m9.1.1.1.1.3a.cmml">cls</mtext></msub><mo id="S3.SS1.p3.9.m9.1.1.1.3" xref="S3.SS1.p3.9.m9.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.9.m9.1b"><apply id="S3.SS1.p3.9.m9.1.1.1.1.cmml" xref="S3.SS1.p3.9.m9.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.9.m9.1.1.1.1.1.cmml" xref="S3.SS1.p3.9.m9.1.1.1">subscript</csymbol><ci id="S3.SS1.p3.9.m9.1.1.1.1.2.cmml" xref="S3.SS1.p3.9.m9.1.1.1.1.2">Φ</ci><ci id="S3.SS1.p3.9.m9.1.1.1.1.3a.cmml" xref="S3.SS1.p3.9.m9.1.1.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS1.p3.9.m9.1.1.1.1.3.cmml" xref="S3.SS1.p3.9.m9.1.1.1.1.3">cls</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.9.m9.1c">\left(\Phi_{\textsc{cls}}\right)</annotation></semantics></math>,
yielding a set of class labels, <math id="S3.SS1.p3.10.m10.1" class="ltx_Math" alttext="\left\{c_{j}\right\}^{M}_{j=1}" display="inline"><semantics id="S3.SS1.p3.10.m10.1a"><msubsup id="S3.SS1.p3.10.m10.1.1" xref="S3.SS1.p3.10.m10.1.1.cmml"><mrow id="S3.SS1.p3.10.m10.1.1.1.1.1" xref="S3.SS1.p3.10.m10.1.1.1.1.2.cmml"><mo id="S3.SS1.p3.10.m10.1.1.1.1.1.2" xref="S3.SS1.p3.10.m10.1.1.1.1.2.cmml">{</mo><msub id="S3.SS1.p3.10.m10.1.1.1.1.1.1" xref="S3.SS1.p3.10.m10.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.10.m10.1.1.1.1.1.1.2" xref="S3.SS1.p3.10.m10.1.1.1.1.1.1.2.cmml">c</mi><mi id="S3.SS1.p3.10.m10.1.1.1.1.1.1.3" xref="S3.SS1.p3.10.m10.1.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.SS1.p3.10.m10.1.1.1.1.1.3" xref="S3.SS1.p3.10.m10.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS1.p3.10.m10.1.1.3" xref="S3.SS1.p3.10.m10.1.1.3.cmml"><mi id="S3.SS1.p3.10.m10.1.1.3.2" xref="S3.SS1.p3.10.m10.1.1.3.2.cmml">j</mi><mo id="S3.SS1.p3.10.m10.1.1.3.1" xref="S3.SS1.p3.10.m10.1.1.3.1.cmml">=</mo><mn id="S3.SS1.p3.10.m10.1.1.3.3" xref="S3.SS1.p3.10.m10.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS1.p3.10.m10.1.1.1.3" xref="S3.SS1.p3.10.m10.1.1.1.3.cmml">M</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.10.m10.1b"><apply id="S3.SS1.p3.10.m10.1.1.cmml" xref="S3.SS1.p3.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.10.m10.1.1.2.cmml" xref="S3.SS1.p3.10.m10.1.1">subscript</csymbol><apply id="S3.SS1.p3.10.m10.1.1.1.cmml" xref="S3.SS1.p3.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.10.m10.1.1.1.2.cmml" xref="S3.SS1.p3.10.m10.1.1">superscript</csymbol><set id="S3.SS1.p3.10.m10.1.1.1.1.2.cmml" xref="S3.SS1.p3.10.m10.1.1.1.1.1"><apply id="S3.SS1.p3.10.m10.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.10.m10.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.10.m10.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.10.m10.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p3.10.m10.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.10.m10.1.1.1.1.1.1.2">𝑐</ci><ci id="S3.SS1.p3.10.m10.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p3.10.m10.1.1.1.1.1.1.3">𝑗</ci></apply></set><ci id="S3.SS1.p3.10.m10.1.1.1.3.cmml" xref="S3.SS1.p3.10.m10.1.1.1.3">𝑀</ci></apply><apply id="S3.SS1.p3.10.m10.1.1.3.cmml" xref="S3.SS1.p3.10.m10.1.1.3"><eq id="S3.SS1.p3.10.m10.1.1.3.1.cmml" xref="S3.SS1.p3.10.m10.1.1.3.1"></eq><ci id="S3.SS1.p3.10.m10.1.1.3.2.cmml" xref="S3.SS1.p3.10.m10.1.1.3.2">𝑗</ci><cn type="integer" id="S3.SS1.p3.10.m10.1.1.3.3.cmml" xref="S3.SS1.p3.10.m10.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.10.m10.1c">\left\{c_{j}\right\}^{M}_{j=1}</annotation></semantics></math>
(<math id="S3.SS1.p3.11.m11.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS1.p3.11.m11.1a"><mi id="S3.SS1.p3.11.m11.1.1" xref="S3.SS1.p3.11.m11.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.11.m11.1b"><ci id="S3.SS1.p3.11.m11.1.1.cmml" xref="S3.SS1.p3.11.m11.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.11.m11.1c">C</annotation></semantics></math> is the size of the category vocabulary).</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.4" class="ltx_p">In closed-vocabulary object detection all parameters listed above are learnt
during training on <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="\mathcal{D}^{\textsc{det}}" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><msup id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p4.1.m1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.2.cmml">𝒟</mi><mtext class="ltx_font_smallcaps" id="S3.SS1.p4.1.m1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.3a.cmml">det</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><apply id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">superscript</csymbol><ci id="S3.SS1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.2">𝒟</ci><ci id="S3.SS1.p4.1.m1.1.1.3a.cmml" xref="S3.SS1.p4.1.m1.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS1.p4.1.m1.1.1.3.cmml" xref="S3.SS1.p4.1.m1.1.1.3">det</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">\mathcal{D}^{\textsc{det}}</annotation></semantics></math>.
While in the open-vocabulary scenario,
the classifiers (<math id="S3.SS1.p4.2.m2.1" class="ltx_Math" alttext="\Phi_{\textsc{cls}}" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><msub id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p4.2.m2.1.1.2" xref="S3.SS1.p4.2.m2.1.1.2.cmml">Φ</mi><mtext class="ltx_font_smallcaps" id="S3.SS1.p4.2.m2.1.1.3" xref="S3.SS1.p4.2.m2.1.1.3a.cmml">cls</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><apply id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.2.m2.1.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p4.2.m2.1.1.2.cmml" xref="S3.SS1.p4.2.m2.1.1.2">Φ</ci><ci id="S3.SS1.p4.2.m2.1.1.3a.cmml" xref="S3.SS1.p4.2.m2.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS1.p4.2.m2.1.1.3.cmml" xref="S3.SS1.p4.2.m2.1.1.3">cls</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">\Phi_{\textsc{cls}}</annotation></semantics></math>) <em id="S3.SS1.p4.4.1" class="ltx_emph ltx_font_italic">are not</em>
learnt during training but are instead generated separately
from an alternative source, <span id="S3.SS1.p4.4.2" class="ltx_ERROR undefined">\eg</span> a pre-trained text encoder.
This allows <math id="S3.SS1.p4.3.m3.1" class="ltx_Math" alttext="\mathcal{C}^{\textsc{test}}\neq\mathcal{C}^{\textsc{det}}" display="inline"><semantics id="S3.SS1.p4.3.m3.1a"><mrow id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml"><msup id="S3.SS1.p4.3.m3.1.1.2" xref="S3.SS1.p4.3.m3.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p4.3.m3.1.1.2.2" xref="S3.SS1.p4.3.m3.1.1.2.2.cmml">𝒞</mi><mtext class="ltx_font_smallcaps" id="S3.SS1.p4.3.m3.1.1.2.3" xref="S3.SS1.p4.3.m3.1.1.2.3a.cmml">test</mtext></msup><mo id="S3.SS1.p4.3.m3.1.1.1" xref="S3.SS1.p4.3.m3.1.1.1.cmml">≠</mo><msup id="S3.SS1.p4.3.m3.1.1.3" xref="S3.SS1.p4.3.m3.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p4.3.m3.1.1.3.2" xref="S3.SS1.p4.3.m3.1.1.3.2.cmml">𝒞</mi><mtext class="ltx_font_smallcaps" id="S3.SS1.p4.3.m3.1.1.3.3" xref="S3.SS1.p4.3.m3.1.1.3.3a.cmml">det</mtext></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><apply id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1"><neq id="S3.SS1.p4.3.m3.1.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1.1"></neq><apply id="S3.SS1.p4.3.m3.1.1.2.cmml" xref="S3.SS1.p4.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p4.3.m3.1.1.2.1.cmml" xref="S3.SS1.p4.3.m3.1.1.2">superscript</csymbol><ci id="S3.SS1.p4.3.m3.1.1.2.2.cmml" xref="S3.SS1.p4.3.m3.1.1.2.2">𝒞</ci><ci id="S3.SS1.p4.3.m3.1.1.2.3a.cmml" xref="S3.SS1.p4.3.m3.1.1.2.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS1.p4.3.m3.1.1.2.3.cmml" xref="S3.SS1.p4.3.m3.1.1.2.3">test</mtext></ci></apply><apply id="S3.SS1.p4.3.m3.1.1.3.cmml" xref="S3.SS1.p4.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p4.3.m3.1.1.3.1.cmml" xref="S3.SS1.p4.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS1.p4.3.m3.1.1.3.2.cmml" xref="S3.SS1.p4.3.m3.1.1.3.2">𝒞</ci><ci id="S3.SS1.p4.3.m3.1.1.3.3a.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS1.p4.3.m3.1.1.3.3.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3">det</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">\mathcal{C}^{\textsc{test}}\neq\mathcal{C}^{\textsc{det}}</annotation></semantics></math>,
as the classifiers, <math id="S3.SS1.p4.4.m4.1" class="ltx_Math" alttext="\Phi_{\textsc{cls}}" display="inline"><semantics id="S3.SS1.p4.4.m4.1a"><msub id="S3.SS1.p4.4.m4.1.1" xref="S3.SS1.p4.4.m4.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p4.4.m4.1.1.2" xref="S3.SS1.p4.4.m4.1.1.2.cmml">Φ</mi><mtext class="ltx_font_smallcaps" id="S3.SS1.p4.4.m4.1.1.3" xref="S3.SS1.p4.4.m4.1.1.3a.cmml">cls</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m4.1b"><apply id="S3.SS1.p4.4.m4.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.4.m4.1.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p4.4.m4.1.1.2.cmml" xref="S3.SS1.p4.4.m4.1.1.2">Φ</ci><ci id="S3.SS1.p4.4.m4.1.1.3a.cmml" xref="S3.SS1.p4.4.m4.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS1.p4.4.m4.1.1.3.cmml" xref="S3.SS1.p4.4.m4.1.1.3">cls</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m4.1c">\Phi_{\textsc{cls}}</annotation></semantics></math>,
for a specific set of user defined classes can be generated at inference time.
In the following sections, we describe different options for constructing such classifiers:
from natural language, from image exemplars, or from a combination of the two.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Text-based Classifiers from Language Descriptions</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Existing OVOD approaches, <span id="S3.SS2.p1.1.1" class="ltx_ERROR undefined">\eg</span> Detic <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhou et al.</span>, <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite> and ViLD <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gu et al.</span>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>, make
use of simple text-based classifiers by encoding the category name with a manual prompt,
<span id="S3.SS2.p1.1.2" class="ltx_ERROR undefined">\eg</span> <span id="S3.SS2.p1.1.3" class="ltx_text ltx_font_typewriter">a photo of a(n) {class name}</span> or <span id="S3.SS2.p1.1.4" class="ltx_text ltx_font_typewriter">a(n) {class name}</span>,
using an appropriate encoder — <span id="S3.SS2.p1.1.5" class="ltx_ERROR undefined">\eg</span>a CLIP text encoder, thereby yielding a set
of classifiers for <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{C}^{\textsc{test}}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><msup id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">𝒞</mi><mtext class="ltx_font_smallcaps" id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3a.cmml">test</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">𝒞</ci><ci id="S3.SS2.p1.1.m1.1.1.3a.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">test</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\mathcal{C}^{\textsc{test}}</annotation></semantics></math>.
This method relies on the text encoder to produce a text-based classifier
entirely from its internal understanding of <span id="S3.SS2.p1.1.6" class="ltx_text ltx_font_typewriter">class name</span>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Instead,
we make use of natural language descriptions of categories
sourced from a large language model (LLM).
Such a design choice gives additional details like visual attributes,
leading to increased discriminative information in the classifier.
This alleviates lexical confusion — <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_typewriter">class name</span> may have two
different meanings, and effectively prevents the need for human efforts to
manually write descriptions or spend time searching external sources for them.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.7" class="ltx_p">Figure <a href="#S2.F3" title="Figure 3 ‣ 2 Related Work ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> outlines our method for generating informative
text-based classifiers.
Specifically, we start by prompting an autoregressive language model with a question:
“<span id="S3.SS2.p3.7.1" class="ltx_text ltx_font_typewriter">What does a(n) {class name} look like?</span>”,
and sample multiple descriptions per class.
We use OpenAI’s API for GPT-3 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Brown et al.</span>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite> and generate <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mn id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><cn type="integer" id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">10</annotation></semantics></math> descriptions per class with
temperature sampling (Figure <a href="#S2.F3" title="Figure 3 ‣ 2 Related Work ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><mn id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><cn type="integer" id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">3</annotation></semantics></math> descriptions per class for clarity,
more examples can be found in Section <a href="#A6" title="Appendix F Additional Example Class Descriptions ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">F</span></a> of the Appendix),
yielding multiple descriptions of the format <span id="S3.SS2.p3.7.2" class="ltx_text ltx_font_typewriter">{class name} is a …</span> or similar.
Given a set of <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><mi id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><ci id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">M</annotation></semantics></math> plain text descriptions <math id="S3.SS2.p3.4.m4.1" class="ltx_Math" alttext="\left\{s_{i}^{c}\right\}_{i=1}^{M}" display="inline"><semantics id="S3.SS2.p3.4.m4.1a"><msubsup id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml"><mrow id="S3.SS2.p3.4.m4.1.1.1.1.1" xref="S3.SS2.p3.4.m4.1.1.1.1.2.cmml"><mo id="S3.SS2.p3.4.m4.1.1.1.1.1.2" xref="S3.SS2.p3.4.m4.1.1.1.1.2.cmml">{</mo><msubsup id="S3.SS2.p3.4.m4.1.1.1.1.1.1" xref="S3.SS2.p3.4.m4.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p3.4.m4.1.1.1.1.1.1.2.2" xref="S3.SS2.p3.4.m4.1.1.1.1.1.1.2.2.cmml">s</mi><mi id="S3.SS2.p3.4.m4.1.1.1.1.1.1.2.3" xref="S3.SS2.p3.4.m4.1.1.1.1.1.1.2.3.cmml">i</mi><mi id="S3.SS2.p3.4.m4.1.1.1.1.1.1.3" xref="S3.SS2.p3.4.m4.1.1.1.1.1.1.3.cmml">c</mi></msubsup><mo id="S3.SS2.p3.4.m4.1.1.1.1.1.3" xref="S3.SS2.p3.4.m4.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS2.p3.4.m4.1.1.1.3" xref="S3.SS2.p3.4.m4.1.1.1.3.cmml"><mi id="S3.SS2.p3.4.m4.1.1.1.3.2" xref="S3.SS2.p3.4.m4.1.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p3.4.m4.1.1.1.3.1" xref="S3.SS2.p3.4.m4.1.1.1.3.1.cmml">=</mo><mn id="S3.SS2.p3.4.m4.1.1.1.3.3" xref="S3.SS2.p3.4.m4.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS2.p3.4.m4.1.1.3" xref="S3.SS2.p3.4.m4.1.1.3.cmml">M</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><apply id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m4.1.1.2.cmml" xref="S3.SS2.p3.4.m4.1.1">superscript</csymbol><apply id="S3.SS2.p3.4.m4.1.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m4.1.1.1.2.cmml" xref="S3.SS2.p3.4.m4.1.1">subscript</csymbol><set id="S3.SS2.p3.4.m4.1.1.1.1.2.cmml" xref="S3.SS2.p3.4.m4.1.1.1.1.1"><apply id="S3.SS2.p3.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1.1.1.1.1">superscript</csymbol><apply id="S3.SS2.p3.4.m4.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.4.m4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m4.1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p3.4.m4.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.4.m4.1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p3.4.m4.1.1.1.1.1.1.2.2">𝑠</ci><ci id="S3.SS2.p3.4.m4.1.1.1.1.1.1.2.3.cmml" xref="S3.SS2.p3.4.m4.1.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S3.SS2.p3.4.m4.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.4.m4.1.1.1.1.1.1.3">𝑐</ci></apply></set><apply id="S3.SS2.p3.4.m4.1.1.1.3.cmml" xref="S3.SS2.p3.4.m4.1.1.1.3"><eq id="S3.SS2.p3.4.m4.1.1.1.3.1.cmml" xref="S3.SS2.p3.4.m4.1.1.1.3.1"></eq><ci id="S3.SS2.p3.4.m4.1.1.1.3.2.cmml" xref="S3.SS2.p3.4.m4.1.1.1.3.2">𝑖</ci><cn type="integer" id="S3.SS2.p3.4.m4.1.1.1.3.3.cmml" xref="S3.SS2.p3.4.m4.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS2.p3.4.m4.1.1.3.cmml" xref="S3.SS2.p3.4.m4.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">\left\{s_{i}^{c}\right\}_{i=1}^{M}</annotation></semantics></math>
for class <math id="S3.SS2.p3.5.m5.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS2.p3.5.m5.1a"><mi id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><ci id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">c</annotation></semantics></math>,
we encode each element of the set with a CLIP text encoder <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Radford et al.</span>, <a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>,
<math id="S3.SS2.p3.6.m6.1" class="ltx_Math" alttext="f_{\textsc{clip-t}}(\cdot)" display="inline"><semantics id="S3.SS2.p3.6.m6.1a"><mrow id="S3.SS2.p3.6.m6.1.2" xref="S3.SS2.p3.6.m6.1.2.cmml"><msub id="S3.SS2.p3.6.m6.1.2.2" xref="S3.SS2.p3.6.m6.1.2.2.cmml"><mi id="S3.SS2.p3.6.m6.1.2.2.2" xref="S3.SS2.p3.6.m6.1.2.2.2.cmml">f</mi><mtext class="ltx_font_smallcaps" id="S3.SS2.p3.6.m6.1.2.2.3" xref="S3.SS2.p3.6.m6.1.2.2.3a.cmml">clip-t</mtext></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p3.6.m6.1.2.1" xref="S3.SS2.p3.6.m6.1.2.1.cmml">​</mo><mrow id="S3.SS2.p3.6.m6.1.2.3.2" xref="S3.SS2.p3.6.m6.1.2.cmml"><mo stretchy="false" id="S3.SS2.p3.6.m6.1.2.3.2.1" xref="S3.SS2.p3.6.m6.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS2.p3.6.m6.1.2.3.2.2" xref="S3.SS2.p3.6.m6.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b"><apply id="S3.SS2.p3.6.m6.1.2.cmml" xref="S3.SS2.p3.6.m6.1.2"><times id="S3.SS2.p3.6.m6.1.2.1.cmml" xref="S3.SS2.p3.6.m6.1.2.1"></times><apply id="S3.SS2.p3.6.m6.1.2.2.cmml" xref="S3.SS2.p3.6.m6.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.6.m6.1.2.2.1.cmml" xref="S3.SS2.p3.6.m6.1.2.2">subscript</csymbol><ci id="S3.SS2.p3.6.m6.1.2.2.2.cmml" xref="S3.SS2.p3.6.m6.1.2.2.2">𝑓</ci><ci id="S3.SS2.p3.6.m6.1.2.2.3a.cmml" xref="S3.SS2.p3.6.m6.1.2.2.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS2.p3.6.m6.1.2.2.3.cmml" xref="S3.SS2.p3.6.m6.1.2.2.3">clip-t</mtext></ci></apply><ci id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">f_{\textsc{clip-t}}(\cdot)</annotation></semantics></math>, and the text-based
classifier for class <math id="S3.SS2.p3.7.m7.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS2.p3.7.m7.1a"><mi id="S3.SS2.p3.7.m7.1.1" xref="S3.SS2.p3.7.m7.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.7.m7.1b"><ci id="S3.SS2.p3.7.m7.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.7.m7.1c">c</annotation></semantics></math> is obtained from the mean of these text encodings:</p>
<table id="A9.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3.m1.1" class="ltx_Math" alttext="\displaystyle\mathbf{w}_{\textsc{text}}^{c}" display="inline"><semantics id="S3.E3.m1.1a"><msubsup id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><mi id="S3.E3.m1.1.1.2.2" xref="S3.E3.m1.1.1.2.2.cmml">𝐰</mi><mtext class="ltx_font_smallcaps" id="S3.E3.m1.1.1.2.3" xref="S3.E3.m1.1.1.2.3a.cmml">text</mtext><mi id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml">c</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1">superscript</csymbol><apply id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.2.1.cmml" xref="S3.E3.m1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.2.2">𝐰</ci><ci id="S3.E3.m1.1.1.2.3a.cmml" xref="S3.E3.m1.1.1.2.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.E3.m1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.2.3">text</mtext></ci></apply><ci id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\displaystyle\mathbf{w}_{\textsc{text}}^{c}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E3.m2.1" class="ltx_Math" alttext="\displaystyle=\frac{1}{M}\sum_{i=1}^{M}f_{\textsc{clip-t}}\left(s_{i}^{c}\right)" display="inline"><semantics id="S3.E3.m2.1a"><mrow id="S3.E3.m2.1.1" xref="S3.E3.m2.1.1.cmml"><mi id="S3.E3.m2.1.1.3" xref="S3.E3.m2.1.1.3.cmml"></mi><mo id="S3.E3.m2.1.1.2" xref="S3.E3.m2.1.1.2.cmml">=</mo><mrow id="S3.E3.m2.1.1.1" xref="S3.E3.m2.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E3.m2.1.1.1.3" xref="S3.E3.m2.1.1.1.3.cmml"><mfrac id="S3.E3.m2.1.1.1.3a" xref="S3.E3.m2.1.1.1.3.cmml"><mn id="S3.E3.m2.1.1.1.3.2" xref="S3.E3.m2.1.1.1.3.2.cmml">1</mn><mi id="S3.E3.m2.1.1.1.3.3" xref="S3.E3.m2.1.1.1.3.3.cmml">M</mi></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S3.E3.m2.1.1.1.2" xref="S3.E3.m2.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m2.1.1.1.1" xref="S3.E3.m2.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E3.m2.1.1.1.1.2" xref="S3.E3.m2.1.1.1.1.2.cmml"><munderover id="S3.E3.m2.1.1.1.1.2a" xref="S3.E3.m2.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E3.m2.1.1.1.1.2.2.2" xref="S3.E3.m2.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E3.m2.1.1.1.1.2.2.3" xref="S3.E3.m2.1.1.1.1.2.2.3.cmml"><mi id="S3.E3.m2.1.1.1.1.2.2.3.2" xref="S3.E3.m2.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E3.m2.1.1.1.1.2.2.3.1" xref="S3.E3.m2.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E3.m2.1.1.1.1.2.2.3.3" xref="S3.E3.m2.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E3.m2.1.1.1.1.2.3" xref="S3.E3.m2.1.1.1.1.2.3.cmml">M</mi></munderover></mstyle><mrow id="S3.E3.m2.1.1.1.1.1" xref="S3.E3.m2.1.1.1.1.1.cmml"><msub id="S3.E3.m2.1.1.1.1.1.3" xref="S3.E3.m2.1.1.1.1.1.3.cmml"><mi id="S3.E3.m2.1.1.1.1.1.3.2" xref="S3.E3.m2.1.1.1.1.1.3.2.cmml">f</mi><mtext class="ltx_font_smallcaps" id="S3.E3.m2.1.1.1.1.1.3.3" xref="S3.E3.m2.1.1.1.1.1.3.3a.cmml">clip-t</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E3.m2.1.1.1.1.1.2" xref="S3.E3.m2.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m2.1.1.1.1.1.1.1" xref="S3.E3.m2.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E3.m2.1.1.1.1.1.1.1.2" xref="S3.E3.m2.1.1.1.1.1.1.1.1.cmml">(</mo><msubsup id="S3.E3.m2.1.1.1.1.1.1.1.1" xref="S3.E3.m2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m2.1.1.1.1.1.1.1.1.2.2" xref="S3.E3.m2.1.1.1.1.1.1.1.1.2.2.cmml">s</mi><mi id="S3.E3.m2.1.1.1.1.1.1.1.1.2.3" xref="S3.E3.m2.1.1.1.1.1.1.1.1.2.3.cmml">i</mi><mi id="S3.E3.m2.1.1.1.1.1.1.1.1.3" xref="S3.E3.m2.1.1.1.1.1.1.1.1.3.cmml">c</mi></msubsup><mo id="S3.E3.m2.1.1.1.1.1.1.1.3" xref="S3.E3.m2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m2.1b"><apply id="S3.E3.m2.1.1.cmml" xref="S3.E3.m2.1.1"><eq id="S3.E3.m2.1.1.2.cmml" xref="S3.E3.m2.1.1.2"></eq><csymbol cd="latexml" id="S3.E3.m2.1.1.3.cmml" xref="S3.E3.m2.1.1.3">absent</csymbol><apply id="S3.E3.m2.1.1.1.cmml" xref="S3.E3.m2.1.1.1"><times id="S3.E3.m2.1.1.1.2.cmml" xref="S3.E3.m2.1.1.1.2"></times><apply id="S3.E3.m2.1.1.1.3.cmml" xref="S3.E3.m2.1.1.1.3"><divide id="S3.E3.m2.1.1.1.3.1.cmml" xref="S3.E3.m2.1.1.1.3"></divide><cn type="integer" id="S3.E3.m2.1.1.1.3.2.cmml" xref="S3.E3.m2.1.1.1.3.2">1</cn><ci id="S3.E3.m2.1.1.1.3.3.cmml" xref="S3.E3.m2.1.1.1.3.3">𝑀</ci></apply><apply id="S3.E3.m2.1.1.1.1.cmml" xref="S3.E3.m2.1.1.1.1"><apply id="S3.E3.m2.1.1.1.1.2.cmml" xref="S3.E3.m2.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m2.1.1.1.1.2.1.cmml" xref="S3.E3.m2.1.1.1.1.2">superscript</csymbol><apply id="S3.E3.m2.1.1.1.1.2.2.cmml" xref="S3.E3.m2.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m2.1.1.1.1.2.2.1.cmml" xref="S3.E3.m2.1.1.1.1.2">subscript</csymbol><sum id="S3.E3.m2.1.1.1.1.2.2.2.cmml" xref="S3.E3.m2.1.1.1.1.2.2.2"></sum><apply id="S3.E3.m2.1.1.1.1.2.2.3.cmml" xref="S3.E3.m2.1.1.1.1.2.2.3"><eq id="S3.E3.m2.1.1.1.1.2.2.3.1.cmml" xref="S3.E3.m2.1.1.1.1.2.2.3.1"></eq><ci id="S3.E3.m2.1.1.1.1.2.2.3.2.cmml" xref="S3.E3.m2.1.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S3.E3.m2.1.1.1.1.2.2.3.3.cmml" xref="S3.E3.m2.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E3.m2.1.1.1.1.2.3.cmml" xref="S3.E3.m2.1.1.1.1.2.3">𝑀</ci></apply><apply id="S3.E3.m2.1.1.1.1.1.cmml" xref="S3.E3.m2.1.1.1.1.1"><times id="S3.E3.m2.1.1.1.1.1.2.cmml" xref="S3.E3.m2.1.1.1.1.1.2"></times><apply id="S3.E3.m2.1.1.1.1.1.3.cmml" xref="S3.E3.m2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m2.1.1.1.1.1.3.1.cmml" xref="S3.E3.m2.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m2.1.1.1.1.1.3.2.cmml" xref="S3.E3.m2.1.1.1.1.1.3.2">𝑓</ci><ci id="S3.E3.m2.1.1.1.1.1.3.3a.cmml" xref="S3.E3.m2.1.1.1.1.1.3.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.E3.m2.1.1.1.1.1.3.3.cmml" xref="S3.E3.m2.1.1.1.1.1.3.3">clip-t</mtext></ci></apply><apply id="S3.E3.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m2.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E3.m2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m2.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m2.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m2.1.1.1.1.1.1.1.1.2.2">𝑠</ci><ci id="S3.E3.m2.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m2.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S3.E3.m2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m2.1.1.1.1.1.1.1.1.3">𝑐</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m2.1c">\displaystyle=\frac{1}{M}\sum_{i=1}^{M}f_{\textsc{clip-t}}\left(s_{i}^{c}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.3" class="ltx_p">At detector training time, text-based classifiers for categories of
interest (<math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="c\in\mathcal{C}^{\textsc{det}}" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><mrow id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml"><mi id="S3.SS2.p4.1.m1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.2.cmml">c</mi><mo id="S3.SS2.p4.1.m1.1.1.1" xref="S3.SS2.p4.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS2.p4.1.m1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p4.1.m1.1.1.3.2" xref="S3.SS2.p4.1.m1.1.1.3.2.cmml">𝒞</mi><mtext class="ltx_font_smallcaps" id="S3.SS2.p4.1.m1.1.1.3.3" xref="S3.SS2.p4.1.m1.1.1.3.3a.cmml">det</mtext></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"><in id="S3.SS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1.1"></in><ci id="S3.SS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2">𝑐</ci><apply id="S3.SS2.p4.1.m1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.1.1.3.1.cmml" xref="S3.SS2.p4.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.p4.1.m1.1.1.3.2.cmml" xref="S3.SS2.p4.1.m1.1.1.3.2">𝒞</ci><ci id="S3.SS2.p4.1.m1.1.1.3.3a.cmml" xref="S3.SS2.p4.1.m1.1.1.3.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS2.p4.1.m1.1.1.3.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3.3">det</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">c\in\mathcal{C}^{\textsc{det}}</annotation></semantics></math> and <math id="S3.SS2.p4.2.m2.1" class="ltx_Math" alttext="c\in\mathcal{C}^{\textsc{img}}" display="inline"><semantics id="S3.SS2.p4.2.m2.1a"><mrow id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml"><mi id="S3.SS2.p4.2.m2.1.1.2" xref="S3.SS2.p4.2.m2.1.1.2.cmml">c</mi><mo id="S3.SS2.p4.2.m2.1.1.1" xref="S3.SS2.p4.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS2.p4.2.m2.1.1.3" xref="S3.SS2.p4.2.m2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p4.2.m2.1.1.3.2" xref="S3.SS2.p4.2.m2.1.1.3.2.cmml">𝒞</mi><mtext class="ltx_font_smallcaps" id="S3.SS2.p4.2.m2.1.1.3.3" xref="S3.SS2.p4.2.m2.1.1.3.3a.cmml">img</mtext></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><apply id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1"><in id="S3.SS2.p4.2.m2.1.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1.1"></in><ci id="S3.SS2.p4.2.m2.1.1.2.cmml" xref="S3.SS2.p4.2.m2.1.1.2">𝑐</ci><apply id="S3.SS2.p4.2.m2.1.1.3.cmml" xref="S3.SS2.p4.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p4.2.m2.1.1.3.1.cmml" xref="S3.SS2.p4.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS2.p4.2.m2.1.1.3.2.cmml" xref="S3.SS2.p4.2.m2.1.1.3.2">𝒞</ci><ci id="S3.SS2.p4.2.m2.1.1.3.3a.cmml" xref="S3.SS2.p4.2.m2.1.1.3.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS2.p4.2.m2.1.1.3.3.cmml" xref="S3.SS2.p4.2.m2.1.1.3.3">img</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">c\in\mathcal{C}^{\textsc{img}}</annotation></semantics></math>) are pre-computed and kept frozen,
the rest of detector parameters are updated <span id="S3.SS2.p4.3.1" class="ltx_ERROR undefined">\ie</span> all parameters
in Equations <a href="#S3.E1" title="Equation 1 ‣ 3.1 Preliminaries ‣ 3 Method ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>-<a href="#S3.E2" title="Equation 2 ‣ 3.1 Preliminaries ‣ 3 Method ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, except <math id="S3.SS2.p4.3.m3.1" class="ltx_Math" alttext="\Phi_{\textsc{cls}}" display="inline"><semantics id="S3.SS2.p4.3.m3.1a"><msub id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml"><mi mathvariant="normal" id="S3.SS2.p4.3.m3.1.1.2" xref="S3.SS2.p4.3.m3.1.1.2.cmml">Φ</mi><mtext class="ltx_font_smallcaps" id="S3.SS2.p4.3.m3.1.1.3" xref="S3.SS2.p4.3.m3.1.1.3a.cmml">cls</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><apply id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.3.m3.1.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p4.3.m3.1.1.2.cmml" xref="S3.SS2.p4.3.m3.1.1.2">Φ</ci><ci id="S3.SS2.p4.3.m3.1.1.3a.cmml" xref="S3.SS2.p4.3.m3.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS2.p4.3.m3.1.1.3.cmml" xref="S3.SS2.p4.3.m3.1.1.3">cls</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">\Phi_{\textsc{cls}}</annotation></semantics></math>.
At inference time, classifiers for testing categories are computed similarly
to enable open-vocabulary object detection.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para ltx_noindent">
<p id="S3.SS2.p5.1" class="ltx_p"><span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_bold">Discussion.</span>
In this paper,
we only consider a single straightforward question as a prompt to the LLM:
“<span id="S3.SS2.p5.1.2" class="ltx_text ltx_font_typewriter">What does a(n) {class name} look like?</span>”.
However, it is also feasible to use alternative question prompts,
<span id="S3.SS2.p5.1.3" class="ltx_ERROR undefined">\eg</span> “<span id="S3.SS2.p5.1.4" class="ltx_text ltx_font_typewriter">How can you identify a(n) {class name}?</span>” or
“<span id="S3.SS2.p5.1.5" class="ltx_text ltx_font_typewriter">Describe what a(n) {class name} looks like?</span>”,
and obtain visual descriptions with the same or similar concepts <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Pratt et al.</span>, <a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p">We investigated using a transformer architecture to aggregate text embeddings
from natural language descriptions,
but we found this was not beneficial to OVOD performance over simply using the mean vector.
In general, each text embedding of a natural language description summarises
the category of interest very well and so the contrastive task,
which is used to train the aggregator (explained in detail for the visual case below),
is very easy with text embeddings yielding no improvement in text-based classifiers for OVOD.
</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Vision-based Classifiers from Image Exemplars</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In addition to constructing classifiers with natural language descriptions,
another natural option is to use image exemplars,
especially in cases where a good description of the category is prohibitively long (such as the painted lady butterfly or <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_italic">Vanessa cardui</span> which has an intricate wing pattern),
or occasionally the class name is not known beforehand,
<span id="S3.SS3.p1.1.2" class="ltx_ERROR undefined">\eg</span> “Deerstalker cap” refers to the hat often worn by Sherlock Holmes.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.6" class="ltx_p">In such scenarios, we propose to construct classifiers by using image exemplars,
as shown in Figure <a href="#S2.F3" title="Figure 3 ‣ 2 Related Work ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
Specifically, given a set of <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mi id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">K</annotation></semantics></math> RGB image exemplars for category <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mi id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">c</annotation></semantics></math>,
<math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="\left\{\mathbf{x}_{i}^{c}\right\}_{i=1}^{K}" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><msubsup id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml"><mrow id="S3.SS3.p2.3.m3.1.1.1.1.1" xref="S3.SS3.p2.3.m3.1.1.1.1.2.cmml"><mo id="S3.SS3.p2.3.m3.1.1.1.1.1.2" xref="S3.SS3.p2.3.m3.1.1.1.1.2.cmml">{</mo><msubsup id="S3.SS3.p2.3.m3.1.1.1.1.1.1" xref="S3.SS3.p2.3.m3.1.1.1.1.1.1.cmml"><mi id="S3.SS3.p2.3.m3.1.1.1.1.1.1.2.2" xref="S3.SS3.p2.3.m3.1.1.1.1.1.1.2.2.cmml">𝐱</mi><mi id="S3.SS3.p2.3.m3.1.1.1.1.1.1.2.3" xref="S3.SS3.p2.3.m3.1.1.1.1.1.1.2.3.cmml">i</mi><mi id="S3.SS3.p2.3.m3.1.1.1.1.1.1.3" xref="S3.SS3.p2.3.m3.1.1.1.1.1.1.3.cmml">c</mi></msubsup><mo id="S3.SS3.p2.3.m3.1.1.1.1.1.3" xref="S3.SS3.p2.3.m3.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS3.p2.3.m3.1.1.1.3" xref="S3.SS3.p2.3.m3.1.1.1.3.cmml"><mi id="S3.SS3.p2.3.m3.1.1.1.3.2" xref="S3.SS3.p2.3.m3.1.1.1.3.2.cmml">i</mi><mo id="S3.SS3.p2.3.m3.1.1.1.3.1" xref="S3.SS3.p2.3.m3.1.1.1.3.1.cmml">=</mo><mn id="S3.SS3.p2.3.m3.1.1.1.3.3" xref="S3.SS3.p2.3.m3.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS3.p2.3.m3.1.1.3" xref="S3.SS3.p2.3.m3.1.1.3.cmml">K</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><apply id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1">superscript</csymbol><apply id="S3.SS3.p2.3.m3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1">subscript</csymbol><set id="S3.SS3.p2.3.m3.1.1.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.1.1.1"><apply id="S3.SS3.p2.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1.1.1.1">superscript</csymbol><apply id="S3.SS3.p2.3.m3.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.1.1.1.1.2.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p2.3.m3.1.1.1.1.1.1.2.2.cmml" xref="S3.SS3.p2.3.m3.1.1.1.1.1.1.2.2">𝐱</ci><ci id="S3.SS3.p2.3.m3.1.1.1.1.1.1.2.3.cmml" xref="S3.SS3.p2.3.m3.1.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S3.SS3.p2.3.m3.1.1.1.1.1.1.3.cmml" xref="S3.SS3.p2.3.m3.1.1.1.1.1.1.3">𝑐</ci></apply></set><apply id="S3.SS3.p2.3.m3.1.1.1.3.cmml" xref="S3.SS3.p2.3.m3.1.1.1.3"><eq id="S3.SS3.p2.3.m3.1.1.1.3.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1.3.1"></eq><ci id="S3.SS3.p2.3.m3.1.1.1.3.2.cmml" xref="S3.SS3.p2.3.m3.1.1.1.3.2">𝑖</ci><cn type="integer" id="S3.SS3.p2.3.m3.1.1.1.3.3.cmml" xref="S3.SS3.p2.3.m3.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS3.p2.3.m3.1.1.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">\left\{\mathbf{x}_{i}^{c}\right\}_{i=1}^{K}</annotation></semantics></math>,
we encode each exemplar with a CLIP visual encoder, <math id="S3.SS3.p2.4.m4.1" class="ltx_Math" alttext="f_{\textsc{CLIP-im}}(\cdot)" display="inline"><semantics id="S3.SS3.p2.4.m4.1a"><mrow id="S3.SS3.p2.4.m4.1.2" xref="S3.SS3.p2.4.m4.1.2.cmml"><msub id="S3.SS3.p2.4.m4.1.2.2" xref="S3.SS3.p2.4.m4.1.2.2.cmml"><mi id="S3.SS3.p2.4.m4.1.2.2.2" xref="S3.SS3.p2.4.m4.1.2.2.2.cmml">f</mi><mtext class="ltx_font_smallcaps" id="S3.SS3.p2.4.m4.1.2.2.3" xref="S3.SS3.p2.4.m4.1.2.2.3a.cmml">CLIP-im</mtext></msub><mo lspace="0em" rspace="0em" id="S3.SS3.p2.4.m4.1.2.1" xref="S3.SS3.p2.4.m4.1.2.1.cmml">​</mo><mrow id="S3.SS3.p2.4.m4.1.2.3.2" xref="S3.SS3.p2.4.m4.1.2.cmml"><mo stretchy="false" id="S3.SS3.p2.4.m4.1.2.3.2.1" xref="S3.SS3.p2.4.m4.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS3.p2.4.m4.1.2.3.2.2" xref="S3.SS3.p2.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><apply id="S3.SS3.p2.4.m4.1.2.cmml" xref="S3.SS3.p2.4.m4.1.2"><times id="S3.SS3.p2.4.m4.1.2.1.cmml" xref="S3.SS3.p2.4.m4.1.2.1"></times><apply id="S3.SS3.p2.4.m4.1.2.2.cmml" xref="S3.SS3.p2.4.m4.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.1.2.2.1.cmml" xref="S3.SS3.p2.4.m4.1.2.2">subscript</csymbol><ci id="S3.SS3.p2.4.m4.1.2.2.2.cmml" xref="S3.SS3.p2.4.m4.1.2.2.2">𝑓</ci><ci id="S3.SS3.p2.4.m4.1.2.2.3a.cmml" xref="S3.SS3.p2.4.m4.1.2.2.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS3.p2.4.m4.1.2.2.3.cmml" xref="S3.SS3.p2.4.m4.1.2.2.3">CLIP-im</mtext></ci></apply><ci id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">f_{\textsc{CLIP-im}}(\cdot)</annotation></semantics></math>,
yielding <math id="S3.SS3.p2.5.m5.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS3.p2.5.m5.1a"><mi id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><ci id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">K</annotation></semantics></math> image embeddings,
which are then passed to a multi-layer Transformer <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Vaswani et al.</span>, <a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2017</span></a>)</cite> with learnable [CLS] token, <math id="S3.SS3.p2.6.m6.1" class="ltx_Math" alttext="\mathbf{t}_{\textsc{CLS}}" display="inline"><semantics id="S3.SS3.p2.6.m6.1a"><msub id="S3.SS3.p2.6.m6.1.1" xref="S3.SS3.p2.6.m6.1.1.cmml"><mi id="S3.SS3.p2.6.m6.1.1.2" xref="S3.SS3.p2.6.m6.1.1.2.cmml">𝐭</mi><mtext class="ltx_font_smallcaps" id="S3.SS3.p2.6.m6.1.1.3" xref="S3.SS3.p2.6.m6.1.1.3a.cmml">CLS</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.1b"><apply id="S3.SS3.p2.6.m6.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m6.1.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.p2.6.m6.1.1.2.cmml" xref="S3.SS3.p2.6.m6.1.1.2">𝐭</ci><ci id="S3.SS3.p2.6.m6.1.1.3a.cmml" xref="S3.SS3.p2.6.m6.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS3.p2.6.m6.1.1.3.cmml" xref="S3.SS3.p2.6.m6.1.1.3">CLS</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.1c">\mathbf{t}_{\textsc{CLS}}</annotation></semantics></math>:</p>
<table id="A9.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E4.m1.1" class="ltx_Math" alttext="\displaystyle\mathbf{w}_{\textsc{img}}^{c}" display="inline"><semantics id="S3.E4.m1.1a"><msubsup id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><mi id="S3.E4.m1.1.1.2.2" xref="S3.E4.m1.1.1.2.2.cmml">𝐰</mi><mtext class="ltx_font_smallcaps" id="S3.E4.m1.1.1.2.3" xref="S3.E4.m1.1.1.2.3a.cmml">img</mtext><mi id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml">c</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1">superscript</csymbol><apply id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.2.1.cmml" xref="S3.E4.m1.1.1">subscript</csymbol><ci id="S3.E4.m1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.2.2">𝐰</ci><ci id="S3.E4.m1.1.1.2.3a.cmml" xref="S3.E4.m1.1.1.2.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.E4.m1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.2.3">img</mtext></ci></apply><ci id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\displaystyle\mathbf{w}_{\textsc{img}}^{c}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E4.m2.2" class="ltx_Math" alttext="\displaystyle=\text{Transformer}\left(\left\{f_{\textsc{CLIP-im}}\left(\mathbf{x}_{i}^{c}\right)\right\}_{i=1}^{K};\mathbf{t}_{\textsc{CLS}}\right)" display="inline"><semantics id="S3.E4.m2.2a"><mrow id="S3.E4.m2.2.2" xref="S3.E4.m2.2.2.cmml"><mi id="S3.E4.m2.2.2.4" xref="S3.E4.m2.2.2.4.cmml"></mi><mo id="S3.E4.m2.2.2.3" xref="S3.E4.m2.2.2.3.cmml">=</mo><mrow id="S3.E4.m2.2.2.2" xref="S3.E4.m2.2.2.2.cmml"><mtext id="S3.E4.m2.2.2.2.4" xref="S3.E4.m2.2.2.2.4a.cmml">Transformer</mtext><mo lspace="0em" rspace="0em" id="S3.E4.m2.2.2.2.3" xref="S3.E4.m2.2.2.2.3.cmml">​</mo><mrow id="S3.E4.m2.2.2.2.2.2" xref="S3.E4.m2.2.2.2.2.3.cmml"><mo id="S3.E4.m2.2.2.2.2.2.3" xref="S3.E4.m2.2.2.2.2.3.cmml">(</mo><msubsup id="S3.E4.m2.1.1.1.1.1.1" xref="S3.E4.m2.1.1.1.1.1.1.cmml"><mrow id="S3.E4.m2.1.1.1.1.1.1.1.1.1" xref="S3.E4.m2.1.1.1.1.1.1.1.1.2.cmml"><mo id="S3.E4.m2.1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m2.1.1.1.1.1.1.1.1.2.cmml">{</mo><mrow id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.3.2.cmml">f</mi><mtext class="ltx_font_smallcaps" id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.3.3a.cmml">CLIP-im</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msubsup id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">𝐱</mi><mi id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi><mi id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">c</mi></msubsup><mo id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E4.m2.1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m2.1.1.1.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.E4.m2.1.1.1.1.1.1.1.3" xref="S3.E4.m2.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E4.m2.1.1.1.1.1.1.1.3.2" xref="S3.E4.m2.1.1.1.1.1.1.1.3.2.cmml">i</mi><mo id="S3.E4.m2.1.1.1.1.1.1.1.3.1" xref="S3.E4.m2.1.1.1.1.1.1.1.3.1.cmml">=</mo><mn id="S3.E4.m2.1.1.1.1.1.1.1.3.3" xref="S3.E4.m2.1.1.1.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.E4.m2.1.1.1.1.1.1.3" xref="S3.E4.m2.1.1.1.1.1.1.3.cmml">K</mi></msubsup><mo id="S3.E4.m2.2.2.2.2.2.4" xref="S3.E4.m2.2.2.2.2.3.cmml">;</mo><msub id="S3.E4.m2.2.2.2.2.2.2" xref="S3.E4.m2.2.2.2.2.2.2.cmml"><mi id="S3.E4.m2.2.2.2.2.2.2.2" xref="S3.E4.m2.2.2.2.2.2.2.2.cmml">𝐭</mi><mtext class="ltx_font_smallcaps" id="S3.E4.m2.2.2.2.2.2.2.3" xref="S3.E4.m2.2.2.2.2.2.2.3a.cmml">CLS</mtext></msub><mo id="S3.E4.m2.2.2.2.2.2.5" xref="S3.E4.m2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m2.2b"><apply id="S3.E4.m2.2.2.cmml" xref="S3.E4.m2.2.2"><eq id="S3.E4.m2.2.2.3.cmml" xref="S3.E4.m2.2.2.3"></eq><csymbol cd="latexml" id="S3.E4.m2.2.2.4.cmml" xref="S3.E4.m2.2.2.4">absent</csymbol><apply id="S3.E4.m2.2.2.2.cmml" xref="S3.E4.m2.2.2.2"><times id="S3.E4.m2.2.2.2.3.cmml" xref="S3.E4.m2.2.2.2.3"></times><ci id="S3.E4.m2.2.2.2.4a.cmml" xref="S3.E4.m2.2.2.2.4"><mtext id="S3.E4.m2.2.2.2.4.cmml" xref="S3.E4.m2.2.2.2.4">Transformer</mtext></ci><list id="S3.E4.m2.2.2.2.2.3.cmml" xref="S3.E4.m2.2.2.2.2.2"><apply id="S3.E4.m2.1.1.1.1.1.1.cmml" xref="S3.E4.m2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.1.1.1.1.2.cmml" xref="S3.E4.m2.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E4.m2.1.1.1.1.1.1.1.cmml" xref="S3.E4.m2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m2.1.1.1.1.1.1">subscript</csymbol><set id="S3.E4.m2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1"><apply id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1"><times id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.2"></times><apply id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.3.2">𝑓</ci><ci id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.3.3a.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.3.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.3.3">CLIP-im</mtext></ci></apply><apply id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2">𝐱</ci><ci id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑐</ci></apply></apply></set><apply id="S3.E4.m2.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.3"><eq id="S3.E4.m2.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.3.1"></eq><ci id="S3.E4.m2.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.3.2">𝑖</ci><cn type="integer" id="S3.E4.m2.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.3.3">1</cn></apply></apply><ci id="S3.E4.m2.1.1.1.1.1.1.3.cmml" xref="S3.E4.m2.1.1.1.1.1.1.3">𝐾</ci></apply><apply id="S3.E4.m2.2.2.2.2.2.2.cmml" xref="S3.E4.m2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m2.2.2.2.2.2.2.1.cmml" xref="S3.E4.m2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E4.m2.2.2.2.2.2.2.2.cmml" xref="S3.E4.m2.2.2.2.2.2.2.2">𝐭</ci><ci id="S3.E4.m2.2.2.2.2.2.2.3a.cmml" xref="S3.E4.m2.2.2.2.2.2.2.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.E4.m2.2.2.2.2.2.2.3.cmml" xref="S3.E4.m2.2.2.2.2.2.2.3">CLS</mtext></ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m2.2c">\displaystyle=\text{Transformer}\left(\left\{f_{\textsc{CLIP-im}}\left(\mathbf{x}_{i}^{c}\right)\right\}_{i=1}^{K};\mathbf{t}_{\textsc{CLS}}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">The Transformer architecture acts to best aggregate the <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mi id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">K</annotation></semantics></math> image exemplars,
and the output from the [CLS] token is used as the vision-based classifier for OVOD.
When training the transformer aggregator, all exemplars are sourced from ImageNet-21k-P <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Ridnik et al.</span>, <a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>.
When generating vision-based classifiers for OVOD, where possible, we source our exemplars
from ImageNet-21k <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Deng et al.</span>, <a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2009</span></a>)</cite> — where this is not possible, we use LVIS and/or VisualGenome
training data to source image exemplars.
Additional details on how we collate exemplars for training and testing are provided
in the Appendix (Section <a href="#A4" title="Appendix D Sourcing Image Exemplars ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>).
This transformer architecture will be referred to as the <em id="S3.SS3.p3.1.1" class="ltx_emph ltx_font_italic">visual aggregator</em>
and its training procedure is described next.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para ltx_noindent">
<p id="S3.SS3.p4.9" class="ltx_p"><span id="S3.SS3.p4.9.1" class="ltx_text ltx_font_bold">Offline Training. </span>
The visual aggregator is trained offline <span id="S3.SS3.p4.9.2" class="ltx_ERROR undefined">\ie</span> it is not updated during detector
training.
The training procedure needs to learn an aggregator which combines multiple
image exemplars to produce effective vision-based classifiers for OVOD
— a classifier for a given class needs to be discriminative <span id="S3.SS3.p4.9.3" class="ltx_ERROR undefined">\wrt</span> other classes.
A CLIP image encoder is used to provide initial embeddings for each exemplar.
We keep the CLIP image encoder frozen during training to improve training
efficiency and prevent catastrophic forgetting in the CLIP representation.
To provide discriminative vision-based classifiers,
contrastive learning is utilised.
For a given class, the output embedding from the visual
aggregator is trained to minimise similarity with the output embedding from other classes and maximise
the similarity with an output embedding from the same class.
To do this, the contrastive InfoNCE <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">van den Oord et al.</span>, <a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite> loss is used.
The visual aggregator should generalise well and not be trained for a specific
downstream OVOD vocabulary,
therefore it is trained with the ImageNet-21k-P dataset <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Ridnik et al.</span>, <a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite> for image classification,
which contains <math id="S3.SS3.p4.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.SS3.p4.1.m1.1a"><mo id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><csymbol cd="latexml" id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">\sim</annotation></semantics></math>11M images across <math id="S3.SS3.p4.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.SS3.p4.2.m2.1a"><mo id="S3.SS3.p4.2.m2.1.1" xref="S3.SS3.p4.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.1b"><csymbol cd="latexml" id="S3.SS3.p4.2.m2.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.1c">\sim</annotation></semantics></math>11K classes.
For category <math id="S3.SS3.p4.3.m3.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS3.p4.3.m3.1a"><mi id="S3.SS3.p4.3.m3.1.1" xref="S3.SS3.p4.3.m3.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.3.m3.1b"><ci id="S3.SS3.p4.3.m3.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.3.m3.1c">c</annotation></semantics></math> during visual aggregator training, at each iteration,
two distinct sets of <math id="S3.SS3.p4.4.m4.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS3.p4.4.m4.1a"><mi id="S3.SS3.p4.4.m4.1.1" xref="S3.SS3.p4.4.m4.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.4.m4.1b"><ci id="S3.SS3.p4.4.m4.1.1.cmml" xref="S3.SS3.p4.4.m4.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.4.m4.1c">K</annotation></semantics></math> exemplars are sampled, augmented and encoded
by the frozen CLIP image encoder.
The two sets are input separately to visual aggregator, outputting
<math id="S3.SS3.p4.5.m5.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S3.SS3.p4.5.m5.1a"><mn id="S3.SS3.p4.5.m5.1.1" xref="S3.SS3.p4.5.m5.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.5.m5.1b"><cn type="integer" id="S3.SS3.p4.5.m5.1.1.cmml" xref="S3.SS3.p4.5.m5.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.5.m5.1c">2</annotation></semantics></math> embeddings from the learnable [CLS] token for class <math id="S3.SS3.p4.6.m6.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS3.p4.6.m6.1a"><mi id="S3.SS3.p4.6.m6.1.1" xref="S3.SS3.p4.6.m6.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.6.m6.1b"><ci id="S3.SS3.p4.6.m6.1.1.cmml" xref="S3.SS3.p4.6.m6.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.6.m6.1c">c</annotation></semantics></math>.
Given a batch size <math id="S3.SS3.p4.7.m7.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S3.SS3.p4.7.m7.1a"><mi id="S3.SS3.p4.7.m7.1.1" xref="S3.SS3.p4.7.m7.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.7.m7.1b"><ci id="S3.SS3.p4.7.m7.1.1.cmml" xref="S3.SS3.p4.7.m7.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.7.m7.1c">B</annotation></semantics></math>, the InfoNCE contrastive loss ensures
sets formed from the same class have similar embeddings and those of different classes
are separated.
Once trained, the visual aggregator and visual encoder are frozen and
provide vision-based classifiers for categories in
<math id="S3.SS3.p4.8.m8.1" class="ltx_Math" alttext="\mathcal{C}^{\textsc{det}}\cup\mathcal{C}^{\textsc{img}}" display="inline"><semantics id="S3.SS3.p4.8.m8.1a"><mrow id="S3.SS3.p4.8.m8.1.1" xref="S3.SS3.p4.8.m8.1.1.cmml"><msup id="S3.SS3.p4.8.m8.1.1.2" xref="S3.SS3.p4.8.m8.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p4.8.m8.1.1.2.2" xref="S3.SS3.p4.8.m8.1.1.2.2.cmml">𝒞</mi><mtext class="ltx_font_smallcaps" id="S3.SS3.p4.8.m8.1.1.2.3" xref="S3.SS3.p4.8.m8.1.1.2.3a.cmml">det</mtext></msup><mo id="S3.SS3.p4.8.m8.1.1.1" xref="S3.SS3.p4.8.m8.1.1.1.cmml">∪</mo><msup id="S3.SS3.p4.8.m8.1.1.3" xref="S3.SS3.p4.8.m8.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p4.8.m8.1.1.3.2" xref="S3.SS3.p4.8.m8.1.1.3.2.cmml">𝒞</mi><mtext class="ltx_font_smallcaps" id="S3.SS3.p4.8.m8.1.1.3.3" xref="S3.SS3.p4.8.m8.1.1.3.3a.cmml">img</mtext></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.8.m8.1b"><apply id="S3.SS3.p4.8.m8.1.1.cmml" xref="S3.SS3.p4.8.m8.1.1"><union id="S3.SS3.p4.8.m8.1.1.1.cmml" xref="S3.SS3.p4.8.m8.1.1.1"></union><apply id="S3.SS3.p4.8.m8.1.1.2.cmml" xref="S3.SS3.p4.8.m8.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p4.8.m8.1.1.2.1.cmml" xref="S3.SS3.p4.8.m8.1.1.2">superscript</csymbol><ci id="S3.SS3.p4.8.m8.1.1.2.2.cmml" xref="S3.SS3.p4.8.m8.1.1.2.2">𝒞</ci><ci id="S3.SS3.p4.8.m8.1.1.2.3a.cmml" xref="S3.SS3.p4.8.m8.1.1.2.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS3.p4.8.m8.1.1.2.3.cmml" xref="S3.SS3.p4.8.m8.1.1.2.3">det</mtext></ci></apply><apply id="S3.SS3.p4.8.m8.1.1.3.cmml" xref="S3.SS3.p4.8.m8.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p4.8.m8.1.1.3.1.cmml" xref="S3.SS3.p4.8.m8.1.1.3">superscript</csymbol><ci id="S3.SS3.p4.8.m8.1.1.3.2.cmml" xref="S3.SS3.p4.8.m8.1.1.3.2">𝒞</ci><ci id="S3.SS3.p4.8.m8.1.1.3.3a.cmml" xref="S3.SS3.p4.8.m8.1.1.3.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS3.p4.8.m8.1.1.3.3.cmml" xref="S3.SS3.p4.8.m8.1.1.3.3">img</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.8.m8.1c">\mathcal{C}^{\textsc{det}}\cup\mathcal{C}^{\textsc{img}}</annotation></semantics></math>/<math id="S3.SS3.p4.9.m9.1" class="ltx_Math" alttext="\mathcal{C}^{\textsc{test}}" display="inline"><semantics id="S3.SS3.p4.9.m9.1a"><msup id="S3.SS3.p4.9.m9.1.1" xref="S3.SS3.p4.9.m9.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p4.9.m9.1.1.2" xref="S3.SS3.p4.9.m9.1.1.2.cmml">𝒞</mi><mtext class="ltx_font_smallcaps" id="S3.SS3.p4.9.m9.1.1.3" xref="S3.SS3.p4.9.m9.1.1.3a.cmml">test</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.9.m9.1b"><apply id="S3.SS3.p4.9.m9.1.1.cmml" xref="S3.SS3.p4.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.9.m9.1.1.1.cmml" xref="S3.SS3.p4.9.m9.1.1">superscript</csymbol><ci id="S3.SS3.p4.9.m9.1.1.2.cmml" xref="S3.SS3.p4.9.m9.1.1.2">𝒞</ci><ci id="S3.SS3.p4.9.m9.1.1.3a.cmml" xref="S3.SS3.p4.9.m9.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS3.p4.9.m9.1.1.3.cmml" xref="S3.SS3.p4.9.m9.1.1.3">test</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.9.m9.1c">\mathcal{C}^{\textsc{test}}</annotation></semantics></math> during detector training/testing.
Additional details on how the visual aggregator is trained are provided
in the Appendix (Section <a href="#A3" title="Appendix C Vision-based Classifier Pipeline Implementation Details ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>).</p>
</div>
<div id="S3.SS3.p5" class="ltx_para ltx_noindent">
<p id="S3.SS3.p5.1" class="ltx_p"><span id="S3.SS3.p5.1.1" class="ltx_text ltx_font_bold">Discussion.</span>
Using image exemplars for open-vocabulary detection may share some similarity to
few-shot object detection, however, there is a key distinction.
In few-shot object detection, the given “novel/rare” annotations (albeit few) are available for training,
<span id="S3.SS3.p5.1.2" class="ltx_ERROR undefined">\eg</span> recent works have found that finetuning a pre-trained object detector on few-shot detection data yields
the best results <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Wang et al.</span>, <a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>; <span class="ltx_text" style="font-size:90%;">Qiao et al.</span>, <a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>; <span class="ltx_text" style="font-size:90%;">Kaul et al.</span>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>,
while in open-vocabulary detection, there are no bounding box annotations
for “novel/rare” categories.
Image exemplars (<span id="S3.SS3.p5.1.3" class="ltx_ERROR undefined">\ie</span> the wholes image without bounding boxes) are used to specify the categories of interest;
we do not update any parameters based on
“novel/rare” category bounding box data,
unlike in few-shot object detection.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.1" class="ltx_p">During ablation experiments in Section <a href="#A1" title="Appendix A Ablation Studies ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>,
we compare the visual aggregator to a simple mean operator,
when obtaining a vision-based classifier from multiple image exemplar embeddings,
and show the benefit of the trained aggregator in this case.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Constructing Classifiers via Multi-Modal Fusion</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.4" class="ltx_p">To go one step further,
a natural extension to the aforementioned methods is to construct classifiers
from multi-modal cues;
intuitively, natural language descriptions and image exemplars may contain complementary information.
For a given class, <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mi id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">c</annotation></semantics></math>,
with text-based classifier, <math id="S3.SS4.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{w}_{\textsc{text}}^{c}" display="inline"><semantics id="S3.SS4.p1.2.m2.1a"><msubsup id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml"><mi id="S3.SS4.p1.2.m2.1.1.2.2" xref="S3.SS4.p1.2.m2.1.1.2.2.cmml">𝐰</mi><mtext class="ltx_font_smallcaps" id="S3.SS4.p1.2.m2.1.1.2.3" xref="S3.SS4.p1.2.m2.1.1.2.3a.cmml">text</mtext><mi id="S3.SS4.p1.2.m2.1.1.3" xref="S3.SS4.p1.2.m2.1.1.3.cmml">c</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><apply id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.2.m2.1.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1">superscript</csymbol><apply id="S3.SS4.p1.2.m2.1.1.2.cmml" xref="S3.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.2.m2.1.1.2.1.cmml" xref="S3.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p1.2.m2.1.1.2.2.cmml" xref="S3.SS4.p1.2.m2.1.1.2.2">𝐰</ci><ci id="S3.SS4.p1.2.m2.1.1.2.3a.cmml" xref="S3.SS4.p1.2.m2.1.1.2.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS4.p1.2.m2.1.1.2.3.cmml" xref="S3.SS4.p1.2.m2.1.1.2.3">text</mtext></ci></apply><ci id="S3.SS4.p1.2.m2.1.1.3.cmml" xref="S3.SS4.p1.2.m2.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">\mathbf{w}_{\textsc{text}}^{c}</annotation></semantics></math>,
and vision-based classifier, <math id="S3.SS4.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{w}_{\textsc{img}}^{c}" display="inline"><semantics id="S3.SS4.p1.3.m3.1a"><msubsup id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml"><mi id="S3.SS4.p1.3.m3.1.1.2.2" xref="S3.SS4.p1.3.m3.1.1.2.2.cmml">𝐰</mi><mtext class="ltx_font_smallcaps" id="S3.SS4.p1.3.m3.1.1.2.3" xref="S3.SS4.p1.3.m3.1.1.2.3a.cmml">img</mtext><mi id="S3.SS4.p1.3.m3.1.1.3" xref="S3.SS4.p1.3.m3.1.1.3.cmml">c</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.1b"><apply id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.3.m3.1.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1">superscript</csymbol><apply id="S3.SS4.p1.3.m3.1.1.2.cmml" xref="S3.SS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.3.m3.1.1.2.1.cmml" xref="S3.SS4.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS4.p1.3.m3.1.1.2.2.cmml" xref="S3.SS4.p1.3.m3.1.1.2.2">𝐰</ci><ci id="S3.SS4.p1.3.m3.1.1.2.3a.cmml" xref="S3.SS4.p1.3.m3.1.1.2.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS4.p1.3.m3.1.1.2.3.cmml" xref="S3.SS4.p1.3.m3.1.1.2.3">img</mtext></ci></apply><ci id="S3.SS4.p1.3.m3.1.1.3.cmml" xref="S3.SS4.p1.3.m3.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.1c">\mathbf{w}_{\textsc{img}}^{c}</annotation></semantics></math>,
the multi-modal classifier, <math id="S3.SS4.p1.4.m4.1" class="ltx_Math" alttext="\mathbf{w}_{\textsc{mm}}^{c}" display="inline"><semantics id="S3.SS4.p1.4.m4.1a"><msubsup id="S3.SS4.p1.4.m4.1.1" xref="S3.SS4.p1.4.m4.1.1.cmml"><mi id="S3.SS4.p1.4.m4.1.1.2.2" xref="S3.SS4.p1.4.m4.1.1.2.2.cmml">𝐰</mi><mtext class="ltx_font_smallcaps" id="S3.SS4.p1.4.m4.1.1.2.3" xref="S3.SS4.p1.4.m4.1.1.2.3a.cmml">mm</mtext><mi id="S3.SS4.p1.4.m4.1.1.3" xref="S3.SS4.p1.4.m4.1.1.3.cmml">c</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.4.m4.1b"><apply id="S3.SS4.p1.4.m4.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.4.m4.1.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1">superscript</csymbol><apply id="S3.SS4.p1.4.m4.1.1.2.cmml" xref="S3.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.4.m4.1.1.2.1.cmml" xref="S3.SS4.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS4.p1.4.m4.1.1.2.2.cmml" xref="S3.SS4.p1.4.m4.1.1.2.2">𝐰</ci><ci id="S3.SS4.p1.4.m4.1.1.2.3a.cmml" xref="S3.SS4.p1.4.m4.1.1.2.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.SS4.p1.4.m4.1.1.2.3.cmml" xref="S3.SS4.p1.4.m4.1.1.2.3">mm</mtext></ci></apply><ci id="S3.SS4.p1.4.m4.1.1.3.cmml" xref="S3.SS4.p1.4.m4.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.4.m4.1c">\mathbf{w}_{\textsc{mm}}^{c}</annotation></semantics></math>,
is computed by a simple fusion method based on addition:</p>
<table id="A9.EGx4" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E5.m1.2" class="ltx_Math" alttext="\displaystyle\mathbf{w}_{\textsc{mm}}^{c}=\frac{\mathbf{w}_{\textsc{text}}^{c}}{\lVert\mathbf{w}_{\textsc{text}}^{c}\rVert_{2}}+\frac{\mathbf{w}_{\textsc{img}}^{c}}{\lVert\mathbf{w}_{\textsc{img}}^{c}\rVert_{2}}" display="inline"><semantics id="S3.E5.m1.2a"><mrow id="S3.E5.m1.2.3" xref="S3.E5.m1.2.3.cmml"><msubsup id="S3.E5.m1.2.3.2" xref="S3.E5.m1.2.3.2.cmml"><mi id="S3.E5.m1.2.3.2.2.2" xref="S3.E5.m1.2.3.2.2.2.cmml">𝐰</mi><mtext class="ltx_font_smallcaps" id="S3.E5.m1.2.3.2.2.3" xref="S3.E5.m1.2.3.2.2.3a.cmml">mm</mtext><mi id="S3.E5.m1.2.3.2.3" xref="S3.E5.m1.2.3.2.3.cmml">c</mi></msubsup><mo id="S3.E5.m1.2.3.1" xref="S3.E5.m1.2.3.1.cmml">=</mo><mrow id="S3.E5.m1.2.3.3" xref="S3.E5.m1.2.3.3.cmml"><mstyle displaystyle="true" id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml"><mfrac id="S3.E5.m1.1.1a" xref="S3.E5.m1.1.1.cmml"><msubsup id="S3.E5.m1.1.1.3" xref="S3.E5.m1.1.1.3.cmml"><mi id="S3.E5.m1.1.1.3.2.2" xref="S3.E5.m1.1.1.3.2.2.cmml">𝐰</mi><mtext class="ltx_font_smallcaps" id="S3.E5.m1.1.1.3.2.3" xref="S3.E5.m1.1.1.3.2.3a.cmml">text</mtext><mi id="S3.E5.m1.1.1.3.3" xref="S3.E5.m1.1.1.3.3.cmml">c</mi></msubsup><msub id="S3.E5.m1.1.1.1" xref="S3.E5.m1.1.1.1.cmml"><mrow id="S3.E5.m1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.2.cmml"><mo fence="true" rspace="0em" id="S3.E5.m1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.2.1.cmml">∥</mo><msubsup id="S3.E5.m1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.2.2" xref="S3.E5.m1.1.1.1.1.1.1.2.2.cmml">𝐰</mi><mtext class="ltx_font_smallcaps" id="S3.E5.m1.1.1.1.1.1.1.2.3" xref="S3.E5.m1.1.1.1.1.1.1.2.3a.cmml">text</mtext><mi id="S3.E5.m1.1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.3.cmml">c</mi></msubsup><mo fence="true" lspace="0em" id="S3.E5.m1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.2.1.cmml">∥</mo></mrow><mn id="S3.E5.m1.1.1.1.3" xref="S3.E5.m1.1.1.1.3.cmml">2</mn></msub></mfrac></mstyle><mo id="S3.E5.m1.2.3.3.1" xref="S3.E5.m1.2.3.3.1.cmml">+</mo><mstyle displaystyle="true" id="S3.E5.m1.2.2" xref="S3.E5.m1.2.2.cmml"><mfrac id="S3.E5.m1.2.2a" xref="S3.E5.m1.2.2.cmml"><msubsup id="S3.E5.m1.2.2.3" xref="S3.E5.m1.2.2.3.cmml"><mi id="S3.E5.m1.2.2.3.2.2" xref="S3.E5.m1.2.2.3.2.2.cmml">𝐰</mi><mtext class="ltx_font_smallcaps" id="S3.E5.m1.2.2.3.2.3" xref="S3.E5.m1.2.2.3.2.3a.cmml">img</mtext><mi id="S3.E5.m1.2.2.3.3" xref="S3.E5.m1.2.2.3.3.cmml">c</mi></msubsup><msub id="S3.E5.m1.2.2.1" xref="S3.E5.m1.2.2.1.cmml"><mrow id="S3.E5.m1.2.2.1.1.1" xref="S3.E5.m1.2.2.1.1.2.cmml"><mo fence="true" rspace="0em" id="S3.E5.m1.2.2.1.1.1.2" xref="S3.E5.m1.2.2.1.1.2.1.cmml">∥</mo><msubsup id="S3.E5.m1.2.2.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.cmml"><mi id="S3.E5.m1.2.2.1.1.1.1.2.2" xref="S3.E5.m1.2.2.1.1.1.1.2.2.cmml">𝐰</mi><mtext class="ltx_font_smallcaps" id="S3.E5.m1.2.2.1.1.1.1.2.3" xref="S3.E5.m1.2.2.1.1.1.1.2.3a.cmml">img</mtext><mi id="S3.E5.m1.2.2.1.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.1.3.cmml">c</mi></msubsup><mo fence="true" lspace="0em" id="S3.E5.m1.2.2.1.1.1.3" xref="S3.E5.m1.2.2.1.1.2.1.cmml">∥</mo></mrow><mn id="S3.E5.m1.2.2.1.3" xref="S3.E5.m1.2.2.1.3.cmml">2</mn></msub></mfrac></mstyle></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.2b"><apply id="S3.E5.m1.2.3.cmml" xref="S3.E5.m1.2.3"><eq id="S3.E5.m1.2.3.1.cmml" xref="S3.E5.m1.2.3.1"></eq><apply id="S3.E5.m1.2.3.2.cmml" xref="S3.E5.m1.2.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.2.3.2.1.cmml" xref="S3.E5.m1.2.3.2">superscript</csymbol><apply id="S3.E5.m1.2.3.2.2.cmml" xref="S3.E5.m1.2.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.2.3.2.2.1.cmml" xref="S3.E5.m1.2.3.2">subscript</csymbol><ci id="S3.E5.m1.2.3.2.2.2.cmml" xref="S3.E5.m1.2.3.2.2.2">𝐰</ci><ci id="S3.E5.m1.2.3.2.2.3a.cmml" xref="S3.E5.m1.2.3.2.2.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.E5.m1.2.3.2.2.3.cmml" xref="S3.E5.m1.2.3.2.2.3">mm</mtext></ci></apply><ci id="S3.E5.m1.2.3.2.3.cmml" xref="S3.E5.m1.2.3.2.3">𝑐</ci></apply><apply id="S3.E5.m1.2.3.3.cmml" xref="S3.E5.m1.2.3.3"><plus id="S3.E5.m1.2.3.3.1.cmml" xref="S3.E5.m1.2.3.3.1"></plus><apply id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1"><divide id="S3.E5.m1.1.1.2.cmml" xref="S3.E5.m1.1.1"></divide><apply id="S3.E5.m1.1.1.3.cmml" xref="S3.E5.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.3">superscript</csymbol><apply id="S3.E5.m1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.3.2.1.cmml" xref="S3.E5.m1.1.1.3">subscript</csymbol><ci id="S3.E5.m1.1.1.3.2.2.cmml" xref="S3.E5.m1.1.1.3.2.2">𝐰</ci><ci id="S3.E5.m1.1.1.3.2.3a.cmml" xref="S3.E5.m1.1.1.3.2.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.E5.m1.1.1.3.2.3.cmml" xref="S3.E5.m1.1.1.3.2.3">text</mtext></ci></apply><ci id="S3.E5.m1.1.1.3.3.cmml" xref="S3.E5.m1.1.1.3.3">𝑐</ci></apply><apply id="S3.E5.m1.1.1.1.cmml" xref="S3.E5.m1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1">subscript</csymbol><apply id="S3.E5.m1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E5.m1.1.1.1.1.2.1.cmml" xref="S3.E5.m1.1.1.1.1.1.2">delimited-∥∥</csymbol><apply id="S3.E5.m1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E5.m1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2.2">𝐰</ci><ci id="S3.E5.m1.1.1.1.1.1.1.2.3a.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.E5.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2.3">text</mtext></ci></apply><ci id="S3.E5.m1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3">𝑐</ci></apply></apply><cn type="integer" id="S3.E5.m1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.3">2</cn></apply></apply><apply id="S3.E5.m1.2.2.cmml" xref="S3.E5.m1.2.2"><divide id="S3.E5.m1.2.2.2.cmml" xref="S3.E5.m1.2.2"></divide><apply id="S3.E5.m1.2.2.3.cmml" xref="S3.E5.m1.2.2.3"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.3.1.cmml" xref="S3.E5.m1.2.2.3">superscript</csymbol><apply id="S3.E5.m1.2.2.3.2.cmml" xref="S3.E5.m1.2.2.3"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.3.2.1.cmml" xref="S3.E5.m1.2.2.3">subscript</csymbol><ci id="S3.E5.m1.2.2.3.2.2.cmml" xref="S3.E5.m1.2.2.3.2.2">𝐰</ci><ci id="S3.E5.m1.2.2.3.2.3a.cmml" xref="S3.E5.m1.2.2.3.2.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.E5.m1.2.2.3.2.3.cmml" xref="S3.E5.m1.2.2.3.2.3">img</mtext></ci></apply><ci id="S3.E5.m1.2.2.3.3.cmml" xref="S3.E5.m1.2.2.3.3">𝑐</ci></apply><apply id="S3.E5.m1.2.2.1.cmml" xref="S3.E5.m1.2.2.1"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.2.cmml" xref="S3.E5.m1.2.2.1">subscript</csymbol><apply id="S3.E5.m1.2.2.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1"><csymbol cd="latexml" id="S3.E5.m1.2.2.1.1.2.1.cmml" xref="S3.E5.m1.2.2.1.1.1.2">delimited-∥∥</csymbol><apply id="S3.E5.m1.2.2.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1">superscript</csymbol><apply id="S3.E5.m1.2.2.1.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.1.1.2.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1">subscript</csymbol><ci id="S3.E5.m1.2.2.1.1.1.1.2.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.2.2">𝐰</ci><ci id="S3.E5.m1.2.2.1.1.1.1.2.3a.cmml" xref="S3.E5.m1.2.2.1.1.1.1.2.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S3.E5.m1.2.2.1.1.1.1.2.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.2.3">img</mtext></ci></apply><ci id="S3.E5.m1.2.2.1.1.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.3">𝑐</ci></apply></apply><cn type="integer" id="S3.E5.m1.2.2.1.3.cmml" xref="S3.E5.m1.2.2.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.2c">\displaystyle\mathbf{w}_{\textsc{mm}}^{c}=\frac{\mathbf{w}_{\textsc{text}}^{c}}{\lVert\mathbf{w}_{\textsc{text}}^{c}\rVert_{2}}+\frac{\mathbf{w}_{\textsc{img}}^{c}}{\lVert\mathbf{w}_{\textsc{img}}^{c}\rVert_{2}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> demonstrates our entire pipeline for generating
text-based, vision-based and multi-modal classifiers, showing how any of
the three classifiers can be used with an open-vocabulary detector to detect a
“falcon”.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para ltx_noindent">
<p id="S3.SS4.p3.1" class="ltx_p"><span id="S3.SS4.p3.1.1" class="ltx_text ltx_font_bold">Discussion.</span>
Section <a href="#S3.SS3" title="3.3 Vision-based Classifiers from Image Exemplars ‣ 3 Method ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> provides details of the visual aggregator,
which yields our vision-based classifiers, but for multi-modal classifiers
we simply compute the vector sum of our <math id="S3.SS4.p3.1.m1.1" class="ltx_Math" alttext="l^{2}" display="inline"><semantics id="S3.SS4.p3.1.m1.1a"><msup id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml"><mi id="S3.SS4.p3.1.m1.1.1.2" xref="S3.SS4.p3.1.m1.1.1.2.cmml">l</mi><mn id="S3.SS4.p3.1.m1.1.1.3" xref="S3.SS4.p3.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><apply id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.1.m1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">superscript</csymbol><ci id="S3.SS4.p3.1.m1.1.1.2.cmml" xref="S3.SS4.p3.1.m1.1.1.2">𝑙</ci><cn type="integer" id="S3.SS4.p3.1.m1.1.1.3.cmml" xref="S3.SS4.p3.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">l^{2}</annotation></semantics></math>-normalised text-based and vision-based classifiers. We investigated using a unified multi-modal aggregator which ingests both text and visual embeddings, sourced from class descriptions and image exemplars, respectively. Such a model did not generate good multi-modal classifiers for OVOD
— distinguishing between sets of text and image embeddings for different classes
becomes trivial as the text embeddings alone are sufficient to solve
the contrastive learning task, thereby ignoring the visual embeddings
altogether. Attempts to modify the training for a unified multi-modal aggregator by using Dropout <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Srivastava et al.</span>, <a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2014</span></a>)</cite> on the text embeddings were not fruitful.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section,
we first introduce the standard dataset and benchmark used
in the literature <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gu et al.</span>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Zhou et al.</span>, <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Feng et al.</span>, <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>.
Section <a href="#S4.SS2" title="4.2 Implementation Details ‣ 4 Experiments ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> provides implementation and training details for our OVOD models,
which use classifiers constructed from natural language descriptions, visual exemplars or the combination of both.
We compare our models with existing works in Section <a href="#S4.SS3" title="4.3 Open-Vocabulary Detection Results ‣ 4 Experiments ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>,
demonstrating state-of-the-art performance.
Additionally, Section <a href="#S4.SS3" title="4.3 Open-Vocabulary Detection Results ‣ 4 Experiments ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a> provides results for cross-dataset transfer.
Section <a href="#S4.SS4" title="4.4 Ablation Study ‣ 4 Experiments ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a> provides an ablation study regarding our design choices.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets and Evaluation Protocol</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Standard LVIS Benchmark.</span>
In this work, most experiments are based on the LVIS object detection dataset <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gupta et al.</span>, <a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>,
containing a large vocabulary and a long-tailed distribution of object instances.
Specifically, the LVIS dataset contains class, bounding box and mask annotations for
1203 classes across 100k images in the MS-COCO dataset <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Lin et al.</span>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2014</span></a>)</cite>.
Annotations are collected in a federated manner,
<span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_italic">i.e.</span> manual annotations for a given image are not necessarily exhaustive.
The classes are divided into three sets — rare, common and frequent
— based on the number of training images containing a given class.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.4" class="ltx_p"><span id="S4.SS1.p2.4.1" class="ltx_text ltx_font_bold">Training Datasets.</span>
To develop open-vocabulary object detectors,
we follow the same setting as proposed in ViLD <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gu et al.</span>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite> and used in Detic <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhou et al.</span>, <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>.
Specifically, the original LVIS training set (<em id="S4.SS1.p2.4.2" class="ltx_emph ltx_font_italic">LVIS-all</em>) is slightly changed by removing all annotations belonging to the “rare” categories.
This removes the annotations of 317 rare classes <em id="S4.SS1.p2.4.3" class="ltx_emph ltx_font_italic">but not</em> the associated images,
in other words, objects belonging to rare categories appear in the training set but are unannotated.
This subset of LVIS training data containing only “common” and “frequent”
annotations is referred to as <em id="S4.SS1.p2.4.4" class="ltx_emph ltx_font_italic">LVIS-base</em>.
LVIS-base serves as <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{D}^{\textsc{det}}" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><msup id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">𝒟</mi><mtext class="ltx_font_smallcaps" id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3a.cmml">det</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">superscript</csymbol><ci id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">𝒟</ci><ci id="S4.SS1.p2.1.m1.1.1.3a.cmml" xref="S4.SS1.p2.1.m1.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3">det</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">\mathcal{D}^{\textsc{det}}</annotation></semantics></math> using notation from Section <a href="#S3.SS1" title="3.1 Preliminaries ‣ 3 Method ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>,
unless stated otherwise.
When using image classification data, <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="\mathcal{D}^{\textsc{img}}" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><msup id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p2.2.m2.1.1.2" xref="S4.SS1.p2.2.m2.1.1.2.cmml">𝒟</mi><mtext class="ltx_font_smallcaps" id="S4.SS1.p2.2.m2.1.1.3" xref="S4.SS1.p2.2.m2.1.1.3a.cmml">img</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><apply id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.2.m2.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">superscript</csymbol><ci id="S4.SS1.p2.2.m2.1.1.2.cmml" xref="S4.SS1.p2.2.m2.1.1.2">𝒟</ci><ci id="S4.SS1.p2.2.m2.1.1.3a.cmml" xref="S4.SS1.p2.2.m2.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S4.SS1.p2.2.m2.1.1.3.cmml" xref="S4.SS1.p2.2.m2.1.1.3">img</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">\mathcal{D}^{\textsc{img}}</annotation></semantics></math>, as extra weak supervision,
we use the subset of categories in ImageNet-21K <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Deng et al.</span>, <a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2009</span></a>)</cite> that overlap with
the LVIS vocabulary and denote this subset as <em id="S4.SS1.p2.4.5" class="ltx_emph ltx_font_italic">IN-L</em>, as in Detic.
IN-L covers <math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="997" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><mn id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml">997</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><cn type="integer" id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">997</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">997</annotation></semantics></math> of the <math id="S4.SS1.p2.4.m4.1" class="ltx_Math" alttext="1203" display="inline"><semantics id="S4.SS1.p2.4.m4.1a"><mn id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml">1203</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><cn type="integer" id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1">1203</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">1203</annotation></semantics></math> classes in the LVIS vocabulary.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Evaluation Protocol. </span>
For evaluation, previous work evaluates OVOD models on the LVIS validation set (<em id="S4.SS1.p3.1.2" class="ltx_emph ltx_font_italic">LVIS-val</em>) for all categories
— treating “rare” classes as novel categories
as it is guaranteed that no groundtruth box annotations whatsoever are provided at the training stage.
The main evaluation metric is the standard mask AP metric averaged over the “rare” classes and
is denoted as APr.
The mask AP averaged across <em id="S4.SS1.p3.1.3" class="ltx_emph ltx_font_italic">all</em> classes is also reported,
indicating overall class performance and is denoted as mAP.
The latter metric is an important consideration as a good model should
improve both APr and mAP; a model should not improve APr at the cost of
worse performance in terms of mAP.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation Details</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">Object Detector Architecture.</span>
The architecture we use is almost identical to that in Detic,
using the CenterNet2 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhou et al.</span>, <a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite> model with a ResNet-50 backbone <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">He et al.</span>, <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>)</cite>
pre-trained on ImageNet-21k-P <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Ridnik et al.</span>, <a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>.
In addition to exploring different ways for constructing classifiers (<math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="\Phi_{\textsc{cls}}" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><msub id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi mathvariant="normal" id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">Φ</mi><mtext class="ltx_font_smallcaps" id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3a.cmml">cls</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">Φ</ci><ci id="S4.SS2.p1.1.m1.1.1.3a.cmml" xref="S4.SS2.p1.1.m1.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">cls</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\Phi_{\textsc{cls}}</annotation></semantics></math>),
as described in Section <a href="#S3" title="3 Method ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>,
we also add a learnable bias before generating
final confidence scores,
the effect of this bias term is investigated in the Appendix (Section <a href="#A1" title="Appendix A Ablation Studies ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>).</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.15" class="ltx_p"><span id="S4.SS2.p2.15.1" class="ltx_text ltx_font_bold">Detector Training.</span>
The training recipe is the same as Detic for fair comparison,
using Federated Loss <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhou et al.</span>, <a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite> and repeat factor sampling <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gupta et al.</span>, <a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>.
While training our OVOD model on detection data only, <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{D}^{\textsc{det}}" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><msup id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">𝒟</mi><mtext class="ltx_font_smallcaps" id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3a.cmml">det</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">superscript</csymbol><ci id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">𝒟</ci><ci id="S4.SS2.p2.1.m1.1.1.3a.cmml" xref="S4.SS2.p2.1.m1.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S4.SS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3">det</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">\mathcal{D}^{\textsc{det}}</annotation></semantics></math>,
we use a <math id="S4.SS2.p2.2.m2.1" class="ltx_math_unparsed" alttext="4\times" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mrow id="S4.SS2.p2.2.m2.1b"><mn id="S4.SS2.p2.2.m2.1.1">4</mn><mo lspace="0.222em" id="S4.SS2.p2.2.m2.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">4\times</annotation></semantics></math> schedule (<math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><mo id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><csymbol cd="latexml" id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">\sim</annotation></semantics></math>58 <em id="S4.SS2.p2.15.2" class="ltx_emph ltx_font_italic">LVIS-base</em> epochs or 90k iterations with batch size of 64).
When using additional image-labelled data (IN-L),
we train jointly on <math id="S4.SS2.p2.4.m4.1" class="ltx_Math" alttext="\mathcal{D}^{\textsc{det}}\cup\mathcal{D}^{\textsc{img}}" display="inline"><semantics id="S4.SS2.p2.4.m4.1a"><mrow id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml"><msup id="S4.SS2.p2.4.m4.1.1.2" xref="S4.SS2.p2.4.m4.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p2.4.m4.1.1.2.2" xref="S4.SS2.p2.4.m4.1.1.2.2.cmml">𝒟</mi><mtext class="ltx_font_smallcaps" id="S4.SS2.p2.4.m4.1.1.2.3" xref="S4.SS2.p2.4.m4.1.1.2.3a.cmml">det</mtext></msup><mo id="S4.SS2.p2.4.m4.1.1.1" xref="S4.SS2.p2.4.m4.1.1.1.cmml">∪</mo><msup id="S4.SS2.p2.4.m4.1.1.3" xref="S4.SS2.p2.4.m4.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p2.4.m4.1.1.3.2" xref="S4.SS2.p2.4.m4.1.1.3.2.cmml">𝒟</mi><mtext class="ltx_font_smallcaps" id="S4.SS2.p2.4.m4.1.1.3.3" xref="S4.SS2.p2.4.m4.1.1.3.3a.cmml">img</mtext></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><apply id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1"><union id="S4.SS2.p2.4.m4.1.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1.1"></union><apply id="S4.SS2.p2.4.m4.1.1.2.cmml" xref="S4.SS2.p2.4.m4.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p2.4.m4.1.1.2.1.cmml" xref="S4.SS2.p2.4.m4.1.1.2">superscript</csymbol><ci id="S4.SS2.p2.4.m4.1.1.2.2.cmml" xref="S4.SS2.p2.4.m4.1.1.2.2">𝒟</ci><ci id="S4.SS2.p2.4.m4.1.1.2.3a.cmml" xref="S4.SS2.p2.4.m4.1.1.2.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S4.SS2.p2.4.m4.1.1.2.3.cmml" xref="S4.SS2.p2.4.m4.1.1.2.3">det</mtext></ci></apply><apply id="S4.SS2.p2.4.m4.1.1.3.cmml" xref="S4.SS2.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p2.4.m4.1.1.3.1.cmml" xref="S4.SS2.p2.4.m4.1.1.3">superscript</csymbol><ci id="S4.SS2.p2.4.m4.1.1.3.2.cmml" xref="S4.SS2.p2.4.m4.1.1.3.2">𝒟</ci><ci id="S4.SS2.p2.4.m4.1.1.3.3a.cmml" xref="S4.SS2.p2.4.m4.1.1.3.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S4.SS2.p2.4.m4.1.1.3.3.cmml" xref="S4.SS2.p2.4.m4.1.1.3.3">img</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">\mathcal{D}^{\textsc{det}}\cup\mathcal{D}^{\textsc{img}}</annotation></semantics></math> using a <math id="S4.SS2.p2.5.m5.1" class="ltx_math_unparsed" alttext="4\times" display="inline"><semantics id="S4.SS2.p2.5.m5.1a"><mrow id="S4.SS2.p2.5.m5.1b"><mn id="S4.SS2.p2.5.m5.1.1">4</mn><mo lspace="0.222em" id="S4.SS2.p2.5.m5.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.1c">4\times</annotation></semantics></math> schedule
(90k iterations) with a sampling ratio of <math id="S4.SS2.p2.6.m6.1" class="ltx_Math" alttext="1:4" display="inline"><semantics id="S4.SS2.p2.6.m6.1a"><mrow id="S4.SS2.p2.6.m6.1.1" xref="S4.SS2.p2.6.m6.1.1.cmml"><mn id="S4.SS2.p2.6.m6.1.1.2" xref="S4.SS2.p2.6.m6.1.1.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S4.SS2.p2.6.m6.1.1.1" xref="S4.SS2.p2.6.m6.1.1.1.cmml">:</mo><mn id="S4.SS2.p2.6.m6.1.1.3" xref="S4.SS2.p2.6.m6.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.6.m6.1b"><apply id="S4.SS2.p2.6.m6.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1"><ci id="S4.SS2.p2.6.m6.1.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1.1">:</ci><cn type="integer" id="S4.SS2.p2.6.m6.1.1.2.cmml" xref="S4.SS2.p2.6.m6.1.1.2">1</cn><cn type="integer" id="S4.SS2.p2.6.m6.1.1.3.cmml" xref="S4.SS2.p2.6.m6.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.6.m6.1c">1:4</annotation></semantics></math> and batch sizes of <math id="S4.SS2.p2.7.m7.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S4.SS2.p2.7.m7.1a"><mn id="S4.SS2.p2.7.m7.1.1" xref="S4.SS2.p2.7.m7.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.7.m7.1b"><cn type="integer" id="S4.SS2.p2.7.m7.1.1.cmml" xref="S4.SS2.p2.7.m7.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.7.m7.1c">64</annotation></semantics></math> and
<math id="S4.SS2.p2.8.m8.1" class="ltx_Math" alttext="256" display="inline"><semantics id="S4.SS2.p2.8.m8.1a"><mn id="S4.SS2.p2.8.m8.1.1" xref="S4.SS2.p2.8.m8.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.8.m8.1b"><cn type="integer" id="S4.SS2.p2.8.m8.1.1.cmml" xref="S4.SS2.p2.8.m8.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.8.m8.1c">256</annotation></semantics></math>, respectively.
This results in <math id="S4.SS2.p2.9.m9.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS2.p2.9.m9.1a"><mo id="S4.SS2.p2.9.m9.1.1" xref="S4.SS2.p2.9.m9.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.9.m9.1b"><csymbol cd="latexml" id="S4.SS2.p2.9.m9.1.1.cmml" xref="S4.SS2.p2.9.m9.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.9.m9.1c">\sim</annotation></semantics></math>15 <em id="S4.SS2.p2.15.3" class="ltx_emph ltx_font_italic">IN-L</em> epochs and an additional
<math id="S4.SS2.p2.10.m10.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS2.p2.10.m10.1a"><mo id="S4.SS2.p2.10.m10.1.1" xref="S4.SS2.p2.10.m10.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.10.m10.1b"><csymbol cd="latexml" id="S4.SS2.p2.10.m10.1.1.cmml" xref="S4.SS2.p2.10.m10.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.10.m10.1c">\sim</annotation></semantics></math>11 <em id="S4.SS2.p2.15.4" class="ltx_emph ltx_font_italic">LVIS-base</em> epochs.
For mini-batches containing images from <math id="S4.SS2.p2.11.m11.1" class="ltx_Math" alttext="\mathcal{D}^{\textsc{det}}" display="inline"><semantics id="S4.SS2.p2.11.m11.1a"><msup id="S4.SS2.p2.11.m11.1.1" xref="S4.SS2.p2.11.m11.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p2.11.m11.1.1.2" xref="S4.SS2.p2.11.m11.1.1.2.cmml">𝒟</mi><mtext class="ltx_font_smallcaps" id="S4.SS2.p2.11.m11.1.1.3" xref="S4.SS2.p2.11.m11.1.1.3a.cmml">det</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.11.m11.1b"><apply id="S4.SS2.p2.11.m11.1.1.cmml" xref="S4.SS2.p2.11.m11.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.11.m11.1.1.1.cmml" xref="S4.SS2.p2.11.m11.1.1">superscript</csymbol><ci id="S4.SS2.p2.11.m11.1.1.2.cmml" xref="S4.SS2.p2.11.m11.1.1.2">𝒟</ci><ci id="S4.SS2.p2.11.m11.1.1.3a.cmml" xref="S4.SS2.p2.11.m11.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S4.SS2.p2.11.m11.1.1.3.cmml" xref="S4.SS2.p2.11.m11.1.1.3">det</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.11.m11.1c">\mathcal{D}^{\textsc{det}}</annotation></semantics></math> and
<math id="S4.SS2.p2.12.m12.1" class="ltx_Math" alttext="\mathcal{D}^{\textsc{img}}" display="inline"><semantics id="S4.SS2.p2.12.m12.1a"><msup id="S4.SS2.p2.12.m12.1.1" xref="S4.SS2.p2.12.m12.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p2.12.m12.1.1.2" xref="S4.SS2.p2.12.m12.1.1.2.cmml">𝒟</mi><mtext class="ltx_font_smallcaps" id="S4.SS2.p2.12.m12.1.1.3" xref="S4.SS2.p2.12.m12.1.1.3a.cmml">img</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.12.m12.1b"><apply id="S4.SS2.p2.12.m12.1.1.cmml" xref="S4.SS2.p2.12.m12.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.12.m12.1.1.1.cmml" xref="S4.SS2.p2.12.m12.1.1">superscript</csymbol><ci id="S4.SS2.p2.12.m12.1.1.2.cmml" xref="S4.SS2.p2.12.m12.1.1.2">𝒟</ci><ci id="S4.SS2.p2.12.m12.1.1.3a.cmml" xref="S4.SS2.p2.12.m12.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S4.SS2.p2.12.m12.1.1.3.cmml" xref="S4.SS2.p2.12.m12.1.1.3">img</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.12.m12.1c">\mathcal{D}^{\textsc{img}}</annotation></semantics></math> we use input resolutions of <math id="S4.SS2.p2.13.m13.1" class="ltx_Math" alttext="640^{2}" display="inline"><semantics id="S4.SS2.p2.13.m13.1a"><msup id="S4.SS2.p2.13.m13.1.1" xref="S4.SS2.p2.13.m13.1.1.cmml"><mn id="S4.SS2.p2.13.m13.1.1.2" xref="S4.SS2.p2.13.m13.1.1.2.cmml">640</mn><mn id="S4.SS2.p2.13.m13.1.1.3" xref="S4.SS2.p2.13.m13.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.13.m13.1b"><apply id="S4.SS2.p2.13.m13.1.1.cmml" xref="S4.SS2.p2.13.m13.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.13.m13.1.1.1.cmml" xref="S4.SS2.p2.13.m13.1.1">superscript</csymbol><cn type="integer" id="S4.SS2.p2.13.m13.1.1.2.cmml" xref="S4.SS2.p2.13.m13.1.1.2">640</cn><cn type="integer" id="S4.SS2.p2.13.m13.1.1.3.cmml" xref="S4.SS2.p2.13.m13.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.13.m13.1c">640^{2}</annotation></semantics></math> and <math id="S4.SS2.p2.14.m14.1" class="ltx_Math" alttext="320^{2}" display="inline"><semantics id="S4.SS2.p2.14.m14.1a"><msup id="S4.SS2.p2.14.m14.1.1" xref="S4.SS2.p2.14.m14.1.1.cmml"><mn id="S4.SS2.p2.14.m14.1.1.2" xref="S4.SS2.p2.14.m14.1.1.2.cmml">320</mn><mn id="S4.SS2.p2.14.m14.1.1.3" xref="S4.SS2.p2.14.m14.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.14.m14.1b"><apply id="S4.SS2.p2.14.m14.1.1.cmml" xref="S4.SS2.p2.14.m14.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.14.m14.1.1.1.cmml" xref="S4.SS2.p2.14.m14.1.1">superscript</csymbol><cn type="integer" id="S4.SS2.p2.14.m14.1.1.2.cmml" xref="S4.SS2.p2.14.m14.1.1.2">320</cn><cn type="integer" id="S4.SS2.p2.14.m14.1.1.3.cmml" xref="S4.SS2.p2.14.m14.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.14.m14.1c">320^{2}</annotation></semantics></math>, respectively.
We conduct our experiments on <math id="S4.SS2.p2.15.m15.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S4.SS2.p2.15.m15.1a"><mn id="S4.SS2.p2.15.m15.1.1" xref="S4.SS2.p2.15.m15.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.15.m15.1b"><cn type="integer" id="S4.SS2.p2.15.m15.1.1.cmml" xref="S4.SS2.p2.15.m15.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.15.m15.1c">4</annotation></semantics></math> 32GB V100 GPUs.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">For image-labelled data, <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{D}^{\textsc{img}}" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><msup id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p3.1.m1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.2.cmml">𝒟</mi><mtext class="ltx_font_smallcaps" id="S4.SS2.p3.1.m1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.3a.cmml">img</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">superscript</csymbol><ci id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2">𝒟</ci><ci id="S4.SS2.p3.1.m1.1.1.3a.cmml" xref="S4.SS2.p3.1.m1.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S4.SS2.p3.1.m1.1.1.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3">img</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">\mathcal{D}^{\textsc{img}}</annotation></semantics></math>,
an image with class label is given,
but no groundtruth bounding box is available.
Following Detic, the largest class-agnostic box proposal is used to produce an RoI feature for the given image, enabling detector training.
See the Detic paper for more details.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para ltx_noindent">
<p id="S4.SS2.p4.1" class="ltx_p"><span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_bold">Text-based Classifier Construction.</span>
To generate plain text class descriptions we use the GPT-3 DaVinci-002 model
available from OpenAI.
For each class in LVIS,
we generate <math id="S4.SS2.p4.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.SS2.p4.1.m1.1a"><mn id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><cn type="integer" id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">10</annotation></semantics></math> descriptions and compute the classifier
with the text encoder from a CLIP ViT-B/32 model <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Radford et al.</span>, <a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>,
as detailed in Section <a href="#S3.SS2" title="3.2 Text-based Classifiers from Language Descriptions ‣ 3 Method ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.
We follow the standard method from CLIP
and use the output embedding corresponding to the final token in the input text.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para ltx_noindent">
<p id="S4.SS2.p5.2" class="ltx_p"><span id="S4.SS2.p5.2.1" class="ltx_text ltx_font_bold">Vision-based Classifier Construction.</span>
The visual aggregator, detailed in Section <a href="#S3.SS3" title="3.3 Vision-based Classifiers from Image Exemplars ‣ 3 Method ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>,
should be general and not specific to any class vocabulary.
To fulfil this goal,
we use the curated ImageNet-21-P dataset <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Ridnik et al.</span>, <a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite> as training data <em id="S4.SS2.p5.2.2" class="ltx_emph ltx_font_italic">for the aggregator</em>.
This dataset, designed for pre-training visual backbones,
filters out classes with few examples from the original ImageNet-21k <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Deng et al.</span>, <a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2009</span></a>)</cite> dataset,
leaving <math id="S4.SS2.p5.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS2.p5.1.m1.1a"><mo id="S4.SS2.p5.1.m1.1.1" xref="S4.SS2.p5.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.1.m1.1b"><csymbol cd="latexml" id="S4.SS2.p5.1.m1.1.1.cmml" xref="S4.SS2.p5.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.1.m1.1c">\sim</annotation></semantics></math>11M images across <math id="S4.SS2.p5.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS2.p5.2.m2.1a"><mo id="S4.SS2.p5.2.m2.1.1" xref="S4.SS2.p5.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.2.m2.1b"><csymbol cd="latexml" id="S4.SS2.p5.2.m2.1.1.cmml" xref="S4.SS2.p5.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.2.m2.1c">\sim</annotation></semantics></math>11K classes.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.4" class="ltx_p">To generate a visual embedding from a given image exemplar,
we use a CLIP ViT-B/32 visual encoder.
For the aggregator, we use <math id="S4.SS2.p6.1.m1.1" class="ltx_Math" alttext="N=4" display="inline"><semantics id="S4.SS2.p6.1.m1.1a"><mrow id="S4.SS2.p6.1.m1.1.1" xref="S4.SS2.p6.1.m1.1.1.cmml"><mi id="S4.SS2.p6.1.m1.1.1.2" xref="S4.SS2.p6.1.m1.1.1.2.cmml">N</mi><mo id="S4.SS2.p6.1.m1.1.1.1" xref="S4.SS2.p6.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.p6.1.m1.1.1.3" xref="S4.SS2.p6.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.1.m1.1b"><apply id="S4.SS2.p6.1.m1.1.1.cmml" xref="S4.SS2.p6.1.m1.1.1"><eq id="S4.SS2.p6.1.m1.1.1.1.cmml" xref="S4.SS2.p6.1.m1.1.1.1"></eq><ci id="S4.SS2.p6.1.m1.1.1.2.cmml" xref="S4.SS2.p6.1.m1.1.1.2">𝑁</ci><cn type="integer" id="S4.SS2.p6.1.m1.1.1.3.cmml" xref="S4.SS2.p6.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.1.m1.1c">N=4</annotation></semantics></math> transformer blocks, with dimension <math id="S4.SS2.p6.2.m2.1" class="ltx_Math" alttext="512" display="inline"><semantics id="S4.SS2.p6.2.m2.1a"><mn id="S4.SS2.p6.2.m2.1.1" xref="S4.SS2.p6.2.m2.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.2.m2.1b"><cn type="integer" id="S4.SS2.p6.2.m2.1.1.cmml" xref="S4.SS2.p6.2.m2.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.2.m2.1c">512</annotation></semantics></math>
(the same as the output dimension of the CLIP visual encoder) and a multilayer
perceptron dimension of <math id="S4.SS2.p6.3.m3.1" class="ltx_Math" alttext="2048" display="inline"><semantics id="S4.SS2.p6.3.m3.1a"><mn id="S4.SS2.p6.3.m3.1.1" xref="S4.SS2.p6.3.m3.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.3.m3.1b"><cn type="integer" id="S4.SS2.p6.3.m3.1.1.cmml" xref="S4.SS2.p6.3.m3.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.3.m3.1c">2048</annotation></semantics></math>.
Comprehensive details of aggregator training is provided in the
Appendix (Section <a href="#A3" title="Appendix C Vision-based Classifier Pipeline Implementation Details ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>).
To test the effectiveness of our visual aggregator,
we generate baseline vision-based classifiers by taking the vector mean of the
CLIP visual embeddings from the <math id="S4.SS2.p6.4.m4.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S4.SS2.p6.4.m4.1a"><mi id="S4.SS2.p6.4.m4.1.1" xref="S4.SS2.p6.4.m4.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.4.m4.1b"><ci id="S4.SS2.p6.4.m4.1.1.cmml" xref="S4.SS2.p6.4.m4.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.4.m4.1c">K</annotation></semantics></math> image exemplars.</p>
</div>
<div id="S4.SS2.p7" class="ltx_para">
<p id="S4.SS2.p7.3" class="ltx_p">When constructing vision-based classifiers for OVOD,
we find making use of test-time augmentation (TTA) improves performance.
Note, TTA here refers to augmentation of the image exemplars used to build
vision-based classifiers <em id="S4.SS2.p7.3.1" class="ltx_emph ltx_font_italic">not</em> test-time augmentation of the test image on
which OVOD is performed.
In our work, each image exemplar is augmented <math id="S4.SS2.p7.1.m1.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.SS2.p7.1.m1.1a"><mn id="S4.SS2.p7.1.m1.1.1" xref="S4.SS2.p7.1.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p7.1.m1.1b"><cn type="integer" id="S4.SS2.p7.1.m1.1.1.cmml" xref="S4.SS2.p7.1.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p7.1.m1.1c">5</annotation></semantics></math> times and input separately
to the visual encoder.
Therefore, given <math id="S4.SS2.p7.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S4.SS2.p7.2.m2.1a"><mi id="S4.SS2.p7.2.m2.1.1" xref="S4.SS2.p7.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p7.2.m2.1b"><ci id="S4.SS2.p7.2.m2.1.1.cmml" xref="S4.SS2.p7.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p7.2.m2.1c">K</annotation></semantics></math> image exemplars, we generate <math id="S4.SS2.p7.3.m3.1" class="ltx_Math" alttext="5K" display="inline"><semantics id="S4.SS2.p7.3.m3.1a"><mrow id="S4.SS2.p7.3.m3.1.1" xref="S4.SS2.p7.3.m3.1.1.cmml"><mn id="S4.SS2.p7.3.m3.1.1.2" xref="S4.SS2.p7.3.m3.1.1.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p7.3.m3.1.1.1" xref="S4.SS2.p7.3.m3.1.1.1.cmml">​</mo><mi id="S4.SS2.p7.3.m3.1.1.3" xref="S4.SS2.p7.3.m3.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p7.3.m3.1b"><apply id="S4.SS2.p7.3.m3.1.1.cmml" xref="S4.SS2.p7.3.m3.1.1"><times id="S4.SS2.p7.3.m3.1.1.1.cmml" xref="S4.SS2.p7.3.m3.1.1.1"></times><cn type="integer" id="S4.SS2.p7.3.m3.1.1.2.cmml" xref="S4.SS2.p7.3.m3.1.1.2">5</cn><ci id="S4.SS2.p7.3.m3.1.1.3.cmml" xref="S4.SS2.p7.3.m3.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p7.3.m3.1c">5K</annotation></semantics></math> visual embeddings to be
ingested by our visual aggregator.
More details on the use of TTA in constructing vision-based classifiers are found
in the Appendix (Section <a href="#A1" title="Appendix A Ablation Studies ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>).</p>
</div>
<div id="S4.SS2.p8" class="ltx_para">
<p id="S4.SS2.p8.1" class="ltx_p">For total clarity regarding when datasets are used
— the visual aggregator is <em id="S4.SS2.p8.1.1" class="ltx_emph ltx_font_italic">trained</em> using ImageNet-21k-P <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Ridnik et al.</span>, <a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>
and the vision-based classifiers for OVOD are generated from relevant image exemplars
<em id="S4.SS2.p8.1.2" class="ltx_emph ltx_font_italic">using the trained aggregator</em> (see
Section <a href="#A4" title="Appendix D Sourcing Image Exemplars ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a> of the Appendix for details on sourcing the
image exemplars).
Detection data (<span id="S4.SS2.p8.1.3" class="ltx_ERROR undefined">\eg</span> <em id="S4.SS2.p8.1.4" class="ltx_emph ltx_font_italic">LVIS-base</em>) is only used to train the
open-vocabulary object detector and image-level data (<span id="S4.SS2.p8.1.5" class="ltx_ERROR undefined">\eg</span> <em id="S4.SS2.p8.1.6" class="ltx_emph ltx_font_italic">IN-L</em>) may be used as
an extra source of weak supervision as in Detic <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhou et al.</span>, <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>.</p>
</div>
<div id="S4.SS2.p9" class="ltx_para ltx_noindent">
<p id="S4.SS2.p9.1" class="ltx_p"><span id="S4.SS2.p9.1.1" class="ltx_text ltx_font_bold">Multi-Modal Classifier Construction.</span>
When computing multi-modal classifiers,
we simply compute the category-wise <math id="S4.SS2.p9.1.m1.1" class="ltx_Math" alttext="l^{2}" display="inline"><semantics id="S4.SS2.p9.1.m1.1a"><msup id="S4.SS2.p9.1.m1.1.1" xref="S4.SS2.p9.1.m1.1.1.cmml"><mi id="S4.SS2.p9.1.m1.1.1.2" xref="S4.SS2.p9.1.m1.1.1.2.cmml">l</mi><mn id="S4.SS2.p9.1.m1.1.1.3" xref="S4.SS2.p9.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p9.1.m1.1b"><apply id="S4.SS2.p9.1.m1.1.1.cmml" xref="S4.SS2.p9.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p9.1.m1.1.1.1.cmml" xref="S4.SS2.p9.1.m1.1.1">superscript</csymbol><ci id="S4.SS2.p9.1.m1.1.1.2.cmml" xref="S4.SS2.p9.1.m1.1.1.2">𝑙</ci><cn type="integer" id="S4.SS2.p9.1.m1.1.1.3.cmml" xref="S4.SS2.p9.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p9.1.m1.1c">l^{2}</annotation></semantics></math>-normalised classifier
from each modality,
regardless of method used to compute them, and take the vector sum.
In all cases the text-based classifiers are sourced from class descriptions
as described in Section <a href="#S3.SS2" title="3.2 Text-based Classifiers from Language Descriptions ‣ 3 Method ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.
We combine our text-based classifiers with our vision-based classifiers,
using the trained visual aggregator.
However, once again, to test the effectiveness of our visual aggregator,
we also combine our text-based classifiers with the baseline vision-based
classifiers described in the previous paragraph.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Open-Vocabulary Detection Results</h3>

<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.1" class="ltx_inline-block ltx_transformed_outer" style="width:388.8pt;height:306pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S4.T1.1.1" class="ltx_p"><span id="S4.T1.1.1.1" class="ltx_text" style="font-size:70%;">

<span id="S4.T1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span class="ltx_tbody">
<span id="S4.T1.1.1.1.1.2.1" class="ltx_tr">
<span id="S4.T1.1.1.1.1.2.1.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_r ltx_border_tt ltx_rowspan ltx_rowspan_2" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.1.1.1.1.2.1.1.1" class="ltx_text">Model</span></span>
<span id="S4.T1.1.1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_rowspan ltx_rowspan_2" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.1.1.1.1.2.1.2.1" class="ltx_text">Backbone</span></span>
<span id="S4.T1.1.1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">Extra</span>
<span id="S4.T1.1.1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt ltx_rowspan ltx_rowspan_2" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.1.1.1.1.2.1.4.1" class="ltx_text">APr</span></span>
<span id="S4.T1.1.1.1.1.2.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt ltx_rowspan ltx_rowspan_2" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.1.1.1.1.2.1.5.1" class="ltx_text">mAP</span></span></span>
<span id="S4.T1.1.1.1.1.3.2" class="ltx_tr">
<span id="S4.T1.1.1.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">Data</span></span>
<span id="S4.T1.1.1.1.1.4.3" class="ltx_tr">
<span id="S4.T1.1.1.1.1.4.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">ViLD <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gu et al.</span>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite></span>
<span id="S4.T1.1.1.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">ResNet-50</span>
<span id="S4.T1.1.1.1.1.4.3.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T1.1.1.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">16.1</span>
<span id="S4.T1.1.1.1.1.4.3.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">22.5</span></span>
<span id="S4.T1.1.1.1.1.5.4" class="ltx_tr">
<span id="S4.T1.1.1.1.1.5.4.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">Detic <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhou et al.</span>, <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite></span>
<span id="S4.T1.1.1.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">ResNet-50</span>
<span id="S4.T1.1.1.1.1.5.4.3" class="ltx_td ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T1.1.1.1.1.5.4.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">16.3</span>
<span id="S4.T1.1.1.1.1.5.4.5" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">30.0</span></span>
<span id="S4.T1.1.1.1.1.6.5" class="ltx_tr">
<span id="S4.T1.1.1.1.1.6.5.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">ViLD-ens <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gu et al.</span>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite></span>
<span id="S4.T1.1.1.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">ResNet-50</span>
<span id="S4.T1.1.1.1.1.6.5.3" class="ltx_td ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T1.1.1.1.1.6.5.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">16.6</span>
<span id="S4.T1.1.1.1.1.6.5.5" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">25.5</span></span>
<span id="S4.T1.1.1.1.1.7.6" class="ltx_tr">
<span id="S4.T1.1.1.1.1.7.6.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">OV-DETR <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zang et al.</span>, <a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite></span>
<span id="S4.T1.1.1.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">ResNet-50 + DETR</span>
<span id="S4.T1.1.1.1.1.7.6.3" class="ltx_td ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T1.1.1.1.1.7.6.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">17.4</span>
<span id="S4.T1.1.1.1.1.7.6.5" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">26.6</span></span>
<span id="S4.T1.1.1.1.1.8.7" class="ltx_tr">
<span id="S4.T1.1.1.1.1.8.7.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">F-VLM <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Kuo et al.</span>, <a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite></span>
<span id="S4.T1.1.1.1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">ResNet-50</span>
<span id="S4.T1.1.1.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.1.1.1.1.8.7.3.1" class="ltx_text">✗</span></span>
<span id="S4.T1.1.1.1.1.8.7.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.1.1.1.1.8.7.4.1" class="ltx_text ltx_font_italic" style="color:#FE0000;">18.6</span></span>
<span id="S4.T1.1.1.1.1.8.7.5" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">24.2</span></span>
<span id="S4.T1.1.1.1.1.9.8" class="ltx_tr">
<span id="S4.T1.1.1.1.1.9.8.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Ours (Text-Based)</span>
<span id="S4.T1.1.1.1.1.9.8.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T1.1.1.1.1.9.8.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T1.1.1.1.1.9.8.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.1.1.1.1.9.8.4.1" class="ltx_text ltx_font_bold" style="color:#3531FF;">19.3</span></span>
<span id="S4.T1.1.1.1.1.9.8.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.1.1.1.1.9.8.5.1" class="ltx_text ltx_font_italic" style="color:#FE0000;">30.3</span></span></span>
<span id="S4.T1.1.1.1.1.10.9" class="ltx_tr">
<span id="S4.T1.1.1.1.1.10.9.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">Ours (Vision-Based)</span>
<span id="S4.T1.1.1.1.1.10.9.2" class="ltx_td ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T1.1.1.1.1.10.9.3" class="ltx_td ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T1.1.1.1.1.10.9.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">18.3</span>
<span id="S4.T1.1.1.1.1.10.9.5" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">29.2</span></span>
<span id="S4.T1.1.1.1.1.11.10" class="ltx_tr">
<span id="S4.T1.1.1.1.1.11.10.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">Ours (Multi-Modal)</span>
<span id="S4.T1.1.1.1.1.11.10.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.1.1.1.1.11.10.2.1" class="ltx_text">ResNet-50</span></span>
<span id="S4.T1.1.1.1.1.11.10.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.1.1.1.1.11.10.3.1" class="ltx_text">✗</span></span>
<span id="S4.T1.1.1.1.1.11.10.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.1.1.1.1.11.10.4.1" class="ltx_text ltx_font_bold" style="color:#3531FF;">19.3</span></span>
<span id="S4.T1.1.1.1.1.11.10.5" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.1.1.1.1.11.10.5.1" class="ltx_text ltx_font_bold" style="color:#3531FF;">30.6</span></span></span>
<span id="S4.T1.1.1.1.1.12.11" class="ltx_tr">
<span id="S4.T1.1.1.1.1.12.11.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">RegCLIP <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhong et al.</span>, <a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite></span>
<span id="S4.T1.1.1.1.1.12.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">ResNet-50</span>
<span id="S4.T1.1.1.1.1.12.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">CC3M</span>
<span id="S4.T1.1.1.1.1.12.11.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">17.1</span>
<span id="S4.T1.1.1.1.1.12.11.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">28.2</span></span>
<span id="S4.T1.1.1.1.1.1" class="ltx_tr">
<span id="S4.T1.1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">OWL-ViT <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Minderer et al.</span>, <a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite><math id="S4.T1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S4.T1.1.1.1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.1.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.1.1.m1.1c">\dagger</annotation></semantics></math></span>
<span id="S4.T1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">ViT-B/32</span>
<span id="S4.T1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">LiT</span>
<span id="S4.T1.1.1.1.1.1.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">19.7</span>
<span id="S4.T1.1.1.1.1.1.5" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">23.3</span></span>
<span id="S4.T1.1.1.1.1.13.12" class="ltx_tr">
<span id="S4.T1.1.1.1.1.13.12.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">Detic <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhou et al.</span>, <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite></span>
<span id="S4.T1.1.1.1.1.13.12.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">ResNet-50</span>
<span id="S4.T1.1.1.1.1.13.12.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">IN-L</span>
<span id="S4.T1.1.1.1.1.13.12.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">24.6</span>
<span id="S4.T1.1.1.1.1.13.12.5" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">32.4</span></span>
<span id="S4.T1.1.1.1.1.14.13" class="ltx_tr">
<span id="S4.T1.1.1.1.1.14.13.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Ours (Text-Based)</span>
<span id="S4.T1.1.1.1.1.14.13.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T1.1.1.1.1.14.13.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T1.1.1.1.1.14.13.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.1.1.1.1.14.13.4.1" class="ltx_text ltx_font_italic" style="color:#FE0000;">25.8</span></span>
<span id="S4.T1.1.1.1.1.14.13.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.1.1.1.1.14.13.5.1" class="ltx_text ltx_font_italic" style="color:#FE0000;">32.7</span></span></span>
<span id="S4.T1.1.1.1.1.15.14" class="ltx_tr">
<span id="S4.T1.1.1.1.1.15.14.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">Ours (Vision-Based)</span>
<span id="S4.T1.1.1.1.1.15.14.2" class="ltx_td ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T1.1.1.1.1.15.14.3" class="ltx_td ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span id="S4.T1.1.1.1.1.15.14.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">23.8</span>
<span id="S4.T1.1.1.1.1.15.14.5" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">31.3</span></span>
<span id="S4.T1.1.1.1.1.16.15" class="ltx_tr">
<span id="S4.T1.1.1.1.1.16.15.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">Ours (Multi-Modal)</span>
<span id="S4.T1.1.1.1.1.16.15.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.1.1.1.1.16.15.2.1" class="ltx_text">ResNet-50</span></span>
<span id="S4.T1.1.1.1.1.16.15.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.1.1.1.1.16.15.3.1" class="ltx_text">IN-L</span></span>
<span id="S4.T1.1.1.1.1.16.15.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.1.1.1.1.16.15.4.1" class="ltx_text ltx_font_bold" style="color:#3531FF;">27.3</span></span>
<span id="S4.T1.1.1.1.1.16.15.5" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.1.1.1.1.16.15.5.1" class="ltx_text ltx_font_bold" style="color:#3531FF;">33.1</span></span></span>
<span id="S4.T1.1.1.1.1.17.16" class="ltx_tr">
<span id="S4.T1.1.1.1.1.17.16.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.1.1.1.1.17.16.1.1" class="ltx_ERROR undefined">\rowcolor</span>[HTML]EFEFEF
<span id="S4.T1.1.1.1.1.17.16.1.2" class="ltx_text" style="color:#656565;"> Fully-Supervised <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhou et al.</span>, <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite></span></span>
<span id="S4.T1.1.1.1.1.17.16.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.1.1.1.1.17.16.2.1" class="ltx_text" style="color:#656565;">ResNet-50</span></span>
<span id="S4.T1.1.1.1.1.17.16.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.1.1.1.1.17.16.3.1" class="ltx_text" style="color:#656565;">✗</span></span>
<span id="S4.T1.1.1.1.1.17.16.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.1.1.1.1.17.16.4.1" class="ltx_text" style="color:#656565;">25.5</span></span>
<span id="S4.T1.1.1.1.1.17.16.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.1.1.1.1.17.16.5.1" class="ltx_text" style="color:#656565;">31.1</span></span></span>
</span>
</span></span></p>
</span></div>
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>
Detection performance on the LVIS Open Vocabulary Detection Benchmark using our
three types of classifier compared with previous works.
Best and second-best performing
models are coloured <span id="S4.T1.30.1" class="ltx_text ltx_font_bold" style="color:#3531FF;">blue</span> and <span id="S4.T1.31.2" class="ltx_text ltx_font_italic" style="color:#FE0000;">red</span>, respectively.
We split models into those which only use LVIS-base as training data (top)
and those which use additional image-level data (bottom).
Furthermore, we show results for a fully-supervised model from Detic trained on
LVIS-all in <span id="S4.T1.32.3" class="ltx_text" style="color:#656565;background-color:#C0C0C0;"> grey</span>.
<math id="S4.T1.4.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S4.T1.4.m1.1b"><mo id="S4.T1.4.m1.1.1" xref="S4.T1.4.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.m1.1c"><ci id="S4.T1.4.m1.1.1.cmml" xref="S4.T1.4.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.m1.1d">\dagger</annotation></semantics></math> OWL-ViT reports bbox AP metrics and was trained on
Objects365 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Shao et al.</span>, <a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite> and VisualGenome <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Krishna et al.</span>, <a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2017</span></a>)</cite> <span id="S4.T1.33.4" class="ltx_text ltx_font_italic">not</span> LVIS-base,
therefore it is possible LVIS-defined “rare” classes are contained in the
detection training data of OWL-ViT.
Due to limited compute resources we present and compare to models which
use ResNet-50 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">He et al.</span>, <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>)</cite> backbones or similar.
We report mask AP metrics except for <math id="S4.T1.5.m2.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S4.T1.5.m2.1b"><mo id="S4.T1.5.m2.1.1" xref="S4.T1.5.m2.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.m2.1c"><ci id="S4.T1.5.m2.1.1.cmml" xref="S4.T1.5.m2.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.m2.1d">\dagger</annotation></semantics></math>.
</figcaption>
</figure>
<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">LVIS OVOD Benchmark.</span>
Table <a href="#S4.T1" title="Table 1 ‣ 4.3 Open-Vocabulary Detection Results ‣ 4 Experiments ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows results on <em id="S4.SS3.p1.1.2" class="ltx_emph ltx_font_italic">LVIS-val</em> for our work,
which uses text-based, vision-based and multi-modal classifiers,
compared to a range of prior work.
We report overall mask AP performance and mask AP for “rare” classes only.
The latter metric is the key measure of OVOD performance.
We separate the comparisons into those models which do not use additional
image-level data (top half of Table <a href="#S4.T1" title="Table 1 ‣ 4.3 Open-Vocabulary Detection Results ‣ 4 Experiments ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>)
and those which do (bottom half of Table <a href="#S4.T1" title="Table 1 ‣ 4.3 Open-Vocabulary Detection Results ‣ 4 Experiments ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
For a fair evaluation, we compare to models from prior works which use a
ResNet-50 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">He et al.</span>, <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>)</cite> backbone.
There are two exceptions:
(1) OWL-ViT <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Minderer et al.</span>, <a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite> which only
investigates using Vision Transformers for OVOD <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Dosovitskiy et al.</span>, <a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite> –
we compare ResNet-50 models to the ViT-B/32 OWL-ViT model as it requires similar compute during
inference in terms of GLOPs (141.5 and 139.6, respectively);
(2) OV-DETR <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zang et al.</span>, <a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite> which uses a DETR-style architecture <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Carion et al.</span>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>
consisting of a ResNet-50 CNN backbone and modified transformer encoder and decoder.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.4" class="ltx_p">In the experiments without using extra data (<math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{D}^{\textsc{img}}=\varnothing" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><msup id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS3.p2.1.m1.1.1.2.2" xref="S4.SS3.p2.1.m1.1.1.2.2.cmml">𝒟</mi><mtext class="ltx_font_smallcaps" id="S4.SS3.p2.1.m1.1.1.2.3" xref="S4.SS3.p2.1.m1.1.1.2.3a.cmml">img</mtext></msup><mo id="S4.SS3.p2.1.m1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.cmml">=</mo><mi mathvariant="normal" id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml">∅</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><eq id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1"></eq><apply id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.1.1.2.1.cmml" xref="S4.SS3.p2.1.m1.1.1.2">superscript</csymbol><ci id="S4.SS3.p2.1.m1.1.1.2.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2.2">𝒟</ci><ci id="S4.SS3.p2.1.m1.1.1.2.3a.cmml" xref="S4.SS3.p2.1.m1.1.1.2.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="S4.SS3.p2.1.m1.1.1.2.3.cmml" xref="S4.SS3.p2.1.m1.1.1.2.3">img</mtext></ci></apply><emptyset id="S4.SS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3"></emptyset></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">\mathcal{D}^{\textsc{img}}=\varnothing</annotation></semantics></math>),
our models with text-based or multi-modal classifiers obtain
the best performance on both APr and overall mAP,
while F-VLM and Detic only performs strongly on APr and mAP, respectively.
Our model with text-based classifiers is most directly comparable to Detic and
our model outperforms Detic by <math id="S4.SS3.p2.2.m2.1" class="ltx_Math" alttext="3.0" display="inline"><semantics id="S4.SS3.p2.2.m2.1a"><mn id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml">3.0</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><cn type="float" id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1">3.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">3.0</annotation></semantics></math> APr.
When using extra data, we make use of a ImageNet-21k subset as in Detic (IN-L).
Models with text-based and multi-modal classifiers outperform
Detic (previous state-of-the-art) by <math id="S4.SS3.p2.3.m3.1" class="ltx_Math" alttext="1.2" display="inline"><semantics id="S4.SS3.p2.3.m3.1a"><mn id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml">1.2</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><cn type="float" id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1">1.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">1.2</annotation></semantics></math> and <math id="S4.SS3.p2.4.m4.1" class="ltx_Math" alttext="2.7" display="inline"><semantics id="S4.SS3.p2.4.m4.1a"><mn id="S4.SS3.p2.4.m4.1.1" xref="S4.SS3.p2.4.m4.1.1.cmml">2.7</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.4.m4.1b"><cn type="float" id="S4.SS3.p2.4.m4.1.1.cmml" xref="S4.SS3.p2.4.m4.1.1">2.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.4.m4.1c">2.7</annotation></semantics></math> APr respectively.
<span id="S4.SS3.p2.4.1" class="ltx_text ltx_font_bold">Note that</span>, our models trained with IN-L even outperform
the fully-supervised baseline using CenterNet2,
<span id="S4.SS3.p2.4.2" class="ltx_ERROR undefined">\ie</span> trained on “rare” class box annotations.
To the best of our knowledge,
this is the first work on open-vocabulary detection that outperforms a
comparable fully-supervised model on the challenging LVIS benchmark.
Note, some other works use larger vision backbones,
<span id="S4.SS3.p2.4.3" class="ltx_ERROR undefined">\eg</span> Swin-B <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Liu et al.</span>, <a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, but due to limited computation resources we only
present and compare to models with ResNet-50 backbones or similar.
Our models with text-based and multi-modal classifiers surpass state-of-the-art
performance when using a ResNet-50 backbone or similar.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.3" class="ltx_inline-block ltx_transformed_outer" style="width:297.1pt;height:108pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S4.T2.3.1" class="ltx_p"><span id="S4.T2.3.1.1" class="ltx_text" style="font-size:70%;">

<span id="S4.T2.3.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_tbody">
<span id="S4.T2.3.1.1.1.1.1" class="ltx_tr">
<span id="S4.T2.3.1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt ltx_rowspan ltx_rowspan_2" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.3.1.1.1.1.1.1.1" class="ltx_text">Model</span></span>
<span id="S4.T2.3.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">Extra</span>
<span id="S4.T2.3.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">Objects365</span>
<span id="S4.T2.3.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">Objects365</span>
<span id="S4.T2.3.1.1.1.1.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">Objects365</span></span>
<span id="S4.T2.3.1.1.1.2.2" class="ltx_tr">
<span id="S4.T2.3.1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">Data</span>
<span id="S4.T2.3.1.1.1.2.2.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">mAP</span>
<span id="S4.T2.3.1.1.1.2.2.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">AP50</span>
<span id="S4.T2.3.1.1.1.2.2.4" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">APr</span></span>
<span id="S4.T2.3.1.1.1.3.3" class="ltx_tr">
<span id="S4.T2.3.1.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Detic <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhou et al.</span>, <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite></span>
<span id="S4.T2.3.1.1.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.3.1.1.1.3.3.2.1" class="ltx_text">✗</span></span>
<span id="S4.T2.3.1.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">13.9</span>
<span id="S4.T2.3.1.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">19.7</span>
<span id="S4.T2.3.1.1.1.3.3.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">9.5</span></span>
<span id="S4.T2.3.1.1.1.4.4" class="ltx_tr">
<span id="S4.T2.3.1.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">Ours (Text-Based)</span>
<span id="S4.T2.3.1.1.1.4.4.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.3.1.1.1.4.4.2.1" class="ltx_text ltx_font_bold" style="color:#3531FF;">14.8</span></span>
<span id="S4.T2.3.1.1.1.4.4.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.3.1.1.1.4.4.3.1" class="ltx_text ltx_font_bold" style="color:#3531FF;">21.0</span></span>
<span id="S4.T2.3.1.1.1.4.4.4" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.3.1.1.1.4.4.4.1" class="ltx_text ltx_font_bold" style="color:#3531FF;">10.1</span></span></span>
<span id="S4.T2.3.1.1.1.5.5" class="ltx_tr">
<span id="S4.T2.3.1.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Detic <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhou et al.</span>, <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite></span>
<span id="S4.T2.3.1.1.1.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.3.1.1.1.5.5.2.1" class="ltx_text">IN-L</span></span>
<span id="S4.T2.3.1.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">15.6</span>
<span id="S4.T2.3.1.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">22.2</span>
<span id="S4.T2.3.1.1.1.5.5.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">12.4</span></span>
<span id="S4.T2.3.1.1.1.6.6" class="ltx_tr">
<span id="S4.T2.3.1.1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">Ours (Text-Based)</span>
<span id="S4.T2.3.1.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.3.1.1.1.6.6.2.1" class="ltx_text ltx_font_bold" style="color:#3531FF;">16.6</span></span>
<span id="S4.T2.3.1.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.3.1.1.1.6.6.3.1" class="ltx_text ltx_font_bold" style="color:#3531FF;">23.1</span></span>
<span id="S4.T2.3.1.1.1.6.6.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T2.3.1.1.1.6.6.4.1" class="ltx_text ltx_font_bold" style="color:#3531FF;">13.1</span></span></span>
</span>
</span></span></p>
</span></div>
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>
Detection performance when training on <em id="S4.T2.21.1" class="ltx_emph ltx_font_italic">LVIS-all</em> (<span id="S4.T2.22.2" class="ltx_ERROR undefined">\ie</span> all LVIS training data)
and evaluating on Objects365 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Shao et al.</span>, <a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>, where the least frequent <math id="S4.T2.2.m1.1" class="ltx_Math" alttext="\frac{1}{3}" display="inline"><semantics id="S4.T2.2.m1.1b"><mfrac id="S4.T2.2.m1.1.1" xref="S4.T2.2.m1.1.1.cmml"><mn id="S4.T2.2.m1.1.1.2" xref="S4.T2.2.m1.1.1.2.cmml">1</mn><mn id="S4.T2.2.m1.1.1.3" xref="S4.T2.2.m1.1.1.3.cmml">3</mn></mfrac><annotation-xml encoding="MathML-Content" id="S4.T2.2.m1.1c"><apply id="S4.T2.2.m1.1.1.cmml" xref="S4.T2.2.m1.1.1"><divide id="S4.T2.2.m1.1.1.1.cmml" xref="S4.T2.2.m1.1.1"></divide><cn type="integer" id="S4.T2.2.m1.1.1.2.cmml" xref="S4.T2.2.m1.1.1.2">1</cn><cn type="integer" id="S4.T2.2.m1.1.1.3.cmml" xref="S4.T2.2.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.m1.1d">\frac{1}{3}</annotation></semantics></math>
of classes are defined as “rare”.
Best performing
models are coloured <span id="S4.T2.23.3" class="ltx_text ltx_font_bold" style="color:#3531FF;">blue</span>.
Our text-based classifiers outperform Detic when transferring to Objects365 across
all classes and “rare” classes only.
Note the Detic <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhou et al.</span>, <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite> models we compare to on Objects365 are not the same as the models listed in the Detic paper,
which use a large Swin-B backbone and all of ImageNet-21k as extra data for weak supervision.
We use a fair comparison Detic model which uses the same training data (see text for more details).
We report box AP metrics for Objects365.
</figcaption>
</figure>
<div id="S4.SS3.p3" class="ltx_para ltx_noindent">
<p id="S4.SS3.p3.4" class="ltx_p"><span id="S4.SS3.p3.4.1" class="ltx_text ltx_font_bold">Cross-dataset Transfer.</span>
Table <a href="#S4.T2" title="Table 2 ‣ 4.3 Open-Vocabulary Detection Results ‣ 4 Experiments ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows results for cross-dataset transfer from LVIS to Objects365 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Shao et al.</span>, <a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>
when using our text-based classifiers.
We compare our work to equivalent models from Detic, reporting box AP metrics as standard in Objects365.
In all cases, these models are trained on <em id="S4.SS3.p3.4.2" class="ltx_emph ltx_font_italic">LVIS-all</em> and the models in the
bottom two rows use <em id="S4.SS3.p3.4.3" class="ltx_emph ltx_font_italic">IN-L</em> as extra weak supervision.
The trained open-vocabulary detectors are evaluated on the Objects365 validation set.
Following Detic, we define “rare” classes in Objects365 as the <math id="S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="\frac{1}{3}" display="inline"><semantics id="S4.SS3.p3.1.m1.1a"><mfrac id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml"><mn id="S4.SS3.p3.1.m1.1.1.2" xref="S4.SS3.p3.1.m1.1.1.2.cmml">1</mn><mn id="S4.SS3.p3.1.m1.1.1.3" xref="S4.SS3.p3.1.m1.1.1.3.cmml">3</mn></mfrac><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><apply id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"><divide id="S4.SS3.p3.1.m1.1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"></divide><cn type="integer" id="S4.SS3.p3.1.m1.1.1.2.cmml" xref="S4.SS3.p3.1.m1.1.1.2">1</cn><cn type="integer" id="S4.SS3.p3.1.m1.1.1.3.cmml" xref="S4.SS3.p3.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">\frac{1}{3}</annotation></semantics></math> of classes
with the lowest frequency in the Objects365 training set.
Note the Detic models we compare to in Table <a href="#S4.T2" title="Table 2 ‣ 4.3 Open-Vocabulary Detection Results ‣ 4 Experiments ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> are not the same as those listed in Table 4 of the Detic paper,
which use a large Swin-B backbone and all of ImageNet-21k as extra data.
Instead, we compare to other Detic models publicly available which are trained on <em id="S4.SS3.p3.4.4" class="ltx_emph ltx_font_italic">LVIS-all</em> (and <em id="S4.SS3.p3.4.5" class="ltx_emph ltx_font_italic">IN-L</em>).
For ease, we provide links to the Detic configuration and checkpoint files used in this cross-dataset transfer evaluation
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Detic
<a target="_blank" href="https://github.com/facebookresearch/Detic/blob/main/configs/BoxSup-C2_L_CLIP_R5021k_640b64_4x.yaml" title="" class="ltx_ref ltx_href">Configuration</a> <a target="_blank" href="https://dl.fbaipublicfiles.com/detic/BoxSup-C2_L_CLIP_R5021k_640b64_4x.pth" title="" class="ltx_ref ltx_href">Checkpoint</a> Extra Data: ✗</span></span></span>
<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Detic
<a target="_blank" href="https://github.com/facebookresearch/Detic/blob/main/configs/Detic_LI_CLIP_R5021k_640b64_4x_ft4x_max-size.yaml" title="" class="ltx_ref ltx_href">Configuration</a> <a target="_blank" href="https://dl.fbaipublicfiles.com/detic/Detic_LI_CLIP_R5021k_640b64_4x_ft4x_max-size.pth" title="" class="ltx_ref ltx_href">Checkpoint</a> Extra Data: <em id="footnote2.1" class="ltx_emph ltx_font_italic">IN-L</em></span></span></span>.
Evaluation on Objects365 after training on LVIS is easily done with Detic or with our text-based classifiers.
In the case of Detic, the simple classifiers based on LVIS class names are replaced with equivalent simple classifiers based
on Objects365 class names.
For our text-based classifiers, plain text descriptions are generated for each of the Objects365 classes and
are encoded as described in Section <a href="#S3.SS2" title="3.2 Text-based Classifiers from Language Descriptions ‣ 3 Method ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> and replace the text-based classifiers for LVIS.
In both models, Detic and ours, all other parameters of the open-vocabulary detector remain the same.
In all cases, using our text-based classifiers gives performance improvements over the equivalent Detic model.
Considering all classes, our method with extra data outperforms Detic by <math id="S4.SS3.p3.2.m2.1" class="ltx_Math" alttext="1.0" display="inline"><semantics id="S4.SS3.p3.2.m2.1a"><mn id="S4.SS3.p3.2.m2.1.1" xref="S4.SS3.p3.2.m2.1.1.cmml">1.0</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.2.m2.1b"><cn type="float" id="S4.SS3.p3.2.m2.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1">1.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m2.1c">1.0</annotation></semantics></math> mAP and <math id="S4.SS3.p3.3.m3.1" class="ltx_Math" alttext="0.9" display="inline"><semantics id="S4.SS3.p3.3.m3.1a"><mn id="S4.SS3.p3.3.m3.1.1" xref="S4.SS3.p3.3.m3.1.1.cmml">0.9</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.3.m3.1b"><cn type="float" id="S4.SS3.p3.3.m3.1.1.cmml" xref="S4.SS3.p3.3.m3.1.1">0.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.3.m3.1c">0.9</annotation></semantics></math> AP50.
For “rare” classes as defined above, our method with extra data outperforms the equivalent Detic model by <math id="S4.SS3.p3.4.m4.1" class="ltx_Math" alttext="0.7" display="inline"><semantics id="S4.SS3.p3.4.m4.1a"><mn id="S4.SS3.p3.4.m4.1.1" xref="S4.SS3.p3.4.m4.1.1.cmml">0.7</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.4.m4.1b"><cn type="float" id="S4.SS3.p3.4.m4.1.1.cmml" xref="S4.SS3.p3.4.m4.1.1">0.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.4.m4.1c">0.7</annotation></semantics></math> APr.
These results demonstrate that our method, which uses text-based classifiers generated from rich class descriptions,
provides additional information compared to using a simple classifier based on the class name only,
even when the training and testing class vocabularies are disjoint.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation Study</h3>

<figure id="S4.F4" class="ltx_figure"><img src="/html/2306.05493/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="80" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Some qualitative detection examples using our model with text-based classifiers, detecting “rare” category instances in <em id="S4.F4.5.1" class="ltx_emph ltx_font_italic">LVIS-val</em>.
Our text-based classifiers are sourced from rich natural language descriptions of a given class by prompting an GPT-3 LLM.</figcaption>
</figure>
<div id="S4.SS4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.p1.5" class="ltx_p"><span id="S4.SS4.p1.5.1" class="ltx_text ltx_font_bold">Results with Vision-Based Classifiers.</span>
Using vision-based classifiers for OVOD is an under-explored area and
so we compare our method,
detailed in Section <a href="#S3.SS3" title="3.3 Vision-based Classifiers from Image Exemplars ‣ 3 Method ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>,
to baseline classifiers in which the same visual encoder is used,
but the action of the aggregator is replaced by performing simple vector mean.
The <span id="S4.SS4.p1.5.2" class="ltx_text" style="background-color:#FFCE93;">orange</span> rows of Table <a href="#S4.T3" title="Table 3 ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> compares
the use of our visual aggregator to performing simple vector mean across visual
embeddings instead.
When no additional image-level data is
used (top two <span id="S4.SS4.p1.5.3" class="ltx_text" style="background-color:#FFCE93;">orange</span> rows) our aggregator (Model B)
boosts performance by <math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="3.5" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mn id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">3.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><cn type="float" id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">3.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">3.5</annotation></semantics></math> APr compared to the vector mean baseline (Model A).
For models which train on additional image-level data (IN-L) our aggregator (Model G) boosts
performance by <math id="S4.SS4.p1.2.m2.1" class="ltx_Math" alttext="2.2" display="inline"><semantics id="S4.SS4.p1.2.m2.1a"><mn id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml">2.2</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><cn type="float" id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1">2.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">2.2</annotation></semantics></math> APr compared to the baseline (Model F).
This comparison demonstrates the utility of our
visual aggregator in constructing better vision-based classifiers
rather than naïvely averaging the <math id="S4.SS4.p1.3.m3.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S4.SS4.p1.3.m3.1a"><mi id="S4.SS4.p1.3.m3.1.1" xref="S4.SS4.p1.3.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.3.m3.1b"><ci id="S4.SS4.p1.3.m3.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.3.m3.1c">K</annotation></semantics></math> visual embeddings.
Note our vision-based classifiers and the baseline classifiers both utilise TTA
as mentioned in Section <a href="#S4.SS2" title="4.2 Implementation Details ‣ 4 Experiments ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> (see Section <a href="#A1" title="Appendix A Ablation Studies ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a> of
the Appendix for details on TTA).
The results in Table <a href="#S4.T3" title="Table 3 ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> use <math id="S4.SS4.p1.4.m4.1" class="ltx_Math" alttext="K=5" display="inline"><semantics id="S4.SS4.p1.4.m4.1a"><mrow id="S4.SS4.p1.4.m4.1.1" xref="S4.SS4.p1.4.m4.1.1.cmml"><mi id="S4.SS4.p1.4.m4.1.1.2" xref="S4.SS4.p1.4.m4.1.1.2.cmml">K</mi><mo id="S4.SS4.p1.4.m4.1.1.1" xref="S4.SS4.p1.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS4.p1.4.m4.1.1.3" xref="S4.SS4.p1.4.m4.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.4.m4.1b"><apply id="S4.SS4.p1.4.m4.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1"><eq id="S4.SS4.p1.4.m4.1.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1.1"></eq><ci id="S4.SS4.p1.4.m4.1.1.2.cmml" xref="S4.SS4.p1.4.m4.1.1.2">𝐾</ci><cn type="integer" id="S4.SS4.p1.4.m4.1.1.3.cmml" xref="S4.SS4.p1.4.m4.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.4.m4.1c">K=5</annotation></semantics></math>.
Further results for <math id="S4.SS4.p1.5.m5.3" class="ltx_Math" alttext="K=1,2,10" display="inline"><semantics id="S4.SS4.p1.5.m5.3a"><mrow id="S4.SS4.p1.5.m5.3.4" xref="S4.SS4.p1.5.m5.3.4.cmml"><mi id="S4.SS4.p1.5.m5.3.4.2" xref="S4.SS4.p1.5.m5.3.4.2.cmml">K</mi><mo id="S4.SS4.p1.5.m5.3.4.1" xref="S4.SS4.p1.5.m5.3.4.1.cmml">=</mo><mrow id="S4.SS4.p1.5.m5.3.4.3.2" xref="S4.SS4.p1.5.m5.3.4.3.1.cmml"><mn id="S4.SS4.p1.5.m5.1.1" xref="S4.SS4.p1.5.m5.1.1.cmml">1</mn><mo id="S4.SS4.p1.5.m5.3.4.3.2.1" xref="S4.SS4.p1.5.m5.3.4.3.1.cmml">,</mo><mn id="S4.SS4.p1.5.m5.2.2" xref="S4.SS4.p1.5.m5.2.2.cmml">2</mn><mo id="S4.SS4.p1.5.m5.3.4.3.2.2" xref="S4.SS4.p1.5.m5.3.4.3.1.cmml">,</mo><mn id="S4.SS4.p1.5.m5.3.3" xref="S4.SS4.p1.5.m5.3.3.cmml">10</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.5.m5.3b"><apply id="S4.SS4.p1.5.m5.3.4.cmml" xref="S4.SS4.p1.5.m5.3.4"><eq id="S4.SS4.p1.5.m5.3.4.1.cmml" xref="S4.SS4.p1.5.m5.3.4.1"></eq><ci id="S4.SS4.p1.5.m5.3.4.2.cmml" xref="S4.SS4.p1.5.m5.3.4.2">𝐾</ci><list id="S4.SS4.p1.5.m5.3.4.3.1.cmml" xref="S4.SS4.p1.5.m5.3.4.3.2"><cn type="integer" id="S4.SS4.p1.5.m5.1.1.cmml" xref="S4.SS4.p1.5.m5.1.1">1</cn><cn type="integer" id="S4.SS4.p1.5.m5.2.2.cmml" xref="S4.SS4.p1.5.m5.2.2">2</cn><cn type="integer" id="S4.SS4.p1.5.m5.3.3.cmml" xref="S4.SS4.p1.5.m5.3.3">10</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.5.m5.3c">K=1,2,10</annotation></semantics></math> are found in the Appendix
(Section <a href="#A5" title="Appendix E Varying Number of Image Exemplars for Vision-based Classifiers ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>).</p>
</div>
<figure id="S4.T3" class="ltx_table">
<p id="S4.T3.1" class="ltx_p"><span id="S4.T3.1.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="S4.T3.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:472.9pt;height:217pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S4.T3.1.1.1.1" class="ltx_p"><span id="S4.T3.1.1.1.1.1" class="ltx_text">

<span id="S4.T3.1.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S4.T3.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:12.0pt;padding-right:12.0pt;"></span>
<span id="S4.T3.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:12.0pt;padding-right:12.0pt;">Visual</span>
<span id="S4.T3.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:12.0pt;padding-right:12.0pt;">Visual</span>
<span id="S4.T3.1.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:12.0pt;padding-right:12.0pt;">Text</span>
<span id="S4.T3.1.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:12.0pt;padding-right:12.0pt;">Extra</span>
<span id="S4.T3.1.1.1.1.1.1.1.1.6" class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="padding-left:12.0pt;padding-right:12.0pt;"></span>
<span id="S4.T3.1.1.1.1.1.1.1.1.7" class="ltx_td ltx_nopad_r ltx_th ltx_th_column ltx_border_tt" style="padding-left:12.0pt;padding-right:12.0pt;"></span></span>
<span id="S4.T3.1.1.1.1.1.1.2.2" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:12.0pt;padding-right:12.0pt;">Model</span>
<span id="S4.T3.1.1.1.1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:12.0pt;padding-right:12.0pt;">Mean?</span>
<span id="S4.T3.1.1.1.1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:12.0pt;padding-right:12.0pt;">Agg.?</span>
<span id="S4.T3.1.1.1.1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:12.0pt;padding-right:12.0pt;">Cls.?</span>
<span id="S4.T3.1.1.1.1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:12.0pt;padding-right:12.0pt;">Data?</span>
<span id="S4.T3.1.1.1.1.1.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:12.0pt;padding-right:12.0pt;">APr</span>
<span id="S4.T3.1.1.1.1.1.1.2.2.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding-left:12.0pt;padding-right:12.0pt;">mAP</span></span>
</span>
<span class="ltx_tbody">
<span id="S4.T3.1.1.1.1.1.1.3.1" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.3.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;"><span id="S4.T3.1.1.1.1.1.1.3.1.1.1" class="ltx_ERROR undefined">\rowcolor</span>[HTML]FFCE93
A</span>
<span id="S4.T3.1.1.1.1.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;">✓</span>
<span id="S4.T3.1.1.1.1.1.1.3.1.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;"></span>
<span id="S4.T3.1.1.1.1.1.1.3.1.4" class="ltx_td ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;"></span>
<span id="S4.T3.1.1.1.1.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;"><span id="S4.T3.1.1.1.1.1.1.3.1.5.1" class="ltx_ERROR undefined">\cellcolor</span>[HTML]FFCE93</span>
<span id="S4.T3.1.1.1.1.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;">14.8</span>
<span id="S4.T3.1.1.1.1.1.1.3.1.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;">28.8</span></span>
<span id="S4.T3.1.1.1.1.1.1.4.2" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.4.2.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:12.0pt;padding-right:12.0pt;"><span id="S4.T3.1.1.1.1.1.1.4.2.1.1" class="ltx_ERROR undefined">\rowcolor</span>[HTML]FFCE93
B</span>
<span id="S4.T3.1.1.1.1.1.1.4.2.2" class="ltx_td ltx_border_r" style="padding-left:12.0pt;padding-right:12.0pt;"></span>
<span id="S4.T3.1.1.1.1.1.1.4.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:12.0pt;padding-right:12.0pt;">✓</span>
<span id="S4.T3.1.1.1.1.1.1.4.2.4" class="ltx_td ltx_border_r" style="padding-left:12.0pt;padding-right:12.0pt;"></span>
<span id="S4.T3.1.1.1.1.1.1.4.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:12.0pt;padding-right:12.0pt;"><span id="S4.T3.1.1.1.1.1.1.4.2.5.1" class="ltx_text"><span id="S4.T3.1.1.1.1.1.1.4.2.5.1.1" class="ltx_ERROR undefined">\cellcolor</span>[HTML]FFCE93✗</span></span>
<span id="S4.T3.1.1.1.1.1.1.4.2.6" class="ltx_td ltx_align_center" style="padding-left:12.0pt;padding-right:12.0pt;">18.3</span>
<span id="S4.T3.1.1.1.1.1.1.4.2.7" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:12.0pt;padding-right:12.0pt;">29.2</span></span>
<span id="S4.T3.1.1.1.1.1.1.5.3" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.5.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;"><span id="S4.T3.1.1.1.1.1.1.5.3.1.1" class="ltx_ERROR undefined">\rowcolor</span>[HTML]DAE8FC
C</span>
<span id="S4.T3.1.1.1.1.1.1.5.3.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;"></span>
<span id="S4.T3.1.1.1.1.1.1.5.3.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;"></span>
<span id="S4.T3.1.1.1.1.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;">✓</span>
<span id="S4.T3.1.1.1.1.1.1.5.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;">✗</span>
<span id="S4.T3.1.1.1.1.1.1.5.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;"><span id="S4.T3.1.1.1.1.1.1.5.3.6.1" class="ltx_text ltx_font_italic" style="color:#FE0000;">19.3</span></span>
<span id="S4.T3.1.1.1.1.1.1.5.3.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;">30.3</span></span>
<span id="S4.T3.1.1.1.1.1.1.6.4" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.6.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;"><span id="S4.T3.1.1.1.1.1.1.6.4.1.1" class="ltx_ERROR undefined">\rowcolor</span>[HTML]C0C0C0
D</span>
<span id="S4.T3.1.1.1.1.1.1.6.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;">✓</span>
<span id="S4.T3.1.1.1.1.1.1.6.4.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;"></span>
<span id="S4.T3.1.1.1.1.1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;">✓</span>
<span id="S4.T3.1.1.1.1.1.1.6.4.5" class="ltx_td ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;"></span>
<span id="S4.T3.1.1.1.1.1.1.6.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;"><span id="S4.T3.1.1.1.1.1.1.6.4.6.1" class="ltx_text ltx_font_bold" style="color:#3531FF;">20.7</span></span>
<span id="S4.T3.1.1.1.1.1.1.6.4.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;"><span id="S4.T3.1.1.1.1.1.1.6.4.7.1" class="ltx_text ltx_font_italic" style="color:#FE0000;">30.5</span></span></span>
<span id="S4.T3.1.1.1.1.1.1.7.5" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.7.5.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:12.0pt;padding-right:12.0pt;"><span id="S4.T3.1.1.1.1.1.1.7.5.1.1" class="ltx_ERROR undefined">\rowcolor</span>[HTML]C0C0C0
E</span>
<span id="S4.T3.1.1.1.1.1.1.7.5.2" class="ltx_td ltx_border_r" style="padding-left:12.0pt;padding-right:12.0pt;"></span>
<span id="S4.T3.1.1.1.1.1.1.7.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:12.0pt;padding-right:12.0pt;">✓</span>
<span id="S4.T3.1.1.1.1.1.1.7.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:12.0pt;padding-right:12.0pt;">✓</span>
<span id="S4.T3.1.1.1.1.1.1.7.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:12.0pt;padding-right:12.0pt;"><span id="S4.T3.1.1.1.1.1.1.7.5.5.1" class="ltx_text">✗</span></span>
<span id="S4.T3.1.1.1.1.1.1.7.5.6" class="ltx_td ltx_align_center" style="padding-left:12.0pt;padding-right:12.0pt;"><span id="S4.T3.1.1.1.1.1.1.7.5.6.1" class="ltx_text ltx_font_italic" style="color:#FE0000;">19.3</span></span>
<span id="S4.T3.1.1.1.1.1.1.7.5.7" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:12.0pt;padding-right:12.0pt;"><span id="S4.T3.1.1.1.1.1.1.7.5.7.1" class="ltx_text ltx_font_bold" style="color:#3531FF;">30.6</span></span></span>
<span id="S4.T3.1.1.1.1.1.1.8.6" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.8.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:12.0pt;padding-right:12.0pt;"><span id="S4.T3.1.1.1.1.1.1.8.6.1.1" class="ltx_ERROR undefined">\rowcolor</span>[HTML]FFCE93
F</span>
<span id="S4.T3.1.1.1.1.1.1.8.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:12.0pt;padding-right:12.0pt;">✓</span>
<span id="S4.T3.1.1.1.1.1.1.8.6.3" class="ltx_td ltx_border_r ltx_border_tt" style="padding-left:12.0pt;padding-right:12.0pt;"></span>
<span id="S4.T3.1.1.1.1.1.1.8.6.4" class="ltx_td ltx_border_r ltx_border_tt" style="padding-left:12.0pt;padding-right:12.0pt;"></span>
<span id="S4.T3.1.1.1.1.1.1.8.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:12.0pt;padding-right:12.0pt;"><span id="S4.T3.1.1.1.1.1.1.8.6.5.1" class="ltx_ERROR undefined">\cellcolor</span>[HTML]FFCE93</span>
<span id="S4.T3.1.1.1.1.1.1.8.6.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:12.0pt;padding-right:12.0pt;">21.6</span>
<span id="S4.T3.1.1.1.1.1.1.8.6.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:12.0pt;padding-right:12.0pt;">31.3</span></span>
<span id="S4.T3.1.1.1.1.1.1.9.7" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.9.7.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:12.0pt;padding-right:12.0pt;"><span id="S4.T3.1.1.1.1.1.1.9.7.1.1" class="ltx_ERROR undefined">\rowcolor</span>[HTML]FFCE93
G</span>
<span id="S4.T3.1.1.1.1.1.1.9.7.2" class="ltx_td ltx_border_r" style="padding-left:12.0pt;padding-right:12.0pt;"></span>
<span id="S4.T3.1.1.1.1.1.1.9.7.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:12.0pt;padding-right:12.0pt;">✓</span>
<span id="S4.T3.1.1.1.1.1.1.9.7.4" class="ltx_td ltx_border_r" style="padding-left:12.0pt;padding-right:12.0pt;"></span>
<span id="S4.T3.1.1.1.1.1.1.9.7.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:12.0pt;padding-right:12.0pt;"><span id="S4.T3.1.1.1.1.1.1.9.7.5.1" class="ltx_text"><span id="S4.T3.1.1.1.1.1.1.9.7.5.1.1" class="ltx_ERROR undefined">\cellcolor</span>[HTML]FFCE93IN-L</span></span>
<span id="S4.T3.1.1.1.1.1.1.9.7.6" class="ltx_td ltx_align_center" style="padding-left:12.0pt;padding-right:12.0pt;">23.8</span>
<span id="S4.T3.1.1.1.1.1.1.9.7.7" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:12.0pt;padding-right:12.0pt;">31.3</span></span>
<span id="S4.T3.1.1.1.1.1.1.10.8" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.10.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;"><span id="S4.T3.1.1.1.1.1.1.10.8.1.1" class="ltx_ERROR undefined">\rowcolor</span>[HTML]DAE8FC
H</span>
<span id="S4.T3.1.1.1.1.1.1.10.8.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;"></span>
<span id="S4.T3.1.1.1.1.1.1.10.8.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;"></span>
<span id="S4.T3.1.1.1.1.1.1.10.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;">✓</span>
<span id="S4.T3.1.1.1.1.1.1.10.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;">IN-L</span>
<span id="S4.T3.1.1.1.1.1.1.10.8.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;">25.8</span>
<span id="S4.T3.1.1.1.1.1.1.10.8.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;">32.7</span></span>
<span id="S4.T3.1.1.1.1.1.1.11.9" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.11.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;"><span id="S4.T3.1.1.1.1.1.1.11.9.1.1" class="ltx_ERROR undefined">\rowcolor</span>[HTML]C0C0C0
I</span>
<span id="S4.T3.1.1.1.1.1.1.11.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;">✓</span>
<span id="S4.T3.1.1.1.1.1.1.11.9.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;"></span>
<span id="S4.T3.1.1.1.1.1.1.11.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;">✓</span>
<span id="S4.T3.1.1.1.1.1.1.11.9.5" class="ltx_td ltx_border_r ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;"></span>
<span id="S4.T3.1.1.1.1.1.1.11.9.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;"><span id="S4.T3.1.1.1.1.1.1.11.9.6.1" class="ltx_text ltx_font_italic" style="color:#FE0000;">26.5</span></span>
<span id="S4.T3.1.1.1.1.1.1.11.9.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:12.0pt;padding-right:12.0pt;"><span id="S4.T3.1.1.1.1.1.1.11.9.7.1" class="ltx_text ltx_font_italic" style="color:#FE0000;">32.8</span></span></span>
<span id="S4.T3.1.1.1.1.1.1.12.10" class="ltx_tr">
<span id="S4.T3.1.1.1.1.1.1.12.10.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:12.0pt;padding-right:12.0pt;"><span id="S4.T3.1.1.1.1.1.1.12.10.1.1" class="ltx_ERROR undefined">\rowcolor</span>[HTML]C0C0C0
J</span>
<span id="S4.T3.1.1.1.1.1.1.12.10.2" class="ltx_td ltx_border_bb ltx_border_r" style="padding-left:12.0pt;padding-right:12.0pt;"></span>
<span id="S4.T3.1.1.1.1.1.1.12.10.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:12.0pt;padding-right:12.0pt;">✓</span>
<span id="S4.T3.1.1.1.1.1.1.12.10.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:12.0pt;padding-right:12.0pt;">✓</span>
<span id="S4.T3.1.1.1.1.1.1.12.10.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:12.0pt;padding-right:12.0pt;"><span id="S4.T3.1.1.1.1.1.1.12.10.5.1" class="ltx_text">IN-L</span></span>
<span id="S4.T3.1.1.1.1.1.1.12.10.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:12.0pt;padding-right:12.0pt;"><span id="S4.T3.1.1.1.1.1.1.12.10.6.1" class="ltx_text ltx_font_bold" style="color:#3531FF;">27.3</span></span>
<span id="S4.T3.1.1.1.1.1.1.12.10.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:12.0pt;padding-right:12.0pt;"><span id="S4.T3.1.1.1.1.1.1.12.10.7.1" class="ltx_text ltx_font_bold" style="color:#3531FF;">33.1</span></span></span>
</span>
</span></span></span>
</span></span></span></p>
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>
Detection performance on the LVIS OVOD benchmark comparing all three of our methods:
(1) <span id="S4.T3.14.1" class="ltx_text" style="background-color:#FFCE93;">orange</span> — vision-based classifiers;
(2) <span id="S4.T3.15.2" class="ltx_text" style="background-color:#DAE8FC;">blue</span> — text-based classifiers;
(3) <span id="S4.T3.16.3" class="ltx_text" style="background-color:#C0C0C0;">grey</span> — multi-modal classifiers.
Results for models trained only on LVIS-base and LVIS-base+IN-L
are shown in the top and bottom halves, respectively.
Visual Mean?: <em id="S4.T3.17.4" class="ltx_emph ltx_font_italic">simple vector mean</em> is used to combine
visual embeddings of image exemplars,
Visual Agg.?: <em id="S4.T3.18.5" class="ltx_emph ltx_font_italic">our visual aggregator</em> is used to combine
visual embeddings,
Text Cls.?: text-based classifiers are used.
Models which use text-based and vision-based classifiers represent our models
with multi-modal classifiers.
We report mask AP metrics.
</figcaption>
</figure>
<div id="S4.SS4.p2" class="ltx_para ltx_noindent">
<p id="S4.SS4.p2.3" class="ltx_p"><span id="S4.SS4.p2.3.1" class="ltx_text ltx_font_bold">Results with Multi-Modal Classifiers.</span>
To evaluate the effectiveness of multi-modal classifiers
we perform similar experiments
as those using vision-based classifiers,
except for each model we combine the vision-based classifiers with the
text-based classifiers,
as described in Section <a href="#S3.SS4" title="3.4 Constructing Classifiers via Multi-Modal Fusion ‣ 3 Method ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>.
Results for multi-modal classifiers are shown in <span id="S4.SS4.p2.3.2" class="ltx_text" style="background-color:#C0C0C0;">grey</span>
rows of Table <a href="#S4.T3" title="Table 3 ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
With no additional image-level data,
the vector mean baseline (Model D)
outperforms the use of our aggregator (Model E) by <math id="S4.SS4.p2.1.m1.1" class="ltx_Math" alttext="1.4" display="inline"><semantics id="S4.SS4.p2.1.m1.1a"><mn id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml">1.4</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><cn type="float" id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1">1.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">1.4</annotation></semantics></math> APr.
However, for models with image-level data (IN-L) our aggregator (Model J) boosts
performance by <math id="S4.SS4.p2.2.m2.1" class="ltx_Math" alttext="0.8" display="inline"><semantics id="S4.SS4.p2.2.m2.1a"><mn id="S4.SS4.p2.2.m2.1.1" xref="S4.SS4.p2.2.m2.1.1.cmml">0.8</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.2.m2.1b"><cn type="float" id="S4.SS4.p2.2.m2.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1">0.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.2.m2.1c">0.8</annotation></semantics></math> APr compared to the baseline (Model I).
Furthermore, comparing the multi-modal classifiers
(<span id="S4.SS4.p2.3.3" class="ltx_text" style="background-color:#C0C0C0;">grey</span> rows in Table <a href="#S4.T3" title="Table 3 ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>)
with text-based classifiers (<span id="S4.SS4.p2.3.4" class="ltx_text" style="background-color:#DAE8FC;">blue</span> rows in Table <a href="#S4.T3" title="Table 3 ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>)
demonstrates that in all cases adding information from image exemplars yields
improved OVOD performance — our best multi-modal model improves performance
over our best text-based model by <math id="S4.SS4.p2.3.m3.1" class="ltx_Math" alttext="1.5" display="inline"><semantics id="S4.SS4.p2.3.m3.1a"><mn id="S4.SS4.p2.3.m3.1.1" xref="S4.SS4.p2.3.m3.1.1.cmml">1.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.3.m3.1b"><cn type="float" id="S4.SS4.p2.3.m3.1.1.cmml" xref="S4.SS4.p2.3.m3.1.1">1.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.3.m3.1c">1.5</annotation></semantics></math> APr confirming that combining
the vision and text modalities utilises complementary information between
the two.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para ltx_noindent">
<p id="S4.SS4.p3.1" class="ltx_p"><span id="S4.SS4.p3.1.1" class="ltx_text ltx_font_bold">Relationship between IN-L and LVIS “rare” classes.</span>
Section <a href="#A2" title="Appendix B A Closer Look at “rare” Class Performance ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a> of the Appendix splits the APr metric into two
based on the “rare” LVIS categories contained in IN-L.
One may expect improvements in APr performance when training on
IN-L to only come from “rare” categories found in IN-L.
Our evaluation finds this not to be the case.
Detailed results can be found in the Appendix (Section <a href="#A2" title="Appendix B A Closer Look at “rare” Class Performance ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>)
which breaks the APr metric into “rare” categories found in IN-L and those not.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para ltx_noindent">
<p id="S4.SS4.p4.1" class="ltx_p"><span id="S4.SS4.p4.1.1" class="ltx_text ltx_font_bold">Additional Ablation Experiments.</span>
Section <a href="#A1" title="Appendix A Ablation Studies ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a> of the Appendix presents ablation experiments which demonstrate:
(1) applying a learnable bias before calculating the final detection score for a region
improves OVOD performance;
(2) improvements in OVOD performance using our text-based classifiers is orthogonal to
applying this learnable bias;
(3) applying TTA on image exemplars yield better vision-based classifiers for OVOD;
(4) comparisons between our text-based classifiers and those generated from manual prompts.
Please refer to Section <a href="#A1" title="Appendix A Ablation Studies ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a> of the Appendix for details and evaluation results
for these experiments.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we tackle open-vocabulary object detection by
investigating the importance of the method used to generate classifiers for
open-vocabulary detection.
This work goes beyond the very simple methods used in prior work to generate
such classifiers — with the class name only.
We present a novel method which combines a large language model (LLM) and a visual-language model (VLM) to produce improved classifiers.
Moreover, we investigate using image exemplars to provide classifiers
for OVOD and present a method for generating such classifiers using a large
classification dataset and a simple transformer based architecture.
Finally, we combine our classifiers from the two modalities to produce
multi-modal classifiers for OVOD.
Our experiments show that our method using natural language only outperforms current state-of-the-art OVOD works,
especially in cases where no extra image-level data is used.
Furthermore, our multi-modal classifiers set new state-of-the-art performance
with a large improvement over prior work.</p>
</div>
</section>
<section id="Sx1" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">We thank Lauren Bates-Brownsword for help with proofreading.
This research was supported by the EPSRC CDT in AIMS EP/L015897/1 and EP/S515541/1,
the EPSRC Programme Grant VisualAI EP/T028572/1,
and a Royal Society Research Professorship.
Weidi Xie would like to acknowledge the National Key R&amp;D Program of China (No. 2022ZD0161400).</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography ltx_centering">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.5.5.1" class="ltx_text" style="font-size:90%;">Bansal et al. (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text" style="font-size:90%;">
Bansal, A., Sikka, K., Sharma, G., Chellappa, R., and Divakaran, A.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.8.1" class="ltx_text" style="font-size:90%;">Zero-shot object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib1.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</em><span id="bib.bib1.11.3" class="ltx_text" style="font-size:90%;">,
September 2018.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.5.5.1" class="ltx_text" style="font-size:90%;">Brown et al. (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P.,
Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,
I., and Amodei, D.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">Language models are few-shot learners.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib2.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib2.11.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.5.5.1" class="ltx_text" style="font-size:90%;">Cai &amp; Vasconcelos (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">
Cai, Z. and Vasconcelos, N.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.8.1" class="ltx_text" style="font-size:90%;">Cascade r-cnn: Delving into high quality object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib3.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em><span id="bib.bib3.11.3" class="ltx_text" style="font-size:90%;">, pp.  6154–6162. IEEE Computer Society, 2018.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.5.5.1" class="ltx_text" style="font-size:90%;">Carion et al. (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">
Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko,
S.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.8.1" class="ltx_text" style="font-size:90%;">End-to-end object detection with transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib4.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</em><span id="bib.bib4.11.3" class="ltx_text" style="font-size:90%;">,
pp.  213–229. Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">
Chen, D.-J., Hsieh, H.-Y., and Liu, T.-L.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">Adaptive image transformer for one-shot object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib5.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em><span id="bib.bib5.11.3" class="ltx_text" style="font-size:90%;">, pp.  12242–12251, 2021.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">
Chen, K., Pang, J., Wang, J., Xiong, Y., Li, X., Sun, S., Feng, W., Liu, Z.,
Shi, J., Ouyang, W., et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.8.1" class="ltx_text" style="font-size:90%;">Hybrid task cascade for instance segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib6.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em><span id="bib.bib6.11.3" class="ltx_text" style="font-size:90%;">, pp.  4974–4983, 2019.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="font-size:90%;">A simple framework for contrastive learning of visual
representations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib7.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Machine
Learning</em><span id="bib.bib7.11.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.5.5.1" class="ltx_text" style="font-size:90%;">Choudhury et al. (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">
Choudhury, S., Laina, I., Rupprecht, C., and Vedaldi, A.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.8.1" class="ltx_text" style="font-size:90%;">The curious layperson: Fine-grained image recognition without expert
labels.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib8.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the British Machine Vision Conference</em><span id="bib.bib8.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.5.5.1" class="ltx_text" style="font-size:90%;">Deng et al. (2009)</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text" style="font-size:90%;">Imagenet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib9.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em><span id="bib.bib9.11.3" class="ltx_text" style="font-size:90%;">, 2009.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.5.5.1" class="ltx_text" style="font-size:90%;">Dosovitskiy et al. (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
Uszkoreit, J., and Houlsby, N.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.8.1" class="ltx_text" style="font-size:90%;">An image is worth 16x16 words: Transformers for image recognition at
scale.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib10.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Learning
Representations</em><span id="bib.bib10.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.5.5.1" class="ltx_text" style="font-size:90%;">Elhoseiny et al. (2017)</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">
Elhoseiny, M., Zhu, Y., Zhang, H., and Elgammal, A.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.8.1" class="ltx_text" style="font-size:90%;">Link the head to the ”beak”: Zero shot learning from noisy text
description at part precision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib11.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em><span id="bib.bib11.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.5.5.1" class="ltx_text" style="font-size:90%;">Feng et al. (2022)</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="font-size:90%;">
Feng, C., Zhong, Y., Jie, Z., Chu, X., Ren, H., Wei, X., Xie, W., and Ma, L.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.8.1" class="ltx_text" style="font-size:90%;">Promptdet: Towards open-vocabulary detection using uncurated images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.9.1" class="ltx_text" style="font-size:90%;">2022.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.4.4.1" class="ltx_text" style="font-size:90%;">Girshick (2015)</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.6.1" class="ltx_text" style="font-size:90%;">
Girshick, R. B.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text" style="font-size:90%;">Fast R-CNN.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib13.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Computer
Vision</em><span id="bib.bib13.10.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.5.5.1" class="ltx_text" style="font-size:90%;">Gu et al. (2022)</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.7.1" class="ltx_text" style="font-size:90%;">
Gu, X., Lin, T.-Y., Kuo, W., and Cui, Y.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.8.1" class="ltx_text" style="font-size:90%;">Open-vocabulary detection via vision and language knowledge
distillation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib14.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Learning
Representations</em><span id="bib.bib14.11.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.5.5.1" class="ltx_text" style="font-size:90%;">Gupta et al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text" style="font-size:90%;">
Gupta, A., Dollar, P., and Girshick, R.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.8.1" class="ltx_text" style="font-size:90%;">LVIS: A dataset for large vocabulary instance segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib15.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em><span id="bib.bib15.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.5.5.1" class="ltx_text" style="font-size:90%;">He et al. (2016)</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text" style="font-size:90%;">
He, K., Zhang, X., Ren, S., and Sun, J.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.8.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib16.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em><span id="bib.bib16.11.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.5.5.1" class="ltx_text" style="font-size:90%;">Hsieh et al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="font-size:90%;">
Hsieh, T.-I., Lo, Y.-C., Chen, H.-T., and Liu, T.-L.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text" style="font-size:90%;">One-shot object detection with co-attention and co-excitation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib17.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib17.11.3" class="ltx_text" style="font-size:90%;">. 2019.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.5.5.1" class="ltx_text" style="font-size:90%;">Jia et al. (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text" style="font-size:90%;">
Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q., Sung,
Y.-H., Li, Z., and Duerig, T.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.8.1" class="ltx_text" style="font-size:90%;">Scaling up visual and vision-language representation learning with
noisy text supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib18.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Machine
Learning</em><span id="bib.bib18.11.3" class="ltx_text" style="font-size:90%;">, pp.  4904–4916, 2021.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.5.5.1" class="ltx_text" style="font-size:90%;">Kamath et al. (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text" style="font-size:90%;">
Kamath, A., Singh, M., LeCun, Y., Synnaeve, G., Misra, I., and Carion, N.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.8.1" class="ltx_text" style="font-size:90%;">Mdetr-modulated detection for end-to-end multi-modal understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib19.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em><span id="bib.bib19.11.3" class="ltx_text" style="font-size:90%;">, pp.  1780–1790, 2021.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.5.5.1" class="ltx_text" style="font-size:90%;">Kang et al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="font-size:90%;">
Kang, B., Liu, Z., Wang, X., Yu, F., Feng, J., and Darrell, T.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.8.1" class="ltx_text" style="font-size:90%;">Few-shot object detection via feature reweighting.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib20.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Computer
Vision</em><span id="bib.bib20.11.3" class="ltx_text" style="font-size:90%;">, pp.  8420–8429, 2019.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.5.5.1" class="ltx_text" style="font-size:90%;">Kaul et al. (2022)</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text" style="font-size:90%;">
Kaul, P., Xie, W., and Zisserman, A.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.8.1" class="ltx_text" style="font-size:90%;">Label, verify, correct: A simple few-shot object detection method.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib21.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em><span id="bib.bib21.11.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.5.5.1" class="ltx_text" style="font-size:90%;">Krishna et al. (2017)</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.7.1" class="ltx_text" style="font-size:90%;">
Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S.,
Kalantidis, Y., Li, L.-J., Shamma, D. A., et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.8.1" class="ltx_text" style="font-size:90%;">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib22.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</em><span id="bib.bib22.10.2" class="ltx_text" style="font-size:90%;">, 123(1):32–73, 2017.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.5.5.1" class="ltx_text" style="font-size:90%;">Kuo et al. (2022)</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.7.1" class="ltx_text" style="font-size:90%;">
Kuo, W., Cui, Y., Gu, X., Piergiovanni, A., and Angelova, A.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.8.1" class="ltx_text" style="font-size:90%;">F-vlm: Open-vocabulary object detection upon frozen vision and
language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib23.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2209.15639</em><span id="bib.bib23.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.5.5.1" class="ltx_text" style="font-size:90%;">Law &amp; Deng (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text" style="font-size:90%;">
Law, H. and Deng, J.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.8.1" class="ltx_text" style="font-size:90%;">Cornernet: Detecting objects as paired keypoints.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib24.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</em><span id="bib.bib24.11.3" class="ltx_text" style="font-size:90%;">,
2018.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. (2022)</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text" style="font-size:90%;">
Li, L. H., Zhang*, P., Zhang*, H., Yang, J., Li, C., Zhong, Y., Wang, L., Yuan,
L., Zhang, L., Hwang, J.-N., Chang, K.-W., and Gao, J.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.8.1" class="ltx_text" style="font-size:90%;">Grounded language-image pre-training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib25.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em><span id="bib.bib25.11.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text" style="font-size:90%;">
Li, Y., Chen, Y., Wang, N., and Zhang, Z.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.8.1" class="ltx_text" style="font-size:90%;">Scale-aware trident networks for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib26.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Computer
Vision</em><span id="bib.bib26.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.5.5.1" class="ltx_text" style="font-size:90%;">Lin et al. (2014)</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.7.1" class="ltx_text" style="font-size:90%;">
Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,
Dollár, P., and Zitnick, C. L.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.8.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib27.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</em><span id="bib.bib27.11.3" class="ltx_text" style="font-size:90%;">,
2014.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.5.5.1" class="ltx_text" style="font-size:90%;">Lin et al. (2017)</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.7.1" class="ltx_text" style="font-size:90%;">
Lin, T.-Y., Goyal, P., Girshick, R., He, K., and Dollár, P.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.8.1" class="ltx_text" style="font-size:90%;">Focal loss for dense object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib28.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Computer
Vision</em><span id="bib.bib28.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. (2016)</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.7.1" class="ltx_text" style="font-size:90%;">
Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., and Berg,
A. C.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.8.1" class="ltx_text" style="font-size:90%;">Ssd: Single shot multibox detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib29.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</em><span id="bib.bib29.11.3" class="ltx_text" style="font-size:90%;">,
pp.  21–37. Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.7.1" class="ltx_text" style="font-size:90%;">
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.8.1" class="ltx_text" style="font-size:90%;">Swin transformer: Hierarchical vision transformer using shifted
windows.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib30.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Computer
Vision</em><span id="bib.bib30.11.3" class="ltx_text" style="font-size:90%;">, pp.  10012–10022, October 2021.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.5.5.1" class="ltx_text" style="font-size:90%;">Loshchilov &amp; Hutter (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.7.1" class="ltx_text" style="font-size:90%;">
Loshchilov, I. and Hutter, F.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.8.1" class="ltx_text" style="font-size:90%;">Decoupled weight decay regularization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib31.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Learning
Representations</em><span id="bib.bib31.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.5.5.1" class="ltx_text" style="font-size:90%;">Menon &amp; Vondrick (2023)</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.7.1" class="ltx_text" style="font-size:90%;">
Menon, S. and Vondrick, C.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.8.1" class="ltx_text" style="font-size:90%;">Visual classification via description from large language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib32.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Learning
Representations</em><span id="bib.bib32.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.4.4.1" class="ltx_text" style="font-size:90%;">Miller (1995)</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.6.1" class="ltx_text" style="font-size:90%;">
Miller, G. A.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.7.1" class="ltx_text" style="font-size:90%;">Wordnet: A lexical database for english.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib33.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Communications of the Association for Computing Machinery</em><span id="bib.bib33.9.2" class="ltx_text" style="font-size:90%;">,
38(11):39–41, nov 1995.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.5.5.1" class="ltx_text" style="font-size:90%;">Minderer et al. (2022)</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.7.1" class="ltx_text" style="font-size:90%;">
Minderer, M., Gritsenko, A., Stone, A., Neumann, M., Weissenborn, D.,
Dosovitskiy, A., Mahendran, A., Arnab, A., Dehghani, M., Shen, Z., Wang, X.,
Zhai, X., Kipf, T., and Houlsby, N.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.8.1" class="ltx_text" style="font-size:90%;">Simple open-vocabulary object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib34.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</em><span id="bib.bib34.11.3" class="ltx_text" style="font-size:90%;">,
pp.  728–755, 2022.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.5.5.1" class="ltx_text" style="font-size:90%;">Osokin et al. (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.7.1" class="ltx_text" style="font-size:90%;">
Osokin, A., Sumin, D., and Lomakin, V.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.8.1" class="ltx_text" style="font-size:90%;">OS2D: One-stage one-shot object detection by matching anchor
features.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib35.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</em><span id="bib.bib35.11.3" class="ltx_text" style="font-size:90%;">,
2020.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.5.5.1" class="ltx_text" style="font-size:90%;">Pratt et al. (2022)</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.7.1" class="ltx_text" style="font-size:90%;">
Pratt, S., Liu, R., and Farhadi, A.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.8.1" class="ltx_text" style="font-size:90%;">What does a platypus look like? generating customized prompts for
zero-shot image classification.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib36.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2209.03320</em><span id="bib.bib36.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.5.5.1" class="ltx_text" style="font-size:90%;">Qiao et al. (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.7.1" class="ltx_text" style="font-size:90%;">
Qiao, L., Zhao, Y., Li, Z., Qiu, X., Wu, J., and Zhang, C.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.8.1" class="ltx_text" style="font-size:90%;">Defrcn: Decoupled faster r-cnn for few-shot object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib37.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Computer
Vision</em><span id="bib.bib37.11.3" class="ltx_text" style="font-size:90%;">, pp.  8681–8690, October 2021.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.5.5.1" class="ltx_text" style="font-size:90%;">Radford et al. (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.7.1" class="ltx_text" style="font-size:90%;">
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.8.1" class="ltx_text" style="font-size:90%;">Learning transferable visual models from natural language
supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib38.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Machine
Learning</em><span id="bib.bib38.11.3" class="ltx_text" style="font-size:90%;">, pp.  8748–8763, 2021.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.5.5.1" class="ltx_text" style="font-size:90%;">Redmon &amp; Farhadi (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.7.1" class="ltx_text" style="font-size:90%;">
Redmon, J. and Farhadi, A.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.8.1" class="ltx_text" style="font-size:90%;">Yolov3: An incremental improvement.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib39.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1804.02767</em><span id="bib.bib39.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.5.5.1" class="ltx_text" style="font-size:90%;">Redmon et al. (2016)</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.7.1" class="ltx_text" style="font-size:90%;">
Redmon, J., Divvala, S. K., Girshick, R. B., and Farhadi, A.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.8.1" class="ltx_text" style="font-size:90%;">You only look once: Unified, real-time object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib40.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em><span id="bib.bib40.11.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.5.5.1" class="ltx_text" style="font-size:90%;">Ren et al. (2016)</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.7.1" class="ltx_text" style="font-size:90%;">
Ren, S., He, K., Girshick, R., and Sun, J.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.8.1" class="ltx_text" style="font-size:90%;">Faster R-CNN: Towards real-time object detection with region
proposal networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib41.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib41.11.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.5.5.1" class="ltx_text" style="font-size:90%;">Ridnik et al. (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.7.1" class="ltx_text" style="font-size:90%;">
Ridnik, T., Ben-Baruch, E., Noy, A., and Zelnik, L.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.8.1" class="ltx_text" style="font-size:90%;">Imagenet-21k pretraining for the masses.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib42.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the Neural Information Processing Systems
Track on Datasets and Benchmarks</em><span id="bib.bib42.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.5.5.1" class="ltx_text" style="font-size:90%;">Shao et al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.7.1" class="ltx_text" style="font-size:90%;">
Shao, S., Li, Z., Zhang, T., Peng, C., Yu, G., Zhang, X., Li, J., and Sun, J.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.8.1" class="ltx_text" style="font-size:90%;">Objects365: A large-scale, high-quality dataset for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib43.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em><span id="bib.bib43.11.3" class="ltx_text" style="font-size:90%;">, pp.  8429–8438, 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.12.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.1109/ICCV.2019.00852</span><span id="bib.bib43.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.5.5.1" class="ltx_text" style="font-size:90%;">Srivastava et al. (2014)</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.7.1" class="ltx_text" style="font-size:90%;">
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
R.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.8.1" class="ltx_text" style="font-size:90%;">Dropout: A simple way to prevent neural networks from overfitting.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib44.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of Machine Learning Research</em><span id="bib.bib44.11.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.5.5.1" class="ltx_text" style="font-size:90%;">Sun et al. (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.7.1" class="ltx_text" style="font-size:90%;">
Sun, B., Li, B., Cai, S., Yuan, Y., and Zhang, C.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.8.1" class="ltx_text" style="font-size:90%;">Fsce: Few-shot object detection via contrastive proposal encoding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib45.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em><span id="bib.bib45.11.3" class="ltx_text" style="font-size:90%;">, June 2021.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.5.5.1" class="ltx_text" style="font-size:90%;">Tan et al. (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.7.1" class="ltx_text" style="font-size:90%;">
Tan, M., Pang, R., and Le, Q. V.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.8.1" class="ltx_text" style="font-size:90%;">Efficientdet: Scalable and efficient object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib46.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em><span id="bib.bib46.11.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.5.5.1" class="ltx_text" style="font-size:90%;">Tian et al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.7.1" class="ltx_text" style="font-size:90%;">
Tian, Z., Shen, C., Chen, H., and He, T.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.8.1" class="ltx_text" style="font-size:90%;">Fcos: Fully convolutional one-stage object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib47.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Computer
Vision</em><span id="bib.bib47.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib48.5.5.1" class="ltx_text" style="font-size:90%;">van den Oord et al. (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib48.7.1" class="ltx_text" style="font-size:90%;">
van den Oord, A., Li, Y., and Vinyals, O.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.8.1" class="ltx_text" style="font-size:90%;">Representation learning with contrastive predictive coding.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib48.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1807.03748</em><span id="bib.bib48.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib49.5.5.1" class="ltx_text" style="font-size:90%;">Vaswani et al. (2017)</span></span>
<span class="ltx_bibblock"><span id="bib.bib49.7.1" class="ltx_text" style="font-size:90%;">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,
Kaiser, L., and Polosukhin, I.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.8.1" class="ltx_text" style="font-size:90%;">Attention is all you need.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib49.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">NIPS</em><span id="bib.bib49.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib50.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib50.7.1" class="ltx_text" style="font-size:90%;">
Wang, X., Huang, T. E., Darrell, T., Gonzalez, J. E., and Yu, F.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.8.1" class="ltx_text" style="font-size:90%;">Frustratingly simple few-shot object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib50.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Machine
Learning</em><span id="bib.bib50.11.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib51.5.5.1" class="ltx_text" style="font-size:90%;">Zang et al. (2022)</span></span>
<span class="ltx_bibblock"><span id="bib.bib51.7.1" class="ltx_text" style="font-size:90%;">
Zang, Y., Li, W., Zhou, K., Huang, C., and Loy, C. C.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.8.1" class="ltx_text" style="font-size:90%;">Open-vocabulary detr with conditional matching.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.9.1" class="ltx_text" style="font-size:90%;">2022.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib52.5.5.1" class="ltx_text" style="font-size:90%;">Zareian et al. (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib52.7.1" class="ltx_text" style="font-size:90%;">
Zareian, A., Rosa, K. D., Hu, D. H., and Chang, S.-F.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.8.1" class="ltx_text" style="font-size:90%;">Open-vocabulary object detection using captions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib52.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em><span id="bib.bib52.11.3" class="ltx_text" style="font-size:90%;">, pp.  14393–14402, 2021.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib53.5.5.1" class="ltx_text" style="font-size:90%;">Zhong et al. (2022)</span></span>
<span class="ltx_bibblock"><span id="bib.bib53.7.1" class="ltx_text" style="font-size:90%;">
Zhong, Y., Yang, J., Zhang, P., Li, C., Codella, N., Li, L. H., Zhou, L., Dai,
X., Yuan, L., Li, Y., and Gao, J.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.8.1" class="ltx_text" style="font-size:90%;">Regionclip: Region-based language-image pretraining.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib53.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em><span id="bib.bib53.11.3" class="ltx_text" style="font-size:90%;">, pp.  16793–16803, 2022.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib54.5.5.1" class="ltx_text" style="font-size:90%;">Zhou et al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib54.7.1" class="ltx_text" style="font-size:90%;">
Zhou, X., Wang, D., and Krähenbühl, P.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.8.1" class="ltx_text" style="font-size:90%;">Objects as points.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib54.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1904.07850</em><span id="bib.bib54.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib55.5.5.1" class="ltx_text" style="font-size:90%;">Zhou et al. (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib55.7.1" class="ltx_text" style="font-size:90%;">
Zhou, X., Koltun, V., and Krähenbühl, P.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.8.1" class="ltx_text" style="font-size:90%;">Probabilistic two-stage detection.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib55.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2103.07461</em><span id="bib.bib55.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib56.5.5.1" class="ltx_text" style="font-size:90%;">Zhou et al. (2022)</span></span>
<span class="ltx_bibblock"><span id="bib.bib56.7.1" class="ltx_text" style="font-size:90%;">
Zhou, X., Girdhar, R., Joulin, A., Krähenbühl, P., and Misra, I.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.8.1" class="ltx_text" style="font-size:90%;">Detecting twenty-thousand classes using image-level supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib56.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</em><span id="bib.bib56.11.3" class="ltx_text" style="font-size:90%;">,
pp.  350–368, 2022.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_centering ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix ltx_centering">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Ablation Studies</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.3" class="ltx_p">We now ablate some of the key components using the open-vocabulary LVIS
benchmark without any extra image classification data
<span id="A1.p1.3.1" class="ltx_ERROR undefined">\ie</span> <math id="A1.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{D}^{\textsc{img}}=\varnothing" display="inline"><semantics id="A1.p1.1.m1.1a"><mrow id="A1.p1.1.m1.1.1" xref="A1.p1.1.m1.1.1.cmml"><msup id="A1.p1.1.m1.1.1.2" xref="A1.p1.1.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="A1.p1.1.m1.1.1.2.2" xref="A1.p1.1.m1.1.1.2.2.cmml">𝒟</mi><mtext class="ltx_font_smallcaps" id="A1.p1.1.m1.1.1.2.3" xref="A1.p1.1.m1.1.1.2.3a.cmml">img</mtext></msup><mo id="A1.p1.1.m1.1.1.1" xref="A1.p1.1.m1.1.1.1.cmml">=</mo><mi mathvariant="normal" id="A1.p1.1.m1.1.1.3" xref="A1.p1.1.m1.1.1.3.cmml">∅</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.1.m1.1b"><apply id="A1.p1.1.m1.1.1.cmml" xref="A1.p1.1.m1.1.1"><eq id="A1.p1.1.m1.1.1.1.cmml" xref="A1.p1.1.m1.1.1.1"></eq><apply id="A1.p1.1.m1.1.1.2.cmml" xref="A1.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="A1.p1.1.m1.1.1.2.1.cmml" xref="A1.p1.1.m1.1.1.2">superscript</csymbol><ci id="A1.p1.1.m1.1.1.2.2.cmml" xref="A1.p1.1.m1.1.1.2.2">𝒟</ci><ci id="A1.p1.1.m1.1.1.2.3a.cmml" xref="A1.p1.1.m1.1.1.2.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="A1.p1.1.m1.1.1.2.3.cmml" xref="A1.p1.1.m1.1.1.2.3">img</mtext></ci></apply><emptyset id="A1.p1.1.m1.1.1.3.cmml" xref="A1.p1.1.m1.1.1.3"></emptyset></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.1.m1.1c">\mathcal{D}^{\textsc{img}}=\varnothing</annotation></semantics></math>, unless stated otherwise.
All metrics have the standard LVIS definition and we report mask AP metrics in all cases.
For reference APr, APc and APf represent mean average precision across “rare”, “common”
and “frequent” classes, respectively, as defined in LVIS.
Moreover, mAP, AP50 and AP75 represent mean average precision across all classes but
for all intersection-over-union (IoU) criteria, IoU<math id="A1.p1.2.m2.1" class="ltx_Math" alttext="=0.5" display="inline"><semantics id="A1.p1.2.m2.1a"><mrow id="A1.p1.2.m2.1.1" xref="A1.p1.2.m2.1.1.cmml"><mi id="A1.p1.2.m2.1.1.2" xref="A1.p1.2.m2.1.1.2.cmml"></mi><mo id="A1.p1.2.m2.1.1.1" xref="A1.p1.2.m2.1.1.1.cmml">=</mo><mn id="A1.p1.2.m2.1.1.3" xref="A1.p1.2.m2.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.2.m2.1b"><apply id="A1.p1.2.m2.1.1.cmml" xref="A1.p1.2.m2.1.1"><eq id="A1.p1.2.m2.1.1.1.cmml" xref="A1.p1.2.m2.1.1.1"></eq><csymbol cd="latexml" id="A1.p1.2.m2.1.1.2.cmml" xref="A1.p1.2.m2.1.1.2">absent</csymbol><cn type="float" id="A1.p1.2.m2.1.1.3.cmml" xref="A1.p1.2.m2.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.2.m2.1c">=0.5</annotation></semantics></math> and IoU<math id="A1.p1.3.m3.1" class="ltx_Math" alttext="=0.75" display="inline"><semantics id="A1.p1.3.m3.1a"><mrow id="A1.p1.3.m3.1.1" xref="A1.p1.3.m3.1.1.cmml"><mi id="A1.p1.3.m3.1.1.2" xref="A1.p1.3.m3.1.1.2.cmml"></mi><mo id="A1.p1.3.m3.1.1.1" xref="A1.p1.3.m3.1.1.1.cmml">=</mo><mn id="A1.p1.3.m3.1.1.3" xref="A1.p1.3.m3.1.1.3.cmml">0.75</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.3.m3.1b"><apply id="A1.p1.3.m3.1.1.cmml" xref="A1.p1.3.m3.1.1"><eq id="A1.p1.3.m3.1.1.1.cmml" xref="A1.p1.3.m3.1.1.1"></eq><csymbol cd="latexml" id="A1.p1.3.m3.1.1.2.cmml" xref="A1.p1.3.m3.1.1.2">absent</csymbol><cn type="float" id="A1.p1.3.m3.1.1.3.cmml" xref="A1.p1.3.m3.1.1.3">0.75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.3.m3.1c">=0.75</annotation></semantics></math>, respectively.</p>
</div>
<figure id="A1.T4" class="ltx_table">
<div id="A1.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:292.6pt;height:54pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="A1.T4.1.1" class="ltx_p"><span id="A1.T4.1.1.1" class="ltx_text">
<span id="A1.T4.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="A1.T4.1.1.1.1.1.1" class="ltx_tr">
<span id="A1.T4.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
Bias?</span>
<span id="A1.T4.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Init. Value</span>
<span id="A1.T4.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mAP</span>
<span id="A1.T4.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">AP50</span>
<span id="A1.T4.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">AP75</span>
<span id="A1.T4.1.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">APr</span>
<span id="A1.T4.1.1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">APc</span>
<span id="A1.T4.1.1.1.1.1.1.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">APf</span></span>
</span>
<span class="ltx_tbody">
<span id="A1.T4.1.1.1.1.2.1" class="ltx_tr">
<span id="A1.T4.1.1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">✗</span>
<span id="A1.T4.1.1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">N/A</span>
<span id="A1.T4.1.1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">29.7</span>
<span id="A1.T4.1.1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">43.7</span>
<span id="A1.T4.1.1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">31.6</span>
<span id="A1.T4.1.1.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">15.6</span>
<span id="A1.T4.1.1.1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">30.5</span>
<span id="A1.T4.1.1.1.1.2.1.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">35.0</span></span>
<span id="A1.T4.1.1.1.1.3.2" class="ltx_tr">
<span id="A1.T4.1.1.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">✓</span>
<span id="A1.T4.1.1.1.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">-2.0</span>
<span id="A1.T4.1.1.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_bb">29.9</span>
<span id="A1.T4.1.1.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_bb">43.5</span>
<span id="A1.T4.1.1.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">32.0</span>
<span id="A1.T4.1.1.1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_bb">17.7</span>
<span id="A1.T4.1.1.1.1.3.2.7" class="ltx_td ltx_align_center ltx_border_bb">30.1</span>
<span id="A1.T4.1.1.1.1.3.2.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">35.0</span></span>
</span>
</span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>
The effect of including a learnable bias on detections scores for OVOD.
The top row does not use a learnable bias, as in Detic.
The bottom row applies a learnable bias prior to computing final detection scores
with logistic sigmoid.
Applying a learnable bias improves performance on novel/rare categories (APr).
We report mask AP metrics.
</figcaption>
</figure>
<div id="A1.p2" class="ltx_para ltx_noindent">
<p id="A1.p2.2" class="ltx_p"><span id="A1.p2.2.1" class="ltx_text ltx_font_bold">Effect of Detection Score Bias.</span>
Table <a href="#A1.T4" title="Table 4 ‣ Appendix A Ablation Studies ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the effect of adding a learnable bias to the
detection scores before applying a logistic sigmoid to get a final detection
score in the range <math id="A1.p2.1.m1.2" class="ltx_Math" alttext="[0,1]" display="inline"><semantics id="A1.p2.1.m1.2a"><mrow id="A1.p2.1.m1.2.3.2" xref="A1.p2.1.m1.2.3.1.cmml"><mo stretchy="false" id="A1.p2.1.m1.2.3.2.1" xref="A1.p2.1.m1.2.3.1.cmml">[</mo><mn id="A1.p2.1.m1.1.1" xref="A1.p2.1.m1.1.1.cmml">0</mn><mo id="A1.p2.1.m1.2.3.2.2" xref="A1.p2.1.m1.2.3.1.cmml">,</mo><mn id="A1.p2.1.m1.2.2" xref="A1.p2.1.m1.2.2.cmml">1</mn><mo stretchy="false" id="A1.p2.1.m1.2.3.2.3" xref="A1.p2.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.p2.1.m1.2b"><interval closure="closed" id="A1.p2.1.m1.2.3.1.cmml" xref="A1.p2.1.m1.2.3.2"><cn type="integer" id="A1.p2.1.m1.1.1.cmml" xref="A1.p2.1.m1.1.1">0</cn><cn type="integer" id="A1.p2.1.m1.2.2.cmml" xref="A1.p2.1.m1.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.1.m1.2c">[0,1]</annotation></semantics></math>
To evaluate the effect of the learnable bias only,
our proposed text-based classifiers sourced from rich class descriptions,
as described in Section <a href="#S3.SS2" title="3.2 Text-based Classifiers from Language Descriptions ‣ 3 Method ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>,
<em id="A1.p2.2.2" class="ltx_emph ltx_font_italic">are not</em> used and instead the same simple
text-based classifiers used in Detic, of form “<span id="A1.p2.2.3" class="ltx_text ltx_font_typewriter">a(n) {class name}</span>”,
are used in this comparison.
We observe adding a learnable bias improves open-vocabulary detection by
<math id="A1.p2.2.m2.1" class="ltx_Math" alttext="2.1" display="inline"><semantics id="A1.p2.2.m2.1a"><mn id="A1.p2.2.m2.1.1" xref="A1.p2.2.m2.1.1.cmml">2.1</mn><annotation-xml encoding="MathML-Content" id="A1.p2.2.m2.1b"><cn type="float" id="A1.p2.2.m2.1.1.cmml" xref="A1.p2.2.m2.1.1">2.1</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.2.m2.1c">2.1</annotation></semantics></math> AP on rare categories compared to not using a bias, as done in Detic.
Without the use of a bias, class-agnostic proposals are not biased towards being
labelled as background.
With respect to a given class,
a proposal is most likely to be negative,
therefore use of a bias makes intuitive sense to reflect this and
stabilises early training of the detector.
Similar findings were found in RetinaNet <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Lin et al.</span>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2017</span></a>)</cite>.</p>
</div>
<figure id="A1.T5" class="ltx_table">
<div id="A1.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:277.5pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="A1.T5.1.1" class="ltx_p"><span id="A1.T5.1.1.1" class="ltx_text">
<span id="A1.T5.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="A1.T5.1.1.1.1.1.1" class="ltx_tr">
<span id="A1.T5.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Model</span>
<span id="A1.T5.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mAP</span>
<span id="A1.T5.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">AP50</span>
<span id="A1.T5.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">AP75</span>
<span id="A1.T5.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">APr</span>
<span id="A1.T5.1.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">APc</span>
<span id="A1.T5.1.1.1.1.1.1.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">APf</span></span>
</span>
<span class="ltx_tbody">
<span id="A1.T5.1.1.1.1.2.1" class="ltx_tr">
<span id="A1.T5.1.1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Detic</span>
<span id="A1.T5.1.1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">30.2</span>
<span id="A1.T5.1.1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">44.2</span>
<span id="A1.T5.1.1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">32.1</span>
<span id="A1.T5.1.1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">16.4</span>
<span id="A1.T5.1.1.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">31.0</span>
<span id="A1.T5.1.1.1.1.2.1.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">35.4</span></span>
<span id="A1.T5.1.1.1.1.3.2" class="ltx_tr">
<span id="A1.T5.1.1.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Ours (w/o bias)</span>
<span id="A1.T5.1.1.1.1.3.2.2" class="ltx_td ltx_align_center">30.4</span>
<span id="A1.T5.1.1.1.1.3.2.3" class="ltx_td ltx_align_center">44.4</span>
<span id="A1.T5.1.1.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">32.3</span>
<span id="A1.T5.1.1.1.1.3.2.5" class="ltx_td ltx_align_center">18.6</span>
<span id="A1.T5.1.1.1.1.3.2.6" class="ltx_td ltx_align_center">30.8</span>
<span id="A1.T5.1.1.1.1.3.2.7" class="ltx_td ltx_nopad_r ltx_align_center">35.2</span></span>
<span id="A1.T5.1.1.1.1.4.3" class="ltx_tr">
<span id="A1.T5.1.1.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">Ours (w/ bias)</span>
<span id="A1.T5.1.1.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">30.3</span>
<span id="A1.T5.1.1.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">44.2</span>
<span id="A1.T5.1.1.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">32.2</span>
<span id="A1.T5.1.1.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_bb">19.3</span>
<span id="A1.T5.1.1.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_bb">30.5</span>
<span id="A1.T5.1.1.1.1.4.3.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">35.0</span></span>
</span>
</span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>
The effect of using our text-based classifiers sourced from rich descriptions.
In contrast, Detic uses simple classifiers based on class names only (top row).
Results for a detector trained using our text-based classifiers but no learnable bias is
shown in the middle row.
Our proposed model makes use of text-based classifiers sourced from rich descriptions
and a learnable bias (bottom row).
Our method for text-based classifiers
improves performance on novel/rare categories (APr) by a large amount.
We report mask AP metrics.
</figcaption>
</figure>
<div id="A1.p3" class="ltx_para ltx_noindent">
<p id="A1.p3.3" class="ltx_p"><span id="A1.p3.3.1" class="ltx_text ltx_font_bold">Natural Language Descriptions.</span>
Table <a href="#A1.T5" title="Table 5 ‣ Appendix A Ablation Studies ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the effect of using rich class descriptions,
sourced from a LLM, rather than forming text-based classifiers
from simple text prompts of the format
“<span id="A1.p3.3.2" class="ltx_text ltx_font_typewriter">a(n) {class name}</span>” as in Detic.
To compare fairly to Detic with detection data only (top row),
we report a set of results which do not make use of the
learnable bias on the detection scores as detailed above (middle row).
Using our text-based classifiers without a learnable bias improves performance on rare categories by
<math id="A1.p3.1.m1.1" class="ltx_Math" alttext="2.2" display="inline"><semantics id="A1.p3.1.m1.1a"><mn id="A1.p3.1.m1.1.1" xref="A1.p3.1.m1.1.1.cmml">2.2</mn><annotation-xml encoding="MathML-Content" id="A1.p3.1.m1.1b"><cn type="float" id="A1.p3.1.m1.1.1.cmml" xref="A1.p3.1.m1.1.1">2.2</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p3.1.m1.1c">2.2</annotation></semantics></math> APr compared to the public Detic model.
Using rich class descriptions, a learnable bias and our method (bottom row) further improves
open-vocabulary detection on novel/rare categories by <math id="A1.p3.2.m2.1" class="ltx_Math" alttext="2.9" display="inline"><semantics id="A1.p3.2.m2.1a"><mn id="A1.p3.2.m2.1.1" xref="A1.p3.2.m2.1.1.cmml">2.9</mn><annotation-xml encoding="MathML-Content" id="A1.p3.2.m2.1b"><cn type="float" id="A1.p3.2.m2.1.1.cmml" xref="A1.p3.2.m2.1.1">2.9</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p3.2.m2.1c">2.9</annotation></semantics></math> and <math id="A1.p3.3.m3.1" class="ltx_Math" alttext="0.7" display="inline"><semantics id="A1.p3.3.m3.1a"><mn id="A1.p3.3.m3.1.1" xref="A1.p3.3.m3.1.1.cmml">0.7</mn><annotation-xml encoding="MathML-Content" id="A1.p3.3.m3.1b"><cn type="float" id="A1.p3.3.m3.1.1.cmml" xref="A1.p3.3.m3.1.1">0.7</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p3.3.m3.1c">0.7</annotation></semantics></math> APr compared to
the public Detic model and our method without a learnable bias, respectively.</p>
</div>
<figure id="A1.T6" class="ltx_table">
<div id="A1.T6.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:257.2pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="A1.T6.3.1" class="ltx_p"><span id="A1.T6.3.1.1" class="ltx_text">
<span id="A1.T6.3.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="A1.T6.3.1.1.1.1.1" class="ltx_tr">
<span id="A1.T6.3.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">
Visual Encoder</span>
<span id="A1.T6.3.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">TTA?</span>
<span id="A1.T6.3.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mAP</span>
<span id="A1.T6.3.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">APr</span>
<span id="A1.T6.3.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">APc</span>
<span id="A1.T6.3.1.1.1.1.1.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">APf</span></span>
</span>
<span class="ltx_tbody">
<span id="A1.T6.3.1.1.1.2.1" class="ltx_tr">
<span id="A1.T6.3.1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_3"><span id="A1.T6.3.1.1.1.2.1.1.1" class="ltx_text">CLIP ViT-B/32</span></span>
<span id="A1.T6.3.1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">✗</span>
<span id="A1.T6.3.1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">29.0</span>
<span id="A1.T6.3.1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">16.3</span>
<span id="A1.T6.3.1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">29.1</span>
<span id="A1.T6.3.1.1.1.2.1.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">34.4</span></span>
<span id="A1.T6.3.1.1.1.3.2" class="ltx_tr">
<span id="A1.T6.3.1.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">✓, harsh</span>
<span id="A1.T6.3.1.1.1.3.2.2" class="ltx_td ltx_align_center">29.0</span>
<span id="A1.T6.3.1.1.1.3.2.3" class="ltx_td ltx_align_center">17.2</span>
<span id="A1.T6.3.1.1.1.3.2.4" class="ltx_td ltx_align_center">28.7</span>
<span id="A1.T6.3.1.1.1.3.2.5" class="ltx_td ltx_nopad_r ltx_align_center">34.6</span></span>
<span id="A1.T6.3.1.1.1.4.3" class="ltx_tr">
<span id="A1.T6.3.1.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">✓, gentle</span>
<span id="A1.T6.3.1.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">29.2</span>
<span id="A1.T6.3.1.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">18.3</span>
<span id="A1.T6.3.1.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_bb">28.7</span>
<span id="A1.T6.3.1.1.1.4.3.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">34.4</span></span>
</span>
</span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>The effect of using test-time augmentation (TTA) when
generating classifier embeddings from image exemplars using a CLIP image encoder.
Both TTA recipes use two common augmentations — <span id="A1.T6.7.1" class="ltx_text ltx_font_typewriter">ColorJitter</span> and
<span id="A1.T6.8.2" class="ltx_text ltx_font_typewriter">RandomHorizontalFlip</span>.
For harsh/gentle TTA — min scale of <span id="A1.T6.9.3" class="ltx_text ltx_font_typewriter">RandomResizedCrop</span> <math id="A1.T6.2.m1.1" class="ltx_Math" alttext="=0.5/0.8" display="inline"><semantics id="A1.T6.2.m1.1b"><mrow id="A1.T6.2.m1.1.1" xref="A1.T6.2.m1.1.1.cmml"><mi id="A1.T6.2.m1.1.1.2" xref="A1.T6.2.m1.1.1.2.cmml"></mi><mo id="A1.T6.2.m1.1.1.1" xref="A1.T6.2.m1.1.1.1.cmml">=</mo><mrow id="A1.T6.2.m1.1.1.3" xref="A1.T6.2.m1.1.1.3.cmml"><mn id="A1.T6.2.m1.1.1.3.2" xref="A1.T6.2.m1.1.1.3.2.cmml">0.5</mn><mo id="A1.T6.2.m1.1.1.3.1" xref="A1.T6.2.m1.1.1.3.1.cmml">/</mo><mn id="A1.T6.2.m1.1.1.3.3" xref="A1.T6.2.m1.1.1.3.3.cmml">0.8</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.T6.2.m1.1c"><apply id="A1.T6.2.m1.1.1.cmml" xref="A1.T6.2.m1.1.1"><eq id="A1.T6.2.m1.1.1.1.cmml" xref="A1.T6.2.m1.1.1.1"></eq><csymbol cd="latexml" id="A1.T6.2.m1.1.1.2.cmml" xref="A1.T6.2.m1.1.1.2">absent</csymbol><apply id="A1.T6.2.m1.1.1.3.cmml" xref="A1.T6.2.m1.1.1.3"><divide id="A1.T6.2.m1.1.1.3.1.cmml" xref="A1.T6.2.m1.1.1.3.1"></divide><cn type="float" id="A1.T6.2.m1.1.1.3.2.cmml" xref="A1.T6.2.m1.1.1.3.2">0.5</cn><cn type="float" id="A1.T6.2.m1.1.1.3.3.cmml" xref="A1.T6.2.m1.1.1.3.3">0.8</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T6.2.m1.1d">=0.5/0.8</annotation></semantics></math>.
Both harsh and gentle TTA perform better than no TTA in terms of performance
on novel/rare categories (APr).
We report mask AP metrics.
</figcaption>
</figure>
<div id="A1.p4" class="ltx_para ltx_noindent">
<p id="A1.p4.5" class="ltx_p"><span id="A1.p4.5.1" class="ltx_text ltx_font_bold">Test-Time Augmentation on Image Exemplars for Vision-Based Classifiers.</span>
Table <a href="#A1.T6" title="Table 6 ‣ Appendix A Ablation Studies ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the effect of using test-time augmentation (TTA)
on image exemplars to produce vision-based classifiers with our trained
aggregator.
For each image exemplar we generate <math id="A1.p4.1.m1.1" class="ltx_Math" alttext="5" display="inline"><semantics id="A1.p4.1.m1.1a"><mn id="A1.p4.1.m1.1.1" xref="A1.p4.1.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="A1.p4.1.m1.1b"><cn type="integer" id="A1.p4.1.m1.1.1.cmml" xref="A1.p4.1.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p4.1.m1.1c">5</annotation></semantics></math> augmentations.
As in the main paper, we use the case of <math id="A1.p4.2.m2.1" class="ltx_Math" alttext="K=5" display="inline"><semantics id="A1.p4.2.m2.1a"><mrow id="A1.p4.2.m2.1.1" xref="A1.p4.2.m2.1.1.cmml"><mi id="A1.p4.2.m2.1.1.2" xref="A1.p4.2.m2.1.1.2.cmml">K</mi><mo id="A1.p4.2.m2.1.1.1" xref="A1.p4.2.m2.1.1.1.cmml">=</mo><mn id="A1.p4.2.m2.1.1.3" xref="A1.p4.2.m2.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p4.2.m2.1b"><apply id="A1.p4.2.m2.1.1.cmml" xref="A1.p4.2.m2.1.1"><eq id="A1.p4.2.m2.1.1.1.cmml" xref="A1.p4.2.m2.1.1.1"></eq><ci id="A1.p4.2.m2.1.1.2.cmml" xref="A1.p4.2.m2.1.1.2">𝐾</ci><cn type="integer" id="A1.p4.2.m2.1.1.3.cmml" xref="A1.p4.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p4.2.m2.1c">K=5</annotation></semantics></math> — for each class in the LVIS vocabulary
we are have <math id="A1.p4.3.m3.1" class="ltx_Math" alttext="5" display="inline"><semantics id="A1.p4.3.m3.1a"><mn id="A1.p4.3.m3.1.1" xref="A1.p4.3.m3.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="A1.p4.3.m3.1b"><cn type="integer" id="A1.p4.3.m3.1.1.cmml" xref="A1.p4.3.m3.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p4.3.m3.1c">5</annotation></semantics></math> RGB image exemplars.
As mentioned in Section <a href="#S4.SS2" title="4.2 Implementation Details ‣ 4 Experiments ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>,
we augment each exemplar <math id="A1.p4.4.m4.1" class="ltx_Math" alttext="5" display="inline"><semantics id="A1.p4.4.m4.1a"><mn id="A1.p4.4.m4.1.1" xref="A1.p4.4.m4.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="A1.p4.4.m4.1b"><cn type="integer" id="A1.p4.4.m4.1.1.cmml" xref="A1.p4.4.m4.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p4.4.m4.1c">5</annotation></semantics></math> times when using TTA.
We consider two augmentation variations, with each containing
<span id="A1.p4.5.2" class="ltx_text ltx_font_typewriter">ColorJitter</span> and <span id="A1.p4.5.3" class="ltx_text ltx_font_typewriter">RandomHorizontalFlip</span>.
The ‘harsh’ variation uses <span id="A1.p4.5.4" class="ltx_text ltx_font_typewriter">RandomResizedCrop(scale=(0.5,1.0))</span>
and the ‘gentle’ variation uses <span id="A1.p4.5.5" class="ltx_text ltx_font_typewriter">RandomResizedCrop(scale=(0.8,1.0))</span>.
We find adding ‘gentle’ TTA performs best, improving open-vocabulary detection
by <math id="A1.p4.5.m5.1" class="ltx_Math" alttext="2.0" display="inline"><semantics id="A1.p4.5.m5.1a"><mn id="A1.p4.5.m5.1.1" xref="A1.p4.5.m5.1.1.cmml">2.0</mn><annotation-xml encoding="MathML-Content" id="A1.p4.5.m5.1b"><cn type="float" id="A1.p4.5.m5.1.1.cmml" xref="A1.p4.5.m5.1.1">2.0</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p4.5.m5.1c">2.0</annotation></semantics></math> AP on rare categories compared to no use of TTA.
In the main paper, when using vision-based classifiers we utilise ‘gentle’ TTA
on the image exemplars.</p>
</div>
<figure id="A1.T7" class="ltx_table">
<div id="A1.T7.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:314.5pt;height:90pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="A1.T7.1.1" class="ltx_p"><span id="A1.T7.1.1.1" class="ltx_text">
<span id="A1.T7.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="A1.T7.1.1.1.1.1.1" class="ltx_tr">
<span id="A1.T7.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Prompt</span>
<span id="A1.T7.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">mAP</span>
<span id="A1.T7.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">APr</span>
<span id="A1.T7.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">APc</span>
<span id="A1.T7.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">APf</span></span>
</span>
<span class="ltx_tbody">
<span id="A1.T7.1.1.1.1.2.1" class="ltx_tr">
<span id="A1.T7.1.1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">a/an <span id="A1.T7.1.1.1.1.2.1.1.1" class="ltx_text ltx_font_typewriter">class name</span></span>
<span id="A1.T7.1.1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">29.9</span>
<span id="A1.T7.1.1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">17.7</span>
<span id="A1.T7.1.1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">30.1</span>
<span id="A1.T7.1.1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">35.0</span></span>
<span id="A1.T7.1.1.1.1.3.2" class="ltx_tr">
<span id="A1.T7.1.1.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r">a photo of a/an <span id="A1.T7.1.1.1.1.3.2.1.1" class="ltx_text ltx_font_typewriter">class name</span></span>
<span id="A1.T7.1.1.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">29.5</span>
<span id="A1.T7.1.1.1.1.3.2.3" class="ltx_td ltx_align_center">15.9</span>
<span id="A1.T7.1.1.1.1.3.2.4" class="ltx_td ltx_align_center">30.0</span>
<span id="A1.T7.1.1.1.1.3.2.5" class="ltx_td ltx_align_center">34.9</span></span>
<span id="A1.T7.1.1.1.1.4.3" class="ltx_tr">
<span id="A1.T7.1.1.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_r">a photo of a/an <span id="A1.T7.1.1.1.1.4.3.1.1" class="ltx_text ltx_font_typewriter">class name</span> in the scene</span>
<span id="A1.T7.1.1.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">29.4</span>
<span id="A1.T7.1.1.1.1.4.3.3" class="ltx_td ltx_align_center">16.2</span>
<span id="A1.T7.1.1.1.1.4.3.4" class="ltx_td ltx_align_center">29.7</span>
<span id="A1.T7.1.1.1.1.4.3.5" class="ltx_td ltx_align_center">34.8</span></span>
<span id="A1.T7.1.1.1.1.5.4" class="ltx_tr">
<span id="A1.T7.1.1.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">Our LLM descriptions</span>
<span id="A1.T7.1.1.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">30.3</span>
<span id="A1.T7.1.1.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">19.3</span>
<span id="A1.T7.1.1.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">30.5</span>
<span id="A1.T7.1.1.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">35.0</span></span>
</span>
</span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>The effect of using manually crafted prompts against our rich class descriptions sourced
from LLMs. All models use a learnable bias on the detection scores and text-based classifiers.
Using our class descriptions improves performance on “rare” classes compared to manually crafted prompts.
</figcaption>
</figure>
<div id="A1.p5" class="ltx_para ltx_noindent">
<p id="A1.p5.2" class="ltx_p"><span id="A1.p5.2.1" class="ltx_text ltx_font_bold">Comparing our LLM Descriptions to Manually Designed Text Prompts.</span>
Table <a href="#A1.T7" title="Table 7 ‣ Appendix A Ablation Studies ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> compares the detector performance between using simple manually
crafted prompts (first three rows) and our rich class descriptions sourced from an LLM (final row),
for constructing text-based classifiers.
We report results for the case where the detector is trained on <em id="A1.p5.2.2" class="ltx_emph ltx_font_italic">LVIS-base</em> only (<span id="A1.p5.2.3" class="ltx_ERROR undefined">\ie</span> no
additional image-level data is used).
In all cases we apply the learnable bias on the detection score.
We note that of the manual prompts, the simplest one, of the form “<span id="A1.p5.2.4" class="ltx_text ltx_font_typewriter">a(n) {class name}</span>”,
performs best across all metrics.
Using our text-based classifiers generated from LLM descriptions improves performance on
rare classes by <math id="A1.p5.1.m1.1" class="ltx_Math" alttext="1.6" display="inline"><semantics id="A1.p5.1.m1.1a"><mn id="A1.p5.1.m1.1.1" xref="A1.p5.1.m1.1.1.cmml">1.6</mn><annotation-xml encoding="MathML-Content" id="A1.p5.1.m1.1b"><cn type="float" id="A1.p5.1.m1.1.1.cmml" xref="A1.p5.1.m1.1.1">1.6</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p5.1.m1.1c">1.6</annotation></semantics></math> APr and by <math id="A1.p5.2.m2.1" class="ltx_Math" alttext="0.4" display="inline"><semantics id="A1.p5.2.m2.1a"><mn id="A1.p5.2.m2.1.1" xref="A1.p5.2.m2.1.1.cmml">0.4</mn><annotation-xml encoding="MathML-Content" id="A1.p5.2.m2.1b"><cn type="float" id="A1.p5.2.m2.1.1.cmml" xref="A1.p5.2.m2.1.1">0.4</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p5.2.m2.1c">0.4</annotation></semantics></math> mAP across all classes.
Performance on “common” and “frequent” classes is largely similar as the availability of
labelled detection data for these classes renders the quality of the classifier less important.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A2" class="ltx_appendix ltx_centering">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>A Closer Look at “rare” Class Performance</h2>

<figure id="A2.T8" class="ltx_table">
<div id="A2.T8.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:377.1pt;height:126pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="A2.T8.1.1" class="ltx_p"><span id="A2.T8.1.1.1" class="ltx_text">
<span id="A2.T8.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="A2.T8.1.1.1.1.1" class="ltx_tr">
<span id="A2.T8.1.1.1.1.1.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">
Model</span>
<span id="A2.T8.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"><math id="A2.T8.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{D}^{\textsc{img}}" display="inline"><semantics id="A2.T8.1.1.1.1.1.1.m1.1a"><msup id="A2.T8.1.1.1.1.1.1.m1.1.1" xref="A2.T8.1.1.1.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.T8.1.1.1.1.1.1.m1.1.1.2" xref="A2.T8.1.1.1.1.1.1.m1.1.1.2.cmml">𝒟</mi><mtext class="ltx_font_smallcaps" id="A2.T8.1.1.1.1.1.1.m1.1.1.3" xref="A2.T8.1.1.1.1.1.1.m1.1.1.3a.cmml">img</mtext></msup><annotation-xml encoding="MathML-Content" id="A2.T8.1.1.1.1.1.1.m1.1b"><apply id="A2.T8.1.1.1.1.1.1.m1.1.1.cmml" xref="A2.T8.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="A2.T8.1.1.1.1.1.1.m1.1.1.1.cmml" xref="A2.T8.1.1.1.1.1.1.m1.1.1">superscript</csymbol><ci id="A2.T8.1.1.1.1.1.1.m1.1.1.2.cmml" xref="A2.T8.1.1.1.1.1.1.m1.1.1.2">𝒟</ci><ci id="A2.T8.1.1.1.1.1.1.m1.1.1.3a.cmml" xref="A2.T8.1.1.1.1.1.1.m1.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="A2.T8.1.1.1.1.1.1.m1.1.1.3.cmml" xref="A2.T8.1.1.1.1.1.1.m1.1.1.3">img</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T8.1.1.1.1.1.1.m1.1c">\mathcal{D}^{\textsc{img}}</annotation></semantics></math></span>
<span id="A2.T8.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mAP</span>
<span id="A2.T8.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">APr</span>
<span id="A2.T8.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">APr-w</span>
<span id="A2.T8.1.1.1.1.1.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">APr-z</span></span>
</span>
<span class="ltx_tbody">
<span id="A2.T8.1.1.1.1.2.1" class="ltx_tr">
<span id="A2.T8.1.1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Detic <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhou et al.</span>, <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite></span>
<span id="A2.T8.1.1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_3"><span id="A2.T8.1.1.1.1.2.1.2.1" class="ltx_text">✗</span></span>
<span id="A2.T8.1.1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">30.2</span>
<span id="A2.T8.1.1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">16.3</span>
<span id="A2.T8.1.1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">15.7</span>
<span id="A2.T8.1.1.1.1.2.1.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">19.7</span></span>
<span id="A2.T8.1.1.1.1.3.2" class="ltx_tr">
<span id="A2.T8.1.1.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Ours (Text-Based)</span>
<span id="A2.T8.1.1.1.1.3.2.2" class="ltx_td ltx_align_center">30.3</span>
<span id="A2.T8.1.1.1.1.3.2.3" class="ltx_td ltx_align_center"><span id="A2.T8.1.1.1.1.3.2.3.1" class="ltx_text ltx_font_bold">19.3</span></span>
<span id="A2.T8.1.1.1.1.3.2.4" class="ltx_td ltx_align_center"><span id="A2.T8.1.1.1.1.3.2.4.1" class="ltx_text ltx_font_bold">19.2</span></span>
<span id="A2.T8.1.1.1.1.3.2.5" class="ltx_td ltx_nopad_r ltx_align_center">19.4</span></span>
<span id="A2.T8.1.1.1.1.4.3" class="ltx_tr">
<span id="A2.T8.1.1.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Ours (Multi-Modal)</span>
<span id="A2.T8.1.1.1.1.4.3.2" class="ltx_td ltx_align_center"><span id="A2.T8.1.1.1.1.4.3.2.1" class="ltx_text ltx_font_bold">30.6</span></span>
<span id="A2.T8.1.1.1.1.4.3.3" class="ltx_td ltx_align_center">19.2</span>
<span id="A2.T8.1.1.1.1.4.3.4" class="ltx_td ltx_align_center">18.5</span>
<span id="A2.T8.1.1.1.1.4.3.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A2.T8.1.1.1.1.4.3.5.1" class="ltx_text ltx_font_bold">22.2</span></span></span>
<span id="A2.T8.1.1.1.1.5.4" class="ltx_tr">
<span id="A2.T8.1.1.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Detic <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhou et al.</span>, <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite></span>
<span id="A2.T8.1.1.1.1.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_3"><span id="A2.T8.1.1.1.1.5.4.2.1" class="ltx_text">✓</span></span>
<span id="A2.T8.1.1.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t">32.4</span>
<span id="A2.T8.1.1.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_t">24.9</span>
<span id="A2.T8.1.1.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_t">25.4</span>
<span id="A2.T8.1.1.1.1.5.4.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">23.0</span></span>
<span id="A2.T8.1.1.1.1.6.5" class="ltx_tr">
<span id="A2.T8.1.1.1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Ours (Text-Based)</span>
<span id="A2.T8.1.1.1.1.6.5.2" class="ltx_td ltx_align_center">32.6</span>
<span id="A2.T8.1.1.1.1.6.5.3" class="ltx_td ltx_align_center">25.8</span>
<span id="A2.T8.1.1.1.1.6.5.4" class="ltx_td ltx_align_center">26.7</span>
<span id="A2.T8.1.1.1.1.6.5.5" class="ltx_td ltx_nopad_r ltx_align_center">21.7</span></span>
<span id="A2.T8.1.1.1.1.7.6" class="ltx_tr">
<span id="A2.T8.1.1.1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">Ours (Multi-Modal)</span>
<span id="A2.T8.1.1.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="A2.T8.1.1.1.1.7.6.2.1" class="ltx_text ltx_font_bold">33.1</span></span>
<span id="A2.T8.1.1.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="A2.T8.1.1.1.1.7.6.3.1" class="ltx_text ltx_font_bold">27.3</span></span>
<span id="A2.T8.1.1.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="A2.T8.1.1.1.1.7.6.4.1" class="ltx_text ltx_font_bold">27.8</span></span>
<span id="A2.T8.1.1.1.1.7.6.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="A2.T8.1.1.1.1.7.6.5.1" class="ltx_text ltx_font_bold">24.9</span></span></span>
</span>
</span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Comparison between Detic and our models using text-based classifiers
and multi-modal classifiers on LVIS.
As IN-L does not cover all classes in the LVIS vocabulary,
we split the rare class metric APr into APr-w and APr-z
which represent rare classes with and without weak annotations from IN-L,
respectively.
Best performing models are shown in <span id="A2.T8.3.1" class="ltx_text ltx_font_bold">bold</span>.
We report mask AP metrics.
</figcaption>
</figure>
<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">Section <a href="#S4.SS1" title="4.1 Datasets and Evaluation Protocol ‣ 4 Experiments ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a> detailed the data used to train our detector.
IN-L contains images from the <math id="A2.p1.1.m1.1" class="ltx_Math" alttext="997" display="inline"><semantics id="A2.p1.1.m1.1a"><mn id="A2.p1.1.m1.1.1" xref="A2.p1.1.m1.1.1.cmml">997</mn><annotation-xml encoding="MathML-Content" id="A2.p1.1.m1.1b"><cn type="integer" id="A2.p1.1.m1.1.1.cmml" xref="A2.p1.1.m1.1.1">997</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.1.m1.1c">997</annotation></semantics></math> classes
from LVIS present in ImageNet-21k <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Deng et al.</span>, <a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2009</span></a>)</cite>.</p>
</div>
<div id="A2.p2" class="ltx_para">
<p id="A2.p2.3" class="ltx_p">IN-L gives weak supervision during detector training.
Out of the <math id="A2.p2.1.m1.1" class="ltx_Math" alttext="337" display="inline"><semantics id="A2.p2.1.m1.1a"><mn id="A2.p2.1.m1.1.1" xref="A2.p2.1.m1.1.1.cmml">337</mn><annotation-xml encoding="MathML-Content" id="A2.p2.1.m1.1b"><cn type="integer" id="A2.p2.1.m1.1.1.cmml" xref="A2.p2.1.m1.1.1">337</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.1.m1.1c">337</annotation></semantics></math> “rare” classes in LVIS,
<math id="A2.p2.2.m2.1" class="ltx_Math" alttext="277" display="inline"><semantics id="A2.p2.2.m2.1a"><mn id="A2.p2.2.m2.1.1" xref="A2.p2.2.m2.1.1.cmml">277</mn><annotation-xml encoding="MathML-Content" id="A2.p2.2.m2.1b"><cn type="integer" id="A2.p2.2.m2.1.1.cmml" xref="A2.p2.2.m2.1.1">277</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.2.m2.1c">277</annotation></semantics></math> are covered by IN-L and are therefore weakly supervised,
leaving <math id="A2.p2.3.m3.1" class="ltx_Math" alttext="60" display="inline"><semantics id="A2.p2.3.m3.1a"><mn id="A2.p2.3.m3.1.1" xref="A2.p2.3.m3.1.1.cmml">60</mn><annotation-xml encoding="MathML-Content" id="A2.p2.3.m3.1b"><cn type="integer" id="A2.p2.3.m3.1.1.cmml" xref="A2.p2.3.m3.1.1">60</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.3.m3.1c">60</annotation></semantics></math> classes for which no weak supervision is available.</p>
</div>
<div id="A2.p3" class="ltx_para">
<p id="A2.p3.1" class="ltx_p">To investigate the improvement in performance when using IN-L,
we split the rare class metric (APr), which reports average precision across rare classes,
into APr-w which averages across rare classes <em id="A2.p3.1.1" class="ltx_emph ltx_font_italic">present</em> in IN-L
and APr-z which averages across rare classes <em id="A2.p3.1.2" class="ltx_emph ltx_font_italic">not present</em> in IN-L which are therefore
truly zero-shot classes.
Note that when no extra image-level data is used,
<span id="A2.p3.1.3" class="ltx_ERROR undefined">\ie</span> <math id="A2.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{D}^{\textsc{img}}=\varnothing" display="inline"><semantics id="A2.p3.1.m1.1a"><mrow id="A2.p3.1.m1.1.1" xref="A2.p3.1.m1.1.1.cmml"><msup id="A2.p3.1.m1.1.1.2" xref="A2.p3.1.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.p3.1.m1.1.1.2.2" xref="A2.p3.1.m1.1.1.2.2.cmml">𝒟</mi><mtext class="ltx_font_smallcaps" id="A2.p3.1.m1.1.1.2.3" xref="A2.p3.1.m1.1.1.2.3a.cmml">img</mtext></msup><mo id="A2.p3.1.m1.1.1.1" xref="A2.p3.1.m1.1.1.1.cmml">=</mo><mi mathvariant="normal" id="A2.p3.1.m1.1.1.3" xref="A2.p3.1.m1.1.1.3.cmml">∅</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.p3.1.m1.1b"><apply id="A2.p3.1.m1.1.1.cmml" xref="A2.p3.1.m1.1.1"><eq id="A2.p3.1.m1.1.1.1.cmml" xref="A2.p3.1.m1.1.1.1"></eq><apply id="A2.p3.1.m1.1.1.2.cmml" xref="A2.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="A2.p3.1.m1.1.1.2.1.cmml" xref="A2.p3.1.m1.1.1.2">superscript</csymbol><ci id="A2.p3.1.m1.1.1.2.2.cmml" xref="A2.p3.1.m1.1.1.2.2">𝒟</ci><ci id="A2.p3.1.m1.1.1.2.3a.cmml" xref="A2.p3.1.m1.1.1.2.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="A2.p3.1.m1.1.1.2.3.cmml" xref="A2.p3.1.m1.1.1.2.3">img</mtext></ci></apply><emptyset id="A2.p3.1.m1.1.1.3.cmml" xref="A2.p3.1.m1.1.1.3"></emptyset></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p3.1.m1.1c">\mathcal{D}^{\textsc{img}}=\varnothing</annotation></semantics></math>, all rare classes are truly zero-shot classes.</p>
</div>
<div id="A2.p4" class="ltx_para">
<p id="A2.p4.1" class="ltx_p">Table <a href="#A2.T8" title="Table 8 ‣ Appendix B A Closer Look at “rare” Class Performance ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows the result of using this breakdown.
These results show training on IN-L improves performance on rare classes
not contained in IN-L, which may not be expected.
The weak supervision from IN-L leads to a reduction in false positives for all
rare classes leading to improved performance across all metrics.
Moreover, our multi-modal classifiers perform best across all metrics.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A3" class="ltx_appendix ltx_centering">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Vision-based Classifier Pipeline Implementation Details</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.3" class="ltx_p">For the transformer architecture of the visual aggregator detailed in
Section <a href="#S3.SS3" title="3.3 Vision-based Classifiers from Image Exemplars ‣ 3 Method ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>, we use <math id="A3.p1.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="A3.p1.1.m1.1a"><mn id="A3.p1.1.m1.1.1" xref="A3.p1.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="A3.p1.1.m1.1b"><cn type="integer" id="A3.p1.1.m1.1.1.cmml" xref="A3.p1.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.1.m1.1c">4</annotation></semantics></math> transformer encoder blocks,
each with a hidden dimension of <math id="A3.p1.2.m2.1" class="ltx_Math" alttext="512" display="inline"><semantics id="A3.p1.2.m2.1a"><mn id="A3.p1.2.m2.1.1" xref="A3.p1.2.m2.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="A3.p1.2.m2.1b"><cn type="integer" id="A3.p1.2.m2.1.1.cmml" xref="A3.p1.2.m2.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.2.m2.1c">512</annotation></semantics></math> and MLP dimension <math id="A3.p1.3.m3.1" class="ltx_Math" alttext="2048" display="inline"><semantics id="A3.p1.3.m3.1a"><mn id="A3.p1.3.m3.1.1" xref="A3.p1.3.m3.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="A3.p1.3.m3.1b"><cn type="integer" id="A3.p1.3.m3.1.1.cmml" xref="A3.p1.3.m3.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.3.m3.1c">2048</annotation></semantics></math>.
As input to the first transformer block, we encode each image exemplar with
a CLIP image encoder which remains frozen throughout training,
yielding one embedding per exemplar.
The set of embeddings are input with a learnable [CLS] token. The output
[CLS] token is used as the final vision-based classifier.</p>
</div>
<div id="A3.p2" class="ltx_para">
<p id="A3.p2.15" class="ltx_p">To train the model we use the ImageNet-21k-P dataset <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Ridnik et al.</span>, <a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite> for 10 epochs.
To speed up and improve training we store a dynamic queue of
size <math id="A3.p2.1.m1.1" class="ltx_Math" alttext="4096\times{K}" display="inline"><semantics id="A3.p2.1.m1.1a"><mrow id="A3.p2.1.m1.1.1" xref="A3.p2.1.m1.1.1.cmml"><mn id="A3.p2.1.m1.1.1.2" xref="A3.p2.1.m1.1.1.2.cmml">4096</mn><mo lspace="0.222em" rspace="0.222em" id="A3.p2.1.m1.1.1.1" xref="A3.p2.1.m1.1.1.1.cmml">×</mo><mi id="A3.p2.1.m1.1.1.3" xref="A3.p2.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="A3.p2.1.m1.1b"><apply id="A3.p2.1.m1.1.1.cmml" xref="A3.p2.1.m1.1.1"><times id="A3.p2.1.m1.1.1.1.cmml" xref="A3.p2.1.m1.1.1.1"></times><cn type="integer" id="A3.p2.1.m1.1.1.2.cmml" xref="A3.p2.1.m1.1.1.2">4096</cn><ci id="A3.p2.1.m1.1.1.3.cmml" xref="A3.p2.1.m1.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.1.m1.1c">4096\times{K}</annotation></semantics></math> CLIP encoded embeddings, with <math id="A3.p2.2.m2.1" class="ltx_Math" alttext="512" display="inline"><semantics id="A3.p2.2.m2.1a"><mn id="A3.p2.2.m2.1.1" xref="A3.p2.2.m2.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="A3.p2.2.m2.1b"><cn type="integer" id="A3.p2.2.m2.1.1.cmml" xref="A3.p2.2.m2.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.2.m2.1c">512</annotation></semantics></math> positions in the
queue updated each iteration, using a last-in first-out policy.
Each set of <math id="A3.p2.3.m3.1" class="ltx_Math" alttext="K" display="inline"><semantics id="A3.p2.3.m3.1a"><mi id="A3.p2.3.m3.1.1" xref="A3.p2.3.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="A3.p2.3.m3.1b"><ci id="A3.p2.3.m3.1.1.cmml" xref="A3.p2.3.m3.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.3.m3.1c">K</annotation></semantics></math> represents encodings from <math id="A3.p2.4.m4.1" class="ltx_Math" alttext="K" display="inline"><semantics id="A3.p2.4.m4.1a"><mi id="A3.p2.4.m4.1.1" xref="A3.p2.4.m4.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="A3.p2.4.m4.1b"><ci id="A3.p2.4.m4.1.1.cmml" xref="A3.p2.4.m4.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.4.m4.1c">K</annotation></semantics></math> randomly sampled images
for a single class. We use <math id="A3.p2.5.m5.1" class="ltx_Math" alttext="K=5" display="inline"><semantics id="A3.p2.5.m5.1a"><mrow id="A3.p2.5.m5.1.1" xref="A3.p2.5.m5.1.1.cmml"><mi id="A3.p2.5.m5.1.1.2" xref="A3.p2.5.m5.1.1.2.cmml">K</mi><mo id="A3.p2.5.m5.1.1.1" xref="A3.p2.5.m5.1.1.1.cmml">=</mo><mn id="A3.p2.5.m5.1.1.3" xref="A3.p2.5.m5.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.p2.5.m5.1b"><apply id="A3.p2.5.m5.1.1.cmml" xref="A3.p2.5.m5.1.1"><eq id="A3.p2.5.m5.1.1.1.cmml" xref="A3.p2.5.m5.1.1.1"></eq><ci id="A3.p2.5.m5.1.1.2.cmml" xref="A3.p2.5.m5.1.1.2">𝐾</ci><cn type="integer" id="A3.p2.5.m5.1.1.3.cmml" xref="A3.p2.5.m5.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.5.m5.1c">K=5</annotation></semantics></math>.
For contrastive training, we use a temperature of <math id="A3.p2.6.m6.1" class="ltx_Math" alttext="0.02" display="inline"><semantics id="A3.p2.6.m6.1a"><mn id="A3.p2.6.m6.1.1" xref="A3.p2.6.m6.1.1.cmml">0.02</mn><annotation-xml encoding="MathML-Content" id="A3.p2.6.m6.1b"><cn type="float" id="A3.p2.6.m6.1.1.cmml" xref="A3.p2.6.m6.1.1">0.02</cn></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.6.m6.1c">0.02</annotation></semantics></math>
in the InfoNCE <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">van den Oord et al.</span>, <a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite> loss function,
the AdamW <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Loshchilov &amp; Hutter</span>, <a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite> optimiser with standard hyperparameters and a
learning rate of <math id="A3.p2.7.m7.1" class="ltx_Math" alttext="0.0002" display="inline"><semantics id="A3.p2.7.m7.1a"><mn id="A3.p2.7.m7.1.1" xref="A3.p2.7.m7.1.1.cmml">0.0002</mn><annotation-xml encoding="MathML-Content" id="A3.p2.7.m7.1b"><cn type="float" id="A3.p2.7.m7.1.1.cmml" xref="A3.p2.7.m7.1.1">0.0002</cn></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.7.m7.1c">0.0002</annotation></semantics></math>.
Furthermore, during training we uniformly sample <math id="A3.p2.8.m8.1" class="ltx_math_unparsed" alttext="k\in[1:K]" display="inline"><semantics id="A3.p2.8.m8.1a"><mrow id="A3.p2.8.m8.1b"><mi id="A3.p2.8.m8.1.1">k</mi><mo id="A3.p2.8.m8.1.2">∈</mo><mrow id="A3.p2.8.m8.1.3"><mo stretchy="false" id="A3.p2.8.m8.1.3.1">[</mo><mn id="A3.p2.8.m8.1.3.2">1</mn><mo lspace="0.278em" rspace="0.278em" id="A3.p2.8.m8.1.3.3">:</mo><mi id="A3.p2.8.m8.1.3.4">K</mi><mo stretchy="false" id="A3.p2.8.m8.1.3.5">]</mo></mrow></mrow><annotation encoding="application/x-tex" id="A3.p2.8.m8.1c">k\in[1:K]</annotation></semantics></math> to simulate
varying numbers of image exemplars being available for
downstream OVOD when <math id="A3.p2.9.m9.1" class="ltx_Math" alttext="\mathcal{C}^{\textsc{test}}" display="inline"><semantics id="A3.p2.9.m9.1a"><msup id="A3.p2.9.m9.1.1" xref="A3.p2.9.m9.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A3.p2.9.m9.1.1.2" xref="A3.p2.9.m9.1.1.2.cmml">𝒞</mi><mtext class="ltx_font_smallcaps" id="A3.p2.9.m9.1.1.3" xref="A3.p2.9.m9.1.1.3a.cmml">test</mtext></msup><annotation-xml encoding="MathML-Content" id="A3.p2.9.m9.1b"><apply id="A3.p2.9.m9.1.1.cmml" xref="A3.p2.9.m9.1.1"><csymbol cd="ambiguous" id="A3.p2.9.m9.1.1.1.cmml" xref="A3.p2.9.m9.1.1">superscript</csymbol><ci id="A3.p2.9.m9.1.1.2.cmml" xref="A3.p2.9.m9.1.1.2">𝒞</ci><ci id="A3.p2.9.m9.1.1.3a.cmml" xref="A3.p2.9.m9.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="A3.p2.9.m9.1.1.3.cmml" xref="A3.p2.9.m9.1.1.3">test</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.9.m9.1c">\mathcal{C}^{\textsc{test}}</annotation></semantics></math> is defined.
Therefore, for a given iteration, there may <math id="A3.p2.10.m10.1" class="ltx_Math" alttext="1-5" display="inline"><semantics id="A3.p2.10.m10.1a"><mrow id="A3.p2.10.m10.1.1" xref="A3.p2.10.m10.1.1.cmml"><mn id="A3.p2.10.m10.1.1.2" xref="A3.p2.10.m10.1.1.2.cmml">1</mn><mo id="A3.p2.10.m10.1.1.1" xref="A3.p2.10.m10.1.1.1.cmml">−</mo><mn id="A3.p2.10.m10.1.1.3" xref="A3.p2.10.m10.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.p2.10.m10.1b"><apply id="A3.p2.10.m10.1.1.cmml" xref="A3.p2.10.m10.1.1"><minus id="A3.p2.10.m10.1.1.1.cmml" xref="A3.p2.10.m10.1.1.1"></minus><cn type="integer" id="A3.p2.10.m10.1.1.2.cmml" xref="A3.p2.10.m10.1.1.2">1</cn><cn type="integer" id="A3.p2.10.m10.1.1.3.cmml" xref="A3.p2.10.m10.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.10.m10.1c">1-5</annotation></semantics></math>, visual embeddings
input per class.
Prior to input to the CLIP image ViT encoder,
we apply random augmentations to each sampled image from ImageNet-21k-P.
We use an augmentation policy similar to SimCLR <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Chen et al.</span>, <a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>,
which includes <span id="A3.p2.15.1" class="ltx_text ltx_font_typewriter">RandomResizedCrop</span>, <span id="A3.p2.15.2" class="ltx_text ltx_font_typewriter">ColorJitter</span>
and <span id="A3.p2.15.3" class="ltx_text ltx_font_typewriter">RandomGrayscale</span>.
We discover that test-time augmentation of the image exemplars available
for the vocabulary in <math id="A3.p2.11.m11.1" class="ltx_Math" alttext="\mathcal{C}^{\textsc{test}}" display="inline"><semantics id="A3.p2.11.m11.1a"><msup id="A3.p2.11.m11.1.1" xref="A3.p2.11.m11.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A3.p2.11.m11.1.1.2" xref="A3.p2.11.m11.1.1.2.cmml">𝒞</mi><mtext class="ltx_font_smallcaps" id="A3.p2.11.m11.1.1.3" xref="A3.p2.11.m11.1.1.3a.cmml">test</mtext></msup><annotation-xml encoding="MathML-Content" id="A3.p2.11.m11.1b"><apply id="A3.p2.11.m11.1.1.cmml" xref="A3.p2.11.m11.1.1"><csymbol cd="ambiguous" id="A3.p2.11.m11.1.1.1.cmml" xref="A3.p2.11.m11.1.1">superscript</csymbol><ci id="A3.p2.11.m11.1.1.2.cmml" xref="A3.p2.11.m11.1.1.2">𝒞</ci><ci id="A3.p2.11.m11.1.1.3a.cmml" xref="A3.p2.11.m11.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="A3.p2.11.m11.1.1.3.cmml" xref="A3.p2.11.m11.1.1.3">test</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.11.m11.1c">\mathcal{C}^{\textsc{test}}</annotation></semantics></math> improves downstream OVOD performance.
For each image exemplar, we generate <math id="A3.p2.12.m12.1" class="ltx_Math" alttext="5" display="inline"><semantics id="A3.p2.12.m12.1a"><mn id="A3.p2.12.m12.1.1" xref="A3.p2.12.m12.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="A3.p2.12.m12.1b"><cn type="integer" id="A3.p2.12.m12.1.1.cmml" xref="A3.p2.12.m12.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.12.m12.1c">5</annotation></semantics></math> test-time augmentations.
Therefore if we have <math id="A3.p2.13.m13.1" class="ltx_Math" alttext="L" display="inline"><semantics id="A3.p2.13.m13.1a"><mi id="A3.p2.13.m13.1.1" xref="A3.p2.13.m13.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="A3.p2.13.m13.1b"><ci id="A3.p2.13.m13.1.1.cmml" xref="A3.p2.13.m13.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.13.m13.1c">L</annotation></semantics></math> image exemplars for a given class in
<math id="A3.p2.14.m14.1" class="ltx_Math" alttext="\mathcal{C}^{\textsc{test}}" display="inline"><semantics id="A3.p2.14.m14.1a"><msup id="A3.p2.14.m14.1.1" xref="A3.p2.14.m14.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A3.p2.14.m14.1.1.2" xref="A3.p2.14.m14.1.1.2.cmml">𝒞</mi><mtext class="ltx_font_smallcaps" id="A3.p2.14.m14.1.1.3" xref="A3.p2.14.m14.1.1.3a.cmml">test</mtext></msup><annotation-xml encoding="MathML-Content" id="A3.p2.14.m14.1b"><apply id="A3.p2.14.m14.1.1.cmml" xref="A3.p2.14.m14.1.1"><csymbol cd="ambiguous" id="A3.p2.14.m14.1.1.1.cmml" xref="A3.p2.14.m14.1.1">superscript</csymbol><ci id="A3.p2.14.m14.1.1.2.cmml" xref="A3.p2.14.m14.1.1.2">𝒞</ci><ci id="A3.p2.14.m14.1.1.3a.cmml" xref="A3.p2.14.m14.1.1.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="A3.p2.14.m14.1.1.3.cmml" xref="A3.p2.14.m14.1.1.3">test</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.14.m14.1c">\mathcal{C}^{\textsc{test}}</annotation></semantics></math>, <math id="A3.p2.15.m15.1" class="ltx_Math" alttext="5L" display="inline"><semantics id="A3.p2.15.m15.1a"><mrow id="A3.p2.15.m15.1.1" xref="A3.p2.15.m15.1.1.cmml"><mn id="A3.p2.15.m15.1.1.2" xref="A3.p2.15.m15.1.1.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="A3.p2.15.m15.1.1.1" xref="A3.p2.15.m15.1.1.1.cmml">​</mo><mi id="A3.p2.15.m15.1.1.3" xref="A3.p2.15.m15.1.1.3.cmml">L</mi></mrow><annotation-xml encoding="MathML-Content" id="A3.p2.15.m15.1b"><apply id="A3.p2.15.m15.1.1.cmml" xref="A3.p2.15.m15.1.1"><times id="A3.p2.15.m15.1.1.1.cmml" xref="A3.p2.15.m15.1.1.1"></times><cn type="integer" id="A3.p2.15.m15.1.1.2.cmml" xref="A3.p2.15.m15.1.1.2">5</cn><ci id="A3.p2.15.m15.1.1.3.cmml" xref="A3.p2.15.m15.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.15.m15.1c">5L</annotation></semantics></math> augmented images are encoded using the CLIP image
encoder and fused using the learnt transformer architecture — the visual
aggregator — as described in Section <a href="#S3.SS3" title="3.3 Vision-based Classifiers from Image Exemplars ‣ 3 Method ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>.
Use of test-time augmentation is ablated in Section <a href="#A1" title="Appendix A Ablation Studies ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A4" class="ltx_appendix ltx_centering">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Sourcing Image Exemplars</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">In this section, we detail how image exemplars are sourced when performing
experiments using vision-based classifiers and multi-modal classifiers,
as described in Section <a href="#S3.SS3" title="3.3 Vision-based Classifiers from Image Exemplars ‣ 3 Method ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> and <a href="#S3.SS4" title="3.4 Constructing Classifiers via Multi-Modal Fusion ‣ 3 Method ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>,
respectively. We start with an empty image exemplar dictionary (IED) for the <math id="A4.p1.1.m1.1" class="ltx_Math" alttext="1203" display="inline"><semantics id="A4.p1.1.m1.1a"><mn id="A4.p1.1.m1.1.1" xref="A4.p1.1.m1.1.1.cmml">1203</mn><annotation-xml encoding="MathML-Content" id="A4.p1.1.m1.1b"><cn type="integer" id="A4.p1.1.m1.1.1.cmml" xref="A4.p1.1.m1.1.1">1203</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.1.m1.1c">1203</annotation></semantics></math>
LVIS classes.</p>
</div>
<div id="A4.p2" class="ltx_para">
<p id="A4.p2.6" class="ltx_p">From constructing the image-level dataset IN-L,
we know that ImageNet-21k <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Deng et al.</span>, <a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2009</span></a>)</cite> contains <math id="A4.p2.1.m1.1" class="ltx_Math" alttext="997" display="inline"><semantics id="A4.p2.1.m1.1a"><mn id="A4.p2.1.m1.1.1" xref="A4.p2.1.m1.1.1.cmml">997</mn><annotation-xml encoding="MathML-Content" id="A4.p2.1.m1.1b"><cn type="integer" id="A4.p2.1.m1.1.1.cmml" xref="A4.p2.1.m1.1.1">997</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p2.1.m1.1c">997</annotation></semantics></math> out of the
<math id="A4.p2.2.m2.1" class="ltx_Math" alttext="1203" display="inline"><semantics id="A4.p2.2.m2.1a"><mn id="A4.p2.2.m2.1.1" xref="A4.p2.2.m2.1.1.cmml">1203</mn><annotation-xml encoding="MathML-Content" id="A4.p2.2.m2.1b"><cn type="integer" id="A4.p2.2.m2.1.1.cmml" xref="A4.p2.2.m2.1.1">1203</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p2.2.m2.1c">1203</annotation></semantics></math> classes in the LVIS vocabulary,
using exact WordNet <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Miller</span>, <a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1995</span></a>)</cite> synset matching.
We add IN-L to our IED.
The result is <math id="A4.p2.3.m3.1" class="ltx_Math" alttext="988" display="inline"><semantics id="A4.p2.3.m3.1a"><mn id="A4.p2.3.m3.1.1" xref="A4.p2.3.m3.1.1.cmml">988</mn><annotation-xml encoding="MathML-Content" id="A4.p2.3.m3.1b"><cn type="integer" id="A4.p2.3.m3.1.1.cmml" xref="A4.p2.3.m3.1.1">988</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p2.3.m3.1c">988</annotation></semantics></math> classes have more than <math id="A4.p2.4.m4.1" class="ltx_Math" alttext="40" display="inline"><semantics id="A4.p2.4.m4.1a"><mn id="A4.p2.4.m4.1.1" xref="A4.p2.4.m4.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="A4.p2.4.m4.1b"><cn type="integer" id="A4.p2.4.m4.1.1.cmml" xref="A4.p2.4.m4.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p2.4.m4.1c">40</annotation></semantics></math> images in ImageNet-21k.
This leaves <math id="A4.p2.5.m5.1" class="ltx_Math" alttext="215" display="inline"><semantics id="A4.p2.5.m5.1a"><mn id="A4.p2.5.m5.1.1" xref="A4.p2.5.m5.1.1.cmml">215</mn><annotation-xml encoding="MathML-Content" id="A4.p2.5.m5.1b"><cn type="integer" id="A4.p2.5.m5.1.1.cmml" xref="A4.p2.5.m5.1.1">215</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p2.5.m5.1c">215</annotation></semantics></math> classes for which there are too few or no image
exemplars (<math id="A4.p2.6.m6.1" class="ltx_Math" alttext="&lt;40" display="inline"><semantics id="A4.p2.6.m6.1a"><mrow id="A4.p2.6.m6.1.1" xref="A4.p2.6.m6.1.1.cmml"><mi id="A4.p2.6.m6.1.1.2" xref="A4.p2.6.m6.1.1.2.cmml"></mi><mo id="A4.p2.6.m6.1.1.1" xref="A4.p2.6.m6.1.1.1.cmml">&lt;</mo><mn id="A4.p2.6.m6.1.1.3" xref="A4.p2.6.m6.1.1.3.cmml">40</mn></mrow><annotation-xml encoding="MathML-Content" id="A4.p2.6.m6.1b"><apply id="A4.p2.6.m6.1.1.cmml" xref="A4.p2.6.m6.1.1"><lt id="A4.p2.6.m6.1.1.1.cmml" xref="A4.p2.6.m6.1.1.1"></lt><csymbol cd="latexml" id="A4.p2.6.m6.1.1.2.cmml" xref="A4.p2.6.m6.1.1.2">absent</csymbol><cn type="integer" id="A4.p2.6.m6.1.1.3.cmml" xref="A4.p2.6.m6.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p2.6.m6.1c">&lt;40</annotation></semantics></math>).</p>
</div>
<div id="A4.p3" class="ltx_para">
<p id="A4.p3.7" class="ltx_p">Next, to try and fill this gap, we turn to LVIS itself.
We add the LVIS training annotations with area greater
than <math id="A4.p3.1.m1.1" class="ltx_Math" alttext="32^{2}" display="inline"><semantics id="A4.p3.1.m1.1a"><msup id="A4.p3.1.m1.1.1" xref="A4.p3.1.m1.1.1.cmml"><mn id="A4.p3.1.m1.1.1.2" xref="A4.p3.1.m1.1.1.2.cmml">32</mn><mn id="A4.p3.1.m1.1.1.3" xref="A4.p3.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="A4.p3.1.m1.1b"><apply id="A4.p3.1.m1.1.1.cmml" xref="A4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="A4.p3.1.m1.1.1.1.cmml" xref="A4.p3.1.m1.1.1">superscript</csymbol><cn type="integer" id="A4.p3.1.m1.1.1.2.cmml" xref="A4.p3.1.m1.1.1.2">32</cn><cn type="integer" id="A4.p3.1.m1.1.1.3.cmml" xref="A4.p3.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p3.1.m1.1c">32^{2}</annotation></semantics></math> to our IED.
There are now <math id="A4.p3.2.m2.1" class="ltx_Math" alttext="1095" display="inline"><semantics id="A4.p3.2.m2.1a"><mn id="A4.p3.2.m2.1.1" xref="A4.p3.2.m2.1.1.cmml">1095</mn><annotation-xml encoding="MathML-Content" id="A4.p3.2.m2.1b"><cn type="integer" id="A4.p3.2.m2.1.1.cmml" xref="A4.p3.2.m2.1.1">1095</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p3.2.m2.1c">1095</annotation></semantics></math> LVIS classes with more than <math id="A4.p3.3.m3.1" class="ltx_Math" alttext="40" display="inline"><semantics id="A4.p3.3.m3.1a"><mn id="A4.p3.3.m3.1.1" xref="A4.p3.3.m3.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="A4.p3.3.m3.1b"><cn type="integer" id="A4.p3.3.m3.1.1.cmml" xref="A4.p3.3.m3.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p3.3.m3.1c">40</annotation></semantics></math> examples,
leaving <math id="A4.p3.4.m4.1" class="ltx_Math" alttext="48" display="inline"><semantics id="A4.p3.4.m4.1a"><mn id="A4.p3.4.m4.1.1" xref="A4.p3.4.m4.1.1.cmml">48</mn><annotation-xml encoding="MathML-Content" id="A4.p3.4.m4.1b"><cn type="integer" id="A4.p3.4.m4.1.1.cmml" xref="A4.p3.4.m4.1.1">48</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p3.4.m4.1c">48</annotation></semantics></math> classes with at least <math id="A4.p3.5.m5.1" class="ltx_Math" alttext="10" display="inline"><semantics id="A4.p3.5.m5.1a"><mn id="A4.p3.5.m5.1.1" xref="A4.p3.5.m5.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="A4.p3.5.m5.1b"><cn type="integer" id="A4.p3.5.m5.1.1.cmml" xref="A4.p3.5.m5.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p3.5.m5.1c">10</annotation></semantics></math> exemplars and <math id="A4.p3.6.m6.1" class="ltx_Math" alttext="60" display="inline"><semantics id="A4.p3.6.m6.1a"><mn id="A4.p3.6.m6.1.1" xref="A4.p3.6.m6.1.1.cmml">60</mn><annotation-xml encoding="MathML-Content" id="A4.p3.6.m6.1b"><cn type="integer" id="A4.p3.6.m6.1.1.cmml" xref="A4.p3.6.m6.1.1">60</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p3.6.m6.1c">60</annotation></semantics></math> classes
with less than <math id="A4.p3.7.m7.1" class="ltx_Math" alttext="10" display="inline"><semantics id="A4.p3.7.m7.1a"><mn id="A4.p3.7.m7.1.1" xref="A4.p3.7.m7.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="A4.p3.7.m7.1b"><cn type="integer" id="A4.p3.7.m7.1.1.cmml" xref="A4.p3.7.m7.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p3.7.m7.1c">10</annotation></semantics></math> exemplars.</p>
</div>
<div id="A4.p4" class="ltx_para">
<p id="A4.p4.8" class="ltx_p">The final dataset we turn to is VisualGenome <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Krishna et al.</span>, <a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2017</span></a>)</cite>, which provides
bounding box annotations for <math id="A4.p4.1.m1.1" class="ltx_Math" alttext="7605" display="inline"><semantics id="A4.p4.1.m1.1a"><mn id="A4.p4.1.m1.1.1" xref="A4.p4.1.m1.1.1.cmml">7605</mn><annotation-xml encoding="MathML-Content" id="A4.p4.1.m1.1b"><cn type="integer" id="A4.p4.1.m1.1.1.cmml" xref="A4.p4.1.m1.1.1">7605</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.1.m1.1c">7605</annotation></semantics></math> WordNet synsets.
We include the annotations from VisualGenome with an exact WordNet synset match
with the LVIS vocabulary to our IED.
We now have <math id="A4.p4.2.m2.1" class="ltx_Math" alttext="1110" display="inline"><semantics id="A4.p4.2.m2.1a"><mn id="A4.p4.2.m2.1.1" xref="A4.p4.2.m2.1.1.cmml">1110</mn><annotation-xml encoding="MathML-Content" id="A4.p4.2.m2.1b"><cn type="integer" id="A4.p4.2.m2.1.1.cmml" xref="A4.p4.2.m2.1.1">1110</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.2.m2.1c">1110</annotation></semantics></math> LVIS classes with at least <math id="A4.p4.3.m3.1" class="ltx_Math" alttext="40" display="inline"><semantics id="A4.p4.3.m3.1a"><mn id="A4.p4.3.m3.1.1" xref="A4.p4.3.m3.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="A4.p4.3.m3.1b"><cn type="integer" id="A4.p4.3.m3.1.1.cmml" xref="A4.p4.3.m3.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.3.m3.1c">40</annotation></semantics></math> exemplars and <math id="A4.p4.4.m4.1" class="ltx_Math" alttext="1160" display="inline"><semantics id="A4.p4.4.m4.1a"><mn id="A4.p4.4.m4.1.1" xref="A4.p4.4.m4.1.1.cmml">1160</mn><annotation-xml encoding="MathML-Content" id="A4.p4.4.m4.1b"><cn type="integer" id="A4.p4.4.m4.1.1.cmml" xref="A4.p4.4.m4.1.1">1160</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.4.m4.1c">1160</annotation></semantics></math> with
at least <math id="A4.p4.5.m5.1" class="ltx_Math" alttext="10" display="inline"><semantics id="A4.p4.5.m5.1a"><mn id="A4.p4.5.m5.1.1" xref="A4.p4.5.m5.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="A4.p4.5.m5.1b"><cn type="integer" id="A4.p4.5.m5.1.1.cmml" xref="A4.p4.5.m5.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.5.m5.1c">10</annotation></semantics></math> exemplars.
Reducing our minimum required number of exemplars per class from <math id="A4.p4.6.m6.1" class="ltx_Math" alttext="40" display="inline"><semantics id="A4.p4.6.m6.1a"><mn id="A4.p4.6.m6.1.1" xref="A4.p4.6.m6.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="A4.p4.6.m6.1b"><cn type="integer" id="A4.p4.6.m6.1.1.cmml" xref="A4.p4.6.m6.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.6.m6.1c">40</annotation></semantics></math> to <math id="A4.p4.7.m7.1" class="ltx_Math" alttext="10" display="inline"><semantics id="A4.p4.7.m7.1a"><mn id="A4.p4.7.m7.1.1" xref="A4.p4.7.m7.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="A4.p4.7.m7.1b"><cn type="integer" id="A4.p4.7.m7.1.1.cmml" xref="A4.p4.7.m7.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.7.m7.1c">10</annotation></semantics></math>
leaves <math id="A4.p4.8.m8.1" class="ltx_Math" alttext="43" display="inline"><semantics id="A4.p4.8.m8.1a"><mn id="A4.p4.8.m8.1.1" xref="A4.p4.8.m8.1.1.cmml">43</mn><annotation-xml encoding="MathML-Content" id="A4.p4.8.m8.1b"><cn type="integer" id="A4.p4.8.m8.1.1.cmml" xref="A4.p4.8.m8.1.1">43</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.8.m8.1c">43</annotation></semantics></math> classes with too few exemplars.</p>
</div>
<div id="A4.p5" class="ltx_para">
<p id="A4.p5.1" class="ltx_p">At this point, we inspect each of the remaining <math id="A4.p5.1.m1.1" class="ltx_Math" alttext="43" display="inline"><semantics id="A4.p5.1.m1.1a"><mn id="A4.p5.1.m1.1.1" xref="A4.p5.1.m1.1.1.cmml">43</mn><annotation-xml encoding="MathML-Content" id="A4.p5.1.m1.1b"><cn type="integer" id="A4.p5.1.m1.1.1.cmml" xref="A4.p5.1.m1.1.1">43</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p5.1.m1.1c">43</annotation></semantics></math> classes by hand and find
that all have other synsets present in ImageNet-21k which are visually identical
or very similar.
For example, “anklet” is a “common” class in LVIS, for which LVIS gives a
definition of “an ornament worn around the ankle” and a WordNet synset of
<span id="A4.p5.1.1" class="ltx_text ltx_font_typewriter">anklet.n.03</span>.
This synset is not found in ImageNet-21k but <span id="A4.p5.1.2" class="ltx_text ltx_font_typewriter">anklet.n.02</span>,
defined as “a sock that reaches just above the ankle” by WordNet, is present
and visual inspection shows these images to
actually exactly match <span id="A4.p5.1.3" class="ltx_text ltx_font_typewriter">anklet.n.03</span>.
Therefore, we add ImageNet-21k images relating to <span id="A4.p5.1.4" class="ltx_text ltx_font_typewriter">anklet.n.02</span> to our IED.
As another example, <span id="A4.p5.1.5" class="ltx_text ltx_font_typewriter">penny.n.02</span> (as in the penny coin) is a “rare”
class in LVIS for which exemplars could not be found automatically.
However, ImageNet-21k contains images of <span id="A4.p5.1.6" class="ltx_text ltx_font_typewriter">coin.n.01</span> which is a hypernym
<span id="A4.p5.1.7" class="ltx_text ltx_font_typewriter">penny.n.02</span>.
The images for <span id="A4.p5.1.8" class="ltx_text ltx_font_typewriter">coin.n.01</span> are visually
extremely similar and often identical to those one would expect for <span id="A4.p5.1.9" class="ltx_text ltx_font_typewriter">penny.n.02</span>
and so we add ImageNet-21k images relating to <span id="A4.p5.1.10" class="ltx_text ltx_font_typewriter">coin.n.02</span> to our IED.</p>
</div>
<div id="A4.p6" class="ltx_para">
<p id="A4.p6.3" class="ltx_p">After applying some human effort as described above,
our IED contains at least <math id="A4.p6.1.m1.1" class="ltx_Math" alttext="40" display="inline"><semantics id="A4.p6.1.m1.1a"><mn id="A4.p6.1.m1.1.1" xref="A4.p6.1.m1.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="A4.p6.1.m1.1b"><cn type="integer" id="A4.p6.1.m1.1.1.cmml" xref="A4.p6.1.m1.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p6.1.m1.1c">40</annotation></semantics></math> image exemplars for <math id="A4.p6.2.m2.1" class="ltx_Math" alttext="1110" display="inline"><semantics id="A4.p6.2.m2.1a"><mn id="A4.p6.2.m2.1.1" xref="A4.p6.2.m2.1.1.cmml">1110</mn><annotation-xml encoding="MathML-Content" id="A4.p6.2.m2.1b"><cn type="integer" id="A4.p6.2.m2.1.1.cmml" xref="A4.p6.2.m2.1.1">1110</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p6.2.m2.1c">1110</annotation></semantics></math> (92% of LVIS classes)
and at least <math id="A4.p6.3.m3.1" class="ltx_Math" alttext="10" display="inline"><semantics id="A4.p6.3.m3.1a"><mn id="A4.p6.3.m3.1.1" xref="A4.p6.3.m3.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="A4.p6.3.m3.1b"><cn type="integer" id="A4.p6.3.m3.1.1.cmml" xref="A4.p6.3.m3.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p6.3.m3.1c">10</annotation></semantics></math> image exemplars for all LVIS classes.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A5" class="ltx_appendix ltx_centering">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Varying Number of Image Exemplars for Vision-based Classifiers</h2>

<figure id="A5.F5" class="ltx_figure"><img src="/html/2306.05493/assets/x5.png" id="A5.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="322" height="322" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Detection performance of our vision-based classifiers on the LVIS OVOD
benchmark. We vary the number of image exemplars available per class, <math id="A5.F5.2.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="A5.F5.2.m1.1b"><mi id="A5.F5.2.m1.1.1" xref="A5.F5.2.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="A5.F5.2.m1.1c"><ci id="A5.F5.2.m1.1.1.cmml" xref="A5.F5.2.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.F5.2.m1.1d">K</annotation></semantics></math>,
to investigate the effect of the number of image exemplars on OVOD performance.
</figcaption>
</figure>
<div id="A5.p1" class="ltx_para">
<p id="A5.p1.4" class="ltx_p">In this section we show results using vision-based classifiers varying
the number of <math id="A5.p1.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="A5.p1.1.m1.1a"><mi id="A5.p1.1.m1.1.1" xref="A5.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="A5.p1.1.m1.1b"><ci id="A5.p1.1.m1.1.1.cmml" xref="A5.p1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.1.m1.1c">K</annotation></semantics></math> image exemplars used per class.
Figure <a href="#A5.F5" title="Figure 5 ‣ Appendix E Varying Number of Image Exemplars for Vision-based Classifiers ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows performance on the LVIS OVOD benchmark for
rare classes using <math id="A5.p1.2.m2.4" class="ltx_Math" alttext="K=1,2,5,10" display="inline"><semantics id="A5.p1.2.m2.4a"><mrow id="A5.p1.2.m2.4.5" xref="A5.p1.2.m2.4.5.cmml"><mi id="A5.p1.2.m2.4.5.2" xref="A5.p1.2.m2.4.5.2.cmml">K</mi><mo id="A5.p1.2.m2.4.5.1" xref="A5.p1.2.m2.4.5.1.cmml">=</mo><mrow id="A5.p1.2.m2.4.5.3.2" xref="A5.p1.2.m2.4.5.3.1.cmml"><mn id="A5.p1.2.m2.1.1" xref="A5.p1.2.m2.1.1.cmml">1</mn><mo id="A5.p1.2.m2.4.5.3.2.1" xref="A5.p1.2.m2.4.5.3.1.cmml">,</mo><mn id="A5.p1.2.m2.2.2" xref="A5.p1.2.m2.2.2.cmml">2</mn><mo id="A5.p1.2.m2.4.5.3.2.2" xref="A5.p1.2.m2.4.5.3.1.cmml">,</mo><mn id="A5.p1.2.m2.3.3" xref="A5.p1.2.m2.3.3.cmml">5</mn><mo id="A5.p1.2.m2.4.5.3.2.3" xref="A5.p1.2.m2.4.5.3.1.cmml">,</mo><mn id="A5.p1.2.m2.4.4" xref="A5.p1.2.m2.4.4.cmml">10</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="A5.p1.2.m2.4b"><apply id="A5.p1.2.m2.4.5.cmml" xref="A5.p1.2.m2.4.5"><eq id="A5.p1.2.m2.4.5.1.cmml" xref="A5.p1.2.m2.4.5.1"></eq><ci id="A5.p1.2.m2.4.5.2.cmml" xref="A5.p1.2.m2.4.5.2">𝐾</ci><list id="A5.p1.2.m2.4.5.3.1.cmml" xref="A5.p1.2.m2.4.5.3.2"><cn type="integer" id="A5.p1.2.m2.1.1.cmml" xref="A5.p1.2.m2.1.1">1</cn><cn type="integer" id="A5.p1.2.m2.2.2.cmml" xref="A5.p1.2.m2.2.2">2</cn><cn type="integer" id="A5.p1.2.m2.3.3.cmml" xref="A5.p1.2.m2.3.3">5</cn><cn type="integer" id="A5.p1.2.m2.4.4.cmml" xref="A5.p1.2.m2.4.4">10</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.2.m2.4c">K=1,2,5,10</annotation></semantics></math>,
where <math id="A5.p1.3.m3.1" class="ltx_Math" alttext="K" display="inline"><semantics id="A5.p1.3.m3.1a"><mi id="A5.p1.3.m3.1.1" xref="A5.p1.3.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="A5.p1.3.m3.1b"><ci id="A5.p1.3.m3.1.1.cmml" xref="A5.p1.3.m3.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.3.m3.1c">K</annotation></semantics></math> is the number of image exemplars per class used.
We compare our method which makes use of our aggregator (red dashed),
which has a transformer architecture, with the simple vector mean
of the embeddings (blue solid) for the <math id="A5.p1.4.m4.1" class="ltx_Math" alttext="K" display="inline"><semantics id="A5.p1.4.m4.1a"><mi id="A5.p1.4.m4.1.1" xref="A5.p1.4.m4.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="A5.p1.4.m4.1b"><ci id="A5.p1.4.m4.1.1.cmml" xref="A5.p1.4.m4.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.4.m4.1c">K</annotation></semantics></math> image exemplars.
In both cases we apply the ‘gentle’ TTA detailed and ablated in Section <a href="#A1" title="Appendix A Ablation Studies ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<div id="A5.p2" class="ltx_para">
<p id="A5.p2.1" class="ltx_p">These results use LVIS-base as detection training data, no additional image-level
labelled data <span id="A5.p2.1.1" class="ltx_ERROR undefined">\ie</span> <math id="A5.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{D}^{\textsc{img}}=\varnothing" display="inline"><semantics id="A5.p2.1.m1.1a"><mrow id="A5.p2.1.m1.1.1" xref="A5.p2.1.m1.1.1.cmml"><msup id="A5.p2.1.m1.1.1.2" xref="A5.p2.1.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="A5.p2.1.m1.1.1.2.2" xref="A5.p2.1.m1.1.1.2.2.cmml">𝒟</mi><mtext class="ltx_font_smallcaps" id="A5.p2.1.m1.1.1.2.3" xref="A5.p2.1.m1.1.1.2.3a.cmml">img</mtext></msup><mo id="A5.p2.1.m1.1.1.1" xref="A5.p2.1.m1.1.1.1.cmml">=</mo><mi mathvariant="normal" id="A5.p2.1.m1.1.1.3" xref="A5.p2.1.m1.1.1.3.cmml">∅</mi></mrow><annotation-xml encoding="MathML-Content" id="A5.p2.1.m1.1b"><apply id="A5.p2.1.m1.1.1.cmml" xref="A5.p2.1.m1.1.1"><eq id="A5.p2.1.m1.1.1.1.cmml" xref="A5.p2.1.m1.1.1.1"></eq><apply id="A5.p2.1.m1.1.1.2.cmml" xref="A5.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="A5.p2.1.m1.1.1.2.1.cmml" xref="A5.p2.1.m1.1.1.2">superscript</csymbol><ci id="A5.p2.1.m1.1.1.2.2.cmml" xref="A5.p2.1.m1.1.1.2.2">𝒟</ci><ci id="A5.p2.1.m1.1.1.2.3a.cmml" xref="A5.p2.1.m1.1.1.2.3"><mtext class="ltx_font_smallcaps" mathsize="70%" id="A5.p2.1.m1.1.1.2.3.cmml" xref="A5.p2.1.m1.1.1.2.3">img</mtext></ci></apply><emptyset id="A5.p2.1.m1.1.1.3.cmml" xref="A5.p2.1.m1.1.1.3"></emptyset></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p2.1.m1.1c">\mathcal{D}^{\textsc{img}}=\varnothing</annotation></semantics></math> and CLIP ViT-B/32 as the
pre-trained visual encoder to produce initial embeddings from each exemplar.</p>
</div>
<div id="A5.p3" class="ltx_para">
<p id="A5.p3.8" class="ltx_p">Figure <a href="#A5.F5" title="Figure 5 ‣ Appendix E Varying Number of Image Exemplars for Vision-based Classifiers ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows that for each value of <math id="A5.p3.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="A5.p3.1.m1.1a"><mi id="A5.p3.1.m1.1.1" xref="A5.p3.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="A5.p3.1.m1.1b"><ci id="A5.p3.1.m1.1.1.cmml" xref="A5.p3.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p3.1.m1.1c">K</annotation></semantics></math>,
the use of our aggregator boosts performance on rare classes demonstrating
the utility of our aggregator at combining the most useful information from the
<math id="A5.p3.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="A5.p3.2.m2.1a"><mi id="A5.p3.2.m2.1.1" xref="A5.p3.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="A5.p3.2.m2.1b"><ci id="A5.p3.2.m2.1.1.cmml" xref="A5.p3.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p3.2.m2.1c">K</annotation></semantics></math> given exemplars.
Our method for <math id="A5.p3.3.m3.1" class="ltx_Math" alttext="K=5" display="inline"><semantics id="A5.p3.3.m3.1a"><mrow id="A5.p3.3.m3.1.1" xref="A5.p3.3.m3.1.1.cmml"><mi id="A5.p3.3.m3.1.1.2" xref="A5.p3.3.m3.1.1.2.cmml">K</mi><mo id="A5.p3.3.m3.1.1.1" xref="A5.p3.3.m3.1.1.1.cmml">=</mo><mn id="A5.p3.3.m3.1.1.3" xref="A5.p3.3.m3.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.p3.3.m3.1b"><apply id="A5.p3.3.m3.1.1.cmml" xref="A5.p3.3.m3.1.1"><eq id="A5.p3.3.m3.1.1.1.cmml" xref="A5.p3.3.m3.1.1.1"></eq><ci id="A5.p3.3.m3.1.1.2.cmml" xref="A5.p3.3.m3.1.1.2">𝐾</ci><cn type="integer" id="A5.p3.3.m3.1.1.3.cmml" xref="A5.p3.3.m3.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p3.3.m3.1c">K=5</annotation></semantics></math> surpasses the performance of <math id="A5.p3.4.m4.1" class="ltx_Math" alttext="K=10" display="inline"><semantics id="A5.p3.4.m4.1a"><mrow id="A5.p3.4.m4.1.1" xref="A5.p3.4.m4.1.1.cmml"><mi id="A5.p3.4.m4.1.1.2" xref="A5.p3.4.m4.1.1.2.cmml">K</mi><mo id="A5.p3.4.m4.1.1.1" xref="A5.p3.4.m4.1.1.1.cmml">=</mo><mn id="A5.p3.4.m4.1.1.3" xref="A5.p3.4.m4.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.p3.4.m4.1b"><apply id="A5.p3.4.m4.1.1.cmml" xref="A5.p3.4.m4.1.1"><eq id="A5.p3.4.m4.1.1.1.cmml" xref="A5.p3.4.m4.1.1.1"></eq><ci id="A5.p3.4.m4.1.1.2.cmml" xref="A5.p3.4.m4.1.1.2">𝐾</ci><cn type="integer" id="A5.p3.4.m4.1.1.3.cmml" xref="A5.p3.4.m4.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p3.4.m4.1c">K=10</annotation></semantics></math> with simple vector
averaging.
For <math id="A5.p3.5.m5.1" class="ltx_Math" alttext="K=1" display="inline"><semantics id="A5.p3.5.m5.1a"><mrow id="A5.p3.5.m5.1.1" xref="A5.p3.5.m5.1.1.cmml"><mi id="A5.p3.5.m5.1.1.2" xref="A5.p3.5.m5.1.1.2.cmml">K</mi><mo id="A5.p3.5.m5.1.1.1" xref="A5.p3.5.m5.1.1.1.cmml">=</mo><mn id="A5.p3.5.m5.1.1.3" xref="A5.p3.5.m5.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.p3.5.m5.1b"><apply id="A5.p3.5.m5.1.1.cmml" xref="A5.p3.5.m5.1.1"><eq id="A5.p3.5.m5.1.1.1.cmml" xref="A5.p3.5.m5.1.1.1"></eq><ci id="A5.p3.5.m5.1.1.2.cmml" xref="A5.p3.5.m5.1.1.2">𝐾</ci><cn type="integer" id="A5.p3.5.m5.1.1.3.cmml" xref="A5.p3.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p3.5.m5.1c">K=1</annotation></semantics></math>, our method improves performance by <math id="A5.p3.6.m6.1" class="ltx_Math" alttext="2.3" display="inline"><semantics id="A5.p3.6.m6.1a"><mn id="A5.p3.6.m6.1.1" xref="A5.p3.6.m6.1.1.cmml">2.3</mn><annotation-xml encoding="MathML-Content" id="A5.p3.6.m6.1b"><cn type="float" id="A5.p3.6.m6.1.1.cmml" xref="A5.p3.6.m6.1.1">2.3</cn></annotation-xml><annotation encoding="application/x-tex" id="A5.p3.6.m6.1c">2.3</annotation></semantics></math> APr which further
demonstrates the improved feature representation — <math id="A5.p3.7.m7.1" class="ltx_Math" alttext="K=1" display="inline"><semantics id="A5.p3.7.m7.1a"><mrow id="A5.p3.7.m7.1.1" xref="A5.p3.7.m7.1.1.cmml"><mi id="A5.p3.7.m7.1.1.2" xref="A5.p3.7.m7.1.1.2.cmml">K</mi><mo id="A5.p3.7.m7.1.1.1" xref="A5.p3.7.m7.1.1.1.cmml">=</mo><mn id="A5.p3.7.m7.1.1.3" xref="A5.p3.7.m7.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.p3.7.m7.1b"><apply id="A5.p3.7.m7.1.1.cmml" xref="A5.p3.7.m7.1.1"><eq id="A5.p3.7.m7.1.1.1.cmml" xref="A5.p3.7.m7.1.1.1"></eq><ci id="A5.p3.7.m7.1.1.2.cmml" xref="A5.p3.7.m7.1.1.2">𝐾</ci><cn type="integer" id="A5.p3.7.m7.1.1.3.cmml" xref="A5.p3.7.m7.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p3.7.m7.1c">K=1</annotation></semantics></math> involves no
aggregation as only <math id="A5.p3.8.m8.1" class="ltx_Math" alttext="1" display="inline"><semantics id="A5.p3.8.m8.1a"><mn id="A5.p3.8.m8.1.1" xref="A5.p3.8.m8.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="A5.p3.8.m8.1b"><cn type="integer" id="A5.p3.8.m8.1.1.cmml" xref="A5.p3.8.m8.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="A5.p3.8.m8.1c">1</annotation></semantics></math> exemplar is available per class.</p>
</div>
<div id="A5.p4" class="ltx_para">
<p id="A5.p4.1" class="ltx_p">Furthermore, we compare to the performance of our text-based classifiers which
make use of rich class descriptions sourced from a GPT-3 model.
Our vision-based classifiers cannot surpass the performance of our text-based
classifiers demonstrating the need for more research into using image exemplars
for OVOD.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A6" class="ltx_appendix ltx_centering">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Additional Example Class Descriptions</h2>

<div id="A6.p1" class="ltx_para">
<p id="A6.p1.1" class="ltx_p">In this section we show a selection of rich class descriptions sourced from the
<span id="A6.p1.1.1" class="ltx_text ltx_font_typewriter">text-davinci-002</span> text completion model from OpenAI.
For each class in the LVIS vocabulary we generate <math id="A6.p1.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="A6.p1.1.m1.1a"><mn id="A6.p1.1.m1.1.1" xref="A6.p1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="A6.p1.1.m1.1b"><cn type="integer" id="A6.p1.1.m1.1.1.cmml" xref="A6.p1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.1.m1.1c">10</annotation></semantics></math> rich descriptions.
We also give the LVIS frequency category — rare, common, frequent.</p>
</div>
<section id="A6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.1 </span>Generated descriptions for “bagpipe” (rare)</h3>

<div id="A6.SS1.p1" class="ltx_para">
<ol id="A6.I1" class="ltx_enumerate">
<li id="A6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A6.I1.i1.p1" class="ltx_para">
<p id="A6.I1.i1.p1.1" class="ltx_p"><span id="A6.I1.i1.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A bagpipe is a wind instrument with a bag that is filled with air.</span></p>
</div>
</li>
<li id="A6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A6.I1.i2.p1" class="ltx_para">
<p id="A6.I1.i2.p1.1" class="ltx_p"><span id="A6.I1.i2.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A bagpipe typically consists of a blowstick, a chanter, and one or more drones.</span></p>
</div>
</li>
<li id="A6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A6.I1.i3.p1" class="ltx_para">
<p id="A6.I1.i3.p1.1" class="ltx_p"><span id="A6.I1.i3.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A bagpipe is a musical instrument that has a bag, a blowpipe, and usually two drones.</span></p>
</div>
</li>
<li id="A6.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="A6.I1.i4.p1" class="ltx_para">
<p id="A6.I1.i4.p1.1" class="ltx_p"><span id="A6.I1.i4.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A bagpipe is a wind instrument with a bag that collects air, a reed pipe for each note, and a blowpipe.</span></p>
</div>
</li>
<li id="A6.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="A6.I1.i5.p1" class="ltx_para">
<p id="A6.I1.i5.p1.1" class="ltx_p"><span id="A6.I1.i5.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A bagpipe is a musical instrument that is played by blowing into a bag of air.</span></p>
</div>
</li>
<li id="A6.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="A6.I1.i6.p1" class="ltx_para">
<p id="A6.I1.i6.p1.1" class="ltx_p"><span id="A6.I1.i6.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">Bagpipes vary in appearance, but most have a bag made from a animal skin, a blowpipe, a chanter, and one or more drones.</span></p>
</div>
</li>
<li id="A6.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span> 
<div id="A6.I1.i7.p1" class="ltx_para">
<p id="A6.I1.i7.p1.1" class="ltx_p"><span id="A6.I1.i7.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A bagpipe is a musical instrument that has a bag, a blowpipe, and usually drone pipes.</span></p>
</div>
</li>
<li id="A6.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span> 
<div id="A6.I1.i8.p1" class="ltx_para">
<p id="A6.I1.i8.p1.1" class="ltx_p"><span id="A6.I1.i8.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A bagpipe is a musical instrument that is usually made out of wood.</span></p>
</div>
</li>
<li id="A6.I1.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9.</span> 
<div id="A6.I1.i9.p1" class="ltx_para">
<p id="A6.I1.i9.p1.1" class="ltx_p"><span id="A6.I1.i9.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A typical Highland bagpipe has a chanter with a double reed, a blowstick, three drones with single reeds, and a bag.</span></p>
</div>
</li>
<li id="A6.I1.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">10.</span> 
<div id="A6.I1.i10.p1" class="ltx_para">
<p id="A6.I1.i10.p1.1" class="ltx_p"><span id="A6.I1.i10.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A bagpipe consists of a blowing bag, a chanter, a drone, and usually one or more drones.</span></p>
</div>
</li>
</ol>
</div>
</section>
<section id="A6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.2 </span>Generated descriptions for “trench coat” (rare)</h3>

<div id="A6.SS2.p1" class="ltx_para">
<ol id="A6.I2" class="ltx_enumerate">
<li id="A6.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A6.I2.i1.p1" class="ltx_para">
<p id="A6.I2.i1.p1.1" class="ltx_p"><span id="A6.I2.i1.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A trench coat is a coat made of heavy cloth, sometimes waterproof, that hangs to about knee length.</span></p>
</div>
</li>
<li id="A6.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A6.I2.i2.p1" class="ltx_para">
<p id="A6.I2.i2.p1.1" class="ltx_p"><span id="A6.I2.i2.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A trench coat looks like a long, military-style coat.</span></p>
</div>
</li>
<li id="A6.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A6.I2.i3.p1" class="ltx_para">
<p id="A6.I2.i3.p1.1" class="ltx_p"><span id="A6.I2.i3.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A trench coat typically looks like a long, belted raincoat.</span></p>
</div>
</li>
<li id="A6.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="A6.I2.i4.p1" class="ltx_para">
<p id="A6.I2.i4.p1.1" class="ltx_p"><span id="A6.I2.i4.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A trench coat is a long, water-resistant coat that is typically worn over top of a suit.</span></p>
</div>
</li>
<li id="A6.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="A6.I2.i5.p1" class="ltx_para">
<p id="A6.I2.i5.p1.1" class="ltx_p"><span id="A6.I2.i5.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A trench coat typically has a removable liner, a double-breasted front, and belted cuffs.</span></p>
</div>
</li>
<li id="A6.I2.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="A6.I2.i6.p1" class="ltx_para">
<p id="A6.I2.i6.p1.1" class="ltx_p"><span id="A6.I2.i6.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A trench coat generally refers to a type of coat that is longer than waist length.</span></p>
</div>
</li>
<li id="A6.I2.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span> 
<div id="A6.I2.i7.p1" class="ltx_para">
<p id="A6.I2.i7.p1.1" class="ltx_p"><span id="A6.I2.i7.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A trench coat is a coat that is usually a little bit longer than waist length, has a tie or a belt around the waist, and has a collar.</span></p>
</div>
</li>
<li id="A6.I2.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span> 
<div id="A6.I2.i8.p1" class="ltx_para">
<p id="A6.I2.i8.p1.1" class="ltx_p"><span id="A6.I2.i8.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A trench coat is a coat made of waterproof material, typically hip-length or longer, with a belt and a collar.</span></p>
</div>
</li>
<li id="A6.I2.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9.</span> 
<div id="A6.I2.i9.p1" class="ltx_para">
<p id="A6.I2.i9.p1.1" class="ltx_p"><span id="A6.I2.i9.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A trench coat is a long, light coat with a belt.</span></p>
</div>
</li>
<li id="A6.I2.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">10.</span> 
<div id="A6.I2.i10.p1" class="ltx_para">
<p id="A6.I2.i10.p1.1" class="ltx_p"><span id="A6.I2.i10.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A trench coat is a raincoat made of heavy-duty fabric, typically poplin, gabardine, or drill.</span></p>
</div>
</li>
</ol>
</div>
</section>
<section id="A6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.3 </span>Generated descriptions for “walrus” (rare)</h3>

<div id="A6.SS3.p1" class="ltx_para">
<ol id="A6.I3" class="ltx_enumerate">
<li id="A6.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A6.I3.i1.p1" class="ltx_para">
<p id="A6.I3.i1.p1.1" class="ltx_p"><span id="A6.I3.i1.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A walrus is a large, flippered marine mammal with a bulky body, short limbs, and a large head with two long tusks protruding from the mouth.</span></p>
</div>
</li>
<li id="A6.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A6.I3.i2.p1" class="ltx_para">
<p id="A6.I3.i2.p1.1" class="ltx_p"><span id="A6.I3.i2.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A walrus is a blubbery mammal with long tusks, whiskers, and a seal-like face.</span></p>
</div>
</li>
<li id="A6.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A6.I3.i3.p1" class="ltx_para">
<p id="A6.I3.i3.p1.1" class="ltx_p"><span id="A6.I3.i3.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A walrus is a large, flippered marine mammal with a long, tusked head.</span></p>
</div>
</li>
<li id="A6.I3.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="A6.I3.i4.p1" class="ltx_para">
<p id="A6.I3.i4.p1.1" class="ltx_p"><span id="A6.I3.i4.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A walrus is a stocky, rounded pinniped with small flippers, short fur, and long tusks.</span></p>
</div>
</li>
<li id="A6.I3.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="A6.I3.i5.p1" class="ltx_para">
<p id="A6.I3.i5.p1.1" class="ltx_p"><span id="A6.I3.i5.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A walrus is a large, flippered marine mammal with a bulky body, short tail, and wide, flat head.</span></p>
</div>
</li>
<li id="A6.I3.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="A6.I3.i6.p1" class="ltx_para">
<p id="A6.I3.i6.p1.1" class="ltx_p"><span id="A6.I3.i6.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A walrus is a large ocean mammal with two long tusks, a thick fur coat, and large flippers.</span></p>
</div>
</li>
<li id="A6.I3.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span> 
<div id="A6.I3.i7.p1" class="ltx_para">
<p id="A6.I3.i7.p1.1" class="ltx_p"><span id="A6.I3.i7.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A walrus is a large flippered marine mammal with a discontinuous distribution about the North Pole in the Arctic Ocean and sub-Arctic seas of the Northern Hemisphere.</span></p>
</div>
</li>
<li id="A6.I3.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span> 
<div id="A6.I3.i8.p1" class="ltx_para">
<p id="A6.I3.i8.p1.1" class="ltx_p"><span id="A6.I3.i8.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A walrus is a large flippered marine mammal with a thick fur coat.</span></p>
</div>
</li>
<li id="A6.I3.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9.</span> 
<div id="A6.I3.i9.p1" class="ltx_para">
<p id="A6.I3.i9.p1.1" class="ltx_p"><span id="A6.I3.i9.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A walrus is a large marine mammal with a body shaped somewhat like a seal.</span></p>
</div>
</li>
<li id="A6.I3.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">10.</span> 
<div id="A6.I3.i10.p1" class="ltx_para">
<p id="A6.I3.i10.p1.1" class="ltx_p"><span id="A6.I3.i10.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A walrus is a seal with a long face and large tusks.</span></p>
</div>
</li>
</ol>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.4 </span>Generated descriptions for “briefcase” (common)</h3>

<div id="A6.SS4.p1" class="ltx_para">
<ol id="A6.I4" class="ltx_enumerate">
<li id="A6.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A6.I4.i1.p1" class="ltx_para">
<p id="A6.I4.i1.p1.1" class="ltx_p"><span id="A6.I4.i1.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A briefcase is a rectangular, portable case used to hold papers, documents, or other materials.</span></p>
</div>
</li>
<li id="A6.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A6.I4.i2.p1" class="ltx_para">
<p id="A6.I4.i2.p1.1" class="ltx_p"><span id="A6.I4.i2.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A briefcase is a small case used to carry documents and other small items.</span></p>
</div>
</li>
<li id="A6.I4.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A6.I4.i3.p1" class="ltx_para">
<p id="A6.I4.i3.p1.1" class="ltx_p"><span id="A6.I4.i3.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A briefcase is a small, rectangular-shaped case that is used to carry important documents or other items.</span></p>
</div>
</li>
<li id="A6.I4.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="A6.I4.i4.p1" class="ltx_para">
<p id="A6.I4.i4.p1.1" class="ltx_p"><span id="A6.I4.i4.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A briefcase is typically a rectangle shaped bag made of leather or synthetic materials.</span></p>
</div>
</li>
<li id="A6.I4.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="A6.I4.i5.p1" class="ltx_para">
<p id="A6.I4.i5.p1.1" class="ltx_p"><span id="A6.I4.i5.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A briefcase generally looks like a small, rectangular case made out of a variety of materials, such as leather, canvas, or nylon.</span></p>
</div>
</li>
<li id="A6.I4.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="A6.I4.i6.p1" class="ltx_para">
<p id="A6.I4.i6.p1.1" class="ltx_p"><span id="A6.I4.i6.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A briefcase is a narrow rectangular case used to carry documents and other valuables.</span></p>
</div>
</li>
<li id="A6.I4.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span> 
<div id="A6.I4.i7.p1" class="ltx_para">
<p id="A6.I4.i7.p1.1" class="ltx_p"><span id="A6.I4.i7.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A briefcase is a box-shaped bag typically used by businesspeople to transport important documents.</span></p>
</div>
</li>
<li id="A6.I4.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span> 
<div id="A6.I4.i8.p1" class="ltx_para">
<p id="A6.I4.i8.p1.1" class="ltx_p"><span id="A6.I4.i8.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A briefcase is a rectangular leather case with a handle.</span></p>
</div>
</li>
<li id="A6.I4.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9.</span> 
<div id="A6.I4.i9.p1" class="ltx_para">
<p id="A6.I4.i9.p1.1" class="ltx_p"><span id="A6.I4.i9.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A briefcase is a small case used to carry documents and other small items.</span></p>
</div>
</li>
<li id="A6.I4.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">10.</span> 
<div id="A6.I4.i10.p1" class="ltx_para">
<p id="A6.I4.i10.p1.1" class="ltx_p"><span id="A6.I4.i10.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A typical briefcase is rectangular and has a handle on the top.</span></p>
</div>
</li>
</ol>
</div>
</section>
<section id="A6.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.5 </span>Generated descriptions for “coin” (common)</h3>

<div id="A6.SS5.p1" class="ltx_para">
<ol id="A6.I5" class="ltx_enumerate">
<li id="A6.I5.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A6.I5.i1.p1" class="ltx_para">
<p id="A6.I5.i1.p1.1" class="ltx_p"><span id="A6.I5.i1.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A coin is a small, flat, round piece of metal or plastic that is used as money.</span></p>
</div>
</li>
<li id="A6.I5.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A6.I5.i2.p1" class="ltx_para">
<p id="A6.I5.i2.p1.1" class="ltx_p"><span id="A6.I5.i2.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A coin has a head side and a tail side.</span></p>
</div>
</li>
<li id="A6.I5.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A6.I5.i3.p1" class="ltx_para">
<p id="A6.I5.i3.p1.1" class="ltx_p"><span id="A6.I5.i3.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A coin is usually a small, flat, round piece of metal or plastic that is used as money.</span></p>
</div>
</li>
<li id="A6.I5.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="A6.I5.i4.p1" class="ltx_para">
<p id="A6.I5.i4.p1.1" class="ltx_p"><span id="A6.I5.i4.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A coin has a round shape and is flat.</span></p>
</div>
</li>
<li id="A6.I5.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="A6.I5.i5.p1" class="ltx_para">
<p id="A6.I5.i5.p1.1" class="ltx_p"><span id="A6.I5.i5.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A coin generally has a circular shape with a raised edge, and two faces --- one on each side.</span></p>
</div>
</li>
<li id="A6.I5.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="A6.I5.i6.p1" class="ltx_para">
<p id="A6.I5.i6.p1.1" class="ltx_p"><span id="A6.I5.i6.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A coin is a small, flat, round piece of metal or plastic that is used as money.</span></p>
</div>
</li>
<li id="A6.I5.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span> 
<div id="A6.I5.i7.p1" class="ltx_para">
<p id="A6.I5.i7.p1.1" class="ltx_p"><span id="A6.I5.i7.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A coin is a round piece of metal with an image on one side and the words ‘‘United States of America’’ on the other.</span></p>
</div>
</li>
<li id="A6.I5.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span> 
<div id="A6.I5.i8.p1" class="ltx_para">
<p id="A6.I5.i8.p1.1" class="ltx_p"><span id="A6.I5.i8.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A coin is a small, round, flat piece of metal or plastic that is used as money.</span></p>
</div>
</li>
<li id="A6.I5.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9.</span> 
<div id="A6.I5.i9.p1" class="ltx_para">
<p id="A6.I5.i9.p1.1" class="ltx_p"><span id="A6.I5.i9.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">Sure, a coin is a small, round, flat piece of metal or plastic that is used as money.</span></p>
</div>
</li>
<li id="A6.I5.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">10.</span> 
<div id="A6.I5.i10.p1" class="ltx_para">
<p id="A6.I5.i10.p1.1" class="ltx_p"><span id="A6.I5.i10.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A coin is a small, round, metal disk with an image on one side and raised lettering on the other.</span></p>
</div>
</li>
</ol>
</div>
</section>
<section id="A6.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.6 </span>Generated descriptions for “waffle” (common)</h3>

<div id="A6.SS6.p1" class="ltx_para">
<ol id="A6.I6" class="ltx_enumerate">
<li id="A6.I6.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A6.I6.i1.p1" class="ltx_para">
<p id="A6.I6.i1.p1.1" class="ltx_p"><span id="A6.I6.i1.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A waffle is a pancake-like pastry that is cooked in a waffle iron and has a distinctively grid-like pattern on the top and bottom.</span></p>
</div>
</li>
<li id="A6.I6.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A6.I6.i2.p1" class="ltx_para">
<p id="A6.I6.i2.p1.1" class="ltx_p"><span id="A6.I6.i2.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A waffle is a dish made from leavened batter or dough that is cooked between two plates that are patterned to give a characteristic size, shape, and surface impression.</span></p>
</div>
</li>
<li id="A6.I6.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A6.I6.i3.p1" class="ltx_para">
<p id="A6.I6.i3.p1.1" class="ltx_p"><span id="A6.I6.i3.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A waffle is a thin, round, batter-based cake that is cooked in a waffle iron and is usually served with syrup.</span></p>
</div>
</li>
<li id="A6.I6.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="A6.I6.i4.p1" class="ltx_para">
<p id="A6.I6.i4.p1.1" class="ltx_p"><span id="A6.I6.i4.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A waffle looks like a round, honeycomb-patterned cake that is cooked in a waffle iron.</span></p>
</div>
</li>
<li id="A6.I6.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="A6.I6.i5.p1" class="ltx_para">
<p id="A6.I6.i5.p1.1" class="ltx_p"><span id="A6.I6.i5.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A waffle is typically a leavened batter or dough that is cooked between two plates that are patterned to give it a characteristic size, shape, and surface impression.</span></p>
</div>
</li>
<li id="A6.I6.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="A6.I6.i6.p1" class="ltx_para">
<p id="A6.I6.i6.p1.1" class="ltx_p"><span id="A6.I6.i6.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A waffle looks like an egg-shaped pancake with deep indentations.</span></p>
</div>
</li>
<li id="A6.I6.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span> 
<div id="A6.I6.i7.p1" class="ltx_para">
<p id="A6.I6.i7.p1.1" class="ltx_p"><span id="A6.I6.i7.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A waffle looks like a grid of squares.</span></p>
</div>
</li>
<li id="A6.I6.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span> 
<div id="A6.I6.i8.p1" class="ltx_para">
<p id="A6.I6.i8.p1.1" class="ltx_p"><span id="A6.I6.i8.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A waffle is a pancake-like pastry that is made from a leavened batter or dough and is cooked between two heated plates.</span></p>
</div>
</li>
<li id="A6.I6.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9.</span> 
<div id="A6.I6.i9.p1" class="ltx_para">
<p id="A6.I6.i9.p1.1" class="ltx_p"><span id="A6.I6.i9.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A waffle is a pancake-like food that is cooked in a waffle iron.</span></p>
</div>
</li>
<li id="A6.I6.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">10.</span> 
<div id="A6.I6.i10.p1" class="ltx_para">
<p id="A6.I6.i10.p1.1" class="ltx_p"><span id="A6.I6.i10.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A waffle is a hotcake with different patterns on it.</span></p>
</div>
</li>
</ol>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A6.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.7 </span>Generated descriptions for “avocado” (frequent)</h3>

<div id="A6.SS7.p1" class="ltx_para">
<ol id="A6.I7" class="ltx_enumerate">
<li id="A6.I7.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A6.I7.i1.p1" class="ltx_para">
<p id="A6.I7.i1.p1.1" class="ltx_p"><span id="A6.I7.i1.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">An avocado looks like a pear-shaped fruit with green or blackish skin.</span></p>
</div>
</li>
<li id="A6.I7.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A6.I7.i2.p1" class="ltx_para">
<p id="A6.I7.i2.p1.1" class="ltx_p"><span id="A6.I7.i2.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">It is a green fruit that has a dark brown or black seed in the center.</span></p>
</div>
</li>
<li id="A6.I7.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A6.I7.i3.p1" class="ltx_para">
<p id="A6.I7.i3.p1.1" class="ltx_p"><span id="A6.I7.i3.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">An avocado is a pear-shaped green fruit with smooth, green skin and a large seed in the center.</span></p>
</div>
</li>
<li id="A6.I7.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="A6.I7.i4.p1" class="ltx_para">
<p id="A6.I7.i4.p1.1" class="ltx_p"><span id="A6.I7.i4.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">An avocado is a fruit that is brown and bumpy on the outside and green and creamy on the inside.</span></p>
</div>
</li>
<li id="A6.I7.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="A6.I7.i5.p1" class="ltx_para">
<p id="A6.I7.i5.p1.1" class="ltx_p"><span id="A6.I7.i5.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">An avocado is a fruit with a dark green or blackish skin and a soft, fleshy inside.</span></p>
</div>
</li>
<li id="A6.I7.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="A6.I7.i6.p1" class="ltx_para">
<p id="A6.I7.i6.p1.1" class="ltx_p"><span id="A6.I7.i6.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">An avocado is a green, pear-shaped fruit with a smooth, fleshy texture.</span></p>
</div>
</li>
<li id="A6.I7.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span> 
<div id="A6.I7.i7.p1" class="ltx_para">
<p id="A6.I7.i7.p1.1" class="ltx_p"><span id="A6.I7.i7.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">An avocado is a pear-shaped fruit with smooth, green skin.</span></p>
</div>
</li>
<li id="A6.I7.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span> 
<div id="A6.I7.i8.p1" class="ltx_para">
<p id="A6.I7.i8.p1.1" class="ltx_p"><span id="A6.I7.i8.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">An avocado is shaped like an egg and has a greenish-brownish skin.</span></p>
</div>
</li>
<li id="A6.I7.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9.</span> 
<div id="A6.I7.i9.p1" class="ltx_para">
<p id="A6.I7.i9.p1.1" class="ltx_p"><span id="A6.I7.i9.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">An avocado is typically a dark green or black color on the outside with a soft, light green or yellow color on the inside.</span></p>
</div>
</li>
<li id="A6.I7.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">10.</span> 
<div id="A6.I7.i10.p1" class="ltx_para">
<p id="A6.I7.i10.p1.1" class="ltx_p"><span id="A6.I7.i10.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">An avocado is a pear-shaped fruit with smooth, green skin and a large, pit in the center.</span></p>
</div>
</li>
</ol>
</div>
</section>
<section id="A6.SS8" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.8 </span>Generated descriptions for “sausage” (frequent)</h3>

<div id="A6.SS8.p1" class="ltx_para">
<ol id="A6.I8" class="ltx_enumerate">
<li id="A6.I8.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A6.I8.i1.p1" class="ltx_para">
<p id="A6.I8.i1.p1.1" class="ltx_p"><span id="A6.I8.i1.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A sausage is a cylindrical shape made of ground meat, typically pork, and a variety of spices and other ingredients.</span></p>
</div>
</li>
<li id="A6.I8.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A6.I8.i2.p1" class="ltx_para">
<p id="A6.I8.i2.p1.1" class="ltx_p"><span id="A6.I8.i2.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A sausage is typically a ground-up mix of pork, beef, or other meats with spices and salt, encased in a thin skin.</span></p>
</div>
</li>
<li id="A6.I8.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A6.I8.i3.p1" class="ltx_para">
<p id="A6.I8.i3.p1.1" class="ltx_p"><span id="A6.I8.i3.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A sausage is typically a cylindrical shaped food made from ground meat, spices, and other ingredients.</span></p>
</div>
</li>
<li id="A6.I8.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="A6.I8.i4.p1" class="ltx_para">
<p id="A6.I8.i4.p1.1" class="ltx_p"><span id="A6.I8.i4.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A sausage is a long, thin, cylindrical piece of meat.</span></p>
</div>
</li>
<li id="A6.I8.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="A6.I8.i5.p1" class="ltx_para">
<p id="A6.I8.i5.p1.1" class="ltx_p"><span id="A6.I8.i5.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">Sausages are typically long, cylindrical shaped foods made from ground meat and spices.</span></p>
</div>
</li>
<li id="A6.I8.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="A6.I8.i6.p1" class="ltx_para">
<p id="A6.I8.i6.p1.1" class="ltx_p"><span id="A6.I8.i6.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A sausage is a tubular meat product typically made from ground pork, beef, or poultry.</span></p>
</div>
</li>
<li id="A6.I8.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span> 
<div id="A6.I8.i7.p1" class="ltx_para">
<p id="A6.I8.i7.p1.1" class="ltx_p"><span id="A6.I8.i7.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A sausage is a cylindrical casing filled with meat, typically pork, and various herbs and spices.</span></p>
</div>
</li>
<li id="A6.I8.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span> 
<div id="A6.I8.i8.p1" class="ltx_para">
<p id="A6.I8.i8.p1.1" class="ltx_p"><span id="A6.I8.i8.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A sausage looks like a tubular shape made of ground up meat that is usually encased in a thin layer of intestine.</span></p>
</div>
</li>
<li id="A6.I8.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9.</span> 
<div id="A6.I8.i9.p1" class="ltx_para">
<p id="A6.I8.i9.p1.1" class="ltx_p"><span id="A6.I8.i9.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A sausage is a cylindrical casing of meat that is typically filled with ground pork, although many other variations exist.</span></p>
</div>
</li>
<li id="A6.I8.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">10.</span> 
<div id="A6.I8.i10.p1" class="ltx_para">
<p id="A6.I8.i10.p1.1" class="ltx_p"><span id="A6.I8.i10.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">When cooked, a sausage is typically cylindrical and can vary in length.</span></p>
</div>
</li>
</ol>
</div>
</section>
<section id="A6.SS9" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.9 </span>Generated descriptions for “spectacles” (frequent)</h3>

<div id="A6.SS9.p1" class="ltx_para">
<ol id="A6.I9" class="ltx_enumerate">
<li id="A6.I9.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A6.I9.i1.p1" class="ltx_para">
<p id="A6.I9.i1.p1.1" class="ltx_p"><span id="A6.I9.i1.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A spectacle is a pair of eyeglasses.</span></p>
</div>
</li>
<li id="A6.I9.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A6.I9.i2.p1" class="ltx_para">
<p id="A6.I9.i2.p1.1" class="ltx_p"><span id="A6.I9.i2.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A spectacles is a type of eyewear that consists of a frame that holds two lenses in front of the eyes.</span></p>
</div>
</li>
<li id="A6.I9.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A6.I9.i3.p1" class="ltx_para">
<p id="A6.I9.i3.p1.1" class="ltx_p"><span id="A6.I9.i3.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">Spectacles are a type of eyewear that helps people see more clearly.</span></p>
</div>
</li>
<li id="A6.I9.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="A6.I9.i4.p1" class="ltx_para">
<p id="A6.I9.i4.p1.1" class="ltx_p"><span id="A6.I9.i4.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">Spectacles are glasses that are worn in order to improve vision.</span></p>
</div>
</li>
<li id="A6.I9.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="A6.I9.i5.p1" class="ltx_para">
<p id="A6.I9.i5.p1.1" class="ltx_p"><span id="A6.I9.i5.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A spectacles usually refers to a glass or plastic lens worn in front of the eye to correct vision, or protect the eye from debris, dust, wind, etc.</span></p>
</div>
</li>
<li id="A6.I9.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="A6.I9.i6.p1" class="ltx_para">
<p id="A6.I9.i6.p1.1" class="ltx_p"><span id="A6.I9.i6.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A spectacles is a type of corrective lens used to improve vision.</span></p>
</div>
</li>
<li id="A6.I9.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span> 
<div id="A6.I9.i7.p1" class="ltx_para">
<p id="A6.I9.i7.p1.1" class="ltx_p"><span id="A6.I9.i7.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A spectacle is a lens worn in front of the eye to correct vision, for cosmetic reasons, or to protect the eye.</span></p>
</div>
</li>
<li id="A6.I9.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span> 
<div id="A6.I9.i8.p1" class="ltx_para">
<p id="A6.I9.i8.p1.1" class="ltx_p"><span id="A6.I9.i8.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A spectacles has a frame that goes around your head and two lenses in front of your eyes.</span></p>
</div>
</li>
<li id="A6.I9.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9.</span> 
<div id="A6.I9.i9.p1" class="ltx_para">
<p id="A6.I9.i9.p1.1" class="ltx_p"><span id="A6.I9.i9.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A spectacles has two glass or plastic lenses in metal or plastic frames that rest on the ears.</span></p>
</div>
</li>
<li id="A6.I9.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">10.</span> 
<div id="A6.I9.i10.p1" class="ltx_para">
<p id="A6.I9.i10.p1.1" class="ltx_p"><span id="A6.I9.i10.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">A pair of spectacles is a frame that holds two eyeglasses lenses in front of a person’s eyes.</span></p>
</div>
</li>
</ol>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="A7" class="ltx_appendix ltx_centering">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Example Image Exemplars</h2>

<div id="A7.p1" class="ltx_para">
<p id="A7.p1.1" class="ltx_p">In this section we show a selection of image exemplars, for LVIS classes,
found using the process described in Section <a href="#A4" title="Appendix D Sourcing Image Exemplars ‣ Multi-Modal Classifiers for Open-Vocabulary Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>.
We also give the LVIS frequency category — rare, common, frequent.
For cases where the image exemplar comes from a dataset with bounding boxes
(LVIS or VisualGenome) we show the bounding box in yellow.</p>
</div>
<figure id="A7.F6" class="ltx_figure"><img src="/html/2306.05493/assets/assets/img_exem/img_exem_puffin.jpg" id="A7.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="122" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Image Exemplars for “puffin” (rare).</figcaption>
</figure>
<figure id="A7.F7" class="ltx_figure"><img src="/html/2306.05493/assets/assets/img_exem/img_exem_apricot.jpg" id="A7.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="159" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Image Exemplars for “apricot” (rare).</figcaption>
</figure>
<figure id="A7.F8" class="ltx_figure"><img src="/html/2306.05493/assets/assets/img_exem/img_exem_flamingo.jpg" id="A7.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="122" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Image Exemplars for “flamingo” (common).</figcaption>
</figure>
<figure id="A7.F9" class="ltx_figure"><img src="/html/2306.05493/assets/assets/img_exem/img_exem_lantern.jpg" id="A7.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="159" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Image Exemplars for “lantern” (common).</figcaption>
</figure>
<figure id="A7.F10" class="ltx_figure"><img src="/html/2306.05493/assets/assets/img_exem/img_exem_aerosol-can.jpg" id="A7.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="122" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Image Exemplars for “aerosol can” (common).</figcaption>
</figure>
<figure id="A7.F11" class="ltx_figure"><img src="/html/2306.05493/assets/assets/img_exem/img_exem_wineglass.jpg" id="A7.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="159" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Image Exemplars for “wineglass” (frequent).</figcaption>
</figure>
<figure id="A7.F12" class="ltx_figure"><img src="/html/2306.05493/assets/assets/img_exem/img_exem_beanie.jpg" id="A7.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="122" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Image Exemplars for “beanie” (frequent).</figcaption>
</figure>
<figure id="A7.F13" class="ltx_figure"><img src="/html/2306.05493/assets/assets/img_exem/img_exem_fire-engine.jpg" id="A7.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="122" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Image Exemplars for “fire engine” (frequent).</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A8" class="ltx_appendix ltx_centering">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix H </span>More Qualitative Results</h2>

<div id="A8.p1" class="ltx_para">
<p id="A8.p1.1" class="ltx_p">In this section we show more rare category detections on the LVIS OVOD benchmark
using our multi-modal classifier trained with IN-L.</p>
</div>
<figure id="A8.F14" class="ltx_figure"><img src="/html/2306.05493/assets/x6.png" id="A8.F14.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="368" height="503" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Additional qualitative results on LVIS OVOD benchmark.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A9" class="ltx_appendix ltx_centering">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix I </span>Dugong</h2>

<figure id="A9.F15" class="ltx_figure"><img src="/html/2306.05493/assets/assets/dugong.jpg" id="A9.F15.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="200" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>An example of a dugong — a visually distinctive marine species with a less
well known name.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2306.05492" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2306.05493" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2306.05493">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2306.05493" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2306.05494" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 00:59:34 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
