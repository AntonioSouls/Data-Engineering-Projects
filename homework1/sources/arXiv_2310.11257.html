<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2310.11257] An empirical study of automatic wildlife detection using drone thermal imaging and object detection</title><meta property="og:description" content="Artificial intelligence has the potential to make valuable contributions to wildlife management through cost-effective methods for the collection and interpretation of wildlife data. Recent advances in remotely piloted…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="An empirical study of automatic wildlife detection using drone thermal imaging and object detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="An empirical study of automatic wildlife detection using drone thermal imaging and object detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2310.11257">

<!--Generated on Tue Feb 27 23:48:33 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_fleqn">
<h1 class="ltx_title ltx_title_document">An empirical study of automatic wildlife detection using drone thermal imaging and object detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Miao Chang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:changmiao@deakin.edu.au">changmiao@deakin.edu.au</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tan Vuong
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:vuongt@deakin.edu.au">vuongt@deakin.edu.au</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Manas Palaparthi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:m.palaparthi@deakin.edu.au">m.palaparthi@deakin.edu.au</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lachlan Howell
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:l.howell@deakin.edu.au">l.howell@deakin.edu.au</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alessio Bonti
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:a.bonti@deakin.edu.au">a.bonti@deakin.edu.au</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mohamed Abdelrazek
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:mohamed.abdelrazek@deakin.edu.au">mohamed.abdelrazek@deakin.edu.au</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Duc Thanh Nguyen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:duc.nguyen@deakin.edu.au">duc.nguyen@deakin.edu.au</a>
</span>
<span class="ltx_contact ltx_role_address">Deakin University,
<br class="ltx_break">School of Information Technology, Geelong VIC 3220, Australia
</span>
<span class="ltx_contact ltx_role_address">Deakin University,
<br class="ltx_break">School of Life and Environmental Sciences, Geelong VIC 3220, Australia
</span>
<span class="ltx_contact ltx_role_address">Deakin University,
<br class="ltx_break">Applied Artificial Intelligence Institute, Burwood VIC 3125, Australia
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Artificial intelligence has the potential to make valuable contributions to wildlife management through cost-effective methods for the collection and interpretation of wildlife data. Recent advances in remotely piloted aircraft systems (RPAS or “drones”) and thermal imaging technology have created new approaches to collect wildlife data. These emerging technologies could provide promising alternatives to standard labourious field techniques as well as cover much larger areas. In this study, we conduct a comprehensive review and empirical study of drone-based wildlife detection. Specifically, we collect a realistic dataset of drone-derived wildlife thermal detections. Wildlife detections, including arboreal (for instance, koalas, phascolarctos cinereus) and ground dwelling species in our collected data are annotated via bounding boxes by experts. We then benchmark state-of-the-art object detection algorithms on our collected dataset. We use these experimental results to identify issues and discuss future directions in automatic animal monitoring using drones.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
Drone, wildlife detection, object detection, artificial intelligence

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Wildlife, ecosystem and habitat protection require monitoring of wildlife species where regular and reliable population assessments and census trends are necessary <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Conventional ground-based survey techniques are time-consuming and labour-intensive. Demand for automatic surveillance systems continues to grow, supported by emerging technologies that have greatly improved the power, accuracy and efficiency of ecological data collection.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Artificial intelligence (AI) research has been used to develop automated wildlife monitoring systems. For instance, research in audio signal processing and machine learning are used to detect and classify bird calls <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Image recognition techniques are adopted to detect and identify animals from camera traps <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Computer vision methods are applied to recognise aquatic creatures captured by autonomous underwater vehicles <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Recently, drones equipped with cameras and automated detection technologies have been applied to wildlife monitoring <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. Drones offer many advantages including large-scale data collection, non-invasive and real-time monitoring, cloud storage, and fast playback <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. In addition, thermal imaging devices when attached to drones, can provide further aid to detecting cryptic animals and distinguishing species from their surrounding environment in low-lighting conditions (e.g., night time) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">While the increasing volume of wildlife data is useful to ecological studies, it raises a demand on effective methods to process and analyse large-scale datasets. Manual animal counting can be labour-intensive and prone to interpreter bias, resulting in less reliable estimates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. For instance, without automated detection capabilities, aerial imagery is not perceived as a viable option for kangaroo monitoring in Australia due to the intensive effort required to process high-volume data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Automatic counting, on the other hand, can quickly process large numbers of images through AI-based methods, reducing deviations, saving time and labour, and improving accuracy. Recent advances in computing hardware and resources have made AI algorithms feasible in many real-world applications.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">A commonly adopted approach to automate the counting process is to apply object detection algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, examples of AI-based methods, to find animal individuals from recorded aerial imagery. Object detection aims to localise instances of objects of interest, e.g., animals, and specify the semantic class, e.g., species, for each detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. As shown in the literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, object detection has proven promise to enable automatic animal counting solutions, solving issues of laborious processing of large-scale wildlife datasets and aiding wildlife monitoring efforts. In addition, the availability of open-source object detectors has made object detection-based solutions feasible.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Given the rising uptake of drones and thermal imaging in automatic wildlife monitoring, we aim to conduct an empirical study of drone-based wildlife monitoring through object detection. Our work differs from other studies, e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, in several aspects. First, the novelty of our work relies on the empirical aspect of our review where we evaluated existing wildlife monitoring methods empirically, rather than just describing the methods as in other surveys, e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Second, compared with <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, we benchmark more modern object detection techniques. Third, since we evaluate existing methods on the same dataset using the same settings, observations drawn from our experiments would be more conclusive. In summary, we make the following contributions in our work.</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">A concise review of drone-based wildlife monitoring research since 2018 (Section <a href="#S2" title="2 Literature review ‣ An empirical study of automatic wildlife detection using drone thermal imaging and object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>);</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">An annotated dataset of wildlife thermal detections captured using drones in forested environments (Section <a href="#S3" title="3 Dataset ‣ An empirical study of automatic wildlife detection using drone thermal imaging and object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>);</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Extensive experiments and benchmark results of many modern state-of-the-art object detection algorithms in detecting thermal signatures (Section <a href="#S4" title="4 Benchmark of animal detection ‣ An empirical study of automatic wildlife detection using drone thermal imaging and object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Literature review</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Drone types and settings</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Depending on the species-specific or habitat-specific characteristics of the studied species, different settings can be applied to the drone. For instance, to collect koala imagery, Winsen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> set the drone at an altitude of 60 m. This setting was subject to the height of the canopy and the terrain of the site, and therefore was adjusted to maintain a flight altitude about 30 m above the canopy. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, the flight altitude was set at 100 m to detect waterbird species and the drone captured images in a high resolution to minimise the long-distance temperature measurement error. Cox et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> evaluated different drone thermal sensors for detecting rabbit burrows and suggested a minimum export rate of 30 Hz to ensure image quality.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Compared with ground surveys, drone-based surveys can collect wildlife data in severe conditions such as night time, winter, due to thermal sensors use. In several cases, both RGB and thermal information are combined to enhance animal detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.We summarise technical parameters of common drone settings in Table <a href="#S2.T1" title="Table 1 ‣ 2.1 Drone types and settings ‣ 2 Literature review ‣ An empirical study of automatic wildlife detection using drone thermal imaging and object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Summary of drone-based animal monitoring studies since 2018.</figcaption>
<table id="S2.T1.16" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.16.17.1" class="ltx_tr">
<td id="S2.T1.16.17.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T1.16.17.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Study (year)</span></td>
<td id="S2.T1.16.17.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.16.17.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Drone &amp; sensor</span></td>
<td id="S2.T1.16.17.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.16.17.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Image resolution</span></td>
<td id="S2.T1.16.17.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.16.17.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Object detector</span></td>
</tr>
<tr id="S2.T1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.1.2.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib31" title="" class="ltx_ref">31</a><span id="S2.T1.1.1.2.2.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.1.1.2.3" class="ltx_text" style="font-size:90%;"> (2018)</span>
</td>
<td id="S2.T1.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.1.1.3.1" class="ltx_text" style="font-size:90%;">SenseFly eBee</span></td>
<td id="S2.T1.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><math id="S2.T1.1.1.1.m1.1" class="ltx_Math" alttext="4000\times 3000" display="inline"><semantics id="S2.T1.1.1.1.m1.1a"><mrow id="S2.T1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.m1.1.1.cmml"><mn mathsize="90%" id="S2.T1.1.1.1.m1.1.1.2" xref="S2.T1.1.1.1.m1.1.1.2.cmml">4000</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S2.T1.1.1.1.m1.1.1.1" xref="S2.T1.1.1.1.m1.1.1.1.cmml">×</mo><mn mathsize="90%" id="S2.T1.1.1.1.m1.1.1.3" xref="S2.T1.1.1.1.m1.1.1.3.cmml">3000</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.m1.1b"><apply id="S2.T1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.m1.1.1"><times id="S2.T1.1.1.1.m1.1.1.1.cmml" xref="S2.T1.1.1.1.m1.1.1.1"></times><cn type="integer" id="S2.T1.1.1.1.m1.1.1.2.cmml" xref="S2.T1.1.1.1.m1.1.1.2">4000</cn><cn type="integer" id="S2.T1.1.1.1.m1.1.1.3.cmml" xref="S2.T1.1.1.1.m1.1.1.3">3000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.m1.1c">4000\times 3000</annotation></semantics></math></td>
<td id="S2.T1.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.1.1.4.1" class="ltx_text" style="font-size:90%;">CNN</span></td>
</tr>
<tr id="S2.T1.16.18.2" class="ltx_tr">
<td id="S2.T1.16.18.2.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S2.T1.16.18.2.2" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.16.18.2.2.1" class="ltx_text" style="font-size:90%;">Canon PowerShot S110 (RGB)</span></td>
<td id="S2.T1.16.18.2.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.16.18.2.4" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S2.T1.2.2" class="ltx_tr">
<td id="S2.T1.2.2.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.2.2.2.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib19" title="" class="ltx_ref">19</a><span id="S2.T1.2.2.2.2.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.2.2.2.3" class="ltx_text" style="font-size:90%;"> (2019)</span>
</td>
<td id="S2.T1.2.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.2.2.3.1" class="ltx_text" style="font-size:90%;">Matrice 600 Pro</span></td>
<td id="S2.T1.2.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><math id="S2.T1.2.2.1.m1.1" class="ltx_Math" alttext="640\times 512" display="inline"><semantics id="S2.T1.2.2.1.m1.1a"><mrow id="S2.T1.2.2.1.m1.1.1" xref="S2.T1.2.2.1.m1.1.1.cmml"><mn mathsize="90%" id="S2.T1.2.2.1.m1.1.1.2" xref="S2.T1.2.2.1.m1.1.1.2.cmml">640</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S2.T1.2.2.1.m1.1.1.1" xref="S2.T1.2.2.1.m1.1.1.1.cmml">×</mo><mn mathsize="90%" id="S2.T1.2.2.1.m1.1.1.3" xref="S2.T1.2.2.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.1.m1.1b"><apply id="S2.T1.2.2.1.m1.1.1.cmml" xref="S2.T1.2.2.1.m1.1.1"><times id="S2.T1.2.2.1.m1.1.1.1.cmml" xref="S2.T1.2.2.1.m1.1.1.1"></times><cn type="integer" id="S2.T1.2.2.1.m1.1.1.2.cmml" xref="S2.T1.2.2.1.m1.1.1.2">640</cn><cn type="integer" id="S2.T1.2.2.1.m1.1.1.3.cmml" xref="S2.T1.2.2.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.1.m1.1c">640\times 512</annotation></semantics></math></td>
<td id="S2.T1.2.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.2.2.4.1" class="ltx_text" style="font-size:90%;">Faster RCNN,</span></td>
</tr>
<tr id="S2.T1.16.19.3" class="ltx_tr">
<td id="S2.T1.16.19.3.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S2.T1.16.19.3.2" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.16.19.3.2.1" class="ltx_text" style="font-size:90%;">FLIR Tau 2 (thermal)</span></td>
<td id="S2.T1.16.19.3.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.16.19.3.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.16.19.3.4.1" class="ltx_text" style="font-size:90%;">YOLO</span></td>
</tr>
<tr id="S2.T1.3.3" class="ltx_tr">
<td id="S2.T1.3.3.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.3.3.2.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="S2.T1.3.3.2.2.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.3.3.2.3" class="ltx_text" style="font-size:90%;"> (2019)</span>
</td>
<td id="S2.T1.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.3.3.3.1" class="ltx_text" style="font-size:90%;">K-mapper</span></td>
<td id="S2.T1.3.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><math id="S2.T1.3.3.1.m1.1" class="ltx_Math" alttext="6480\times 4320" display="inline"><semantics id="S2.T1.3.3.1.m1.1a"><mrow id="S2.T1.3.3.1.m1.1.1" xref="S2.T1.3.3.1.m1.1.1.cmml"><mn mathsize="90%" id="S2.T1.3.3.1.m1.1.1.2" xref="S2.T1.3.3.1.m1.1.1.2.cmml">6480</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S2.T1.3.3.1.m1.1.1.1" xref="S2.T1.3.3.1.m1.1.1.1.cmml">×</mo><mn mathsize="90%" id="S2.T1.3.3.1.m1.1.1.3" xref="S2.T1.3.3.1.m1.1.1.3.cmml">4320</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.1.m1.1b"><apply id="S2.T1.3.3.1.m1.1.1.cmml" xref="S2.T1.3.3.1.m1.1.1"><times id="S2.T1.3.3.1.m1.1.1.1.cmml" xref="S2.T1.3.3.1.m1.1.1.1"></times><cn type="integer" id="S2.T1.3.3.1.m1.1.1.2.cmml" xref="S2.T1.3.3.1.m1.1.1.2">6480</cn><cn type="integer" id="S2.T1.3.3.1.m1.1.1.3.cmml" xref="S2.T1.3.3.1.m1.1.1.3">4320</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.1.m1.1c">6480\times 4320</annotation></semantics></math></td>
<td id="S2.T1.3.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.3.3.4.1" class="ltx_text" style="font-size:90%;">RetinaNet, SSD,</span></td>
</tr>
<tr id="S2.T1.16.20.4" class="ltx_tr">
<td id="S2.T1.16.20.4.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S2.T1.16.20.4.2" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.16.20.4.2.1" class="ltx_text" style="font-size:90%;">NX-500 (RGB)</span></td>
<td id="S2.T1.16.20.4.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.16.20.4.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.16.20.4.4.1" class="ltx_text" style="font-size:90%;">R-FCN,</span></td>
</tr>
<tr id="S2.T1.16.21.5" class="ltx_tr">
<td id="S2.T1.16.21.5.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S2.T1.16.21.5.2" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.16.21.5.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.16.21.5.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.16.21.5.4.1" class="ltx_text" style="font-size:90%;">Faster RCNN,</span></td>
</tr>
<tr id="S2.T1.16.22.6" class="ltx_tr">
<td id="S2.T1.16.22.6.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S2.T1.16.22.6.2" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.16.22.6.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.16.22.6.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.16.22.6.4.1" class="ltx_text" style="font-size:90%;">YOLO</span></td>
</tr>
<tr id="S2.T1.4.4" class="ltx_tr">
<td id="S2.T1.4.4.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.4.4.2.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S2.T1.4.4.2.2.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.4.4.2.3" class="ltx_text" style="font-size:90%;"> (2021)</span>
</td>
<td id="S2.T1.4.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.4.4.3.1" class="ltx_text" style="font-size:90%;">Matrice 210</span></td>
<td id="S2.T1.4.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<math id="S2.T1.4.4.1.m1.1" class="ltx_Math" alttext="640\times 512" display="inline"><semantics id="S2.T1.4.4.1.m1.1a"><mrow id="S2.T1.4.4.1.m1.1.1" xref="S2.T1.4.4.1.m1.1.1.cmml"><mn mathsize="90%" id="S2.T1.4.4.1.m1.1.1.2" xref="S2.T1.4.4.1.m1.1.1.2.cmml">640</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S2.T1.4.4.1.m1.1.1.1" xref="S2.T1.4.4.1.m1.1.1.1.cmml">×</mo><mn mathsize="90%" id="S2.T1.4.4.1.m1.1.1.3" xref="S2.T1.4.4.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.4.4.1.m1.1b"><apply id="S2.T1.4.4.1.m1.1.1.cmml" xref="S2.T1.4.4.1.m1.1.1"><times id="S2.T1.4.4.1.m1.1.1.1.cmml" xref="S2.T1.4.4.1.m1.1.1.1"></times><cn type="integer" id="S2.T1.4.4.1.m1.1.1.2.cmml" xref="S2.T1.4.4.1.m1.1.1.2">640</cn><cn type="integer" id="S2.T1.4.4.1.m1.1.1.3.cmml" xref="S2.T1.4.4.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.4.1.m1.1c">640\times 512</annotation></semantics></math><span id="S2.T1.4.4.1.1" class="ltx_text" style="font-size:90%;"> (thermal)</span>
</td>
<td id="S2.T1.4.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.4.4.4.1" class="ltx_text" style="font-size:90%;">Sobel detector</span></td>
</tr>
<tr id="S2.T1.5.5" class="ltx_tr">
<td id="S2.T1.5.5.2" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S2.T1.5.5.3" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.5.5.3.1" class="ltx_text" style="font-size:90%;">FLIR Zenmuse XT2 (thermal, RGB)</span></td>
<td id="S2.T1.5.5.1" class="ltx_td ltx_align_left ltx_border_r">
<math id="S2.T1.5.5.1.m1.1" class="ltx_Math" alttext="4000\times 3000" display="inline"><semantics id="S2.T1.5.5.1.m1.1a"><mrow id="S2.T1.5.5.1.m1.1.1" xref="S2.T1.5.5.1.m1.1.1.cmml"><mn mathsize="90%" id="S2.T1.5.5.1.m1.1.1.2" xref="S2.T1.5.5.1.m1.1.1.2.cmml">4000</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S2.T1.5.5.1.m1.1.1.1" xref="S2.T1.5.5.1.m1.1.1.1.cmml">×</mo><mn mathsize="90%" id="S2.T1.5.5.1.m1.1.1.3" xref="S2.T1.5.5.1.m1.1.1.3.cmml">3000</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.5.5.1.m1.1b"><apply id="S2.T1.5.5.1.m1.1.1.cmml" xref="S2.T1.5.5.1.m1.1.1"><times id="S2.T1.5.5.1.m1.1.1.1.cmml" xref="S2.T1.5.5.1.m1.1.1.1"></times><cn type="integer" id="S2.T1.5.5.1.m1.1.1.2.cmml" xref="S2.T1.5.5.1.m1.1.1.2">4000</cn><cn type="integer" id="S2.T1.5.5.1.m1.1.1.3.cmml" xref="S2.T1.5.5.1.m1.1.1.3">3000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.5.5.1.m1.1c">4000\times 3000</annotation></semantics></math><span id="S2.T1.5.5.1.1" class="ltx_text" style="font-size:90%;"> (RGB)</span>
</td>
<td id="S2.T1.5.5.4" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S2.T1.6.6" class="ltx_tr">
<td id="S2.T1.6.6.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.6.6.2.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib21" title="" class="ltx_ref">21</a><span id="S2.T1.6.6.2.2.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.6.6.2.3" class="ltx_text" style="font-size:90%;"> (2021)</span>
</td>
<td id="S2.T1.6.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.6.6.3.1" class="ltx_text" style="font-size:90%;">DJI Inspire-I RPA</span></td>
<td id="S2.T1.6.6.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><math id="S2.T1.6.6.1.m1.1" class="ltx_Math" alttext="640\times 512" display="inline"><semantics id="S2.T1.6.6.1.m1.1a"><mrow id="S2.T1.6.6.1.m1.1.1" xref="S2.T1.6.6.1.m1.1.1.cmml"><mn mathsize="90%" id="S2.T1.6.6.1.m1.1.1.2" xref="S2.T1.6.6.1.m1.1.1.2.cmml">640</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S2.T1.6.6.1.m1.1.1.1" xref="S2.T1.6.6.1.m1.1.1.1.cmml">×</mo><mn mathsize="90%" id="S2.T1.6.6.1.m1.1.1.3" xref="S2.T1.6.6.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.6.6.1.m1.1b"><apply id="S2.T1.6.6.1.m1.1.1.cmml" xref="S2.T1.6.6.1.m1.1.1"><times id="S2.T1.6.6.1.m1.1.1.1.cmml" xref="S2.T1.6.6.1.m1.1.1.1"></times><cn type="integer" id="S2.T1.6.6.1.m1.1.1.2.cmml" xref="S2.T1.6.6.1.m1.1.1.2">640</cn><cn type="integer" id="S2.T1.6.6.1.m1.1.1.3.cmml" xref="S2.T1.6.6.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.6.6.1.m1.1c">640\times 512</annotation></semantics></math></td>
<td id="S2.T1.6.6.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.6.6.4.1" class="ltx_text" style="font-size:90%;">YOLO</span></td>
</tr>
<tr id="S2.T1.16.23.7" class="ltx_tr">
<td id="S2.T1.16.23.7.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S2.T1.16.23.7.2" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.16.23.7.2.1" class="ltx_text" style="font-size:90%;">FLIR Zenmuse (thermal)</span></td>
<td id="S2.T1.16.23.7.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.16.23.7.4" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S2.T1.7.7" class="ltx_tr">
<td id="S2.T1.7.7.2" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S2.T1.7.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.7.7.3.1" class="ltx_text" style="font-size:90%;">DJI Matrice 210 RPA</span></td>
<td id="S2.T1.7.7.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><math id="S2.T1.7.7.1.m1.1" class="ltx_Math" alttext="640\times 512" display="inline"><semantics id="S2.T1.7.7.1.m1.1a"><mrow id="S2.T1.7.7.1.m1.1.1" xref="S2.T1.7.7.1.m1.1.1.cmml"><mn mathsize="90%" id="S2.T1.7.7.1.m1.1.1.2" xref="S2.T1.7.7.1.m1.1.1.2.cmml">640</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S2.T1.7.7.1.m1.1.1.1" xref="S2.T1.7.7.1.m1.1.1.1.cmml">×</mo><mn mathsize="90%" id="S2.T1.7.7.1.m1.1.1.3" xref="S2.T1.7.7.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.7.7.1.m1.1b"><apply id="S2.T1.7.7.1.m1.1.1.cmml" xref="S2.T1.7.7.1.m1.1.1"><times id="S2.T1.7.7.1.m1.1.1.1.cmml" xref="S2.T1.7.7.1.m1.1.1.1"></times><cn type="integer" id="S2.T1.7.7.1.m1.1.1.2.cmml" xref="S2.T1.7.7.1.m1.1.1.2">640</cn><cn type="integer" id="S2.T1.7.7.1.m1.1.1.3.cmml" xref="S2.T1.7.7.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.7.7.1.m1.1c">640\times 512</annotation></semantics></math></td>
<td id="S2.T1.7.7.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.7.7.4.1" class="ltx_text" style="font-size:90%;">YOLO</span></td>
</tr>
<tr id="S2.T1.16.24.8" class="ltx_tr">
<td id="S2.T1.16.24.8.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S2.T1.16.24.8.2" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.16.24.8.2.1" class="ltx_text" style="font-size:90%;">FLIR Zenmuse XT 640 (thermal)</span></td>
<td id="S2.T1.16.24.8.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.16.24.8.4" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S2.T1.8.8" class="ltx_tr">
<td id="S2.T1.8.8.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.8.8.2.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib26" title="" class="ltx_ref">26</a><span id="S2.T1.8.8.2.2.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.8.8.2.3" class="ltx_text" style="font-size:90%;"> (2021)</span>
</td>
<td id="S2.T1.8.8.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.8.8.3.1" class="ltx_text" style="font-size:90%;">DJI Phantom III</span></td>
<td id="S2.T1.8.8.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><math id="S2.T1.8.8.1.m1.1" class="ltx_Math" alttext="4000\times 3000" display="inline"><semantics id="S2.T1.8.8.1.m1.1a"><mrow id="S2.T1.8.8.1.m1.1.1" xref="S2.T1.8.8.1.m1.1.1.cmml"><mn mathsize="90%" id="S2.T1.8.8.1.m1.1.1.2" xref="S2.T1.8.8.1.m1.1.1.2.cmml">4000</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S2.T1.8.8.1.m1.1.1.1" xref="S2.T1.8.8.1.m1.1.1.1.cmml">×</mo><mn mathsize="90%" id="S2.T1.8.8.1.m1.1.1.3" xref="S2.T1.8.8.1.m1.1.1.3.cmml">3000</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.8.8.1.m1.1b"><apply id="S2.T1.8.8.1.m1.1.1.cmml" xref="S2.T1.8.8.1.m1.1.1"><times id="S2.T1.8.8.1.m1.1.1.1.cmml" xref="S2.T1.8.8.1.m1.1.1.1"></times><cn type="integer" id="S2.T1.8.8.1.m1.1.1.2.cmml" xref="S2.T1.8.8.1.m1.1.1.2">4000</cn><cn type="integer" id="S2.T1.8.8.1.m1.1.1.3.cmml" xref="S2.T1.8.8.1.m1.1.1.3">3000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.8.8.1.m1.1c">4000\times 3000</annotation></semantics></math></td>
<td id="S2.T1.8.8.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.8.8.4.1" class="ltx_text" style="font-size:90%;">SSD, YOLO</span></td>
</tr>
<tr id="S2.T1.16.25.9" class="ltx_tr">
<td id="S2.T1.16.25.9.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S2.T1.16.25.9.2" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.16.25.9.2.1" class="ltx_text" style="font-size:90%;">1/2.3” CMOS, effect. pixels: 12.4M (RGB)</span></td>
<td id="S2.T1.16.25.9.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.16.25.9.4" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S2.T1.9.9" class="ltx_tr">
<td id="S2.T1.9.9.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.9.9.2.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib27" title="" class="ltx_ref">27</a><span id="S2.T1.9.9.2.2.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.9.9.2.3" class="ltx_text" style="font-size:90%;"> (2021)</span>
</td>
<td id="S2.T1.9.9.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.9.9.3.1" class="ltx_text" style="font-size:90%;">Matrice 600 Pro</span></td>
<td id="S2.T1.9.9.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><math id="S2.T1.9.9.1.m1.1" class="ltx_Math" alttext="640\times 512" display="inline"><semantics id="S2.T1.9.9.1.m1.1a"><mrow id="S2.T1.9.9.1.m1.1.1" xref="S2.T1.9.9.1.m1.1.1.cmml"><mn mathsize="90%" id="S2.T1.9.9.1.m1.1.1.2" xref="S2.T1.9.9.1.m1.1.1.2.cmml">640</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S2.T1.9.9.1.m1.1.1.1" xref="S2.T1.9.9.1.m1.1.1.1.cmml">×</mo><mn mathsize="90%" id="S2.T1.9.9.1.m1.1.1.3" xref="S2.T1.9.9.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.9.9.1.m1.1b"><apply id="S2.T1.9.9.1.m1.1.1.cmml" xref="S2.T1.9.9.1.m1.1.1"><times id="S2.T1.9.9.1.m1.1.1.1.cmml" xref="S2.T1.9.9.1.m1.1.1.1"></times><cn type="integer" id="S2.T1.9.9.1.m1.1.1.2.cmml" xref="S2.T1.9.9.1.m1.1.1.2">640</cn><cn type="integer" id="S2.T1.9.9.1.m1.1.1.3.cmml" xref="S2.T1.9.9.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.9.9.1.m1.1c">640\times 512</annotation></semantics></math></td>
<td id="S2.T1.9.9.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.9.9.4.1" class="ltx_text" style="font-size:90%;">Faster RCNN,</span></td>
</tr>
<tr id="S2.T1.16.26.10" class="ltx_tr">
<td id="S2.T1.16.26.10.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S2.T1.16.26.10.2" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.16.26.10.2.1" class="ltx_text" style="font-size:90%;">FLIR Tau 2 (thermal)</span></td>
<td id="S2.T1.16.26.10.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.16.26.10.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.16.26.10.4.1" class="ltx_text" style="font-size:90%;">YOLO</span></td>
</tr>
<tr id="S2.T1.10.10" class="ltx_tr">
<td id="S2.T1.10.10.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.10.10.2.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="S2.T1.10.10.2.2.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.10.10.2.3" class="ltx_text" style="font-size:90%;"> (2021)</span>
</td>
<td id="S2.T1.10.10.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.10.10.3.1" class="ltx_text" style="font-size:90%;">DJI Matrice 600 Pro</span></td>
<td id="S2.T1.10.10.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><math id="S2.T1.10.10.1.m1.1" class="ltx_Math" alttext="640\times 512" display="inline"><semantics id="S2.T1.10.10.1.m1.1a"><mrow id="S2.T1.10.10.1.m1.1.1" xref="S2.T1.10.10.1.m1.1.1.cmml"><mn mathsize="90%" id="S2.T1.10.10.1.m1.1.1.2" xref="S2.T1.10.10.1.m1.1.1.2.cmml">640</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S2.T1.10.10.1.m1.1.1.1" xref="S2.T1.10.10.1.m1.1.1.1.cmml">×</mo><mn mathsize="90%" id="S2.T1.10.10.1.m1.1.1.3" xref="S2.T1.10.10.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.10.10.1.m1.1b"><apply id="S2.T1.10.10.1.m1.1.1.cmml" xref="S2.T1.10.10.1.m1.1.1"><times id="S2.T1.10.10.1.m1.1.1.1.cmml" xref="S2.T1.10.10.1.m1.1.1.1"></times><cn type="integer" id="S2.T1.10.10.1.m1.1.1.2.cmml" xref="S2.T1.10.10.1.m1.1.1.2">640</cn><cn type="integer" id="S2.T1.10.10.1.m1.1.1.3.cmml" xref="S2.T1.10.10.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.10.10.1.m1.1c">640\times 512</annotation></semantics></math></td>
<td id="S2.T1.10.10.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.10.10.4.1" class="ltx_text" style="font-size:90%;">Faster RCNN,</span></td>
</tr>
<tr id="S2.T1.16.27.11" class="ltx_tr">
<td id="S2.T1.16.27.11.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S2.T1.16.27.11.2" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.16.27.11.2.1" class="ltx_text" style="font-size:90%;">FLIR Tau 2 (thermal)</span></td>
<td id="S2.T1.16.27.11.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.16.27.11.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.16.27.11.4.1" class="ltx_text" style="font-size:90%;">YOLO</span></td>
</tr>
<tr id="S2.T1.11.11" class="ltx_tr">
<td id="S2.T1.11.11.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.11.11.2.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S2.T1.11.11.2.2.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.11.11.2.3" class="ltx_text" style="font-size:90%;"> (2022)</span>
</td>
<td id="S2.T1.11.11.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.11.11.3.1" class="ltx_text" style="font-size:90%;">Dahua</span></td>
<td id="S2.T1.11.11.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><math id="S2.T1.11.11.1.m1.1" class="ltx_Math" alttext="1024\times 768" display="inline"><semantics id="S2.T1.11.11.1.m1.1a"><mrow id="S2.T1.11.11.1.m1.1.1" xref="S2.T1.11.11.1.m1.1.1.cmml"><mn mathsize="90%" id="S2.T1.11.11.1.m1.1.1.2" xref="S2.T1.11.11.1.m1.1.1.2.cmml">1024</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S2.T1.11.11.1.m1.1.1.1" xref="S2.T1.11.11.1.m1.1.1.1.cmml">×</mo><mn mathsize="90%" id="S2.T1.11.11.1.m1.1.1.3" xref="S2.T1.11.11.1.m1.1.1.3.cmml">768</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.11.11.1.m1.1b"><apply id="S2.T1.11.11.1.m1.1.1.cmml" xref="S2.T1.11.11.1.m1.1.1"><times id="S2.T1.11.11.1.m1.1.1.1.cmml" xref="S2.T1.11.11.1.m1.1.1.1"></times><cn type="integer" id="S2.T1.11.11.1.m1.1.1.2.cmml" xref="S2.T1.11.11.1.m1.1.1.2">1024</cn><cn type="integer" id="S2.T1.11.11.1.m1.1.1.3.cmml" xref="S2.T1.11.11.1.m1.1.1.3">768</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.11.11.1.m1.1c">1024\times 768</annotation></semantics></math></td>
<td id="S2.T1.11.11.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.11.11.4.1" class="ltx_text" style="font-size:90%;">Depth density</span></td>
</tr>
<tr id="S2.T1.16.28.12" class="ltx_tr">
<td id="S2.T1.16.28.12.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S2.T1.16.28.12.2" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.16.28.12.2.1" class="ltx_text" style="font-size:90%;">ONVIF (RGB)</span></td>
<td id="S2.T1.16.28.12.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.16.28.12.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.16.28.12.4.1" class="ltx_text" style="font-size:90%;">network</span></td>
</tr>
<tr id="S2.T1.12.12" class="ltx_tr">
<td id="S2.T1.12.12.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.12.12.2.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="S2.T1.12.12.2.2.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.12.12.2.3" class="ltx_text" style="font-size:90%;"> (2022)</span>
</td>
<td id="S2.T1.12.12.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.12.12.3.1" class="ltx_text" style="font-size:90%;">Leica Aibot A X20</span></td>
<td id="S2.T1.12.12.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<math id="S2.T1.12.12.1.m1.1" class="ltx_Math" alttext="640\times 512" display="inline"><semantics id="S2.T1.12.12.1.m1.1a"><mrow id="S2.T1.12.12.1.m1.1.1" xref="S2.T1.12.12.1.m1.1.1.cmml"><mn mathsize="90%" id="S2.T1.12.12.1.m1.1.1.2" xref="S2.T1.12.12.1.m1.1.1.2.cmml">640</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S2.T1.12.12.1.m1.1.1.1" xref="S2.T1.12.12.1.m1.1.1.1.cmml">×</mo><mn mathsize="90%" id="S2.T1.12.12.1.m1.1.1.3" xref="S2.T1.12.12.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.12.12.1.m1.1b"><apply id="S2.T1.12.12.1.m1.1.1.cmml" xref="S2.T1.12.12.1.m1.1.1"><times id="S2.T1.12.12.1.m1.1.1.1.cmml" xref="S2.T1.12.12.1.m1.1.1.1"></times><cn type="integer" id="S2.T1.12.12.1.m1.1.1.2.cmml" xref="S2.T1.12.12.1.m1.1.1.2">640</cn><cn type="integer" id="S2.T1.12.12.1.m1.1.1.3.cmml" xref="S2.T1.12.12.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.12.12.1.m1.1c">640\times 512</annotation></semantics></math><span id="S2.T1.12.12.1.1" class="ltx_text" style="font-size:90%;"> (thermal)</span>
</td>
<td id="S2.T1.12.12.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.12.12.4.1" class="ltx_text" style="font-size:90%;">Image matching</span></td>
</tr>
<tr id="S2.T1.13.13" class="ltx_tr">
<td id="S2.T1.13.13.2" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S2.T1.13.13.3" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.13.13.3.1" class="ltx_text" style="font-size:90%;">WIRIS Pro (thermal, RGB)</span></td>
<td id="S2.T1.13.13.1" class="ltx_td ltx_align_left ltx_border_r">
<math id="S2.T1.13.13.1.m1.1" class="ltx_Math" alttext="1920\times 1080" display="inline"><semantics id="S2.T1.13.13.1.m1.1a"><mrow id="S2.T1.13.13.1.m1.1.1" xref="S2.T1.13.13.1.m1.1.1.cmml"><mn mathsize="90%" id="S2.T1.13.13.1.m1.1.1.2" xref="S2.T1.13.13.1.m1.1.1.2.cmml">1920</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S2.T1.13.13.1.m1.1.1.1" xref="S2.T1.13.13.1.m1.1.1.1.cmml">×</mo><mn mathsize="90%" id="S2.T1.13.13.1.m1.1.1.3" xref="S2.T1.13.13.1.m1.1.1.3.cmml">1080</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.13.13.1.m1.1b"><apply id="S2.T1.13.13.1.m1.1.1.cmml" xref="S2.T1.13.13.1.m1.1.1"><times id="S2.T1.13.13.1.m1.1.1.1.cmml" xref="S2.T1.13.13.1.m1.1.1.1"></times><cn type="integer" id="S2.T1.13.13.1.m1.1.1.2.cmml" xref="S2.T1.13.13.1.m1.1.1.2">1920</cn><cn type="integer" id="S2.T1.13.13.1.m1.1.1.3.cmml" xref="S2.T1.13.13.1.m1.1.1.3">1080</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.13.13.1.m1.1c">1920\times 1080</annotation></semantics></math><span id="S2.T1.13.13.1.1" class="ltx_text" style="font-size:90%;"> (RGB)</span>
</td>
<td id="S2.T1.13.13.4" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S2.T1.14.14" class="ltx_tr">
<td id="S2.T1.14.14.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.14.14.2.1.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib28" title="" class="ltx_ref">28</a><span id="S2.T1.14.14.2.2.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.T1.14.14.2.3" class="ltx_text" style="font-size:90%;"> (2022)</span>
</td>
<td id="S2.T1.14.14.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.14.14.3.1" class="ltx_text" style="font-size:90%;">DJI Inspire-II</span></td>
<td id="S2.T1.14.14.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><math id="S2.T1.14.14.1.m1.1" class="ltx_Math" alttext="6016\times 4008" display="inline"><semantics id="S2.T1.14.14.1.m1.1a"><mrow id="S2.T1.14.14.1.m1.1.1" xref="S2.T1.14.14.1.m1.1.1.cmml"><mn mathsize="90%" id="S2.T1.14.14.1.m1.1.1.2" xref="S2.T1.14.14.1.m1.1.1.2.cmml">6016</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S2.T1.14.14.1.m1.1.1.1" xref="S2.T1.14.14.1.m1.1.1.1.cmml">×</mo><mn mathsize="90%" id="S2.T1.14.14.1.m1.1.1.3" xref="S2.T1.14.14.1.m1.1.1.3.cmml">4008</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.14.14.1.m1.1b"><apply id="S2.T1.14.14.1.m1.1.1.cmml" xref="S2.T1.14.14.1.m1.1.1"><times id="S2.T1.14.14.1.m1.1.1.1.cmml" xref="S2.T1.14.14.1.m1.1.1.1"></times><cn type="integer" id="S2.T1.14.14.1.m1.1.1.2.cmml" xref="S2.T1.14.14.1.m1.1.1.2">6016</cn><cn type="integer" id="S2.T1.14.14.1.m1.1.1.3.cmml" xref="S2.T1.14.14.1.m1.1.1.3">4008</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.14.14.1.m1.1c">6016\times 4008</annotation></semantics></math></td>
<td id="S2.T1.14.14.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.14.14.4.1" class="ltx_text" style="font-size:90%;">RetinaNet</span></td>
</tr>
<tr id="S2.T1.16.29.13" class="ltx_tr">
<td id="S2.T1.16.29.13.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S2.T1.16.29.13.2" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.16.29.13.2.1" class="ltx_text" style="font-size:90%;">Zenmuse X7 (RGB)</span></td>
<td id="S2.T1.16.29.13.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.16.29.13.4" class="ltx_td ltx_border_r"></td>
</tr>
<tr id="S2.T1.15.15" class="ltx_tr">
<td id="S2.T1.15.15.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T1.15.15.2.1" class="ltx_text" style="font-size:90%;">Ours (2023)</span></td>
<td id="S2.T1.15.15.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.15.15.3.1" class="ltx_text" style="font-size:90%;">Mavic 2 Enterprise Advanced</span></td>
<td id="S2.T1.15.15.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<math id="S2.T1.15.15.1.m1.1" class="ltx_Math" alttext="640\times 512" display="inline"><semantics id="S2.T1.15.15.1.m1.1a"><mrow id="S2.T1.15.15.1.m1.1.1" xref="S2.T1.15.15.1.m1.1.1.cmml"><mn mathsize="90%" id="S2.T1.15.15.1.m1.1.1.2" xref="S2.T1.15.15.1.m1.1.1.2.cmml">640</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S2.T1.15.15.1.m1.1.1.1" xref="S2.T1.15.15.1.m1.1.1.1.cmml">×</mo><mn mathsize="90%" id="S2.T1.15.15.1.m1.1.1.3" xref="S2.T1.15.15.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.15.15.1.m1.1b"><apply id="S2.T1.15.15.1.m1.1.1.cmml" xref="S2.T1.15.15.1.m1.1.1"><times id="S2.T1.15.15.1.m1.1.1.1.cmml" xref="S2.T1.15.15.1.m1.1.1.1"></times><cn type="integer" id="S2.T1.15.15.1.m1.1.1.2.cmml" xref="S2.T1.15.15.1.m1.1.1.2">640</cn><cn type="integer" id="S2.T1.15.15.1.m1.1.1.3.cmml" xref="S2.T1.15.15.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.15.15.1.m1.1c">640\times 512</annotation></semantics></math><span id="S2.T1.15.15.1.1" class="ltx_text" style="font-size:90%;"> (thermal)</span>
</td>
<td id="S2.T1.15.15.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.15.15.4.1" class="ltx_text" style="font-size:90%;">RetinaNet,</span></td>
</tr>
<tr id="S2.T1.16.16" class="ltx_tr">
<td id="S2.T1.16.16.2" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S2.T1.16.16.3" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.16.16.3.1" class="ltx_text" style="font-size:90%;">Uncooled VOx Microbolometer (thermal)</span></td>
<td id="S2.T1.16.16.1" class="ltx_td ltx_align_left ltx_border_r">
<math id="S2.T1.16.16.1.m1.1" class="ltx_Math" alttext="3840\times 2160" display="inline"><semantics id="S2.T1.16.16.1.m1.1a"><mrow id="S2.T1.16.16.1.m1.1.1" xref="S2.T1.16.16.1.m1.1.1.cmml"><mn mathsize="90%" id="S2.T1.16.16.1.m1.1.1.2" xref="S2.T1.16.16.1.m1.1.1.2.cmml">3840</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S2.T1.16.16.1.m1.1.1.1" xref="S2.T1.16.16.1.m1.1.1.1.cmml">×</mo><mn mathsize="90%" id="S2.T1.16.16.1.m1.1.1.3" xref="S2.T1.16.16.1.m1.1.1.3.cmml">2160</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.16.16.1.m1.1b"><apply id="S2.T1.16.16.1.m1.1.1.cmml" xref="S2.T1.16.16.1.m1.1.1"><times id="S2.T1.16.16.1.m1.1.1.1.cmml" xref="S2.T1.16.16.1.m1.1.1.1"></times><cn type="integer" id="S2.T1.16.16.1.m1.1.1.2.cmml" xref="S2.T1.16.16.1.m1.1.1.2">3840</cn><cn type="integer" id="S2.T1.16.16.1.m1.1.1.3.cmml" xref="S2.T1.16.16.1.m1.1.1.3">2160</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.16.16.1.m1.1c">3840\times 2160</annotation></semantics></math><span id="S2.T1.16.16.1.1" class="ltx_text" style="font-size:90%;"> (RGB)</span>
</td>
<td id="S2.T1.16.16.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.16.16.4.1" class="ltx_text" style="font-size:90%;">Faster RCNN,</span></td>
</tr>
<tr id="S2.T1.16.30.14" class="ltx_tr">
<td id="S2.T1.16.30.14.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S2.T1.16.30.14.2" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.16.30.14.2.1" class="ltx_text" style="font-size:90%;">1/2” CMOS, effect. pixels: 48M (RGB)</span></td>
<td id="S2.T1.16.30.14.3" class="ltx_td ltx_border_r"></td>
<td id="S2.T1.16.30.14.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.16.30.14.4.1" class="ltx_text" style="font-size:90%;">YOLO, FCOS,</span></td>
</tr>
<tr id="S2.T1.16.31.15" class="ltx_tr">
<td id="S2.T1.16.31.15.1" class="ltx_td ltx_border_b ltx_border_l ltx_border_r"></td>
<td id="S2.T1.16.31.15.2" class="ltx_td ltx_border_b ltx_border_r"></td>
<td id="S2.T1.16.31.15.3" class="ltx_td ltx_border_b ltx_border_r"></td>
<td id="S2.T1.16.31.15.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r"><span id="S2.T1.16.31.15.4.1" class="ltx_text" style="font-size:90%;">DDETR</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Animal detection</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">After images are collected, object detection algorithms are used to detect animals. Animal detection aims to locate animal species in an input image. The task of animal detection has various challenges. Since drones acquire images from a high altitude, animal signatures become extremely small and many important characteristics (e.g., colour, texture, shape cannot be well captured). Animals also often blend into cluttered backgrounds (e.g., dense vegetation), making them indistinguishable from their surrounding environment.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Commonly used object detectors for animal detection from drone-based imagery include RetinaNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, Faster RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, and YOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. These detectors are network architectures divided into two branches to perform two sub-tasks: bounding box estimation (for object localisation) and bounding box classification (for object identification). When adapting these tools to animal detection, one needs to customise the last layer in the classification branch of the detector with new neurons corresponding to animal classes of interest, and then re-train the modified detector on an animal dataset. Some other older detectors such as SSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> and R-FCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> were used for bird detection in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Several methods create their own model for animal detection. For instance, Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> proposed a depth density estimation network (based on ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>) to generate a heat map of waterbirds density. Kellenberger et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and Barbedo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> divided an input image into a regular grid, on which a CNN was built to estimate the probability of having animals on every grid cell.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">A common practice is to adopt a model pre-trained on large-scale datasets, e.g., ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, then fine-tune it on a target domain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. Since each detector has its own advantages, to maximise the benefit brought by multiple detectors, one can combine different detectors via fusing their detection results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> or through ensemble learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. Corcoran et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> applied temporal information to perform animal tracking to remove false alarms. Ulhaq et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> replaced the upsampling operators in the YOLO’s architecture with dilated convolutions to address feature corruption in the upsampling steps. Several strategies were proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> to handle class imbalance issue in training sets, including using class distribution in a training set to weight the loss function and cropping training images to reduce background bias.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">Traditional image processing techniques also exist for animal detection. Lee et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> applied the Sobel edge detector to extract edges from input images, then verified important edges using thermal information, requiring a lot of user-defined parameters. Mirka et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> detected monkeys by matching moving objects between frames using ORB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> (a handcrafted image descriptor like SIFT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>). We present object detectors used in existing studies in Table <a href="#S2.T1" title="Table 1 ‣ 2.1 Drone types and settings ‣ 2 Literature review ‣ An empirical study of automatic wildlife detection using drone thermal imaging and object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Field method and data collection</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.3" class="ltx_p">We deployed a Mavic 2 Enterprise Advanced equipped with a <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="640\times 512" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mn id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">640</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><times id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">640</cn><cn type="integer" id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">640\times 512</annotation></semantics></math>-pixel thermal camera (<math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mo id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><csymbol cd="latexml" id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\sim</annotation></semantics></math>9 mm focal length), and a <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="3840\times 2160" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mn id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">3840</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.3.m3.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">2160</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><times id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1"></times><cn type="integer" id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">3840</cn><cn type="integer" id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">2160</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">3840\times 2160</annotation></semantics></math>-pixel RGB camera (see Table <a href="#S2.T1" title="Table 1 ‣ 2.1 Drone types and settings ‣ 2 Literature review ‣ An empirical study of automatic wildlife detection using drone thermal imaging and object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Both sensors worked at 30 Hz. The white-hot thermal pallet was used, and the thermal sensor was set to high gain.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.8" class="ltx_p">We focused on developing a real-world dataset of all suspected wildlife thermal signatures detected in a given study area. We surveyed 25 ha plots of native forest in three locations in East Gippsland, Victoria, Australia, including two plots in Gelantipy (37<sup id="S3.SS1.p2.8.1" class="ltx_sup"><span id="S3.SS1.p2.8.1.1" class="ltx_text ltx_font_italic">∘</span></sup>13’31.65”S, 148<sup id="S3.SS1.p2.8.2" class="ltx_sup"><span id="S3.SS1.p2.8.2.1" class="ltx_text ltx_font_italic">∘</span></sup>15’34.34”E and 37<sup id="S3.SS1.p2.8.3" class="ltx_sup"><span id="S3.SS1.p2.8.3.1" class="ltx_text ltx_font_italic">∘</span></sup>16’53.30”S, 148<sup id="S3.SS1.p2.8.4" class="ltx_sup"><span id="S3.SS1.p2.8.4.1" class="ltx_text ltx_font_italic">∘</span></sup>14’39.24”E) and two plots in Orbost (37<sup id="S3.SS1.p2.8.5" class="ltx_sup"><span id="S3.SS1.p2.8.5.1" class="ltx_text ltx_font_italic">∘</span></sup>44’26.88”S, 148<sup id="S3.SS1.p2.8.6" class="ltx_sup"><span id="S3.SS1.p2.8.6.1" class="ltx_text ltx_font_italic">∘</span></sup>12’15.21”E and 37<sup id="S3.SS1.p2.8.7" class="ltx_sup"><span id="S3.SS1.p2.8.7.1" class="ltx_text ltx_font_italic">∘</span></sup>43’59.75”S, 148<sup id="S3.SS1.p2.8.8" class="ltx_sup"><span id="S3.SS1.p2.8.8.1" class="ltx_text ltx_font_italic">∘</span></sup>12’16.41”E) across 17-20 May and 6-8 June 2022 in Gelantipy, and across 20-23 June 2022 in Orbost. Sites in Gelantipy consisted of Montane Grassy Woodland (composed of Broad-leaved peppermint, Eucalyptus dives, and Candlebark, Eucalyptus rubida) and Valley Grassy Forest (composed of River peppermint, Eucalyptus elata, Manna gum, Eucalyptus viminalis, White stringybark, Eucalyptus globoidea, Red box, Eucalyptus polyanthemos). Sites in Orbost consisted of Lowland forest with dominant tree species Silvertop Ash (Eucalyptus sieberi).</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.4" class="ltx_p">Minimum temperatures for the survey period ranged from 0.3–8<sup id="S3.SS1.p3.4.1" class="ltx_sup"><span id="S3.SS1.p3.4.1.1" class="ltx_text ltx_font_italic">∘</span></sup>C, and maximum temperatures ranged from 6.1–18.4<sup id="S3.SS1.p3.4.2" class="ltx_sup"><span id="S3.SS1.p3.4.2.1" class="ltx_text ltx_font_italic">∘</span></sup>C across the survey period. Wind speed was consistently <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><mo id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><lt id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">&lt;</annotation></semantics></math>7.7 m/s. Surveys occurred between <math id="S3.SS1.p3.4.m4.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.SS1.p3.4.m4.1a"><mo id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><csymbol cd="latexml" id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">\sim</annotation></semantics></math>6pm–2am to maximise temperature differential and contrast between animals and the surrounding environment. Flights were programmed using UgCS 4.9.814 and were flown in a lawn mower pattern composed of parallel linear survey lines with 10% overlap. Flight altitude was 50-70 m above ground level depending on site-specific terrain and tree height. Flight speed was 5-6 m/s. Since the data were captured at night, only thermal images were retained and used in our experiments.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">Surveys were conducted under Deakin University’s Animal Ethics Committee Wildlife-Burwood protocol: B08-2022.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data annotation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">After collecting imagery data, we manually extracted images containing suspected thermal detections of wildlife species. We used CVAT<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://cvat.org" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://cvat.org</a></span></span></span>, a free and open-source annotation tool to label bounding boxes of suspected wildlife thermal signatures in the extracted images. During surveys in-field, we were only able to validate a small number of detected thermal signatures (64 signatures) using on-ground observers due to weather and other external factors. Due to a lack of thermal signature validation, we annotated only one class: “suspected wildlife signature”. We note that this limitation does not compromise the aim of this empirical study, benchmarking available object detection models on their detection accuracy and efficiency.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.2" class="ltx_p">Annotated thermal signatures are very small (<math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="32\times 32" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mn id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">32</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><times id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></times><cn type="integer" id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">32</cn><cn type="integer" id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">32\times 32</annotation></semantics></math> pixels relative to the image resolution of <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="640\times 512" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mn id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">640</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><times id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"></times><cn type="integer" id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">640</cn><cn type="integer" id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">640\times 512</annotation></semantics></math> pixels). Annotation resulted in a dataset of 1,210 images with 1,814 annotated thermal signatures. We split the dataset into two subsets: training (850 images with 1,273 thermal signatures) and validation (360 images with 541 thermal signatures). We visualise several annotation results in Figure <a href="#S3.F1" title="Figure 1 ‣ 3.2 Data annotation ‣ 3 Dataset ‣ An empirical study of automatic wildlife detection using drone thermal imaging and object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2310.11257/assets/Annotation.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="216" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of annotated animals (with close-ups).</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Benchmark of animal detection</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Baselines</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We experimented with the following object detection baselines: RetinaNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, Faster RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, YOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, FCOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, and DDETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. These were selected due to their high detection rate and fast computational speed, proven in many applications and on various datasets. RetinaNet, YOLO, and Faster RCNN make use of anchor boxes in finding object bounding boxes. In contrast, FCOS and DDETR are anchor box-free methods. DDETR is built based on Vision Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, a modern network architecture.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Some detectors have different variants. For instance, YOLO has a long history of development with many variants corresponding to different amendments in the architecture (for both feature learning and object location prediction) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. In this paper, we experimented with the versions 4, 5, and 7 of YOLO due to their contemporary as well as proven superiority over other versions in the YOLO family <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. Moreover, YOLOv5 (the fifth version of YOLO) has also shown promise to detect objects in drone imagery <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. For YOLOv4, we evaluated the original version in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> and its variants, e.g., with the use of Cross Stage Partial connections <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> (csp), with different activation functions (mish, leaky), and network sizes (x: large, s: small). For YOLOv5, we experimented with 4 variants corresponding to 4 different scales (s: small, m: medium, x: large, and x: extra-large). For YOLOv7 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, we benchmarked variants developed for different hardware specifications, e.g., edge GPU (YOLOv7-tiny), normal GPU (YOLOv7), cloud GPU (YOLOv7-w6), different sub-structures in the network architecture, e.g., YOLOv7-x, YOLOv7-e6e, YOLOv7-d6, and different backbones, e.g., YOLOv7-PRB, i.e., with the Parallel Residual Bi-Fusion (PRB) Feature Pyramid Network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">For Faster RCNN, we evaluated commonly used backbones including ResNet50, ResNet101 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> and HRNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> (which is designed for high-resolution images). Both the ResNet50 and ResNet101 were also used and evaluated in RetinaNet. These variants have varying numbers of parameters, resulting in different training and inference speed. We summarise the object detectors experimented in our study in Table <a href="#S4.T2" title="Table 2 ‣ 4.1 Baselines ‣ 4 Benchmark of animal detection ‣ An empirical study of automatic wildlife detection using drone thermal imaging and object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Summary of object detection models regarding to Floating Point Operations Per Second - FLOPS (G), number of parameters (M), training time (hours:minutes), inference speed (frames per second - fps).</figcaption>
<table id="S4.T2.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.4.4" class="ltx_tr">
<th id="S4.T2.4.4.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.4.4.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Model</span></th>
<th id="S4.T2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S4.T2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">FLOPS</span><span id="S4.T2.1.1.1.2" class="ltx_text" style="font-size:90%;"> </span><math id="S4.T2.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.1.1.1.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S4.T2.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">No. Param</span><span id="S4.T2.2.2.2.2" class="ltx_text" style="font-size:90%;"> </span><math id="S4.T2.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.2.2.2.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.m1.1b"><ci id="S4.T2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T2.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S4.T2.3.3.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Training time</span><span id="S4.T2.3.3.3.2" class="ltx_text" style="font-size:90%;"> </span><math id="S4.T2.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.3.3.3.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T2.3.3.3.m1.1.1" xref="S4.T2.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.m1.1b"><ci id="S4.T2.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T2.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S4.T2.4.4.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Inference speed</span><span id="S4.T2.4.4.4.2" class="ltx_text" style="font-size:90%;"> </span><math id="S4.T2.4.4.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.4.4.4.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T2.4.4.4.m1.1.1" xref="S4.T2.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.m1.1b"><ci id="S4.T2.4.4.4.m1.1.1.cmml" xref="S4.T2.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.m1.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.4.5.1" class="ltx_tr">
<th id="S4.T2.4.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.4.5.1.1.1" class="ltx_text" style="font-size:90%;">RetinaNet (ResNet50)</span></th>
<td id="S4.T2.4.5.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.5.1.2.1" class="ltx_text" style="font-size:90%;">81.69</span></td>
<td id="S4.T2.4.5.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.5.1.3.1" class="ltx_text" style="font-size:90%;">36.10</span></td>
<td id="S4.T2.4.5.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.5.1.4.1" class="ltx_text" style="font-size:90%;">1h:32m</span></td>
<td id="S4.T2.4.5.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.5.1.5.1" class="ltx_text" style="font-size:90%;">21.20</span></td>
</tr>
<tr id="S4.T2.4.6.2" class="ltx_tr">
<th id="S4.T2.4.6.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.4.6.2.1.1" class="ltx_text" style="font-size:90%;">RetinaNet (ResNet101)</span></th>
<td id="S4.T2.4.6.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.6.2.2.1" class="ltx_text" style="font-size:90%;">110.22</span></td>
<td id="S4.T2.4.6.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.6.2.3.1" class="ltx_text" style="font-size:90%;">54.99</span></td>
<td id="S4.T2.4.6.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.6.2.4.1" class="ltx_text" style="font-size:90%;">1h:55m</span></td>
<td id="S4.T2.4.6.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.6.2.5.1" class="ltx_text" style="font-size:90%;">17.80</span></td>
</tr>
<tr id="S4.T2.4.7.3" class="ltx_tr">
<th id="S4.T2.4.7.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.4.7.3.1.1" class="ltx_text" style="font-size:90%;">Faster RCNN (ResNet50)</span></th>
<td id="S4.T2.4.7.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.7.3.2.1" class="ltx_text" style="font-size:90%;">78.12</span></td>
<td id="S4.T2.4.7.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.7.3.3.1" class="ltx_text" style="font-size:90%;">41.12</span></td>
<td id="S4.T2.4.7.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.7.3.4.1" class="ltx_text" style="font-size:90%;">1h:50m</span></td>
<td id="S4.T2.4.7.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.7.3.5.1" class="ltx_text" style="font-size:90%;">18.70</span></td>
</tr>
<tr id="S4.T2.4.8.4" class="ltx_tr">
<th id="S4.T2.4.8.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.4.8.4.1.1" class="ltx_text" style="font-size:90%;">Faster RCNN (ResNet101)</span></th>
<td id="S4.T2.4.8.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.8.4.2.1" class="ltx_text" style="font-size:90%;">108.55</span></td>
<td id="S4.T2.4.8.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.8.4.3.1" class="ltx_text" style="font-size:90%;">60.11</span></td>
<td id="S4.T2.4.8.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.8.4.4.1" class="ltx_text" style="font-size:90%;">2h:55m</span></td>
<td id="S4.T2.4.8.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.8.4.5.1" class="ltx_text" style="font-size:90%;">24.10</span></td>
</tr>
<tr id="S4.T2.4.9.5" class="ltx_tr">
<th id="S4.T2.4.9.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.4.9.5.1.1" class="ltx_text" style="font-size:90%;">Faster RCNN (HRNet)</span></th>
<td id="S4.T2.4.9.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.9.5.2.1" class="ltx_text" style="font-size:90%;">109.46</span></td>
<td id="S4.T2.4.9.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.9.5.3.1" class="ltx_text" style="font-size:90%;">46.87</span></td>
<td id="S4.T2.4.9.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.9.5.4.1" class="ltx_text" style="font-size:90%;">2h:00m</span></td>
<td id="S4.T2.4.9.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.9.5.5.1" class="ltx_text" style="font-size:90%;">16.90</span></td>
</tr>
<tr id="S4.T2.4.10.6" class="ltx_tr">
<th id="S4.T2.4.10.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.4.10.6.1.1" class="ltx_text" style="font-size:90%;">YOLOv4</span></th>
<td id="S4.T2.4.10.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.10.6.2.1" class="ltx_text" style="font-size:90%;">107.10</span></td>
<td id="S4.T2.4.10.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.10.6.3.1" class="ltx_text" style="font-size:90%;">63.94</span></td>
<td id="S4.T2.4.10.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.10.6.4.1" class="ltx_text" style="font-size:90%;">1h:10m</span></td>
<td id="S4.T2.4.10.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.10.6.5.1" class="ltx_text" style="font-size:90%;">19.90</span></td>
</tr>
<tr id="S4.T2.4.11.7" class="ltx_tr">
<th id="S4.T2.4.11.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.4.11.7.1.1" class="ltx_text" style="font-size:90%;">YOLOv4-csp-s-leaky</span></th>
<td id="S4.T2.4.11.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.11.7.2.1" class="ltx_text" style="font-size:90%;">16.40</span></td>
<td id="S4.T2.4.11.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.11.7.3.1" class="ltx_text" style="font-size:90%;">8.06</span></td>
<td id="S4.T2.4.11.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.11.7.4.1" class="ltx_text" style="font-size:90%;">0h:43m</span></td>
<td id="S4.T2.4.11.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.11.7.5.1" class="ltx_text" style="font-size:90%;">13.20</span></td>
</tr>
<tr id="S4.T2.4.12.8" class="ltx_tr">
<th id="S4.T2.4.12.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.4.12.8.1.1" class="ltx_text" style="font-size:90%;">YOLOv4-csp-leaky</span></th>
<td id="S4.T2.4.12.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.12.8.2.1" class="ltx_text" style="font-size:90%;">107.50</span></td>
<td id="S4.T2.4.12.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.12.8.3.1" class="ltx_text" style="font-size:90%;">52.50</span></td>
<td id="S4.T2.4.12.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.12.8.4.1" class="ltx_text" style="font-size:90%;">0h:53m</span></td>
<td id="S4.T2.4.12.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.12.8.5.1" class="ltx_text" style="font-size:90%;">18.20</span></td>
</tr>
<tr id="S4.T2.4.13.9" class="ltx_tr">
<th id="S4.T2.4.13.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.4.13.9.1.1" class="ltx_text" style="font-size:90%;">YOLOv4-csp-x-leaky</span></th>
<td id="S4.T2.4.13.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.13.9.2.1" class="ltx_text" style="font-size:90%;">184.40</span></td>
<td id="S4.T2.4.13.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.13.9.3.1" class="ltx_text" style="font-size:90%;">99.22</span></td>
<td id="S4.T2.4.13.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.13.9.4.1" class="ltx_text" style="font-size:90%;">1h:20m</span></td>
<td id="S4.T2.4.13.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.13.9.5.1" class="ltx_text" style="font-size:90%;">21.90</span></td>
</tr>
<tr id="S4.T2.4.14.10" class="ltx_tr">
<th id="S4.T2.4.14.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.4.14.10.1.1" class="ltx_text" style="font-size:90%;">YOLOv4-csp-s-mish</span></th>
<td id="S4.T2.4.14.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.14.10.2.1" class="ltx_text" style="font-size:90%;">16.40</span></td>
<td id="S4.T2.4.14.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.14.10.3.1" class="ltx_text" style="font-size:90%;">8.05</span></td>
<td id="S4.T2.4.14.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.14.10.4.1" class="ltx_text" style="font-size:90%;">0h:30m</span></td>
<td id="S4.T2.4.14.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.14.10.5.1" class="ltx_text" style="font-size:90%;">14.20</span></td>
</tr>
<tr id="S4.T2.4.15.11" class="ltx_tr">
<th id="S4.T2.4.15.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.4.15.11.1.1" class="ltx_text" style="font-size:90%;">YOLOv4-csp-mish</span></th>
<td id="S4.T2.4.15.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.15.11.2.1" class="ltx_text" style="font-size:90%;">107.50</span></td>
<td id="S4.T2.4.15.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.15.11.3.1" class="ltx_text" style="font-size:90%;">52.50</span></td>
<td id="S4.T2.4.15.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.15.11.4.1" class="ltx_text" style="font-size:90%;">0h:57m</span></td>
<td id="S4.T2.4.15.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.15.11.5.1" class="ltx_text" style="font-size:90%;">18.40</span></td>
</tr>
<tr id="S4.T2.4.16.12" class="ltx_tr">
<th id="S4.T2.4.16.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.4.16.12.1.1" class="ltx_text" style="font-size:90%;">YOLOv4-csp-x-mish</span></th>
<td id="S4.T2.4.16.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.16.12.2.1" class="ltx_text" style="font-size:90%;">184.40</span></td>
<td id="S4.T2.4.16.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.16.12.3.1" class="ltx_text" style="font-size:90%;">99.22</span></td>
<td id="S4.T2.4.16.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.16.12.4.1" class="ltx_text" style="font-size:90%;">1h:45m</span></td>
<td id="S4.T2.4.16.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.16.12.5.1" class="ltx_text" style="font-size:90%;">21.90</span></td>
</tr>
<tr id="S4.T2.4.17.13" class="ltx_tr">
<th id="S4.T2.4.17.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.4.17.13.1.1" class="ltx_text" style="font-size:90%;">YOLOv5s</span></th>
<td id="S4.T2.4.17.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.17.13.2.1" class="ltx_text" style="font-size:90%;">15.90</span></td>
<td id="S4.T2.4.17.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.17.13.3.1" class="ltx_text" style="font-size:90%;">7.02</span></td>
<td id="S4.T2.4.17.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.17.13.4.1" class="ltx_text" style="font-size:90%;">0h:21m</span></td>
<td id="S4.T2.4.17.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.17.13.5.1" class="ltx_text" style="font-size:90%;">73.40</span></td>
</tr>
<tr id="S4.T2.4.18.14" class="ltx_tr">
<th id="S4.T2.4.18.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.4.18.14.1.1" class="ltx_text" style="font-size:90%;">YOLOv5m</span></th>
<td id="S4.T2.4.18.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.18.14.2.1" class="ltx_text" style="font-size:90%;">48.20</span></td>
<td id="S4.T2.4.18.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.18.14.3.1" class="ltx_text" style="font-size:90%;">20.87</span></td>
<td id="S4.T2.4.18.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.18.14.4.1" class="ltx_text" style="font-size:90%;">0h:28m</span></td>
<td id="S4.T2.4.18.14.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.18.14.5.1" class="ltx_text" style="font-size:90%;">52.42</span></td>
</tr>
<tr id="S4.T2.4.19.15" class="ltx_tr">
<th id="S4.T2.4.19.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.4.19.15.1.1" class="ltx_text" style="font-size:90%;">YOLOv5l</span></th>
<td id="S4.T2.4.19.15.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.19.15.2.1" class="ltx_text" style="font-size:90%;">108.20</span></td>
<td id="S4.T2.4.19.15.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.19.15.3.1" class="ltx_text" style="font-size:90%;">46.13</span></td>
<td id="S4.T2.4.19.15.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.19.15.4.1" class="ltx_text" style="font-size:90%;">0h:45m</span></td>
<td id="S4.T2.4.19.15.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.19.15.5.1" class="ltx_text" style="font-size:90%;">40.78</span></td>
</tr>
<tr id="S4.T2.4.20.16" class="ltx_tr">
<th id="S4.T2.4.20.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.4.20.16.1.1" class="ltx_text" style="font-size:90%;">YOLOv5x</span></th>
<td id="S4.T2.4.20.16.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.20.16.2.1" class="ltx_text" style="font-size:90%;">203.80</span></td>
<td id="S4.T2.4.20.16.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.20.16.3.1" class="ltx_text" style="font-size:90%;">86.17</span></td>
<td id="S4.T2.4.20.16.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.20.16.4.1" class="ltx_text" style="font-size:90%;">1h:22m</span></td>
<td id="S4.T2.4.20.16.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.20.16.5.1" class="ltx_text" style="font-size:90%;">26.21</span></td>
</tr>
<tr id="S4.T2.4.21.17" class="ltx_tr">
<th id="S4.T2.4.21.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.4.21.17.1.1" class="ltx_text" style="font-size:90%;">YOLOv7</span></th>
<td id="S4.T2.4.21.17.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.21.17.2.1" class="ltx_text" style="font-size:90%;">105.10</span></td>
<td id="S4.T2.4.21.17.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.21.17.3.1" class="ltx_text" style="font-size:90%;">37.20</span></td>
<td id="S4.T2.4.21.17.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.21.17.4.1" class="ltx_text" style="font-size:90%;">1h:17m</span></td>
<td id="S4.T2.4.21.17.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.21.17.5.1" class="ltx_text" style="font-size:90%;">11.90</span></td>
</tr>
<tr id="S4.T2.4.22.18" class="ltx_tr">
<th id="S4.T2.4.22.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.4.22.18.1.1" class="ltx_text" style="font-size:90%;">YOLOv7-tiny</span></th>
<td id="S4.T2.4.22.18.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.22.18.2.1" class="ltx_text" style="font-size:90%;">13.20</span></td>
<td id="S4.T2.4.22.18.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.22.18.3.1" class="ltx_text" style="font-size:90%;">6.01</span></td>
<td id="S4.T2.4.22.18.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.22.18.4.1" class="ltx_text" style="font-size:90%;">1h:00m</span></td>
<td id="S4.T2.4.22.18.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.22.18.5.1" class="ltx_text" style="font-size:90%;">3.80</span></td>
</tr>
<tr id="S4.T2.4.23.19" class="ltx_tr">
<th id="S4.T2.4.23.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.4.23.19.1.1" class="ltx_text" style="font-size:90%;">YOLOv7-w6</span></th>
<td id="S4.T2.4.23.19.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.23.19.2.1" class="ltx_text" style="font-size:90%;">101.80</span></td>
<td id="S4.T2.4.23.19.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.23.19.3.1" class="ltx_text" style="font-size:90%;">80.90</span></td>
<td id="S4.T2.4.23.19.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.23.19.4.1" class="ltx_text" style="font-size:90%;">2h:36m</span></td>
<td id="S4.T2.4.23.19.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.23.19.5.1" class="ltx_text" style="font-size:90%;">10.80</span></td>
</tr>
<tr id="S4.T2.4.24.20" class="ltx_tr">
<th id="S4.T2.4.24.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.4.24.20.1.1" class="ltx_text" style="font-size:90%;">YOLOv7-x</span></th>
<td id="S4.T2.4.24.20.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.24.20.2.1" class="ltx_text" style="font-size:90%;">188.90</span></td>
<td id="S4.T2.4.24.20.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.24.20.3.1" class="ltx_text" style="font-size:90%;">70.82</span></td>
<td id="S4.T2.4.24.20.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.24.20.4.1" class="ltx_text" style="font-size:90%;">5h:28m</span></td>
<td id="S4.T2.4.24.20.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.24.20.5.1" class="ltx_text" style="font-size:90%;">13.10</span></td>
</tr>
<tr id="S4.T2.4.25.21" class="ltx_tr">
<th id="S4.T2.4.25.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.4.25.21.1.1" class="ltx_text" style="font-size:90%;">YOLOv7-e6e</span></th>
<td id="S4.T2.4.25.21.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.25.21.2.1" class="ltx_text" style="font-size:90%;">225.40</span></td>
<td id="S4.T2.4.25.21.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.25.21.3.1" class="ltx_text" style="font-size:90%;">164.82</span></td>
<td id="S4.T2.4.25.21.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.25.21.4.1" class="ltx_text" style="font-size:90%;">3h:25m</span></td>
<td id="S4.T2.4.25.21.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.25.21.5.1" class="ltx_text" style="font-size:90%;">20.40</span></td>
</tr>
<tr id="S4.T2.4.26.22" class="ltx_tr">
<th id="S4.T2.4.26.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.4.26.22.1.1" class="ltx_text" style="font-size:90%;">YOLOv7-d6</span></th>
<td id="S4.T2.4.26.22.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.26.22.2.1" class="ltx_text" style="font-size:90%;">198.30</span></td>
<td id="S4.T2.4.26.22.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.26.22.3.1" class="ltx_text" style="font-size:90%;">152.89</span></td>
<td id="S4.T2.4.26.22.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.26.22.4.1" class="ltx_text" style="font-size:90%;">5h:48m</span></td>
<td id="S4.T2.4.26.22.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.26.22.5.1" class="ltx_text" style="font-size:90%;">17.20</span></td>
</tr>
<tr id="S4.T2.4.27.23" class="ltx_tr">
<th id="S4.T2.4.27.23.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.4.27.23.1.1" class="ltx_text" style="font-size:90%;">YOLOv7-PRB</span></th>
<td id="S4.T2.4.27.23.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.27.23.2.1" class="ltx_text" style="font-size:90%;">255.30</span></td>
<td id="S4.T2.4.27.23.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.27.23.3.1" class="ltx_text" style="font-size:90%;">102.15</span></td>
<td id="S4.T2.4.27.23.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.27.23.4.1" class="ltx_text" style="font-size:90%;">1h:35m</span></td>
<td id="S4.T2.4.27.23.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.27.23.5.1" class="ltx_text" style="font-size:90%;">25.20</span></td>
</tr>
<tr id="S4.T2.4.28.24" class="ltx_tr">
<th id="S4.T2.4.28.24.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.4.28.24.1.1" class="ltx_text" style="font-size:90%;">FCOS</span></th>
<td id="S4.T2.4.28.24.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.28.24.2.1" class="ltx_text" style="font-size:90%;">109.06</span></td>
<td id="S4.T2.4.28.24.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.28.24.3.1" class="ltx_text" style="font-size:90%;">50.78</span></td>
<td id="S4.T2.4.28.24.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.28.24.4.1" class="ltx_text" style="font-size:90%;">1h:20m</span></td>
<td id="S4.T2.4.28.24.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.4.28.24.5.1" class="ltx_text" style="font-size:90%;">26.40</span></td>
</tr>
<tr id="S4.T2.4.29.25" class="ltx_tr">
<th id="S4.T2.4.29.25.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.4.29.25.1.1" class="ltx_text" style="font-size:90%;">DDETR</span></th>
<td id="S4.T2.4.29.25.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.4.29.25.2.1" class="ltx_text" style="font-size:90%;">27.40</span></td>
<td id="S4.T2.4.29.25.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.4.29.25.3.1" class="ltx_text" style="font-size:90%;">39.82</span></td>
<td id="S4.T2.4.29.25.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.4.29.25.4.1" class="ltx_text" style="font-size:90%;">2h:58m</span></td>
<td id="S4.T2.4.29.25.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.4.29.25.5.1" class="ltx_text" style="font-size:90%;">16.20</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">We adopted publicly released code repositories including Ultralytics<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/ultralytics/yolov5" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ultralytics/yolov5</a></span></span></span> for YOLOv5, mmdetection<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://github.com/open-mmlab/mmdetection" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/open-mmlab/mmdetection</a></span></span></span> for RetinaNet, Faster RCNN, and FCOS, and the official implementation by the authors of YOLOv4<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://github.com/WongKinYiu/PyTorch_YOLOv4" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/WongKinYiu/PyTorch_YOLOv4</a></span></span></span>, YOLOv7<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/WongKinYiu/yolov7" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/WongKinYiu/yolov7</a></span></span></span>, YOLOv7-PRB<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://github.com/pingyang1117/PRBNet_PyTorch" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/pingyang1117/PRBNet_PyTorch</a></span></span></span>, and DDETR<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="https://github.com/fundamentalvision/Deformable-DETR" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/fundamentalvision/Deformable-DETR</a></span></span></span>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Experiments, results, and discussions</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We implemented all the baselines in Pytorch 1.8.0 with Cuda 10.1. All experiments were conducted on 20 cores of Intel(R) Xeon(R) CPU E5-2630 v4@2.20GHz and 8 NVIDIA GeForce GTX 1080 Ti GPUs. Each model and its variants were trained using the same settings (see Table <a href="#S4.T3" title="Table 3 ‣ 4.2 Experiments, results, and discussions ‣ 4 Benchmark of animal detection ‣ An empirical study of automatic wildlife detection using drone thermal imaging and object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Experimental settings.</figcaption>
<table id="S4.T3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.3.1.1" class="ltx_tr">
<th id="S4.T3.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.3.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Model</span></th>
<th id="S4.T3.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.3.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Learning rate</span></th>
<th id="S4.T3.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.3.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Optimiser</span></th>
<th id="S4.T3.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.3.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Batch size</span></th>
<th id="S4.T3.3.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T3.3.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Epochs</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.3.2.1" class="ltx_tr">
<td id="S4.T3.3.2.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.3.2.1.1.1" class="ltx_text" style="font-size:90%;">RetinaNet</span></td>
<td id="S4.T3.3.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.2.1.2.1" class="ltx_text" style="font-size:90%;">0.001</span></td>
<td id="S4.T3.3.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.2.1.3.1" class="ltx_text" style="font-size:90%;">SGD</span></td>
<td id="S4.T3.3.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.2.1.4.1" class="ltx_text" style="font-size:90%;">16</span></td>
<td id="S4.T3.3.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.2.1.5.1" class="ltx_text" style="font-size:90%;">100</span></td>
</tr>
<tr id="S4.T3.3.3.2" class="ltx_tr">
<td id="S4.T3.3.3.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.3.3.2.1.1" class="ltx_text" style="font-size:90%;">Faster RCNN</span></td>
<td id="S4.T3.3.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.3.2.2.1" class="ltx_text" style="font-size:90%;">0.02</span></td>
<td id="S4.T3.3.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.3.2.3.1" class="ltx_text" style="font-size:90%;">SGD</span></td>
<td id="S4.T3.3.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.3.2.4.1" class="ltx_text" style="font-size:90%;">16</span></td>
<td id="S4.T3.3.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.3.2.5.1" class="ltx_text" style="font-size:90%;">100</span></td>
</tr>
<tr id="S4.T3.3.4.3" class="ltx_tr">
<td id="S4.T3.3.4.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.3.4.3.1.1" class="ltx_text" style="font-size:90%;">YOLOv4</span></td>
<td id="S4.T3.3.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.4.3.2.1" class="ltx_text" style="font-size:90%;">0.01</span></td>
<td id="S4.T3.3.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.4.3.3.1" class="ltx_text" style="font-size:90%;">SGD</span></td>
<td id="S4.T3.3.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.4.3.4.1" class="ltx_text" style="font-size:90%;">8</span></td>
<td id="S4.T3.3.4.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.4.3.5.1" class="ltx_text" style="font-size:90%;">100</span></td>
</tr>
<tr id="S4.T3.3.5.4" class="ltx_tr">
<td id="S4.T3.3.5.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.3.5.4.1.1" class="ltx_text" style="font-size:90%;">YOLOv5</span></td>
<td id="S4.T3.3.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.5.4.2.1" class="ltx_text" style="font-size:90%;">0.01</span></td>
<td id="S4.T3.3.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.5.4.3.1" class="ltx_text" style="font-size:90%;">SGD</span></td>
<td id="S4.T3.3.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.5.4.4.1" class="ltx_text" style="font-size:90%;">32</span></td>
<td id="S4.T3.3.5.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.5.4.5.1" class="ltx_text" style="font-size:90%;">100</span></td>
</tr>
<tr id="S4.T3.3.6.5" class="ltx_tr">
<td id="S4.T3.3.6.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.3.6.5.1.1" class="ltx_text" style="font-size:90%;">YOLOv7</span></td>
<td id="S4.T3.3.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.6.5.2.1" class="ltx_text" style="font-size:90%;">0.01</span></td>
<td id="S4.T3.3.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.6.5.3.1" class="ltx_text" style="font-size:90%;">SGD</span></td>
<td id="S4.T3.3.6.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.6.5.4.1" class="ltx_text" style="font-size:90%;">4</span></td>
<td id="S4.T3.3.6.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.6.5.5.1" class="ltx_text" style="font-size:90%;">100</span></td>
</tr>
<tr id="S4.T3.3.7.6" class="ltx_tr">
<td id="S4.T3.3.7.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.3.7.6.1.1" class="ltx_text" style="font-size:90%;">FCOS</span></td>
<td id="S4.T3.3.7.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.7.6.2.1" class="ltx_text" style="font-size:90%;">0.005</span></td>
<td id="S4.T3.3.7.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.7.6.3.1" class="ltx_text" style="font-size:90%;">SGD</span></td>
<td id="S4.T3.3.7.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.7.6.4.1" class="ltx_text" style="font-size:90%;">16</span></td>
<td id="S4.T3.3.7.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.3.7.6.5.1" class="ltx_text" style="font-size:90%;">100</span></td>
</tr>
<tr id="S4.T3.3.8.7" class="ltx_tr">
<td id="S4.T3.3.8.7.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T3.3.8.7.1.1" class="ltx_text" style="font-size:90%;">DDETR</span></td>
<td id="S4.T3.3.8.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.3.8.7.2.1" class="ltx_text" style="font-size:90%;">0.0002</span></td>
<td id="S4.T3.3.8.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.3.8.7.3.1" class="ltx_text" style="font-size:90%;">Adam</span></td>
<td id="S4.T3.3.8.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.3.8.7.4.1" class="ltx_text" style="font-size:90%;">8</span></td>
<td id="S4.T3.3.8.7.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T3.3.8.7.5.1" class="ltx_text" style="font-size:90%;">100</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2310.11257/assets/x1.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="259" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Detection accuracy of state-of-the-art baselines on our validation set.</figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">We evaluated the detection performance of the baselines using the mean average precision (mAP) metric calculated by varying the detection scores of detected objects. True positives and false alarms were determined via the intersection over union (IOU) of detected objects and ground-truth objects. We calculated the mAP by thresholding the IOU using a threshold of 0.5 (mAP@0.5) and by averaging the mAP with thresholds varying in <math id="S4.SS2.p2.1.m1.2" class="ltx_Math" alttext="[0.5,0.95]" display="inline"><semantics id="S4.SS2.p2.1.m1.2a"><mrow id="S4.SS2.p2.1.m1.2.3.2" xref="S4.SS2.p2.1.m1.2.3.1.cmml"><mo stretchy="false" id="S4.SS2.p2.1.m1.2.3.2.1" xref="S4.SS2.p2.1.m1.2.3.1.cmml">[</mo><mn id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">0.5</mn><mo id="S4.SS2.p2.1.m1.2.3.2.2" xref="S4.SS2.p2.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS2.p2.1.m1.2.2" xref="S4.SS2.p2.1.m1.2.2.cmml">0.95</mn><mo stretchy="false" id="S4.SS2.p2.1.m1.2.3.2.3" xref="S4.SS2.p2.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.2b"><interval closure="closed" id="S4.SS2.p2.1.m1.2.3.1.cmml" xref="S4.SS2.p2.1.m1.2.3.2"><cn type="float" id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">0.5</cn><cn type="float" id="S4.SS2.p2.1.m1.2.2.cmml" xref="S4.SS2.p2.1.m1.2.2">0.95</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.2c">[0.5,0.95]</annotation></semantics></math> (mAP@0.5-0.95).</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">We report the mAP@0.5 and mAP@0.5-0.95 of all the baselines and their variants on our validation set in Figure <a href="#S4.F2" title="Figure 2 ‣ 4.2 Experiments, results, and discussions ‣ 4 Benchmark of animal detection ‣ An empirical study of automatic wildlife detection using drone thermal imaging and object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We also show the detection accuracy (mAP@0.5) vs. the inference speed (fps) of all the experimented baselines in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2 Experiments, results, and discussions ‣ 4 Benchmark of animal detection ‣ An empirical study of automatic wildlife detection using drone thermal imaging and object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. To provide a deeper analysis into the performance of the baselines, we provide their precision-recall (PR) curves in Figure <a href="#S4.F4" title="Figure 4 ‣ 4.2 Experiments, results, and discussions ‣ 4 Benchmark of animal detection ‣ An empirical study of automatic wildlife detection using drone thermal imaging and object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, where relevant baselines are grouped into different graphs.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2310.11257/assets/x2.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="263" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Detection accuracy vs. inference speed of state-of-the-art baselines on our validation set.</figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2310.11257/assets/x3.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="226" height="226" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2310.11257/assets/x4.png" id="S4.F4.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="226" height="226" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2310.11257/assets/x5.png" id="S4.F4.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="226" height="226" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2310.11257/assets/x6.png" id="S4.F4.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="226" height="226" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2310.11257/assets/x7.png" id="S4.F4.g5" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="226" height="226" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2310.11257/assets/x8.png" id="S4.F4.g6" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="226" height="226" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>PR curves of the baselines on our validation set.</figcaption>
</figure>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">As shown in experimental results (Figure <a href="#S4.F2" title="Figure 2 ‣ 4.2 Experiments, results, and discussions ‣ 4 Benchmark of animal detection ‣ An empirical study of automatic wildlife detection using drone thermal imaging and object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2 Experiments, results, and discussions ‣ 4 Benchmark of animal detection ‣ An empirical study of automatic wildlife detection using drone thermal imaging and object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), the YOLO’s family achieves superior performance over other baselines in both detection accuracy and computational efficiency. Its mAP@0.5 and mAP@0.5-0.95 are among the best, where YOLOv5x achieves the highest accuracy (0.78 mAP@0.5, 0.33 mAP@0.5-0.95), and performs reasonably fast (26.21 fps). We also observed that YOLOv5 is the best detector in the YOLO’s family. YOLOv7 slightly outperforms YOLOv4 in terms of the detection accuracy while both the baselines have comparable performance in terms of the inference speed. In general, all variants of the YOLO’s family clearly show the trade-off between model capacity and efficiency (i.e., larger models perform slower but get better accuracy). For instance, the tiny variant of YOLOv7 (YOLOv7-tiny) obtains the highest detection rate (0.55 mAP@0.5 and 0.20 mAP@0.5-0.95) among all the variants of the YOLOv7 baseline but also incurs the slowest speed (3.8 fps). Except for YOLOv7-tiny, both the YOLOv4’s and YOLOv7’s variants perform at around 10 to 25 fps.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">The RetinaNet baseline ranks second overall and its best architecture (with highest detection accuracy) is ResNet101. However, the difference in the detection accuracy between the ResNet50 and ResNet101 backbones is marginal (about 0.02 mAP@0.5 and 0.01 mAP@0.5-0.95). This baseline also shows relatively slow inference speed (21.2 fps and 17.8 fps for the ResNet50 and ResNet101 backbones respectively).</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.2" class="ltx_p">The Faster RCNN baseline seems to take the third place although the difference in its range of detection rates compared with that of the RetinaNet baseline is negligible. In particular, the range of detection rates of the Faster RCNN detector is <math id="S4.SS2.p6.1.m1.2" class="ltx_Math" alttext="[0.572,0.6]" display="inline"><semantics id="S4.SS2.p6.1.m1.2a"><mrow id="S4.SS2.p6.1.m1.2.3.2" xref="S4.SS2.p6.1.m1.2.3.1.cmml"><mo stretchy="false" id="S4.SS2.p6.1.m1.2.3.2.1" xref="S4.SS2.p6.1.m1.2.3.1.cmml">[</mo><mn id="S4.SS2.p6.1.m1.1.1" xref="S4.SS2.p6.1.m1.1.1.cmml">0.572</mn><mo id="S4.SS2.p6.1.m1.2.3.2.2" xref="S4.SS2.p6.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS2.p6.1.m1.2.2" xref="S4.SS2.p6.1.m1.2.2.cmml">0.6</mn><mo stretchy="false" id="S4.SS2.p6.1.m1.2.3.2.3" xref="S4.SS2.p6.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.1.m1.2b"><interval closure="closed" id="S4.SS2.p6.1.m1.2.3.1.cmml" xref="S4.SS2.p6.1.m1.2.3.2"><cn type="float" id="S4.SS2.p6.1.m1.1.1.cmml" xref="S4.SS2.p6.1.m1.1.1">0.572</cn><cn type="float" id="S4.SS2.p6.1.m1.2.2.cmml" xref="S4.SS2.p6.1.m1.2.2">0.6</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.1.m1.2c">[0.572,0.6]</annotation></semantics></math> while it is <math id="S4.SS2.p6.2.m2.2" class="ltx_Math" alttext="[0.587,0.613]" display="inline"><semantics id="S4.SS2.p6.2.m2.2a"><mrow id="S4.SS2.p6.2.m2.2.3.2" xref="S4.SS2.p6.2.m2.2.3.1.cmml"><mo stretchy="false" id="S4.SS2.p6.2.m2.2.3.2.1" xref="S4.SS2.p6.2.m2.2.3.1.cmml">[</mo><mn id="S4.SS2.p6.2.m2.1.1" xref="S4.SS2.p6.2.m2.1.1.cmml">0.587</mn><mo id="S4.SS2.p6.2.m2.2.3.2.2" xref="S4.SS2.p6.2.m2.2.3.1.cmml">,</mo><mn id="S4.SS2.p6.2.m2.2.2" xref="S4.SS2.p6.2.m2.2.2.cmml">0.613</mn><mo stretchy="false" id="S4.SS2.p6.2.m2.2.3.2.3" xref="S4.SS2.p6.2.m2.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.2.m2.2b"><interval closure="closed" id="S4.SS2.p6.2.m2.2.3.1.cmml" xref="S4.SS2.p6.2.m2.2.3.2"><cn type="float" id="S4.SS2.p6.2.m2.1.1.cmml" xref="S4.SS2.p6.2.m2.1.1">0.587</cn><cn type="float" id="S4.SS2.p6.2.m2.2.2.cmml" xref="S4.SS2.p6.2.m2.2.2">0.613</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.2.m2.2c">[0.587,0.613]</annotation></semantics></math> for the RetinaNet detector. Experimental results also show that the combination of the Faster RCNN baseline with the HRNet backbone outperforms other variants made of the ResNet50 and ResNet101 backbones. This is probably due to the design of the HRNet backbone to deal with high-resolution images. Note that, as shown in Table <a href="#S4.T2" title="Table 2 ‣ 4.1 Baselines ‣ 4 Benchmark of animal detection ‣ An empirical study of automatic wildlife detection using drone thermal imaging and object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, despite making use of less complex architecture (with less parameters and thus lower FLOPS), the Faster RCNN with the HRNet backbone takes longer inference time, compared with the RetinaNet with the ResNet101 backbone.</p>
</div>
<div id="S4.SS2.p7" class="ltx_para">
<p id="S4.SS2.p7.1" class="ltx_p">The two modern detectors, FCOS and DDETR, do not show advantages in our study. In addition, since those methods are anchor-free, they take time to learn bounding box locations but perform at the same speed (e.g., Faster RCNN) or slower than anchor-based baselines (e.g., YOLOv5).</p>
</div>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2310.11257/assets/x9.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="226" height="226" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2310.11257/assets/x10.png" id="S4.F5.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="226" height="226" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2310.11257/assets/x11.png" id="S4.F5.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="226" height="226" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2310.11257/assets/x12.png" id="S4.F5.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="226" height="226" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2310.11257/assets/x13.png" id="S4.F5.g5" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="226" height="226" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2310.11257/assets/x14.png" id="S4.F5.g6" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="226" height="226" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Learning curves of the baselines on our training set.</figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2310.11257/assets/03.jpg" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="293" height="235" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2310.11257/assets/06.jpg" id="S4.F6.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="293" height="235" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2310.11257/assets/D-2022-08-01-15h52m12s769_jpg.rf.ff3cc87a51fe588edc7e8b31e5c98c64.jpg" id="S4.F6.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="293" height="235" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2310.11257/assets/detections-2022-08-03-12h51m22s201_png.rf.53fddc2b0ace33a1d5d96c1e9aff5308.jpg" id="S4.F6.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="293" height="235" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2310.11257/assets/detections-2022-08-03-13h05m34s069_png.rf.720a6eccfa3bec57048de2e470878f07.jpg" id="S4.F6.g5" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="293" height="235" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2310.11257/assets/detections-2022-08-08-22h35m31s845_png.rf.91b44cb5823c473e40417b0f7fcdcb2e.jpg" id="S4.F6.g6" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="293" height="235" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2310.11257/assets/detections-2022-08-08-22h42m54s262_png.rf.9010b72d3a3468ac6938393b3d6419e0.jpg" id="S4.F6.g7" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="293" height="235" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2310.11257/assets/detections-2022-08-14-08h58m03s988_png.rf.7b2ea4061418dcd6d2018d3e94f8c219.jpg" id="S4.F6.g8" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="293" height="235" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Several detection results of YOLOv5x. Detected objects and annotated objects are presented in green and red.</figcaption>
</figure>
<div id="S4.SS2.p8" class="ltx_para">
<p id="S4.SS2.p8.1" class="ltx_p">To further investigate the learning process of the baselines, we plot the learning curves of all the baselines in Figure <a href="#S4.F5" title="Figure 5 ‣ 4.2 Experiments, results, and discussions ‣ 4 Benchmark of animal detection ‣ An empirical study of automatic wildlife detection using drone thermal imaging and object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. We observed that all the baselines perform consistently on both the training and validation set. First, all the baselines show the convergence of their losses during the training, proving their learning ability. Second, the ranking of performance in both training and testing of all the baselines remains consistent. Specifically, as shown in Figure <a href="#S4.F5" title="Figure 5 ‣ 4.2 Experiments, results, and discussions ‣ 4 Benchmark of animal detection ‣ An empirical study of automatic wildlife detection using drone thermal imaging and object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, YOLOv5 still appears as the best detector during training. It keeps improving during the training (the learning curves slide down continuously) and achieves the best error rate among all the baselines (around 0.025, shown in the vertical axis). In contrast, except for DDETR, other baselines quickly reach their stationary points. For instance, the learning curves of all the variants of Faster RCNN saturate at early epochs (around 10 epochs) with an error rate of 0.2. Likewise, RetinaNet follows this learning trend at early epochs, but then slightly improves. The learning of YOLOv4 and YOLOv7 is less stable, compared with the other baselines, evident by strong fluctuations in their loss curves. Both FCOS and DDETR, despite converging in the learning process, present high error rates. Specifically, the lowest error rates of both FCOS and DDETR are at several magnitude of those of the other baselines. DDETR incurs the highest error rate among all the baselines.</p>
</div>
<div id="S4.SS2.p9" class="ltx_para">
<p id="S4.SS2.p9.1" class="ltx_p">To showcase detection results, we choose YOLOv5x as it is the best detector and illustrate several results of this variant in Figure <a href="#S4.F6" title="Figure 6 ‣ 4.2 Experiments, results, and discussions ‣ 4 Benchmark of animal detection ‣ An empirical study of automatic wildlife detection using drone thermal imaging and object detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. In general, existing object detectors show their capability of detecting wildlife thermal signatures in realistic conditions. It remains challenging to identify animals presented in small size and in cluttered backgrounds. There requires a more effective design to upsample features in deep architectures to learn features from small-sized objects. We observed that, once detected wildlife species moved, they could be more easily detected by human annotators. This suggests the use of temporal information in improving animal detection.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This paper presents an empirical review of drone-based wildlife monitoring research since 2018. We provide a concise overview of existing methods regarding hardware specifications and settings, and object detection methods used in animal detection. To benchmark animal detection methods, we collected and annotated a real-world dataset of wildlife thermal signatures in a forested environment, and evaluated state-of-the-art object detectors on our dataset. Experimental results show that YOLOv5 significantly outperforms other baselines in both detection accuracy and computational speed. The benchmark results also suggest potential research directions including small-sized object detection and using temporal information in detecting animals from drone-derived imagery.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgments</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We acknowledge funding from CSIRO under the National Koala Monitoring Program and funding from the Victorian Government’s Department of Environment, Land, Water and Planning to conduct the surveys described in this study. We would also like to thank Desley Whisson for assistance selecting survey sites. We also thank Shelby Ryan for providing flight plans for the surveys described in this study. We also acknowledge Lachlan Clarke and Simon Ruff from the Victorian Government’s Department of Environment, Land, Water and Planning for facilitating access to our survey sites. We also acknowledge Blake Allan for assistance reviewing and facilitating these drone operations as Deakin University’s Chief Remote Pilot.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
S. Legge, D. B. Lindenmayer, N. M. Robinson, B. C. Scheele, D. M. Southwell,
B. C. Wintle (Eds.),
<a target="_blank" href="https://ebooks.publish.csiro.au/content/9781486307722/9781486307722" title="" class="ltx_ref ltx_href">Monitoring
Threatened Species and Ecological Communities</a>, CSIRO Publishing, 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1071/9781486307722" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1071/9781486307722</span></a>.

<br class="ltx_break">URL <a target="_blank" href="https://ebooks.publish.csiro.au/content/9781486307722/9781486307722" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ebooks.publish.csiro.au/content/9781486307722/9781486307722</a>

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
L. Neal, F. Briggs, R. Raich, X. Z. Fern, Time-frequency segmentation of bird
song in noisy acoustic environments, in: IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), 2011, pp. 2012–2015.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
D. T. Nguyen, P. O. Ogunbona, W. Li, E. Tasker, J. Yearwood, Detection of
ground parrot vocalisation: A multiple instance learning approach, Journal of
the Acoustical Society of America 142 (3) (2017) 1281–1290.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Y. Maegawa, Y. Ushigome, M. Suzuki, K. Taguchi, K. Kobayashi, C. Haga,
T. Matsui, A new survey method using convolutional neural networks for
automatic classification of bird calls, Ecological Informatics 61 (2021)
101164.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
T. Denton, S. Wisdom, J. R. Hershey, Improving bird classification with
unsupervised sound separation, in: IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 636–640.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
X. Yu, J. Wang, R. Kays, P. A. Jansen, T. Wang, T. S. Huang, Automated
identification of animal species in camera trap images, EURASIP Journal of
Image and Video Processing 2013 (2013) 52.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Z. Zhang, Z. He, G. Cao, W. Cao, Animal detection from highly cluttered natural
scenes using spatiotemporal object region proposals and patch verification,
IEEE Transactions on Multimedia 18 (10) (2016) 2079–2092.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
R. Chandrakar, R. Raja, R. Miri, Animal detection based on deep convolutional
neural networks with genetic segmentation, Multimedia Tools and Applications
81 (2022) 42149–42162.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Z. Chen, H. Gao, Z. Zhang, H. Zhou, X. Wang, Y. Tia, Underwater salient object
detection by combining 2D and 3D visual features, Neurocomputing 391
(2020) 249–259.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
N. E. Merencilla, A. S. Alon, G. J. O. Fernando, E. M. Cepe, D. C. Malunao,
Shark-EYE: A deep inference convolutional neural network of shark detection
for underwater diving surveillance, in: International Conference on
Computational Intelligence and Knowledge Economy (ICCIKE), 2021, pp.
384–388.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
X. Wei, L. Yu, S. Tian, P. Feng, X. Ning, Underwater target detection with an
attention mechanism and improved scale, Multimedia Tools and Applications
80 (25) (2021) 33747–33761.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Y. Yang, H.-G. Yeh, W. Zhang, C. J. Lee, E. N. Meese, C. G. Lowe, Feature
extraction, selection, and K-nearest neighbors algorithm for shark behavior
classification based on imbalanced dataset, IEEE Sensors Journal 21 (5)
(2021) 6429–6439.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
C. Yeh, C. Lin, L. Kang, C. Huang, M. Lin, C. Chang, C. Wang, Lightweight deep
neural network for joint learning of underwater object detection and color
conversion, IEEE Transactions on Neural Networks Learning Systems 33 (11)
(2022) 6129–6143.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
E. Bennitt, H. L. A, Bartlam-Brooks, T. Y. Hubel, A. M. Wilson, Terrestrial
mammalian wildlife responses to unmanned aerial systems approaches,
Scientific Reports 9 (2142) (2019) 1–10.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
E. Corcoran, M. Winsen, A. Sudholz, G. Hamilton, Automated detection of
wildlife using drones: Synthesis, opportunities and constraints, Methods in
Ecology and Evolution 12 (6) (2021) 1103–1114.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
E. Wu, H. Wang, H. Lu, W. Zhu, Y. Jia, L. Wen, C. Choi, H. Guo, B. Li, L. Sun,
G. Lei, J. Lei, H. Jian, Unlocking the potential of deep learning for
migratory waterbirds monitoring using surveillance video, Remote Sensing
14 (3) (2022) 514.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
B. Mirka, D. A. Stow, G. Paulus, A. C. Loerch, L. L. Coulter, L. An, R. L.
Lewison, L. S. Pflüger, Evaluation of thermal infrared imaging from
uninhabited aerial vehicles for arboreal wildlife surveillance, Environmental
Monitoring and Assessment 194 (512) (2022) 1–15.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
S. Lee, Y. Song, S. Kil, Feasibility analyses of real-time detection of
wildlife using uav-derived thermal and RGB images, Remote Sensing 13 (11)
(2021) 2169.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
E. Corcoran, S. Denman, J. Hanger, B. Wilson, G. Hamilton, Automated detection
of koalas using low-level aerial surveillance and machine learning,
Scientific Reports 9 (3208) (2019) 1–9.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
S. Hong, Y. Han, S. Kim, A. Lee, G. Kim, Application of deep-learning methods
to bird detection using unmanned aerial vehicle imagery, Sensors 19 (7)
(2019) 1651.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
A. Ulhaq, P. Adams, T. E. Cox, A. Khan, T. Low, M. Paul, Automated detection of
animals in low-resolution airborne thermal imagery, Remote Sensing 13 (16)
(2021) 3276.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
T. Lin, P. Goyal, R. B. Girshick, K. He, P. Dollár, Focal loss for dense
object detection, in: IEEE/CVF International Conference on Computer Vision
(ICCV), 2017, pp. 2999–3007.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
S. Ren, K. He, R. B. Girshick, J. Sun, Faster R-CNN: towards real-time object
detection with region proposal networks, IEEE Transactions on Pattern
Analysis and Machine Intelligence 39 (6) (2017) 1137–1149.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
J. Redmon, A. Farhadi, YOLO9000: better, faster, stronger, in: IEEE/CVF
Computer Vision and Pattern Recognition (CVPR), 2017, pp. 6517–6525.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Z.-Q. Zhao, P. Zheng, S. tao Xu, X. Wu, Object detection with deep learning: A
review, IEEE Transactions on Neural Networks and Learning Systems 30 (11)
(2019) 3212–3232.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
T. Petso, R. S. J. Jr., D. Mpoeleng, E. Bennitt, W. Mmereki, Automatic animal
identification from drone camera based on point pattern analysis of herd
behaviour, Ecological Informatics 66 (2021) 101485.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
M. Winsen, S. Denman, E. Corcoran, G. Hamilton, Automated detection of koalas
with deep learning ensembles, Remote Sensing 14 (10) (2022) 2432.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
B. Weinstein, L. Garner, V. Saccomanno, A. Steinkraus, A. Ortega, K. Brush,
G. Yenni, A. McKellar, R. Converse, C. Lippitt, A. Wegmann, N. Holmes,
A. Edney, T. Hart, M. Jessopp, R. Clarke, D. Marchowski, H. Senyondo,
R. Dotson, E. White, P. Frederick, S. Ernest, A general deep learning model
for bird detection in high-resolution airborne imagery, Ecological
Applications 32 (8) (2022) e2694.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
T. Petso, R. S. J. Jr., D. Mpoeleng, Review on methods used for wildlife
species and individual identification, European Journal of Wildlife Research
68 (3) (2022) 1–18.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
T. E. Cox, R. Matthews, G. Halverson, S. Morris, Hot stuff in the bushes:
Thermal imagers and the detection of burrows in vegetated sites, Ecology and
Evolution 11 (2021) 6406–6414.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
B. Kellenberger, D. Marcos, D. Tuia, Detecting mammals in UAV images: Best
practices to address a substantially imbalanced dataset with deep learning,
Remote Sensing of Environment 216 (2018) 139–153.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
A. Sudholz, S. Denman, A. Pople, M. Brennan, M. Amos, G. Hamilton, A comparison
of manual and automated detection of rusa deer (Rusa timorensis) from
RPAS-derived thermal imagery, Wildlife Research 49 (1) (2021) 46–53.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, A. C. Berg,
SSD: Single shot multibox detector, in: European Conference on Computer
Vision (ECCV), 2016, pp. 21–37.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
J. Dai, Y. Li, K. He, J. Sun, R-FCN: object detection via region-based fully
convolutional networks, in: Conference on Neural Information Processing
Systems (NIPS), 2016, pp. 379–387.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition,
in: IEEE/CVF Computer Vision and Pattern Recognition (CVPR), 2016, pp.
770–778.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
J. G. A. Barbedo, L. V. Koenigkan, P. M. Santos, A. R. B. Ribeiro, Counting
cattle in UAV images—dealing with clustered animals and animal/background
contrast changes, Sensors 20 (7) (2020).

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
J. Deng, W. Dong, R. Socher, L. Li, K. Li, L. Fei-Fei, Imagenet: A
large-scale hierarchical image database, in: IEEE/CVF Computer Vision and
Pattern Recognition (CVPR), 2009, pp. 248–255.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
T. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Dollár, C. L. Zitnick, Microsoft COCO: common objects in context,
in: European Conference on Computer Vision (ECCV), 2014, pp. 740–755.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
E. Rublee, V. Rabaud, K. Konolige, G. R. Bradski, ORB: an efficient
alternative to SIFT or SURF, in: IEEE/CVF International Conference on
Computer Vision (ICCV), 2011, pp. 2564–2571.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
D. G. Lowe, Distinctive image features from scale-invariant keypoints,
International Journal of Computer Vision 60 (2) (2004) 91–110.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Z. Tian, C. Shen, H. Chen, T. He, FCOS: fully convolutional one-stage object
detection, in: IEEE/CVF International Conference on Computer Vision
(ICCV), 2019, pp. 9626–9635.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
X. Zhu, W. Su, L. Lu, B. Li, X. Wang, J. Dai, Deformable DETR: deformable
transformers for end-to-end object detection, in: International Conference on
Learning Representation (ICLR), 2021, pp. 1–16.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit,
N. Houlsby, An image is worth 16x16 words: Transformers for image recognition
at scale, in: International Conference on Learning Representation (ICLR),
2021.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
P. Jiang, D. Ergu, F. Liu, Y. Cai, B. Ma, A review of YOLO algorithm
developments, Procedia Computer Science 199 (2022) 1066–1073.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
U. Nepal, H. Eslamiat, Comparing YOLOv3, YOLOv4 and YOLOv5 for autonomous
landing spot detection in faulty UAVs, Sensors 22 (464) (2022) 1–15.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
C. Wang, A. Bochkovskiy, H. M. Liao, YOLOv7: Trainable bag-of-freebies sets
new state-of-the-art for real-time object detectors, CoRR abs/2207.02696
(2022).

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
R. Baidya, H. Jeong, YOLOv5 with convmixer prediction heads for precise
object detection in drone imagery, Sensors 22 (21) (2022) 1–16.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
A. Bochkovskiy, C. Wang, H. M. Liao, YOLOv4: Optimal speed and accuracy of
object detection, CoRR abs/2004.10934 (2020).

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
C. Wang, A. Bochkovskiy, H. M. Liao, Scaled-YOLOv4: Scaling cross stage
partial network, in: IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), 2021, pp. 13029–13038.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
C.-Y. Wang, H.-Y. Mark Liao, Y.-H. Wu, P.-Y. Chen, J.-W. Hsieh, I.-H. Yeh,
CSPNet: A new backbone that can enhance learning capability of CNN, in:
IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops
(CVPRW), 2020, pp. 390–391.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
P. Chen, M. Chang, J. Hsieh, Y. Chen, Parallel residual bi-fusion feature
pyramid network for accurate single-shot object detection, IEEE
Transactions on Image Processing 30 (2021) 9099–9111.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
K. Sun, B. Xiao, D. Liu, J. Wang, Deep high-resolution representation learning
for human pose estimation, in: IEEE/CVF Computer Vision and Pattern
Recognition (CVPR), 2019, pp. 5686–5696.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2310.11256" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2310.11257" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2310.11257">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2310.11257" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2310.11258" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 23:48:33 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
