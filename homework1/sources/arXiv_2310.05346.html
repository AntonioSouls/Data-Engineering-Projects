<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2310.05346] Anyview: Generalizable Indoor 3D Object Detection with Variable Frames</title><meta property="og:description" content="In this paper, we propose a novel network framework for indoor 3D object detection to handle variable input frame numbers in practical scenarios.
Existing methods only consider fixed frames of input data for a single d…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Anyview: Generalizable Indoor 3D Object Detection with Variable Frames">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Anyview: Generalizable Indoor 3D Object Detection with Variable Frames">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2310.05346">

<!--Generated on Wed Feb 28 01:50:36 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
3D object detection,  generalizability.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Anyview: Generalizable Indoor 3D Object Detection with Variable Frames</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhenyu Wu<sup id="id11.11.id1" class="ltx_sup"><span id="id11.11.id1.1" class="ltx_text ltx_font_italic">1</span></sup>, Xiuwei Xu<sup id="id12.12.id2" class="ltx_sup"><span id="id12.12.id2.1" class="ltx_text ltx_font_italic">2</span></sup>, Ziwei Wang<sup id="id13.13.id3" class="ltx_sup"><span id="id13.13.id3.1" class="ltx_text ltx_font_italic">2</span></sup>, Chong Xia<sup id="id14.14.id4" class="ltx_sup"><span id="id14.14.id4.1" class="ltx_text ltx_font_italic">2</span></sup>, Linqing Zhao<sup id="id15.15.id5" class="ltx_sup"><span id="id15.15.id5.1" class="ltx_text ltx_font_italic">3</span></sup>, Jiwen Lu<sup id="id16.16.id6" class="ltx_sup"><span id="id16.16.id6.1" class="ltx_text ltx_font_italic">2</span></sup> and Haibin Yan<sup id="id17.17.id7" class="ltx_sup"><span id="id17.17.id7.1" class="ltx_text ltx_font_italic">∗1</span></sup>
</span><span class="ltx_author_notes">*Corresponding author.<sup id="id18.18.id1" class="ltx_sup"><span id="id18.18.id1.1" class="ltx_text ltx_font_italic">1</span></sup>Zhenyu Wu and Haibin Yan are with the School of Automation, Beijing University of Posts and Telecommunications, Beijing, 100876, China. <span id="id19.19.id2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{wuzhenyu, eyanhaibin}@bupt.edu.cn.</span><sup id="id20.20.id1" class="ltx_sup"><span id="id20.20.id1.1" class="ltx_text ltx_font_italic">2</span></sup>Xiuwei Xu, Ziwei Wang, Chong Xia, and Jiwen Lu are with the Department of Automation, Tsinghua University, and Beijing National Research Center for Information Science and Technology (BNRist), Beijing, 100084, China. <span id="id21.21.id2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{xxw21, wang-zw18, xiac20}@mails.tsinghua.edu.cn.</span><span id="id22.22.id3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">lujiwen@tsinghua.edu.cn.</span><sup id="id23.23.id1" class="ltx_sup"><span id="id23.23.id1.1" class="ltx_text ltx_font_italic">3</span></sup>Linqing Zhao is with the School of Electrical and Information Engineering, Tianjin University, Tianjin, 300072, China. <span id="id24.24.id2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{linqingzhao}@tju.edu.cn</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id25.id1" class="ltx_p">In this paper, we propose a novel network framework for indoor 3D object detection to handle variable input frame numbers in practical scenarios.
Existing methods only consider fixed frames of input data for a single detector, such as monocular RGB-D images or point clouds reconstructed from dense multi-view RGB-D images.
While in practical application scenes such as robot navigation and manipulation, the raw input to the 3D detectors is the RGB-D images with variable frame numbers instead of the reconstructed scene point cloud.
However, the previous approaches can only handle fixed frame input data and have poor performance with variable frame input.
In order to facilitate 3D object detection methods suitable for practical tasks,
we present a novel 3D detection framework named AnyView for our practical applications, which generalizes well across different numbers of input frames with a single model.
To be specific, we propose a geometric learner to mine the local geometric features of each input RGB-D image frame and implement local-global feature interaction through a designed spatial mixture module.
Meanwhile, we further utilize a dynamic token strategy to adaptively adjust the number of extracted features for each frame, which ensures consistent global feature density and further enhances the generalization after fusion.
Extensive experiments on the ScanNet dataset show our method achieves both great generalizability and high detection accuracy with a simple and clean architecture containing a similar amount of parameters with the baselines.

</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
3D object detection, generalizability.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">3D object detection is a fundamental scene understanding problem for robotic manipulation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>, autonomous driving <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, and AR/VR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, which aims to detect the 3D bounding boxes and semantic labels from point clouds or images.
Due to the different types of sensors used in different application scenarios, 3D object detection methods usually vary a lot for indoor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and outdoor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> scenes. We focus on indoor 3D object detection, where the mainstream sensor is RGB-D cameras and the scenes are crowded with objects of multiple categories and sizes.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2310.05346/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="431" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>(a) Previous 3D object detection methods rely on reconstructed point clouds from densely sampled RGB-D images, which requires a large amount of time to process the data and generalizes poorly to scenes in different scales. (b) Our method directly detects objects from sparse RGB-D images of any number of views, which shows good generalizability and provides more trade-off.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Although great improvement in performance has been achieved by advanced architecture design, existing methods train and evaluate 3D detectors on only fixed frames of input data, such as monocular RGB-D images (on SUN-RGBD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> benchmark) and scene-level reconstructed point clouds (sampled from meshes) from multi-view RGB-D images (on ScanNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> benchmark).
There is still a huge gap between these benchmarks and practical applications.
Specifically, due to the various task budgets in practical application scenes, the number of frames input to the 3D detectors is dynamic, and previous frameworks trained on fixed frame inputs are challenging to generalize.
The cost of training diverse models for tasks with various sizes of inputs is huge, severely limiting the deployment of the models on edge devices.
In order to solve this problem, it is necessary to further research and develop more efficient generalized indoor 3D object detection with variable frames.
For example, in online tasks such as robot navigation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>,
the information captured by the agent is strictly limited by a cost budget constraint, which leads to a varying number of input frames.
As we will show in Section <a href="#S3.SS1" title="III-A Problem Statement ‣ III Analysis ‣ Anyview: Generalizable Indoor 3D Object Detection with Variable Frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>, previous models generalize poorly across different input scales, which brings a huge burden as we need to prepare a series of models trained on different scales of input data to cope with various scenarios.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In practical scenes, the raw input to the indoor 3D object detectors is the RGB-D images with variable frame numbers.
The detectors require the ability to process inputs with various frame numbers at fixed parameters to complete different tasks.
To simulate this, we propose a novel processing framework named AnyView for indoor 3D object detection as shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Anyview: Generalizable Indoor 3D Object Detection with Variable Frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
Previous approaches require a complete reconstruction of the scene point cloud through dense RGB-D images before input.
This limitation results in the models only being able to process a fixed frame number of RGB-D image inputs, leading to weak generalization.
In addition, a large amount of resources are wasted in the scene reconstruction process.
Compared with previous approaches, our proposed AnyView can handle variable frame numbers of input data to satisfy the cost budgets of various practical applications.
Specifically, we first unify the modality of input data as sparse multi-view RGB-D images and the corresponding camera parameters, which are compatible with current benchmark datasets (e.g. ScanNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and SUN-RGBD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>).
Instead of concatenating multi-view point clouds into a whole and extracting a fixed number of global features of scenes, we learn geometric clues for each frame independently and design a transformer-based architecture to merge and refine the extracted features of various frames to achieve the ability to efficiently process variable frame inputs.
Specifically, we propose a geometric learner to mine local geometric structural features and employ a spatial mixtures module to fuse local details with global semantics.
In this way, AnyView is able to extract rich representations for input scenes consisting of any number of frames, bridging the gap between the local geometric features of each frame and the global semantic features.
We further propose the randomized view and rectangular dropout data enhancement strategies to encourage the model to adapt to various scales of inputs and ignore frames with less geometric information, respectively.
In addition, to further enhance the generalization of AnyView and consistent point cloud feature density, we propose a dynamic token strategy to adaptively adjust the number of extracted features per frame according to input frame count, which ensures the parameter compatibility and uniformity of the point cloud feature scale.
AnyView fuses the multiview point cloud with camera parameters and refines the initial proposal with a transformer decoder based on the extracted scene representations.
Extensive experiments on ScanNet show that our method outperforms previous methods in both accuracy and generalizability with a similar amount of parameters.
In the online detection simulation on the ScanNet dataset, AnyView achieves 55.23%mAP@25, which is 14.14% higher than 3DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Advances in 3D sensors have led to a surge of methods designed for 3D object detection with various modality. The large amount of related works can be divided into three groups based on the modality of 3D data. We also briefly discuss the sensors and datasets corresponding to these modalities.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Point-based 3D object detection:</span>
This kind of methods take in 3D scenes represented by pure point clouds, which are acquired by LIDAR sensor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> or 3D reconstruction from multi-view RGB-D images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
Early 3D object detection methods mainly include sliding-window methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and template-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
Deep learning-based end-to-end 3D object detection methods began to emerge in recent years, which are mainly based on PointNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> or sparse CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> backbones.
PointNet-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> consume point clouds directly with the set abstraction operation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, which enables flexible receptive fields for 3D feature learning.
Sparse CNN-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> project the point clouds to regular grids to be processed by advanced 2D or 3D CNN architectures.
Tang <em id="S2.p2.1.2" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> further combine the two representations to learn more discriminative features.
Liu <em id="S2.p2.1.3" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> proposed a dynamic point cloud joint detection framework, which uses spatio-temporal information to improve the problem of target loss due to severe occlusions.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Recently numerous works have employed bird’s eye view BEV to improve the point cloud feature extraction backbone network.
Deng <em id="S2.p3.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> proposed the use of point cloud projection onto perspective views and BEV to capture multiview features, which employed a novel bilaterally guided multiview fusion block to phantom 3D representations leveraging the complementary information in perspective views and BEV.
An <em id="S2.p3.1.2" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> designed a saliency detection task to enhance the detector for sparse point cloud object recognition, exploiting voxels and BEV saliency maps as asymptotic attention to resist redundant features in the background region.
Xie <em id="S2.p3.1.3" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> proposed a novel local global feature adaptive aggregation mechanism to fuse contextual information at a fine-grained level and improve the quality of proposals with weighted relational awareness.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Several researchers have enhanced point cloud feature extraction by fusing information from multiple sources.
Liu <em id="S2.p4.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> proposed a dynamic point cloud joint detection framework, which uses spatio-temporal information to improve the problem of target loss due to severe occlusions.
Shan <em id="S2.p4.1.2" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> proposed to utilize the PTT module for modeling feature interactions between the voting phase and proposal generation phase of object detection, which enhances the feature extraction of sparse point cloud objects.
However, point-based detection models have high inference latency due to inefficient sampling algorithms.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p"><span id="S2.p5.1.1" class="ltx_text ltx_font_bold">RGBD-based 3D object detection:</span>
RGBD camera is the mainstream 3D sensor for indoor scene understanding tasks. Existing RGBD-based 3D object detection methods take in monocular RGB-D image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> or point clouds reconstructed from multi-view RGB-D images (mentioned above). Here we only review the former methods which make full use of both RGB and depth information.
Prior methods broadly fall into three categories: 2D-driven, 3D-driven, and modal fusion. 2D-driven methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> first detect object in images and then use the results to assist search in 3D space.
Pahwa <em id="S2.p5.1.2" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> proposed to utilize per-frame depth information and multi-view scene information to obtain accurate 3D object suggestions and generate 3D bounding boxes for potential 3D regions of interest.
Kundu <em id="S2.p5.1.3" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> presented a microscopic rendering and comparison loss by employing voxel patterns to represent 3D objects, which allows 3D shapes and poses to be learned via 2D supervision.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">The most common approach to 3D-driven is to feed image information into a 3D feature extraction network as an additional channel to a point cloud or voxel.
Song <em id="S2.p6.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> localized objects on a voxelized point cloud by sliding a 3D detection window, and category information was obtained from RGB pixels attached to the point cloud.
Song <em id="S2.p6.1.2" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> proposed to process RGB-D inputs by sparse convolution and generate 3D proposals, and subsequently determine object categories by a designed joint object recognition network.
However, this type of simple point cloud and image combination approach may destroy the fine-grained local geometric structural features of the point cloud, resulting in inefficient fusion between the point cloud and image modalities.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">Modal fusion methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> use the geometric correspondence between 2D and 3D to guide the predictions of the network and enrich the representation by concatenating multi-modal features.
Zhu <em id="S2.p7.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> employed virtual point cloud images to effectively bridge the resolution gap between image modality and point cloud modality to address the information loss due to multimodal data fusion.
Bai <em id="S2.p7.1.2" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> employed Transformer to adaptively fuse 3D point cloud proposals with image features and processed difficult samples in the point cloud with an image-guided strategy, effectively improving the robustness of low-quality images.
However, there is still a huge performance gap between RGBD-based methods and point cloud-based methods.</p>
</div>
<div id="S2.p8" class="ltx_para">
<p id="S2.p8.1" class="ltx_p"><span id="S2.p8.1.1" class="ltx_text ltx_font_bold">Multi-sensor 3D object detection:</span>
Recently, multi-sensor fusion has aroused increased interest in the 3D detection community.
Depending on the fine-grained level of the fusion approach, existing approaches can be classified into region-level and point-level fusion methods.</p>
</div>
<div id="S2.p9" class="ltx_para">
<p id="S2.p9.1" class="ltx_p">Region-level fusion methods generate object proposals in 3D space and project them to images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> or lift image proposals into a 3D frustum to reduce the search space in 3D space <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.
Qi <em id="S2.p9.1.1" class="ltx_emph ltx_font_italic">et al.</em> ] <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> implemented multimodal inference directly employing a 2D object detector with a 3D point cloud feature extraction network, this framework was able to maintain high accuracy even in the presence of strong occlusions or very sparse points and inspired a series of subsequent work
Wang <em id="S2.p9.1.2" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> proposed to project region proposals on 2D images as frustums in 3D space and utilize 3D backbone network extraction to achieve region-level feature fusion.
Liu <em id="S2.p9.1.3" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> proposed a fusion module based on a depth-attention mechanism to achieve multimodal feature differential fusion at various depths and addressed the unstable efficiency of modal fusion of point clouds and images.
Paigwar <em id="S2.p9.1.4" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> extended the fusion approach to pillar representation and proposed a novel masked point cloud approach to improve object localization accuracy.</p>
</div>
<div id="S2.p10" class="ltx_para">
<p id="S2.p10.1" class="ltx_p">Point-level fusion methods, on the other hand, usually enhance the LIDAR points with the image semantic features.
There are several researches aiming to enhance the input point cloud with image features and then employ a 3D object detector to process the enhanced point cloud for higher accuracy.
The image features can be fused to input LIDAR points <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> or middle feature points <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>.
Xu <em id="S2.p10.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> utilized point-to-pixel projection matrices to attach 2D semantic segmentation labels to 3D point clouds and employed an attentional mechanism to adaptively fuse semantic information to significantly improve model performance.
Yin <em id="S2.p10.1.2" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> proposed to generate a dense 3D virtual point cloud via a 2D images object detector to solve the point cloud sparsity problem and this approach can be naturally integrated into any LiDAR data.
Vora <em id="S2.p10.1.3" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> proposed to project 3D point cloud information to the 2D image semantic segmentation results and can feed the integrated information back to any 3D point cloud object detection models, which provides a better trade-off between accuracy and speed.
After that, point-based methods are adopted to detect objects from the decorated point clouds.
However, effectively fusing information from multiple modalities remains an open challenge.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Analysis</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we first describe the problem statement from existing benchmarks for indoor 3D object detection.
Then we propose a more unified and reasonable setting for indoor 3D object detection task. Finally we show the failing case of previous models in practical scenarios.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Problem Statement</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In order to better utilize 3D detector in practical tasks where the raw input is a variable number of RGB-D images, we propose a new setting for indoor 3D object detection.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.11" class="ltx_p"><span id="S3.SS1.p2.11.1" class="ltx_text ltx_font_bold">Inference:</span>
Unlike the input data format of the existing benchmarks, in order to simulate the inference process of the model in the practical scene, we take the sparse RGB-D images and their camera parameters as the unified input data of the 3D detector to provide the intelligent body with the scene object detection results, which can be summarized as follows:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="\mathcal{B}_{N}=\mathcal{D}(\left\{I_{k},R_{k}^{c},T_{k}^{c}\right\}_{k=1}^{N}),R_{k}^{c}\in\mathbb{R}^{3\times 3},T_{k}^{c}\in\mathbb{R}^{3\times 1}" display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.3.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.3.2.cmml">ℬ</mi><mi id="S3.E1.m1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.3.3.cmml">N</mi></msub><mo id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.3.cmml">𝒟</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><msubsup id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.4.cmml"><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.4" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.4.cmml">{</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">I</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">k</mi></msub><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.5" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.4.cmml">,</mo><msubsup id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.2.cmml">R</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.cmml">k</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml">c</mi></msubsup><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.6" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.4.cmml">,</mo><msubsup id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.3.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.3.2.2.cmml">T</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.3.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.3.2.3.cmml">k</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.3.3.cmml">c</mi></msubsup><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.7" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.4.cmml">}</mo></mrow><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.3.5" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.5.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.5.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.5.2.cmml">k</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.3.5.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.5.1.cmml">=</mo><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.3.5.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.5.3.cmml">1</mn></mrow><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.5" xref="S3.E1.m1.1.1.1.1.1.1.1.1.5.cmml">N</mi></msubsup><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.3a.cmml">,</mo><mrow id="S3.E1.m1.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.3.cmml"><mrow id="S3.E1.m1.2.2.2.2.1.1" xref="S3.E1.m1.2.2.2.2.1.1.cmml"><msubsup id="S3.E1.m1.2.2.2.2.1.1.2" xref="S3.E1.m1.2.2.2.2.1.1.2.cmml"><mi id="S3.E1.m1.2.2.2.2.1.1.2.2.2" xref="S3.E1.m1.2.2.2.2.1.1.2.2.2.cmml">R</mi><mi id="S3.E1.m1.2.2.2.2.1.1.2.2.3" xref="S3.E1.m1.2.2.2.2.1.1.2.2.3.cmml">k</mi><mi id="S3.E1.m1.2.2.2.2.1.1.2.3" xref="S3.E1.m1.2.2.2.2.1.1.2.3.cmml">c</mi></msubsup><mo id="S3.E1.m1.2.2.2.2.1.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.cmml">∈</mo><msup id="S3.E1.m1.2.2.2.2.1.1.3" xref="S3.E1.m1.2.2.2.2.1.1.3.cmml"><mi id="S3.E1.m1.2.2.2.2.1.1.3.2" xref="S3.E1.m1.2.2.2.2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.E1.m1.2.2.2.2.1.1.3.3" xref="S3.E1.m1.2.2.2.2.1.1.3.3.cmml"><mn id="S3.E1.m1.2.2.2.2.1.1.3.3.2" xref="S3.E1.m1.2.2.2.2.1.1.3.3.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.2.2.2.2.1.1.3.3.1" xref="S3.E1.m1.2.2.2.2.1.1.3.3.1.cmml">×</mo><mn id="S3.E1.m1.2.2.2.2.1.1.3.3.3" xref="S3.E1.m1.2.2.2.2.1.1.3.3.3.cmml">3</mn></mrow></msup></mrow><mo id="S3.E1.m1.2.2.2.2.2.3" xref="S3.E1.m1.2.2.2.2.3a.cmml">,</mo><mrow id="S3.E1.m1.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.cmml"><msubsup id="S3.E1.m1.2.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.2.cmml"><mi id="S3.E1.m1.2.2.2.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.2.2.2.cmml">T</mi><mi id="S3.E1.m1.2.2.2.2.2.2.2.2.3" xref="S3.E1.m1.2.2.2.2.2.2.2.2.3.cmml">k</mi><mi id="S3.E1.m1.2.2.2.2.2.2.2.3" xref="S3.E1.m1.2.2.2.2.2.2.2.3.cmml">c</mi></msubsup><mo id="S3.E1.m1.2.2.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.2.2.1.cmml">∈</mo><msup id="S3.E1.m1.2.2.2.2.2.2.3" xref="S3.E1.m1.2.2.2.2.2.2.3.cmml"><mi id="S3.E1.m1.2.2.2.2.2.2.3.2" xref="S3.E1.m1.2.2.2.2.2.2.3.2.cmml">ℝ</mi><mrow id="S3.E1.m1.2.2.2.2.2.2.3.3" xref="S3.E1.m1.2.2.2.2.2.2.3.3.cmml"><mn id="S3.E1.m1.2.2.2.2.2.2.3.3.2" xref="S3.E1.m1.2.2.2.2.2.2.3.3.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.2.2.2.2.2.2.3.3.1" xref="S3.E1.m1.2.2.2.2.2.2.3.3.1.cmml">×</mo><mn id="S3.E1.m1.2.2.2.2.2.2.3.3.3" xref="S3.E1.m1.2.2.2.2.2.2.3.3.3.cmml">1</mn></mrow></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.3a.cmml" xref="S3.E1.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"></eq><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2">ℬ</ci><ci id="S3.E1.m1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3">𝑁</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.2"></times><ci id="S3.E1.m1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.3">𝒟</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.3.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1">subscript</csymbol><set id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3"><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝐼</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑘</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.2">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.2">𝑅</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.2.2.3">𝑘</ci></apply><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.2.2.3">𝑐</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.3">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.3.2.2">𝑇</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.3.2.3">𝑘</ci></apply><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.3.3">𝑐</ci></apply></set><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.3.5.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.5"><eq id="S3.E1.m1.1.1.1.1.1.1.1.1.3.5.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.5.1"></eq><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.5.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.5.2">𝑘</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.1.3.5.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.5.3">1</cn></apply></apply><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.5.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.5">𝑁</ci></apply></apply></apply><apply id="S3.E1.m1.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.3a.cmml" xref="S3.E1.m1.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S3.E1.m1.2.2.2.2.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1"><in id="S3.E1.m1.2.2.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1"></in><apply id="S3.E1.m1.2.2.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.1.1.2.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2">superscript</csymbol><apply id="S3.E1.m1.2.2.2.2.1.1.2.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.1.1.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.1.1.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2.2.2">𝑅</ci><ci id="S3.E1.m1.2.2.2.2.1.1.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2.2.3">𝑘</ci></apply><ci id="S3.E1.m1.2.2.2.2.1.1.2.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2.3">𝑐</ci></apply><apply id="S3.E1.m1.2.2.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.1.1.3.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3">superscript</csymbol><ci id="S3.E1.m1.2.2.2.2.1.1.3.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3.2">ℝ</ci><apply id="S3.E1.m1.2.2.2.2.1.1.3.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3.3"><times id="S3.E1.m1.2.2.2.2.1.1.3.3.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3.3.1"></times><cn type="integer" id="S3.E1.m1.2.2.2.2.1.1.3.3.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3.3.2">3</cn><cn type="integer" id="S3.E1.m1.2.2.2.2.1.1.3.3.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3.3.3">3</cn></apply></apply></apply><apply id="S3.E1.m1.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2"><in id="S3.E1.m1.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1"></in><apply id="S3.E1.m1.2.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2">superscript</csymbol><apply id="S3.E1.m1.2.2.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2.2">𝑇</ci><ci id="S3.E1.m1.2.2.2.2.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2.3">𝑘</ci></apply><ci id="S3.E1.m1.2.2.2.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.3">𝑐</ci></apply><apply id="S3.E1.m1.2.2.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.2.2.3.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.3">superscript</csymbol><ci id="S3.E1.m1.2.2.2.2.2.2.3.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.3.2">ℝ</ci><apply id="S3.E1.m1.2.2.2.2.2.2.3.3.cmml" xref="S3.E1.m1.2.2.2.2.2.2.3.3"><times id="S3.E1.m1.2.2.2.2.2.2.3.3.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.3.3.1"></times><cn type="integer" id="S3.E1.m1.2.2.2.2.2.2.3.3.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.3.3.2">3</cn><cn type="integer" id="S3.E1.m1.2.2.2.2.2.2.3.3.3.cmml" xref="S3.E1.m1.2.2.2.2.2.2.3.3.3">1</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\mathcal{B}_{N}=\mathcal{D}(\left\{I_{k},R_{k}^{c},T_{k}^{c}\right\}_{k=1}^{N}),R_{k}^{c}\in\mathbb{R}^{3\times 3},T_{k}^{c}\in\mathbb{R}^{3\times 1}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p2.10" class="ltx_p">where <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">N</annotation></semantics></math> represents the number of RGB-D image frames input to the 3D object detector <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\mathcal{D}</annotation></semantics></math>, and <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{B}_{N}" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><msub id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">ℬ</mi><mi id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml">N</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2">ℬ</ci><ci id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">\mathcal{B}_{N}</annotation></semantics></math> represents the reconstructed point cloud detection results of <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mi id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><ci id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">N</annotation></semantics></math> frames of RGB-D images.
<math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="I_{k}" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><msub id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml"><mi id="S3.SS1.p2.5.m5.1.1.2" xref="S3.SS1.p2.5.m5.1.1.2.cmml">I</mi><mi id="S3.SS1.p2.5.m5.1.1.3" xref="S3.SS1.p2.5.m5.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><apply id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2">𝐼</ci><ci id="S3.SS1.p2.5.m5.1.1.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">I_{k}</annotation></semantics></math>, <math id="S3.SS1.p2.6.m6.1" class="ltx_Math" alttext="R_{k}^{c}" display="inline"><semantics id="S3.SS1.p2.6.m6.1a"><msubsup id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml"><mi id="S3.SS1.p2.6.m6.1.1.2.2" xref="S3.SS1.p2.6.m6.1.1.2.2.cmml">R</mi><mi id="S3.SS1.p2.6.m6.1.1.2.3" xref="S3.SS1.p2.6.m6.1.1.2.3.cmml">k</mi><mi id="S3.SS1.p2.6.m6.1.1.3" xref="S3.SS1.p2.6.m6.1.1.3.cmml">c</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><apply id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">superscript</csymbol><apply id="S3.SS1.p2.6.m6.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.2.1.cmml" xref="S3.SS1.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p2.6.m6.1.1.2.2.cmml" xref="S3.SS1.p2.6.m6.1.1.2.2">𝑅</ci><ci id="S3.SS1.p2.6.m6.1.1.2.3.cmml" xref="S3.SS1.p2.6.m6.1.1.2.3">𝑘</ci></apply><ci id="S3.SS1.p2.6.m6.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">R_{k}^{c}</annotation></semantics></math>, and <math id="S3.SS1.p2.7.m7.1" class="ltx_Math" alttext="T_{k}^{c}" display="inline"><semantics id="S3.SS1.p2.7.m7.1a"><msubsup id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml"><mi id="S3.SS1.p2.7.m7.1.1.2.2" xref="S3.SS1.p2.7.m7.1.1.2.2.cmml">T</mi><mi id="S3.SS1.p2.7.m7.1.1.2.3" xref="S3.SS1.p2.7.m7.1.1.2.3.cmml">k</mi><mi id="S3.SS1.p2.7.m7.1.1.3" xref="S3.SS1.p2.7.m7.1.1.3.cmml">c</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><apply id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">superscript</csymbol><apply id="S3.SS1.p2.7.m7.1.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.2.1.cmml" xref="S3.SS1.p2.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.2.2.cmml" xref="S3.SS1.p2.7.m7.1.1.2.2">𝑇</ci><ci id="S3.SS1.p2.7.m7.1.1.2.3.cmml" xref="S3.SS1.p2.7.m7.1.1.2.3">𝑘</ci></apply><ci id="S3.SS1.p2.7.m7.1.1.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">T_{k}^{c}</annotation></semantics></math> correspond to the <math id="S3.SS1.p2.8.m8.1" class="ltx_Math" alttext="k_{th}" display="inline"><semantics id="S3.SS1.p2.8.m8.1a"><msub id="S3.SS1.p2.8.m8.1.1" xref="S3.SS1.p2.8.m8.1.1.cmml"><mi id="S3.SS1.p2.8.m8.1.1.2" xref="S3.SS1.p2.8.m8.1.1.2.cmml">k</mi><mrow id="S3.SS1.p2.8.m8.1.1.3" xref="S3.SS1.p2.8.m8.1.1.3.cmml"><mi id="S3.SS1.p2.8.m8.1.1.3.2" xref="S3.SS1.p2.8.m8.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.8.m8.1.1.3.1" xref="S3.SS1.p2.8.m8.1.1.3.1.cmml">​</mo><mi id="S3.SS1.p2.8.m8.1.1.3.3" xref="S3.SS1.p2.8.m8.1.1.3.3.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.1b"><apply id="S3.SS1.p2.8.m8.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.1.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.p2.8.m8.1.1.2.cmml" xref="S3.SS1.p2.8.m8.1.1.2">𝑘</ci><apply id="S3.SS1.p2.8.m8.1.1.3.cmml" xref="S3.SS1.p2.8.m8.1.1.3"><times id="S3.SS1.p2.8.m8.1.1.3.1.cmml" xref="S3.SS1.p2.8.m8.1.1.3.1"></times><ci id="S3.SS1.p2.8.m8.1.1.3.2.cmml" xref="S3.SS1.p2.8.m8.1.1.3.2">𝑡</ci><ci id="S3.SS1.p2.8.m8.1.1.3.3.cmml" xref="S3.SS1.p2.8.m8.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.1c">k_{th}</annotation></semantics></math> frame of RGB-D image, rotation matrix, and translation matrix, respectively.
<math id="S3.SS1.p2.9.m9.3" class="ltx_Math" alttext="\left\{I_{k},R_{k}^{c},T_{k}^{c}\right\}_{k=1}^{N}" display="inline"><semantics id="S3.SS1.p2.9.m9.3a"><msubsup id="S3.SS1.p2.9.m9.3.3" xref="S3.SS1.p2.9.m9.3.3.cmml"><mrow id="S3.SS1.p2.9.m9.3.3.3.3.3" xref="S3.SS1.p2.9.m9.3.3.3.3.4.cmml"><mo id="S3.SS1.p2.9.m9.3.3.3.3.3.4" xref="S3.SS1.p2.9.m9.3.3.3.3.4.cmml">{</mo><msub id="S3.SS1.p2.9.m9.1.1.1.1.1.1" xref="S3.SS1.p2.9.m9.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.9.m9.1.1.1.1.1.1.2" xref="S3.SS1.p2.9.m9.1.1.1.1.1.1.2.cmml">I</mi><mi id="S3.SS1.p2.9.m9.1.1.1.1.1.1.3" xref="S3.SS1.p2.9.m9.1.1.1.1.1.1.3.cmml">k</mi></msub><mo id="S3.SS1.p2.9.m9.3.3.3.3.3.5" xref="S3.SS1.p2.9.m9.3.3.3.3.4.cmml">,</mo><msubsup id="S3.SS1.p2.9.m9.2.2.2.2.2.2" xref="S3.SS1.p2.9.m9.2.2.2.2.2.2.cmml"><mi id="S3.SS1.p2.9.m9.2.2.2.2.2.2.2.2" xref="S3.SS1.p2.9.m9.2.2.2.2.2.2.2.2.cmml">R</mi><mi id="S3.SS1.p2.9.m9.2.2.2.2.2.2.2.3" xref="S3.SS1.p2.9.m9.2.2.2.2.2.2.2.3.cmml">k</mi><mi id="S3.SS1.p2.9.m9.2.2.2.2.2.2.3" xref="S3.SS1.p2.9.m9.2.2.2.2.2.2.3.cmml">c</mi></msubsup><mo id="S3.SS1.p2.9.m9.3.3.3.3.3.6" xref="S3.SS1.p2.9.m9.3.3.3.3.4.cmml">,</mo><msubsup id="S3.SS1.p2.9.m9.3.3.3.3.3.3" xref="S3.SS1.p2.9.m9.3.3.3.3.3.3.cmml"><mi id="S3.SS1.p2.9.m9.3.3.3.3.3.3.2.2" xref="S3.SS1.p2.9.m9.3.3.3.3.3.3.2.2.cmml">T</mi><mi id="S3.SS1.p2.9.m9.3.3.3.3.3.3.2.3" xref="S3.SS1.p2.9.m9.3.3.3.3.3.3.2.3.cmml">k</mi><mi id="S3.SS1.p2.9.m9.3.3.3.3.3.3.3" xref="S3.SS1.p2.9.m9.3.3.3.3.3.3.3.cmml">c</mi></msubsup><mo id="S3.SS1.p2.9.m9.3.3.3.3.3.7" xref="S3.SS1.p2.9.m9.3.3.3.3.4.cmml">}</mo></mrow><mrow id="S3.SS1.p2.9.m9.3.3.3.5" xref="S3.SS1.p2.9.m9.3.3.3.5.cmml"><mi id="S3.SS1.p2.9.m9.3.3.3.5.2" xref="S3.SS1.p2.9.m9.3.3.3.5.2.cmml">k</mi><mo id="S3.SS1.p2.9.m9.3.3.3.5.1" xref="S3.SS1.p2.9.m9.3.3.3.5.1.cmml">=</mo><mn id="S3.SS1.p2.9.m9.3.3.3.5.3" xref="S3.SS1.p2.9.m9.3.3.3.5.3.cmml">1</mn></mrow><mi id="S3.SS1.p2.9.m9.3.3.5" xref="S3.SS1.p2.9.m9.3.3.5.cmml">N</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m9.3b"><apply id="S3.SS1.p2.9.m9.3.3.cmml" xref="S3.SS1.p2.9.m9.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.9.m9.3.3.4.cmml" xref="S3.SS1.p2.9.m9.3.3">superscript</csymbol><apply id="S3.SS1.p2.9.m9.3.3.3.cmml" xref="S3.SS1.p2.9.m9.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.9.m9.3.3.3.4.cmml" xref="S3.SS1.p2.9.m9.3.3">subscript</csymbol><set id="S3.SS1.p2.9.m9.3.3.3.3.4.cmml" xref="S3.SS1.p2.9.m9.3.3.3.3.3"><apply id="S3.SS1.p2.9.m9.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.9.m9.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.9.m9.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.9.m9.1.1.1.1.1.1.2">𝐼</ci><ci id="S3.SS1.p2.9.m9.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.9.m9.1.1.1.1.1.1.3">𝑘</ci></apply><apply id="S3.SS1.p2.9.m9.2.2.2.2.2.2.cmml" xref="S3.SS1.p2.9.m9.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.9.m9.2.2.2.2.2.2.1.cmml" xref="S3.SS1.p2.9.m9.2.2.2.2.2.2">superscript</csymbol><apply id="S3.SS1.p2.9.m9.2.2.2.2.2.2.2.cmml" xref="S3.SS1.p2.9.m9.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.9.m9.2.2.2.2.2.2.2.1.cmml" xref="S3.SS1.p2.9.m9.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p2.9.m9.2.2.2.2.2.2.2.2.cmml" xref="S3.SS1.p2.9.m9.2.2.2.2.2.2.2.2">𝑅</ci><ci id="S3.SS1.p2.9.m9.2.2.2.2.2.2.2.3.cmml" xref="S3.SS1.p2.9.m9.2.2.2.2.2.2.2.3">𝑘</ci></apply><ci id="S3.SS1.p2.9.m9.2.2.2.2.2.2.3.cmml" xref="S3.SS1.p2.9.m9.2.2.2.2.2.2.3">𝑐</ci></apply><apply id="S3.SS1.p2.9.m9.3.3.3.3.3.3.cmml" xref="S3.SS1.p2.9.m9.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.9.m9.3.3.3.3.3.3.1.cmml" xref="S3.SS1.p2.9.m9.3.3.3.3.3.3">superscript</csymbol><apply id="S3.SS1.p2.9.m9.3.3.3.3.3.3.2.cmml" xref="S3.SS1.p2.9.m9.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.9.m9.3.3.3.3.3.3.2.1.cmml" xref="S3.SS1.p2.9.m9.3.3.3.3.3.3">subscript</csymbol><ci id="S3.SS1.p2.9.m9.3.3.3.3.3.3.2.2.cmml" xref="S3.SS1.p2.9.m9.3.3.3.3.3.3.2.2">𝑇</ci><ci id="S3.SS1.p2.9.m9.3.3.3.3.3.3.2.3.cmml" xref="S3.SS1.p2.9.m9.3.3.3.3.3.3.2.3">𝑘</ci></apply><ci id="S3.SS1.p2.9.m9.3.3.3.3.3.3.3.cmml" xref="S3.SS1.p2.9.m9.3.3.3.3.3.3.3">𝑐</ci></apply></set><apply id="S3.SS1.p2.9.m9.3.3.3.5.cmml" xref="S3.SS1.p2.9.m9.3.3.3.5"><eq id="S3.SS1.p2.9.m9.3.3.3.5.1.cmml" xref="S3.SS1.p2.9.m9.3.3.3.5.1"></eq><ci id="S3.SS1.p2.9.m9.3.3.3.5.2.cmml" xref="S3.SS1.p2.9.m9.3.3.3.5.2">𝑘</ci><cn type="integer" id="S3.SS1.p2.9.m9.3.3.3.5.3.cmml" xref="S3.SS1.p2.9.m9.3.3.3.5.3">1</cn></apply></apply><ci id="S3.SS1.p2.9.m9.3.3.5.cmml" xref="S3.SS1.p2.9.m9.3.3.5">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m9.3c">\left\{I_{k},R_{k}^{c},T_{k}^{c}\right\}_{k=1}^{N}</annotation></semantics></math> can represent both local region and a whole scene.
To reduce the time for data collection, the maximum number of views is set to 50(<math id="S3.SS1.p2.10.m10.1" class="ltx_Math" alttext="N\leq 50" display="inline"><semantics id="S3.SS1.p2.10.m10.1a"><mrow id="S3.SS1.p2.10.m10.1.1" xref="S3.SS1.p2.10.m10.1.1.cmml"><mi id="S3.SS1.p2.10.m10.1.1.2" xref="S3.SS1.p2.10.m10.1.1.2.cmml">N</mi><mo id="S3.SS1.p2.10.m10.1.1.1" xref="S3.SS1.p2.10.m10.1.1.1.cmml">≤</mo><mn id="S3.SS1.p2.10.m10.1.1.3" xref="S3.SS1.p2.10.m10.1.1.3.cmml">50</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.10.m10.1b"><apply id="S3.SS1.p2.10.m10.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1"><leq id="S3.SS1.p2.10.m10.1.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1.1"></leq><ci id="S3.SS1.p2.10.m10.1.1.2.cmml" xref="S3.SS1.p2.10.m10.1.1.2">𝑁</ci><cn type="integer" id="S3.SS1.p2.10.m10.1.1.3.cmml" xref="S3.SS1.p2.10.m10.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.10.m10.1c">N\leq 50</annotation></semantics></math>), which is far less than the number required for 3D reconstruction.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.2" class="ltx_p"><span id="S3.SS1.p3.2.1" class="ltx_text ltx_font_bold">Model:</span>
Since the number of input views will change according to the task in practical scene deployments, the 3D object detectors <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">\mathcal{D}</annotation></semantics></math> must be highly flexible in order to efficiently process RGB-D images with the variable number of frames input.
Therefore, the training process of models <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><ci id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">\mathcal{D}</annotation></semantics></math> needs to incorporate adaptations which enable D to process point clouds with arbitrary scale inputs without additional fine-tuning.
This attribute significantly improves the utility and efficiency of the 3D detection applications, enabling them to easily handle a wide range of task input scenes and provide accurate object detection results.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.3" class="ltx_p"><span id="S3.SS1.p4.3.1" class="ltx_text ltx_font_bold">Evaluation:</span>
The performance of model should not be evaluated only on one scale of input. We prepare a variety of validation sets containing different scales of input data, such as monocular input, few-view input and scene-level input. The number of views <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><mi id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><ci id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">N</annotation></semantics></math> can range from <math id="S3.SS1.p4.2.m2.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><mn id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><cn type="integer" id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">1</annotation></semantics></math> to <math id="S3.SS1.p4.3.m3.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S3.SS1.p4.3.m3.1a"><mn id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><cn type="integer" id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">50</annotation></semantics></math>.
Two representative methods are chosen: ImVoteNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> for monocular RGB-D input and 3DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> for scene-level point cloud input.
We prepare three benchmarks from the raw RGB-D videos provided by ScanNet: ScanNet-SV, ScanNet-Rec, and ScanNet-MV<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Refer to Section <a href="#S5.SS1" title="V-A Experiments Setup ‣ V Experiment ‣ Anyview: Generalizable Indoor 3D Object Detection with Variable Frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-A</span></span></a> for more details.</span></span></span>. ScanNet-SV (single view) is a monocular RGB-D benchmark organized similar to SUN-RGBD.ScanNet-Rec (reconstruction) is the previous scene-level benchmark. ScanNet-MV (multi-view) shares the same ground truth with ScanNet-Rec but provides multi-view RGB-D images as input instead of reconstructed point clouds.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Generalizability of previous model. The gray cell indicates on which benchmark the model is trained.</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S3.T1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.2.1.1" class="ltx_p" style="width:45.5pt;"><span id="S3.T1.1.1.1.2.1.1.1" class="ltx_text" style="font-size:80%;">ScanNet-SV</span></span>
</span>
</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S3.T1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.3.1.1" class="ltx_p" style="width:45.5pt;"><span id="S3.T1.1.1.1.3.1.1.1" class="ltx_text" style="font-size:80%;">ScanNet-Rec</span></span>
</span>
</th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S3.T1.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="S3.T1.1.1.1.4.1.1.1" class="ltx_text" style="font-size:80%;">ScanNet-MV</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.2.1.1.1" class="ltx_text" style="font-size:80%;">ImVoteNet</span></th>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="background-color:#D9D9D9;padding-left:5.0pt;padding-right:5.0pt;">
<span id="S3.T1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.1.2.1.1" class="ltx_p" style="width:45.5pt;"><span id="S3.T1.1.2.1.2.1.1.1" class="ltx_text" style="font-size:80%;background-color:#D9D9D9;">31.6 / 13.5</span></span>
</span>
</td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S3.T1.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.1.3.1.1" class="ltx_p" style="width:45.5pt;"><span id="S3.T1.1.2.1.3.1.1.1" class="ltx_text" style="font-size:80%;">–</span></span>
</span>
</td>
<td id="S3.T1.1.2.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S3.T1.1.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.1.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="S3.T1.1.2.1.4.1.1.1" class="ltx_text" style="font-size:80%;">43.3 / 20.4</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<th id="S3.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">3DETR</span></th>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S3.T1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.2.2.1.1" class="ltx_p" style="width:45.5pt;"><span id="S3.T1.1.3.2.2.1.1.1" class="ltx_text" style="font-size:80%;">19.8 / 6.1</span></span>
</span>
</td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" style="background-color:#D9D9D9;padding-left:5.0pt;padding-right:5.0pt;">
<span id="S3.T1.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.2.3.1.1" class="ltx_p" style="width:45.5pt;"><span id="S3.T1.1.3.2.3.1.1.1" class="ltx_text" style="font-size:80%;background-color:#D9D9D9;">64.5 / 44.0</span></span>
</span>
</td>
<td id="S3.T1.1.3.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S3.T1.1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.2.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="S3.T1.1.3.2.4.1.1.1" class="ltx_text" style="font-size:80%;">44.7 / 25.6</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">We train ImVoteNet and 3DETR on ScanNet-SV and ScanNet-Rec respectively and evaluate the models on all benchmarks. When applying 3DETR on ScanNet-MV, we fuse the multi-view depth maps into point clouds according to the camera parameters. As for applying ImVoteNet on ScanNet-MV, we predict bounding boxes for each view and fuse the results by 3D NMS. As shown in Table <a href="#S3.T1" title="TABLE I ‣ III-A Problem Statement ‣ III Analysis ‣ Anyview: Generalizable Indoor 3D Object Detection with Variable Frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, 3DETR trained on reconstructed point clouds performs poorly on the monocular RGB-D benchmark, and both ImVoteNet and 3DETR fail to achieve a satisfactory performance on the multi-view RGB-D benchmark.
This experimental result indicates that the performance of the detector on previous benchmarks may not reflect its performance in practical applications.
And the poor generalizability of previous models poses a huge challenge in applying the existing models in practical tasks, where time for data collection is limited and the scale of input data is changeable.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Approach</span>
</h2>

<figure id="S4.F2" class="ltx_figure"><img src="/html/2310.05346/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="188" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The framework of <em id="S4.F2.2.1" class="ltx_emph ltx_font_italic">AnyView</em>. Given multi-view RGB-D images of variable number, we first convert the depth maps to single view point clouds according to intrinsic camera parameters. Then a shared geometry learner is applied to extract scene proxies for each view, which are local feature descriptors independent of input scale. Scene proxies from different views are mixed into world coordinates according to the extrinsic camera parameters. We adopt transformer encoder to refine the scene proxies while keeping their input scale-independent property by self-attention mechanism. A transformer decoder is used to refine the object proposals, where the initial proposals are sampled from the whole scene by FPS. The dashed blue box indicates the number of inside elements is changable during inference.</figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we propose a new framework called AnyView for detecting 3D objects from multi-view RGB-D images. We first show the overall framework of AnyView. Then we introduce the input scale-independent training strategy. Finally we detail the dynamic token technique for accuracy-computation tradeoff during inference time.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Overall Framework</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The overall framework of AnyView is illustrated in Fig. <a href="#S4.F2" title="Figure 2 ‣ IV Approach ‣ Anyview: Generalizable Indoor 3D Object Detection with Variable Frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
Given multi-view RGB-D images as input, AnyView focuses on the utilization of depth maps and camera parameters.
We will introduce our method in detail as follows.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.5" class="ltx_p">Formally, the images are represented by <math id="S4.SS1.p2.1.m1.4" class="ltx_Math" alttext="\{I_{1},I_{2},...,I_{N}\}" display="inline"><semantics id="S4.SS1.p2.1.m1.4a"><mrow id="S4.SS1.p2.1.m1.4.4.3" xref="S4.SS1.p2.1.m1.4.4.4.cmml"><mo stretchy="false" id="S4.SS1.p2.1.m1.4.4.3.4" xref="S4.SS1.p2.1.m1.4.4.4.cmml">{</mo><msub id="S4.SS1.p2.1.m1.2.2.1.1" xref="S4.SS1.p2.1.m1.2.2.1.1.cmml"><mi id="S4.SS1.p2.1.m1.2.2.1.1.2" xref="S4.SS1.p2.1.m1.2.2.1.1.2.cmml">I</mi><mn id="S4.SS1.p2.1.m1.2.2.1.1.3" xref="S4.SS1.p2.1.m1.2.2.1.1.3.cmml">1</mn></msub><mo id="S4.SS1.p2.1.m1.4.4.3.5" xref="S4.SS1.p2.1.m1.4.4.4.cmml">,</mo><msub id="S4.SS1.p2.1.m1.3.3.2.2" xref="S4.SS1.p2.1.m1.3.3.2.2.cmml"><mi id="S4.SS1.p2.1.m1.3.3.2.2.2" xref="S4.SS1.p2.1.m1.3.3.2.2.2.cmml">I</mi><mn id="S4.SS1.p2.1.m1.3.3.2.2.3" xref="S4.SS1.p2.1.m1.3.3.2.2.3.cmml">2</mn></msub><mo id="S4.SS1.p2.1.m1.4.4.3.6" xref="S4.SS1.p2.1.m1.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">…</mi><mo id="S4.SS1.p2.1.m1.4.4.3.7" xref="S4.SS1.p2.1.m1.4.4.4.cmml">,</mo><msub id="S4.SS1.p2.1.m1.4.4.3.3" xref="S4.SS1.p2.1.m1.4.4.3.3.cmml"><mi id="S4.SS1.p2.1.m1.4.4.3.3.2" xref="S4.SS1.p2.1.m1.4.4.3.3.2.cmml">I</mi><mi id="S4.SS1.p2.1.m1.4.4.3.3.3" xref="S4.SS1.p2.1.m1.4.4.3.3.3.cmml">N</mi></msub><mo stretchy="false" id="S4.SS1.p2.1.m1.4.4.3.8" xref="S4.SS1.p2.1.m1.4.4.4.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.4b"><set id="S4.SS1.p2.1.m1.4.4.4.cmml" xref="S4.SS1.p2.1.m1.4.4.3"><apply id="S4.SS1.p2.1.m1.2.2.1.1.cmml" xref="S4.SS1.p2.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.2.2.1.1.1.cmml" xref="S4.SS1.p2.1.m1.2.2.1.1">subscript</csymbol><ci id="S4.SS1.p2.1.m1.2.2.1.1.2.cmml" xref="S4.SS1.p2.1.m1.2.2.1.1.2">𝐼</ci><cn type="integer" id="S4.SS1.p2.1.m1.2.2.1.1.3.cmml" xref="S4.SS1.p2.1.m1.2.2.1.1.3">1</cn></apply><apply id="S4.SS1.p2.1.m1.3.3.2.2.cmml" xref="S4.SS1.p2.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.3.3.2.2.1.cmml" xref="S4.SS1.p2.1.m1.3.3.2.2">subscript</csymbol><ci id="S4.SS1.p2.1.m1.3.3.2.2.2.cmml" xref="S4.SS1.p2.1.m1.3.3.2.2.2">𝐼</ci><cn type="integer" id="S4.SS1.p2.1.m1.3.3.2.2.3.cmml" xref="S4.SS1.p2.1.m1.3.3.2.2.3">2</cn></apply><ci id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">…</ci><apply id="S4.SS1.p2.1.m1.4.4.3.3.cmml" xref="S4.SS1.p2.1.m1.4.4.3.3"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.4.4.3.3.1.cmml" xref="S4.SS1.p2.1.m1.4.4.3.3">subscript</csymbol><ci id="S4.SS1.p2.1.m1.4.4.3.3.2.cmml" xref="S4.SS1.p2.1.m1.4.4.3.3.2">𝐼</ci><ci id="S4.SS1.p2.1.m1.4.4.3.3.3.cmml" xref="S4.SS1.p2.1.m1.4.4.3.3.3">𝑁</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.4c">\{I_{1},I_{2},...,I_{N}\}</annotation></semantics></math>. We sample <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mi id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><ci id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">K</annotation></semantics></math> points for each depth map and convert them into camera coordinate by intrinsic camera calibration matrix, which are denoted as <math id="S4.SS1.p2.3.m3.3" class="ltx_Math" alttext="\{P_{c1},P_{c2},...,P_{cN}\},P_{ci}\in\mathbb{R}^{K\times 3}" display="inline"><semantics id="S4.SS1.p2.3.m3.3a"><mrow id="S4.SS1.p2.3.m3.3.3" xref="S4.SS1.p2.3.m3.3.3.cmml"><mrow id="S4.SS1.p2.3.m3.3.3.2.2" xref="S4.SS1.p2.3.m3.3.3.2.3.cmml"><mrow id="S4.SS1.p2.3.m3.2.2.1.1.1.3" xref="S4.SS1.p2.3.m3.2.2.1.1.1.4.cmml"><mo stretchy="false" id="S4.SS1.p2.3.m3.2.2.1.1.1.3.4" xref="S4.SS1.p2.3.m3.2.2.1.1.1.4.cmml">{</mo><msub id="S4.SS1.p2.3.m3.2.2.1.1.1.1.1" xref="S4.SS1.p2.3.m3.2.2.1.1.1.1.1.cmml"><mi id="S4.SS1.p2.3.m3.2.2.1.1.1.1.1.2" xref="S4.SS1.p2.3.m3.2.2.1.1.1.1.1.2.cmml">P</mi><mrow id="S4.SS1.p2.3.m3.2.2.1.1.1.1.1.3" xref="S4.SS1.p2.3.m3.2.2.1.1.1.1.1.3.cmml"><mi id="S4.SS1.p2.3.m3.2.2.1.1.1.1.1.3.2" xref="S4.SS1.p2.3.m3.2.2.1.1.1.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.3.m3.2.2.1.1.1.1.1.3.1" xref="S4.SS1.p2.3.m3.2.2.1.1.1.1.1.3.1.cmml">​</mo><mn id="S4.SS1.p2.3.m3.2.2.1.1.1.1.1.3.3" xref="S4.SS1.p2.3.m3.2.2.1.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S4.SS1.p2.3.m3.2.2.1.1.1.3.5" xref="S4.SS1.p2.3.m3.2.2.1.1.1.4.cmml">,</mo><msub id="S4.SS1.p2.3.m3.2.2.1.1.1.2.2" xref="S4.SS1.p2.3.m3.2.2.1.1.1.2.2.cmml"><mi id="S4.SS1.p2.3.m3.2.2.1.1.1.2.2.2" xref="S4.SS1.p2.3.m3.2.2.1.1.1.2.2.2.cmml">P</mi><mrow id="S4.SS1.p2.3.m3.2.2.1.1.1.2.2.3" xref="S4.SS1.p2.3.m3.2.2.1.1.1.2.2.3.cmml"><mi id="S4.SS1.p2.3.m3.2.2.1.1.1.2.2.3.2" xref="S4.SS1.p2.3.m3.2.2.1.1.1.2.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.3.m3.2.2.1.1.1.2.2.3.1" xref="S4.SS1.p2.3.m3.2.2.1.1.1.2.2.3.1.cmml">​</mo><mn id="S4.SS1.p2.3.m3.2.2.1.1.1.2.2.3.3" xref="S4.SS1.p2.3.m3.2.2.1.1.1.2.2.3.3.cmml">2</mn></mrow></msub><mo id="S4.SS1.p2.3.m3.2.2.1.1.1.3.6" xref="S4.SS1.p2.3.m3.2.2.1.1.1.4.cmml">,</mo><mi mathvariant="normal" id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml">…</mi><mo id="S4.SS1.p2.3.m3.2.2.1.1.1.3.7" xref="S4.SS1.p2.3.m3.2.2.1.1.1.4.cmml">,</mo><msub id="S4.SS1.p2.3.m3.2.2.1.1.1.3.3" xref="S4.SS1.p2.3.m3.2.2.1.1.1.3.3.cmml"><mi id="S4.SS1.p2.3.m3.2.2.1.1.1.3.3.2" xref="S4.SS1.p2.3.m3.2.2.1.1.1.3.3.2.cmml">P</mi><mrow id="S4.SS1.p2.3.m3.2.2.1.1.1.3.3.3" xref="S4.SS1.p2.3.m3.2.2.1.1.1.3.3.3.cmml"><mi id="S4.SS1.p2.3.m3.2.2.1.1.1.3.3.3.2" xref="S4.SS1.p2.3.m3.2.2.1.1.1.3.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.3.m3.2.2.1.1.1.3.3.3.1" xref="S4.SS1.p2.3.m3.2.2.1.1.1.3.3.3.1.cmml">​</mo><mi id="S4.SS1.p2.3.m3.2.2.1.1.1.3.3.3.3" xref="S4.SS1.p2.3.m3.2.2.1.1.1.3.3.3.3.cmml">N</mi></mrow></msub><mo stretchy="false" id="S4.SS1.p2.3.m3.2.2.1.1.1.3.8" xref="S4.SS1.p2.3.m3.2.2.1.1.1.4.cmml">}</mo></mrow><mo id="S4.SS1.p2.3.m3.3.3.2.2.3" xref="S4.SS1.p2.3.m3.3.3.2.3.cmml">,</mo><msub id="S4.SS1.p2.3.m3.3.3.2.2.2" xref="S4.SS1.p2.3.m3.3.3.2.2.2.cmml"><mi id="S4.SS1.p2.3.m3.3.3.2.2.2.2" xref="S4.SS1.p2.3.m3.3.3.2.2.2.2.cmml">P</mi><mrow id="S4.SS1.p2.3.m3.3.3.2.2.2.3" xref="S4.SS1.p2.3.m3.3.3.2.2.2.3.cmml"><mi id="S4.SS1.p2.3.m3.3.3.2.2.2.3.2" xref="S4.SS1.p2.3.m3.3.3.2.2.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.3.m3.3.3.2.2.2.3.1" xref="S4.SS1.p2.3.m3.3.3.2.2.2.3.1.cmml">​</mo><mi id="S4.SS1.p2.3.m3.3.3.2.2.2.3.3" xref="S4.SS1.p2.3.m3.3.3.2.2.2.3.3.cmml">i</mi></mrow></msub></mrow><mo id="S4.SS1.p2.3.m3.3.3.3" xref="S4.SS1.p2.3.m3.3.3.3.cmml">∈</mo><msup id="S4.SS1.p2.3.m3.3.3.4" xref="S4.SS1.p2.3.m3.3.3.4.cmml"><mi id="S4.SS1.p2.3.m3.3.3.4.2" xref="S4.SS1.p2.3.m3.3.3.4.2.cmml">ℝ</mi><mrow id="S4.SS1.p2.3.m3.3.3.4.3" xref="S4.SS1.p2.3.m3.3.3.4.3.cmml"><mi id="S4.SS1.p2.3.m3.3.3.4.3.2" xref="S4.SS1.p2.3.m3.3.3.4.3.2.cmml">K</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p2.3.m3.3.3.4.3.1" xref="S4.SS1.p2.3.m3.3.3.4.3.1.cmml">×</mo><mn id="S4.SS1.p2.3.m3.3.3.4.3.3" xref="S4.SS1.p2.3.m3.3.3.4.3.3.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.3b"><apply id="S4.SS1.p2.3.m3.3.3.cmml" xref="S4.SS1.p2.3.m3.3.3"><in id="S4.SS1.p2.3.m3.3.3.3.cmml" xref="S4.SS1.p2.3.m3.3.3.3"></in><list id="S4.SS1.p2.3.m3.3.3.2.3.cmml" xref="S4.SS1.p2.3.m3.3.3.2.2"><set id="S4.SS1.p2.3.m3.2.2.1.1.1.4.cmml" xref="S4.SS1.p2.3.m3.2.2.1.1.1.3"><apply id="S4.SS1.p2.3.m3.2.2.1.1.1.1.1.cmml" xref="S4.SS1.p2.3.m3.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.3.m3.2.2.1.1.1.1.1.1.cmml" xref="S4.SS1.p2.3.m3.2.2.1.1.1.1.1">subscript</csymbol><ci id="S4.SS1.p2.3.m3.2.2.1.1.1.1.1.2.cmml" xref="S4.SS1.p2.3.m3.2.2.1.1.1.1.1.2">𝑃</ci><apply id="S4.SS1.p2.3.m3.2.2.1.1.1.1.1.3.cmml" xref="S4.SS1.p2.3.m3.2.2.1.1.1.1.1.3"><times id="S4.SS1.p2.3.m3.2.2.1.1.1.1.1.3.1.cmml" xref="S4.SS1.p2.3.m3.2.2.1.1.1.1.1.3.1"></times><ci id="S4.SS1.p2.3.m3.2.2.1.1.1.1.1.3.2.cmml" xref="S4.SS1.p2.3.m3.2.2.1.1.1.1.1.3.2">𝑐</ci><cn type="integer" id="S4.SS1.p2.3.m3.2.2.1.1.1.1.1.3.3.cmml" xref="S4.SS1.p2.3.m3.2.2.1.1.1.1.1.3.3">1</cn></apply></apply><apply id="S4.SS1.p2.3.m3.2.2.1.1.1.2.2.cmml" xref="S4.SS1.p2.3.m3.2.2.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.SS1.p2.3.m3.2.2.1.1.1.2.2.1.cmml" xref="S4.SS1.p2.3.m3.2.2.1.1.1.2.2">subscript</csymbol><ci id="S4.SS1.p2.3.m3.2.2.1.1.1.2.2.2.cmml" xref="S4.SS1.p2.3.m3.2.2.1.1.1.2.2.2">𝑃</ci><apply id="S4.SS1.p2.3.m3.2.2.1.1.1.2.2.3.cmml" xref="S4.SS1.p2.3.m3.2.2.1.1.1.2.2.3"><times id="S4.SS1.p2.3.m3.2.2.1.1.1.2.2.3.1.cmml" xref="S4.SS1.p2.3.m3.2.2.1.1.1.2.2.3.1"></times><ci id="S4.SS1.p2.3.m3.2.2.1.1.1.2.2.3.2.cmml" xref="S4.SS1.p2.3.m3.2.2.1.1.1.2.2.3.2">𝑐</ci><cn type="integer" id="S4.SS1.p2.3.m3.2.2.1.1.1.2.2.3.3.cmml" xref="S4.SS1.p2.3.m3.2.2.1.1.1.2.2.3.3">2</cn></apply></apply><ci id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">…</ci><apply id="S4.SS1.p2.3.m3.2.2.1.1.1.3.3.cmml" xref="S4.SS1.p2.3.m3.2.2.1.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS1.p2.3.m3.2.2.1.1.1.3.3.1.cmml" xref="S4.SS1.p2.3.m3.2.2.1.1.1.3.3">subscript</csymbol><ci id="S4.SS1.p2.3.m3.2.2.1.1.1.3.3.2.cmml" xref="S4.SS1.p2.3.m3.2.2.1.1.1.3.3.2">𝑃</ci><apply id="S4.SS1.p2.3.m3.2.2.1.1.1.3.3.3.cmml" xref="S4.SS1.p2.3.m3.2.2.1.1.1.3.3.3"><times id="S4.SS1.p2.3.m3.2.2.1.1.1.3.3.3.1.cmml" xref="S4.SS1.p2.3.m3.2.2.1.1.1.3.3.3.1"></times><ci id="S4.SS1.p2.3.m3.2.2.1.1.1.3.3.3.2.cmml" xref="S4.SS1.p2.3.m3.2.2.1.1.1.3.3.3.2">𝑐</ci><ci id="S4.SS1.p2.3.m3.2.2.1.1.1.3.3.3.3.cmml" xref="S4.SS1.p2.3.m3.2.2.1.1.1.3.3.3.3">𝑁</ci></apply></apply></set><apply id="S4.SS1.p2.3.m3.3.3.2.2.2.cmml" xref="S4.SS1.p2.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p2.3.m3.3.3.2.2.2.1.cmml" xref="S4.SS1.p2.3.m3.3.3.2.2.2">subscript</csymbol><ci id="S4.SS1.p2.3.m3.3.3.2.2.2.2.cmml" xref="S4.SS1.p2.3.m3.3.3.2.2.2.2">𝑃</ci><apply id="S4.SS1.p2.3.m3.3.3.2.2.2.3.cmml" xref="S4.SS1.p2.3.m3.3.3.2.2.2.3"><times id="S4.SS1.p2.3.m3.3.3.2.2.2.3.1.cmml" xref="S4.SS1.p2.3.m3.3.3.2.2.2.3.1"></times><ci id="S4.SS1.p2.3.m3.3.3.2.2.2.3.2.cmml" xref="S4.SS1.p2.3.m3.3.3.2.2.2.3.2">𝑐</ci><ci id="S4.SS1.p2.3.m3.3.3.2.2.2.3.3.cmml" xref="S4.SS1.p2.3.m3.3.3.2.2.2.3.3">𝑖</ci></apply></apply></list><apply id="S4.SS1.p2.3.m3.3.3.4.cmml" xref="S4.SS1.p2.3.m3.3.3.4"><csymbol cd="ambiguous" id="S4.SS1.p2.3.m3.3.3.4.1.cmml" xref="S4.SS1.p2.3.m3.3.3.4">superscript</csymbol><ci id="S4.SS1.p2.3.m3.3.3.4.2.cmml" xref="S4.SS1.p2.3.m3.3.3.4.2">ℝ</ci><apply id="S4.SS1.p2.3.m3.3.3.4.3.cmml" xref="S4.SS1.p2.3.m3.3.3.4.3"><times id="S4.SS1.p2.3.m3.3.3.4.3.1.cmml" xref="S4.SS1.p2.3.m3.3.3.4.3.1"></times><ci id="S4.SS1.p2.3.m3.3.3.4.3.2.cmml" xref="S4.SS1.p2.3.m3.3.3.4.3.2">𝐾</ci><cn type="integer" id="S4.SS1.p2.3.m3.3.3.4.3.3.cmml" xref="S4.SS1.p2.3.m3.3.3.4.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.3c">\{P_{c1},P_{c2},...,P_{cN}\},P_{ci}\in\mathbb{R}^{K\times 3}</annotation></semantics></math>. The rotations and translations from camera coordinate to world coordinate for each view are <math id="S4.SS1.p2.4.m4.4" class="ltx_Math" alttext="\{R_{1}^{c},R_{2}^{c},...,R_{N}^{c}\}" display="inline"><semantics id="S4.SS1.p2.4.m4.4a"><mrow id="S4.SS1.p2.4.m4.4.4.3" xref="S4.SS1.p2.4.m4.4.4.4.cmml"><mo stretchy="false" id="S4.SS1.p2.4.m4.4.4.3.4" xref="S4.SS1.p2.4.m4.4.4.4.cmml">{</mo><msubsup id="S4.SS1.p2.4.m4.2.2.1.1" xref="S4.SS1.p2.4.m4.2.2.1.1.cmml"><mi id="S4.SS1.p2.4.m4.2.2.1.1.2.2" xref="S4.SS1.p2.4.m4.2.2.1.1.2.2.cmml">R</mi><mn id="S4.SS1.p2.4.m4.2.2.1.1.2.3" xref="S4.SS1.p2.4.m4.2.2.1.1.2.3.cmml">1</mn><mi id="S4.SS1.p2.4.m4.2.2.1.1.3" xref="S4.SS1.p2.4.m4.2.2.1.1.3.cmml">c</mi></msubsup><mo id="S4.SS1.p2.4.m4.4.4.3.5" xref="S4.SS1.p2.4.m4.4.4.4.cmml">,</mo><msubsup id="S4.SS1.p2.4.m4.3.3.2.2" xref="S4.SS1.p2.4.m4.3.3.2.2.cmml"><mi id="S4.SS1.p2.4.m4.3.3.2.2.2.2" xref="S4.SS1.p2.4.m4.3.3.2.2.2.2.cmml">R</mi><mn id="S4.SS1.p2.4.m4.3.3.2.2.2.3" xref="S4.SS1.p2.4.m4.3.3.2.2.2.3.cmml">2</mn><mi id="S4.SS1.p2.4.m4.3.3.2.2.3" xref="S4.SS1.p2.4.m4.3.3.2.2.3.cmml">c</mi></msubsup><mo id="S4.SS1.p2.4.m4.4.4.3.6" xref="S4.SS1.p2.4.m4.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml">…</mi><mo id="S4.SS1.p2.4.m4.4.4.3.7" xref="S4.SS1.p2.4.m4.4.4.4.cmml">,</mo><msubsup id="S4.SS1.p2.4.m4.4.4.3.3" xref="S4.SS1.p2.4.m4.4.4.3.3.cmml"><mi id="S4.SS1.p2.4.m4.4.4.3.3.2.2" xref="S4.SS1.p2.4.m4.4.4.3.3.2.2.cmml">R</mi><mi id="S4.SS1.p2.4.m4.4.4.3.3.2.3" xref="S4.SS1.p2.4.m4.4.4.3.3.2.3.cmml">N</mi><mi id="S4.SS1.p2.4.m4.4.4.3.3.3" xref="S4.SS1.p2.4.m4.4.4.3.3.3.cmml">c</mi></msubsup><mo stretchy="false" id="S4.SS1.p2.4.m4.4.4.3.8" xref="S4.SS1.p2.4.m4.4.4.4.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.4b"><set id="S4.SS1.p2.4.m4.4.4.4.cmml" xref="S4.SS1.p2.4.m4.4.4.3"><apply id="S4.SS1.p2.4.m4.2.2.1.1.cmml" xref="S4.SS1.p2.4.m4.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.4.m4.2.2.1.1.1.cmml" xref="S4.SS1.p2.4.m4.2.2.1.1">superscript</csymbol><apply id="S4.SS1.p2.4.m4.2.2.1.1.2.cmml" xref="S4.SS1.p2.4.m4.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.4.m4.2.2.1.1.2.1.cmml" xref="S4.SS1.p2.4.m4.2.2.1.1">subscript</csymbol><ci id="S4.SS1.p2.4.m4.2.2.1.1.2.2.cmml" xref="S4.SS1.p2.4.m4.2.2.1.1.2.2">𝑅</ci><cn type="integer" id="S4.SS1.p2.4.m4.2.2.1.1.2.3.cmml" xref="S4.SS1.p2.4.m4.2.2.1.1.2.3">1</cn></apply><ci id="S4.SS1.p2.4.m4.2.2.1.1.3.cmml" xref="S4.SS1.p2.4.m4.2.2.1.1.3">𝑐</ci></apply><apply id="S4.SS1.p2.4.m4.3.3.2.2.cmml" xref="S4.SS1.p2.4.m4.3.3.2.2"><csymbol cd="ambiguous" id="S4.SS1.p2.4.m4.3.3.2.2.1.cmml" xref="S4.SS1.p2.4.m4.3.3.2.2">superscript</csymbol><apply id="S4.SS1.p2.4.m4.3.3.2.2.2.cmml" xref="S4.SS1.p2.4.m4.3.3.2.2"><csymbol cd="ambiguous" id="S4.SS1.p2.4.m4.3.3.2.2.2.1.cmml" xref="S4.SS1.p2.4.m4.3.3.2.2">subscript</csymbol><ci id="S4.SS1.p2.4.m4.3.3.2.2.2.2.cmml" xref="S4.SS1.p2.4.m4.3.3.2.2.2.2">𝑅</ci><cn type="integer" id="S4.SS1.p2.4.m4.3.3.2.2.2.3.cmml" xref="S4.SS1.p2.4.m4.3.3.2.2.2.3">2</cn></apply><ci id="S4.SS1.p2.4.m4.3.3.2.2.3.cmml" xref="S4.SS1.p2.4.m4.3.3.2.2.3">𝑐</ci></apply><ci id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1">…</ci><apply id="S4.SS1.p2.4.m4.4.4.3.3.cmml" xref="S4.SS1.p2.4.m4.4.4.3.3"><csymbol cd="ambiguous" id="S4.SS1.p2.4.m4.4.4.3.3.1.cmml" xref="S4.SS1.p2.4.m4.4.4.3.3">superscript</csymbol><apply id="S4.SS1.p2.4.m4.4.4.3.3.2.cmml" xref="S4.SS1.p2.4.m4.4.4.3.3"><csymbol cd="ambiguous" id="S4.SS1.p2.4.m4.4.4.3.3.2.1.cmml" xref="S4.SS1.p2.4.m4.4.4.3.3">subscript</csymbol><ci id="S4.SS1.p2.4.m4.4.4.3.3.2.2.cmml" xref="S4.SS1.p2.4.m4.4.4.3.3.2.2">𝑅</ci><ci id="S4.SS1.p2.4.m4.4.4.3.3.2.3.cmml" xref="S4.SS1.p2.4.m4.4.4.3.3.2.3">𝑁</ci></apply><ci id="S4.SS1.p2.4.m4.4.4.3.3.3.cmml" xref="S4.SS1.p2.4.m4.4.4.3.3.3">𝑐</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.4c">\{R_{1}^{c},R_{2}^{c},...,R_{N}^{c}\}</annotation></semantics></math> and <math id="S4.SS1.p2.5.m5.4" class="ltx_Math" alttext="\{T_{1}^{c},T_{2}^{c},...,T_{N}^{c}\}" display="inline"><semantics id="S4.SS1.p2.5.m5.4a"><mrow id="S4.SS1.p2.5.m5.4.4.3" xref="S4.SS1.p2.5.m5.4.4.4.cmml"><mo stretchy="false" id="S4.SS1.p2.5.m5.4.4.3.4" xref="S4.SS1.p2.5.m5.4.4.4.cmml">{</mo><msubsup id="S4.SS1.p2.5.m5.2.2.1.1" xref="S4.SS1.p2.5.m5.2.2.1.1.cmml"><mi id="S4.SS1.p2.5.m5.2.2.1.1.2.2" xref="S4.SS1.p2.5.m5.2.2.1.1.2.2.cmml">T</mi><mn id="S4.SS1.p2.5.m5.2.2.1.1.2.3" xref="S4.SS1.p2.5.m5.2.2.1.1.2.3.cmml">1</mn><mi id="S4.SS1.p2.5.m5.2.2.1.1.3" xref="S4.SS1.p2.5.m5.2.2.1.1.3.cmml">c</mi></msubsup><mo id="S4.SS1.p2.5.m5.4.4.3.5" xref="S4.SS1.p2.5.m5.4.4.4.cmml">,</mo><msubsup id="S4.SS1.p2.5.m5.3.3.2.2" xref="S4.SS1.p2.5.m5.3.3.2.2.cmml"><mi id="S4.SS1.p2.5.m5.3.3.2.2.2.2" xref="S4.SS1.p2.5.m5.3.3.2.2.2.2.cmml">T</mi><mn id="S4.SS1.p2.5.m5.3.3.2.2.2.3" xref="S4.SS1.p2.5.m5.3.3.2.2.2.3.cmml">2</mn><mi id="S4.SS1.p2.5.m5.3.3.2.2.3" xref="S4.SS1.p2.5.m5.3.3.2.2.3.cmml">c</mi></msubsup><mo id="S4.SS1.p2.5.m5.4.4.3.6" xref="S4.SS1.p2.5.m5.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S4.SS1.p2.5.m5.1.1" xref="S4.SS1.p2.5.m5.1.1.cmml">…</mi><mo id="S4.SS1.p2.5.m5.4.4.3.7" xref="S4.SS1.p2.5.m5.4.4.4.cmml">,</mo><msubsup id="S4.SS1.p2.5.m5.4.4.3.3" xref="S4.SS1.p2.5.m5.4.4.3.3.cmml"><mi id="S4.SS1.p2.5.m5.4.4.3.3.2.2" xref="S4.SS1.p2.5.m5.4.4.3.3.2.2.cmml">T</mi><mi id="S4.SS1.p2.5.m5.4.4.3.3.2.3" xref="S4.SS1.p2.5.m5.4.4.3.3.2.3.cmml">N</mi><mi id="S4.SS1.p2.5.m5.4.4.3.3.3" xref="S4.SS1.p2.5.m5.4.4.3.3.3.cmml">c</mi></msubsup><mo stretchy="false" id="S4.SS1.p2.5.m5.4.4.3.8" xref="S4.SS1.p2.5.m5.4.4.4.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.5.m5.4b"><set id="S4.SS1.p2.5.m5.4.4.4.cmml" xref="S4.SS1.p2.5.m5.4.4.3"><apply id="S4.SS1.p2.5.m5.2.2.1.1.cmml" xref="S4.SS1.p2.5.m5.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.5.m5.2.2.1.1.1.cmml" xref="S4.SS1.p2.5.m5.2.2.1.1">superscript</csymbol><apply id="S4.SS1.p2.5.m5.2.2.1.1.2.cmml" xref="S4.SS1.p2.5.m5.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.5.m5.2.2.1.1.2.1.cmml" xref="S4.SS1.p2.5.m5.2.2.1.1">subscript</csymbol><ci id="S4.SS1.p2.5.m5.2.2.1.1.2.2.cmml" xref="S4.SS1.p2.5.m5.2.2.1.1.2.2">𝑇</ci><cn type="integer" id="S4.SS1.p2.5.m5.2.2.1.1.2.3.cmml" xref="S4.SS1.p2.5.m5.2.2.1.1.2.3">1</cn></apply><ci id="S4.SS1.p2.5.m5.2.2.1.1.3.cmml" xref="S4.SS1.p2.5.m5.2.2.1.1.3">𝑐</ci></apply><apply id="S4.SS1.p2.5.m5.3.3.2.2.cmml" xref="S4.SS1.p2.5.m5.3.3.2.2"><csymbol cd="ambiguous" id="S4.SS1.p2.5.m5.3.3.2.2.1.cmml" xref="S4.SS1.p2.5.m5.3.3.2.2">superscript</csymbol><apply id="S4.SS1.p2.5.m5.3.3.2.2.2.cmml" xref="S4.SS1.p2.5.m5.3.3.2.2"><csymbol cd="ambiguous" id="S4.SS1.p2.5.m5.3.3.2.2.2.1.cmml" xref="S4.SS1.p2.5.m5.3.3.2.2">subscript</csymbol><ci id="S4.SS1.p2.5.m5.3.3.2.2.2.2.cmml" xref="S4.SS1.p2.5.m5.3.3.2.2.2.2">𝑇</ci><cn type="integer" id="S4.SS1.p2.5.m5.3.3.2.2.2.3.cmml" xref="S4.SS1.p2.5.m5.3.3.2.2.2.3">2</cn></apply><ci id="S4.SS1.p2.5.m5.3.3.2.2.3.cmml" xref="S4.SS1.p2.5.m5.3.3.2.2.3">𝑐</ci></apply><ci id="S4.SS1.p2.5.m5.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1">…</ci><apply id="S4.SS1.p2.5.m5.4.4.3.3.cmml" xref="S4.SS1.p2.5.m5.4.4.3.3"><csymbol cd="ambiguous" id="S4.SS1.p2.5.m5.4.4.3.3.1.cmml" xref="S4.SS1.p2.5.m5.4.4.3.3">superscript</csymbol><apply id="S4.SS1.p2.5.m5.4.4.3.3.2.cmml" xref="S4.SS1.p2.5.m5.4.4.3.3"><csymbol cd="ambiguous" id="S4.SS1.p2.5.m5.4.4.3.3.2.1.cmml" xref="S4.SS1.p2.5.m5.4.4.3.3">subscript</csymbol><ci id="S4.SS1.p2.5.m5.4.4.3.3.2.2.cmml" xref="S4.SS1.p2.5.m5.4.4.3.3.2.2">𝑇</ci><ci id="S4.SS1.p2.5.m5.4.4.3.3.2.3.cmml" xref="S4.SS1.p2.5.m5.4.4.3.3.2.3">𝑁</ci></apply><ci id="S4.SS1.p2.5.m5.4.4.3.3.3.cmml" xref="S4.SS1.p2.5.m5.4.4.3.3.3">𝑐</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.5.m5.4c">\{T_{1}^{c},T_{2}^{c},...,T_{N}^{c}\}</annotation></semantics></math>.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.3" class="ltx_p"><span id="S4.SS1.p3.3.1" class="ltx_text ltx_font_bold">Extract scene proxies:</span>
Previous 3D detection methods are only able to process point clouds as a whole. So a natural solution for consuming multi-view RGB-D images is to fuse the depth maps into a whole scene <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="P=\mathcal{C}(R_{i}^{c}\cdot P_{ci}+T_{i}^{c})" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mrow id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mi id="S4.SS1.p3.1.m1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.3.cmml">P</mi><mo id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2.cmml">=</mo><mrow id="S4.SS1.p3.1.m1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p3.1.m1.1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.1.3.cmml">𝒞</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p3.1.m1.1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.1.2.cmml">​</mo><mrow id="S4.SS1.p3.1.m1.1.1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS1.p3.1.m1.1.1.1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS1.p3.1.m1.1.1.1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.cmml"><mrow id="S4.SS1.p3.1.m1.1.1.1.1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.cmml"><msubsup id="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.2" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.2.cmml"><mi id="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.2.2.2" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.2.2.2.cmml">R</mi><mi id="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.2.2.3" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.2.2.3.cmml">i</mi><mi id="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.2.3" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.2.3.cmml">c</mi></msubsup><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.1" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.1.cmml">⋅</mo><msub id="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.3" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.3.2" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.3.2.cmml">P</mi><mrow id="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.3.3" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.3.3.cmml"><mi id="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.3.3.2" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.3.3.1" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.3.3.1.cmml">​</mo><mi id="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.3.3.3" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.3.3.3.cmml">i</mi></mrow></msub></mrow><mo id="S4.SS1.p3.1.m1.1.1.1.1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.1.cmml">+</mo><msubsup id="S4.SS1.p3.1.m1.1.1.1.1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.3.cmml"><mi id="S4.SS1.p3.1.m1.1.1.1.1.1.1.3.2.2" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.3.2.2.cmml">T</mi><mi id="S4.SS1.p3.1.m1.1.1.1.1.1.1.3.2.3" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.3.2.3.cmml">i</mi><mi id="S4.SS1.p3.1.m1.1.1.1.1.1.1.3.3" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.3.3.cmml">c</mi></msubsup></mrow><mo stretchy="false" id="S4.SS1.p3.1.m1.1.1.1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><eq id="S4.SS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2"></eq><ci id="S4.SS1.p3.1.m1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3">𝑃</ci><apply id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1"><times id="S4.SS1.p3.1.m1.1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.1.2"></times><ci id="S4.SS1.p3.1.m1.1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.1.1.1.3">𝒞</ci><apply id="S4.SS1.p3.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1"><plus id="S4.SS1.p3.1.m1.1.1.1.1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.1"></plus><apply id="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.2"><ci id="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.1">⋅</ci><apply id="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.2.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.2">superscript</csymbol><apply id="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.2.2.2">𝑅</ci><ci id="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.2.2.3">𝑖</ci></apply><ci id="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.2.3">𝑐</ci></apply><apply id="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.3.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.3.2">𝑃</ci><apply id="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.3.3"><times id="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.3.3.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.3.3.1"></times><ci id="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.3.3.2.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.3.3.2">𝑐</ci><ci id="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.3.3.3.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.2.3.3.3">𝑖</ci></apply></apply></apply><apply id="S4.SS1.p3.1.m1.1.1.1.1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.1.1.1.1.1.1.3.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S4.SS1.p3.1.m1.1.1.1.1.1.1.3.2.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.1.1.1.1.1.1.3.2.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.SS1.p3.1.m1.1.1.1.1.1.1.3.2.2.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.3.2.2">𝑇</ci><ci id="S4.SS1.p3.1.m1.1.1.1.1.1.1.3.2.3.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.3.2.3">𝑖</ci></apply><ci id="S4.SS1.p3.1.m1.1.1.1.1.1.1.3.3.cmml" xref="S4.SS1.p3.1.m1.1.1.1.1.1.1.3.3">𝑐</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">P=\mathcal{C}(R_{i}^{c}\cdot P_{ci}+T_{i}^{c})</annotation></semantics></math>, where <math id="S4.SS1.p3.2.m2.1" class="ltx_Math" alttext="\mathcal{C}" display="inline"><semantics id="S4.SS1.p3.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><ci id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">\mathcal{C}</annotation></semantics></math> denotes concatenation operation.
However, point clouds generated from different numbers of views vary a lot in local geometry structure and global semantics, which will lead to deteriorated scene representation when numbers of view for training and inference are not the same. To decouple scene representation with the scale of input data, we propose to extract <math id="S4.SS1.p3.3.m3.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S4.SS1.p3.3.m3.1a"><mi id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.1b"><ci id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.1c">T</annotation></semantics></math> scene proxies for each view independently with a shared geometry learner:</p>
<table id="S4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E2.m1.2" class="ltx_Math" alttext="\mathcal{P}_{i}=G(P_{ci}),\mathcal{P}_{i}\in\mathbb{R}^{T\times C}" display="block"><semantics id="S4.E2.m1.2a"><mrow id="S4.E2.m1.2.2.2" xref="S4.E2.m1.2.2.3.cmml"><mrow id="S4.E2.m1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml"><msub id="S4.E2.m1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.1.1.1.1.3.2" xref="S4.E2.m1.1.1.1.1.3.2.cmml">𝒫</mi><mi id="S4.E2.m1.1.1.1.1.3.3" xref="S4.E2.m1.1.1.1.1.3.3.cmml">i</mi></msub><mo id="S4.E2.m1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.2.cmml">=</mo><mrow id="S4.E2.m1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.cmml"><mi id="S4.E2.m1.1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.1.3.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S4.E2.m1.1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E2.m1.1.1.1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S4.E2.m1.1.1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E2.m1.1.1.1.1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.1.1.1.1.2.cmml">P</mi><mrow id="S4.E2.m1.1.1.1.1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E2.m1.1.1.1.1.1.1.1.1.3.2" xref="S4.E2.m1.1.1.1.1.1.1.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.1.1.1.1.3.1" xref="S4.E2.m1.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S4.E2.m1.1.1.1.1.1.1.1.1.3.3" xref="S4.E2.m1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></mrow></msub><mo stretchy="false" id="S4.E2.m1.1.1.1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E2.m1.2.2.2.3" xref="S4.E2.m1.2.2.3a.cmml">,</mo><mrow id="S4.E2.m1.2.2.2.2" xref="S4.E2.m1.2.2.2.2.cmml"><msub id="S4.E2.m1.2.2.2.2.2" xref="S4.E2.m1.2.2.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.2.2.2.2.2.2" xref="S4.E2.m1.2.2.2.2.2.2.cmml">𝒫</mi><mi id="S4.E2.m1.2.2.2.2.2.3" xref="S4.E2.m1.2.2.2.2.2.3.cmml">i</mi></msub><mo id="S4.E2.m1.2.2.2.2.1" xref="S4.E2.m1.2.2.2.2.1.cmml">∈</mo><msup id="S4.E2.m1.2.2.2.2.3" xref="S4.E2.m1.2.2.2.2.3.cmml"><mi id="S4.E2.m1.2.2.2.2.3.2" xref="S4.E2.m1.2.2.2.2.3.2.cmml">ℝ</mi><mrow id="S4.E2.m1.2.2.2.2.3.3" xref="S4.E2.m1.2.2.2.2.3.3.cmml"><mi id="S4.E2.m1.2.2.2.2.3.3.2" xref="S4.E2.m1.2.2.2.2.3.3.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S4.E2.m1.2.2.2.2.3.3.1" xref="S4.E2.m1.2.2.2.2.3.3.1.cmml">×</mo><mi id="S4.E2.m1.2.2.2.2.3.3.3" xref="S4.E2.m1.2.2.2.2.3.3.3.cmml">C</mi></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.2b"><apply id="S4.E2.m1.2.2.3.cmml" xref="S4.E2.m1.2.2.2"><csymbol cd="ambiguous" id="S4.E2.m1.2.2.3a.cmml" xref="S4.E2.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S4.E2.m1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1"><eq id="S4.E2.m1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.2"></eq><apply id="S4.E2.m1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.2">𝒫</ci><ci id="S4.E2.m1.1.1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.1.1.3.3">𝑖</ci></apply><apply id="S4.E2.m1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1"><times id="S4.E2.m1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.1.2"></times><ci id="S4.E2.m1.1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.1.3">𝐺</ci><apply id="S4.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.2">𝑃</ci><apply id="S4.E2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.3"><times id="S4.E2.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.3.1"></times><ci id="S4.E2.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.3.2">𝑐</ci><ci id="S4.E2.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply></apply><apply id="S4.E2.m1.2.2.2.2.cmml" xref="S4.E2.m1.2.2.2.2"><in id="S4.E2.m1.2.2.2.2.1.cmml" xref="S4.E2.m1.2.2.2.2.1"></in><apply id="S4.E2.m1.2.2.2.2.2.cmml" xref="S4.E2.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.E2.m1.2.2.2.2.2.1.cmml" xref="S4.E2.m1.2.2.2.2.2">subscript</csymbol><ci id="S4.E2.m1.2.2.2.2.2.2.cmml" xref="S4.E2.m1.2.2.2.2.2.2">𝒫</ci><ci id="S4.E2.m1.2.2.2.2.2.3.cmml" xref="S4.E2.m1.2.2.2.2.2.3">𝑖</ci></apply><apply id="S4.E2.m1.2.2.2.2.3.cmml" xref="S4.E2.m1.2.2.2.2.3"><csymbol cd="ambiguous" id="S4.E2.m1.2.2.2.2.3.1.cmml" xref="S4.E2.m1.2.2.2.2.3">superscript</csymbol><ci id="S4.E2.m1.2.2.2.2.3.2.cmml" xref="S4.E2.m1.2.2.2.2.3.2">ℝ</ci><apply id="S4.E2.m1.2.2.2.2.3.3.cmml" xref="S4.E2.m1.2.2.2.2.3.3"><times id="S4.E2.m1.2.2.2.2.3.3.1.cmml" xref="S4.E2.m1.2.2.2.2.3.3.1"></times><ci id="S4.E2.m1.2.2.2.2.3.3.2.cmml" xref="S4.E2.m1.2.2.2.2.3.3.2">𝑇</ci><ci id="S4.E2.m1.2.2.2.2.3.3.3.cmml" xref="S4.E2.m1.2.2.2.2.3.3.3">𝐶</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.2c">\mathcal{P}_{i}=G(P_{ci}),\mathcal{P}_{i}\in\mathbb{R}^{T\times C}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S4.SS1.p3.6" class="ltx_p">Scene proxies <math id="S4.SS1.p3.4.m1.1" class="ltx_Math" alttext="\mathcal{P}_{i}" display="inline"><semantics id="S4.SS1.p3.4.m1.1a"><msub id="S4.SS1.p3.4.m1.1.1" xref="S4.SS1.p3.4.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p3.4.m1.1.1.2" xref="S4.SS1.p3.4.m1.1.1.2.cmml">𝒫</mi><mi id="S4.SS1.p3.4.m1.1.1.3" xref="S4.SS1.p3.4.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.4.m1.1b"><apply id="S4.SS1.p3.4.m1.1.1.cmml" xref="S4.SS1.p3.4.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.4.m1.1.1.1.cmml" xref="S4.SS1.p3.4.m1.1.1">subscript</csymbol><ci id="S4.SS1.p3.4.m1.1.1.2.cmml" xref="S4.SS1.p3.4.m1.1.1.2">𝒫</ci><ci id="S4.SS1.p3.4.m1.1.1.3.cmml" xref="S4.SS1.p3.4.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.4.m1.1c">\mathcal{P}_{i}</annotation></semantics></math> are local feature descriptors which represent the geometry structures of single view point clouds.
We implement the geometry learner <math id="S4.SS1.p3.5.m2.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S4.SS1.p3.5.m2.1a"><mi id="S4.SS1.p3.5.m2.1.1" xref="S4.SS1.p3.5.m2.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.5.m2.1b"><ci id="S4.SS1.p3.5.m2.1.1.cmml" xref="S4.SS1.p3.5.m2.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.5.m2.1c">G</annotation></semantics></math> as two set abstraction (SA) layers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, the first layer with a constrained receptive field to focus on local geometric details and the second with a large receptive field to aggregate the local details into geometry structures. As <math id="S4.SS1.p3.6.m3.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S4.SS1.p3.6.m3.1a"><mi id="S4.SS1.p3.6.m3.1.1" xref="S4.SS1.p3.6.m3.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.6.m3.1b"><ci id="S4.SS1.p3.6.m3.1.1.cmml" xref="S4.SS1.p3.6.m3.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.6.m3.1c">G</annotation></semantics></math> is applied on each view independently, the extracted scene proxies will ignore global semantics and be robust to the scales of input data.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.3" class="ltx_p"><span id="S4.SS1.p4.3.1" class="ltx_text ltx_font_bold">Interactions among scene proxies:</span>
We obtain richer scene representations through interactions among scene proxies. In order to keep the input scale-independent property of scene proxies, we hope the interaction to be linear combination <math id="S4.SS1.p4.1.m1.1" class="ltx_Math" alttext="\mathcal{P}_{ij}=\sum_{i=1}^{N}\sum_{j=1}^{T}{\alpha_{ij}\mathcal{P}_{ij}}" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><mrow id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml"><msub id="S4.SS1.p4.1.m1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p4.1.m1.1.1.2.2" xref="S4.SS1.p4.1.m1.1.1.2.2.cmml">𝒫</mi><mrow id="S4.SS1.p4.1.m1.1.1.2.3" xref="S4.SS1.p4.1.m1.1.1.2.3.cmml"><mi id="S4.SS1.p4.1.m1.1.1.2.3.2" xref="S4.SS1.p4.1.m1.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.1.m1.1.1.2.3.1" xref="S4.SS1.p4.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S4.SS1.p4.1.m1.1.1.2.3.3" xref="S4.SS1.p4.1.m1.1.1.2.3.3.cmml">j</mi></mrow></msub><mo rspace="0.111em" id="S4.SS1.p4.1.m1.1.1.1" xref="S4.SS1.p4.1.m1.1.1.1.cmml">=</mo><mrow id="S4.SS1.p4.1.m1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.3.cmml"><msubsup id="S4.SS1.p4.1.m1.1.1.3.1" xref="S4.SS1.p4.1.m1.1.1.3.1.cmml"><mo rspace="0em" id="S4.SS1.p4.1.m1.1.1.3.1.2.2" xref="S4.SS1.p4.1.m1.1.1.3.1.2.2.cmml">∑</mo><mrow id="S4.SS1.p4.1.m1.1.1.3.1.2.3" xref="S4.SS1.p4.1.m1.1.1.3.1.2.3.cmml"><mi id="S4.SS1.p4.1.m1.1.1.3.1.2.3.2" xref="S4.SS1.p4.1.m1.1.1.3.1.2.3.2.cmml">i</mi><mo id="S4.SS1.p4.1.m1.1.1.3.1.2.3.1" xref="S4.SS1.p4.1.m1.1.1.3.1.2.3.1.cmml">=</mo><mn id="S4.SS1.p4.1.m1.1.1.3.1.2.3.3" xref="S4.SS1.p4.1.m1.1.1.3.1.2.3.3.cmml">1</mn></mrow><mi id="S4.SS1.p4.1.m1.1.1.3.1.3" xref="S4.SS1.p4.1.m1.1.1.3.1.3.cmml">N</mi></msubsup><mrow id="S4.SS1.p4.1.m1.1.1.3.2" xref="S4.SS1.p4.1.m1.1.1.3.2.cmml"><msubsup id="S4.SS1.p4.1.m1.1.1.3.2.1" xref="S4.SS1.p4.1.m1.1.1.3.2.1.cmml"><mo id="S4.SS1.p4.1.m1.1.1.3.2.1.2.2" xref="S4.SS1.p4.1.m1.1.1.3.2.1.2.2.cmml">∑</mo><mrow id="S4.SS1.p4.1.m1.1.1.3.2.1.2.3" xref="S4.SS1.p4.1.m1.1.1.3.2.1.2.3.cmml"><mi id="S4.SS1.p4.1.m1.1.1.3.2.1.2.3.2" xref="S4.SS1.p4.1.m1.1.1.3.2.1.2.3.2.cmml">j</mi><mo id="S4.SS1.p4.1.m1.1.1.3.2.1.2.3.1" xref="S4.SS1.p4.1.m1.1.1.3.2.1.2.3.1.cmml">=</mo><mn id="S4.SS1.p4.1.m1.1.1.3.2.1.2.3.3" xref="S4.SS1.p4.1.m1.1.1.3.2.1.2.3.3.cmml">1</mn></mrow><mi id="S4.SS1.p4.1.m1.1.1.3.2.1.3" xref="S4.SS1.p4.1.m1.1.1.3.2.1.3.cmml">T</mi></msubsup><mrow id="S4.SS1.p4.1.m1.1.1.3.2.2" xref="S4.SS1.p4.1.m1.1.1.3.2.2.cmml"><msub id="S4.SS1.p4.1.m1.1.1.3.2.2.2" xref="S4.SS1.p4.1.m1.1.1.3.2.2.2.cmml"><mi id="S4.SS1.p4.1.m1.1.1.3.2.2.2.2" xref="S4.SS1.p4.1.m1.1.1.3.2.2.2.2.cmml">α</mi><mrow id="S4.SS1.p4.1.m1.1.1.3.2.2.2.3" xref="S4.SS1.p4.1.m1.1.1.3.2.2.2.3.cmml"><mi id="S4.SS1.p4.1.m1.1.1.3.2.2.2.3.2" xref="S4.SS1.p4.1.m1.1.1.3.2.2.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.1.m1.1.1.3.2.2.2.3.1" xref="S4.SS1.p4.1.m1.1.1.3.2.2.2.3.1.cmml">​</mo><mi id="S4.SS1.p4.1.m1.1.1.3.2.2.2.3.3" xref="S4.SS1.p4.1.m1.1.1.3.2.2.2.3.3.cmml">j</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S4.SS1.p4.1.m1.1.1.3.2.2.1" xref="S4.SS1.p4.1.m1.1.1.3.2.2.1.cmml">​</mo><msub id="S4.SS1.p4.1.m1.1.1.3.2.2.3" xref="S4.SS1.p4.1.m1.1.1.3.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p4.1.m1.1.1.3.2.2.3.2" xref="S4.SS1.p4.1.m1.1.1.3.2.2.3.2.cmml">𝒫</mi><mrow id="S4.SS1.p4.1.m1.1.1.3.2.2.3.3" xref="S4.SS1.p4.1.m1.1.1.3.2.2.3.3.cmml"><mi id="S4.SS1.p4.1.m1.1.1.3.2.2.3.3.2" xref="S4.SS1.p4.1.m1.1.1.3.2.2.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.1.m1.1.1.3.2.2.3.3.1" xref="S4.SS1.p4.1.m1.1.1.3.2.2.3.3.1.cmml">​</mo><mi id="S4.SS1.p4.1.m1.1.1.3.2.2.3.3.3" xref="S4.SS1.p4.1.m1.1.1.3.2.2.3.3.3.cmml">j</mi></mrow></msub></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><apply id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"><eq id="S4.SS1.p4.1.m1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1.1"></eq><apply id="S4.SS1.p4.1.m1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p4.1.m1.1.1.2.1.cmml" xref="S4.SS1.p4.1.m1.1.1.2">subscript</csymbol><ci id="S4.SS1.p4.1.m1.1.1.2.2.cmml" xref="S4.SS1.p4.1.m1.1.1.2.2">𝒫</ci><apply id="S4.SS1.p4.1.m1.1.1.2.3.cmml" xref="S4.SS1.p4.1.m1.1.1.2.3"><times id="S4.SS1.p4.1.m1.1.1.2.3.1.cmml" xref="S4.SS1.p4.1.m1.1.1.2.3.1"></times><ci id="S4.SS1.p4.1.m1.1.1.2.3.2.cmml" xref="S4.SS1.p4.1.m1.1.1.2.3.2">𝑖</ci><ci id="S4.SS1.p4.1.m1.1.1.2.3.3.cmml" xref="S4.SS1.p4.1.m1.1.1.2.3.3">𝑗</ci></apply></apply><apply id="S4.SS1.p4.1.m1.1.1.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3"><apply id="S4.SS1.p4.1.m1.1.1.3.1.cmml" xref="S4.SS1.p4.1.m1.1.1.3.1"><csymbol cd="ambiguous" id="S4.SS1.p4.1.m1.1.1.3.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1.3.1">superscript</csymbol><apply id="S4.SS1.p4.1.m1.1.1.3.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.3.1"><csymbol cd="ambiguous" id="S4.SS1.p4.1.m1.1.1.3.1.2.1.cmml" xref="S4.SS1.p4.1.m1.1.1.3.1">subscript</csymbol><sum id="S4.SS1.p4.1.m1.1.1.3.1.2.2.cmml" xref="S4.SS1.p4.1.m1.1.1.3.1.2.2"></sum><apply id="S4.SS1.p4.1.m1.1.1.3.1.2.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3.1.2.3"><eq id="S4.SS1.p4.1.m1.1.1.3.1.2.3.1.cmml" xref="S4.SS1.p4.1.m1.1.1.3.1.2.3.1"></eq><ci id="S4.SS1.p4.1.m1.1.1.3.1.2.3.2.cmml" xref="S4.SS1.p4.1.m1.1.1.3.1.2.3.2">𝑖</ci><cn type="integer" id="S4.SS1.p4.1.m1.1.1.3.1.2.3.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3.1.2.3.3">1</cn></apply></apply><ci id="S4.SS1.p4.1.m1.1.1.3.1.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3.1.3">𝑁</ci></apply><apply id="S4.SS1.p4.1.m1.1.1.3.2.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2"><apply id="S4.SS1.p4.1.m1.1.1.3.2.1.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2.1"><csymbol cd="ambiguous" id="S4.SS1.p4.1.m1.1.1.3.2.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2.1">superscript</csymbol><apply id="S4.SS1.p4.1.m1.1.1.3.2.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2.1"><csymbol cd="ambiguous" id="S4.SS1.p4.1.m1.1.1.3.2.1.2.1.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2.1">subscript</csymbol><sum id="S4.SS1.p4.1.m1.1.1.3.2.1.2.2.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2.1.2.2"></sum><apply id="S4.SS1.p4.1.m1.1.1.3.2.1.2.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2.1.2.3"><eq id="S4.SS1.p4.1.m1.1.1.3.2.1.2.3.1.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2.1.2.3.1"></eq><ci id="S4.SS1.p4.1.m1.1.1.3.2.1.2.3.2.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2.1.2.3.2">𝑗</ci><cn type="integer" id="S4.SS1.p4.1.m1.1.1.3.2.1.2.3.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2.1.2.3.3">1</cn></apply></apply><ci id="S4.SS1.p4.1.m1.1.1.3.2.1.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2.1.3">𝑇</ci></apply><apply id="S4.SS1.p4.1.m1.1.1.3.2.2.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2.2"><times id="S4.SS1.p4.1.m1.1.1.3.2.2.1.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2.2.1"></times><apply id="S4.SS1.p4.1.m1.1.1.3.2.2.2.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p4.1.m1.1.1.3.2.2.2.1.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2.2.2">subscript</csymbol><ci id="S4.SS1.p4.1.m1.1.1.3.2.2.2.2.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2.2.2.2">𝛼</ci><apply id="S4.SS1.p4.1.m1.1.1.3.2.2.2.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2.2.2.3"><times id="S4.SS1.p4.1.m1.1.1.3.2.2.2.3.1.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2.2.2.3.1"></times><ci id="S4.SS1.p4.1.m1.1.1.3.2.2.2.3.2.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2.2.2.3.2">𝑖</ci><ci id="S4.SS1.p4.1.m1.1.1.3.2.2.2.3.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2.2.2.3.3">𝑗</ci></apply></apply><apply id="S4.SS1.p4.1.m1.1.1.3.2.2.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2.2.3"><csymbol cd="ambiguous" id="S4.SS1.p4.1.m1.1.1.3.2.2.3.1.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2.2.3">subscript</csymbol><ci id="S4.SS1.p4.1.m1.1.1.3.2.2.3.2.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2.2.3.2">𝒫</ci><apply id="S4.SS1.p4.1.m1.1.1.3.2.2.3.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2.2.3.3"><times id="S4.SS1.p4.1.m1.1.1.3.2.2.3.3.1.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2.2.3.3.1"></times><ci id="S4.SS1.p4.1.m1.1.1.3.2.2.3.3.2.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2.2.3.3.2">𝑖</ci><ci id="S4.SS1.p4.1.m1.1.1.3.2.2.3.3.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2.2.3.3.3">𝑗</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">\mathcal{P}_{ij}=\sum_{i=1}^{N}\sum_{j=1}^{T}{\alpha_{ij}\mathcal{P}_{ij}}</annotation></semantics></math>, where <math id="S4.SS1.p4.2.m2.1" class="ltx_Math" alttext="\sum_{i}\sum_{j}{\alpha_{ij}}=1" display="inline"><semantics id="S4.SS1.p4.2.m2.1a"><mrow id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml"><mrow id="S4.SS1.p4.2.m2.1.1.2" xref="S4.SS1.p4.2.m2.1.1.2.cmml"><msub id="S4.SS1.p4.2.m2.1.1.2.1" xref="S4.SS1.p4.2.m2.1.1.2.1.cmml"><mo id="S4.SS1.p4.2.m2.1.1.2.1.2" xref="S4.SS1.p4.2.m2.1.1.2.1.2.cmml">∑</mo><mi id="S4.SS1.p4.2.m2.1.1.2.1.3" xref="S4.SS1.p4.2.m2.1.1.2.1.3.cmml">i</mi></msub><mrow id="S4.SS1.p4.2.m2.1.1.2.2" xref="S4.SS1.p4.2.m2.1.1.2.2.cmml"><msub id="S4.SS1.p4.2.m2.1.1.2.2.1" xref="S4.SS1.p4.2.m2.1.1.2.2.1.cmml"><mo lspace="0.167em" id="S4.SS1.p4.2.m2.1.1.2.2.1.2" xref="S4.SS1.p4.2.m2.1.1.2.2.1.2.cmml">∑</mo><mi id="S4.SS1.p4.2.m2.1.1.2.2.1.3" xref="S4.SS1.p4.2.m2.1.1.2.2.1.3.cmml">j</mi></msub><msub id="S4.SS1.p4.2.m2.1.1.2.2.2" xref="S4.SS1.p4.2.m2.1.1.2.2.2.cmml"><mi id="S4.SS1.p4.2.m2.1.1.2.2.2.2" xref="S4.SS1.p4.2.m2.1.1.2.2.2.2.cmml">α</mi><mrow id="S4.SS1.p4.2.m2.1.1.2.2.2.3" xref="S4.SS1.p4.2.m2.1.1.2.2.2.3.cmml"><mi id="S4.SS1.p4.2.m2.1.1.2.2.2.3.2" xref="S4.SS1.p4.2.m2.1.1.2.2.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.2.m2.1.1.2.2.2.3.1" xref="S4.SS1.p4.2.m2.1.1.2.2.2.3.1.cmml">​</mo><mi id="S4.SS1.p4.2.m2.1.1.2.2.2.3.3" xref="S4.SS1.p4.2.m2.1.1.2.2.2.3.3.cmml">j</mi></mrow></msub></mrow></mrow><mo id="S4.SS1.p4.2.m2.1.1.1" xref="S4.SS1.p4.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS1.p4.2.m2.1.1.3" xref="S4.SS1.p4.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.1b"><apply id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1"><eq id="S4.SS1.p4.2.m2.1.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1.1"></eq><apply id="S4.SS1.p4.2.m2.1.1.2.cmml" xref="S4.SS1.p4.2.m2.1.1.2"><apply id="S4.SS1.p4.2.m2.1.1.2.1.cmml" xref="S4.SS1.p4.2.m2.1.1.2.1"><csymbol cd="ambiguous" id="S4.SS1.p4.2.m2.1.1.2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1.2.1">subscript</csymbol><sum id="S4.SS1.p4.2.m2.1.1.2.1.2.cmml" xref="S4.SS1.p4.2.m2.1.1.2.1.2"></sum><ci id="S4.SS1.p4.2.m2.1.1.2.1.3.cmml" xref="S4.SS1.p4.2.m2.1.1.2.1.3">𝑖</ci></apply><apply id="S4.SS1.p4.2.m2.1.1.2.2.cmml" xref="S4.SS1.p4.2.m2.1.1.2.2"><apply id="S4.SS1.p4.2.m2.1.1.2.2.1.cmml" xref="S4.SS1.p4.2.m2.1.1.2.2.1"><csymbol cd="ambiguous" id="S4.SS1.p4.2.m2.1.1.2.2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1.2.2.1">subscript</csymbol><sum id="S4.SS1.p4.2.m2.1.1.2.2.1.2.cmml" xref="S4.SS1.p4.2.m2.1.1.2.2.1.2"></sum><ci id="S4.SS1.p4.2.m2.1.1.2.2.1.3.cmml" xref="S4.SS1.p4.2.m2.1.1.2.2.1.3">𝑗</ci></apply><apply id="S4.SS1.p4.2.m2.1.1.2.2.2.cmml" xref="S4.SS1.p4.2.m2.1.1.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p4.2.m2.1.1.2.2.2.1.cmml" xref="S4.SS1.p4.2.m2.1.1.2.2.2">subscript</csymbol><ci id="S4.SS1.p4.2.m2.1.1.2.2.2.2.cmml" xref="S4.SS1.p4.2.m2.1.1.2.2.2.2">𝛼</ci><apply id="S4.SS1.p4.2.m2.1.1.2.2.2.3.cmml" xref="S4.SS1.p4.2.m2.1.1.2.2.2.3"><times id="S4.SS1.p4.2.m2.1.1.2.2.2.3.1.cmml" xref="S4.SS1.p4.2.m2.1.1.2.2.2.3.1"></times><ci id="S4.SS1.p4.2.m2.1.1.2.2.2.3.2.cmml" xref="S4.SS1.p4.2.m2.1.1.2.2.2.3.2">𝑖</ci><ci id="S4.SS1.p4.2.m2.1.1.2.2.2.3.3.cmml" xref="S4.SS1.p4.2.m2.1.1.2.2.2.3.3">𝑗</ci></apply></apply></apply></apply><cn type="integer" id="S4.SS1.p4.2.m2.1.1.3.cmml" xref="S4.SS1.p4.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.1c">\sum_{i}\sum_{j}{\alpha_{ij}}=1</annotation></semantics></math>.
In addition, since the number of view (<math id="S4.SS1.p4.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS1.p4.3.m3.1a"><mi id="S4.SS1.p4.3.m3.1.1" xref="S4.SS1.p4.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.3.m3.1b"><ci id="S4.SS1.p4.3.m3.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.3.m3.1c">N</annotation></semantics></math>) is variable, the number of interacting scene proxies is also variable.
Benefiting from the nature of self-attention, these requirements can be elegantly achieved by transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.3" class="ltx_p">Specifically, we first mix the scene proxies in world coordinates by transforming their coordinates:</p>
<table id="S4.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E3.m1.6" class="ltx_Math" alttext="\left\{\boldsymbol{P}_{ij}^{x},\boldsymbol{P}_{ij}^{y},\boldsymbol{P}_{ij}^{z}\right\}=R_{i}\cdot\left\{\boldsymbol{P}_{ij}^{x},\boldsymbol{P}_{ij}^{y},\boldsymbol{P}_{ij}^{z}\right\}+T_{i}" display="block"><semantics id="S4.E3.m1.6a"><mrow id="S4.E3.m1.6.6" xref="S4.E3.m1.6.6.cmml"><mrow id="S4.E3.m1.3.3.3.3" xref="S4.E3.m1.3.3.3.4.cmml"><mo id="S4.E3.m1.3.3.3.3.4" xref="S4.E3.m1.3.3.3.4.cmml">{</mo><msubsup id="S4.E3.m1.1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.1.cmml"><mi id="S4.E3.m1.1.1.1.1.1.2.2" xref="S4.E3.m1.1.1.1.1.1.2.2.cmml">𝑷</mi><mrow id="S4.E3.m1.1.1.1.1.1.2.3" xref="S4.E3.m1.1.1.1.1.1.2.3.cmml"><mi id="S4.E3.m1.1.1.1.1.1.2.3.2" xref="S4.E3.m1.1.1.1.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.1.1.1.2.3.1" xref="S4.E3.m1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S4.E3.m1.1.1.1.1.1.2.3.3" xref="S4.E3.m1.1.1.1.1.1.2.3.3.cmml">j</mi></mrow><mi id="S4.E3.m1.1.1.1.1.1.3" xref="S4.E3.m1.1.1.1.1.1.3.cmml">x</mi></msubsup><mo id="S4.E3.m1.3.3.3.3.5" xref="S4.E3.m1.3.3.3.4.cmml">,</mo><msubsup id="S4.E3.m1.2.2.2.2.2" xref="S4.E3.m1.2.2.2.2.2.cmml"><mi id="S4.E3.m1.2.2.2.2.2.2.2" xref="S4.E3.m1.2.2.2.2.2.2.2.cmml">𝑷</mi><mrow id="S4.E3.m1.2.2.2.2.2.2.3" xref="S4.E3.m1.2.2.2.2.2.2.3.cmml"><mi id="S4.E3.m1.2.2.2.2.2.2.3.2" xref="S4.E3.m1.2.2.2.2.2.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.2.2.2.2.2.2.3.1" xref="S4.E3.m1.2.2.2.2.2.2.3.1.cmml">​</mo><mi id="S4.E3.m1.2.2.2.2.2.2.3.3" xref="S4.E3.m1.2.2.2.2.2.2.3.3.cmml">j</mi></mrow><mi id="S4.E3.m1.2.2.2.2.2.3" xref="S4.E3.m1.2.2.2.2.2.3.cmml">y</mi></msubsup><mo id="S4.E3.m1.3.3.3.3.6" xref="S4.E3.m1.3.3.3.4.cmml">,</mo><msubsup id="S4.E3.m1.3.3.3.3.3" xref="S4.E3.m1.3.3.3.3.3.cmml"><mi id="S4.E3.m1.3.3.3.3.3.2.2" xref="S4.E3.m1.3.3.3.3.3.2.2.cmml">𝑷</mi><mrow id="S4.E3.m1.3.3.3.3.3.2.3" xref="S4.E3.m1.3.3.3.3.3.2.3.cmml"><mi id="S4.E3.m1.3.3.3.3.3.2.3.2" xref="S4.E3.m1.3.3.3.3.3.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.3.3.3.3.3.2.3.1" xref="S4.E3.m1.3.3.3.3.3.2.3.1.cmml">​</mo><mi id="S4.E3.m1.3.3.3.3.3.2.3.3" xref="S4.E3.m1.3.3.3.3.3.2.3.3.cmml">j</mi></mrow><mi id="S4.E3.m1.3.3.3.3.3.3" xref="S4.E3.m1.3.3.3.3.3.3.cmml">z</mi></msubsup><mo id="S4.E3.m1.3.3.3.3.7" xref="S4.E3.m1.3.3.3.4.cmml">}</mo></mrow><mo id="S4.E3.m1.6.6.7" xref="S4.E3.m1.6.6.7.cmml">=</mo><mrow id="S4.E3.m1.6.6.6" xref="S4.E3.m1.6.6.6.cmml"><mrow id="S4.E3.m1.6.6.6.3" xref="S4.E3.m1.6.6.6.3.cmml"><msub id="S4.E3.m1.6.6.6.3.5" xref="S4.E3.m1.6.6.6.3.5.cmml"><mi id="S4.E3.m1.6.6.6.3.5.2" xref="S4.E3.m1.6.6.6.3.5.2.cmml">R</mi><mi id="S4.E3.m1.6.6.6.3.5.3" xref="S4.E3.m1.6.6.6.3.5.3.cmml">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S4.E3.m1.6.6.6.3.4" xref="S4.E3.m1.6.6.6.3.4.cmml">⋅</mo><mrow id="S4.E3.m1.6.6.6.3.3.3" xref="S4.E3.m1.6.6.6.3.3.4.cmml"><mo id="S4.E3.m1.6.6.6.3.3.3.4" xref="S4.E3.m1.6.6.6.3.3.4.cmml">{</mo><msubsup id="S4.E3.m1.4.4.4.1.1.1.1" xref="S4.E3.m1.4.4.4.1.1.1.1.cmml"><mi id="S4.E3.m1.4.4.4.1.1.1.1.2.2" xref="S4.E3.m1.4.4.4.1.1.1.1.2.2.cmml">𝑷</mi><mrow id="S4.E3.m1.4.4.4.1.1.1.1.2.3" xref="S4.E3.m1.4.4.4.1.1.1.1.2.3.cmml"><mi id="S4.E3.m1.4.4.4.1.1.1.1.2.3.2" xref="S4.E3.m1.4.4.4.1.1.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.4.4.4.1.1.1.1.2.3.1" xref="S4.E3.m1.4.4.4.1.1.1.1.2.3.1.cmml">​</mo><mi id="S4.E3.m1.4.4.4.1.1.1.1.2.3.3" xref="S4.E3.m1.4.4.4.1.1.1.1.2.3.3.cmml">j</mi></mrow><mi id="S4.E3.m1.4.4.4.1.1.1.1.3" xref="S4.E3.m1.4.4.4.1.1.1.1.3.cmml">x</mi></msubsup><mo id="S4.E3.m1.6.6.6.3.3.3.5" xref="S4.E3.m1.6.6.6.3.3.4.cmml">,</mo><msubsup id="S4.E3.m1.5.5.5.2.2.2.2" xref="S4.E3.m1.5.5.5.2.2.2.2.cmml"><mi id="S4.E3.m1.5.5.5.2.2.2.2.2.2" xref="S4.E3.m1.5.5.5.2.2.2.2.2.2.cmml">𝑷</mi><mrow id="S4.E3.m1.5.5.5.2.2.2.2.2.3" xref="S4.E3.m1.5.5.5.2.2.2.2.2.3.cmml"><mi id="S4.E3.m1.5.5.5.2.2.2.2.2.3.2" xref="S4.E3.m1.5.5.5.2.2.2.2.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.5.5.5.2.2.2.2.2.3.1" xref="S4.E3.m1.5.5.5.2.2.2.2.2.3.1.cmml">​</mo><mi id="S4.E3.m1.5.5.5.2.2.2.2.2.3.3" xref="S4.E3.m1.5.5.5.2.2.2.2.2.3.3.cmml">j</mi></mrow><mi id="S4.E3.m1.5.5.5.2.2.2.2.3" xref="S4.E3.m1.5.5.5.2.2.2.2.3.cmml">y</mi></msubsup><mo id="S4.E3.m1.6.6.6.3.3.3.6" xref="S4.E3.m1.6.6.6.3.3.4.cmml">,</mo><msubsup id="S4.E3.m1.6.6.6.3.3.3.3" xref="S4.E3.m1.6.6.6.3.3.3.3.cmml"><mi id="S4.E3.m1.6.6.6.3.3.3.3.2.2" xref="S4.E3.m1.6.6.6.3.3.3.3.2.2.cmml">𝑷</mi><mrow id="S4.E3.m1.6.6.6.3.3.3.3.2.3" xref="S4.E3.m1.6.6.6.3.3.3.3.2.3.cmml"><mi id="S4.E3.m1.6.6.6.3.3.3.3.2.3.2" xref="S4.E3.m1.6.6.6.3.3.3.3.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.6.6.6.3.3.3.3.2.3.1" xref="S4.E3.m1.6.6.6.3.3.3.3.2.3.1.cmml">​</mo><mi id="S4.E3.m1.6.6.6.3.3.3.3.2.3.3" xref="S4.E3.m1.6.6.6.3.3.3.3.2.3.3.cmml">j</mi></mrow><mi id="S4.E3.m1.6.6.6.3.3.3.3.3" xref="S4.E3.m1.6.6.6.3.3.3.3.3.cmml">z</mi></msubsup><mo id="S4.E3.m1.6.6.6.3.3.3.7" xref="S4.E3.m1.6.6.6.3.3.4.cmml">}</mo></mrow></mrow><mo id="S4.E3.m1.6.6.6.4" xref="S4.E3.m1.6.6.6.4.cmml">+</mo><msub id="S4.E3.m1.6.6.6.5" xref="S4.E3.m1.6.6.6.5.cmml"><mi id="S4.E3.m1.6.6.6.5.2" xref="S4.E3.m1.6.6.6.5.2.cmml">T</mi><mi id="S4.E3.m1.6.6.6.5.3" xref="S4.E3.m1.6.6.6.5.3.cmml">i</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.6b"><apply id="S4.E3.m1.6.6.cmml" xref="S4.E3.m1.6.6"><eq id="S4.E3.m1.6.6.7.cmml" xref="S4.E3.m1.6.6.7"></eq><set id="S4.E3.m1.3.3.3.4.cmml" xref="S4.E3.m1.3.3.3.3"><apply id="S4.E3.m1.1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1.1">superscript</csymbol><apply id="S4.E3.m1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.1.2.1.cmml" xref="S4.E3.m1.1.1.1.1.1">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.1.2.2.cmml" xref="S4.E3.m1.1.1.1.1.1.2.2">𝑷</ci><apply id="S4.E3.m1.1.1.1.1.1.2.3.cmml" xref="S4.E3.m1.1.1.1.1.1.2.3"><times id="S4.E3.m1.1.1.1.1.1.2.3.1.cmml" xref="S4.E3.m1.1.1.1.1.1.2.3.1"></times><ci id="S4.E3.m1.1.1.1.1.1.2.3.2.cmml" xref="S4.E3.m1.1.1.1.1.1.2.3.2">𝑖</ci><ci id="S4.E3.m1.1.1.1.1.1.2.3.3.cmml" xref="S4.E3.m1.1.1.1.1.1.2.3.3">𝑗</ci></apply></apply><ci id="S4.E3.m1.1.1.1.1.1.3.cmml" xref="S4.E3.m1.1.1.1.1.1.3">𝑥</ci></apply><apply id="S4.E3.m1.2.2.2.2.2.cmml" xref="S4.E3.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.2.2.2.1.cmml" xref="S4.E3.m1.2.2.2.2.2">superscript</csymbol><apply id="S4.E3.m1.2.2.2.2.2.2.cmml" xref="S4.E3.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.2.2.2.2.1.cmml" xref="S4.E3.m1.2.2.2.2.2">subscript</csymbol><ci id="S4.E3.m1.2.2.2.2.2.2.2.cmml" xref="S4.E3.m1.2.2.2.2.2.2.2">𝑷</ci><apply id="S4.E3.m1.2.2.2.2.2.2.3.cmml" xref="S4.E3.m1.2.2.2.2.2.2.3"><times id="S4.E3.m1.2.2.2.2.2.2.3.1.cmml" xref="S4.E3.m1.2.2.2.2.2.2.3.1"></times><ci id="S4.E3.m1.2.2.2.2.2.2.3.2.cmml" xref="S4.E3.m1.2.2.2.2.2.2.3.2">𝑖</ci><ci id="S4.E3.m1.2.2.2.2.2.2.3.3.cmml" xref="S4.E3.m1.2.2.2.2.2.2.3.3">𝑗</ci></apply></apply><ci id="S4.E3.m1.2.2.2.2.2.3.cmml" xref="S4.E3.m1.2.2.2.2.2.3">𝑦</ci></apply><apply id="S4.E3.m1.3.3.3.3.3.cmml" xref="S4.E3.m1.3.3.3.3.3"><csymbol cd="ambiguous" id="S4.E3.m1.3.3.3.3.3.1.cmml" xref="S4.E3.m1.3.3.3.3.3">superscript</csymbol><apply id="S4.E3.m1.3.3.3.3.3.2.cmml" xref="S4.E3.m1.3.3.3.3.3"><csymbol cd="ambiguous" id="S4.E3.m1.3.3.3.3.3.2.1.cmml" xref="S4.E3.m1.3.3.3.3.3">subscript</csymbol><ci id="S4.E3.m1.3.3.3.3.3.2.2.cmml" xref="S4.E3.m1.3.3.3.3.3.2.2">𝑷</ci><apply id="S4.E3.m1.3.3.3.3.3.2.3.cmml" xref="S4.E3.m1.3.3.3.3.3.2.3"><times id="S4.E3.m1.3.3.3.3.3.2.3.1.cmml" xref="S4.E3.m1.3.3.3.3.3.2.3.1"></times><ci id="S4.E3.m1.3.3.3.3.3.2.3.2.cmml" xref="S4.E3.m1.3.3.3.3.3.2.3.2">𝑖</ci><ci id="S4.E3.m1.3.3.3.3.3.2.3.3.cmml" xref="S4.E3.m1.3.3.3.3.3.2.3.3">𝑗</ci></apply></apply><ci id="S4.E3.m1.3.3.3.3.3.3.cmml" xref="S4.E3.m1.3.3.3.3.3.3">𝑧</ci></apply></set><apply id="S4.E3.m1.6.6.6.cmml" xref="S4.E3.m1.6.6.6"><plus id="S4.E3.m1.6.6.6.4.cmml" xref="S4.E3.m1.6.6.6.4"></plus><apply id="S4.E3.m1.6.6.6.3.cmml" xref="S4.E3.m1.6.6.6.3"><ci id="S4.E3.m1.6.6.6.3.4.cmml" xref="S4.E3.m1.6.6.6.3.4">⋅</ci><apply id="S4.E3.m1.6.6.6.3.5.cmml" xref="S4.E3.m1.6.6.6.3.5"><csymbol cd="ambiguous" id="S4.E3.m1.6.6.6.3.5.1.cmml" xref="S4.E3.m1.6.6.6.3.5">subscript</csymbol><ci id="S4.E3.m1.6.6.6.3.5.2.cmml" xref="S4.E3.m1.6.6.6.3.5.2">𝑅</ci><ci id="S4.E3.m1.6.6.6.3.5.3.cmml" xref="S4.E3.m1.6.6.6.3.5.3">𝑖</ci></apply><set id="S4.E3.m1.6.6.6.3.3.4.cmml" xref="S4.E3.m1.6.6.6.3.3.3"><apply id="S4.E3.m1.4.4.4.1.1.1.1.cmml" xref="S4.E3.m1.4.4.4.1.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m1.4.4.4.1.1.1.1.1.cmml" xref="S4.E3.m1.4.4.4.1.1.1.1">superscript</csymbol><apply id="S4.E3.m1.4.4.4.1.1.1.1.2.cmml" xref="S4.E3.m1.4.4.4.1.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m1.4.4.4.1.1.1.1.2.1.cmml" xref="S4.E3.m1.4.4.4.1.1.1.1">subscript</csymbol><ci id="S4.E3.m1.4.4.4.1.1.1.1.2.2.cmml" xref="S4.E3.m1.4.4.4.1.1.1.1.2.2">𝑷</ci><apply id="S4.E3.m1.4.4.4.1.1.1.1.2.3.cmml" xref="S4.E3.m1.4.4.4.1.1.1.1.2.3"><times id="S4.E3.m1.4.4.4.1.1.1.1.2.3.1.cmml" xref="S4.E3.m1.4.4.4.1.1.1.1.2.3.1"></times><ci id="S4.E3.m1.4.4.4.1.1.1.1.2.3.2.cmml" xref="S4.E3.m1.4.4.4.1.1.1.1.2.3.2">𝑖</ci><ci id="S4.E3.m1.4.4.4.1.1.1.1.2.3.3.cmml" xref="S4.E3.m1.4.4.4.1.1.1.1.2.3.3">𝑗</ci></apply></apply><ci id="S4.E3.m1.4.4.4.1.1.1.1.3.cmml" xref="S4.E3.m1.4.4.4.1.1.1.1.3">𝑥</ci></apply><apply id="S4.E3.m1.5.5.5.2.2.2.2.cmml" xref="S4.E3.m1.5.5.5.2.2.2.2"><csymbol cd="ambiguous" id="S4.E3.m1.5.5.5.2.2.2.2.1.cmml" xref="S4.E3.m1.5.5.5.2.2.2.2">superscript</csymbol><apply id="S4.E3.m1.5.5.5.2.2.2.2.2.cmml" xref="S4.E3.m1.5.5.5.2.2.2.2"><csymbol cd="ambiguous" id="S4.E3.m1.5.5.5.2.2.2.2.2.1.cmml" xref="S4.E3.m1.5.5.5.2.2.2.2">subscript</csymbol><ci id="S4.E3.m1.5.5.5.2.2.2.2.2.2.cmml" xref="S4.E3.m1.5.5.5.2.2.2.2.2.2">𝑷</ci><apply id="S4.E3.m1.5.5.5.2.2.2.2.2.3.cmml" xref="S4.E3.m1.5.5.5.2.2.2.2.2.3"><times id="S4.E3.m1.5.5.5.2.2.2.2.2.3.1.cmml" xref="S4.E3.m1.5.5.5.2.2.2.2.2.3.1"></times><ci id="S4.E3.m1.5.5.5.2.2.2.2.2.3.2.cmml" xref="S4.E3.m1.5.5.5.2.2.2.2.2.3.2">𝑖</ci><ci id="S4.E3.m1.5.5.5.2.2.2.2.2.3.3.cmml" xref="S4.E3.m1.5.5.5.2.2.2.2.2.3.3">𝑗</ci></apply></apply><ci id="S4.E3.m1.5.5.5.2.2.2.2.3.cmml" xref="S4.E3.m1.5.5.5.2.2.2.2.3">𝑦</ci></apply><apply id="S4.E3.m1.6.6.6.3.3.3.3.cmml" xref="S4.E3.m1.6.6.6.3.3.3.3"><csymbol cd="ambiguous" id="S4.E3.m1.6.6.6.3.3.3.3.1.cmml" xref="S4.E3.m1.6.6.6.3.3.3.3">superscript</csymbol><apply id="S4.E3.m1.6.6.6.3.3.3.3.2.cmml" xref="S4.E3.m1.6.6.6.3.3.3.3"><csymbol cd="ambiguous" id="S4.E3.m1.6.6.6.3.3.3.3.2.1.cmml" xref="S4.E3.m1.6.6.6.3.3.3.3">subscript</csymbol><ci id="S4.E3.m1.6.6.6.3.3.3.3.2.2.cmml" xref="S4.E3.m1.6.6.6.3.3.3.3.2.2">𝑷</ci><apply id="S4.E3.m1.6.6.6.3.3.3.3.2.3.cmml" xref="S4.E3.m1.6.6.6.3.3.3.3.2.3"><times id="S4.E3.m1.6.6.6.3.3.3.3.2.3.1.cmml" xref="S4.E3.m1.6.6.6.3.3.3.3.2.3.1"></times><ci id="S4.E3.m1.6.6.6.3.3.3.3.2.3.2.cmml" xref="S4.E3.m1.6.6.6.3.3.3.3.2.3.2">𝑖</ci><ci id="S4.E3.m1.6.6.6.3.3.3.3.2.3.3.cmml" xref="S4.E3.m1.6.6.6.3.3.3.3.2.3.3">𝑗</ci></apply></apply><ci id="S4.E3.m1.6.6.6.3.3.3.3.3.cmml" xref="S4.E3.m1.6.6.6.3.3.3.3.3">𝑧</ci></apply></set></apply><apply id="S4.E3.m1.6.6.6.5.cmml" xref="S4.E3.m1.6.6.6.5"><csymbol cd="ambiguous" id="S4.E3.m1.6.6.6.5.1.cmml" xref="S4.E3.m1.6.6.6.5">subscript</csymbol><ci id="S4.E3.m1.6.6.6.5.2.cmml" xref="S4.E3.m1.6.6.6.5.2">𝑇</ci><ci id="S4.E3.m1.6.6.6.5.3.cmml" xref="S4.E3.m1.6.6.6.5.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.6c">\left\{\boldsymbol{P}_{ij}^{x},\boldsymbol{P}_{ij}^{y},\boldsymbol{P}_{ij}^{z}\right\}=R_{i}\cdot\left\{\boldsymbol{P}_{ij}^{x},\boldsymbol{P}_{ij}^{y},\boldsymbol{P}_{ij}^{z}\right\}+T_{i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S4.SS1.p5.2" class="ltx_p">where <math id="S4.SS1.p5.1.m1.3" class="ltx_Math" alttext="{\boldsymbol{P}_{ij}^{x,y,z}}" display="inline"><semantics id="S4.SS1.p5.1.m1.3a"><msubsup id="S4.SS1.p5.1.m1.3.4" xref="S4.SS1.p5.1.m1.3.4.cmml"><mi id="S4.SS1.p5.1.m1.3.4.2.2" xref="S4.SS1.p5.1.m1.3.4.2.2.cmml">𝑷</mi><mrow id="S4.SS1.p5.1.m1.3.4.2.3" xref="S4.SS1.p5.1.m1.3.4.2.3.cmml"><mi id="S4.SS1.p5.1.m1.3.4.2.3.2" xref="S4.SS1.p5.1.m1.3.4.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p5.1.m1.3.4.2.3.1" xref="S4.SS1.p5.1.m1.3.4.2.3.1.cmml">​</mo><mi id="S4.SS1.p5.1.m1.3.4.2.3.3" xref="S4.SS1.p5.1.m1.3.4.2.3.3.cmml">j</mi></mrow><mrow id="S4.SS1.p5.1.m1.3.3.3.5" xref="S4.SS1.p5.1.m1.3.3.3.4.cmml"><mi id="S4.SS1.p5.1.m1.1.1.1.1" xref="S4.SS1.p5.1.m1.1.1.1.1.cmml">x</mi><mo id="S4.SS1.p5.1.m1.3.3.3.5.1" xref="S4.SS1.p5.1.m1.3.3.3.4.cmml">,</mo><mi id="S4.SS1.p5.1.m1.2.2.2.2" xref="S4.SS1.p5.1.m1.2.2.2.2.cmml">y</mi><mo id="S4.SS1.p5.1.m1.3.3.3.5.2" xref="S4.SS1.p5.1.m1.3.3.3.4.cmml">,</mo><mi id="S4.SS1.p5.1.m1.3.3.3.3" xref="S4.SS1.p5.1.m1.3.3.3.3.cmml">z</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.1.m1.3b"><apply id="S4.SS1.p5.1.m1.3.4.cmml" xref="S4.SS1.p5.1.m1.3.4"><csymbol cd="ambiguous" id="S4.SS1.p5.1.m1.3.4.1.cmml" xref="S4.SS1.p5.1.m1.3.4">superscript</csymbol><apply id="S4.SS1.p5.1.m1.3.4.2.cmml" xref="S4.SS1.p5.1.m1.3.4"><csymbol cd="ambiguous" id="S4.SS1.p5.1.m1.3.4.2.1.cmml" xref="S4.SS1.p5.1.m1.3.4">subscript</csymbol><ci id="S4.SS1.p5.1.m1.3.4.2.2.cmml" xref="S4.SS1.p5.1.m1.3.4.2.2">𝑷</ci><apply id="S4.SS1.p5.1.m1.3.4.2.3.cmml" xref="S4.SS1.p5.1.m1.3.4.2.3"><times id="S4.SS1.p5.1.m1.3.4.2.3.1.cmml" xref="S4.SS1.p5.1.m1.3.4.2.3.1"></times><ci id="S4.SS1.p5.1.m1.3.4.2.3.2.cmml" xref="S4.SS1.p5.1.m1.3.4.2.3.2">𝑖</ci><ci id="S4.SS1.p5.1.m1.3.4.2.3.3.cmml" xref="S4.SS1.p5.1.m1.3.4.2.3.3">𝑗</ci></apply></apply><list id="S4.SS1.p5.1.m1.3.3.3.4.cmml" xref="S4.SS1.p5.1.m1.3.3.3.5"><ci id="S4.SS1.p5.1.m1.1.1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1.1.1">𝑥</ci><ci id="S4.SS1.p5.1.m1.2.2.2.2.cmml" xref="S4.SS1.p5.1.m1.2.2.2.2">𝑦</ci><ci id="S4.SS1.p5.1.m1.3.3.3.3.cmml" xref="S4.SS1.p5.1.m1.3.3.3.3">𝑧</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.1.m1.3c">{\boldsymbol{P}_{ij}^{x,y,z}}</annotation></semantics></math> represents the spatial position of <math id="S4.SS1.p5.2.m2.1" class="ltx_Math" alttext="\mathcal{P}_{ij}" display="inline"><semantics id="S4.SS1.p5.2.m2.1a"><msub id="S4.SS1.p5.2.m2.1.1" xref="S4.SS1.p5.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p5.2.m2.1.1.2" xref="S4.SS1.p5.2.m2.1.1.2.cmml">𝒫</mi><mrow id="S4.SS1.p5.2.m2.1.1.3" xref="S4.SS1.p5.2.m2.1.1.3.cmml"><mi id="S4.SS1.p5.2.m2.1.1.3.2" xref="S4.SS1.p5.2.m2.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p5.2.m2.1.1.3.1" xref="S4.SS1.p5.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS1.p5.2.m2.1.1.3.3" xref="S4.SS1.p5.2.m2.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.2.m2.1b"><apply id="S4.SS1.p5.2.m2.1.1.cmml" xref="S4.SS1.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p5.2.m2.1.1.1.cmml" xref="S4.SS1.p5.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p5.2.m2.1.1.2.cmml" xref="S4.SS1.p5.2.m2.1.1.2">𝒫</ci><apply id="S4.SS1.p5.2.m2.1.1.3.cmml" xref="S4.SS1.p5.2.m2.1.1.3"><times id="S4.SS1.p5.2.m2.1.1.3.1.cmml" xref="S4.SS1.p5.2.m2.1.1.3.1"></times><ci id="S4.SS1.p5.2.m2.1.1.3.2.cmml" xref="S4.SS1.p5.2.m2.1.1.3.2">𝑖</ci><ci id="S4.SS1.p5.2.m2.1.1.3.3.cmml" xref="S4.SS1.p5.2.m2.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.2.m2.1c">\mathcal{P}_{ij}</annotation></semantics></math> in the camera coordinate system, and we project them into the world coordinate system to obtain global information at a higher level with the camera projection matrix.
Then the scene proxies are fed into the transformer encoder with their coordinates converted into Fourier positional embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. We adopt radius mask <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> on the self-attention matrix to conduct interactions from local to global.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.1" class="ltx_p"><span id="S4.SS1.p6.1.1" class="ltx_text ltx_font_bold">Decode objects from the whole scene:</span>
Given the <math id="S4.SS1.p6.1.m1.1" class="ltx_Math" alttext="N_{T}=N\times T" display="inline"><semantics id="S4.SS1.p6.1.m1.1a"><mrow id="S4.SS1.p6.1.m1.1.1" xref="S4.SS1.p6.1.m1.1.1.cmml"><msub id="S4.SS1.p6.1.m1.1.1.2" xref="S4.SS1.p6.1.m1.1.1.2.cmml"><mi id="S4.SS1.p6.1.m1.1.1.2.2" xref="S4.SS1.p6.1.m1.1.1.2.2.cmml">N</mi><mi id="S4.SS1.p6.1.m1.1.1.2.3" xref="S4.SS1.p6.1.m1.1.1.2.3.cmml">T</mi></msub><mo id="S4.SS1.p6.1.m1.1.1.1" xref="S4.SS1.p6.1.m1.1.1.1.cmml">=</mo><mrow id="S4.SS1.p6.1.m1.1.1.3" xref="S4.SS1.p6.1.m1.1.1.3.cmml"><mi id="S4.SS1.p6.1.m1.1.1.3.2" xref="S4.SS1.p6.1.m1.1.1.3.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p6.1.m1.1.1.3.1" xref="S4.SS1.p6.1.m1.1.1.3.1.cmml">×</mo><mi id="S4.SS1.p6.1.m1.1.1.3.3" xref="S4.SS1.p6.1.m1.1.1.3.3.cmml">T</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.1.m1.1b"><apply id="S4.SS1.p6.1.m1.1.1.cmml" xref="S4.SS1.p6.1.m1.1.1"><eq id="S4.SS1.p6.1.m1.1.1.1.cmml" xref="S4.SS1.p6.1.m1.1.1.1"></eq><apply id="S4.SS1.p6.1.m1.1.1.2.cmml" xref="S4.SS1.p6.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p6.1.m1.1.1.2.1.cmml" xref="S4.SS1.p6.1.m1.1.1.2">subscript</csymbol><ci id="S4.SS1.p6.1.m1.1.1.2.2.cmml" xref="S4.SS1.p6.1.m1.1.1.2.2">𝑁</ci><ci id="S4.SS1.p6.1.m1.1.1.2.3.cmml" xref="S4.SS1.p6.1.m1.1.1.2.3">𝑇</ci></apply><apply id="S4.SS1.p6.1.m1.1.1.3.cmml" xref="S4.SS1.p6.1.m1.1.1.3"><times id="S4.SS1.p6.1.m1.1.1.3.1.cmml" xref="S4.SS1.p6.1.m1.1.1.3.1"></times><ci id="S4.SS1.p6.1.m1.1.1.3.2.cmml" xref="S4.SS1.p6.1.m1.1.1.3.2">𝑁</ci><ci id="S4.SS1.p6.1.m1.1.1.3.3.cmml" xref="S4.SS1.p6.1.m1.1.1.3.3">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.1.m1.1c">N_{T}=N\times T</annotation></semantics></math> features from the encoder, we adopt a transformer decoder to refine object queries layer by layer as in DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. The object queries are initial proposals, which are generated by furthest point sampling from the whole scene to ensure coverage:</p>
<table id="S4.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E4.m1.1" class="ltx_Math" alttext="\mathbf{Q}=\mathcal{M}({\mathcal{F}}({\mathcal{C}}(R_{i}^{c}\cdot P_{ci}+T_{i}^{c})))" display="block"><semantics id="S4.E4.m1.1a"><mrow id="S4.E4.m1.1.1" xref="S4.E4.m1.1.1.cmml"><mi id="S4.E4.m1.1.1.3" xref="S4.E4.m1.1.1.3.cmml">𝐐</mi><mo id="S4.E4.m1.1.1.2" xref="S4.E4.m1.1.1.2.cmml">=</mo><mrow id="S4.E4.m1.1.1.1" xref="S4.E4.m1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E4.m1.1.1.1.3" xref="S4.E4.m1.1.1.1.3.cmml">ℳ</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.1.1.1.2" xref="S4.E4.m1.1.1.1.2.cmml">​</mo><mrow id="S4.E4.m1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E4.m1.1.1.1.1.1.2" xref="S4.E4.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E4.m1.1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E4.m1.1.1.1.1.1.1.3" xref="S4.E4.m1.1.1.1.1.1.1.3.cmml">ℱ</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.1.1.1.1.1.1.2" xref="S4.E4.m1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S4.E4.m1.1.1.1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E4.m1.1.1.1.1.1.1.1.1.2" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E4.m1.1.1.1.1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E4.m1.1.1.1.1.1.1.1.1.1.3" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.cmml">𝒞</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.1.1.1.1.1.1.1.1.1.2" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><msubsup id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml">R</mi><mi id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml">i</mi><mi id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml">c</mi></msubsup><mo lspace="0.222em" rspace="0.222em" id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.1" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">⋅</mo><msub id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml">P</mi><mrow id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml"><mi id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.2" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.1" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.1.cmml">​</mo><mi id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.3" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.3.cmml">i</mi></mrow></msub></mrow><mo id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">+</mo><msubsup id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml">T</mi><mi id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml">i</mi><mi id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">c</mi></msubsup></mrow><mo stretchy="false" id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.E4.m1.1.1.1.1.1.1.1.1.3" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.E4.m1.1.1.1.1.1.3" xref="S4.E4.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m1.1b"><apply id="S4.E4.m1.1.1.cmml" xref="S4.E4.m1.1.1"><eq id="S4.E4.m1.1.1.2.cmml" xref="S4.E4.m1.1.1.2"></eq><ci id="S4.E4.m1.1.1.3.cmml" xref="S4.E4.m1.1.1.3">𝐐</ci><apply id="S4.E4.m1.1.1.1.cmml" xref="S4.E4.m1.1.1.1"><times id="S4.E4.m1.1.1.1.2.cmml" xref="S4.E4.m1.1.1.1.2"></times><ci id="S4.E4.m1.1.1.1.3.cmml" xref="S4.E4.m1.1.1.1.3">ℳ</ci><apply id="S4.E4.m1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.1.1.1.1.1"><times id="S4.E4.m1.1.1.1.1.1.1.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.2"></times><ci id="S4.E4.m1.1.1.1.1.1.1.3.cmml" xref="S4.E4.m1.1.1.1.1.1.1.3">ℱ</ci><apply id="S4.E4.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1"><times id="S4.E4.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.2"></times><ci id="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.3">𝒞</ci><apply id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1"><plus id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.1"></plus><apply id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2"><ci id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.1">⋅</ci><apply id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2">superscript</csymbol><apply id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2">𝑅</ci><ci id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3">𝑖</ci></apply><ci id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3">𝑐</ci></apply><apply id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2">𝑃</ci><apply id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3"><times id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.1"></times><ci id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.2">𝑐</ci><ci id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.3.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.3">𝑖</ci></apply></apply></apply><apply id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2">𝑇</ci><ci id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3">𝑖</ci></apply><ci id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3">𝑐</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m1.1c">\mathbf{Q}=\mathcal{M}({\mathcal{F}}({\mathcal{C}}(R_{i}^{c}\cdot P_{ci}+T_{i}^{c})))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S4.SS1.p6.3" class="ltx_p">where <math id="S4.SS1.p6.2.m1.1" class="ltx_Math" alttext="\mathcal{M}" display="inline"><semantics id="S4.SS1.p6.2.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p6.2.m1.1.1" xref="S4.SS1.p6.2.m1.1.1.cmml">ℳ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.2.m1.1b"><ci id="S4.SS1.p6.2.m1.1.1.cmml" xref="S4.SS1.p6.2.m1.1.1">ℳ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.2.m1.1c">\mathcal{M}</annotation></semantics></math> represents the MLP and <math id="S4.SS1.p6.3.m2.1" class="ltx_Math" alttext="\mathcal{F}" display="inline"><semantics id="S4.SS1.p6.3.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p6.3.m2.1.1" xref="S4.SS1.p6.3.m2.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.3.m2.1b"><ci id="S4.SS1.p6.3.m2.1.1.cmml" xref="S4.SS1.p6.3.m2.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.3.m2.1c">\mathcal{F}</annotation></semantics></math> represents the farthest point sampling algorithm.
The refined queries are converted into box parameters and supervised following the procedure in 3DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Scale-independent Training</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Although the architecture design of AnyView can extract features independent of input scale and detect 3D objects from any number of views, we find proper data augmentation is essential for training the model, which are listed below.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.5" class="ltx_p"><span id="S4.SS2.p2.5.1" class="ltx_text ltx_font_bold">Random view dropping:</span>
During training, the numbers of views of different inputs in a batch are kept the same in order to parallelize the computation. However, enabling the network to process variable number of views is necessary as it encourages the transformer to adapt to different scale of attention map. To this end, we randomly drop <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mn id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><cn type="integer" id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">0</cn></annotation-xml></semantics></math> to <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="\frac{N}{2}" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mfrac id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mi id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml">N</mi><mn id="S4.SS2.p2.2.m2.1.1.3" xref="S4.SS2.p2.2.m2.1.1.3.cmml">2</mn></mfrac><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><divide id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"></divide><ci id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2">𝑁</ci><cn type="integer" id="S4.SS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">\frac{N}{2}</annotation></semantics></math> views and pad the dropped points with 0. To cut down the interactions between the dropped scene proxies and other tokens (scene proxies for encoder, object queries for decoder), we apply a binary mask on each attention map to indicate which of scene proxies are invalid (with coordinates <math id="S4.SS2.p2.3.m3.3" class="ltx_Math" alttext="(0,0,0)" display="inline"><semantics id="S4.SS2.p2.3.m3.3a"><mrow id="S4.SS2.p2.3.m3.3.4.2" xref="S4.SS2.p2.3.m3.3.4.1.cmml"><mo stretchy="false" id="S4.SS2.p2.3.m3.3.4.2.1" xref="S4.SS2.p2.3.m3.3.4.1.cmml">(</mo><mn id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml">0</mn><mo id="S4.SS2.p2.3.m3.3.4.2.2" xref="S4.SS2.p2.3.m3.3.4.1.cmml">,</mo><mn id="S4.SS2.p2.3.m3.2.2" xref="S4.SS2.p2.3.m3.2.2.cmml">0</mn><mo id="S4.SS2.p2.3.m3.3.4.2.3" xref="S4.SS2.p2.3.m3.3.4.1.cmml">,</mo><mn id="S4.SS2.p2.3.m3.3.3" xref="S4.SS2.p2.3.m3.3.3.cmml">0</mn><mo stretchy="false" id="S4.SS2.p2.3.m3.3.4.2.4" xref="S4.SS2.p2.3.m3.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.3b"><vector id="S4.SS2.p2.3.m3.3.4.1.cmml" xref="S4.SS2.p2.3.m3.3.4.2"><cn type="integer" id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">0</cn><cn type="integer" id="S4.SS2.p2.3.m3.2.2.cmml" xref="S4.SS2.p2.3.m3.2.2">0</cn><cn type="integer" id="S4.SS2.p2.3.m3.3.3.cmml" xref="S4.SS2.p2.3.m3.3.3">0</cn></vector></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.3c">(0,0,0)</annotation></semantics></math>).
The binary mask will set the inner products between invalid scene proxies and other tokens to <math id="S4.SS2.p2.4.m4.1" class="ltx_Math" alttext="-\infty" display="inline"><semantics id="S4.SS2.p2.4.m4.1a"><mrow id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml"><mo id="S4.SS2.p2.4.m4.1.1a" xref="S4.SS2.p2.4.m4.1.1.cmml">−</mo><mi mathvariant="normal" id="S4.SS2.p2.4.m4.1.1.2" xref="S4.SS2.p2.4.m4.1.1.2.cmml">∞</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><apply id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1"><minus id="S4.SS2.p2.4.m4.1.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1"></minus><infinity id="S4.SS2.p2.4.m4.1.1.2.cmml" xref="S4.SS2.p2.4.m4.1.1.2"></infinity></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">-\infty</annotation></semantics></math>, so after softmax operation these attention value will be <math id="S4.SS2.p2.5.m5.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S4.SS2.p2.5.m5.1a"><mn id="S4.SS2.p2.5.m5.1.1" xref="S4.SS2.p2.5.m5.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m5.1b"><cn type="integer" id="S4.SS2.p2.5.m5.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1">0</cn></annotation-xml></semantics></math>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.3" class="ltx_p"><span id="S4.SS2.p3.3.1" class="ltx_text ltx_font_bold">Global random cuboid:</span>
The geometry structure of single view point clouds may vary from view to view. For example, when the distance between the RGB-D camera and the scene varies, the density of point clouds varies as well. When one view only contains floors and walls, the scene proxies extracted from it are uninformative. To allow the network to handle depth maps taken from various shooting situations and learn to ignore scene proxies with less information, we randomly crop a cuboid <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> from the fused point clouds <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="P={\rm\mathcal{C}}(R_{i}^{c}\cdot P_{ci}+T_{i}^{c})" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mrow id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mi id="S4.SS2.p3.1.m1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.3.cmml">P</mi><mo id="S4.SS2.p3.1.m1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.2.cmml">=</mo><mrow id="S4.SS2.p3.1.m1.1.1.1" xref="S4.SS2.p3.1.m1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p3.1.m1.1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.1.3.cmml">𝒞</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p3.1.m1.1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.1.2.cmml">​</mo><mrow id="S4.SS2.p3.1.m1.1.1.1.1.1" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.p3.1.m1.1.1.1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS2.p3.1.m1.1.1.1.1.1.1" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.cmml"><mrow id="S4.SS2.p3.1.m1.1.1.1.1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.cmml"><msubsup id="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.2" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.2.cmml"><mi id="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.2.2.2" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.2.2.2.cmml">R</mi><mi id="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.2.2.3" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.2.2.3.cmml">i</mi><mi id="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.2.3" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.2.3.cmml">c</mi></msubsup><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.1" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.1.cmml">⋅</mo><msub id="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.3" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.3.2" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.3.2.cmml">P</mi><mrow id="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.3.3" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.3.3.cmml"><mi id="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.3.3.2" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.3.3.1" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.3.3.1.cmml">​</mo><mi id="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.3.3.3" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.3.3.3.cmml">i</mi></mrow></msub></mrow><mo id="S4.SS2.p3.1.m1.1.1.1.1.1.1.1" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.1.cmml">+</mo><msubsup id="S4.SS2.p3.1.m1.1.1.1.1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.3.cmml"><mi id="S4.SS2.p3.1.m1.1.1.1.1.1.1.3.2.2" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.3.2.2.cmml">T</mi><mi id="S4.SS2.p3.1.m1.1.1.1.1.1.1.3.2.3" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.3.2.3.cmml">i</mi><mi id="S4.SS2.p3.1.m1.1.1.1.1.1.1.3.3" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.3.3.cmml">c</mi></msubsup></mrow><mo stretchy="false" id="S4.SS2.p3.1.m1.1.1.1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><eq id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2"></eq><ci id="S4.SS2.p3.1.m1.1.1.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3">𝑃</ci><apply id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1"><times id="S4.SS2.p3.1.m1.1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.1.2"></times><ci id="S4.SS2.p3.1.m1.1.1.1.3.cmml" xref="S4.SS2.p3.1.m1.1.1.1.3">𝒞</ci><apply id="S4.SS2.p3.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1.1"><plus id="S4.SS2.p3.1.m1.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.1"></plus><apply id="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.2"><ci id="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.1">⋅</ci><apply id="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.2.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.2">superscript</csymbol><apply id="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.2.2.2">𝑅</ci><ci id="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.2.2.3">𝑖</ci></apply><ci id="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.2.3">𝑐</ci></apply><apply id="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.3.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.3.2">𝑃</ci><apply id="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.3.3"><times id="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.3.3.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.3.3.1"></times><ci id="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.3.3.2.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.3.3.2">𝑐</ci><ci id="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.3.3.3.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.2.3.3.3">𝑖</ci></apply></apply></apply><apply id="S4.SS2.p3.1.m1.1.1.1.1.1.1.3.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p3.1.m1.1.1.1.1.1.1.3.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S4.SS2.p3.1.m1.1.1.1.1.1.1.3.2.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p3.1.m1.1.1.1.1.1.1.3.2.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.SS2.p3.1.m1.1.1.1.1.1.1.3.2.2.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.3.2.2">𝑇</ci><ci id="S4.SS2.p3.1.m1.1.1.1.1.1.1.3.2.3.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.3.2.3">𝑖</ci></apply><ci id="S4.SS2.p3.1.m1.1.1.1.1.1.1.3.3.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.3.3">𝑐</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">P={\rm\mathcal{C}}(R_{i}^{c}\cdot P_{ci}+T_{i}^{c})</annotation></semantics></math> and set other points outside the cuboid to <math id="S4.SS2.p3.2.m2.3" class="ltx_Math" alttext="(0,0,0)" display="inline"><semantics id="S4.SS2.p3.2.m2.3a"><mrow id="S4.SS2.p3.2.m2.3.4.2" xref="S4.SS2.p3.2.m2.3.4.1.cmml"><mo stretchy="false" id="S4.SS2.p3.2.m2.3.4.2.1" xref="S4.SS2.p3.2.m2.3.4.1.cmml">(</mo><mn id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml">0</mn><mo id="S4.SS2.p3.2.m2.3.4.2.2" xref="S4.SS2.p3.2.m2.3.4.1.cmml">,</mo><mn id="S4.SS2.p3.2.m2.2.2" xref="S4.SS2.p3.2.m2.2.2.cmml">0</mn><mo id="S4.SS2.p3.2.m2.3.4.2.3" xref="S4.SS2.p3.2.m2.3.4.1.cmml">,</mo><mn id="S4.SS2.p3.2.m2.3.3" xref="S4.SS2.p3.2.m2.3.3.cmml">0</mn><mo stretchy="false" id="S4.SS2.p3.2.m2.3.4.2.4" xref="S4.SS2.p3.2.m2.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.3b"><vector id="S4.SS2.p3.2.m2.3.4.1.cmml" xref="S4.SS2.p3.2.m2.3.4.2"><cn type="integer" id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1">0</cn><cn type="integer" id="S4.SS2.p3.2.m2.2.2.cmml" xref="S4.SS2.p3.2.m2.2.2">0</cn><cn type="integer" id="S4.SS2.p3.2.m2.3.3.cmml" xref="S4.SS2.p3.2.m2.3.3">0</cn></vector></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.3c">(0,0,0)</annotation></semantics></math>.
After that, we transform the point clouds back to the camera coordinates of each view and keep <math id="S4.SS2.p3.3.m3.3" class="ltx_Math" alttext="(0,0,0)" display="inline"><semantics id="S4.SS2.p3.3.m3.3a"><mrow id="S4.SS2.p3.3.m3.3.4.2" xref="S4.SS2.p3.3.m3.3.4.1.cmml"><mo stretchy="false" id="S4.SS2.p3.3.m3.3.4.2.1" xref="S4.SS2.p3.3.m3.3.4.1.cmml">(</mo><mn id="S4.SS2.p3.3.m3.1.1" xref="S4.SS2.p3.3.m3.1.1.cmml">0</mn><mo id="S4.SS2.p3.3.m3.3.4.2.2" xref="S4.SS2.p3.3.m3.3.4.1.cmml">,</mo><mn id="S4.SS2.p3.3.m3.2.2" xref="S4.SS2.p3.3.m3.2.2.cmml">0</mn><mo id="S4.SS2.p3.3.m3.3.4.2.3" xref="S4.SS2.p3.3.m3.3.4.1.cmml">,</mo><mn id="S4.SS2.p3.3.m3.3.3" xref="S4.SS2.p3.3.m3.3.3.cmml">0</mn><mo stretchy="false" id="S4.SS2.p3.3.m3.3.4.2.4" xref="S4.SS2.p3.3.m3.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m3.3b"><vector id="S4.SS2.p3.3.m3.3.4.1.cmml" xref="S4.SS2.p3.3.m3.3.4.2"><cn type="integer" id="S4.SS2.p3.3.m3.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1">0</cn><cn type="integer" id="S4.SS2.p3.3.m3.2.2.cmml" xref="S4.SS2.p3.3.m3.2.2">0</cn><cn type="integer" id="S4.SS2.p3.3.m3.3.3.cmml" xref="S4.SS2.p3.3.m3.3.3">0</cn></vector></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m3.3c">(0,0,0)</annotation></semantics></math> unchanged.
This strategy mak some views lose geometric information but still provide valid scene proxies, which effectively reduces overfitting and improves the generalizability of AnyView.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Dynamic Tokens for Inference</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.6" class="ltx_p">AnyView extracts <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="N_{T}" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><msub id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mi id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">N</mi><mi id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">𝑁</ci><ci id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">N_{T}</annotation></semantics></math> scene proxies for a <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mi id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><ci id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">N</annotation></semantics></math>-view input scene. In our setting, <math id="S4.SS3.p1.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS3.p1.3.m3.1a"><mi id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><ci id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">N</annotation></semantics></math> may vary from 1 to 50 during inference time, which results in an order of magnitude change in the number of scene proxies. In previous work such as VoteNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> and 3DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, the encoder of detector extract 1024 or 2048 seed points for one scene, no matter monocular RGB-D input or scene-level point clouds input. To reach this order of magnitude, <math id="S4.SS3.p1.4.m4.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S4.SS3.p1.4.m4.1a"><mi id="S4.SS3.p1.4.m4.1.1" xref="S4.SS3.p1.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.4.m4.1b"><ci id="S4.SS3.p1.4.m4.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.4.m4.1c">T</annotation></semantics></math> should be around 40 when <math id="S4.SS3.p1.5.m5.1" class="ltx_Math" alttext="N=50" display="inline"><semantics id="S4.SS3.p1.5.m5.1a"><mrow id="S4.SS3.p1.5.m5.1.1" xref="S4.SS3.p1.5.m5.1.1.cmml"><mi id="S4.SS3.p1.5.m5.1.1.2" xref="S4.SS3.p1.5.m5.1.1.2.cmml">N</mi><mo id="S4.SS3.p1.5.m5.1.1.1" xref="S4.SS3.p1.5.m5.1.1.1.cmml">=</mo><mn id="S4.SS3.p1.5.m5.1.1.3" xref="S4.SS3.p1.5.m5.1.1.3.cmml">50</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.5.m5.1b"><apply id="S4.SS3.p1.5.m5.1.1.cmml" xref="S4.SS3.p1.5.m5.1.1"><eq id="S4.SS3.p1.5.m5.1.1.1.cmml" xref="S4.SS3.p1.5.m5.1.1.1"></eq><ci id="S4.SS3.p1.5.m5.1.1.2.cmml" xref="S4.SS3.p1.5.m5.1.1.2">𝑁</ci><cn type="integer" id="S4.SS3.p1.5.m5.1.1.3.cmml" xref="S4.SS3.p1.5.m5.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.5.m5.1c">N=50</annotation></semantics></math> and be around 2000 when <math id="S4.SS3.p1.6.m6.1" class="ltx_Math" alttext="N=1" display="inline"><semantics id="S4.SS3.p1.6.m6.1a"><mrow id="S4.SS3.p1.6.m6.1.1" xref="S4.SS3.p1.6.m6.1.1.cmml"><mi id="S4.SS3.p1.6.m6.1.1.2" xref="S4.SS3.p1.6.m6.1.1.2.cmml">N</mi><mo id="S4.SS3.p1.6.m6.1.1.1" xref="S4.SS3.p1.6.m6.1.1.1.cmml">=</mo><mn id="S4.SS3.p1.6.m6.1.1.3" xref="S4.SS3.p1.6.m6.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.6.m6.1b"><apply id="S4.SS3.p1.6.m6.1.1.cmml" xref="S4.SS3.p1.6.m6.1.1"><eq id="S4.SS3.p1.6.m6.1.1.1.cmml" xref="S4.SS3.p1.6.m6.1.1.1"></eq><ci id="S4.SS3.p1.6.m6.1.1.2.cmml" xref="S4.SS3.p1.6.m6.1.1.2">𝑁</ci><cn type="integer" id="S4.SS3.p1.6.m6.1.1.3.cmml" xref="S4.SS3.p1.6.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.6.m6.1c">N=1</annotation></semantics></math>. For the former case, AnyView only extracts 40 scene proxies for monocular input, which is not enough to fully represent the scene. While for the latter case, the number of scene proxies will be larger than 10000 when there are more than 10 views. This heavy computation cost is unaffordable in practical applications.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">To our best knowledge, all previous networks which use SA layers to extract point features keep a fixed output point number during training and inference. However, as the parameters in SA layer are only relevant with the number of channels, changing the output number seems feasible. The reason why previous methods fix the output number are two-fold: 1) if the output number of a SA layer is changed, the shape of input tensor for the next layer will change and may be incompatible with the parameters; 2) even if the next layer can process input in dynamic shape, the density of point clouds in this level is different from the one during training, which makes the layer unable to extract accurate feature representation.
To overcome this problem in our case, we devise a dynamic token strategy for the geometry learner by keeping the output number of the first SA layer fixed and change the output number of the second one. Thus <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mi id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><ci id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">T</annotation></semantics></math> can be defined as:</p>
<table id="S4.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E5.m1.2" class="ltx_Math" alttext="T={\rm min}\{Z/N,O_{SA_{1}}\}" display="block"><semantics id="S4.E5.m1.2a"><mrow id="S4.E5.m1.2.2" xref="S4.E5.m1.2.2.cmml"><mi id="S4.E5.m1.2.2.4" xref="S4.E5.m1.2.2.4.cmml">T</mi><mo id="S4.E5.m1.2.2.3" xref="S4.E5.m1.2.2.3.cmml">=</mo><mrow id="S4.E5.m1.2.2.2" xref="S4.E5.m1.2.2.2.cmml"><mi id="S4.E5.m1.2.2.2.4" xref="S4.E5.m1.2.2.2.4.cmml">min</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.2.2.2.3" xref="S4.E5.m1.2.2.2.3.cmml">​</mo><mrow id="S4.E5.m1.2.2.2.2.2" xref="S4.E5.m1.2.2.2.2.3.cmml"><mo stretchy="false" id="S4.E5.m1.2.2.2.2.2.3" xref="S4.E5.m1.2.2.2.2.3.cmml">{</mo><mrow id="S4.E5.m1.1.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.1.1.cmml"><mi id="S4.E5.m1.1.1.1.1.1.1.2" xref="S4.E5.m1.1.1.1.1.1.1.2.cmml">Z</mi><mo id="S4.E5.m1.1.1.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.1.1.1.cmml">/</mo><mi id="S4.E5.m1.1.1.1.1.1.1.3" xref="S4.E5.m1.1.1.1.1.1.1.3.cmml">N</mi></mrow><mo id="S4.E5.m1.2.2.2.2.2.4" xref="S4.E5.m1.2.2.2.2.3.cmml">,</mo><msub id="S4.E5.m1.2.2.2.2.2.2" xref="S4.E5.m1.2.2.2.2.2.2.cmml"><mi id="S4.E5.m1.2.2.2.2.2.2.2" xref="S4.E5.m1.2.2.2.2.2.2.2.cmml">O</mi><mrow id="S4.E5.m1.2.2.2.2.2.2.3" xref="S4.E5.m1.2.2.2.2.2.2.3.cmml"><mi id="S4.E5.m1.2.2.2.2.2.2.3.2" xref="S4.E5.m1.2.2.2.2.2.2.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.2.2.2.2.2.2.3.1" xref="S4.E5.m1.2.2.2.2.2.2.3.1.cmml">​</mo><msub id="S4.E5.m1.2.2.2.2.2.2.3.3" xref="S4.E5.m1.2.2.2.2.2.2.3.3.cmml"><mi id="S4.E5.m1.2.2.2.2.2.2.3.3.2" xref="S4.E5.m1.2.2.2.2.2.2.3.3.2.cmml">A</mi><mn id="S4.E5.m1.2.2.2.2.2.2.3.3.3" xref="S4.E5.m1.2.2.2.2.2.2.3.3.3.cmml">1</mn></msub></mrow></msub><mo stretchy="false" id="S4.E5.m1.2.2.2.2.2.5" xref="S4.E5.m1.2.2.2.2.3.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E5.m1.2b"><apply id="S4.E5.m1.2.2.cmml" xref="S4.E5.m1.2.2"><eq id="S4.E5.m1.2.2.3.cmml" xref="S4.E5.m1.2.2.3"></eq><ci id="S4.E5.m1.2.2.4.cmml" xref="S4.E5.m1.2.2.4">𝑇</ci><apply id="S4.E5.m1.2.2.2.cmml" xref="S4.E5.m1.2.2.2"><times id="S4.E5.m1.2.2.2.3.cmml" xref="S4.E5.m1.2.2.2.3"></times><ci id="S4.E5.m1.2.2.2.4.cmml" xref="S4.E5.m1.2.2.2.4">min</ci><set id="S4.E5.m1.2.2.2.2.3.cmml" xref="S4.E5.m1.2.2.2.2.2"><apply id="S4.E5.m1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1"><divide id="S4.E5.m1.1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1"></divide><ci id="S4.E5.m1.1.1.1.1.1.1.2.cmml" xref="S4.E5.m1.1.1.1.1.1.1.2">𝑍</ci><ci id="S4.E5.m1.1.1.1.1.1.1.3.cmml" xref="S4.E5.m1.1.1.1.1.1.1.3">𝑁</ci></apply><apply id="S4.E5.m1.2.2.2.2.2.2.cmml" xref="S4.E5.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.E5.m1.2.2.2.2.2.2.1.cmml" xref="S4.E5.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S4.E5.m1.2.2.2.2.2.2.2.cmml" xref="S4.E5.m1.2.2.2.2.2.2.2">𝑂</ci><apply id="S4.E5.m1.2.2.2.2.2.2.3.cmml" xref="S4.E5.m1.2.2.2.2.2.2.3"><times id="S4.E5.m1.2.2.2.2.2.2.3.1.cmml" xref="S4.E5.m1.2.2.2.2.2.2.3.1"></times><ci id="S4.E5.m1.2.2.2.2.2.2.3.2.cmml" xref="S4.E5.m1.2.2.2.2.2.2.3.2">𝑆</ci><apply id="S4.E5.m1.2.2.2.2.2.2.3.3.cmml" xref="S4.E5.m1.2.2.2.2.2.2.3.3"><csymbol cd="ambiguous" id="S4.E5.m1.2.2.2.2.2.2.3.3.1.cmml" xref="S4.E5.m1.2.2.2.2.2.2.3.3">subscript</csymbol><ci id="S4.E5.m1.2.2.2.2.2.2.3.3.2.cmml" xref="S4.E5.m1.2.2.2.2.2.2.3.3.2">𝐴</ci><cn type="integer" id="S4.E5.m1.2.2.2.2.2.2.3.3.3.cmml" xref="S4.E5.m1.2.2.2.2.2.2.3.3.3">1</cn></apply></apply></apply></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5.m1.2c">T={\rm min}\{Z/N,O_{SA_{1}}\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S4.SS3.p2.3" class="ltx_p">where <math id="S4.SS3.p2.2.m1.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S4.SS3.p2.2.m1.1a"><mi id="S4.SS3.p2.2.m1.1.1" xref="S4.SS3.p2.2.m1.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m1.1b"><ci id="S4.SS3.p2.2.m1.1.1.cmml" xref="S4.SS3.p2.2.m1.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m1.1c">Z</annotation></semantics></math> is a predefined constant and <math id="S4.SS3.p2.3.m2.1" class="ltx_Math" alttext="O_{SA_{1}}" display="inline"><semantics id="S4.SS3.p2.3.m2.1a"><msub id="S4.SS3.p2.3.m2.1.1" xref="S4.SS3.p2.3.m2.1.1.cmml"><mi id="S4.SS3.p2.3.m2.1.1.2" xref="S4.SS3.p2.3.m2.1.1.2.cmml">O</mi><mrow id="S4.SS3.p2.3.m2.1.1.3" xref="S4.SS3.p2.3.m2.1.1.3.cmml"><mi id="S4.SS3.p2.3.m2.1.1.3.2" xref="S4.SS3.p2.3.m2.1.1.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.3.m2.1.1.3.1" xref="S4.SS3.p2.3.m2.1.1.3.1.cmml">​</mo><msub id="S4.SS3.p2.3.m2.1.1.3.3" xref="S4.SS3.p2.3.m2.1.1.3.3.cmml"><mi id="S4.SS3.p2.3.m2.1.1.3.3.2" xref="S4.SS3.p2.3.m2.1.1.3.3.2.cmml">A</mi><mn id="S4.SS3.p2.3.m2.1.1.3.3.3" xref="S4.SS3.p2.3.m2.1.1.3.3.3.cmml">1</mn></msub></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m2.1b"><apply id="S4.SS3.p2.3.m2.1.1.cmml" xref="S4.SS3.p2.3.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.3.m2.1.1.1.cmml" xref="S4.SS3.p2.3.m2.1.1">subscript</csymbol><ci id="S4.SS3.p2.3.m2.1.1.2.cmml" xref="S4.SS3.p2.3.m2.1.1.2">𝑂</ci><apply id="S4.SS3.p2.3.m2.1.1.3.cmml" xref="S4.SS3.p2.3.m2.1.1.3"><times id="S4.SS3.p2.3.m2.1.1.3.1.cmml" xref="S4.SS3.p2.3.m2.1.1.3.1"></times><ci id="S4.SS3.p2.3.m2.1.1.3.2.cmml" xref="S4.SS3.p2.3.m2.1.1.3.2">𝑆</ci><apply id="S4.SS3.p2.3.m2.1.1.3.3.cmml" xref="S4.SS3.p2.3.m2.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS3.p2.3.m2.1.1.3.3.1.cmml" xref="S4.SS3.p2.3.m2.1.1.3.3">subscript</csymbol><ci id="S4.SS3.p2.3.m2.1.1.3.3.2.cmml" xref="S4.SS3.p2.3.m2.1.1.3.3.2">𝐴</ci><cn type="integer" id="S4.SS3.p2.3.m2.1.1.3.3.3.cmml" xref="S4.SS3.p2.3.m2.1.1.3.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m2.1c">O_{SA_{1}}</annotation></semantics></math> is the the output number of the first SA layer.
In this way, the second SA layer receives the same number of input points as in training time and thus the output features (i.e. scene proxies) are accurate. Although the number of scene proxies varies, the transformer encoder is still able to interact and refine them due to the linear combination nature of self-attention mechanism and our scale-independent training strategy.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.3" class="ltx_p">Meanwhile, our proposed AnyView can be well applied to the online detection task as it extracts features <math id="S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{P}_{i}" display="inline"><semantics id="S4.SS3.p3.1.m1.1a"><msub id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS3.p3.1.m1.1.1.2" xref="S4.SS3.p3.1.m1.1.1.2.cmml">𝒫</mi><mi id="S4.SS3.p3.1.m1.1.1.3" xref="S4.SS3.p3.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><apply id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p3.1.m1.1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p3.1.m1.1.1.2.cmml" xref="S4.SS3.p3.1.m1.1.1.2">𝒫</ci><ci id="S4.SS3.p3.1.m1.1.1.3.cmml" xref="S4.SS3.p3.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">\mathcal{P}_{i}</annotation></semantics></math> individually for each view <math id="S4.SS3.p3.2.m2.1" class="ltx_Math" alttext="V_{i}" display="inline"><semantics id="S4.SS3.p3.2.m2.1a"><msub id="S4.SS3.p3.2.m2.1.1" xref="S4.SS3.p3.2.m2.1.1.cmml"><mi id="S4.SS3.p3.2.m2.1.1.2" xref="S4.SS3.p3.2.m2.1.1.2.cmml">V</mi><mi id="S4.SS3.p3.2.m2.1.1.3" xref="S4.SS3.p3.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.2.m2.1b"><apply id="S4.SS3.p3.2.m2.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p3.2.m2.1.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.p3.2.m2.1.1.2.cmml" xref="S4.SS3.p3.2.m2.1.1.2">𝑉</ci><ci id="S4.SS3.p3.2.m2.1.1.3.cmml" xref="S4.SS3.p3.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m2.1c">V_{i}</annotation></semantics></math> when processing RGB-D inputs and then integrates the features of different views <math id="S4.SS3.p3.3.m3.3" class="ltx_Math" alttext="\left\{\mathcal{P}_{1},...,\mathcal{P}_{i}\right\}" display="inline"><semantics id="S4.SS3.p3.3.m3.3a"><mrow id="S4.SS3.p3.3.m3.3.3.2" xref="S4.SS3.p3.3.m3.3.3.3.cmml"><mo id="S4.SS3.p3.3.m3.3.3.2.3" xref="S4.SS3.p3.3.m3.3.3.3.cmml">{</mo><msub id="S4.SS3.p3.3.m3.2.2.1.1" xref="S4.SS3.p3.3.m3.2.2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS3.p3.3.m3.2.2.1.1.2" xref="S4.SS3.p3.3.m3.2.2.1.1.2.cmml">𝒫</mi><mn id="S4.SS3.p3.3.m3.2.2.1.1.3" xref="S4.SS3.p3.3.m3.2.2.1.1.3.cmml">1</mn></msub><mo id="S4.SS3.p3.3.m3.3.3.2.4" xref="S4.SS3.p3.3.m3.3.3.3.cmml">,</mo><mi mathvariant="normal" id="S4.SS3.p3.3.m3.1.1" xref="S4.SS3.p3.3.m3.1.1.cmml">…</mi><mo id="S4.SS3.p3.3.m3.3.3.2.5" xref="S4.SS3.p3.3.m3.3.3.3.cmml">,</mo><msub id="S4.SS3.p3.3.m3.3.3.2.2" xref="S4.SS3.p3.3.m3.3.3.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS3.p3.3.m3.3.3.2.2.2" xref="S4.SS3.p3.3.m3.3.3.2.2.2.cmml">𝒫</mi><mi id="S4.SS3.p3.3.m3.3.3.2.2.3" xref="S4.SS3.p3.3.m3.3.3.2.2.3.cmml">i</mi></msub><mo id="S4.SS3.p3.3.m3.3.3.2.6" xref="S4.SS3.p3.3.m3.3.3.3.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.3.m3.3b"><set id="S4.SS3.p3.3.m3.3.3.3.cmml" xref="S4.SS3.p3.3.m3.3.3.2"><apply id="S4.SS3.p3.3.m3.2.2.1.1.cmml" xref="S4.SS3.p3.3.m3.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p3.3.m3.2.2.1.1.1.cmml" xref="S4.SS3.p3.3.m3.2.2.1.1">subscript</csymbol><ci id="S4.SS3.p3.3.m3.2.2.1.1.2.cmml" xref="S4.SS3.p3.3.m3.2.2.1.1.2">𝒫</ci><cn type="integer" id="S4.SS3.p3.3.m3.2.2.1.1.3.cmml" xref="S4.SS3.p3.3.m3.2.2.1.1.3">1</cn></apply><ci id="S4.SS3.p3.3.m3.1.1.cmml" xref="S4.SS3.p3.3.m3.1.1">…</ci><apply id="S4.SS3.p3.3.m3.3.3.2.2.cmml" xref="S4.SS3.p3.3.m3.3.3.2.2"><csymbol cd="ambiguous" id="S4.SS3.p3.3.m3.3.3.2.2.1.cmml" xref="S4.SS3.p3.3.m3.3.3.2.2">subscript</csymbol><ci id="S4.SS3.p3.3.m3.3.3.2.2.2.cmml" xref="S4.SS3.p3.3.m3.3.3.2.2.2">𝒫</ci><ci id="S4.SS3.p3.3.m3.3.3.2.2.3.cmml" xref="S4.SS3.p3.3.m3.3.3.2.2.3">𝑖</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.3.m3.3c">\left\{\mathcal{P}_{1},...,\mathcal{P}_{i}\right\}</annotation></semantics></math> in the world coordinate system through an attention mechanism.
The ability to detect objects online fast and accurately in dynamic 3D scenes is crucial for autonomous navigation, environment understanding, and other tasks in the field of autonomous driving and robotics.
Our proposed method provides a reliable and efficient solution for online detection, which can enable robots to effectively perceive and interact with the environment.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Experiment</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we conduct experiments to show the generalizability of our approach. We first describe the datasets and experimental settings. Then we comprehensively compare our approach with previous indoor 3D object detection methods on a series of benchmarks. Visualization results are provided for more intuitive comparison. Finally we design several ablation studies to verify the effectiveness of our architecture design and training/inference strategies.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Number of parameters of different 3D detectors.</figcaption>
<table id="S5.T2.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.1.1.1" class="ltx_tr">
<td id="S5.T2.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T2.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.1.1.1.1.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T2.1.1.1.1.1.1.1" class="ltx_text" style="font-size:80%;">ImVoteNet</span></span>
</span>
</td>
<td id="S5.T2.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T2.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.1.1.1.2.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T2.1.1.1.2.1.1.1" class="ltx_text" style="font-size:80%;">ImVoxelNet</span></span>
</span>
</td>
<td id="S5.T2.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T2.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.1.1.1.3.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T2.1.1.1.3.1.1.1" class="ltx_text" style="font-size:80%;">3DETR-m</span></span>
</span>
</td>
<td id="S5.T2.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T2.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.1.1.1.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T2.1.1.1.4.1.1.1" class="ltx_text" style="font-size:80%;">AnyView</span></span>
</span>
</td>
</tr>
<tr id="S5.T2.1.2.2" class="ltx_tr">
<td id="S5.T2.1.2.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T2.1.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.1.2.2.1.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T2.1.2.2.1.1.1.1" class="ltx_text" style="font-size:80%;">1.8M</span></span>
</span>
</td>
<td id="S5.T2.1.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T2.1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.1.2.2.2.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T2.1.2.2.2.1.1.1" class="ltx_text" style="font-size:80%;">104.6M</span></span>
</span>
</td>
<td id="S5.T2.1.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T2.1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.1.2.2.3.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T2.1.2.2.3.1.1.1" class="ltx_text" style="font-size:80%;">7.4M</span></span>
</span>
</td>
<td id="S5.T2.1.2.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S5.T2.1.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.1.2.2.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T2.1.2.2.4.1.1.1" class="ltx_text" style="font-size:80%;">7.6M</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.5.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.6.2" class="ltx_text ltx_font_italic">Experiments Setup</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p"><span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_bold">Datasets and benchmarks:</span>
We conduct experiments on the ScanNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> dataset, which is a richly annotated dataset of indoor scenes with 1201 training scenes and 312 validation scenes.
For each scene, ScanNet provides RGB-D video as well as the reconstructed mesh. We uniformly sample 50 frames from the video for each scene, which serves as the maximum data available for training and evaluating in our proposed setting.
Meanwhile, we further validate that the proposed approach has the ability to accurately detect objects in practical deployment scenes by verifying the performance of each model on the online detection task.
We set three benchmarks as defined in Section <a href="#S3.SS1" title="III-A Problem Statement ‣ III Analysis ‣ Anyview: Generalizable Indoor 3D Object Detection with Variable Frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>. ScanNet-Rec is the mainstream scene-level benchmark used by previous works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, whose input data is reconstructed point cloud of a whole scene and ground-truth is generated by computing the axis-aligned bounding boxes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> for objects in 18 selected categories.
In addition, we designed ScanNet-Online benchmarks to explore the performance of previous 3D object detection models with AnyView in practical deployment scenes. Specifically, the input to ScanNet-Online is each frame in a sequence of RGB-D images, and the model needs to dynamically detect objects in the scene based on the new input frames.
ScanNet-MV shares the same ground truth with ScanNet-Rec, while the input data are multi-view RGB-D images. If not additionally mentioned, the default number of views is 50. ScanNet-MV is also a scene-level benchmark, which means all RGB-D images from one scene are considered as a single input sample for the network.
ScanNet-SV is a monocular RGB-D benchmark with a single view RGB-D image as input data. For each image, we select the bounding boxes whose center points are within the image from the corresponding scene as its ground truth..
ScanNet-Online utilizes the ScanNet-MV benchmark in the 50 views setting, and the input is each frame of sequential RGB-D images.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p"><span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_bold">Compared methods:</span>
We train VoteNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> and ImVoteNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> on ScanNet-SV. When applying the models on ScanNet-MV, we predict bounding boxes for each view and fuse the predictions by 3D NMS.
3DETR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> is chosen as the model trained on ScanNet-Rec, which is the mainstream transformer-based 3D detector with less inductive bias.
The comparison between 3DETR and AnyView is particularly relevant since they share similar amount of parameters and both adopt DETR-like detection decoder.
To apply 3DETR on ScanNet-MV, we preprocess the multi-view RGB-D images by fusing the depth maps from each view according to the camera parameters.
We train our AnyView model as well as 3DETR and ImVoxelNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> on ScanNet-MV, each model representing an input modality: 3DETR for scene-level point clouds, ImVoxelNet for multi-view RGB and AnyView for multi-view RGB-D.
We list the number of parameters of these models in Table <a href="#S5.T2" title="TABLE II ‣ V Experiment ‣ Anyview: Generalizable Indoor 3D Object Detection with Variable Frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">TABLE III: </span>3D object detection results (mAP@0.25 and mAP@0.5) and corresponding training/inference setting of different methods on scene-level and monocular benchmarks. Contents in bracket indicate the modality used by the methods.</figcaption>
<table id="S5.T3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T3.3.1.1" class="ltx_tr">
<th id="S5.T3.3.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"></th>
<th id="S5.T3.3.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="2"><span id="S5.T3.3.1.1.2.1" class="ltx_text" style="font-size:90%;">Method</span></th>
<td id="S5.T3.3.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Training</span></td>
<td id="S5.T3.3.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Evaluation</span></td>
<td id="S5.T3.3.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="2"><span id="S5.T3.3.1.1.5.1" class="ltx_text" style="font-size:90%;">mAP</span></td>
</tr>
<tr id="S5.T3.3.2.2" class="ltx_tr">
<th id="S5.T3.3.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></th>
<td id="S5.T3.3.2.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.2.2.2.1" class="ltx_text" style="font-size:90%;">Benchmark</span></td>
<td id="S5.T3.3.2.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.2.2.3.1" class="ltx_text" style="font-size:90%;">Benchmark</span></td>
<td id="S5.T3.3.2.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.2.2.4.1" class="ltx_text" style="font-size:90%;">@0.25</span></td>
<td id="S5.T3.3.2.2.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.2.2.5.1" class="ltx_text" style="font-size:90%;">@0.5</span></td>
</tr>
<tr id="S5.T3.3.3.3" class="ltx_tr">
<th id="S5.T3.3.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="12"><span id="S5.T3.3.3.3.1.1" class="ltx_text" style="font-size:90%;">
<span id="S5.T3.3.3.3.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.3pt;height:42.5pt;vertical-align:-18.1pt;"><span class="ltx_transformed_inner" style="width:42.5pt;transform:translate(-18.13pt,0pt) rotate(-90deg) ;">
<span id="S5.T3.3.3.3.1.1.1.1" class="ltx_p">Scene-level</span>
</span></span></span></th>
<th id="S5.T3.3.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.3.3.2.1" class="ltx_text" style="font-size:90%;">3DETR</span></th>
<td id="S5.T3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.3.3.3.1" class="ltx_text" style="font-size:90%;">Rec</span></td>
<td id="S5.T3.3.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.3.3.4.1" class="ltx_text" style="font-size:90%;">Rec</span></td>
<td id="S5.T3.3.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.3.3.5.1" class="ltx_text" style="font-size:90%;">62.7</span></td>
<td id="S5.T3.3.3.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.3.3.6.1" class="ltx_text" style="font-size:90%;">37.5</span></td>
</tr>
<tr id="S5.T3.3.4.4" class="ltx_tr">
<th id="S5.T3.3.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.4.4.1.1" class="ltx_text" style="font-size:90%;">3DETR-m</span></th>
<td id="S5.T3.3.4.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.4.4.2.1" class="ltx_text" style="font-size:90%;">Rec</span></td>
<td id="S5.T3.3.4.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.4.4.3.1" class="ltx_text" style="font-size:90%;">Rec</span></td>
<td id="S5.T3.3.4.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.4.4.4.1" class="ltx_text" style="font-size:90%;">65.0</span></td>
<td id="S5.T3.3.4.4.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.4.4.5.1" class="ltx_text" style="font-size:90%;">47.0</span></td>
</tr>
<tr id="S5.T3.3.5.5" class="ltx_tr">
<th id="S5.T3.3.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.5.5.1.1" class="ltx_text" style="font-size:90%;">3DETR</span></th>
<td id="S5.T3.3.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.5.5.2.1" class="ltx_text" style="font-size:90%;">MV(D)</span></td>
<td id="S5.T3.3.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.5.5.3.1" class="ltx_text" style="font-size:90%;">Rec</span></td>
<td id="S5.T3.3.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.5.5.4.1" class="ltx_text" style="font-size:90%;">53.6</span></td>
<td id="S5.T3.3.5.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.5.5.5.1" class="ltx_text" style="font-size:90%;">33.0</span></td>
</tr>
<tr id="S5.T3.3.6.6" class="ltx_tr">
<th id="S5.T3.3.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.6.6.1.1" class="ltx_text" style="font-size:90%;">3DETR-m</span></th>
<td id="S5.T3.3.6.6.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.6.6.2.1" class="ltx_text" style="font-size:90%;">MV(D)</span></td>
<td id="S5.T3.3.6.6.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.6.6.3.1" class="ltx_text" style="font-size:90%;">Rec</span></td>
<td id="S5.T3.3.6.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.6.6.4.1" class="ltx_text" style="font-size:90%;">56.9</span></td>
<td id="S5.T3.3.6.6.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.6.6.5.1" class="ltx_text" style="font-size:90%;">37.8</span></td>
</tr>
<tr id="S5.T3.3.7.7" class="ltx_tr">
<th id="S5.T3.3.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.7.7.1.1" class="ltx_text" style="font-size:90%;">3DETR</span></th>
<td id="S5.T3.3.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.7.7.2.1" class="ltx_text" style="font-size:90%;">Rec</span></td>
<td id="S5.T3.3.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.7.7.3.1" class="ltx_text" style="font-size:90%;">MV(D)</span></td>
<td id="S5.T3.3.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.7.7.4.1" class="ltx_text" style="font-size:90%;">37.8</span></td>
<td id="S5.T3.3.7.7.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.7.7.5.1" class="ltx_text" style="font-size:90%;">22.2</span></td>
</tr>
<tr id="S5.T3.3.8.8" class="ltx_tr">
<th id="S5.T3.3.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.8.8.1.1" class="ltx_text" style="font-size:90%;">3DETR-m</span></th>
<td id="S5.T3.3.8.8.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.8.8.2.1" class="ltx_text" style="font-size:90%;">Rec</span></td>
<td id="S5.T3.3.8.8.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.8.8.3.1" class="ltx_text" style="font-size:90%;">MV(D)</span></td>
<td id="S5.T3.3.8.8.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.8.8.4.1" class="ltx_text" style="font-size:90%;">44.7</span></td>
<td id="S5.T3.3.8.8.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.8.8.5.1" class="ltx_text" style="font-size:90%;">25.6</span></td>
</tr>
<tr id="S5.T3.3.9.9" class="ltx_tr">
<th id="S5.T3.3.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.9.9.1.1" class="ltx_text" style="font-size:90%;">VoteNet</span></th>
<td id="S5.T3.3.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.9.9.2.1" class="ltx_text" style="font-size:90%;">SV(D)</span></td>
<td id="S5.T3.3.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.9.9.3.1" class="ltx_text" style="font-size:90%;">MV(D)</span></td>
<td id="S5.T3.3.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.9.9.4.1" class="ltx_text" style="font-size:90%;">40.9</span></td>
<td id="S5.T3.3.9.9.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.9.9.5.1" class="ltx_text" style="font-size:90%;">20.6</span></td>
</tr>
<tr id="S5.T3.3.10.10" class="ltx_tr">
<th id="S5.T3.3.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.10.10.1.1" class="ltx_text" style="font-size:90%;">ImVoteNet</span></th>
<td id="S5.T3.3.10.10.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.10.10.2.1" class="ltx_text" style="font-size:90%;">SV(RGBD)</span></td>
<td id="S5.T3.3.10.10.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.10.10.3.1" class="ltx_text" style="font-size:90%;">MV(RGBD)</span></td>
<td id="S5.T3.3.10.10.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.10.10.4.1" class="ltx_text" style="font-size:90%;">43.3</span></td>
<td id="S5.T3.3.10.10.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.10.10.5.1" class="ltx_text" style="font-size:90%;">20.4</span></td>
</tr>
<tr id="S5.T3.3.11.11" class="ltx_tr">
<th id="S5.T3.3.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.11.11.1.1" class="ltx_text" style="font-size:90%;">ImVoxelNet</span></th>
<td id="S5.T3.3.11.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.11.11.2.1" class="ltx_text" style="font-size:90%;">MV(RGB)</span></td>
<td id="S5.T3.3.11.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.11.11.3.1" class="ltx_text" style="font-size:90%;">MV(RGB)</span></td>
<td id="S5.T3.3.11.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.11.11.4.1" class="ltx_text" style="font-size:90%;">46.6</span></td>
<td id="S5.T3.3.11.11.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.11.11.5.1" class="ltx_text" style="font-size:90%;">25.2</span></td>
</tr>
<tr id="S5.T3.3.12.12" class="ltx_tr">
<th id="S5.T3.3.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.12.12.1.1" class="ltx_text" style="font-size:90%;">3DETR</span></th>
<td id="S5.T3.3.12.12.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.12.12.2.1" class="ltx_text" style="font-size:90%;">MV(D)</span></td>
<td id="S5.T3.3.12.12.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.12.12.3.1" class="ltx_text" style="font-size:90%;">MV(D)</span></td>
<td id="S5.T3.3.12.12.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.12.12.4.1" class="ltx_text" style="font-size:90%;">51.7</span></td>
<td id="S5.T3.3.12.12.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.12.12.5.1" class="ltx_text" style="font-size:90%;">31.0</span></td>
</tr>
<tr id="S5.T3.3.13.13" class="ltx_tr">
<th id="S5.T3.3.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.13.13.1.1" class="ltx_text" style="font-size:90%;">3DETR-m</span></th>
<td id="S5.T3.3.13.13.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.13.13.2.1" class="ltx_text" style="font-size:90%;">MV(D)</span></td>
<td id="S5.T3.3.13.13.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.13.13.3.1" class="ltx_text" style="font-size:90%;">MV(D)</span></td>
<td id="S5.T3.3.13.13.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.13.13.4.1" class="ltx_text" style="font-size:90%;">54.7</span></td>
<td id="S5.T3.3.13.13.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.13.13.5.1" class="ltx_text" style="font-size:90%;">35.3</span></td>
</tr>
<tr id="S5.T3.3.14.14" class="ltx_tr">
<th id="S5.T3.3.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.14.14.1.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;">AnyView</span></th>
<td id="S5.T3.3.14.14.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.14.14.2.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;">MV(D)</span></td>
<td id="S5.T3.3.14.14.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.14.14.3.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;">MV(D)</span></td>
<td id="S5.T3.3.14.14.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.14.14.4.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;">60.7</span></td>
<td id="S5.T3.3.14.14.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.14.14.5.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;">35.8</span></td>
</tr>
<tr id="S5.T3.3.15.15" class="ltx_tr">
<th id="S5.T3.3.15.15.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="8"><span id="S5.T3.3.15.15.1.1" class="ltx_text" style="font-size:90%;">
<span id="S5.T3.3.15.15.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.3pt;height:42pt;vertical-align:-17.9pt;"><span class="ltx_transformed_inner" style="width:42.0pt;transform:translate(-17.89pt,0pt) rotate(-90deg) ;">
<span id="S5.T3.3.15.15.1.1.1.1" class="ltx_p">Monocular</span>
</span></span></span></th>
<th id="S5.T3.3.15.15.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.15.15.2.1" class="ltx_text" style="font-size:90%;">VoteNet</span></th>
<td id="S5.T3.3.15.15.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.15.15.3.1" class="ltx_text" style="font-size:90%;">SV(D)</span></td>
<td id="S5.T3.3.15.15.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.15.15.4.1" class="ltx_text" style="font-size:90%;">SV(D)</span></td>
<td id="S5.T3.3.15.15.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.15.15.5.1" class="ltx_text" style="font-size:90%;">30.1</span></td>
<td id="S5.T3.3.15.15.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.15.15.6.1" class="ltx_text" style="font-size:90%;">13.9</span></td>
</tr>
<tr id="S5.T3.3.16.16" class="ltx_tr">
<th id="S5.T3.3.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.16.16.1.1" class="ltx_text" style="font-size:90%;">ImVoteNet</span></th>
<td id="S5.T3.3.16.16.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.16.16.2.1" class="ltx_text" style="font-size:90%;">SV(RGBD)</span></td>
<td id="S5.T3.3.16.16.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.16.16.3.1" class="ltx_text" style="font-size:90%;">SV(RGBD)</span></td>
<td id="S5.T3.3.16.16.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.16.16.4.1" class="ltx_text" style="font-size:90%;">31.6</span></td>
<td id="S5.T3.3.16.16.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.16.16.5.1" class="ltx_text" style="font-size:90%;">13.5</span></td>
</tr>
<tr id="S5.T3.3.17.17" class="ltx_tr">
<th id="S5.T3.3.17.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.17.17.1.1" class="ltx_text" style="font-size:90%;">3DETR</span></th>
<td id="S5.T3.3.17.17.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.17.17.2.1" class="ltx_text" style="font-size:90%;">Rec</span></td>
<td id="S5.T3.3.17.17.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.17.17.3.1" class="ltx_text" style="font-size:90%;">SV(D)</span></td>
<td id="S5.T3.3.17.17.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.17.17.4.1" class="ltx_text" style="font-size:90%;">14.7</span></td>
<td id="S5.T3.3.17.17.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.17.17.5.1" class="ltx_text" style="font-size:90%;">3.9</span></td>
</tr>
<tr id="S5.T3.3.18.18" class="ltx_tr">
<th id="S5.T3.3.18.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.18.18.1.1" class="ltx_text" style="font-size:90%;">3DETR-m</span></th>
<td id="S5.T3.3.18.18.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.18.18.2.1" class="ltx_text" style="font-size:90%;">Rec</span></td>
<td id="S5.T3.3.18.18.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.18.18.3.1" class="ltx_text" style="font-size:90%;">SV(D)</span></td>
<td id="S5.T3.3.18.18.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.18.18.4.1" class="ltx_text" style="font-size:90%;">19.8</span></td>
<td id="S5.T3.3.18.18.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.18.18.5.1" class="ltx_text" style="font-size:90%;">6.1</span></td>
</tr>
<tr id="S5.T3.3.19.19" class="ltx_tr">
<th id="S5.T3.3.19.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.19.19.1.1" class="ltx_text" style="font-size:90%;">ImVoxelNet</span></th>
<td id="S5.T3.3.19.19.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.19.19.2.1" class="ltx_text" style="font-size:90%;">MV(RGB)</span></td>
<td id="S5.T3.3.19.19.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.19.19.3.1" class="ltx_text" style="font-size:90%;">SV(RGB)</span></td>
<td id="S5.T3.3.19.19.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.19.19.4.1" class="ltx_text" style="font-size:90%;">21.2</span></td>
<td id="S5.T3.3.19.19.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.19.19.5.1" class="ltx_text" style="font-size:90%;">8.3</span></td>
</tr>
<tr id="S5.T3.3.20.20" class="ltx_tr">
<th id="S5.T3.3.20.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.20.20.1.1" class="ltx_text" style="font-size:90%;">3DETR</span></th>
<td id="S5.T3.3.20.20.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.20.20.2.1" class="ltx_text" style="font-size:90%;">MV(D)</span></td>
<td id="S5.T3.3.20.20.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.20.20.3.1" class="ltx_text" style="font-size:90%;">SV(D)</span></td>
<td id="S5.T3.3.20.20.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.20.20.4.1" class="ltx_text" style="font-size:90%;">24.7</span></td>
<td id="S5.T3.3.20.20.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.20.20.5.1" class="ltx_text" style="font-size:90%;">10.9</span></td>
</tr>
<tr id="S5.T3.3.21.21" class="ltx_tr">
<th id="S5.T3.3.21.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.21.21.1.1" class="ltx_text" style="font-size:90%;">3DETR-m</span></th>
<td id="S5.T3.3.21.21.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.21.21.2.1" class="ltx_text" style="font-size:90%;">MV(D)</span></td>
<td id="S5.T3.3.21.21.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.21.21.3.1" class="ltx_text" style="font-size:90%;">SV(D)</span></td>
<td id="S5.T3.3.21.21.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.21.21.4.1" class="ltx_text" style="font-size:90%;">27.7</span></td>
<td id="S5.T3.3.21.21.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.21.21.5.1" class="ltx_text" style="font-size:90%;">12.8</span></td>
</tr>
<tr id="S5.T3.3.22.22" class="ltx_tr">
<th id="S5.T3.3.22.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.22.22.1.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;">AnyView</span></th>
<td id="S5.T3.3.22.22.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.22.22.2.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;">MV(D)</span></td>
<td id="S5.T3.3.22.22.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.22.22.3.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;">SV(D)</span></td>
<td id="S5.T3.3.22.22.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.22.22.4.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;">32.7</span></td>
<td id="S5.T3.3.22.22.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T3.3.22.22.5.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;">15.3</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p"><span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_bold">Implementation details:</span>
We downsample the RGB-D videos from ScanNet to a resolution of <math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="320\times 240" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><mrow id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml"><mn id="S5.SS1.p3.1.m1.1.1.2" xref="S5.SS1.p3.1.m1.1.1.2.cmml">320</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS1.p3.1.m1.1.1.1" xref="S5.SS1.p3.1.m1.1.1.1.cmml">×</mo><mn id="S5.SS1.p3.1.m1.1.1.3" xref="S5.SS1.p3.1.m1.1.1.3.cmml">240</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><apply id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1"><times id="S5.SS1.p3.1.m1.1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1.1"></times><cn type="integer" id="S5.SS1.p3.1.m1.1.1.2.cmml" xref="S5.SS1.p3.1.m1.1.1.2">320</cn><cn type="integer" id="S5.SS1.p3.1.m1.1.1.3.cmml" xref="S5.SS1.p3.1.m1.1.1.3">240</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">320\times 240</annotation></semantics></math>.
Following previous setting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, we sample 20000 points per scene for ScanNet-SV and 40000 points per scene for ScanNet-Rec as the input point clouds.
For ScanNet-MV, we sample 20000 points per view for methods that make predictions on each view independently and sample 40000 points per scene for methods that fuse multi-view point clouds as a whole.
While for AnyView, we sample 5000 points per view to reduce computation cost.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.7" class="ltx_p">The geometry learner of AnyView consists of two SA layers, the first with radius <math id="S5.SS1.p4.1.m1.1" class="ltx_Math" alttext="0.2m" display="inline"><semantics id="S5.SS1.p4.1.m1.1a"><mrow id="S5.SS1.p4.1.m1.1.1" xref="S5.SS1.p4.1.m1.1.1.cmml"><mn id="S5.SS1.p4.1.m1.1.1.2" xref="S5.SS1.p4.1.m1.1.1.2.cmml">0.2</mn><mo lspace="0em" rspace="0em" id="S5.SS1.p4.1.m1.1.1.1" xref="S5.SS1.p4.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS1.p4.1.m1.1.1.3" xref="S5.SS1.p4.1.m1.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.1.m1.1b"><apply id="S5.SS1.p4.1.m1.1.1.cmml" xref="S5.SS1.p4.1.m1.1.1"><times id="S5.SS1.p4.1.m1.1.1.1.cmml" xref="S5.SS1.p4.1.m1.1.1.1"></times><cn type="float" id="S5.SS1.p4.1.m1.1.1.2.cmml" xref="S5.SS1.p4.1.m1.1.1.2">0.2</cn><ci id="S5.SS1.p4.1.m1.1.1.3.cmml" xref="S5.SS1.p4.1.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.1.m1.1c">0.2m</annotation></semantics></math>, output number of points 256 and MLP channels <math id="S5.SS1.p4.2.m2.4" class="ltx_Math" alttext="[3,64,128,256]" display="inline"><semantics id="S5.SS1.p4.2.m2.4a"><mrow id="S5.SS1.p4.2.m2.4.5.2" xref="S5.SS1.p4.2.m2.4.5.1.cmml"><mo stretchy="false" id="S5.SS1.p4.2.m2.4.5.2.1" xref="S5.SS1.p4.2.m2.4.5.1.cmml">[</mo><mn id="S5.SS1.p4.2.m2.1.1" xref="S5.SS1.p4.2.m2.1.1.cmml">3</mn><mo id="S5.SS1.p4.2.m2.4.5.2.2" xref="S5.SS1.p4.2.m2.4.5.1.cmml">,</mo><mn id="S5.SS1.p4.2.m2.2.2" xref="S5.SS1.p4.2.m2.2.2.cmml">64</mn><mo id="S5.SS1.p4.2.m2.4.5.2.3" xref="S5.SS1.p4.2.m2.4.5.1.cmml">,</mo><mn id="S5.SS1.p4.2.m2.3.3" xref="S5.SS1.p4.2.m2.3.3.cmml">128</mn><mo id="S5.SS1.p4.2.m2.4.5.2.4" xref="S5.SS1.p4.2.m2.4.5.1.cmml">,</mo><mn id="S5.SS1.p4.2.m2.4.4" xref="S5.SS1.p4.2.m2.4.4.cmml">256</mn><mo stretchy="false" id="S5.SS1.p4.2.m2.4.5.2.5" xref="S5.SS1.p4.2.m2.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.2.m2.4b"><list id="S5.SS1.p4.2.m2.4.5.1.cmml" xref="S5.SS1.p4.2.m2.4.5.2"><cn type="integer" id="S5.SS1.p4.2.m2.1.1.cmml" xref="S5.SS1.p4.2.m2.1.1">3</cn><cn type="integer" id="S5.SS1.p4.2.m2.2.2.cmml" xref="S5.SS1.p4.2.m2.2.2">64</cn><cn type="integer" id="S5.SS1.p4.2.m2.3.3.cmml" xref="S5.SS1.p4.2.m2.3.3">128</cn><cn type="integer" id="S5.SS1.p4.2.m2.4.4.cmml" xref="S5.SS1.p4.2.m2.4.4">256</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.2.m2.4c">[3,64,128,256]</annotation></semantics></math>, the second with radius <math id="S5.SS1.p4.3.m3.1" class="ltx_Math" alttext="0.8m" display="inline"><semantics id="S5.SS1.p4.3.m3.1a"><mrow id="S5.SS1.p4.3.m3.1.1" xref="S5.SS1.p4.3.m3.1.1.cmml"><mn id="S5.SS1.p4.3.m3.1.1.2" xref="S5.SS1.p4.3.m3.1.1.2.cmml">0.8</mn><mo lspace="0em" rspace="0em" id="S5.SS1.p4.3.m3.1.1.1" xref="S5.SS1.p4.3.m3.1.1.1.cmml">​</mo><mi id="S5.SS1.p4.3.m3.1.1.3" xref="S5.SS1.p4.3.m3.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.3.m3.1b"><apply id="S5.SS1.p4.3.m3.1.1.cmml" xref="S5.SS1.p4.3.m3.1.1"><times id="S5.SS1.p4.3.m3.1.1.1.cmml" xref="S5.SS1.p4.3.m3.1.1.1"></times><cn type="float" id="S5.SS1.p4.3.m3.1.1.2.cmml" xref="S5.SS1.p4.3.m3.1.1.2">0.8</cn><ci id="S5.SS1.p4.3.m3.1.1.3.cmml" xref="S5.SS1.p4.3.m3.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.3.m3.1c">0.8m</annotation></semantics></math> and MLP channels <math id="S5.SS1.p4.4.m4.4" class="ltx_Math" alttext="[256,256,256,256]" display="inline"><semantics id="S5.SS1.p4.4.m4.4a"><mrow id="S5.SS1.p4.4.m4.4.5.2" xref="S5.SS1.p4.4.m4.4.5.1.cmml"><mo stretchy="false" id="S5.SS1.p4.4.m4.4.5.2.1" xref="S5.SS1.p4.4.m4.4.5.1.cmml">[</mo><mn id="S5.SS1.p4.4.m4.1.1" xref="S5.SS1.p4.4.m4.1.1.cmml">256</mn><mo id="S5.SS1.p4.4.m4.4.5.2.2" xref="S5.SS1.p4.4.m4.4.5.1.cmml">,</mo><mn id="S5.SS1.p4.4.m4.2.2" xref="S5.SS1.p4.4.m4.2.2.cmml">256</mn><mo id="S5.SS1.p4.4.m4.4.5.2.3" xref="S5.SS1.p4.4.m4.4.5.1.cmml">,</mo><mn id="S5.SS1.p4.4.m4.3.3" xref="S5.SS1.p4.4.m4.3.3.cmml">256</mn><mo id="S5.SS1.p4.4.m4.4.5.2.4" xref="S5.SS1.p4.4.m4.4.5.1.cmml">,</mo><mn id="S5.SS1.p4.4.m4.4.4" xref="S5.SS1.p4.4.m4.4.4.cmml">256</mn><mo stretchy="false" id="S5.SS1.p4.4.m4.4.5.2.5" xref="S5.SS1.p4.4.m4.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.4.m4.4b"><list id="S5.SS1.p4.4.m4.4.5.1.cmml" xref="S5.SS1.p4.4.m4.4.5.2"><cn type="integer" id="S5.SS1.p4.4.m4.1.1.cmml" xref="S5.SS1.p4.4.m4.1.1">256</cn><cn type="integer" id="S5.SS1.p4.4.m4.2.2.cmml" xref="S5.SS1.p4.4.m4.2.2">256</cn><cn type="integer" id="S5.SS1.p4.4.m4.3.3.cmml" xref="S5.SS1.p4.4.m4.3.3">256</cn><cn type="integer" id="S5.SS1.p4.4.m4.4.4.cmml" xref="S5.SS1.p4.4.m4.4.4">256</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.4.m4.4c">[256,256,256,256]</annotation></semantics></math>. During training, we set <math id="S5.SS1.p4.5.m5.1" class="ltx_Math" alttext="T=40" display="inline"><semantics id="S5.SS1.p4.5.m5.1a"><mrow id="S5.SS1.p4.5.m5.1.1" xref="S5.SS1.p4.5.m5.1.1.cmml"><mi id="S5.SS1.p4.5.m5.1.1.2" xref="S5.SS1.p4.5.m5.1.1.2.cmml">T</mi><mo id="S5.SS1.p4.5.m5.1.1.1" xref="S5.SS1.p4.5.m5.1.1.1.cmml">=</mo><mn id="S5.SS1.p4.5.m5.1.1.3" xref="S5.SS1.p4.5.m5.1.1.3.cmml">40</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.5.m5.1b"><apply id="S5.SS1.p4.5.m5.1.1.cmml" xref="S5.SS1.p4.5.m5.1.1"><eq id="S5.SS1.p4.5.m5.1.1.1.cmml" xref="S5.SS1.p4.5.m5.1.1.1"></eq><ci id="S5.SS1.p4.5.m5.1.1.2.cmml" xref="S5.SS1.p4.5.m5.1.1.2">𝑇</ci><cn type="integer" id="S5.SS1.p4.5.m5.1.1.3.cmml" xref="S5.SS1.p4.5.m5.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.5.m5.1c">T=40</annotation></semantics></math>. While in inference time, we set <math id="S5.SS1.p4.6.m6.1" class="ltx_Math" alttext="Z=2000" display="inline"><semantics id="S5.SS1.p4.6.m6.1a"><mrow id="S5.SS1.p4.6.m6.1.1" xref="S5.SS1.p4.6.m6.1.1.cmml"><mi id="S5.SS1.p4.6.m6.1.1.2" xref="S5.SS1.p4.6.m6.1.1.2.cmml">Z</mi><mo id="S5.SS1.p4.6.m6.1.1.1" xref="S5.SS1.p4.6.m6.1.1.1.cmml">=</mo><mn id="S5.SS1.p4.6.m6.1.1.3" xref="S5.SS1.p4.6.m6.1.1.3.cmml">2000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.6.m6.1b"><apply id="S5.SS1.p4.6.m6.1.1.cmml" xref="S5.SS1.p4.6.m6.1.1"><eq id="S5.SS1.p4.6.m6.1.1.1.cmml" xref="S5.SS1.p4.6.m6.1.1.1"></eq><ci id="S5.SS1.p4.6.m6.1.1.2.cmml" xref="S5.SS1.p4.6.m6.1.1.2">𝑍</ci><cn type="integer" id="S5.SS1.p4.6.m6.1.1.3.cmml" xref="S5.SS1.p4.6.m6.1.1.3">2000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.6.m6.1c">Z=2000</annotation></semantics></math>.
Following the configurations of 3DETR-m, we adopt 3 transformer encoders and 8 decoders in AnyView. The radius mask for encoder is set to <math id="S5.SS1.p4.7.m7.3" class="ltx_Math" alttext="[0.8m,0.8m,1.2m]" display="inline"><semantics id="S5.SS1.p4.7.m7.3a"><mrow id="S5.SS1.p4.7.m7.3.3.3" xref="S5.SS1.p4.7.m7.3.3.4.cmml"><mo stretchy="false" id="S5.SS1.p4.7.m7.3.3.3.4" xref="S5.SS1.p4.7.m7.3.3.4.cmml">[</mo><mrow id="S5.SS1.p4.7.m7.1.1.1.1" xref="S5.SS1.p4.7.m7.1.1.1.1.cmml"><mn id="S5.SS1.p4.7.m7.1.1.1.1.2" xref="S5.SS1.p4.7.m7.1.1.1.1.2.cmml">0.8</mn><mo lspace="0em" rspace="0em" id="S5.SS1.p4.7.m7.1.1.1.1.1" xref="S5.SS1.p4.7.m7.1.1.1.1.1.cmml">​</mo><mi id="S5.SS1.p4.7.m7.1.1.1.1.3" xref="S5.SS1.p4.7.m7.1.1.1.1.3.cmml">m</mi></mrow><mo id="S5.SS1.p4.7.m7.3.3.3.5" xref="S5.SS1.p4.7.m7.3.3.4.cmml">,</mo><mrow id="S5.SS1.p4.7.m7.2.2.2.2" xref="S5.SS1.p4.7.m7.2.2.2.2.cmml"><mn id="S5.SS1.p4.7.m7.2.2.2.2.2" xref="S5.SS1.p4.7.m7.2.2.2.2.2.cmml">0.8</mn><mo lspace="0em" rspace="0em" id="S5.SS1.p4.7.m7.2.2.2.2.1" xref="S5.SS1.p4.7.m7.2.2.2.2.1.cmml">​</mo><mi id="S5.SS1.p4.7.m7.2.2.2.2.3" xref="S5.SS1.p4.7.m7.2.2.2.2.3.cmml">m</mi></mrow><mo id="S5.SS1.p4.7.m7.3.3.3.6" xref="S5.SS1.p4.7.m7.3.3.4.cmml">,</mo><mrow id="S5.SS1.p4.7.m7.3.3.3.3" xref="S5.SS1.p4.7.m7.3.3.3.3.cmml"><mn id="S5.SS1.p4.7.m7.3.3.3.3.2" xref="S5.SS1.p4.7.m7.3.3.3.3.2.cmml">1.2</mn><mo lspace="0em" rspace="0em" id="S5.SS1.p4.7.m7.3.3.3.3.1" xref="S5.SS1.p4.7.m7.3.3.3.3.1.cmml">​</mo><mi id="S5.SS1.p4.7.m7.3.3.3.3.3" xref="S5.SS1.p4.7.m7.3.3.3.3.3.cmml">m</mi></mrow><mo stretchy="false" id="S5.SS1.p4.7.m7.3.3.3.7" xref="S5.SS1.p4.7.m7.3.3.4.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.7.m7.3b"><list id="S5.SS1.p4.7.m7.3.3.4.cmml" xref="S5.SS1.p4.7.m7.3.3.3"><apply id="S5.SS1.p4.7.m7.1.1.1.1.cmml" xref="S5.SS1.p4.7.m7.1.1.1.1"><times id="S5.SS1.p4.7.m7.1.1.1.1.1.cmml" xref="S5.SS1.p4.7.m7.1.1.1.1.1"></times><cn type="float" id="S5.SS1.p4.7.m7.1.1.1.1.2.cmml" xref="S5.SS1.p4.7.m7.1.1.1.1.2">0.8</cn><ci id="S5.SS1.p4.7.m7.1.1.1.1.3.cmml" xref="S5.SS1.p4.7.m7.1.1.1.1.3">𝑚</ci></apply><apply id="S5.SS1.p4.7.m7.2.2.2.2.cmml" xref="S5.SS1.p4.7.m7.2.2.2.2"><times id="S5.SS1.p4.7.m7.2.2.2.2.1.cmml" xref="S5.SS1.p4.7.m7.2.2.2.2.1"></times><cn type="float" id="S5.SS1.p4.7.m7.2.2.2.2.2.cmml" xref="S5.SS1.p4.7.m7.2.2.2.2.2">0.8</cn><ci id="S5.SS1.p4.7.m7.2.2.2.2.3.cmml" xref="S5.SS1.p4.7.m7.2.2.2.2.3">𝑚</ci></apply><apply id="S5.SS1.p4.7.m7.3.3.3.3.cmml" xref="S5.SS1.p4.7.m7.3.3.3.3"><times id="S5.SS1.p4.7.m7.3.3.3.3.1.cmml" xref="S5.SS1.p4.7.m7.3.3.3.3.1"></times><cn type="float" id="S5.SS1.p4.7.m7.3.3.3.3.2.cmml" xref="S5.SS1.p4.7.m7.3.3.3.3.2">1.2</cn><ci id="S5.SS1.p4.7.m7.3.3.3.3.3.cmml" xref="S5.SS1.p4.7.m7.3.3.3.3.3">𝑚</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.7.m7.3c">[0.8m,0.8m,1.2m]</annotation></semantics></math>.
In terms of data augmentation, both random view dropping and global random cuboid are applied with probability 0.75.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para">
<p id="S5.SS1.p5.6" class="ltx_p">As for the online detection setting, in processing the online detection RGB-D input flow <math id="S5.SS1.p5.1.m1.3" class="ltx_Math" alttext="F=\left\{f_{1},...,f_{i}\right\}" display="inline"><semantics id="S5.SS1.p5.1.m1.3a"><mrow id="S5.SS1.p5.1.m1.3.3" xref="S5.SS1.p5.1.m1.3.3.cmml"><mi id="S5.SS1.p5.1.m1.3.3.4" xref="S5.SS1.p5.1.m1.3.3.4.cmml">F</mi><mo id="S5.SS1.p5.1.m1.3.3.3" xref="S5.SS1.p5.1.m1.3.3.3.cmml">=</mo><mrow id="S5.SS1.p5.1.m1.3.3.2.2" xref="S5.SS1.p5.1.m1.3.3.2.3.cmml"><mo id="S5.SS1.p5.1.m1.3.3.2.2.3" xref="S5.SS1.p5.1.m1.3.3.2.3.cmml">{</mo><msub id="S5.SS1.p5.1.m1.2.2.1.1.1" xref="S5.SS1.p5.1.m1.2.2.1.1.1.cmml"><mi id="S5.SS1.p5.1.m1.2.2.1.1.1.2" xref="S5.SS1.p5.1.m1.2.2.1.1.1.2.cmml">f</mi><mn id="S5.SS1.p5.1.m1.2.2.1.1.1.3" xref="S5.SS1.p5.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S5.SS1.p5.1.m1.3.3.2.2.4" xref="S5.SS1.p5.1.m1.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="S5.SS1.p5.1.m1.1.1" xref="S5.SS1.p5.1.m1.1.1.cmml">…</mi><mo id="S5.SS1.p5.1.m1.3.3.2.2.5" xref="S5.SS1.p5.1.m1.3.3.2.3.cmml">,</mo><msub id="S5.SS1.p5.1.m1.3.3.2.2.2" xref="S5.SS1.p5.1.m1.3.3.2.2.2.cmml"><mi id="S5.SS1.p5.1.m1.3.3.2.2.2.2" xref="S5.SS1.p5.1.m1.3.3.2.2.2.2.cmml">f</mi><mi id="S5.SS1.p5.1.m1.3.3.2.2.2.3" xref="S5.SS1.p5.1.m1.3.3.2.2.2.3.cmml">i</mi></msub><mo id="S5.SS1.p5.1.m1.3.3.2.2.6" xref="S5.SS1.p5.1.m1.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.1.m1.3b"><apply id="S5.SS1.p5.1.m1.3.3.cmml" xref="S5.SS1.p5.1.m1.3.3"><eq id="S5.SS1.p5.1.m1.3.3.3.cmml" xref="S5.SS1.p5.1.m1.3.3.3"></eq><ci id="S5.SS1.p5.1.m1.3.3.4.cmml" xref="S5.SS1.p5.1.m1.3.3.4">𝐹</ci><set id="S5.SS1.p5.1.m1.3.3.2.3.cmml" xref="S5.SS1.p5.1.m1.3.3.2.2"><apply id="S5.SS1.p5.1.m1.2.2.1.1.1.cmml" xref="S5.SS1.p5.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p5.1.m1.2.2.1.1.1.1.cmml" xref="S5.SS1.p5.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S5.SS1.p5.1.m1.2.2.1.1.1.2.cmml" xref="S5.SS1.p5.1.m1.2.2.1.1.1.2">𝑓</ci><cn type="integer" id="S5.SS1.p5.1.m1.2.2.1.1.1.3.cmml" xref="S5.SS1.p5.1.m1.2.2.1.1.1.3">1</cn></apply><ci id="S5.SS1.p5.1.m1.1.1.cmml" xref="S5.SS1.p5.1.m1.1.1">…</ci><apply id="S5.SS1.p5.1.m1.3.3.2.2.2.cmml" xref="S5.SS1.p5.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S5.SS1.p5.1.m1.3.3.2.2.2.1.cmml" xref="S5.SS1.p5.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S5.SS1.p5.1.m1.3.3.2.2.2.2.cmml" xref="S5.SS1.p5.1.m1.3.3.2.2.2.2">𝑓</ci><ci id="S5.SS1.p5.1.m1.3.3.2.2.2.3.cmml" xref="S5.SS1.p5.1.m1.3.3.2.2.2.3">𝑖</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.1.m1.3c">F=\left\{f_{1},...,f_{i}\right\}</annotation></semantics></math>, our proposed AnyView will save the extracted <math id="S5.SS1.p5.2.m2.1" class="ltx_Math" alttext="\mathcal{P}_{i}" display="inline"><semantics id="S5.SS1.p5.2.m2.1a"><msub id="S5.SS1.p5.2.m2.1.1" xref="S5.SS1.p5.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS1.p5.2.m2.1.1.2" xref="S5.SS1.p5.2.m2.1.1.2.cmml">𝒫</mi><mi id="S5.SS1.p5.2.m2.1.1.3" xref="S5.SS1.p5.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.2.m2.1b"><apply id="S5.SS1.p5.2.m2.1.1.cmml" xref="S5.SS1.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.p5.2.m2.1.1.1.cmml" xref="S5.SS1.p5.2.m2.1.1">subscript</csymbol><ci id="S5.SS1.p5.2.m2.1.1.2.cmml" xref="S5.SS1.p5.2.m2.1.1.2">𝒫</ci><ci id="S5.SS1.p5.2.m2.1.1.3.cmml" xref="S5.SS1.p5.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.2.m2.1c">\mathcal{P}_{i}</annotation></semantics></math> for each RGB-D image frame <math id="S5.SS1.p5.3.m3.1" class="ltx_Math" alttext="f_{i}" display="inline"><semantics id="S5.SS1.p5.3.m3.1a"><msub id="S5.SS1.p5.3.m3.1.1" xref="S5.SS1.p5.3.m3.1.1.cmml"><mi id="S5.SS1.p5.3.m3.1.1.2" xref="S5.SS1.p5.3.m3.1.1.2.cmml">f</mi><mi id="S5.SS1.p5.3.m3.1.1.3" xref="S5.SS1.p5.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.3.m3.1b"><apply id="S5.SS1.p5.3.m3.1.1.cmml" xref="S5.SS1.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS1.p5.3.m3.1.1.1.cmml" xref="S5.SS1.p5.3.m3.1.1">subscript</csymbol><ci id="S5.SS1.p5.3.m3.1.1.2.cmml" xref="S5.SS1.p5.3.m3.1.1.2">𝑓</ci><ci id="S5.SS1.p5.3.m3.1.1.3.cmml" xref="S5.SS1.p5.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.3.m3.1c">f_{i}</annotation></semantics></math>, at the input of the next RGB-D frame <math id="S5.SS1.p5.4.m4.1" class="ltx_Math" alttext="f_{i+1}" display="inline"><semantics id="S5.SS1.p5.4.m4.1a"><msub id="S5.SS1.p5.4.m4.1.1" xref="S5.SS1.p5.4.m4.1.1.cmml"><mi id="S5.SS1.p5.4.m4.1.1.2" xref="S5.SS1.p5.4.m4.1.1.2.cmml">f</mi><mrow id="S5.SS1.p5.4.m4.1.1.3" xref="S5.SS1.p5.4.m4.1.1.3.cmml"><mi id="S5.SS1.p5.4.m4.1.1.3.2" xref="S5.SS1.p5.4.m4.1.1.3.2.cmml">i</mi><mo id="S5.SS1.p5.4.m4.1.1.3.1" xref="S5.SS1.p5.4.m4.1.1.3.1.cmml">+</mo><mn id="S5.SS1.p5.4.m4.1.1.3.3" xref="S5.SS1.p5.4.m4.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.4.m4.1b"><apply id="S5.SS1.p5.4.m4.1.1.cmml" xref="S5.SS1.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS1.p5.4.m4.1.1.1.cmml" xref="S5.SS1.p5.4.m4.1.1">subscript</csymbol><ci id="S5.SS1.p5.4.m4.1.1.2.cmml" xref="S5.SS1.p5.4.m4.1.1.2">𝑓</ci><apply id="S5.SS1.p5.4.m4.1.1.3.cmml" xref="S5.SS1.p5.4.m4.1.1.3"><plus id="S5.SS1.p5.4.m4.1.1.3.1.cmml" xref="S5.SS1.p5.4.m4.1.1.3.1"></plus><ci id="S5.SS1.p5.4.m4.1.1.3.2.cmml" xref="S5.SS1.p5.4.m4.1.1.3.2">𝑖</ci><cn type="integer" id="S5.SS1.p5.4.m4.1.1.3.3.cmml" xref="S5.SS1.p5.4.m4.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.4.m4.1c">f_{i+1}</annotation></semantics></math>, it will directly read <math id="S5.SS1.p5.5.m5.3" class="ltx_Math" alttext="\left\{\mathcal{P}_{1},...,\mathcal{P}_{i}\right\}" display="inline"><semantics id="S5.SS1.p5.5.m5.3a"><mrow id="S5.SS1.p5.5.m5.3.3.2" xref="S5.SS1.p5.5.m5.3.3.3.cmml"><mo id="S5.SS1.p5.5.m5.3.3.2.3" xref="S5.SS1.p5.5.m5.3.3.3.cmml">{</mo><msub id="S5.SS1.p5.5.m5.2.2.1.1" xref="S5.SS1.p5.5.m5.2.2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS1.p5.5.m5.2.2.1.1.2" xref="S5.SS1.p5.5.m5.2.2.1.1.2.cmml">𝒫</mi><mn id="S5.SS1.p5.5.m5.2.2.1.1.3" xref="S5.SS1.p5.5.m5.2.2.1.1.3.cmml">1</mn></msub><mo id="S5.SS1.p5.5.m5.3.3.2.4" xref="S5.SS1.p5.5.m5.3.3.3.cmml">,</mo><mi mathvariant="normal" id="S5.SS1.p5.5.m5.1.1" xref="S5.SS1.p5.5.m5.1.1.cmml">…</mi><mo id="S5.SS1.p5.5.m5.3.3.2.5" xref="S5.SS1.p5.5.m5.3.3.3.cmml">,</mo><msub id="S5.SS1.p5.5.m5.3.3.2.2" xref="S5.SS1.p5.5.m5.3.3.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS1.p5.5.m5.3.3.2.2.2" xref="S5.SS1.p5.5.m5.3.3.2.2.2.cmml">𝒫</mi><mi id="S5.SS1.p5.5.m5.3.3.2.2.3" xref="S5.SS1.p5.5.m5.3.3.2.2.3.cmml">i</mi></msub><mo id="S5.SS1.p5.5.m5.3.3.2.6" xref="S5.SS1.p5.5.m5.3.3.3.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.5.m5.3b"><set id="S5.SS1.p5.5.m5.3.3.3.cmml" xref="S5.SS1.p5.5.m5.3.3.2"><apply id="S5.SS1.p5.5.m5.2.2.1.1.cmml" xref="S5.SS1.p5.5.m5.2.2.1.1"><csymbol cd="ambiguous" id="S5.SS1.p5.5.m5.2.2.1.1.1.cmml" xref="S5.SS1.p5.5.m5.2.2.1.1">subscript</csymbol><ci id="S5.SS1.p5.5.m5.2.2.1.1.2.cmml" xref="S5.SS1.p5.5.m5.2.2.1.1.2">𝒫</ci><cn type="integer" id="S5.SS1.p5.5.m5.2.2.1.1.3.cmml" xref="S5.SS1.p5.5.m5.2.2.1.1.3">1</cn></apply><ci id="S5.SS1.p5.5.m5.1.1.cmml" xref="S5.SS1.p5.5.m5.1.1">…</ci><apply id="S5.SS1.p5.5.m5.3.3.2.2.cmml" xref="S5.SS1.p5.5.m5.3.3.2.2"><csymbol cd="ambiguous" id="S5.SS1.p5.5.m5.3.3.2.2.1.cmml" xref="S5.SS1.p5.5.m5.3.3.2.2">subscript</csymbol><ci id="S5.SS1.p5.5.m5.3.3.2.2.2.cmml" xref="S5.SS1.p5.5.m5.3.3.2.2.2">𝒫</ci><ci id="S5.SS1.p5.5.m5.3.3.2.2.3.cmml" xref="S5.SS1.p5.5.m5.3.3.2.2.3">𝑖</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.5.m5.3c">\left\{\mathcal{P}_{1},...,\mathcal{P}_{i}\right\}</annotation></semantics></math>, which will be fed into the detector along with <math id="S5.SS1.p5.6.m6.1" class="ltx_Math" alttext="\mathcal{P}_{i+1}" display="inline"><semantics id="S5.SS1.p5.6.m6.1a"><msub id="S5.SS1.p5.6.m6.1.1" xref="S5.SS1.p5.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS1.p5.6.m6.1.1.2" xref="S5.SS1.p5.6.m6.1.1.2.cmml">𝒫</mi><mrow id="S5.SS1.p5.6.m6.1.1.3" xref="S5.SS1.p5.6.m6.1.1.3.cmml"><mi id="S5.SS1.p5.6.m6.1.1.3.2" xref="S5.SS1.p5.6.m6.1.1.3.2.cmml">i</mi><mo id="S5.SS1.p5.6.m6.1.1.3.1" xref="S5.SS1.p5.6.m6.1.1.3.1.cmml">+</mo><mn id="S5.SS1.p5.6.m6.1.1.3.3" xref="S5.SS1.p5.6.m6.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.6.m6.1b"><apply id="S5.SS1.p5.6.m6.1.1.cmml" xref="S5.SS1.p5.6.m6.1.1"><csymbol cd="ambiguous" id="S5.SS1.p5.6.m6.1.1.1.cmml" xref="S5.SS1.p5.6.m6.1.1">subscript</csymbol><ci id="S5.SS1.p5.6.m6.1.1.2.cmml" xref="S5.SS1.p5.6.m6.1.1.2">𝒫</ci><apply id="S5.SS1.p5.6.m6.1.1.3.cmml" xref="S5.SS1.p5.6.m6.1.1.3"><plus id="S5.SS1.p5.6.m6.1.1.3.1.cmml" xref="S5.SS1.p5.6.m6.1.1.3.1"></plus><ci id="S5.SS1.p5.6.m6.1.1.3.2.cmml" xref="S5.SS1.p5.6.m6.1.1.3.2">𝑖</ci><cn type="integer" id="S5.SS1.p5.6.m6.1.1.3.3.cmml" xref="S5.SS1.p5.6.m6.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.6.m6.1c">\mathcal{P}_{i+1}</annotation></semantics></math> to predict the result.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>3D object detection results (mAP@0.25 and mAP@0.5) of different models on ScanNet-SV and ScanNet-MV. Here ScanNet-MV is further divided according to the number of views and how these views are combined. Uniform/Continuous means the views are sampled uniformly/adjacently from the whole 50 views. Gray cells show on which benchmark the model is trained.</figcaption>
<table id="S5.T4.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.3.1.1" class="ltx_tr">
<th id="S5.T4.3.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S5.T4.3.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S5.T4.3.1.1.2.1" class="ltx_text" style="font-size:90%;">Methods</span></th>
<th id="S5.T4.3.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt">
<span id="S5.T4.3.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.1.1.3.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.1.1.3.1.1.1" class="ltx_text" style="font-size:90%;">ScanNet-SV</span></span>
</span>
</th>
<th id="S5.T4.3.1.1.4" class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_tt" colspan="6"><span id="S5.T4.3.1.1.4.1" class="ltx_text" style="font-size:90%;">ScanNet-MV</span></th>
</tr>
<tr id="S5.T4.3.2.2" class="ltx_tr">
<th id="S5.T4.3.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S5.T4.3.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r">
<span id="S5.T4.3.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.2.2.2.1.1" class="ltx_p" style="width:45.5pt;"></span>
</span>
</th>
<th id="S5.T4.3.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r">
<span id="S5.T4.3.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.2.2.3.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.2.2.3.1.1.1" class="ltx_text" style="font-size:90%;"> 5</span></span>
</span>
</th>
<th id="S5.T4.3.2.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r">
<span id="S5.T4.3.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.2.2.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.2.2.4.1.1.1" class="ltx_text" style="font-size:90%;">10</span></span>
</span>
</th>
<th id="S5.T4.3.2.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r">
<span id="S5.T4.3.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.2.2.5.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.2.2.5.1.1.1" class="ltx_text" style="font-size:90%;">15</span></span>
</span>
</th>
<th id="S5.T4.3.2.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r">
<span id="S5.T4.3.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.2.2.6.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.2.2.6.1.1.1" class="ltx_text" style="font-size:90%;">30</span></span>
</span>
</th>
<th id="S5.T4.3.2.2.7" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r">
<span id="S5.T4.3.2.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.2.2.7.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.2.2.7.1.1.1" class="ltx_text" style="font-size:90%;">40</span></span>
</span>
</th>
<th id="S5.T4.3.2.2.8" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column">
<span id="S5.T4.3.2.2.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.2.2.8.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.2.2.8.1.1.1" class="ltx_text" style="font-size:90%;">50</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.3.3.1" class="ltx_tr">
<th id="S5.T4.3.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="7"><span id="S5.T4.3.3.1.1.1" class="ltx_text" style="font-size:90%;">
<span id="S5.T4.3.3.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.3pt;height:32.5pt;vertical-align:-13.1pt;"><span class="ltx_transformed_inner" style="width:32.5pt;transform:translate(-13.14pt,0pt) rotate(-90deg) ;">
<span id="S5.T4.3.3.1.1.1.1.1" class="ltx_p">Uniform</span>
</span></span></span></th>
<th id="S5.T4.3.3.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T4.3.3.1.2.1" class="ltx_text" style="font-size:90%;">ImVoteNet</span></th>
<td id="S5.T4.3.3.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="background-color:#D9D9D9;">
<span id="S5.T4.3.3.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.3.1.3.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.3.1.3.1.1.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;">31.6 / 13.5</span></span>
</span>
</td>
<td id="S5.T4.3.3.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T4.3.3.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.3.1.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.3.1.4.1.1.1" class="ltx_text" style="font-size:90%;">30.2 / 13.7</span></span>
</span>
</td>
<td id="S5.T4.3.3.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T4.3.3.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.3.1.5.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.3.1.5.1.1.1" class="ltx_text" style="font-size:90%;">37.3 / 17.9</span></span>
</span>
</td>
<td id="S5.T4.3.3.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T4.3.3.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.3.1.6.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.3.1.6.1.1.1" class="ltx_text" style="font-size:90%;">40.3 / 18.5</span></span>
</span>
</td>
<td id="S5.T4.3.3.1.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T4.3.3.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.3.1.7.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.3.1.7.1.1.1" class="ltx_text" style="font-size:90%;">43.5 / 20.6</span></span>
</span>
</td>
<td id="S5.T4.3.3.1.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T4.3.3.1.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.3.1.8.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.3.1.8.1.1.1" class="ltx_text" style="font-size:90%;">42.9 / 20.4</span></span>
</span>
</td>
<td id="S5.T4.3.3.1.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T4.3.3.1.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.3.1.9.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.3.1.9.1.1.1" class="ltx_text" style="font-size:90%;">43.3 / 20.4</span></span>
</span>
</td>
</tr>
<tr id="S5.T4.3.4.2" class="ltx_tr">
<th id="S5.T4.3.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S5.T4.3.4.2.1.1" class="ltx_text" style="font-size:90%;">3DETR-m(10)</span></th>
<td id="S5.T4.3.4.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.4.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.4.2.2.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.4.2.2.1.1.1" class="ltx_text" style="font-size:90%;">29.8 / 12.5</span></span>
</span>
</td>
<td id="S5.T4.3.4.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.4.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.4.2.3.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.4.2.3.1.1.1" class="ltx_text" style="font-size:90%;">39.7 / 20.0</span></span>
</span>
</td>
<td id="S5.T4.3.4.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="background-color:#D9D9D9;">
<span id="S5.T4.3.4.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.4.2.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.4.2.4.1.1.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;">49.1 / 28.0</span></span>
</span>
</td>
<td id="S5.T4.3.4.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.4.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.4.2.5.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.4.2.5.1.1.1" class="ltx_text" style="font-size:90%;">50.8 / 29.0</span></span>
</span>
</td>
<td id="S5.T4.3.4.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.4.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.4.2.6.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.4.2.6.1.1.1" class="ltx_text" style="font-size:90%;">50.3 / 28.9</span></span>
</span>
</td>
<td id="S5.T4.3.4.2.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.4.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.4.2.7.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.4.2.7.1.1.1" class="ltx_text" style="font-size:90%;">49.6 / 28.9</span></span>
</span>
</td>
<td id="S5.T4.3.4.2.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T4.3.4.2.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.4.2.8.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.4.2.8.1.1.1" class="ltx_text" style="font-size:90%;">48.9 / 27.7</span></span>
</span>
</td>
</tr>
<tr id="S5.T4.3.5.3" class="ltx_tr">
<th id="S5.T4.3.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S5.T4.3.5.3.1.1" class="ltx_text" style="font-size:90%;">3DETR-m(30)</span></th>
<td id="S5.T4.3.5.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.5.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.5.3.2.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.5.3.2.1.1.1" class="ltx_text" style="font-size:90%;">29.1 / 13.1</span></span>
</span>
</td>
<td id="S5.T4.3.5.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.5.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.5.3.3.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.5.3.3.1.1.1" class="ltx_text" style="font-size:90%;">36.8 / 19.5</span></span>
</span>
</td>
<td id="S5.T4.3.5.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.5.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.5.3.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.5.3.4.1.1.1" class="ltx_text" style="font-size:90%;">50.1 / 29.6</span></span>
</span>
</td>
<td id="S5.T4.3.5.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.5.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.5.3.5.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.5.3.5.1.1.1" class="ltx_text" style="font-size:90%;">53.1 / 31.4</span></span>
</span>
</td>
<td id="S5.T4.3.5.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="background-color:#D9D9D9;">
<span id="S5.T4.3.5.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.5.3.6.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.5.3.6.1.1.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;">55.6 / 33.9</span></span>
</span>
</td>
<td id="S5.T4.3.5.3.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.5.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.5.3.7.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.5.3.7.1.1.1" class="ltx_text" style="font-size:90%;">55.7 / 34.3</span></span>
</span>
</td>
<td id="S5.T4.3.5.3.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T4.3.5.3.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.5.3.8.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.5.3.8.1.1.1" class="ltx_text" style="font-size:90%;">55.6 / 34.7</span></span>
</span>
</td>
</tr>
<tr id="S5.T4.3.6.4" class="ltx_tr">
<th id="S5.T4.3.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S5.T4.3.6.4.1.1" class="ltx_text" style="font-size:90%;">3DETR-m(50)</span></th>
<td id="S5.T4.3.6.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.6.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.6.4.2.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.6.4.2.1.1.1" class="ltx_text" style="font-size:90%;">27.7 / 12.8</span></span>
</span>
</td>
<td id="S5.T4.3.6.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.6.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.6.4.3.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.6.4.3.1.1.1" class="ltx_text" style="font-size:90%;">34.9 / 17.9</span></span>
</span>
</td>
<td id="S5.T4.3.6.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.6.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.6.4.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.6.4.4.1.1.1" class="ltx_text" style="font-size:90%;">48.6 / 29.0</span></span>
</span>
</td>
<td id="S5.T4.3.6.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.6.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.6.4.5.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.6.4.5.1.1.1" class="ltx_text" style="font-size:90%;">52.5 / 33.3</span></span>
</span>
</td>
<td id="S5.T4.3.6.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.6.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.6.4.6.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.6.4.6.1.1.1" class="ltx_text" style="font-size:90%;">55.3 / 36.3</span></span>
</span>
</td>
<td id="S5.T4.3.6.4.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.6.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.6.4.7.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.6.4.7.1.1.1" class="ltx_text" style="font-size:90%;">55.6 / 35.9</span></span>
</span>
</td>
<td id="S5.T4.3.6.4.8" class="ltx_td ltx_align_justify ltx_align_top" style="background-color:#D9D9D9;">
<span id="S5.T4.3.6.4.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.6.4.8.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.6.4.8.1.1.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;">54.7 / 35.3</span></span>
</span>
</td>
</tr>
<tr id="S5.T4.3.7.5" class="ltx_tr">
<th id="S5.T4.3.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S5.T4.3.7.5.1.1" class="ltx_text" style="font-size:90%;">AnyView(10)</span></th>
<td id="S5.T4.3.7.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.7.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.7.5.2.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.7.5.2.1.1.1" class="ltx_text" style="font-size:90%;">31.3 / 13.7</span></span>
</span>
</td>
<td id="S5.T4.3.7.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.7.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.7.5.3.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.7.5.3.1.1.1" class="ltx_text" style="font-size:90%;">42.2 / 20.4</span></span>
</span>
</td>
<td id="S5.T4.3.7.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="background-color:#D9D9D9;">
<span id="S5.T4.3.7.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.7.5.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.7.5.4.1.1.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;">51.3 / 27.2</span></span>
</span>
</td>
<td id="S5.T4.3.7.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.7.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.7.5.5.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.7.5.5.1.1.1" class="ltx_text" style="font-size:90%;">52.8 / 29.0</span></span>
</span>
</td>
<td id="S5.T4.3.7.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.7.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.7.5.6.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.7.5.6.1.1.1" class="ltx_text" style="font-size:90%;">53.2 / 29.9</span></span>
</span>
</td>
<td id="S5.T4.3.7.5.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.7.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.7.5.7.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.7.5.7.1.1.1" class="ltx_text" style="font-size:90%;">54.9 / 30.4</span></span>
</span>
</td>
<td id="S5.T4.3.7.5.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T4.3.7.5.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.7.5.8.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.7.5.8.1.1.1" class="ltx_text" style="font-size:90%;">54.8 / 30.1</span></span>
</span>
</td>
</tr>
<tr id="S5.T4.3.8.6" class="ltx_tr">
<th id="S5.T4.3.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S5.T4.3.8.6.1.1" class="ltx_text" style="font-size:90%;">AnyView(30)</span></th>
<td id="S5.T4.3.8.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.8.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.8.6.2.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.8.6.2.1.1.1" class="ltx_text" style="font-size:90%;">30.5 / 12.7</span></span>
</span>
</td>
<td id="S5.T4.3.8.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.8.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.8.6.3.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.8.6.3.1.1.1" class="ltx_text" style="font-size:90%;">39.8 / 20.1</span></span>
</span>
</td>
<td id="S5.T4.3.8.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.8.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.8.6.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.8.6.4.1.1.1" class="ltx_text" style="font-size:90%;">52.4 / 29.2</span></span>
</span>
</td>
<td id="S5.T4.3.8.6.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.8.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.8.6.5.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.8.6.5.1.1.1" class="ltx_text" style="font-size:90%;">57.1 / 31.6</span></span>
</span>
</td>
<td id="S5.T4.3.8.6.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="background-color:#D9D9D9;">
<span id="S5.T4.3.8.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.8.6.6.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.8.6.6.1.1.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;">59.1 / 35.2</span></span>
</span>
</td>
<td id="S5.T4.3.8.6.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.8.6.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.8.6.7.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.8.6.7.1.1.1" class="ltx_text" style="font-size:90%;">59.9 / </span><span id="S5.T4.3.8.6.7.1.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">36.8</span></span>
</span>
</td>
<td id="S5.T4.3.8.6.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T4.3.8.6.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.8.6.8.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.8.6.8.1.1.1" class="ltx_text" style="font-size:90%;">60.1 / </span><span id="S5.T4.3.8.6.8.1.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">37.4</span></span>
</span>
</td>
</tr>
<tr id="S5.T4.3.9.7" class="ltx_tr">
<th id="S5.T4.3.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S5.T4.3.9.7.1.1" class="ltx_text" style="font-size:90%;">AnyView(50)</span></th>
<td id="S5.T4.3.9.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.9.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.9.7.2.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.9.7.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">32.7</span><span id="S5.T4.3.9.7.2.1.1.2" class="ltx_text" style="font-size:90%;"> / </span><span id="S5.T4.3.9.7.2.1.1.3" class="ltx_text ltx_font_bold" style="font-size:90%;">15.3</span></span>
</span>
</td>
<td id="S5.T4.3.9.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.9.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.9.7.3.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.9.7.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">45.0</span><span id="S5.T4.3.9.7.3.1.1.2" class="ltx_text" style="font-size:90%;"> / </span><span id="S5.T4.3.9.7.3.1.1.3" class="ltx_text ltx_font_bold" style="font-size:90%;">22.0</span></span>
</span>
</td>
<td id="S5.T4.3.9.7.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.9.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.9.7.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.9.7.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">53.8</span><span id="S5.T4.3.9.7.4.1.1.2" class="ltx_text" style="font-size:90%;"> / </span><span id="S5.T4.3.9.7.4.1.1.3" class="ltx_text ltx_font_bold" style="font-size:90%;">30.5</span></span>
</span>
</td>
<td id="S5.T4.3.9.7.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.9.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.9.7.5.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.9.7.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">57.5</span><span id="S5.T4.3.9.7.5.1.1.2" class="ltx_text" style="font-size:90%;"> / </span><span id="S5.T4.3.9.7.5.1.1.3" class="ltx_text ltx_font_bold" style="font-size:90%;">33.5</span></span>
</span>
</td>
<td id="S5.T4.3.9.7.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.9.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.9.7.6.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.9.7.6.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">59.5</span><span id="S5.T4.3.9.7.6.1.1.2" class="ltx_text" style="font-size:90%;"> / </span><span id="S5.T4.3.9.7.6.1.1.3" class="ltx_text ltx_font_bold" style="font-size:90%;">35.7</span></span>
</span>
</td>
<td id="S5.T4.3.9.7.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.9.7.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.9.7.7.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.9.7.7.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">60.6</span><span id="S5.T4.3.9.7.7.1.1.2" class="ltx_text" style="font-size:90%;"> / 35.7</span></span>
</span>
</td>
<td id="S5.T4.3.9.7.8" class="ltx_td ltx_align_justify ltx_align_top" style="background-color:#D9D9D9;">
<span id="S5.T4.3.9.7.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.9.7.8.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.9.7.8.1.1.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;"><span id="S5.T4.3.9.7.8.1.1.1.1" class="ltx_text ltx_font_bold">60.7</span> / 35.8</span></span>
</span>
</td>
</tr>
<tr id="S5.T4.3.10.8" class="ltx_tr">
<th id="S5.T4.3.10.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" rowspan="7"><span id="S5.T4.3.10.8.1.1" class="ltx_text" style="font-size:90%;">
<span id="S5.T4.3.10.8.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.1pt;height:44.6pt;vertical-align:-19.2pt;"><span class="ltx_transformed_inner" style="width:44.6pt;transform:translate(-19.2pt,0pt) rotate(-90deg) ;">
<span id="S5.T4.3.10.8.1.1.1.1" class="ltx_p">Continuous</span>
</span></span></span></th>
<th id="S5.T4.3.10.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T4.3.10.8.2.1" class="ltx_text" style="font-size:90%;">ImVoteNet</span></th>
<td id="S5.T4.3.10.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="background-color:#D9D9D9;">
<span id="S5.T4.3.10.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.10.8.3.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.10.8.3.1.1.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;">31.6 / 13.5</span></span>
</span>
</td>
<td id="S5.T4.3.10.8.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T4.3.10.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.10.8.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.10.8.4.1.1.1" class="ltx_text" style="font-size:90%;">18.1 / 8.8</span></span>
</span>
</td>
<td id="S5.T4.3.10.8.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T4.3.10.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.10.8.5.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.10.8.5.1.1.1" class="ltx_text" style="font-size:90%;">25.2 / 13.5</span></span>
</span>
</td>
<td id="S5.T4.3.10.8.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T4.3.10.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.10.8.6.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.10.8.6.1.1.1" class="ltx_text" style="font-size:90%;">30.4 / 15.7</span></span>
</span>
</td>
<td id="S5.T4.3.10.8.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T4.3.10.8.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.10.8.7.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.10.8.7.1.1.1" class="ltx_text" style="font-size:90%;">40.5 / 18.4</span></span>
</span>
</td>
<td id="S5.T4.3.10.8.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T4.3.10.8.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.10.8.8.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.10.8.8.1.1.1" class="ltx_text" style="font-size:90%;">43.1 / 20.0</span></span>
</span>
</td>
<td id="S5.T4.3.10.8.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T4.3.10.8.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.10.8.9.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.10.8.9.1.1.1" class="ltx_text" style="font-size:90%;">43.3 / 20.4</span></span>
</span>
</td>
</tr>
<tr id="S5.T4.3.11.9" class="ltx_tr">
<th id="S5.T4.3.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S5.T4.3.11.9.1.1" class="ltx_text" style="font-size:90%;">3DETR-m(10)</span></th>
<td id="S5.T4.3.11.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.11.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.11.9.2.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.11.9.2.1.1.1" class="ltx_text" style="font-size:90%;">29.8 / 12.5</span></span>
</span>
</td>
<td id="S5.T4.3.11.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.11.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.11.9.3.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.11.9.3.1.1.1" class="ltx_text" style="font-size:90%;">19.9 / 9.9</span></span>
</span>
</td>
<td id="S5.T4.3.11.9.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="background-color:#D9D9D9;">
<span id="S5.T4.3.11.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.11.9.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.11.9.4.1.1.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;">27.7 / 15.3</span></span>
</span>
</td>
<td id="S5.T4.3.11.9.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.11.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.11.9.5.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.11.9.5.1.1.1" class="ltx_text" style="font-size:90%;">34.0 / 18.1</span></span>
</span>
</td>
<td id="S5.T4.3.11.9.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.11.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.11.9.6.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.11.9.6.1.1.1" class="ltx_text" style="font-size:90%;">44.2 / 24.1</span></span>
</span>
</td>
<td id="S5.T4.3.11.9.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.11.9.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.11.9.7.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.11.9.7.1.1.1" class="ltx_text" style="font-size:90%;">48.7 / 27.8</span></span>
</span>
</td>
<td id="S5.T4.3.11.9.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T4.3.11.9.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.11.9.8.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.11.9.8.1.1.1" class="ltx_text" style="font-size:90%;">48.9 / 27.7</span></span>
</span>
</td>
</tr>
<tr id="S5.T4.3.12.10" class="ltx_tr">
<th id="S5.T4.3.12.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S5.T4.3.12.10.1.1" class="ltx_text" style="font-size:90%;">3DETR-m(30)</span></th>
<td id="S5.T4.3.12.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.12.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.12.10.2.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.12.10.2.1.1.1" class="ltx_text" style="font-size:90%;">29.1 / 13.1</span></span>
</span>
</td>
<td id="S5.T4.3.12.10.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.12.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.12.10.3.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.12.10.3.1.1.1" class="ltx_text" style="font-size:90%;">18.5 / 9.6</span></span>
</span>
</td>
<td id="S5.T4.3.12.10.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.12.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.12.10.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.12.10.4.1.1.1" class="ltx_text" style="font-size:90%;">26.6 / 15.1</span></span>
</span>
</td>
<td id="S5.T4.3.12.10.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.12.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.12.10.5.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.12.10.5.1.1.1" class="ltx_text" style="font-size:90%;">35.4 / 19.7</span></span>
</span>
</td>
<td id="S5.T4.3.12.10.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="background-color:#D9D9D9;">
<span id="S5.T4.3.12.10.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.12.10.6.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.12.10.6.1.1.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;">48.0 / 29.4</span></span>
</span>
</td>
<td id="S5.T4.3.12.10.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.12.10.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.12.10.7.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.12.10.7.1.1.1" class="ltx_text" style="font-size:90%;">54.0 / 33.6</span></span>
</span>
</td>
<td id="S5.T4.3.12.10.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T4.3.12.10.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.12.10.8.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.12.10.8.1.1.1" class="ltx_text" style="font-size:90%;">55.6 / 34.7</span></span>
</span>
</td>
</tr>
<tr id="S5.T4.3.13.11" class="ltx_tr">
<th id="S5.T4.3.13.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S5.T4.3.13.11.1.1" class="ltx_text" style="font-size:90%;">3DETR-m(50)</span></th>
<td id="S5.T4.3.13.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.13.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.13.11.2.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.13.11.2.1.1.1" class="ltx_text" style="font-size:90%;">27.7 / 12.8</span></span>
</span>
</td>
<td id="S5.T4.3.13.11.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.13.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.13.11.3.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.13.11.3.1.1.1" class="ltx_text" style="font-size:90%;">17.9 / 9.7</span></span>
</span>
</td>
<td id="S5.T4.3.13.11.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.13.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.13.11.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.13.11.4.1.1.1" class="ltx_text" style="font-size:90%;">26.6 / 15.8</span></span>
</span>
</td>
<td id="S5.T4.3.13.11.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.13.11.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.13.11.5.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.13.11.5.1.1.1" class="ltx_text" style="font-size:90%;">34.9 / 19.8</span></span>
</span>
</td>
<td id="S5.T4.3.13.11.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.13.11.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.13.11.6.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.13.11.6.1.1.1" class="ltx_text" style="font-size:90%;">46.9 / 28.8</span></span>
</span>
</td>
<td id="S5.T4.3.13.11.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.13.11.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.13.11.7.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.13.11.7.1.1.1" class="ltx_text" style="font-size:90%;">52.7 / 33.0</span></span>
</span>
</td>
<td id="S5.T4.3.13.11.8" class="ltx_td ltx_align_justify ltx_align_top" style="background-color:#D9D9D9;">
<span id="S5.T4.3.13.11.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.13.11.8.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.13.11.8.1.1.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;">54.7 / 35.3</span></span>
</span>
</td>
</tr>
<tr id="S5.T4.3.14.12" class="ltx_tr">
<th id="S5.T4.3.14.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S5.T4.3.14.12.1.1" class="ltx_text" style="font-size:90%;">AnyView(10)</span></th>
<td id="S5.T4.3.14.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.14.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.14.12.2.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.14.12.2.1.1.1" class="ltx_text" style="font-size:90%;">31.3 / 13.7</span></span>
</span>
</td>
<td id="S5.T4.3.14.12.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.14.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.14.12.3.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.14.12.3.1.1.1" class="ltx_text" style="font-size:90%;">20.6 / 10.6</span></span>
</span>
</td>
<td id="S5.T4.3.14.12.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="background-color:#D9D9D9;">
<span id="S5.T4.3.14.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.14.12.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.14.12.4.1.1.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;">30.2 / 15.7</span></span>
</span>
</td>
<td id="S5.T4.3.14.12.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.14.12.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.14.12.5.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.14.12.5.1.1.1" class="ltx_text" style="font-size:90%;">36.0 / 17.7</span></span>
</span>
</td>
<td id="S5.T4.3.14.12.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.14.12.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.14.12.6.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.14.12.6.1.1.1" class="ltx_text" style="font-size:90%;">47.9 / 26.1</span></span>
</span>
</td>
<td id="S5.T4.3.14.12.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.14.12.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.14.12.7.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.14.12.7.1.1.1" class="ltx_text" style="font-size:90%;">53.6 / 28.7</span></span>
</span>
</td>
<td id="S5.T4.3.14.12.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T4.3.14.12.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.14.12.8.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.14.12.8.1.1.1" class="ltx_text" style="font-size:90%;">54.8 / 30.1</span></span>
</span>
</td>
</tr>
<tr id="S5.T4.3.15.13" class="ltx_tr">
<th id="S5.T4.3.15.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S5.T4.3.15.13.1.1" class="ltx_text" style="font-size:90%;">AnyView(30)</span></th>
<td id="S5.T4.3.15.13.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.15.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.15.13.2.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.15.13.2.1.1.1" class="ltx_text" style="font-size:90%;">30.5 / 12.7</span></span>
</span>
</td>
<td id="S5.T4.3.15.13.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.15.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.15.13.3.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.15.13.3.1.1.1" class="ltx_text" style="font-size:90%;">19.9 / 10.1</span></span>
</span>
</td>
<td id="S5.T4.3.15.13.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.15.13.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.15.13.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.15.13.4.1.1.1" class="ltx_text" style="font-size:90%;">29.9 / 16.5</span></span>
</span>
</td>
<td id="S5.T4.3.15.13.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.15.13.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.15.13.5.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.15.13.5.1.1.1" class="ltx_text" style="font-size:90%;">36.8 / 21.4</span></span>
</span>
</td>
<td id="S5.T4.3.15.13.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="background-color:#D9D9D9;">
<span id="S5.T4.3.15.13.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.15.13.6.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.15.13.6.1.1.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;">52.3 / <span id="S5.T4.3.15.13.6.1.1.1.1" class="ltx_text ltx_font_bold">30.0</span></span></span>
</span>
</td>
<td id="S5.T4.3.15.13.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T4.3.15.13.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.15.13.7.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.15.13.7.1.1.1" class="ltx_text" style="font-size:90%;">57.1 / 34.3</span></span>
</span>
</td>
<td id="S5.T4.3.15.13.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T4.3.15.13.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.15.13.8.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.15.13.8.1.1.1" class="ltx_text" style="font-size:90%;">60.1 / </span><span id="S5.T4.3.15.13.8.1.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">37.4</span></span>
</span>
</td>
</tr>
<tr id="S5.T4.3.16.14" class="ltx_tr">
<th id="S5.T4.3.16.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S5.T4.3.16.14.1.1" class="ltx_text" style="font-size:90%;">AnyView(50)</span></th>
<td id="S5.T4.3.16.14.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r">
<span id="S5.T4.3.16.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.16.14.2.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.16.14.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">32.7</span><span id="S5.T4.3.16.14.2.1.1.2" class="ltx_text" style="font-size:90%;"> / </span><span id="S5.T4.3.16.14.2.1.1.3" class="ltx_text ltx_font_bold" style="font-size:90%;">15.3</span></span>
</span>
</td>
<td id="S5.T4.3.16.14.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r">
<span id="S5.T4.3.16.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.16.14.3.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.16.14.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">22.1</span><span id="S5.T4.3.16.14.3.1.1.2" class="ltx_text" style="font-size:90%;"> / </span><span id="S5.T4.3.16.14.3.1.1.3" class="ltx_text ltx_font_bold" style="font-size:90%;">11.3</span></span>
</span>
</td>
<td id="S5.T4.3.16.14.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r">
<span id="S5.T4.3.16.14.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.16.14.4.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.16.14.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">32.9</span><span id="S5.T4.3.16.14.4.1.1.2" class="ltx_text" style="font-size:90%;"> / </span><span id="S5.T4.3.16.14.4.1.1.3" class="ltx_text ltx_font_bold" style="font-size:90%;">18.6</span></span>
</span>
</td>
<td id="S5.T4.3.16.14.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r">
<span id="S5.T4.3.16.14.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.16.14.5.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.16.14.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">40.6</span><span id="S5.T4.3.16.14.5.1.1.2" class="ltx_text" style="font-size:90%;"> / </span><span id="S5.T4.3.16.14.5.1.1.3" class="ltx_text ltx_font_bold" style="font-size:90%;">21.8</span></span>
</span>
</td>
<td id="S5.T4.3.16.14.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r">
<span id="S5.T4.3.16.14.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.16.14.6.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.16.14.6.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">54.4</span><span id="S5.T4.3.16.14.6.1.1.2" class="ltx_text" style="font-size:90%;"> / 29.8</span></span>
</span>
</td>
<td id="S5.T4.3.16.14.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r">
<span id="S5.T4.3.16.14.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.16.14.7.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.16.14.7.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">58.7</span><span id="S5.T4.3.16.14.7.1.1.2" class="ltx_text" style="font-size:90%;"> / </span><span id="S5.T4.3.16.14.7.1.1.3" class="ltx_text ltx_font_bold" style="font-size:90%;">34.4</span></span>
</span>
</td>
<td id="S5.T4.3.16.14.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="background-color:#D9D9D9;">
<span id="S5.T4.3.16.14.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.3.16.14.8.1.1" class="ltx_p" style="width:45.5pt;"><span id="S5.T4.3.16.14.8.1.1.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;"><span id="S5.T4.3.16.14.8.1.1.1.1" class="ltx_text ltx_font_bold">60.7</span> / 35.8</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2310.05346/assets/x3.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="189" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Visual results on ScanNet. We compare the predictions of 3DETR-m and AnyView after NMS with the ground-truth bounding boxes on different scales of input. Top: the whole scene consisting of 50 views, Bottom: a part of scene consisting of 10 adjacent views. AnyView successfully detects all the doors without color information, which shows its strong ability for extracting local geometric information. The predictions of AnyView is also more consistent across different input scales.</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.5.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.6.2" class="ltx_text ltx_font_italic">Results and Analysis</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p"><span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_bold">On scene-level and monocular benchmarks:</span>
We show the performance of different models on scene-level (Rec and MV) and monocular (SV) benchmarks in Table <a href="#S5.T3" title="TABLE III ‣ V-A Experiments Setup ‣ V Experiment ‣ Anyview: Generalizable Indoor 3D Object Detection with Variable Frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. As discussed in Section <a href="#S3.SS1" title="III-A Problem Statement ‣ III Analysis ‣ Anyview: Generalizable Indoor 3D Object Detection with Variable Frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>, previous models trained on ScanNet-Rec or ScanNet-SV generalizes poorly to other settings.
Although 3DETR-m trained on ScanNet-Rec achieves the best performance (65.0/47.0) on scene-level benchmarks when evaluated on ScanNet-Rec, it requires reconstructed point clouds which are not available in many practical scenarios. We find 3DETR-m trained on ScanNet-MV gets lower performance (54.7/35.3) on scene-level benchmarks, which indicates the previous ScanNet-Rec benchmark is too idealistic and the point clouds fused from multi-view depth maps are more challenging for 3D object detection.
Among models trained on ScanNet-MV, ImVoxelNet gets relatively lower performance as color is less informative than depth, especially for 3D object detection. 3DETR-m generalizes better to single view inputs (54.7/35.3) when trained on ScanNet-MV, which shows ScanNet-MV is not only a more practical benchmark for evaluation, but also beneficial for training a generalizable 3D detector.
Observing the rows in gray, AnyView achieves leading result (60.7/35.8) on the challenging ScanNet-MV benchmark and also generalizes well (32.7/15.3) to ScanNet-SV, which even outperforms ImVoteNet (31.6/13.5) trained on ScanNet-SV.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p"><span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_bold">On wider range of input views:</span>
We further extend ScanNet-MV to a series of fine-grained benchmarks for more comprehensive analysis.
We train models on 10/30/50 views uniformly sampled from the whole 50 views in ScanNet-MV and evaluate them on different numbers of views. We devise two settings for partial scenes: Uniform and Continuous, indicating the evaluation of model is conducted on uniformly/adjacently sampled views.
As shown in Table <a href="#S5.T4" title="TABLE IV ‣ V-A Experiments Setup ‣ V Experiment ‣ Anyview: Generalizable Indoor 3D Object Detection with Variable Frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>, 3DETR-m trained on small number of views gets low performance on large number of views, and vice versa. On the contrary, with the growing of numbers of training views, AnyView shows consistent improvement across various scales of input. Even if combining the best results of three 3DETR-m models, AnyView still surpasses them by a large margin with a single set of parameters.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">The detection results of 3DETR-m and AnyView after NMS is shown in Fig. <a href="#S5.F3" title="Figure 3 ‣ V-A Experiments Setup ‣ V Experiment ‣ Anyview: Generalizable Indoor 3D Object Detection with Variable Frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, where a whole scene consisting of 50 views and a part of it consisting 10 adjacent views are chosen as the input.
3DETR-m fails to detect any of the doors for the whole scene. That is because 3DETR-m fuses multi-view point clouds as a whole, which makes the detector hard to focus on thin objects like doors and windows whose geometric information is weak.
It also aggregates outliers into misleading clusters, resulting in false positive.
On the contrary, AnyView extracts scene proxies for each view independently, which owns better understanding of local geometry structures and successfully detects all the doors for both scenes. Benefiting from the input scale-independent property, AnyView outputs consistent predictions for different input scales.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.5.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.6.2" class="ltx_text ltx_font_italic">Ablation Study</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">We conduct ablation studies to show how different architecture designs and training/inference strategies influence the performance of the proposed AnyView framework. The models are trained on ScanNet-MV with 30 views.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p"><span id="S5.SS3.p2.1.1" class="ltx_text ltx_font_bold">Performance w.r.t. architecture design:</span>
We ablate three parts of architecture designs of AnyView, as shown in Table <a href="#S5.T5" title="TABLE V ‣ V-C Ablation Study ‣ V Experiment ‣ Anyview: Generalizable Indoor 3D Object Detection with Variable Frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>. GlobalQuery indicates the object queries are sampled from the concatenated scene (✓) instead of the coordinates of scene proxies. CoordsTrans means whether to apply geometry learner in the camera coordinate (✓) of each view or in the world coordinate. PE<sub id="S5.SS3.p2.1.2" class="ltx_sub"><span id="S5.SS3.p2.1.2.1" class="ltx_text ltx_font_italic">enc</span></sub> indicates whether to use positional embeddings for the transformer encoder. The first two rows show that high object query coverage of the scene is beneficial for the transformer decoder. The third row shows unifying the coordinate makes the geometry learner extract more robust scene proxies. As the features of point clouds already contain positional information, 3DETR finds positional embeddings are not necessary. However, in AnyView the scene proxies are independently extracted, so the spatial relationship between scene proxies from different views is weakened. We find Fourier positional embeddings with a MLP performs best.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">TABLE V: </span>The effects of global query sampling, coordinate transformation and positional embeddings for transformer encoder on the final performance of AnyView on ScanNet-MV.</figcaption>
<table id="S5.T5.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T5.1.1" class="ltx_tr">
<th id="S5.T5.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="2"><span id="S5.T5.1.1.2.1" class="ltx_text" style="font-size:90%;">GlobalQuery</span></th>
<th id="S5.T5.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="2"><span id="S5.T5.1.1.3.1" class="ltx_text" style="font-size:90%;">CoordsTrans</span></th>
<th id="S5.T5.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="2"><span id="S5.T5.1.1.1.1" class="ltx_text" style="font-size:90%;">PE<sub id="S5.T5.1.1.1.1.1" class="ltx_sub"><span id="S5.T5.1.1.1.1.1.1" class="ltx_text ltx_font_italic">enc</span></sub></span></th>
<th id="S5.T5.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="2"><span id="S5.T5.1.1.4.1" class="ltx_text" style="font-size:90%;">mAP</span></th>
</tr>
<tr id="S5.T5.2.3.1" class="ltx_tr">
<th id="S5.T5.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T5.2.3.1.1.1" class="ltx_text" style="font-size:90%;">@0.25</span></th>
<th id="S5.T5.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T5.2.3.1.2.1" class="ltx_text" style="font-size:90%;">@0.5</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T5.2.4.1" class="ltx_tr">
<th id="S5.T5.2.4.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></th>
<td id="S5.T5.2.4.1.2" class="ltx_td ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S5.T5.2.4.1.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S5.T5.2.4.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T5.2.4.1.4.1" class="ltx_text" style="font-size:90%;">58.5</span></td>
<td id="S5.T5.2.4.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T5.2.4.1.5.1" class="ltx_text" style="font-size:90%;">33.1</span></td>
</tr>
<tr id="S5.T5.2.5.2" class="ltx_tr">
<th id="S5.T5.2.5.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T5.2.5.2.1.1" class="ltx_text" style="font-size:90%;">✓</span></th>
<td id="S5.T5.2.5.2.2" class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S5.T5.2.5.2.3" class="ltx_td ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S5.T5.2.5.2.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T5.2.5.2.4.1" class="ltx_text" style="font-size:90%;">59.2</span></td>
<td id="S5.T5.2.5.2.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T5.2.5.2.5.1" class="ltx_text" style="font-size:90%;">34.4</span></td>
</tr>
<tr id="S5.T5.2.6.3" class="ltx_tr">
<th id="S5.T5.2.6.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T5.2.6.3.1.1" class="ltx_text" style="font-size:90%;">✓</span></th>
<td id="S5.T5.2.6.3.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T5.2.6.3.2.1" class="ltx_text" style="font-size:90%;">✓</span></td>
<td id="S5.T5.2.6.3.3" class="ltx_td ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S5.T5.2.6.3.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T5.2.6.3.4.1" class="ltx_text" style="font-size:90%;">59.6</span></td>
<td id="S5.T5.2.6.3.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T5.2.6.3.5.1" class="ltx_text" style="font-size:90%;">34.9</span></td>
</tr>
<tr id="S5.T5.2.7.4" class="ltx_tr">
<th id="S5.T5.2.7.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T5.2.7.4.1.1" class="ltx_text" style="font-size:90%;">✓</span></th>
<td id="S5.T5.2.7.4.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T5.2.7.4.2.1" class="ltx_text" style="font-size:90%;">✓</span></td>
<td id="S5.T5.2.7.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T5.2.7.4.3.1" class="ltx_text" style="font-size:90%;">Fourier</span></td>
<td id="S5.T5.2.7.4.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T5.2.7.4.4.1" class="ltx_text" style="font-size:90%;">60.6</span></td>
<td id="S5.T5.2.7.4.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T5.2.7.4.5.1" class="ltx_text" style="font-size:90%;">35.8</span></td>
</tr>
<tr id="S5.T5.2.2" class="ltx_tr" style="background-color:#D9D9D9;">
<th id="S5.T5.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T5.2.2.2.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;">✓</span></th>
<td id="S5.T5.2.2.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T5.2.2.3.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;">✓</span></td>
<td id="S5.T5.2.2.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T5.2.2.1.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;">MLP<math id="S5.T5.2.2.1.1.m1.1" class="ltx_Math" alttext="\circ" display="inline"><semantics id="S5.T5.2.2.1.1.m1.1a"><mo mathbackground="#D9D9D9" id="S5.T5.2.2.1.1.m1.1.1" xref="S5.T5.2.2.1.1.m1.1.1.cmml">∘</mo><annotation-xml encoding="MathML-Content" id="S5.T5.2.2.1.1.m1.1b"><compose id="S5.T5.2.2.1.1.m1.1.1.cmml" xref="S5.T5.2.2.1.1.m1.1.1"></compose></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.2.1.1.m1.1c">\circ</annotation></semantics></math>Fourier</span></td>
<td id="S5.T5.2.2.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T5.2.2.4.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;">60.1</span></td>
<td id="S5.T5.2.2.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T5.2.2.5.1" class="ltx_text" style="font-size:90%;background-color:#D9D9D9;">37.4</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p"><span id="S5.SS3.p3.1.1" class="ltx_text ltx_font_bold">Performance w.r.t. data augmentations:</span>
We further investigate the effects of two proposed data augment strategies.
ScanNet-MV measures the detection accuracy and ScanNet-SV measures the generalizability of the detector.
As shown in Fig. <a href="#S5.F4" title="Figure 4 ‣ V-C Ablation Study ‣ V Experiment ‣ Anyview: Generalizable Indoor 3D Object Detection with Variable Frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, with the growing of minimum keeping ratio for random view dropping, the performance of AnyView on ScanNet-MV grows but its generalizability to single view input has deteriorated.
In terms of the probability of augmentation, it is shown that low probability leads to overfitting and poor generalizability, while very high probability hurts the performance of AnyView.
Therefore we choose dropping <math id="S5.SS3.p3.1.m1.2" class="ltx_Math" alttext="[0,\frac{N}{2}]" display="inline"><semantics id="S5.SS3.p3.1.m1.2a"><mrow id="S5.SS3.p3.1.m1.2.3.2" xref="S5.SS3.p3.1.m1.2.3.1.cmml"><mo stretchy="false" id="S5.SS3.p3.1.m1.2.3.2.1" xref="S5.SS3.p3.1.m1.2.3.1.cmml">[</mo><mn id="S5.SS3.p3.1.m1.1.1" xref="S5.SS3.p3.1.m1.1.1.cmml">0</mn><mo id="S5.SS3.p3.1.m1.2.3.2.2" xref="S5.SS3.p3.1.m1.2.3.1.cmml">,</mo><mfrac id="S5.SS3.p3.1.m1.2.2" xref="S5.SS3.p3.1.m1.2.2.cmml"><mi id="S5.SS3.p3.1.m1.2.2.2" xref="S5.SS3.p3.1.m1.2.2.2.cmml">N</mi><mn id="S5.SS3.p3.1.m1.2.2.3" xref="S5.SS3.p3.1.m1.2.2.3.cmml">2</mn></mfrac><mo stretchy="false" id="S5.SS3.p3.1.m1.2.3.2.3" xref="S5.SS3.p3.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.1.m1.2b"><interval closure="closed" id="S5.SS3.p3.1.m1.2.3.1.cmml" xref="S5.SS3.p3.1.m1.2.3.2"><cn type="integer" id="S5.SS3.p3.1.m1.1.1.cmml" xref="S5.SS3.p3.1.m1.1.1">0</cn><apply id="S5.SS3.p3.1.m1.2.2.cmml" xref="S5.SS3.p3.1.m1.2.2"><divide id="S5.SS3.p3.1.m1.2.2.1.cmml" xref="S5.SS3.p3.1.m1.2.2"></divide><ci id="S5.SS3.p3.1.m1.2.2.2.cmml" xref="S5.SS3.p3.1.m1.2.2.2">𝑁</ci><cn type="integer" id="S5.SS3.p3.1.m1.2.2.3.cmml" xref="S5.SS3.p3.1.m1.2.2.3">2</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.1.m1.2c">[0,\frac{N}{2}]</annotation></semantics></math> views and augmenting with probability 0.75.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2310.05346/assets/x4.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="329" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The effects of minimum keeping ratio for random view dropping and probability for data augmentation on the final performance of AnyView on ScanNet-MV and ScanNet-SV.</figcaption>
</figure>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2310.05346/assets/figures/figure_vis_v2.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="323" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Online detection visualization results. We demonstrate the detection results for 15, 30, 40, and 50 frames during the navigation(scene0086_00, scene00430_00 and scene0474_00 on the ScanNet dataset).Red bounding boxes and green bounding boxes represent false-positive and true-positive samples, respectively.</figcaption>
</figure>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.5" class="ltx_p"><span id="S5.SS3.p4.5.1" class="ltx_text ltx_font_bold">Performance w.r.t. number of tokens:</span>
Fig. <a href="#S5.F6" title="Figure 6 ‣ V-D Online 3D Object Detection ‣ V Experiment ‣ Anyview: Generalizable Indoor 3D Object Detection with Variable Frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> illustrates the effects of our dynamic token strategy, where the number of scene proxies for each view is <math id="S5.SS3.p4.1.m1.7" class="ltx_Math" alttext="T={\rm min}\{X/N,256\},X\in\{500,1000,2000,4000\}" display="inline"><semantics id="S5.SS3.p4.1.m1.7a"><mrow id="S5.SS3.p4.1.m1.7.7.2" xref="S5.SS3.p4.1.m1.7.7.3.cmml"><mrow id="S5.SS3.p4.1.m1.6.6.1.1" xref="S5.SS3.p4.1.m1.6.6.1.1.cmml"><mi id="S5.SS3.p4.1.m1.6.6.1.1.3" xref="S5.SS3.p4.1.m1.6.6.1.1.3.cmml">T</mi><mo id="S5.SS3.p4.1.m1.6.6.1.1.2" xref="S5.SS3.p4.1.m1.6.6.1.1.2.cmml">=</mo><mrow id="S5.SS3.p4.1.m1.6.6.1.1.1" xref="S5.SS3.p4.1.m1.6.6.1.1.1.cmml"><mi id="S5.SS3.p4.1.m1.6.6.1.1.1.3" xref="S5.SS3.p4.1.m1.6.6.1.1.1.3.cmml">min</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p4.1.m1.6.6.1.1.1.2" xref="S5.SS3.p4.1.m1.6.6.1.1.1.2.cmml">​</mo><mrow id="S5.SS3.p4.1.m1.6.6.1.1.1.1.1" xref="S5.SS3.p4.1.m1.6.6.1.1.1.1.2.cmml"><mo stretchy="false" id="S5.SS3.p4.1.m1.6.6.1.1.1.1.1.2" xref="S5.SS3.p4.1.m1.6.6.1.1.1.1.2.cmml">{</mo><mrow id="S5.SS3.p4.1.m1.6.6.1.1.1.1.1.1" xref="S5.SS3.p4.1.m1.6.6.1.1.1.1.1.1.cmml"><mi id="S5.SS3.p4.1.m1.6.6.1.1.1.1.1.1.2" xref="S5.SS3.p4.1.m1.6.6.1.1.1.1.1.1.2.cmml">X</mi><mo id="S5.SS3.p4.1.m1.6.6.1.1.1.1.1.1.1" xref="S5.SS3.p4.1.m1.6.6.1.1.1.1.1.1.1.cmml">/</mo><mi id="S5.SS3.p4.1.m1.6.6.1.1.1.1.1.1.3" xref="S5.SS3.p4.1.m1.6.6.1.1.1.1.1.1.3.cmml">N</mi></mrow><mo id="S5.SS3.p4.1.m1.6.6.1.1.1.1.1.3" xref="S5.SS3.p4.1.m1.6.6.1.1.1.1.2.cmml">,</mo><mn id="S5.SS3.p4.1.m1.1.1" xref="S5.SS3.p4.1.m1.1.1.cmml">256</mn><mo stretchy="false" id="S5.SS3.p4.1.m1.6.6.1.1.1.1.1.4" xref="S5.SS3.p4.1.m1.6.6.1.1.1.1.2.cmml">}</mo></mrow></mrow></mrow><mo id="S5.SS3.p4.1.m1.7.7.2.3" xref="S5.SS3.p4.1.m1.7.7.3a.cmml">,</mo><mrow id="S5.SS3.p4.1.m1.7.7.2.2" xref="S5.SS3.p4.1.m1.7.7.2.2.cmml"><mi id="S5.SS3.p4.1.m1.7.7.2.2.2" xref="S5.SS3.p4.1.m1.7.7.2.2.2.cmml">X</mi><mo id="S5.SS3.p4.1.m1.7.7.2.2.1" xref="S5.SS3.p4.1.m1.7.7.2.2.1.cmml">∈</mo><mrow id="S5.SS3.p4.1.m1.7.7.2.2.3.2" xref="S5.SS3.p4.1.m1.7.7.2.2.3.1.cmml"><mo stretchy="false" id="S5.SS3.p4.1.m1.7.7.2.2.3.2.1" xref="S5.SS3.p4.1.m1.7.7.2.2.3.1.cmml">{</mo><mn id="S5.SS3.p4.1.m1.2.2" xref="S5.SS3.p4.1.m1.2.2.cmml">500</mn><mo id="S5.SS3.p4.1.m1.7.7.2.2.3.2.2" xref="S5.SS3.p4.1.m1.7.7.2.2.3.1.cmml">,</mo><mn id="S5.SS3.p4.1.m1.3.3" xref="S5.SS3.p4.1.m1.3.3.cmml">1000</mn><mo id="S5.SS3.p4.1.m1.7.7.2.2.3.2.3" xref="S5.SS3.p4.1.m1.7.7.2.2.3.1.cmml">,</mo><mn id="S5.SS3.p4.1.m1.4.4" xref="S5.SS3.p4.1.m1.4.4.cmml">2000</mn><mo id="S5.SS3.p4.1.m1.7.7.2.2.3.2.4" xref="S5.SS3.p4.1.m1.7.7.2.2.3.1.cmml">,</mo><mn id="S5.SS3.p4.1.m1.5.5" xref="S5.SS3.p4.1.m1.5.5.cmml">4000</mn><mo stretchy="false" id="S5.SS3.p4.1.m1.7.7.2.2.3.2.5" xref="S5.SS3.p4.1.m1.7.7.2.2.3.1.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.1.m1.7b"><apply id="S5.SS3.p4.1.m1.7.7.3.cmml" xref="S5.SS3.p4.1.m1.7.7.2"><csymbol cd="ambiguous" id="S5.SS3.p4.1.m1.7.7.3a.cmml" xref="S5.SS3.p4.1.m1.7.7.2.3">formulae-sequence</csymbol><apply id="S5.SS3.p4.1.m1.6.6.1.1.cmml" xref="S5.SS3.p4.1.m1.6.6.1.1"><eq id="S5.SS3.p4.1.m1.6.6.1.1.2.cmml" xref="S5.SS3.p4.1.m1.6.6.1.1.2"></eq><ci id="S5.SS3.p4.1.m1.6.6.1.1.3.cmml" xref="S5.SS3.p4.1.m1.6.6.1.1.3">𝑇</ci><apply id="S5.SS3.p4.1.m1.6.6.1.1.1.cmml" xref="S5.SS3.p4.1.m1.6.6.1.1.1"><times id="S5.SS3.p4.1.m1.6.6.1.1.1.2.cmml" xref="S5.SS3.p4.1.m1.6.6.1.1.1.2"></times><ci id="S5.SS3.p4.1.m1.6.6.1.1.1.3.cmml" xref="S5.SS3.p4.1.m1.6.6.1.1.1.3">min</ci><set id="S5.SS3.p4.1.m1.6.6.1.1.1.1.2.cmml" xref="S5.SS3.p4.1.m1.6.6.1.1.1.1.1"><apply id="S5.SS3.p4.1.m1.6.6.1.1.1.1.1.1.cmml" xref="S5.SS3.p4.1.m1.6.6.1.1.1.1.1.1"><divide id="S5.SS3.p4.1.m1.6.6.1.1.1.1.1.1.1.cmml" xref="S5.SS3.p4.1.m1.6.6.1.1.1.1.1.1.1"></divide><ci id="S5.SS3.p4.1.m1.6.6.1.1.1.1.1.1.2.cmml" xref="S5.SS3.p4.1.m1.6.6.1.1.1.1.1.1.2">𝑋</ci><ci id="S5.SS3.p4.1.m1.6.6.1.1.1.1.1.1.3.cmml" xref="S5.SS3.p4.1.m1.6.6.1.1.1.1.1.1.3">𝑁</ci></apply><cn type="integer" id="S5.SS3.p4.1.m1.1.1.cmml" xref="S5.SS3.p4.1.m1.1.1">256</cn></set></apply></apply><apply id="S5.SS3.p4.1.m1.7.7.2.2.cmml" xref="S5.SS3.p4.1.m1.7.7.2.2"><in id="S5.SS3.p4.1.m1.7.7.2.2.1.cmml" xref="S5.SS3.p4.1.m1.7.7.2.2.1"></in><ci id="S5.SS3.p4.1.m1.7.7.2.2.2.cmml" xref="S5.SS3.p4.1.m1.7.7.2.2.2">𝑋</ci><set id="S5.SS3.p4.1.m1.7.7.2.2.3.1.cmml" xref="S5.SS3.p4.1.m1.7.7.2.2.3.2"><cn type="integer" id="S5.SS3.p4.1.m1.2.2.cmml" xref="S5.SS3.p4.1.m1.2.2">500</cn><cn type="integer" id="S5.SS3.p4.1.m1.3.3.cmml" xref="S5.SS3.p4.1.m1.3.3">1000</cn><cn type="integer" id="S5.SS3.p4.1.m1.4.4.cmml" xref="S5.SS3.p4.1.m1.4.4">2000</cn><cn type="integer" id="S5.SS3.p4.1.m1.5.5.cmml" xref="S5.SS3.p4.1.m1.5.5">4000</cn></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.1.m1.7c">T={\rm min}\{X/N,256\},X\in\{500,1000,2000,4000\}</annotation></semantics></math>.
We find when the number of scene proxies is too small (<math id="S5.SS3.p4.2.m2.1" class="ltx_Math" alttext="N_{T}\leq 1000" display="inline"><semantics id="S5.SS3.p4.2.m2.1a"><mrow id="S5.SS3.p4.2.m2.1.1" xref="S5.SS3.p4.2.m2.1.1.cmml"><msub id="S5.SS3.p4.2.m2.1.1.2" xref="S5.SS3.p4.2.m2.1.1.2.cmml"><mi id="S5.SS3.p4.2.m2.1.1.2.2" xref="S5.SS3.p4.2.m2.1.1.2.2.cmml">N</mi><mi id="S5.SS3.p4.2.m2.1.1.2.3" xref="S5.SS3.p4.2.m2.1.1.2.3.cmml">T</mi></msub><mo id="S5.SS3.p4.2.m2.1.1.1" xref="S5.SS3.p4.2.m2.1.1.1.cmml">≤</mo><mn id="S5.SS3.p4.2.m2.1.1.3" xref="S5.SS3.p4.2.m2.1.1.3.cmml">1000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.2.m2.1b"><apply id="S5.SS3.p4.2.m2.1.1.cmml" xref="S5.SS3.p4.2.m2.1.1"><leq id="S5.SS3.p4.2.m2.1.1.1.cmml" xref="S5.SS3.p4.2.m2.1.1.1"></leq><apply id="S5.SS3.p4.2.m2.1.1.2.cmml" xref="S5.SS3.p4.2.m2.1.1.2"><csymbol cd="ambiguous" id="S5.SS3.p4.2.m2.1.1.2.1.cmml" xref="S5.SS3.p4.2.m2.1.1.2">subscript</csymbol><ci id="S5.SS3.p4.2.m2.1.1.2.2.cmml" xref="S5.SS3.p4.2.m2.1.1.2.2">𝑁</ci><ci id="S5.SS3.p4.2.m2.1.1.2.3.cmml" xref="S5.SS3.p4.2.m2.1.1.2.3">𝑇</ci></apply><cn type="integer" id="S5.SS3.p4.2.m2.1.1.3.cmml" xref="S5.SS3.p4.2.m2.1.1.3">1000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.2.m2.1c">N_{T}\leq 1000</annotation></semantics></math>), the performance of AnyView will even drop with the growth of view numbers.
With the increasing number of <math id="S5.SS3.p4.3.m3.1" class="ltx_Math" alttext="N_{T}" display="inline"><semantics id="S5.SS3.p4.3.m3.1a"><msub id="S5.SS3.p4.3.m3.1.1" xref="S5.SS3.p4.3.m3.1.1.cmml"><mi id="S5.SS3.p4.3.m3.1.1.2" xref="S5.SS3.p4.3.m3.1.1.2.cmml">N</mi><mi id="S5.SS3.p4.3.m3.1.1.3" xref="S5.SS3.p4.3.m3.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.3.m3.1b"><apply id="S5.SS3.p4.3.m3.1.1.cmml" xref="S5.SS3.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS3.p4.3.m3.1.1.1.cmml" xref="S5.SS3.p4.3.m3.1.1">subscript</csymbol><ci id="S5.SS3.p4.3.m3.1.1.2.cmml" xref="S5.SS3.p4.3.m3.1.1.2">𝑁</ci><ci id="S5.SS3.p4.3.m3.1.1.3.cmml" xref="S5.SS3.p4.3.m3.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.3.m3.1c">N_{T}</annotation></semantics></math>, the performance of AnyView is significantly improved for any view number.
However, when NT is greater than 2000, the rate of performance increase decreases significantly, introducing additional computational cost without sufficient performance improvement, and <math id="S5.SS3.p4.4.m4.1" class="ltx_Math" alttext="N_{T}=4000" display="inline"><semantics id="S5.SS3.p4.4.m4.1a"><mrow id="S5.SS3.p4.4.m4.1.1" xref="S5.SS3.p4.4.m4.1.1.cmml"><msub id="S5.SS3.p4.4.m4.1.1.2" xref="S5.SS3.p4.4.m4.1.1.2.cmml"><mi id="S5.SS3.p4.4.m4.1.1.2.2" xref="S5.SS3.p4.4.m4.1.1.2.2.cmml">N</mi><mi id="S5.SS3.p4.4.m4.1.1.2.3" xref="S5.SS3.p4.4.m4.1.1.2.3.cmml">T</mi></msub><mo id="S5.SS3.p4.4.m4.1.1.1" xref="S5.SS3.p4.4.m4.1.1.1.cmml">=</mo><mn id="S5.SS3.p4.4.m4.1.1.3" xref="S5.SS3.p4.4.m4.1.1.3.cmml">4000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.4.m4.1b"><apply id="S5.SS3.p4.4.m4.1.1.cmml" xref="S5.SS3.p4.4.m4.1.1"><eq id="S5.SS3.p4.4.m4.1.1.1.cmml" xref="S5.SS3.p4.4.m4.1.1.1"></eq><apply id="S5.SS3.p4.4.m4.1.1.2.cmml" xref="S5.SS3.p4.4.m4.1.1.2"><csymbol cd="ambiguous" id="S5.SS3.p4.4.m4.1.1.2.1.cmml" xref="S5.SS3.p4.4.m4.1.1.2">subscript</csymbol><ci id="S5.SS3.p4.4.m4.1.1.2.2.cmml" xref="S5.SS3.p4.4.m4.1.1.2.2">𝑁</ci><ci id="S5.SS3.p4.4.m4.1.1.2.3.cmml" xref="S5.SS3.p4.4.m4.1.1.2.3">𝑇</ci></apply><cn type="integer" id="S5.SS3.p4.4.m4.1.1.3.cmml" xref="S5.SS3.p4.4.m4.1.1.3">4000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.4.m4.1c">N_{T}=4000</annotation></semantics></math> even leads to a decrease in accuracy.
So finally we set <math id="S5.SS3.p4.5.m5.1" class="ltx_Math" alttext="N_{T}=2000" display="inline"><semantics id="S5.SS3.p4.5.m5.1a"><mrow id="S5.SS3.p4.5.m5.1.1" xref="S5.SS3.p4.5.m5.1.1.cmml"><msub id="S5.SS3.p4.5.m5.1.1.2" xref="S5.SS3.p4.5.m5.1.1.2.cmml"><mi id="S5.SS3.p4.5.m5.1.1.2.2" xref="S5.SS3.p4.5.m5.1.1.2.2.cmml">N</mi><mi id="S5.SS3.p4.5.m5.1.1.2.3" xref="S5.SS3.p4.5.m5.1.1.2.3.cmml">T</mi></msub><mo id="S5.SS3.p4.5.m5.1.1.1" xref="S5.SS3.p4.5.m5.1.1.1.cmml">=</mo><mn id="S5.SS3.p4.5.m5.1.1.3" xref="S5.SS3.p4.5.m5.1.1.3.cmml">2000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.5.m5.1b"><apply id="S5.SS3.p4.5.m5.1.1.cmml" xref="S5.SS3.p4.5.m5.1.1"><eq id="S5.SS3.p4.5.m5.1.1.1.cmml" xref="S5.SS3.p4.5.m5.1.1.1"></eq><apply id="S5.SS3.p4.5.m5.1.1.2.cmml" xref="S5.SS3.p4.5.m5.1.1.2"><csymbol cd="ambiguous" id="S5.SS3.p4.5.m5.1.1.2.1.cmml" xref="S5.SS3.p4.5.m5.1.1.2">subscript</csymbol><ci id="S5.SS3.p4.5.m5.1.1.2.2.cmml" xref="S5.SS3.p4.5.m5.1.1.2.2">𝑁</ci><ci id="S5.SS3.p4.5.m5.1.1.2.3.cmml" xref="S5.SS3.p4.5.m5.1.1.2.3">𝑇</ci></apply><cn type="integer" id="S5.SS3.p4.5.m5.1.1.3.cmml" xref="S5.SS3.p4.5.m5.1.1.3">2000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.5.m5.1c">N_{T}=2000</annotation></semantics></math> for a better trade-off between performance and cost.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS4.5.1.1" class="ltx_text">V-D</span> </span><span id="S5.SS4.6.2" class="ltx_text ltx_font_italic">Online 3D Object Detection</span>
</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">We conduct online object detection experiments to demonstrate the performance of AnyView with any-scale point cloud input.
We select 3DETR as the baseline network, and these models are all trained on ScanNet-MV with 50 views.
During the validation process, the views of each scene are fed into AnyView in a frame-by-frame format, and the aggregated point cloud reconstruction results are fed into the 3DETR.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p"><span id="S5.SS4.p2.1.1" class="ltx_text ltx_font_bold">Quantitative results:</span>
We demonstrate the average accuracy of AnyView and 3DETR in the online detection setting, as illustrated in Table <a href="#S5.T6" title="TABLE VI ‣ V-D Online 3D Object Detection ‣ V Experiment ‣ Anyview: Generalizable Indoor 3D Object Detection with Variable Frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>.
Our proposed AnyView achieves an average accuracy of (55.32/30.94) in the online detection setting.
Compared to the MV(D) setting, which only decreases by 5.47 and 4.86 on the mAP@25 and mAP@50 measures, respectively.
However, since 3DETR is unable to adapt to any size point cloud, the accuracy decreases significantly in the online setting, by 13.61 vs. 11.48, respectively.
Meanwhile, we have visualized the curve of model performance with frame rate, as shown in Fig. <a href="#S5.F7" title="Figure 7 ‣ V-D Online 3D Object Detection ‣ V Experiment ‣ Anyview: Generalizable Indoor 3D Object Detection with Variable Frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
As the number of input RGB-D frames is too few, both AnyView and 3DETR are less accurate due to the sparse point cloud, though AnyView still outperforms 3DETR by 7.26 vs. 6.68.
As the number of input RGB-D frames increases, AnyView improves accuracy at a faster rate and stabilizes (View Number = 10). Meanwhile, the accuracy of AnyView has reached 52.78 vs. 29.18, which is only 6.71 vs. 6.64 lower than the overall scene.
However, the accuracy of 3DETR has been improving with the number of reconstructed scene frames due to the failure to adapt to sparsely distributed scene point clouds.
The performance of 3DETR stabilizes only after 40 frames of RGB-D image reconstruction and is 4.62 lower on the mAP@25 measure compared to AnyView.
All the above results indicate that our proposed AnyView can be able to handle scene point clouds of any size and has the ability of online detection.</p>
</div>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2310.05346/assets/x5.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="207" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>The effects of token number during inference on the final performance of AnyView on ScanNet-MV.</figcaption>
</figure>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p"><span id="S5.SS4.p3.1.1" class="ltx_text ltx_font_bold">Qualitative results:</span>
We visualize the detection results of AnyView with 3DETR at different frame numbers as illustrated in Fig. <a href="#S5.F5" title="Figure 5 ‣ V-C Ablation Study ‣ V Experiment ‣ Anyview: Generalizable Indoor 3D Object Detection with Variable Frames" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
In the case of a large degree of site cloud fragmentation (15 Views), 3DETR has more false-positive samples, while AnyView still maintains a high detection accuracy.
As the number of viewpoints of the reconstructed scene point cloud increases, the false-positive samples detected by 3DETR are suppressed to some extent, and better results are achieved at 50 Views.
While the detection results of AnyView remain stable, the increase in the number of viewpoints does not lead to a significant increase in performance.
The best performance is also achieved at 50 Views.
The visualization results of online detection further demonstrate that the proposed AnyView can efficiently handle input scenes with different numbers of views and has the capability of online detection.</p>
</div>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2310.05346/assets/figures/figure_online_data.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="312" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>The effect of the number of image frames on the overall scene detection accuracy in the Online setting.</figcaption>
</figure>
<figure id="S5.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">TABLE VI: </span>3D object detection results at the online setting (MAP@0.25 and MAP@0.5), where S-L denotes Scene-level</figcaption>
<table id="S5.T6.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T6.3.1.1" class="ltx_tr">
<th id="S5.T6.3.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"></th>
<th id="S5.T6.3.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="2"><span id="S5.T6.3.1.1.2.1" class="ltx_text" style="font-size:90%;">Method</span></th>
<th id="S5.T6.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T6.3.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Training</span></th>
<th id="S5.T6.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T6.3.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Evaluation</span></th>
<th id="S5.T6.3.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="2"><span id="S5.T6.3.1.1.5.1" class="ltx_text" style="font-size:90%;">mAP</span></th>
</tr>
<tr id="S5.T6.3.2.2" class="ltx_tr">
<th id="S5.T6.3.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></th>
<td id="S5.T6.3.2.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T6.3.2.2.2.1" class="ltx_text" style="font-size:90%;">Benchmark</span></td>
<td id="S5.T6.3.2.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T6.3.2.2.3.1" class="ltx_text" style="font-size:90%;">Benchmark</span></td>
<td id="S5.T6.3.2.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T6.3.2.2.4.1" class="ltx_text" style="font-size:90%;">@0.25</span></td>
<td id="S5.T6.3.2.2.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T6.3.2.2.5.1" class="ltx_text" style="font-size:90%;">@0.5</span></td>
</tr>
<tr id="S5.T6.3.3.3" class="ltx_tr">
<th id="S5.T6.3.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="2"><span id="S5.T6.3.3.3.1.1" class="ltx_text" style="font-size:90%;">
<span id="S5.T6.3.3.3.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.1pt;height:13.6pt;vertical-align:-3.7pt;"><span class="ltx_transformed_inner" style="width:13.6pt;transform:translate(-3.74pt,0pt) rotate(-90deg) ;">
<span id="S5.T6.3.3.3.1.1.1.1" class="ltx_p">S-L</span>
</span></span></span></th>
<th id="S5.T6.3.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T6.3.3.3.2.1" class="ltx_text" style="font-size:90%;">3DETR</span></th>
<th id="S5.T6.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T6.3.3.3.3.1" class="ltx_text" style="font-size:90%;">MV(D)</span></th>
<th id="S5.T6.3.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T6.3.3.3.4.1" class="ltx_text" style="font-size:90%;">Online</span></th>
<th id="S5.T6.3.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T6.3.3.3.5.1" class="ltx_text" style="font-size:90%;">41.09</span></th>
<th id="S5.T6.3.3.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T6.3.3.3.6.1" class="ltx_text" style="font-size:90%;">23.82</span></th>
</tr>
<tr id="S5.T6.3.4.4" class="ltx_tr">
<th id="S5.T6.3.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T6.3.4.4.1.1" class="ltx_text" style="font-size:90%;">AnyView</span></th>
<td id="S5.T6.3.4.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T6.3.4.4.2.1" class="ltx_text" style="font-size:90%;">MV(D)</span></td>
<td id="S5.T6.3.4.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T6.3.4.4.3.1" class="ltx_text" style="font-size:90%;">Online</span></td>
<td id="S5.T6.3.4.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T6.3.4.4.4.1" class="ltx_text" style="font-size:90%;">55.23</span></td>
<td id="S5.T6.3.4.4.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T6.3.4.4.5.1" class="ltx_text" style="font-size:90%;">30.94</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we challenge the existing benchmarks for indoor 3D object detection in a perspective of input data.
We propose a new practical setting for this task, which unifies the input modality as multi-view RGB-D images with variable input frame numbers and evaluates 3D detectors on various scales of input data.
We design a new transformer-based framework named AnyView for practical applications, which is able to process scenes consisting of any number of frames and extract input scale-independent scene representations.
Benefiting from the nature of the self-attention mechanism and our scale-independent training strategy, AnyView is able to change the number of scene representations extracted for each input frame during inference and flexibly handle various numbers of input frames.
Extensive experiments on the ScanNet dataset show that AnyView achieves both great generalizability and high detection accuracy while containing a similar amount of parameters with the baselines.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">The main limitation of this work is the weak generalization to new categories of objects in practical scenes.
Meanwhile, the dynamic token mechanism is only adaptive in terms of quantity, which may lead to loss of information.
Future work hopes to achieve dynamic adaptation with token pruning techniques and open vocabulary detection.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work was supported in part by the National Natural Science Foundation of China under Grant 62376032, Grant U22B2050, and Grant 62125603.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Pei An, Yucong Duan, Yuliang Huang, Jie Ma, Yanfei Chen, Liheng Wang, You Yang,
and Qiong Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Sp-det: Leveraging saliency prediction for voxel-based 3d object
detection in sparse point cloud.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Multimedia</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.5.1" class="ltx_text" style="font-size:90%;">doi:</span><span id="bib.bib1.6.2" class="ltx_text" style="font-size:90%;color:#0000FF;">
<a target="_blank" href="http://dx.doi.org/10.1109/TMM.2023.3304054" title="" class="ltx_ref ltx_href">10.1109/TMM.2023.3304054</a></span><span id="bib.bib1.7.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Xuyang Bai, Zeyu Hu, Xinge Zhu, Qingqiu Huang, Yilun Chen, Hongbo Fu, and
Chiew-Lan Tai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Transfusion: Robust lidar-camera fusion for 3d object detection with
transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, pages 1090–1099, 2022.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
Kirillov, and Sergey Zagoruyko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">End-to-end object detection with transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, pages 213–229.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Multi-view 3d object detection network for autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, pages 1907–1915, 2017.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser,
and Matthias Nießner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Scannet: Richly-annotated 3d reconstructions of indoor scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 2432–2443, 2017.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Jiajun Deng, Wengang Zhou, Yanyong Zhang, and Houqiang Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">From multi-view to hollow-3d: Hallucinated hollow-3d r-cnn for 3d
object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Circuits and Systems for Video Technology</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">,
31(12):4722–4734, 2021.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Zhuo Deng and Longin Jan Latecki.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Amodal detection of 3d objects: Inferring 3d bounding boxes from 2d
ones in rgb-depth images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, pages 5762–5770, 2017.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Baojie Fan, Kexin Zhang, and Jiandong Tian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Hcpvf: Hierarchical cascaded point-voxel fusion for 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Circuits and Systems for Video Technology</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">,
2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.5.1" class="ltx_text" style="font-size:90%;">doi:</span><span id="bib.bib8.6.2" class="ltx_text" style="font-size:90%;color:#0000FF;">
<a target="_blank" href="http://dx.doi.org/10.1109/TCSVT.2023.3268849" title="" class="ltx_ref ltx_href">10.1109/TCSVT.2023.3268849</a></span><span id="bib.bib8.7.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Mingtao Feng, Syed Zulqarnain Gilani, Yaonan Wang, Liang Zhang, and Ajmal Mian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Relation graph network for 3d object detection in point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Image Processing</span><span id="bib.bib9.4.2" class="ltx_text" style="font-size:90%;">, 30:92–107, 2020.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Andreas Geiger, Philip Lenz, and Raquel Urtasun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Are we ready for autonomous driving? the kitti vision benchmark
suite.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 3354–3361, 2012.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Benjamin Graham, Martin Engelcke, and Laurens Van Der Maaten.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">3d semantic segmentation with submanifold sparse convolutional
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages 9224–9232, 2018.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Noriaki Hirose, Shun Taguchi, Fei Xia, Roberto Martín-Martín, Kosuke
Tahara, Masanori Ishigaki, and Silvio Savarese.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Probabilistic visual navigation with bidirectional image prediction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/RSJ International Conference on Intelligent Robots and
Systems</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, pages 1539–1546. IEEE, 2021.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Dinh-Cuong Hoang, Johannes A Stork, and Todor Stoyanov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Voting and attention-based pose relation learning for object pose
estimation from 3d point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Robotics and Automation Letters</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">, 7(4):8980–8987, 2022.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Ji Hou, Angela Dai, and Matthias Nießner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">3d-sis: 3d semantic instance segmentation of rgb-d scans.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, pages 4421–4430, 2019.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Shan Jiayao, Sifan Zhou, Yubo Cui, and Zheng Fang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Real-time 3d single object tracking with transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Multimedia</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">, 25:2339–2353, 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.5.1" class="ltx_text" style="font-size:90%;">doi:</span><span id="bib.bib15.6.2" class="ltx_text" style="font-size:90%;color:#0000FF;">
<a target="_blank" href="http://dx.doi.org/10.1109/TMM.2022.3146714" title="" class="ltx_ref ltx_href">10.1109/TMM.2022.3146714</a></span><span id="bib.bib15.7.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Abhijit Kundu, Yin Li, and James M Rehg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">3d-rcnn: Instance-level 3d object reconstruction via
render-and-compare.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages 3559–3568, 2018.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Yangyan Li, Angela Dai, Leonidas Guibas, and Matthias Nießner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Database-assisted object retrieval for real-time 3d reconstruction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Graphics Forum</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, volume 34, pages 435–446, 2015.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Yishi Li, Yuhao Zhang, and Rui Lai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Tinypillarnet: Tiny pillar-based network for 3d point cloud object
detection at edge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Circuits and Systems for Video Technology</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">,
2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.5.1" class="ltx_text" style="font-size:90%;">doi:</span><span id="bib.bib18.6.2" class="ltx_text" style="font-size:90%;color:#0000FF;">
<a target="_blank" href="http://dx.doi.org/10.1109/TCSVT.2023.3297620" title="" class="ltx_ref ltx_href">10.1109/TCSVT.2023.3297620</a></span><span id="bib.bib18.7.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Or Litany, Tal Remez, Daniel Freedman, Lior Shapira, Alex Bronstein, and Ran
Gal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Asist: automatic semantically invariant scene transformation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision and Image Understanding</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, 157:284–299, 2017.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Hao Liu, Yanni Ma, Qingyong Hu, and Yulan Guo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Centertube: Tracking multiple 3d objects with 4d tubelets in dynamic
point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Multimedia</span><span id="bib.bib20.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.5.1" class="ltx_text" style="font-size:90%;">doi:</span><span id="bib.bib20.6.2" class="ltx_text" style="font-size:90%;color:#0000FF;">
<a target="_blank" href="http://dx.doi.org/10.1109/TMM.2023.3241548" title="" class="ltx_ref ltx_href">10.1109/TMM.2023.3241548</a></span><span id="bib.bib20.7.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Zhanwen Liu, Juanru Cheng, Jin Fan, Shan Lin, Yang Wang, and Xiangmo Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Multi-modal fusion based on depth adaptive mechanism for 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Multimedia</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.5.1" class="ltx_text" style="font-size:90%;">doi:</span><span id="bib.bib21.6.2" class="ltx_text" style="font-size:90%;color:#0000FF;">
<a target="_blank" href="http://dx.doi.org/10.1109/TMM.2023.3270638" title="" class="ltx_ref ltx_href">10.1109/TMM.2023.3270638</a></span><span id="bib.bib21.7.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Zhe Liu, Tengteng Huang, Bingling Li, Xiwu Chen, Xi Wang, and Xiang Bai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Epnet++: Cascade bi-directional fusion for multi-modal 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">,
45(7):8324–8341, 2023.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Ishan Misra, Rohit Girdhar, and Armand Joulin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">An end-to-end transformer model for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, pages 2906–2917, 2021.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Indoor segmentation and support inference from rgbd images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">,
2012.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Ramanpreet Singh Pahwa, Jiangbo Lu, Nianjuan Jiang, Tian Tsong Ng, and Minh N
Do.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Locating 3d object proposals: A depth-based online approach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Circuits and Systems for Video Technology</span><span id="bib.bib25.4.2" class="ltx_text" style="font-size:90%;">,
28(3):626–639, 2016.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Anshul Paigwar, David Sierra-Gonzalez, Özgür Erkent, and Christian
Laugier.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Frustum-pointpillars: A multi-stage approach for 3d object detection
using rgb camera and lidar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, pages 2926–2933, 2021.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Jin-Chun Piao and Shin-Dug Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Real-time visual–inertial slam based on adaptive keyframe selection
for mobile ar applications.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Multimedia</span><span id="bib.bib27.4.2" class="ltx_text" style="font-size:90%;">, 21(11):2827–2836, 2019.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Charles R Qi, Xinlei Chen, Or Litany, and Leonidas J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Imvotenet: Boosting 3d object detection in point clouds with image
votes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, pages 4404–4413, 2020.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Charles R. Qi, Or Litany, Kaiming He, and Leonidas J. Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Deep hough voting for 3d object detection in point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, pages 9277–9286, 2019.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Frustum pointnets for 3d object detection from rgb-d data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, pages 918–927, 2018.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Pointnet: Deep learning on point sets for 3d classification and
segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, pages 652–660, 2017.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Pointnet++: Deep hierarchical feature learning on point sets in a
metric space.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, pages
5099–5108, 2017.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Colin Rennie, Rahul Shome, Kostas E Bekris, and Alberto F De Souza.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">A dataset for improved rgbd-based object detection and pose
estimation for warehouse pick-and-place.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Robotics and Automation Letters</span><span id="bib.bib33.4.2" class="ltx_text" style="font-size:90%;">, 1(2):1179–1185, 2016.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Danila Rukhovich, Anna Vorontsova, and Anton Konushin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Imvoxelnet: Image to voxels projection for monocular and multi-view
general-purpose 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Winter Conference on Applications
of Computer Vision</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, pages 2397–2406, 2022.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Shaoshuai Shi, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">From points to parts: 3d object detection from point cloud with
part-aware and part-aggregation network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib35.4.2" class="ltx_text" style="font-size:90%;">,
43(8):2647–2664, 2021.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Kiwoo Shin, Youngwook Paul Kwon, and Masayoshi Tomizuka.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Roarnet: A robust 3d object detection based on region approximation
refinement.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Intelligent Vehicles Symposium</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, pages 2510–2515. IEEE,
2019.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Sun rgb-d: A rgb-d scene understanding benchmark suite.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, pages 567–576, 2015.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Shuran Song and Jianxiong Xiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Sliding shapes for 3d object detection in depth images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">,
pages 634–651, 2014.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Shuran Song and Jianxiong Xiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Deep sliding shapes for amodal 3d object detection in rgb-d images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, pages 808–816, 2016.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin
Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Fourier features let networks learn high frequency functions in low
dimensional domains.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib40.4.2" class="ltx_text" style="font-size:90%;">,
33:7537–7547, 2020.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Haotian Tang, Zhijian Liu, Shengyu Zhao, Yujun Lin, Ji Lin, Hanrui Wang, and
Song Han.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Searching efficient 3d architectures with sparse point-voxel
convolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">,
2020.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Guofeng Tong, Zheng Li, Hao Peng, and Yaqi Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Multi-source features fusion single stage 3d object detection with
transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Robotics and Automation Letters</span><span id="bib.bib42.4.2" class="ltx_text" style="font-size:90%;">, 8(4):2062–2069, 2023.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Attention is all you need.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, pages
5998–6008, 2017.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Sourabh Vora, Alex H Lang, Bassam Helou, and Oscar Beijbom.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Pointpainting: Sequential fusion for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, pages 4604–4612, 2020.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Deming Wang, Guangliang Zhou, Yi Yan, Huiyi Chen, and Qijun Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Geopose: Dense reconstruction guided 6d object pose estimation with
geometric consistency.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Multimedia</span><span id="bib.bib45.4.2" class="ltx_text" style="font-size:90%;">, 24:4394–4408, 2021.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Jian Wang, Fan Li, Xuchong Zhang, and Hongbin Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Adversarial obstacle generation against lidar-based 3d object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Multimedia</span><span id="bib.bib46.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.5.1" class="ltx_text" style="font-size:90%;">doi:</span><span id="bib.bib46.6.2" class="ltx_text" style="font-size:90%;color:#0000FF;">
<a target="_blank" href="http://dx.doi.org/10.1109/TMM.2023.3302018" title="" class="ltx_ref ltx_href">10.1109/TMM.2023.3302018</a></span><span id="bib.bib46.7.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Shuhua Wang, Ke Lu, Jian Xue, and Yang Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Da-net: Density-aware 3d object detection network for point clouds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Multimedia</span><span id="bib.bib47.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.5.1" class="ltx_text" style="font-size:90%;">doi:</span><span id="bib.bib47.6.2" class="ltx_text" style="font-size:90%;color:#0000FF;">
<a target="_blank" href="http://dx.doi.org/10.1109/TMM.2023.3245359" title="" class="ltx_ref ltx_href">10.1109/TMM.2023.3245359</a></span><span id="bib.bib47.7.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Zhixin Wang and Kui Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Frustum convnet: Sliding frustums to aggregate local point-wise
features for amodal 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/RSJ International Conference on Intelligent Robots and
Systems</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, pages 1742–1749. IEEE, 2019.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Fei Xia, William B Shen, Chengshu Li, Priya Kasimbeg, Micael Edmond Tchapmi,
Alexander Toshev, Roberto Martín-Martín, and Silvio Savarese.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Interactive gibson benchmark: A benchmark for interactive navigation
in cluttered environments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Robotics and Automation Letters</span><span id="bib.bib49.4.2" class="ltx_text" style="font-size:90%;">, 5(2):713–720, 2020.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Bangquan Xie, Liang Yang, Ailin Wei, Xiaoxiong Weng, and Bing Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Mutrans: Multiple transformers for fusing feature pyramid on 2d and
3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Image Processing</span><span id="bib.bib50.4.2" class="ltx_text" style="font-size:90%;">, 32:4407–4415, 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.5.1" class="ltx_text" style="font-size:90%;">doi:</span><span id="bib.bib50.6.2" class="ltx_text" style="font-size:90%;color:#0000FF;">
<a target="_blank" href="http://dx.doi.org/10.1109/TIP.2023.3299190" title="" class="ltx_ref ltx_href">10.1109/TIP.2023.3299190</a></span><span id="bib.bib50.7.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Tao Xie, Li Wang, Ke Wang, Ruifeng Li, Xinyu Zhang, Haoming Zhang, Linqi Yang,
Huaping Liu, and Jun Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Farp-net: Local-global feature aggregation and relation-aware
proposals for 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Multimedia</span><span id="bib.bib51.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.5.1" class="ltx_text" style="font-size:90%;">doi:</span><span id="bib.bib51.6.2" class="ltx_text" style="font-size:90%;color:#0000FF;">
<a target="_blank" href="http://dx.doi.org/10.1109/TMM.2023.3275366" title="" class="ltx_ref ltx_href">10.1109/TMM.2023.3275366</a></span><span id="bib.bib51.7.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Danfei Xu, Dragomir Anguelov, and Ashesh Jain.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Pointfusion: Deep sensor fusion for 3d bounding box estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib52.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib52.5.3" class="ltx_text" style="font-size:90%;">, pages 244–253, 2018.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Shaoqing Xu, Dingfu Zhou, Jin Fang, Junbo Yin, Zhou Bin, and Liangjun Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Fusionpainting: Multimodal fusion with adaptive attention for 3d
object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib53.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Intelligent Transportation Systems
Conference</span><span id="bib.bib53.5.3" class="ltx_text" style="font-size:90%;">, pages 3047–3054. IEEE, 2021.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Tong Xue, Abdallah El Ali, Tianyi Zhang, Gangyi Ding, and Pablo Cesar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Ceap-360vr: A continuous physiological and behavioral emotion
annotation dataset for 360 vr videos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Multimedia</span><span id="bib.bib54.4.2" class="ltx_text" style="font-size:90%;">, 25:243–255, 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.5.1" class="ltx_text" style="font-size:90%;">doi:</span><span id="bib.bib54.6.2" class="ltx_text" style="font-size:90%;color:#0000FF;">
<a target="_blank" href="http://dx.doi.org/10.1109/TMM.2021.3124080" title="" class="ltx_ref ltx_href">10.1109/TMM.2021.3124080</a></span><span id="bib.bib54.7.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Yan Yan, Yuxing Mao, and Bo Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">Second: Sparsely embedded convolutional detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Sensors</span><span id="bib.bib55.4.2" class="ltx_text" style="font-size:90%;">, 18(10):3337, 2018.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Tianwei Yin, Xingyi Zhou, and Philipp Krähenbühl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Multimodal virtual point 3d detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib56.4.2" class="ltx_text" style="font-size:90%;">,
34:16494–16507, 2021.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
Haitao Zeng, Xinhang Song, and Shuqiang Jiang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">Multi-object navigation using potential target position policy
function.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Image Processing</span><span id="bib.bib57.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.5.1" class="ltx_text" style="font-size:90%;">doi:</span><span id="bib.bib57.6.2" class="ltx_text" style="font-size:90%;color:#0000FF;">
<a target="_blank" href="http://dx.doi.org/10.1109/TIP.2023.3263110" title="" class="ltx_ref ltx_href">10.1109/TIP.2023.3263110</a></span><span id="bib.bib57.7.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
Zaiwei Zhang, Rohit Girdhar, Armand Joulin, and Ishan Misra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">Self-supervised pretraining of 3d features on any point-cloud.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib58.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib58.5.3" class="ltx_text" style="font-size:90%;">, pages 10252–10263, 2021.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:90%;">
Kun Zhao, Lingfei Ma, Yu Meng, Li Liu, Junbo Wang, José Marcato Junior,
Wesley Nunes Gonçalves, and Jonathan Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.2.1" class="ltx_text" style="font-size:90%;">3d vehicle detection using multi-level fusion from point clouds and
images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Intelligent Transportation Systems</span><span id="bib.bib59.4.2" class="ltx_text" style="font-size:90%;">,
23(9):15146–15154, 2022.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text" style="font-size:90%;">
Lichen Zhao, Jinyang Guo, Dong Xu, and Lu Sheng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.2.1" class="ltx_text" style="font-size:90%;">Transformer3d-det: Improving 3d object detection by vote refinement.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Circuits and Systems for Video Technology</span><span id="bib.bib60.4.2" class="ltx_text" style="font-size:90%;">,
31(12):4735–4746, 2021.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text" style="font-size:90%;">
Zheyuan Zhou, Liang Du, Xiaoqing Ye, Zhikang Zou, Xiao Tan, Li Zhang, Xiangyang
Xue, and Jianfeng Feng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.2.1" class="ltx_text" style="font-size:90%;">Sgm3d: stereo guided monocular 3d object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Robotics and Automation Letters</span><span id="bib.bib61.4.2" class="ltx_text" style="font-size:90%;">, 7(4):10478–10485, 2022.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text" style="font-size:90%;">
Hanqi Zhu, Jiajun Deng, Yu Zhang, Jianmin Ji, Qiuyu Mao, Houqiang Li, and
Yanyong Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.2.1" class="ltx_text" style="font-size:90%;">Vpfnet: Improving 3d object detection with virtual point based lidar
and stereo data fusion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Multimedia</span><span id="bib.bib62.4.2" class="ltx_text" style="font-size:90%;">, pages 1–14, 2022.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.5.1" class="ltx_text" style="font-size:90%;">doi:</span><span id="bib.bib62.6.2" class="ltx_text" style="font-size:90%;color:#0000FF;">
<a target="_blank" href="http://dx.doi.org/10.1109/TMM.2022.3189778" title="" class="ltx_ref ltx_href">10.1109/TMM.2022.3189778</a></span><span id="bib.bib62.7.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2310.05345" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2310.05346" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2310.05346">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2310.05346" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2310.05348" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 01:50:36 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
