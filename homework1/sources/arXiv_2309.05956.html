<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2309.05956] Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation</title><meta property="og:description" content="We propose a new paradigm to automatically generate training data with accurate labels at scale using the text-to-image synthesis frameworks (e.g., DALL-E, Stable Diffusion, etc.).
The proposed approach111This is an ex…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2309.05956">

<!--Generated on Wed Feb 28 07:00:39 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Beyond Generation: Harnessing Text to Image Models for 
<br class="ltx_break">Object Detection and Segmentation
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Yunhao Ge<sup id="id12.12.id1" class="ltx_sup">∗</sup><sup id="id13.13.id2" class="ltx_sup">⋄</sup>   
Jiashu Xu<sup id="id14.14.id3" class="ltx_sup">∗</sup><sup id="id15.15.id4" class="ltx_sup">†</sup>   
Brian Nlong Zhao<sup id="id16.16.id5" class="ltx_sup">⋄</sup>   
Neel Joshi<sup id="id17.17.id6" class="ltx_sup"><span id="id17.17.id6.1" class="ltx_text ltx_font_italic">ϰ</span></sup>   
Laurent Itti<sup id="id18.18.id7" class="ltx_sup">⋄</sup>   
Vibhav Vineet<sup id="id19.19.id8" class="ltx_sup"><span id="id19.19.id8.1" class="ltx_text ltx_font_italic">ϰ</span></sup>
<br class="ltx_break"><sup id="id20.20.id9" class="ltx_sup">⋄</sup>University of Southern California   
<sup id="id21.21.id10" class="ltx_sup">†</sup>Harvard University   
<sup id="id22.22.id11" class="ltx_sup"><span id="id22.22.id11.1" class="ltx_text ltx_font_italic">ϰ</span></sup>Microsoft Research
<br class="ltx_break"><span id="id23.23.id12" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{yunhaoge, briannlz, itti}@usc.edu<span id="id23.23.id12.1" class="ltx_text ltx_font_serif">   
</span>jxu1@g.harvard.edu<span id="id23.23.id12.2" class="ltx_text ltx_font_serif">   
</span>{neel, Vibhav.Vineet}@microsoft.com<span id="id23.23.id12.3" class="ltx_text ltx_font_serif">
</span></span>

<br class="ltx_break"><span id="id24.24.id13" class="ltx_text" style="font-size:90%;">* = Equal Contribution;    <a target="_blank" href="https://github.com/gyhandy/Text2Image-for-Detection" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/gyhandy/Text2Image-for-Detection</a>
</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id25.id1" class="ltx_p">We propose a new paradigm to automatically generate training data with accurate labels at scale using the text-to-image synthesis frameworks (e.g., DALL<span id="id25.id1.1" class="ltx_text">-</span>E, Stable Diffusion, etc.).
The proposed approach<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>This is an extension of DALL-E for detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite></span></span></span> decouples training data generation into foreground object generation, and contextually coherent background generation.
To generate foreground objects, we employ a straightforward textual template, incorporating the object class name as input prompts. This is fed into a text-to-image synthesis framework, producing various foreground images set against isolated backgrounds. A foreground-background segmentation algorithm is then used to generate foreground object masks.
To generate context images, we begin by creating language descriptions of the context. This is achieved by applying an image captioning method to a small set of images representing the desired context. These textual descriptions are then transformed into a diverse array of context images via a text-to-image synthesis framework. Subsequently, we composite these with the foreground object masks produced in the initial step, utilizing a cut-and-paste method, to formulate the training data.
We demonstrate the advantages of our approach
on five object detection and segmentation datasets, including Pascal VOC and COCO.
We found that detectors trained solely on synthetic data produced by our method achieve performance comparable to those trained on real data (Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Moreover, a combination of real and synthetic data yields even much better results. Further analysis indicates that the synthetic data distribution complements the real data distribution effectively. Additionally, we emphasize the compositional nature of our data generation approach in out-of-distribution and zero-shot data generation scenarios.
We open-source our code at <a target="_blank" href="https://github.com/gyhandy/Text2Image-for-Detection" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/gyhandy/Text2Image-for-Detection</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2309.05956/assets/x1.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="334" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.6.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.7.2" class="ltx_text" style="font-size:90%;">
(a) Comparison of DALL-E for detection pipeline and traditional human-centric pipeline (b) Using pure synthetic data from the text-to-image model (<span id="S1.F1.7.2.1" class="ltx_text ltx_font_typewriter">syn</span>) could lead on-par performance with using all real data (<span id="S1.F1.7.2.2" class="ltx_text ltx_font_typewriter">real</span>), mixing real and synthetic (<span id="S1.F1.7.2.3" class="ltx_text ltx_font_typewriter">syn + real</span>) gives strong performance gains (+22.5 mAP).
</span></figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Training modern deep learning models necessitates large labeled datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>, <a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>, <a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>]</cite>. Yet, acquiring such datasets poses significant challenges due to the high costs and extensive human effort involved, making the process both expensive and time-intensive.
This leads us to a pivotal question: <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">Is it possible to efficiently generate large-scale labeled data that also delivers high accuracy for subsequent tasks?</span>
We believe that any such approach should satisfy these qualities (<a href="#S1.T1" title="In 1 Introduction ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>): minimal human involvement, automatic generalization of the images for any new classes and environments, scalable, generation of high quality and diverse set of images, explainable, compositional, and privacy-preserving.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To this end, synthetic techniques could be used as promising avenues for generating labeled data for training computer vision models.
One popular approach is to use computer graphics to generate data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">55</span></a>, <a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">60</span></a>, <a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">63</span></a>]</cite>.
Despite their potential, these strategies predominantly hinge on acquiring 3D models of both objects and scenes. This often necessitates specialized skills, such as 3D modeling expertise, which inherently curtails the scalability of such methods.
Another viable approach is using NeRF based rendering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>, <a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>.
These strategies typically entail retraining models to accommodate new object classes. As such, they lack the flexibility to effortlessly scale across a vast array of object classifications.
Lastly, a third approach is
object cut and paste <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>,
which pastes foregrounds onto the backgrounds.
However, such an approach demands a diverse and reliable source of foreground object masks as well as coherent backgrounds, which can be challenging to procure.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Recently, there has been a revolutionary advancement in text-to-image (T2I) synthesis models such as DALL-E <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>, Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a>]</cite>, RU-DALLE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>,
CogView <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>,
Imagen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite>,
MUSE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>, and eDiff-I <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>.
Not only are these models capable of crafting high-quality images, but they also excel in illustrating intricate scenes, understanding semantics, and capturing the compositional nuances of the real world.
Therefore,
they might act as a natural bridge between humans and image synthesis.
Nevertheless, despite their capability to function as synthetic data creators, these models lack the ability to produce region-level bounding boxes or pixel-level segmentation. Consequently, they remain unsuitable for downstream tasks such as object detection or instance segmentation.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<div id="S1.T1.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:238.5pt;height:60.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-94.9pt,23.9pt) scale(0.556728185197752,0.556728185197752) ;">
<table id="S1.T1.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T1.2.1.1.1" class="ltx_tr">
<th id="S1.T1.2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Method</th>
<th id="S1.T1.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Quality</th>
<th id="S1.T1.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Less Human</th>
<th id="S1.T1.2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Adapt</th>
<th id="S1.T1.2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Scalable</th>
<th id="S1.T1.2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Explain.</th>
<th id="S1.T1.2.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Privacy</th>
<th id="S1.T1.2.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Comp.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T1.2.1.2.1" class="ltx_tr">
<td id="S1.T1.2.1.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Human capture</td>
<td id="S1.T1.2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.1.2.1.2.1" class="ltx_text" style="color:#0000FF;">✓</span></td>
<td id="S1.T1.2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.1.2.1.3.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S1.T1.2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.1.2.1.4.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S1.T1.2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.1.2.1.5.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S1.T1.2.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.1.2.1.6.1" class="ltx_text" style="color:#0000FF;">✓</span></td>
<td id="S1.T1.2.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.1.2.1.7.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S1.T1.2.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.2.1.2.1.8.1" class="ltx_text" style="color:#0000FF;">✓</span></td>
</tr>
<tr id="S1.T1.2.1.3.2" class="ltx_tr">
<td id="S1.T1.2.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r">Web image</td>
<td id="S1.T1.2.1.3.2.2" class="ltx_td ltx_align_center"><span id="S1.T1.2.1.3.2.2.1" class="ltx_text" style="color:#0000FF;">✓</span></td>
<td id="S1.T1.2.1.3.2.3" class="ltx_td ltx_align_center"><span id="S1.T1.2.1.3.2.3.1" class="ltx_text" style="color:#0000FF;">✓</span></td>
<td id="S1.T1.2.1.3.2.4" class="ltx_td ltx_align_center"><span id="S1.T1.2.1.3.2.4.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S1.T1.2.1.3.2.5" class="ltx_td ltx_align_center"><span id="S1.T1.2.1.3.2.5.1" class="ltx_text" style="color:#0000FF;">✓</span></td>
<td id="S1.T1.2.1.3.2.6" class="ltx_td ltx_align_center"><span id="S1.T1.2.1.3.2.6.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S1.T1.2.1.3.2.7" class="ltx_td ltx_align_center"><span id="S1.T1.2.1.3.2.7.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S1.T1.2.1.3.2.8" class="ltx_td ltx_align_center"><span id="S1.T1.2.1.3.2.8.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
</tr>
<tr id="S1.T1.2.1.4.3" class="ltx_tr">
<td id="S1.T1.2.1.4.3.1" class="ltx_td ltx_align_center ltx_border_r">Public dataset</td>
<td id="S1.T1.2.1.4.3.2" class="ltx_td ltx_align_center"><span id="S1.T1.2.1.4.3.2.1" class="ltx_text" style="color:#0000FF;">✓</span></td>
<td id="S1.T1.2.1.4.3.3" class="ltx_td ltx_align_center"><span id="S1.T1.2.1.4.3.3.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S1.T1.2.1.4.3.4" class="ltx_td ltx_align_center"><span id="S1.T1.2.1.4.3.4.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S1.T1.2.1.4.3.5" class="ltx_td ltx_align_center"><span id="S1.T1.2.1.4.3.5.1" class="ltx_text" style="color:#0000FF;">✓</span></td>
<td id="S1.T1.2.1.4.3.6" class="ltx_td ltx_align_center"><span id="S1.T1.2.1.4.3.6.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S1.T1.2.1.4.3.7" class="ltx_td ltx_align_center"><span id="S1.T1.2.1.4.3.7.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S1.T1.2.1.4.3.8" class="ltx_td ltx_align_center"><span id="S1.T1.2.1.4.3.8.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
</tr>
<tr id="S1.T1.2.1.5.4" class="ltx_tr">
<td id="S1.T1.2.1.5.4.1" class="ltx_td ltx_align_center ltx_border_r">Generative models</td>
<td id="S1.T1.2.1.5.4.2" class="ltx_td ltx_align_center"><span id="S1.T1.2.1.5.4.2.1" class="ltx_text" style="color:#0000FF;">✓</span></td>
<td id="S1.T1.2.1.5.4.3" class="ltx_td ltx_align_center"><span id="S1.T1.2.1.5.4.3.1" class="ltx_text" style="color:#0000FF;">✓</span></td>
<td id="S1.T1.2.1.5.4.4" class="ltx_td ltx_align_center"><span id="S1.T1.2.1.5.4.4.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S1.T1.2.1.5.4.5" class="ltx_td ltx_align_center"><span id="S1.T1.2.1.5.4.5.1" class="ltx_text" style="color:#0000FF;">✓</span></td>
<td id="S1.T1.2.1.5.4.6" class="ltx_td ltx_align_center"><span id="S1.T1.2.1.5.4.6.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
<td id="S1.T1.2.1.5.4.7" class="ltx_td ltx_align_center"><span id="S1.T1.2.1.5.4.7.1" class="ltx_text" style="color:#0000FF;">✓</span></td>
<td id="S1.T1.2.1.5.4.8" class="ltx_td ltx_align_center"><span id="S1.T1.2.1.5.4.8.1" class="ltx_text" style="color:#FF0000;">✗</span></td>
</tr>
<tr id="S1.T1.2.1.6.5" class="ltx_tr">
<td id="S1.T1.2.1.6.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Ours</td>
<td id="S1.T1.2.1.6.5.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S1.T1.2.1.6.5.2.1" class="ltx_text" style="color:#0000FF;">✓</span></td>
<td id="S1.T1.2.1.6.5.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S1.T1.2.1.6.5.3.1" class="ltx_text" style="color:#0000FF;">✓</span></td>
<td id="S1.T1.2.1.6.5.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S1.T1.2.1.6.5.4.1" class="ltx_text" style="color:#0000FF;">✓</span></td>
<td id="S1.T1.2.1.6.5.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S1.T1.2.1.6.5.5.1" class="ltx_text" style="color:#0000FF;">✓</span></td>
<td id="S1.T1.2.1.6.5.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S1.T1.2.1.6.5.6.1" class="ltx_text" style="color:#0000FF;">✓</span></td>
<td id="S1.T1.2.1.6.5.7" class="ltx_td ltx_align_center ltx_border_b"><span id="S1.T1.2.1.6.5.7.1" class="ltx_text" style="color:#0000FF;">✓</span></td>
<td id="S1.T1.2.1.6.5.8" class="ltx_td ltx_align_center ltx_border_b"><span id="S1.T1.2.1.6.5.8.1" class="ltx_text" style="color:#0000FF;">✓</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S1.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S1.T1.4.2" class="ltx_text" style="font-size:90%;">
Desired quality of context generation method: images should be high quality and diverse, less human involvement, generalization of the images for any new environment, scalable, explainable, privacy-preserving, and compositional.
</span></figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this work, we explore if <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">these T2I models can be harnessed to generate expansive training datasets equipped with accurate labels, specifically tailored for tasks such as object detection and instance segmentation.</span>
Moreover, we primarily focus on <span id="S1.p4.1.2" class="ltx_text ltx_font_italic">low-resource regime</span> wherein downstream users have access to a limited training dataset but demand a high-performing, robust, and broadly applicable object detector or instance segmentation.
We opt for this context because acquiring on-demand human annotations tailored to the specific requirements of each individual downstream user can be prohibitively expensive, making the role of synthetic datasets even more crucial.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We propose a novel data generation paradigm that use T2I to produce large-scale high-quality and contextually-coherent pseudo-labeled datasets with minimal human involvement.
Essentially, we retrieve pertinent information to curate a specific dataset aimed at enhancing
downstream discriminative object detectors and instance segmentation, from the general knowledge base of generative T2I.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Our pipeline is bifurcated into two components: foreground object mask generation and contextual background generation (<a href="#S3.F2" title="In 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>).
For zero-shot scenarios where only class names are available,
we employ a set of simple templates for each of the interested objects, such as “A photo of dog in a white background” to elicit dog-related images from the T2I.
Given that our templates are devised to position objects against easily-segmentable backdrops, straightforward background-foreground segmentation techniques can be employed to extract precise foreground object masks.
For generating backgrounds, we employ analogous templates to craft clear backgrounds devoid of any target objects.
After CLIP filtering,
foreground object masks are pasted onto the backgrounds via cut and paste <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>, resulting in the final pseudo-labeled dataset.
For few-shot scenarios where additional real training images are available,
we specifically produce contextually-aligned coherent backgrounds from these shots as
background context plays a pivotal role in learning strong object recognition models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite>.
For example, placing airplanes and boats in their natural context helped to improve accuracy, <em id="S1.p6.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p6.1.2" class="ltx_text"></span> airplanes are generally found in the sky and boats are on the water.
We caption each shot, extract contextual keywords (<em id="S1.p6.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p6.1.4" class="ltx_text"></span> “grass field” from “A dog lying on grass field”) from these captions, contextually augment those captions (<em id="S1.p6.1.5" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p6.1.6" class="ltx_text"></span> “A real photo of forest”), and produce coherent images by feeding T2I those augmented captions.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The proposed pipeline satisfies all desired properties of data generation (<a href="#S1.T1" title="In 1 Introduction ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>).
It minimizes the requirement for human intervention in both the initial provision of training data and the subsequent steps of synthetic data generation.
T2I enables privacy-preserving and scalable generation.
We obtain explainable and compositional data generation by operating within the language domain before feeding to T2I.
Adding or removing objects or settings can be easily done. For example, a description as “an environment with a table” can be easily modified to a kitchen environment by utilizing the compositional properties of language as “a kitchen environment with a table.”
Furthermore, even when out-of-distribution training instances are given, by simple language edit, we can easily force generated synthetic dataset to be better matched with the test distribution (<em id="S1.p7.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p7.1.2" class="ltx_text"></span> by substituting “cartoon kitchen” with “real kitchen” to bridge the gap between cartoon training set and real test set).</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">Our main contributions are three-fold:
(1) We propose a language-driven compositional synthetic dataset generation that automatically generate large-scale high-quality foreground objects and contextually coherent backgrounds.
(2) We demonstrate strong performance gains across three downstream tasks and five widely-used benchmarks, under various low-resource regmines including 0, 1, and 10-shot settings.
(3) We highlight that the compositionality in our synthetic data generation results in a closer alignment with the test distribution, even when dealing with out-of-distribution training instances.
To the best of our knowledge, this is the first work to use vision and language models for generating object detection and segmentation datasets.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related works</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Text-to-Image Synthesis Models.</h5>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">T2I approaches like DALL-E <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>, RU-DALLE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>, Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a>]</cite>, CogView <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> have revolutionized high-quality image generation.
These approaches leverage benefits of using large transformer models trained using large vision and text data. Though they can generate high quality images of real world scenes, they are not able to generate ground truth labels for objects. For example, they are not able to provide bounding box and per-pixel annotation for objects. In our work, we propose an automatic approach to generate high quality images with ground truth bounding box and per-pixel labels from T2I models.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Synthetic Data Generation.</h5>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">A series of works on using synthetic data for training computer vision problems have been proposed. Some of them include using graphics pipeline or computer games to generate high quality labelled data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">55</span></a>, <a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">54</span></a>, <a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a>, <a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>, <a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">64</span></a>, <a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>. Generally using graphics pipeline requires having 3D models of both objects and environment, that may limit their scalability. Some of them use generative models (e.g., GAN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>, <a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> or zero-shot synthesis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> to augment datasets and remove bias. However, they need a relatively large initial dataset to train the model, and not easy to generalize to new domains.</p>
</div>
<div id="S2.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p2.1" class="ltx_p">The idea of pasting foreground objects on background images has emerged as a easy and scalable approach for large scale data generation.
The idea has been used to solve vision problems like object instance segmentation tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>, object detection and pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">61</span></a>, <a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>, <a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">49</span></a>, <a href="#bib.bib62" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">62</span></a>, <a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">65</span></a>]</cite>, and more <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>, <a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">70</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>.
These approaches generally require accurate foreground object masks. This limits their scalability. While we utilize a cut-and-paste approach, in contrast to previous works in this space, our work can generate foreground object masks for any new object class.
</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Language for object recognition.</h5>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">Vision-language based models have been developed for image captioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">67</span></a>, <a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>, visual question answering tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>, <a href="#bib.bib66" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">66</span></a>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>, <a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite> and others <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>, <a href="#bib.bib69" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">69</span></a>, <a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite>. In recent years, vision-language based models have been developed for self-supervised training.
The recent CLIP appraoch <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite> showed how training a model on large image-text pairs can generalize to several image classification datasets where current image based models performed very poorly. Such vision-language models havec been used to solve other tasks like object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>, <a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>, <a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite> and semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite>. These works demonstrate benefits of using language in solving vision tasks. However, these methods can not be used generate new data for any new tasks or environment settings.
Large T2I synthesis models can be used to generate new data for a new task. A recent concurrent work X-Paste <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">72</span></a>]</cite> has used Stable Diffusion to solve object detection.
We are motivated by generation quality of these text to image generation methods in this work.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2309.05956/assets/x2.png" id="S3.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="180" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.9.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.10.2" class="ltx_text" style="font-size:90%;">
(a) <span id="S3.F2.10.2.1" class="ltx_text ltx_font_bold">Foreground generation:</span> (top row, <a href="#S3.SS1" title="3.1 Zero-shot Foreground Synthesis ‣ 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>) verbalizes class name into templates understandable by T2I models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>, <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a>]</cite>, which synthesize desired foreground images with easy-to-separate backgrounds. Off-the-shelf foreground/background segmentation methods are then used to extract foreground segments from foreground images.
(b) <span id="S3.F2.10.2.2" class="ltx_text ltx_font_bold">Background generation:</span> (bottom row, <a href="#S3.SS2" title="3.2 Language-driven Context Synthesis ‣ 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>)
an image captioning method (<em id="S3.F2.10.2.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.F2.10.2.4" class="ltx_text"></span> SCST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite>) captions user-provided images (CDIs).
Context words (<em id="S3.F2.10.2.5" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.F2.10.2.6" class="ltx_text"></span> “grass field”) are extracted and the augmented caption is feed into T2I to generate background images.
(c) CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite> is used (<a href="#S3.SS3" title="3.3 CLIP Sanity Check ‣ 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>) to maintain the quality of both foregrounds and backgrounds, as well as ensure that the generated images do not have unwanted class.
(d) Finally, we composite (<a href="#S3.SS4" title="3.4 Cut-paste Composition ‣ 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.4</span></a>) the foreground segments and background images to obtain synthetic images with accurate labels.
</span></figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2309.05956/assets/x3.png" id="S3.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="202" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.5.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.6.2" class="ltx_text" style="font-size:90%;">
When CDIs can not perfectly describe the real test scenario, the compositional property of language can help to correct the context description.
For instance, if the initial description contains noisy information “man and a woman”, we can remove the noisy information to generate a congruent context description.
Images with a <span id="S3.F3.6.2.1" class="ltx_text" style="color:#FF0000;">red</span> frame show the generated image without language intervention and the <span id="S3.F3.6.2.2" class="ltx_text" style="color:#00FF00;">green</span> frame shows the images after the intervention.
cccccbkrkkdllkdhegjbknulebflteevrrhtdjuefjeh
</span></figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We layout our method in <a href="#S3.F2" title="In 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>.
This work aims to enhance object detection and instance segmentation models by efficiently generating a diverse and extensive collection of pseudo-labeled synthetic data using text-to-image models.
One key observation is that each image can be divided into backgrounds and foregrounds.
In this work, we propose to generate synthetic foreground objects (with mask
extraction) (<a href="#S3.SS1" title="3.1 Zero-shot Foreground Synthesis ‣ 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>) and contextually coherent backgrounds (<a href="#S3.SS2" title="3.2 Language-driven Context Synthesis ‣ 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>) separately.
Subsequently, after CLIP filtering on both (<a href="#S3.SS3" title="3.3 CLIP Sanity Check ‣ 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>), the synthesized foregrounds are then composited onto the synthesized backgrounds via cut-paste <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> (<a href="#S3.SS4" title="3.4 Cut-paste Composition ‣ 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.4</span></a>).
Examples of synthetic datasets can be viewed in <a href="#S4.F8" title="In Mixing real data with synthetic data. ‣ 4.2 More Baselines and Ablation Studies ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a>, <a href="#S4.F9" title="In 4.3 Synthetic data distribution complements the real data distribution. ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">9</span></a> and <a href="#Sx1.F13" title="In Appendix ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">13</span></a>.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">We utilize off-the-shelf text-to-image
generative models (T2I) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>, <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a>, <a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite> to generate both the foreground masks and contextual backgrounds.
Firstly, T2I efficiently compresses web-scale image-text data, ensuring both portability and scalability. The text-to-image generation model can produce an endless array of high-quality images, with our method offering a controllable generation process. Our experiments and subsequent analysis confirm that the synthetic data distribution effectively complements the real data distribution, as detailed in <a href="#S4.SS3" title="4.3 Synthetic data distribution complements the real data distribution. ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.3</span></a>.
Also, the generative model can create novel scenarios that were not present in the training data (<a href="#S4.SS4" title="4.4 Generalization to more tasks ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.4</span></a>, <a href="#S4.SS5" title="4.5 Compositionality in Synthetic Dataset ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.5</span></a>).
Furthermore, language-based generation facilitates compositionality (<a href="#S4.SS5" title="4.5 Compositionality in Synthetic Dataset ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.5</span></a>).
Lastly, the synthetic nature of the data generation procedure ensures privacy preservation.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">We underscore that our method, thanks to the capabilities of T2I in generating diverse images, particularly excels in low-resource regimes where ground truth data is limited.
Following the terminology of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a>, <a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite>,
in this work, we focus mainly on <span id="S3.p3.1.1" class="ltx_text ltx_font_bold">zero-shot</span> (where no ground-truth data is provided but a list of desired objects) and <span id="S3.p3.1.2" class="ltx_text ltx_font_bold">few-shot</span> (where one or ten images per class are given).
We also included experiments to show our methods improve using full-set as well (<a href="#S4.T4" title="In 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.2.1.1" class="ltx_tr">
<td id="S3.T2.2.1.1.1" class="ltx_td ltx_align_center ltx_border_t">A photo of &lt;object&gt;</td>
<td id="S3.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_border_t">A realistic photo of &lt;object&gt;</td>
<td id="S3.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_border_t">A photo of &lt;object&gt; in pure background</td>
</tr>
<tr id="S3.T2.2.2.2" class="ltx_tr">
<td id="S3.T2.2.2.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">&lt;object&gt; in a white background</td>
<td id="S3.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">&lt;object&gt; without background</td>
<td id="S3.T2.2.2.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">&lt;object&gt; isolated on white background</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S3.T2.4.2" class="ltx_text" style="font-size:90%;">Six manually designed templates for generating foreground images zero-shot. Here &lt;object&gt; will be replaced by label names such as “bus”. The design philosophy is to put objects in a clean background for easy foreground extraction.</span></figcaption>
</figure>
<figure id="S3.T3" class="ltx_table">
<table id="S3.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.2.1.1" class="ltx_tr">
<th id="S3.T3.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">empty living room</th>
<th id="S3.T3.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">empty kitch</th>
<th id="S3.T3.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">blue sky</th>
<th id="S3.T3.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">empty city street, color</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.2.2.1" class="ltx_tr">
<td id="S3.T3.2.2.1.1" class="ltx_td ltx_align_center ltx_border_t">empty city road, color</td>
<td id="S3.T3.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">empty lake</td>
<td id="S3.T3.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t">empty sea</td>
<td id="S3.T3.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t">railway without train</td>
</tr>
<tr id="S3.T3.2.3.2" class="ltx_tr">
<td id="S3.T3.2.3.2.1" class="ltx_td ltx_align_center ltx_border_t">empty railway, color</td>
<td id="S3.T3.2.3.2.2" class="ltx_td ltx_align_center ltx_border_t">trees</td>
<td id="S3.T3.2.3.2.3" class="ltx_td ltx_align_center ltx_border_t">forest</td>
<td id="S3.T3.2.3.2.4" class="ltx_td ltx_align_center ltx_border_t">empty street, colored</td>
</tr>
<tr id="S3.T3.2.4.3" class="ltx_tr">
<td id="S3.T3.2.4.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">farms</td>
<td id="S3.T3.2.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">nature</td>
<td id="S3.T3.2.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">empty farm</td>
<td id="S3.T3.2.4.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">stable</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T3.3.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S3.T3.4.2" class="ltx_text" style="font-size:90%;">Sixteen handcraft templates for generating coherent background images zero-shot. The full template is “A real photo of &lt;context&gt;” where &lt;context&gt; is substituted with one of the above 16 places. The design philosophy is to create natural images but without any interested objects (thus “empty”) since we would not have segmentation labels for those objects if they are generated.</span></figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Zero-shot Foreground Synthesis</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In object detection or segmentation, the quality of foreground objects significantly influences the performance of downstream tasks
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>.
This poses a significant challenge in low-resource scenarios, as training a model that can effectively generalize with limited exposure to seen foregrounds is inherently difficult. v
To address this, we aim to harness the available prior knowledge to generate a vast and diverse collection of high-quality foregrounds<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Samples of the synthesized images can be found in <a href="#S3.F2" title="In 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#S4.F8" title="In Mixing real data with synthetic data. ‣ 4.2 More Baselines and Ablation Studies ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a>.</span></span></span>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Specifically, we manually design six fixed prompt templates
(<a href="#S3.T2" title="In 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>), where &lt;object&gt; is substituted with the class name like “dog.”
We then feed those verbalized templates into the T2I, which generates high-quality iconic object images.
The templates are designed to elicit images where interested objects are centered on a simple isolated background, enabling straightforward extraction of the foreground object masks
using an unsupervised segmentation method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite>, as analyzed in <a href="#S4.SS3" title="4.3 Synthetic data distribution complements the real data distribution. ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.3</span></a>. Examples of generated foregrounds and extracted masks can be viewed in <a href="#S4.F8" title="In Mixing real data with synthetic data. ‣ 4.2 More Baselines and Ablation Studies ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a> and <a href="#Sx1.F14" title="In Appendix ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">14</span></a>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Language-driven Context Synthesis</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Merely having high-quality foregrounds is insufficient for training a robust downstream model. It is equally crucial to position those foregrounds within suitable in-context backgrounds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>, <a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>, <a href="#bib.bib71" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">71</span></a>, <a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>, <a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">70</span></a>]</cite>.
In our experiments (<a href="#S4.SS2" title="4.2 More Baselines and Ablation Studies ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>) we discovered that the choice of backgrounds can significantly impact performance, with different backgrounds having the potential to degrade results dramatically. Thus, we propose to utilize T2I to generate coherent backgrounds as relevant “context” for downstream models.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Other than creating contextually coherent backgrounds, one additional benefit is that, as contexts are described in natural language, compositional generation is possibly by editing the descriptions <em id="S3.SS2.p2.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.SS2.p2.1.2" class="ltx_text"></span> adding or removing a keyword.
For example, in <a href="#S3.F3" title="In 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a> the word “kitchen” can be added or “people” can be removed to align more closely with the test distribution.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Zero-shot scenario</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">Similar to <a href="#S3.SS1" title="3.1 Zero-shot Foreground Synthesis ‣ 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>, for the zero-shot scenario, we design sixteen background templates
(<a href="#S3.T3" title="In 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>)
intended to generate backgrounds without interested objects, avoiding
“misplaced” objects with unknown object masks.
The selection of background templates is primarily based on a cursory examination of various images from the training set, Pascal VOC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> and MS COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite>, in our case.
However, it is important to highlight that this process is minimal, and only high-level background descriptions are required.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Few-shot scenario</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">In a more relaxed few-shot setting, a few exemplars, dubbed <em id="S3.SS2.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">Context Description Images (CDI)</em>, are given, all sampled from a distribution that is contextually close to the test distribution.
CDIs provide valuable contextual cues.
We term this setting “few-shot” but note that provided CDIs <span id="S3.SS2.SSS2.p1.1.2" class="ltx_text ltx_font_bold">do not have to be sampled from the training distribution</span> (<a href="#S3.F3" title="In 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a>). <em id="S3.SS2.SSS2.p1.1.3" class="ltx_emph ltx_font_italic">E.g</em>.<span id="S3.SS2.SSS2.p1.1.4" class="ltx_text"></span> if the test distribution includes a kitchen environment, the small set of kitchen images can be taken from any public dataset or web images (<a href="#S4.SS4" title="4.4 Generalization to more tasks ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.4</span></a>, <a href="#S4.SS5" title="4.5 Compositionality in Synthetic Dataset ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.5</span></a>).
Our goal is to mine beneficial context to encourage the synthesized background distribution to more closely match the test distribution by imposing the context constraint.
Lastly, we note that our method significantly reduces human labor, since our method requires a minimal number of CDI.
In fact, our method works sufficiently well using <span id="S3.SS2.SSS2.p1.1.5" class="ltx_text ltx_font_italic">as little as 1 CDI</span> (<a href="#S4.F10" title="In Object Instance Detection. ‣ 4.4 Generalization to more tasks ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">10</span></a>, <a href="#S4.SS5" title="4.5 Compositionality in Synthetic Dataset ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.5</span></a>).</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS2.p2.1" class="ltx_p"><span id="S3.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Context Description.</span>
We first describe the context information in natural language via image captioning.
We leverage self-critique sequence training (SCST) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite> to generate a set of diverse textual captions for input CDIs.
Yet we note that our method is agnostic to the image captioning method and any other methods <em id="S3.SS2.SSS2.p2.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.SS2.SSS2.p2.1.3" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>, <a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite> can also be applied.</p>
</div>
<div id="S3.SS2.SSS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS2.p3.1" class="ltx_p"><span id="S3.SS2.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Context Extraction and Augmentation.</span>
Our focus is primarily on the background rather than the objects themselves, as the generated objects do not have labels.
Therefore we need to extract background contexts out of the captions that might contain objects.
We built a simple parser that extracts context words (<em id="S3.SS2.SSS2.p3.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.SS2.SSS2.p3.1.3" class="ltx_text"></span>, “grass field”) or removes unwanted objects (<em id="S3.SS2.SSS2.p3.1.4" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.SS2.SSS2.p3.1.5" class="ltx_text"></span> detecting nouns such as “dogs”).
These cleansed captions are then transformed into a few augmented contexts (<em id="S3.SS2.SSS2.p3.1.6" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.SS2.SSS2.p3.1.7" class="ltx_text"></span> “A real photo of grass field” and “A real photo of forest”) via simple heuristics.
While it is possible to automate the entire context extraction process using large language models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>, <a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>]</cite>, we leave that as future work.</p>
</div>
<div id="S3.SS2.SSS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS2.p4.1" class="ltx_p"><span id="S3.SS2.SSS2.p4.1.1" class="ltx_text ltx_font_bold">Context-guided Synthesis.</span>
We feed augmented contexts into the T2I to produce a diverse set of contextually coherent backgrounds.
As these augmented contexts are derived from the CDIs, they are more contextual-aligned with the test distribution; also they should contain no objects since these are removed in the extraction process.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>CLIP Sanity Check</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Due to the limitations of T2I, we observed that it occasionally generated irrelevant or nonsensical images that strongly deviated from the input text.
Moreover,
T2I method learns associations between the context and objects,
leading to instances where horses would frequently appear in the context of a “stable,” even when the text explicitly states “An empty stable.”
Thus, in <a href="#S4.SS2" title="4.2 More Baselines and Ablation Studies ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.2</span></a> we find it
indispensable to post-process generated foregrounds and backgrounds via CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite> filtering.
Specifically, we use CLIP to rank images via two rules: images are semantically faithful to the input text and semantically dissimilar to any interest classes.
This step ensures the generations align more closely with desired semantics.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Cut-paste Composition</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">To create the final pseudo-labeled training data, we composite foreground object masks (<a href="#S3.SS1" title="3.1 Zero-shot Foreground Synthesis ‣ 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>) onto the backgrounds (<a href="#S3.SS2" title="3.2 Language-driven Context Synthesis ‣ 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>) using cut-paste <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>.
At each step, a set of four foreground object masks is selected and pasted into a sampled background image, and such procedure is repeated until all foreground masks are pasted.
The foreground mask, after random 2D augmentation such as rotation and scaling, is pasted on a random location in the image.
In addition, following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>, we apply a Gaussian blur on the object boundary with blur kernel <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mi id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">\sigma</annotation></semantics></math> to blend pasted foregrounds.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.2.3.1" class="ltx_tr">
<td id="S4.T4.2.3.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T4.2.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">#CDI</span></td>
<td id="S4.T4.2.3.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T4.2.3.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Method</span></td>
<td id="S4.T4.2.3.1.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T4.2.3.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Foreground</span></td>
<td id="S4.T4.2.3.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt"><span id="S4.T4.2.3.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Background</span></td>
<td id="S4.T4.2.3.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.2.3.1.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">mAP@50</span></td>
<td id="S4.T4.2.3.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.2.3.1.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">mAP</span></td>
</tr>
<tr id="S4.T4.2.4.2" class="ltx_tr">
<td id="S4.T4.2.4.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.4.2.1.1" class="ltx_text" style="font-size:90%;">1,464 (Fullset)</span></td>
<td id="S4.T4.2.4.2.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S4.T4.2.4.2.2.1" class="ltx_text" style="font-size:90%;">Pure Real </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T4.2.4.2.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a><span id="S4.T4.2.4.2.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T4.2.4.2.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T4.2.4.2.3.1" class="ltx_text" style="font-size:90%;">Real</span></td>
<td id="S4.T4.2.4.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T4.2.4.2.4.1" class="ltx_text" style="font-size:90%;">Real</span></td>
<td id="S4.T4.2.4.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.4.2.5.1" class="ltx_text" style="font-size:90%;">45.50</span></td>
<td id="S4.T4.2.4.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.4.2.6.1" class="ltx_text" style="font-size:90%;">17.00</span></td>
</tr>
<tr id="S4.T4.2.5.3" class="ltx_tr">
<td id="S4.T4.2.5.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.2.5.3.1.1" class="ltx_text" style="font-size:90%;">0 (0 shot)</span></td>
<td id="S4.T4.2.5.3.2" class="ltx_td ltx_align_left ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T4.2.5.3.2.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Pure Syn</span></td>
<td id="S4.T4.2.5.3.3" class="ltx_td ltx_align_left ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T4.2.5.3.3.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn</span></td>
<td id="S4.T4.2.5.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T4.2.5.3.4.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn</span></td>
<td id="S4.T4.2.5.3.5" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T4.2.5.3.5.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">43.24</span></td>
<td id="S4.T4.2.5.3.6" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T4.2.5.3.6.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">19.78</span></td>
</tr>
<tr id="S4.T4.1.1" class="ltx_tr">
<td id="S4.T4.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="5"><span id="S4.T4.1.1.1.1" class="ltx_text" style="font-size:90%;">
<span id="S4.T4.1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:54.1pt;height:9.1pt;vertical-align:-2.3pt;"><span class="ltx_transformed_inner" style="width:54.1pt;transform:translate(0pt,3.38pt) rotate(-0deg) ;">
<span id="S4.T4.1.1.1.1.1.1" class="ltx_p"><math id="S4.T4.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="20\cdot 1" display="inline"><semantics id="S4.T4.1.1.1.1.1.1.m1.1a"><mrow id="S4.T4.1.1.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.1.1.m1.1.1.cmml"><mn id="S4.T4.1.1.1.1.1.1.m1.1.1.2" xref="S4.T4.1.1.1.1.1.1.m1.1.1.2.cmml">20</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T4.1.1.1.1.1.1.m1.1.1.1" xref="S4.T4.1.1.1.1.1.1.m1.1.1.1.cmml">⋅</mo><mn id="S4.T4.1.1.1.1.1.1.m1.1.1.3" xref="S4.T4.1.1.1.1.1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.1.1.m1.1b"><apply id="S4.T4.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.1.1.m1.1.1"><ci id="S4.T4.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T4.1.1.1.1.1.1.m1.1.1.1">⋅</ci><cn type="integer" id="S4.T4.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T4.1.1.1.1.1.1.m1.1.1.2">20</cn><cn type="integer" id="S4.T4.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T4.1.1.1.1.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.1.1.m1.1c">20\cdot 1</annotation></semantics></math> (1 shot)</span>
</span></span></span></td>
<td id="S4.T4.1.1.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S4.T4.1.1.2.1" class="ltx_text" style="font-size:90%;">Pure Real </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T4.1.1.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a><span id="S4.T4.1.1.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T4.1.1.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T4.1.1.3.1" class="ltx_text" style="font-size:90%;">Real</span></td>
<td id="S4.T4.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T4.1.1.4.1" class="ltx_text" style="font-size:90%;">Real</span></td>
<td id="S4.T4.1.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.5.1" class="ltx_text" style="font-size:90%;">0.14</span></td>
<td id="S4.T4.1.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.1.6.1" class="ltx_text" style="font-size:90%;">0.04</span></td>
</tr>
<tr id="S4.T4.2.6.4" class="ltx_tr">
<td id="S4.T4.2.6.4.1" class="ltx_td ltx_align_left">
<span id="S4.T4.2.6.4.1.1" class="ltx_text" style="font-size:90%;">+ cut paste </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T4.2.6.4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a><span id="S4.T4.2.6.4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T4.2.6.4.2" class="ltx_td ltx_align_left"><span id="S4.T4.2.6.4.2.1" class="ltx_text" style="font-size:90%;">Real</span></td>
<td id="S4.T4.2.6.4.3" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T4.2.6.4.3.1" class="ltx_text" style="font-size:90%;">Real</span></td>
<td id="S4.T4.2.6.4.4" class="ltx_td ltx_align_center"><span id="S4.T4.2.6.4.4.1" class="ltx_text" style="font-size:90%;">6.03</span></td>
<td id="S4.T4.2.6.4.5" class="ltx_td ltx_align_center"><span id="S4.T4.2.6.4.5.1" class="ltx_text" style="font-size:90%;">2.07</span></td>
</tr>
<tr id="S4.T4.2.7.5" class="ltx_tr">
<td id="S4.T4.2.7.5.1" class="ltx_td ltx_align_left" style="background-color:#E6E6E6;"><span id="S4.T4.2.7.5.1.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn Fg</span></td>
<td id="S4.T4.2.7.5.2" class="ltx_td ltx_align_left" style="background-color:#E6E6E6;"><span id="S4.T4.2.7.5.2.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn + Real</span></td>
<td id="S4.T4.2.7.5.3" class="ltx_td ltx_align_left ltx_border_r" style="background-color:#E6E6E6;"><span id="S4.T4.2.7.5.3.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Real</span></td>
<td id="S4.T4.2.7.5.4" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T4.2.7.5.4.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">37.97</span></td>
<td id="S4.T4.2.7.5.5" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T4.2.7.5.5.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">17.53</span></td>
</tr>
<tr id="S4.T4.2.8.6" class="ltx_tr">
<td id="S4.T4.2.8.6.1" class="ltx_td ltx_align_left" style="background-color:#E6E6E6;"><span id="S4.T4.2.8.6.1.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Pure Syn</span></td>
<td id="S4.T4.2.8.6.2" class="ltx_td ltx_align_left" style="background-color:#E6E6E6;"><span id="S4.T4.2.8.6.2.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn</span></td>
<td id="S4.T4.2.8.6.3" class="ltx_td ltx_align_left ltx_border_r" style="background-color:#E6E6E6;"><span id="S4.T4.2.8.6.3.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn</span></td>
<td id="S4.T4.2.8.6.4" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T4.2.8.6.4.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">44.24</span></td>
<td id="S4.T4.2.8.6.5" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T4.2.8.6.5.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">20.63</span></td>
</tr>
<tr id="S4.T4.2.9.7" class="ltx_tr">
<td id="S4.T4.2.9.7.1" class="ltx_td ltx_align_left" style="background-color:#E6E6E6;"><span id="S4.T4.2.9.7.1.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn + real</span></td>
<td id="S4.T4.2.9.7.2" class="ltx_td ltx_align_left" style="background-color:#E6E6E6;"><span id="S4.T4.2.9.7.2.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn + Real</span></td>
<td id="S4.T4.2.9.7.3" class="ltx_td ltx_align_left ltx_border_r" style="background-color:#E6E6E6;"><span id="S4.T4.2.9.7.3.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn + Real</span></td>
<td id="S4.T4.2.9.7.4" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T4.2.9.7.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#E6E6E6;">45.62<span id="S4.T4.2.9.7.4.1.1" class="ltx_text ltx_font_medium"> <span id="S4.T4.2.9.7.4.1.1.1" class="ltx_text" style="color:#FF0000;">(+39.59)</span></span></span></td>
<td id="S4.T4.2.9.7.5" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T4.2.9.7.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#E6E6E6;">21.45<span id="S4.T4.2.9.7.5.1.1" class="ltx_text ltx_font_medium"> <span id="S4.T4.2.9.7.5.1.1.1" class="ltx_text" style="color:#FF0000;">(+19.38)</span></span></span></td>
</tr>
<tr id="S4.T4.2.2" class="ltx_tr">
<td id="S4.T4.2.2.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" rowspan="7"><span id="S4.T4.2.2.1.1" class="ltx_text" style="font-size:90%;">
<span id="S4.T4.2.2.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:63.1pt;height:9.1pt;vertical-align:-2.3pt;"><span class="ltx_transformed_inner" style="width:63.1pt;transform:translate(0pt,3.38pt) rotate(-0deg) ;">
<span id="S4.T4.2.2.1.1.1.1" class="ltx_p"><math id="S4.T4.2.2.1.1.1.1.m1.1" class="ltx_Math" alttext="20\cdot 10" display="inline"><semantics id="S4.T4.2.2.1.1.1.1.m1.1a"><mrow id="S4.T4.2.2.1.1.1.1.m1.1.1" xref="S4.T4.2.2.1.1.1.1.m1.1.1.cmml"><mn id="S4.T4.2.2.1.1.1.1.m1.1.1.2" xref="S4.T4.2.2.1.1.1.1.m1.1.1.2.cmml">20</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T4.2.2.1.1.1.1.m1.1.1.1" xref="S4.T4.2.2.1.1.1.1.m1.1.1.1.cmml">⋅</mo><mn id="S4.T4.2.2.1.1.1.1.m1.1.1.3" xref="S4.T4.2.2.1.1.1.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.1.1.1.1.m1.1b"><apply id="S4.T4.2.2.1.1.1.1.m1.1.1.cmml" xref="S4.T4.2.2.1.1.1.1.m1.1.1"><ci id="S4.T4.2.2.1.1.1.1.m1.1.1.1.cmml" xref="S4.T4.2.2.1.1.1.1.m1.1.1.1">⋅</ci><cn type="integer" id="S4.T4.2.2.1.1.1.1.m1.1.1.2.cmml" xref="S4.T4.2.2.1.1.1.1.m1.1.1.2">20</cn><cn type="integer" id="S4.T4.2.2.1.1.1.1.m1.1.1.3.cmml" xref="S4.T4.2.2.1.1.1.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.1.1.1.1.m1.1c">20\cdot 10</annotation></semantics></math> (10 shot)</span>
</span></span></span></td>
<td id="S4.T4.2.2.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S4.T4.2.2.2.1" class="ltx_text" style="font-size:90%;">Pure Real </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T4.2.2.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a><span id="S4.T4.2.2.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T4.2.2.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T4.2.2.3.1" class="ltx_text" style="font-size:90%;">Real</span></td>
<td id="S4.T4.2.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T4.2.2.4.1" class="ltx_text" style="font-size:90%;">Real</span></td>
<td id="S4.T4.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.2.5.1" class="ltx_text" style="font-size:90%;">9.12</span></td>
<td id="S4.T4.2.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.2.6.1" class="ltx_text" style="font-size:90%;">2.35</span></td>
</tr>
<tr id="S4.T4.2.10.8" class="ltx_tr">
<td id="S4.T4.2.10.8.1" class="ltx_td ltx_align_left">
<span id="S4.T4.2.10.8.1.1" class="ltx_text" style="font-size:90%;">+ cut paste </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T4.2.10.8.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a><span id="S4.T4.2.10.8.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T4.2.10.8.2" class="ltx_td ltx_align_left"><span id="S4.T4.2.10.8.2.1" class="ltx_text" style="font-size:90%;">Real</span></td>
<td id="S4.T4.2.10.8.3" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T4.2.10.8.3.1" class="ltx_text" style="font-size:90%;">Real</span></td>
<td id="S4.T4.2.10.8.4" class="ltx_td ltx_align_center"><span id="S4.T4.2.10.8.4.1" class="ltx_text" style="font-size:90%;">29.60</span></td>
<td id="S4.T4.2.10.8.5" class="ltx_td ltx_align_center"><span id="S4.T4.2.10.8.5.1" class="ltx_text" style="font-size:90%;">10.82</span></td>
</tr>
<tr id="S4.T4.2.11.9" class="ltx_tr">
<td id="S4.T4.2.11.9.1" class="ltx_td ltx_align_left" style="background-color:#E6E6E6;"><span id="S4.T4.2.11.9.1.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn Fg</span></td>
<td id="S4.T4.2.11.9.2" class="ltx_td ltx_align_left" style="background-color:#E6E6E6;"><span id="S4.T4.2.11.9.2.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn + Real</span></td>
<td id="S4.T4.2.11.9.3" class="ltx_td ltx_align_left ltx_border_r" style="background-color:#E6E6E6;"><span id="S4.T4.2.11.9.3.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Real</span></td>
<td id="S4.T4.2.11.9.4" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T4.2.11.9.4.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">48.14</span></td>
<td id="S4.T4.2.11.9.5" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T4.2.11.9.5.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">21.62</span></td>
</tr>
<tr id="S4.T4.2.12.10" class="ltx_tr">
<td id="S4.T4.2.12.10.1" class="ltx_td ltx_align_left" style="background-color:#E6E6E6;"><span id="S4.T4.2.12.10.1.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Pure Syn</span></td>
<td id="S4.T4.2.12.10.2" class="ltx_td ltx_align_left" style="background-color:#E6E6E6;"><span id="S4.T4.2.12.10.2.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn</span></td>
<td id="S4.T4.2.12.10.3" class="ltx_td ltx_align_left ltx_border_r" style="background-color:#E6E6E6;"><span id="S4.T4.2.12.10.3.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn</span></td>
<td id="S4.T4.2.12.10.4" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T4.2.12.10.4.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">45.12</span></td>
<td id="S4.T4.2.12.10.5" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T4.2.12.10.5.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">22.38</span></td>
</tr>
<tr id="S4.T4.2.13.11" class="ltx_tr">
<td id="S4.T4.2.13.11.1" class="ltx_td ltx_align_left" style="background-color:#E6E6E6;"><span id="S4.T4.2.13.11.1.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn + real</span></td>
<td id="S4.T4.2.13.11.2" class="ltx_td ltx_align_left" style="background-color:#E6E6E6;"><span id="S4.T4.2.13.11.2.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn + Real</span></td>
<td id="S4.T4.2.13.11.3" class="ltx_td ltx_align_left ltx_border_r" style="background-color:#E6E6E6;"><span id="S4.T4.2.13.11.3.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn + Real</span></td>
<td id="S4.T4.2.13.11.4" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T4.2.13.11.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#E6E6E6;">51.82<span id="S4.T4.2.13.11.4.1.1" class="ltx_text ltx_font_medium"> <span id="S4.T4.2.13.11.4.1.1.1" class="ltx_text" style="color:#FF0000;">(+22.22)</span></span></span></td>
<td id="S4.T4.2.13.11.5" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T4.2.13.11.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#E6E6E6;">25.87<span id="S4.T4.2.13.11.5.1.1" class="ltx_text ltx_font_medium"> <span id="S4.T4.2.13.11.5.1.1.1" class="ltx_text" style="color:#FF0000;">(+15.05)</span></span></span></td>
</tr>
<tr id="S4.T4.2.14.12" class="ltx_tr">
<td id="S4.T4.2.14.12.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T4.2.14.12.1.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn + 1,464 real</span></td>
<td id="S4.T4.2.14.12.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T4.2.14.12.2.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn + 1,464 Real</span></td>
<td id="S4.T4.2.14.12.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T4.2.14.12.3.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn + 1,464 Real</span></td>
<td id="S4.T4.2.14.12.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T4.2.14.12.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#E6E6E6;">68.38<span id="S4.T4.2.14.12.4.1.1" class="ltx_text ltx_font_medium"> <span id="S4.T4.2.14.12.4.1.1.1" class="ltx_text" style="color:#FF0000;">(+22.88)</span></span></span></td>
<td id="S4.T4.2.14.12.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T4.2.14.12.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#E6E6E6;">35.96<span id="S4.T4.2.14.12.5.1.1" class="ltx_text ltx_font_medium"> <span id="S4.T4.2.14.12.5.1.1.1" class="ltx_text" style="color:#FF0000;">(+18.96)</span></span></span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>
We harness T2I (Stable Diffusion) to generate a large-scaled high-quality synthetic foregrounds and backgrounds, and improve VOC object detection.
Column mAP is computed as the average of IoU ranging from 50 to 95 with step size 5.</figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We now present experiments to demonstrate the effectiveness of the large-scale synthetic data generated using our proposed approach.
In <a href="#S4.SS1" title="4.1 Object Detection ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.1</span></a>, we first provide detailed results on object detection task on the Pascal VOC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> and COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite> datasets in low-resource regimes including zero-shot, 1-shot and 10-shot settings.
We are interested in low-resource settings, which are more common in practice yet highly challenging due to the limited information provided. In this case, CDIs are the shots given.
We emphasize that our method particularly shines in such settings due to the capability to create a large diverse set of high-quality coherent synthetic training datasets.
In addition, we present ablation studies (<a href="#S4.SS2" title="4.2 More Baselines and Ablation Studies ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>) to investigate the impact of different design choices. Importantly, further analysis (<a href="#S4.SS3" title="4.3 Synthetic data distribution complements the real data distribution. ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.3</span></a>) demonstrates that the synthetic data distribution effectively complements the real data distribution.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Next, in <a href="#S4.SS4" title="4.4 Generalization to more tasks ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.4</span></a>, we show that our method can generalize to more tasks, including instance segmentation tasks on VOC and COCO, and instance detection tasks on GMU-Kitchen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>, Active Vision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>, and YCB-video datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">68</span></a>]</cite>.
Finally, in <a href="#S4.SS5" title="4.5 Compositionality in Synthetic Dataset ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.5</span></a>, we also provide results highlighting the compositional nature of our data generation process.</p>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.2" class="ltx_p"><span id="S4.p3.2.1" class="ltx_text ltx_font_bold">Model, training, and evaluation criterion.</span> We use MaskRCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite> with a ResNet-50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite> backbone for compatibility among object detection, instance segmentation, and object instance detection tasks. We set the learning rate as <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="0.001" display="inline"><semantics id="S4.p3.1.m1.1a"><mn id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><cn type="float" id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">0.001</annotation></semantics></math> with a weight decay <math id="S4.p3.2.m2.1" class="ltx_Math" alttext="0.0005" display="inline"><semantics id="S4.p3.2.m2.1a"><mn id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml">0.0005</mn><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><cn type="float" id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1">0.0005</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">0.0005</annotation></semantics></math> and train the models to convergence for both baselines and our approaches.
We report mean average precision (mAP) for the results.
In <a href="#S4.SS2" title="4.2 More Baselines and Ablation Studies ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.2</span></a> we additionally experiment with MaskRCNN with DINO-pretrained ResNet-50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite> as well as the recent transformer-based method EVA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>.</p>
</div>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">Variants of synthetic dataset.</span>
We evaluate three variants of our methods:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Pure Syn</span> uses purely synthetic foregrounds (<a href="#S3.SS1" title="3.1 Zero-shot Foreground Synthesis ‣ 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>) and zero-shot backgrounds (<a href="#S3.SS2.SSS1" title="3.2.1 Zero-shot scenario ‣ 3.2 Language-driven Context Synthesis ‣ 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2.1</span></a>) for zero-shot.
For few-shot settings where CDIs are available, contextual backgrounds from CDIs (<a href="#S3.SS2.SSS2" title="3.2.2 Few-shot scenario ‣ 3.2 Language-driven Context Synthesis ‣ 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2.2</span></a>) are used on top of template backgrounds.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;padding-top:-2.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Syn Fg</span> pastes synthetic foregrounds (<a href="#S3.SS1" title="3.1 Zero-shot Foreground Synthesis ‣ 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>) on real backgrounds. Note that foregrounds from original real backgrounds are retained.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;padding-top:-2.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Syn + Real</span> further incorporates synthetic backgrounds.
In other words, synthetic datasets are blended with the entire real dataset.</p>
</div>
</li>
</ul>
<p id="S4.p4.2" class="ltx_p">We mainly compare our methods with <span id="S4.p4.2.1" class="ltx_text ltx_font_bold">Pure Real</span>, which trains MaskRCNN fully-supervised using the available real training set, and <span id="S4.p4.2.2" class="ltx_text ltx_font_bold">Pure Real + cut paste</span>, which utilizes cut-paste <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite> to generate a relevant synthetic dataset from real images.
We note that the latter is an upper bound of what can be achieved with provided real images without leveraging external sources, such as T2I, as in our work.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Object Detection</h3>

<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.1.2.1" class="ltx_tr">
<th id="S4.T5.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S4.T5.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">#CDI</span></th>
<th id="S4.T5.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.1.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Method</span></th>
<th id="S4.T5.1.2.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.1.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Foreground</span></th>
<th id="S4.T5.1.2.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S4.T5.1.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Background</span></th>
<th id="S4.T5.1.2.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.1.2.1.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">mAP@50</span></th>
<th id="S4.T5.1.2.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.1.2.1.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">mAP</span></th>
</tr>
<tr id="S4.T5.1.3.2" class="ltx_tr">
<th id="S4.T5.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T5.1.3.2.1.1" class="ltx_text" style="font-size:90%;">0 (0 shot)</span></th>
<th id="S4.T5.1.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T5.1.3.2.2.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Pure Syn</span></th>
<th id="S4.T5.1.3.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T5.1.3.2.3.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn</span></th>
<th id="S4.T5.1.3.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T5.1.3.2.4.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn</span></th>
<th id="S4.T5.1.3.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T5.1.3.2.5.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">16.30</span></th>
<th id="S4.T5.1.3.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T5.1.3.2.6.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">8.40</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.1.1" class="ltx_tr">
<td id="S4.T5.1.1.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" rowspan="5"><span id="S4.T5.1.1.1.1" class="ltx_text" style="font-size:90%;">
<span id="S4.T5.1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:54.1pt;height:9.1pt;vertical-align:-2.3pt;"><span class="ltx_transformed_inner" style="width:54.1pt;transform:translate(0pt,3.38pt) rotate(-0deg) ;">
<span id="S4.T5.1.1.1.1.1.1" class="ltx_p"><math id="S4.T5.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="80\cdot 1" display="inline"><semantics id="S4.T5.1.1.1.1.1.1.m1.1a"><mrow id="S4.T5.1.1.1.1.1.1.m1.1.1" xref="S4.T5.1.1.1.1.1.1.m1.1.1.cmml"><mn id="S4.T5.1.1.1.1.1.1.m1.1.1.2" xref="S4.T5.1.1.1.1.1.1.m1.1.1.2.cmml">80</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T5.1.1.1.1.1.1.m1.1.1.1" xref="S4.T5.1.1.1.1.1.1.m1.1.1.1.cmml">⋅</mo><mn id="S4.T5.1.1.1.1.1.1.m1.1.1.3" xref="S4.T5.1.1.1.1.1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.1.1.1.m1.1b"><apply id="S4.T5.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.1.1.1.m1.1.1"><ci id="S4.T5.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T5.1.1.1.1.1.1.m1.1.1.1">⋅</ci><cn type="integer" id="S4.T5.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T5.1.1.1.1.1.1.m1.1.1.2">80</cn><cn type="integer" id="S4.T5.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T5.1.1.1.1.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.1.1.1.m1.1c">80\cdot 1</annotation></semantics></math> (1 shot)</span>
</span></span></span></td>
<td id="S4.T5.1.1.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S4.T5.1.1.2.1" class="ltx_text" style="font-size:90%;">Pure Real </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T5.1.1.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a><span id="S4.T5.1.1.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T5.1.1.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T5.1.1.3.1" class="ltx_text" style="font-size:90%;">Real</span></td>
<td id="S4.T5.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T5.1.1.4.1" class="ltx_text" style="font-size:90%;">Real</span></td>
<td id="S4.T5.1.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.1.5.1" class="ltx_text" style="font-size:90%;">1.47</span></td>
<td id="S4.T5.1.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.1.6.1" class="ltx_text" style="font-size:90%;">0.92</span></td>
</tr>
<tr id="S4.T5.1.4.1" class="ltx_tr">
<td id="S4.T5.1.4.1.1" class="ltx_td ltx_align_left">
<span id="S4.T5.1.4.1.1.1" class="ltx_text" style="font-size:90%;">+ cut paste </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T5.1.4.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a><span id="S4.T5.1.4.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T5.1.4.1.2" class="ltx_td ltx_align_left"><span id="S4.T5.1.4.1.2.1" class="ltx_text" style="font-size:90%;">Real</span></td>
<td id="S4.T5.1.4.1.3" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T5.1.4.1.3.1" class="ltx_text" style="font-size:90%;">Real</span></td>
<td id="S4.T5.1.4.1.4" class="ltx_td ltx_align_center"><span id="S4.T5.1.4.1.4.1" class="ltx_text" style="font-size:90%;">2.89</span></td>
<td id="S4.T5.1.4.1.5" class="ltx_td ltx_align_center"><span id="S4.T5.1.4.1.5.1" class="ltx_text" style="font-size:90%;">1.23</span></td>
</tr>
<tr id="S4.T5.1.5.2" class="ltx_tr">
<td id="S4.T5.1.5.2.1" class="ltx_td ltx_align_left" style="background-color:#E6E6E6;"><span id="S4.T5.1.5.2.1.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn Fg</span></td>
<td id="S4.T5.1.5.2.2" class="ltx_td ltx_align_left" style="background-color:#E6E6E6;"><span id="S4.T5.1.5.2.2.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn + Real</span></td>
<td id="S4.T5.1.5.2.3" class="ltx_td ltx_align_left ltx_border_r" style="background-color:#E6E6E6;"><span id="S4.T5.1.5.2.3.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Real</span></td>
<td id="S4.T5.1.5.2.4" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T5.1.5.2.4.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">17.87</span></td>
<td id="S4.T5.1.5.2.5" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T5.1.5.2.5.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">8.64</span></td>
</tr>
<tr id="S4.T5.1.6.3" class="ltx_tr">
<td id="S4.T5.1.6.3.1" class="ltx_td ltx_align_left" style="background-color:#E6E6E6;"><span id="S4.T5.1.6.3.1.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Pure Syn</span></td>
<td id="S4.T5.1.6.3.2" class="ltx_td ltx_align_left" style="background-color:#E6E6E6;"><span id="S4.T5.1.6.3.2.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn</span></td>
<td id="S4.T5.1.6.3.3" class="ltx_td ltx_align_left ltx_border_r" style="background-color:#E6E6E6;"><span id="S4.T5.1.6.3.3.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn</span></td>
<td id="S4.T5.1.6.3.4" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T5.1.6.3.4.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">16.80</span></td>
<td id="S4.T5.1.6.3.5" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T5.1.6.3.5.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">8.59</span></td>
</tr>
<tr id="S4.T5.1.7.4" class="ltx_tr">
<td id="S4.T5.1.7.4.1" class="ltx_td ltx_align_left ltx_border_bb" style="background-color:#E6E6E6;"><span id="S4.T5.1.7.4.1.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn + real</span></td>
<td id="S4.T5.1.7.4.2" class="ltx_td ltx_align_left ltx_border_bb" style="background-color:#E6E6E6;"><span id="S4.T5.1.7.4.2.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn + Real</span></td>
<td id="S4.T5.1.7.4.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="background-color:#E6E6E6;"><span id="S4.T5.1.7.4.3.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn + Real</span></td>
<td id="S4.T5.1.7.4.4" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#E6E6E6;"><span id="S4.T5.1.7.4.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#E6E6E6;">20.82<span id="S4.T5.1.7.4.4.1.1" class="ltx_text ltx_font_medium"> <span id="S4.T5.1.7.4.4.1.1.1" class="ltx_text" style="color:#FF0000;">(+17.93)</span></span></span></td>
<td id="S4.T5.1.7.4.5" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#E6E6E6;"><span id="S4.T5.1.7.4.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#E6E6E6;">10.63<span id="S4.T5.1.7.4.5.1.1" class="ltx_text ltx_font_medium"> <span id="S4.T5.1.7.4.5.1.1.1" class="ltx_text" style="color:#FF0000;">(+9.40)</span></span></span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 5: </span>Stable Diffusion generated foregrounds and contextual backgrounds enhance object detection on COCO dataset.</figcaption>
</figure>
<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">For object detection, we present zero-/one-/ten-shot results on PASCAL VOC 2012 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> in <a href="#S4.T4" title="In 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a> and zero-/one-shot results on MS COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite> in <a href="#S4.T5" title="In 4.1 Object Detection ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">5</span></a>. VOC encompasses 20 target objects, with 1,464 images for training and 1,449 for validation. While there exists an augmented training set comprising 10,582 images, it lacks instance segmentation masks, which are required by cut-and-paste augmentation. Given our emphasis on low-resource scenarios, we prioritize smaller training sets. MS COCO features 80 target objects with a total of 120k training instances.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Implementation details of synthetic datasets.</span>
We use the instance segmentation mask labels from the training set as our real foreground masks, and CDI is the number of training images in this case.
Take 0-/10-shot VOC as example,<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>1-shot VOC and MS COCO is similar and omit for briefity.</span></span></span> detailed statistics is provided in <a href="#S4.T6" title="In 4.1 Object Detection ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">6</span></a>.
For all three variants of synthetic datasets, we leverage
fixed foreground templates to generate high-quality foregrounds (<a href="#S3.T2" title="In 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>, <a href="#S3.SS1" title="3.1 Zero-shot Foreground Synthesis ‣ 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>).
For each of the 20 objects, we generate 500 images for each of the six templates, then use CLIP (<a href="#S3.SS3" title="3.3 CLIP Sanity Check ‣ 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>) to downselect 200 best images, thus aggregating to a total of 24k synthetic foregrounds.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">In the <span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">0 shot Pure Syn</span> scenario, we lack access to any training instances and must depend on fixed background templates to generate coherent backgrounds, as detailed in <a href="#S3.T3" title="In 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#S3.SS2.SSS1" title="3.2.1 Zero-shot scenario ‣ 3.2 Language-driven Context Synthesis ‣ 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2.1</span></a>. For each of the 16 templates, we produce 600 images. Notably, the generated images are of high caliber and typically exclude target objects. As a result, we filter out 5% of the images using only CLIP, as described in <a href="#S3.SS3" title="3.3 CLIP Sanity Check ‣ 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>, leaving us with 9,120 synthetic backgrounds. Onto each of these backgrounds, we superimpose four synthetic foregrounds, as outlined in <a href="#S3.SS4" title="3.4 Cut-paste Composition ‣ 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.4</span></a>. This results in a synthetic dataset complete with segmentation labels for the foregrounds. We repeat this procedure until the count of final pseudo-labeled images reaches 60k.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">In the <span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold">10 shot Pure Syn</span>, we have an additional 200 CDIs at our disposal.
Instead of directly training on these real images, they serve to facilitate the creation of more contextually coherent backgrounds through context-guided synthesis, as detailed in <a href="#S3.SS2.SSS2" title="3.2.2 Few-shot scenario ‣ 3.2 Language-driven Context Synthesis ‣ 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2.2</span></a>.
Specifically, for each CDI, two captions are generated, from which 80 images are produced. Using CLIP, we then narrow this down to 30 images per CDI, as described in <a href="#S3.SS3" title="3.3 CLIP Sanity Check ‣ 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>.
This results in a total of <math id="S4.SS1.p4.1.m1.1" class="ltx_Math" alttext="2\times 200\times 30=12\text{k}" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><mrow id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml"><mrow id="S4.SS1.p4.1.m1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.2.cmml"><mn id="S4.SS1.p4.1.m1.1.1.2.2" xref="S4.SS1.p4.1.m1.1.1.2.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p4.1.m1.1.1.2.1" xref="S4.SS1.p4.1.m1.1.1.2.1.cmml">×</mo><mn id="S4.SS1.p4.1.m1.1.1.2.3" xref="S4.SS1.p4.1.m1.1.1.2.3.cmml">200</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p4.1.m1.1.1.2.1a" xref="S4.SS1.p4.1.m1.1.1.2.1.cmml">×</mo><mn id="S4.SS1.p4.1.m1.1.1.2.4" xref="S4.SS1.p4.1.m1.1.1.2.4.cmml">30</mn></mrow><mo id="S4.SS1.p4.1.m1.1.1.1" xref="S4.SS1.p4.1.m1.1.1.1.cmml">=</mo><mrow id="S4.SS1.p4.1.m1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.3.cmml"><mn id="S4.SS1.p4.1.m1.1.1.3.2" xref="S4.SS1.p4.1.m1.1.1.3.2.cmml">12</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p4.1.m1.1.1.3.1" xref="S4.SS1.p4.1.m1.1.1.3.1.cmml">​</mo><mtext id="S4.SS1.p4.1.m1.1.1.3.3" xref="S4.SS1.p4.1.m1.1.1.3.3a.cmml">k</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><apply id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"><eq id="S4.SS1.p4.1.m1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1.1"></eq><apply id="S4.SS1.p4.1.m1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.2"><times id="S4.SS1.p4.1.m1.1.1.2.1.cmml" xref="S4.SS1.p4.1.m1.1.1.2.1"></times><cn type="integer" id="S4.SS1.p4.1.m1.1.1.2.2.cmml" xref="S4.SS1.p4.1.m1.1.1.2.2">2</cn><cn type="integer" id="S4.SS1.p4.1.m1.1.1.2.3.cmml" xref="S4.SS1.p4.1.m1.1.1.2.3">200</cn><cn type="integer" id="S4.SS1.p4.1.m1.1.1.2.4.cmml" xref="S4.SS1.p4.1.m1.1.1.2.4">30</cn></apply><apply id="S4.SS1.p4.1.m1.1.1.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3"><times id="S4.SS1.p4.1.m1.1.1.3.1.cmml" xref="S4.SS1.p4.1.m1.1.1.3.1"></times><cn type="integer" id="S4.SS1.p4.1.m1.1.1.3.2.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2">12</cn><ci id="S4.SS1.p4.1.m1.1.1.3.3a.cmml" xref="S4.SS1.p4.1.m1.1.1.3.3"><mtext id="S4.SS1.p4.1.m1.1.1.3.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3.3">k</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">2\times 200\times 30=12\text{k}</annotation></semantics></math> contextually relevant backgrounds.
We then apply the cut-paste method, as outlined in <a href="#S3.SS4" title="3.4 Cut-paste Composition ‣ 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.4</span></a>, using the <span id="S4.SS1.p4.1.2" class="ltx_text ltx_font_italic">same</span> zero-shot synthetic foregrounds on these expanded backgrounds, culminating in a final dataset of 60k.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">For the <span id="S4.SS1.p5.1.1" class="ltx_text ltx_font_bold">10 shot Syn+Real</span>, our training encompasses not just the synthetic datasets from Pure Syn, but also the actual 200 images. Within those <math id="S4.SS1.p5.1.m1.1" class="ltx_Math" alttext="20\times 10" display="inline"><semantics id="S4.SS1.p5.1.m1.1a"><mrow id="S4.SS1.p5.1.m1.1.1" xref="S4.SS1.p5.1.m1.1.1.cmml"><mn id="S4.SS1.p5.1.m1.1.1.2" xref="S4.SS1.p5.1.m1.1.1.2.cmml">20</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p5.1.m1.1.1.1" xref="S4.SS1.p5.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS1.p5.1.m1.1.1.3" xref="S4.SS1.p5.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.1.m1.1b"><apply id="S4.SS1.p5.1.m1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1"><times id="S4.SS1.p5.1.m1.1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.p5.1.m1.1.1.2.cmml" xref="S4.SS1.p5.1.m1.1.1.2">20</cn><cn type="integer" id="S4.SS1.p5.1.m1.1.1.3.cmml" xref="S4.SS1.p5.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.1.m1.1c">20\times 10</annotation></semantics></math> real images, we also obtain multiple real foregrounds: in our selection of 10 shot data, there are 541 unique foregrounds.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<table id="S4.T6.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T6.2.1.1" class="ltx_tr">
<th id="S4.T6.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T6.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Experiment</span></th>
<th id="S4.T6.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T6.2.1.1.2.1" class="ltx_text" style="font-size:90%;"># Real images</span></th>
<td id="S4.T6.2.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T6.2.1.1.3.1" class="ltx_text" style="font-size:90%;"># Foreground</span></td>
<td id="S4.T6.2.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T6.2.1.1.4.1" class="ltx_text" style="font-size:90%;"># Background</span></td>
<td id="S4.T6.2.1.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.2.1.1.5.1" class="ltx_text" style="font-size:90%;"># Training set size</span></td>
</tr>
<tr id="S4.T6.2.2.2" class="ltx_tr">
<th id="S4.T6.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T6.2.2.2.1.1" class="ltx_text" style="font-size:90%;">Fullset (Pure Real)</span></th>
<th id="S4.T6.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T6.2.2.2.2.1" class="ltx_text" style="font-size:90%;">1464</span></th>
<td id="S4.T6.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T6.2.2.2.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T6.2.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T6.2.2.2.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T6.2.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.2.2.2.5.1" class="ltx_text" style="font-size:90%;">1,464</span></td>
</tr>
<tr id="S4.T6.2.3.3" class="ltx_tr">
<th id="S4.T6.2.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T6.2.3.3.1.1" class="ltx_text" style="font-size:90%;">0-shot (Pure Syn)</span></th>
<th id="S4.T6.2.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T6.2.3.3.2.1" class="ltx_text" style="font-size:90%;">0</span></th>
<td id="S4.T6.2.3.3.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T6.2.3.3.3.1" class="ltx_text" style="font-size:90%;">24k</span></td>
<td id="S4.T6.2.3.3.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T6.2.3.3.4.1" class="ltx_text" style="font-size:90%;">9120</span></td>
<td id="S4.T6.2.3.3.5" class="ltx_td ltx_align_center"><span id="S4.T6.2.3.3.5.1" class="ltx_text" style="font-size:90%;">60k</span></td>
</tr>
<tr id="S4.T6.2.4.4" class="ltx_tr">
<th id="S4.T6.2.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T6.2.4.4.1.1" class="ltx_text" style="font-size:90%;">10-shot (Pure Real)</span></th>
<th id="S4.T6.2.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T6.2.4.4.2.1" class="ltx_text" style="font-size:90%;">200</span></th>
<td id="S4.T6.2.4.4.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T6.2.4.4.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T6.2.4.4.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T6.2.4.4.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T6.2.4.4.5" class="ltx_td ltx_align_center"><span id="S4.T6.2.4.4.5.1" class="ltx_text" style="font-size:90%;">200</span></td>
</tr>
<tr id="S4.T6.2.5.5" class="ltx_tr">
<th id="S4.T6.2.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T6.2.5.5.1.1" class="ltx_text" style="font-size:90%;">10-shot (Pure Real + cut paste)</span></th>
<th id="S4.T6.2.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T6.2.5.5.2.1" class="ltx_text" style="font-size:90%;">200</span></th>
<td id="S4.T6.2.5.5.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T6.2.5.5.3.1" class="ltx_text" style="font-size:90%;">200</span></td>
<td id="S4.T6.2.5.5.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T6.2.5.5.4.1" class="ltx_text" style="font-size:90%;">200</span></td>
<td id="S4.T6.2.5.5.5" class="ltx_td ltx_align_center"><span id="S4.T6.2.5.5.5.1" class="ltx_text" style="font-size:90%;">60k</span></td>
</tr>
<tr id="S4.T6.2.6.6" class="ltx_tr">
<th id="S4.T6.2.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T6.2.6.6.1.1" class="ltx_text" style="font-size:90%;">10-shot (Syn Fg)</span></th>
<th id="S4.T6.2.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T6.2.6.6.2.1" class="ltx_text" style="font-size:90%;">200</span></th>
<td id="S4.T6.2.6.6.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T6.2.6.6.3.1" class="ltx_text" style="font-size:90%;">24k</span></td>
<td id="S4.T6.2.6.6.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T6.2.6.6.4.1" class="ltx_text" style="font-size:90%;">200</span></td>
<td id="S4.T6.2.6.6.5" class="ltx_td ltx_align_center"><span id="S4.T6.2.6.6.5.1" class="ltx_text" style="font-size:90%;">60k</span></td>
</tr>
<tr id="S4.T6.2.7.7" class="ltx_tr">
<th id="S4.T6.2.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T6.2.7.7.1.1" class="ltx_text" style="font-size:90%;">10-shot (Pure Syn)</span></th>
<th id="S4.T6.2.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T6.2.7.7.2.1" class="ltx_text" style="font-size:90%;">200</span></th>
<td id="S4.T6.2.7.7.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T6.2.7.7.3.1" class="ltx_text" style="font-size:90%;">24k</span></td>
<td id="S4.T6.2.7.7.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T6.2.7.7.4.1" class="ltx_text" style="font-size:90%;">9120+12k</span></td>
<td id="S4.T6.2.7.7.5" class="ltx_td ltx_align_center"><span id="S4.T6.2.7.7.5.1" class="ltx_text" style="font-size:90%;">60k</span></td>
</tr>
<tr id="S4.T6.2.8.8" class="ltx_tr">
<th id="S4.T6.2.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T6.2.8.8.1.1" class="ltx_text" style="font-size:90%;">10-shot (Syn + Real)</span></th>
<th id="S4.T6.2.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T6.2.8.8.2.1" class="ltx_text" style="font-size:90%;">200</span></th>
<td id="S4.T6.2.8.8.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T6.2.8.8.3.1" class="ltx_text" style="font-size:90%;">24k+541</span></td>
<td id="S4.T6.2.8.8.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T6.2.8.8.4.1" class="ltx_text" style="font-size:90%;">9120+12k+200</span></td>
<td id="S4.T6.2.8.8.5" class="ltx_td ltx_align_center"><span id="S4.T6.2.8.8.5.1" class="ltx_text" style="font-size:90%;">60k</span></td>
</tr>
<tr id="S4.T6.2.9.9" class="ltx_tr">
<th id="S4.T6.2.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r"><span id="S4.T6.2.9.9.1.1" class="ltx_text" style="font-size:90%;">10-shot (Syn + Real) + 1464</span></th>
<th id="S4.T6.2.9.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r"><span id="S4.T6.2.9.9.2.1" class="ltx_text" style="font-size:90%;">1464</span></th>
<td id="S4.T6.2.9.9.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T6.2.9.9.3.1" class="ltx_text" style="font-size:90%;">24k+541</span></td>
<td id="S4.T6.2.9.9.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T6.2.9.9.4.1" class="ltx_text" style="font-size:90%;">9120+12k+200</span></td>
<td id="S4.T6.2.9.9.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T6.2.9.9.5.1" class="ltx_text" style="font-size:90%;">60k + 1464</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T6.3.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="S4.T6.4.2" class="ltx_text" style="font-size:90%;">Detail statsitics of synthetic datasets created for VOC.</span></figcaption>
</figure>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.1" class="ltx_p"><a href="#S4.F9" title="In 4.3 Synthetic data distribution complements the real data distribution. ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">9</span></a> and <a href="#Sx1.F13" title="In Appendix ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">13</span></a> show more example training images generated by our pipeline on PASCAL VOC dataset: both foreground object and background context images are generated by our method with Stable Diffusion.</p>
</div>
<div id="S4.SS1.p7" class="ltx_para">
<p id="S4.SS1.p7.1" class="ltx_p">We use Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a>]</cite> as the T2I, which takes 20 P40 GPUs 50h to generate 10-shot synthetic dataset.
We emphasize that
generation is a
one-time process that can train various downstream tasks and
models, and that
our method is automatic and requires little human involvement to collect synthetic data.
Collecting COCO or VOC
manually takes much longer time with extra privacy or scalability issues.</p>
</div>
<div id="S4.SS1.p8" class="ltx_para ltx_noindent">
<p id="S4.SS1.p8.1" class="ltx_p"><span id="S4.SS1.p8.1.1" class="ltx_text ltx_font_bold">Synthetic foregrounds and backgrounds benefit downstream task.</span>
On VOC, we first notice that
a model trained solely on synthetic data in the absence of <em id="S4.SS1.p8.1.2" class="ltx_emph ltx_font_italic">any</em> real images (0 shot Pure Syn) achieves comparable performance to a model trained on 1.4k real images (Pure Real).
This suggests that synthetic datasets can effectively improve downstream performance when only prior knowledge of interested objects is known.
We then observe that Pure Syn performance improves as adding CDIs, reinforcing our assumption that contextual information encoded in the backgrounds provides valuable cues in learning.
Further, we note that Syn Fg in few-shot setting outperforms Pure Real + cut paste significantly, implying that
inclusion of large-scale synthesized foregrounds enables the detection model to learn a more comprehensive understanding of objects due to the diversity and coherency of the synthetic data.
Lastly, further performance gains by adding synthetic backgrounds (Syn + real) show that blending both real and synthetic leads to the best performance, <em id="S4.SS1.p8.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.SS1.p8.1.4" class="ltx_text"></span> +22.22 net improvement over Pure Real + cut paste in 10 shot regime.
Combining just 10-shot synthesized data with the 1,464 real training images achieves 68.38 mAP@50, a substantial +22.88 net improvement over using the 1,464 real training images alone.</p>
</div>
<div id="S4.SS1.p9" class="ltx_para">
<p id="S4.SS1.p9.1" class="ltx_p">On COCO, we observe a similar trend as VOC, <em id="S4.SS1.p9.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS1.p9.1.2" class="ltx_text"></span> synthetic datasets provide a strong learning signal, while mixing both synthetic and real gives the most performance boost.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>More Baselines and Ablation Studies</h3>

<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Agnostic to backbones.</h5>

<figure id="S4.F4" class="ltx_figure"><img src="/html/2309.05956/assets/ablation_on_backbone.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="409" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">Our synthetic dataset generation is agnostic to different models and backbones.</span></figcaption>
</figure>
<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p1.1" class="ltx_p">We first show that our synthetic data generation pipeline is model-agnostic.
In <a href="#S4.F4" title="In Agnostic to backbones. ‣ 4.2 More Baselines and Ablation Studies ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a>, we present performance of two additional models: transformer-based EVA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite> and DINO self-supervised pretrained ResNet-50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite> on VOC.
We observe a similar trend across the model choice:
With only 10 shot exemplars, our approach can outperform fullest (pure real), which is 7x larger.
On the other hand, the model trained with our 0-shot-generated dataset can significantly surpass the best model available for training on 10 shot with cut-and-paste augmentation.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Contextual backgrounds are crucial.</h5>

<div id="S4.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px2.p1.1" class="ltx_p">We already demonstrated that in-context coherent backgrounds are beneficial in <a href="#S4.SS1" title="4.1 Object Detection ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.1</span></a>.
We further investigate what are the best contextual backgrounds.
As shown in <a href="#S4.F5" title="In Contextual backgrounds are crucial. ‣ 4.2 More Baselines and Ablation Studies ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a>, on VOC we compare our synthetic contextual background from context-guided synthesis (<a href="#S3.SS2" title="3.2 Language-driven Context Synthesis ‣ 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>) with three other contexts:
(1) Search Engine: substitute T2I with a search engine. Specifically, we directly collect backgrounds from Google search by using the same prompts as described in <a href="#S3.SS2" title="3.2 Language-driven Context Synthesis ‣ 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>;
(2) Other real datasets: use MS COCO dataset as background.
We randomly sample COCO images that contain only the remaining 60 classes so that they are disjoint from VOC 20 objects;
(3) Black background: replace each of the contextual backgrounds with pure black backgrounds.
Contextual backgrounds consistently outperform other baselines, suggesting that contextual cues in the backgrounds are important to learn a detection model.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2309.05956/assets/ablation_on_syn_bg.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="370" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;">Contextual backgrounds generated by our approach provide valuable cues.</span></figcaption>
</figure>
</section>
<section id="S4.SS2.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">CLIP and context extraction controls semantic quality and cleanness.</h5>

<div id="S4.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px3.p1.1" class="ltx_p">We use CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite> to filter and rank the synthesized context backgrounds (<a href="#S3.SS2" title="3.2 Language-driven Context Synthesis ‣ 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>).
In <a href="#S4.F6" title="In CLIP and context extraction controls semantic quality and cleanness. ‣ 4.2 More Baselines and Ablation Studies ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a>, we train VOC object detection model without CLIP.
We observe at most -21.65 net decrease in performance, which implies that using CLIP as a variance reduction step is essential in pruning noisy or nonsensical backgrounds that might be generated by T2I.
We additionally ablate the effect of context extraction, as captions might contain interested objects. We found without extraction, interested objects contained in the captions will often be reflected in the synthetic images, and thus mislead the model during training as no annotation is provided in the pseudo-labeled dataset (<a href="#S3.SS4" title="3.4 Cut-paste Composition ‣ 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.4</span></a>).</p>
</div>
<div id="S4.SS2.SSS0.Px3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS0.Px3.p2.1" class="ltx_p"><span id="S4.SS2.SSS0.Px3.p2.1.1" class="ltx_text ltx_font_bold">Extracting information from the generative model to enhance a discriminative model.</span>
Our foreground generation only requires class labels, so we can potentially generate as many foreground objects and their corresponding masks per class as we want.
In <a href="#S4.F6" title="In CLIP and context extraction controls semantic quality and cleanness. ‣ 4.2 More Baselines and Ablation Studies ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a>, we observe performance improvement as the number of foreground objects increases.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2309.05956/assets/ablation_on_syn_fg.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="415" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.4.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.5.2" class="ltx_text" style="font-size:90%;">
Our generated synthetic foregrounds (<span id="S4.F6.5.2.1" class="ltx_text ltx_font_typewriter">fg</span>) are high-quality and diverse, and adding more helps.
On the other hand, CLIP filtering and context extraction are crucial tricks to ensure the quality of synthetic backgrounds (<span id="S4.F6.5.2.2" class="ltx_text ltx_font_typewriter">bg</span>).</span></figcaption>
</figure>
</section>
<section id="S4.SS2.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Robustness to foreground extraction methods.</h5>

<div id="S4.SS2.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px4.p1.1" class="ltx_p">We next empirically demonstrate that foreground images consist of easy-to-separate background images. To this end, we use two off-the-shelf image segmentation methods, Entity Segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite> and PP-Matting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite> to segment out the foreground objects. We use the segmented foreground to generate training data and show their results on the Pascal VOC object detection task in <a href="#S4.T7" title="In Robustness to foreground extraction methods. ‣ 4.2 More Baselines and Ablation Studies ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">7</span></a>. We observe that we achieve similar mAP scores in both these settings, which demonstrates that our approach is generally robust to the selection of the image segmentation method.</p>
</div>
<figure id="S4.T7" class="ltx_table">
<table id="S4.T7.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T7.2.3.1" class="ltx_tr">
<th id="S4.T7.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S4.T7.2.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">#CDI</span></th>
<th id="S4.T7.2.3.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S4.T7.2.3.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Method</span></th>
<td id="S4.T7.2.3.1.3" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T7.2.3.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">EntSeg</span><span id="S4.T7.2.3.1.3.2" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T7.2.3.1.3.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a><span id="S4.T7.2.3.1.3.4.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T7.2.3.1.4" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T7.2.3.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">PP-Matting</span><span id="S4.T7.2.3.1.4.2" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T7.2.3.1.4.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a><span id="S4.T7.2.3.1.4.4.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
<tr id="S4.T7.2.4.2" class="ltx_tr">
<th id="S4.T7.2.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T7.2.4.2.1.1" class="ltx_text" style="font-size:90%;">0 shot</span></th>
<th id="S4.T7.2.4.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T7.2.4.2.2.1" class="ltx_text" style="font-size:90%;">Pure Syn</span></th>
<td id="S4.T7.2.4.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.2.4.2.3.1" class="ltx_text" style="font-size:90%;">43.24</span></td>
<td id="S4.T7.2.4.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.2.4.2.4.1" class="ltx_text" style="font-size:90%;">46.11</span></td>
</tr>
<tr id="S4.T7.1.1" class="ltx_tr">
<th id="S4.T7.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S4.T7.1.1.1.1" class="ltx_text" style="font-size:90%;">
<span id="S4.T7.1.1.1.1.1" class="ltx_inline-block ltx_align_center">
<span id="S4.T7.1.1.1.1.1.1" class="ltx_p"><math id="S4.T7.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="20\cdot 1" display="inline"><semantics id="S4.T7.1.1.1.1.1.1.m1.1a"><mrow id="S4.T7.1.1.1.1.1.1.m1.1.1" xref="S4.T7.1.1.1.1.1.1.m1.1.1.cmml"><mn id="S4.T7.1.1.1.1.1.1.m1.1.1.2" xref="S4.T7.1.1.1.1.1.1.m1.1.1.2.cmml">20</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T7.1.1.1.1.1.1.m1.1.1.1" xref="S4.T7.1.1.1.1.1.1.m1.1.1.1.cmml">⋅</mo><mn id="S4.T7.1.1.1.1.1.1.m1.1.1.3" xref="S4.T7.1.1.1.1.1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.1.1.1.1.1.1.m1.1b"><apply id="S4.T7.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T7.1.1.1.1.1.1.m1.1.1"><ci id="S4.T7.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T7.1.1.1.1.1.1.m1.1.1.1">⋅</ci><cn type="integer" id="S4.T7.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T7.1.1.1.1.1.1.m1.1.1.2">20</cn><cn type="integer" id="S4.T7.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T7.1.1.1.1.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.1.1.1.1.1.1.m1.1c">20\cdot 1</annotation></semantics></math></span>
<span id="S4.T7.1.1.1.1.1.2" class="ltx_p">(1 shot)</span>
</span></span></th>
<th id="S4.T7.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T7.1.1.2.1" class="ltx_text" style="font-size:90%;">Syn Fg</span></th>
<td id="S4.T7.1.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.1.1.3.1" class="ltx_text" style="font-size:90%;">37.97</span></td>
<td id="S4.T7.1.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.1.1.4.1" class="ltx_text" style="font-size:90%;">42.82</span></td>
</tr>
<tr id="S4.T7.2.5.3" class="ltx_tr">
<th id="S4.T7.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T7.2.5.3.1.1" class="ltx_text" style="font-size:90%;">Pure Syn</span></th>
<td id="S4.T7.2.5.3.2" class="ltx_td ltx_align_center"><span id="S4.T7.2.5.3.2.1" class="ltx_text" style="font-size:90%;">44.24</span></td>
<td id="S4.T7.2.5.3.3" class="ltx_td ltx_align_center"><span id="S4.T7.2.5.3.3.1" class="ltx_text" style="font-size:90%;">44.84</span></td>
</tr>
<tr id="S4.T7.2.6.4" class="ltx_tr">
<th id="S4.T7.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T7.2.6.4.1.1" class="ltx_text" style="font-size:90%;">Syn + real</span></th>
<td id="S4.T7.2.6.4.2" class="ltx_td ltx_align_center"><span id="S4.T7.2.6.4.2.1" class="ltx_text" style="font-size:90%;">45.62</span></td>
<td id="S4.T7.2.6.4.3" class="ltx_td ltx_align_center"><span id="S4.T7.2.6.4.3.1" class="ltx_text" style="font-size:90%;">46.71</span></td>
</tr>
<tr id="S4.T7.2.2" class="ltx_tr">
<th id="S4.T7.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" rowspan="3"><span id="S4.T7.2.2.1.1" class="ltx_text" style="font-size:90%;">
<span id="S4.T7.2.2.1.1.1" class="ltx_inline-block ltx_align_center">
<span id="S4.T7.2.2.1.1.1.1" class="ltx_p"><math id="S4.T7.2.2.1.1.1.1.m1.1" class="ltx_Math" alttext="20\cdot 10" display="inline"><semantics id="S4.T7.2.2.1.1.1.1.m1.1a"><mrow id="S4.T7.2.2.1.1.1.1.m1.1.1" xref="S4.T7.2.2.1.1.1.1.m1.1.1.cmml"><mn id="S4.T7.2.2.1.1.1.1.m1.1.1.2" xref="S4.T7.2.2.1.1.1.1.m1.1.1.2.cmml">20</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T7.2.2.1.1.1.1.m1.1.1.1" xref="S4.T7.2.2.1.1.1.1.m1.1.1.1.cmml">⋅</mo><mn id="S4.T7.2.2.1.1.1.1.m1.1.1.3" xref="S4.T7.2.2.1.1.1.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.2.2.1.1.1.1.m1.1b"><apply id="S4.T7.2.2.1.1.1.1.m1.1.1.cmml" xref="S4.T7.2.2.1.1.1.1.m1.1.1"><ci id="S4.T7.2.2.1.1.1.1.m1.1.1.1.cmml" xref="S4.T7.2.2.1.1.1.1.m1.1.1.1">⋅</ci><cn type="integer" id="S4.T7.2.2.1.1.1.1.m1.1.1.2.cmml" xref="S4.T7.2.2.1.1.1.1.m1.1.1.2">20</cn><cn type="integer" id="S4.T7.2.2.1.1.1.1.m1.1.1.3.cmml" xref="S4.T7.2.2.1.1.1.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.2.2.1.1.1.1.m1.1c">20\cdot 10</annotation></semantics></math></span>
<span id="S4.T7.2.2.1.1.1.2" class="ltx_p">(10 shot)</span>
</span></span></th>
<th id="S4.T7.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T7.2.2.2.1" class="ltx_text" style="font-size:90%;">Syn Fg</span></th>
<td id="S4.T7.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.2.2.3.1" class="ltx_text" style="font-size:90%;">48.14</span></td>
<td id="S4.T7.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.2.2.4.1" class="ltx_text" style="font-size:90%;">47.78</span></td>
</tr>
<tr id="S4.T7.2.7.5" class="ltx_tr">
<th id="S4.T7.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T7.2.7.5.1.1" class="ltx_text" style="font-size:90%;">Pure Syn</span></th>
<td id="S4.T7.2.7.5.2" class="ltx_td ltx_align_center"><span id="S4.T7.2.7.5.2.1" class="ltx_text" style="font-size:90%;">45.12</span></td>
<td id="S4.T7.2.7.5.3" class="ltx_td ltx_align_center"><span id="S4.T7.2.7.5.3.1" class="ltx_text" style="font-size:90%;">43.01</span></td>
</tr>
<tr id="S4.T7.2.8.6" class="ltx_tr">
<th id="S4.T7.2.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S4.T7.2.8.6.1.1" class="ltx_text" style="font-size:90%;">Syn + real</span></th>
<td id="S4.T7.2.8.6.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T7.2.8.6.2.1" class="ltx_text" style="font-size:90%;">51.82</span></td>
<td id="S4.T7.2.8.6.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T7.2.8.6.3.1" class="ltx_text" style="font-size:90%;">52.39</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 7: </span>
Our approach is robust to foreground extraction methods.</figcaption>
</figure>
</section>
<section id="S4.SS2.SSS0.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Mixing real data with synthetic data.</h5>

<figure id="S4.F7" class="ltx_figure"><img src="/html/2309.05956/assets/ablation_on_fraction_of_real.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="359" height="271" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.3.2" class="ltx_text" style="font-size:90%;">Mixing of real and synthetic data further improves downstream models.</span></figcaption>
</figure>
<div id="S4.SS2.SSS0.Px5.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px5.p1.2" class="ltx_p">We show the effect of incorporating different percentages of real-world training images together with our synthesized images on Pascal VOC object detection.
In <a href="#S4.F7" title="In Mixing real data with synthetic data. ‣ 4.2 More Baselines and Ablation Studies ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7</span></a>, we experiment with adding additional <math id="S4.SS2.SSS0.Px5.p1.1.m1.4" class="ltx_Math" alttext="5,25,50,75" display="inline"><semantics id="S4.SS2.SSS0.Px5.p1.1.m1.4a"><mrow id="S4.SS2.SSS0.Px5.p1.1.m1.4.5.2" xref="S4.SS2.SSS0.Px5.p1.1.m1.4.5.1.cmml"><mn id="S4.SS2.SSS0.Px5.p1.1.m1.1.1" xref="S4.SS2.SSS0.Px5.p1.1.m1.1.1.cmml">5</mn><mo id="S4.SS2.SSS0.Px5.p1.1.m1.4.5.2.1" xref="S4.SS2.SSS0.Px5.p1.1.m1.4.5.1.cmml">,</mo><mn id="S4.SS2.SSS0.Px5.p1.1.m1.2.2" xref="S4.SS2.SSS0.Px5.p1.1.m1.2.2.cmml">25</mn><mo id="S4.SS2.SSS0.Px5.p1.1.m1.4.5.2.2" xref="S4.SS2.SSS0.Px5.p1.1.m1.4.5.1.cmml">,</mo><mn id="S4.SS2.SSS0.Px5.p1.1.m1.3.3" xref="S4.SS2.SSS0.Px5.p1.1.m1.3.3.cmml">50</mn><mo id="S4.SS2.SSS0.Px5.p1.1.m1.4.5.2.3" xref="S4.SS2.SSS0.Px5.p1.1.m1.4.5.1.cmml">,</mo><mn id="S4.SS2.SSS0.Px5.p1.1.m1.4.4" xref="S4.SS2.SSS0.Px5.p1.1.m1.4.4.cmml">75</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px5.p1.1.m1.4b"><list id="S4.SS2.SSS0.Px5.p1.1.m1.4.5.1.cmml" xref="S4.SS2.SSS0.Px5.p1.1.m1.4.5.2"><cn type="integer" id="S4.SS2.SSS0.Px5.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px5.p1.1.m1.1.1">5</cn><cn type="integer" id="S4.SS2.SSS0.Px5.p1.1.m1.2.2.cmml" xref="S4.SS2.SSS0.Px5.p1.1.m1.2.2">25</cn><cn type="integer" id="S4.SS2.SSS0.Px5.p1.1.m1.3.3.cmml" xref="S4.SS2.SSS0.Px5.p1.1.m1.3.3">50</cn><cn type="integer" id="S4.SS2.SSS0.Px5.p1.1.m1.4.4.cmml" xref="S4.SS2.SSS0.Px5.p1.1.m1.4.4">75</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px5.p1.1.m1.4c">5,25,50,75</annotation></semantics></math> and <math id="S4.SS2.SSS0.Px5.p1.2.m2.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S4.SS2.SSS0.Px5.p1.2.m2.1a"><mn id="S4.SS2.SSS0.Px5.p1.2.m2.1.1" xref="S4.SS2.SSS0.Px5.p1.2.m2.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px5.p1.2.m2.1b"><cn type="integer" id="S4.SS2.SSS0.Px5.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS0.Px5.p1.2.m2.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px5.p1.2.m2.1c">100</annotation></semantics></math> percent of real images on top of the synthetic dataset generated in <a href="#S4.SS1" title="4.1 Object Detection ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.1</span></a>.
We observe strong performance gains compared to relying only on the same amount of real images or after applying cut-paste <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>, <em id="S4.SS2.SSS0.Px5.p1.2.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.SS2.SSS0.Px5.p1.2.2" class="ltx_text"></span> improving from 51.82 mAP to 68.38.
In <a href="#S4.SS4" title="4.4 Generalization to more tasks ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.4</span></a>, we show that such behavior is not limited to VOC dataset only.</p>
</div>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2309.05956/assets/x4.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="461" height="688" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S4.F8.3.2" class="ltx_text" style="font-size:90%;">Synthetic data distribution complements the real data distribution. Our foreground generation helps even more on the highly occluded classes.</span></figcaption>
</figure>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Synthetic data distribution complements the real data distribution.</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Our results demonstrate that large T2I synthesis models can be used to generate high-quality training data for several large-scale problems. In order to analyze the behavior of our approach, we look at the object detection scores on VOC before and after adding synthetic data to full real training data.
Further, in <a href="#S4.F8" title="In Mixing real data with synthetic data. ‣ 4.2 More Baselines and Ablation Studies ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a> (c), we calculate the per-class mAP and look at the relative improvement and overall scores.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">We made two crucial observations. Firstly, there’s a substantial relative increase in accuracy across all classes, with mAP50 values ranging from 30% to 100%. This suggests that the synthesized images significantly enhance the performance of downstream detection tasks. The marked improvement underscores how effectively the synthetic data distribution complements the real data distribution.
Secondly, to delve deeper into the synergy between synthetic and real data distributions, we examined the performance on a per-class basis. Notably, we witnessed marked enhancements in specific classes, such as the sofa, dining table, and chair.
We hypothesize that the significant improvement observed in indoor classes can be attributed to the generation of clean and unobstructed foreground objects using our approach. In real-world scenarios, these objects are typically occluded due to the presence of humans or other objects, as depicted in the first two rows of <a href="#S4.F8" title="In Mixing real data with synthetic data. ‣ 4.2 More Baselines and Ablation Studies ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a> (a). Consequently, training images may exhibit similar characteristics, with a high degree of occlusion. However, our approach enables the generation of a diverse set of clean objects, which supplements the quality of the training data from the real world, as illustrated in the remaining two rows of <a href="#S4.F8" title="In Mixing real data with synthetic data. ‣ 4.2 More Baselines and Ablation Studies ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a> (a).
This allows the training models to learn from both clean and occluded examples.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Qualitative examples and the final object detection results on the test set are presented in <a href="#S4.F8" title="In Mixing real data with synthetic data. ‣ 4.2 More Baselines and Ablation Studies ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a> (b), demonstrating that the model trained with a combination of synthetic and real data outperforms the model trained solely on real data, particularly for highly occluded categories.</p>
</div>
<figure id="S4.F9" class="ltx_figure"><img src="/html/2309.05956/assets/4_random_dalle_syn_new.png" id="S4.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="272" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S4.F9.3.2" class="ltx_text" style="font-size:90%;">
Pseudo-labeled synthetic images generated by our pipeline.</span></figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Generalization to more tasks</h3>

<section id="S4.SS4.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Instance Segmentation.</h5>

<div id="S4.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px1.p1.1" class="ltx_p">Our approach can generalize well to low-resource instance segmentation task on both Pascal VOC and COCO.
Following same settings as <a href="#S4.SS1" title="4.1 Object Detection ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.1</span></a>, in <a href="#S4.T8" title="In Instance Segmentation. ‣ 4.4 Generalization to more tasks ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">8</span></a>, we observe similar patterns across two datasets:
0 shot pure synthetic dataset yields strong performance while mixing real images further boosts the performance.</p>
</div>
<figure id="S4.T8" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.T8.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<table id="S4.T8.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T8.2.2.3.1" class="ltx_tr">
<th id="S4.T8.2.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S4.T8.2.2.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">#CDI</span></th>
<th id="S4.T8.2.2.3.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S4.T8.2.2.3.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Method</span></th>
<td id="S4.T8.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T8.2.2.3.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">mAP@50</span></td>
<td id="S4.T8.2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T8.2.2.3.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">mAP</span></td>
</tr>
<tr id="S4.T8.2.2.4.2" class="ltx_tr">
<th id="S4.T8.2.2.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">
<div id="S4.T8.2.2.4.2.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:4.5pt;height:5.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:4.5pt;transform:translate(0pt,0pt) rotate(-0deg) ;">
<p id="S4.T8.2.2.4.2.1.1.1" class="ltx_p"><span id="S4.T8.2.2.4.2.1.1.1.1" class="ltx_text" style="font-size:90%;">0</span></p>
</span></div>
</th>
<th id="S4.T8.2.2.4.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T8.2.2.4.2.2.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Pure Syn</span></th>
<td id="S4.T8.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T8.2.2.4.2.3.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">42.42</span></td>
<td id="S4.T8.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T8.2.2.4.2.4.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">22.38</span></td>
</tr>
<tr id="S4.T8.1.1.1" class="ltx_tr">
<th id="S4.T8.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="4"><span id="S4.T8.1.1.1.1.1" class="ltx_text" style="font-size:90%;">
<span id="S4.T8.1.1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:17.8pt;height:31pt;vertical-align:-6.6pt;"><span class="ltx_transformed_inner" style="width:31.1pt;transform:translate(-6.63pt,0pt) rotate(-90deg) ;">
<span id="S4.T8.1.1.1.1.1.1.1" class="ltx_inline-block ltx_align_center">
<span id="S4.T8.1.1.1.1.1.1.1.1" class="ltx_p"><math id="S4.T8.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="20\cdot 1" display="inline"><semantics id="S4.T8.1.1.1.1.1.1.1.1.m1.1a"><mrow id="S4.T8.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.T8.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mn id="S4.T8.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S4.T8.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">20</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T8.1.1.1.1.1.1.1.1.m1.1.1.1" xref="S4.T8.1.1.1.1.1.1.1.1.m1.1.1.1.cmml">⋅</mo><mn id="S4.T8.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S4.T8.1.1.1.1.1.1.1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T8.1.1.1.1.1.1.1.1.m1.1b"><apply id="S4.T8.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T8.1.1.1.1.1.1.1.1.m1.1.1"><ci id="S4.T8.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T8.1.1.1.1.1.1.1.1.m1.1.1.1">⋅</ci><cn type="integer" id="S4.T8.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T8.1.1.1.1.1.1.1.1.m1.1.1.2">20</cn><cn type="integer" id="S4.T8.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T8.1.1.1.1.1.1.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.1.1.1.1.1.1.1.1.m1.1c">20\cdot 1</annotation></semantics></math></span>
<span id="S4.T8.1.1.1.1.1.1.1.2" class="ltx_p">(1 shot)</span>
</span>
</span></span></span></th>
<th id="S4.T8.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">
<span id="S4.T8.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Pure Real </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T8.1.1.1.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a><span id="S4.T8.1.1.1.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T8.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T8.1.1.1.3.1" class="ltx_text" style="font-size:90%;">0.00</span></td>
<td id="S4.T8.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T8.1.1.1.4.1" class="ltx_text" style="font-size:90%;">0.00</span></td>
</tr>
<tr id="S4.T8.2.2.5.3" class="ltx_tr">
<th id="S4.T8.2.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T8.2.2.5.3.1.1" class="ltx_text" style="font-size:90%;">+ cut paste </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T8.2.2.5.3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a><span id="S4.T8.2.2.5.3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T8.2.2.5.3.2" class="ltx_td ltx_align_center"><span id="S4.T8.2.2.5.3.2.1" class="ltx_text" style="font-size:90%;">1.27</span></td>
<td id="S4.T8.2.2.5.3.3" class="ltx_td ltx_align_center"><span id="S4.T8.2.2.5.3.3.1" class="ltx_text" style="font-size:90%;">0.88</span></td>
</tr>
<tr id="S4.T8.2.2.6.4" class="ltx_tr">
<th id="S4.T8.2.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="background-color:#E6E6E6;"><span id="S4.T8.2.2.6.4.1.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn Fg</span></th>
<td id="S4.T8.2.2.6.4.2" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T8.2.2.6.4.2.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">42.23</span></td>
<td id="S4.T8.2.2.6.4.3" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T8.2.2.6.4.3.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">21.80</span></td>
</tr>
<tr id="S4.T8.2.2.7.5" class="ltx_tr">
<th id="S4.T8.2.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="background-color:#E6E6E6;"><span id="S4.T8.2.2.7.5.1.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn + real</span></th>
<td id="S4.T8.2.2.7.5.2" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T8.2.2.7.5.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#E6E6E6;">46.74<span id="S4.T8.2.2.7.5.2.1.1" class="ltx_text ltx_font_medium"> <span id="S4.T8.2.2.7.5.2.1.1.1" class="ltx_text" style="color:#FF0000;">(+45.47)</span></span></span></td>
<td id="S4.T8.2.2.7.5.3" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T8.2.2.7.5.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#E6E6E6;">24.81<span id="S4.T8.2.2.7.5.3.1.1" class="ltx_text ltx_font_medium"> <span id="S4.T8.2.2.7.5.3.1.1.1" class="ltx_text" style="color:#FF0000;">(+23.93)</span></span></span></td>
</tr>
<tr id="S4.T8.2.2.2" class="ltx_tr">
<th id="S4.T8.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" rowspan="4"><span id="S4.T8.2.2.2.1.1" class="ltx_text" style="font-size:90%;">
<span id="S4.T8.2.2.2.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:17.8pt;height:35.6pt;vertical-align:-8.9pt;"><span class="ltx_transformed_inner" style="width:35.6pt;transform:translate(-8.88pt,0pt) rotate(-90deg) ;">
<span id="S4.T8.2.2.2.1.1.1.1" class="ltx_inline-block ltx_align_center">
<span id="S4.T8.2.2.2.1.1.1.1.1" class="ltx_p"><math id="S4.T8.2.2.2.1.1.1.1.1.m1.1" class="ltx_Math" alttext="20\cdot 10" display="inline"><semantics id="S4.T8.2.2.2.1.1.1.1.1.m1.1a"><mrow id="S4.T8.2.2.2.1.1.1.1.1.m1.1.1" xref="S4.T8.2.2.2.1.1.1.1.1.m1.1.1.cmml"><mn id="S4.T8.2.2.2.1.1.1.1.1.m1.1.1.2" xref="S4.T8.2.2.2.1.1.1.1.1.m1.1.1.2.cmml">20</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T8.2.2.2.1.1.1.1.1.m1.1.1.1" xref="S4.T8.2.2.2.1.1.1.1.1.m1.1.1.1.cmml">⋅</mo><mn id="S4.T8.2.2.2.1.1.1.1.1.m1.1.1.3" xref="S4.T8.2.2.2.1.1.1.1.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T8.2.2.2.1.1.1.1.1.m1.1b"><apply id="S4.T8.2.2.2.1.1.1.1.1.m1.1.1.cmml" xref="S4.T8.2.2.2.1.1.1.1.1.m1.1.1"><ci id="S4.T8.2.2.2.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T8.2.2.2.1.1.1.1.1.m1.1.1.1">⋅</ci><cn type="integer" id="S4.T8.2.2.2.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T8.2.2.2.1.1.1.1.1.m1.1.1.2">20</cn><cn type="integer" id="S4.T8.2.2.2.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T8.2.2.2.1.1.1.1.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.2.2.2.1.1.1.1.1.m1.1c">20\cdot 10</annotation></semantics></math></span>
<span id="S4.T8.2.2.2.1.1.1.1.2" class="ltx_p">(10 shot)</span>
</span>
</span></span></span></th>
<th id="S4.T8.2.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">
<span id="S4.T8.2.2.2.2.1" class="ltx_text" style="font-size:90%;">Pure Real </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T8.2.2.2.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a><span id="S4.T8.2.2.2.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T8.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T8.2.2.2.3.1" class="ltx_text" style="font-size:90%;">5.21</span></td>
<td id="S4.T8.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T8.2.2.2.4.1" class="ltx_text" style="font-size:90%;">2.09</span></td>
</tr>
<tr id="S4.T8.2.2.8.6" class="ltx_tr">
<th id="S4.T8.2.2.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T8.2.2.8.6.1.1" class="ltx_text" style="font-size:90%;">+ cut paste </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T8.2.2.8.6.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a><span id="S4.T8.2.2.8.6.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T8.2.2.8.6.2" class="ltx_td ltx_align_center"><span id="S4.T8.2.2.8.6.2.1" class="ltx_text" style="font-size:90%;">24.50</span></td>
<td id="S4.T8.2.2.8.6.3" class="ltx_td ltx_align_center"><span id="S4.T8.2.2.8.6.3.1" class="ltx_text" style="font-size:90%;">9.24</span></td>
</tr>
<tr id="S4.T8.2.2.9.7" class="ltx_tr">
<th id="S4.T8.2.2.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="background-color:#E6E6E6;"><span id="S4.T8.2.2.9.7.1.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn Fg</span></th>
<td id="S4.T8.2.2.9.7.2" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T8.2.2.9.7.2.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">51.29</span></td>
<td id="S4.T8.2.2.9.7.3" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T8.2.2.9.7.3.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">29.11</span></td>
</tr>
<tr id="S4.T8.2.2.10.8" class="ltx_tr">
<th id="S4.T8.2.2.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="background-color:#E6E6E6;"><span id="S4.T8.2.2.10.8.1.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn + real</span></th>
<td id="S4.T8.2.2.10.8.2" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#E6E6E6;"><span id="S4.T8.2.2.10.8.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#E6E6E6;">55.19<span id="S4.T8.2.2.10.8.2.1.1" class="ltx_text ltx_font_medium"> <span id="S4.T8.2.2.10.8.2.1.1.1" class="ltx_text" style="color:#FF0000;">(+30.69)</span></span></span></td>
<td id="S4.T8.2.2.10.8.3" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#E6E6E6;"><span id="S4.T8.2.2.10.8.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#E6E6E6;">30.77<span id="S4.T8.2.2.10.8.3.1.1" class="ltx_text ltx_font_medium"> <span id="S4.T8.2.2.10.8.3.1.1.1" class="ltx_text" style="color:#FF0000;">(+21.53)</span></span></span></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T8.3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<table id="S4.T8.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T8.3.1.2.1" class="ltx_tr">
<th id="S4.T8.3.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"><span id="S4.T8.3.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">#CDI</span></th>
<th id="S4.T8.3.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"><span id="S4.T8.3.1.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Method</span></th>
<th id="S4.T8.3.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T8.3.1.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">mAP@50</span></th>
<th id="S4.T8.3.1.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T8.3.1.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">mAP</span></th>
</tr>
<tr id="S4.T8.3.1.3.2" class="ltx_tr">
<th id="S4.T8.3.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">
<div id="S4.T8.3.1.3.2.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:4.5pt;height:5.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:4.5pt;transform:translate(0pt,0pt) rotate(-0deg) ;">
<p id="S4.T8.3.1.3.2.1.1.1" class="ltx_p"><span id="S4.T8.3.1.3.2.1.1.1.1" class="ltx_text" style="font-size:90%;">0</span></p>
</span></div>
</th>
<th id="S4.T8.3.1.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T8.3.1.3.2.2.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Pure Syn</span></th>
<th id="S4.T8.3.1.3.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T8.3.1.3.2.3.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">15.04</span></th>
<th id="S4.T8.3.1.3.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T8.3.1.3.2.4.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">8.40</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T8.3.1.1" class="ltx_tr">
<th id="S4.T8.3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" rowspan="4"><span id="S4.T8.3.1.1.1.1" class="ltx_text" style="font-size:90%;">
<span id="S4.T8.3.1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:17.8pt;height:31pt;vertical-align:-6.6pt;"><span class="ltx_transformed_inner" style="width:31.1pt;transform:translate(-6.63pt,0pt) rotate(-90deg) ;">
<span id="S4.T8.3.1.1.1.1.1.1" class="ltx_inline-block ltx_align_center">
<span id="S4.T8.3.1.1.1.1.1.1.1" class="ltx_p"><math id="S4.T8.3.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="80\cdot 1" display="inline"><semantics id="S4.T8.3.1.1.1.1.1.1.1.m1.1a"><mrow id="S4.T8.3.1.1.1.1.1.1.1.m1.1.1" xref="S4.T8.3.1.1.1.1.1.1.1.m1.1.1.cmml"><mn id="S4.T8.3.1.1.1.1.1.1.1.m1.1.1.2" xref="S4.T8.3.1.1.1.1.1.1.1.m1.1.1.2.cmml">80</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T8.3.1.1.1.1.1.1.1.m1.1.1.1" xref="S4.T8.3.1.1.1.1.1.1.1.m1.1.1.1.cmml">⋅</mo><mn id="S4.T8.3.1.1.1.1.1.1.1.m1.1.1.3" xref="S4.T8.3.1.1.1.1.1.1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T8.3.1.1.1.1.1.1.1.m1.1b"><apply id="S4.T8.3.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T8.3.1.1.1.1.1.1.1.m1.1.1"><ci id="S4.T8.3.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T8.3.1.1.1.1.1.1.1.m1.1.1.1">⋅</ci><cn type="integer" id="S4.T8.3.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T8.3.1.1.1.1.1.1.1.m1.1.1.2">80</cn><cn type="integer" id="S4.T8.3.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T8.3.1.1.1.1.1.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.3.1.1.1.1.1.1.1.m1.1c">80\cdot 1</annotation></semantics></math></span>
<span id="S4.T8.3.1.1.1.1.1.1.2" class="ltx_p">(1 shot)</span>
</span>
</span></span></span></th>
<th id="S4.T8.3.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">
<span id="S4.T8.3.1.1.2.1" class="ltx_text" style="font-size:90%;">Pure Real </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T8.3.1.1.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a><span id="S4.T8.3.1.1.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T8.3.1.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T8.3.1.1.3.1" class="ltx_text" style="font-size:90%;">0.03</span></td>
<td id="S4.T8.3.1.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T8.3.1.1.4.1" class="ltx_text" style="font-size:90%;">0.00</span></td>
</tr>
<tr id="S4.T8.3.1.4.1" class="ltx_tr">
<th id="S4.T8.3.1.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T8.3.1.4.1.1.1" class="ltx_text" style="font-size:90%;">+ cut paste </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T8.3.1.4.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a><span id="S4.T8.3.1.4.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T8.3.1.4.1.2" class="ltx_td ltx_align_center"><span id="S4.T8.3.1.4.1.2.1" class="ltx_text" style="font-size:90%;">3.83</span></td>
<td id="S4.T8.3.1.4.1.3" class="ltx_td ltx_align_center"><span id="S4.T8.3.1.4.1.3.1" class="ltx_text" style="font-size:90%;">1.87</span></td>
</tr>
<tr id="S4.T8.3.1.5.2" class="ltx_tr">
<th id="S4.T8.3.1.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="background-color:#E6E6E6;"><span id="S4.T8.3.1.5.2.1.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn Fg</span></th>
<td id="S4.T8.3.1.5.2.2" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T8.3.1.5.2.2.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">15.10</span></td>
<td id="S4.T8.3.1.5.2.3" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T8.3.1.5.2.3.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">8.23</span></td>
</tr>
<tr id="S4.T8.3.1.6.3" class="ltx_tr">
<th id="S4.T8.3.1.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="background-color:#E6E6E6;"><span id="S4.T8.3.1.6.3.1.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">Syn + real</span></th>
<td id="S4.T8.3.1.6.3.2" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#E6E6E6;"><span id="S4.T8.3.1.6.3.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#E6E6E6;">17.56<span id="S4.T8.3.1.6.3.2.1.1" class="ltx_text ltx_font_medium"> <span id="S4.T8.3.1.6.3.2.1.1.1" class="ltx_text" style="color:#FF0000;">(+13.73)</span></span></span></td>
<td id="S4.T8.3.1.6.3.3" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#E6E6E6;"><span id="S4.T8.3.1.6.3.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;background-color:#E6E6E6;">9.12<span id="S4.T8.3.1.6.3.3.1.1" class="ltx_text ltx_font_medium"> <span id="S4.T8.3.1.6.3.3.1.1.1" class="ltx_text" style="color:#FF0000;">(+7.25)</span></span></span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Table 8: </span>Instance Segmentation for VOC (left) and COCO (above). Our methods generalize to other tasks and are competitive even in 1 shot.</figcaption>
</figure>
</div>
</div>
</figure>
</section>
<section id="S4.SS4.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Object Instance Detection.</h5>

<div id="S4.SS4.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px2.p1.1" class="ltx_p">We evaluate our method on object instance detection tasks using three benchmarks: GMU-Kitchen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>, Active Vision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>, and YCB-video datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite>.
For a fair comparison with <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>, we instead use the object instance masks provided with the datasets and <em id="S4.SS4.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">only synthesize backgrounds</em>.
In <a href="#S4.T10" title="In Object Instance Detection. ‣ 4.4 Generalization to more tasks ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">10</span></a> we compare our synthetic contextual backgrounds, generated from Ru-DALLE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>, with context images from UW dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite> following the setup of prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>.
Significant performance boosts indicate that our method is able to create congruent context images compared to real-world relevant images from public datasets.
Similar to <a href="#S4.F8" title="In Mixing real data with synthetic data. ‣ 4.2 More Baselines and Ablation Studies ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a>, we report per-category mAP on GMU-Kitchen in <a href="#Sx1.T12" title="In Appendix ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">12</span></a>.</p>
</div>
<div id="S4.SS4.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS4.SSS0.Px2.p2.3" class="ltx_p">Furthermore, similar to experiments done in <a href="#S4.SS2" title="4.2 More Baselines and Ablation Studies ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>, we investigate if the behavior of improved performance via blending real and synthetic can be transferred to this task.
In <a href="#S4.T9" title="In Object Instance Detection. ‣ 4.4 Generalization to more tasks ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">9</span></a> on the GMU kitchen, we
use all synthetic backgrounds, but with various percentages of the real training images.
We observe that using only a subset of real-world data (<math id="S4.SS4.SSS0.Px2.p2.1.m1.1" class="ltx_Math" alttext="70\%" display="inline"><semantics id="S4.SS4.SSS0.Px2.p2.1.m1.1a"><mrow id="S4.SS4.SSS0.Px2.p2.1.m1.1.1" xref="S4.SS4.SSS0.Px2.p2.1.m1.1.1.cmml"><mn id="S4.SS4.SSS0.Px2.p2.1.m1.1.1.2" xref="S4.SS4.SSS0.Px2.p2.1.m1.1.1.2.cmml">70</mn><mo id="S4.SS4.SSS0.Px2.p2.1.m1.1.1.1" xref="S4.SS4.SSS0.Px2.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px2.p2.1.m1.1b"><apply id="S4.SS4.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S4.SS4.SSS0.Px2.p2.1.m1.1.1"><csymbol cd="latexml" id="S4.SS4.SSS0.Px2.p2.1.m1.1.1.1.cmml" xref="S4.SS4.SSS0.Px2.p2.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS4.SSS0.Px2.p2.1.m1.1.1.2.cmml" xref="S4.SS4.SSS0.Px2.p2.1.m1.1.1.2">70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px2.p2.1.m1.1c">70\%</annotation></semantics></math>) with our synthesized images achieves better performance than full (100%) real-world data only. This suggests the advantages of our data generation approach saving the amount of human efforts required in labeling the real-world data significantly.
Further, we also observe that accuracy gradually improves from <math id="S4.SS4.SSS0.Px2.p2.2.m2.1" class="ltx_Math" alttext="78.3\%" display="inline"><semantics id="S4.SS4.SSS0.Px2.p2.2.m2.1a"><mrow id="S4.SS4.SSS0.Px2.p2.2.m2.1.1" xref="S4.SS4.SSS0.Px2.p2.2.m2.1.1.cmml"><mn id="S4.SS4.SSS0.Px2.p2.2.m2.1.1.2" xref="S4.SS4.SSS0.Px2.p2.2.m2.1.1.2.cmml">78.3</mn><mo id="S4.SS4.SSS0.Px2.p2.2.m2.1.1.1" xref="S4.SS4.SSS0.Px2.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px2.p2.2.m2.1b"><apply id="S4.SS4.SSS0.Px2.p2.2.m2.1.1.cmml" xref="S4.SS4.SSS0.Px2.p2.2.m2.1.1"><csymbol cd="latexml" id="S4.SS4.SSS0.Px2.p2.2.m2.1.1.1.cmml" xref="S4.SS4.SSS0.Px2.p2.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS4.SSS0.Px2.p2.2.m2.1.1.2.cmml" xref="S4.SS4.SSS0.Px2.p2.2.m2.1.1.2">78.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px2.p2.2.m2.1c">78.3\%</annotation></semantics></math> to <math id="S4.SS4.SSS0.Px2.p2.3.m3.1" class="ltx_Math" alttext="91.4\%" display="inline"><semantics id="S4.SS4.SSS0.Px2.p2.3.m3.1a"><mrow id="S4.SS4.SSS0.Px2.p2.3.m3.1.1" xref="S4.SS4.SSS0.Px2.p2.3.m3.1.1.cmml"><mn id="S4.SS4.SSS0.Px2.p2.3.m3.1.1.2" xref="S4.SS4.SSS0.Px2.p2.3.m3.1.1.2.cmml">91.4</mn><mo id="S4.SS4.SSS0.Px2.p2.3.m3.1.1.1" xref="S4.SS4.SSS0.Px2.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px2.p2.3.m3.1b"><apply id="S4.SS4.SSS0.Px2.p2.3.m3.1.1.cmml" xref="S4.SS4.SSS0.Px2.p2.3.m3.1.1"><csymbol cd="latexml" id="S4.SS4.SSS0.Px2.p2.3.m3.1.1.1.cmml" xref="S4.SS4.SSS0.Px2.p2.3.m3.1.1.1">percent</csymbol><cn type="float" id="S4.SS4.SSS0.Px2.p2.3.m3.1.1.2.cmml" xref="S4.SS4.SSS0.Px2.p2.3.m3.1.1.2">91.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px2.p2.3.m3.1c">91.4\%</annotation></semantics></math> as we increase the amount of real-world data.</p>
</div>
<figure id="S4.F10" class="ltx_figure"><img src="/html/2309.05956/assets/x5.png" id="S4.F10.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="131" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F10.4.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S4.F10.5.2" class="ltx_text" style="font-size:90%;">
Contextual backgrounds generated from user-provided CDI.
We note that even if the user provides <span id="S4.F10.5.2.1" class="ltx_text ltx_font_italic">as little as 1 CDI</span>, our approach can still generate large-scale coherent images.</span></figcaption>
</figure>
<figure id="S4.T9" class="ltx_table">
<table id="S4.T9.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T9.6.1.1" class="ltx_tr">
<th id="S4.T9.6.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Dataset</th>
<th id="S4.T9.6.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">CC</th>
<th id="S4.T9.6.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">CM</th>
<th id="S4.T9.6.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">HB</th>
<th id="S4.T9.6.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">HS</th>
<th id="S4.T9.6.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">MR</th>
<th id="S4.T9.6.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">NV1</th>
<th id="S4.T9.6.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">NV2</th>
<th id="S4.T9.6.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">PO</th>
<th id="S4.T9.6.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">PS</th>
<th id="S4.T9.6.1.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Pbbq</th>
<th id="S4.T9.6.1.1.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">RB</th>
<th id="S4.T9.6.1.1.13" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAP</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T9.6.2.1" class="ltx_tr">
<td id="S4.T9.6.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Syn (ours)</td>
<td id="S4.T9.6.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">79.0</td>
<td id="S4.T9.6.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">92.9</td>
<td id="S4.T9.6.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">90.4</td>
<td id="S4.T9.6.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">44.9</td>
<td id="S4.T9.6.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.0</td>
<td id="S4.T9.6.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">92.1</td>
<td id="S4.T9.6.2.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">88.0</td>
<td id="S4.T9.6.2.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.5</td>
<td id="S4.T9.6.2.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">64.1</td>
<td id="S4.T9.6.2.1.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">75.7</td>
<td id="S4.T9.6.2.1.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">80.2</td>
<td id="S4.T9.6.2.1.13" class="ltx_td ltx_align_center ltx_border_t">78.3</td>
</tr>
<tr id="S4.T9.6.3.2" class="ltx_tr">
<td id="S4.T9.6.3.2.1" class="ltx_td ltx_align_center ltx_border_r">100% Real</td>
<td id="S4.T9.6.3.2.2" class="ltx_td ltx_align_center ltx_border_r">81.9</td>
<td id="S4.T9.6.3.2.3" class="ltx_td ltx_align_center ltx_border_r">95.3</td>
<td id="S4.T9.6.3.2.4" class="ltx_td ltx_align_center ltx_border_r">92.0</td>
<td id="S4.T9.6.3.2.5" class="ltx_td ltx_align_center ltx_border_r">87.3</td>
<td id="S4.T9.6.3.2.6" class="ltx_td ltx_align_center ltx_border_r">86.5</td>
<td id="S4.T9.6.3.2.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T9.6.3.2.7.1" class="ltx_text ltx_font_bold">96.8</span></td>
<td id="S4.T9.6.3.2.8" class="ltx_td ltx_align_center ltx_border_r">88.9</td>
<td id="S4.T9.6.3.2.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T9.6.3.2.9.1" class="ltx_text ltx_font_bold">80.5</span></td>
<td id="S4.T9.6.3.2.10" class="ltx_td ltx_align_center ltx_border_r">92.3</td>
<td id="S4.T9.6.3.2.11" class="ltx_td ltx_align_center ltx_border_r">88.9</td>
<td id="S4.T9.6.3.2.12" class="ltx_td ltx_align_center ltx_border_r">58.6</td>
<td id="S4.T9.6.3.2.13" class="ltx_td ltx_align_center">86.3</td>
</tr>
<tr id="S4.T9.6.4.3" class="ltx_tr">
<td id="S4.T9.6.4.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Syn (ours) + 10% Real</td>
<td id="S4.T9.6.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">90.5</td>
<td id="S4.T9.6.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">96.9</td>
<td id="S4.T9.6.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">93.2</td>
<td id="S4.T9.6.4.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">74.0</td>
<td id="S4.T9.6.4.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">60.4</td>
<td id="S4.T9.6.4.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">90.7</td>
<td id="S4.T9.6.4.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">86.5</td>
<td id="S4.T9.6.4.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">48.7</td>
<td id="S4.T9.6.4.3.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">97.7</td>
<td id="S4.T9.6.4.3.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">86.4</td>
<td id="S4.T9.6.4.3.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">72.1</td>
<td id="S4.T9.6.4.3.13" class="ltx_td ltx_align_center ltx_border_t">81.6</td>
</tr>
<tr id="S4.T9.6.5.4" class="ltx_tr">
<td id="S4.T9.6.5.4.1" class="ltx_td ltx_align_center ltx_border_r">Syn (ours) + 40% Real</td>
<td id="S4.T9.6.5.4.2" class="ltx_td ltx_align_center ltx_border_r">91.8</td>
<td id="S4.T9.6.5.4.3" class="ltx_td ltx_align_center ltx_border_r">97.4</td>
<td id="S4.T9.6.5.4.4" class="ltx_td ltx_align_center ltx_border_r">94.5</td>
<td id="S4.T9.6.5.4.5" class="ltx_td ltx_align_center ltx_border_r">84.9</td>
<td id="S4.T9.6.5.4.6" class="ltx_td ltx_align_center ltx_border_r">75.1</td>
<td id="S4.T9.6.5.4.7" class="ltx_td ltx_align_center ltx_border_r">90.7</td>
<td id="S4.T9.6.5.4.8" class="ltx_td ltx_align_center ltx_border_r">78.6</td>
<td id="S4.T9.6.5.4.9" class="ltx_td ltx_align_center ltx_border_r">52.1</td>
<td id="S4.T9.6.5.4.10" class="ltx_td ltx_align_center ltx_border_r">96.9</td>
<td id="S4.T9.6.5.4.11" class="ltx_td ltx_align_center ltx_border_r">87.6</td>
<td id="S4.T9.6.5.4.12" class="ltx_td ltx_align_center ltx_border_r">77.9</td>
<td id="S4.T9.6.5.4.13" class="ltx_td ltx_align_center">84.3</td>
</tr>
<tr id="S4.T9.6.6.5" class="ltx_tr">
<td id="S4.T9.6.6.5.1" class="ltx_td ltx_align_center ltx_border_r">Syn (ours) + 70% Real</td>
<td id="S4.T9.6.6.5.2" class="ltx_td ltx_align_center ltx_border_r">92.7</td>
<td id="S4.T9.6.6.5.3" class="ltx_td ltx_align_center ltx_border_r">98.2</td>
<td id="S4.T9.6.6.5.4" class="ltx_td ltx_align_center ltx_border_r">95.2</td>
<td id="S4.T9.6.6.5.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T9.6.6.5.5.1" class="ltx_text ltx_font_bold">90.9</span></td>
<td id="S4.T9.6.6.5.6" class="ltx_td ltx_align_center ltx_border_r">88.0</td>
<td id="S4.T9.6.6.5.7" class="ltx_td ltx_align_center ltx_border_r">93.1</td>
<td id="S4.T9.6.6.5.8" class="ltx_td ltx_align_center ltx_border_r">89.7</td>
<td id="S4.T9.6.6.5.9" class="ltx_td ltx_align_center ltx_border_r">50.3</td>
<td id="S4.T9.6.6.5.10" class="ltx_td ltx_align_center ltx_border_r">97.6</td>
<td id="S4.T9.6.6.5.11" class="ltx_td ltx_align_center ltx_border_r">92.2</td>
<td id="S4.T9.6.6.5.12" class="ltx_td ltx_align_center ltx_border_r">78.3</td>
<td id="S4.T9.6.6.5.13" class="ltx_td ltx_align_center">87.9</td>
</tr>
<tr id="S4.T9.6.7.6" class="ltx_tr">
<td id="S4.T9.6.7.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Syn (ours) + 100% Real</td>
<td id="S4.T9.6.7.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T9.6.7.6.2.1" class="ltx_text ltx_font_bold">94.4</span></td>
<td id="S4.T9.6.7.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T9.6.7.6.3.1" class="ltx_text ltx_font_bold">98.2</span></td>
<td id="S4.T9.6.7.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T9.6.7.6.4.1" class="ltx_text ltx_font_bold">95.2</span></td>
<td id="S4.T9.6.7.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">90.7</td>
<td id="S4.T9.6.7.6.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T9.6.7.6.6.1" class="ltx_text ltx_font_bold">92.5</span></td>
<td id="S4.T9.6.7.6.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">94.1</td>
<td id="S4.T9.6.7.6.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T9.6.7.6.8.1" class="ltx_text ltx_font_bold">93.0</span></td>
<td id="S4.T9.6.7.6.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">72.8</td>
<td id="S4.T9.6.7.6.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T9.6.7.6.10.1" class="ltx_text ltx_font_bold">98.3</span></td>
<td id="S4.T9.6.7.6.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T9.6.7.6.11.1" class="ltx_text ltx_font_bold">98.7</span></td>
<td id="S4.T9.6.7.6.12" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T9.6.7.6.12.1" class="ltx_text ltx_font_bold">79.8</span></td>
<td id="S4.T9.6.7.6.13" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T9.6.7.6.13.1" class="ltx_text ltx_font_bold">91.4</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T9.7.3.1" class="ltx_text" style="font-size:90%;">Table 9</span>: </span><span id="S4.T9.4.2" class="ltx_text" style="font-size:90%;">We highlight that our synthesized data together with 70 <math id="S4.T9.3.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.T9.3.1.m1.1b"><mo id="S4.T9.3.1.m1.1.1" xref="S4.T9.3.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.T9.3.1.m1.1c"><csymbol cd="latexml" id="S4.T9.3.1.m1.1.1.cmml" xref="S4.T9.3.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.3.1.m1.1d">\%</annotation></semantics></math> amount of real data achieves better performance than full (100 <math id="S4.T9.4.2.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.T9.4.2.m2.1b"><mo id="S4.T9.4.2.m2.1.1" xref="S4.T9.4.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.T9.4.2.m2.1c"><csymbol cd="latexml" id="S4.T9.4.2.m2.1.1.cmml" xref="S4.T9.4.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.4.2.m2.1d">\%</annotation></semantics></math>) set of real data only. This highlights the benefit of our approach in reducing total human efforts. Syn (ours) means ru-DALLE synthesized 1500 diverse images (use UW as CDI). Top row terms are: CC: Coca Cola, CM: Coffee mate, HB: honey bunches, HS: hunt’s sauce, MR: mahatma rice, NV1: nature V1, NV2: nature V2, PO: palmolive orange, PS: pop secret, Pbbq: pringles bbg, RB: red bull.</span></figcaption>
</figure>
<figure id="S4.T10" class="ltx_table">
<table id="S4.T10.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T10.2.1.1" class="ltx_tr">
<th id="S4.T10.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S4.T10.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Dataset</span></th>
<th id="S4.T10.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S4.T10.2.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">GMU</span></th>
<th id="S4.T10.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S4.T10.2.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Active Vision</span></th>
<th id="S4.T10.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T10.2.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">YCB-video</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T10.2.2.1" class="ltx_tr">
<td id="S4.T10.2.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T10.2.2.1.1.1" class="ltx_text" style="font-size:90%;">UW-Kitchen</span></td>
<td id="S4.T10.2.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T10.2.2.1.2.1" class="ltx_text" style="font-size:90%;">76.1</span></td>
<td id="S4.T10.2.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T10.2.2.1.3.1" class="ltx_text" style="font-size:90%;">22.6</span></td>
<td id="S4.T10.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T10.2.2.1.4.1" class="ltx_text" style="font-size:90%;">38.3</span></td>
</tr>
<tr id="S4.T10.2.3.2" class="ltx_tr">
<td id="S4.T10.2.3.2.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T10.2.3.2.1.1" class="ltx_text" style="font-size:90%;">DALL-E (ours)</span></td>
<td id="S4.T10.2.3.2.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T10.2.3.2.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">80.1<sub id="S4.T10.2.3.2.2.1.1" class="ltx_sub"><span id="S4.T10.2.3.2.2.1.1.1" class="ltx_text ltx_font_medium" style="color:#FF0000;">+4.0</span></sub></span></td>
<td id="S4.T10.2.3.2.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T10.2.3.2.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">25.8<sub id="S4.T10.2.3.2.3.1.1" class="ltx_sub"><span id="S4.T10.2.3.2.3.1.1.1" class="ltx_text ltx_font_medium" style="color:#FF0000;">+3.2</span></sub></span></td>
<td id="S4.T10.2.3.2.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T10.2.3.2.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">45.5<sub id="S4.T10.2.3.2.4.1.1" class="ltx_sub"><span id="S4.T10.2.3.2.4.1.1.1" class="ltx_text ltx_font_medium" style="color:#FF0000;">+7.2</span></sub></span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T10.3.1.1" class="ltx_text" style="font-size:90%;">Table 10</span>: </span><span id="S4.T10.4.2" class="ltx_text" style="font-size:90%;">
Contextual synthetic backgrounds produced by our approach significantly enhance object instance detection accuracy across three datasets.
</span></figcaption>
</figure>
</section>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Compositionality in Synthetic Dataset</h3>

<figure id="S4.T11" class="ltx_table">
<div id="S4.T11.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:137.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(75.1pt,-23.9pt) scale(1.53042057252769,1.53042057252769) ;">
<table id="S4.T11.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T11.2.1.1.1" class="ltx_tr">
<th id="S4.T11.2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S4.T11.2.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Dataset</span></th>
<th id="S4.T11.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S4.T11.2.1.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Only CDI</span></th>
<th id="S4.T11.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S4.T11.2.1.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">No Intervention</span></th>
<th id="S4.T11.2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T11.2.1.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">After Intervention</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T11.2.1.2.1" class="ltx_tr">
<td id="S4.T11.2.1.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T11.2.1.2.1.1.1" class="ltx_text" style="font-size:80%;">Cartoon Kitchen</span></td>
<td id="S4.T11.2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T11.2.1.2.1.2.1" class="ltx_text" style="font-size:80%;">11.2</span></td>
<td id="S4.T11.2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T11.2.1.2.1.3.1" class="ltx_text" style="font-size:80%;">70.0</span></td>
<td id="S4.T11.2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T11.2.1.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">76.7<sub id="S4.T11.2.1.2.1.4.1.1" class="ltx_sub"><span id="S4.T11.2.1.2.1.4.1.1.1" class="ltx_text ltx_font_medium" style="color:#FF0000;">+6.7</span></sub></span></td>
</tr>
<tr id="S4.T11.2.1.3.2" class="ltx_tr">
<td id="S4.T11.2.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T11.2.1.3.2.1.1" class="ltx_text" style="font-size:80%;">Skeleton Kitchen</span></td>
<td id="S4.T11.2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T11.2.1.3.2.2.1" class="ltx_text" style="font-size:80%;">10.3</span></td>
<td id="S4.T11.2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T11.2.1.3.2.3.1" class="ltx_text" style="font-size:80%;">64.6</span></td>
<td id="S4.T11.2.1.3.2.4" class="ltx_td ltx_align_center"><span id="S4.T11.2.1.3.2.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">74.8<sub id="S4.T11.2.1.3.2.4.1.1" class="ltx_sub"><span id="S4.T11.2.1.3.2.4.1.1.1" class="ltx_text ltx_font_medium" style="color:#FF0000;">+10.2</span></sub></span></td>
</tr>
<tr id="S4.T11.2.1.4.3" class="ltx_tr">
<td id="S4.T11.2.1.4.3.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T11.2.1.4.3.1.1" class="ltx_text" style="font-size:80%;">Objects in Kitchen</span></td>
<td id="S4.T11.2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T11.2.1.4.3.2.1" class="ltx_text" style="font-size:80%;">9.4</span></td>
<td id="S4.T11.2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T11.2.1.4.3.3.1" class="ltx_text" style="font-size:80%;">71.8</span></td>
<td id="S4.T11.2.1.4.3.4" class="ltx_td ltx_align_center"><span id="S4.T11.2.1.4.3.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">77.0<sub id="S4.T11.2.1.4.3.4.1.1" class="ltx_sub"><span id="S4.T11.2.1.4.3.4.1.1.1" class="ltx_text ltx_font_medium" style="color:#FF0000;">+5.2</span></sub></span></td>
</tr>
<tr id="S4.T11.2.1.5.4" class="ltx_tr">
<td id="S4.T11.2.1.5.4.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T11.2.1.5.4.1.1" class="ltx_text" style="font-size:80%;">Kitchens with Human</span></td>
<td id="S4.T11.2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T11.2.1.5.4.2.1" class="ltx_text" style="font-size:80%;">10.2</span></td>
<td id="S4.T11.2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T11.2.1.5.4.3.1" class="ltx_text" style="font-size:80%;">70.9</span></td>
<td id="S4.T11.2.1.5.4.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T11.2.1.5.4.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">76.9<sub id="S4.T11.2.1.5.4.4.1.1" class="ltx_sub"><span id="S4.T11.2.1.5.4.4.1.1.1" class="ltx_text ltx_font_medium" style="color:#FF0000;">+6.0</span></sub></span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T11.3.1.1" class="ltx_text" style="font-size:90%;">Table 11</span>: </span><span id="S4.T11.4.2" class="ltx_text" style="font-size:90%;">
Even if the user provides out-of-distribution CDI, our approach is able to produce a synthetic dataset tailored towards actual test distribution by in-domain intervention.
</span></figcaption>
</figure>
<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">As mentioned in <a href="#S3.SS2" title="3.2 Language-driven Context Synthesis ‣ 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>, the compositional nature of our language-based context image generation allows us to <span id="S4.SS5.p1.1.1" class="ltx_text ltx_font_italic">remove</span> noisy information, <span id="S4.SS5.p1.1.2" class="ltx_text ltx_font_italic">add</span> relevant but missing information from the original textual description of the CDIs, or <span id="S4.SS5.p1.1.3" class="ltx_text ltx_font_italic">change the style</span> of the received CDI to a more desired one.
For instance, the language description of a kitchen with people present in it may contain “people” as a distractor that may hamper the quality of the generated images and negatively affect the accuracy.
Using our pipeline, we can remove the distractor by detecting and removing it (“people”) from the caption before feeding them into T2I (<a href="#S3.F3" title="In 3 Method ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<div id="S4.SS5.p2" class="ltx_para ltx_noindent">
<p id="S4.SS5.p2.1" class="ltx_p"><span id="S4.SS5.p2.1.1" class="ltx_text ltx_font_bold">Contextual backgrounds from only one CDI.</span>
To demonstrate the compositionality of our approach, we extend the experimental setting from Object Instance Detection in <a href="#S4.SS2" title="4.2 More Baselines and Ablation Studies ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>: focusing on contextual background generation from <em id="S4.SS5.p2.1.2" class="ltx_emph ltx_font_italic">only one</em> CDI on which GMU-Kitchen ground-truth objects are pasted.
In this case, CDI means the input image, but <em id="S4.SS5.p2.1.3" class="ltx_emph ltx_font_italic">no longer in-domain</em>.
We consider 4 scenarios (Cartoon kitchen, Skeleton kitchen, objects in Kitchen and Kitchen with human), where provided CDIs are <em id="S4.SS5.p2.1.4" class="ltx_emph ltx_font_italic">out of distribution</em> from the target domain, which is a real-world kitchen without humans.</p>
</div>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.1" class="ltx_p">There are two main challenges to (1) the conventional method struggles to learn effectively with as few as one training image (2) the provided training image is out-of-domain.
In <a href="#S4.T11" title="In 4.5 Compositionality in Synthetic Dataset ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">11</span></a>,
we demonstrate that our synthetic data generation pipeline can address both of these challenges.
Specifically, when training solely on a synthetic dataset constructed by pasting objects onto a single CDI (<span id="S4.SS5.p3.1.1" class="ltx_text ltx_font_typewriter">Only CDI</span>), performance across various settings is suboptimal.
This underscores the inherent difficulties the model faces when learning from limited images and a narrow diversity.
However,
our methodology can produce a vast collection of high-quality backgrounds that are contextually relevant.
The superior performance of the <span id="S4.SS5.p3.1.2" class="ltx_text ltx_font_typewriter">No Intervention</span> results compared to the <span id="S4.SS5.p3.1.3" class="ltx_text ltx_font_typewriter">Only CDI</span> results substantiates our hypothesis.
Qualitative results are presented in <a href="#S4.F10" title="In Object Instance Detection. ‣ 4.4 Generalization to more tasks ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">10</span></a> as additional empirical support that T2I is able to generate many relevant images from a single CDI using our approach.</p>
</div>
<div id="S4.SS5.p4" class="ltx_para ltx_noindent">
<p id="S4.SS5.p4.1" class="ltx_p"><span id="S4.SS5.p4.1.1" class="ltx_text ltx_font_bold">Mitigate domain gap via language compositionality.</span>
While directly applying our approach leads to a significant performance improvement across all four scenarios, there remains room for enhancement due to the existing domain gap. Given that the contextual backgrounds are generated using augmented captions, <em id="S4.SS5.p4.1.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS5.p4.1.3" class="ltx_text"></span> operating in the language space, in-domain intervention becomes feasible.
Specifically, we have the flexibility to add, remove, or alter contextual words, thereby influencing the style of the generated images. Consider the following interventions for each of the four scenarios we tested:</p>
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p">(<a href="#Sx1.F15" title="In Appendix ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">15</span></a>) we substitute the word “cartoon” with “real” to produce real kitchen images to better match test-time distribution.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p">(<a href="#Sx1.F16" title="In Appendix ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">16</span></a>) the word “sketch image” is replaced with “real.”</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p id="S4.I2.i3.p1.1" class="ltx_p">(<a href="#Sx1.F17" title="In Appendix ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">17</span></a>) user-provided CDI only contains the kitchen object, which makes the generated images focus on the specific objects predominantly. To align more closely with the test-time distribution, we add the word “a kitchen of.”</p>
</div>
</li>
<li id="S4.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i4.p1" class="ltx_para">
<p id="S4.I2.i4.p1.1" class="ltx_p">(<a href="#Sx1.F18" title="In Appendix ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">18</span></a>) as the test-time distribution involves real kitchen images without human presence, we remove words such as “a couple of people,” and add “without people.”</p>
</div>
</li>
</ul>
<p id="S4.SS5.p4.2" class="ltx_p">In <a href="#S4.T11" title="In 4.5 Compositionality in Synthetic Dataset ‣ 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">11</span></a> we observe up to 10.2 performance gain of <span id="S4.SS5.p4.2.1" class="ltx_text ltx_font_typewriter">After Intervention</span> over <span id="S4.SS5.p4.2.2" class="ltx_text ltx_font_typewriter">No Intervention</span>, which demonstrates the effectiveness of intervention in bridging the domain gap.
Such interventions ensure a closer alignment between the synthesis distribution and the desired test distribution.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We have proposed a new paradigm to generate large-scale labeled data for object detection and segmentation tasks using large vision and language-based text-to-image synthesis frameworks. We demonstrate effortless labeled data generation on popular benchmarks for object detection tasks. Computer vision models trained using these data improve the performance over the models trained with large real data. Thus reducing the need for expensive human labeling process. We also highlight the compositional nature of our data generation approach on out-of-distribution and zero-shot data generation scenarios. We believe our approach opens door to democratizing computer vision models.

<br class="ltx_break"><span id="S5.p1.1.1" class="ltx_text ltx_font_bold">Limitations.</span> Since we rely on T2I synthesis models for data generation, we are limited by two issues. First, our approach does not provide control for illumination, viewpoints, object pose and other such data generation properties. Second, our current approach can not generate labelled data for 3D geometry tasks like 3D object pose estimation tasks. We leave these problems as interesting future works.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Acknowledgements</span>
This work was supported in part by Amazon ML Fellowship, C-BRIC (one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA), DARPA (HR00112190134) and the Army Research Office (W911NF2020053). The authors affirm that the views expressed herein are solely their own, and do not represent the views of the United States government or any agency thereof.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.2.2.1" class="ltx_text" style="font-size:90%;">[1]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.4.1" class="ltx_text" style="font-size:90%;">
sberbank-ai/ru-dalle: Generate images from texts. in russian.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/sberbank-ai/ru-dalle" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/sberbank-ai/ru-dalle</a><span id="bib.bib1.5.1" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.6.1" class="ltx_text" style="font-size:90%;">(Accessed on 03/07/2022).
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.2.2.1" class="ltx_text" style="font-size:90%;">[2]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.4.1" class="ltx_text" style="font-size:90%;">
Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.5.1" class="ltx_text" style="font-size:90%;">Don’t just assume; look and answer: Overcoming priors for visual
question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib2.8.3" class="ltx_text" style="font-size:90%;">, pages 4971–4980, 2018.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.2.2.1" class="ltx_text" style="font-size:90%;">[3]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.4.1" class="ltx_text" style="font-size:90%;">
Phil Ammirato, Patrick Poirson, Eunbyung Park, Jana Košecká, and
Alexander C Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.5.1" class="ltx_text" style="font-size:90%;">A dataset for developing and benchmarking active vision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 IEEE International Conference on Robotics and Automation
(ICRA)</span><span id="bib.bib3.8.3" class="ltx_text" style="font-size:90%;">, pages 1378–1385. IEEE, 2017.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.2.2.1" class="ltx_text" style="font-size:90%;">[4]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.4.1" class="ltx_text" style="font-size:90%;">
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen
Gould, and Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.5.1" class="ltx_text" style="font-size:90%;">Bottom-up and top-down attention for image captioning and visual
question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib4.8.3" class="ltx_text" style="font-size:90%;">, pages 6077–6086, 2018.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.2.2.1" class="ltx_text" style="font-size:90%;">[5]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.4.1" class="ltx_text" style="font-size:90%;">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C Lawrence Zitnick, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.5.1" class="ltx_text" style="font-size:90%;">Vqa: Visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib5.8.3" class="ltx_text" style="font-size:90%;">, pages 2425–2433, 2015.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.2.2.1" class="ltx_text" style="font-size:90%;">[6]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.4.1" class="ltx_text" style="font-size:90%;">
Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten
Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras,
and Ming-Yu Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.5.1" class="ltx_text" style="font-size:90%;">ediff-i: Text-to-image diffusion models with an ensemble of expert
denoisers, 2022.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.2.2.1" class="ltx_text" style="font-size:90%;">[7]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.4.1" class="ltx_text" style="font-size:90%;">
Christopher Bowles, Liang Chen, Ricardo Guerrero, Paul Bentley, Roger Gunn,
Alexander Hammers, David Alexander Dickie, Maria Valdés Hernández,
Joanna Wardlaw, and Daniel Rueckert.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.5.1" class="ltx_text" style="font-size:90%;">Gan augmentation: Augmenting training data using generative
adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1810.10863</span><span id="bib.bib7.7.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.2.2.1" class="ltx_text" style="font-size:90%;">[8]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.4.1" class="ltx_text" style="font-size:90%;">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.5.1" class="ltx_text" style="font-size:90%;">Language models are few-shot learners.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib8.7.2" class="ltx_text" style="font-size:90%;">,
33:1877–1901, 2020.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.2.2.1" class="ltx_text" style="font-size:90%;">[9]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.4.1" class="ltx_text" style="font-size:90%;">
Berk Calli, Arjun Singh, Aaron Walsman, Siddhartha Srinivasa, Pieter Abbeel,
and Aaron M Dollar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.5.1" class="ltx_text" style="font-size:90%;">The ycb object and model set: Towards common benchmarks for
manipulation research.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2015 international conference on advanced robotics (ICAR)</span><span id="bib.bib9.8.3" class="ltx_text" style="font-size:90%;">,
pages 510–517. IEEE, 2015.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.2.2.1" class="ltx_text" style="font-size:90%;">[10]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.4.1" class="ltx_text" style="font-size:90%;">
Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal,
Piotr Bojanowski, and Armand Joulin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.5.1" class="ltx_text" style="font-size:90%;">Emerging properties in self-supervised vision transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on
computer vision</span><span id="bib.bib10.8.3" class="ltx_text" style="font-size:90%;">, pages 9650–9660, 2021.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.2.2.1" class="ltx_text" style="font-size:90%;">[11]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.4.1" class="ltx_text" style="font-size:90%;">
Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang,
Ming-Hsuan Yang, Kevin Murphy, William T. Freeman, Michael Rubinstein,
Yuanzhen Li, and Dilip Krishnan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.5.1" class="ltx_text" style="font-size:90%;">Muse: Text-to-image generation via masked generative transformers,
2023.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.2.2.1" class="ltx_text" style="font-size:90%;">[12]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.4.1" class="ltx_text" style="font-size:90%;">
Guowei Chen, Yi Liu, Jian Wang, Juncai Peng, Yuying Hao, Lutao Chu, Shiyu Tang,
Zewu Wu, Zeyu Chen, Zhiliang Yu, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.5.1" class="ltx_text" style="font-size:90%;">Pp-matting: High-accuracy natural image matting.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2204.09433</span><span id="bib.bib12.7.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.2.2.1" class="ltx_text" style="font-size:90%;">[13]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.4.1" class="ltx_text" style="font-size:90%;">
Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and
Dhruv Batra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.5.1" class="ltx_text" style="font-size:90%;">Embodied question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib13.8.3" class="ltx_text" style="font-size:90%;">, pages 1–10, 2018.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.2.2.1" class="ltx_text" style="font-size:90%;">[14]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.4.1" class="ltx_text" style="font-size:90%;">
Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang
Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.5.1" class="ltx_text" style="font-size:90%;">Cogview: Mastering text-to-image generation via transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib14.7.2" class="ltx_text" style="font-size:90%;">, 34, 2021.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.2.2.1" class="ltx_text" style="font-size:90%;">[15]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.4.1" class="ltx_text" style="font-size:90%;">
Santosh K Divvala, Derek Hoiem, James H Hays, Alexei A Efros, and Martial
Hebert.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.5.1" class="ltx_text" style="font-size:90%;">An empirical study of context in object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2009 IEEE Conference on computer vision and Pattern
Recognition</span><span id="bib.bib15.8.3" class="ltx_text" style="font-size:90%;">, pages 1271–1278. IEEE, 2009.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.2.2.1" class="ltx_text" style="font-size:90%;">[16]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.4.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas,
Vladimir Golkov, Patrick van der Smagt, Daniel Cremers, and Thomas Brox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.5.1" class="ltx_text" style="font-size:90%;">Flownet: Learning optical flow with convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib16.8.3" class="ltx_text" style="font-size:90%;">, December 2015.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.2.2.1" class="ltx_text" style="font-size:90%;">[17]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.4.1" class="ltx_text" style="font-size:90%;">
Nikita Dvornik, Julien Mairal, and Cordelia Schmid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.5.1" class="ltx_text" style="font-size:90%;">Modeling visual context is key to augmenting object detection
datasets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV)</span><span id="bib.bib17.8.3" class="ltx_text" style="font-size:90%;">, pages 364–380, 2018.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.2.2.1" class="ltx_text" style="font-size:90%;">[18]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.4.1" class="ltx_text" style="font-size:90%;">
Debidatta Dwibedi, Ishan Misra, and Martial Hebert.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.5.1" class="ltx_text" style="font-size:90%;">Cut, paste and learn: Surprisingly easy synthesis for instance
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib18.8.3" class="ltx_text" style="font-size:90%;">, pages 1301–1310, 2017.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.2.2.1" class="ltx_text" style="font-size:90%;">[19]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.4.1" class="ltx_text" style="font-size:90%;">
Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew
Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.5.1" class="ltx_text" style="font-size:90%;">The pascal visual object classes (voc) challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International journal of computer vision</span><span id="bib.bib19.7.2" class="ltx_text" style="font-size:90%;">, 88(2):303–338, 2010.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.2.2.1" class="ltx_text" style="font-size:90%;">[20]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.4.1" class="ltx_text" style="font-size:90%;">
Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun
Huang, Xinlong Wang, and Yue Cao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.5.1" class="ltx_text" style="font-size:90%;">Eva: Exploring the limits of masked visual representation learning at
scale.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib20.8.3" class="ltx_text" style="font-size:90%;">, pages 19358–19369, 2023.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.2.2.1" class="ltx_text" style="font-size:90%;">[21]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.4.1" class="ltx_text" style="font-size:90%;">
Yunhao Ge, Sami Abu-El-Haija, Gan Xin, and Laurent Itti.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.5.1" class="ltx_text" style="font-size:90%;">Zero-shot synthesis with group-supervised learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2009.06586</span><span id="bib.bib21.7.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.2.2.1" class="ltx_text" style="font-size:90%;">[22]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.4.1" class="ltx_text" style="font-size:90%;">
Yunhao Ge, Harkirat Behl, Jiashu Xu, Suriya Gunasekar, Neel Joshi, Yale Song,
Xin Wang, Laurent Itti, and Vibhav Vineet.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.5.1" class="ltx_text" style="font-size:90%;">Neural-sim: Learning to generate training data with nerf.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.6.1" class="ltx_text" style="font-size:90%;">In Shai Avidan, Gabriel Brostow, Moustapha Cissé, Giovanni Maria
Farinella, and Tal Hassner, editors, </span><span id="bib.bib22.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision – ECCV 2022</span><span id="bib.bib22.8.3" class="ltx_text" style="font-size:90%;">,
pages 477–493, Cham, 2022. Springer Nature Switzerland.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.2.2.1" class="ltx_text" style="font-size:90%;">[23]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.4.1" class="ltx_text" style="font-size:90%;">
Yunhao Ge, Jiashu Xu, Brian Nlong Zhao, Neel Joshi, Laurent Itti, and Vibhav
Vineet.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.5.1" class="ltx_text" style="font-size:90%;">Dall-e for detection: Language-driven compositional image synthesis
for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv: 2206.09592</span><span id="bib.bib23.7.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.2.2.1" class="ltx_text" style="font-size:90%;">[24]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.4.1" class="ltx_text" style="font-size:90%;">
Yunhao Ge, Jiaping Zhao, and Laurent Itti.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.5.1" class="ltx_text" style="font-size:90%;">Pose augmentation: Class-agnostic object pose transformation for
object recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib24.8.3" class="ltx_text" style="font-size:90%;">, pages 138–155.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.2.2.1" class="ltx_text" style="font-size:90%;">[25]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.4.1" class="ltx_text" style="font-size:90%;">
Georgios Georgakis, Md Alimoor Reza, Arsalan Mousavian, Phi-Hung Le, and Jana
Košecká.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.5.1" class="ltx_text" style="font-size:90%;">Multiview rgb-d dataset for object instance detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2016 Fourth International Conference on 3D Vision (3DV)</span><span id="bib.bib25.8.3" class="ltx_text" style="font-size:90%;">,
pages 426–434. IEEE, 2016.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.2.2.1" class="ltx_text" style="font-size:90%;">[26]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.4.1" class="ltx_text" style="font-size:90%;">
Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk,
Quoc V Le, and Barret Zoph.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.5.1" class="ltx_text" style="font-size:90%;">Simple copy-paste is a strong data augmentation method for instance
segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib26.8.3" class="ltx_text" style="font-size:90%;">, pages 2918–2928, 2021.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.2.2.1" class="ltx_text" style="font-size:90%;">[27]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.4.1" class="ltx_text" style="font-size:90%;">
Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.5.1" class="ltx_text" style="font-size:90%;">Zero-shot detection via vision and language knowledge distillation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv e-prints</span><span id="bib.bib27.7.2" class="ltx_text" style="font-size:90%;">, pages arXiv–2104, 2021.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.2.2.1" class="ltx_text" style="font-size:90%;">[28]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.4.1" class="ltx_text" style="font-size:90%;">
Danna Gurari, Yinan Zhao, Meng Zhang, and Nilavra Bhattacharya.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.5.1" class="ltx_text" style="font-size:90%;">Captioning images taken by people who are blind.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib28.8.3" class="ltx_text" style="font-size:90%;">, pages 417–434.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.2.2.1" class="ltx_text" style="font-size:90%;">[29]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.4.1" class="ltx_text" style="font-size:90%;">
Ankur Handa, Viorica Patraucean, Vijay Badrinarayanan, Simon Stent, and Roberto
Cipolla.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.5.1" class="ltx_text" style="font-size:90%;">Scenenet: Understanding real world indoor scenes with synthetic data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib29.7.2" class="ltx_text" style="font-size:90%;">, abs/1511.07041, 2015.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.2.2.1" class="ltx_text" style="font-size:90%;">[30]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.4.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.5.1" class="ltx_text" style="font-size:90%;">Mask r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib30.8.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.2.2.1" class="ltx_text" style="font-size:90%;">[31]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.4.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.5.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib31.8.3" class="ltx_text" style="font-size:90%;">, pages 770–778, 2016.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.2.2.1" class="ltx_text" style="font-size:90%;">[32]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.4.1" class="ltx_text" style="font-size:90%;">
Stefan Hinterstoisser, Vincent Lepetit, Paul Wohlhart, and Kurt Konolige.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.5.1" class="ltx_text" style="font-size:90%;">On pre-trained image features and synthetic images for deep learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1710.10710</span><span id="bib.bib32.7.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.2.2.1" class="ltx_text" style="font-size:90%;">[33]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.4.1" class="ltx_text" style="font-size:90%;">
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.5.1" class="ltx_text" style="font-size:90%;">Denoising diffusion probabilistic models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib33.7.2" class="ltx_text" style="font-size:90%;">,
33:6840–6851, 2020.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.2.2.1" class="ltx_text" style="font-size:90%;">[34]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.4.1" class="ltx_text" style="font-size:90%;">
Jonathan Ho and Tim Salimans.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.5.1" class="ltx_text" style="font-size:90%;">Classifier-free diffusion guidance.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2207.12598</span><span id="bib.bib34.7.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.2.2.1" class="ltx_text" style="font-size:90%;">[35]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.4.1" class="ltx_text" style="font-size:90%;">
Tomáš Hodaň, Vibhav Vineet, Ran Gal, Emanuel Shalev, Jon
Hanzelka, Treb Connell, Pedro Urbina, Sudipta Sinha, and Brian Guenter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.5.1" class="ltx_text" style="font-size:90%;">Photorealistic image synthesis for object instance detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ICIP</span><span id="bib.bib35.7.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.2.2.1" class="ltx_text" style="font-size:90%;">[36]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.4.1" class="ltx_text" style="font-size:90%;">
Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and
Nicolas Carion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.5.1" class="ltx_text" style="font-size:90%;">Mdetr-modulated detection for end-to-end multi-modal understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib36.8.3" class="ltx_text" style="font-size:90%;">, pages 1780–1790, 2021.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.2.2.1" class="ltx_text" style="font-size:90%;">[37]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.4.1" class="ltx_text" style="font-size:90%;">
Donghoon Lee, Sifei Liu, Jinwei Gu, Ming-Yu Liu, Ming-Hsuan Yang, and Jan
Kautz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.5.1" class="ltx_text" style="font-size:90%;">Context-aware synthesis and placement of object instances.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib37.7.2" class="ltx_text" style="font-size:90%;">, 31, 2018.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.2.2.1" class="ltx_text" style="font-size:90%;">[38]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.4.1" class="ltx_text" style="font-size:90%;">
Alexander C Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, and Deepak
Pathak.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.5.1" class="ltx_text" style="font-size:90%;">Your diffusion model is secretly a zero-shot classifier.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2303.16203</span><span id="bib.bib38.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.2.2.1" class="ltx_text" style="font-size:90%;">[39]</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.4.1" class="ltx_text" style="font-size:90%;">
Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, and René
Ranftl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.5.1" class="ltx_text" style="font-size:90%;">Language-driven semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2201.03546</span><span id="bib.bib39.7.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.2.2.1" class="ltx_text" style="font-size:90%;">[40]</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.4.1" class="ltx_text" style="font-size:90%;">
Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.5.1" class="ltx_text" style="font-size:90%;">Unicoder-vl: A universal encoder for vision and language by
cross-modal pre-training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial
Intelligence</span><span id="bib.bib40.8.3" class="ltx_text" style="font-size:90%;">, volume 34, pages 11336–11344, 2020.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.2.2.1" class="ltx_text" style="font-size:90%;">[41]</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.4.1" class="ltx_text" style="font-size:90%;">
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.5.1" class="ltx_text" style="font-size:90%;">Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib41.8.3" class="ltx_text" style="font-size:90%;">, pages
12888–12900. PMLR, 2022.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.2.2.1" class="ltx_text" style="font-size:90%;">[42]</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.4.1" class="ltx_text" style="font-size:90%;">
Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li,
Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.5.1" class="ltx_text" style="font-size:90%;">Grounded language-image pre-training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib42.8.3" class="ltx_text" style="font-size:90%;">, pages 10965–10975, 2022.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.2.2.1" class="ltx_text" style="font-size:90%;">[43]</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.4.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.5.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib43.8.3" class="ltx_text" style="font-size:90%;">, pages 740–755.
Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.2.2.1" class="ltx_text" style="font-size:90%;">[44]</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.4.1" class="ltx_text" style="font-size:90%;">
Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.5.1" class="ltx_text" style="font-size:90%;">12-in-1: Multi-task vision and language representation learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib44.8.3" class="ltx_text" style="font-size:90%;">, pages 10437–10446, 2020.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.2.2.1" class="ltx_text" style="font-size:90%;">[45]</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.4.1" class="ltx_text" style="font-size:90%;">
Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi
Ramamoorthi, and Ren Ng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.5.1" class="ltx_text" style="font-size:90%;">Nerf: Representing scenes as neural radiance fields for view
synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib45.8.3" class="ltx_text" style="font-size:90%;">, pages 405–421.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.2.2.1" class="ltx_text" style="font-size:90%;">[46]</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.4.1" class="ltx_text" style="font-size:90%;">
Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja
Fidler, Raquel Urtasun, and Alan Yuille.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.5.1" class="ltx_text" style="font-size:90%;">The role of context for object detection and semantic segmentation in
the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib46.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib46.8.3" class="ltx_text" style="font-size:90%;">, pages 891–898, 2014.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.2.2.1" class="ltx_text" style="font-size:90%;">[47]</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.4.1" class="ltx_text" style="font-size:90%;">
OpenAI.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.5.1" class="ltx_text" style="font-size:90%;">Gpt-4 technical report.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">PREPRINT</span><span id="bib.bib47.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib48.2.2.1" class="ltx_text" style="font-size:90%;">[48]</span></span>
<span class="ltx_bibblock"><span id="bib.bib48.4.1" class="ltx_text" style="font-size:90%;">
Lu Qi, Jason Kuen, Yi Wang, Jiuxiang Gu, Hengshuang Zhao, Zhe Lin, Philip Torr,
and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.5.1" class="ltx_text" style="font-size:90%;">Open-world entity segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2107.14228</span><span id="bib.bib48.7.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib49.2.2.1" class="ltx_text" style="font-size:90%;">[49]</span></span>
<span class="ltx_bibblock"><span id="bib.bib49.4.1" class="ltx_text" style="font-size:90%;">
Mahdi Rad and Vincent Lepetit.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.5.1" class="ltx_text" style="font-size:90%;">Bb8: A scalable, accurate, robust to partial occlusion method for
predicting the 3d poses of challenging objects without using depth.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Computer Vision</span><span id="bib.bib49.8.3" class="ltx_text" style="font-size:90%;">, volume 1,
page 5, 2017.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib50.2.2.1" class="ltx_text" style="font-size:90%;">[50]</span></span>
<span class="ltx_bibblock"><span id="bib.bib50.4.1" class="ltx_text" style="font-size:90%;">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.5.1" class="ltx_text" style="font-size:90%;">Learning transferable visual models from natural language
supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib50.8.3" class="ltx_text" style="font-size:90%;">, pages
8748–8763. PMLR, 2021.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib51.2.2.1" class="ltx_text" style="font-size:90%;">[51]</span></span>
<span class="ltx_bibblock"><span id="bib.bib51.4.1" class="ltx_text" style="font-size:90%;">
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
Radford, Mark Chen, and Ilya Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.5.1" class="ltx_text" style="font-size:90%;">Zero-shot text-to-image generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib51.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib51.8.3" class="ltx_text" style="font-size:90%;">, pages
8821–8831. PMLR, 2021.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib52.2.2.1" class="ltx_text" style="font-size:90%;">[52]</span></span>
<span class="ltx_bibblock"><span id="bib.bib52.4.1" class="ltx_text" style="font-size:90%;">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.5.1" class="ltx_text" style="font-size:90%;">Faster r-cnn: towards real-time object detection with region proposal
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</span><span id="bib.bib52.7.2" class="ltx_text" style="font-size:90%;">,
(6):1137–1149, 2017.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib53.2.2.1" class="ltx_text" style="font-size:90%;">[53]</span></span>
<span class="ltx_bibblock"><span id="bib.bib53.4.1" class="ltx_text" style="font-size:90%;">
Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava
Goel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.5.1" class="ltx_text" style="font-size:90%;">Self-critical sequence training for image captioning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib53.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib53.8.3" class="ltx_text" style="font-size:90%;">, pages 7008–7024, 2017.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib54.2.2.1" class="ltx_text" style="font-size:90%;">[54]</span></span>
<span class="ltx_bibblock"><span id="bib.bib54.4.1" class="ltx_text" style="font-size:90%;">
Stephan R. Richter, Zeeshan Hayder, and Vladlen Koltun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.5.1" class="ltx_text" style="font-size:90%;">Playing for benchmarks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib54.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib54.8.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib55.2.2.1" class="ltx_text" style="font-size:90%;">[55]</span></span>
<span class="ltx_bibblock"><span id="bib.bib55.4.1" class="ltx_text" style="font-size:90%;">
Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.5.1" class="ltx_text" style="font-size:90%;">Playing for data: Ground truth from computer games.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib55.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib55.8.3" class="ltx_text" style="font-size:90%;">, pages 102–118.
Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib56.2.2.1" class="ltx_text" style="font-size:90%;">[56]</span></span>
<span class="ltx_bibblock"><span id="bib.bib56.4.1" class="ltx_text" style="font-size:90%;">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
Ommer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.5.1" class="ltx_text" style="font-size:90%;">High-resolution image synthesis with latent diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib56.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib56.8.3" class="ltx_text" style="font-size:90%;">, pages 10684–10695, 2022.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib57.2.2.1" class="ltx_text" style="font-size:90%;">[57]</span></span>
<span class="ltx_bibblock"><span id="bib.bib57.4.1" class="ltx_text" style="font-size:90%;">
Germán Ros, Laura Sellart, Joanna Materzynska, David Vázquez, and
Antonio M. López.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.5.1" class="ltx_text" style="font-size:90%;">The SYNTHIA dataset: A large collection of synthetic images for
semantic segmentation of urban scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib57.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib57.8.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib58.2.2.1" class="ltx_text" style="font-size:90%;">[58]</span></span>
<span class="ltx_bibblock"><span id="bib.bib58.4.1" class="ltx_text" style="font-size:90%;">
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily
Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo-Lopes, Burcu Karagol
Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.5.1" class="ltx_text" style="font-size:90%;">Photorealistic text-to-image diffusion models with deep language
understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.6.1" class="ltx_text" style="font-size:90%;">In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
editors, </span><span id="bib.bib58.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib58.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib59.2.2.1" class="ltx_text" style="font-size:90%;">[59]</span></span>
<span class="ltx_bibblock"><span id="bib.bib59.4.1" class="ltx_text" style="font-size:90%;">
Evan Shelhamer, Jonathan Long, and Trevor Darrell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.5.1" class="ltx_text" style="font-size:90%;">Fully convolutional networks for semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">PAMI</span><span id="bib.bib59.7.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib60.2.2.1" class="ltx_text" style="font-size:90%;">[60]</span></span>
<span class="ltx_bibblock"><span id="bib.bib60.4.1" class="ltx_text" style="font-size:90%;">
Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Manolis Savva, and Thomas
Funkhouser.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.5.1" class="ltx_text" style="font-size:90%;">Semantic scene completion from a single depth image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of 30th IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib60.7.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib61.2.2.1" class="ltx_text" style="font-size:90%;">[61]</span></span>
<span class="ltx_bibblock"><span id="bib.bib61.4.1" class="ltx_text" style="font-size:90%;">
Hao Su, Charles Ruizhongtai Qi, Yangyan Li, and Leonidas J. Guibas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.5.1" class="ltx_text" style="font-size:90%;">Render for CNN: viewpoint estimation in images using cnns trained
with rendered 3d model views.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib61.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2015 IEEE International Conference on Computer Vision,
ICCV 2015, Santiago, Chile, December 7-13, 2015</span><span id="bib.bib61.8.3" class="ltx_text" style="font-size:90%;">, pages 2686–2694, 2015.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib62.2.2.1" class="ltx_text" style="font-size:90%;">[62]</span></span>
<span class="ltx_bibblock"><span id="bib.bib62.4.1" class="ltx_text" style="font-size:90%;">
Bugra Tekin, Sudipta N. Sinha, and Pascal Fua.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.5.1" class="ltx_text" style="font-size:90%;">Real-time seamless single shot 6d object pose prediction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib62.7.2" class="ltx_text" style="font-size:90%;">, abs/1711.08848, 2017.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib63.2.2.1" class="ltx_text" style="font-size:90%;">[63]</span></span>
<span class="ltx_bibblock"><span id="bib.bib63.4.1" class="ltx_text" style="font-size:90%;">
J. Tremblay, T. To, and S. Birchfield.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.5.1" class="ltx_text" style="font-size:90%;">Falling things: A synthetic dataset for 3d object detection and pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib63.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops (CVPRW)</span><span id="bib.bib63.8.3" class="ltx_text" style="font-size:90%;">, pages 2119–21193, Los Alamitos, CA, USA, jun
2018. IEEE Computer Society.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib64.2.2.1" class="ltx_text" style="font-size:90%;">[64]</span></span>
<span class="ltx_bibblock"><span id="bib.bib64.4.1" class="ltx_text" style="font-size:90%;">
Jonathan Tremblay, Thang To, and Stan Birchfield.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.5.1" class="ltx_text" style="font-size:90%;">Falling things: A synthetic dataset for 3d object detection and
pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib64.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib64.8.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib65.2.2.1" class="ltx_text" style="font-size:90%;">[65]</span></span>
<span class="ltx_bibblock"><span id="bib.bib65.4.1" class="ltx_text" style="font-size:90%;">
Shashank Tripathi, Siddhartha Chandra, Amit Agrawal, Ambrish Tyagi, James M
Rehg, and Visesh Chari.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.5.1" class="ltx_text" style="font-size:90%;">Learning to generate synthetic data via compositing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib65.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib65.8.3" class="ltx_text" style="font-size:90%;">, pages 461–470, 2019.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib66.2.2.1" class="ltx_text" style="font-size:90%;">[66]</span></span>
<span class="ltx_bibblock"><span id="bib.bib66.4.1" class="ltx_text" style="font-size:90%;">
Ramakrishna Vedantam, Karan Desai, Stefan Lee, Marcus Rohrbach, Dhruv Batra,
and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.5.1" class="ltx_text" style="font-size:90%;">Probabilistic neural symbolic models for interpretable visual
question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib66.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib66.8.3" class="ltx_text" style="font-size:90%;">, pages
6428–6437. PMLR, 2019.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib67.2.2.1" class="ltx_text" style="font-size:90%;">[67]</span></span>
<span class="ltx_bibblock"><span id="bib.bib67.4.1" class="ltx_text" style="font-size:90%;">
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.5.1" class="ltx_text" style="font-size:90%;">Show and tell: A neural image caption generator.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib67.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib67.8.3" class="ltx_text" style="font-size:90%;">, pages 3156–3164, 2015.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib68.2.2.1" class="ltx_text" style="font-size:90%;">[68]</span></span>
<span class="ltx_bibblock"><span id="bib.bib68.4.1" class="ltx_text" style="font-size:90%;">
Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.5.1" class="ltx_text" style="font-size:90%;">Posecnn: A convolutional neural network for 6d object pose estimation
in cluttered scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1711.00199</span><span id="bib.bib68.7.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib69.2.2.1" class="ltx_text" style="font-size:90%;">[69]</span></span>
<span class="ltx_bibblock"><span id="bib.bib69.4.1" class="ltx_text" style="font-size:90%;">
Xu Yang, Hanwang Zhang, Guojun Qi, and Jianfei Cai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.5.1" class="ltx_text" style="font-size:90%;">Causal attention for vision-language tasks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib69.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib69.8.3" class="ltx_text" style="font-size:90%;">, pages 9847–9857, 2021.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib70.2.2.1" class="ltx_text" style="font-size:90%;">[70]</span></span>
<span class="ltx_bibblock"><span id="bib.bib70.4.1" class="ltx_text" style="font-size:90%;">
Woo-Han Yun, Taewoo Kim, Jaeyeon Lee, Jaehong Kim, and Junmo Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.5.1" class="ltx_text" style="font-size:90%;">Cut-and-paste dataset generation for balancing domain gaps in object
instance detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Access</span><span id="bib.bib70.7.2" class="ltx_text" style="font-size:90%;">, 9:14319–14329, 2021.
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib71.2.2.1" class="ltx_text" style="font-size:90%;">[71]</span></span>
<span class="ltx_bibblock"><span id="bib.bib71.4.1" class="ltx_text" style="font-size:90%;">
Lingzhi Zhang, Tarmily Wen, Jie Min, Jiancong Wang, David Han, and Jianbo Shi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.5.1" class="ltx_text" style="font-size:90%;">Learning object placement by inpainting for compositional data
augmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib71.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib71.8.3" class="ltx_text" style="font-size:90%;">, pages 566–581.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib72.2.2.1" class="ltx_text" style="font-size:90%;">[72]</span></span>
<span class="ltx_bibblock"><span id="bib.bib72.4.1" class="ltx_text" style="font-size:90%;">
Hanqing Zhao, Dianmo Sheng, Jianmin Bao, Dongdong Chen, Dong Chen, Fang Wen, Lu
Yuan, Ce Liu, Wenbo Zhou, Qi Chu, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.5.1" class="ltx_text" style="font-size:90%;">X-paste: Revisit copy-paste at scale with clip and stablediffusion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2212.03863</span><span id="bib.bib72.7.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
Appendix
</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">We show inference results of Mask RCNN on VOC (<a href="#Sx1.F11" title="In Appendix ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">11</span></a>) and GMU-Kitchen (<a href="#Sx1.F12" title="In Appendix ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">12</span></a>). <a href="#Sx1.F12" title="In Appendix ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">12</span></a> shows the quantitative results on GMU-Kitchen dataset.</p>
</div>
<figure id="Sx1.F11" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2309.05956/assets/x6.png" id="Sx1.F11.1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="363" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2309.05956/assets/12_VOC_output_occlusion.png" id="Sx1.F11.2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="598" height="105" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Sx1.F11.6.2.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="Sx1.F11.4.1" class="ltx_text" style="font-size:90%;">Prediction of Faster RCNN trained with 10 shot Pure Syn + Real in <a href="#S4.T4" title="In 4 Experiments ‣ Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a>. The colorful box is the prediction of the model, together with the predicted class and confidence. We only show prediction with confidence <math id="Sx1.F11.4.1.m1.1" class="ltx_Math" alttext="&gt;0.9" display="inline"><semantics id="Sx1.F11.4.1.m1.1b"><mrow id="Sx1.F11.4.1.m1.1.1" xref="Sx1.F11.4.1.m1.1.1.cmml"><mi id="Sx1.F11.4.1.m1.1.1.2" xref="Sx1.F11.4.1.m1.1.1.2.cmml"></mi><mo id="Sx1.F11.4.1.m1.1.1.1" xref="Sx1.F11.4.1.m1.1.1.1.cmml">&gt;</mo><mn id="Sx1.F11.4.1.m1.1.1.3" xref="Sx1.F11.4.1.m1.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx1.F11.4.1.m1.1c"><apply id="Sx1.F11.4.1.m1.1.1.cmml" xref="Sx1.F11.4.1.m1.1.1"><gt id="Sx1.F11.4.1.m1.1.1.1.cmml" xref="Sx1.F11.4.1.m1.1.1.1"></gt><csymbol cd="latexml" id="Sx1.F11.4.1.m1.1.1.2.cmml" xref="Sx1.F11.4.1.m1.1.1.2">absent</csymbol><cn type="float" id="Sx1.F11.4.1.m1.1.1.3.cmml" xref="Sx1.F11.4.1.m1.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.F11.4.1.m1.1d">&gt;0.9</annotation></semantics></math>. The different color of the box indicates the different prediction of the box. Note how our model is able to handle complex real world images including occlusion, multiple objects, instances, etc.</span></figcaption>
</figure>
<figure id="Sx1.F12" class="ltx_figure"><img src="/html/2309.05956/assets/13_kitchen_detectron_output.png" id="Sx1.F12.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="236" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Sx1.F12.5.2.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="Sx1.F12.3.1" class="ltx_text" style="font-size:90%;">Qualitative detection results on GMU-Kitchen dataset dataset. The colorful box is the prediction of the model, together with the predicted class and confidence. We only show prediction with confidence <math id="Sx1.F12.3.1.m1.1" class="ltx_Math" alttext="&gt;0.9" display="inline"><semantics id="Sx1.F12.3.1.m1.1b"><mrow id="Sx1.F12.3.1.m1.1.1" xref="Sx1.F12.3.1.m1.1.1.cmml"><mi id="Sx1.F12.3.1.m1.1.1.2" xref="Sx1.F12.3.1.m1.1.1.2.cmml"></mi><mo id="Sx1.F12.3.1.m1.1.1.1" xref="Sx1.F12.3.1.m1.1.1.1.cmml">&gt;</mo><mn id="Sx1.F12.3.1.m1.1.1.3" xref="Sx1.F12.3.1.m1.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx1.F12.3.1.m1.1c"><apply id="Sx1.F12.3.1.m1.1.1.cmml" xref="Sx1.F12.3.1.m1.1.1"><gt id="Sx1.F12.3.1.m1.1.1.1.cmml" xref="Sx1.F12.3.1.m1.1.1.1"></gt><csymbol cd="latexml" id="Sx1.F12.3.1.m1.1.1.2.cmml" xref="Sx1.F12.3.1.m1.1.1.2">absent</csymbol><cn type="float" id="Sx1.F12.3.1.m1.1.1.3.cmml" xref="Sx1.F12.3.1.m1.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.F12.3.1.m1.1d">&gt;0.9</annotation></semantics></math>. The different color of the box indicates the different prediction of the box.</span></figcaption>
</figure>
<figure id="Sx1.T12" class="ltx_table">
<table id="Sx1.T12.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx1.T12.2.1.1" class="ltx_tr">
<th id="Sx1.T12.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Dataset</th>
<th id="Sx1.T12.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">#CDI</th>
<th id="Sx1.T12.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">CC</th>
<th id="Sx1.T12.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">CM</th>
<th id="Sx1.T12.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">HB</th>
<th id="Sx1.T12.2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">HS</th>
<th id="Sx1.T12.2.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">MR</th>
<th id="Sx1.T12.2.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">NV1</th>
<th id="Sx1.T12.2.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">NV2</th>
<th id="Sx1.T12.2.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">PO</th>
<th id="Sx1.T12.2.1.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">PS</th>
<th id="Sx1.T12.2.1.1.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Pbbq</th>
<th id="Sx1.T12.2.1.1.13" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">RB</th>
<th id="Sx1.T12.2.1.1.14" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAP</th>
</tr>
<tr id="Sx1.T12.2.2.2" class="ltx_tr">
<th id="Sx1.T12.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Real GMU train</th>
<th id="Sx1.T12.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">-</th>
<th id="Sx1.T12.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">81.9</th>
<th id="Sx1.T12.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">95.3</th>
<th id="Sx1.T12.2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">92.0</th>
<th id="Sx1.T12.2.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">87.3</th>
<th id="Sx1.T12.2.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">86.5</th>
<th id="Sx1.T12.2.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">96.8</th>
<th id="Sx1.T12.2.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">88.9</th>
<th id="Sx1.T12.2.2.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">80.5</th>
<th id="Sx1.T12.2.2.2.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">92.3</th>
<th id="Sx1.T12.2.2.2.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">88.9</th>
<th id="Sx1.T12.2.2.2.13" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">58.6</th>
<th id="Sx1.T12.2.2.2.14" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">86.3</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx1.T12.2.3.1" class="ltx_tr">
<th id="Sx1.T12.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Black</th>
<th id="Sx1.T12.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">1500</th>
<td id="Sx1.T12.2.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">42.3</td>
<td id="Sx1.T12.2.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">62.4</td>
<td id="Sx1.T12.2.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">64.7</td>
<td id="Sx1.T12.2.3.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5.3</td>
<td id="Sx1.T12.2.3.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">23.3</td>
<td id="Sx1.T12.2.3.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">61.1</td>
<td id="Sx1.T12.2.3.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">56.5</td>
<td id="Sx1.T12.2.3.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">75.3</td>
<td id="Sx1.T12.2.3.1.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.6</td>
<td id="Sx1.T12.2.3.1.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">26.7</td>
<td id="Sx1.T12.2.3.1.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">33.9</td>
<td id="Sx1.T12.2.3.1.14" class="ltx_td ltx_align_center ltx_border_t">41.2</td>
</tr>
<tr id="Sx1.T12.2.4.2" class="ltx_tr">
<th id="Sx1.T12.2.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">CDI</th>
<th id="Sx1.T12.2.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">10</th>
<td id="Sx1.T12.2.4.2.3" class="ltx_td ltx_align_center ltx_border_r">51.4</td>
<td id="Sx1.T12.2.4.2.4" class="ltx_td ltx_align_center ltx_border_r">26.4</td>
<td id="Sx1.T12.2.4.2.5" class="ltx_td ltx_align_center ltx_border_r">2.1</td>
<td id="Sx1.T12.2.4.2.6" class="ltx_td ltx_align_center ltx_border_r">12.2</td>
<td id="Sx1.T12.2.4.2.7" class="ltx_td ltx_align_center ltx_border_r">12.1</td>
<td id="Sx1.T12.2.4.2.8" class="ltx_td ltx_align_center ltx_border_r">0.4</td>
<td id="Sx1.T12.2.4.2.9" class="ltx_td ltx_align_center ltx_border_r">0.1</td>
<td id="Sx1.T12.2.4.2.10" class="ltx_td ltx_align_center ltx_border_r">1.0</td>
<td id="Sx1.T12.2.4.2.11" class="ltx_td ltx_align_center ltx_border_r">0.1</td>
<td id="Sx1.T12.2.4.2.12" class="ltx_td ltx_align_center ltx_border_r">29.8</td>
<td id="Sx1.T12.2.4.2.13" class="ltx_td ltx_align_center ltx_border_r">30.0</td>
<td id="Sx1.T12.2.4.2.14" class="ltx_td ltx_align_center">15.0</td>
</tr>
<tr id="Sx1.T12.2.5.3" class="ltx_tr">
<th id="Sx1.T12.2.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Random (COCO)</th>
<th id="Sx1.T12.2.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">1500</th>
<td id="Sx1.T12.2.5.3.3" class="ltx_td ltx_align_center ltx_border_r">50.7</td>
<td id="Sx1.T12.2.5.3.4" class="ltx_td ltx_align_center ltx_border_r">80.1</td>
<td id="Sx1.T12.2.5.3.5" class="ltx_td ltx_align_center ltx_border_r">77.5</td>
<td id="Sx1.T12.2.5.3.6" class="ltx_td ltx_align_center ltx_border_r">15.3</td>
<td id="Sx1.T12.2.5.3.7" class="ltx_td ltx_align_center ltx_border_r">32.2</td>
<td id="Sx1.T12.2.5.3.8" class="ltx_td ltx_align_center ltx_border_r">81.7</td>
<td id="Sx1.T12.2.5.3.9" class="ltx_td ltx_align_center ltx_border_r">87.9</td>
<td id="Sx1.T12.2.5.3.10" class="ltx_td ltx_align_center ltx_border_r">71.7</td>
<td id="Sx1.T12.2.5.3.11" class="ltx_td ltx_align_center ltx_border_r">66.8</td>
<td id="Sx1.T12.2.5.3.12" class="ltx_td ltx_align_center ltx_border_r">59.0</td>
<td id="Sx1.T12.2.5.3.13" class="ltx_td ltx_align_center ltx_border_r">68.5</td>
<td id="Sx1.T12.2.5.3.14" class="ltx_td ltx_align_center">62.8</td>
</tr>
<tr id="Sx1.T12.2.6.4" class="ltx_tr">
<th id="Sx1.T12.2.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Random (ru-DALLE)</th>
<th id="Sx1.T12.2.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">1500</th>
<td id="Sx1.T12.2.6.4.3" class="ltx_td ltx_align_center ltx_border_r">64.8</td>
<td id="Sx1.T12.2.6.4.4" class="ltx_td ltx_align_center ltx_border_r">86.9</td>
<td id="Sx1.T12.2.6.4.5" class="ltx_td ltx_align_center ltx_border_r">78.7</td>
<td id="Sx1.T12.2.6.4.6" class="ltx_td ltx_align_center ltx_border_r">49.2</td>
<td id="Sx1.T12.2.6.4.7" class="ltx_td ltx_align_center ltx_border_r">62.2</td>
<td id="Sx1.T12.2.6.4.8" class="ltx_td ltx_align_center ltx_border_r">84.8</td>
<td id="Sx1.T12.2.6.4.9" class="ltx_td ltx_align_center ltx_border_r">83.8</td>
<td id="Sx1.T12.2.6.4.10" class="ltx_td ltx_align_center ltx_border_r">72.6</td>
<td id="Sx1.T12.2.6.4.11" class="ltx_td ltx_align_center ltx_border_r">70.9</td>
<td id="Sx1.T12.2.6.4.12" class="ltx_td ltx_align_center ltx_border_r">57.2</td>
<td id="Sx1.T12.2.6.4.13" class="ltx_td ltx_align_center ltx_border_r">24.1</td>
<td id="Sx1.T12.2.6.4.14" class="ltx_td ltx_align_center">66.8</td>
</tr>
<tr id="Sx1.T12.2.7.5" class="ltx_tr">
<th id="Sx1.T12.2.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">UW-Kitchen</th>
<th id="Sx1.T12.2.7.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">1500</th>
<td id="Sx1.T12.2.7.5.3" class="ltx_td ltx_align_center ltx_border_r">75.7</td>
<td id="Sx1.T12.2.7.5.4" class="ltx_td ltx_align_center ltx_border_r">91.1</td>
<td id="Sx1.T12.2.7.5.5" class="ltx_td ltx_align_center ltx_border_r">87.7</td>
<td id="Sx1.T12.2.7.5.6" class="ltx_td ltx_align_center ltx_border_r">51.6</td>
<td id="Sx1.T12.2.7.5.7" class="ltx_td ltx_align_center ltx_border_r">66.5</td>
<td id="Sx1.T12.2.7.5.8" class="ltx_td ltx_align_center ltx_border_r">91.5</td>
<td id="Sx1.T12.2.7.5.9" class="ltx_td ltx_align_center ltx_border_r">88.7</td>
<td id="Sx1.T12.2.7.5.10" class="ltx_td ltx_align_center ltx_border_r">76.2</td>
<td id="Sx1.T12.2.7.5.11" class="ltx_td ltx_align_center ltx_border_r">63.2</td>
<td id="Sx1.T12.2.7.5.12" class="ltx_td ltx_align_center ltx_border_r">70.5</td>
<td id="Sx1.T12.2.7.5.13" class="ltx_td ltx_align_center ltx_border_r">75.2</td>
<td id="Sx1.T12.2.7.5.14" class="ltx_td ltx_align_center">76.1</td>
</tr>
<tr id="Sx1.T12.2.8.6" class="ltx_tr">
<th id="Sx1.T12.2.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Syn (ours)</th>
<th id="Sx1.T12.2.8.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">1500</th>
<td id="Sx1.T12.2.8.6.3" class="ltx_td ltx_align_center ltx_border_r">79.0</td>
<td id="Sx1.T12.2.8.6.4" class="ltx_td ltx_align_center ltx_border_r">92.9</td>
<td id="Sx1.T12.2.8.6.5" class="ltx_td ltx_align_center ltx_border_r">90.4</td>
<td id="Sx1.T12.2.8.6.6" class="ltx_td ltx_align_center ltx_border_r">44.9</td>
<td id="Sx1.T12.2.8.6.7" class="ltx_td ltx_align_center ltx_border_r">77.0</td>
<td id="Sx1.T12.2.8.6.8" class="ltx_td ltx_align_center ltx_border_r">92.1</td>
<td id="Sx1.T12.2.8.6.9" class="ltx_td ltx_align_center ltx_border_r">88.0</td>
<td id="Sx1.T12.2.8.6.10" class="ltx_td ltx_align_center ltx_border_r"><span id="Sx1.T12.2.8.6.10.1" class="ltx_text ltx_font_bold">77.5</span></td>
<td id="Sx1.T12.2.8.6.11" class="ltx_td ltx_align_center ltx_border_r">64.1</td>
<td id="Sx1.T12.2.8.6.12" class="ltx_td ltx_align_center ltx_border_r">75.7</td>
<td id="Sx1.T12.2.8.6.13" class="ltx_td ltx_align_center ltx_border_r">80.2</td>
<td id="Sx1.T12.2.8.6.14" class="ltx_td ltx_align_center">78.3</td>
</tr>
<tr id="Sx1.T12.2.9.7" class="ltx_tr">
<th id="Sx1.T12.2.9.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Syn (ours)</th>
<th id="Sx1.T12.2.9.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">2400</th>
<td id="Sx1.T12.2.9.7.3" class="ltx_td ltx_align_center ltx_border_r">79.5</td>
<td id="Sx1.T12.2.9.7.4" class="ltx_td ltx_align_center ltx_border_r">93.4</td>
<td id="Sx1.T12.2.9.7.5" class="ltx_td ltx_align_center ltx_border_r">88.5</td>
<td id="Sx1.T12.2.9.7.6" class="ltx_td ltx_align_center ltx_border_r">59.0</td>
<td id="Sx1.T12.2.9.7.7" class="ltx_td ltx_align_center ltx_border_r">71.5</td>
<td id="Sx1.T12.2.9.7.8" class="ltx_td ltx_align_center ltx_border_r">91.4</td>
<td id="Sx1.T12.2.9.7.9" class="ltx_td ltx_align_center ltx_border_r">88.1</td>
<td id="Sx1.T12.2.9.7.10" class="ltx_td ltx_align_center ltx_border_r">76.1</td>
<td id="Sx1.T12.2.9.7.11" class="ltx_td ltx_align_center ltx_border_r">78.7</td>
<td id="Sx1.T12.2.9.7.12" class="ltx_td ltx_align_center ltx_border_r">75.7</td>
<td id="Sx1.T12.2.9.7.13" class="ltx_td ltx_align_center ltx_border_r"><span id="Sx1.T12.2.9.7.13.1" class="ltx_text ltx_font_bold">80.6</span></td>
<td id="Sx1.T12.2.9.7.14" class="ltx_td ltx_align_center">80.1</td>
</tr>
<tr id="Sx1.T12.2.10.8" class="ltx_tr">
<th id="Sx1.T12.2.10.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">Syn (ours)+Real</th>
<th id="Sx1.T12.2.10.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">1500</th>
<td id="Sx1.T12.2.10.8.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="Sx1.T12.2.10.8.3.1" class="ltx_text ltx_font_bold">94.4</span></td>
<td id="Sx1.T12.2.10.8.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="Sx1.T12.2.10.8.4.1" class="ltx_text ltx_font_bold">98.2</span></td>
<td id="Sx1.T12.2.10.8.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="Sx1.T12.2.10.8.5.1" class="ltx_text ltx_font_bold">95.2</span></td>
<td id="Sx1.T12.2.10.8.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="Sx1.T12.2.10.8.6.1" class="ltx_text ltx_font_bold">90.7</span></td>
<td id="Sx1.T12.2.10.8.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="Sx1.T12.2.10.8.7.1" class="ltx_text ltx_font_bold">92.5</span></td>
<td id="Sx1.T12.2.10.8.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="Sx1.T12.2.10.8.8.1" class="ltx_text ltx_font_bold">94.1</span></td>
<td id="Sx1.T12.2.10.8.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="Sx1.T12.2.10.8.9.1" class="ltx_text ltx_font_bold">93.0</span></td>
<td id="Sx1.T12.2.10.8.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">72.8</td>
<td id="Sx1.T12.2.10.8.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="Sx1.T12.2.10.8.11.1" class="ltx_text ltx_font_bold">98.3</span></td>
<td id="Sx1.T12.2.10.8.12" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="Sx1.T12.2.10.8.12.1" class="ltx_text ltx_font_bold">98.7</span></td>
<td id="Sx1.T12.2.10.8.13" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">79.8</td>
<td id="Sx1.T12.2.10.8.14" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="Sx1.T12.2.10.8.14.1" class="ltx_text ltx_font_bold">91.4</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="Sx1.T12.3.1.1" class="ltx_text" style="font-size:90%;">Table 12</span>: </span><span id="Sx1.T12.4.2" class="ltx_text" style="font-size:90%;">Quantitative results on GMU-Kitchen dataset. We compare our approach with several prior approaches. Our approach achieves highest accuracy over the baselines. Combining GMU kitchen real training samples with our synthetic data yields the best results on this dataset.
</span></figcaption>
</figure>
<figure id="Sx1.F13" class="ltx_figure"><img src="/html/2309.05956/assets/6_supp_voc_final_imgs.png" id="Sx1.F13.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="460" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Sx1.F13.3.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>: </span><span id="Sx1.F13.4.2" class="ltx_text" style="font-size:90%;">Training images generated by our pipeline for PASCAL VOC dataset: both foreground object and background context images are generated by our method with Stable Diffusion. </span></figcaption>
</figure>
<figure id="Sx1.F14" class="ltx_figure"><img src="/html/2309.05956/assets/7_fg_extract.png" id="Sx1.F14.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="568" height="549" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Sx1.F14.3.1.1" class="ltx_text" style="font-size:90%;">Figure 14</span>: </span><span id="Sx1.F14.4.2" class="ltx_text" style="font-size:90%;">Examples of foreground segment extraction from generated images.
</span></figcaption>
</figure>
<figure id="Sx1.F15" class="ltx_figure"><img src="/html/2309.05956/assets/8_cartoon_appendix.png" id="Sx1.F15.1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="479" height="743" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Sx1.F15.4.1.1" class="ltx_text" style="font-size:90%;">Figure 15</span>: </span><span id="Sx1.F15.5.2" class="ltx_text" style="font-size:90%;">Visualization of context images generated by DALL-E for noisy and out-of-distribution CDIs. In the left column, inputs CDIs are cartoon kitchen which provide noise information about environment. The compositional property allow us <span id="Sx1.F15.5.2.1" class="ltx_text ltx_font_bold">change the style</span> from cartoon to real and generate high-quality context images (right column).</span></figcaption>
</figure>
<figure id="Sx1.F16" class="ltx_figure"><img src="/html/2309.05956/assets/9_sketch_appendix.png" id="Sx1.F16.1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="479" height="743" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Sx1.F16.4.1.1" class="ltx_text" style="font-size:90%;">Figure 16</span>: </span><span id="Sx1.F16.5.2" class="ltx_text" style="font-size:90%;">Visualization of context images generated by DALL-E for noisy and out-of-distribution CDIs. In the left column, inputs CDIs are sketch kitchen which provide noise information about environment. The compositional property allow us <span id="Sx1.F16.5.2.1" class="ltx_text ltx_font_bold">change the style</span> from sketch to real and generate high-quality context images (right column).</span></figcaption>
</figure>
<figure id="Sx1.F17" class="ltx_figure"><img src="/html/2309.05956/assets/10_com_add_appendix.png" id="Sx1.F17.1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="479" height="743" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Sx1.F17.4.1.1" class="ltx_text" style="font-size:90%;">Figure 17</span>: </span><span id="Sx1.F17.5.2" class="ltx_text" style="font-size:90%;">Visualization of context images generated by DALL-E for noisy and out-of-distribution CDIs. In the left column, inputs CDIs do not convey full knowledge about the context. The compositional property allow us <span id="Sx1.F17.5.2.1" class="ltx_text ltx_font_bold">add</span> the context words to captions and generate high-quality context images (right column).</span></figcaption>
</figure>
<figure id="Sx1.F18" class="ltx_figure"><img src="/html/2309.05956/assets/11_com_remove_appendix.png" id="Sx1.F18.1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="479" height="743" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Sx1.F18.4.1.1" class="ltx_text" style="font-size:90%;">Figure 18</span>: </span><span id="Sx1.F18.5.2" class="ltx_text" style="font-size:90%;">Visualization of context images generated by DALL-E for noisy and out-of-distribution CDIs. In the left column, inputs CDIs have distractor objects (people). The compositional property allow us <span id="Sx1.F18.5.2.1" class="ltx_text ltx_font_bold">remove</span> the distractor words from captions and generate high-quality context images (right column). </span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2309.05955" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2309.05956" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2309.05956">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2309.05956" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2309.05957" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 07:00:39 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
