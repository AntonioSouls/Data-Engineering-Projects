<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2309.05257] FusionFormer: A Multi-sensory Fusion in Bird’s-Eye-View and Temporal Consistent Transformer for 3D Object Detection</title><meta property="og:description" content="Multi-sensor modal fusion has demonstrated strong advantages in 3D object detection tasks. However, existing methods that fuse multi-modal features require transforming features into the bird’s eye view space and may l…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="FusionFormer: A Multi-sensory Fusion in Bird’s-Eye-View and Temporal Consistent Transformer for 3D Object Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="FusionFormer: A Multi-sensory Fusion in Bird’s-Eye-View and Temporal Consistent Transformer for 3D Object Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2309.05257">

<!--Generated on Wed Feb 28 06:29:26 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">FusionFormer: A Multi-sensory Fusion in 
<br class="ltx_break">Bird’s-Eye-View and Temporal Consistent 
<br class="ltx_break">Transformer for 3D Object Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chunyong Hu<sup id="id21.19.id1" class="ltx_sup"><span id="id21.19.id1.1" class="ltx_text ltx_font_italic">1∗</span></sup>, Hang Zheng<sup id="id22.20.id2" class="ltx_sup"><span id="id22.20.id2.1" class="ltx_text ltx_font_italic">1∗</span></sup>, Kun Li<sup id="id23.21.id3" class="ltx_sup"><span id="id23.21.id3.1" class="ltx_text ltx_font_italic">1</span></sup>, Jianyun Xu<sup id="id24.22.id4" class="ltx_sup"><span id="id24.22.id4.1" class="ltx_text ltx_font_italic">1</span></sup>, Weibo Mao<sup id="id25.23.id5" class="ltx_sup"><span id="id25.23.id5.1" class="ltx_text ltx_font_italic">1</span></sup>, Maochun Luo<sup id="id26.24.id6" class="ltx_sup"><span id="id26.24.id6.1" class="ltx_text ltx_font_italic">1</span></sup>, 
<br class="ltx_break"><span id="id27.25.id7" class="ltx_ERROR undefined">\And</span>Lingxuan Wang<sup id="id28.26.id8" class="ltx_sup"><span id="id28.26.id8.1" class="ltx_text ltx_font_italic">1</span></sup>, Mingxia Chen<sup id="id29.27.id9" class="ltx_sup"><span id="id29.27.id9.1" class="ltx_text ltx_font_italic">1</span></sup>, Qihao Peng<sup id="id30.28.id10" class="ltx_sup"><span id="id30.28.id10.1" class="ltx_text ltx_font_italic">1</span></sup>, Kaixuan Liu<sup id="id31.29.id11" class="ltx_sup"><span id="id31.29.id11.1" class="ltx_text ltx_font_italic">1</span></sup>, Yiru Zhao<sup id="id32.30.id12" class="ltx_sup"><span id="id32.30.id12.1" class="ltx_text ltx_font_italic">1</span></sup>, Peihan Hao<sup id="id33.31.id13" class="ltx_sup"><span id="id33.31.id13.1" class="ltx_text ltx_font_italic">1</span></sup>,
<br class="ltx_break"><span id="id34.32.id14" class="ltx_ERROR undefined">\And</span>Minzhe Liu<sup id="id35.33.id15" class="ltx_sup"><span id="id35.33.id15.1" class="ltx_text ltx_font_italic">1</span></sup>, Kaicheng Yu<sup id="id36.34.id16" class="ltx_sup"><span id="id36.34.id16.1" class="ltx_text ltx_font_italic">1,2§</span></sup>
</span><span class="ltx_author_notes"><sup id="id37.35.id1" class="ltx_sup"><span id="id37.35.id1.1" class="ltx_text ltx_font_italic">∗</span></sup>Equal Contribution. <sup id="id38.36.id2" class="ltx_sup"><span id="id38.36.id2.1" class="ltx_text ltx_font_italic">§</span></sup>Corresponding Author.<sup id="id39.37.id1" class="ltx_sup"><span id="id39.37.id1.1" class="ltx_text ltx_font_italic">1</span></sup> Autonomous Driving Lab, Cainiao Network, China<sup id="id40.38.id1" class="ltx_sup"><span id="id40.38.id1.1" class="ltx_text ltx_font_italic">2</span></sup> Autonomous Intelligence Lab, Westlake University, ChinaEmail:<span id="id41.39.id1" class="ltx_text ltx_font_typewriter">kyu@westlake.edu.cn</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id20.2" class="ltx_p">Multi-sensor modal fusion has demonstrated strong advantages in 3D object detection tasks. However, existing methods that fuse multi-modal features require transforming features into the bird’s eye view space and may lose certain information on Z-axis, thus leading to inferior performance. To this end, we propose a novel end-to-end multi-modal fusion transformer-based framework, dubbed FusionFormer, that incorporates deformable attention and residual structures within the fusion encoding module. Specifically, by developing a uniform sampling strategy, our method can easily sample from 2D image and 3D voxel features spontaneously, thus exploiting flexible adaptability and avoiding explicit transformation to the bird’s eye view space during the feature concatenation process. We further implement a residual structure in our feature encoder to ensure the model’s robustness in case of missing an input modality. Through extensive experiments on a popular autonomous driving benchmark dataset, nuScenes, our method achieves state-of-the-art single model performance of <math id="id19.1.m1.1" class="ltx_Math" alttext="72.6\%" display="inline"><semantics id="id19.1.m1.1a"><mrow id="id19.1.m1.1.1" xref="id19.1.m1.1.1.cmml"><mn id="id19.1.m1.1.1.2" xref="id19.1.m1.1.1.2.cmml">72.6</mn><mo id="id19.1.m1.1.1.1" xref="id19.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="id19.1.m1.1b"><apply id="id19.1.m1.1.1.cmml" xref="id19.1.m1.1.1"><csymbol cd="latexml" id="id19.1.m1.1.1.1.cmml" xref="id19.1.m1.1.1.1">percent</csymbol><cn type="float" id="id19.1.m1.1.1.2.cmml" xref="id19.1.m1.1.1.2">72.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id19.1.m1.1c">72.6\%</annotation></semantics></math> mAP and <math id="id20.2.m2.1" class="ltx_Math" alttext="75.1\%" display="inline"><semantics id="id20.2.m2.1a"><mrow id="id20.2.m2.1.1" xref="id20.2.m2.1.1.cmml"><mn id="id20.2.m2.1.1.2" xref="id20.2.m2.1.1.2.cmml">75.1</mn><mo id="id20.2.m2.1.1.1" xref="id20.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="id20.2.m2.1b"><apply id="id20.2.m2.1.1.cmml" xref="id20.2.m2.1.1"><csymbol cd="latexml" id="id20.2.m2.1.1.1.cmml" xref="id20.2.m2.1.1.1">percent</csymbol><cn type="float" id="id20.2.m2.1.1.2.cmml" xref="id20.2.m2.1.1.2">75.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id20.2.m2.1c">75.1\%</annotation></semantics></math> NDS in the 3D object detection task without test time augmentation.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2309.05257/assets/x1.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="418" height="191" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S1.F1.6.1" class="ltx_text ltx_font_bold">Comparison between state-of-the-art methods and our FusionFormer.</span> <span id="S1.F1.7.2" class="ltx_text ltx_font_bold">(a)</span> In BEVFusion-based methods, the camera features and points features are transformed into BEV space and fused with concatenation. <span id="S1.F1.8.3" class="ltx_text ltx_font_bold">(b)</span> In CMT, the points voxel features are first compressed into BEV features, then are encoded with the same positional encoding as image features. Then, each object query is passed into a transformer decoder to generate the prediction result. <span id="S1.F1.9.4" class="ltx_text ltx_font_bold">(c)</span> In FusionFormer, the fusion of multi-modal features is achieved by sequentially interacting BEV queries with original point cloud voxel features and image features. This interaction leverages the depth references provided by point cloud features for the view transformer of image features, while the image features complement the sparsity of point cloud features. As a result, more accurate and dense fused BEV representations are obtained. Additionally, FusionFormer incorporates a temporal fusion encoding module, enabling the fusion of BEV features from historical frames. </figcaption>
</figure>
<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Autonomous driving technologies typically rely on multiple sensors for safety considerations, such as LiDAR <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib5" title="" class="ltx_ref">2023</a>; Yin et al., <a href="#bib.bib40" title="" class="ltx_ref">2021</a>; Wang et al., <a href="#bib.bib32" title="" class="ltx_ref">2020</a>; Lang et al., <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite>, cameras <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib34" title="" class="ltx_ref">2021b</a>; <a href="#bib.bib35" title="" class="ltx_ref">2022</a>)</cite>, and radar <cite class="ltx_cite ltx_citemacro_citep">(Meyer &amp; Kuschk, <a href="#bib.bib22" title="" class="ltx_ref">2019</a>; Meyer et al., <a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite>. These sensors possess distinct characteristics. For example, LiDAR can provide accurate yet sparse point clouds with 3D information, while images have dense features but lack such depth information. To enhance performance, multi-modal fusion can be used to integrate the strengths of these sensors. By combining information from multiple sensors, autonomous driving systems can achieve better accuracy and robustness, making them more reliable for real-world applications. Concatenating multi-modality features via simple concatenation in bird’s eye view (BEV) space becomes a defacto standard to achieve state-of-the-art performance. As shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ FusionFormer: A Multi-sensory Fusion in Bird’s-Eye-View and Temporal Consistent Transformer for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, current fusion framework fuses features from LiDAR point cloud and images in BEV space via simple concatenation <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib21" title="" class="ltx_ref">2023</a>; Liang et al., <a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite> or a certain transformer architecture <cite class="ltx_cite ltx_citemacro_citep">(Yan et al., <a href="#bib.bib38" title="" class="ltx_ref">2023</a>)</cite>. However, we conjecture that these approaches has certain two limitations.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">In order to fuse information at BEV level, we must first transform the 2D image features into 3D via certain geometry view transformation <cite class="ltx_cite ltx_citemacro_citep">(Philion &amp; Fidler, <a href="#bib.bib27" title="" class="ltx_ref">2020</a>)</cite>. This process requires using a monocular depth estimation module which is an ill-posed problem and can generate inaccurate feature alignment. We believe that a superior approach is to exploit features from sparse point cloud to assist this process. Concurrently, <cite class="ltx_cite ltx_citemacro_citet">Yan et al. (<a href="#bib.bib38" title="" class="ltx_ref">2023</a>)</cite> proposes a transformer to leverage positional encoding to encode image features, which can be viewed as an alternative approach to alleviate this issue. However, all aforementioned methods explicitly transform the point voxel features into BEV space before the fusion module by compressing the Z-axis dimensional features into vectors. This may hinder the performance of downstream tasks that involves height information, such as 3D object detection where one needs to predict the height of the bounding box.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">To tackle above problems, we propose a novel multi-modal fusion framework for 3D object detection, dubbed FusionFormer to address these challenges. As shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ FusionFormer: A Multi-sensory Fusion in Bird’s-Eye-View and Temporal Consistent Transformer for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (c), FusionFormer can generate fused BEV features by sequentially fusing LiDAR and image features with deformable attention <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a href="#bib.bib44" title="" class="ltx_ref">2020</a>)</cite>, which inherently samples features at the reference points corresponding to the BEV queries. By developing a uniform sampling strategy, our FusionFormer can easily sample from 2D image and 3D voxel features at the same time thus exhibits flexible adaptability across different modality inputs, and avoids explicit transformation and the need of monocular depth estimation. As a result, multi-modal features can be input in their original forms avoiding the information loss when transforming into BEV features. During the fusion encoding process, the point cloud features can serve as depth references for the view transform of image features, while the dense semantic features from images reciprocally complement the sparsity of point cloud features, leading to the generation of more accurate and dense fused BEV features. Notably, the multi-modal fusion encoder incorporates residual structures, ensuring the model’s robustness in the presence of missing point cloud or image features. We also propose a plug-and-play temporal fusion module along with our FusionFormer to support temporal fusion of BEV features from previous frames.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">In addition, to verify the effectiveness and flexibility of our approaches, we use voxel features obtained from monocular depth estimation of only images to replace the features obtained from LiDAR point clouds to construct a FusionFormer that only uses camera modality.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">In summary, we present the following contributions in this paper:</p>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We notice that state-of-the-art multi-modality frameworks need explicitly compressing the voxel features into BEV space before fusing with image features might lead to inferior performance, and propose a novel transformer based framework with a uniform sampling strategy to address this issue.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We also demonstrate that our method is flexible and can be transformed into a camera only 3D object detector by replacing the LiDAR features to image features with monocular depth estimation.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.i3.p1.2" class="ltx_p">Our method achieves state-of-the-art single model performance of <math id="S1.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="72.6\%" display="inline"><semantics id="S1.I1.i3.p1.1.m1.1a"><mrow id="S1.I1.i3.p1.1.m1.1.1" xref="S1.I1.i3.p1.1.m1.1.1.cmml"><mn id="S1.I1.i3.p1.1.m1.1.1.2" xref="S1.I1.i3.p1.1.m1.1.1.2.cmml">72.6</mn><mo id="S1.I1.i3.p1.1.m1.1.1.1" xref="S1.I1.i3.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.I1.i3.p1.1.m1.1b"><apply id="S1.I1.i3.p1.1.m1.1.1.cmml" xref="S1.I1.i3.p1.1.m1.1.1"><csymbol cd="latexml" id="S1.I1.i3.p1.1.m1.1.1.1.cmml" xref="S1.I1.i3.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S1.I1.i3.p1.1.m1.1.1.2.cmml" xref="S1.I1.i3.p1.1.m1.1.1.2">72.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i3.p1.1.m1.1c">72.6\%</annotation></semantics></math> mAP and <math id="S1.I1.i3.p1.2.m2.1" class="ltx_Math" alttext="75.1\%" display="inline"><semantics id="S1.I1.i3.p1.2.m2.1a"><mrow id="S1.I1.i3.p1.2.m2.1.1" xref="S1.I1.i3.p1.2.m2.1.1.cmml"><mn id="S1.I1.i3.p1.2.m2.1.1.2" xref="S1.I1.i3.p1.2.m2.1.1.2.cmml">75.1</mn><mo id="S1.I1.i3.p1.2.m2.1.1.1" xref="S1.I1.i3.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.I1.i3.p1.2.m2.1b"><apply id="S1.I1.i3.p1.2.m2.1.1.cmml" xref="S1.I1.i3.p1.2.m2.1.1"><csymbol cd="latexml" id="S1.I1.i3.p1.2.m2.1.1.1.cmml" xref="S1.I1.i3.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="S1.I1.i3.p1.2.m2.1.1.2.cmml" xref="S1.I1.i3.p1.2.m2.1.1.2">75.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i3.p1.2.m2.1c">75.1\%</annotation></semantics></math> NDS in the 3D object detection task of the nuScenes dataset without test time augmentation.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Visual-centric 3D Object Detection.</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">In recent years, camera-based 3D object detection has gained increasing attention in the field of autonomous driving. Early approaches relied on predicting the 3D parameters of objects based on the results of 2D object detection <cite class="ltx_cite ltx_citemacro_citep">(Park et al., <a href="#bib.bib25" title="" class="ltx_ref">2021</a>; Wang et al., <a href="#bib.bib34" title="" class="ltx_ref">2021b</a>)</cite>. Recently, BEV-based 3D object detection has become a hot research topic <cite class="ltx_cite ltx_citemacro_citep">(Xie et al., <a href="#bib.bib36" title="" class="ltx_ref">2022</a>)</cite>. Compared to previous methods, BEV-based 3D object detection can directly output 3D object detection results around the vehicle using multi-view camera images, without requiring post-processing of detection results in overlapping regions. Inspired by LSS <cite class="ltx_cite ltx_citemacro_citep">(Philion &amp; Fidler, <a href="#bib.bib27" title="" class="ltx_ref">2020</a>)</cite>, recent works like BEVDet <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a href="#bib.bib9" title="" class="ltx_ref">2021</a>)</cite> and BEVDepth <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite> have used bin-based depth prediction to transform multi-view camera features into BEV space. PETR <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib19" title="" class="ltx_ref">2022a</a>)</cite> achieves a camera-based BEV method with transformer by adding 3D position encoding. DETR3D <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib35" title="" class="ltx_ref">2022</a>)</cite> and BEVFormer <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib15" title="" class="ltx_ref">2022c</a>)</cite> use deformable attention to make the query under BEV space interact with local features related to its position projection range during the transformer process, achieving the transformation from multi-view camera space to BEV space.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">LiDAR-centric 3D Object Detection.</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">LiDAR-based 3D object detection methods can be categorized into different types based on the representation form of point cloud features. Point-wise methods extract features directly from individual points and output 3D object detection results end-to-end <cite class="ltx_cite ltx_citemacro_citep">(Qi et al., <a href="#bib.bib28" title="" class="ltx_ref">2018</a>; Paigwar et al., <a href="#bib.bib24" title="" class="ltx_ref">2019</a>)</cite>. BEV-based methods, on the other hand, construct intermediate feature forms before transforming them into BEV space <cite class="ltx_cite ltx_citemacro_citep">(Yin et al., <a href="#bib.bib40" title="" class="ltx_ref">2021</a>)</cite>. For instance, VoxelNet <cite class="ltx_cite ltx_citemacro_citep">(Zhou &amp; Tuzel, <a href="#bib.bib42" title="" class="ltx_ref">2018</a>)</cite> voxelizes the raw point cloud and applies sparse 3D convolutions to obtain voxel features. These features are subsequently compressed along the Z dimension to obtain BEV features. In contrast, Pointpillar <cite class="ltx_cite ltx_citemacro_citep">(Lang et al., <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite> projects the point cloud into multiple pillars and pools the points within each pillar to extract features for BEV-based detection.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Temporal-aware 3D Object Detection.</h4>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">Temporal fusion has emerged as a hot research topic in the field of 3D object detection for its ability to enhance detection stability and perception of target motion. BEVFormer <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib15" title="" class="ltx_ref">2022c</a>)</cite> uses spatiotemporal attention to fuse the historical BEV features of the previous frame with current image features. BEVDet4D <cite class="ltx_cite ltx_citemacro_citep">(Huang &amp; Huang, <a href="#bib.bib8" title="" class="ltx_ref">2022</a>)</cite> employs concatenation to fuse temporally aligned BEV features from adjacent frames. SOLOFusion <cite class="ltx_cite ltx_citemacro_citep">(Park et al., <a href="#bib.bib26" title="" class="ltx_ref">2022</a>)</cite> further leverages this approach to achieve long-term temporal fusion. Some methods perform temporal information fusion directly on the original feature sequences based on query. For instance, PETRv2 <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib20" title="" class="ltx_ref">2022b</a>)</cite> employs global attention and temporal position encoding to fuse temporal information, while Sparse4D <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite> models the relationship between multiple frames based on sparse attention. Additionally, StreamPETR <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib33" title="" class="ltx_ref">2023b</a>)</cite> introduces a method for long-term fusion by leveraging object queries from past frames.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Multi-modal 3D Object Detection.</h4>

<div id="S2.SS0.SSS0.Px4.p1" class="ltx_para ltx_noindent">
<p id="S2.SS0.SSS0.Px4.p1.1" class="ltx_p">Fusing multi-sensory features becomes a de-facto standard in 3D perception tasks. BEVFusion-based methods <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib21" title="" class="ltx_ref">2023</a>; Liang et al., <a href="#bib.bib16" title="" class="ltx_ref">2022</a>; Cai et al., <a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite> obtain image BEV features using view transform <cite class="ltx_cite ltx_citemacro_citep">(Philion &amp; Fidler, <a href="#bib.bib27" title="" class="ltx_ref">2020</a>; Li et al., <a href="#bib.bib15" title="" class="ltx_ref">2022c</a>)</cite> and concatenates them with LIDAR BEV features via simple concatenation. However, such simple stragety may fail to fully exploit the complementary information between multi-modal features. Another line of approaches construct transformer <cite class="ltx_cite ltx_citemacro_citep">(Bai et al., <a href="#bib.bib1" title="" class="ltx_ref">2022</a>; Wang et al., <a href="#bib.bib31" title="" class="ltx_ref">2023a</a>; Yang et al., <a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite> based architectures to perform interaction between image and point-cloud features. These methods relies simultaneously on both image and point cloud modal features, which presents challenges in cases of robustness scenarios when missing a modality data. Concurrently, <cite class="ltx_cite ltx_citemacro_citet">Yan et al. (<a href="#bib.bib38" title="" class="ltx_ref">2023</a>)</cite> proposes a method, dubbed CMT, which adopts 3D position encoding to achieve end-to-end multimodal fusion-based 3D object detection using transformer. Nonetheless, the aforementioned fusion methods rely on compressing point cloud voxel features into BEV representations, which can result in the loss of the height information.
To tackle this, UVTR <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib13" title="" class="ltx_ref">2022b</a>)</cite> introduced knowledge transfer to perform voxel-level multi-modal fusion by directly combining LiDAR voxel features with image voxel features obtained through LSS. However, this approach did not yield notable improvements in performance. Unlike these approaches, FusionFormer demonstrates enhanced adaptability to the input format of multimodal features, allowing direct utilization of point cloud features in voxel form. Moreover, by incorporating deformable attention and residual structures within the fusion encoding module, FusionFormer can achieve both multimodal feature complementarity and robustness in handling missing modal data.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2309.05257/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="422" height="112" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S3.F2.3.1" class="ltx_text ltx_font_bold">(a) The framework of the FusionFormer.</span> The LiDAR point cloud and multi-view images are processed separately in their respective backbone networks to extract voxel features and image features. These features are then inputted into a multi-modal fusion encoder (MMFE) to generate the fused BEV features. The fused BEV features of the current frame, along with the BEV features from historical frames, are jointly fed into a temporal fusion encoder (TFE) to obtain the multi-modal temporal fused BEV features. Finally, the features are utilized in the detection head to produce the final 3D object detection results. <span id="S3.F2.4.2" class="ltx_text ltx_font_bold">(b) The architecture of the Multi-modal Fusion Encoder (MMFE).</span> The BEV queries are initialized and subsequently subjected to self-attention. They are then sequentially utilized for cross-attention with the point cloud voxel features and image features. The resulting BEV queries, updated through a feed-forward network, are propagated as inputs to the subsequent encoder layers. Following multiple layers of fusion encoding, the ultimate fused BEV feature is obtained.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">Here we present our method in detail. Figure <a href="#S3.F2" title="Figure 2 ‣ 3 Method ‣ FusionFormer: A Multi-sensory Fusion in Bird’s-Eye-View and Temporal Consistent Transformer for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (a) illustrates our proposed FusionFormer for multimodal temporal fusion. By utilizing a fusion encoder based on deformable attention <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite>, LiDAR and image features are transformed into fused BEV features. Compared to previous approaches such as BEVFusion <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib21" title="" class="ltx_ref">2023</a>; Liang et al., <a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite>, FusionFormer can adapt to different feature representations of different modalities without requiring pre-transformation into BEV space. The image branch can retain its original 2D feature representation, while the point cloud branch can be represented as BEV features or voxel features. Detailed information regarding the image branch and point cloud branch can be found in the A.1 section of the appendix. The temporal fusion module utilizes deformable attention to fuse BEV features from the current and previous frames that have been temporally aligned. Then the processed multimodal temporal fusion BEV features are input into the detection task head to obtain 3D object detection results.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Multi-modal Fusion Encoder</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">As illustrated in Figure <a href="#S3.F2" title="Figure 2 ‣ 3 Method ‣ FusionFormer: A Multi-sensory Fusion in Bird’s-Eye-View and Temporal Consistent Transformer for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (b), the fusion encoding module consists of 6 layers, each incorporating self-attention, points cross-attention, and images cross-attention. In accordance with the standard transformer architecture, the BEV queries are subjected to self-attention following initialization. Subsequently, points cross-attention is executed to facilitate the integration of LiDAR features, which is further enhanced through images cross-attention to fuse image features. The encoding layer outputs the updated queries as input to the next layer after being processed through a feed-forward network. After 6 layers of fusion encoding, the final multimodal fusion BEV features are obtained.</p>
</div>
<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">BEV Queries.</h4>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px1.p1.3" class="ltx_p">We partition the BEV space within the surrounding region of interest (ROI) range around the vehicle’s center into a grid of <math id="S3.SS1.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="H{\times}W" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.1.m1.1a"><mrow id="S3.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.1" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml">×</mo><mi id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml">W</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1"><times id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.1"></times><ci id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2">𝐻</ci><ci id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3">𝑊</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.1.m1.1c">H{\times}W</annotation></semantics></math> cells. Correspondingly, we define a set of learnable parameters <math id="S3.SS1.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.2.m2.1a"><mi id="S3.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.2.m2.1b"><ci id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.2.m2.1c">Q</annotation></semantics></math> to serve as the queries for the BEV space. Each <math id="S3.SS1.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.3.m3.1a"><mi id="S3.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.3.m3.1b"><ci id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.3.m3.1c">q</annotation></semantics></math> corresponds to a cell in the BEV space. Prior to inputting Q into the fusion encoder, the BEV queries are subjected to position encoding based on their corresponding BEV spatial coordinates <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib15" title="" class="ltx_ref">2022c</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Self-Attention.</h4>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">To reduce computational resource usage, we implemented the self-attention based on deformable attention. Each BEV query interacts only with its corresponding queries within the ROI range. This process is achieved through feature sampling at the 2D reference points for each query as illustrated below:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.4" class="ltx_Math" alttext="SA(Q_{p})=DefAttn(Q_{p},p,Q)" display="block"><semantics id="S3.E1.m1.4a"><mrow id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml"><mrow id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.cmml"><mi id="S3.E1.m1.3.3.1.3" xref="S3.E1.m1.3.3.1.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.2" xref="S3.E1.m1.3.3.1.2.cmml">​</mo><mi id="S3.E1.m1.3.3.1.4" xref="S3.E1.m1.3.3.1.4.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.2a" xref="S3.E1.m1.3.3.1.2.cmml">​</mo><mrow id="S3.E1.m1.3.3.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.3.3.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.2.cmml">Q</mi><mi id="S3.E1.m1.3.3.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.3.cmml">p</mi></msub><mo stretchy="false" id="S3.E1.m1.3.3.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.4.4.3" xref="S3.E1.m1.4.4.3.cmml">=</mo><mrow id="S3.E1.m1.4.4.2" xref="S3.E1.m1.4.4.2.cmml"><mi id="S3.E1.m1.4.4.2.3" xref="S3.E1.m1.4.4.2.3.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.2.2" xref="S3.E1.m1.4.4.2.2.cmml">​</mo><mi id="S3.E1.m1.4.4.2.4" xref="S3.E1.m1.4.4.2.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.2.2a" xref="S3.E1.m1.4.4.2.2.cmml">​</mo><mi id="S3.E1.m1.4.4.2.5" xref="S3.E1.m1.4.4.2.5.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.2.2b" xref="S3.E1.m1.4.4.2.2.cmml">​</mo><mi id="S3.E1.m1.4.4.2.6" xref="S3.E1.m1.4.4.2.6.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.2.2c" xref="S3.E1.m1.4.4.2.2.cmml">​</mo><mi id="S3.E1.m1.4.4.2.7" xref="S3.E1.m1.4.4.2.7.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.2.2d" xref="S3.E1.m1.4.4.2.2.cmml">​</mo><mi id="S3.E1.m1.4.4.2.8" xref="S3.E1.m1.4.4.2.8.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.2.2e" xref="S3.E1.m1.4.4.2.2.cmml">​</mo><mi id="S3.E1.m1.4.4.2.9" xref="S3.E1.m1.4.4.2.9.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.2.2f" xref="S3.E1.m1.4.4.2.2.cmml">​</mo><mrow id="S3.E1.m1.4.4.2.1.1" xref="S3.E1.m1.4.4.2.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.2.1.1.2" xref="S3.E1.m1.4.4.2.1.2.cmml">(</mo><msub id="S3.E1.m1.4.4.2.1.1.1" xref="S3.E1.m1.4.4.2.1.1.1.cmml"><mi id="S3.E1.m1.4.4.2.1.1.1.2" xref="S3.E1.m1.4.4.2.1.1.1.2.cmml">Q</mi><mi id="S3.E1.m1.4.4.2.1.1.1.3" xref="S3.E1.m1.4.4.2.1.1.1.3.cmml">p</mi></msub><mo id="S3.E1.m1.4.4.2.1.1.3" xref="S3.E1.m1.4.4.2.1.2.cmml">,</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">p</mi><mo id="S3.E1.m1.4.4.2.1.1.4" xref="S3.E1.m1.4.4.2.1.2.cmml">,</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">Q</mi><mo stretchy="false" id="S3.E1.m1.4.4.2.1.1.5" xref="S3.E1.m1.4.4.2.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.4b"><apply id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4"><eq id="S3.E1.m1.4.4.3.cmml" xref="S3.E1.m1.4.4.3"></eq><apply id="S3.E1.m1.3.3.1.cmml" xref="S3.E1.m1.3.3.1"><times id="S3.E1.m1.3.3.1.2.cmml" xref="S3.E1.m1.3.3.1.2"></times><ci id="S3.E1.m1.3.3.1.3.cmml" xref="S3.E1.m1.3.3.1.3">𝑆</ci><ci id="S3.E1.m1.3.3.1.4.cmml" xref="S3.E1.m1.3.3.1.4">𝐴</ci><apply id="S3.E1.m1.3.3.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2">𝑄</ci><ci id="S3.E1.m1.3.3.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3">𝑝</ci></apply></apply><apply id="S3.E1.m1.4.4.2.cmml" xref="S3.E1.m1.4.4.2"><times id="S3.E1.m1.4.4.2.2.cmml" xref="S3.E1.m1.4.4.2.2"></times><ci id="S3.E1.m1.4.4.2.3.cmml" xref="S3.E1.m1.4.4.2.3">𝐷</ci><ci id="S3.E1.m1.4.4.2.4.cmml" xref="S3.E1.m1.4.4.2.4">𝑒</ci><ci id="S3.E1.m1.4.4.2.5.cmml" xref="S3.E1.m1.4.4.2.5">𝑓</ci><ci id="S3.E1.m1.4.4.2.6.cmml" xref="S3.E1.m1.4.4.2.6">𝐴</ci><ci id="S3.E1.m1.4.4.2.7.cmml" xref="S3.E1.m1.4.4.2.7">𝑡</ci><ci id="S3.E1.m1.4.4.2.8.cmml" xref="S3.E1.m1.4.4.2.8">𝑡</ci><ci id="S3.E1.m1.4.4.2.9.cmml" xref="S3.E1.m1.4.4.2.9">𝑛</ci><vector id="S3.E1.m1.4.4.2.1.2.cmml" xref="S3.E1.m1.4.4.2.1.1"><apply id="S3.E1.m1.4.4.2.1.1.1.cmml" xref="S3.E1.m1.4.4.2.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.2.1.1.1.1.cmml" xref="S3.E1.m1.4.4.2.1.1.1">subscript</csymbol><ci id="S3.E1.m1.4.4.2.1.1.1.2.cmml" xref="S3.E1.m1.4.4.2.1.1.1.2">𝑄</ci><ci id="S3.E1.m1.4.4.2.1.1.1.3.cmml" xref="S3.E1.m1.4.4.2.1.1.1.3">𝑝</ci></apply><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑝</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝑄</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.4c">SA(Q_{p})=DefAttn(Q_{p},p,Q)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.SSS0.Px2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px2.p2.2" class="ltx_p">where <math id="S3.SS1.SSS0.Px2.p2.1.m1.1" class="ltx_Math" alttext="Q_{p}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p2.1.m1.1a"><msub id="S3.SS1.SSS0.Px2.p2.1.m1.1.1" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.2" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.2.cmml">Q</mi><mi id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p2.1.m1.1b"><apply id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.2">𝑄</ci><ci id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p2.1.m1.1c">Q_{p}</annotation></semantics></math> represents the BEV query at point <math id="S3.SS1.SSS0.Px2.p2.2.m2.2" class="ltx_Math" alttext="p=(x,y)" display="inline"><semantics id="S3.SS1.SSS0.Px2.p2.2.m2.2a"><mrow id="S3.SS1.SSS0.Px2.p2.2.m2.2.3" xref="S3.SS1.SSS0.Px2.p2.2.m2.2.3.cmml"><mi id="S3.SS1.SSS0.Px2.p2.2.m2.2.3.2" xref="S3.SS1.SSS0.Px2.p2.2.m2.2.3.2.cmml">p</mi><mo id="S3.SS1.SSS0.Px2.p2.2.m2.2.3.1" xref="S3.SS1.SSS0.Px2.p2.2.m2.2.3.1.cmml">=</mo><mrow id="S3.SS1.SSS0.Px2.p2.2.m2.2.3.3.2" xref="S3.SS1.SSS0.Px2.p2.2.m2.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px2.p2.2.m2.2.3.3.2.1" xref="S3.SS1.SSS0.Px2.p2.2.m2.2.3.3.1.cmml">(</mo><mi id="S3.SS1.SSS0.Px2.p2.2.m2.1.1" xref="S3.SS1.SSS0.Px2.p2.2.m2.1.1.cmml">x</mi><mo id="S3.SS1.SSS0.Px2.p2.2.m2.2.3.3.2.2" xref="S3.SS1.SSS0.Px2.p2.2.m2.2.3.3.1.cmml">,</mo><mi id="S3.SS1.SSS0.Px2.p2.2.m2.2.2" xref="S3.SS1.SSS0.Px2.p2.2.m2.2.2.cmml">y</mi><mo stretchy="false" id="S3.SS1.SSS0.Px2.p2.2.m2.2.3.3.2.3" xref="S3.SS1.SSS0.Px2.p2.2.m2.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p2.2.m2.2b"><apply id="S3.SS1.SSS0.Px2.p2.2.m2.2.3.cmml" xref="S3.SS1.SSS0.Px2.p2.2.m2.2.3"><eq id="S3.SS1.SSS0.Px2.p2.2.m2.2.3.1.cmml" xref="S3.SS1.SSS0.Px2.p2.2.m2.2.3.1"></eq><ci id="S3.SS1.SSS0.Px2.p2.2.m2.2.3.2.cmml" xref="S3.SS1.SSS0.Px2.p2.2.m2.2.3.2">𝑝</ci><interval closure="open" id="S3.SS1.SSS0.Px2.p2.2.m2.2.3.3.1.cmml" xref="S3.SS1.SSS0.Px2.p2.2.m2.2.3.3.2"><ci id="S3.SS1.SSS0.Px2.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px2.p2.2.m2.1.1">𝑥</ci><ci id="S3.SS1.SSS0.Px2.p2.2.m2.2.2.cmml" xref="S3.SS1.SSS0.Px2.p2.2.m2.2.2">𝑦</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p2.2.m2.2c">p=(x,y)</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Points Cross-Attention.</h4>

<div id="S3.SS1.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px3.p1.1" class="ltx_p">The points cross-attention layer is also implemented based on deformable attention, but the specific manner in which points cross-attention is implemented varies depending on the form of the LiDAR points features. For the case where BEV features are used as input, we implement the points cross-attention layer as follows:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.5" class="ltx_Math" alttext="PCA_{2D}(Q_{p},B_{pts})=DefAttn(Q_{p},P_{2D},B_{pts})" display="block"><semantics id="S3.E2.m1.5a"><mrow id="S3.E2.m1.5.5" xref="S3.E2.m1.5.5.cmml"><mrow id="S3.E2.m1.2.2.2" xref="S3.E2.m1.2.2.2.cmml"><mi id="S3.E2.m1.2.2.2.4" xref="S3.E2.m1.2.2.2.4.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.3" xref="S3.E2.m1.2.2.2.3.cmml">​</mo><mi id="S3.E2.m1.2.2.2.5" xref="S3.E2.m1.2.2.2.5.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.3a" xref="S3.E2.m1.2.2.2.3.cmml">​</mo><msub id="S3.E2.m1.2.2.2.6" xref="S3.E2.m1.2.2.2.6.cmml"><mi id="S3.E2.m1.2.2.2.6.2" xref="S3.E2.m1.2.2.2.6.2.cmml">A</mi><mrow id="S3.E2.m1.2.2.2.6.3" xref="S3.E2.m1.2.2.2.6.3.cmml"><mn id="S3.E2.m1.2.2.2.6.3.2" xref="S3.E2.m1.2.2.2.6.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.6.3.1" xref="S3.E2.m1.2.2.2.6.3.1.cmml">​</mo><mi id="S3.E2.m1.2.2.2.6.3.3" xref="S3.E2.m1.2.2.2.6.3.3.cmml">D</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.3b" xref="S3.E2.m1.2.2.2.3.cmml">​</mo><mrow id="S3.E2.m1.2.2.2.2.2" xref="S3.E2.m1.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.2.2.2.2.2.3" xref="S3.E2.m1.2.2.2.2.3.cmml">(</mo><msub id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml">Q</mi><mi id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.3.cmml">p</mi></msub><mo id="S3.E2.m1.2.2.2.2.2.4" xref="S3.E2.m1.2.2.2.2.3.cmml">,</mo><msub id="S3.E2.m1.2.2.2.2.2.2" xref="S3.E2.m1.2.2.2.2.2.2.cmml"><mi id="S3.E2.m1.2.2.2.2.2.2.2" xref="S3.E2.m1.2.2.2.2.2.2.2.cmml">B</mi><mrow id="S3.E2.m1.2.2.2.2.2.2.3" xref="S3.E2.m1.2.2.2.2.2.2.3.cmml"><mi id="S3.E2.m1.2.2.2.2.2.2.3.2" xref="S3.E2.m1.2.2.2.2.2.2.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.2.2.2.3.1" xref="S3.E2.m1.2.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.E2.m1.2.2.2.2.2.2.3.3" xref="S3.E2.m1.2.2.2.2.2.2.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.2.2.2.3.1a" xref="S3.E2.m1.2.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.E2.m1.2.2.2.2.2.2.3.4" xref="S3.E2.m1.2.2.2.2.2.2.3.4.cmml">s</mi></mrow></msub><mo stretchy="false" id="S3.E2.m1.2.2.2.2.2.5" xref="S3.E2.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.5.5.6" xref="S3.E2.m1.5.5.6.cmml">=</mo><mrow id="S3.E2.m1.5.5.5" xref="S3.E2.m1.5.5.5.cmml"><mi id="S3.E2.m1.5.5.5.5" xref="S3.E2.m1.5.5.5.5.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.5.4" xref="S3.E2.m1.5.5.5.4.cmml">​</mo><mi id="S3.E2.m1.5.5.5.6" xref="S3.E2.m1.5.5.5.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.5.4a" xref="S3.E2.m1.5.5.5.4.cmml">​</mo><mi id="S3.E2.m1.5.5.5.7" xref="S3.E2.m1.5.5.5.7.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.5.4b" xref="S3.E2.m1.5.5.5.4.cmml">​</mo><mi id="S3.E2.m1.5.5.5.8" xref="S3.E2.m1.5.5.5.8.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.5.4c" xref="S3.E2.m1.5.5.5.4.cmml">​</mo><mi id="S3.E2.m1.5.5.5.9" xref="S3.E2.m1.5.5.5.9.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.5.4d" xref="S3.E2.m1.5.5.5.4.cmml">​</mo><mi id="S3.E2.m1.5.5.5.10" xref="S3.E2.m1.5.5.5.10.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.5.4e" xref="S3.E2.m1.5.5.5.4.cmml">​</mo><mi id="S3.E2.m1.5.5.5.11" xref="S3.E2.m1.5.5.5.11.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.5.4f" xref="S3.E2.m1.5.5.5.4.cmml">​</mo><mrow id="S3.E2.m1.5.5.5.3.3" xref="S3.E2.m1.5.5.5.3.4.cmml"><mo stretchy="false" id="S3.E2.m1.5.5.5.3.3.4" xref="S3.E2.m1.5.5.5.3.4.cmml">(</mo><msub id="S3.E2.m1.3.3.3.1.1.1" xref="S3.E2.m1.3.3.3.1.1.1.cmml"><mi id="S3.E2.m1.3.3.3.1.1.1.2" xref="S3.E2.m1.3.3.3.1.1.1.2.cmml">Q</mi><mi id="S3.E2.m1.3.3.3.1.1.1.3" xref="S3.E2.m1.3.3.3.1.1.1.3.cmml">p</mi></msub><mo id="S3.E2.m1.5.5.5.3.3.5" xref="S3.E2.m1.5.5.5.3.4.cmml">,</mo><msub id="S3.E2.m1.4.4.4.2.2.2" xref="S3.E2.m1.4.4.4.2.2.2.cmml"><mi id="S3.E2.m1.4.4.4.2.2.2.2" xref="S3.E2.m1.4.4.4.2.2.2.2.cmml">P</mi><mrow id="S3.E2.m1.4.4.4.2.2.2.3" xref="S3.E2.m1.4.4.4.2.2.2.3.cmml"><mn id="S3.E2.m1.4.4.4.2.2.2.3.2" xref="S3.E2.m1.4.4.4.2.2.2.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.4.2.2.2.3.1" xref="S3.E2.m1.4.4.4.2.2.2.3.1.cmml">​</mo><mi id="S3.E2.m1.4.4.4.2.2.2.3.3" xref="S3.E2.m1.4.4.4.2.2.2.3.3.cmml">D</mi></mrow></msub><mo id="S3.E2.m1.5.5.5.3.3.6" xref="S3.E2.m1.5.5.5.3.4.cmml">,</mo><msub id="S3.E2.m1.5.5.5.3.3.3" xref="S3.E2.m1.5.5.5.3.3.3.cmml"><mi id="S3.E2.m1.5.5.5.3.3.3.2" xref="S3.E2.m1.5.5.5.3.3.3.2.cmml">B</mi><mrow id="S3.E2.m1.5.5.5.3.3.3.3" xref="S3.E2.m1.5.5.5.3.3.3.3.cmml"><mi id="S3.E2.m1.5.5.5.3.3.3.3.2" xref="S3.E2.m1.5.5.5.3.3.3.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.5.3.3.3.3.1" xref="S3.E2.m1.5.5.5.3.3.3.3.1.cmml">​</mo><mi id="S3.E2.m1.5.5.5.3.3.3.3.3" xref="S3.E2.m1.5.5.5.3.3.3.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.5.3.3.3.3.1a" xref="S3.E2.m1.5.5.5.3.3.3.3.1.cmml">​</mo><mi id="S3.E2.m1.5.5.5.3.3.3.3.4" xref="S3.E2.m1.5.5.5.3.3.3.3.4.cmml">s</mi></mrow></msub><mo stretchy="false" id="S3.E2.m1.5.5.5.3.3.7" xref="S3.E2.m1.5.5.5.3.4.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.5b"><apply id="S3.E2.m1.5.5.cmml" xref="S3.E2.m1.5.5"><eq id="S3.E2.m1.5.5.6.cmml" xref="S3.E2.m1.5.5.6"></eq><apply id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2.2"><times id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.3"></times><ci id="S3.E2.m1.2.2.2.4.cmml" xref="S3.E2.m1.2.2.2.4">𝑃</ci><ci id="S3.E2.m1.2.2.2.5.cmml" xref="S3.E2.m1.2.2.2.5">𝐶</ci><apply id="S3.E2.m1.2.2.2.6.cmml" xref="S3.E2.m1.2.2.2.6"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.6.1.cmml" xref="S3.E2.m1.2.2.2.6">subscript</csymbol><ci id="S3.E2.m1.2.2.2.6.2.cmml" xref="S3.E2.m1.2.2.2.6.2">𝐴</ci><apply id="S3.E2.m1.2.2.2.6.3.cmml" xref="S3.E2.m1.2.2.2.6.3"><times id="S3.E2.m1.2.2.2.6.3.1.cmml" xref="S3.E2.m1.2.2.2.6.3.1"></times><cn type="integer" id="S3.E2.m1.2.2.2.6.3.2.cmml" xref="S3.E2.m1.2.2.2.6.3.2">2</cn><ci id="S3.E2.m1.2.2.2.6.3.3.cmml" xref="S3.E2.m1.2.2.2.6.3.3">𝐷</ci></apply></apply><interval closure="open" id="S3.E2.m1.2.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.2.2"><apply id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2">𝑄</ci><ci id="S3.E2.m1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3">𝑝</ci></apply><apply id="S3.E2.m1.2.2.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.2.2.1.cmml" xref="S3.E2.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E2.m1.2.2.2.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2">𝐵</ci><apply id="S3.E2.m1.2.2.2.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.2.2.2.3"><times id="S3.E2.m1.2.2.2.2.2.2.3.1.cmml" xref="S3.E2.m1.2.2.2.2.2.2.3.1"></times><ci id="S3.E2.m1.2.2.2.2.2.2.3.2.cmml" xref="S3.E2.m1.2.2.2.2.2.2.3.2">𝑝</ci><ci id="S3.E2.m1.2.2.2.2.2.2.3.3.cmml" xref="S3.E2.m1.2.2.2.2.2.2.3.3">𝑡</ci><ci id="S3.E2.m1.2.2.2.2.2.2.3.4.cmml" xref="S3.E2.m1.2.2.2.2.2.2.3.4">𝑠</ci></apply></apply></interval></apply><apply id="S3.E2.m1.5.5.5.cmml" xref="S3.E2.m1.5.5.5"><times id="S3.E2.m1.5.5.5.4.cmml" xref="S3.E2.m1.5.5.5.4"></times><ci id="S3.E2.m1.5.5.5.5.cmml" xref="S3.E2.m1.5.5.5.5">𝐷</ci><ci id="S3.E2.m1.5.5.5.6.cmml" xref="S3.E2.m1.5.5.5.6">𝑒</ci><ci id="S3.E2.m1.5.5.5.7.cmml" xref="S3.E2.m1.5.5.5.7">𝑓</ci><ci id="S3.E2.m1.5.5.5.8.cmml" xref="S3.E2.m1.5.5.5.8">𝐴</ci><ci id="S3.E2.m1.5.5.5.9.cmml" xref="S3.E2.m1.5.5.5.9">𝑡</ci><ci id="S3.E2.m1.5.5.5.10.cmml" xref="S3.E2.m1.5.5.5.10">𝑡</ci><ci id="S3.E2.m1.5.5.5.11.cmml" xref="S3.E2.m1.5.5.5.11">𝑛</ci><vector id="S3.E2.m1.5.5.5.3.4.cmml" xref="S3.E2.m1.5.5.5.3.3"><apply id="S3.E2.m1.3.3.3.1.1.1.cmml" xref="S3.E2.m1.3.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.3.1.1.1.1.cmml" xref="S3.E2.m1.3.3.3.1.1.1">subscript</csymbol><ci id="S3.E2.m1.3.3.3.1.1.1.2.cmml" xref="S3.E2.m1.3.3.3.1.1.1.2">𝑄</ci><ci id="S3.E2.m1.3.3.3.1.1.1.3.cmml" xref="S3.E2.m1.3.3.3.1.1.1.3">𝑝</ci></apply><apply id="S3.E2.m1.4.4.4.2.2.2.cmml" xref="S3.E2.m1.4.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.4.2.2.2.1.cmml" xref="S3.E2.m1.4.4.4.2.2.2">subscript</csymbol><ci id="S3.E2.m1.4.4.4.2.2.2.2.cmml" xref="S3.E2.m1.4.4.4.2.2.2.2">𝑃</ci><apply id="S3.E2.m1.4.4.4.2.2.2.3.cmml" xref="S3.E2.m1.4.4.4.2.2.2.3"><times id="S3.E2.m1.4.4.4.2.2.2.3.1.cmml" xref="S3.E2.m1.4.4.4.2.2.2.3.1"></times><cn type="integer" id="S3.E2.m1.4.4.4.2.2.2.3.2.cmml" xref="S3.E2.m1.4.4.4.2.2.2.3.2">2</cn><ci id="S3.E2.m1.4.4.4.2.2.2.3.3.cmml" xref="S3.E2.m1.4.4.4.2.2.2.3.3">𝐷</ci></apply></apply><apply id="S3.E2.m1.5.5.5.3.3.3.cmml" xref="S3.E2.m1.5.5.5.3.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.5.3.3.3.1.cmml" xref="S3.E2.m1.5.5.5.3.3.3">subscript</csymbol><ci id="S3.E2.m1.5.5.5.3.3.3.2.cmml" xref="S3.E2.m1.5.5.5.3.3.3.2">𝐵</ci><apply id="S3.E2.m1.5.5.5.3.3.3.3.cmml" xref="S3.E2.m1.5.5.5.3.3.3.3"><times id="S3.E2.m1.5.5.5.3.3.3.3.1.cmml" xref="S3.E2.m1.5.5.5.3.3.3.3.1"></times><ci id="S3.E2.m1.5.5.5.3.3.3.3.2.cmml" xref="S3.E2.m1.5.5.5.3.3.3.3.2">𝑝</ci><ci id="S3.E2.m1.5.5.5.3.3.3.3.3.cmml" xref="S3.E2.m1.5.5.5.3.3.3.3.3">𝑡</ci><ci id="S3.E2.m1.5.5.5.3.3.3.3.4.cmml" xref="S3.E2.m1.5.5.5.3.3.3.3.4">𝑠</ci></apply></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.5c">PCA_{2D}(Q_{p},B_{pts})=DefAttn(Q_{p},P_{2D},B_{pts})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.SSS0.Px3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px3.p2.3" class="ltx_p">where <math id="S3.SS1.SSS0.Px3.p2.1.m1.1" class="ltx_Math" alttext="B_{pts}" display="inline"><semantics id="S3.SS1.SSS0.Px3.p2.1.m1.1a"><msub id="S3.SS1.SSS0.Px3.p2.1.m1.1.1" xref="S3.SS1.SSS0.Px3.p2.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p2.1.m1.1.1.2" xref="S3.SS1.SSS0.Px3.p2.1.m1.1.1.2.cmml">B</mi><mrow id="S3.SS1.SSS0.Px3.p2.1.m1.1.1.3" xref="S3.SS1.SSS0.Px3.p2.1.m1.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px3.p2.1.m1.1.1.3.2" xref="S3.SS1.SSS0.Px3.p2.1.m1.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px3.p2.1.m1.1.1.3.1" xref="S3.SS1.SSS0.Px3.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px3.p2.1.m1.1.1.3.3" xref="S3.SS1.SSS0.Px3.p2.1.m1.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px3.p2.1.m1.1.1.3.1a" xref="S3.SS1.SSS0.Px3.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px3.p2.1.m1.1.1.3.4" xref="S3.SS1.SSS0.Px3.p2.1.m1.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p2.1.m1.1b"><apply id="S3.SS1.SSS0.Px3.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p2.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p2.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p2.1.m1.1.1.2">𝐵</ci><apply id="S3.SS1.SSS0.Px3.p2.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p2.1.m1.1.1.3"><times id="S3.SS1.SSS0.Px3.p2.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px3.p2.1.m1.1.1.3.1"></times><ci id="S3.SS1.SSS0.Px3.p2.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px3.p2.1.m1.1.1.3.2">𝑝</ci><ci id="S3.SS1.SSS0.Px3.p2.1.m1.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px3.p2.1.m1.1.1.3.3">𝑡</ci><ci id="S3.SS1.SSS0.Px3.p2.1.m1.1.1.3.4.cmml" xref="S3.SS1.SSS0.Px3.p2.1.m1.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p2.1.m1.1c">B_{pts}</annotation></semantics></math> represents the BEV features output by the LiDAR branch, and <math id="S3.SS1.SSS0.Px3.p2.2.m2.2" class="ltx_Math" alttext="P_{2D}=(x_{2D},y_{2D})" display="inline"><semantics id="S3.SS1.SSS0.Px3.p2.2.m2.2a"><mrow id="S3.SS1.SSS0.Px3.p2.2.m2.2.2" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.cmml"><msub id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.4" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.4.cmml"><mi id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.4.2" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.4.2.cmml">P</mi><mrow id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.4.3" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.4.3.cmml"><mn id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.4.3.2" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.4.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.4.3.1" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.4.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.4.3.3" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.4.3.3.cmml">D</mi></mrow></msub><mo id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.3" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.3.cmml">=</mo><mrow id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.3" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.3.cmml">(</mo><msub id="S3.SS1.SSS0.Px3.p2.2.m2.1.1.1.1.1" xref="S3.SS1.SSS0.Px3.p2.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p2.2.m2.1.1.1.1.1.2" xref="S3.SS1.SSS0.Px3.p2.2.m2.1.1.1.1.1.2.cmml">x</mi><mrow id="S3.SS1.SSS0.Px3.p2.2.m2.1.1.1.1.1.3" xref="S3.SS1.SSS0.Px3.p2.2.m2.1.1.1.1.1.3.cmml"><mn id="S3.SS1.SSS0.Px3.p2.2.m2.1.1.1.1.1.3.2" xref="S3.SS1.SSS0.Px3.p2.2.m2.1.1.1.1.1.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px3.p2.2.m2.1.1.1.1.1.3.1" xref="S3.SS1.SSS0.Px3.p2.2.m2.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px3.p2.2.m2.1.1.1.1.1.3.3" xref="S3.SS1.SSS0.Px3.p2.2.m2.1.1.1.1.1.3.3.cmml">D</mi></mrow></msub><mo id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.4" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.3.cmml">,</mo><msub id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.2" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.2.cmml"><mi id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.2.2" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.2.2.cmml">y</mi><mrow id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.2.3" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.2.3.cmml"><mn id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.2.3.2" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.2.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.2.3.1" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.2.3.3" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.2.3.3.cmml">D</mi></mrow></msub><mo stretchy="false" id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.5" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p2.2.m2.2b"><apply id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.cmml" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2"><eq id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.3.cmml" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.3"></eq><apply id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.4.cmml" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.4"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.4.1.cmml" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.4">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.4.2.cmml" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.4.2">𝑃</ci><apply id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.4.3.cmml" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.4.3"><times id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.4.3.1.cmml" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.4.3.1"></times><cn type="integer" id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.4.3.2.cmml" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.4.3.2">2</cn><ci id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.4.3.3.cmml" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.4.3.3">𝐷</ci></apply></apply><interval closure="open" id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.3.cmml" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2"><apply id="S3.SS1.SSS0.Px3.p2.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p2.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p2.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p2.2.m2.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p2.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p2.2.m2.1.1.1.1.1.2">𝑥</ci><apply id="S3.SS1.SSS0.Px3.p2.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p2.2.m2.1.1.1.1.1.3"><times id="S3.SS1.SSS0.Px3.p2.2.m2.1.1.1.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px3.p2.2.m2.1.1.1.1.1.3.1"></times><cn type="integer" id="S3.SS1.SSS0.Px3.p2.2.m2.1.1.1.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px3.p2.2.m2.1.1.1.1.1.3.2">2</cn><ci id="S3.SS1.SSS0.Px3.p2.2.m2.1.1.1.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px3.p2.2.m2.1.1.1.1.1.3.3">𝐷</ci></apply></apply><apply id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.2.cmml" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.2.1.cmml" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.2.2.cmml" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.2.2">𝑦</ci><apply id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.2.3.cmml" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.2.3"><times id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.2.3.1.cmml" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.2.3.1"></times><cn type="integer" id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.2.3.2.cmml" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.2.3.2">2</cn><ci id="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.2.3.3.cmml" xref="S3.SS1.SSS0.Px3.p2.2.m2.2.2.2.2.2.3.3">𝐷</ci></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p2.2.m2.2c">P_{2D}=(x_{2D},y_{2D})</annotation></semantics></math> represents the 2D projection of the coordinate <math id="S3.SS1.SSS0.Px3.p2.3.m3.2" class="ltx_Math" alttext="p=(x,y)" display="inline"><semantics id="S3.SS1.SSS0.Px3.p2.3.m3.2a"><mrow id="S3.SS1.SSS0.Px3.p2.3.m3.2.3" xref="S3.SS1.SSS0.Px3.p2.3.m3.2.3.cmml"><mi id="S3.SS1.SSS0.Px3.p2.3.m3.2.3.2" xref="S3.SS1.SSS0.Px3.p2.3.m3.2.3.2.cmml">p</mi><mo id="S3.SS1.SSS0.Px3.p2.3.m3.2.3.1" xref="S3.SS1.SSS0.Px3.p2.3.m3.2.3.1.cmml">=</mo><mrow id="S3.SS1.SSS0.Px3.p2.3.m3.2.3.3.2" xref="S3.SS1.SSS0.Px3.p2.3.m3.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px3.p2.3.m3.2.3.3.2.1" xref="S3.SS1.SSS0.Px3.p2.3.m3.2.3.3.1.cmml">(</mo><mi id="S3.SS1.SSS0.Px3.p2.3.m3.1.1" xref="S3.SS1.SSS0.Px3.p2.3.m3.1.1.cmml">x</mi><mo id="S3.SS1.SSS0.Px3.p2.3.m3.2.3.3.2.2" xref="S3.SS1.SSS0.Px3.p2.3.m3.2.3.3.1.cmml">,</mo><mi id="S3.SS1.SSS0.Px3.p2.3.m3.2.2" xref="S3.SS1.SSS0.Px3.p2.3.m3.2.2.cmml">y</mi><mo stretchy="false" id="S3.SS1.SSS0.Px3.p2.3.m3.2.3.3.2.3" xref="S3.SS1.SSS0.Px3.p2.3.m3.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p2.3.m3.2b"><apply id="S3.SS1.SSS0.Px3.p2.3.m3.2.3.cmml" xref="S3.SS1.SSS0.Px3.p2.3.m3.2.3"><eq id="S3.SS1.SSS0.Px3.p2.3.m3.2.3.1.cmml" xref="S3.SS1.SSS0.Px3.p2.3.m3.2.3.1"></eq><ci id="S3.SS1.SSS0.Px3.p2.3.m3.2.3.2.cmml" xref="S3.SS1.SSS0.Px3.p2.3.m3.2.3.2">𝑝</ci><interval closure="open" id="S3.SS1.SSS0.Px3.p2.3.m3.2.3.3.1.cmml" xref="S3.SS1.SSS0.Px3.p2.3.m3.2.3.3.2"><ci id="S3.SS1.SSS0.Px3.p2.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px3.p2.3.m3.1.1">𝑥</ci><ci id="S3.SS1.SSS0.Px3.p2.3.m3.2.2.cmml" xref="S3.SS1.SSS0.Px3.p2.3.m3.2.2">𝑦</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p2.3.m3.2c">p=(x,y)</annotation></semantics></math> onto the point cloud BEV space.</p>
</div>
<div id="S3.SS1.SSS0.Px3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px3.p3.1" class="ltx_p">For the case where voxel features are used as input, the points cross-attention layer is implemented as follows:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.7" class="ltx_Math" alttext="PCA_{3D}(Q_{p},V_{pts})=\sum_{i=1}^{N_{ref}}DefAttn(Q_{p},P_{3D}(p,i),V_{pts})" display="block"><semantics id="S3.E3.m1.7a"><mrow id="S3.E3.m1.7.7" xref="S3.E3.m1.7.7.cmml"><mrow id="S3.E3.m1.4.4.2" xref="S3.E3.m1.4.4.2.cmml"><mi id="S3.E3.m1.4.4.2.4" xref="S3.E3.m1.4.4.2.4.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.2.3" xref="S3.E3.m1.4.4.2.3.cmml">​</mo><mi id="S3.E3.m1.4.4.2.5" xref="S3.E3.m1.4.4.2.5.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.2.3a" xref="S3.E3.m1.4.4.2.3.cmml">​</mo><msub id="S3.E3.m1.4.4.2.6" xref="S3.E3.m1.4.4.2.6.cmml"><mi id="S3.E3.m1.4.4.2.6.2" xref="S3.E3.m1.4.4.2.6.2.cmml">A</mi><mrow id="S3.E3.m1.4.4.2.6.3" xref="S3.E3.m1.4.4.2.6.3.cmml"><mn id="S3.E3.m1.4.4.2.6.3.2" xref="S3.E3.m1.4.4.2.6.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.2.6.3.1" xref="S3.E3.m1.4.4.2.6.3.1.cmml">​</mo><mi id="S3.E3.m1.4.4.2.6.3.3" xref="S3.E3.m1.4.4.2.6.3.3.cmml">D</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.2.3b" xref="S3.E3.m1.4.4.2.3.cmml">​</mo><mrow id="S3.E3.m1.4.4.2.2.2" xref="S3.E3.m1.4.4.2.2.3.cmml"><mo stretchy="false" id="S3.E3.m1.4.4.2.2.2.3" xref="S3.E3.m1.4.4.2.2.3.cmml">(</mo><msub id="S3.E3.m1.3.3.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.cmml"><mi id="S3.E3.m1.3.3.1.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.1.2.cmml">Q</mi><mi id="S3.E3.m1.3.3.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.3.cmml">p</mi></msub><mo id="S3.E3.m1.4.4.2.2.2.4" xref="S3.E3.m1.4.4.2.2.3.cmml">,</mo><msub id="S3.E3.m1.4.4.2.2.2.2" xref="S3.E3.m1.4.4.2.2.2.2.cmml"><mi id="S3.E3.m1.4.4.2.2.2.2.2" xref="S3.E3.m1.4.4.2.2.2.2.2.cmml">V</mi><mrow id="S3.E3.m1.4.4.2.2.2.2.3" xref="S3.E3.m1.4.4.2.2.2.2.3.cmml"><mi id="S3.E3.m1.4.4.2.2.2.2.3.2" xref="S3.E3.m1.4.4.2.2.2.2.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.2.2.2.2.3.1" xref="S3.E3.m1.4.4.2.2.2.2.3.1.cmml">​</mo><mi id="S3.E3.m1.4.4.2.2.2.2.3.3" xref="S3.E3.m1.4.4.2.2.2.2.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.2.2.2.2.3.1a" xref="S3.E3.m1.4.4.2.2.2.2.3.1.cmml">​</mo><mi id="S3.E3.m1.4.4.2.2.2.2.3.4" xref="S3.E3.m1.4.4.2.2.2.2.3.4.cmml">s</mi></mrow></msub><mo stretchy="false" id="S3.E3.m1.4.4.2.2.2.5" xref="S3.E3.m1.4.4.2.2.3.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S3.E3.m1.7.7.6" xref="S3.E3.m1.7.7.6.cmml">=</mo><mrow id="S3.E3.m1.7.7.5" xref="S3.E3.m1.7.7.5.cmml"><munderover id="S3.E3.m1.7.7.5.4" xref="S3.E3.m1.7.7.5.4.cmml"><mo movablelimits="false" id="S3.E3.m1.7.7.5.4.2.2" xref="S3.E3.m1.7.7.5.4.2.2.cmml">∑</mo><mrow id="S3.E3.m1.7.7.5.4.2.3" xref="S3.E3.m1.7.7.5.4.2.3.cmml"><mi id="S3.E3.m1.7.7.5.4.2.3.2" xref="S3.E3.m1.7.7.5.4.2.3.2.cmml">i</mi><mo id="S3.E3.m1.7.7.5.4.2.3.1" xref="S3.E3.m1.7.7.5.4.2.3.1.cmml">=</mo><mn id="S3.E3.m1.7.7.5.4.2.3.3" xref="S3.E3.m1.7.7.5.4.2.3.3.cmml">1</mn></mrow><msub id="S3.E3.m1.7.7.5.4.3" xref="S3.E3.m1.7.7.5.4.3.cmml"><mi id="S3.E3.m1.7.7.5.4.3.2" xref="S3.E3.m1.7.7.5.4.3.2.cmml">N</mi><mrow id="S3.E3.m1.7.7.5.4.3.3" xref="S3.E3.m1.7.7.5.4.3.3.cmml"><mi id="S3.E3.m1.7.7.5.4.3.3.2" xref="S3.E3.m1.7.7.5.4.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.5.4.3.3.1" xref="S3.E3.m1.7.7.5.4.3.3.1.cmml">​</mo><mi id="S3.E3.m1.7.7.5.4.3.3.3" xref="S3.E3.m1.7.7.5.4.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.5.4.3.3.1a" xref="S3.E3.m1.7.7.5.4.3.3.1.cmml">​</mo><mi id="S3.E3.m1.7.7.5.4.3.3.4" xref="S3.E3.m1.7.7.5.4.3.3.4.cmml">f</mi></mrow></msub></munderover><mrow id="S3.E3.m1.7.7.5.3" xref="S3.E3.m1.7.7.5.3.cmml"><mi id="S3.E3.m1.7.7.5.3.5" xref="S3.E3.m1.7.7.5.3.5.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.5.3.4" xref="S3.E3.m1.7.7.5.3.4.cmml">​</mo><mi id="S3.E3.m1.7.7.5.3.6" xref="S3.E3.m1.7.7.5.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.5.3.4a" xref="S3.E3.m1.7.7.5.3.4.cmml">​</mo><mi id="S3.E3.m1.7.7.5.3.7" xref="S3.E3.m1.7.7.5.3.7.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.5.3.4b" xref="S3.E3.m1.7.7.5.3.4.cmml">​</mo><mi id="S3.E3.m1.7.7.5.3.8" xref="S3.E3.m1.7.7.5.3.8.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.5.3.4c" xref="S3.E3.m1.7.7.5.3.4.cmml">​</mo><mi id="S3.E3.m1.7.7.5.3.9" xref="S3.E3.m1.7.7.5.3.9.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.5.3.4d" xref="S3.E3.m1.7.7.5.3.4.cmml">​</mo><mi id="S3.E3.m1.7.7.5.3.10" xref="S3.E3.m1.7.7.5.3.10.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.5.3.4e" xref="S3.E3.m1.7.7.5.3.4.cmml">​</mo><mi id="S3.E3.m1.7.7.5.3.11" xref="S3.E3.m1.7.7.5.3.11.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.5.3.4f" xref="S3.E3.m1.7.7.5.3.4.cmml">​</mo><mrow id="S3.E3.m1.7.7.5.3.3.3" xref="S3.E3.m1.7.7.5.3.3.4.cmml"><mo stretchy="false" id="S3.E3.m1.7.7.5.3.3.3.4" xref="S3.E3.m1.7.7.5.3.3.4.cmml">(</mo><msub id="S3.E3.m1.5.5.3.1.1.1.1" xref="S3.E3.m1.5.5.3.1.1.1.1.cmml"><mi id="S3.E3.m1.5.5.3.1.1.1.1.2" xref="S3.E3.m1.5.5.3.1.1.1.1.2.cmml">Q</mi><mi id="S3.E3.m1.5.5.3.1.1.1.1.3" xref="S3.E3.m1.5.5.3.1.1.1.1.3.cmml">p</mi></msub><mo id="S3.E3.m1.7.7.5.3.3.3.5" xref="S3.E3.m1.7.7.5.3.3.4.cmml">,</mo><mrow id="S3.E3.m1.6.6.4.2.2.2.2" xref="S3.E3.m1.6.6.4.2.2.2.2.cmml"><msub id="S3.E3.m1.6.6.4.2.2.2.2.2" xref="S3.E3.m1.6.6.4.2.2.2.2.2.cmml"><mi id="S3.E3.m1.6.6.4.2.2.2.2.2.2" xref="S3.E3.m1.6.6.4.2.2.2.2.2.2.cmml">P</mi><mrow id="S3.E3.m1.6.6.4.2.2.2.2.2.3" xref="S3.E3.m1.6.6.4.2.2.2.2.2.3.cmml"><mn id="S3.E3.m1.6.6.4.2.2.2.2.2.3.2" xref="S3.E3.m1.6.6.4.2.2.2.2.2.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S3.E3.m1.6.6.4.2.2.2.2.2.3.1" xref="S3.E3.m1.6.6.4.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.E3.m1.6.6.4.2.2.2.2.2.3.3" xref="S3.E3.m1.6.6.4.2.2.2.2.2.3.3.cmml">D</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.6.6.4.2.2.2.2.1" xref="S3.E3.m1.6.6.4.2.2.2.2.1.cmml">​</mo><mrow id="S3.E3.m1.6.6.4.2.2.2.2.3.2" xref="S3.E3.m1.6.6.4.2.2.2.2.3.1.cmml"><mo stretchy="false" id="S3.E3.m1.6.6.4.2.2.2.2.3.2.1" xref="S3.E3.m1.6.6.4.2.2.2.2.3.1.cmml">(</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">p</mi><mo id="S3.E3.m1.6.6.4.2.2.2.2.3.2.2" xref="S3.E3.m1.6.6.4.2.2.2.2.3.1.cmml">,</mo><mi id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml">i</mi><mo stretchy="false" id="S3.E3.m1.6.6.4.2.2.2.2.3.2.3" xref="S3.E3.m1.6.6.4.2.2.2.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.7.7.5.3.3.3.6" xref="S3.E3.m1.7.7.5.3.3.4.cmml">,</mo><msub id="S3.E3.m1.7.7.5.3.3.3.3" xref="S3.E3.m1.7.7.5.3.3.3.3.cmml"><mi id="S3.E3.m1.7.7.5.3.3.3.3.2" xref="S3.E3.m1.7.7.5.3.3.3.3.2.cmml">V</mi><mrow id="S3.E3.m1.7.7.5.3.3.3.3.3" xref="S3.E3.m1.7.7.5.3.3.3.3.3.cmml"><mi id="S3.E3.m1.7.7.5.3.3.3.3.3.2" xref="S3.E3.m1.7.7.5.3.3.3.3.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.5.3.3.3.3.3.1" xref="S3.E3.m1.7.7.5.3.3.3.3.3.1.cmml">​</mo><mi id="S3.E3.m1.7.7.5.3.3.3.3.3.3" xref="S3.E3.m1.7.7.5.3.3.3.3.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.5.3.3.3.3.3.1a" xref="S3.E3.m1.7.7.5.3.3.3.3.3.1.cmml">​</mo><mi id="S3.E3.m1.7.7.5.3.3.3.3.3.4" xref="S3.E3.m1.7.7.5.3.3.3.3.3.4.cmml">s</mi></mrow></msub><mo stretchy="false" id="S3.E3.m1.7.7.5.3.3.3.7" xref="S3.E3.m1.7.7.5.3.3.4.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.7b"><apply id="S3.E3.m1.7.7.cmml" xref="S3.E3.m1.7.7"><eq id="S3.E3.m1.7.7.6.cmml" xref="S3.E3.m1.7.7.6"></eq><apply id="S3.E3.m1.4.4.2.cmml" xref="S3.E3.m1.4.4.2"><times id="S3.E3.m1.4.4.2.3.cmml" xref="S3.E3.m1.4.4.2.3"></times><ci id="S3.E3.m1.4.4.2.4.cmml" xref="S3.E3.m1.4.4.2.4">𝑃</ci><ci id="S3.E3.m1.4.4.2.5.cmml" xref="S3.E3.m1.4.4.2.5">𝐶</ci><apply id="S3.E3.m1.4.4.2.6.cmml" xref="S3.E3.m1.4.4.2.6"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.2.6.1.cmml" xref="S3.E3.m1.4.4.2.6">subscript</csymbol><ci id="S3.E3.m1.4.4.2.6.2.cmml" xref="S3.E3.m1.4.4.2.6.2">𝐴</ci><apply id="S3.E3.m1.4.4.2.6.3.cmml" xref="S3.E3.m1.4.4.2.6.3"><times id="S3.E3.m1.4.4.2.6.3.1.cmml" xref="S3.E3.m1.4.4.2.6.3.1"></times><cn type="integer" id="S3.E3.m1.4.4.2.6.3.2.cmml" xref="S3.E3.m1.4.4.2.6.3.2">3</cn><ci id="S3.E3.m1.4.4.2.6.3.3.cmml" xref="S3.E3.m1.4.4.2.6.3.3">𝐷</ci></apply></apply><interval closure="open" id="S3.E3.m1.4.4.2.2.3.cmml" xref="S3.E3.m1.4.4.2.2.2"><apply id="S3.E3.m1.3.3.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.2">𝑄</ci><ci id="S3.E3.m1.3.3.1.1.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.3">𝑝</ci></apply><apply id="S3.E3.m1.4.4.2.2.2.2.cmml" xref="S3.E3.m1.4.4.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.2.2.2.2.1.cmml" xref="S3.E3.m1.4.4.2.2.2.2">subscript</csymbol><ci id="S3.E3.m1.4.4.2.2.2.2.2.cmml" xref="S3.E3.m1.4.4.2.2.2.2.2">𝑉</ci><apply id="S3.E3.m1.4.4.2.2.2.2.3.cmml" xref="S3.E3.m1.4.4.2.2.2.2.3"><times id="S3.E3.m1.4.4.2.2.2.2.3.1.cmml" xref="S3.E3.m1.4.4.2.2.2.2.3.1"></times><ci id="S3.E3.m1.4.4.2.2.2.2.3.2.cmml" xref="S3.E3.m1.4.4.2.2.2.2.3.2">𝑝</ci><ci id="S3.E3.m1.4.4.2.2.2.2.3.3.cmml" xref="S3.E3.m1.4.4.2.2.2.2.3.3">𝑡</ci><ci id="S3.E3.m1.4.4.2.2.2.2.3.4.cmml" xref="S3.E3.m1.4.4.2.2.2.2.3.4">𝑠</ci></apply></apply></interval></apply><apply id="S3.E3.m1.7.7.5.cmml" xref="S3.E3.m1.7.7.5"><apply id="S3.E3.m1.7.7.5.4.cmml" xref="S3.E3.m1.7.7.5.4"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.5.4.1.cmml" xref="S3.E3.m1.7.7.5.4">superscript</csymbol><apply id="S3.E3.m1.7.7.5.4.2.cmml" xref="S3.E3.m1.7.7.5.4"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.5.4.2.1.cmml" xref="S3.E3.m1.7.7.5.4">subscript</csymbol><sum id="S3.E3.m1.7.7.5.4.2.2.cmml" xref="S3.E3.m1.7.7.5.4.2.2"></sum><apply id="S3.E3.m1.7.7.5.4.2.3.cmml" xref="S3.E3.m1.7.7.5.4.2.3"><eq id="S3.E3.m1.7.7.5.4.2.3.1.cmml" xref="S3.E3.m1.7.7.5.4.2.3.1"></eq><ci id="S3.E3.m1.7.7.5.4.2.3.2.cmml" xref="S3.E3.m1.7.7.5.4.2.3.2">𝑖</ci><cn type="integer" id="S3.E3.m1.7.7.5.4.2.3.3.cmml" xref="S3.E3.m1.7.7.5.4.2.3.3">1</cn></apply></apply><apply id="S3.E3.m1.7.7.5.4.3.cmml" xref="S3.E3.m1.7.7.5.4.3"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.5.4.3.1.cmml" xref="S3.E3.m1.7.7.5.4.3">subscript</csymbol><ci id="S3.E3.m1.7.7.5.4.3.2.cmml" xref="S3.E3.m1.7.7.5.4.3.2">𝑁</ci><apply id="S3.E3.m1.7.7.5.4.3.3.cmml" xref="S3.E3.m1.7.7.5.4.3.3"><times id="S3.E3.m1.7.7.5.4.3.3.1.cmml" xref="S3.E3.m1.7.7.5.4.3.3.1"></times><ci id="S3.E3.m1.7.7.5.4.3.3.2.cmml" xref="S3.E3.m1.7.7.5.4.3.3.2">𝑟</ci><ci id="S3.E3.m1.7.7.5.4.3.3.3.cmml" xref="S3.E3.m1.7.7.5.4.3.3.3">𝑒</ci><ci id="S3.E3.m1.7.7.5.4.3.3.4.cmml" xref="S3.E3.m1.7.7.5.4.3.3.4">𝑓</ci></apply></apply></apply><apply id="S3.E3.m1.7.7.5.3.cmml" xref="S3.E3.m1.7.7.5.3"><times id="S3.E3.m1.7.7.5.3.4.cmml" xref="S3.E3.m1.7.7.5.3.4"></times><ci id="S3.E3.m1.7.7.5.3.5.cmml" xref="S3.E3.m1.7.7.5.3.5">𝐷</ci><ci id="S3.E3.m1.7.7.5.3.6.cmml" xref="S3.E3.m1.7.7.5.3.6">𝑒</ci><ci id="S3.E3.m1.7.7.5.3.7.cmml" xref="S3.E3.m1.7.7.5.3.7">𝑓</ci><ci id="S3.E3.m1.7.7.5.3.8.cmml" xref="S3.E3.m1.7.7.5.3.8">𝐴</ci><ci id="S3.E3.m1.7.7.5.3.9.cmml" xref="S3.E3.m1.7.7.5.3.9">𝑡</ci><ci id="S3.E3.m1.7.7.5.3.10.cmml" xref="S3.E3.m1.7.7.5.3.10">𝑡</ci><ci id="S3.E3.m1.7.7.5.3.11.cmml" xref="S3.E3.m1.7.7.5.3.11">𝑛</ci><vector id="S3.E3.m1.7.7.5.3.3.4.cmml" xref="S3.E3.m1.7.7.5.3.3.3"><apply id="S3.E3.m1.5.5.3.1.1.1.1.cmml" xref="S3.E3.m1.5.5.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.3.1.1.1.1.1.cmml" xref="S3.E3.m1.5.5.3.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.5.5.3.1.1.1.1.2.cmml" xref="S3.E3.m1.5.5.3.1.1.1.1.2">𝑄</ci><ci id="S3.E3.m1.5.5.3.1.1.1.1.3.cmml" xref="S3.E3.m1.5.5.3.1.1.1.1.3">𝑝</ci></apply><apply id="S3.E3.m1.6.6.4.2.2.2.2.cmml" xref="S3.E3.m1.6.6.4.2.2.2.2"><times id="S3.E3.m1.6.6.4.2.2.2.2.1.cmml" xref="S3.E3.m1.6.6.4.2.2.2.2.1"></times><apply id="S3.E3.m1.6.6.4.2.2.2.2.2.cmml" xref="S3.E3.m1.6.6.4.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.6.6.4.2.2.2.2.2.1.cmml" xref="S3.E3.m1.6.6.4.2.2.2.2.2">subscript</csymbol><ci id="S3.E3.m1.6.6.4.2.2.2.2.2.2.cmml" xref="S3.E3.m1.6.6.4.2.2.2.2.2.2">𝑃</ci><apply id="S3.E3.m1.6.6.4.2.2.2.2.2.3.cmml" xref="S3.E3.m1.6.6.4.2.2.2.2.2.3"><times id="S3.E3.m1.6.6.4.2.2.2.2.2.3.1.cmml" xref="S3.E3.m1.6.6.4.2.2.2.2.2.3.1"></times><cn type="integer" id="S3.E3.m1.6.6.4.2.2.2.2.2.3.2.cmml" xref="S3.E3.m1.6.6.4.2.2.2.2.2.3.2">3</cn><ci id="S3.E3.m1.6.6.4.2.2.2.2.2.3.3.cmml" xref="S3.E3.m1.6.6.4.2.2.2.2.2.3.3">𝐷</ci></apply></apply><interval closure="open" id="S3.E3.m1.6.6.4.2.2.2.2.3.1.cmml" xref="S3.E3.m1.6.6.4.2.2.2.2.3.2"><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">𝑝</ci><ci id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2">𝑖</ci></interval></apply><apply id="S3.E3.m1.7.7.5.3.3.3.3.cmml" xref="S3.E3.m1.7.7.5.3.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.5.3.3.3.3.1.cmml" xref="S3.E3.m1.7.7.5.3.3.3.3">subscript</csymbol><ci id="S3.E3.m1.7.7.5.3.3.3.3.2.cmml" xref="S3.E3.m1.7.7.5.3.3.3.3.2">𝑉</ci><apply id="S3.E3.m1.7.7.5.3.3.3.3.3.cmml" xref="S3.E3.m1.7.7.5.3.3.3.3.3"><times id="S3.E3.m1.7.7.5.3.3.3.3.3.1.cmml" xref="S3.E3.m1.7.7.5.3.3.3.3.3.1"></times><ci id="S3.E3.m1.7.7.5.3.3.3.3.3.2.cmml" xref="S3.E3.m1.7.7.5.3.3.3.3.3.2">𝑝</ci><ci id="S3.E3.m1.7.7.5.3.3.3.3.3.3.cmml" xref="S3.E3.m1.7.7.5.3.3.3.3.3.3">𝑡</ci><ci id="S3.E3.m1.7.7.5.3.3.3.3.3.4.cmml" xref="S3.E3.m1.7.7.5.3.3.3.3.3.4">𝑠</ci></apply></apply></vector></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.7c">PCA_{3D}(Q_{p},V_{pts})=\sum_{i=1}^{N_{ref}}DefAttn(Q_{p},P_{3D}(p,i),V_{pts})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.SSS0.Px3.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px3.p4.1" class="ltx_p">where <math id="S3.SS1.SSS0.Px3.p4.1.m1.1" class="ltx_Math" alttext="V_{pts}" display="inline"><semantics id="S3.SS1.SSS0.Px3.p4.1.m1.1a"><msub id="S3.SS1.SSS0.Px3.p4.1.m1.1.1" xref="S3.SS1.SSS0.Px3.p4.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p4.1.m1.1.1.2" xref="S3.SS1.SSS0.Px3.p4.1.m1.1.1.2.cmml">V</mi><mrow id="S3.SS1.SSS0.Px3.p4.1.m1.1.1.3" xref="S3.SS1.SSS0.Px3.p4.1.m1.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px3.p4.1.m1.1.1.3.2" xref="S3.SS1.SSS0.Px3.p4.1.m1.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px3.p4.1.m1.1.1.3.1" xref="S3.SS1.SSS0.Px3.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px3.p4.1.m1.1.1.3.3" xref="S3.SS1.SSS0.Px3.p4.1.m1.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px3.p4.1.m1.1.1.3.1a" xref="S3.SS1.SSS0.Px3.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px3.p4.1.m1.1.1.3.4" xref="S3.SS1.SSS0.Px3.p4.1.m1.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p4.1.m1.1b"><apply id="S3.SS1.SSS0.Px3.p4.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p4.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p4.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p4.1.m1.1.1.2">𝑉</ci><apply id="S3.SS1.SSS0.Px3.p4.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p4.1.m1.1.1.3"><times id="S3.SS1.SSS0.Px3.p4.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px3.p4.1.m1.1.1.3.1"></times><ci id="S3.SS1.SSS0.Px3.p4.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px3.p4.1.m1.1.1.3.2">𝑝</ci><ci id="S3.SS1.SSS0.Px3.p4.1.m1.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px3.p4.1.m1.1.1.3.3">𝑡</ci><ci id="S3.SS1.SSS0.Px3.p4.1.m1.1.1.3.4.cmml" xref="S3.SS1.SSS0.Px3.p4.1.m1.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p4.1.m1.1c">V_{pts}</annotation></semantics></math> represents the voxel features output by the LiDAR branch.</p>
</div>
<div id="S3.SS1.SSS0.Px3.p5" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px3.p5.7" class="ltx_p">To obtain the 3D reference points, we first expand the grid cell corresponding to each BEV query with a height dimension, similar to the pillar representation <cite class="ltx_cite ltx_citemacro_citep">(Lang et al., <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite>. Then, from each pillar corresponding to a query, we sample a fixed number of <math id="S3.SS1.SSS0.Px3.p5.1.m1.1" class="ltx_Math" alttext="N_{ref}" display="inline"><semantics id="S3.SS1.SSS0.Px3.p5.1.m1.1a"><msub id="S3.SS1.SSS0.Px3.p5.1.m1.1.1" xref="S3.SS1.SSS0.Px3.p5.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p5.1.m1.1.1.2" xref="S3.SS1.SSS0.Px3.p5.1.m1.1.1.2.cmml">N</mi><mrow id="S3.SS1.SSS0.Px3.p5.1.m1.1.1.3" xref="S3.SS1.SSS0.Px3.p5.1.m1.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px3.p5.1.m1.1.1.3.2" xref="S3.SS1.SSS0.Px3.p5.1.m1.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px3.p5.1.m1.1.1.3.1" xref="S3.SS1.SSS0.Px3.p5.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px3.p5.1.m1.1.1.3.3" xref="S3.SS1.SSS0.Px3.p5.1.m1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px3.p5.1.m1.1.1.3.1a" xref="S3.SS1.SSS0.Px3.p5.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px3.p5.1.m1.1.1.3.4" xref="S3.SS1.SSS0.Px3.p5.1.m1.1.1.3.4.cmml">f</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p5.1.m1.1b"><apply id="S3.SS1.SSS0.Px3.p5.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p5.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p5.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p5.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p5.1.m1.1.1.2">𝑁</ci><apply id="S3.SS1.SSS0.Px3.p5.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p5.1.m1.1.1.3"><times id="S3.SS1.SSS0.Px3.p5.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px3.p5.1.m1.1.1.3.1"></times><ci id="S3.SS1.SSS0.Px3.p5.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px3.p5.1.m1.1.1.3.2">𝑟</ci><ci id="S3.SS1.SSS0.Px3.p5.1.m1.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px3.p5.1.m1.1.1.3.3">𝑒</ci><ci id="S3.SS1.SSS0.Px3.p5.1.m1.1.1.3.4.cmml" xref="S3.SS1.SSS0.Px3.p5.1.m1.1.1.3.4">𝑓</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p5.1.m1.1c">N_{ref}</annotation></semantics></math> reference points, which are projected onto the point cloud voxel space using the projection equation <math id="S3.SS1.SSS0.Px3.p5.2.m2.1" class="ltx_Math" alttext="P_{3D}" display="inline"><semantics id="S3.SS1.SSS0.Px3.p5.2.m2.1a"><msub id="S3.SS1.SSS0.Px3.p5.2.m2.1.1" xref="S3.SS1.SSS0.Px3.p5.2.m2.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p5.2.m2.1.1.2" xref="S3.SS1.SSS0.Px3.p5.2.m2.1.1.2.cmml">P</mi><mrow id="S3.SS1.SSS0.Px3.p5.2.m2.1.1.3" xref="S3.SS1.SSS0.Px3.p5.2.m2.1.1.3.cmml"><mn id="S3.SS1.SSS0.Px3.p5.2.m2.1.1.3.2" xref="S3.SS1.SSS0.Px3.p5.2.m2.1.1.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px3.p5.2.m2.1.1.3.1" xref="S3.SS1.SSS0.Px3.p5.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px3.p5.2.m2.1.1.3.3" xref="S3.SS1.SSS0.Px3.p5.2.m2.1.1.3.3.cmml">D</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p5.2.m2.1b"><apply id="S3.SS1.SSS0.Px3.p5.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px3.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p5.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p5.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p5.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p5.2.m2.1.1.2">𝑃</ci><apply id="S3.SS1.SSS0.Px3.p5.2.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p5.2.m2.1.1.3"><times id="S3.SS1.SSS0.Px3.p5.2.m2.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px3.p5.2.m2.1.1.3.1"></times><cn type="integer" id="S3.SS1.SSS0.Px3.p5.2.m2.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px3.p5.2.m2.1.1.3.2">3</cn><ci id="S3.SS1.SSS0.Px3.p5.2.m2.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px3.p5.2.m2.1.1.3.3">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p5.2.m2.1c">P_{3D}</annotation></semantics></math>. Specifically, for each query located at <math id="S3.SS1.SSS0.Px3.p5.3.m3.2" class="ltx_Math" alttext="p=(x,y)" display="inline"><semantics id="S3.SS1.SSS0.Px3.p5.3.m3.2a"><mrow id="S3.SS1.SSS0.Px3.p5.3.m3.2.3" xref="S3.SS1.SSS0.Px3.p5.3.m3.2.3.cmml"><mi id="S3.SS1.SSS0.Px3.p5.3.m3.2.3.2" xref="S3.SS1.SSS0.Px3.p5.3.m3.2.3.2.cmml">p</mi><mo id="S3.SS1.SSS0.Px3.p5.3.m3.2.3.1" xref="S3.SS1.SSS0.Px3.p5.3.m3.2.3.1.cmml">=</mo><mrow id="S3.SS1.SSS0.Px3.p5.3.m3.2.3.3.2" xref="S3.SS1.SSS0.Px3.p5.3.m3.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px3.p5.3.m3.2.3.3.2.1" xref="S3.SS1.SSS0.Px3.p5.3.m3.2.3.3.1.cmml">(</mo><mi id="S3.SS1.SSS0.Px3.p5.3.m3.1.1" xref="S3.SS1.SSS0.Px3.p5.3.m3.1.1.cmml">x</mi><mo id="S3.SS1.SSS0.Px3.p5.3.m3.2.3.3.2.2" xref="S3.SS1.SSS0.Px3.p5.3.m3.2.3.3.1.cmml">,</mo><mi id="S3.SS1.SSS0.Px3.p5.3.m3.2.2" xref="S3.SS1.SSS0.Px3.p5.3.m3.2.2.cmml">y</mi><mo stretchy="false" id="S3.SS1.SSS0.Px3.p5.3.m3.2.3.3.2.3" xref="S3.SS1.SSS0.Px3.p5.3.m3.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p5.3.m3.2b"><apply id="S3.SS1.SSS0.Px3.p5.3.m3.2.3.cmml" xref="S3.SS1.SSS0.Px3.p5.3.m3.2.3"><eq id="S3.SS1.SSS0.Px3.p5.3.m3.2.3.1.cmml" xref="S3.SS1.SSS0.Px3.p5.3.m3.2.3.1"></eq><ci id="S3.SS1.SSS0.Px3.p5.3.m3.2.3.2.cmml" xref="S3.SS1.SSS0.Px3.p5.3.m3.2.3.2">𝑝</ci><interval closure="open" id="S3.SS1.SSS0.Px3.p5.3.m3.2.3.3.1.cmml" xref="S3.SS1.SSS0.Px3.p5.3.m3.2.3.3.2"><ci id="S3.SS1.SSS0.Px3.p5.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px3.p5.3.m3.1.1">𝑥</ci><ci id="S3.SS1.SSS0.Px3.p5.3.m3.2.2.cmml" xref="S3.SS1.SSS0.Px3.p5.3.m3.2.2">𝑦</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p5.3.m3.2c">p=(x,y)</annotation></semantics></math>, a set of height anchors <math id="S3.SS1.SSS0.Px3.p5.4.m4.1" class="ltx_Math" alttext="{\{}z_{i}{\}}_{i=1}^{N_{ref}}" display="inline"><semantics id="S3.SS1.SSS0.Px3.p5.4.m4.1a"><msubsup id="S3.SS1.SSS0.Px3.p5.4.m4.1.1" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.cmml"><mrow id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.1.1" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.1.1.2" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.1.2.cmml">{</mo><msub id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.1.1.1" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.1.1.1.2" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.1.1.1.2.cmml">z</mi><mi id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.1.1.1.3" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.1.1.3" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.3" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.3.2" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.3.2.cmml">i</mi><mo id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.3.1" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.3.1.cmml">=</mo><mn id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.3.3" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.3.3.cmml">1</mn></mrow><msub id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.2" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.2.cmml">N</mi><mrow id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.3" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.3.cmml"><mi id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.3.2" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.3.1" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.3.3" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.3.1a" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.3.4" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.3.4.cmml">f</mi></mrow></msub></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p5.4.m4.1b"><apply id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1">superscript</csymbol><apply id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1">subscript</csymbol><set id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.1.1"><apply id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.1.1.1.2">𝑧</ci><ci id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.1.1.1.3">𝑖</ci></apply></set><apply id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.3"><eq id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.3.1"></eq><ci id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.3.2">𝑖</ci><cn type="integer" id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.1.3.3">1</cn></apply></apply><apply id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.2">𝑁</ci><apply id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.3"><times id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.3.1.cmml" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.3.1"></times><ci id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.3.2.cmml" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.3.2">𝑟</ci><ci id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.3.3.cmml" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.3.3">𝑒</ci><ci id="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.3.4.cmml" xref="S3.SS1.SSS0.Px3.p5.4.m4.1.1.3.3.4">𝑓</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p5.4.m4.1c">{\{}z_{i}{\}}_{i=1}^{N_{ref}}</annotation></semantics></math> are defined along its <math id="S3.SS1.SSS0.Px3.p5.5.m5.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S3.SS1.SSS0.Px3.p5.5.m5.1a"><mi id="S3.SS1.SSS0.Px3.p5.5.m5.1.1" xref="S3.SS1.SSS0.Px3.p5.5.m5.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p5.5.m5.1b"><ci id="S3.SS1.SSS0.Px3.p5.5.m5.1.1.cmml" xref="S3.SS1.SSS0.Px3.p5.5.m5.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p5.5.m5.1c">Z</annotation></semantics></math>-axis. Consequently, for each BEV query <math id="S3.SS1.SSS0.Px3.p5.6.m6.1" class="ltx_Math" alttext="Q_{p}" display="inline"><semantics id="S3.SS1.SSS0.Px3.p5.6.m6.1a"><msub id="S3.SS1.SSS0.Px3.p5.6.m6.1.1" xref="S3.SS1.SSS0.Px3.p5.6.m6.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p5.6.m6.1.1.2" xref="S3.SS1.SSS0.Px3.p5.6.m6.1.1.2.cmml">Q</mi><mi id="S3.SS1.SSS0.Px3.p5.6.m6.1.1.3" xref="S3.SS1.SSS0.Px3.p5.6.m6.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p5.6.m6.1b"><apply id="S3.SS1.SSS0.Px3.p5.6.m6.1.1.cmml" xref="S3.SS1.SSS0.Px3.p5.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p5.6.m6.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p5.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p5.6.m6.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p5.6.m6.1.1.2">𝑄</ci><ci id="S3.SS1.SSS0.Px3.p5.6.m6.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p5.6.m6.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p5.6.m6.1c">Q_{p}</annotation></semantics></math>, a corresponding set of 3D reference points <math id="S3.SS1.SSS0.Px3.p5.7.m7.3" class="ltx_Math" alttext="(x,y,z_{i})_{i=1}^{N_{ref}}" display="inline"><semantics id="S3.SS1.SSS0.Px3.p5.7.m7.3a"><msubsup id="S3.SS1.SSS0.Px3.p5.7.m7.3.3" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.cmml"><mrow id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.1.1" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.1.1.2" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.1.2.cmml">(</mo><mi id="S3.SS1.SSS0.Px3.p5.7.m7.1.1" xref="S3.SS1.SSS0.Px3.p5.7.m7.1.1.cmml">x</mi><mo id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.1.1.3" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.1.2.cmml">,</mo><mi id="S3.SS1.SSS0.Px3.p5.7.m7.2.2" xref="S3.SS1.SSS0.Px3.p5.7.m7.2.2.cmml">y</mi><mo id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.1.1.4" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.1.2.cmml">,</mo><msub id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.1.1.1" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.1.1.1.2" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.1.1.1.2.cmml">z</mi><mi id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.1.1.1.3" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.1.1.5" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.1.2.cmml">)</mo></mrow><mrow id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.3" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.3.cmml"><mi id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.3.2" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.3.2.cmml">i</mi><mo id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.3.1" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.3.1.cmml">=</mo><mn id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.3.3" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.3.3.cmml">1</mn></mrow><msub id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.cmml"><mi id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.2" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.2.cmml">N</mi><mrow id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.3" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.3.cmml"><mi id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.3.2" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.3.1" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.3.3" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.3.1a" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.3.4" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.3.4.cmml">f</mi></mrow></msub></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p5.7.m7.3b"><apply id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.cmml" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.2.cmml" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3">superscript</csymbol><apply id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.cmml" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.2.cmml" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3">subscript</csymbol><vector id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.1.1"><ci id="S3.SS1.SSS0.Px3.p5.7.m7.1.1.cmml" xref="S3.SS1.SSS0.Px3.p5.7.m7.1.1">𝑥</ci><ci id="S3.SS1.SSS0.Px3.p5.7.m7.2.2.cmml" xref="S3.SS1.SSS0.Px3.p5.7.m7.2.2">𝑦</ci><apply id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.1.1.1.2">𝑧</ci><ci id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.1.1.1.3">𝑖</ci></apply></vector><apply id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.3.cmml" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.3"><eq id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.3.1.cmml" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.3.1"></eq><ci id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.3.2.cmml" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.3.2">𝑖</ci><cn type="integer" id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.3.3.cmml" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.1.3.3">1</cn></apply></apply><apply id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.cmml" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.1.cmml" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.2.cmml" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.2">𝑁</ci><apply id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.3.cmml" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.3"><times id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.3.1.cmml" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.3.1"></times><ci id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.3.2.cmml" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.3.2">𝑟</ci><ci id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.3.3.cmml" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.3.3">𝑒</ci><ci id="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.3.4.cmml" xref="S3.SS1.SSS0.Px3.p5.7.m7.3.3.3.3.4">𝑓</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p5.7.m7.3c">(x,y,z_{i})_{i=1}^{N_{ref}}</annotation></semantics></math> is obtained. And the projection equation is as follow:</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.5" class="ltx_Math" alttext="P_{3D}(p,i)=(x_{pts},y_{pts},z_{pts})" display="block"><semantics id="S3.E4.m1.5a"><mrow id="S3.E4.m1.5.5" xref="S3.E4.m1.5.5.cmml"><mrow id="S3.E4.m1.5.5.5" xref="S3.E4.m1.5.5.5.cmml"><msub id="S3.E4.m1.5.5.5.2" xref="S3.E4.m1.5.5.5.2.cmml"><mi id="S3.E4.m1.5.5.5.2.2" xref="S3.E4.m1.5.5.5.2.2.cmml">P</mi><mrow id="S3.E4.m1.5.5.5.2.3" xref="S3.E4.m1.5.5.5.2.3.cmml"><mn id="S3.E4.m1.5.5.5.2.3.2" xref="S3.E4.m1.5.5.5.2.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S3.E4.m1.5.5.5.2.3.1" xref="S3.E4.m1.5.5.5.2.3.1.cmml">​</mo><mi id="S3.E4.m1.5.5.5.2.3.3" xref="S3.E4.m1.5.5.5.2.3.3.cmml">D</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.5.5.5.1" xref="S3.E4.m1.5.5.5.1.cmml">​</mo><mrow id="S3.E4.m1.5.5.5.3.2" xref="S3.E4.m1.5.5.5.3.1.cmml"><mo stretchy="false" id="S3.E4.m1.5.5.5.3.2.1" xref="S3.E4.m1.5.5.5.3.1.cmml">(</mo><mi id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">p</mi><mo id="S3.E4.m1.5.5.5.3.2.2" xref="S3.E4.m1.5.5.5.3.1.cmml">,</mo><mi id="S3.E4.m1.2.2" xref="S3.E4.m1.2.2.cmml">i</mi><mo stretchy="false" id="S3.E4.m1.5.5.5.3.2.3" xref="S3.E4.m1.5.5.5.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.5.5.4" xref="S3.E4.m1.5.5.4.cmml">=</mo><mrow id="S3.E4.m1.5.5.3.3" xref="S3.E4.m1.5.5.3.4.cmml"><mo stretchy="false" id="S3.E4.m1.5.5.3.3.4" xref="S3.E4.m1.5.5.3.4.cmml">(</mo><msub id="S3.E4.m1.3.3.1.1.1" xref="S3.E4.m1.3.3.1.1.1.cmml"><mi id="S3.E4.m1.3.3.1.1.1.2" xref="S3.E4.m1.3.3.1.1.1.2.cmml">x</mi><mrow id="S3.E4.m1.3.3.1.1.1.3" xref="S3.E4.m1.3.3.1.1.1.3.cmml"><mi id="S3.E4.m1.3.3.1.1.1.3.2" xref="S3.E4.m1.3.3.1.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.1.1.1.3.1" xref="S3.E4.m1.3.3.1.1.1.3.1.cmml">​</mo><mi id="S3.E4.m1.3.3.1.1.1.3.3" xref="S3.E4.m1.3.3.1.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.1.1.1.3.1a" xref="S3.E4.m1.3.3.1.1.1.3.1.cmml">​</mo><mi id="S3.E4.m1.3.3.1.1.1.3.4" xref="S3.E4.m1.3.3.1.1.1.3.4.cmml">s</mi></mrow></msub><mo id="S3.E4.m1.5.5.3.3.5" xref="S3.E4.m1.5.5.3.4.cmml">,</mo><msub id="S3.E4.m1.4.4.2.2.2" xref="S3.E4.m1.4.4.2.2.2.cmml"><mi id="S3.E4.m1.4.4.2.2.2.2" xref="S3.E4.m1.4.4.2.2.2.2.cmml">y</mi><mrow id="S3.E4.m1.4.4.2.2.2.3" xref="S3.E4.m1.4.4.2.2.2.3.cmml"><mi id="S3.E4.m1.4.4.2.2.2.3.2" xref="S3.E4.m1.4.4.2.2.2.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.4.4.2.2.2.3.1" xref="S3.E4.m1.4.4.2.2.2.3.1.cmml">​</mo><mi id="S3.E4.m1.4.4.2.2.2.3.3" xref="S3.E4.m1.4.4.2.2.2.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.4.4.2.2.2.3.1a" xref="S3.E4.m1.4.4.2.2.2.3.1.cmml">​</mo><mi id="S3.E4.m1.4.4.2.2.2.3.4" xref="S3.E4.m1.4.4.2.2.2.3.4.cmml">s</mi></mrow></msub><mo id="S3.E4.m1.5.5.3.3.6" xref="S3.E4.m1.5.5.3.4.cmml">,</mo><msub id="S3.E4.m1.5.5.3.3.3" xref="S3.E4.m1.5.5.3.3.3.cmml"><mi id="S3.E4.m1.5.5.3.3.3.2" xref="S3.E4.m1.5.5.3.3.3.2.cmml">z</mi><mrow id="S3.E4.m1.5.5.3.3.3.3" xref="S3.E4.m1.5.5.3.3.3.3.cmml"><mi id="S3.E4.m1.5.5.3.3.3.3.2" xref="S3.E4.m1.5.5.3.3.3.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.5.5.3.3.3.3.1" xref="S3.E4.m1.5.5.3.3.3.3.1.cmml">​</mo><mi id="S3.E4.m1.5.5.3.3.3.3.3" xref="S3.E4.m1.5.5.3.3.3.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.5.5.3.3.3.3.1a" xref="S3.E4.m1.5.5.3.3.3.3.1.cmml">​</mo><mi id="S3.E4.m1.5.5.3.3.3.3.4" xref="S3.E4.m1.5.5.3.3.3.3.4.cmml">s</mi></mrow></msub><mo stretchy="false" id="S3.E4.m1.5.5.3.3.7" xref="S3.E4.m1.5.5.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.5b"><apply id="S3.E4.m1.5.5.cmml" xref="S3.E4.m1.5.5"><eq id="S3.E4.m1.5.5.4.cmml" xref="S3.E4.m1.5.5.4"></eq><apply id="S3.E4.m1.5.5.5.cmml" xref="S3.E4.m1.5.5.5"><times id="S3.E4.m1.5.5.5.1.cmml" xref="S3.E4.m1.5.5.5.1"></times><apply id="S3.E4.m1.5.5.5.2.cmml" xref="S3.E4.m1.5.5.5.2"><csymbol cd="ambiguous" id="S3.E4.m1.5.5.5.2.1.cmml" xref="S3.E4.m1.5.5.5.2">subscript</csymbol><ci id="S3.E4.m1.5.5.5.2.2.cmml" xref="S3.E4.m1.5.5.5.2.2">𝑃</ci><apply id="S3.E4.m1.5.5.5.2.3.cmml" xref="S3.E4.m1.5.5.5.2.3"><times id="S3.E4.m1.5.5.5.2.3.1.cmml" xref="S3.E4.m1.5.5.5.2.3.1"></times><cn type="integer" id="S3.E4.m1.5.5.5.2.3.2.cmml" xref="S3.E4.m1.5.5.5.2.3.2">3</cn><ci id="S3.E4.m1.5.5.5.2.3.3.cmml" xref="S3.E4.m1.5.5.5.2.3.3">𝐷</ci></apply></apply><interval closure="open" id="S3.E4.m1.5.5.5.3.1.cmml" xref="S3.E4.m1.5.5.5.3.2"><ci id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1">𝑝</ci><ci id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2">𝑖</ci></interval></apply><vector id="S3.E4.m1.5.5.3.4.cmml" xref="S3.E4.m1.5.5.3.3"><apply id="S3.E4.m1.3.3.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1">subscript</csymbol><ci id="S3.E4.m1.3.3.1.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.1.2">𝑥</ci><apply id="S3.E4.m1.3.3.1.1.1.3.cmml" xref="S3.E4.m1.3.3.1.1.1.3"><times id="S3.E4.m1.3.3.1.1.1.3.1.cmml" xref="S3.E4.m1.3.3.1.1.1.3.1"></times><ci id="S3.E4.m1.3.3.1.1.1.3.2.cmml" xref="S3.E4.m1.3.3.1.1.1.3.2">𝑝</ci><ci id="S3.E4.m1.3.3.1.1.1.3.3.cmml" xref="S3.E4.m1.3.3.1.1.1.3.3">𝑡</ci><ci id="S3.E4.m1.3.3.1.1.1.3.4.cmml" xref="S3.E4.m1.3.3.1.1.1.3.4">𝑠</ci></apply></apply><apply id="S3.E4.m1.4.4.2.2.2.cmml" xref="S3.E4.m1.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.2.2.2.1.cmml" xref="S3.E4.m1.4.4.2.2.2">subscript</csymbol><ci id="S3.E4.m1.4.4.2.2.2.2.cmml" xref="S3.E4.m1.4.4.2.2.2.2">𝑦</ci><apply id="S3.E4.m1.4.4.2.2.2.3.cmml" xref="S3.E4.m1.4.4.2.2.2.3"><times id="S3.E4.m1.4.4.2.2.2.3.1.cmml" xref="S3.E4.m1.4.4.2.2.2.3.1"></times><ci id="S3.E4.m1.4.4.2.2.2.3.2.cmml" xref="S3.E4.m1.4.4.2.2.2.3.2">𝑝</ci><ci id="S3.E4.m1.4.4.2.2.2.3.3.cmml" xref="S3.E4.m1.4.4.2.2.2.3.3">𝑡</ci><ci id="S3.E4.m1.4.4.2.2.2.3.4.cmml" xref="S3.E4.m1.4.4.2.2.2.3.4">𝑠</ci></apply></apply><apply id="S3.E4.m1.5.5.3.3.3.cmml" xref="S3.E4.m1.5.5.3.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.5.5.3.3.3.1.cmml" xref="S3.E4.m1.5.5.3.3.3">subscript</csymbol><ci id="S3.E4.m1.5.5.3.3.3.2.cmml" xref="S3.E4.m1.5.5.3.3.3.2">𝑧</ci><apply id="S3.E4.m1.5.5.3.3.3.3.cmml" xref="S3.E4.m1.5.5.3.3.3.3"><times id="S3.E4.m1.5.5.3.3.3.3.1.cmml" xref="S3.E4.m1.5.5.3.3.3.3.1"></times><ci id="S3.E4.m1.5.5.3.3.3.3.2.cmml" xref="S3.E4.m1.5.5.3.3.3.3.2">𝑝</ci><ci id="S3.E4.m1.5.5.3.3.3.3.3.cmml" xref="S3.E4.m1.5.5.3.3.3.3.3">𝑡</ci><ci id="S3.E4.m1.5.5.3.3.3.3.4.cmml" xref="S3.E4.m1.5.5.3.3.3.3.4">𝑠</ci></apply></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.5c">P_{3D}(p,i)=(x_{pts},y_{pts},z_{pts})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.SSS0.Px3.p6" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px3.p6.2" class="ltx_p">where <math id="S3.SS1.SSS0.Px3.p6.1.m1.2" class="ltx_Math" alttext="P_{3D}(p,i)" display="inline"><semantics id="S3.SS1.SSS0.Px3.p6.1.m1.2a"><mrow id="S3.SS1.SSS0.Px3.p6.1.m1.2.3" xref="S3.SS1.SSS0.Px3.p6.1.m1.2.3.cmml"><msub id="S3.SS1.SSS0.Px3.p6.1.m1.2.3.2" xref="S3.SS1.SSS0.Px3.p6.1.m1.2.3.2.cmml"><mi id="S3.SS1.SSS0.Px3.p6.1.m1.2.3.2.2" xref="S3.SS1.SSS0.Px3.p6.1.m1.2.3.2.2.cmml">P</mi><mrow id="S3.SS1.SSS0.Px3.p6.1.m1.2.3.2.3" xref="S3.SS1.SSS0.Px3.p6.1.m1.2.3.2.3.cmml"><mn id="S3.SS1.SSS0.Px3.p6.1.m1.2.3.2.3.2" xref="S3.SS1.SSS0.Px3.p6.1.m1.2.3.2.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px3.p6.1.m1.2.3.2.3.1" xref="S3.SS1.SSS0.Px3.p6.1.m1.2.3.2.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px3.p6.1.m1.2.3.2.3.3" xref="S3.SS1.SSS0.Px3.p6.1.m1.2.3.2.3.3.cmml">D</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px3.p6.1.m1.2.3.1" xref="S3.SS1.SSS0.Px3.p6.1.m1.2.3.1.cmml">​</mo><mrow id="S3.SS1.SSS0.Px3.p6.1.m1.2.3.3.2" xref="S3.SS1.SSS0.Px3.p6.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px3.p6.1.m1.2.3.3.2.1" xref="S3.SS1.SSS0.Px3.p6.1.m1.2.3.3.1.cmml">(</mo><mi id="S3.SS1.SSS0.Px3.p6.1.m1.1.1" xref="S3.SS1.SSS0.Px3.p6.1.m1.1.1.cmml">p</mi><mo id="S3.SS1.SSS0.Px3.p6.1.m1.2.3.3.2.2" xref="S3.SS1.SSS0.Px3.p6.1.m1.2.3.3.1.cmml">,</mo><mi id="S3.SS1.SSS0.Px3.p6.1.m1.2.2" xref="S3.SS1.SSS0.Px3.p6.1.m1.2.2.cmml">i</mi><mo stretchy="false" id="S3.SS1.SSS0.Px3.p6.1.m1.2.3.3.2.3" xref="S3.SS1.SSS0.Px3.p6.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p6.1.m1.2b"><apply id="S3.SS1.SSS0.Px3.p6.1.m1.2.3.cmml" xref="S3.SS1.SSS0.Px3.p6.1.m1.2.3"><times id="S3.SS1.SSS0.Px3.p6.1.m1.2.3.1.cmml" xref="S3.SS1.SSS0.Px3.p6.1.m1.2.3.1"></times><apply id="S3.SS1.SSS0.Px3.p6.1.m1.2.3.2.cmml" xref="S3.SS1.SSS0.Px3.p6.1.m1.2.3.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p6.1.m1.2.3.2.1.cmml" xref="S3.SS1.SSS0.Px3.p6.1.m1.2.3.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p6.1.m1.2.3.2.2.cmml" xref="S3.SS1.SSS0.Px3.p6.1.m1.2.3.2.2">𝑃</ci><apply id="S3.SS1.SSS0.Px3.p6.1.m1.2.3.2.3.cmml" xref="S3.SS1.SSS0.Px3.p6.1.m1.2.3.2.3"><times id="S3.SS1.SSS0.Px3.p6.1.m1.2.3.2.3.1.cmml" xref="S3.SS1.SSS0.Px3.p6.1.m1.2.3.2.3.1"></times><cn type="integer" id="S3.SS1.SSS0.Px3.p6.1.m1.2.3.2.3.2.cmml" xref="S3.SS1.SSS0.Px3.p6.1.m1.2.3.2.3.2">3</cn><ci id="S3.SS1.SSS0.Px3.p6.1.m1.2.3.2.3.3.cmml" xref="S3.SS1.SSS0.Px3.p6.1.m1.2.3.2.3.3">𝐷</ci></apply></apply><interval closure="open" id="S3.SS1.SSS0.Px3.p6.1.m1.2.3.3.1.cmml" xref="S3.SS1.SSS0.Px3.p6.1.m1.2.3.3.2"><ci id="S3.SS1.SSS0.Px3.p6.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p6.1.m1.1.1">𝑝</ci><ci id="S3.SS1.SSS0.Px3.p6.1.m1.2.2.cmml" xref="S3.SS1.SSS0.Px3.p6.1.m1.2.2">𝑖</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p6.1.m1.2c">P_{3D}(p,i)</annotation></semantics></math> is the projection of the i-th 3D reference point of BEV query <math id="S3.SS1.SSS0.Px3.p6.2.m2.1" class="ltx_Math" alttext="Q_{p}" display="inline"><semantics id="S3.SS1.SSS0.Px3.p6.2.m2.1a"><msub id="S3.SS1.SSS0.Px3.p6.2.m2.1.1" xref="S3.SS1.SSS0.Px3.p6.2.m2.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p6.2.m2.1.1.2" xref="S3.SS1.SSS0.Px3.p6.2.m2.1.1.2.cmml">Q</mi><mi id="S3.SS1.SSS0.Px3.p6.2.m2.1.1.3" xref="S3.SS1.SSS0.Px3.p6.2.m2.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p6.2.m2.1b"><apply id="S3.SS1.SSS0.Px3.p6.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px3.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p6.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p6.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p6.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p6.2.m2.1.1.2">𝑄</ci><ci id="S3.SS1.SSS0.Px3.p6.2.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p6.2.m2.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p6.2.m2.1c">Q_{p}</annotation></semantics></math> in the LiDAR space.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Images Cross-Attention.</h4>

<div id="S3.SS1.SSS0.Px4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px4.p1.1" class="ltx_p">The implementation of the images cross-attention is similar to the points cross-attention with voxel features as input. Since the images have multi views, the 3D reference points of each query can only be projected onto a subset of the camera views. Following BEVFormer <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib15" title="" class="ltx_ref">2022c</a>)</cite>, we denote the views that can be projected as <math id="S3.SS1.SSS0.Px4.p1.1.m1.1" class="ltx_Math" alttext="V_{hit}" display="inline"><semantics id="S3.SS1.SSS0.Px4.p1.1.m1.1a"><msub id="S3.SS1.SSS0.Px4.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2.cmml">V</mi><mrow id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.1" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.1a" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.4" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2">𝑉</ci><apply id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3"><times id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.1"></times><ci id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2">ℎ</ci><ci id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3">𝑖</ci><ci id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.4.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p1.1.m1.1c">V_{hit}</annotation></semantics></math>. Therefore, the images cross-attention process can be expressed as:</p>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.8" class="ltx_Math" alttext="ICA(Q_{p},F)={\frac{1}{V_{hit}}}{\sum_{i=1}^{N_{ref}}{\sum_{j=1}^{V_{hit}}DefAttn(Q_{p},P(p,i,j),F_{j})}}" display="block"><semantics id="S3.E5.m1.8a"><mrow id="S3.E5.m1.8.8" xref="S3.E5.m1.8.8.cmml"><mrow id="S3.E5.m1.5.5.1" xref="S3.E5.m1.5.5.1.cmml"><mi id="S3.E5.m1.5.5.1.3" xref="S3.E5.m1.5.5.1.3.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.5.5.1.2" xref="S3.E5.m1.5.5.1.2.cmml">​</mo><mi id="S3.E5.m1.5.5.1.4" xref="S3.E5.m1.5.5.1.4.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.5.5.1.2a" xref="S3.E5.m1.5.5.1.2.cmml">​</mo><mi id="S3.E5.m1.5.5.1.5" xref="S3.E5.m1.5.5.1.5.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.5.5.1.2b" xref="S3.E5.m1.5.5.1.2.cmml">​</mo><mrow id="S3.E5.m1.5.5.1.1.1" xref="S3.E5.m1.5.5.1.1.2.cmml"><mo stretchy="false" id="S3.E5.m1.5.5.1.1.1.2" xref="S3.E5.m1.5.5.1.1.2.cmml">(</mo><msub id="S3.E5.m1.5.5.1.1.1.1" xref="S3.E5.m1.5.5.1.1.1.1.cmml"><mi id="S3.E5.m1.5.5.1.1.1.1.2" xref="S3.E5.m1.5.5.1.1.1.1.2.cmml">Q</mi><mi id="S3.E5.m1.5.5.1.1.1.1.3" xref="S3.E5.m1.5.5.1.1.1.1.3.cmml">p</mi></msub><mo id="S3.E5.m1.5.5.1.1.1.3" xref="S3.E5.m1.5.5.1.1.2.cmml">,</mo><mi id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml">F</mi><mo stretchy="false" id="S3.E5.m1.5.5.1.1.1.4" xref="S3.E5.m1.5.5.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E5.m1.8.8.5" xref="S3.E5.m1.8.8.5.cmml">=</mo><mrow id="S3.E5.m1.8.8.4" xref="S3.E5.m1.8.8.4.cmml"><mfrac id="S3.E5.m1.8.8.4.5" xref="S3.E5.m1.8.8.4.5.cmml"><mn id="S3.E5.m1.8.8.4.5.2" xref="S3.E5.m1.8.8.4.5.2.cmml">1</mn><msub id="S3.E5.m1.8.8.4.5.3" xref="S3.E5.m1.8.8.4.5.3.cmml"><mi id="S3.E5.m1.8.8.4.5.3.2" xref="S3.E5.m1.8.8.4.5.3.2.cmml">V</mi><mrow id="S3.E5.m1.8.8.4.5.3.3" xref="S3.E5.m1.8.8.4.5.3.3.cmml"><mi id="S3.E5.m1.8.8.4.5.3.3.2" xref="S3.E5.m1.8.8.4.5.3.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.8.8.4.5.3.3.1" xref="S3.E5.m1.8.8.4.5.3.3.1.cmml">​</mo><mi id="S3.E5.m1.8.8.4.5.3.3.3" xref="S3.E5.m1.8.8.4.5.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.8.8.4.5.3.3.1a" xref="S3.E5.m1.8.8.4.5.3.3.1.cmml">​</mo><mi id="S3.E5.m1.8.8.4.5.3.3.4" xref="S3.E5.m1.8.8.4.5.3.3.4.cmml">t</mi></mrow></msub></mfrac><mo lspace="0em" rspace="0em" id="S3.E5.m1.8.8.4.4" xref="S3.E5.m1.8.8.4.4.cmml">​</mo><mrow id="S3.E5.m1.8.8.4.3" xref="S3.E5.m1.8.8.4.3.cmml"><munderover id="S3.E5.m1.8.8.4.3.4" xref="S3.E5.m1.8.8.4.3.4.cmml"><mo movablelimits="false" rspace="0em" id="S3.E5.m1.8.8.4.3.4.2.2" xref="S3.E5.m1.8.8.4.3.4.2.2.cmml">∑</mo><mrow id="S3.E5.m1.8.8.4.3.4.2.3" xref="S3.E5.m1.8.8.4.3.4.2.3.cmml"><mi id="S3.E5.m1.8.8.4.3.4.2.3.2" xref="S3.E5.m1.8.8.4.3.4.2.3.2.cmml">i</mi><mo id="S3.E5.m1.8.8.4.3.4.2.3.1" xref="S3.E5.m1.8.8.4.3.4.2.3.1.cmml">=</mo><mn id="S3.E5.m1.8.8.4.3.4.2.3.3" xref="S3.E5.m1.8.8.4.3.4.2.3.3.cmml">1</mn></mrow><msub id="S3.E5.m1.8.8.4.3.4.3" xref="S3.E5.m1.8.8.4.3.4.3.cmml"><mi id="S3.E5.m1.8.8.4.3.4.3.2" xref="S3.E5.m1.8.8.4.3.4.3.2.cmml">N</mi><mrow id="S3.E5.m1.8.8.4.3.4.3.3" xref="S3.E5.m1.8.8.4.3.4.3.3.cmml"><mi id="S3.E5.m1.8.8.4.3.4.3.3.2" xref="S3.E5.m1.8.8.4.3.4.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.8.8.4.3.4.3.3.1" xref="S3.E5.m1.8.8.4.3.4.3.3.1.cmml">​</mo><mi id="S3.E5.m1.8.8.4.3.4.3.3.3" xref="S3.E5.m1.8.8.4.3.4.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.8.8.4.3.4.3.3.1a" xref="S3.E5.m1.8.8.4.3.4.3.3.1.cmml">​</mo><mi id="S3.E5.m1.8.8.4.3.4.3.3.4" xref="S3.E5.m1.8.8.4.3.4.3.3.4.cmml">f</mi></mrow></msub></munderover><mrow id="S3.E5.m1.8.8.4.3.3" xref="S3.E5.m1.8.8.4.3.3.cmml"><munderover id="S3.E5.m1.8.8.4.3.3.4" xref="S3.E5.m1.8.8.4.3.3.4.cmml"><mo movablelimits="false" id="S3.E5.m1.8.8.4.3.3.4.2.2" xref="S3.E5.m1.8.8.4.3.3.4.2.2.cmml">∑</mo><mrow id="S3.E5.m1.8.8.4.3.3.4.2.3" xref="S3.E5.m1.8.8.4.3.3.4.2.3.cmml"><mi id="S3.E5.m1.8.8.4.3.3.4.2.3.2" xref="S3.E5.m1.8.8.4.3.3.4.2.3.2.cmml">j</mi><mo id="S3.E5.m1.8.8.4.3.3.4.2.3.1" xref="S3.E5.m1.8.8.4.3.3.4.2.3.1.cmml">=</mo><mn id="S3.E5.m1.8.8.4.3.3.4.2.3.3" xref="S3.E5.m1.8.8.4.3.3.4.2.3.3.cmml">1</mn></mrow><msub id="S3.E5.m1.8.8.4.3.3.4.3" xref="S3.E5.m1.8.8.4.3.3.4.3.cmml"><mi id="S3.E5.m1.8.8.4.3.3.4.3.2" xref="S3.E5.m1.8.8.4.3.3.4.3.2.cmml">V</mi><mrow id="S3.E5.m1.8.8.4.3.3.4.3.3" xref="S3.E5.m1.8.8.4.3.3.4.3.3.cmml"><mi id="S3.E5.m1.8.8.4.3.3.4.3.3.2" xref="S3.E5.m1.8.8.4.3.3.4.3.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.8.8.4.3.3.4.3.3.1" xref="S3.E5.m1.8.8.4.3.3.4.3.3.1.cmml">​</mo><mi id="S3.E5.m1.8.8.4.3.3.4.3.3.3" xref="S3.E5.m1.8.8.4.3.3.4.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.8.8.4.3.3.4.3.3.1a" xref="S3.E5.m1.8.8.4.3.3.4.3.3.1.cmml">​</mo><mi id="S3.E5.m1.8.8.4.3.3.4.3.3.4" xref="S3.E5.m1.8.8.4.3.3.4.3.3.4.cmml">t</mi></mrow></msub></munderover><mrow id="S3.E5.m1.8.8.4.3.3.3" xref="S3.E5.m1.8.8.4.3.3.3.cmml"><mi id="S3.E5.m1.8.8.4.3.3.3.5" xref="S3.E5.m1.8.8.4.3.3.3.5.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.8.8.4.3.3.3.4" xref="S3.E5.m1.8.8.4.3.3.3.4.cmml">​</mo><mi id="S3.E5.m1.8.8.4.3.3.3.6" xref="S3.E5.m1.8.8.4.3.3.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.8.8.4.3.3.3.4a" xref="S3.E5.m1.8.8.4.3.3.3.4.cmml">​</mo><mi id="S3.E5.m1.8.8.4.3.3.3.7" xref="S3.E5.m1.8.8.4.3.3.3.7.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.8.8.4.3.3.3.4b" xref="S3.E5.m1.8.8.4.3.3.3.4.cmml">​</mo><mi id="S3.E5.m1.8.8.4.3.3.3.8" xref="S3.E5.m1.8.8.4.3.3.3.8.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.8.8.4.3.3.3.4c" xref="S3.E5.m1.8.8.4.3.3.3.4.cmml">​</mo><mi id="S3.E5.m1.8.8.4.3.3.3.9" xref="S3.E5.m1.8.8.4.3.3.3.9.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.8.8.4.3.3.3.4d" xref="S3.E5.m1.8.8.4.3.3.3.4.cmml">​</mo><mi id="S3.E5.m1.8.8.4.3.3.3.10" xref="S3.E5.m1.8.8.4.3.3.3.10.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.8.8.4.3.3.3.4e" xref="S3.E5.m1.8.8.4.3.3.3.4.cmml">​</mo><mi id="S3.E5.m1.8.8.4.3.3.3.11" xref="S3.E5.m1.8.8.4.3.3.3.11.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.8.8.4.3.3.3.4f" xref="S3.E5.m1.8.8.4.3.3.3.4.cmml">​</mo><mrow id="S3.E5.m1.8.8.4.3.3.3.3.3" xref="S3.E5.m1.8.8.4.3.3.3.3.4.cmml"><mo stretchy="false" id="S3.E5.m1.8.8.4.3.3.3.3.3.4" xref="S3.E5.m1.8.8.4.3.3.3.3.4.cmml">(</mo><msub id="S3.E5.m1.6.6.2.1.1.1.1.1.1" xref="S3.E5.m1.6.6.2.1.1.1.1.1.1.cmml"><mi id="S3.E5.m1.6.6.2.1.1.1.1.1.1.2" xref="S3.E5.m1.6.6.2.1.1.1.1.1.1.2.cmml">Q</mi><mi id="S3.E5.m1.6.6.2.1.1.1.1.1.1.3" xref="S3.E5.m1.6.6.2.1.1.1.1.1.1.3.cmml">p</mi></msub><mo id="S3.E5.m1.8.8.4.3.3.3.3.3.5" xref="S3.E5.m1.8.8.4.3.3.3.3.4.cmml">,</mo><mrow id="S3.E5.m1.7.7.3.2.2.2.2.2.2" xref="S3.E5.m1.7.7.3.2.2.2.2.2.2.cmml"><mi id="S3.E5.m1.7.7.3.2.2.2.2.2.2.2" xref="S3.E5.m1.7.7.3.2.2.2.2.2.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.7.7.3.2.2.2.2.2.2.1" xref="S3.E5.m1.7.7.3.2.2.2.2.2.2.1.cmml">​</mo><mrow id="S3.E5.m1.7.7.3.2.2.2.2.2.2.3.2" xref="S3.E5.m1.7.7.3.2.2.2.2.2.2.3.1.cmml"><mo stretchy="false" id="S3.E5.m1.7.7.3.2.2.2.2.2.2.3.2.1" xref="S3.E5.m1.7.7.3.2.2.2.2.2.2.3.1.cmml">(</mo><mi id="S3.E5.m1.2.2" xref="S3.E5.m1.2.2.cmml">p</mi><mo id="S3.E5.m1.7.7.3.2.2.2.2.2.2.3.2.2" xref="S3.E5.m1.7.7.3.2.2.2.2.2.2.3.1.cmml">,</mo><mi id="S3.E5.m1.3.3" xref="S3.E5.m1.3.3.cmml">i</mi><mo id="S3.E5.m1.7.7.3.2.2.2.2.2.2.3.2.3" xref="S3.E5.m1.7.7.3.2.2.2.2.2.2.3.1.cmml">,</mo><mi id="S3.E5.m1.4.4" xref="S3.E5.m1.4.4.cmml">j</mi><mo stretchy="false" id="S3.E5.m1.7.7.3.2.2.2.2.2.2.3.2.4" xref="S3.E5.m1.7.7.3.2.2.2.2.2.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E5.m1.8.8.4.3.3.3.3.3.6" xref="S3.E5.m1.8.8.4.3.3.3.3.4.cmml">,</mo><msub id="S3.E5.m1.8.8.4.3.3.3.3.3.3" xref="S3.E5.m1.8.8.4.3.3.3.3.3.3.cmml"><mi id="S3.E5.m1.8.8.4.3.3.3.3.3.3.2" xref="S3.E5.m1.8.8.4.3.3.3.3.3.3.2.cmml">F</mi><mi id="S3.E5.m1.8.8.4.3.3.3.3.3.3.3" xref="S3.E5.m1.8.8.4.3.3.3.3.3.3.3.cmml">j</mi></msub><mo stretchy="false" id="S3.E5.m1.8.8.4.3.3.3.3.3.7" xref="S3.E5.m1.8.8.4.3.3.3.3.4.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.8b"><apply id="S3.E5.m1.8.8.cmml" xref="S3.E5.m1.8.8"><eq id="S3.E5.m1.8.8.5.cmml" xref="S3.E5.m1.8.8.5"></eq><apply id="S3.E5.m1.5.5.1.cmml" xref="S3.E5.m1.5.5.1"><times id="S3.E5.m1.5.5.1.2.cmml" xref="S3.E5.m1.5.5.1.2"></times><ci id="S3.E5.m1.5.5.1.3.cmml" xref="S3.E5.m1.5.5.1.3">𝐼</ci><ci id="S3.E5.m1.5.5.1.4.cmml" xref="S3.E5.m1.5.5.1.4">𝐶</ci><ci id="S3.E5.m1.5.5.1.5.cmml" xref="S3.E5.m1.5.5.1.5">𝐴</ci><interval closure="open" id="S3.E5.m1.5.5.1.1.2.cmml" xref="S3.E5.m1.5.5.1.1.1"><apply id="S3.E5.m1.5.5.1.1.1.1.cmml" xref="S3.E5.m1.5.5.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.5.5.1.1.1.1.1.cmml" xref="S3.E5.m1.5.5.1.1.1.1">subscript</csymbol><ci id="S3.E5.m1.5.5.1.1.1.1.2.cmml" xref="S3.E5.m1.5.5.1.1.1.1.2">𝑄</ci><ci id="S3.E5.m1.5.5.1.1.1.1.3.cmml" xref="S3.E5.m1.5.5.1.1.1.1.3">𝑝</ci></apply><ci id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1">𝐹</ci></interval></apply><apply id="S3.E5.m1.8.8.4.cmml" xref="S3.E5.m1.8.8.4"><times id="S3.E5.m1.8.8.4.4.cmml" xref="S3.E5.m1.8.8.4.4"></times><apply id="S3.E5.m1.8.8.4.5.cmml" xref="S3.E5.m1.8.8.4.5"><divide id="S3.E5.m1.8.8.4.5.1.cmml" xref="S3.E5.m1.8.8.4.5"></divide><cn type="integer" id="S3.E5.m1.8.8.4.5.2.cmml" xref="S3.E5.m1.8.8.4.5.2">1</cn><apply id="S3.E5.m1.8.8.4.5.3.cmml" xref="S3.E5.m1.8.8.4.5.3"><csymbol cd="ambiguous" id="S3.E5.m1.8.8.4.5.3.1.cmml" xref="S3.E5.m1.8.8.4.5.3">subscript</csymbol><ci id="S3.E5.m1.8.8.4.5.3.2.cmml" xref="S3.E5.m1.8.8.4.5.3.2">𝑉</ci><apply id="S3.E5.m1.8.8.4.5.3.3.cmml" xref="S3.E5.m1.8.8.4.5.3.3"><times id="S3.E5.m1.8.8.4.5.3.3.1.cmml" xref="S3.E5.m1.8.8.4.5.3.3.1"></times><ci id="S3.E5.m1.8.8.4.5.3.3.2.cmml" xref="S3.E5.m1.8.8.4.5.3.3.2">ℎ</ci><ci id="S3.E5.m1.8.8.4.5.3.3.3.cmml" xref="S3.E5.m1.8.8.4.5.3.3.3">𝑖</ci><ci id="S3.E5.m1.8.8.4.5.3.3.4.cmml" xref="S3.E5.m1.8.8.4.5.3.3.4">𝑡</ci></apply></apply></apply><apply id="S3.E5.m1.8.8.4.3.cmml" xref="S3.E5.m1.8.8.4.3"><apply id="S3.E5.m1.8.8.4.3.4.cmml" xref="S3.E5.m1.8.8.4.3.4"><csymbol cd="ambiguous" id="S3.E5.m1.8.8.4.3.4.1.cmml" xref="S3.E5.m1.8.8.4.3.4">superscript</csymbol><apply id="S3.E5.m1.8.8.4.3.4.2.cmml" xref="S3.E5.m1.8.8.4.3.4"><csymbol cd="ambiguous" id="S3.E5.m1.8.8.4.3.4.2.1.cmml" xref="S3.E5.m1.8.8.4.3.4">subscript</csymbol><sum id="S3.E5.m1.8.8.4.3.4.2.2.cmml" xref="S3.E5.m1.8.8.4.3.4.2.2"></sum><apply id="S3.E5.m1.8.8.4.3.4.2.3.cmml" xref="S3.E5.m1.8.8.4.3.4.2.3"><eq id="S3.E5.m1.8.8.4.3.4.2.3.1.cmml" xref="S3.E5.m1.8.8.4.3.4.2.3.1"></eq><ci id="S3.E5.m1.8.8.4.3.4.2.3.2.cmml" xref="S3.E5.m1.8.8.4.3.4.2.3.2">𝑖</ci><cn type="integer" id="S3.E5.m1.8.8.4.3.4.2.3.3.cmml" xref="S3.E5.m1.8.8.4.3.4.2.3.3">1</cn></apply></apply><apply id="S3.E5.m1.8.8.4.3.4.3.cmml" xref="S3.E5.m1.8.8.4.3.4.3"><csymbol cd="ambiguous" id="S3.E5.m1.8.8.4.3.4.3.1.cmml" xref="S3.E5.m1.8.8.4.3.4.3">subscript</csymbol><ci id="S3.E5.m1.8.8.4.3.4.3.2.cmml" xref="S3.E5.m1.8.8.4.3.4.3.2">𝑁</ci><apply id="S3.E5.m1.8.8.4.3.4.3.3.cmml" xref="S3.E5.m1.8.8.4.3.4.3.3"><times id="S3.E5.m1.8.8.4.3.4.3.3.1.cmml" xref="S3.E5.m1.8.8.4.3.4.3.3.1"></times><ci id="S3.E5.m1.8.8.4.3.4.3.3.2.cmml" xref="S3.E5.m1.8.8.4.3.4.3.3.2">𝑟</ci><ci id="S3.E5.m1.8.8.4.3.4.3.3.3.cmml" xref="S3.E5.m1.8.8.4.3.4.3.3.3">𝑒</ci><ci id="S3.E5.m1.8.8.4.3.4.3.3.4.cmml" xref="S3.E5.m1.8.8.4.3.4.3.3.4">𝑓</ci></apply></apply></apply><apply id="S3.E5.m1.8.8.4.3.3.cmml" xref="S3.E5.m1.8.8.4.3.3"><apply id="S3.E5.m1.8.8.4.3.3.4.cmml" xref="S3.E5.m1.8.8.4.3.3.4"><csymbol cd="ambiguous" id="S3.E5.m1.8.8.4.3.3.4.1.cmml" xref="S3.E5.m1.8.8.4.3.3.4">superscript</csymbol><apply id="S3.E5.m1.8.8.4.3.3.4.2.cmml" xref="S3.E5.m1.8.8.4.3.3.4"><csymbol cd="ambiguous" id="S3.E5.m1.8.8.4.3.3.4.2.1.cmml" xref="S3.E5.m1.8.8.4.3.3.4">subscript</csymbol><sum id="S3.E5.m1.8.8.4.3.3.4.2.2.cmml" xref="S3.E5.m1.8.8.4.3.3.4.2.2"></sum><apply id="S3.E5.m1.8.8.4.3.3.4.2.3.cmml" xref="S3.E5.m1.8.8.4.3.3.4.2.3"><eq id="S3.E5.m1.8.8.4.3.3.4.2.3.1.cmml" xref="S3.E5.m1.8.8.4.3.3.4.2.3.1"></eq><ci id="S3.E5.m1.8.8.4.3.3.4.2.3.2.cmml" xref="S3.E5.m1.8.8.4.3.3.4.2.3.2">𝑗</ci><cn type="integer" id="S3.E5.m1.8.8.4.3.3.4.2.3.3.cmml" xref="S3.E5.m1.8.8.4.3.3.4.2.3.3">1</cn></apply></apply><apply id="S3.E5.m1.8.8.4.3.3.4.3.cmml" xref="S3.E5.m1.8.8.4.3.3.4.3"><csymbol cd="ambiguous" id="S3.E5.m1.8.8.4.3.3.4.3.1.cmml" xref="S3.E5.m1.8.8.4.3.3.4.3">subscript</csymbol><ci id="S3.E5.m1.8.8.4.3.3.4.3.2.cmml" xref="S3.E5.m1.8.8.4.3.3.4.3.2">𝑉</ci><apply id="S3.E5.m1.8.8.4.3.3.4.3.3.cmml" xref="S3.E5.m1.8.8.4.3.3.4.3.3"><times id="S3.E5.m1.8.8.4.3.3.4.3.3.1.cmml" xref="S3.E5.m1.8.8.4.3.3.4.3.3.1"></times><ci id="S3.E5.m1.8.8.4.3.3.4.3.3.2.cmml" xref="S3.E5.m1.8.8.4.3.3.4.3.3.2">ℎ</ci><ci id="S3.E5.m1.8.8.4.3.3.4.3.3.3.cmml" xref="S3.E5.m1.8.8.4.3.3.4.3.3.3">𝑖</ci><ci id="S3.E5.m1.8.8.4.3.3.4.3.3.4.cmml" xref="S3.E5.m1.8.8.4.3.3.4.3.3.4">𝑡</ci></apply></apply></apply><apply id="S3.E5.m1.8.8.4.3.3.3.cmml" xref="S3.E5.m1.8.8.4.3.3.3"><times id="S3.E5.m1.8.8.4.3.3.3.4.cmml" xref="S3.E5.m1.8.8.4.3.3.3.4"></times><ci id="S3.E5.m1.8.8.4.3.3.3.5.cmml" xref="S3.E5.m1.8.8.4.3.3.3.5">𝐷</ci><ci id="S3.E5.m1.8.8.4.3.3.3.6.cmml" xref="S3.E5.m1.8.8.4.3.3.3.6">𝑒</ci><ci id="S3.E5.m1.8.8.4.3.3.3.7.cmml" xref="S3.E5.m1.8.8.4.3.3.3.7">𝑓</ci><ci id="S3.E5.m1.8.8.4.3.3.3.8.cmml" xref="S3.E5.m1.8.8.4.3.3.3.8">𝐴</ci><ci id="S3.E5.m1.8.8.4.3.3.3.9.cmml" xref="S3.E5.m1.8.8.4.3.3.3.9">𝑡</ci><ci id="S3.E5.m1.8.8.4.3.3.3.10.cmml" xref="S3.E5.m1.8.8.4.3.3.3.10">𝑡</ci><ci id="S3.E5.m1.8.8.4.3.3.3.11.cmml" xref="S3.E5.m1.8.8.4.3.3.3.11">𝑛</ci><vector id="S3.E5.m1.8.8.4.3.3.3.3.4.cmml" xref="S3.E5.m1.8.8.4.3.3.3.3.3"><apply id="S3.E5.m1.6.6.2.1.1.1.1.1.1.cmml" xref="S3.E5.m1.6.6.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.6.6.2.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.6.6.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E5.m1.6.6.2.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.6.6.2.1.1.1.1.1.1.2">𝑄</ci><ci id="S3.E5.m1.6.6.2.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.6.6.2.1.1.1.1.1.1.3">𝑝</ci></apply><apply id="S3.E5.m1.7.7.3.2.2.2.2.2.2.cmml" xref="S3.E5.m1.7.7.3.2.2.2.2.2.2"><times id="S3.E5.m1.7.7.3.2.2.2.2.2.2.1.cmml" xref="S3.E5.m1.7.7.3.2.2.2.2.2.2.1"></times><ci id="S3.E5.m1.7.7.3.2.2.2.2.2.2.2.cmml" xref="S3.E5.m1.7.7.3.2.2.2.2.2.2.2">𝑃</ci><vector id="S3.E5.m1.7.7.3.2.2.2.2.2.2.3.1.cmml" xref="S3.E5.m1.7.7.3.2.2.2.2.2.2.3.2"><ci id="S3.E5.m1.2.2.cmml" xref="S3.E5.m1.2.2">𝑝</ci><ci id="S3.E5.m1.3.3.cmml" xref="S3.E5.m1.3.3">𝑖</ci><ci id="S3.E5.m1.4.4.cmml" xref="S3.E5.m1.4.4">𝑗</ci></vector></apply><apply id="S3.E5.m1.8.8.4.3.3.3.3.3.3.cmml" xref="S3.E5.m1.8.8.4.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E5.m1.8.8.4.3.3.3.3.3.3.1.cmml" xref="S3.E5.m1.8.8.4.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E5.m1.8.8.4.3.3.3.3.3.3.2.cmml" xref="S3.E5.m1.8.8.4.3.3.3.3.3.3.2">𝐹</ci><ci id="S3.E5.m1.8.8.4.3.3.3.3.3.3.3.cmml" xref="S3.E5.m1.8.8.4.3.3.3.3.3.3.3">𝑗</ci></apply></vector></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.8c">ICA(Q_{p},F)={\frac{1}{V_{hit}}}{\sum_{i=1}^{N_{ref}}{\sum_{j=1}^{V_{hit}}DefAttn(Q_{p},P(p,i,j),F_{j})}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.SSS0.Px4.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px4.p2.8" class="ltx_p">where <math id="S3.SS1.SSS0.Px4.p2.1.m1.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS1.SSS0.Px4.p2.1.m1.1a"><mi id="S3.SS1.SSS0.Px4.p2.1.m1.1.1" xref="S3.SS1.SSS0.Px4.p2.1.m1.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p2.1.m1.1b"><ci id="S3.SS1.SSS0.Px4.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p2.1.m1.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p2.1.m1.1c">j</annotation></semantics></math> is the index of the camera view, <math id="S3.SS1.SSS0.Px4.p2.2.m2.1" class="ltx_Math" alttext="F_{j}" display="inline"><semantics id="S3.SS1.SSS0.Px4.p2.2.m2.1a"><msub id="S3.SS1.SSS0.Px4.p2.2.m2.1.1" xref="S3.SS1.SSS0.Px4.p2.2.m2.1.1.cmml"><mi id="S3.SS1.SSS0.Px4.p2.2.m2.1.1.2" xref="S3.SS1.SSS0.Px4.p2.2.m2.1.1.2.cmml">F</mi><mi id="S3.SS1.SSS0.Px4.p2.2.m2.1.1.3" xref="S3.SS1.SSS0.Px4.p2.2.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p2.2.m2.1b"><apply id="S3.SS1.SSS0.Px4.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p2.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p2.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px4.p2.2.m2.1.1.2">𝐹</ci><ci id="S3.SS1.SSS0.Px4.p2.2.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px4.p2.2.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p2.2.m2.1c">F_{j}</annotation></semantics></math> represents the image features of the <math id="S3.SS1.SSS0.Px4.p2.3.m3.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS1.SSS0.Px4.p2.3.m3.1a"><mi id="S3.SS1.SSS0.Px4.p2.3.m3.1.1" xref="S3.SS1.SSS0.Px4.p2.3.m3.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p2.3.m3.1b"><ci id="S3.SS1.SSS0.Px4.p2.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px4.p2.3.m3.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p2.3.m3.1c">j</annotation></semantics></math>-th camera, and <math id="S3.SS1.SSS0.Px4.p2.4.m4.3" class="ltx_Math" alttext="P(p,i,j)" display="inline"><semantics id="S3.SS1.SSS0.Px4.p2.4.m4.3a"><mrow id="S3.SS1.SSS0.Px4.p2.4.m4.3.4" xref="S3.SS1.SSS0.Px4.p2.4.m4.3.4.cmml"><mi id="S3.SS1.SSS0.Px4.p2.4.m4.3.4.2" xref="S3.SS1.SSS0.Px4.p2.4.m4.3.4.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px4.p2.4.m4.3.4.1" xref="S3.SS1.SSS0.Px4.p2.4.m4.3.4.1.cmml">​</mo><mrow id="S3.SS1.SSS0.Px4.p2.4.m4.3.4.3.2" xref="S3.SS1.SSS0.Px4.p2.4.m4.3.4.3.1.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px4.p2.4.m4.3.4.3.2.1" xref="S3.SS1.SSS0.Px4.p2.4.m4.3.4.3.1.cmml">(</mo><mi id="S3.SS1.SSS0.Px4.p2.4.m4.1.1" xref="S3.SS1.SSS0.Px4.p2.4.m4.1.1.cmml">p</mi><mo id="S3.SS1.SSS0.Px4.p2.4.m4.3.4.3.2.2" xref="S3.SS1.SSS0.Px4.p2.4.m4.3.4.3.1.cmml">,</mo><mi id="S3.SS1.SSS0.Px4.p2.4.m4.2.2" xref="S3.SS1.SSS0.Px4.p2.4.m4.2.2.cmml">i</mi><mo id="S3.SS1.SSS0.Px4.p2.4.m4.3.4.3.2.3" xref="S3.SS1.SSS0.Px4.p2.4.m4.3.4.3.1.cmml">,</mo><mi id="S3.SS1.SSS0.Px4.p2.4.m4.3.3" xref="S3.SS1.SSS0.Px4.p2.4.m4.3.3.cmml">j</mi><mo stretchy="false" id="S3.SS1.SSS0.Px4.p2.4.m4.3.4.3.2.4" xref="S3.SS1.SSS0.Px4.p2.4.m4.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p2.4.m4.3b"><apply id="S3.SS1.SSS0.Px4.p2.4.m4.3.4.cmml" xref="S3.SS1.SSS0.Px4.p2.4.m4.3.4"><times id="S3.SS1.SSS0.Px4.p2.4.m4.3.4.1.cmml" xref="S3.SS1.SSS0.Px4.p2.4.m4.3.4.1"></times><ci id="S3.SS1.SSS0.Px4.p2.4.m4.3.4.2.cmml" xref="S3.SS1.SSS0.Px4.p2.4.m4.3.4.2">𝑃</ci><vector id="S3.SS1.SSS0.Px4.p2.4.m4.3.4.3.1.cmml" xref="S3.SS1.SSS0.Px4.p2.4.m4.3.4.3.2"><ci id="S3.SS1.SSS0.Px4.p2.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px4.p2.4.m4.1.1">𝑝</ci><ci id="S3.SS1.SSS0.Px4.p2.4.m4.2.2.cmml" xref="S3.SS1.SSS0.Px4.p2.4.m4.2.2">𝑖</ci><ci id="S3.SS1.SSS0.Px4.p2.4.m4.3.3.cmml" xref="S3.SS1.SSS0.Px4.p2.4.m4.3.3">𝑗</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p2.4.m4.3c">P(p,i,j)</annotation></semantics></math> represents the projection point of the <math id="S3.SS1.SSS0.Px4.p2.5.m5.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS1.SSS0.Px4.p2.5.m5.1a"><mi id="S3.SS1.SSS0.Px4.p2.5.m5.1.1" xref="S3.SS1.SSS0.Px4.p2.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p2.5.m5.1b"><ci id="S3.SS1.SSS0.Px4.p2.5.m5.1.1.cmml" xref="S3.SS1.SSS0.Px4.p2.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p2.5.m5.1c">i</annotation></semantics></math>-th 3D reference point <math id="S3.SS1.SSS0.Px4.p2.6.m6.3" class="ltx_Math" alttext="(x,y,z_{i})" display="inline"><semantics id="S3.SS1.SSS0.Px4.p2.6.m6.3a"><mrow id="S3.SS1.SSS0.Px4.p2.6.m6.3.3.1" xref="S3.SS1.SSS0.Px4.p2.6.m6.3.3.2.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px4.p2.6.m6.3.3.1.2" xref="S3.SS1.SSS0.Px4.p2.6.m6.3.3.2.cmml">(</mo><mi id="S3.SS1.SSS0.Px4.p2.6.m6.1.1" xref="S3.SS1.SSS0.Px4.p2.6.m6.1.1.cmml">x</mi><mo id="S3.SS1.SSS0.Px4.p2.6.m6.3.3.1.3" xref="S3.SS1.SSS0.Px4.p2.6.m6.3.3.2.cmml">,</mo><mi id="S3.SS1.SSS0.Px4.p2.6.m6.2.2" xref="S3.SS1.SSS0.Px4.p2.6.m6.2.2.cmml">y</mi><mo id="S3.SS1.SSS0.Px4.p2.6.m6.3.3.1.4" xref="S3.SS1.SSS0.Px4.p2.6.m6.3.3.2.cmml">,</mo><msub id="S3.SS1.SSS0.Px4.p2.6.m6.3.3.1.1" xref="S3.SS1.SSS0.Px4.p2.6.m6.3.3.1.1.cmml"><mi id="S3.SS1.SSS0.Px4.p2.6.m6.3.3.1.1.2" xref="S3.SS1.SSS0.Px4.p2.6.m6.3.3.1.1.2.cmml">z</mi><mi id="S3.SS1.SSS0.Px4.p2.6.m6.3.3.1.1.3" xref="S3.SS1.SSS0.Px4.p2.6.m6.3.3.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.SSS0.Px4.p2.6.m6.3.3.1.5" xref="S3.SS1.SSS0.Px4.p2.6.m6.3.3.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p2.6.m6.3b"><vector id="S3.SS1.SSS0.Px4.p2.6.m6.3.3.2.cmml" xref="S3.SS1.SSS0.Px4.p2.6.m6.3.3.1"><ci id="S3.SS1.SSS0.Px4.p2.6.m6.1.1.cmml" xref="S3.SS1.SSS0.Px4.p2.6.m6.1.1">𝑥</ci><ci id="S3.SS1.SSS0.Px4.p2.6.m6.2.2.cmml" xref="S3.SS1.SSS0.Px4.p2.6.m6.2.2">𝑦</ci><apply id="S3.SS1.SSS0.Px4.p2.6.m6.3.3.1.1.cmml" xref="S3.SS1.SSS0.Px4.p2.6.m6.3.3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p2.6.m6.3.3.1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p2.6.m6.3.3.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p2.6.m6.3.3.1.1.2.cmml" xref="S3.SS1.SSS0.Px4.p2.6.m6.3.3.1.1.2">𝑧</ci><ci id="S3.SS1.SSS0.Px4.p2.6.m6.3.3.1.1.3.cmml" xref="S3.SS1.SSS0.Px4.p2.6.m6.3.3.1.1.3">𝑖</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p2.6.m6.3c">(x,y,z_{i})</annotation></semantics></math> of query <math id="S3.SS1.SSS0.Px4.p2.7.m7.1" class="ltx_Math" alttext="Q_{p}" display="inline"><semantics id="S3.SS1.SSS0.Px4.p2.7.m7.1a"><msub id="S3.SS1.SSS0.Px4.p2.7.m7.1.1" xref="S3.SS1.SSS0.Px4.p2.7.m7.1.1.cmml"><mi id="S3.SS1.SSS0.Px4.p2.7.m7.1.1.2" xref="S3.SS1.SSS0.Px4.p2.7.m7.1.1.2.cmml">Q</mi><mi id="S3.SS1.SSS0.Px4.p2.7.m7.1.1.3" xref="S3.SS1.SSS0.Px4.p2.7.m7.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p2.7.m7.1b"><apply id="S3.SS1.SSS0.Px4.p2.7.m7.1.1.cmml" xref="S3.SS1.SSS0.Px4.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p2.7.m7.1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p2.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p2.7.m7.1.1.2.cmml" xref="S3.SS1.SSS0.Px4.p2.7.m7.1.1.2">𝑄</ci><ci id="S3.SS1.SSS0.Px4.p2.7.m7.1.1.3.cmml" xref="S3.SS1.SSS0.Px4.p2.7.m7.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p2.7.m7.1c">Q_{p}</annotation></semantics></math> in the image coordinate system of the <math id="S3.SS1.SSS0.Px4.p2.8.m8.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS1.SSS0.Px4.p2.8.m8.1a"><mi id="S3.SS1.SSS0.Px4.p2.8.m8.1.1" xref="S3.SS1.SSS0.Px4.p2.8.m8.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p2.8.m8.1b"><ci id="S3.SS1.SSS0.Px4.p2.8.m8.1.1.cmml" xref="S3.SS1.SSS0.Px4.p2.8.m8.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p2.8.m8.1c">j</annotation></semantics></math>-th camera.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" style="width:190.8pt;"><img src="/html/2309.05257/assets/x3.png" id="S3.F4.1.g1" class="ltx_graphics ltx_img_landscape" width="360" height="271" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S3.F4.1.2.1" class="ltx_text ltx_font_bold">Temporal Fusion Encoder (TFE).</span> The initial set of BEV queries is formed by utilizing the BEV features of the current frame. These queries are then subjected to cross-attention with historical BEV features, including the current frame. The resulting queries are updated through a feed-forward network and serve as inputs for the subsequent layer. Through multiple layers of temporal fusion encoding, the final output is obtained, representing the temporally fused BEV feature.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" style="width:190.8pt;"><img src="/html/2309.05257/assets/x4.png" id="S3.F4.2.g1" class="ltx_graphics ltx_img_landscape" width="423" height="262" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span id="S3.F4.2.2.1" class="ltx_text ltx_font_bold">Fusion with depth prediction.</span> After being processed by the backbone network, the multi-view image features are split into two branches. One branch utilizes a feature pyramid network (FPN) to extract multi-scale image features. The other branch employs a monocular depth prediction network to estimate depth and utilizes 3D convolution to encode the depth predictions. The multi-scale image features and the depth embedding are jointly input into the encoder to obtain the BEV features.</figcaption>
</figure>
</div>
</div>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Temporal Fusion Encoder</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">As shown in Figure <a href="#S3.F4" title="Figure 4 ‣ Images Cross-Attention. ‣ 3.1 Multi-modal Fusion Encoder ‣ 3 Method ‣ FusionFormer: A Multi-sensory Fusion in Bird’s-Eye-View and Temporal Consistent Transformer for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the temporal fusion encoder (TFE) consists of three layers, each comprising BEV temporal-attention and feedforward networks. At the first layer, the queries are initialized with the BEV features of the current frame and then updated through temporal-attention using historical BEV features. The resulting queries are passed through a feedforward network and serve as input to the next layer. After three layers of fusion encoding, the final temporal fusion BEV features are obtained. The temporal-attention process can be expressed as:</p>
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E6.m1.5" class="ltx_Math" alttext="TCA(Q_{p},B)={\sum_{i=0}^{T}{DefAttn(Q_{p},P,B_{t-i})}}" display="block"><semantics id="S3.E6.m1.5a"><mrow id="S3.E6.m1.5.5" xref="S3.E6.m1.5.5.cmml"><mrow id="S3.E6.m1.3.3.1" xref="S3.E6.m1.3.3.1.cmml"><mi id="S3.E6.m1.3.3.1.3" xref="S3.E6.m1.3.3.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.3.3.1.2" xref="S3.E6.m1.3.3.1.2.cmml">​</mo><mi id="S3.E6.m1.3.3.1.4" xref="S3.E6.m1.3.3.1.4.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.3.3.1.2a" xref="S3.E6.m1.3.3.1.2.cmml">​</mo><mi id="S3.E6.m1.3.3.1.5" xref="S3.E6.m1.3.3.1.5.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.3.3.1.2b" xref="S3.E6.m1.3.3.1.2.cmml">​</mo><mrow id="S3.E6.m1.3.3.1.1.1" xref="S3.E6.m1.3.3.1.1.2.cmml"><mo stretchy="false" id="S3.E6.m1.3.3.1.1.1.2" xref="S3.E6.m1.3.3.1.1.2.cmml">(</mo><msub id="S3.E6.m1.3.3.1.1.1.1" xref="S3.E6.m1.3.3.1.1.1.1.cmml"><mi id="S3.E6.m1.3.3.1.1.1.1.2" xref="S3.E6.m1.3.3.1.1.1.1.2.cmml">Q</mi><mi id="S3.E6.m1.3.3.1.1.1.1.3" xref="S3.E6.m1.3.3.1.1.1.1.3.cmml">p</mi></msub><mo id="S3.E6.m1.3.3.1.1.1.3" xref="S3.E6.m1.3.3.1.1.2.cmml">,</mo><mi id="S3.E6.m1.1.1" xref="S3.E6.m1.1.1.cmml">B</mi><mo stretchy="false" id="S3.E6.m1.3.3.1.1.1.4" xref="S3.E6.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S3.E6.m1.5.5.4" xref="S3.E6.m1.5.5.4.cmml">=</mo><mrow id="S3.E6.m1.5.5.3" xref="S3.E6.m1.5.5.3.cmml"><munderover id="S3.E6.m1.5.5.3.3" xref="S3.E6.m1.5.5.3.3.cmml"><mo movablelimits="false" id="S3.E6.m1.5.5.3.3.2.2" xref="S3.E6.m1.5.5.3.3.2.2.cmml">∑</mo><mrow id="S3.E6.m1.5.5.3.3.2.3" xref="S3.E6.m1.5.5.3.3.2.3.cmml"><mi id="S3.E6.m1.5.5.3.3.2.3.2" xref="S3.E6.m1.5.5.3.3.2.3.2.cmml">i</mi><mo id="S3.E6.m1.5.5.3.3.2.3.1" xref="S3.E6.m1.5.5.3.3.2.3.1.cmml">=</mo><mn id="S3.E6.m1.5.5.3.3.2.3.3" xref="S3.E6.m1.5.5.3.3.2.3.3.cmml">0</mn></mrow><mi id="S3.E6.m1.5.5.3.3.3" xref="S3.E6.m1.5.5.3.3.3.cmml">T</mi></munderover><mrow id="S3.E6.m1.5.5.3.2" xref="S3.E6.m1.5.5.3.2.cmml"><mi id="S3.E6.m1.5.5.3.2.4" xref="S3.E6.m1.5.5.3.2.4.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.5.5.3.2.3" xref="S3.E6.m1.5.5.3.2.3.cmml">​</mo><mi id="S3.E6.m1.5.5.3.2.5" xref="S3.E6.m1.5.5.3.2.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.5.5.3.2.3a" xref="S3.E6.m1.5.5.3.2.3.cmml">​</mo><mi id="S3.E6.m1.5.5.3.2.6" xref="S3.E6.m1.5.5.3.2.6.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.5.5.3.2.3b" xref="S3.E6.m1.5.5.3.2.3.cmml">​</mo><mi id="S3.E6.m1.5.5.3.2.7" xref="S3.E6.m1.5.5.3.2.7.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.5.5.3.2.3c" xref="S3.E6.m1.5.5.3.2.3.cmml">​</mo><mi id="S3.E6.m1.5.5.3.2.8" xref="S3.E6.m1.5.5.3.2.8.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.5.5.3.2.3d" xref="S3.E6.m1.5.5.3.2.3.cmml">​</mo><mi id="S3.E6.m1.5.5.3.2.9" xref="S3.E6.m1.5.5.3.2.9.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.5.5.3.2.3e" xref="S3.E6.m1.5.5.3.2.3.cmml">​</mo><mi id="S3.E6.m1.5.5.3.2.10" xref="S3.E6.m1.5.5.3.2.10.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.5.5.3.2.3f" xref="S3.E6.m1.5.5.3.2.3.cmml">​</mo><mrow id="S3.E6.m1.5.5.3.2.2.2" xref="S3.E6.m1.5.5.3.2.2.3.cmml"><mo stretchy="false" id="S3.E6.m1.5.5.3.2.2.2.3" xref="S3.E6.m1.5.5.3.2.2.3.cmml">(</mo><msub id="S3.E6.m1.4.4.2.1.1.1.1" xref="S3.E6.m1.4.4.2.1.1.1.1.cmml"><mi id="S3.E6.m1.4.4.2.1.1.1.1.2" xref="S3.E6.m1.4.4.2.1.1.1.1.2.cmml">Q</mi><mi id="S3.E6.m1.4.4.2.1.1.1.1.3" xref="S3.E6.m1.4.4.2.1.1.1.1.3.cmml">p</mi></msub><mo id="S3.E6.m1.5.5.3.2.2.2.4" xref="S3.E6.m1.5.5.3.2.2.3.cmml">,</mo><mi id="S3.E6.m1.2.2" xref="S3.E6.m1.2.2.cmml">P</mi><mo id="S3.E6.m1.5.5.3.2.2.2.5" xref="S3.E6.m1.5.5.3.2.2.3.cmml">,</mo><msub id="S3.E6.m1.5.5.3.2.2.2.2" xref="S3.E6.m1.5.5.3.2.2.2.2.cmml"><mi id="S3.E6.m1.5.5.3.2.2.2.2.2" xref="S3.E6.m1.5.5.3.2.2.2.2.2.cmml">B</mi><mrow id="S3.E6.m1.5.5.3.2.2.2.2.3" xref="S3.E6.m1.5.5.3.2.2.2.2.3.cmml"><mi id="S3.E6.m1.5.5.3.2.2.2.2.3.2" xref="S3.E6.m1.5.5.3.2.2.2.2.3.2.cmml">t</mi><mo id="S3.E6.m1.5.5.3.2.2.2.2.3.1" xref="S3.E6.m1.5.5.3.2.2.2.2.3.1.cmml">−</mo><mi id="S3.E6.m1.5.5.3.2.2.2.2.3.3" xref="S3.E6.m1.5.5.3.2.2.2.2.3.3.cmml">i</mi></mrow></msub><mo stretchy="false" id="S3.E6.m1.5.5.3.2.2.2.6" xref="S3.E6.m1.5.5.3.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.5b"><apply id="S3.E6.m1.5.5.cmml" xref="S3.E6.m1.5.5"><eq id="S3.E6.m1.5.5.4.cmml" xref="S3.E6.m1.5.5.4"></eq><apply id="S3.E6.m1.3.3.1.cmml" xref="S3.E6.m1.3.3.1"><times id="S3.E6.m1.3.3.1.2.cmml" xref="S3.E6.m1.3.3.1.2"></times><ci id="S3.E6.m1.3.3.1.3.cmml" xref="S3.E6.m1.3.3.1.3">𝑇</ci><ci id="S3.E6.m1.3.3.1.4.cmml" xref="S3.E6.m1.3.3.1.4">𝐶</ci><ci id="S3.E6.m1.3.3.1.5.cmml" xref="S3.E6.m1.3.3.1.5">𝐴</ci><interval closure="open" id="S3.E6.m1.3.3.1.1.2.cmml" xref="S3.E6.m1.3.3.1.1.1"><apply id="S3.E6.m1.3.3.1.1.1.1.cmml" xref="S3.E6.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.3.3.1.1.1.1.1.cmml" xref="S3.E6.m1.3.3.1.1.1.1">subscript</csymbol><ci id="S3.E6.m1.3.3.1.1.1.1.2.cmml" xref="S3.E6.m1.3.3.1.1.1.1.2">𝑄</ci><ci id="S3.E6.m1.3.3.1.1.1.1.3.cmml" xref="S3.E6.m1.3.3.1.1.1.1.3">𝑝</ci></apply><ci id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.1.1">𝐵</ci></interval></apply><apply id="S3.E6.m1.5.5.3.cmml" xref="S3.E6.m1.5.5.3"><apply id="S3.E6.m1.5.5.3.3.cmml" xref="S3.E6.m1.5.5.3.3"><csymbol cd="ambiguous" id="S3.E6.m1.5.5.3.3.1.cmml" xref="S3.E6.m1.5.5.3.3">superscript</csymbol><apply id="S3.E6.m1.5.5.3.3.2.cmml" xref="S3.E6.m1.5.5.3.3"><csymbol cd="ambiguous" id="S3.E6.m1.5.5.3.3.2.1.cmml" xref="S3.E6.m1.5.5.3.3">subscript</csymbol><sum id="S3.E6.m1.5.5.3.3.2.2.cmml" xref="S3.E6.m1.5.5.3.3.2.2"></sum><apply id="S3.E6.m1.5.5.3.3.2.3.cmml" xref="S3.E6.m1.5.5.3.3.2.3"><eq id="S3.E6.m1.5.5.3.3.2.3.1.cmml" xref="S3.E6.m1.5.5.3.3.2.3.1"></eq><ci id="S3.E6.m1.5.5.3.3.2.3.2.cmml" xref="S3.E6.m1.5.5.3.3.2.3.2">𝑖</ci><cn type="integer" id="S3.E6.m1.5.5.3.3.2.3.3.cmml" xref="S3.E6.m1.5.5.3.3.2.3.3">0</cn></apply></apply><ci id="S3.E6.m1.5.5.3.3.3.cmml" xref="S3.E6.m1.5.5.3.3.3">𝑇</ci></apply><apply id="S3.E6.m1.5.5.3.2.cmml" xref="S3.E6.m1.5.5.3.2"><times id="S3.E6.m1.5.5.3.2.3.cmml" xref="S3.E6.m1.5.5.3.2.3"></times><ci id="S3.E6.m1.5.5.3.2.4.cmml" xref="S3.E6.m1.5.5.3.2.4">𝐷</ci><ci id="S3.E6.m1.5.5.3.2.5.cmml" xref="S3.E6.m1.5.5.3.2.5">𝑒</ci><ci id="S3.E6.m1.5.5.3.2.6.cmml" xref="S3.E6.m1.5.5.3.2.6">𝑓</ci><ci id="S3.E6.m1.5.5.3.2.7.cmml" xref="S3.E6.m1.5.5.3.2.7">𝐴</ci><ci id="S3.E6.m1.5.5.3.2.8.cmml" xref="S3.E6.m1.5.5.3.2.8">𝑡</ci><ci id="S3.E6.m1.5.5.3.2.9.cmml" xref="S3.E6.m1.5.5.3.2.9">𝑡</ci><ci id="S3.E6.m1.5.5.3.2.10.cmml" xref="S3.E6.m1.5.5.3.2.10">𝑛</ci><vector id="S3.E6.m1.5.5.3.2.2.3.cmml" xref="S3.E6.m1.5.5.3.2.2.2"><apply id="S3.E6.m1.4.4.2.1.1.1.1.cmml" xref="S3.E6.m1.4.4.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.4.4.2.1.1.1.1.1.cmml" xref="S3.E6.m1.4.4.2.1.1.1.1">subscript</csymbol><ci id="S3.E6.m1.4.4.2.1.1.1.1.2.cmml" xref="S3.E6.m1.4.4.2.1.1.1.1.2">𝑄</ci><ci id="S3.E6.m1.4.4.2.1.1.1.1.3.cmml" xref="S3.E6.m1.4.4.2.1.1.1.1.3">𝑝</ci></apply><ci id="S3.E6.m1.2.2.cmml" xref="S3.E6.m1.2.2">𝑃</ci><apply id="S3.E6.m1.5.5.3.2.2.2.2.cmml" xref="S3.E6.m1.5.5.3.2.2.2.2"><csymbol cd="ambiguous" id="S3.E6.m1.5.5.3.2.2.2.2.1.cmml" xref="S3.E6.m1.5.5.3.2.2.2.2">subscript</csymbol><ci id="S3.E6.m1.5.5.3.2.2.2.2.2.cmml" xref="S3.E6.m1.5.5.3.2.2.2.2.2">𝐵</ci><apply id="S3.E6.m1.5.5.3.2.2.2.2.3.cmml" xref="S3.E6.m1.5.5.3.2.2.2.2.3"><minus id="S3.E6.m1.5.5.3.2.2.2.2.3.1.cmml" xref="S3.E6.m1.5.5.3.2.2.2.2.3.1"></minus><ci id="S3.E6.m1.5.5.3.2.2.2.2.3.2.cmml" xref="S3.E6.m1.5.5.3.2.2.2.2.3.2">𝑡</ci><ci id="S3.E6.m1.5.5.3.2.2.2.2.3.3.cmml" xref="S3.E6.m1.5.5.3.2.2.2.2.3.3">𝑖</ci></apply></apply></vector></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.5c">TCA(Q_{p},B)={\sum_{i=0}^{T}{DefAttn(Q_{p},P,B_{t-i})}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.2" class="ltx_p">where <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="B_{t-i}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><msub id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">B</mi><mrow id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml"><mi id="S3.SS2.p2.1.m1.1.1.3.2" xref="S3.SS2.p2.1.m1.1.1.3.2.cmml">t</mi><mo id="S3.SS2.p2.1.m1.1.1.3.1" xref="S3.SS2.p2.1.m1.1.1.3.1.cmml">−</mo><mi id="S3.SS2.p2.1.m1.1.1.3.3" xref="S3.SS2.p2.1.m1.1.1.3.3.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">𝐵</ci><apply id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3"><minus id="S3.SS2.p2.1.m1.1.1.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3.1"></minus><ci id="S3.SS2.p2.1.m1.1.1.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.3.2">𝑡</ci><ci id="S3.SS2.p2.1.m1.1.1.3.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">B_{t-i}</annotation></semantics></math> represents the BEV feature at time <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="t-i" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">t</mi><mo id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.cmml">−</mo><mi id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><minus id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"></minus><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">𝑡</ci><ci id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">t-i</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Fusion with Depth Prediction</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">The flexibility of FusionFormer enables us to approximate the point cloud branch in scenarios where only camera images are available by adding an image-based monocular depth prediction branch. As illustrated in Figure <a href="#S3.F4" title="Figure 4 ‣ Images Cross-Attention. ‣ 3.1 Multi-modal Fusion Encoder ‣ 3 Method ‣ FusionFormer: A Multi-sensory Fusion in Bird’s-Eye-View and Temporal Consistent Transformer for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we propose a depth prediction network to generate interval-based depth predictions from input image features. 3D convolution is utilized to encode the depth prediction results as voxel features in each camera frustum. Depth cross-attention is then employed to fuse the depth features. The process of depth cross-attention is defined as follows:</p>
<table id="S3.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E7.m1.8" class="ltx_Math" alttext="DCA(Q_{p},D)={\frac{1}{V_{hit}}}{\sum_{i=1}^{N_{ref}}{\sum_{j=1}^{V_{hit}}DefAttn(Q_{p},P(p,i,j),D_{j})}}" display="block"><semantics id="S3.E7.m1.8a"><mrow id="S3.E7.m1.8.8" xref="S3.E7.m1.8.8.cmml"><mrow id="S3.E7.m1.5.5.1" xref="S3.E7.m1.5.5.1.cmml"><mi id="S3.E7.m1.5.5.1.3" xref="S3.E7.m1.5.5.1.3.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.5.5.1.2" xref="S3.E7.m1.5.5.1.2.cmml">​</mo><mi id="S3.E7.m1.5.5.1.4" xref="S3.E7.m1.5.5.1.4.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.5.5.1.2a" xref="S3.E7.m1.5.5.1.2.cmml">​</mo><mi id="S3.E7.m1.5.5.1.5" xref="S3.E7.m1.5.5.1.5.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.5.5.1.2b" xref="S3.E7.m1.5.5.1.2.cmml">​</mo><mrow id="S3.E7.m1.5.5.1.1.1" xref="S3.E7.m1.5.5.1.1.2.cmml"><mo stretchy="false" id="S3.E7.m1.5.5.1.1.1.2" xref="S3.E7.m1.5.5.1.1.2.cmml">(</mo><msub id="S3.E7.m1.5.5.1.1.1.1" xref="S3.E7.m1.5.5.1.1.1.1.cmml"><mi id="S3.E7.m1.5.5.1.1.1.1.2" xref="S3.E7.m1.5.5.1.1.1.1.2.cmml">Q</mi><mi id="S3.E7.m1.5.5.1.1.1.1.3" xref="S3.E7.m1.5.5.1.1.1.1.3.cmml">p</mi></msub><mo id="S3.E7.m1.5.5.1.1.1.3" xref="S3.E7.m1.5.5.1.1.2.cmml">,</mo><mi id="S3.E7.m1.1.1" xref="S3.E7.m1.1.1.cmml">D</mi><mo stretchy="false" id="S3.E7.m1.5.5.1.1.1.4" xref="S3.E7.m1.5.5.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E7.m1.8.8.5" xref="S3.E7.m1.8.8.5.cmml">=</mo><mrow id="S3.E7.m1.8.8.4" xref="S3.E7.m1.8.8.4.cmml"><mfrac id="S3.E7.m1.8.8.4.5" xref="S3.E7.m1.8.8.4.5.cmml"><mn id="S3.E7.m1.8.8.4.5.2" xref="S3.E7.m1.8.8.4.5.2.cmml">1</mn><msub id="S3.E7.m1.8.8.4.5.3" xref="S3.E7.m1.8.8.4.5.3.cmml"><mi id="S3.E7.m1.8.8.4.5.3.2" xref="S3.E7.m1.8.8.4.5.3.2.cmml">V</mi><mrow id="S3.E7.m1.8.8.4.5.3.3" xref="S3.E7.m1.8.8.4.5.3.3.cmml"><mi id="S3.E7.m1.8.8.4.5.3.3.2" xref="S3.E7.m1.8.8.4.5.3.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.8.8.4.5.3.3.1" xref="S3.E7.m1.8.8.4.5.3.3.1.cmml">​</mo><mi id="S3.E7.m1.8.8.4.5.3.3.3" xref="S3.E7.m1.8.8.4.5.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.8.8.4.5.3.3.1a" xref="S3.E7.m1.8.8.4.5.3.3.1.cmml">​</mo><mi id="S3.E7.m1.8.8.4.5.3.3.4" xref="S3.E7.m1.8.8.4.5.3.3.4.cmml">t</mi></mrow></msub></mfrac><mo lspace="0em" rspace="0em" id="S3.E7.m1.8.8.4.4" xref="S3.E7.m1.8.8.4.4.cmml">​</mo><mrow id="S3.E7.m1.8.8.4.3" xref="S3.E7.m1.8.8.4.3.cmml"><munderover id="S3.E7.m1.8.8.4.3.4" xref="S3.E7.m1.8.8.4.3.4.cmml"><mo movablelimits="false" rspace="0em" id="S3.E7.m1.8.8.4.3.4.2.2" xref="S3.E7.m1.8.8.4.3.4.2.2.cmml">∑</mo><mrow id="S3.E7.m1.8.8.4.3.4.2.3" xref="S3.E7.m1.8.8.4.3.4.2.3.cmml"><mi id="S3.E7.m1.8.8.4.3.4.2.3.2" xref="S3.E7.m1.8.8.4.3.4.2.3.2.cmml">i</mi><mo id="S3.E7.m1.8.8.4.3.4.2.3.1" xref="S3.E7.m1.8.8.4.3.4.2.3.1.cmml">=</mo><mn id="S3.E7.m1.8.8.4.3.4.2.3.3" xref="S3.E7.m1.8.8.4.3.4.2.3.3.cmml">1</mn></mrow><msub id="S3.E7.m1.8.8.4.3.4.3" xref="S3.E7.m1.8.8.4.3.4.3.cmml"><mi id="S3.E7.m1.8.8.4.3.4.3.2" xref="S3.E7.m1.8.8.4.3.4.3.2.cmml">N</mi><mrow id="S3.E7.m1.8.8.4.3.4.3.3" xref="S3.E7.m1.8.8.4.3.4.3.3.cmml"><mi id="S3.E7.m1.8.8.4.3.4.3.3.2" xref="S3.E7.m1.8.8.4.3.4.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.8.8.4.3.4.3.3.1" xref="S3.E7.m1.8.8.4.3.4.3.3.1.cmml">​</mo><mi id="S3.E7.m1.8.8.4.3.4.3.3.3" xref="S3.E7.m1.8.8.4.3.4.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.8.8.4.3.4.3.3.1a" xref="S3.E7.m1.8.8.4.3.4.3.3.1.cmml">​</mo><mi id="S3.E7.m1.8.8.4.3.4.3.3.4" xref="S3.E7.m1.8.8.4.3.4.3.3.4.cmml">f</mi></mrow></msub></munderover><mrow id="S3.E7.m1.8.8.4.3.3" xref="S3.E7.m1.8.8.4.3.3.cmml"><munderover id="S3.E7.m1.8.8.4.3.3.4" xref="S3.E7.m1.8.8.4.3.3.4.cmml"><mo movablelimits="false" id="S3.E7.m1.8.8.4.3.3.4.2.2" xref="S3.E7.m1.8.8.4.3.3.4.2.2.cmml">∑</mo><mrow id="S3.E7.m1.8.8.4.3.3.4.2.3" xref="S3.E7.m1.8.8.4.3.3.4.2.3.cmml"><mi id="S3.E7.m1.8.8.4.3.3.4.2.3.2" xref="S3.E7.m1.8.8.4.3.3.4.2.3.2.cmml">j</mi><mo id="S3.E7.m1.8.8.4.3.3.4.2.3.1" xref="S3.E7.m1.8.8.4.3.3.4.2.3.1.cmml">=</mo><mn id="S3.E7.m1.8.8.4.3.3.4.2.3.3" xref="S3.E7.m1.8.8.4.3.3.4.2.3.3.cmml">1</mn></mrow><msub id="S3.E7.m1.8.8.4.3.3.4.3" xref="S3.E7.m1.8.8.4.3.3.4.3.cmml"><mi id="S3.E7.m1.8.8.4.3.3.4.3.2" xref="S3.E7.m1.8.8.4.3.3.4.3.2.cmml">V</mi><mrow id="S3.E7.m1.8.8.4.3.3.4.3.3" xref="S3.E7.m1.8.8.4.3.3.4.3.3.cmml"><mi id="S3.E7.m1.8.8.4.3.3.4.3.3.2" xref="S3.E7.m1.8.8.4.3.3.4.3.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.8.8.4.3.3.4.3.3.1" xref="S3.E7.m1.8.8.4.3.3.4.3.3.1.cmml">​</mo><mi id="S3.E7.m1.8.8.4.3.3.4.3.3.3" xref="S3.E7.m1.8.8.4.3.3.4.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.8.8.4.3.3.4.3.3.1a" xref="S3.E7.m1.8.8.4.3.3.4.3.3.1.cmml">​</mo><mi id="S3.E7.m1.8.8.4.3.3.4.3.3.4" xref="S3.E7.m1.8.8.4.3.3.4.3.3.4.cmml">t</mi></mrow></msub></munderover><mrow id="S3.E7.m1.8.8.4.3.3.3" xref="S3.E7.m1.8.8.4.3.3.3.cmml"><mi id="S3.E7.m1.8.8.4.3.3.3.5" xref="S3.E7.m1.8.8.4.3.3.3.5.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.8.8.4.3.3.3.4" xref="S3.E7.m1.8.8.4.3.3.3.4.cmml">​</mo><mi id="S3.E7.m1.8.8.4.3.3.3.6" xref="S3.E7.m1.8.8.4.3.3.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.8.8.4.3.3.3.4a" xref="S3.E7.m1.8.8.4.3.3.3.4.cmml">​</mo><mi id="S3.E7.m1.8.8.4.3.3.3.7" xref="S3.E7.m1.8.8.4.3.3.3.7.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.8.8.4.3.3.3.4b" xref="S3.E7.m1.8.8.4.3.3.3.4.cmml">​</mo><mi id="S3.E7.m1.8.8.4.3.3.3.8" xref="S3.E7.m1.8.8.4.3.3.3.8.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.8.8.4.3.3.3.4c" xref="S3.E7.m1.8.8.4.3.3.3.4.cmml">​</mo><mi id="S3.E7.m1.8.8.4.3.3.3.9" xref="S3.E7.m1.8.8.4.3.3.3.9.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.8.8.4.3.3.3.4d" xref="S3.E7.m1.8.8.4.3.3.3.4.cmml">​</mo><mi id="S3.E7.m1.8.8.4.3.3.3.10" xref="S3.E7.m1.8.8.4.3.3.3.10.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.8.8.4.3.3.3.4e" xref="S3.E7.m1.8.8.4.3.3.3.4.cmml">​</mo><mi id="S3.E7.m1.8.8.4.3.3.3.11" xref="S3.E7.m1.8.8.4.3.3.3.11.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.8.8.4.3.3.3.4f" xref="S3.E7.m1.8.8.4.3.3.3.4.cmml">​</mo><mrow id="S3.E7.m1.8.8.4.3.3.3.3.3" xref="S3.E7.m1.8.8.4.3.3.3.3.4.cmml"><mo stretchy="false" id="S3.E7.m1.8.8.4.3.3.3.3.3.4" xref="S3.E7.m1.8.8.4.3.3.3.3.4.cmml">(</mo><msub id="S3.E7.m1.6.6.2.1.1.1.1.1.1" xref="S3.E7.m1.6.6.2.1.1.1.1.1.1.cmml"><mi id="S3.E7.m1.6.6.2.1.1.1.1.1.1.2" xref="S3.E7.m1.6.6.2.1.1.1.1.1.1.2.cmml">Q</mi><mi id="S3.E7.m1.6.6.2.1.1.1.1.1.1.3" xref="S3.E7.m1.6.6.2.1.1.1.1.1.1.3.cmml">p</mi></msub><mo id="S3.E7.m1.8.8.4.3.3.3.3.3.5" xref="S3.E7.m1.8.8.4.3.3.3.3.4.cmml">,</mo><mrow id="S3.E7.m1.7.7.3.2.2.2.2.2.2" xref="S3.E7.m1.7.7.3.2.2.2.2.2.2.cmml"><mi id="S3.E7.m1.7.7.3.2.2.2.2.2.2.2" xref="S3.E7.m1.7.7.3.2.2.2.2.2.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.7.7.3.2.2.2.2.2.2.1" xref="S3.E7.m1.7.7.3.2.2.2.2.2.2.1.cmml">​</mo><mrow id="S3.E7.m1.7.7.3.2.2.2.2.2.2.3.2" xref="S3.E7.m1.7.7.3.2.2.2.2.2.2.3.1.cmml"><mo stretchy="false" id="S3.E7.m1.7.7.3.2.2.2.2.2.2.3.2.1" xref="S3.E7.m1.7.7.3.2.2.2.2.2.2.3.1.cmml">(</mo><mi id="S3.E7.m1.2.2" xref="S3.E7.m1.2.2.cmml">p</mi><mo id="S3.E7.m1.7.7.3.2.2.2.2.2.2.3.2.2" xref="S3.E7.m1.7.7.3.2.2.2.2.2.2.3.1.cmml">,</mo><mi id="S3.E7.m1.3.3" xref="S3.E7.m1.3.3.cmml">i</mi><mo id="S3.E7.m1.7.7.3.2.2.2.2.2.2.3.2.3" xref="S3.E7.m1.7.7.3.2.2.2.2.2.2.3.1.cmml">,</mo><mi id="S3.E7.m1.4.4" xref="S3.E7.m1.4.4.cmml">j</mi><mo stretchy="false" id="S3.E7.m1.7.7.3.2.2.2.2.2.2.3.2.4" xref="S3.E7.m1.7.7.3.2.2.2.2.2.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E7.m1.8.8.4.3.3.3.3.3.6" xref="S3.E7.m1.8.8.4.3.3.3.3.4.cmml">,</mo><msub id="S3.E7.m1.8.8.4.3.3.3.3.3.3" xref="S3.E7.m1.8.8.4.3.3.3.3.3.3.cmml"><mi id="S3.E7.m1.8.8.4.3.3.3.3.3.3.2" xref="S3.E7.m1.8.8.4.3.3.3.3.3.3.2.cmml">D</mi><mi id="S3.E7.m1.8.8.4.3.3.3.3.3.3.3" xref="S3.E7.m1.8.8.4.3.3.3.3.3.3.3.cmml">j</mi></msub><mo stretchy="false" id="S3.E7.m1.8.8.4.3.3.3.3.3.7" xref="S3.E7.m1.8.8.4.3.3.3.3.4.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.8b"><apply id="S3.E7.m1.8.8.cmml" xref="S3.E7.m1.8.8"><eq id="S3.E7.m1.8.8.5.cmml" xref="S3.E7.m1.8.8.5"></eq><apply id="S3.E7.m1.5.5.1.cmml" xref="S3.E7.m1.5.5.1"><times id="S3.E7.m1.5.5.1.2.cmml" xref="S3.E7.m1.5.5.1.2"></times><ci id="S3.E7.m1.5.5.1.3.cmml" xref="S3.E7.m1.5.5.1.3">𝐷</ci><ci id="S3.E7.m1.5.5.1.4.cmml" xref="S3.E7.m1.5.5.1.4">𝐶</ci><ci id="S3.E7.m1.5.5.1.5.cmml" xref="S3.E7.m1.5.5.1.5">𝐴</ci><interval closure="open" id="S3.E7.m1.5.5.1.1.2.cmml" xref="S3.E7.m1.5.5.1.1.1"><apply id="S3.E7.m1.5.5.1.1.1.1.cmml" xref="S3.E7.m1.5.5.1.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.5.5.1.1.1.1.1.cmml" xref="S3.E7.m1.5.5.1.1.1.1">subscript</csymbol><ci id="S3.E7.m1.5.5.1.1.1.1.2.cmml" xref="S3.E7.m1.5.5.1.1.1.1.2">𝑄</ci><ci id="S3.E7.m1.5.5.1.1.1.1.3.cmml" xref="S3.E7.m1.5.5.1.1.1.1.3">𝑝</ci></apply><ci id="S3.E7.m1.1.1.cmml" xref="S3.E7.m1.1.1">𝐷</ci></interval></apply><apply id="S3.E7.m1.8.8.4.cmml" xref="S3.E7.m1.8.8.4"><times id="S3.E7.m1.8.8.4.4.cmml" xref="S3.E7.m1.8.8.4.4"></times><apply id="S3.E7.m1.8.8.4.5.cmml" xref="S3.E7.m1.8.8.4.5"><divide id="S3.E7.m1.8.8.4.5.1.cmml" xref="S3.E7.m1.8.8.4.5"></divide><cn type="integer" id="S3.E7.m1.8.8.4.5.2.cmml" xref="S3.E7.m1.8.8.4.5.2">1</cn><apply id="S3.E7.m1.8.8.4.5.3.cmml" xref="S3.E7.m1.8.8.4.5.3"><csymbol cd="ambiguous" id="S3.E7.m1.8.8.4.5.3.1.cmml" xref="S3.E7.m1.8.8.4.5.3">subscript</csymbol><ci id="S3.E7.m1.8.8.4.5.3.2.cmml" xref="S3.E7.m1.8.8.4.5.3.2">𝑉</ci><apply id="S3.E7.m1.8.8.4.5.3.3.cmml" xref="S3.E7.m1.8.8.4.5.3.3"><times id="S3.E7.m1.8.8.4.5.3.3.1.cmml" xref="S3.E7.m1.8.8.4.5.3.3.1"></times><ci id="S3.E7.m1.8.8.4.5.3.3.2.cmml" xref="S3.E7.m1.8.8.4.5.3.3.2">ℎ</ci><ci id="S3.E7.m1.8.8.4.5.3.3.3.cmml" xref="S3.E7.m1.8.8.4.5.3.3.3">𝑖</ci><ci id="S3.E7.m1.8.8.4.5.3.3.4.cmml" xref="S3.E7.m1.8.8.4.5.3.3.4">𝑡</ci></apply></apply></apply><apply id="S3.E7.m1.8.8.4.3.cmml" xref="S3.E7.m1.8.8.4.3"><apply id="S3.E7.m1.8.8.4.3.4.cmml" xref="S3.E7.m1.8.8.4.3.4"><csymbol cd="ambiguous" id="S3.E7.m1.8.8.4.3.4.1.cmml" xref="S3.E7.m1.8.8.4.3.4">superscript</csymbol><apply id="S3.E7.m1.8.8.4.3.4.2.cmml" xref="S3.E7.m1.8.8.4.3.4"><csymbol cd="ambiguous" id="S3.E7.m1.8.8.4.3.4.2.1.cmml" xref="S3.E7.m1.8.8.4.3.4">subscript</csymbol><sum id="S3.E7.m1.8.8.4.3.4.2.2.cmml" xref="S3.E7.m1.8.8.4.3.4.2.2"></sum><apply id="S3.E7.m1.8.8.4.3.4.2.3.cmml" xref="S3.E7.m1.8.8.4.3.4.2.3"><eq id="S3.E7.m1.8.8.4.3.4.2.3.1.cmml" xref="S3.E7.m1.8.8.4.3.4.2.3.1"></eq><ci id="S3.E7.m1.8.8.4.3.4.2.3.2.cmml" xref="S3.E7.m1.8.8.4.3.4.2.3.2">𝑖</ci><cn type="integer" id="S3.E7.m1.8.8.4.3.4.2.3.3.cmml" xref="S3.E7.m1.8.8.4.3.4.2.3.3">1</cn></apply></apply><apply id="S3.E7.m1.8.8.4.3.4.3.cmml" xref="S3.E7.m1.8.8.4.3.4.3"><csymbol cd="ambiguous" id="S3.E7.m1.8.8.4.3.4.3.1.cmml" xref="S3.E7.m1.8.8.4.3.4.3">subscript</csymbol><ci id="S3.E7.m1.8.8.4.3.4.3.2.cmml" xref="S3.E7.m1.8.8.4.3.4.3.2">𝑁</ci><apply id="S3.E7.m1.8.8.4.3.4.3.3.cmml" xref="S3.E7.m1.8.8.4.3.4.3.3"><times id="S3.E7.m1.8.8.4.3.4.3.3.1.cmml" xref="S3.E7.m1.8.8.4.3.4.3.3.1"></times><ci id="S3.E7.m1.8.8.4.3.4.3.3.2.cmml" xref="S3.E7.m1.8.8.4.3.4.3.3.2">𝑟</ci><ci id="S3.E7.m1.8.8.4.3.4.3.3.3.cmml" xref="S3.E7.m1.8.8.4.3.4.3.3.3">𝑒</ci><ci id="S3.E7.m1.8.8.4.3.4.3.3.4.cmml" xref="S3.E7.m1.8.8.4.3.4.3.3.4">𝑓</ci></apply></apply></apply><apply id="S3.E7.m1.8.8.4.3.3.cmml" xref="S3.E7.m1.8.8.4.3.3"><apply id="S3.E7.m1.8.8.4.3.3.4.cmml" xref="S3.E7.m1.8.8.4.3.3.4"><csymbol cd="ambiguous" id="S3.E7.m1.8.8.4.3.3.4.1.cmml" xref="S3.E7.m1.8.8.4.3.3.4">superscript</csymbol><apply id="S3.E7.m1.8.8.4.3.3.4.2.cmml" xref="S3.E7.m1.8.8.4.3.3.4"><csymbol cd="ambiguous" id="S3.E7.m1.8.8.4.3.3.4.2.1.cmml" xref="S3.E7.m1.8.8.4.3.3.4">subscript</csymbol><sum id="S3.E7.m1.8.8.4.3.3.4.2.2.cmml" xref="S3.E7.m1.8.8.4.3.3.4.2.2"></sum><apply id="S3.E7.m1.8.8.4.3.3.4.2.3.cmml" xref="S3.E7.m1.8.8.4.3.3.4.2.3"><eq id="S3.E7.m1.8.8.4.3.3.4.2.3.1.cmml" xref="S3.E7.m1.8.8.4.3.3.4.2.3.1"></eq><ci id="S3.E7.m1.8.8.4.3.3.4.2.3.2.cmml" xref="S3.E7.m1.8.8.4.3.3.4.2.3.2">𝑗</ci><cn type="integer" id="S3.E7.m1.8.8.4.3.3.4.2.3.3.cmml" xref="S3.E7.m1.8.8.4.3.3.4.2.3.3">1</cn></apply></apply><apply id="S3.E7.m1.8.8.4.3.3.4.3.cmml" xref="S3.E7.m1.8.8.4.3.3.4.3"><csymbol cd="ambiguous" id="S3.E7.m1.8.8.4.3.3.4.3.1.cmml" xref="S3.E7.m1.8.8.4.3.3.4.3">subscript</csymbol><ci id="S3.E7.m1.8.8.4.3.3.4.3.2.cmml" xref="S3.E7.m1.8.8.4.3.3.4.3.2">𝑉</ci><apply id="S3.E7.m1.8.8.4.3.3.4.3.3.cmml" xref="S3.E7.m1.8.8.4.3.3.4.3.3"><times id="S3.E7.m1.8.8.4.3.3.4.3.3.1.cmml" xref="S3.E7.m1.8.8.4.3.3.4.3.3.1"></times><ci id="S3.E7.m1.8.8.4.3.3.4.3.3.2.cmml" xref="S3.E7.m1.8.8.4.3.3.4.3.3.2">ℎ</ci><ci id="S3.E7.m1.8.8.4.3.3.4.3.3.3.cmml" xref="S3.E7.m1.8.8.4.3.3.4.3.3.3">𝑖</ci><ci id="S3.E7.m1.8.8.4.3.3.4.3.3.4.cmml" xref="S3.E7.m1.8.8.4.3.3.4.3.3.4">𝑡</ci></apply></apply></apply><apply id="S3.E7.m1.8.8.4.3.3.3.cmml" xref="S3.E7.m1.8.8.4.3.3.3"><times id="S3.E7.m1.8.8.4.3.3.3.4.cmml" xref="S3.E7.m1.8.8.4.3.3.3.4"></times><ci id="S3.E7.m1.8.8.4.3.3.3.5.cmml" xref="S3.E7.m1.8.8.4.3.3.3.5">𝐷</ci><ci id="S3.E7.m1.8.8.4.3.3.3.6.cmml" xref="S3.E7.m1.8.8.4.3.3.3.6">𝑒</ci><ci id="S3.E7.m1.8.8.4.3.3.3.7.cmml" xref="S3.E7.m1.8.8.4.3.3.3.7">𝑓</ci><ci id="S3.E7.m1.8.8.4.3.3.3.8.cmml" xref="S3.E7.m1.8.8.4.3.3.3.8">𝐴</ci><ci id="S3.E7.m1.8.8.4.3.3.3.9.cmml" xref="S3.E7.m1.8.8.4.3.3.3.9">𝑡</ci><ci id="S3.E7.m1.8.8.4.3.3.3.10.cmml" xref="S3.E7.m1.8.8.4.3.3.3.10">𝑡</ci><ci id="S3.E7.m1.8.8.4.3.3.3.11.cmml" xref="S3.E7.m1.8.8.4.3.3.3.11">𝑛</ci><vector id="S3.E7.m1.8.8.4.3.3.3.3.4.cmml" xref="S3.E7.m1.8.8.4.3.3.3.3.3"><apply id="S3.E7.m1.6.6.2.1.1.1.1.1.1.cmml" xref="S3.E7.m1.6.6.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.6.6.2.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.6.6.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E7.m1.6.6.2.1.1.1.1.1.1.2.cmml" xref="S3.E7.m1.6.6.2.1.1.1.1.1.1.2">𝑄</ci><ci id="S3.E7.m1.6.6.2.1.1.1.1.1.1.3.cmml" xref="S3.E7.m1.6.6.2.1.1.1.1.1.1.3">𝑝</ci></apply><apply id="S3.E7.m1.7.7.3.2.2.2.2.2.2.cmml" xref="S3.E7.m1.7.7.3.2.2.2.2.2.2"><times id="S3.E7.m1.7.7.3.2.2.2.2.2.2.1.cmml" xref="S3.E7.m1.7.7.3.2.2.2.2.2.2.1"></times><ci id="S3.E7.m1.7.7.3.2.2.2.2.2.2.2.cmml" xref="S3.E7.m1.7.7.3.2.2.2.2.2.2.2">𝑃</ci><vector id="S3.E7.m1.7.7.3.2.2.2.2.2.2.3.1.cmml" xref="S3.E7.m1.7.7.3.2.2.2.2.2.2.3.2"><ci id="S3.E7.m1.2.2.cmml" xref="S3.E7.m1.2.2">𝑝</ci><ci id="S3.E7.m1.3.3.cmml" xref="S3.E7.m1.3.3">𝑖</ci><ci id="S3.E7.m1.4.4.cmml" xref="S3.E7.m1.4.4">𝑗</ci></vector></apply><apply id="S3.E7.m1.8.8.4.3.3.3.3.3.3.cmml" xref="S3.E7.m1.8.8.4.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E7.m1.8.8.4.3.3.3.3.3.3.1.cmml" xref="S3.E7.m1.8.8.4.3.3.3.3.3.3">subscript</csymbol><ci id="S3.E7.m1.8.8.4.3.3.3.3.3.3.2.cmml" xref="S3.E7.m1.8.8.4.3.3.3.3.3.3.2">𝐷</ci><ci id="S3.E7.m1.8.8.4.3.3.3.3.3.3.3.cmml" xref="S3.E7.m1.8.8.4.3.3.3.3.3.3.3">𝑗</ci></apply></vector></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.8c">DCA(Q_{p},D)={\frac{1}{V_{hit}}}{\sum_{i=1}^{N_{ref}}{\sum_{j=1}^{V_{hit}}DefAttn(Q_{p},P(p,i,j),D_{j})}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.4" class="ltx_p">where <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="D_{j}" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><msub id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">D</mi><mi id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">𝐷</ci><ci id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">D_{j}</annotation></semantics></math> denotes the encoded depth prediction features of the j-th camera, and <math id="S3.SS3.p2.2.m2.3" class="ltx_Math" alttext="P(p,i,j)" display="inline"><semantics id="S3.SS3.p2.2.m2.3a"><mrow id="S3.SS3.p2.2.m2.3.4" xref="S3.SS3.p2.2.m2.3.4.cmml"><mi id="S3.SS3.p2.2.m2.3.4.2" xref="S3.SS3.p2.2.m2.3.4.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.2.m2.3.4.1" xref="S3.SS3.p2.2.m2.3.4.1.cmml">​</mo><mrow id="S3.SS3.p2.2.m2.3.4.3.2" xref="S3.SS3.p2.2.m2.3.4.3.1.cmml"><mo stretchy="false" id="S3.SS3.p2.2.m2.3.4.3.2.1" xref="S3.SS3.p2.2.m2.3.4.3.1.cmml">(</mo><mi id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">p</mi><mo id="S3.SS3.p2.2.m2.3.4.3.2.2" xref="S3.SS3.p2.2.m2.3.4.3.1.cmml">,</mo><mi id="S3.SS3.p2.2.m2.2.2" xref="S3.SS3.p2.2.m2.2.2.cmml">i</mi><mo id="S3.SS3.p2.2.m2.3.4.3.2.3" xref="S3.SS3.p2.2.m2.3.4.3.1.cmml">,</mo><mi id="S3.SS3.p2.2.m2.3.3" xref="S3.SS3.p2.2.m2.3.3.cmml">j</mi><mo stretchy="false" id="S3.SS3.p2.2.m2.3.4.3.2.4" xref="S3.SS3.p2.2.m2.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.3b"><apply id="S3.SS3.p2.2.m2.3.4.cmml" xref="S3.SS3.p2.2.m2.3.4"><times id="S3.SS3.p2.2.m2.3.4.1.cmml" xref="S3.SS3.p2.2.m2.3.4.1"></times><ci id="S3.SS3.p2.2.m2.3.4.2.cmml" xref="S3.SS3.p2.2.m2.3.4.2">𝑃</ci><vector id="S3.SS3.p2.2.m2.3.4.3.1.cmml" xref="S3.SS3.p2.2.m2.3.4.3.2"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">𝑝</ci><ci id="S3.SS3.p2.2.m2.2.2.cmml" xref="S3.SS3.p2.2.m2.2.2">𝑖</ci><ci id="S3.SS3.p2.2.m2.3.3.cmml" xref="S3.SS3.p2.2.m2.3.3">𝑗</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.3c">P(p,i,j)</annotation></semantics></math> represents the projection point of the i-th 3D reference point <math id="S3.SS3.p2.3.m3.3" class="ltx_Math" alttext="(x,y,z_{i})" display="inline"><semantics id="S3.SS3.p2.3.m3.3a"><mrow id="S3.SS3.p2.3.m3.3.3.1" xref="S3.SS3.p2.3.m3.3.3.2.cmml"><mo stretchy="false" id="S3.SS3.p2.3.m3.3.3.1.2" xref="S3.SS3.p2.3.m3.3.3.2.cmml">(</mo><mi id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">x</mi><mo id="S3.SS3.p2.3.m3.3.3.1.3" xref="S3.SS3.p2.3.m3.3.3.2.cmml">,</mo><mi id="S3.SS3.p2.3.m3.2.2" xref="S3.SS3.p2.3.m3.2.2.cmml">y</mi><mo id="S3.SS3.p2.3.m3.3.3.1.4" xref="S3.SS3.p2.3.m3.3.3.2.cmml">,</mo><msub id="S3.SS3.p2.3.m3.3.3.1.1" xref="S3.SS3.p2.3.m3.3.3.1.1.cmml"><mi id="S3.SS3.p2.3.m3.3.3.1.1.2" xref="S3.SS3.p2.3.m3.3.3.1.1.2.cmml">z</mi><mi id="S3.SS3.p2.3.m3.3.3.1.1.3" xref="S3.SS3.p2.3.m3.3.3.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS3.p2.3.m3.3.3.1.5" xref="S3.SS3.p2.3.m3.3.3.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.3b"><vector id="S3.SS3.p2.3.m3.3.3.2.cmml" xref="S3.SS3.p2.3.m3.3.3.1"><ci id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">𝑥</ci><ci id="S3.SS3.p2.3.m3.2.2.cmml" xref="S3.SS3.p2.3.m3.2.2">𝑦</ci><apply id="S3.SS3.p2.3.m3.3.3.1.1.cmml" xref="S3.SS3.p2.3.m3.3.3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.3.3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.3.3.1.1">subscript</csymbol><ci id="S3.SS3.p2.3.m3.3.3.1.1.2.cmml" xref="S3.SS3.p2.3.m3.3.3.1.1.2">𝑧</ci><ci id="S3.SS3.p2.3.m3.3.3.1.1.3.cmml" xref="S3.SS3.p2.3.m3.3.3.1.1.3">𝑖</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.3c">(x,y,z_{i})</annotation></semantics></math> of query <math id="S3.SS3.p2.4.m4.1" class="ltx_Math" alttext="Q_{p}" display="inline"><semantics id="S3.SS3.p2.4.m4.1a"><msub id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml"><mi id="S3.SS3.p2.4.m4.1.1.2" xref="S3.SS3.p2.4.m4.1.1.2.cmml">Q</mi><mi id="S3.SS3.p2.4.m4.1.1.3" xref="S3.SS3.p2.4.m4.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><apply id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.p2.4.m4.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.2">𝑄</ci><ci id="S3.SS3.p2.4.m4.1.1.3.cmml" xref="S3.SS3.p2.4.m4.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">Q_{p}</annotation></semantics></math> onto the frustum coordinate system of the j-th camera.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">This section presents the performance of our proposed FusionFormer on the task of 3D object detection, along with several ablation studies that analyze the benefits of each module in our framework.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setups</h3>

<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Datasets and metrics.</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">We conducted experiments on the nuScenes dataset <cite class="ltx_cite ltx_citemacro_citep">(Caesar et al., <a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite> to evaluate the performance of our proposed method for 3D object detection in autonomous driving. The nuScenes dataset consists of 1.4 million 3D detection boxes from 10 different categories, with each frame of data containing 6 surround-view camera images and LiDAR point cloud data. We employ the nuScenes detection metrics NDS and mAP as evaluation metrics for our experiments.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Implementation details.</h4>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS0.Px2.p1.3" class="ltx_p">We conducted algorithmic experiments using the open-source project MMDetection3D <cite class="ltx_cite ltx_citemacro_citep">(Contributors, <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite> based on PyTorch. Specifically, we selected VoVNet-99 <cite class="ltx_cite ltx_citemacro_citep">(Lee &amp; Park, <a href="#bib.bib11" title="" class="ltx_ref">2020</a>)</cite> as the backbone for the image branch, generating multi-scale image features through FPN <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib17" title="" class="ltx_ref">2017</a>)</cite>. The input image size was set to <math id="S4.SS1.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="1600{\times}640" display="inline"><semantics id="S4.SS1.SSS0.Px2.p1.1.m1.1a"><mrow id="S4.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml"><mn id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml">1600</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.1" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml">640</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.1.m1.1b"><apply id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1"><times id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.2">1600</cn><cn type="integer" id="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS0.Px2.p1.1.m1.1.1.3">640</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.1.m1.1c">1600{\times}640</annotation></semantics></math>. For the LiDAR point cloud branch, VoxelNet <cite class="ltx_cite ltx_citemacro_citep">(Zhou &amp; Tuzel, <a href="#bib.bib42" title="" class="ltx_ref">2018</a>)</cite> was used as the backbone. The input LiDAR point cloud was voxelized with a size of <math id="S4.SS1.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="0.075m" display="inline"><semantics id="S4.SS1.SSS0.Px2.p1.2.m2.1a"><mrow id="S4.SS1.SSS0.Px2.p1.2.m2.1.1" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.cmml"><mn id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml">0.075</mn><mo lspace="0em" rspace="0em" id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.1" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.3" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.2.m2.1b"><apply id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1"><times id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.1"></times><cn type="float" id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.2">0.075</cn><ci id="S4.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S4.SS1.SSS0.Px2.p1.2.m2.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.2.m2.1c">0.075m</annotation></semantics></math>. The size of the BEV queries was set to <math id="S4.SS1.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="200{\times}200" display="inline"><semantics id="S4.SS1.SSS0.Px2.p1.3.m3.1a"><mrow id="S4.SS1.SSS0.Px2.p1.3.m3.1.1" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.cmml"><mn id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.2" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.2.cmml">200</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.1" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.1.cmml">×</mo><mn id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.cmml">200</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p1.3.m3.1b"><apply id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1"><times id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.1"></times><cn type="integer" id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.2">200</cn><cn type="integer" id="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3.cmml" xref="S4.SS1.SSS0.Px2.p1.3.m3.1.1.3">200</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p1.3.m3.1c">200{\times}200</annotation></semantics></math>. During the training process, we loaded the pre-trained weights of the image branch backbone on Fcos3D <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib34" title="" class="ltx_ref">2021b</a>)</cite>. The point cloud branch did not require pre-trained weights and was directly trained end-to-end with the model. We present a 3D detection head based on Deformable DETR <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a href="#bib.bib44" title="" class="ltx_ref">2020</a>)</cite> that outputs 3D detection boxes and velocity predictions directly from BEV features without the need for non-maximum suppressing. To address the unstable matching problem encounterined in DETR-like detection heads and accelerate training convergence, we applied the query denoising strategy <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib12" title="" class="ltx_ref">2022a</a>)</cite> during the training process. The model was trained for 24 epochs with the class-balanced grouping and sampling (CBGS) strategy <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a href="#bib.bib43" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance comparison on the nuScenes test set. ”L” is LiDAR. ”C” is camera. ”T” is temporal. The results are evaluated using a single model without any test-time-augmentation or ensembling techniques.</figcaption>
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.1.1.1.1.1" class="ltx_text ltx_inline-block" style="width:35.8pt;">Methods</span></th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.1.1.1.2.1" class="ltx_text ltx_inline-block" style="width:27.8pt;">Modality</span></th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.1.1.1.3.1" class="ltx_text ltx_inline-block" style="width:19.9pt;">NDS↑</span></th>
<th id="S4.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.1.1.1.4.1" class="ltx_text ltx_inline-block" style="width:15.9pt;">mAP↑</span></th>
<th id="S4.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.1.1.1.5.1" class="ltx_text ltx_inline-block" style="width:19.9pt;">mATE↓</span></th>
<th id="S4.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.1.1.1.6.1" class="ltx_text ltx_inline-block" style="width:23.8pt;">mASE↓</span></th>
<th id="S4.T1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.1.1.1.7.1" class="ltx_text ltx_inline-block" style="width:23.8pt;">mAOE↓</span></th>
<th id="S4.T1.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.1.1.1.8.1" class="ltx_text ltx_inline-block" style="width:23.8pt;">mAVE↓</span></th>
<th id="S4.T1.1.1.1.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.1.1.1.9.1" class="ltx_text ltx_inline-block" style="width:23.8pt;">mAAE↓</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<td id="S4.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">PointPainting(<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib29" title="" class="ltx_ref">Vora et al.</a></cite>)</td>
<td id="S4.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CL</td>
<td id="S4.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">61.0</td>
<td id="S4.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">54.1</td>
<td id="S4.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">38.0</td>
<td id="S4.T1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">26.0</td>
<td id="S4.T1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">54.1</td>
<td id="S4.T1.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t">29.3</td>
<td id="S4.T1.1.2.1.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">13.1</td>
</tr>
<tr id="S4.T1.1.3.2" class="ltx_tr">
<td id="S4.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_r">PointAugmenting(<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib30" title="" class="ltx_ref">Wang et al.</a></cite>)</td>
<td id="S4.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">CL</td>
<td id="S4.T1.1.3.2.3" class="ltx_td ltx_align_center">71.1</td>
<td id="S4.T1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">66.8</td>
<td id="S4.T1.1.3.2.5" class="ltx_td ltx_align_center">25.3</td>
<td id="S4.T1.1.3.2.6" class="ltx_td ltx_align_center"><span id="S4.T1.1.3.2.6.1" class="ltx_text ltx_font_bold">23.5</span></td>
<td id="S4.T1.1.3.2.7" class="ltx_td ltx_align_center">35.4</td>
<td id="S4.T1.1.3.2.8" class="ltx_td ltx_align_center">26.6</td>
<td id="S4.T1.1.3.2.9" class="ltx_td ltx_nopad_r ltx_align_center">12.3</td>
</tr>
<tr id="S4.T1.1.4.3" class="ltx_tr">
<td id="S4.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_border_r">MVP(<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib4" title="" class="ltx_ref">Chen et al.</a></cite>)</td>
<td id="S4.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">CL</td>
<td id="S4.T1.1.4.3.3" class="ltx_td ltx_align_center">70.5</td>
<td id="S4.T1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">66.4</td>
<td id="S4.T1.1.4.3.5" class="ltx_td ltx_align_center">26.3</td>
<td id="S4.T1.1.4.3.6" class="ltx_td ltx_align_center">23.8</td>
<td id="S4.T1.1.4.3.7" class="ltx_td ltx_align_center">32.1</td>
<td id="S4.T1.1.4.3.8" class="ltx_td ltx_align_center">31.3</td>
<td id="S4.T1.1.4.3.9" class="ltx_td ltx_nopad_r ltx_align_center">13.4</td>
</tr>
<tr id="S4.T1.1.5.4" class="ltx_tr">
<td id="S4.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_border_r">FusionPainting(<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib37" title="" class="ltx_ref">Xu et al.</a></cite>)</td>
<td id="S4.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">CL</td>
<td id="S4.T1.1.5.4.3" class="ltx_td ltx_align_center">71.6</td>
<td id="S4.T1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r">68.1</td>
<td id="S4.T1.1.5.4.5" class="ltx_td ltx_align_center">25.6</td>
<td id="S4.T1.1.5.4.6" class="ltx_td ltx_align_center">23.6</td>
<td id="S4.T1.1.5.4.7" class="ltx_td ltx_align_center">34.6</td>
<td id="S4.T1.1.5.4.8" class="ltx_td ltx_align_center">27.4</td>
<td id="S4.T1.1.5.4.9" class="ltx_td ltx_nopad_r ltx_align_center">13.2</td>
</tr>
<tr id="S4.T1.1.6.5" class="ltx_tr">
<td id="S4.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_border_r">TransFusion(<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib1" title="" class="ltx_ref">Bai et al.</a></cite>)</td>
<td id="S4.T1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r">CL</td>
<td id="S4.T1.1.6.5.3" class="ltx_td ltx_align_center">71.7</td>
<td id="S4.T1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r">68.9</td>
<td id="S4.T1.1.6.5.5" class="ltx_td ltx_align_center">25.9</td>
<td id="S4.T1.1.6.5.6" class="ltx_td ltx_align_center">24.3</td>
<td id="S4.T1.1.6.5.7" class="ltx_td ltx_align_center">35.9</td>
<td id="S4.T1.1.6.5.8" class="ltx_td ltx_align_center">28.8</td>
<td id="S4.T1.1.6.5.9" class="ltx_td ltx_nopad_r ltx_align_center">12.7</td>
</tr>
<tr id="S4.T1.1.7.6" class="ltx_tr">
<td id="S4.T1.1.7.6.1" class="ltx_td ltx_align_left ltx_border_r">BEVFusion(<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib21" title="" class="ltx_ref">Liu et al.</a></cite>)</td>
<td id="S4.T1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r">CL</td>
<td id="S4.T1.1.7.6.3" class="ltx_td ltx_align_center">72.9</td>
<td id="S4.T1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r">70.2</td>
<td id="S4.T1.1.7.6.5" class="ltx_td ltx_align_center">26.1</td>
<td id="S4.T1.1.7.6.6" class="ltx_td ltx_align_center">23.9</td>
<td id="S4.T1.1.7.6.7" class="ltx_td ltx_align_center">32.9</td>
<td id="S4.T1.1.7.6.8" class="ltx_td ltx_align_center">26.0</td>
<td id="S4.T1.1.7.6.9" class="ltx_td ltx_nopad_r ltx_align_center">13.4</td>
</tr>
<tr id="S4.T1.1.8.7" class="ltx_tr">
<td id="S4.T1.1.8.7.1" class="ltx_td ltx_align_left ltx_border_r">BEVFusion(<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib16" title="" class="ltx_ref">Liang et al.</a></cite>)</td>
<td id="S4.T1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r">CL</td>
<td id="S4.T1.1.8.7.3" class="ltx_td ltx_align_center">73.3</td>
<td id="S4.T1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_r">71.3</td>
<td id="S4.T1.1.8.7.5" class="ltx_td ltx_align_center"><span id="S4.T1.1.8.7.5.1" class="ltx_text ltx_font_bold">25.0</span></td>
<td id="S4.T1.1.8.7.6" class="ltx_td ltx_align_center">24.0</td>
<td id="S4.T1.1.8.7.7" class="ltx_td ltx_align_center">35.9</td>
<td id="S4.T1.1.8.7.8" class="ltx_td ltx_align_center">25.4</td>
<td id="S4.T1.1.8.7.9" class="ltx_td ltx_nopad_r ltx_align_center">13.2</td>
</tr>
<tr id="S4.T1.1.9.8" class="ltx_tr">
<td id="S4.T1.1.9.8.1" class="ltx_td ltx_align_left ltx_border_r">UVTR(<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib13" title="" class="ltx_ref">Li et al.</a></cite>)</td>
<td id="S4.T1.1.9.8.2" class="ltx_td ltx_align_center ltx_border_r">CL</td>
<td id="S4.T1.1.9.8.3" class="ltx_td ltx_align_center">71.1</td>
<td id="S4.T1.1.9.8.4" class="ltx_td ltx_align_center ltx_border_r">67.1</td>
<td id="S4.T1.1.9.8.5" class="ltx_td ltx_align_center">30.6</td>
<td id="S4.T1.1.9.8.6" class="ltx_td ltx_align_center">24.5</td>
<td id="S4.T1.1.9.8.7" class="ltx_td ltx_align_center">35.1</td>
<td id="S4.T1.1.9.8.8" class="ltx_td ltx_align_center"><span id="S4.T1.1.9.8.8.1" class="ltx_text ltx_font_bold">22.5</span></td>
<td id="S4.T1.1.9.8.9" class="ltx_td ltx_nopad_r ltx_align_center">12.4</td>
</tr>
<tr id="S4.T1.1.10.9" class="ltx_tr">
<td id="S4.T1.1.10.9.1" class="ltx_td ltx_align_left ltx_border_r">CMT(<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib38" title="" class="ltx_ref">Yan et al.</a></cite>)</td>
<td id="S4.T1.1.10.9.2" class="ltx_td ltx_align_center ltx_border_r">CL</td>
<td id="S4.T1.1.10.9.3" class="ltx_td ltx_align_center">74.1</td>
<td id="S4.T1.1.10.9.4" class="ltx_td ltx_align_center ltx_border_r">72.0</td>
<td id="S4.T1.1.10.9.5" class="ltx_td ltx_align_center">27.9</td>
<td id="S4.T1.1.10.9.6" class="ltx_td ltx_align_center"><span id="S4.T1.1.10.9.6.1" class="ltx_text ltx_font_bold">23.5</span></td>
<td id="S4.T1.1.10.9.7" class="ltx_td ltx_align_center">30.8</td>
<td id="S4.T1.1.10.9.8" class="ltx_td ltx_align_center">25.9</td>
<td id="S4.T1.1.10.9.9" class="ltx_td ltx_nopad_r ltx_align_center">11.2</td>
</tr>
<tr id="S4.T1.1.11.10" class="ltx_tr">
<td id="S4.T1.1.11.10.1" class="ltx_td ltx_align_left ltx_border_r">DeepInteraction(<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib39" title="" class="ltx_ref">Yang et al.</a></cite>)</td>
<td id="S4.T1.1.11.10.2" class="ltx_td ltx_align_center ltx_border_r">CL</td>
<td id="S4.T1.1.11.10.3" class="ltx_td ltx_align_center">73.4</td>
<td id="S4.T1.1.11.10.4" class="ltx_td ltx_align_center ltx_border_r">70.8</td>
<td id="S4.T1.1.11.10.5" class="ltx_td ltx_align_center">25.7</td>
<td id="S4.T1.1.11.10.6" class="ltx_td ltx_align_center">24.0</td>
<td id="S4.T1.1.11.10.7" class="ltx_td ltx_align_center">32.5</td>
<td id="S4.T1.1.11.10.8" class="ltx_td ltx_align_center">24.5</td>
<td id="S4.T1.1.11.10.9" class="ltx_td ltx_nopad_r ltx_align_center">12.8</td>
</tr>
<tr id="S4.T1.1.12.11" class="ltx_tr">
<td id="S4.T1.1.12.11.1" class="ltx_td ltx_align_left ltx_border_r">BEVFusion4D(<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib3" title="" class="ltx_ref">Cai et al.</a></cite>)</td>
<td id="S4.T1.1.12.11.2" class="ltx_td ltx_align_center ltx_border_r">CLT</td>
<td id="S4.T1.1.12.11.3" class="ltx_td ltx_align_center">74.7</td>
<td id="S4.T1.1.12.11.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.1.12.11.4.1" class="ltx_text ltx_font_bold">73.3</span></td>
<td id="S4.T1.1.12.11.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.1.12.11.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.1.12.11.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.1.12.11.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.1.12.11.9" class="ltx_td ltx_nopad_r ltx_align_center">-</td>
</tr>
<tr id="S4.T1.1.13.12" class="ltx_tr">
<td id="S4.T1.1.13.12.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">FusionFormer</td>
<td id="S4.T1.1.13.12.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">CLT</td>
<td id="S4.T1.1.13.12.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.1.13.12.3.1" class="ltx_text ltx_font_bold">75.1</span></td>
<td id="S4.T1.1.13.12.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">72.6</td>
<td id="S4.T1.1.13.12.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">26.7</td>
<td id="S4.T1.1.13.12.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">23.6</td>
<td id="S4.T1.1.13.12.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.1.13.12.7.1" class="ltx_text ltx_font_bold">28.6</span></td>
<td id="S4.T1.1.13.12.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.1.13.12.8.1" class="ltx_text ltx_font_bold">22.5</span></td>
<td id="S4.T1.1.13.12.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.1.13.12.9.1" class="ltx_text ltx_font_bold">10.5</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance comparison on the nuScenes val set. ”L” is LiDAR. ”C” is camera. ”T” is temporal. The ”-S” indicates that the model only utilizes single-frame BEV features without incorporating temporal fusion techniques. The results are evaluated using a single model without any test-time-augmentation or ensembling techniques.</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Methods</th>
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Image Backbone</td>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LiDAR Backbone</td>
<td id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Modality</td>
<td id="S4.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">mAP↑</td>
<td id="S4.T2.1.1.1.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">NDS↑</td>
</tr>
<tr id="S4.T2.1.2.2" class="ltx_tr">
<th id="S4.T2.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">TransFusion(<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib1" title="" class="ltx_ref">Bai et al.</a></cite>)</th>
<td id="S4.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">DLA34</td>
<td id="S4.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">voxel0075</td>
<td id="S4.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CL</td>
<td id="S4.T2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">67.5</td>
<td id="S4.T2.1.2.2.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">71.3</td>
</tr>
<tr id="S4.T2.1.3.3" class="ltx_tr">
<th id="S4.T2.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">BEVFusion(<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib21" title="" class="ltx_ref">Liu et al.</a></cite>)</th>
<td id="S4.T2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r">Swin-T</td>
<td id="S4.T2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r">voxel0075</td>
<td id="S4.T2.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r">CL</td>
<td id="S4.T2.1.3.3.5" class="ltx_td ltx_align_center">68.5</td>
<td id="S4.T2.1.3.3.6" class="ltx_td ltx_nopad_r ltx_align_center">71.4</td>
</tr>
<tr id="S4.T2.1.4.4" class="ltx_tr">
<th id="S4.T2.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">BEVFusion(<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib16" title="" class="ltx_ref">Liang et al.</a></cite>)</th>
<td id="S4.T2.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r">Swin-T</td>
<td id="S4.T2.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r">voxel0075</td>
<td id="S4.T2.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r">CL</td>
<td id="S4.T2.1.4.4.5" class="ltx_td ltx_align_center">67.9</td>
<td id="S4.T2.1.4.4.6" class="ltx_td ltx_nopad_r ltx_align_center">71.0</td>
</tr>
<tr id="S4.T2.1.5.5" class="ltx_tr">
<th id="S4.T2.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">UVTR(<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib13" title="" class="ltx_ref">Li et al.</a></cite>)</th>
<td id="S4.T2.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r">R101</td>
<td id="S4.T2.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r">voxel0075</td>
<td id="S4.T2.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r">CL</td>
<td id="S4.T2.1.5.5.5" class="ltx_td ltx_align_center">65.4</td>
<td id="S4.T2.1.5.5.6" class="ltx_td ltx_nopad_r ltx_align_center">70.2</td>
</tr>
<tr id="S4.T2.1.6.6" class="ltx_tr">
<th id="S4.T2.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">CMT(<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib38" title="" class="ltx_ref">Yan et al.</a></cite>)</th>
<td id="S4.T2.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r">VoV-99</td>
<td id="S4.T2.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r">voxel0075</td>
<td id="S4.T2.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r">CL</td>
<td id="S4.T2.1.6.6.5" class="ltx_td ltx_align_center">70.3</td>
<td id="S4.T2.1.6.6.6" class="ltx_td ltx_nopad_r ltx_align_center">72.9</td>
</tr>
<tr id="S4.T2.1.7.7" class="ltx_tr">
<th id="S4.T2.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">DeepInteraction(<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib39" title="" class="ltx_ref">Yang et al.</a></cite>)</th>
<td id="S4.T2.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r">R50</td>
<td id="S4.T2.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r">voxel0075</td>
<td id="S4.T2.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r">CL</td>
<td id="S4.T2.1.7.7.5" class="ltx_td ltx_align_center">69.9</td>
<td id="S4.T2.1.7.7.6" class="ltx_td ltx_nopad_r ltx_align_center">72.6</td>
</tr>
<tr id="S4.T2.1.8.8" class="ltx_tr">
<th id="S4.T2.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">BEVFusion4D-S(<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib3" title="" class="ltx_ref">Cai et al.</a></cite>)</th>
<td id="S4.T2.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r">Swin-T</td>
<td id="S4.T2.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r">voxel0075</td>
<td id="S4.T2.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r">CL</td>
<td id="S4.T2.1.8.8.5" class="ltx_td ltx_align_center">70.9</td>
<td id="S4.T2.1.8.8.6" class="ltx_td ltx_nopad_r ltx_align_center">72.9</td>
</tr>
<tr id="S4.T2.1.9.9" class="ltx_tr">
<th id="S4.T2.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">BEVFusion4D(<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib3" title="" class="ltx_ref">Cai et al.</a></cite>)</th>
<td id="S4.T2.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r">Swin-T</td>
<td id="S4.T2.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r">voxel0075</td>
<td id="S4.T2.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r">CLT</td>
<td id="S4.T2.1.9.9.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.9.9.5.1" class="ltx_text ltx_font_bold">72.0</span></td>
<td id="S4.T2.1.9.9.6" class="ltx_td ltx_nopad_r ltx_align_center">73.5</td>
</tr>
<tr id="S4.T2.1.10.10" class="ltx_tr">
<th id="S4.T2.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">FusionFormer-S</th>
<td id="S4.T2.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">VoV-99</td>
<td id="S4.T2.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">voxel0075</td>
<td id="S4.T2.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CL</td>
<td id="S4.T2.1.10.10.5" class="ltx_td ltx_align_center ltx_border_t">70.0</td>
<td id="S4.T2.1.10.10.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">73.2</td>
</tr>
<tr id="S4.T2.1.11.11" class="ltx_tr">
<th id="S4.T2.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r">FusionFormer</th>
<td id="S4.T2.1.11.11.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">VoV-99</td>
<td id="S4.T2.1.11.11.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">voxel0075</td>
<td id="S4.T2.1.11.11.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">CLT</td>
<td id="S4.T2.1.11.11.5" class="ltx_td ltx_align_center ltx_border_b">71.4</td>
<td id="S4.T2.1.11.11.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_b"><span id="S4.T2.1.11.11.6.1" class="ltx_text ltx_font_bold">74.1</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Comparison with State-of-the-Art Methods</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.4" class="ltx_p">As shown in Table <a href="#S4.T1" title="Table 1 ‣ Implementation details. ‣ 4.1 Experimental Setups ‣ 4 Experiments ‣ FusionFormer: A Multi-sensory Fusion in Bird’s-Eye-View and Temporal Consistent Transformer for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, FusionFormer achieves <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="75.1\%" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">75.1</mn><mo id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">75.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">75.1\%</annotation></semantics></math> NDS and <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="72.6\%" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mn id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">72.6</mn><mo id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">72.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">72.6\%</annotation></semantics></math> mAP on the nuScenes test dataset for 3D object detection, outperforming state-of-the-art methods. We used a single model fused with 8 frames of historical BEV features without any test-time-augmentation or ensembling techniques. We also compared the performance of FusionFormer with other methods on the nuScenes val dataset as shown in Table <a href="#S4.T2" title="Table 2 ‣ Implementation details. ‣ 4.1 Experimental Setups ‣ 4 Experiments ‣ FusionFormer: A Multi-sensory Fusion in Bird’s-Eye-View and Temporal Consistent Transformer for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Our proposed FusionFormer achieves state-of-the-art performance on both single-frame and temporal fusion scenarios with NDS scores of <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="73.2\%" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mrow id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mn id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml">73.2</mn><mo id="S4.SS2.p1.3.m3.1.1.1" xref="S4.SS2.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><csymbol cd="latexml" id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2">73.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">73.2\%</annotation></semantics></math> and <math id="S4.SS2.p1.4.m4.1" class="ltx_Math" alttext="74.1\%" display="inline"><semantics id="S4.SS2.p1.4.m4.1a"><mrow id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml"><mn id="S4.SS2.p1.4.m4.1.1.2" xref="S4.SS2.p1.4.m4.1.1.2.cmml">74.1</mn><mo id="S4.SS2.p1.4.m4.1.1.1" xref="S4.SS2.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><apply id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1"><csymbol cd="latexml" id="S4.SS2.p1.4.m4.1.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p1.4.m4.1.1.2.cmml" xref="S4.SS2.p1.4.m4.1.1.2">74.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">74.1\%</annotation></semantics></math>. Several detection results on the nuScenes test set of FusionFormer are shown in Figure <a href="#S4.F5" title="Figure 5 ‣ 4.2 Comparison with State-of-the-Art Methods ‣ 4 Experiments ‣ FusionFormer: A Multi-sensory Fusion in Bird’s-Eye-View and Temporal Consistent Transformer for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2309.05257/assets/fusionformer_qualitative_result.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="548" height="150" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span id="S4.F5.6.1" class="ltx_text ltx_font_bold">Qualitative detection results in the nuScenes test set.</span> Bounding boxes with different colors represent Cars(<span id="S4.F5.7.2" class="ltx_text" style="color:#FF9E00;">•</span>), Pedestrians(<span id="S4.F5.8.3" class="ltx_text" style="color:#0000E6;">•</span>), Bus(<span id="S4.F5.9.4" class="ltx_text" style="color:#FF7F50;">•</span>) and Truck(<span id="S4.F5.10.5" class="ltx_text" style="color:#FF6347;">•</span>).</figcaption>
</figure>
<div id="S4.SS2.2" class="ltx_logical-block ltx_minipage ltx_align_middle" style="width:397.5pt;">
<figure id="S4.SS2.1.fig1" class="ltx_figure ltx_minipage ltx_align_middle" style="width:178.9pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 3: </span>Results of camera based 3D detection fused with depth prediction.</figcaption>
<table id="S4.SS2.1.fig1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.SS2.1.fig1.1.1.1" class="ltx_tr">
<th id="S4.SS2.1.fig1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Method</th>
<th id="S4.SS2.1.fig1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAP↑</th>
<th id="S4.SS2.1.fig1.1.1.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t">NDS↑</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.SS2.1.fig1.1.2.1" class="ltx_tr">
<th id="S4.SS2.1.fig1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">BEVFormer(<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib15" title="" class="ltx_ref">Li et al.</a></cite>)</th>
<td id="S4.SS2.1.fig1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">41.6</td>
<td id="S4.SS2.1.fig1.1.2.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">51.7</td>
</tr>
<tr id="S4.SS2.1.fig1.1.3.2" class="ltx_tr">
<th id="S4.SS2.1.fig1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">FusionFormer-Depth</th>
<td id="S4.SS2.1.fig1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.SS2.1.fig1.1.3.2.2.1" class="ltx_text ltx_font_bold">43.9</span></td>
<td id="S4.SS2.1.fig1.1.3.2.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_b"><span id="S4.SS2.1.fig1.1.3.2.3.1" class="ltx_text ltx_font_bold">53.3</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.SS2.2.fig2" class="ltx_figure ltx_minipage ltx_align_middle" style="width:198.7pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 4: </span>Robustness performance on the nuScenes val set. ”L” is LiDAR. ”C” is camera.</figcaption>
<table id="S4.SS2.2.fig2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.SS2.2.fig2.1.1.1" class="ltx_tr">
<th id="S4.SS2.2.fig2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Method</th>
<th id="S4.SS2.2.fig2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Modality</th>
<th id="S4.SS2.2.fig2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAP↑</th>
<th id="S4.SS2.2.fig2.1.1.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t">NDS↑</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.SS2.2.fig2.1.2.1" class="ltx_tr">
<th id="S4.SS2.2.fig2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">FusionFormer</th>
<td id="S4.SS2.2.fig2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">C</td>
<td id="S4.SS2.2.fig2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">34.3</td>
<td id="S4.SS2.2.fig2.1.2.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">45.5</td>
</tr>
<tr id="S4.SS2.2.fig2.1.3.2" class="ltx_tr">
<th id="S4.SS2.2.fig2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">FusionFormer</th>
<td id="S4.SS2.2.fig2.1.3.2.2" class="ltx_td ltx_align_center">L</td>
<td id="S4.SS2.2.fig2.1.3.2.3" class="ltx_td ltx_align_center">62.5</td>
<td id="S4.SS2.2.fig2.1.3.2.4" class="ltx_td ltx_nopad_r ltx_align_center">68.6</td>
</tr>
<tr id="S4.SS2.2.fig2.1.4.3" class="ltx_tr">
<th id="S4.SS2.2.fig2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">FusionFormer</th>
<td id="S4.SS2.2.fig2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b">CL</td>
<td id="S4.SS2.2.fig2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b">71.4</td>
<td id="S4.SS2.2.fig2.1.4.3.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_b">74.1</td>
</tr>
</tbody>
</table>
</figure>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Camera Based 3D detection Fused with Depth Prediction</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.4" class="ltx_p">As shown in Table <a href="#S4.SS2" title="4.2 Comparison with State-of-the-Art Methods ‣ 4 Experiments ‣ FusionFormer: A Multi-sensory Fusion in Bird’s-Eye-View and Temporal Consistent Transformer for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>, FusionFormer achieves <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="53.3\%" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mn id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">53.3</mn><mo id="S4.SS3.p1.1.m1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">53.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">53.3\%</annotation></semantics></math> NDS and <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="43.9\%" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mrow id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><mn id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2.cmml">43.9</mn><mo id="S4.SS3.p1.2.m2.1.1.1" xref="S4.SS3.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.p1.2.m2.1.1.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2">43.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">43.9\%</annotation></semantics></math> mAP on the nuScenes val dataset with only camera images input by fused with the depth prediction results. Compared with the baseline BEVFormer, the NDS and mAP increased by <math id="S4.SS3.p1.3.m3.1" class="ltx_Math" alttext="1.6\%" display="inline"><semantics id="S4.SS3.p1.3.m3.1a"><mrow id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml"><mn id="S4.SS3.p1.3.m3.1.1.2" xref="S4.SS3.p1.3.m3.1.1.2.cmml">1.6</mn><mo id="S4.SS3.p1.3.m3.1.1.1" xref="S4.SS3.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><apply id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1"><csymbol cd="latexml" id="S4.SS3.p1.3.m3.1.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.p1.3.m3.1.1.2.cmml" xref="S4.SS3.p1.3.m3.1.1.2">1.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">1.6\%</annotation></semantics></math> and <math id="S4.SS3.p1.4.m4.1" class="ltx_Math" alttext="2.3\%" display="inline"><semantics id="S4.SS3.p1.4.m4.1a"><mrow id="S4.SS3.p1.4.m4.1.1" xref="S4.SS3.p1.4.m4.1.1.cmml"><mn id="S4.SS3.p1.4.m4.1.1.2" xref="S4.SS3.p1.4.m4.1.1.2.cmml">2.3</mn><mo id="S4.SS3.p1.4.m4.1.1.1" xref="S4.SS3.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.4.m4.1b"><apply id="S4.SS3.p1.4.m4.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1"><csymbol cd="latexml" id="S4.SS3.p1.4.m4.1.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.p1.4.m4.1.1.2.cmml" xref="S4.SS3.p1.4.m4.1.1.2">2.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.4.m4.1c">2.3\%</annotation></semantics></math> respectively. In particular, we found that after introducing the depth prediction branch, the BEV features output by the encoder can converge better. This may be because the depth information carried by the depth prediction branch allows the model to focus more accurately on the target location. As shown in Figure <a href="#S4.F6" title="Figure 6 ‣ 4.4 Robustness Study ‣ 4 Experiments ‣ FusionFormer: A Multi-sensory Fusion in Bird’s-Eye-View and Temporal Consistent Transformer for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> (a), compared to BEVFormer, the BEV features obtained through FusionFormer-Depth are noticeably more focused on the target location.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Robustness Study</h3>

<div id="S4.SS4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.p1.1" class="ltx_p">During the training process, we incorporated modality mask <cite class="ltx_cite ltx_citemacro_citep">(Yan et al., <a href="#bib.bib38" title="" class="ltx_ref">2023</a>; Yu et al., <a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite> to enhance the model’s robustness to missing modality data. As demonstrated in Table <a href="#S4.SS2" title="4.2 Comparison with State-of-the-Art Methods ‣ 4 Experiments ‣ FusionFormer: A Multi-sensory Fusion in Bird’s-Eye-View and Temporal Consistent Transformer for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>, our model can produce desirable results even in scenarios where image or point cloud data is missing, showcasing its strong robustness. These findings highlight the potential of our approach for addressing challenges in multi-modal learning and its potential for practical real-world applications.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2309.05257/assets/x5.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="422" height="212" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span id="S4.F6.3.1" class="ltx_text ltx_font_bold">(a) Visualization of the camera BEV features of BEVFormer and FusionFormer-Depth.</span> The BEV features obtained through FusionFormer-Depth are noticeably more focused on the target location than BEVFormer. <span id="S4.F6.4.2" class="ltx_text ltx_font_bold">(b) Illustrations of the fused BEV features of different fusion methods.</span> The car labeled in the image are not annotated in the ground truth because they are far away and the LiDAR captures fewer points. FusionFomer is capable of better integrating multimodal features and can detect distant objects using image information even when the point cloud is sparse.</figcaption>
</figure>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Ablation Study</h3>

<div id="S4.SS5.p1" class="ltx_para ltx_noindent">
<p id="S4.SS5.p1.3" class="ltx_p">In this section, we investigate the influence of each module on the performance of our proposed multi-modal fusion model for 3D detection. We adopted ResNet-50 <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a href="#bib.bib7" title="" class="ltx_ref">2016</a>)</cite> as the backbone for the image branch, with an input resolution of <math id="S4.SS5.p1.1.m1.1" class="ltx_Math" alttext="800{\times}320" display="inline"><semantics id="S4.SS5.p1.1.m1.1a"><mrow id="S4.SS5.p1.1.m1.1.1" xref="S4.SS5.p1.1.m1.1.1.cmml"><mn id="S4.SS5.p1.1.m1.1.1.2" xref="S4.SS5.p1.1.m1.1.1.2.cmml">800</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS5.p1.1.m1.1.1.1" xref="S4.SS5.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS5.p1.1.m1.1.1.3" xref="S4.SS5.p1.1.m1.1.1.3.cmml">320</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.1.m1.1b"><apply id="S4.SS5.p1.1.m1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1"><times id="S4.SS5.p1.1.m1.1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS5.p1.1.m1.1.1.2.cmml" xref="S4.SS5.p1.1.m1.1.1.2">800</cn><cn type="integer" id="S4.SS5.p1.1.m1.1.1.3.cmml" xref="S4.SS5.p1.1.m1.1.1.3">320</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.1.m1.1c">800{\times}320</annotation></semantics></math> for the image and a voxel size of <math id="S4.SS5.p1.2.m2.1" class="ltx_Math" alttext="0.1m" display="inline"><semantics id="S4.SS5.p1.2.m2.1a"><mrow id="S4.SS5.p1.2.m2.1.1" xref="S4.SS5.p1.2.m2.1.1.cmml"><mn id="S4.SS5.p1.2.m2.1.1.2" xref="S4.SS5.p1.2.m2.1.1.2.cmml">0.1</mn><mo lspace="0em" rspace="0em" id="S4.SS5.p1.2.m2.1.1.1" xref="S4.SS5.p1.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS5.p1.2.m2.1.1.3" xref="S4.SS5.p1.2.m2.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.2.m2.1b"><apply id="S4.SS5.p1.2.m2.1.1.cmml" xref="S4.SS5.p1.2.m2.1.1"><times id="S4.SS5.p1.2.m2.1.1.1.cmml" xref="S4.SS5.p1.2.m2.1.1.1"></times><cn type="float" id="S4.SS5.p1.2.m2.1.1.2.cmml" xref="S4.SS5.p1.2.m2.1.1.2">0.1</cn><ci id="S4.SS5.p1.2.m2.1.1.3.cmml" xref="S4.SS5.p1.2.m2.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.2.m2.1c">0.1m</annotation></semantics></math> for the point cloud branch, outputting <math id="S4.SS5.p1.3.m3.1" class="ltx_Math" alttext="150{\times}150" display="inline"><semantics id="S4.SS5.p1.3.m3.1a"><mrow id="S4.SS5.p1.3.m3.1.1" xref="S4.SS5.p1.3.m3.1.1.cmml"><mn id="S4.SS5.p1.3.m3.1.1.2" xref="S4.SS5.p1.3.m3.1.1.2.cmml">150</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS5.p1.3.m3.1.1.1" xref="S4.SS5.p1.3.m3.1.1.1.cmml">×</mo><mn id="S4.SS5.p1.3.m3.1.1.3" xref="S4.SS5.p1.3.m3.1.1.3.cmml">150</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.3.m3.1b"><apply id="S4.SS5.p1.3.m3.1.1.cmml" xref="S4.SS5.p1.3.m3.1.1"><times id="S4.SS5.p1.3.m3.1.1.1.cmml" xref="S4.SS5.p1.3.m3.1.1.1"></times><cn type="integer" id="S4.SS5.p1.3.m3.1.1.2.cmml" xref="S4.SS5.p1.3.m3.1.1.2">150</cn><cn type="integer" id="S4.SS5.p1.3.m3.1.1.3.cmml" xref="S4.SS5.p1.3.m3.1.1.3">150</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.3.m3.1c">150{\times}150</annotation></semantics></math> BEV features. It is noteworthy that, all the experiments presented in this section were based on single frame without incorporating temporal fusion techniques. The models were trained for 24 epochs without utilizing the CBGS <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a href="#bib.bib43" title="" class="ltx_ref">2019</a>)</cite> strategy.</p>
</div>
<section id="S4.SS5.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">LiDAR Features.</h4>

<div id="S4.SS5.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS5.SSS0.Px1.p1.1" class="ltx_p">In order to evaluate the impact of fusing voxel features from point cloud, we conducted experiments by comparing the model’s performance with LiDAR features using BEV and voxel representations. Table <a href="#S4.SS5.SSS0.Px1" title="LiDAR Features. ‣ 4.5 Ablation Study ‣ 4 Experiments ‣ FusionFormer: A Multi-sensory Fusion in Bird’s-Eye-View and Temporal Consistent Transformer for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.5</span></a> presents the results of all models. In contrast to inputting LiDAR features in the form of BEV, the use of voxel input format leads to superior model performance. Notably, the prediction errors for object center location and orientation are significantly reduced. This may be attributed to the preservation of more object structural information of the Z-axis in the voxel format, resulting in more accurate detection outcomes.</p>
</div>
<div id="S4.SS5.SSS0.Px1.2" class="ltx_logical-block ltx_minipage ltx_align_middle" style="width:397.5pt;">
<figure id="S4.SS5.SSS0.Px1.1.fig1" class="ltx_figure ltx_minipage ltx_align_top" style="width:190.8pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 5: </span>Study for the representation of the LiDAR feature on the nuScenes val set.</figcaption>
<table id="S4.SS5.SSS0.Px1.1.fig1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.SS5.SSS0.Px1.1.fig1.1.1.1" class="ltx_tr">
<th id="S4.SS5.SSS0.Px1.1.fig1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">LiDAR</th>
<th id="S4.SS5.SSS0.Px1.1.fig1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAP↑</th>
<th id="S4.SS5.SSS0.Px1.1.fig1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">NDS↑</th>
<th id="S4.SS5.SSS0.Px1.1.fig1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mATE↓</th>
<th id="S4.SS5.SSS0.Px1.1.fig1.1.1.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t">mAOE↓</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.SS5.SSS0.Px1.1.fig1.1.2.1" class="ltx_tr">
<th id="S4.SS5.SSS0.Px1.1.fig1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">BEV</th>
<td id="S4.SS5.SSS0.Px1.1.fig1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">61.3</td>
<td id="S4.SS5.SSS0.Px1.1.fig1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">66.1</td>
<td id="S4.SS5.SSS0.Px1.1.fig1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">35.7</td>
<td id="S4.SS5.SSS0.Px1.1.fig1.1.2.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">36.9</td>
</tr>
<tr id="S4.SS5.SSS0.Px1.1.fig1.1.3.2" class="ltx_tr">
<th id="S4.SS5.SSS0.Px1.1.fig1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">Voxel</th>
<td id="S4.SS5.SSS0.Px1.1.fig1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.SS5.SSS0.Px1.1.fig1.1.3.2.2.1" class="ltx_text ltx_font_bold">62.7</span></td>
<td id="S4.SS5.SSS0.Px1.1.fig1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.SS5.SSS0.Px1.1.fig1.1.3.2.3.1" class="ltx_text ltx_font_bold">67.3</span></td>
<td id="S4.SS5.SSS0.Px1.1.fig1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.SS5.SSS0.Px1.1.fig1.1.3.2.4.1" class="ltx_text ltx_font_bold">34.4</span></td>
<td id="S4.SS5.SSS0.Px1.1.fig1.1.3.2.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_b"><span id="S4.SS5.SSS0.Px1.1.fig1.1.3.2.5.1" class="ltx_text ltx_font_bold">31.4</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.SS5.SSS0.Px1.2.fig2" class="ltx_figure ltx_minipage ltx_align_top" style="width:190.8pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 6: </span>Ablation study of the modality fusion module on the nuScenes val set.</figcaption>
<table id="S4.SS5.SSS0.Px1.2.fig2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.SS5.SSS0.Px1.2.fig2.1.1.1" class="ltx_tr">
<th id="S4.SS5.SSS0.Px1.2.fig2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Fusion Method</th>
<th id="S4.SS5.SSS0.Px1.2.fig2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAP↑</th>
<th id="S4.SS5.SSS0.Px1.2.fig2.1.1.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t">NDS↑</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.SS5.SSS0.Px1.2.fig2.1.2.1" class="ltx_tr">
<th id="S4.SS5.SSS0.Px1.2.fig2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Addition</th>
<td id="S4.SS5.SSS0.Px1.2.fig2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">59.3</td>
<td id="S4.SS5.SSS0.Px1.2.fig2.1.2.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">64.6</td>
</tr>
<tr id="S4.SS5.SSS0.Px1.2.fig2.1.3.2" class="ltx_tr">
<th id="S4.SS5.SSS0.Px1.2.fig2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">concatenation</th>
<td id="S4.SS5.SSS0.Px1.2.fig2.1.3.2.2" class="ltx_td ltx_align_center">59.2</td>
<td id="S4.SS5.SSS0.Px1.2.fig2.1.3.2.3" class="ltx_td ltx_nopad_r ltx_align_center">64.5</td>
</tr>
<tr id="S4.SS5.SSS0.Px1.2.fig2.1.4.3" class="ltx_tr">
<th id="S4.SS5.SSS0.Px1.2.fig2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">Ours</th>
<td id="S4.SS5.SSS0.Px1.2.fig2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.SS5.SSS0.Px1.2.fig2.1.4.3.2.1" class="ltx_text ltx_font_bold">62.7</span></td>
<td id="S4.SS5.SSS0.Px1.2.fig2.1.4.3.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_b"><span id="S4.SS5.SSS0.Px1.2.fig2.1.4.3.3.1" class="ltx_text ltx_font_bold">67.3</span></td>
</tr>
</tbody>
</table>
</figure>
</div>
</section>
<section id="S4.SS5.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Modality Fusion.</h4>

<div id="S4.SS5.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS5.SSS0.Px2.p1.1" class="ltx_p">We conducted a comparative analysis of our proposed modality fusion method with other fusion methods to evaluate their performance. In the case of the fusion methods of addition and concatenation, the image BEV features were obtained through BEVFormer<cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib15" title="" class="ltx_ref">2022c</a>)</cite>. The experimental results are presented in Table <a href="#S4.SS5.SSS0.Px1" title="LiDAR Features. ‣ 4.5 Ablation Study ‣ 4 Experiments ‣ FusionFormer: A Multi-sensory Fusion in Bird’s-Eye-View and Temporal Consistent Transformer for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.5</span></a>. As shown in Figure <a href="#S4.F6" title="Figure 6 ‣ 4.4 Robustness Study ‣ 4 Experiments ‣ FusionFormer: A Multi-sensory Fusion in Bird’s-Eye-View and Temporal Consistent Transformer for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> (b), compared to other fusion methods, the fused BEV features obtained through FusionFormer exhibit a stronger response to the targets. Specifically, the distant cars labeled in the image are excluded from the ground truth (GT) annotations due to the limited points captured by LiDAR. Consequently, conventional multimodal fusion methods, such as simple addition and concatenation, fail to effectively incorporate these distant objects. In contrast, our proposed method, FusionFormer, enables enhanced fusion of multimodal features. It leverages the complementary information from image data to detect distant objects even in scenarios with sparse point cloud data.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">In this paper, we propose a novel transformer-based framework with a uniform sampling strategy that overcomes the limitations of existing multi-modality frameworks. Our approach eliminates the need for compressing voxel features into BEV space before fusion with image features, resulting in superior performance. We demonstrate the versatility of our method by transforming it into a camera-only 3D object detector, utilizing image features obtained through monocular depth estimation instead of LiDAR features. Our method achieves state-of-the-art performance in the 3D object detection task on the nuScenes dataset. In future, we will explore the applications of FusionFormer in other tasks, such as map segmentation.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. (2022)</span>
<span class="ltx_bibblock">
Xuyang Bai, Zeyu Hu, Xinge Zhu, Qingqiu Huang, Yilun Chen, Hongbo Fu, and Chiew-Lan Tai.

</span>
<span class="ltx_bibblock">Transfusion: Robust lidar-camera fusion for 3d object detection with transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  1090–1099, 2022.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caesar et al. (2020)</span>
<span class="ltx_bibblock">
Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom.

</span>
<span class="ltx_bibblock">nuscenes: A multimodal dataset for autonomous driving.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  11621–11631, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et al. (2023)</span>
<span class="ltx_bibblock">
Hongxiang Cai, Zeyuan Zhang, Zhenyu Zhou, Ziyin Li, Wenbo Ding, and Jiuhua Zhao.

</span>
<span class="ltx_bibblock">Bevfusion4d: Learning lidar-camera fusion under bird’s-eye-view via cross-modality guidance and temporal aggregation.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.17099</em>, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2017)</span>
<span class="ltx_bibblock">
Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia.

</span>
<span class="ltx_bibblock">Multi-view 3d object detection network for autonomous driving.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</em>, pp.  1907–1915, 2017.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023)</span>
<span class="ltx_bibblock">
Yukang Chen, Jianhui Liu, Xiangyu Zhang, Xiaojuan Qi, and Jiaya Jia.

</span>
<span class="ltx_bibblock">Voxelnext: Fully sparse voxelnet for 3d object detection and tracking.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  21674–21683, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Contributors (2020)</span>
<span class="ltx_bibblock">
MMDetection3D Contributors.

</span>
<span class="ltx_bibblock">MMDetection3D: OpenMMLab next-generation platform for general 3D object detection.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/open-mmlab/mmdetection3d" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/open-mmlab/mmdetection3d</a>, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2016)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pp.  770–778, 2016.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang &amp; Huang (2022)</span>
<span class="ltx_bibblock">
Junjie Huang and Guan Huang.

</span>
<span class="ltx_bibblock">Bevdet4d: Exploit temporal cues in multi-camera 3d object detection.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.17054</em>, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2021)</span>
<span class="ltx_bibblock">
Junjie Huang, Guan Huang, Zheng Zhu, Yun Ye, and Dalong Du.

</span>
<span class="ltx_bibblock">Bevdet: High-performance multi-camera 3d object detection in bird-eye-view.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.11790</em>, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lang et al. (2019)</span>
<span class="ltx_bibblock">
Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom.

</span>
<span class="ltx_bibblock">Pointpillars: Fast encoders for object detection from point clouds.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  12697–12705, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee &amp; Park (2020)</span>
<span class="ltx_bibblock">
Youngwan Lee and Jongyoul Park.

</span>
<span class="ltx_bibblock">Centermask: Real-time anchor-free instance segmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  13906–13915, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022a)</span>
<span class="ltx_bibblock">
Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni, and Lei Zhang.

</span>
<span class="ltx_bibblock">Dn-detr: Accelerate detr training by introducing query denoising.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  13619–13627, 2022a.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022b)</span>
<span class="ltx_bibblock">
Yanwei Li, Yilun Chen, Xiaojuan Qi, Zeming Li, Jian Sun, and Jiaya Jia.

</span>
<span class="ltx_bibblock">Unifying voxel-based representation with transformer for 3d object detection.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 35:18442–18455, 2022b.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Yinhao Li, Zheng Ge, Guanyi Yu, Jinrong Yang, Zengran Wang, Yukang Shi, Jianjian Sun, and Zeming Li.

</span>
<span class="ltx_bibblock">Bevdepth: Acquisition of reliable depth for multi-view 3d object detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 37, pp.  1477–1485, 2023.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022c)</span>
<span class="ltx_bibblock">
Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, and Jifeng Dai.

</span>
<span class="ltx_bibblock">Bevformer: Learning bird’s-eye-view representation from multi-camera images via spatiotemporal transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>, pp.  1–18. Springer, 2022c.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. (2022)</span>
<span class="ltx_bibblock">
Tingting Liang, Hongwei Xie, Kaicheng Yu, Zhongyu Xia, Zhiwei Lin, Yongtao Wang, Tao Tang, Bing Wang, and Zhi Tang.

</span>
<span class="ltx_bibblock">Bevfusion: A simple and robust lidar-camera fusion framework.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 35:10421–10434, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2017)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.

</span>
<span class="ltx_bibblock">Feature pyramid networks for object detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pp.  2117–2125, 2017.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2022)</span>
<span class="ltx_bibblock">
Xuewu Lin, Tianwei Lin, Zixiang Pei, Lichao Huang, and Zhizhong Su.

</span>
<span class="ltx_bibblock">Sparse4d: Multi-view 3d object detection with sparse spatial-temporal fusion.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.10581</em>, 2022.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022a)</span>
<span class="ltx_bibblock">
Yingfei Liu, Tiancai Wang, Xiangyu Zhang, and Jian Sun.

</span>
<span class="ltx_bibblock">Petr: Position embedding transformation for multi-view 3d object detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pp.  531–548. Springer, 2022a.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022b)</span>
<span class="ltx_bibblock">
Yingfei Liu, Junjie Yan, Fan Jia, Shuailin Li, Aqi Gao, Tiancai Wang, Xiangyu Zhang, and Jian Sun.

</span>
<span class="ltx_bibblock">Petrv2: A unified framework for 3d perception from multi-camera images.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2206.01256</em>, 2022b.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang, Huizi Mao, Daniela L Rus, and Song Han.

</span>
<span class="ltx_bibblock">Bevfusion: Multi-task multi-sensor fusion with unified bird’s-eye view representation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">2023 IEEE International Conference on Robotics and Automation (ICRA)</em>, pp.  2774–2781. IEEE, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meyer &amp; Kuschk (2019)</span>
<span class="ltx_bibblock">
Michael Meyer and Georg Kuschk.

</span>
<span class="ltx_bibblock">Automotive radar dataset for deep learning based 3d object detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">2019 16th european radar conference (EuRAD)</em>, pp.  129–132. IEEE, 2019.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meyer et al. (2021)</span>
<span class="ltx_bibblock">
Michael Meyer, Georg Kuschk, and Sven Tomforde.

</span>
<span class="ltx_bibblock">Graph convolutional networks for 3d object detection on radar data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  3060–3069, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paigwar et al. (2019)</span>
<span class="ltx_bibblock">
Anshul Paigwar, Ozgur Erkent, Christian Wolf, and Christian Laugier.

</span>
<span class="ltx_bibblock">Attentional pointnet for 3d-object detection in point clouds.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</em>, pp.  0–0, 2019.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. (2021)</span>
<span class="ltx_bibblock">
Dennis Park, Rares Ambrus, Vitor Guizilini, Jie Li, and Adrien Gaidon.

</span>
<span class="ltx_bibblock">Is pseudo-lidar needed for monocular 3d object detection?

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  3142–3152, 2021.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. (2022)</span>
<span class="ltx_bibblock">
Jinhyung Park, Chenfeng Xu, Shijia Yang, Kurt Keutzer, Kris Kitani, Masayoshi Tomizuka, and Wei Zhan.

</span>
<span class="ltx_bibblock">Time will tell: New outlooks and a baseline for temporal multi-view 3d object detection.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.02443</em>, 2022.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Philion &amp; Fidler (2020)</span>
<span class="ltx_bibblock">
Jonah Philion and Sanja Fidler.

</span>
<span class="ltx_bibblock">Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIV 16</em>, pp.  194–210. Springer, 2020.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et al. (2018)</span>
<span class="ltx_bibblock">
Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J Guibas.

</span>
<span class="ltx_bibblock">Frustum pointnets for 3d object detection from rgb-d data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pp.  918–927, 2018.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vora et al. (2020)</span>
<span class="ltx_bibblock">
Sourabh Vora, Alex H Lang, Bassam Helou, and Oscar Beijbom.

</span>
<span class="ltx_bibblock">Pointpainting: Sequential fusion for 3d object detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  4604–4612, 2020.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021a)</span>
<span class="ltx_bibblock">
Chunwei Wang, Chao Ma, Ming Zhu, and Xiaokang Yang.

</span>
<span class="ltx_bibblock">Pointaugmenting: Cross-modal augmentation for 3d object detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  11794–11803, 2021a.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023a)</span>
<span class="ltx_bibblock">
Haiyang Wang, Hao Tang, Shaoshuai Shi, Aoxue Li, Zhenguo Li, Bernt Schiele, and Liwei Wang.

</span>
<span class="ltx_bibblock">Unitr: A unified and efficient multi-modal transformer for bird’s-eye-view representation.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.07732</em>, 2023a.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020)</span>
<span class="ltx_bibblock">
Jun Wang, Shiyi Lan, Mingfei Gao, and Larry S Davis.

</span>
<span class="ltx_bibblock">Infofocus: 3d object detection for autonomous driving with dynamic information modeling.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part X 16</em>, pp.  405–420. Springer, 2020.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023b)</span>
<span class="ltx_bibblock">
Shihao Wang, Yingfei Liu, Tiancai Wang, Ying Li, and Xiangyu Zhang.

</span>
<span class="ltx_bibblock">Exploring object-centric temporal modeling for efficient multi-view 3d object detection.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.11926</em>, 2023b.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021b)</span>
<span class="ltx_bibblock">
Tai Wang, Xinge Zhu, Jiangmiao Pang, and Dahua Lin.

</span>
<span class="ltx_bibblock">Fcos3d: Fully convolutional one-stage monocular 3d object detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  913–922, 2021b.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Yue Wang, Vitor Campagnolo Guizilini, Tianyuan Zhang, Yilun Wang, Hang Zhao, and Justin Solomon.

</span>
<span class="ltx_bibblock">Detr3d: 3d object detection from multi-view images via 3d-to-2d queries.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Conference on Robot Learning</em>, pp.  180–191. PMLR, 2022.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. (2022)</span>
<span class="ltx_bibblock">
Enze Xie, Zhiding Yu, Daquan Zhou, Jonah Philion, Anima Anandkumar, Sanja Fidler, Ping Luo, and Jose M Alvarez.

</span>
<span class="ltx_bibblock">M <sup id="bib.bib36.2.1" class="ltx_sup"><span id="bib.bib36.2.1.1" class="ltx_text ltx_font_italic">2</span></sup> bev: Multi-camera joint 3d detection and segmentation with unified birds-eye view representation.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.05088</em>, 2022.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2021)</span>
<span class="ltx_bibblock">
Shaoqing Xu, Dingfu Zhou, Jin Fang, Junbo Yin, Zhou Bin, and Liangjun Zhang.

</span>
<span class="ltx_bibblock">Fusionpainting: Multimodal fusion with adaptive attention for 3d object detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">2021 IEEE International Intelligent Transportation Systems Conference (ITSC)</em>, pp.  3047–3054. IEEE, 2021.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al. (2023)</span>
<span class="ltx_bibblock">
Junjie Yan, Yingfei Liu, Jianjian Sun, Fan Jia, Shuailin Li, Tiancai Wang, and Xiangyu Zhang.

</span>
<span class="ltx_bibblock">Cross modal transformer via coordinates encoding for 3d object dectection.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.01283</em>, 2023.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2022)</span>
<span class="ltx_bibblock">
Zeyu Yang, Jiaqi Chen, Zhenwei Miao, Wei Li, Xiatian Zhu, and Li Zhang.

</span>
<span class="ltx_bibblock">Deepinteraction: 3d object detection via modality interaction.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 35:1992–2005, 2022.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al. (2021)</span>
<span class="ltx_bibblock">
Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl.

</span>
<span class="ltx_bibblock">Center-based 3d object detection and tracking.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  11784–11793, 2021.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023)</span>
<span class="ltx_bibblock">
Kaicheng Yu, Tang Tao, Hongwei Xie, Zhiwei Lin, Tingting Liang, Bing Wang, Peng Chen, Dayang Hao, Yongtao Wang, and Xiaodan Liang.

</span>
<span class="ltx_bibblock">Benchmarking the robustness of lidar-camera fusion for 3d object detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  3187–3197, 2023.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou &amp; Tuzel (2018)</span>
<span class="ltx_bibblock">
Yin Zhou and Oncel Tuzel.

</span>
<span class="ltx_bibblock">Voxelnet: End-to-end learning for point cloud based 3d object detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pp.  4490–4499, 2018.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2019)</span>
<span class="ltx_bibblock">
Benjin Zhu, Zhengkai Jiang, Xiangxin Zhou, Zeming Li, and Gang Yu.

</span>
<span class="ltx_bibblock">Class-balanced grouping and sampling for point cloud 3d object detection.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.09492</em>, 2019.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2020)</span>
<span class="ltx_bibblock">
Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai.

</span>
<span class="ltx_bibblock">Deformable detr: Deformable transformers for end-to-end object detection.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.04159</em>, 2020.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Multi-modal Branches</h3>

<section id="A1.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Camera Branch.</h4>

<div id="A1.SS1.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="A1.SS1.SSS0.Px1.p1.1" class="ltx_p">To extract image features from multi-view camera images, a backbone network, such as ResNet-50, is employed. These image features are then processed by a feature pyramid network (FPN), which generates multi-scale image features.</p>
</div>
</section>
<section id="A1.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">LiDAR Branch.</h4>

<div id="A1.SS1.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="A1.SS1.SSS0.Px2.p1.1" class="ltx_p">FusionFormer is designed to accommodate diverse representations of multi-modal features. This study explores two different representation forms of LiDAR features, specifically BEV and voxel features. The original point cloud data is voxelized, and then processed through sparse 3D convolution operations. In one case, voxel features are obtained by encoding the volumetric representation using 3D convolution operations. In another case, the Z-axis of the features are compressed into the channel dimension, and BEV features are obtained with 2D convolution operations.</p>
</div>
</section>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Efficiency Study</h3>

<div id="A1.SS2.p1" class="ltx_para ltx_noindent">
<p id="A1.SS2.p1.2" class="ltx_p">As shown in Table <a href="#A1.T7" title="Table 7 ‣ A.2 Efficiency Study ‣ Appendix A Appendix ‣ FusionFormer: A Multi-sensory Fusion in Bird’s-Eye-View and Temporal Consistent Transformer for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, we compare the efficiency of FusionFormer and existing methods. The FPS and performance are tested on a single Tesla A100 GPU with the best model setting of official repositories. In comparison to BEVFusion, FusionFormer demonstrates superior performance with notable improvements of <math id="A1.SS2.p1.1.m1.1" class="ltx_Math" alttext="3.1\%" display="inline"><semantics id="A1.SS2.p1.1.m1.1a"><mrow id="A1.SS2.p1.1.m1.1.1" xref="A1.SS2.p1.1.m1.1.1.cmml"><mn id="A1.SS2.p1.1.m1.1.1.2" xref="A1.SS2.p1.1.m1.1.1.2.cmml">3.1</mn><mo id="A1.SS2.p1.1.m1.1.1.1" xref="A1.SS2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.1.m1.1b"><apply id="A1.SS2.p1.1.m1.1.1.cmml" xref="A1.SS2.p1.1.m1.1.1"><csymbol cd="latexml" id="A1.SS2.p1.1.m1.1.1.1.cmml" xref="A1.SS2.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="A1.SS2.p1.1.m1.1.1.2.cmml" xref="A1.SS2.p1.1.m1.1.1.2">3.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.1.m1.1c">3.1\%</annotation></semantics></math> in mAP and <math id="A1.SS2.p1.2.m2.1" class="ltx_Math" alttext="2.7\%" display="inline"><semantics id="A1.SS2.p1.2.m2.1a"><mrow id="A1.SS2.p1.2.m2.1.1" xref="A1.SS2.p1.2.m2.1.1.cmml"><mn id="A1.SS2.p1.2.m2.1.1.2" xref="A1.SS2.p1.2.m2.1.1.2.cmml">2.7</mn><mo id="A1.SS2.p1.2.m2.1.1.1" xref="A1.SS2.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.2.m2.1b"><apply id="A1.SS2.p1.2.m2.1.1.cmml" xref="A1.SS2.p1.2.m2.1.1"><csymbol cd="latexml" id="A1.SS2.p1.2.m2.1.1.1.cmml" xref="A1.SS2.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="A1.SS2.p1.2.m2.1.1.2.cmml" xref="A1.SS2.p1.2.m2.1.1.2">2.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.2.m2.1c">2.7\%</annotation></semantics></math> in NDS, while maintaining a similar processing speed. These results highlight the significant advancements achieved by FusionFormer in the field of object detection.</p>
</div>
<figure id="A1.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Efficiency comparison on the nuScenes val set. ”L” is LiDAR. ”C” is camera. ”T” is temporal. The ”-S” indicates that the model only utilizes single-frame BEV features without incorporating temporal fusion techniques.</figcaption>
<table id="A1.T7.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T7.1.1.1" class="ltx_tr">
<th id="A1.T7.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Methods</th>
<th id="A1.T7.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Modality</th>
<td id="A1.T7.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t">mAP↑</td>
<td id="A1.T7.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NDS↑</td>
<td id="A1.T7.1.1.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">FPS</td>
</tr>
<tr id="A1.T7.1.2.2" class="ltx_tr">
<th id="A1.T7.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">TransFusion</th>
<th id="A1.T7.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">CL</th>
<td id="A1.T7.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">67.5</td>
<td id="A1.T7.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">71.3</td>
<td id="A1.T7.1.2.2.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">3.2</td>
</tr>
<tr id="A1.T7.1.3.3" class="ltx_tr">
<th id="A1.T7.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">BEVFusion</th>
<th id="A1.T7.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">CL</th>
<td id="A1.T7.1.3.3.3" class="ltx_td ltx_align_center">68.5</td>
<td id="A1.T7.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r">71.4</td>
<td id="A1.T7.1.3.3.5" class="ltx_td ltx_nopad_r ltx_align_center">4.2</td>
</tr>
<tr id="A1.T7.1.4.4" class="ltx_tr">
<th id="A1.T7.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">UVTR</th>
<th id="A1.T7.1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">CL</th>
<td id="A1.T7.1.4.4.3" class="ltx_td ltx_align_center">65.4</td>
<td id="A1.T7.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r">70.2</td>
<td id="A1.T7.1.4.4.5" class="ltx_td ltx_nopad_r ltx_align_center">2.6</td>
</tr>
<tr id="A1.T7.1.5.5" class="ltx_tr">
<th id="A1.T7.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">CMT</th>
<th id="A1.T7.1.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">CL</th>
<td id="A1.T7.1.5.5.3" class="ltx_td ltx_align_center">70.3</td>
<td id="A1.T7.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r">72.9</td>
<td id="A1.T7.1.5.5.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A1.T7.1.5.5.5.1" class="ltx_text ltx_font_bold">6.0</span></td>
</tr>
<tr id="A1.T7.1.6.6" class="ltx_tr">
<th id="A1.T7.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">DeepInteraction</th>
<th id="A1.T7.1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">CL</th>
<td id="A1.T7.1.6.6.3" class="ltx_td ltx_align_center">69.8</td>
<td id="A1.T7.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r">72.6</td>
<td id="A1.T7.1.6.6.5" class="ltx_td ltx_nopad_r ltx_align_center">1.7</td>
</tr>
<tr id="A1.T7.1.7.7" class="ltx_tr">
<th id="A1.T7.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">FusionFormer-S</th>
<th id="A1.T7.1.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">CL</th>
<td id="A1.T7.1.7.7.3" class="ltx_td ltx_align_center ltx_border_t">70.0</td>
<td id="A1.T7.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">73.2</td>
<td id="A1.T7.1.7.7.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">4.0</td>
</tr>
<tr id="A1.T7.1.8.8" class="ltx_tr">
<th id="A1.T7.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r">FusionFormer</th>
<th id="A1.T7.1.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">CLT</th>
<td id="A1.T7.1.8.8.3" class="ltx_td ltx_align_center ltx_border_b"><span id="A1.T7.1.8.8.3.1" class="ltx_text ltx_font_bold">71.4</span></td>
<td id="A1.T7.1.8.8.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="A1.T7.1.8.8.4.1" class="ltx_text ltx_font_bold">74.1</span></td>
<td id="A1.T7.1.8.8.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_b">3.8</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Qualitative Detection Results</h3>

<div id="A1.SS3.p1" class="ltx_para ltx_noindent">
<p id="A1.SS3.p1.1" class="ltx_p">In this section, we showcase further detection results of FusionFormer on the nuScenes test set. As depicted in Figure <a href="#A1.F7" title="Figure 7 ‣ A.3 Qualitative Detection Results ‣ Appendix A Appendix ‣ FusionFormer: A Multi-sensory Fusion in Bird’s-Eye-View and Temporal Consistent Transformer for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, FusionFormer exhibits exceptional performance in detecting objects at long distances. Notably, the incorporation of the temporal fusion module enables FusionFormer to effectively recall occluded objects by leveraging the fused historical frame BEV features. This capability proves valuable in scenarios where objects may be partially or fully obstructed. The presented results highlight the robustness and effectiveness of FusionFormer in addressing the challenges of object detection in complex environments.</p>
</div>
<figure id="A1.F7" class="ltx_figure"><img src="/html/2309.05257/assets/x6.png" id="A1.F7.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="422" height="587" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><span id="A1.F7.6.1" class="ltx_text ltx_font_bold">More qualitative detection results in the nuScenes test set.</span> Bounding boxes with different colors represent Cars(<span id="A1.F7.7.2" class="ltx_text" style="color:#FF9E00;">•</span>), Pedestrians(<span id="A1.F7.8.3" class="ltx_text" style="color:#0000E6;">•</span>), Bus(<span id="A1.F7.9.4" class="ltx_text" style="color:#FF7F50;">•</span>) and Truck(<span id="A1.F7.10.5" class="ltx_text" style="color:#FF6347;">•</span>).</figcaption>
</figure>
</section>
<section id="A1.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Additional Ablation Study</h3>

<div id="A1.SS4.p1" class="ltx_para ltx_noindent">
<p id="A1.SS4.p1.3" class="ltx_p">In this section, we investigate the influence of other factors on the performance of FusionFormer. We adopt ResNet-50 as the backbone for the image branch, with an input resolution of <math id="A1.SS4.p1.1.m1.1" class="ltx_Math" alttext="800{\times}320" display="inline"><semantics id="A1.SS4.p1.1.m1.1a"><mrow id="A1.SS4.p1.1.m1.1.1" xref="A1.SS4.p1.1.m1.1.1.cmml"><mn id="A1.SS4.p1.1.m1.1.1.2" xref="A1.SS4.p1.1.m1.1.1.2.cmml">800</mn><mo lspace="0.222em" rspace="0.222em" id="A1.SS4.p1.1.m1.1.1.1" xref="A1.SS4.p1.1.m1.1.1.1.cmml">×</mo><mn id="A1.SS4.p1.1.m1.1.1.3" xref="A1.SS4.p1.1.m1.1.1.3.cmml">320</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS4.p1.1.m1.1b"><apply id="A1.SS4.p1.1.m1.1.1.cmml" xref="A1.SS4.p1.1.m1.1.1"><times id="A1.SS4.p1.1.m1.1.1.1.cmml" xref="A1.SS4.p1.1.m1.1.1.1"></times><cn type="integer" id="A1.SS4.p1.1.m1.1.1.2.cmml" xref="A1.SS4.p1.1.m1.1.1.2">800</cn><cn type="integer" id="A1.SS4.p1.1.m1.1.1.3.cmml" xref="A1.SS4.p1.1.m1.1.1.3">320</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS4.p1.1.m1.1c">800{\times}320</annotation></semantics></math> for the image and a voxel size of <math id="A1.SS4.p1.2.m2.1" class="ltx_Math" alttext="0.1m" display="inline"><semantics id="A1.SS4.p1.2.m2.1a"><mrow id="A1.SS4.p1.2.m2.1.1" xref="A1.SS4.p1.2.m2.1.1.cmml"><mn id="A1.SS4.p1.2.m2.1.1.2" xref="A1.SS4.p1.2.m2.1.1.2.cmml">0.1</mn><mo lspace="0em" rspace="0em" id="A1.SS4.p1.2.m2.1.1.1" xref="A1.SS4.p1.2.m2.1.1.1.cmml">​</mo><mi id="A1.SS4.p1.2.m2.1.1.3" xref="A1.SS4.p1.2.m2.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.SS4.p1.2.m2.1b"><apply id="A1.SS4.p1.2.m2.1.1.cmml" xref="A1.SS4.p1.2.m2.1.1"><times id="A1.SS4.p1.2.m2.1.1.1.cmml" xref="A1.SS4.p1.2.m2.1.1.1"></times><cn type="float" id="A1.SS4.p1.2.m2.1.1.2.cmml" xref="A1.SS4.p1.2.m2.1.1.2">0.1</cn><ci id="A1.SS4.p1.2.m2.1.1.3.cmml" xref="A1.SS4.p1.2.m2.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS4.p1.2.m2.1c">0.1m</annotation></semantics></math> for the point cloud branch, outputting <math id="A1.SS4.p1.3.m3.1" class="ltx_Math" alttext="150{\times}150" display="inline"><semantics id="A1.SS4.p1.3.m3.1a"><mrow id="A1.SS4.p1.3.m3.1.1" xref="A1.SS4.p1.3.m3.1.1.cmml"><mn id="A1.SS4.p1.3.m3.1.1.2" xref="A1.SS4.p1.3.m3.1.1.2.cmml">150</mn><mo lspace="0.222em" rspace="0.222em" id="A1.SS4.p1.3.m3.1.1.1" xref="A1.SS4.p1.3.m3.1.1.1.cmml">×</mo><mn id="A1.SS4.p1.3.m3.1.1.3" xref="A1.SS4.p1.3.m3.1.1.3.cmml">150</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS4.p1.3.m3.1b"><apply id="A1.SS4.p1.3.m3.1.1.cmml" xref="A1.SS4.p1.3.m3.1.1"><times id="A1.SS4.p1.3.m3.1.1.1.cmml" xref="A1.SS4.p1.3.m3.1.1.1"></times><cn type="integer" id="A1.SS4.p1.3.m3.1.1.2.cmml" xref="A1.SS4.p1.3.m3.1.1.2">150</cn><cn type="integer" id="A1.SS4.p1.3.m3.1.1.3.cmml" xref="A1.SS4.p1.3.m3.1.1.3">150</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS4.p1.3.m3.1c">150{\times}150</annotation></semantics></math> BEV features. With the exception of the temporal fusion section, all other experiments presented in this section are based on single frame.</p>
</div>
<div id="A1.SS4.1" class="ltx_sectional-block ltx_minipage">
<div id="A1.SS4.1.1" class="ltx_sectional-block ltx_minipage">
<section id="A1.SS4.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Temporal Fusion.</h4>

<div id="A1.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="A1.SS4.SSS0.Px1.p1.1" class="ltx_p">We conducted a comparative study between our proposed temporal fusion module and the concatenation method used by prior temporal fusion approaches under different temporal sequences. All models were trained for 24 epochs and the CBGS strategy was employed during the training process. The experimental results are presented in Table <a href="#A1.SS4" title="A.4 Additional Ablation Study ‣ Appendix A Appendix ‣ FusionFormer: A Multi-sensory Fusion in Bird’s-Eye-View and Temporal Consistent Transformer for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.4</span></a>. Compared to the previous temporal fusion method using channel concatenation, our deformable attention-based temporal fusion method demonstrates better performance in 3D object detection.</p>
</div>
</section>
</div>
<figure id="A1.SS4.1.fig1" class="ltx_figure ltx_minipage ltx_align_top" style="width:190.8pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 8: </span>Study of the temporal fusion module on the nuScenes val set.</figcaption>
<table id="A1.SS4.1.fig1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.SS4.1.fig1.1.1.1" class="ltx_tr">
<th id="A1.SS4.1.fig1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<th id="A1.SS4.1.fig1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2"><span id="A1.SS4.1.fig1.1.1.1.2.1" class="ltx_text ltx_font_bold">Concate</span></th>
<th id="A1.SS4.1.fig1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2"><span id="A1.SS4.1.fig1.1.1.1.3.1" class="ltx_text ltx_font_bold">Ours</span></th>
</tr>
<tr id="A1.SS4.1.fig1.1.2.2" class="ltx_tr">
<th id="A1.SS4.1.fig1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">T</th>
<th id="A1.SS4.1.fig1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAP↑</th>
<th id="A1.SS4.1.fig1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">NDS↑</th>
<th id="A1.SS4.1.fig1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAP↑</th>
<th id="A1.SS4.1.fig1.1.2.2.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t">NDS↑</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.SS4.1.fig1.1.3.1" class="ltx_tr">
<th id="A1.SS4.1.fig1.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">1</th>
<td id="A1.SS4.1.fig1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">66.48</td>
<td id="A1.SS4.1.fig1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">70.39</td>
<td id="A1.SS4.1.fig1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">66.48</td>
<td id="A1.SS4.1.fig1.1.3.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">70.39</td>
</tr>
<tr id="A1.SS4.1.fig1.1.4.2" class="ltx_tr">
<th id="A1.SS4.1.fig1.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">2</th>
<td id="A1.SS4.1.fig1.1.4.2.2" class="ltx_td ltx_align_center">67.71</td>
<td id="A1.SS4.1.fig1.1.4.2.3" class="ltx_td ltx_align_center">71.01</td>
<td id="A1.SS4.1.fig1.1.4.2.4" class="ltx_td ltx_align_center">67.85</td>
<td id="A1.SS4.1.fig1.1.4.2.5" class="ltx_td ltx_nopad_r ltx_align_center">71.20</td>
</tr>
<tr id="A1.SS4.1.fig1.1.5.3" class="ltx_tr">
<th id="A1.SS4.1.fig1.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">4</th>
<td id="A1.SS4.1.fig1.1.5.3.2" class="ltx_td ltx_align_center">68.18</td>
<td id="A1.SS4.1.fig1.1.5.3.3" class="ltx_td ltx_align_center">71.36</td>
<td id="A1.SS4.1.fig1.1.5.3.4" class="ltx_td ltx_align_center">68.24</td>
<td id="A1.SS4.1.fig1.1.5.3.5" class="ltx_td ltx_nopad_r ltx_align_center">71.51</td>
</tr>
<tr id="A1.SS4.1.fig1.1.6.4" class="ltx_tr">
<th id="A1.SS4.1.fig1.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b">8</th>
<td id="A1.SS4.1.fig1.1.6.4.2" class="ltx_td ltx_align_center ltx_border_b">68.31</td>
<td id="A1.SS4.1.fig1.1.6.4.3" class="ltx_td ltx_align_center ltx_border_b">71.49</td>
<td id="A1.SS4.1.fig1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_b">68.56</td>
<td id="A1.SS4.1.fig1.1.6.4.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_b">71.66</td>
</tr>
</tbody>
</table>
</figure>
</div>
<div id="A1.SS4.2" class="ltx_sectional-block ltx_minipage">
<div id="A1.SS4.2.1" class="ltx_sectional-block ltx_minipage">
<section id="A1.SS4.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">CBGS.</h4>

<div id="A1.SS4.SSS0.Px2.p1" class="ltx_para">
<p id="A1.SS4.SSS0.Px2.p1.1" class="ltx_p">We evaluated the impact of utilizing the class-balanced grouping and sampling (CBGS) strategy during the training process on the model’s performance. Table <a href="#A1.SS4" title="A.4 Additional Ablation Study ‣ Appendix A Appendix ‣ FusionFormer: A Multi-sensory Fusion in Bird’s-Eye-View and Temporal Consistent Transformer for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.4</span></a> presents the results of this comparison. The application of the CBGS strategy resulted in a balanced distribution of samples across different categories, leading to a notable enhancement in the performance of the model.</p>
</div>
</section>
</div>
<figure id="A1.SS4.2.fig1" class="ltx_figure ltx_minipage ltx_align_top" style="width:190.8pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 9: </span>Ablation study of the CBGS strategy on the nuScenes val set.</figcaption>
<table id="A1.SS4.2.fig1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.SS4.2.fig1.1.1.1" class="ltx_tr">
<th id="A1.SS4.2.fig1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">CBGS</th>
<th id="A1.SS4.2.fig1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAP↑</th>
<th id="A1.SS4.2.fig1.1.1.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t">NDS↑</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.SS4.2.fig1.1.2.1" class="ltx_tr">
<td id="A1.SS4.2.fig1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="A1.SS4.2.fig1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">66.5</td>
<td id="A1.SS4.2.fig1.1.2.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">70.4</td>
</tr>
<tr id="A1.SS4.2.fig1.1.3.2" class="ltx_tr">
<td id="A1.SS4.2.fig1.1.3.2.1" class="ltx_td ltx_border_b"></td>
<td id="A1.SS4.2.fig1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_b">62.7</td>
<td id="A1.SS4.2.fig1.1.3.2.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_b">67.3</td>
</tr>
</tbody>
</table>
</figure>
</div>
<div id="A1.SS4.3" class="ltx_sectional-block ltx_minipage">
<div id="A1.SS4.3.1" class="ltx_sectional-block ltx_minipage">
<section id="A1.SS4.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Modality Order.</h4>

<div id="A1.SS4.SSS0.Px3.p1" class="ltx_para">
<p id="A1.SS4.SSS0.Px3.p1.1" class="ltx_p">In our proposed method, multi-modal features are sequentially fed into the fusion encoder at each layer. To evaluate the impact of the input order of multi-modal features, we compared the performance of the model with different input orders. The results are presented in Table <a href="#A1.SS4" title="A.4 Additional Ablation Study ‣ Appendix A Appendix ‣ FusionFormer: A Multi-sensory Fusion in Bird’s-Eye-View and Temporal Consistent Transformer for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.4</span></a>. All models were trained for 24 epochs without utilizing the CBGS strategy.</p>
</div>
</section>
</div>
<figure id="A1.SS4.3.fig1" class="ltx_figure ltx_minipage ltx_align_top" style="width:190.8pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 10: </span>Ablation study of the input order of modality features on the nuScenes val set.</figcaption>
<table id="A1.SS4.3.fig1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.SS4.3.fig1.1.1.1" class="ltx_tr">
<th id="A1.SS4.3.fig1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Order</th>
<th id="A1.SS4.3.fig1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAP↑</th>
<th id="A1.SS4.3.fig1.1.1.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t">NDS↑</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.SS4.3.fig1.1.2.1" class="ltx_tr">
<td id="A1.SS4.3.fig1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">LC</td>
<td id="A1.SS4.3.fig1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">62.7</td>
<td id="A1.SS4.3.fig1.1.2.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">67.3</td>
</tr>
<tr id="A1.SS4.3.fig1.1.3.2" class="ltx_tr">
<td id="A1.SS4.3.fig1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_b">CL</td>
<td id="A1.SS4.3.fig1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_b">62.5</td>
<td id="A1.SS4.3.fig1.1.3.2.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_b">67.1</td>
</tr>
</tbody>
</table>
</figure>
</div>
<div id="A1.SS4.4" class="ltx_sectional-block ltx_minipage">
<div id="A1.SS4.4.1" class="ltx_sectional-block ltx_minipage">
<section id="A1.SS4.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Voxel Size.</h4>

<div id="A1.SS4.SSS0.Px4.p1" class="ltx_para">
<p id="A1.SS4.SSS0.Px4.p1.1" class="ltx_p">We conducted an experiment to evaluate the impact of voxel size on the performance of our proposed method. The results are presented in Table <a href="#A1.SS4" title="A.4 Additional Ablation Study ‣ Appendix A Appendix ‣ FusionFormer: A Multi-sensory Fusion in Bird’s-Eye-View and Temporal Consistent Transformer for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.4</span></a>. The models were trained for 24 epochs and did not use the CBGS strategy.</p>
</div>
</section>
</div>
<figure id="A1.SS4.4.fig1" class="ltx_figure ltx_minipage ltx_align_top" style="width:190.8pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 11: </span>Ablation study of the voxel size on the nuScenes val set.</figcaption>
<table id="A1.SS4.4.fig1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.SS4.4.fig1.1.1.1" class="ltx_tr">
<th id="A1.SS4.4.fig1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Voxel size</th>
<th id="A1.SS4.4.fig1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAP↑</th>
<th id="A1.SS4.4.fig1.1.1.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t">NDS↑</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.SS4.4.fig1.1.2.1" class="ltx_tr">
<th id="A1.SS4.4.fig1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">0.075m</th>
<td id="A1.SS4.4.fig1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">63.2</td>
<td id="A1.SS4.4.fig1.1.2.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">67.8</td>
</tr>
<tr id="A1.SS4.4.fig1.1.3.2" class="ltx_tr">
<th id="A1.SS4.4.fig1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">0.100m</th>
<td id="A1.SS4.4.fig1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_b">62.5</td>
<td id="A1.SS4.4.fig1.1.3.2.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_b">67.1</td>
</tr>
</tbody>
</table>
</figure>
</div>
<div id="A1.SS4.6" class="ltx_sectional-block ltx_minipage">
<div id="A1.SS4.6.3" class="ltx_sectional-block ltx_minipage">
<section id="A1.SS4.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Image Size.</h4>

<div id="A1.SS4.SSS0.Px5.p1" class="ltx_para">
<p id="A1.SS4.SSS0.Px5.p1.1" class="ltx_p">We conducted an experiment to evaluate the impact of image size on the performance of our proposed method. The results are presented in Table <a href="#A1.SS4" title="A.4 Additional Ablation Study ‣ Appendix A Appendix ‣ FusionFormer: A Multi-sensory Fusion in Bird’s-Eye-View and Temporal Consistent Transformer for 3D Object Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.4</span></a>. The models were trained for 24 epochs and did not use the CBGS strategy.</p>
</div>
</section>
</div>
<figure id="A1.SS4.6.2" class="ltx_figure ltx_minipage ltx_align_top" style="width:190.8pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 12: </span>Ablation study of the image size on the nuScenes val set.</figcaption>
<table id="A1.SS4.6.2.2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.SS4.6.2.2.2.3.1" class="ltx_tr">
<th id="A1.SS4.6.2.2.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Image size</th>
<th id="A1.SS4.6.2.2.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAP↑</th>
<th id="A1.SS4.6.2.2.2.3.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t">NDS↑</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.SS4.5.1.1.1.1" class="ltx_tr">
<th id="A1.SS4.5.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><math id="A1.SS4.5.1.1.1.1.1.m1.1" class="ltx_Math" alttext="1600{\times}600" display="inline"><semantics id="A1.SS4.5.1.1.1.1.1.m1.1a"><mrow id="A1.SS4.5.1.1.1.1.1.m1.1.1" xref="A1.SS4.5.1.1.1.1.1.m1.1.1.cmml"><mn id="A1.SS4.5.1.1.1.1.1.m1.1.1.2" xref="A1.SS4.5.1.1.1.1.1.m1.1.1.2.cmml">1600</mn><mo lspace="0.222em" rspace="0.222em" id="A1.SS4.5.1.1.1.1.1.m1.1.1.1" xref="A1.SS4.5.1.1.1.1.1.m1.1.1.1.cmml">×</mo><mn id="A1.SS4.5.1.1.1.1.1.m1.1.1.3" xref="A1.SS4.5.1.1.1.1.1.m1.1.1.3.cmml">600</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS4.5.1.1.1.1.1.m1.1b"><apply id="A1.SS4.5.1.1.1.1.1.m1.1.1.cmml" xref="A1.SS4.5.1.1.1.1.1.m1.1.1"><times id="A1.SS4.5.1.1.1.1.1.m1.1.1.1.cmml" xref="A1.SS4.5.1.1.1.1.1.m1.1.1.1"></times><cn type="integer" id="A1.SS4.5.1.1.1.1.1.m1.1.1.2.cmml" xref="A1.SS4.5.1.1.1.1.1.m1.1.1.2">1600</cn><cn type="integer" id="A1.SS4.5.1.1.1.1.1.m1.1.1.3.cmml" xref="A1.SS4.5.1.1.1.1.1.m1.1.1.3">600</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS4.5.1.1.1.1.1.m1.1c">1600{\times}600</annotation></semantics></math></th>
<td id="A1.SS4.5.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t">64.4</td>
<td id="A1.SS4.5.1.1.1.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">68.1</td>
</tr>
<tr id="A1.SS4.6.2.2.2.2" class="ltx_tr">
<th id="A1.SS4.6.2.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b"><math id="A1.SS4.6.2.2.2.2.1.m1.1" class="ltx_Math" alttext="800{\times}320" display="inline"><semantics id="A1.SS4.6.2.2.2.2.1.m1.1a"><mrow id="A1.SS4.6.2.2.2.2.1.m1.1.1" xref="A1.SS4.6.2.2.2.2.1.m1.1.1.cmml"><mn id="A1.SS4.6.2.2.2.2.1.m1.1.1.2" xref="A1.SS4.6.2.2.2.2.1.m1.1.1.2.cmml">800</mn><mo lspace="0.222em" rspace="0.222em" id="A1.SS4.6.2.2.2.2.1.m1.1.1.1" xref="A1.SS4.6.2.2.2.2.1.m1.1.1.1.cmml">×</mo><mn id="A1.SS4.6.2.2.2.2.1.m1.1.1.3" xref="A1.SS4.6.2.2.2.2.1.m1.1.1.3.cmml">320</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS4.6.2.2.2.2.1.m1.1b"><apply id="A1.SS4.6.2.2.2.2.1.m1.1.1.cmml" xref="A1.SS4.6.2.2.2.2.1.m1.1.1"><times id="A1.SS4.6.2.2.2.2.1.m1.1.1.1.cmml" xref="A1.SS4.6.2.2.2.2.1.m1.1.1.1"></times><cn type="integer" id="A1.SS4.6.2.2.2.2.1.m1.1.1.2.cmml" xref="A1.SS4.6.2.2.2.2.1.m1.1.1.2">800</cn><cn type="integer" id="A1.SS4.6.2.2.2.2.1.m1.1.1.3.cmml" xref="A1.SS4.6.2.2.2.2.1.m1.1.1.3">320</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS4.6.2.2.2.2.1.m1.1c">800{\times}320</annotation></semantics></math></th>
<td id="A1.SS4.6.2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_b">62.5</td>
<td id="A1.SS4.6.2.2.2.2.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_b">67.1</td>
</tr>
</tbody>
</table>
</figure>
</div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2309.05256" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2309.05257" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2309.05257">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2309.05257" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2309.05258" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 06:29:26 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
